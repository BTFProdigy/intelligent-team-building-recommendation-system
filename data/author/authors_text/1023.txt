Examining the Role of Statistical and Linguistic Knowledge 
Sources in a General-Knowledge Question-Answering System 
Cla i re  Card ie  1 and V incent  Ng  1 and Dav id  P ie rce  1 and Chr i s  Buck ley  2 
Depar tment  of Computer  Science, Cornell  University, I thaca, NY 148531 
SaB IR  Research 2 
E-mail: cardie,yung,pierce@cs.cornel l .edu, chr isb@sabir .com 
Abstract 
We describe and evaluate an implemented system 
for general-knowledge question answering. The sys- 
tem combines techniques for standard ad-hoc infor- 
mation retrieval (IR), query-dependent text summa- 
rization, and shallow syntactic and semantic sen- 
tence analysis. In a series of experiments we examine 
the role of each statistical and linguistic knowledge 
source in the question-answering system. In con- 
trast to previous results, we find first that statisti- 
cal knowledge of word co-occurrences a computed 
by IR vector space methods can be used to quickly 
and accurately locate the relevant documents for 
each question. The use of query-dependent text 
summarization techniques, however, provides only 
small increases in performance and severely limits 
recall levels when inaccurate. Nevertheless, it is the 
text summarization component that allows subse- 
quent linguistic filters to focus on relevant passages. 
We find that even very weak linguistic knowledge 
can offer substantial improvements over purely IR- 
based techniques for question answering, especially 
when smoothly integrated with statistical prefer- 
ences computed by the IR subsystems. 
1 In t roduct ion  
In this paper, we describe and evaluate an imple- 
mented system for general-knowledge question an- 
swering. Open-ended question-answering systems 
that allow users to pose a question of any type, in 
any language, without domain restrictions, remain 
beyond the scope of today's text-processing systems. 
We investigate instead a restricted, but nevertheless 
useful variation of the problem (TREC-8, 2000): 
Given a large text collection and a set of 
questions specified in English, find answers 
to the questions in the collection. 
In addition, the restricted task guarantees that: 
? the answer exists in the collection, 
? all supporting information for the answer lies in 
a single document, and 
? the answer is short m less than 50 bytes in 
length. 
Consider, for example, the question Which country 
has the largest part of the Amazon rain forest?, taken 
from the TREC8 Question Answering development 
corpus. The answer (in document LA032590-0089) 
is Brazil 
Previous research as addressed similar question- 
answering (QA) scenarios using a variety of natu- 
ral language processing (NLP) and information re- 
trieval (IR) techniques. Lehnert (1978) tackles the 
difficult task of answering questions in the context of 
story understanding. Unlike our restricted QA task, 
questions to Lehnert's ystem often require answers 
that are not explicitly mentioned in the story. Her 
goal then is to answer questions by making infer- 
ences about actions and actors in the story using 
world knowledge in the form of scripts, plans, and 
goals (Schank and Abelson, 1977). More recently, 
Burke et al (1995; 1997) describe a system that an- 
swers natural anguage questions using a database of 
question-answer pairs built from existing frequently- 
asked question (FAQ) files. Their FAQFinder sys- 
tem uses IR techniques to match the given question 
to questions in the database. It then uses the Word- 
Net lexical semantic knowledge base (Miller et al, 
1990; Fellbaum, 1998) to improve the quality of the 
match. 
Kupiec (1993) investigates a closed-class QA task 
that is similar in many respects to the TREC8 
QA task that we address here: the system answers 
general-knowledge questions using an encyclopedia. 
In addition, Kupiec assumes that all answers are 
noun phrases. Although our task does not explic- 
itly include a "noun phrase" constraint, the answer 
length restriction effectively imposes the same bias 
toward noun phrase answers. Kupiec's MURAX sys- 
tem applies a combination of statistical (IR) and 
linguistic (NLP) techniques. A series of secondary 
boolean search queries with proximity constraints i
combined with shallow parsing methods to find rele- 
vant sections of the encyclopedia, to extract answer 
hypotheses, and to confirm phrase relations speci- 
fied in the question. In an evaluation on 70 "Trivial 
180 
question 
document 
collection 
Retrieval i documents, i 
i text passages i 
i .-i 
i 
IR Subsystems i 
Summarization 
Parsing 
Semantic 
Types 
Linguistic 
Relationships 
Linguistic Filters 
answer 
hypotheses 
! 
Figure 1: General Architecture of the Question-Answering System 
Pursuit" who and what questions, Kupiec concludes 
that robust natural language analysis can add to the 
quality of the information retrieval process. In addi- 
tion, he claims that, for their closed-class QA task, 
vector space IR methods (Salton et al, 1975) appear 
inadequate. 
We present here a new approach to the re- 
stricted question-answering task described above. 
Like MURAX, our system draws from both statisti- 
cal and linguistic sources to find answers to general- 
knowledge questions. The underlying architecture of 
the system, however, is very different: it combines 
vector space IR techniques for document retrieval, a
vector space approach to query-dependent text sum- 
marization, shallow corpus-based syntactic analysis, 
and knowledge-based semantic analysis. We eval- 
uate the system on the TREC8 QA development 
corpus as well as the TREC8 QA test corpus. In 
particular, all parameters for the final QA system 
are determined using the development corpus. Our 
current results are encouraging but not outstanding: 
the system is able to correctly answer 22 out of 38 of 
the development questions and 91 out of 200 of the 
test questions given five guesses for each question. 
Furthermore, the first guess is correct for 16 out of 
the 22 development questions and 53 out of 91 of the 
test questions. 
More importantly, we investigate the relative role 
of each statistical and linguistic knowledge source 
in the proposed IR/NLP question-answering system. 
In contrast o previous results, we find that sta- 
tistical knowledge of word co-occurrences a com- 
puted by vector space models of IR can be used to 
quickly and accurately ocate relevant documents in
the restricted QA task. When used in isolation, vec- 
tor space methods for query-dependent text summa- 
rization, however, provide relatively small increases 
in performance. In addition, we find that the text 
summarization component can severely limit recall 
levels. Nevertheless, it is the summarization compo- 
nent that allows the linguistic filters to focus on rele- 
vant passages. In particular, we find that very weak 
linguistic knowledge can offer substantial improve- 
ments over purely IR-based techniques for question 
answering, especially when smoothly integrated with 
the statistical preferences computed by the IR sub- 
systems. 
In the next section, we describe the general archi- 
tecture of the question-answering system. Section 3 
describes the baseline system and its information re- 
trieval component. Sections 4-7 describe and evalu- 
ate a series of variations to the baseline system that 
incorporate, in turn, query-dependent text summa- 
rization, a syntactic filter, a semantic filter, and an 
algorithm that allows syntactic knowledge to influ- 
ence the initial ordering of summary extracts. Sec- 
tion 8 compares our approach to some of those in 
the recent TREC8 QA evaluation (TREC-8, 2000) 
and describes directions for future work. 
2 System Arch i tec ture  
The basic architecture of the question-answering sys- 
tem is depicted in Figure 1. It contains two main 
components: the IR subsystems and the linguistic 
filters. As a preliminary, ofl\]ine step, the IR sub- 
system first indexes the text collection from which 
answers are to be extracted. Given a question, the 
goal of the IR component is then to return a ranked 
list of those text chunks (e.g. documents, entences, 
or paragraphs) from the indexed collection that are 
most relevant o the query and from which answer 
hypotheses can he extracted. Next, the QA system 
optionally applies one or more linguistic filters to 
the text chunks to extract an ordered list of answer 
hypotheses. The top hypotheses are concatenated to 
form five 50-byte guesses as allowed by the TREC8 
guidelines. Note that many of these guesses may 
be difficult to read and judged as incorrect by the 
181 
TREC8 assessors: we will also describe the results 
of generating single phrases as guesses wherever this 
is possible. 
In the sections below, we present and evaluate a
series of instantiations of this general architecture, 
each of which makes different assumptions regarding 
the type of information that will best support he 
QA task. The next section begins by describing the 
baseline QA system. 
3 The Vector Space Model for 
Document Retrieval 
It is clear that a successful QA system will need 
some way to find the documents hat are most rele- 
vant to the user's question. In a baseline system, we 
assume that standard IR techniques can be used for 
this task. In contrast to MURAX, however, we hy- 
pothesize that the vector space retrieval model will 
suffice. In the vector space model, both the ques- 
tion and the documents are represented asvectors 
with one entry for every unique word that appears 
in the collection. Each entry is the term weight, a 
real number that indicates the presence or absence 
of the word in the text. The similarity between a
question vector, Q = ql ,q2,. . .  ,qn, and a document 
vector, D = dl, d2,. . . ,  tin, is traditionally computed 
using a cosine similarity measure: 
n 
8im(Q,D)  = Z d, .q, 
i..~ l
Using this measure, the IR system returns a ranked 
list of those documents most similar to the question. 
The  Baseline QA System: The  Smart  Vec- 
tor  Space Model .  For the IR component of the 
baseline QA system, we use Smart (Salton, 1971), 
a sophisticated text-processing system based on the 
vector space model and employed as the retrieval 
engine for a number of the top-performing systems 
at recent Text REtrieval Conferences (e.g. Buckley 
et al, 1998a, 1998b). Given a question, Smart re- 
turns a ranked list of the documents most relevant 
to the question. For the baseline QA system and all 
subsequent variations, we use Smart with standard 
term-weighting strategies I and do not use automatic 
relevance f edback (Buckley, 1995). In addition, the 
baseline system applies no linguistic filters. To gen- 
erate answers for a particular question, the system 
starts at the beginning of the top-ranked ocument 
returned by Smart for the question and constructs 
five 50-byte chunks consisting of document text with 
stopwords removed. 
lWe use Lnu term weighting for documents and Itu term 
weighting for the question (Singhal et al, 1996). 
Evaluation. As noted above, we evaluate ach 
variation of our QA system on 38 TREC8 devel- 
opment questions and 200 TREC8 test questions. 
The indexed collection is TREC disks 4 and 5 (with- 
out Congressional Records). Results for the baseline 
Smart IR QA system are shown in the first row of 
Table 1. The system gets 3 out of 38 development 
questions and 29 out of 200 test questions correct. 
We judge the system correct if any of the five guesses 
contains each word of one of the answers. The final 
column of results hows the mean answer ank across 
all questions correctly answered. 
Smart is actually performing much better than its 
scores would suggest. For 18 of the 38 development 
questions, the answer appears in the top-ranked doc- 
ument; for 33 questions, the answer appears in one 
of the top seven documents. For only two questions 
does Smart fail to retrieve a good document in the 
top 25 documents. For the test corpus, over half 
of the 200 questions are answered in the top-ranked 
document (110); over 75% of the questions (155) are 
answered in top five documents. Only 19 questions 
were not answered in the top 20 documents. 
4 Query-Dependent Text 
Summar izat ion  fo r  Quest ion  
Answering 
We next hypothesize that query-dependent text 
summarization algorithms will improve the perfor- 
mance of the QA system by focusing the system 
on the most relevant portions of the retrieved oc- 
uments. The goal for query-dependent summariza- 
tion algorithms is to provide a short summary of 
a document with respect to a specific query. Al- 
though a number of methods for query-dependent 
text summarization are beginning to be developed 
and evaluated in a variety of realistic settings (Mani 
et al, 1999), we again propose the use of vector space 
methods from IR, which can be easily extended to 
the summarization task (Salton et al, 1994): 
1. Given a question and a document, divide the 
document into chunks (e.g. sentences, para- 
graphs, 200-word passages). 
2. Generate the vector epresentation forthe ques- 
tion and for each document chunk. 
3. Use the cosine similarity measure to determine 
the similarity of each chunk to the question. 
4. Return as the query-dependent summary the 
most similar chunks up to a predetermined sum- 
mary length (e.g. 10% or 20% of the original 
document). 
This approach to text summarization was shown 
to be quite successful in the recent SUMMAC eval- 
uation of text summarization systems (Mani et al, 
1999; Buckley et al, 1999). Our general assumption 
182 
here is that Ii~ approaches can be used to quickly 
and accurately find both relevant documents and 
relevant document portions. In related work, Chali 
et al (1999) also propose text summarization tech- 
niques as a primary component for their QA system. 
They employ a combination of vector-space meth- 
ods and lexical chaining to derive their sentence- 
based summaries. We hypothesize that  deeper anal- 
ysis of the summary extracts is better accomplished 
by methods from NLP that can determine syntac- 
tic and semantic relationships between relevant con- 
stituents. There is a risk in using query-dependent 
summaries to focus the search for answer hypothe- 
ses, however: if the summarization algorithm is inac- 
curate, the desired answers will occur outside of the 
summaries and will not be accessible to subsequent 
components of the QA system. 
The Query -Dependent  Text  Summar izat ion  
QA System.  In the next version of the QA sys- 
tem, we augment he baseline system to perform 
query-dependent text summarization for the top k 
retrieved ocuments. More specifically, the IR sub- 
system returns the summary extracts (sentences or 
paragraphs) for the top k documents after sort- 
ing them according to their cosine similarity scores 
w.r.t, the question. As before, no linguistic filters are 
applied, and answers are generated by constructing 
50-byte chunks from the ordered extracts after re- 
moving stopwords. In the experiments below, k = 7 
for the development questions and k = 6 for the test 
questions. 2 
Eva luat ion .  Results for the Text Summarization 
QA system using sentence-based summaries are 
shown in the second row of Table 1. Here we see 
a relatively small improvement: the system now 
answers four development and 45 test questions 
correctly. The mean answer rank, however, im- 
proves noticeably from 3.33 to 2.25 for the develop- 
ment corpus and from 3.07 to 2.67 for the test cor- 
pus. Paragraph-based summaries yield similar but 
slightly smaller improvements; as a result, sentence 
summaries are used exclusively in subsequent sec- 
tions. Unfortunately, the system's reliance on query- 
dependent text summarization actually limits its po- 
tential: in only 23 of the 38 development questions 
(61%), for example, does the correct answer appear 
in the summary for one of the top k -- 7 documents. 
The QA system cannot hope to answer correctly any 
of the remaining 15 questions. For only 135 of the 
200 questions in the test corpus (67.5%) does the 
correct answer appear in the summary for one of 
2The value for k was chosen so that at least 80% of the 
questions in the set had answers appearing in the retrieved 
documents ranked 1-k. We have not experimented exten- 
sively with many values of k and expect that better perfor- 
mance can be obtained by tuning k for each text collection. 
the top k -- 6 documents. 3 It is possible that au- 
tomatic relevance feedback or coreference r solution 
would improve performance. We are investigating 
these options in current work. 
The decision of whether or not to incorporate text 
summarization i the QA system depends, in part, 
on the ability of subsequent processing components 
(i.e. the linguistic filters) to locate answer hypothe- 
ses. If subsequent components are very good at 
discarding implausible answers, then summarization 
methods may limit system performance. Therefore, 
we investigate next the use of two linguistic filters in 
conjunction with the query-dependent text summa- 
rization methods evaluated here. 
5 Incorporat ing  the  Noun Phrase  
F i l te r  
The restricted QA task that we investigate requires 
answers to be short - -  no more than 50 bytes in 
length. This effectively eliminates how or why ques- 
tions from consideration. Almost all of the remain- 
ing question types are likely to have noun phrases as 
answers. In the TREC8 development corpus, for ex- 
ample, 36 of 38 questions have noun phrase answers. 
As a result, we next investigate the use of a 
very simple linguistic filter that considers only noun 
phrases as answer hypotheses. The filter operates on 
the ordered list of summary extracts for a particular 
question and produces a list of answer hypotheses, 
one for each noun phrase (NP) in the extracts in the 
left-to-right order in which they appeared. 
The  NP-based  QA System.  Our implementa- 
tion of the NP-based QA system uses the Empire 
noun phrase finder, which is described in detail in 
Cardie and Pierce (1998). Empire identifies base 
NPs - -  non-recursive noun phrases - -  using a very 
simple algorithm that matches part-of-speech tag se- 
quences based on a learned noun phrase grammar. 
The approach is able to achieve 94% precision and 
recall for base NPs derived from the Penn Treebank 
Wall Street Journal (Marcus et al, 1993). In the 
experiments below, the NP filter follows the applica- 
tion of the document retrieval and text summariza- 
tion components. Pronoun answer hypotheses are 
discarded, and the NPs are assembled into 50-byte 
chunks. 
Eva luat ion.  Results for the NP-based QA sys- 
tem are shown in the third row of Table 1. The 
noun phrase filter markedly improves system per- 
formance for the development corpus, nearly dou- 
3Paragraph-based summaries provide better coverage on 
the test corpus than sentence-based summaries: for 151 ques- 
tions, the correct answer appears in the summary for one of 
the top k documents. This suggests that paragraph sum- 
maries might be better suited for use with more sophisticated 
linguistic filters that are capable of discerning the answer in 
the larger summary. 
1~"~ 183
Development Corpus Test Corpus 
Smart Vector Space Model 
Query-Dependent Text Summarization 
Text Summarization + NPs 
Text Summarization + NPs + Semantic Type 
Text Summarization with Syntactic Ordering + 
NPs + Semantic Type 
Correct (%) MAR 
3/38 0.079 3.33 
4/38 0.105 2.25 
7/38 0.184 2.29 
21/38 0.553 1.38 
22/38 0.579 1.32 
Correct(%) MAR 
29/200 0.145 3.07 
45/200 0.225 2.67 
50/200 0.250 2.66 
86/200 0.430 1.90 
91/200 0.455 1.82 
Table 1: Evaluation of the Role of Statistical and Limited Linguistic Knowledge for the TREC8 Question 
Answering Task. Results for 38 development and 200 test questions are shown. The mean answer ank 
(MAR) is computed w.r.t, all questions correctly answered. 
bling the number of questions answered correctly. 
We found these results somewhat surprising since 
this linguistic filter is rather weak: we expected it
to work well only in combination with the semantic 
filter described below. The noun phrase filter has 
much less of an effect on the test corpus, improving 
performance on questions answered from 45 to 50. 
In a separate experiment, we applied the NP filter 
to the baseline system that includes no text summa? 
rization component. Here the NP filter does not 
improve performance - - the system gets only two 
questions correct. This indicates that the NP filter 
depends critically on the text summarization com- 
ponent. As a result, we will continue to use query- 
dependent text summarization i  the experiments 
below. 
The NP filter provides the first opportunity to 
look at single-phrase answers. The preceding QA 
systems produced answers that were rather unnat- 
urally chunked into 50-byte strings. When such 
chunking is disabled, only one development and 20 
test questions are answered. The difference in per- 
formance between the NP filter with chunking and 
the NP filter alone clearly indicates that the NP fil- 
ter is extracting ood guesses, but that subsequent 
linguistic processing is needed to promote the best 
guesses to the top of the ranked guess list. 
6 Incorporat ing  Semant ic  Type  
In fo rmat ion  
The NP filter does not explicitly consider the ques- 
tion in its search for noun phrase answers. It is clear, 
however, that a QA system must pay greater atten- 
tion to the syntactic and semantic onstraints spec- 
ified in the question. For example, a question like 
Who was president of the US in 19957 indicates 
that the answer is likely to be a person. In addition, 
there should be supporting evidence from the answer 
document that the person was president, and, more 
specifically, held this office in the US and in 1995. 
We introduce here a second linguistic filter that 
considers the primary semantic onstraint from the 
question. The filter begins by determining the ques- 
tion type, i.e. the semantic type requested in the 
question. It then takes the ordered set of summary 
extracts supplied by the IR subsytem, uses the syn- 
tactic filter from Section 5 to extract NPs, and gen- 
erates an answer hypothesis for every noun phrase 
that is semantically compatible with the question 
type. Our implementation of this semantic class fil- 
ter is described below. The filter currently makes no 
attempt to confirm other linguistic relations men- 
tioned in the question. 
The  Semantic Type  Checking QA System. 
For most questions, the question word itself deter- 
mines the semantic type of the answer. This is true 
for who, where, and when questions, for example, 
which request a person, place, and time expression 
as an answer. For many which and what questions, 
however, determining the question type requires ad- 
ditional syntactic analysis. For these, we currently 
extract the head noun in the question as the question 
type. For example, in Which country has the largest 
part o$ the Amazon rain :forest? we identify country 
as the question type. Our heuristics for determining 
question type were based on the development cor- 
pus and were designed to be general, but have not 
yet been directly evaluated on a separate question 
corpus. 
? Given the question type and an answer hypoth- 
esis, the Semantic Type Checking QA System then 
uses WordNet o check that an appropriate ancestor- 
descendent relationship holds. Given Brazil as an 
answer hypothesis for the above question, for exam- 
ple, Wordnet's type hierarchy confirms that Brazil 
is a subtype of country, allowing the system to con- 
clude that the semantic type of the answer hypoth- 
esis matches the question type. 
For words (mostly proper nouns) that do not ap- 
pear in WordNet, heuristics are used to determine 
semantic type. There are heuristics to recognize 
13 basic question types: Person, Location, Date, 
Month, Year, Time, Age, Weight, Area, Volume, 
Length, Amount, and Number. For Person ques- 
tions, for example, the system relies primarily on a 
rule that checks for capitalization and abbreviations 
' I IOA  184
in order to identify phrases that correspond to peo- 
ple. There are approximately 20 such rules that to- 
gether cover all 13 question types listed above. The 
rules effectively operate as a very simple named en- 
tity identifier. 
Eva luat ion .  Results for the Semantic Type 
Checking variation of the QA system are shown in 
the fourth row of Table 1. Here we see a dramatic 
increase in performance: the system answers three 
times as many development questions (21) correctly 
over the previous variation. This is especially en- 
couraging iven that the IR and text summarization 
components limit the maximum number correct o 
23. In addition, the mean answer rank improves 
from 2.29 to 1.38. A closer look at Table 1, however, 
indicates problems with the semantic type checking 
linguistic filter. While performance on the develop- 
ment corpus increases by 37 percentage points (from 
18.4% correct to 55.3% correct), relative gains for 
the test corpus are much smaller. There is only an 
improvement of 18 percentage points, from 25.0% 
correct (50/200) to 43.0% correct (86/200). This 
is a clear indication that the heuristics used in the 
semantic type checking component, which were de- 
signed based on the development corpus, do not gen- 
eralize well to different question sets. Replacing the 
current heuristics with a Named Entity identifica- 
tion component or learning the heuristics using stan- 
dard inductive learning techniques should help w i th  
the scalability of this linguistic filter. 
Nevertheless, it is somewhat surprising that very 
weak syntactic information (the NP filter) and weak 
semantic lass information (question type checking) 
can produce such improvements. In particular, it 
appears that it is reasonable to rely implicitly on 
the IR subsystems to enforce the other linguistic re- 
lationships pecified in the query (e.g. that Clinton 
is president, hat this office was held in the US and 
in 1995). 
Finally, when 50-byte chunking is disabled for 
the semantic type checking QA variation, there is 
a decrease in the number of questions correctly an- 
swered, to 19 and 57 for the development and test 
corpus, respectively. 
7 Syntact i c  P re ferences  fo r  Order ing  
Summary  Ext rac ts  
Syntactic and semantic linguistic knowledge has 
been used thus far as post-processing filters that lo- 
cate and confirm answer hypotheses from the statis- 
tically specified summary extracts. We hypothesized 
that further improvements might be made by allow- 
ing this linguistic knowledge to influence the initial 
ordering of text chunks for the linguistic filters. In a 
final system, we begin to investigate this claim. Our 
general approach is to define a new scoring mea- 
sure that operates on the summary extracts and can 
be used to reorder the extracts based on linguistic 
knowledge. 
The  QA System wi th  L inguist ic  Reorder ing  
o f  Summary  Ext racts .  As described above, our 
final version of the QA system ranks summary ex- 
tracts according to both their vector space similarity 
to the question as well as linguistic evidence that the 
answer lies within the extract. In particular, each 
summary extract E for question q is ranked accord- 
ing to a new score, Sq: 
sq(E) = w(E) . LRq(E) 
The intuition behind the new score is to prefer sum- 
mary extracts that exhibit the same linguistic rela- 
tionships as the question (as indicated by LRq) and 
to give more weight (as indicated by w) to linguistic 
relationship matches in extracts from higher-ranked 
documents. More specifically, LRq(E ) is the num- 
ber of linguistic relationships from the question that 
appear in E. In the experiments below, LRq(E) 
is just the number of base NPs from the question 
that appear in the summary extract. In future 
work, we plan to include other pairwise linguistic 
relationships (e.g. subject-verb relationships, verb- 
object relationships, pp-attachment relationships). 
The weight w(E) is a number between 0 and 1 that 
is based on the retrieval rank r of the document that 
contains E: 
w(E) = max(m, 1 - p. r) 
In our experiments, m = 0.5 and p = 0.1. Both 
values were selected manually based on the develop- 
ment corpus; an extensive search for the best such 
values was not done. 
The summary extracts are sorted according to the 
new scoring measure and the ranked list of sentences 
is provided to the linguistic filters as before. 
Eva luat ion .  Results for this final variation of the 
QA system are shown in the bottom row of Table 1. 
Here we see a fairly minor increase in performance 
over the use of linguistic filters alone: the system 
answers only one more question correctly than the 
previous variation for the development corpus and 
answers five additional questions for the test cor- 
pus. The mean answer rank improves only negligi- 
bly. Sixteen of the 22 correct answers (73%) appear 
as the top-ranked guess for the development corpus; 
only 53 out of 91 correct answers (58%) appear as 
the top-ranked guess for the test corpus. Unfortu- 
nately, when 50-byte chunking is disabled, system 
performance drops precipitously, by 5% (to 20 out 
of 38) for the development corpus and by 13% (to 
65 out of 200) for the test corpus. As noted above, 
this indicates that the filters are finding the answers, 
but more sophisticated linguistic sorting is needed 
to promote the best answers to the top. Through 
185 
its LRq term, the new scoring measure does pro- 
vide a mechanism for allowing other linguistic re- 
lationships to influence the initial ordering of sum- 
mary extracts. The current results, however, indi- 
cate that with only very weak syntactic information 
(i.e. base noun phrases), the new scoring measure 
is only marginally successful in reordering the sum- 
mary extracts based on syntactic information. 
As noted above, the final system (with the liberal 
50-byte answer chunker) correctly answers 22 out of 
38 questions for the development corpus. Of the 16 
errors, the text retrieval component is responsible for 
five (31.2%), the text summarization component for 
ten (62.5%), and the linguistic filters for one (6.3%). 
In this analysis we consider the linguistic filters re- 
sponsible for an error if they were unable to pro- 
mote an available answer hypothesis to one of the 
top five guesses. A slightly different situation arises 
for the test corpus: of the 109 errors, the text re- 
trieval component is responsible for 39 (35.8%), the 
text summarization component for 26 (23.9%), and 
the linguistic filters for 44 (40.4%). As discussed in 
Section 6, the heuristics that comprise the semantic 
type checking filter do not scale to the test corpus 
and are the primary reason for the larger percentage 
of errors attributed to the linguistic filters for that 
corpus. 
8 Re la ted  Work  and  Conc lus ions  
We have described and evaluated a series of 
question-answering systems, each of which incorpo- 
rates a different combination of statistical and lin- 
guistic knowledge sources. We find that even very 
weak linguistic knowledge can offer substantial im- 
provements over purely IR-based techniques espe- 
cially when smoothly integrated with the text pas- 
sage preferences computed by the IR subsystems. 
Although our primary goal was to investigate the 
use of statistical and linguistic knowledge sources, it 
is possible to compare our approach and our results 
to those for systems in the recent TREC8 QA evalu- 
ation. Scores on the TREC8 test corpus for systems 
participating in the QA evaluation ranged between 
3 and 146 correct. Discarding the top three scores 
and the worst three scores, the remaining eight sys- 
tems achieved between 52 and 91 correct. Using the 
liberal answer chunker, our final QA system equals 
the best of these systems (91 correct); without it, 
our score of 65 correct places our QA system near 
the middle of this group of eight. 
Like the work described here, virtually all of the 
top-ranked TREC8 systems use a combination of 
IR and shallow NLP for their QA systems. IBM's 
AnSel system (Prager et al, 2000), for example, 
employs finite-state patterns as its primary shallow 
NLP component. These are used to recognize a 
fairly broad set of about 20 named entities. The 
IR component indexes only text passages associ- 
ated with these entities. The AT&T QA system 
(Singhal et al, 2000), the Qanda system (Breck et 
al., 2000), and the SyncMatcher system (Oard et 
al., 2000) all employ vector-space methods from IR, 
named entity identifiers, and a fairly simple ques- 
tion type determiner. In addition, SyncMatcher 
uses a broad-coverage d pendency parser to enforce 
phrase relationship constraints. Instead of the vec- 
tor space model, the LASSO system (Moldovan et 
al., 2000) uses boolean search operators for para- 
graph retrieval. Recognition of answer hypotheses 
in their system relies on identifying named entities. 
Finally, the Cymphony QA system (Srihari and Li, 
2000) relies heavily on named entity identification; it 
also employs tandard IR techniques and a shallow 
parser. 
In terms of statistical and linguistic knowledge 
sources employed, the primary difference between 
these systems and ours is our lack of an adequate 
named entity tagger. Incorporation of such a tag- 
ger will be a focus of future work. In addition, we 
believe that the retrieval and summarization compo- 
nents can be improved by incorporating automatic 
relevance feedback (Buckley, 1995) and coreference 
resolution. Morton (1999), for example, shows that 
coreference r solution improves passage retrieval for 
their question-answering system. We also plan to 
reconsider paragraph-based summaries given their 
coverage on the test corpus. The most critical area 
for improvement, however, is the linguistic filters. 
The semantic type filter will be greatly improved by 
the addition of a named entity tagger, but we believe 
that additional gains can be attained by augmenting 
named entity identification with information from 
WordNet. Finally, we currently make no attempt to 
confirm any phrase relations from the query. With- 
out this, system performance will remain severely 
limited. 
9 Acknowledgments  
This work was supported in part by NSF Grants IRI- 
9624639 and GER-9454149. 
Re ferences  
E. Breck, J. Burger, L. Ferro, D. House, M. Light, 
and I. Mani. 2000. A Sys Called Qanda. In 
E. Voorhees, editor, Proceedings of the Eighth 
Text REtrieval Conference TREC 8. NIST Spe- 
cial Publication. In press. 
C. Buckley, M. Mitra, J. Walz, and C. Cardie. 
1998a. SMART high precision: TREC 7. In 
E. Voorhees, editor, Proceedings of the Seventh 
Text REtrieval Conference TREC 7, pages 285- 
298. NIST Special Publication 500-242. 
C. Buckley, M. Mitra, J. Walz, and C. Cardie. 
1998b. Using clustering and superconcepts within 
186 
SMART : TREC 6. In E. Voorhees, editor, Pro- 
ceedings of the Sixth Text REtrieval Conference 
TREC 6, pages 107-124. NIST Special Publica- 
tion 500-240. 
C. Buckley, C. Cardie, S. Mardis, M. Mitra, 
D. Pierce, K. Wagstaff, and J. Walz. 1999. The 
Smart/Empire TIPSTER IR System. In Proceed- 
ings, TIPSTER Text Program (Phase III). Mor- 
gan Kauhnann. To appear. 
Chris Buckley. 1995. Massive Query Expansion 
/or Relevance Feedback. Cornell University, Ph.D. 
Thesis, Ithaca, New York. 
R. Burke, K. Hammond, and J. Kozlovsky. 
1995. Knowledge-Based Information Retrieval 
from Semi-Structured Text. In Working Notes of 
the AAAI Fall Symposium on AI Applications in 
Knowledge Navigation and Retrieval, pages 19-24. 
AAAI Press. 
R. Burke, K. Hammond, V. Kulyukin, S. Lyti- 
hen, N. Tomuro, and S. Schoenberg. 1997. ques- 
tion answering from Frequently-Asked Question 
Files. Technical Report TR-97-05, University of 
Chicago. 
C. Cardie and D. Pierce. 1998. Error-Driven Prun- 
ing of Treebank Grammars for Base Noun Phrase 
Identification. In Proceedings of the 36th An- 
nual Meeting of the Association .for Computa- 
tional Linguistics and COLING-98, pages 218- 
224, University of Montreal, Montreal, Canada. 
Association for Computational Linguistics. 
Y. Chali, S. Matwin, and S. Szpakowicz. 1999. 
Query-Biased Text Summarization as a Question- 
Answering Technique. In Proceedings o.f the AAAI 
Fall Symposium on Question Answering Systems, 
pages 52-56. AAAI Press. AAAI TR FS-99-02. 
C. Fellbaum. 1998. WordNet: An Electronical Lex- 
iced Database. MIT Press, Cambridge, MA. 
J. Kupiec. 1993. MURAX: A Robust Linguistic ap- 
proach For Question Answering Using An On- 
Line Encyclopedia. In Proceedings of A CM SI- 
GIR, pages 181-190. 
W. Lehnert. 1978. The Process o/ Question Answer- 
ing. Lawrence Erlbaum Associates, Hillsdale, NJ. 
I. Mani, T. Firmin, D. House, G. Klein, B. Sund- 
heim, and L. Hirschman. 1999. The TIPSTER 
SUMMAC Text Summarization Evaluation. In 
Ninth Annual Meeting o.f the EACL, University 
of Bergen, Bergen, Norway. 
M. Marcus, M. Marcinkiewicz, and B. Santorini. 
1993. Building a Large Annotated Corpus of En- 
glish: The Penn Treebank. Computational Lin- 
guistics, 19(2):313-330. 
G. A. Miller, R. Beckwith, C. FeUbaum, D. Gross, 
and K. J. Miller. 1990. WordNet: an on-line lex- 
ical database. International Journal of Lexicogra- 
phy, 3(4):235-245. 
D. Moldovan, S. Harabagiu, M. Pa~ca, R. Mihal- 
cea, R. Goodrum, R. Girju, and V. Rus. 2000. 
LASSO: A Tool for Surfing the Answer Net. In 
E. Voorhees, editor, Proceedings of the Eighth 
Text REtrieval Conference TREC 8. NIST Spe- 
cial Publication. In press. 
T. S. Morton. 1999. Using Coreference to Im- 
prove Passage Retrieval for Question Answering. 
In Proceedings of the AAAI Fall Symposium on 
Question Answering Systems, pages 72-74. AAAI 
Press. AAAI TR FS-99-02. 
D. W. Oard, J. Wang, D. Lin, and I. Soboroff. 2000. 
TREC-8 Experiments at Maryland: CLIR, QA 
and Routing. In E. Voorhees, editor, Proceedings 
o.f the Eighth Text REtrieval Conference TREC 8. 
NIST Special Publication. In press. 
J. Prager, D. Radev, E. Brown, A. Coden, and 
V. Samn. 2000. The Use of Predictive Anno- 
tation for Question Answering in TRECS. In 
E. Voorhees, editor, Proceedings o/ the Eighth 
Text REtrieval Conference TREC 8. NIST Spe- 
cial Publication. In press. 
G. Salton, A. Wong, and C.S. Yang. 1975. A vector 
space model for information retrieval. Communi- 
cations o/the ACM, 18(11):613-620. 
G. Salton, J. Allan, C. Buckley, and M. Mitra. 1994. 
Automatic analysis, theme generation and sum- 
marization of machine-readable t xts. Science, 
264:1421-1426, June. 
Gerard Salton, editor. 1971. The SMART Re- 
trieval System--Experiments in Automatic Doc- 
ument Processing. Prentice Hall Inc., Englewood 
Cliffs, NJ. 
R. C. Schank and R. P. Abelson. 1977. Scripts, 
plans, goals, and understanding. Lawrence Erl- 
bantu Associates, Hillsdale, NJ. 
Amit Singhal, Chris Buckley, and Mandar Mitra. 
1996. Pivoted document length normalization. In 
H. Frei, D. Harman, P. Schauble, and R. Wilkin- 
son, editors, Proceedings o/the Nineteenth An- 
nual International ACM SIGIR Conference on 
Research and Development in Information Re- 
trieval, pages 21-29. Association for Computing 
Machinery. 
A. Singhal, S. Abney, M. Bacchiani, M. Collins, 
D. Hindle, and F. Pereira. 2000. AT&T at TREC- 
8. In E. Voorhees, editor, Proceedings of the 
Eighth Text REtrieval Conference TREC 8. NIST 
Special Publication. In press. 
R. Srihari and W. Li. 2000. Question Answer- 
ing Supported by Information Extraction. In 
E. Voorhees, editor, Proceedings of the Eighth 
Text REtrieval Conference TREC 8. NIST Spe- 
cial Publication. In press. 
TREC-8. 2000. Proceedings of the Eighth Text RE- 
trieval Conference TREC 8. NIST. In press. 
1Q'7  187
 
		Playing the Telephone Game: Determining the Hierarchical Structure of
Perspective and Speech Expressions
Eric Breck and Claire Cardie
Department of Computer Science
Cornell University
Ithaca, NY 14853
USA
ebreck,cardie@cs.cornell.edu
Abstract
News articles report on facts, events, and opin-
ions with the intent of conveying the truth.
However, the facts, events, and opinions appear-
ing in the text are often known only second-
or third-hand, and as any child who has played
?telephone? knows, this relaying of facts often
garbles the original message. Properly under-
standing the information filtering structures that
govern the interpretation of these facts, then, is
critical to appropriately analyzing them. In this
work, we present a learning approach that cor-
rectly determines the hierarchical structure of
information filtering expressions 78.30% of the
time.
1 Introduction
Newswire text has long been a primary target for
natural language processing (NLP) techniques such
as information extraction, summarization, and ques-
tion answering (e.g. MUC (1998); NIS (2003);
DUC (2003)). However, newswire does not offer
direct access to facts, events, and opinions; rather,
journalists report what they have experienced, and
report on the experiences of others. That is, facts,
events, and opinions are filtered by the point of
view of the writer and other sources. Unfortu-
nately, this filtering of information through multiple
sources (and multiple points of view) complicates
the natural language interpretation process because
the reader (human or machine) must take into ac-
count the biases introduced by this indirection. It
is important for understanding both newswire and
narrative text (Wiebe, 1994), therefore, to appropri-
ately recognize expressions of point of view, and to
associate them with their direct and indirect sources.
This paper introduces two kinds of expression
that can filter information. First, we define a per-
spective expression to be the minimal span of text
that denotes the presence of an explicit opinion,
evaluation, emotion, speculation, belief, sentiment,
etc.1 Private state is the general term typically used
1Note that implicit expressions of perspective, i.e. Wiebe et
to refer to these mental and emotional states that
cannot be directly observed or verified (Quirk et al,
1985). Further, we define the source of a perspec-
tive expression to be the experiencer of that private
state, that is, the person or entity whose opinion
or emotion is being conveyed in the text. Second,
speech expressions simply convey the words of an-
other individual ? and by the choice of words, the
reporter filters the original source?s intent. Consider
for example, the following sentences (in which per-
spective expressions are denoted in bold, speech ex-
pressions are underlined, and sources are denoted in
italics):
1. Charlie was angry at Alice?s claim that Bob was
unhappy.
2. Philip Clapp, president of the National Environ-
ment Trust, sums up well the general thrust of the
reaction of environmental movements: ?There is no
reason at all to believe that the polluters are sud-
denly going to become reasonable.?
Perspective expressions in Sentence 1 describe the
emotions or opinion of three sources: Charlie?s
anger, Bob?s unhappiness, and Alice?s belief. Per-
spective expressions in Sentence 2, on the other
hand, introduce the explicit opinion of one source,
i.e. the reaction of the environmental movements.
Speech expressions also perform filtering in these
examples. The reaction of the environmental move-
ments is filtered by Clapp?s summarization, which,
in turn, is filtered by the writer?s choice of quotation.
In addition, the fact that Bob was unhappy is filtered
through Alice?s claim, which, in turn, is filtered by
the writer?s choice of words for the sentence. Sim-
ilarly, it is only according to the writer that Charlie
is angry.
The specific goal of the research described here
is to accurately identify the hierarchical structure of
perspective and speech expressions (pse?s) in text.2
al.?s (2003) ?expressive subjective elements? are not the subject
of study here.
2For the rest of this paper, then, we ignore the distinction
between perspective and speech expressions, so in future ex-
Given sentences 1 and 2 and their pse?s, for exam-
ple, we will present methods that produce the struc-
tures shown in Figure 1, which represent the multi-
stage information filtering that should be taken into
account in the interpretation of the text.
Sentence 1:
writer?s implicit speech event
claim
unhappy
angry
Sentence 2:
writer?s implicit speech event
sums up
reaction
Figure 1: Hierarchical structure of the perspective and
speech expressions in sentences 1 and 2
We propose a supervised machine learning ap-
proach to the problem that relies on a small set
of syntactically-based features. More specifically,
the method first trains a binary classifier to make
pairwise parent-child decisions among the pse?s in
the same sentence, and then combines the deci-
sions to determine their global hierarchical struc-
ture. We compare the approach to two heuristic-
based baselines ? one that simply assumes that ev-
ery pse is filtered only through the writer, and a
second that is based on syntactic dominance rela-
tions in the associated parse tree. In an evaluation
using the opinion-annotated NRRC corpus (Wiebe
et al, 2002), the learning-based approach achieves
an accuracy of 78.30%, significantly higher than
both the simple baseline approach (65.57%) and the
parse-based baseline (71.64%). We believe that this
study provides a first step towards understanding the
multi-stage filtering process that can bias and garble
the information present in newswire text.
The rest of the paper is organized as follows. We
present related work in Section 2 and describe the
machine learning approach in Section 3. The ex-
perimental methodology and results are presented
in Sections 4 and 5, respectively. Section 6 summa-
rizes our conclusions and plans for future work.
2 The Larger Problem and Related Work
This paper addresses the problem of identifying the
hierarchical structure of perspective and speech ex-
pressions. We view this as a necessary and im-
portant component of a larger perspective-analysis
amples, both types of pse appear in boldface. Note that the
acronym ?pse? has been used previously with a different mean-
ing (Wiebe, 1994).
pse class count
writer 9808
verb 7623
noun 2293
no parse 278
adjective 197
adverb 50
other 370
Table 1: Breakdown of classes of pse?s. ?writer? de-
notes pse?s with the writer as source. ?No parse? denotes
pse?s in sentences where the parse failed, and so the part
of speech could not be determined.
number of pse?s number of sentences
1 3612
2 3256
3 1810
4 778
5 239
>5 113
Table 2: Breakdown of number of pse?s per sentence
system. Such a system would be able to identify
all pse?s in a document, as well as identify their
structure. The system would also identify the direct
source of each pse. Finally, the system would iden-
tify the text corresponding to the content of a private
state or the speech expressed by a pse.3 Such a sys-
tem might analyze sentence 2 as follows:
(source: writer
pse: (implicit speech event)
content: Philip ... reasonable.?)
(source: clapp
pse: sums up
content: ?There ... reasonable.?)
(source: environmental movements
pse: reaction
content: (no text))
As far as we are aware, no single system ex-
ists that simultaneously solves all these problems.
There is, however, quite a bit of work that addresses
various pieces of this larger task, which we will now
survey.
Gerard (2000) proposes a computational model
of the reader of a news article. Her model provides
for multiple levels of hierarchical beliefs, such as
the nesting of a primary source?s belief within that
of a reporter. However, Gerard does not provide al-
gorithms for extracting this structure directly from
newswire texts.
Bethard et al (2004) seek to extract propositional
3In (Wiebe, 2002), this is referred to as the inside.
opinions and their holders. They define an opinion
as ?a sentence, or part of a sentence that would an-
swer the question ?How does X feel about Y?? ? A
propositional opinion is an opinion ?localized in the
propositional argument? of certain verbs, such as
?believe? or ?realize?. Their task then corresponds
to identifying a pse, its associated direct source, and
the content of the private state. However, they con-
sider as pse?s only verbs, and further restrict atten-
tion to verbs with a propositional argument, which
is a subset of the perspective and speech expressions
that we consider here. Table 1, for example, shows
the diversity of word classes that correspond to pse?s
in our corpus. Perhaps more importantly for the
purposes of this paper, their work does not address
information filtering issues, i.e. problems that arise
when an opinion has been filtered through multiple
sources. Namely, Bethard et al (2004) do not con-
sider sentences that contain multiple pse?s, and do
not, therefore, need to identify any indirect sources
of opinions. As shown in Table 2, however, we
find that sentences with multiple non-writer pse?s
(i.e. sentences that contain 3 or more total pse?s)
comprise a significant portion (29.98%) of our cor-
pus. An advantage over our work, however, is that
Bethard et al (2004) do not require separate solu-
tions to pse identification and the identification of
their direct sources.
Automatic identification of sources has also
been addressed indirectly by Gildea and Jurafsky?s
(2002) work on semantic role identification in that
finding sources often corresponds to finding the
filler of the agent role for verbs. Their methods then
might be used to identify sources and associate them
with pse?s that are verbs or portions of verb phrases.
Whether their work will also apply to pse?s that are
realized as other parts of speech is an open question.
Wiebe (1994), studies methods to track the
change of ?point of view? in narrative text (fiction).
That is, the ?writer? of one sentence may not corre-
spond to the writer of the next sentence. Although
this is not as frequent in newswire text as in fiction,
it will still need to be addressed in a solution to the
larger problem.
Bergler (1993) examines the lexical semantics of
speech event verbs in the context of generative lex-
icon theory. While not specifically addressing our
problem, the ?semantic dimensions? of reporting
verbs that she extracts might be very useful as fea-
tures in our approach.
Finally, Wiebe et al (2003) present preliminary
results for the automatic identification of perspec-
tive and speech expressions using corpus-based
techniques. While the results are promising (66% F-
was
Charlie angry
at
claim
?s
Alice
that
was
Bob unhappy
Figure 2: Dependency parse of sentence 1 according to
the Collins parser.
measure), the problem is still clearly unsolved. As
explained below, we will instead rely on manually
tagged pse?s for the studies presented here.
3 The Approach
Our task is to find the hierarchical structure among
the pse?s in individual sentences. One?s first im-
pression might be that this structure should be ob-
vious from the syntax: one pse should filter an-
other roughly when it dominates the other in a de-
pendency parse. This heuristic, for example, would
succeed for ?claim? and ?unhappy? in sentence 1,
whose pse structure is given in Figure 1 and parse
structure (as produced by the Collins parser) in Fig-
ure 2. 4
Even in sentence 1, though, we can see that
the problem is more complex: ?angry? dominates
?claim? in the parse tree, but does not filter it. Un-
fortunately, an analysis of the parse-based heuristic
on our training data (the data set will be described
in Section 4), uncovered numerous, rather than just
a few, sources of error. Therefore, rather than trying
to handcraft a more complex collection of heuris-
tics, we chose to adopt a supervised machine learn-
ing approach that relies on features identified in this
analysis. In particular, we will first train a binary
classifier to make pairwise decisions as to whether
a given pse is the immediate parent of another. We
then use a simple approach to combine these de-
cisions to find the hierarchical information-filtering
structure of all pse?s in a sentence.
We assume that we have a training corpus of
4For this heuristic and the features that follow, we will speak
of the pse?s as if they had a position in the parse tree. However,
since pse?s are often multiple words, and do not necessarily
form a constituent, this is not entirely accurate. The parse node
corresponding to a pse will be the highest node in the depen-
dency parse corresponding to a word in the pse. We consider
the writer?s implicit pse to correspond to the root of the parse.
sentences, annotated with pse?s and their hier-
archical pse structure (Section 4 describes the
corpus). Training instances for the binary clas-
sifier are pairs of pse?s from the same sentence,
?psetarget, pseparent?5. We assign a class value
of 1 to a training instance if pseparent is the
immediate parent of psetarget in the manually
annotated hierarchical structure for the sentence,
and 0 otherwise. For sentence 1, there are nine
training instances generated: ?claim,writer?,
?angry,writer?, ?unhappy, claim? (class 1),
?claim, angry?, ?claim, unhappy?, ?angry, claim?,
?angry, unhappy?, ?unhappy,writer?,
?unhappy, angry? (class 0). The features used
to describe each training instance are explained
below.
During testing, we construct the hierarchical pse
structure of an entire sentence as follows. For each
pse in the sentence, ask the binary classifier to judge
each other pse as a potential parent, and choose the
pse with the highest confidence6. Finally, join these
immediate-parent links to form a tree.7
One might also try comparing pairs of potential
parents for a given pse, or other more direct means
of ranking potential parents. We chose what seemed
to be the simplest method for this first attempt at the
problem.
3.1 Features
Here we motivate and describe the 23 features used
in our model. Unless otherwise stated, all features
are binary (1 if the described condition is true, 0
otherwise).
Parse-based features (6). Based on the perfor-
mance of the parse-based heuristic, we include a
pseparent-dominates-psetarget feature in our feature
set. To compensate for parse errors, however, we
also include a variant of this that is 1 if the parent of
pseparent dominates psetarget.
Many filtering expressions filter pse?s that occur
in their complements, but not in adjuncts. There-
fore, we add variants of the previous two syntax-
based features that denote whether the parent node
5We skip sentences where there is no decision to make (sen-
tences with zero or one non-writer pse). Since the writer pse is
the root of every structure, we do not generate instances with
the writer pse in the psetarget position.
6There is an ambiguity if the classifier assigns the same con-
fidence to two potential parents. For evaluation purposes, we
consider the classifier?s response incorrect if any of the highest-
scoring potential parents are incorrect.
7The directed graph resulting from flawed automatic pre-
dictions might not be a tree (i.e. it might be cyclic and discon-
nected). Since this occurs very rarely (5 out of 9808 sentences
on the test data), we do not attempt to correct any non-tree
graphs.
dominates psetarget, but only if the first dependency
relation is an object relation.
For similar reasons, we include a feature calculat-
ing the domination relation based on a partial parse.
Consider the following sentence:
3. He was criticized more than recognized for his
policy.
One of ?criticized? or ?recognized? will be the root
of this dependency parse, thus dominating the other,
and suggesting (incorrectly) that it filters the other
pse. Because a partial parse does not attach all con-
stituents, such spurious dominations are eliminated.
The partial parse feature is 1 for fewer instances
than pseparent-dominates-psetarget , but it is more
indicative of a positive instance when it is 1.
So that the model can adjust when the parse is
not present, we include a feature that is 1 for all
instances generated from sentences on which the
parser failed.
Positional features (5). Forcing the model to de-
cide whether pseparent is the parent of psetarget
without knowledge of the other pse?s in the sen-
tence is somewhat artificial. We therefore include
several features that encode the relative position of
pseparent and psetarget in the sentence. Specifi-
cally, we add a feature that is 1 if pseparent is the
root of the parse (and similarly for psetarget ). We
also include a feature giving the ordinal position of
pseparent among the pse?s in the sentence, relative
to psetarget (-1 means pseparent is the pse that im-
mediately precedes psetarget, 1 means immediately
following, and so forth). To allow the model to vary
when there are more potential parents to choose
from, we include a feature giving the total number
of pse?s in the sentence.
Special parents and lexical features (6). Some
particular pse?s are special, so we specify indicator
features for four types of parents: the writer pse,
and the lexical items ?said? (the most common non-
writer pse) and ?according to?. ?According to? is
special because it is generally not very high in the
parse, but semantically tends to filter everything else
in the sentence.
In addition, we include as features the part of
speech of pseparent and psetarget (reduced to noun,
verb, adjective, adverb, or other), since intuitively
we expected distinct parts of speech to behave dif-
ferently in their filtering.
Genre-specific features (6). Finally, journalistic
writing contains a few special forms that are not al-
ways parsed accurately. Examples are:
4. ?Alice disagrees with me,? Bob argued.
5. Charlie, she noted, dislikes Chinese food.
The parser may not recognize that ?noted? and
?argued? should dominate all other pse?s in sen-
tences 4 and 5, so we attempt to recognize when
a sentence falls into one of these two patterns.
For ?disagrees, argued? generated from sentence 4,
features pseparent-pattern-1 and psetarget-pattern-
1 would be 1, while for ?dislikes, noted? generated
from sentence 5, feature pseparent-pattern-2 would
be 1. We also add features that denote whether the
pse in question falls between matching quote marks.
Finally, a simple feature indicates whether pseparent
is the last word in the sentence.
3.2 Resources
We rely on a variety of resources to generate our fea-
tures. The corpus (see Section 4) is distributed with
annotations for sentence breaks, tokenization, and
part of speech information automatically generated
by the GATE toolkit (Cunningham et al, 2002).8
For parsing we use the Collins (1999) parser.9 For
partial parses, we employ CASS (Abney, 1997). Fi-
nally, we use a simple finite-state recognizer to iden-
tify (possibly nested) quoted phrases.
For classifier construction, we use the IND pack-
age (Buntine, 1993) to train decision trees (we use
the mml tree style, a minimum message length cri-
terion with Bayesian smoothing).
4 Data Description
The data for these experiments come from version
1.1 of the NRRC corpus (Wiebe et al, 2002).10. The
corpus consists of 535 newswire documents (mostly
from the FBIS), of which we used 66 (1375 sen-
tences) for developing the heuristics and features,
while keeping the remaining 469 (9808 sentences)
blind (used for 10-fold cross-validation).
Although the NRRC corpus provides annotations
for all pse?s, it does not provide annotations to de-
note directly their hierarchical structure within a
8GATE?s sentences sometimes extend across paragraph
boundaries, which seems never to be warranted. Inaccurately
joining sentences has the effect of adding more noise to our
problem, so we split GATE?s sentences at paragraph bound-
aries, and introduce writer pse?s for the newly created sen-
tences.
9We convert the parse to a dependency format that makes
some of our features simpler using a method similar to the one
described in Xia and Palmer (2001). We also employ a method
from Adam Lopez at the University of Maryland to find gram-
matical relationships between words (subject, object, etc.).
10The original corpus is available at http:
//nrrc.mitre.org/NRRC/Docs_Data/MPQA_
04/approval_mpqa.htm. Code and data used in our
experiments are available at http://www.cs.cornell.
edu/?ebreck/breck04playing/.
sentence. This structure must be extracted from
an attribute of each pse annotation, which lists the
pse?s direct and indirect sources. For example, the
?source chain? for ?unhappy? in sentence 1, would
be (writer, Alice, Bob). The source chains allow
us to automatically recover the hierarchical struc-
ture of the pse?s: the parent of a pse with source
chain (s0, s1, . . . sn?1, sn) is the pse with source
chain (s0, s1, . . . sn?1). Unfortunately, ambiguities
can arise. Consider the following sentence:
6. Bob said, ?you?re welcome? because he was glad
to see that Mary was happy.
Both ?said? and ?was glad? have the source chain
(writer, Bob),11 while ?was happy? has the source
chain (writer, Bob, Mary). It is therefore not clear
from the manual annotations whether ?was happy?
should have ?was glad? or ?said? as its parent.
5.82% of the pse?s have ambiguous parentage (i.e.
the recovery step finds a set of parents P (pse) with
|P (pse)| > 1). For training, we assign a class value
of 1 to all instances ?pse, par?, par ? P (pse). For
testing, if an algorithm attaches pse to any element
of P (pse), we score the link as correct (see Sec-
tion 5.1). Since ultimately our goal is to find the
sources through which information is filtered (rather
than the pse?s), we believe this is justified.
For training and testing, we used only those sen-
tences that contain at least two non-writer pse?s12
? for all other sentences, there is only one way to
construct the hierarchical structure. Again, Table 2
presents a breakdown (for the test set) of the num-
ber of pse?s per sentence ? thus we only use approx-
imately one-third of all the sentences in the corpus.
5 Results and Discussion
5.1 Evaluation
How do we evaluate the performance of an au-
tomatic method of determining the hierarchical
structure of pse?s? Lin (1995) proposes a method
for evaluating dependency parses: the score for
a sentence is the fraction of correct parent links
identified; the score for the corpus is the aver-
age sentence score. Formally, the score for a
11The annotators also performed coreference resolution on
sources.
12Under certain circumstances, such as paragraph-long
quotes, the writer of a sentence will not be the same as the
writer of a document. In such sentences, the NRRC corpus con-
tains additional pse?s for any other sources besides the writer of
the document. Since we are concerned in this work only with
one sentence at a time, we discard all such implicit pse?s be-
sides the writer of the sentence. Also, in a few cases, more than
one pse in a sentence was marked as having the writer as its
source. We believe this to be an error and so discarded all but
one writer pse.
metric size heurOne heurTwo decTree
Lin 2940 65.57% 71.64% 78.30%
perf 2940 36.02% 45.37% 54.52%
bin 21933 73.20% 77.73% 82.12%
bin + 7882 60.63% 66.94% 70.35%
bin ? 14051 80.24% 83.78% 88.72%
Table 3: Performance on test data. ?Lin? is Lin?s depen-
dency score, ?perf? is the fraction of sentences whose
structure was identified perfectly, and ?bin? is the perfor-
mance of the binary classifier (broken down for positive
and negative instances). ?Size? is the number of sen-
tences or pse pairs.
# pse?s # sents heurOne heurTwo decTree
3 1810 70.88% 75.41% 81.82%
4 778 59.17% 67.82% 74.38%
5 239 53.87% 61.92% 68.93%
>5 113 49.31% 58.03% 68.68%
Table 4: Performance by number of pse?s per sentence
method evaluated on the entire corpus (?Lin?) is
?
s?S
|{pse|pse?Non writer pse?s(s)?parent(pse)=autopar(pse))}|
|Non writer pse?s(s)|
|S| ,
where S is the set of all sentences in the corpus,
Non writer pse ?s(s) is the set of non-writer pse?s
in sentence s, parent(pse) is the correct parent
of pse, and autopar(pse) is the automatically
identified parent of pse.
We also present results using two other (related)
metrics. The ?perf? metric measures the fraction
of sentences whose structure is determined entirely
correctly (i.e. ?perf?ectly). ?Bin? is the accuracy of
the binary classifier (with a 0.5 threshold) on the in-
stances created from the test corpus. We also report
the performance on positive and negative instances.
5.2 Results
We compare the learning-based approach (decTree)
to the heuristic-based approaches introduced in Sec-
tion 3 ? heurOne assumes that all pse?s are at-
tached to the writer?s implicit pse; heurTwo is the
parse-based heuristic that relies solely on the domi-
nance relation13.
We use 10-fold cross-validation on the evalua-
tion data to generate training and test data (although
the heuristics, of course, do not require training).
The results of the decision tree method and the two
heuristics are presented in Table 3.
13That is, heurTwo attaches a pse to the pse most immedi-
ately dominating it in the dependency tree. If no other pse
dominates it, a pse is attached to the writer?s pse.
5.3 Discussion
Encouragingly, our machine learning method uni-
formly and significantly14 outperforms the two
heuristic methods, on all metrics and in sentences
with any number of pse?s. The difference is most
striking in the ?perf? metric, which is perhaps
the most intuitive. Also, the syntax-based heuris-
tic (heurTwo) significantly15 outperforms heurOne,
confirming our intuitions that syntax is important in
this task.
As the binary classifer sees many more negative
instances than positive, it is unsurprising that its per-
formance is much better on negative instances. This
suggests that we might benefit from machine learn-
ing methods for dealing with unbalanced datasets.
Examining the errors of the machine learning sys-
tem on the development set, we see that for half
of the pse?s with erroneously identified parents, the
parent is either the writer?s pse, or a pse like ?said?
in sentences 4 and 5 having scope over the entire
sentence. For example,
7. ?Our concern is whether persons used to the role
of policy implementors can objectively assess and
critique executive policies which impinge on hu-
man rights,? said Ramdas.
Our model chose the parent of ?assess and critique?
to be ?said? rather than ?concern.? We also see from
Table 4 that the model performs more poorly on sen-
tences with more pse?s. We believe that this reflects
a weakness in our decision to combine binary deci-
sions, because the model has learned that in general,
a ?said? or writer?s pse (near the root of the struc-
ture) is likely to be the parent, while it sees many
fewer examples of pse?s such as ?concern? that lie
in the middle of the tree.
Although we have ignored the distinction
throughout this paper, error analysis suggests
speech event pse?s behave differently than private
state pse?s with respect to how closely syntax re-
flects their hierarchical structure. It may behoove
us to add features to allow the model to take this
into account. Other sources of error include er-
roneous sentence boundary detection, parenthetical
statements (which the parser does not treat correctly
for our purposes) and other parse errors, partial quo-
tations, as well as some errors in the annotation.
Examining the learned trees is difficult because
of their size, but looking at one tree to depth three
14p < 0.01, using an approximate randomization test with
9,999 trials. See (Eisner, 1996, page 17) and (Chinchor et al,
1993, pages 430-433) for descriptions of this method.
15Using the same test as above, p < 0.01, except for the
performance on sentences with more than 5 pse?s, because of
the small amount of data, where p < 0.02.
reveals a fairly intuitive model. Ignoring the prob-
abilities, the tree decides pseparent is the parent
of psetarget if and only if pseparent is the writer?s
pse (and psetarget is not in quotation marks), or
if pseparent is the word ?said.? For all the trees
learned, the root feature was either the writer pse
test or the partial-parse-based domination feature.
6 Conclusions and Future Work
We have presented the concept of perspective and
speech expressions, and argued that determining
their hierarchical structure is important for natural
language understanding of perspective. We have
shown that identifying the hierarchical structure of
pse?s is amenable to automated analysis via a ma-
chine learning approach, although there is room for
improvement in the results.
In the future, we plan to address the related tasks
discussed in Section 2, especially identifying pse?s
and their immediate sources. We are also interested
in ways of improving the machine learning formu-
lation of the current task, such as optimizing the
binary classifier on the whole-sentence evaluation,
or defining a different binary task that is easier to
learn. Nevertheless, we believe that our results pro-
vide a step towards the development of natural lan-
guage systems that can extract and summarize the
viewpoints and perspectives expressed in text while
taking into account the multi-stage information fil-
tering process that can mislead more na??ve systems.
Acknowledgments
This work was supported in part by NSF Grant IIS-
0208028 and by an NSF Graduate Research Fellowship.
We thank Rebecca Hwa for creating the dependency
parses. We also thank the Cornell NLP group for help-
ful suggestions on drafts of this paper. Finally, we thank
Janyce Wiebe and Theresa Wilson for draft suggestions
and advice regarding this problem and the NRRC corpus.
References
Steven Abney. 1997. The SCOL manual. cass is avail-
able from http://www.vinartus.net/spa/scol1h.tar.gz.
Sabine Bergler. 1993. Semantic dimensions in the field
of reporting verbs. In Proceedings of the Ninth An-
nual Conference of the University of Waterloo Centre
for the New Oxford English Dictionary and Text Re-
search, Oxford, England, September.
Steven Bethard, Hong Yu, Ashley Thornton, Vasileios
Hatzivassiloglou, and Dan Jurafsky. 2004. Automatic
extraction of opinion propositions and their holders.
In Working Notes of the AAAI Spring Symposium on
Exploring Attitude and Affect in Text: Theories and
Applications. March 22-24, 2004, Stanford.
Wray Buntine. 1993. Learning classification trees. In
D. J. Hand, editor, Artificial Intelligence frontiers in
statistics, pages 182?201. Chapman & Hall,London.
Available at http://ic.arc.nasa.gov/projects/bayes-
group/ind/IND-program.html.
Nancy Chinchor, Lynette Hirschman, and David Lewis.
1993. Evaluating message understanding systems:
An analysis of the third message understanding
conference (MUC-3). Computational Linguistics,
19(3):409?450.
Michael John Collins. 1999. Head-driven Statistical
Models for Natural Language Parsing. Ph.D. thesis,
University of Pennsylvania, Philadelphia.
Hamish Cunningham, Diana Maynard, Kalina Bont-
cheva, and Valentin Tablan. 2002. GATE: A frame-
work and graphical development environment for ro-
bust nlp tools and applications. In Proceedings of the
40th Anniversary Meeting of the Association for Com-
putational Linguistics (ACL ?02), Philadelphia, July.
2003. Proceedings of the Workshop on Text Summariza-
tion, Edmonton, Alberta, Canada, May. Presented at
the 2003 Human Language Technology Conference.
Jason Eisner. 1996. An empirical comparison of proba-
bility models for dependency grammar. Technical Re-
port IRCS-96-11, IRCS, University of Pennsylvania.
Christine Gerard. 2000. Modelling readers of news ar-
ticles using nested beliefs. Master?s thesis, Concordia
University, Montre?al, Que?bec, Canada.
Daniel Gildea and Daniel Jurafsky. 2002. Automatic la-
beling of semantic roles. Computational Linguistics,
28(3):245?288.
Dekang Lin. 1995. A dependency-based method for
evaluating broad-coverage parsers. In IJCAI, pages
1420?1427.
1998. Proceedings of the Seventh Message Understand-
ing Conference (MUC-7). Morgan Kaufman, April.
NIST. 2003. Proceedings of The Twelfth Text REtrieval
Conference (TREC 2003), Gaithersburg, MD, Novem-
ber. NIST special publication SP 500-255.
Randolph Quirk, Sidney Greenbaum, Geoffrey Leech,
and Jan Svartvik. 1985. A Comprehensive Grammar
of the English Language. Longman, New York.
J. Wiebe, E. Breck, C. Buckley, C. Cardie, P. Davis,
B. Fraser, D. Litman, D. Pierce, E. Riloff, and T. Wil-
son. 2002. NRRC Summer Workshop on Multiple-
Perspective Question Answering Final Report. Tech
report, NRRC, Bedford, MA.
J. Wiebe, E. Breck, C. Buckley, C. Cardie, P. Davis,
B. Fraser, D. Litman, D. Pierce, E. Riloff, T. Wilson,
D. Day, and M. Maybury. 2003. Recognizing and Or-
ganizing Opinions Expressed in the World Press. In
Papers from the AAAI Spring Symposium on New Di-
rections in Question Answering (AAAI tech report SS-
03-07). March 24-26, 2003. Stanford.
Janyce Wiebe. 1994. Tracking point of view in narrative.
Computational Linguistics, 20(2):233?287.
Janyce Wiebe. 2002. Instructions for annotating opin-
ions in newspaper articles. Technical Report TR-02-
101, Dept. of Comp. Sci., University of Pittsburgh.
Fei Xia and Martha Palmer. 2001. Converting depen-
dency structures to phrase structures. In Proc. of the
HLT Conference.
Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008), pages 817?824
Manchester, August 2008
Topic Identification for Fine-Grained Opinion Analysis
Veselin Stoyanov and Claire Cardie
Department of Computer Science
Cornell University
{stoyanov,cardie}@cs.cornell.edu
Abstract
Within the area of general-purpose fine-
grained subjectivity analysis, opinion topic
identification has, to date, received little
attention due to both the difficulty of the
task and the lack of appropriately anno-
tated resources. In this paper, we pro-
vide an operational definition of opinion
topic and present an algorithm for opinion
topic identification that, following our new
definition, treats the task as a problem in
topic coreference resolution. We develop a
methodology for the manual annotation of
opinion topics and use it to annotate topic
information for a portion of an existing
general-purpose opinion corpus. In exper-
iments using the corpus, our topic identi-
fication approach statistically significantly
outperforms several non-trivial baselines
according to three evaluation measures.
1 Introduction
Subjectivity analysis is concerned with extract-
ing information about attitudes, beliefs, emotions,
opinions, evaluations, sentiment and other private
states expressed in texts. In contrast to the prob-
lem of identifying subjectivity or sentiment at the
document level (e.g. Pang et al (2002), Turney
(2002)), we are interested in fine-grained subjec-
tivity analysis, which is concerned with subjec-
tivity at the phrase or clause level. We expect
fine-grained subjectivity analysis to be useful for
question-answering, summarization, information
extraction and search engine support for queries of
c
? 2008. Licensed under the Creative Commons
Attribution-Noncommercial-Share Alike 3.0 Unported li-
cense (http://creativecommons.org/licenses/by-nc-sa/3.0/).
Some rights reserved.
the form ?How/what does entity X feel/think about
topic Y??, for which document-level opinion anal-
ysis methods can be problematic.
Fine-grained subjectivity analyses typically
identify SUBJECTIVE EXPRESSIONS in context, charac-
terize their POLARITY (e.g. positive, neutral or neg-
ative) and INTENSITY (e.g. weak, medium, strong,
extreme), and identify the associated SOURCE, or
OPINION HOLDER, as well as the TOPIC, or TARGET, of
the opinion. While substantial progress has been
made in automating some of these tasks, opinion
topic identification has received by far the least at-
tention due to both the difficulty of the task and the
lack of appropriately annotated resources.
1
This paper addresses the problem of topic iden-
tification for fine-grained opinion analysis of gen-
eral text.
2
We begin by providing a new, opera-
tional definition of opinion topic in which the topic
of an opinion depends on the context in which
its associated opinion expression occurs. We also
present a novel method for general-purpose opin-
ion topic identification that, following our new def-
inition, treats the problem as an exercise in topic
coreference resolution. We evaluate the approach
using the existing MPQA corpus (Wiebe et al,
2005), which we extend with manual annotations
that encode topic information (and refer to here-
after as the MPQA
TOPIC
corpus).
Inter-annotator agreement results for the manual
annotations are reasonably strong across a num-
ber of metrics and the results of experiments that
evaluate our topic identification method in the con-
text of fine-grained opinion analysis are promising:
1
Section 3 on related work provides additional discussion.
2
The identification of products and their components and
attributes from product reviews is a related, but quite different
task from that addressed here. Section 3 briefly discusses, and
provides references, to the most relevant research in that area.
817
using either automatically or manually identified
topic spans, we achieve topic coreference scores
that statistically significantly outperform two topic
segmentation baselines across three coreference
resolution evaluation measures (B
3
, ? and CEAF).
For the B
3
metric, for example, the best base-
line achieves a topic coreference score on the
MPQA
TOPIC
corpus of 0.55 while our topic coref-
erence algorithm scores 0.57 and 0.71 using au-
tomatically, and manually, identified topic spans,
respectively.
In the remainder of the paper, we define opin-
ion topics (Section 2), present related work (Sec-
tion 3), and motivate and describe the key idea
of topic coreference that underlies our methodol-
ogy for both the manual and automatic annota-
tion of opinion topics (Section 4). Creation of
the MPQA
TOPIC
corpus is described in Section 5
and our topic identification algorithm, in Section 6.
The evaluation methodology and results are pre-
sented in Sections 7 and 8, respectively.
2 Definitions and Examples
Consider the following opinion sentences:
(1)[
OH
John] adores [
TARGET+TOPIC SPAN
Marseille] and
visits it often.
(2)[
OH
Al] thinks that [
TARGET SPAN
[
TOPIC SPAN?
the
government] should [
TOPIC SPAN?
tax gas] more in order to
[
TOPIC SPAN?
curb [
TOPIC SPAN?
CO
2
emissions]]].
A fine-grained subjectivity analysis should iden-
tify: the OPINION EXPRESSION
3
as ?adores? in Exam-
ple 1 and ?thinks? in Example 2; the POLARITY as
positive in Example 1 and neutral in Example 2;
the INTENSITY as medium and low, respectively; and
the OPINION HOLDER (OH) as ?John? and ?Al?, re-
spectively. To be able to discuss the opinion TOPIC
in each example, we begin with three definitions:
? Topic. The TOPIC of a fine-grained opinion is
the real-world object, event or abstract entity that is
the subject of the opinion as intended by the opin-
ion holder.
? Topic span. The TOPIC SPAN associated with an
OPINION EXPRESSION is the closest, minimal span of
text that mentions the topic.
? Target span. In contrast, we use TARGET SPAN
to denote the span of text that covers the syntactic
3
For simplicity, we will use the term opinion throughout
the paper to cover all types of private states expressed in sub-
jective language.
surface form comprising the contents of the opin-
ion.
In Example 1, for instance, ?Marseille? is both
the TOPIC SPAN and the TARGET SPAN associated with
the city of Marseille, which is the TOPIC of the opin-
ion. In Example 2, the TARGET SPAN consists of the
text that comprises the complement of the subjec-
tive verb ?thinks?. Example 2 illustrates why opin-
ion topic identification is difficult: within the sin-
gle target span of the opinion, there are multiple
potential topics, each identified with its own topic
span. Without more context, however, it is impos-
sible to know which phrase indicates the intended
topic. If followed by sentence 3, however,
(3)Although he doesn?t like government-imposed taxes, he
thinks that a fuel tax is the only effective solution.
the topic of Al?s opinion in 2 is much clearer ? it
is likely to be fuel tax, denoted via the TOPIC SPAN
?tax gas? or ?tax?.
3 Related Work
As previously mentioned, there has been much re-
cent progress in extracting fine-grained subjectiv-
ity information from general text. Previous efforts
have focused on the extraction of opinion expres-
sions in context (e.g. Bethard et al (2004), Breck
et al (2007)), the assignment of polarity to these
expressions (e.g. Wilson et al (2005), Kim and
Hovy (2006)), source extraction (e.g. Bethard et
al. (2004), Choi et al (2005)), and identification of
the source-expresses-opinion relation (e.g. Choi et
al. (2006)), i.e. linking sources to the opinions that
they express.
Not surprisingly, progress has been driven by
the creation of language resources. In this regard,
Wiebe et al?s (2005) opinion annotation scheme
for subjective expressions was used to create the
MPQA corpus, which consists of 535 documents
manually annotated for phrase-level expressions of
opinions, their sources, polarities, and intensities.
Although other opinion corpora exist (e.g. Bethard
et al (2004), Voorhees and Buckland (2003), the
product review corpora of Liu
4
), we are not aware
of any corpus that rivals the scale and depth of the
MPQA corpus.
In the related area of opinion extraction from
product reviews, several research efforts have fo-
cused on the extraction of the topic of the opin-
ion (e.g. Kobayashi et al (2004), Yi et al (2003),
4
http://www.cs.uic.edu/ liub/FBS/sentiment-analysis.html
818
Popescu and Etzioni (2005), Hu and Liu (2004)).
For this specialized text genre, it has been suf-
ficient to limit the notion of topic to mentions
of product names and components and their at-
tributes. Thus, topic extraction has been effec-
tively substituted with a lexicon look-up and tech-
niques have focused on how to learn or acquire an
appropriate lexicon for the task. While the tech-
niques have been very successful for this genre
of text, they have not been applied outside the
product reviews domain. Further, there are anal-
yses (Wiebe et al, 2005) and experiments (Wilson
et al, 2005) that indicate that lexicon-lookup ap-
proaches to subjectivity analysis will have limited
success on general texts.
Outside the product review domain, there has
been little effort devoted to opinion topic annota-
tion. The MPQA corpus, for example, was orig-
inally intended to include topic annotations, but
the task was abandoned after confirming that it
was very difficult (Wiebe, 2005; Wilson, 2005),
although target span annotation is currently under-
way. While useful, target spans alone will be insuf-
ficient for many applications: they neither contain
information indicating which opinions are about
the same topic, nor provide a concise textual rep-
resentation of the topics.
Due to the lack of appropriately annotated cor-
pora, the problem of opinion topic extraction has
been largely unexplored in NLP. A notable excep-
tion is the work of Kim and Hovy (2006). They
propose a model that extracts opinion topics for
subjective expressions signaled by verbs and ad-
jectives. Their model relies on semantic frames
and extracts as the topic the syntactic constituent
at a specific argument position for the given verb
or adjective. In other words, Kim and Hovy extract
what we refer to as the target spans, and do so for
a subset of the opinion-bearing words in the text.
Although on many occasions target spans coincide
with opinion topics (as in Example 1), we have ob-
served that on many other occasions this is not the
case (as in Example 2). Furthermore, hampered by
the lack of resources with manually annotated tar-
gets, Kim and Hovy could provide only a limited
evaluation.
As we have defined it, opinion topic identifica-
tion bears some resemblance to topic segmenta-
tion, the goal of which is to partition a text into
a linear sequence of topically coherent segments.
Existing methods for topic segmentation typically
assume that fragments of text (e.g. sentences or
sequences of words of a fixed length) with sim-
ilar lexical distribution are about the same topic;
the goal of these methods is to find the boundaries
where the lexical distribution changes (e.g. Choi
(2000), Malioutov and Barzilay (2006)). Opin-
ion topic identification differs from topic segmen-
tation in that opinion topics are not necessarily spa-
tially coherent ? there may be two opinions in
the same sentence on different topics, as well as
opinions that are on the same topic separated by
opinions that do not share that topic. Nevertheless,
we will compare our topic identification approach
to a state-of-the-art topic segmentation algorithm
(Choi, 2000) in the evaluation.
Other work has successfully adopted the use of
clustering to discover entity relations by identify-
ing entities that appear in the same sentence and
clustering the intervening context (e.g. Hasegawa
et al (2004), Rosenfeld and Feldman (2007)). This
work, however, considers named entities and heads
of proper noun phrases rather than topic spans,
and the relations learned are those commonly held
between NPs (e.g. senator-of-state, city-of-state,
chairman-of-organization) rather than a more gen-
eral coreference relation.
4 A Coreference Approach to Topic
Identification
Given our initial definition of opinion topics (Sec-
tion 2), the next task is to determine which com-
putational approaches might be employed for au-
tomatic opinion topic identification. We begin this
exercise by considering some of the problematic
characteristics of opinion topics.
Multiple potential topics. As noted earlier via
Example 2, a serious problem in opinion topic
identification is the mention of multiple potential
topics within the target span of the opinion. Al-
though an issue for all opinions, this problem is
typically more pronounced in opinions that do not
carry sentiment (as in Example 2). Our current
definition of opinion topic requires the NLP sys-
tem (or a human annotator) to decide which of the
entities described in the target span, if any, refers
to the intended topic. This decision can be aided
by the following change to our definition of opin-
ion topic, which introduces the idea of a context-
dependent information focus: the TOPIC of an opin-
ion is the real-world entity that is the subject of the
opinion as intended by the opinion holder based
819
on the discourse context.
With this modified definition in hand, and given
Example 3 as the succeeding context for Example
2, we argue that the intended subject, and hence
the TOPIC, of Al?s opinion in 2 can be quickly iden-
tified as the FUEL TAX, which is denoted by the TOPIC
SPANS ?tax gas? in 2 and ?fuel tax? in 3.
Opinion topics not always explicitly mentioned.
In stark contrast to the above, on many occasions
the topic is not mentioned explicitly at all within
the target span, as in the following example:
(5)[
OH
John] identified the violation of Palestinian human
rights as one of the main factors. TOPIC: ISRAELI-
PALESTINIAN CONFLICT
We have further observed that the opinion topic
is often not mentioned within the same paragraph
and, on a few occasions, not even within the same
document as the opinion expression.
4.1 Our Solution: Topic Coreference
With the above examples and problems in mind,
we hypothesize that the notion of topic corefer-
ence will facilitate both the manual and automatic
identification of opinion topics: We say that two
opinions are topic-coreferent if they share the
same opinion topic. In particular, we conjec-
ture that judging whether or not two opinions are
topic-coreferent is easier than specifying the topic
of each opinion (due to the problems described
above).
5 Constructing the MPQA
TOPIC
Corpus
Relying on the notion of topic coreference, we next
introduce a newmethodology for the manual anno-
tation of opinion topics in text:
1. The annotator begins with a corpus of documents that
has been annotated w.r.t. OPINION EXPRESSIONS. With
each opinion expression, the corpus provides POLARITY and
OPINION HOLDER information. (We use the aforementioned
MPQA corpus.)
2. The annotator maintains a list of the opinion expressions
that remain to be annotated (initially, all opinion expressions
in the document) as well as a list of the current groupings (i.e.
clusters) of opinion expressions that have been identified as
topic-coreferent (initially this list is empty).
3. For each opinion expression, in turn, the annotator decides
whether the opinion is on the same topic as the opinions in
one of the existing clusters or should start a new cluster, and
inserts the opinion in the appropriate cluster.
4. The annotator labels each cluster with a string that de-
scribes the opinion topic that covers all opinions in the cluster.
5. The annotator marks the TOPIC SPAN of each opinion.
(This can be done at any point in the process.)
The manual annotation procedure is de-
scribed in a set of instructions available at
http://www.cs.cornell.edu/?ves. In addition, we
created a GUI that facilitates the annotation proce-
dure. With the help of these resources, one person
annotated opinion topics for a randomly selected
set of 150 of the 535 documents in the MPQA
corpus. In addition, 20 of the 150 documents were
selected at random and annotated by a second
annotator for the purposes of an inter-annotator
agreement study, the results of which are presented
in Section 8.1. The MPQA
TOPIC
and the procedure
by which it was created are described in more
detail in (Stoyanov and Cardie, 2008).
6 The Topic Coreference Algorithm
As mentioned in Section 4, our computational ap-
proach to opinion topic identification is based on
topic coreference: For each document (1) find the
clusters of coreferent opinions, and (2) label the
clusters with the name of the topic. In this paper
we focus only on the first task, topic coreference
resolution ? the most critical step for topic identi-
fication. We conjecture that the second step can be
performed through frequency analysis of the terms
in each of the clusters and leave it for future work.
Topic coreference resolution resembles another
well-known problem in NLP ? noun phrase (NP)
coreference resolution. Therefore, we adapt a
standard machine learning-based approach to NP
coreference resolution (Soon et al, 2001; Ng and
Cardie, 2002) for our purposes. Our adaptation has
three steps: (i) identify the topic spans; (ii) perform
pairwise classification of the associated opinions
as to whether or not they are topic-coreferent; and,
(iii) cluster the opinions according to the results of
(ii). Each step is discussed in more detail below.
6.1 Identifying Topic Spans
Decisions about topic coreference should depend
on the text spans that express the topic. Ideally,
we would be able to recover the topic span of each
opinion and use its content for the topic corefer-
ence decision. However, the topic span depends on
the topic itself, so it is unrealistic that topic spans
can be recovered with simple methods. Neverthe-
less, in this initial work, we investigate two sim-
820
ple methods for automatic topic span identification
and compare them to two manual approaches:
? Sentence. Assume that the topic span is the
whole sentence containing the opinion.
? Automatic. A rule-based method for identi-
fying the topic span (developed using MPQA
documents that are not part of MPQA
TOPIC
).
Rules depend on the syntactic constituent
type of the opinion expression and rely on
syntactic parsing and grammatical role label-
ing.
? Manual. Use the topic span marked by the
human annotator. We included this method
to provide an upper bound on performance of
the topic span extractor.
? Modified Manual. Meant to be a more real-
istic use of the manual topic span annotations,
this method returns the manually identified
topic span only when it is within the sentence
of the opinion expression. When this span
is outside the sentence boundary, this method
returns the opinion sentence.
Of the 4976 opinions annotated across the 150
documents of MPQA
TOPIC
, the topic spans associ-
ated with 4293 were within the same sentence as
the opinion; 3653 were within the span extracted
by our topic span extractor. Additionally, the topic
spans of 173 opinions were outside of the para-
graph containing the opinion.
6.2 Pairwise Topic Coreference Classification
The heart of our method is a pairwise topic coref-
erence classifier. Given a pair of opinions (and
their associated polarity and opinion holder infor-
mation), the goal of the classifier is to determine
whether the opinions are topic-coreferent. We use
the manually annotated data to automatically learn
the pairwise classifier. Given a training document,
we construct a training example for every pair of
opinions in the document (each pair is represented
as a feature vector). The pair is labeled as a posi-
tive example if the two opinions belong to the same
topic cluster, and a negative example otherwise.
Pairwise coreference classification relies criti-
cally on the expressiveness of the features used
to describe the opinion pair. We use three cate-
gories of features: positional, lexico-semantic and
opinion-based features.
Positional features These features are intended
to exploit the fact that opinions that are close to
each other are more likely to be on the same topic.
We use six positional features:
? Same Sentence/Paragraph
5
True if the two
opinions are in the same sentence/paragraph.
? Consecutive Sentences/Paragraphs True if
the two opinions are in consecutive sen-
tences/paragraphs.
? Number of Sentences/Paragraphs The
number of sentences/paragraphs that separate
the two opinions.
TOPIC SPAN-based lexico-semantic features The
features in this group rely on the topic spans and
are recomputed w.r.t. each of the four topic span
methods. The intuition behind this group of fea-
tures is that topic-coreferent opinions are likely to
exhibit lexical and semantic similarity within the
topic span.
? tf.idf The cosine similarity of the tf.idf
weighted vectors of the terms contained in the
two spans.
? Word overlap True if the two topic spans
contain any contain words in common.
? NP coref True if the two spans contain NPs
that are determined to be coreferent by a sim-
ple rule-based coreference system.
? NE overlap True if the two topic spans con-
tain named entities that can be considered
aliases of each other.
Opinion features The features in this group de-
pend on the attributes of the opinion. In the cur-
rent work, we obtain these features directly from
the manual annotations of the MPQA
TOPIC
corpus,
but they might also be obtained from automatically
identified opinion information using the methods
referenced in Section 3.
? Source Match True if the two opinions have
the same opinion holder.
? Polarity Match True if the two opinions have
the same polarity.
5
We use sentence/paragraph to describe two features ? one
based on the sentence and one on the paragraph.
821
? Source-PolarityMatch False if the two opin-
ions have the same opinion holder but con-
flicting polarities (since it is unlikely that a
source will have two opinions with conflict-
ing polarities on the same topic).
We employ three classifiers for pairwise corefer-
ence classification ? an averaged perceptron (Fre-
und and Schapire, 1998), SVM
light
(Joachims,
1998) and a rule-learner ? RIPPER (Cohen, 1995).
However, we report results only for the averaged
perceptron, which exhibited the best performance.
6.3 Clustering
Pairwise classification provides an estimate of the
likelihood that two opinions are topic-coreferent.
To form the topic clusters, we follow the pairwise
classification with a clustering step. We selected
a simple clustering algorithm ? single-link cluster-
ing, which has shown good performance for NP
coreference. Given a threshold, single-link cluster-
ing proceeds by assigning pairs of opinions with a
topic-coreference score above the threshold to the
same topic cluster and then performs transitive clo-
sure of the clusters.
6
7 Evaluation Methodology
For training and evaluation we use the 150-
document MPQA
TOPIC
corpus. All machine learn-
ing methods were tested via 10-fold cross valida-
tion. In each round of cross validation, we use
eight of the data partitions for training and one for
parameter estimation (we varied the threshold for
the clustering algorithm), and test on the remaining
partition. We report results for the three evaluation
measures of Section 7 using the four topic span
extraction methods introduced in Section 6. The
threshold is tuned separately for each evaluation
measure. As noted earlier, all runs obtain opinion
information from the MPQA
TOPIC
corpus (i.e. this
work does not incorporate automatic opinion ex-
traction).
7.1 Topic Coreference Baselines
We compare our topic coreference system to four
baselines. The first two are the ?default? baselines:
? one topic ? assigns all opinions to the same
cluster.
6
Experiments using best-first and last-first clustering ap-
proaches provided similar or worse results.
? one opinion per cluster ? assigns each opin-
ion to its own cluster.
The other two baselines attempt to perform topic
segmentation (discussed in Section 3) and assign
all opinions within the same segment to the same
opinion topic:
? same paragraph ? simple topic segmenta-
tion by splitting documents into segments at
paragraph boundaries.
? Choi 2000 ? Choi?s (2000) state-of-the-art
approach to finding segment boundaries. We
use the freely available C99 software de-
scribed in Choi (2000), varying a parameter
that allows us to control the average number
of sentences per segment and reporting the
best result on the test data.
7.2 Evaluation Metrics
Because there is disagreement among researchers
w.r.t. the proper evaluation measure for NP coref-
erence resolution, we use three generally accepted
metrics
7
to evaluate our topic coreference system.
B-CUBED. B-CUBED (B
3
) is a commonly
used NP coreference metric (Bagga and Baldwin,
1998). It calculates precision and recall for each
item (in our case, each opinion) based on the num-
ber of correctly identified coreference links, and
then computes the average of the item scores in
each document. Precision/recall for an item i is
computed as the proportion of items in the inter-
section of the response (system-generated) and key
(gold standard) clusters containing i divided by the
number of items in the response/key cluster.
CEAF. As a representative of another group of
coreference measures that rely on mapping re-
sponse clusters to key clusters, we selected Luo?s
(2005) CEAF score (short for Constrained Entity-
Alignment F-Measure). Similar to the ACE (2005)
score, CEAF operates by computing an optimal
mapping of response clusters to key clusters and
assessing the goodness of the match of each of the
mapped clusters.
Krippendorff?s ?. Finally, we use Passonneau?s
(2004) generalization of Krippendorff?s (1980) ?
? a standard metric employed for inter-annotator
7
The MUC scoring algorithm (Vilain et al, 1995) was
omitted because it led to an unjustifiably high MUC F-score
(.920) for the ONE TOPIC baseline.
822
B3
? CEAF
All opinions .6424 .5476 .6904
Sentiment opinions .7180 .7285 .7967
Strong opinions .7374 .7669 .8217
Table 1: Inter-annotator agreement results.
reliability studies. Krippendorff?s ? is based
on a probabilistic interpretation of the agreement
of coders as compared to agreement by chance.
While Passonneau?s innovation makes it possible
to apply Krippendorff?s ? to coreference clusters,
the probabilistic interpretation of the statistic is un-
fortunately lost.
8 Results
8.1 Inter-annotator Agreement
As mentioned previously, out of the 150 anno-
tated documents, 20 were annotated by two anno-
tators for the purpose of studying the agreement
between coders. Inter-annotator agreement results
are shown in Table 1. We compute agreement for
three subsets of opinions: all available opinions,
only the sentiment-bearing opinions and the sub-
set of sentiment-bearing opinions judged to have
polarity of medium or higher.
The results support our conjecture that topics
of sentiment-bearing opinions are much easier to
identify: inter-annotator agreement for opinions
with non-neutral polarity (SENTIMENT OPINIONS) im-
proves by a large margin for all measures. As in
other work in subjectivity annotation, we find that
strong sentiment-bearing opinions are easier to an-
notate than sentiment-bearing opinions in general.
Generally, the ? score aims to probabilistically
capture the agreement of annotation data and sep-
arate it from chance agreement. It is generally ac-
cepted that an ? score of .667 indicates reliable
agreement. The score that we observed for the
overall agreement was an? of .547, which is below
the generally accepted level, while ? for the two
subsets of sentiment-bearing opinions is above .72.
However, as discussed above, due to the way that
it is adapted to the problem of coreference resolu-
tion, the ? score loses its probabilistic interpreta-
tion. For example, the ? score requires that a pair-
wise distance function between clusters is speci-
fied. We used one sensible choice for such a func-
tion (we measured the distance between clusters A
and B as dist(A,B) = (2? |A?B|)/(|A|+ |B|)),
B
3
? CEAF
One topic .3739 -.1017 .2976
One opinion per cluster .2941 .2238 .2741
Same paragraph .5542 .3123 .5090
Choi .5399 .3734 .5370
Sentence .5749 .4032 .5393
Rule-based .5730 .4056 .5420
Modified manual .6416 .5134 .6124
Manual .7097 .6585 .6184
Table 2: Results for the topic coreference algo-
rithms.
but other sensible choices for the distance lead to
much higher scores. Furthermore, we observed
that the behavior of the ? score can be rather er-
ratic ? small changes in one of the clusterings can
lead to big differences in the score.
Perhaps a better indicator of the reliability of
the coreference annotation is a comparison with
the baselines, shown in the top half of Table 2.
All baselines score significantly lower than the
inter-annotator agreement scores. With one excep-
tion, the inter-annotator agreement scores are also
higher than those for the learning-based approach
(results shown in the lower half of Table 2), as
would typically be expected. The exception is the
classifier that uses the manual topic spans, but as
we argued earlier these spans carry significant in-
formation about the decision of the annotator.
8.2 Baselines
Results for the four baselines are shown in the first
four rows of Table 2. As expected, the two base-
lines performing topic segmentation show substan-
tially better scores than the two ?default? base-
lines.
8.3 Learning methods
Results for the learning-based approaches are
shown in the bottom half of Table 2. First, we
see that each of the learning-based methods out-
performs the baselines. This is the case even when
sentences are employed as a coarse substitute for
the true topic span. A Wilcoxon Signed-Rank test
shows that differences from the baselines for the
learning-based runs are statistically significant for
the B
3
and ? measures (p < 0.01); for CEAF,
using sentences as topic spans for the learning al-
gorithm outperforms the SAME PARAGRAPH baseline
(p < 0.05), but the results are inconclusive when
823
compared with the system of CHOI.
In addition, relying on manual topic span infor-
mation (MANUAL and MODIFIED MANUAL) allows the
learning-based approach to perform significantly
better than the two runs that use automatically
identified spans (p < 0.01, for all three measures).
The improvement in the scores hints at the impor-
tance of improving automatic topic span extrac-
tion, which will be a focus of our future work.
9 Conclusions
We presented a new, operational definition of opin-
ion topics in the context of fine-grained subjec-
tivity analysis. Based on this definition, we in-
troduced an approach to opinion topic identifi-
cation that relies on the identification of topic-
coreferent opinions. We further employed the
opinion topic definition for the manual annotation
of opinion topics to create the MPQA
TOPIC
corpus.
Inter-annotator agreement results show that opin-
ion topic annotation can be performed reliably.
Finally, we proposed an automatic approach for
identifying topic-coreferent opinions, which sig-
nificantly outperforms all baselines across three
coreference evaluation metrics.
Acknowledgments The authors of this paper
would like to thank Janyce Wiebe and Theresa
Wilson for many insightful discussions. This work
was supported in part by National Science Foun-
dation Grants BCS- 0624277 and IIS-0535099 and
by DHS Grant N0014-07-1-0152.
References
ACE. 2005. The NIST ACE evaluation website.
http://www.nist.gov/speech/tests/ace/.
Bagga, A. and B. Baldwin. 1998. Algorithms for scoring
coreference chains. In In Proceedings of MUC7.
Bethard, S., H. Yu, A. Thornton, V. Hativassiloglou, and
D. Jurafsky. 2004. Automatic extraction of opinion propo-
sitions and their holders. In 2004 AAAI Spring Symposium
on Exploring Attitude and Affect in Text.
Breck, E., Y. Choi, and C. Cardie. 2007. Identifying expres-
sions of opinion in context. In Proceedings of IJCAI.
Choi, Y., C. Cardie, E. Riloff, and S. Patwardhan. 2005. Iden-
tifying sources of opinions with conditional random fields
and extraction patterns. In Proceedings of EMNLP.
Choi, Y., E. Breck, and C. Cardie. 2006. Joint extraction of
entities and relations for opinion recognition. In Proceed-
ings of EMNLP.
Choi, F. 2000. Advances in domain independent linear text
segmentation. Proceedings of NAACL.
Cohen, W. 1995. Fast effective rule induction. In Proceed-
ings of ICML.
Freund, Y. and R. Schapire. 1998. Large margin classifi-
cation using the perceptron algorithm. In Proceedings of
Computational Learing Theory.
Hasegawa, T., S. Sekine, and R. Grishman. 2004. Discover-
ing relations among named entities from large corpora. In
Proceedings of ACL.
Hu, M. and B. Liu. 2004. Mining opinion features in cus-
tomer reviews. In AAAI.
Joachims, T. 1998. Making large-scale support vector ma-
chine learning practical. In B. Sch?olkopf, C. Burges,
A. Smola, editor, Advances in Kernel Methods: Support
Vector Machines. MIT Press, Cambridge, MA.
Kim, S. and E. Hovy. 2006. Extracting opinions, opinion
holders, and topics expressed in online news media text.
In Proceedings of ACL/COLING Workshop on Sentiment
and Subjectivity in Text.
Kobayashi, N., K. Inui, Y. Matsumoto, K. Tateishi, and
T. Fukushima. 2004. Collecting evaluative expressions
for opinion extraction. In Proceedings of IJCNLP.
Krippendorff, K. 1980. Content Analysis: An Introduction to
Its Methodology. Sage Publications, Beverly Hills, CA.
Luo, X. 2005. On coreference resolution performance met-
rics. In Proceedings of EMNLP.
Malioutov, I. and R. Barzilay. 2006. Minimum cut model
for spoken lecture segmentation. In Proceedings of
ACL/COLING.
Ng, V. and C. Cardie. 2002. Improving machine learning
approaches to coreference resolution. In In Proceedings of
ACL.
Pang, B., L. Lee, and S. Vaithyanathan. 2002. Thumbs
up? Sentiment classification using machine learning tech-
niques. In Proceedings of EMNLP.
Passonneau, R. 2004. Computing reliability for coreference
annotation. In Proceedings of LREC.
Popescu, A. and O. Etzioni. 2005. Extracting product
features and opinions from reviews. In Proceedings of
HLT/EMNLP.
Rosenfeld, B. and R. Feldman. 2007. Clustering for unsuper-
vised relation identification. In Proceedings of CIKM.
Soon, W., H. Ng, and D. Lim. 2001. A machine learning
approach to coreference resolution of noun phrases. Com-
putational Linguistics, 27(4).
Stoyanov, V. and C. Cardie. 2008. Annotating topics of opin-
ions. In Proceedings of LREC.
Turney, P. 2002. Thumbs up or thumbs down? Semantic ori-
entation applied to unsupervised classification of reviews.
In Proceedings of ACL.
Vilain, M., J. Burger, J. Aberdeen, D. Connolly, and
L. Hirschman. 1995. A model-theoretic coreference scor-
ing scheme. In Proceedings of the MUC6.
Voorhees, E. and L. Buckland. 2003. Overview of the
TREC 2003 Question Answering Track. In Proceedings
of TREC 12.
Wiebe, J., T. Wilson, and C. Cardie. 2005. Annotating ex-
pressions of opinions and emotions in language. Language
Resources and Evaluation, 1(2).
Wiebe, J. 2005. Personal communication.
Wilson, T., J. Wiebe, and P. Hoffmann. 2005. Recognizing
contextual polarity in phrase-level sentiment analysis. In
Proceedings of HLT/EMNLP.
Wilson, T. 2005. Personal communication.
Yi, J., T. Nasukawa, R. Bunescu, and W. Niblack. 2003. Sen-
timent analyzer: Extracting sentiments about a given topic
using natural language processing techniques. In Proceed-
ings of ICDM.
824
Coling 2008: Companion volume ? Posters and Demonstrations, pages 15?18
Manchester, August 2008
The power of negative thinking:
Exploiting label disagreement in the min-cut classification framework
Mohit Bansal
Dept. of Computer Science & Engineering
Indian Institute of Technology Kanpur
mbansal47@gmail.com
Claire Cardie and Lillian Lee
Dept. of Computer Science
Cornell University
{cardie,llee}@cs.cornell.edu
Abstract
Treating classification as seeking minimum
cuts in the appropriate graph has proven ef-
fective in a number of applications. The
power of this approach lies in its abil-
ity to incorporate label-agreement prefer-
ences among pairs of instances in a prov-
ably tractable way. Label disagreement
preferences are another potentially rich
source of information, but prior NLP work
within the minimum-cut paradigm has not
explicitly incorporated it. Here, we re-
port on work in progress that examines
several novel heuristics for incorporating
such information. Our results, produced
within the context of a politically-oriented
sentiment-classification task, demonstrate
that these heuristics allow for the addition
of label-disagreement information in a way
that improves classification accuracy while
preserving the efficiency guarantees of the
minimum-cut framework.
1 Introduction
Classification algorithms based on formulating the
classification task as one of finding minimum s-t
cuts in edge-weighted graphs ? henceforth min-
imum cuts or min cuts ? have been successfully
employed in vision, computational biology, and
natural language processing. Within NLP, appli-
cations include sentiment-analysis problems (Pang
and Lee, 2004; Agarwal and Bhattacharyya, 2005;
Thomas et al, 2006) and content selection for text
generation (Barzilay and Lapata, 2005).
c
? 2008. Licensed under the Creative Commons
Attribution-Noncommercial-Share Alike 3.0 Unported li-
cense (http://creativecommons.org/licenses/by-nc-sa/3.0/).
Some rights reserved.
As a classification framework, the minimum-
cut approach is quite attractive. First, it provides
a principled, yet flexible mechanism for allowing
problem-specific relational information ? includ-
ing several types of both hard and soft constraints
? to influence a collection of classification deci-
sions. Second, in many important cases, such as
when all the edge weights are non-negative, find-
ing a minimum cut can be done in a provably effi-
cient manner.
To date, however, researchers have restricted the
semantics of the constraints mentioned above to
encode pair-wise ?agreement? information only.
There is a computational reason for this restriction:
?agreement? and ?disagreement? information are
arguably most naturally expressed via positive and
negative edge weights, respectively; but in general,
the inclusion of even a relatively small number of
negative edge weights makes finding a minimum
cut NP-hard (McCormick et al, 2003).
To avoid this computational issue, we propose
several heuristics that encode disagreement infor-
mation with non-negative edge weights. We in-
stantiate our approach on a sentiment-polarity clas-
sification task ? determining whether individual
conversational turns in U.S. Congressional floor
debates support or oppose some given legislation.
Our preliminary results demonstrate promising im-
provements over the prior work of Thomas et al
(2006), who considered only the use of agreement
information in this domain.
2 Method
2.1 Min-cut classification framework
Binary classification problems are usually ap-
proached by considering each classification deci-
sion in isolation. More formally, let X
test
=
15
{x
1
, x
2
, . . . , x
n
} be the test instances, drawn from
some universe X , and let C = {c
1
, c
2
} be
the two possible classes. Then, the usual ap-
proach can often be framed as labeling each x
i
according to some individual-preference function
Ind:X ? C?<, such as the signed distance to
the dividing hyperplane according to an SVM or
the posterior class probability assigned by a Naive
Bayes classifier.
But when it is difficult to accurately classify a
particular x
i
in isolation, there is a key insight
that can help: knowing that x
i
has the same la-
bel as an easily-categorized x
j
makes labeling x
i
easy. Thus, suppose we also have an association-
preference function Assoc:X ?X?< express-
ing a reward for placing two items in the same
class; an example might be the output of an agree-
ment classifier or a similarity function. Then, we
can search for a classification function c(x
i
|X
test
)
? note that all of X
test
can affect an instance?s la-
bel ? that minimizes the total ?pining? of the test
items for the class they were not assigned to due to
either their individual or associational preferences:
?
i
Ind(x
i
, c(x
i
|X
test
)) +
??
?
i,j:c(x
i
|X
test
)=c(x
j
|X
test
)
Assoc(x
i
, x
j
),
where c(x
i
|X
test
) is the class ?opposite? to
c(x
i
|X
test
), and the free parameter ? regulates
the emphasis on agreement information. Solutions
to the above minimization problem correspond to
minimum s-t cuts in a certain graph, and if both
Ind and Assoc are non-negative functions, then,
surprisingly, minimum cuts can be found in poly-
nomial time; see Kleinberg and Tardos (2006, Sec-
tion 7.10) for details. But, as mentioned above,
allowing negative values makes finding a solution
intractable in the general case.
2.2 Prior work discards some negative values
The starting point for our work is Thomas et
al. (2006) (henceforth TPL). The reason for this
choice is that TPL used minimum-cut-based classi-
fication wherein signed distances to dividing SVM
hyperplanes were employed to define Ind(x, c)
and Assoc(x, x?). It was natural to use SVMs,
since association was determined by classification
rather than similarity ? specifically, categorizing
references by one congressperson to another as re-
flecting agreement or not ? but as a result, neg-
ative association-preferences (e.g., negative dis-
tance to a hyperplane) had to be accounted for.
We formalize TPL?s approach at a high
level as follows. Let Ind?:X ? C?< and
Assoc
?
:X ?X?< be initial individual- and
association-preference functions, such as the
signed distances mentioned above. TPL create two
non-negative conversion functions f :<? [0, 1]
and g:<? [0, 1], and then define
Ind(x
i
, c) := f(Ind
?
(x
i
, c))
Assoc(x
i
, x
j
) := g(Assoc
?
(x
i
, x
j
))
so that an optimal classification can be found in
polynomial time, as discussed above. We omit the
exact definitions of f and g in order to focus on
what is important here: roughly speaking, f and
g normalize values and handle outliers1, with the
following crucial exception. While negative initial
individual preferences for one class can be trans-
lated into positive individual preferences for the
other, there is no such mechanism for negative val-
ues of Assoc?; so TPL resort to defining g to be
0 for negative arguments. They thus discard po-
tentially key information regarding the strength of
label disagreement preferences.
2.3 Encoding negative associations
Instead of discarding the potentially crucial label-
disagreement information represented by negative
Assoc
? values, we propose heuristics that seek to
incorporate this valuable information, but that keep
Ind and Assoc non-negative (by piggy-backing off
of TPL?s pre-existing conversion-function strat-
egy2) to preserve the min-cut-classification effi-
ciency guarantees.
We illustrate our heuristics with a running
example. Consider a simplified setting with only
two instances x
1
and x
2
; f(z) = z; g(z) = 0 if
z < 0, 1 otherwise; and Ind? values (numbers
labeling left or right arrows in the diagrams below,
e.g., Ind?(x
1
, c
1
) = .7) and Assoc? value (the -2
labeling the up-and-down arrow) as depicted here:
? [.7]? x
1
?[.3]?
c
1
m [?2] c
2
? [.6]? x
2
?[.4]?
Then, the resulting TPL Ind and Assoc values are
1Thus, strictly speaking, f and g also depend on Ind?,
Assoc
?
, and X
test
, but we suppress this dependence for nota-
tional compactness.
2Our approach also applies to definitions of f and g dif-
ferent from TPL?s.
16
? [.7]? x
1
?[.3]?
c
1
m [0] c
2
? [.6]? x
2
?[.4]?
Note that since the initial -2 association value is
ignored, c(x
1
|X
test
) = c(x
2
|X
test
) = c
1
appears
to be a good classification according to TPL.
The Scale all up heuristic Rather than discard
disagreement information, a simple strategy is to
just scale up all initial association preferences by a
sufficiently large positive constant N :
Ind(x
i
, c) := f(Ind
?
(x
i
, c))
Assoc(x
i
, x
j
) := g(Assoc
?
(x
i
, x
j
) + N)
For N = 3 in our example, we get
? [.7]? x
1
?[.3]?
c
1
m [1] c
2
? [.6]? x
2
?[.4]?
This heuristic ensures that the more negative the
Assoc
? value, the lower the cost of separating the
relevant item pair (whereas TPL don?t distinguish
between negative Assoc? values). The heuristic
below tries to be more proactive, forcing such
pairs to receive different labels.
The SetTo heuristic We proceed through
x
1
, x
2
, . . . in order. Each time we encounter
an x
i
where Assoc?(x
i
, x
j
) < 0 for some
j > i, we try to force x
i
and x
j
into dif-
ferent classes by altering the four relevant
individual-preferences affecting this pair of in-
stances, namely, f(Ind?(x
i
, c
1
)), f(Ind
?
(x
i
, c
2
)),
f(Ind
?
(x
j
, c
1
)), and f(Ind?(x
j
, c
2
)). Assume
without loss of generality that the largest of
these values is the first one. If we respect
that preference to put x
i
in c
1
, then according
to the association-preference information, it
follows that we should put x
j
in c
2
. We can
instantiate this chain of reasoning by setting
Ind(x
i
, c
1
) := max(?, f(Ind
?
(x
i
, c
1
)))
Ind(x
i
, c
2
) := min(1? ?, f(Ind
?
(x
i
, c
2
)))
Ind(x
j
, c
1
) := min(1? ?, f(Ind
?
(x
j
, c
1
)))
Ind(x
j
, c
2
) := max(?, f(Ind
?
(x
j
, c
2
)))
for some constant ? ? (.5, 1], and making no
change to TPL?s definition of Assoc. For ? = .8
in our example, we get
? [.8]? x
1
?[.2]?
c
1
m [0] c
2
? [.2]? x
2
?[.8]?
Note that as we proceed through x
1
, x
2
, . . . in
order, some earlier changes may be undone.
The IncBy heuristic A more conservative ver-
sion of the above heuristic is to increment and
decrement the individual-preference values so that
they are somewhat preserved, rather than com-
pletely replace them with fixed constants:
Ind(x
i
, c
1
) := min(1, f(Ind
?
(x
i
, c
1
)) + ?)
Ind(x
i
, c
2
) := max(0, f(Ind
?
(x
i
, c
2
))? ?)
Ind(x
j
, c
1
) := max(0, f(Ind
?
(x
j
, c
1
))? ?)
Ind(x
j
, c
2
) := min(1, f(Ind
?
(x
j
, c
2
)) + ?)
For ? = .1, our example becomes
? [.8]? x
1
?[.2]?
c
1
m [0] c
2
? [.5]? x
2
?[.5]?
3 Evaluation
For evaluation, we adopt the sentiment-
classification problem tackled by TPL: clas-
sifying speech segments (individual conversational
turns) in a U.S. Congressional floor debate as
to whether they support or oppose the legis-
lation under discussion. TPL describe many
reasons why this is an important problem. For
our purposes, this task is also very convenient
because all of TPL?s computed raw and converted
Ind
? and Assoc? data are publicly available at
www.cs.cornell.edu/home/llee/data/convote.html.
Thus, we used their calculated values to imple-
ment our algorithms as well as to reproduce their
original results.3
One issue of note is that TPL actually in-
ferred association preferences between speakers,
not speech segments. We do the same when ap-
plying SetTo or IncBy to a pair {x
i
, x
j
} by con-
sidering the average of f(Ind?(x
k
, c
1
)) over all
x
k
uttered by the speaker of x
i
, instead of just
f(Ind
?
(x
i
, c
1
)). The other three relevant individ-
ual values are treated similarly. We also make
appropriate modifications (according to SetTo and
IncBy) to the individual preferences of all such x
k
simultaneously, not just x
i
, and similarly for x
j
.
A related issue is that TPL assume that all
speech segments by the same speaker should have
the same label. To make experimental compar-
isons meaningful, we follow TPL in considering
two different instantiations of this assumption. In
segment-based classification, Assoc(x
i
, x
j
) is set
to an arbitrarily high positive constant if the same
speaker uttered both x
i
and x
j
. In speaker-based
classification, Ind?(x
i
, c) is produced by running
3For brevity, we omit TPL?s ?high-threshold? variants.
17
 60
 62
 64
 66
 68
 70
 72
 74
 76
 78
SetTo(.6)SVM SetTo(1) IncBy(.25)IncBy(.15)TPL IncBy(.05)Scale all up SetTo(.8)
pe
rc
en
t c
or
re
ct
ALGORITHMS
Test-set classification accuracies, using held-out parameter estimation
segment-based, test
speaker-based, test
best TPL, test
Figure 1: Experimental results. ?SVM?: classification using only individual-preference information.
Values of ? are indicated in parentheses next to the relevant algorithm names.
an SVM on the concatenation of all speeches ut-
tered by x
i
?s speaker.
Space limits preclude inclusion of further de-
tails; please see TPL for more information.
3.1 Results and future plans
The association-emphasis parameter ? was trained
on held-out data, with ties broken in favor of the
largest ? in order to emphasize association in-
formation. We used Andrew Goldberg?s HIPR
code (http://avglab.com/andrew/soft.html) to com-
pute minimum cuts. The resultant test-set classifi-
cation accuracies are presented in Figure 1.
We see that Scale all up performs worse
than TPL, but the more proactive heuristics
(SetTo, IncBy) almost always outperform TPL on
segment-based classification, sometimes substan-
tially so, and outperform TPL on speaker-based
classification for half of the variations. We there-
fore conclude that label disagreement informa-
tion is indeed valuable; and that incorporating la-
bel disagreement information on top of the (posi-
tive) label agreement information that TPL lever-
aged can be achieved using simple heuristics; and
that good performance enhancements result with-
out any concomitant significant loss of efficiency.
These results are preliminary, and the diver-
gence in behaviors between different heuristics
in different settings requires investigation. Ad-
ditional future work includes investigating more
sophisticated (but often therefore less tractable)
formalisms for joint classification; and looking
at whether approximation algorithms for finding
minimum cuts in graphs with negative edge capac-
ities can be effective.
Acknowledgments We thank Jon Kleinberg and the
reviewers for helpful comments. Portions of this work
were done while the first author was visiting Cornell Uni-
versity. This paper is based upon work supported in part
by the National Science Foundation under grant nos. IIS-
0329064, BCS-0624277, and IIS-0535099, a Cornell Univer-
sity Provost?s Award for Distinguished Scholarship, a Yahoo!
Research Alliance gift, an Alfred P. Sloan Research Fellow-
ship, and by DHS grant N0014-07-1-0152. Any opinions,
findings, and conclusions or recommendations expressed are
those of the authors and do not necessarily reflect the views
or official policies, either expressed or implied, of any spon-
soring institutions, the U.S. government, or any other entity.
References
A. Agarwal, P. Bhattacharyya. 2005. Sentiment analysis: A
new approach for effective use of linguistic knowledge and
exploiting similarities in a set of documents to be classi-
fied. ICON.
R. Barzilay, M. Lapata. 2005. Collective content selection for
concept-to-text generation. HLT/EMNLP, pp. 331?338.
J. Kleinberg, ?E. Tardos. 2006. Algorithm Design. Addison
Wesley.
S. T. McCormick, M. R. Rao, G. Rinaldi. 2003. Easy and dif-
ficult objective functions for max cut. Mathematical Pro-
gramming, Series B(94):459?466.
B. Pang, L. Lee. 2004. A sentimental education: Sentiment
analysis using subjectivity summarization based on mini-
mum cuts. ACL, pp. 271?278.
M. Thomas, B. Pang, L. Lee. 2006. Get out the vote: De-
termining support or opposition from Congressional floor-
debate transcripts. EMNLP, pp. 327?335.
18
Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 793?801,
Honolulu, October 2008. c?2008 Association for Computational Linguistics
Learning with Compositional Semantics as Structural Inference for
Subsentential Sentiment Analysis
Yejin Choi and Claire Cardie
Department of Computer Science
Cornell University
Ithaca, NY 14853
{ychoi,cardie}@cs.cornell.edu
Abstract
Determining the polarity of a sentiment-
bearing expression requires more than a sim-
ple bag-of-words approach. In particular,
words or constituents within the expression
can interact with each other to yield a particu-
lar overall polarity. In this paper, we view such
subsentential interactions in light of composi-
tional semantics, and present a novel learning-
based approach that incorporates structural in-
ference motivated by compositional seman-
tics into the learning procedure. Our exper-
iments show that (1) simple heuristics based
on compositional semantics can perform bet-
ter than learning-based methods that do not in-
corporate compositional semantics (accuracy
of 89.7% vs. 89.1%), but (2) a method that
integrates compositional semantics into learn-
ing performs better than all other alterna-
tives (90.7%). We also find that ?content-
word negators?, not widely employed in pre-
vious work, play an important role in de-
termining expression-level polarity. Finally,
in contrast to conventional wisdom, we find
that expression-level classification accuracy
uniformly decreases as additional, potentially
disambiguating, context is considered.
1 Introduction
Determining the polarity of sentiment-bearing ex-
pressions at or below the sentence level requires
more than a simple bag-of-words approach. One of
the difficulties is that words or constituents within
the expression can interact with each other to yield
a particular overall polarity. To facilitate our discus-
sion, consider the following examples:
1: [I did [not]? have any [doubt]? about it.]+
2: [The report [eliminated]? my [doubt]?.]+
3: [They could [not]? [eliminate]? my [doubt]?.]?
In the first example, ?doubt? in isolation carries
a negative sentiment, but the overall polarity of the
sentence is positive because there is a negator ?not?,
which flips the polarity. In the second example, both
?eliminated? and ?doubt? carry negative sentiment
in isolation, but the overall polarity of the sentence
is positive because ?eliminated? acts as a negator for
its argument ?doubt?. In the last example, there are
effectively two negators ? ?not? and ?eliminated? ?
which reverse the polarity of ?doubt? twice, result-
ing in the negative polarity for the overall sentence.
These examples demonstrate that words or con-
stituents interact with each other to yield the
expression-level polarity. And a system that sim-
ply takes the majority vote of the polarity of indi-
vidual words will not work well on the above exam-
ples. Indeed, much of the previous learning-based
research on this topic tries to incorporate salient in-
teractions by encoding them as features. One ap-
proach includes features based on contextual va-
lence shifters1 (Polanyi and Zaenen, 2004), which
are words that affect the polarity or intensity of sen-
timent over neighboring text spans (e.g., Kennedy
and Inkpen (2005), Wilson et al (2005), Shaikh et
al. (2007)). Another approach encodes frequent sub-
sentential patterns (e.g., McDonald et al (2007)) as
features; these might indirectly capture some of the
subsentential interactions that affect polarity. How-
1For instance, ?never?, ?nowhere?, ?little?, ?most?, ?lack?,
?scarcely?, ?deeply?.
793
ever, both types of approach are based on learning
models with a flat bag-of-features: some structural
information can be encoded as higher order features,
but the final representation of the input is still a flat
feature vector that is inherently too limited to ade-
quately reflect the complex structural nature of the
underlying subsentential interactions. (Liang et al,
2008)
Moilanen and Pulman (2007), on the other hand,
handle the structural nature of the interactions more
directly using the ideas from compositional seman-
tics (e.g., Montague (1974), Dowty et al (1981)). In
short, the Principle of Compositionality states that
the meaning of a compound expression is a func-
tion of the meaning of its parts and of the syntac-
tic rules by which they are combined (e.g., Mon-
tague (1974), Dowty et al (1981)). And Moilanen
and Pulman (2007) develop a collection of compo-
sition rules to assign a sentiment value to individual
expressions, clauses, or sentences. Their approach
can be viewed as a type of structural inference, but
their hand-written rules have not been empirically
compared to learning-based alternatives, which one
might expect to be more effective in handling some
aspects of the polarity classification task.
In this paper, we begin to close the gap between
learning-based approaches to expression-level po-
larity classification and those founded on composi-
tional semantics: we present a novel learning-based
approach that incorporates structural inference mo-
tivated by compositional semantics into the learning
procedure.
Adopting the view point of compositional seman-
tics, our working assumption is that the polarity of a
sentiment-bearing expression can be determined in a
two-step process: (1) assess the polarities of the con-
stituents of the expression, and then (2) apply a rela-
tively simple set of inference rules to combine them
recursively. Rather than a rigid application of hand-
written compositional inference rules, however, we
hypothesize that an ideal solution to the expression-
level polarity classification task will be a method
that can exploit ideas from compositional seman-
tics while providing the flexibility needed to handle
the complexities of real-world natural language ?
exceptions, unknown words, missing semantic fea-
tures, and inaccurate or missing rules. The learning-
based approach proposed in this paper takes a first
step in this direction.
In addition to the novel learning approach, this
paper presents new insights for content-word nega-
tors, which we define as content words that can
negate the polarity of neighboring words or con-
stituents. (e.g., words such as ?eliminated? in the
example sentences). Unlike function-word nega-
tors, such as ?not? or ?never?, content-word nega-
tors have been recognized and utilized less actively
in previous work. (Notable exceptions include e.g.,
Niu et al (2005), Wilson et al (2005), and Moilanen
and Pulman (2007).2)
In our experiments, we compare learning- and
non-learning-based approaches to expression-level
polarity classification ? with and without com-
positional semantics ? and find that (1) simple
heuristics based on compositional semantics outper-
form (89.7% in accuracy) other reasonable heuris-
tics that do not incorporate compositional seman-
tics (87.7%); they can also perform better than sim-
ple learning-based methods that do not incorporate
compositional semantics (89.1%), (2) combining
learning with the heuristic rules based on compo-
sitional semantics further improves the performance
(90.7%), (3) content-word negators play an impor-
tant role in determining the expression-level polar-
ity, and, somewhat surprisingly, we find that (4)
expression-level classification accuracy uniformly
decreases as additional, potentially disambiguating,
context is considered.
In what follows, we first explore heuristic-based
approaches in ?2, then we present learning-based ap-
proaches in ?3. Next we present experimental results
in ?4, followed by related work in ?5.
2 Heuristic-Based Methods
This section describes a set of heuristic-based meth-
ods for determining the polarity of a sentiment-
bearing expression. Each assesses the polarity of the
words or constituents using a polarity lexicon that
indicates whether a word has positive or negative
polarity, and finds negators in the given expression
using a negator lexicon. The methods then infer the
expression-level polarity using voting-based heuris-
tics (? 2.1) or heuristics that incorporate composi-
tional semantics (?2.2). The lexicons are described
2See ?5. Related Work for detailed discussion.
794
VOTE NEG(1) NEG(N) NEGEX(1) NEGEX(N) COMPO
type of negators none function-word function-word & content-word
maximum # of negations applied 0 1 n 1 n n
scope of negators N/A over the entire expression compositional
Table 1: Heuristic methods. (n refers to the number of negators found in a given expression.)
Rules Examples
1 Polarity( not [arg1] ) = ? Polarity( arg1 ) not [bad]arg1.
2 Polarity( [VP] [NP] ) = Compose( [VP], [NP] ) [destroyed]VP [the terrorism]NP .
3 Polarity( [VP1] to [VP2] ) = Compose( [VP1], [VP2] ) [refused]V P1 to [deceive]V P2 the man.
4 Polarity( [adj] to [VP] ) = Compose( [adj], [VP] ) [unlikely]adj to [destroy]V P the planet.
5 Polarity( [NP1] [IN] [NP2] ) = Compose( [NP1], [NP2] ) [lack]NP1 [of]IN [crime]NP2 in rural areas.
6 Polarity( [NP] [VP] ) = Compose( [VP], [NP] ) [pollution]NP [has decreased]V P .
7 Polarity( [NP] be [adj] ) = Compose( [adj], [NP] ) [harm]NP is [minimal]adj .
Definition of Compose( arg1, arg2 )
Compose( arg1, arg2 ) =
For COMPOMC: if (arg1 is a negator) then ? Polarity( arg2 )
(COMPOsition with Majority Class) else if (Polarity( arg1 ) == Polarity( arg2 )) then Polarity( arg1 )
else the majority polarity of data
Compose( arg1, arg2 ) =
For COMPOPR: if (arg1 is a negator) then ? Polarity( arg2 )
(COMPOsition with PRiority) else Polarity( arg1 )
Table 2: Compositional inference rules motivated by compositional semantics.
in ?2.3.
2.1 Voting
We first explore five simple heuristics based on vot-
ing. VOTE is defined as the majority polarity vote
by words in a given expression. That is, we count
the number of positive polarity words and negative
polarity words in a given expression, and assign the
majority polarity to the expression. In the case of a
tie, we default to the prevailing polarity of the data.
For NEG(1), we first determine the majority polar-
ity vote as above, and then if the expression contains
any function-word negator, flip the polarity of the
majority vote once. NEG(N) is similar to NEG(1), ex-
cept we flip the polarity of the majority vote n times
after the majority vote, where n is the number of
function-word negators in a given expression.
NEGEX(1) and NEGEX(N) are defined similarly as
NEG(1) and NEG(N) above, except both function-
word negators and content-word negators are con-
sidered as negators when flipping the polarity of the
majority vote. See Table 1 for summary. Note that a
word can be both a negator and have a negative prior
polarity. For the purpose of voting, if a word is de-
fined as a negator per the voting scheme, then that
word does not participate in the majority vote.
For brevity, we refer to NEG(1) and NEG(N) col-
lectively as NEG, and NEGEX(1) and NEGEX(N) col-
lectively as NEGEX.
2.2 Compositional semantics
Whereas the heuristics above use voting-based in-
ference, those below employ a set of hand-written
rules motivated by compositional semantics. Table 2
shows the definition of the rules along with moti-
vating examples. In order to apply a rule, we first
detect a syntactic pattern (e.g., [destroyed]V P [the
terrorism]NP ), then apply the Compose function as
defined in Table 2 (e.g., Compose([destroyed], [the
terrorism]) by rule #2).3
3Our implementation uses part-of-speech tags and function-
words to coarsely determine the patterns. An implementation
795
Compose first checks whether the first argument is
a negator, and if so, flips the polarity of the second
argument. Otherwise, Compose resolves the polar-
ities of its two arguments. Note that if the second
argument is a negator, we do not flip the polarity of
the first argument, because the first argument in gen-
eral is not in the semantic scope of the negation.4 In-
stead, we treat the second argument as a constituent
with negative polarity.
We experiment with two variations of the Com-
pose function depending on how conflicting polari-
ties are resolved: COMPOMC uses a Compose func-
tion that defaults to the Majority Class of the po-
larity of the data,5 while COMPOPR uses a Compose
function that selects the polarity of the argument that
has higher semantic PRiority. For brevity, we refer
to COMPOPR and COMPOMC collectively as COMPO.
2.3 Lexicons
The polarity lexicon is initialized with the lexicon
of Wilson et al (2005) and then expanded using the
General Inquirer dictionary.6 In particular, a word
contained in at least two of the following categories
is considered as positive: POSITIV, PSTV, POSAFF,
PLEASUR, VIRTUE, INCREAS, and a word contained
in at least one of the following categories is consid-
ered as negative: NEGATIV, NGTV, NEGAFF, PAIN,
VICE, HOSTILE, FAIL, ENLLOSS, WLBLOSS, TRAN-
LOSS.
For the (function- and content-word) negator lex-
icon, we collect a handful of seed words as well as
General Inquirer words that appear in either NOTLW
or DECREAS category. Then we expand the list of
content-negators using the synonym information of
WordNet (Miller, 1995) to take a simple vote among
senses.
based on parse trees might further improve the performance.
4Moilanen and Pulman (2007) provide more detailed dis-
cussion on the semantic scope of negations and the semantic
priorities in resolving polarities.
5The majority polarity of the data we use for our experi-
ments is negative.
6Available at http://www.wjh.harvard.edu/?inquirer/.
When consulting the General Inquirer dictionary, senses with
less than 5% frequency and senses specific to an idiom are
dropped.
3 Learning-Based Methods
While we expect that a set of hand-written heuristic
rules motivated by compositional semantics can be
effective for determining the polarity of a sentiment-
bearing expression, we do not expect them to be per-
fect. Interpreting natural language is such a com-
plex task that writing a perfect set of rules would
be extremely challenging. Therefore, a more ideal
solution would be a learning-based method that can
exploit ideas from compositional semantics while
providing the flexibility to the rigid application of
the heuristic rules. To this end, we present a novel
learning-based approach that incorporates inference
rules inspired by compositional semantics into the
learning procedure (?3.2). To assess the effect of
compositional semantics in the learning-basedmeth-
ods, we also experiment with a simple classifica-
tion approach that does not incorporate composi-
tional semantics (?3.1). The details of these two
approaches are elaborated in the following subsec-
tions.
3.1 Simple Classification (SC)
Given an expression x consisting of n words x1,
..., xn, the task is to determine the polarity y ?
{positive, negative} of x. In our simple binary
classification approach, x is represented as a vec-
tor of features f(x), and the prediction y is given by
argmaxyw?f(x, y), wherew is a vector of parameters
learned from training data. In our experiment, we
use an online SVM algorithm called MIRA (Margin
Infused Relaxed Algorithm) (Crammer and Singer,
2003)7 for training.
For each x, we encode the following features:
? Lexical: We add every word xi in x, and also
add the lemma of xi produced by the CASS
partial parser toolkit (Abney, 1996).
? Dictionary: In order to mitigate the problem of
unseen words in the test data, we add features
that describeword categories based on theGen-
eral Inquirer dictionary. We add this feature for
each xi that is not a stop word.
? Vote: We experiment with two variations of
voting-related features: for SC-VOTE, we add
7We use the Java implementation of this algorithm
available at http://www.seas.upenn.edu/?strctlrn/StructLearn
/StructLearn.html.
796
Simple Classification Classification with Compositional Inference
y ? argmaxy score(y) Find K best z and denote them as Z = {z(1), ..., z(K)}
l? loss flat(y?, y) s.t. ? i < j, score(z(i)) > score(z(j))
w? update(w, l, y?, y) zbad ? mink z(k) s.t. loss compo(y?, z(k), x) > 0
(if such zbad not found in Z, skip parameter update for this.)
If loss compo(y?, z?, x) > 0
zgood ? mink z(k) s.t. loss compo(y?, z(k), x) = 0
z? ? zgood
(if such zgood not found in Z, stick to the original z?.)
l ? loss compo(y?, zbad, x)? loss compo(y?, z?, x)
w? update(w, l, z?, zbad)
Definitions of score functions and loss functions
score(y) := w ? f(x, y) score(z) :=
?
i score(zi) :=
?
i w ? f(x, zi, i)
loss flat(y?, y) := if (y? = y) 0 else 1 loss compo(y?, z, x) := if (y? = C(x, z)) 0 else 1
Figure 1: Training procedures. y? ? {positive, negative} denotes the true label for a given expression x = x1, ..., xn.
z? denotes the pseudo gold standard for hidden variables z.
a feature that indicates the dominant polarity of
words in the given expression, without consid-
ering the effect of negators. For SC-NEGEX,
we count the number of content-word nega-
tors as well as function-word negators to de-
termine whether the final polarity should be
flipped. Then we add a conjunctive feature that
indicates the dominant polarity together with
whether the final polarity should be flipped. For
brevity, we refer to SC-VOTE and SC-NEGEX
collectively as SC.
Notice that in this simple binary classification set-
ting, it is inherently difficult to capture the compo-
sitional structure among words in x, because f(x, y)
is merely a flat bag of features, and the prediction
is governed simply by the dot product of f(x, y) and
the parameter vector w.
3.2 Classification with Compositional
Inference (CCI)
Next, instead of determining y directly from x,
we introduce hidden variables z = (z1, ..., zn)
as intermediate decision variables, where zi ?
{positive, negative, negator, none}, so that zi
represents whether xi is a word with posi-
tive/negative polarity, or a negator, or none of the
above. For simplicity, we let each intermediate de-
cision variable zi (a) be determined independently
from other intermediate decision variables, and (b)
For each token xi,
if xi is a word in the negator lexicon
then z?i ? negator
else if xi is in the polarity lexicon as negative
then z?i ? negative
else if xi is in the polarity lexicon as positive
then z?i ? positive
else
then z?i ? none
Figure 2: Constructing Soft Gold Standard z?
depend only on the input x, so that zi = argmaxziw ?
f(x, zi, i), where f(x, zi, i) is the feature vector en-
coding around the ith word (described on the next
page). Once we determine the intermediate decision
variables, we apply the heuristic rules motivated by
compositional semantics (from Table 2) in order to
obtain the final polarity y of x. That is, y = C(x, z),
where C is the function that applies the composi-
tional inference, either COMPOPR or COMPOMC.
For training, there are two issues we need to
handle: the first issue is dealing with the hidden
variables z. Because the structure of composi-
tional inference C does not allow dynamic program-
ming, it is intractable to perform exact expectation-
maximization style training that requires enumerat-
ing all possible values of the hidden variables z. In-
stead, we propose a simple and tractable training
797
rule based on the creation of a soft gold standard for
z. In particular, we exploit the fact that in our task,
we can automatically construct a reasonably accu-
rate gold standard for z, denoted as z?: as shown in
Figure 2, we simply rely on the negator and polar-
ity lexicons. Because z? is not always correct, we
allow the training procedure to replace z? with po-
tentially better assignments as learning proceeds: in
the event that the soft gold standard z? leads to an in-
correct prediction, we search for an assignment that
leads to a correct prediction to replace z?. The exact
procedure is given in Figure 1, and will be discussed
again shortly.
Figure 1 shows how we modify the parameter up-
date rule of MIRA (Crammer and Singer, 2003) to
reflect the aspect of compositional inference. In the
event that the soft gold standard z? leads to an incor-
rect prediction, we search for zgood, the assignment
with highest score that leads to a correct prediction,
and replace z? with zgood. In the event of no such
zgood being found among the K-best assignments of
z, we stick with z?.
The second issue is finding the assignment of z
with the highest score(z) = ?i w ? f(x, zi, i) that
leads to an incorrect prediction y = C(x, z). Be-
cause the structure of compositional inference C
does not allow dynamic programming, finding such
an assignment is again intractable. We resort to enu-
merating only over K-best assignments instead. If
none of the K-best assignments of z leads to an in-
correct prediction y, then we skip the training in-
stance for parameter update.
Features. For each xi in x, we encode the follow-
ing features:
? Lexical: We include the current word xi as well
as the lemma of xi produced by CASS partial
parser toolkit (Abney, 1996). We also add a
boolean feature to indicate whether the current
word is a stop word.
? Dictionary: In order to mitigate the problem
with unseen words in the test data, we add fea-
tures that describe word categories based on the
General Inquirer dictionary. We add this fea-
ture for each xi that is not a stop word. We
also add a number of boolean features that pro-
vide following properties of xi using the polar-
ity lexicon and the negator lexicon:
? whether xi is a function-word negator
? whether xi is a content-word negator
? whether xi is a negator of any kind
? the polarity of xi according to Wilson et
al. (2005)?s polarity lexicon
? the polarity of xi according to the lexicon
derived from the General Inquirer dictio-
nary
? conjunction of the above two features
? Vote: We encode the same vote feature that we
use for SC-NEGEX described in ? 3.1.
As in the heuristic-based compositional semantics
approach (? 2.2), we experiment with two variations
of this learning-based approach: CCI-COMPOPR
and CCI-COMPOMC, whose compositional infer-
ence rules are COMPOPR and COMPOMC respec-
tively. For brevity, we refer to both variations col-
lectively as CCI-COMPO.
4 Experiments
The experiments below evaluate our heuristic- and
learning-based methods for subsentential sentiment
analysis (? 4.1). In addition, we explore the role
of context by expanding the boundaries of the
sentiment-bearing expressions (? 4.2).
4.1 Evaluation with given boundaries
For evaluation, we use the Multi-Perspective Ques-
tion Answering (MPQA) corpus (Wiebe et al,
2005), which consists of 535 newswire documents
manually annotated with phrase-level subjectivity
information. We evaluate on all strong (i.e., inten-
sity of expression is ?medium? or higher), sentiment-
bearing (i.e., polarity is ?positive? or ?negative?) ex-
pressions.8 As a result, we can assume the bound-
aries of the expressions are given. Performance is
reported using 10-fold cross-validation on 400 doc-
uments; a separate 135 documents were used as a
development set. Based on pilot experiments on the
development data, we set parameters for MIRA as
follows: slack variable to 0.5, and the number of
incorrect labels (constraints) for each parameter up-
date to 1. The number of iterations (epochs) for
training is set to 1 for simple classification, and to 4
8We discard expressions with confidencemarked as ?uncer-
tain?.
798
Heuristic-Based Learning-Based
VOTE NEG NEG NEG NEG COMPO COMPO SC SC CCI CCI
(1) (N) EX EX MC PR VOTE NEG COMPO COMPO
(1) (N) EX MC PR
86.5 82.0 82.2 87.7 87.7 89.7 89.4 88.5 89.1 90.6 90.7
Table 3: Performance (in accuracy) on MPQA dataset.
Heuristic-Based Learning-Based
VOTE NEG NEG NEG NEG COMPO COMPO SC SC CCI CCI
Data (1) (N) EX EX MC PR VOTE NEG COMPO COMPO
(1) (N) EX MC PR
[-0,+0] 86.5 82.0 82.2 87.7 87.7 89.7 89.4 88.5 89.1 90.6 90.7
[-1,+1] 86.4 81.0 81.2 87.2 87.2 89.3 89.0 88.3 88.4 89.5 89.4
[-5,+5] 85.9 79.0 79.4 85.7 85.6 88.2 88.0 86.4 87.1 88.7 88.7
[-?,+?] 85.3 75.8 76.9 83.9 83.9 87.0 86.9 85.8 85.8 87.3 87.5
Table 4: Performance (in accuracy) on MPQA data set with varying boundaries of expressions.
for classification with compositional inference. We
use K = 20 for classification with compositional
inference.
Results. Performance is reported in Table 3. In-
terestingly, the heuristic-based methods NEG (?
82.2%) that only consider function-word negators
perform even worse than VOTE (86.5%), which does
not consider negators. On the other hand, theNEGEX
methods (87.7%) that do consider content-word
negators as well as function-word negators perform
better than VOTE. This confirms the importance of
content-word negators for determining the polari-
ties of expressions. The heuristic-based methods
motivated by compositional semantics COMPO fur-
ther improve the performance over NEGEX, achiev-
ing up to 89.7% accuracy. In fact, these heuris-
tics perform even better than the SC learning-based
methods (? 89.1%). This shows that heuristics that
take into account the compositional structure of the
expression can perform better than learning-based
methods that do not exploit such structure.
Finally, the learning-based methods that in-
corporate compositional inference CCI-COMPO (?
90.7%) perform better than all of the previous
methods. The difference between CCI-COMPOPR
(90.7%) and SC-NEGEX (89.1%) is statistically sig-
nificant at the .05 level by paired t-test. The dif-
ference between COMPO and any other heuristic that
is not based on computational semantics is also
statistically significant. In addition, the difference
between CCICOMPOPR (learning-based) and COM-
POMC (non-learning-based) is statistically signifi-
cant, as is the difference between NEGEX and VOTE.
4.2 Evaluation with noisy boundaries
One might wonder whether employing additional
context outside the annotated expression boundaries
could further improve the performance. Indeed, con-
ventional wisdom would say that it is necessary to
employ such contextual information (e.g., Wilson et
al. (2005)). In any case, it is important to determine
whether our results will apply to more real-world
settings where human-annotated expression bound-
aries are not available.
To address these questions, we gradually relax
our previous assumption that the exact boundaries of
expressions are given: for each annotation bound-
ary, we expand the boundary by x words for each
direction, up to sentence boundaries, where x ?
{1, 5,?}. We stop expanding the boundary if it
will collide with the boundary of an expression with
a different polarity, so that we can consistently re-
cover the expression-level gold standard for evalua-
tion. This expansion is applied to both the training
and test data, and the performance is reported in Ta-
ble 4. From this experiment, we make the following
observations:
? Expanding the boundaries hurts the perfor-
799
mance for any method. This shows that most of
relevant context for judging the polarity is con-
tained within the expression boundaries, and
motivates the task of finding the boundaries of
opinion expressions.
? The NEGEX methods perform better than VOTE
only when the expression boundaries are rea-
sonably accurate. When the expression bound-
aries are expanded up to sentence boundaries,
they perform worse than VOTE. We conjecture
this is because the scope of negators tends to be
limited to inside of expression boundaries.
? The COMPO methods always perform better
than any other heuristic-based methods. And
their performance does not decrease as steeply
as the NEGEX methods as the expression
boundaries expand. We conjecture this is be-
cause methods based on compositional seman-
tics can handle the scope of negators more ade-
quately.
? Among the learning-based methods, those that
involve compositional inference (CCI-COMPO)
always perform better than those that do not
(SC) for any boundaries. And learning with
compositional inference tend to perform bet-
ter than the rigid application of heuristic rules
(COMPO), although the relative performance
gain decreases once the boundaries are relaxed.
5 Related Work
The task focused on in this paper is similar to that
of Wilson et al (2005) in that the general goal of the
task is to determine the polarity in context at a sub-
sentence level. However, Wilson et al (2005) for-
mulated the task differently by limiting their evalua-
tion to individual words that appear in their polarity
lexicon. Also, their approach was based on a flat bag
of features, and only a few examples of what we call
content-word negators were employed.
Our use of compositional semantics for the task
of polarity classification is preceded by Moilanen
and Pulman (2007), but our work differs in that
we integrate the key idea of compositional seman-
tics into learning-based methods, and that we per-
form empirical comparisons among reasonable al-
ternative approaches. For comparison, we evalu-
ated our approaches on the polarity classification
task from SemEval-07 (Strapparava and Mihalcea,
2007). We achieve 88.6% accuracy with COMPOPR,
90.1% with SCNEGEX, and 87.6% with CCICOM-
POMC.9 There are a number of possible reasons for
our lower performance vs. Moilanen and Pulman
(2007) on this data set. First, SemEval-07 does not
include a training data set for this task, so we use
400 documents from the MPQA corpus instead. In
addition, the SemEval-07 data is very different from
the MPQA data in that (1) the polarity annotation
is given only at the sentence level, (2) the sentences
are shorter, with simpler structure, and not as many
negators as the MPQA sentences, and (3) there are
many more instances with positive polarity than in
the MPQA corpus.
Nairn et al (2006) also employ a ?polarity? prop-
agation algorithm in their approach to the semantic
interpretation of implicatives. However, their notion
of polarity is quite different from that assumed here
and in the literature on sentiment analysis. In partic-
ular, it refers to the degree of ?commitment? of the
author to the truth or falsity of a complement clause
for a textual entailment task.
McDonald et al (2007) use a structured model
to determine the sentence-level polarity and the
document-level polarity simultaneously. But deci-
sions at each sentence level does not consider struc-
tural inference within the sentence.
Among the studies that examined content-word
negators, Niu et al (2005) manually collected a
small set of such words (referred as ?words that
change phases?), but their lexicon was designed
mainly for the medical domain and the type of nega-
tors was rather limited. Wilson et al (2005) also
manually collected a handful of content-word nega-
tors (referred as ?general polarity shifters?), but not
extensively. Moilanen and Pulman (2007) collected
a more extensive set of negators semi-automatically
using WordNet 2.1, but the empirical effect of such
words was not explicitly investigated.
9For lack of space, we only report our performance on in-
stances with strong intensities as defined in Moilanen and Pul-
man (2007), which amounts to only 208 test instances. The
cross-validation set of MPQA contains 4.9k instances.
800
6 Conclusion
In this paper, we consider the task of determining
the polarity of a sentiment-bearing expression, con-
sidering the effect of interactions among words or
constituents in light of compositional semantics. We
presented a novel learning-based approach that in-
corporates structural inference motivated by compo-
sitional semantics into the learning procedure. Our
approach can be considered as a small step toward
bridging the gap between computational semantics
and machine learning methods. Our experimen-
tal results suggest that this direction of research is
promising. Future research includes an approach
that learns the compositional inference rules from
data.
Acknowledgments
This work was supported in part by National Science
Foundation Grants BCS-0624277 and IIS-0535099
and by Department of Homeland Security Grant
N0014-07-1-0152. We also thank Eric Breck, Lil-
lian Lee, Mats Rooth, the members of the Cornell
NLP reading seminar, and the EMNLP reviewers for
insightful comments on the submitted version of the
paper.
References
Steven Abney. 1996. Partial parsing via finite-state
cascades. Journal of Natural Language Engineering,
2(4):337344.
Koby Crammer and Yoram Singer. 2003. Ultraconserva-
tive online algorithms for multiclass problems. JMLR
3:951.
David R. Dowty, Robert E. Wall and Stanley Peters.
1981. Introduction to Montague Semantics.
Andrea Esuli and Fabrizio Sebastiani. 2006. SentiWord-
Net: A Publicly Available Lexical Resource for Opin-
ion Mining. In Proceedings of 5th Conference on Lan-
guage Resources and Evaluation (LREC),.
Percy Liang, Hal Daume? III and Dan Klein. 2008. Struc-
ture Compilation: Trading Structure for Features. In
International Conference on Machine Learning.
Minqing Hu and Bing Liu. 2004. Mining and sum-
marizing customer reviews. In Proceedings of the
ACM SIGKDD International Conference on Knowl-
edge Discovery & Data Mining (KDD-2004).
Alistair Kennedy and Diana Inkpen. 2005. Senti-
ment Classification of Movie and Product Reviews Us-
ing Contextual Valence Shifters. In Proceedings of
FINEXIN 2005, Workshop on the Analysis of Infor-
mal and Formal Information Exchange during Nego-
tiations.
Soo-Min Kim and Eduard Hovy. 2004. Determining the
sentiment of opinions. In Proceedings of COLING.
Ryan McDonald, Kerry Hannan, Tyler Neylon, Mike
Wells and Jeff Reynar. 2007. Structured Models for
Fine-to-Coarse Sentiment Analysis. In Proceedings of
Association for Computational Linguistics (ACL) .
George A. Miller. 1995. WordNet: a lexical database for
English. In Communications of the ACM, 38(11):3941
Richard Montague. 1974. Formal Philosophy; Selected
papers of Richard Montague. Yale University Press.
Karo Moilanen and Stephen Pulman. 2007. Sentiment
Composition. In Proceedings of Recent Advances in
Natural Language Processing (RANLP 2007).
Rowan Nairn, Cleo Condoravdi and Lauri Karttunen
2006. Computing relative polarity for textual infer-
ence. In Inference in Computational Semantics (ICoS-
5).
Yun Niu, Xiaodan Zhu, Jianhua Li and Graeme Hirst.
2005. Analysis of polarity information inmedical text.
In Proceedings of the American Medical Informatics
Association 2005 Annual Symposium (AMIA).
Livia Polanyi and Annie Zaenen. 2004. Contextual lex-
ical valence shifters. In Exploring Attitude and Affect
in Text: Theories and Applications: Papers from the
2004 Spring Symposium, AAAI.
Mostafa Shaikh, Helmut Prendinger and Mitsuru
Ishizuka. 2007. Assessing sentiment of text by se-
mantic dependency and contextual valence analysis.
In Proc 2nd Int?l Conf on Affective Computing and In-
telligent Interaction (ACII?07).
Carlo Strapparava and Rada Mihalcea. 2007. Semeval-
2007 task 14: Affective text. In Proceedings of Se-
mEval.
Janyce Wiebe, Theresa Wilson and Claire Cardie. 2005.
Annotating expressions of opinions and emotions
in language. In Language Resources and Evalua-
tion (formerly Computers and the Humanities), 39(2-
3):165210.
Theresa Wilson, Janyce Wiebe and Paul Hoffmann.
2005. Recognizing contextual polarity in phrase-level
sentiment analysis. In Proceedings of HLT/EMNLP.
801
Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 590?598,
Singapore, 6-7 August 2009. c?2009 ACL and AFNLP
Adapting a Polarity Lexicon using Integer Linear Programming
for Domain-Specific Sentiment Classification
Yejin Choi and Claire Cardie
Department of Computer Science
Cornell University
Ithaca, NY 14853
{ychoi,cardie}@cs.cornell.edu
Abstract
Polarity lexicons have been a valuable re-
source for sentiment analysis and opinion
mining. There are a number of such lexi-
cal resources available, but it is often sub-
optimal to use them as is, because general
purpose lexical resources do not reflect
domain-specific lexical usage. In this pa-
per, we propose a novel method based on
integer linear programming that can adapt
an existing lexicon into a new one to re-
flect the characteristics of the data more
directly. In particular, our method collec-
tively considers the relations among words
and opinion expressions to derive the most
likely polarity of each lexical item (posi-
tive, neutral, negative, or negator) for the
given domain. Experimental results show
that our lexicon adaptation technique im-
proves the performance of fine-grained po-
larity classification.
1 Introduction
Polarity lexicons have been a valuable resource for
sentiment analysis and opinionmining. In particu-
lar, they have been an essential ingredient for fine-
grained sentiment analysis (e.g., Kim and Hovy
(2004), Kennedy and Inkpen (2005), Wilson et al
(2005)). Even though the polarity lexicon plays an
important role (Section 3.1), it has received rela-
tively less attention in previous research. In most
cases, polarity lexicon construction is discussed
only briefly as a preprocessing step for a sentiment
analysis task (e.g., Hu and Liu (2004), Moilanen
and Pulman (2007)), but the effect of different al-
ternative polarity lexicons is not explicitly inves-
tigated. Conversely, research efforts that focus
on constructing a general purpose polarity lexicon
(e.g., Takamura et al (2005), Andreevskaia and
Bergler (2006), Esuli and Sebastiani (2006), Rao
and Ravichandran (2009)) generally evaluate the
lexicon in isolation from any potentially relevant
NLP task, and it is unclear how the new lexicon
might affect end-to-end performance of a concrete
NLP application.
It might even be unrealistic to expect that there
can be a general-purpose lexical resource that
can be effective across all relevant NLP applica-
tions, as general-purpose lexicons will not reflect
domain-specific lexical usage. Indeed, Blitzer
et al (2007) note that the polarity of a particu-
lar word can carry opposite sentiment depending
on the domain (e.g., Andreevskaia and Bergler
(2008)).
In this paper, we propose a novel method based
on integer linear programming to adapt an existing
polarity lexicon into a new one to reflect the char-
acteristics of the data more directly. In particular,
our method considers the relations among words
and opinion expressions collectively to derive the
most likely polarity of each word for the given do-
main.
Figure 1 depicts the key insight of our approach
using a bipartite graph. On the left hand side, each
node represents a word, and on the right hand side,
each node represents an opinion expression. There
is an edge between a word wi and an opinion ex-
pression ej , if the word wi appears in the expres-
sion ej . We assume the possible polarity of each
expression is one of the following three values:
{positive, neutral, negative}, while the possible
polarity of each word is one of: {positive, neutral,
negative or negator}. Strictly speaking, negator is
not a value for polarity, but we include them in our
lexicon, because valence shifters or negators have
been shown to play an important role for sentiment
analysis (e.g., Polanyi and Zaenen (2004), Moila-
nen and Pulman (2007), Choi and Cardie (2008)).
Typically, the ultimate goal of the sentiment
analysis task is to determine the expression-level
(or sentiment/ document-level) polarities, rather
590
than the correct word-level polarities with respect
to the domain. Therefore, word-level polarities
can be considered as latent information. In this pa-
per, we show how we can improve the word-level
polarities of a general-purpose polarity lexicon by
utilizing the expression-level polarities, and in re-
turn, how the adapted word-level polarities can
improve the expression-level polarities.
In Figure 1, there are two types of relations
we could exploit when adapting a general-purpose
polarity lexicon into a domain-specific one. The
first are word-to-word relations within each ex-
pression. That is, if we are not sure about the
polarity of a certain word, we can still make a
guess based on the polarities of other words within
the same expression and knowledge of the polar-
ity of the expression. The second type of relations
are word-to-expression relations: e.g., some words
appear in expressions that take on a variety of po-
larities, while other words are associated with ex-
pressions of one polarity class or another.
In relation to previous research, analyz-
ing word-to-word (intra-expression) relations
is most related to techniques that determine
expression-level polarity in context (e.g., Wilson
et al (2005)), while exploring word-to-expression
(inter-expression) relations has connections to
techniques that employ more of a global-view of
corpus statistics (e.g., Kanayama and Nasukawa
(2006)).1
While most previous research exploits only one
or the other type of relation, we propose a unified
method that can exploit both types of semantic re-
lation, while adapting a general purpose polarity
lexicon into a domain specific one. We formulate
our lexicon adaptation task using integer linear
programming (ILP), which has been shown to be
very effective when solving problems with com-
plex constraints (e.g., Roth and Yih (2004), Denis
and Baldridge (2007)). And the word-to-word and
word-to-expression relations discussed above can
be encoded as soft and hard constraints in ILP. Un-
fortunately, one class of constraint that we would
like to encode (see Section 2) will require an
exponentially many number of constraints when
grounded into an actual ILP problem. We there-
fore propose an approximation scheme to make
the problem more practically solvable.
We evaluate the effect of the adapted lex-
1In case of document-level polarity classification, word-
to-expression relations correspond to word-to-document re-
lations.
exp 
exp
exp
expw w w 
w

w

w  w
w w
w
+
?
w
w
w
w 
w
=
+
?
?
=
?
Figure 1: The relations among words and expres-
sions. + indicates positive, - indicates negative, =
indicates neutral, and ? indicates a negator.
icon in the context of a concrete NLP task:
expression-level polarity classification. Experi-
mental results show that our lexicon adaptation
technique improves the accuracy of two com-
petitive expression-level polarity classifiers from
64.2% - 70.4% to 67.0% - 71.2%..
2 An Integer Linear Programming
Approach
In this section, we describe how we formulate the
lexicon adaptation task using integer linear pro-
gramming. Before we begin, we assume that we
have a general-purpose polarity lexicon L, and a
polarity classification algorithm f(el,L), that can
determine the polarity of the opinion expression el
based on the words in el and the initial lexicon L.
The polarity classification algorithm f(?) can be
either a heuristic-based one, or a machine-learning
based one ? we consider it as a black box for now.
Constraints for word-level polarities: For
each word xi, we define four binary variables:
x+i , x=i , x
?
i , x?i to represent positive, neutral, neg-
ative polarity, and negators respectively. If x?i = 1
for some ? ? {+,=,?,?}, then the word xi has
the polarity ?. The following inequality constraint
states that at least one polarity value must be cho-
sen for each word.
x+i + x=i + x?i + x?i >= 1 (1)
If we allow only one polarity per word, then the
above inequality constraint should be modified as
an equality constraint. Although most words tend
to associate with a single polarity, some can take
on more than one polarity. In order to capture this
observation, we introduce an auxiliary binary vari-
able ?i for each word xi. Then the next inequality
591
constraint states that at most two polarities can be
chosen for each word.
x+i + x=i + x?i + x?i <= 1 + ?i (2)
Next we introduce the initial part of our objec-
tive function.
maximize
?
i
(
w+i x
+
i + w=i x=i
+ w?i x?i + w?i x?i
? w??i
)
+ ? ? ? (3)
For the auxiliary variable ?i, we apply a con-
stant weight w? to discourage ILP from choosing
more than one polarity for each word. We can al-
low more than two polarities for each word, by
adding extra auxiliary variables and weights. For
each variable x?i , we define its weight w?i , which
indicates how likely it is that word xi carries the
polarity ?. We define the value of w?i using two
different types of information as follows:
w?i := Lw?i + Cw?i
where Lw?i is the degree of polarity ? for word xi
determined by the general-purpose polarity lexi-
con L, and Cw?i is the degree of polarity ? deter-
mined by the corpus statistics as follows:2
Cw?i :=
# of xi in expressions with polarity ?
# of xi in the corpus C
Note that the occurrence of word xi in an ex-
pression ej with a polarity ? does not necessar-
ily mean that the polarity of xi should also be
?, as the interpretation of the polarity of an ex-
pression is more than just a linear sum of the
word-level polarities (e.g., Moilanen and Pulman
(2007)). Nonetheless, not all expressions require
a complicated inference procedure to determine
their polarity. Therefore, Cw?i still provides useful
information about the likely polarity of each word
based on the corpus statistics.
From the perspective of Chomskyan linguistics,
the weights Lw?i based on the prior polarity from
the lexicon can be considered as having a ?com-
petence? component , while Cw?i derived from
the corpus counts can be considered as a ?perfor-
mance? component (Noam Chomsky (1965)).
2If a word xi is in an expression that is not an opinion,
then we count it as an occurrence with neutral polarity.
Constraints for content-word negators: Next
we describe a constraint that exploits knowledge
of the typical distribution of content-word nega-
tors in natural language. Content-word negators
are words that are not function words, but act se-
mantically as negators (Choi and Cardie, 2008).3
Although it is possible to artificially construct a
very convoluted sentence with lots of negations, it
is unlikely for multiple layers of negations to ap-
pear very often in natural language (Pickett et al
(1996)). Therefore, we allow at most one content-
word negator for each expression el. Because we
do not restrict the number of function-word nega-
tors, our constraint still gives room for multiple
layers of negations.
?
i??(el)
x?i <= 1 (4)
In the above constraint, ?(el) indicates the set
of indices of content words appearing in el . For
instance, if i ? ?(el), then xi appears in el. This
constraint can be polished further to accommodate
longer expressions where multiple content-word
negators are more likely to appear, by adding a
separate constraint with a sliding window.
Constraints for expression-level polarities:
Before we begin, we introduce pi(el) that will be
used often in the remaining section. For each ex-
pression el, we define pi(el) to be the set of con-
tent words appearing in el, together with the most
likely polarity proposed by a general-purpose po-
larity lexicon L. For instance, if x+i ? pi(el), then
the polarity of word xi is + according to L.
Next we encode constraints that consider
expression-level polarities. If the polarity classifi-
cation algorithm f(el,L) makes an incorrect pre-
diction for el using the original lexicon L, then we
need to encourage ILP to fix the error by suggest-
ing different word-level polarities. We capture this
idea by the following constraint:
?
x?i?pi(el)
x?i <= |pi(el)| ? 1 + ?l (5)
The auxiliary binary variable ?l is introduced
for each el so that the assignment pi(el) does not
have to be changed if paying for the cost w? in the
objective function. (See equation (10).) That is,
suppose the ILP solver assigns ?1? to all variables
3Examples of content-word negators are destroy, elimi-
nate, prevent etc.
592
in ?(el), (which corresponds to keeping the orig-
inal lexicon as it is for all words in the given ex-
pression el), then the auxiliary variable ?l must be
also set as ?1? in order to satisfy the constraint (5).
Because ?l is associated with a negative weight
in the objective function, doing so will act against
maximizing the objective function. This way, we
discourage the ILP solver to preserve the original
lexicon as it is.
To verify the constraint (5) further, suppose that
the ILP solver assigns ?1? for all variables in ?(el)
except for one variable. (Notice that doing so cor-
responds to proposing a new polarity for one of
the words in the given expression el.) Then the
constraint (5) will hold regardless of whether the
ILP solver assigns ?0? or ?1? to ?l. Because ?l is
associated with a negative weight in the objective
function, the ILP solver will then assign ?0? to ?l to
maximize the objective function. In other words,
we encourage the ILP solver to modify the original
lexicon for the given expression el .
We use this type of soft constraint in order to
cope with the following two noise factors: first, it
is possible that some annotations are noisy. Sec-
ond, f(el,L) is not perfect, and might not be able
to make a correct prediction even with the correct
word-level polarities.
Next we encode a constraint that is the oppo-
site of the previous one. That is, if the polarity
classification algorithm f(el,L) makes a correct
prediction on el using the original lexicon L, then
we encourage ILP to keep the original word-level
polarities for words in el.
?
x?i?pi(el)
x?i >= |pi(el)| ? |pi(el)|?l (6)
Interpretation of constraint (6) with the auxil-
iary binary variable ?l is similar to that of con-
straint (5) elaborated above.
Notice that in equation (5), we encouraged ILP
to fix the current lexicon L for words in el, but
we have not specified the consequence of a mod-
ified lexicon (L?) in terms of expression-level po-
larity classification f(el,L?). Certain changes to
L might not fix the prediction error for el, and
those might even cause extra incorrect predictions
for other expressions. Then it would seem that we
need to replicate constraints (5) & (6) for all per-
mutations of word-level polarities. However, do-
ing so would incur exponentially many number of
constraints (4|el|) for each expression.4
To make the problem more practically solv-
able, we only consider changes to the lexicon that
are within edit-one distance with respect to pi(el).
More formally, let us define pi?(el) to be the set of
content words appearing in el, together with the
most likely polarity proposed by a modified polar-
ity lexicon L?. Then we need to consider all pi?(el)
such that |pi?(el)? pi(el)| = |pi(el)| ? 1. There are
(4?1)|el| number of different pi?(el), and we index
them as pi?k(el). We then add following constraints
similarly as equation (5) & (6):
?
x?i?pi?k(el)
x?i <= |pi?k(el)| ? 1 + ?(l,k) (7)
if the polarity classification algorithm f(?) makes
an incorrect prediction based on pi?k(el). And,
?
x?i?pi?k(el)
x?i >= |pi?k(el)| ? |pi?k(el)|?(l,k) (8)
if the polarity classification algorithm f(?) makes
a correct prediction based on pi?k(el). Remember
that none of the constraints (5) - (8) enforces as-
signment pi(el) or pi?k(el) as a hard constraint. In
order to enforce at least one of them to be chosen,
we add the following constraint:
?
x?i?pi(el)
x?i >= |pi(el)| ? 1 (9)
This constraint ensures that the modified lexi-
con L? is not drastically different from L. Assum-
ing that the initial lexicon L is a reasonably good
one, constraining the search space for L? will reg-
ulate that L? does not turn into a degenerative one
that overfits to the current corpus C.
Objective function: Finally, we introduce our
full objective function.
4For certain simple polarity classification algorithm
f(el,L), it is possible to write polynomially many number of
constraints. However our approach intends to be more gen-
eral by treating f(el,L) as a black box, so that algorithms
that do not factor nicely can also be considered as an option.
593
maximize
?
i
(
w+i x
+
i + w=i x=i
+ w?i x?i + w?i x?i
? w??i
)
?
?
l
w??l?l
?
?
l,k
w??(l,k)?(l,k) (10)
We have already described the first part of the
objective function (equation (3)), thus we only de-
scribe the last two terms here. w? is defined simi-
larly as w?; it is a constant weight that applies for
any auxiliary binary variable ?l and ?(l,k).
We further define ?l and ?(l,k) as secondary
weights, or amplifiers to adjust the constant weight
w?. To enlighten the motivation behind the am-
plifiers ?l and ?(l,k), we bring out the following
observations:
1. Among the incorrect predictions for
expression-level polarity classification,
some are more incorrect than the other.
For instance, classifying positive class to
negative class is more wrong than classifying
positive class to neutral class. Therefore, the
cost of not fixing very incorrect predictions
should be higher than the cost of not fixing
less incorrect predictions. (See [R2] and
[R3] in Table 1.)
2. If the current assignment pi(el) for expression
el yields a correct prediction using the classi-
fier y(el,L), then there is not much point in
changingL toL?, even if y(el,L?) also yields
a correct prediction. In this case, we would
like to assign slightly higher confidence in the
original lexicon L then the new one L?. (See
[R1] in Table 1.)
3. Likewise, if the current assignment pi(el) for
expression el yields an incorrect prediction
using the classifier y(el,L), then there is not
much point in changing L to L?, if y(el,L?)
also yields an equally incorrect prediction.
Again we assign slightly higher confidence in
the original lexicon L than the new one L? in
such cases. (Compare each row in [R2] with
a corresponding row in [R3] in Table 1.)
[R1] If pi(el) correct ?l ? 1.5
If pi?k(el) correct ?(l,k) ? 1.0
[R2] If pi(el) very incorrect ?l ? 1.0
If pi(el) less incorrect ?l ? 0.5
[R3] If pi?k(el) very incorrect ?(l,k) ? 1.5
If pi?k(el) less incorrect ?(l,k) ? 1.0
Table 1: The value of amplifiers ?l and ?(l,k).
To summarize, for correct predictions, the de-
gree of ? determines the degree of cost of (unde-
sirably) altering the current lexicon for el. For in-
correct predictions, the degree of ? determines the
degree of cost of not fixing the current lexicon for
el.
3 Experiments
In the experiment section, we seek for answers for
the following questions:
Q1 What is the effect of a polarity lexicon on the
expression-level polarity classification task?
In particular, is it useful when using a ma-
chine learning technique that might be able to
learn the necessary polarity information just
based on the words in the training data, with-
out consulting a dictionary? (Section 3.1)
Q2 What is the effect of an adapted polarity lex-
icon on the expression-level polarity classifi-
cation task? (Section 3.2)
Notice that we include the neutral polarity in the
polarity classification. It makes our task much
harder (e.g., Wilson et al (2009)) than those that
assume inputs are guaranteed to be either strongly
positive or negative (e.g., Pang et al (2002), Choi
and Cardie (2008)). But in practice, one can-
not expect that a given input is strongly polar, as
automatically extracted opinions are bound to be
noisy. Furthermore, Wiebe et al (2005) discuss
that some opinion expressions do carry a neutral
polarity.
We experiment with the Multi-Perspective
Question Answering (MPQA) corpus (Wiebe et
al., 2005) for evaluation. It contains 535 newswire
documents annotated with phrase-level subjectiv-
ity information. We evaluate on all opinion ex-
pressions that are known to have high level of
inter-annotator agreement. That is, we include
opinions with intensity marked as ?medium? or
594
higher, and exclude those with annotation confi-
dence marked as ?uncertain?. To focus our study
on the direct influence of the polarity lexicon upon
the sentiment classification task, we assume the
boundaries of the expressions are given. How-
ever, our approach can be readily used in tan-
dem with a system that extracts opinion expres-
sions (e.g., Kim and Hovy (2005), Breck et al
(2007)). Performance is reported using 10-fold
cross-validation on 400 documents, and a separate
135 documents were used as a development set.
For the general-purpose polarity lexicon, we ex-
pand the polarity lexicon of Wilson et al (2005)
with General Inquirer dictionary as suggested by
Choi and Cardie (2008).
We report the performance in twomeasures: ac-
curacy for 3-way classification, and average error
distance. The reason why we consider average er-
ror distance is because classifying a positive class
into a negative class is worse than classifying a
positive class into a neutral one. We define the er-
ror distance between ?neutral? class and any other
class as 1, while the error distance between ?posi-
tive? class and ?negative? class as 2. If a predicted
polarity is correct, then the error distance is 0. We
compute the error distance of each prediction and
take the average over all predictions in the test
data.
3.1 Experiment-I: Effect of a Polarity
Lexicon
To verify the effect of a polarity lexicon on the
expression-level polarity classification task, we
experiment with simple classification-based ma-
chine learning technique. We use the Mallet
(McCallum, 2002) implementation of Conditional
Random Fields (CRFs) (Lafferty et al, 2001).5 To
highlight the influence of a polarity lexicon, we
compare the performance of CRFs with and with-
out features derived from polarity lexicons.
Features: We encode basic features as words
and lemmas for all content words in the given ex-
pression. The performance of CRFs using only the
basic features are given in the first row of the Ta-
ble 2. Next we encode features derived from po-
larity lexicons as follows.
? The output of Vote & Flip algorithm. (Sec-
tion 3.2 & Figure 2.)
5We use the CRF implementation of Mallet (McCallum,
2002) with Markov-order 0, which is equivalent to Maximum
Entropy models (Berger et al (1996)).
Accuracy Avg. Error Distance
Without Lexicon 63.9 0.440
With Lexicon 70.4 0.334
Table 2: Effect of a polarity lexicon on expression-
level classification using CRFs
? Number of positive, neutral, negative, and
negators in the given expression.
? Number of positive (or negative) words in
conjunction with number of negators.
? (boolean) Whether the number of positive
words dominates negative ones.
? (boolean) Whether the number of negative
words dominates positive ones.
? (boolean) None of the above two cases
? Each of the above three boolean values in
conjunction with the number of negators.
Results: Table 2 shows the performance of
CRFs with and without features that consult the
general-purpose lexicon. As expected, CRFs can
perform reasonably well (accuracy = 63.9%) even
without consulting the dictionary, by learning di-
rectly from the data. However, having the polarity
lexicon boosts the performance significantly (ac-
curacy = 70.4%), demonstrating that lexical re-
sources are very helpful for fine-grained sentiment
analysis. The difference in performance is statisti-
cally significant by paired t-test for both accuracy
(p < 0.01) and average error distance (p < 0.01).
3.2 Experiment-II: Adapting a Polarity
Lexicon
In this section, we assess the quality of the adapted
lexicon in the context of an expression-level polar-
ity classification task. In order to perform the lex-
icon adaptation via ILP, we need an expression-
level polarity classification algorithm f(el,L) as
described in Section 2. According to Choi and
Cardie (2008), voting algorithms that recognize
content-word negators achieve a competitive per-
formance, so we will use a variant of it for sim-
plicity. Because none of the algorithms proposed
by Choi and Cardie (2008) is designed to handle
the neutral polarity, we invent our own version as
shown in Figure 2.
595
For each expression ei,
nPositive? # of positive words in ei
nNeutral ? # of neutral words in ei
nNegative? # of negative words in ei
nNegator ? # of negating words in ei
if (nNegator % 2 = 0)
then fF lipPolarity ? false
else
then fF lipPolarity ? true
if (nPositive > nNegative) & ? fF lipPolarity
then Polarity(ei)? positive
else if (nPositive > nNegative) & fF lipPolarity
then Polarity(ei)? negative
else if (nPositive < nNegative) & ? fF lipPolarity
then Polarity(ei)? negative
else if (nPositive < nNegative) & fF lipPolarity
then Polarity(ei)? neutral
else if nNeutral > 0
then Polarity(ei)? neutral
else
then Polarity(ei)? default polarity (the most
prominent polarity in the corpus)
Figure 2: Vote & Flip Algorithm
It might look a bit complex at first glance,
but the intuition is simple. The variable
fFlipPolarity determines whether we need to
flip the overall majority polarity based on the num-
ber of negators in the given expression. If the
positive (or negative) polarity words dominate the
given expression, and if there is no need to flip
the majority polarity, then we take the positive (or
negative) polarity as the overall polarity. If the
positive (or negative) polarity words dominate the
given expression, and if we need to flip the major-
ity polarity, then we take the negative (or neutral)
polarity as the overall polarity.
Notice that the result of flipping the negative po-
larity is neutral, not positive. In our pilot study, we
found that this strategy works better than flipping
the negative polarity to positive.6 Finally, if the
number of positive words and the negative words
tie, and there is any neutral word, then we assign
the neutral polarity. In this case, we don?t worry if
6This finding is not surprising. For instance, if we con-
sider the polarity of ?She did not get hurt much from the ac-
cident.?, it can be viewed as neutral; although it is good that
one did not hurt much, it is still bad that there was an acci-
dent. Hence it gives a mixed feeling, which corresponds to
the neutral polarity.
there is a negator, because flipping a neutral polar-
ity would still result in a neutral polarity. If none of
above condition is met, than we default to the most
prominent polarity of the data, which is the nega-
tive polarity in the MPQA corpus. We name this
simple algorithm as Vote & Flip algorithm. The
performance is shown in the first row in Table 2.
Next we describe the implementation part of the
ILP. For 10 fold-cross validation, we formulate the
ILP problem using the training data (360 docu-
ments), and then test the effect of the adapted lex-
icon on the remaining 40 documents. We include
only those content words that appeared more than
3 times in the training data. From the pilot test us-
ing the development set, we picked the value of
w? as 0.1. We found that having the auxiliary
variables ?l which allow more than one polarity
per word does not necessarily help with the per-
formance, so we omitted them. We suspect it is
because the polarity classifiers we experimented
with is not highly capable of disambiguating dif-
ferent lexical usages and select the right polarity
for a given context. We use CPLEX integer pro-
gramming solver to solve our ILP problems. On a
machine with 4GHz CPU, it took several minutes
to solve each ILP problem.
In order to assess the effect of the adapted lex-
icon using CRFs, we need to first train the CRFs
model. Using the same training set used for the
lexicon adaptation would be suboptimal, because
the features generated from the adapted lexicon
will be unrealistically good in that particular data.
Therefore, we prepared a separate training data for
CRFs using 135 documents from the development
set.
Results: Table 3 shows the comparison of the
original lexicon and the adapted lexicon in terms
of polarity classification performance using the
Vote & Flip algorithm. The adapted lexicon im-
proves the accuracy as well as reducing the aver-
age error distance. The difference in performance
is statistically significant by paired t-test for both
accuracy (p < 0.01) and average error distance
(p < 0.01).
Table 4 shows the comparison of the original
lexicon and the adapted lexicon using CRFs. The
improvement is not as substantial as that of Vote &
Flip algorithm but the difference in performance is
also statistically significant for both accuracy (p =
0.03) and average error distance (p = 0.04).
596
Accuracy Avg. Error Distance
Original Lexicon 64.2 0.395
Adapted Lexicon 67.0 0.365
Table 3: Effect of an adapted polarity lexicon on
expression-level classification using the Vote &
Flip Algorithm
Accuracy Avg. Error Distance
Original Lexicon 70.4 0.334
Adapted Lexicon 71.2 0.327
Table 4: Effect of an adapted polarity lexicon on
expression-level classification using CRFs
4 Related Work
There are a number of previous work that focus
on building polarity lexicons (e.g., Takamura et
al. (2005), Kaji and Kitsuregawa (2007), Rao and
Ravichandran (2009)). But most of them evalu-
ated their lexicon in isolation from any potentially
relevant NLP task, and it is unclear how the new
lexicon might affect end-to-end performance of a
concrete NLP application. Our work differs in that
we try to draw a bridge between general purpose
lexical resources and a domain-specific NLP ap-
plication.
Kim and Hovy (2005) and Banea et al (2008)
present bootstrapping methods to construct a sub-
jectivity lexicon and measure the effect of the new
lexicon for sentence-level subjectivity classifica-
tion. However, their lexicons only tell whether a
word is a subjective one, but not the polarity of the
sentiment. Furthermore, the construction of lexi-
con is still an isolated step from the classification
task. Our work on the other hand allows the classi-
fication task to directly influence the construction
of lexicon, enabling the lexicon to be adapted for
a concrete NLP application and for a specific do-
main.
Wilson et al (2005) pioneered the expression-
level polarity classification task using the MPQA
corpus. The experimental results are not directly
comparable to ours, because Wilson et al (2005)
limit the evaluation only for the words that ap-
peared in their polarity lexicon. Choi and Cardie
(2008) also focus on the expression-level polarity
classification, but their evaluation setting is not as
practical as ours in that they assume the inputs are
guaranteed to be either strongly positive or nega-
tive.
5 Conclusion
In this paper, we present a novel lexicon adapta-
tion technique based on integer linear program-
ming to reflect the characteristics of the domain
more directly. In particular, our method collec-
tively considers the relations among words and
opinion expressions to derive the most likely po-
larity of each lexical item for the given domain.
We evaluate the effect of our lexicon adaptation
technique in the context of a concrete NLP ap-
plication: expression-level polarity classification.
The positive results from our experiments encour-
age further research for lexical resource adaptation
techniques.
Acknowledgments
This work was supported in part by National Sci-
ence Foundation Grant BCS-0624277 and by the
Department of Homeland Security under ONR
Grant N0014-07-1-0152. We also thank the
EMNLP reviewers for insightful comments.
References
Alina Andreevskaia and Sabine Bergler. 2008. When
Specialists and Generalists Work Together: Over-
coming Domain Dependence in Sentiment Tagging.
ACL
Alina Andreevskaia and Sabine Bergler. 2006. Min-
ing WordNet For a Fuzzy Sentiment: Sentiment Tag
Extraction From WordNet Glosses. EACL
Carmen Banea, Rada Mihalcea, and JanyceWiebe.
2008. A Bootstrapping Method for Building Sub-
jectivity Lexicons for Languages with Scarce Re-
sources. LREC
Adam Berger, Stephen Della Pietra, and Vincent Della
Pietra. 1996. A maximum entropy approach to nat-
ural language processing. In Computational Lin-
guistics, 22(1)
John Blitzer, Mark Dredze, and Fernando Pereira.
2007. Biographies, Bollywood, Boom-boxes, and
Blenders: Domain Adaptation for Sentiment Classi-
fication. Association for Computational Linguistics
- ACL 2007
Eric Breck, Yejin Choi and Claire Cardie. 2007. Iden-
tifyingExpressions of Opinion in Context. In IJCAI.
Yejin Choi and Claire Cardie. 2008. Learning with
Compositional Semantics as Structural Inference for
Subsentential Sentiment Analysis. EMNLP
Noam Chomsky. 1965. Aspects of the theory of syn-
tax. Cambridge, MA: MIT Press.
597
Pascal Denis and Jason Baldridge. 2007. Joint deter-
mination of anaphoricity and coreference resolution
using integer programming. NAACL
Andrea Esuli and Fabrizio Sebastiani. 2006. Senti-
WordNet: A Publicly Available Lexical Resource
for Opinion Mining. In Proceedings of 5th Con-
ference on Language Resources and Evaluation
(LREC),.
Minqing Hu and Bing Liu. 2004. Mining and sum-
marizing customer reviews. In Proceedings of the
ACM SIGKDD International Conference on Knowl-
edge Discovery & Data Mining (KDD-2004).
Nobuhiro Kaji and Masaru Kitsuregawa. 2007. Build-
ing Lexicon for Sentiment Analysis from Massive
Collection of HTML Documents. In EMNLP-
CoNLL.
Hiroshi Kanayama Tetsuya Nasukawa. 2006. Fully
Automatic Lexicon Expansion for Domain-oriented
Sentiment Analysis. In ACL.
Alistair Kennedy and Diana Inkpen. 2005. Sentiment
Classification of Movie and Product Reviews Us-
ing Contextual Valence Shifters. In Proceedings of
FINEXIN 2005, Workshop on the Analysis of Infor-
mal and Formal Information Exchange during Ne-
gotiations.
Soo-Min Kim and Eduard Hovy. 2004. Determining
the sentiment of opinions. In Proceedings of COL-
ING.
Soo-Min Kim and Eduard Hovy. 2005. Automatic De-
tection of Opinion Bearing Words and Sentences. In
Companion Volume to the Proceedings of the Sec-
ond International Joint Conference on Natural Lan-
guage Processing (IJCNLP-05)
John Lafferty, Andrew Kachites McCallum and Fer-
nando Pereira. 2001. Conditional Random Fields:
Probabilistic Models for Segmenting and Labeling
Sequence Data. In ICML.
Andrew Kachites McCallum. 2002. MAL-
LET: A Machine Learning for Language Toolkit.
http://mallet.cs.umass.edu.
Karo Moilanen and Stephen Pulman. 2007. Sentiment
Composition. In Proceedings of Recent Advances in
Natural Language Processing (RANLP 2007).
Bo Pang, Lillian Lee and Shivakumar Vaithyanathan.
2002. Thumbs up? Sentiment Classification using
Machine Learning Techniques. In EMNLP.
Joseph Pickett et al 1996. The American heritage
book of English usage: A practical and authoritative
guide to contemporary English. Houghton Mifflin
Company.
Livia Polanyi and Annie Zaenen. 2004. Contextual
lexical valence shifters. In Exploring Attitude and
Affect in Text: Theories and Applications: Papers
from the 2004 Spring Symposium, AAAI.
Delip Rao and Deepak Ravichandran. 2009. Semi-
Supervised Polarity Lexicon Induction. In EACL.
Dan Roth and Wen-tau Yih. 2004. A Linear Program-
ming Formulation for Global Inference in Natural
Language Tasks. In CoNLL.
Hiroya Takamura, Takashi Inui, andManabu Okumura.
2005. Extracting semantic orientations of words us-
ing spin model. In ACL.
Janyce Wiebe, Theresa Wilson and Claire Cardie.
2005. Annotating expressions of opinions and emo-
tions in language. In LanguageResources and Eval-
uation (formerly Computers and the Humanities),
39(2-3):165210.
Theresa Wilson, Janyce Wiebe and Paul Hoffmann.
2005. Recognizing contextual polarity in phrase-
level sentiment analysis. In Proceedings of
HLT/EMNLP.
Theresa Wilson, Janyce Wiebe, and Paul Hoffmann.
2009. Recognizing Contextual Polarity: an explo-
ration of features for phrase-level sentiment analy-
sis. In Computational Linguistics 35(3).
598
Multidocument Summarization via Information Extraction 
Michael White and Tanya Korelsky 
CoGenTex, Inc. 
Ithaca, NY 
mike,tanya@cogentex.com 
Claire Cardie, Vincent Ng, David Pierce, and 
Kiri Wagstaff 
Department of Computer Science 
Cornell University, Ithaca, NY 
cardie,yung,pierce,wkiri@cs.cornell.edu 
 
ABSTRACT 
We present and evaluate the initial version of RIPTIDES, a 
system that combines information extraction, extraction-based 
summarization, and natural language generation to support user-
directed multidocument summarization. 
1. INTRODUCTION 
Although recent years has seen increased and successful research 
efforts in the areas of single-document summarization, 
multidocument summarization, and information extraction, very 
few investigations have explored the potential of merging 
summarization and information extraction techniques.  This paper 
presents and evaluates the initial version of RIPTIDES, a system 
that combines information extraction (IE), extraction-based 
summarization, and natural language generation to support user-
directed multidocument summarization.  (RIPTIDES stands for 
RapIdly Portable Translingual Information extraction and 
interactive multiDocumEnt Summarization.)  Following [10], we 
hypothesize that IE-supported summarization will enable the 
generation of more accurate and targeted summaries in specific 
domains than is possible with current domain-independent 
techniques.  
In the sections below, we describe the initial implementation and 
evaluation of the RIPTIDES IE-supported summarization system.  
We conclude with a brief discussion of related and ongoing work. 
2. SYSTEM DESIGN 
Figure 1 depicts the IE-supported summarization system. The 
system first requires that the user select (1) a set of documents in 
which to search for information, and (2) one or more scenario 
templates (extraction domains) to activate. The user optionally 
provides filters and preferences on the scenario template slots, 
specifying what information s/he wants to be reported in the 
summary. RIPTIDES next applies its Information Extraction 
subsystem to generate a database of extracted events for the 
selected domain and then invokes the Summarizer to generate a 
natural language summary of the extracted information subject to 
the user?s constraints. In the subsections below, we describe the 
IE system and the Summarizer in turn. 
2.1 IE System 
The domain for the initial IE-supported summarization system and 
its evaluation is natural disasters.  Very briefly, a top-level natural 
disasters scenario template contains: document-level information 
(e.g. docno, date-time); zero or more agent elements denoting 
each person, group, and organization in the text; and zero or 
more disaster elements.  Agent elements encode standard 
information for named entities (e.g. name, position, geo-political 
unit).  For the most part, disaster elements also contain standard 
event-related fields (e.g. type, number, date, time, location, 
damage sub-elements).  
The final product of the RIPTIDES system, however, is not a set 
of scenario templates, but a user-directed multidocument 
summary.  This difference in goals influences a number of 
template design issues.  First, disaster elements must distinguish 
different reports or views of the same event from multiple sources.  
As a result, the system creates a separate disaster event for each 
such account.  Disaster elements should also include the reporting 
agent, date, time, and location whenever possible.  In addition, 
damage elements (i.e. human and physical effects) are best 
grouped according to the reporting event.  Finally, a slight 
broadening of the IE task was necessary in that extracted text was 
not constrained to noun phrases.  In particular, adjectival and 
adverbial phrases that encode reporter confidence, and sentences 
and clauses denoting relief effort progress appear beneficial for 
creating informed summaries.  Figure 2 shows the scenario 
template for one of 25 texts tracking the 1998 earthquake in 
Afghanistan (TDT2 Topic 89).  The texts were also manually 
annotated for noun phrase coreference; any phrase involved in a 
coreference relation appears underlined in the running text. 
The RIPTIDES system for the most part employs a traditional IE 
architecture [4].  In addition, we use an in-house implementation 
of the TIPSTER architecture [8] to manage all linguistic 
annotations.  A preprocessor first finds sentences and tokens.  For 
syntactic analysis, we currently use the Charniak [5] parser, which 
creates Penn Treebank-style parses [9] rather than the partial 
parses used in most IE systems.  Output from the parser is 
converted automatically into TIPSTER parse and part-of-speech 
annotations, which are added to the set of linguistic annotations 
for the document.  The extraction phase of the system identifies 
domain-specific relations among relevant entities in the text.  It 
relies on Autoslog-XML, an XSLT implementation of the 
Autoslog-TS system [12], to acquire extraction patterns.  
Autoslog-XML is a weakly supervised learning system that 
requires two sets of texts for training ? one set comprises texts 
relevant to the domain of interest and the other, texts not relevant 
 
 
 
to the domain.  Based on these and a small set of extraction 
pattern templates, the system finds a ranked list of possible 
extraction patterns, which a user then annotates with the 
appropriate extraction label (e.g. victim). Once acquired, the 
patterns are applied to new documents to extract slot fillers for the 
domain.  Selectional restrictions on allowable slot fillers are 
implemented using WordNet [6] and BBN?s Identifinder [3] 
named entity component.  In the current version of the system, no 
coreference resolution is attempted; instead, we rely on a very 
simple set of heuristics to guide the creation of output templates.  
The disaster scenario templates extracted for each text are 
provided as input to the summarization component along with all 
linguistic annotations accrued in the IE phase.  No relief slots are 
included in the output at present, since there was insufficient 
annotated data to train a reliable sentence categorizer. 
2.2 The Summarizer 
In order to include relief and other potentially relevant 
information not currently found in the scenario templates, the 
Summarizer extracts selected sentences from the input articles and 
adds them to the summaries generated from the scenario 
templates.  The extracted sentences are listed under the heading 
Selected News Excerpts, as shown in the two sample summaries 
appearing in Figures 3 and 4, and discussed further in Section 
2.2.5 below. 
2.2.1 Summarization Stages 
The Summarizer produces each summary in three main stages.  In 
the first stage, the output templates are merged into an event-
oriented structure, while keeping track of source information.  The 
merge operation currently relies on simple heuristics to group 
extracted facts that are comparable; for example, during this phase 
damage reports are grouped according to whether they pertain to 
the event as a whole, or instead to damage in the same particular 
location.  Heuristics are also used in this stage to determine the 
most relevant damage reports, taking into account specificity, 
recency and news source.  Towards the same objective but using a 
more surface-oriented means, simple word-overlap clustering is 
used to group sentences from different documents into clusters 
that are likely to report similar content.  In the second stage, a 
base importance score is first assigned to each slot/sentence based 
on a combination of document position, document recency and 
group/cluster membership.  The base importance scores are then 
adjusted according to user-specified preferences and matching 
scenario
 templates
A powerful earthquake struck Afghanistan on May 
30 at 11:25? 
Damage 
VOA (06/02/1998) estimated that 5,000 were killed 
by the earthquake, whereas AP (APW, 06/02/1998) 
instead reported ? 
Relief Status 
CNN (06/02/1998): Food, water, medicine 
and other supplies have started to arrive.  
[?] 
NLG of
summary
content
selection
multi-document
template
merging
text collection
IE
System
user information
need
event-oriented
structure
event-oriented
structure with slot
importance scores
summary
Summarizer
slot   filler
slot   filler
slot   filler
slot   filler
...   
slot   filler
slot   filler
slot   filler
slot   filler
...   
slot   filler
slot   filler
slot   filler
slot   filler
...   
slot   filler
slot   filler
slot   filler
slot   filler
...   
Figure 1.  RIPTIDES System Design 
criteria.  The adjusted scores are used to select the most important 
slots/sentences to include in the summary, subject to the user-
specified word limit.  In the third and final stage, the summary is 
generated from the resulting content pool using a combination of 
top-down, schema-like text building rules and surface-oriented 
revisions.  The extracted sentences are simply listed in document 
order, grouped into blocks of adjacent sentences. 
2.2.2 Specificity of Numeric Estimates 
In order to intelligently merge and summarize scenario templates, 
we found it necessary to explicitly handle numeric estimates of 
varying specificity.  While we did find specific numbers (such as 
3,000) in some damage estimates, we also found cases with no 
number phrase at all (e.g. entire villages).  In between these 
extremes, we found vague estimates (thousands) and ranges of 
numbers (anywhere from 2,000 to 5,000).  We also found phrases 
that cannot be easily compared (more than half the region?s 
residents). 
To merge related damage information, we first calculate the 
numeric specificity of the estimate as one of the values NONE, 
VAGUE, RANGE, SPECIFIC, or INCOMPARABLE, based on the presence 
of a small set of trigger words and phrases (e.g. several, as many 
as, from ? to).  Next, we identify the most specific current 
estimates by news source, where a later estimate is considered to 
update an earlier estimate if it is at least as specific.  Finally, we 
determine two types of derived information units, namely (1) the 
minimum and maximum estimates across the news sources, and 
(2) any intermediate estimates that are lower than the maximum 
estimate.1   
In the content determination stage, scores are assigned to the 
derived information units based on the maximum score of the 
underlying units.  In the summary generation stage, a handful of 
text planning rules are used to organize the text for these derived 
units, highlighting agreement and disagreement across sources. 
2.2.3 Improving the Coherence of Extracted 
Sentences 
In our initial attempt to include extracted sentences, we simply 
chose the top ranking sentences that would fit within the word 
limit, subject to the constraint that no more than one sentence per 
cluster could be chosen, in order to help avoid redundancy.  We 
found that this approach often yielded summaries with very poor 
coherence, as many of the included sentences were difficult to 
make sense of in isolation.   
To improve the coherence of the extracted sentences, we have 
experimented with trying to boost coherence by favoring 
sentences in the context of the highest-ranking sentences over 
those with lower ranking scores, following the hypothesis that it is 
better to cover fewer topics in more depth than to change topics 
excessively.  In particular, we assign a score to a set of sentences 
by summing the base scores plus increasing coherence boosts for 
adjacent sentences, sentences that precede ones with an initial 
                                                                
1
 Less specific estimates such as ?hundreds? are considered lower 
than more specific numbers such as ?5000? when they are lower 
by more than a factor of 10. 
 Document no.: ABC19980530.1830.0342  
Date/time: 05/30/1998 18:35:42.49  
Disaster Type: earthquake  
?location: Afghanistan  
?date: today  
?magnitude: 6.9  
?magnitude-confidence: high  
?epicenter: a remote part of the country  
?damage:  
               human-effect:  
                   victim: Thousands of people  
                   number: Thousands  
                  outcome: dead  
                  confidence: medium  
                  confidence-marker: feared  
               physical-effect:  
                  object: entire villages  
                  outcome: damaged  
                  confidence: medium  
                  confidence-marker: Details now hard to 
                                                come by / reports say  
PAKISTAN MAY BE PREPARING 
FOR ANOTHER TEST  
Thousands of people are feared dead following... (voice-
over) ...a powerful earthquake that hit Afghanistan today. 
The quake registered 6.9 on the Richter scale, centered in 
a remote part of the country. (on camera) Details now 
hard to come by, but reports say entire villages were 
buried by the quake.  
 
Figure 2.  Example scenario template for the natural disasters domain 
Earthquake strikes quake-devastated villages in 
northern Afghanistan 
A earthquake struck quake-devastated villages in northern 
Afghanistan Saturday. The earthquake had a magnitude of 6.9 
on the Richter scale on the Richter scale. 
Damage 
Estimates of the death toll varied. CNN (06/02/1998) provided 
the highest estimate of 4,000 dead, whereas ABC 
(06/01/1998) gave the lowest estimate of 140 dead. 
In capital: Estimates of the number injured varied. 
Selected News Excerpts 
CNN (06/01/98):  
Thousands are dead and thousands more are still missing. Red 
cross officials say the first priority is the injured. Getting 
medicine to them is difficult due to the remoteness of the 
villages affected by the quake.  
PRI (06/01/98):  
We spoke to the head of the international red cross there, Bob 
McCaro on a satellite phone link. He says it?s difficult to 
know the full extent of the damage because the region is so 
remote. There?s very little infrastructure.  
PRI (06/01/98):  
Bob McCaro is the head of the international red cross in the 
neighboring country of Pakistan. He?s been speaking to us 
from there on the line.  
APW (06/02/98):  
The United Nations, the Red Cross and other agencies have 
three borrowed helicopters to deliver medical aid.  
Figure 4.  200 word summary of actual IE output, with 
emphasis on Red Cross 
pronoun, and sentences that preceded ones with strongly 
connecting discourse markers such as however, nevertheless, etc.  
We have also softened the constraint on multiple sampling from 
the same cluster, making use of a redundancy penalty in such 
cases.  We then perform a randomized local search for a good set 
of sentences according to these scoring criteria.  
2.2.4 Implementation 
The Summarizer is implemented using the Apache 
implementation of XSLT [1] and CoGenTex?s Exemplars 
Framework [13].  The Apache XSLT implementation has 
provided a convenient way to rapidly develop a prototype 
implementation of the first two processing stages using a series of 
XML transformations.  In the first step of the third summary 
generation stage, the text building component of the Exemplars 
Framework constructs a ?rough draft? of the summary text.  In 
this rough draft version, XML markup is used to partially encode 
the rhetorical, referential, semantic and morpho-syntactic structure 
of the text.  In the second generation step, the Exemplars text 
polishing component makes use of this markup to trigger surface-
Earthquake strikes Afghanistan 
A powerful earthquake struck Afghanistan last Saturday at 
11:25. The earthquake was centered in a remote part of the 
country and had a magnitude of 6.9 on the Richter scale. 
Damage 
Estimates of the death toll varied. VOA (06/02/1998) 
provided the highest estimate of 5,000 dead. CNN 
(05/31/1998) and CNN (06/02/1998) supplied lower estimates 
of 3,000 and up to 4,000 dead, whereas APW (06/02/1998) 
gave the lowest estimate of anywhere from 2,000 to 5,000 
dead. People were injured, while thousands more were 
missing. Thousands were homeless. 
Quake-devastated villages were damaged. Estimates of the 
number of villages destroyed varied. CNN (05/31/1998) 
provided the highest estimate of 50 destroyed, whereas VOA 
(06/04/1998) gave the lowest estimate of at least 25 destroyed. 
In Afghanistan, thousands of people were killed. 
Further Details 
Heavy after shocks shook northern afghanistan. More homes 
were destroyed. More villages were damaged. 
Landslides or mud slides hit the area. 
Another massive quake struck the same region three months 
earlier. Some 2,300 victims were injured. 
Selected News Excerpts 
ABC (05/30/98):  
PAKISTAN MAY BE PREPARING FOR ANOTHER TEST 
Thousands of people are feared dead following...  
ABC (06/01/98):  
RESCUE WORKERS CHALLENGED IN AFGHANISTAN 
There has been serious death and devastation overseas. In 
Afghanistan...  
CNN (06/02/98):  
Food, water, medicine and other supplies have started to 
arrive. But a U.N. relief coordinator says it?s a "scenario from 
hell".  
Figure 3.  200 word summary of simulated IE output, with 
emphasis on damage 
oriented revision rules that smooth the text into a more polished 
form.  A distinguishing feature of our text polishing approach is 
the use of a bootstrapping tool to partially automate the 
acquisition of application-specific revision rules from examples. 
2.2.5 Sample Summaries 
Figures 3 and 4 show two sample summaries that were included in 
our evaluation (see Section 3 for details).  The summary in Figure 
3 was generated from simulated output of the IE system, with 
preference given to damage information; the summary in Figure 4 
was generated from the actual output of the current IE system, 
with preference given to information including the words Red 
Cross.   
While the summary in Figure 3 does a reasonable job of reporting 
the various current estimates of the death toll, the estimates of the 
death toll shown in Figure 4 are less accurate, because the IE 
system failed to extract some reports, and the Summarizer failed 
to correctly merge others.  In particular, note that the lowest 
estimate of 140 dead attributed to ABC is actually a report about 
the number of school children killed in a particular town.  Since 
no location was given for this estimate by the IE system, the 
Summarizer?s simple heuristic for localized damaged reports ? 
namely, to consider a damage report to be localized if a location is 
given that is not in the same sentence as the initial disaster 
description ? did not work here.  The summary in Figure 3 also 
suffered from some problems with merging:  the inclusion of a 
paragraph about thousands killed in Afghanistan is due to an 
incorrect classification of this report as a localized one (owing to 
an error in sentence boundary detection), and the discussion of the 
number of villages damaged should have included a report of at 
least 80 towns or villages damaged. 
Besides the problems related to slot extraction and merging 
mentioned above, the summaries shown in Figures 3 and 4 suffer 
from relatively poor fluency.  In particular, the summaries could 
benefit from better use of descriptive terms from the original 
articles, as well as better methods of sentence combination and 
rhetorical structuring.  Nevertheless, as will be discussed further 
in Section 4, we suggest that the summaries show the potential for 
our techniques to intelligently combine information from many 
articles on the same natural disaster. 
3. EVALUATION AND INITIAL RESULTS 
To evaluate the initial version of the IE-supported summarization 
system, we used Topic 89 from the TDT2 collection ? 25 texts 
on the 1998 Afghanistan earthquake. Each document was 
annotated manually with the natural disaster scenario templates 
that comprise the desired output of the IE system. In addition, 
treebank-style syntactic structure annotations were added 
automatically using the Charniak parser.  Finally, MUC-style 
noun phrase coreference annotations were supplied manually.  All 
annotations are in XML.  The manual and automatic annotations 
were automatically merged, leading to inaccurate annotation 
extents in some cases.  
Next, the Topic 89 texts were split into a development corpus and 
a test corpus.  The development corpus was used to build the 
summarization system; the evaluation summaries were generated 
from the test corpus.  We report on three different variants of the 
RIPTIDES system here: in the first variant (RIPTIDES-SIM1), an 
earlier version of the Summarizer uses the simulated output of the 
IE system as its input, including the relief annotations; in the 
second variant (RIPTIDES-SIM2), the current version of the 
Summarizer uses the simulated output of the IE system, without 
the relief annotations; and in the third variant (RIPTIDES-IE), the 
Summarizer uses the actual output of the IE system as its input.2   
Summaries generated by the RIPTIDES variants were compared 
to a Baseline system consisting of a simple, sentence-extraction 
multidocument summarizer relying only on document position, 
recency, and word overlap clustering.  (As explained in the 
previous section, we have found that word overlap clustering 
provides a bare bones way to help determine what information is 
repeated in multiple articles, thereby indicating importance to the 
document set as a whole, as well as to help reduce redundancy in 
the resulting summaries.)  In addition, the RIPTIDES and 
Baseline system summaries were compared against the summaries 
of two human authors.  All of the summaries were graded with 
respect to content, organization, and readability on an A-F scale 
by three graduate students, all of whom were unfamiliar with this 
project.  Note that the grades for RIPTIDES-SIM1, the Baseline 
system, and the two human authors were assigned during a first 
evaluation in October, 2000, whereas the grades for RIPTIDES-
SIM2 and RIPTIDES-IE were assigned by the same graders in an 
update to this evaluation in April, 2001. 
Each system and author was asked to generate four summaries of 
different lengths and emphases: (1) a 100-word summary of the 
May 30 and May 31 articles; (2) a 400-word summary of all test 
articles, emphasizing specific, factual information; (3) a 200-word 
summary of all test articles, focusing on the damage caused by the 
quake, and excluding information about relief efforts, and (4) a 
200-word summary of all test articles, focusing on the relief 
efforts, and highlighting the Red Cross?s role in these efforts.  
The results are shown in Tables 1 and 2.  Table 1 provides the 
overall grade for each system or author averaged across all graders 
and summaries, where each assigned grade has first been 
converted to a number (with A=4.0 and F=0.0) and the average 
converted back to a letter grade.  Table 2 shows the mean and 
standard deviations of the overall, content, organization, and 
readability scores for the RIPTIDES and the Baseline systems 
averaged across all graders and summaries.  Where the differences 
vs. the Baseline system are significant according to the t-test, the 
p-values are shown. 
Given the amount of development effort that has gone into the 
system to date, we were not surprised that the RIPTIDES variants 
fared poorly when compared against the manually written 
summaries, with RIPTIDES-SIM2 receiving an average grade of 
C, vs. A- and B+ for the human authors.  Nevertheless, we were 
pleased to find that RIPTIDES-SIM2 scored a full grade ahead of 
the Baseline summarizer, which received a D, and that 
                                                                
2
 Note that since the summarizers for the second and third variants 
did not have access to the relief sentence categorizations, we 
decided to exclude from their input the two articles (one 
training, one test) classified by TDT2 Topic 89 as only 
containing brief mentions of the event of interest, as otherwise 
they would have no means of excluding the largely irrelevant 
material in these documents. 
RIPTIDES-IE managed a slightly higher grade of D+, despite the 
immature state of the IE system.  As Table 2 shows, the 
differences in the overall scores were significant for all three 
RIPTIDES variants, as were the scores for organization and 
readability, though not for content in the cases of RIPTIDES-
SIM1 and RIPTIDES-IE. 
4. RELATED AND ONGOING WORK 
The RIPTIDES system is most similar to the SUMMONS system 
of Radev and McKeown [10], which summarized the results of 
MUC-4 IE systems in the terrorism domain.  As a pioneering 
effort, the SUMMONS system was the first to suggest the 
potential of combining IE with NLG in a summarization system, 
though no evaluation was performed.  In comparison to 
SUMMONS, RIPTIDES appears to be designed to more 
completely summarize larger input document sets, since it focuses 
more on finding the most relevant current information, and since 
it includes extracted sentences to round out the summaries.  
Another important difference is that SUMMONS sidestepped the 
problem of comparing reported numbers of varying specificity 
(e.g. several thousand vs. anywhere from 2000 to 5000 vs. up to 
4000 vs. 5000), whereas we have implemented rules for doing so.  
Finally, we have begun to address some of the difficult issues that 
arise in merging information from multiple documents into a 
coherent event-oriented view, though considerable challenges 
remain to be addressed in this area. 
The sentence extraction part of the RIPTIDES system is similar to 
the domain-independent multidocument summarizers of Goldstein 
et al [7] and Radev et al [11] in the way it clusters sentences 
across documents to help determine which sentences are central to 
the collection, as well as to reduce redundancy amongst sentences 
included in the summary.  It is simpler than these systems insofar 
as it does not make use of comparisons to the centroid of the 
document set.  As pointed out in [2], it is difficult in general for 
multidocument summarizers to produce coherent summaries, 
since it is less straightforward to rely on the order of sentences in 
the underlying documents than in the case of single-document 
summarization.  Having also noted this problem, we have focused 
our efforts in this area on attempting to balance coherence and 
informativeness in selecting sets of sentences to include in the 
summary. 
In ongoing work, we are investigating techniques for improving 
merging accuracy and summary fluency in the context of 
summarizing the more than 150 news articles we have collected 
from the web about each of the recent earthquakes in Central 
America and India (January, 2001).  We also plan to investigate 
using tables and hypertext drill-down as a means to help the user 
verify the accuracy of the summarized information. 
By perusing the web collections mentioned above, we can see that 
trying to manually extricate the latest damage estimates from 150+ 
news articles from multiple sources on the same natural disaster 
would be very tedious.  Although estimates do usually converge, 
they often change rapidly at first, and then are gradually dropped 
from later articles, and thus simply looking at the latest article is 
not satisfactory.  While significant challenges remain, we suggest 
that our initial system development and evaluation shows that our 
approach has the potential to accurately summarize damage 
estimates, as well as identify other key story items using shallower 
techniques, and thereby help alleviate information overload in 
specific domains. 
5. ACKNOWLEDGMENTS 
We thank Daryl McCullough for implementing the coherence 
boosting randomized local search, and we thank Ted Caldwell, 
Daryl McCullough, Corien Bakermans, Elizabeth Conrey, 
Purnima Menon and Betsy Vick for their participation as authors 
and graders.  This work has been partially supported by DARPA 
TIDES contract no. N66001-00-C-8009. 
6. REFERENCES 
[1] The Apache XML Project.  2001.  ?Xalan Java.?  
http://xml.apache.org/. 
Table 1 
Baseline RIPTIDES-SIM1 RIPTIDES-SIM2 RIPTIDES-IE Person 1 Person 2 
D C/C- C D+ A- B+ 
 
 
Table 2 
 Baseline RIPTIDES-SIM1 RIPTIDES-SIM2 RIPTIDES-IE 
Overall 0.96 +/- 0.37 1.86 +/- 0.56 (p=.005) 2.1 +/- 0.59 (p=.005) 1.21 +/- 0.46 (p=.05) 
Content 1.44 +/- 1.0 1.78 +/- 0.68 2.2 +/- 0.65 (p=.005) 1.18 +/- 0.6 
Organization 0.64 +/- 0.46 2.48 +/- 0.56 (p=.005) 2.08 +/- 0.77 (p=.005) 1.08 +/- 0.65 (p=.05) 
Readability 0.75 +/- 0.6 1.58 +/- 0.61 (p=.005) 2.05 +/- 0.65 (p=.005) 1.18 +/- 0.62 (p=.05) 
 
[2] Barzilay, R., Elhadad, N. and McKeown, K.  2001.  
?Sentence Ordering in Multidocument Summarization.?  In 
Proceedings of HLT 2001. 
[3] Bikel, D., Schwartz, R. and Weischedel, R.  1999.  ?An 
Algorithm that Learns What's in a Name.?  Machine 
Learning 34:1-3, 211-231. 
[4] Cardie, C. 1997. ?Empirical Methods in Information 
Extraction.?  AI Magazine 18(4): 65-79. 
[5] Charniak, E.  1999.  ?A maximum-entropy-inspired parser.?  
Brown University Technical Report CS99-12. 
[6] Fellbaum, C.  1998.  WordNet: An Electronic Lexical 
Database.  MIT Press, Cambridge, MA. 
[7] Goldstein, J., Mittal, V., Carbonell, J. and Kantrowitz, M.  
2000.  ?Multi-document summarization by sentence 
extraction.?  In Proceedings of the ANLP/NAACL Workshop 
on Automatic Summarization, Seattle, WA. 
[8] Grishman, R.  1996.  ?TIPSTER Architecture Design 
Document Version 2.2.?  DARPA, available at 
http://www.tipster.org/. 
[9] Marcus, M., Marcinkiewicz, M. and Santorini, B.  1993.  
?Building a Large, Annotated Corpus of English: The Penn 
Treebank.?  Computational Linguistics 19:2, 313-330. 
[10] Radev, D. R. and McKeown, K. R.  1998. ?Generating 
natural language summaries from multiple on-line sources.?  
Computational Linguistics 24(3):469-500. 
[11] Radev, D. R., Jing, H. and Budzikowska, M.  2000.  
?Summarization of multiple documents: clustering, sentence 
extraction, and evaluation.?  In Proceedings of the 
ANLP/NAACL Workshop on Summarization, Seattle, WA. 
[12] Riloff, E.  1996.  ?Automatically Generating Extraction 
Patterns from Untagged Text.?  In Proceedings of the 
Thirteenth National Conference on Artificial Intelligence, 
Portland, OR, 1044-1049.  AAAI Press / MIT Press. 
[13] White, M. and Caldwell, T.  1998.  ?EXEMPLARS: A 
Practical, Extensible Framework for Dynamic Text 
Generation.?  In Proceedings of the Ninth International 
Workshop on Natural Language Generation, Niagara-on-
the-Lake, Canada, 266-275. 
 
 
Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural Language
Processing (HLT/EMNLP), pages 355?362, Vancouver, October 2005. c?2005 Association for Computational Linguistics
Identifying Sources of Opinions with Conditional Random Fields and
Extraction Patterns
Yejin Choi and Claire Cardie
Department of Computer Science
Cornell University
Ithaca, NY 14853
{ychoi,cardie}@cs.cornell.edu
Ellen Riloff and Siddharth Patwardhan
School of Computing
University of Utah
Salt Lake City, UT 84112
{riloff,sidd}@cs.utah.edu
Abstract
Recent systems have been developed for
sentiment classification, opinion recogni-
tion, and opinion analysis (e.g., detect-
ing polarity and strength). We pursue an-
other aspect of opinion analysis: identi-
fying the sources of opinions, emotions,
and sentiments. We view this problem as
an information extraction task and adopt
a hybrid approach that combines Con-
ditional Random Fields (Lafferty et al,
2001) and a variation of AutoSlog (Riloff,
1996a). While CRFs model source iden-
tification as a sequence tagging task, Au-
toSlog learns extraction patterns. Our re-
sults show that the combination of these
two methods performs better than either
one alone. The resulting system identifies
opinion sources with 79.3% precision and
59.5% recall using a head noun matching
measure, and 81.2% precision and 60.6%
recall using an overlap measure.
1 Introduction
In recent years, there has been a great deal of in-
terest in methods for automatically identifying opin-
ions, emotions, and sentiments in text. Much of
this research explores sentiment classification, a text
categorization task in which the goal is to classify
a document as having positive or negative polar-
ity (e.g., Das and Chen (2001), Pang et al (2002),
Turney (2002), Dave et al (2003), Pang and Lee
(2004)). Other research efforts analyze opinion ex-
pressions at the sentence level or below to recog-
nize opinions, their polarity, and their strength (e.g.,
Dave et al (2003), Pang and Lee (2004), Wilson et
al. (2004), Yu and Hatzivassiloglou (2003), Wiebe
and Riloff (2005)). Many applications could ben-
efit from these opinion analyzers, including prod-
uct reputation tracking (e.g., Morinaga et al (2002),
Yi et al (2003)), opinion-oriented summarization
(e.g., Cardie et al (2004)), and question answering
(e.g., Bethard et al (2004), Yu and Hatzivassiloglou
(2003)).
We focus here on another aspect of opinion
analysis: automatically identifying the sources of
the opinions. Identifying opinion sources will
be especially critical for opinion-oriented question-
answering systems (e.g., systems that answer ques-
tions of the form ?How does [X] feel about [Y]??)
and opinion-oriented summarization systems, both
of which need to distinguish the opinions of one
source from those of another.1
The goal of our research is to identify direct and
indirect sources of opinions, emotions, sentiments,
and other private states that are expressed in text.
To illustrate the nature of this problem, consider the
examples below:
S1: Taiwan-born voters favoring independence...
1In related work, we investigate methods to identify the
opinion expressions (e.g., Riloff and Wiebe (2003), Wiebe and
Riloff (2005), Wilson et al (2005)) and the nesting structure
of sources (e.g., Breck and Cardie (2004)). The target of each
opinion, i.e., what the opinion is directed towards, is currently
being annotated manually for our corpus.
355
S2: According to the report, the human rights
record in China is horrendous.
S3: International officers believe that the EU will
prevail.
S4: International officers said US officials want the
EU to prevail.
In S1, the phrase ?Taiwan-born voters? is the di-
rect (i.e., first-hand) source of the ?favoring? sen-
timent. In S2, ?the report? is the direct source of
the opinion about China?s human rights record. In
S3, ?International officers? are the direct source of
an opinion regarding the EU. The same phrase in
S4, however, denotes an indirect (i.e., second-hand,
third-hand, etc.) source of an opinion whose direct
source is ?US officials?.
In this paper, we view source identification as an
information extraction task and tackle the problem
using sequence tagging and pattern matching tech-
niques simultaneously. Using syntactic, semantic,
and orthographic lexical features, dependency parse
features, and opinion recognition features, we train a
linear-chain Conditional Random Field (CRF) (Laf-
ferty et al, 2001) to identify opinion sources. In ad-
dition, we employ features based on automatically
learned extraction patterns and perform feature in-
duction on the CRF model.
We evaluate our hybrid approach using the NRRC
corpus (Wiebe et al, 2005), which is manually
annotated with direct and indirect opinion source
information. Experimental results show that the
CRF model performs well, and that both the extrac-
tion patterns and feature induction produce perfor-
mance gains. The resulting system identifies opinion
sources with 79.3% precision and 59.5% recall us-
ing a head noun matching measure, and 81.2% pre-
cision and 60.6% recall using an overlap measure.
2 The Big Picture
The goal of information extraction (IE) systems is
to extract information about events, including the
participants of the events. This task goes beyond
Named Entity recognition (e.g., Bikel et al (1997))
because it requires the recognition of role relation-
ships. For example, an IE system that extracts in-
formation about corporate acquisitions must distin-
guish between the company that is doing the acquir-
ing and the company that is being acquired. Sim-
ilarly, an IE system that extracts information about
terrorism must distinguish between the person who
is the perpetrator and the person who is the victim.
We hypothesized that IE techniques would be well-
suited for source identification because an opinion
statement can be viewed as a kind of speech event
with the source as the agent.
We investigate two very different learning-based
methods from information extraction for the prob-
lem of opinion source identification: graphical mod-
els and extraction pattern learning. In particular, we
consider Conditional Random Fields (Lafferty et al,
2001) and a variation of AutoSlog (Riloff, 1996a).
CRFs have been used successfully for Named En-
tity recognition (e.g., McCallum and Li (2003),
Sarawagi and Cohen (2004)), and AutoSlog has per-
formed well on information extraction tasks in sev-
eral domains (Riloff, 1996a). While CRFs treat
source identification as a sequence tagging task, Au-
toSlog views the problem as a pattern-matching task,
acquiring symbolic patterns that rely on both the
syntax and lexical semantics of a sentence. We hy-
pothesized that a combination of the two techniques
would perform better than either one alone.
Section 3 describes the CRF approach to identify-
ing opinion sources and the features that the system
uses. Section 4 then presents a new variation of Au-
toSlog, AutoSlog-SE, which generates IE patterns to
extract sources. Section 5 describes the hybrid sys-
tem: we encode the IE patterns as additional features
in the CRF model. Finally, Section 6 presents our
experimental results and error analysis.
3 Semantic Tagging via Conditional
Random Fields
We defined the problem of opinion source identifi-
cation as a sequence tagging task via CRFs as fol-
lows. Given a sequence of tokens, x = x1x2...xn,
we need to generate a sequence of tags, or labels,
y = y1y2...yn. We define the set of possible label
values as ?S?, ?T?, ?-?, where ?S? is the first to-
ken (or Start) of a source, ?T? is a non-initial token
(i.e., a conTinuation) of a source, and ?-? is a token
that is not part of any source.2
A detailed description of CRFs can be found in
2This is equivalent to the IOB tagging scheme used in syn-
tactic chunkers (Ramshaw and Marcus, 1995).
356
Lafferty et al (2001). For our sequence tagging
problem, we create a linear-chain CRF based on
an undirected graph G = (V,E), where V is the
set of random variables Y = {Yi|1 ? i ? n},
one for each of n tokens in an input sentence;
and E = {(Yi?1, Yi)|1 < i ? n} is the set
of n ? 1 edges forming a linear chain. For each
sentence x, we define a non-negative clique poten-
tial exp(
?K
k=1 ?kfk(yi?1, yi, x)) for each edge, and
exp(?K?k=1 ??kf ?k(yi, x)) for each node, where fk(...)
is a binary feature indicator function, ?k is a weight
assigned for each feature function, and K and K ?
are the number of features defined for edges and
nodes respectively. Following Lafferty et al (2001),
the conditional probability of a sequence of labels y
given a sequence of tokens x is:
P (y|x) = 1Zx
exp
?
X
i,k
?k fk(yi?1, yi, x)+
X
i,k
??k f ?k(yi, x)
?
(1)
Zx =
X
y
exp
?
X
i,k
?k fk(yi?1, yi, x) +
X
i,k
??k f ?k(yi, x)
?
(2)
where Zx is a normalization constant for each
x. Given the training data D, a set of sen-
tences paired with their correct ?ST-? source la-
bel sequences, the parameters of the model are
trained to maximize the conditional log-likelihood
?
(x,y)?D P (y|x). For inference, given a sentence x
in the test data, the tagging sequence y is given by
argmaxy?P (y?|x).
3.1 Features
To develop features, we considered three properties
of opinion sources. First, the sources of opinions are
mostly noun phrases. Second, the source phrases
should be semantic entities that can bear or express
opinions. Third, the source phrases should be di-
rectly related to an opinion expression. When con-
sidering only the first and second criteria, this task
reduces to named entity recognition. Because of the
third condition, however, the task requires the recog-
nition of opinion expressions and a more sophisti-
cated encoding of sentence structure to capture re-
lationships between source phrases and opinion ex-
pressions.
With these properties in mind, we define the fol-
lowing features for each token/word xi in an input
sentence. For pedagogical reasons, we will describe
some of the features as being multi-valued or cate-
gorical features. In practice, however, all features
are binarized for the CRF model.
Capitalization features We use two boolean fea-
tures to represent the capitalization of a word:
all-capital, initial-capital.
Part-of-speech features Based on the lexical cat-
egories produced by GATE (Cunningham et al,
2002), each token xi is classified into one of a set
of coarse part-of-speech tags: noun, verb, adverb,
wh-word, determiner, punctuation, etc. We do the
same for neighboring words in a [?2,+2] window
in order to assist noun phrase segmentation.
Opinion lexicon features For each token xi, we in-
clude a binary feature that indicates whether or not
the word is in our opinion lexicon ? a set of words
that indicate the presence of an opinion. We do the
same for neighboring words in a [?1,+1] window.
Additionally, we include for xi a feature that in-
dicates the opinion subclass associated with xi, if
available from the lexicon. (e.g., ?bless? is clas-
sified as ?moderately subjective? according to the
lexicon, while ?accuse? and ?berate? are classified
more specifically as ?judgments?.) The lexicon is
initially populated with approximately 500 opinion
words 3 from (Wiebe et al, 2002), and then aug-
mented with opinion words identified in the training
data. The training data contains manually produced
phrase-level annotations for all expressions of opin-
ions, emotions, etc. (Wiebe et al, 2005). We col-
lected all content words that occurred in the training
set such that at least 50% of their occurrences were
in opinion annotations.
Dependency tree features For each token xi, we
create features based on the parse tree produced by
the Collins (1999) dependency parser. The purpose
of the features is to (1) encode structural informa-
tion, and (2) indicate whether xi is involved in any
grammatical relations with an opinion word. Two
pre-processing steps are required before features can
be constructed:
3Some words are drawn from Levin (1993); others are from
Framenet lemmas (Baker et al 1998) associated with commu-
nication verbs.
357
1. Syntactic chunking. We traverse the depen-
dency tree using breadth-first search to identify
and group syntactically related nodes, produc-
ing a flatter, more concise tree. Each syntac-
tic ?chunk? is also assigned a grammatical role
(e.g., subject, object, verb modifier, time,
location, of-pp, by-pp) based on its con-
stituents. Possessives (e.g., ?Clinton?s idea?)
and the phrase ?according to X? are handled as
special cases in the chunking process.
2. Opinion word propagation. Although the
opinion lexicon contains only content words
and no multi-word phrases, actual opinions of-
ten comprise an entire phrase, e.g., ?is really
willing? or ?in my opinion?. As a result, we
mark as an opinion the entire chunk that con-
tains an opinion word. This allows each token
in the chunk to act as an opinion word for fea-
ture encoding.
After syntactic chunking and opinion word propa-
gation, we create the following dependency tree fea-
tures for each token xi:
? the grammatical role of its chunk
? the grammatical role of xi?1?s chunk
? whether the parent chunk includes an opinion
word
? whether xi?s chunk is in an argument position
with respect to the parent chunk
? whether xi represents a constituent boundary
Semantic class features We use 7 binary fea-
tures to encode the semantic class of each word
xi: authority, government, human, media,
organization or company, proper name,
and other. The other class captures 13 seman-
tic classes that cannot be sources, such as vehicle
and time.
Semantic class information is derived from named
entity and semantic class labels assigned to xi by the
Sundance shallow parser (Riloff, 2004). Sundance
uses named entity recognition rules to label noun
phrases as belonging to named entity classes, and
assigns semantic tags to individual words based on
a semantic dictionary. Table 1 shows the hierarchy
that Sundance uses for semantic classes associated
with opinion sources. Sundance is also used to rec-
ognize and instantiate the source extraction patterns
PROPER NAMEAUTHORITY LOCATION
CITY
COUNTRY
PLANET
PROVINCE
PERSON NAME
PERSON DESC
NATIONALITY
TITLE
COMPANY
GOVERNMENT
MEDIA
ORGANIZATION
HUMAN
SOURCE
Figure 1: The semantic hierarchy for opinion
sources
that are learned by AutoSlog-SE, which is described
in the next section.
4 Semantic Tagging via Extraction
Patterns
We also learn patterns to extract opinion sources us-
ing a statistical adaptation of the AutoSlog IE learn-
ing algorithm. AutoSlog (Riloff, 1996a) is a super-
vised extraction pattern learner that takes a train-
ing corpus of texts and their associated answer keys
as input. A set of heuristics looks at the context
surrounding each answer and proposes a lexico-
syntactic pattern to extract that answer from the text.
The heuristics are not perfect, however, so the result-
ing set of patterns needs to be manually reviewed by
a person.
In order to build a fully automatic system that
does not depend on manual review, we combined
AutoSlog?s heuristics with statistics from the an-
notated training data to create a fully automatic
supervised learner. We will refer to this learner
as AutoSlog-SE (Statistically Enhanced variation
of AutoSlog). AutoSlog-SE?s learning process has
three steps:
Step 1: AutoSlog?s heuristics are applied to every
noun phrase (NP) in the training corpus. This
generates a set of extraction patterns that, col-
lectively, can extract every NP in the training
corpus.
Step 2: The learned patterns are augmented with
selectional restrictions that semantically con-
strain the types of noun phrases that are legiti-
mate extractions for opinion sources. We used
358
the semantic classes shown in Figure 1 as se-
lectional restrictions.
Step 3: The patterns are applied to the training cor-
pus and statistics are gathered about their ex-
tractions. We count the number of extrac-
tions that match annotations in the corpus (cor-
rect extractions) and the number of extractions
that do not match annotations (incorrect extrac-
tions). These counts are then used to estimate
the probability that the pattern will extract an
opinion source in new texts:
P (source | patterni) =
correct sources
correct sources + incorrect sources
This learning process generates a set of extraction
patterns coupled with probabilities. In the next sec-
tion, we explain how these extraction patterns are
represented as features in the CRF model.
5 Extraction Pattern Features for the CRF
The extraction patterns provide two kinds of infor-
mation. SourcePatt indicates whether a word
activates any source extraction pattern. For exam-
ple, the word ?complained? activates the pattern
?<subj> complained? because it anchors the ex-
pression. SourceExtr indicates whether a word is
extracted by any source pattern. For example, in the
sentence ?President Jacques Chirac frequently com-
plained about France?s economy?, the words ?Pres-
ident?, ?Jacques?, and ?Chirac? would all be ex-
tracted by the ?<subj> complained? pattern.
Each extraction pattern has frequency and prob-
ability values produced by AutoSlog-SE, hence we
create four IE pattern-based features for each token
xi: SourcePatt-Freq, SourceExtr-Freq,
SourcePatt-Prob, and SourceExtr-Prob,
where the frequency values are divided into three
ranges: {0, 1, 2+} and the probability values are di-
vided into five ranges of equal size.
6 Experiments
We used the Multi-Perspective Question Answering
(MPQA) corpus4 for our experiments. This corpus
4The MPQA corpus can be freely obtained at
http://nrrc.mitre.org/NRRC/publications.htm.
consists of 535 documents that have been manu-
ally annotated with opinion-related information in-
cluding direct and indirect sources. We used 135
documents as a tuning set for model development
and feature engineering, and used the remaining 400
documents for evaluation, performing 10-fold cross
validation. These texts are English language ver-
sions of articles that come from many countries and
cover many topics.5
We evaluate performance using 3 measures: over-
lap match (OL), head match (HM), and exact match
(EM). OL is a lenient measure that considers an ex-
traction to be correct if it overlaps with any of the an-
notated words. HM is a more conservative measure
that considers an extraction to be correct if its head
matches the head of the annotated source. We report
these somewhat loose measures because the annota-
tors vary in where they place the exact boundaries
of a source. EM is the strictest measure that requires
an exact match between the extracted words and the
annotated words. We use three evaluation metrics:
recall, precision, and F-measure with recall and pre-
cision equally weighted.
6.1 Baselines
We developed three baseline systems to assess the
difficulty of our task. Baseline-1 labels as sources
all phrases that belong to the semantic categories
authority, government, human, media,
organization or company, proper name.
Table 1 shows that the precision is poor, suggest-
ing that the third condition described in Section 3.1
(opinion recognition) does play an important role in
source identification. The recall is much higher but
still limited due to sources that fall outside of the se-
mantic categories or are not recognized as belong-
ing to these categories. Baseline-2 labels a noun
phrase as a source if any of the following are true:
(1) the NP is the subject of a verb phrase containing
an opinion word, (2) the NP follows ?according to?,
(3) the NP contains a possessive and is preceded by
an opinion word, or (4) the NP follows ?by? and at-
taches to an opinion word. Baseline-2?s heuristics
are designed to address the first and the third condi-
tions in Section 3.1. Table 1 shows that Baseline-2
is substantially better than Baseline-1. Baseline-3
5This data was obtained from the Foreign Broadcast Infor-
mation Service (FBIS), a U.S. government agency.
359
Recall Prec F1
OL 77.3 28.8 42.0
Baseline-1 HM 71.4 28.6 40.8
EM 65.4 20.9 31.7
OL 62.4 60.5 61.4
Baseline-2 HM 59.7 58.2 58.9
EM 50.8 48.9 49.8
OL 49.9 72.6 59.2
Baseline-3 HM 47.4 72.5 57.3
EM 44.3 58.2 50.3
OL 48.5 81.3 60.8
Extraction Patterns HM 46.9 78.5 58.7
EM 41.9 70.2 52.5
CRF: OL 56.1 81.0 66.3
basic features HM 55.1 79.2 65.0
EM 50.0 72.4 59.2
CRF: OL 59.1 82.4 68.9
basic + IE pattern HM 58.1 80.5 67.5
features EM 52.5 73.3 61.2
CRF-FI: OL 57.7 80.7 67.3
basic features HM 56.8 78.8 66.0
EM 51.7 72.4 60.3
CRF-FI: OL 60.6 81.2 69.4
basic + IE pattern HM 59.5 79.3 68.0
features EM 54.1 72.7 62.0
Table 1: Source identification performance table
labels a noun phrase as a source if it satisfies both
Baseline-1 and Baseline-2?s conditions (this should
satisfy all three conditions described in Section 3.1).
As shown in Table 1, the precision of this approach
is the best of the three baselines, but the recall is the
lowest.
6.2 Extraction Pattern Experiment
We evaluated the performance of the learned extrac-
tion patterns on the source identification task. The
learned patterns were applied to the test data and
the extracted sources were scored against the manual
annotations.6 Table 1 shows that the extraction pat-
terns produced lower recall than the baselines, but
with considerably higher precision. These results
show that the extraction patterns alone can identify
6These results were obtained using the patterns that had a
probability > .50 and frequency > 1.
nearly half of the opinion sources with good accu-
racy.
6.3 CRF Experiments
We developed our CRF model using the MALLET
code from McCallum (2002). For training, we used
a Gaussian prior of 0.25, selected based on the tun-
ing data. We evaluate the CRF using the basic fea-
tures from Section 3, both with and without the IE
pattern features from Section 5. Table 1 shows that
the CRF with basic features outperforms all of the
baselines as well as the extraction patterns, achiev-
ing an F-measure of 66.3 using the OL measure,
65.0 using the HM measure, and 59.2 using the
EM measure. Adding the IE pattern features fur-
ther increases performance, boosting recall by about
3 points for all of the measures and slightly increas-
ing precision as well.
CRF with feature induction. One limitation of
log-linear function models like CRFs is that they
cannot form a decision boundary from conjunctions
of existing features, unless conjunctions are explic-
itly given as part of the feature vector. For the
task of identifying opinion sources, we observed
that the model could benefit from conjunctive fea-
tures. For instance, instead of using two separate
features, HUMAN and PARENT-CHUNK-INCLUDES-
OPINION-EXPRESSION, the conjunction of the two
is more informative.
For this reason, we applied the CRF feature in-
duction approach introduced by McCallum (2003).
As shown in Table 1, where CRF-FI stands for the
CRF model with feature induction, we see consis-
tent improvements by automatically generating con-
junctive features. The final system, which com-
bines the basic features, the IE pattern features,
and feature induction achieves an F-measure of 69.4
(recall=60.6%, precision=81.2%) for the OL mea-
sure, an F-measure of 68.0 (recall=59.5%, preci-
sion=79.3%) for the HM measure, and an F-measure
of 62.0 (recall=54.1%, precision=72.7%) for the EM
measure.
6.4 Error Analysis
An analysis of the errors indicated some common
mistakes:
? Some errors resulted from error propagation in
360
our subsystems. Errors from the sentence bound-
ary detector in GATE (Cunningham et al, 2002)
were especially problematic because they caused
the Collins parser to fail, resulting in no depen-
dency tree information.
? Some errors were due to complex and unusual
sentence structure, which our rather simple fea-
ture encoding for CRF could not capture well.
? Some errors were due to the limited coverage of
the opinion lexicon. We failed to recognize some
cases when idiomatic or vague expressions were
used to express opinions.
Below are some examples of errors that we found
interesting. Doubly underlined phrases indicate in-
correctly extracted sources (either false positives
or false negatives). Opinion words are singly
underlined.
False positives:
(1) Actually, these three countries do have one common
denominator, i.e., that their values and policies do not
agree with those of the United States and none of them
are on good terms with the United States.
(2) Perhaps this is why Fidel Castro has not spoken out
against what might go on in Guantanamo.
In (1), ?their values and policies? seems like a rea-
sonable phrase to extract, but the annotation does not
mark this as a source, perhaps because it is some-
what abstract. In (2), ?spoken out? is negated, which
means that the verb phrase does not bear an opinion,
but our system failed to recognize the negation.
False negatives:
(3) And for this reason, too, they have a moral duty to
speak out, as Swedish Foreign Minister Anna Lindh,
among others, did yesterday.
(4) In particular, Iran and Iraq are at loggerheads with
each other to this day.
Example (3) involves a complex sentence structure
that our system could not deal with. (4) involves an
uncommon opinion expression that our system did
not recognize.
7 Related Work
To our knowledge, our research is the first to auto-
matically identify opinion sources using the MPQA
opinion annotation scheme. The most closely re-
lated work on opinion analysis is Bethard et al
(2004), who use machine learning techniques to
identify propositional opinions and their holders
(sources). However, their work is more limited
in scope than ours in several ways. Their work
only addresses propositional opinions, which are
?localized in the propositional argument? of cer-
tain verbs such as ?believe? or ?realize?. In con-
trast, our work aims to find sources for all opinions,
emotions, and sentiments, including those that are
not related to a verb at all. Furthermore, Berthard
et al?s task definition only requires the identifica-
tion of direct sources, while our task requires the
identification of both direct and indirect sources.
Bethard et al evaluate their system on manually
annotated FrameNet (Baker et al, 1998) and Prop-
Bank (Palmer et al, 2005) sentences and achieve
48% recall with 57% precision.
Our IE pattern learner can be viewed as a cross
between AutoSlog (Riloff, 1996a) and AutoSlog-
TS (Riloff, 1996b). AutoSlog is a supervised learner
that requires annotated training data but does not
compute statistics. AutoSlog-TS is a weakly super-
vised learner that does not require annotated data
but generates coarse statistics that measure each pat-
tern?s correlation with relevant and irrelevant docu-
ments. Consequently, the patterns learned by both
AutoSlog and AutoSlog-TS need to be manually re-
viewed by a person to achieve good accuracy. In
contrast, our IE learner, AutoSlog-SE, computes
statistics directly from the annotated training data,
creating a fully automatic variation of AutoSlog.
8 Conclusion
We have described a hybrid approach to the problem
of extracting sources of opinions in text. We cast
this problem as an information extraction task, using
both CRFs and extraction patterns. Our research is
the first to identify both direct and indirect sources
for all types of opinions, emotions, and sentiments.
Directions for future work include trying to in-
crease recall by identifying relationships between
opinions and sources that cross sentence boundaries,
and relationships between multiple opinion expres-
sions by the same source. For example, the fact that
a coreferring noun phrase was marked as a source
in one sentence could be a useful clue for extracting
the source from another sentence. The probability or
the strength of an opinion expression may also play
a useful role in encouraging or suppressing source
extraction.
361
9 Acknowledgments
We thank the reviewers for their many helpful com-
ments, and the Cornell NLP group for their advice
and suggestions for improvement. This work was
supported by the Advanced Research and Develop-
ment Activity (ARDA), by NSF Grants IIS-0208028
and IIS-0208985, and by the Xerox Foundation.
References
C. Baker, C. Fillmore & J. Lowe. 1998. The Berkeley
FrameNet Project. In Proceedings of the COLING-ACL.
S. Bethard, H. Yu, A. Thornton, V. Hativassiloglou & D. Juraf-
sky. 2004. Automatic extraction of opinion propositions and
their holders. In Proceedings of AAAI Spring Symposium on
Exploring Attitude and Affect in Text.
D. Bikel, S. Miller, R. Schwartz & R. Weischedel. 1997.
Nymble: A High-Performance Learning Name-Finder. In
Proceedings of the Fifth Conference on Applied Natural Lan-
guage Processing.
E. Breck & C. Cardie. 2004. Playing the Telephone Game:
Determining the Hierarchical Structure of Perspective and
Speech Expressions. In Proceedings of 20th International
Conference on Computational Linguistics.
C. Cardie, J. Wiebe, T. Wilson & D. Litman. 2004. Low-
level annotations and summary representations of opinions
for multiperspective QA. In New Directions in Question An-
swering. AAAI Press/MIT Press.
M. Collins. 1999. Head-driven Statistical Models for Natural
Language Parsing. Ph.D. thesis, University of Pennsylvania.
H. Cunningham, D. Maynard, K. Bontcheva & V. Tablan. 2002.
GATE: A Framework and Graphical Development Environ-
ment for Robust NLP Tools and Applications. In Proceed-
ings of the 40th Anniversary Meeting of the Association for
Computational Linguistics.
S. Das & M. Chen. 2001. Yahoo for amazon: Extracting market
sentiment from stock message boards. In Proceedings of the
8th Asia Pacific Finance Association Annual Conference.
K. Dave, S. Lawrence & D. Pennock. 2003. Mining the peanut
gallery: Opinion extraction and semantic classification of
product reviews. In International World Wide Web Confer-
ence.
J. Lafferty, A. K. McCallum & F. Pereira. 2001. Conditional
Random Fields: Probabilistic Models for Segmenting and
Labeling Sequence Data. In Proceedings of 18th Interna-
tional Conference on Machine Learning.
B. Levin. 1993. English Verb Classes and Alternations: A
Preliminary Investigation. University of Chicago Press.
A. K. McCallum. 2002. MALLET: A Machine Learning for
Language Toolkit. http://mallet.cs.umass.edu.
A. K. McCallum. 2003. Efficiently Inducing Features of Con-
ditional Random Fields. In Conference on Uncertainty in
Artificial Intelligence.
A. K. McCallum & W. Li. 2003. Early Results for Named
Entity Recognition with Conditional Random Fields, Feature
Induction and Web-Enhanced Lexicons. In Conference on
Natural Language Learning.
S. Morinaga, K. Yamanishi, K. Tateishi & T. Fukushima 2002.
Mining Product Reputations on the Web. In Proceedings of
the 8th Internatinal Conference on Knowledge Discover and
Data Mining.
M. Palmer, D. Gildea & P. Kingsbury. 2005. The Proposition
Bank: An Annotated Corpus of Semantic Roles. In Compu-
tational Linguistics 31.
B. Pang, L. Lee & S. Vaithyanathan. 2002. Thumbs up? sen-
timent classification using machine learning techniques. In
Proceedings of the 2002 Conference on Empirical Methods
in Natural Language Processing.
B. Pang & L. Lee. 2004. A sentimental education: Sentiment
analysis using subjectivity summarization based on mini-
mum cuts. In Proceedings of the 42nd Annual Meeting of
the Association for Computational Linguistics.
L. A. Ramshaw & M. P. Marcus. 1995. Nymble: A High-
Performance Learning Name-Finder. In Proceedings of the
3rd Workshop on Very Large Corpora.
E. Riloff. 1996a. An Empirical Study of Automated Dictionary
Construction for Information Extraction in Three Domains.
In Artificial Intelligence, Vol. 85.
E. Riloff. 1996b. Automatically Generating Extraction Patterns
from Untagged Text. In Proceedings of the 13th National
Conference on Artificial Intelligence.
E. Riloff & J. Wiebe. 2003. Learning extraction patterns for
subjective expressions. In Proceesings of 2003 Conference
on Empirical Methods in Natural Language Processing.
E. Riloff & W. Phillips. 2004. An Introduction to the Sun-
dance and AutoSlog Systems Technical Report UUCS-04-
015, School of Computing, University of Utah.
S. Sarawagi & W. W. Cohen. 2004. Semi-Markov Condi-
tional Random Fields for Information Extraction 18th An-
nual Conference on Neural Information Processing Systems.
P. Turney. 2002. Thumbs up or thumbs down? semantic orien-
tation applied to unsupervised classification of reviews. In
Proceedings of the 40th Annual Meeting of the Association
for Computational Linguistics.
T. Wilson, J. Wiebe & R. Hwa. 2004. Just how mad are you?
finding strong and weak opinion clauses. In Proceedings of
the 9th National Conference on Artificial Intelligence.
T. Wilson, P. Hoffmann, S. Somasundaran, J. Kessler, J. Wiebe,
Y. Choi, C. Cardie, E. Riloff & S. Patwardhan. 2005. Opin-
ionFinder: A system for subjectivity analysis. Demonstra-
tion Description in Conference on Empirical Methods in
Natural Language Processing.
J. Yi, T. Nasukawa, R. Bunescu & W. Niblack. 2003. Sentiment
Analyzer: Extracting Sentiments about a Given Topic using
Natural Language Processing Techniques. In Proceedings of
the 3rd IEEE International Conference on Data Mining.
H. Yu & V. Hatzivassiloglou. 2003. Towards answering opin-
ion questions: Separating facts from opinions and identify-
ing the polarity of opinion sentences. In Proceedings of the
Conference on Empirical Methods in Natural Language Pro-
cessing.
J. Wiebe, E. Breck, C. Buckley, C. Cardie, P. Davis, B. Fraser,
D. Litman, D. Pierce, E. Riloff & T. Wilson. 2002. NRRC
Summer Workshop on Multiple-Perspective Question An-
swering: Final Report.
J. Wiebe & E. Riloff. 2005. Creating subjective and objective
sentence classifiers from unannotated texts. Sixth Interna-
tional Conference on Intelligent Text Processing and Com-
putational Linguistics.
J. Wiebe, T. Wilson & C. Cardie. 2005. Annotating expressions
of opinions and emotions in language. Language Resources
and Evaluation, 1(2).
362
Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural Language
Processing (HLT/EMNLP), pages 539?546, Vancouver, October 2005. c?2005 Association for Computational Linguistics
Optimizing to Arbitrary NLP Metrics using Ensemble Selection
Art Munson, Claire Cardie, Rich Caruana
Department of Computer Science
Cornell University
Ithaca, NY 14850
{mmunson, cardie, caruana}@cs.cornell.edu
Abstract
While there have been many successful applica-
tions of machine learning methods to tasks in NLP,
learning algorithms are not typically designed to
optimize NLP performance metrics. This paper
evaluates an ensemble selection framework de-
signed to optimize arbitrary metrics and automate
the process of algorithm selection and parameter
tuning. We report the results of experiments that in-
stantiate the framework for three NLP tasks, using
six learning algorithms, a wide variety of parame-
terizations, and 15 performance metrics. Based on
our results, we make recommendations for subse-
quent machine-learning-based research for natural
language learning.
1 Introduction
Among the most successful natural language learn-
ing techniques for a wide variety of linguistic phe-
nomena are supervised inductive learning algo-
rithms for classification. Because of their capa-
bilities for accurate, robust, and efficient linguistic
knowledge acquisition, they have been employed in
many natural language processing (NLP) tasks.
Unfortunately, supervised classification algo-
rithms are typically designed to optimize accuracy
(e.g. decision trees) or mean squared error (e.g. neu-
ral networks). For many NLP tasks, however, these
standard performance measures are inappropriate.
For example, NLP data can be highly skewed in its
distribution of positive and negative examples. In
these situations, another metric (perhaps F-measure
or a task-specific measure) that focuses on the per-
formance of the minority cases is more appropriate.
Indeed, as the NLP field matures more consideration
will be given to evaluating the performance of NLP
components in context (e.g. Is the system easy to use
by end users? Does the component respect user pref-
erences? How well does the entire system solve the
specific problem?), leading to new and complicated
metrics. Optimizing machine learning algorithms to
arbitrary performance metrics, however, is not easily
done.
To exacerbate matters, the metric of interest might
change depending on how the natural language
learning (NLL) component is employed. Some ap-
plications might need components with high re-
call, for example; others, high precision or high F-
measure or low root mean squared error. To obtain
good results w.r.t. the new metric, however, a dif-
ferent parameterization or different algorithm alto-
gether might be called for, requiring re-training the
classifier(s) from scratch.
Caruana et al (2004) have recently proposed en-
semble selection as a technique for building an en-
semble of classifiers that is optimized to an arbitrary
performance metric. The approach trains a large
number of classifiers using multiple algorithms and
parameter settings, with the idea that at least some
of the classifiers will perform well on any given per-
formance measure. The best set of classifiers, w.r.t.
the target metric, is then greedily selected. (Select-
ing a set of size 1 is equivalent to parameter and
algorithm tuning.) Like other ensemble learning
methods (e.g. bagging (Breiman, 1996) and boost-
ing (Freund and Schapire, 1996)), ensemble selec-
tion has been shown to exhibit reliably better perfor-
mance than any of the contributing classifiers for a
number of learning tasks.
In addition, ensemble selection provides an ancil-
539
lary benefit: no human expertise is required in se-
lecting an appropriate machine learning algorithm or
in choosing suitable parameter settings to get good
performance. This is particularly attractive for the
NLP community where researchers often rely on the
same one or two algorithms and use default param-
eter settings for simplicity. Ensemble selection is a
tool usable by non-experts to find good classifiers
optimized to task-specific metrics.
This paper evaluates the utility of the ensemble se-
lection framework for NLL. We use three NLP tasks
for the empirical evaluation: noun phrase corefer-
ence resolution and two problems from sentiment
analysis ? identifying private state frames and the
hierarchy among them. The evaluation employs six
learning algorithms, a wide selection of parameteri-
zations, 8 standard metrics, and 7 task-specific met-
rics. Because ensemble selection subsumes param-
eter and algorithm selection, we also measure the
impact of parameter and algorithm tuning.
Perhaps not surprisingly, we find first that no one
algorithm or parameter configuration performs the
best across all tasks or across all metrics. In ad-
dition, an algorithm?s ?tuned? performance (i.e. the
performance after tuning parameter settings) almost
universally matches or exceeds the algorithm?s de-
fault performance (i.e. when using default parame-
ter settings). Out of 154 total cases, the tuned clas-
sifier outperforms the default classifier 114 times,
matches performance 28 times, and underperforms
12 times. Together, these results indicate the impor-
tance of algorithm and parameter selection for com-
parative empirical NLL studies. In particular, our
results show the danger of relying on the same one
or two algorithms for all tasks. These results cast
doubt on conclusions regarding differences in algo-
rithm performance for NLL experiments that give
inadequate attention to parameter selection.
The results of our experiments that use ensem-
ble selection to optimize the ensemble to arbitrary
metrics are mixed. We see reliable improvements
in performance across almost all of the metrics for
only two of the three tasks; for the other data set,
ensemble selection tends to hurt performance (al-
though losses are very small). Perhaps more impor-
tantly for our purposes, we find that ensemble se-
lection provides small, but consistent gains in per-
formance when considering only the more complex,
task-specific metrics ? metrics that learning algo-
rithms would find difficult to optimize.
The rest of the paper is organized as follows. Sec-
tion 2 describes the general learning framework and
provides an overview of ensemble selection. We
present the particular instantiation of the framework
employed in our experiments in Section 3. Section 4
describes the three NLP tasks. Experimental results
are given in Section 5. Related work and conclu-
sions follow (sections 6 and 7).
2 Ensemble Selection Framework
2.1 Terminology
We use the term model to refer to a classifier pro-
duced by some learning algorithm using some par-
ticular set of parameters. A model?s configuration
is simply the algorithm and parameter settings used
to create the classifier. A model family is the set of
models made by varying the parameters for one ma-
chine learning algorithm. Finally, a model library
is a collection of models trained for a given task.
2.2 Framework
Abstractly, the framework is the following:
1. Select a variety of learning algorithms.
2. For each algorithm, choose a wide range of set-
tings for the algorithm?s parameters.
3. Divide data into training, tuning, and test sets.
4. Build model library.
5. Select target metrics appropriate to problem.
6. Tune parameter settings and/or run ensemble
selection algorithm for target metrics.
Building the model library consists of (a) using
the training data to train models for all the learning
algorithms under all desired combinations of param-
eter settings, and (b) applying each model to the tun-
ing and test set instances and storing the predictions
for use in step (6). Note that models are placed in the
library regardless of performance, even though some
models have very bad performance. Intuitively, this
is because there is no way to know a priori which
models will perform well on a given metric. Note
that producing the base models is fully automatic
and requires no expert tuning.
Parameter Tuning: The goal of parameter tun-
ing is to identify the best model for the task accord-
ing to each target metric. Parameter tuning is han-
dled in the standard way: for each metric, we select
540
the model from the model library with the highest
performance on the tuning data and report its perfor-
mance on the test data.
Ensemble Selection Algorithm: The ensemble
selection algorithm (Caruana et al, 2004) ignores
model-specific details by only using the predictions
made by the models: the ensemble makes predic-
tions by averaging the predictions of its constituents.
Advantageously, this only requires that predictions
made by different models fall in the same range, and
that they can be averaged in a meaningful way. Oth-
erwise, models can take any form, including other
ensemble methods (e.g. bagging or boosting). Con-
ceptually, ensemble selection builds on top of the
models in the library and uses their performance as
a starting point from which to improve.
The basic ensemble selection algorithm is:
a. Start with an empty ensemble.
b. Add the model that results in the best perfor-
mance for the current ensemble with respect to
the tuning data and the target metric.
c. Repeat (b) for a large, fixed number of itera-
tions.
d. The final ensemble is the ensemble from the
best performing iteration on the tuning data for
the target metric.
To prevent the algorithm from overfitting the tun-
ing data we use two enhancements given by Caruana
et al (2004). First, in step (b) the same model can be
selected multiple times (i.e. selection with replace-
ment). Second, the ensemble is initialized with the
top N models (again, with respect to the target met-
ric on the tuning data). N is picked automatically
such that removing or adding a model decreases per-
formance on the tuning data.1
The main advantage to this framework is its
reusability. After an instantiation of the framework
exists, it is straightforward to apply it to multiple
NLL tasks and to add additional metrics. Steps (1)
and (2) only need to be done once, regardless of the
number of tasks and metrics explored. Steps (3)-(5)
need only be done once per NLL task. Importantly,
the model library is created once for each task (i.e.
each model configuration is only trained once) re-
gardless of the number (or addition) of performance
1We also experimented with the bagging improvement de-
scribed by Caruana et al (2004). In our experiments using bag-
ging hurt the performance of ensemble selection.
metrics. Finally, finding a classifier or ensemble op-
timized to a new metric (step (6)) does not require
re-building the model library and is very fast com-
pared to training the classifiers?it only requires av-
eraging the stored predictions. For example, training
the model library for our smallest data set took mul-
tiple days; ensemble selection built optimized en-
sembles for each metric in a few minutes.
3 Framework Instantiation
In this section we describe our instantiation of the
ensemble selection framework.
Algorithms: We use bagged decision trees
(Breiman, 1996), boosted decision stumps (Fre-
und and Schapire, 1996), k-nearest neighbor, a rule
learner, and support vector machines (SVM?s). We
use the following implementations of these algo-
rithms, respectively: IND decision tree package
(Buntine, 1993); WEKA toolkit (Witten and Frank,
2000); TiMBL (Daelemans et al, 2000); RIPPER
(Cohen, 1995); and SVMlight (Joachims, 1999). Ad-
ditionally, we use logistic regression (LR) for coref-
erence resolution because an implementation using
the MALLET (McCallum, 2002) toolkit was readily
available for the task. The predictions from all algo-
rithms are scaled to the range [0, 1] with values close
to 1 indicating positive predictions and values close
to 0 indicating negative predictions.2
Parameter Settings: Table 1 lists the parame-
ter settings we vary for each algorithm. Additional
domain-specific parameters are also varied for coref-
erence resolution models (see Section 4.1). The
model libraries contain models corresponding to the
cross product of the various parameter settings for a
given algorithm.
Standard Performance Metrics: We evaluate
the framework with 8 metrics: accuracy (ACC),
average precision (APR), break even point (BEP),
F-measure (F1), mean cross entropy (MXE), root
mean squared error (RMS), area under the ROC
curve (ROC), and SAR. Caruana et al (2004) de-
fine SAR as SAR = ACC+ROC+(1?RMS)3 . We also
evaluate the effects of model selection with task-
specific metrics. These are described in Section 4.
Our F-measure places equal emphasis on precision
2We follow Caruana et al (2004) in using Platt (2000) scal-
ing to convert the SVM predictions from the range (??,?) to
the required [0, 1] by fitting them to a sigmoid.
541
Algorithm Parameter Values
Bagged Trees ? tree type bayes, c4, cart, cart0, id3, mml, smml
# bags 1, 5, 10, 25
Boosted
Stumps
# iterations 2, 4, 8, . . . , 256, . . . 1024, 2048
LR ? gaussian gamma 0.001, 0.005, 0.01, 0.05, 0.1, 0.5, 1, 5, 10, 50
K-NN k 1, 3, 5
search algorithm ib1, igtree
similarity metric overlap, modified value difference
feature weighting gain ratio, information gain, chi-squared, shared variance
Rule Learner class learning order unordered, pos first, neg first, heuristic determined order
loss ratio 0.5, 1, 1.5, 2, 3, 4
SVM margin tradeoff* 10?7, 10?6, . . . , 10?2, 10?1, . . . , 102
kernel linear, rbf
rbf gamma parm 0.001, 0.005, 0.01, 0.05, 0.1, 0.5, 1, 2
Table 1: Summary of model configurations used in experiments. The default settings for each algorithm are in bold.
? Bagged trees are not used for identifying PSF?s since the IND package does not support features with more than 255 values.
Also, for coreference resolution the number of bags is not varied and is always 25. ? LR is only used for coreference resolution.
* SVMlight determines the default margin tradeoff based on data properties. We calculate this value for each data set and use the
closest setting among our configurations.
and recall (i.e. ? = 1). Note that precision and re-
call are calculated with respect to the positive class.
Ensemble Selection: For the sentiment analysis
tasks, ensemble selection iterates 150 times; for the
coreference task, the algorithm iterates 200 times.
This should be enough iterations, given that the
model libraries contain 202, 173, and 338 mod-
els. Because computing the MUC-F1 and BCUBED
metrics (see Section 4.1) is expensive, ensemble se-
lection only iterates 50 times for these metrics.
4 Tasks
Because of space issues, we necessarily provide
only brief descriptions of each NLL task. Readers
are referred to the cited papers to obtain detailed de-
scriptions.
4.1 Noun Phrase Coreference Resolution
The goal for a standard noun phrase coreference res-
olution system is to identify the noun phrases in a
document and determine which of them refer to the
same entity. Entities can be people, places, things,
etc. The resulting partitioning of noun phrases cre-
ates reference chains with one chain per entity.
We use the same problem formulation as Soon et
al. (2001) and Ng and Cardie (2002) ? a combi-
nation of classification and clustering. Briefly, ev-
ery noun phrase is paired with all preceding noun
phrases, creating multiple pairs. For the training
data, the pairs are labeled as coreferent or not. A
binary classifier is trained to predict the pair labels.
During classification, the predicted labels are used
to form clusters. Two noun phrases A and B share
a cluster if they are either predicted as coreferent by
the classifier or if they are transitively predicted as
coreferent through one or more other noun phrases.
Instance selection (Soon et al, 2001; Ng, 2004) is
used to increase the percentage of positive instances
in the training set.3
We use the learning features described by Ng
and Cardie (2002). All learning algorithms are
trained with the full set of features. Additionally,
the rule learner, SVM, and LR are also trained with
a hand-selected subset of the features that Ng and
Cardie (2002) find to outperform the full feature set.
Essentially this is an additional parameter to set for
the learning task.
Special Metrics: Rather than focusing on per-
formance at the pairwise coreference classification
level, performance for this task is typically reported
using either the MUC metric (Vilain et al, 1995)
or the BCUBED metric (Bagga and Baldwin, 1998).
Both of these metrics measure the degree that pre-
dicted coreference chains agree with an answer key.
In particular they measure the chain-level precision
and recall (and the corresponding F-measure). We
abbreviate these metrics MUC-F1, and B3F1.
Data Set: For our experiments we use the MUC-
6 corpus, which contains 60 documents annotated
with coreference information. The training, tuning,
and test sets consist of documents 1-20, 21-30, and
3Soon-1 instance selection is used for all algorithms; we
also use soon-2 (Ng, 2004) instance selection for the rule
learner.
542
31-60, respectively.
4.2 Identifying Private State Frames
NLP research has recently started looking at how
to detect and understand subjectivity in discourse.
A key step in this direction is automatically identi-
fying phrases that express subjectivity. In this set-
ting, subjectivity is defined to include implicit and
explicit opinions, internal thoughts and emotions,
and bias introduced through second-hand reporting.
Phrases expressing any of these are called private
state frames, which we will abbreviate as PSF.
We build directly on experiments done by Wiebe
et al (2003). The task is to learn to identify explicit
single-word PSF?s in context. One learning instance
is created for every word in the corpus. Classifica-
tion labels each instance as a PSF or not. We use the
same features as Wiebe et al
Special Metrics: Because the data is highly
skewed (2% positive instances), performance mea-
sures that focus on how well the minority class is
learned are of primary interest. The F-measure de-
fined in Section 3 is a natural choice. We also eval-
uate performance using geometric accuracy, defined
as GACC =
?
posacc? negacc where posacc and
negacc are the accuracy with respect to the positive
and negative instances (Kubat and Matwin, 1997).
Conceivably, an automatic PSF extractor with
high precision and mediocre recall could be used
to automate the annotation process. For this reason
we measure the performance with an unbalanced F-
measure that emphasizes precision. Specifically, we
try ? = 0.5 (F0.5) and ? = 0.2 (F0.2).
Data Set: We use 400 documents from the
MPQA corpus (2002), a collection of news stories
manually annotated with PSF information. The 400
documents are randomly split to get 320 training, 40
tuning, and 40 testing documents.
4.3 Determining PSF Hierarchy
The third task is taken from Breck and Cardie
(2004). Explicit PSF?s each have a source that cor-
responds to the person or entity expressing the sub-
jectivity. In the presence of second-hand reporting,
sources are often nested. This has the effect of filter-
ing subjectivity through a chain of sources.
Given sentences annotated with PSF information
(i.e. which spans are PSF?s), the task is to discover
the hierarchy among the PSF?s that corresponds to
the nesting of their respective sources. From each
sentence, multiple instances are created by pair-
ing every PSF with every other PSF in the sen-
tence.4 Let (PSFparent, PSFtarget) denote one of
these instances. The classification task is to decide
if PSFparent is the parent of PSFtarget in the hi-
erarchy and to associate a confidence with that pre-
diction. The complete hierarchy can easily be con-
structed from the predictions by choosing for each
PSF its most confidently predicted parent.
Special Metrics: Breck and Cardie (2004) mea-
sure task performance with three metrics. The first
is the accuracy of the predictions over the instances.
The second is a derivative of a measure used to score
dependency parses. Essentially, a sentence?s score is
the fraction of parent links correctly identified. The
score for a set of sentences is the average of the indi-
vidual sentence scores. We refer to this measure as
average sentence accuracy (SENTACC). The third
measure is the percentage of sentences whose hier-
archical structures are perfectly determined (PERF-
SENT).
Data Set: We use the same data set and fea-
tures as Breck and Cardie (2004). The annotated
sentences from 469 documents in the MPQA Cor-
pus (MPQA Corpus, 2002) are randomly split into
training (80%), tuning (10%), and test (10%) sets.
5 Experiments and Results
We evaluate the potential benefits of the ensemble
selection framework with two experiments. The first
experiment measures the performance improvement
yielded by parameter tuning and finds the best per-
forming algorithm. The second experiment mea-
sures the performance improvement from ensemble
selection.
Performance improvements are measured in
terms of performance gain. Let a and b be the mea-
sured performances for two models A and B on
some metric. A?s gain over B is simply a ? b. A
performed worse than B if the gain is negative.5
4Sentences containing fewer than two PSF?s are discarded
and not used.
5MXE and RMS have inverted scales where the best perfor-
mance is 0. Gain for these metrics equals (1? a)? (1? b) so
that positive gains are always good. Similarly, where raw MXE
and RMS scores are reported we show 1? score.
543
Metric BAG Parm? BST Parm? LR Parm? KNN Parm? RULE Parm? SVM Parm? Avg?
ACC 0.9861 -0.0000 0.9861 -0.0001 0.9849 0.0006 0.9724 0.0131 0.9840 0.0023 0.9859 -0.0001 0.0026
APR 0.5373 0.0000 0.5475 0.0000 0.3195 -0.0004 0.1917 0.2843 0.2491 0.1127 0.5046 0.0323 0.0715
BEP 0.6010 0.0000 0.5577 0.0193 0.3747 -0.0022 0.3243 0.2057 0.3771 0.1862 0.5965 0.0045 0.0689
F1 0.5231 0.0664 0.3881 0.0000 0.4600 0.0087 0.4105 0.1383 0.4453 0.1407 0.3527 0.0571 0.0685
MXE 0.9433 0.0082 0.9373 0.0000 0.5400 0.1828 0.4953 0.3734 0.9128 0.0222 0.9366 0.0077 0.0990
RMS 0.8925 0.0041 0.8882 0.0000 0.6288 0.1278 0.8334 0.0559 0.8756 0.0097 0.8859 0.0047 0.0337
ROC 0.9258 0.0158 0.9466 0.0000 0.4275 0.0022 0.7746 0.0954 0.6845 0.1990 0.8418 0.0551 0.0612
co
re
fe
re
nc
e
SAR 0.9255 0.0069 0.9309 0.0000 0.6736 -0.0037 0.8515 0.0538 0.8396 0.0695 0.8955 0.0165 0.0238
MUC-F1 0.6691 0.0000 0.6242 0.0046 0.6405 0.0344 0.5340 0.1185 0.6500 0.0291 0.5181 0.1216 0.0514
B3F1 0.4625 0.0000 0.4512 0.0000 0.4423 0.0425 0.0965 0.3357 0.4249 0.0675 0.3323 0.1430 0.0981
ACC ? ? 0.9854 0.0007 ? ? 0.9873 0.0011 0.9862 0.0003 0.9886 0.0000 0.0005
APR ? ? 0.6430 0.0316 ? ? 0.5588 0.1948 0.4335 0.0381 0.7697 0.0372 0.0754
BEP ? ? 0.5954 0.0165 ? ? 0.6727 0.0302 0.4436 0.0718 0.6961 0.0385 0.0393
F1 ? ? 0.5643 0.0276 ? ? 0.6837 0.0019 0.5770 0.0367 0.6741 0.0062 0.0181
MXE ? ? 0.9342 0.0029 ? ? 0.8089 0.1425 0.9265 0.0062 0.9572 0.0093 0.0402
RMS ? ? 0.8838 0.0028 ? ? 0.8896 0.0118 0.8839 0.0020 0.9000 0.0068 0.0058
ROC ? ? 0.9576 0.0121 ? ? 0.8566 0.1149 0.7181 0.1593 0.9659 0.0188 0.0763
PS
F
id
en
tifi
ca
tio
n
SAR ? ? 0.9329 0.0052 ? ? 0.9021 0.0407 0.8541 0.0532 0.9420 0.0085 0.0269
GACC ? ? 0.6607 -0.0004 ? ? 0.7962 0.0223 0.6610 0.0506 0.7401 0.0209 0.0233
F0.5 ? ? 0.6829 0.0221 ? ? 0.7150 0.0503 0.7132 0.0000 0.7811 -0.0054 0.0167
F0.2 ? ? 0.7701 0.0157 ? ? 0.7331 0.0875 0.8171 0.0110 0.8542 0.0045 0.0297
ACC 0.8133 0.0000 0.7554 0.0009 ? ? 0.7940 0.0000 0.7446 0.0428 0.7761 0.0381 0.0164
APR 0.8166 0.0296 0.7455 0.0013 ? ? 0.8035 0.0000 0.5957 0.1996 0.6363 0.1520 0.0765
BEP 0.7385 -0.0066 0.6597 -0.0030 ? ? 0.7096 0.0000 0.6317 0.0567 0.6940 0.0432 0.0181
F1 0.7286 0.0033 0.6810 0.0226 ? ? 0.7000 0.0000 0.6774 0.0525 0.6933 0.0400 0.0237
MXE 0.6091 0.0166 0.4940 0.0076 ? ? 0.0379 0.4715 0.4022 0.1197 0.4681 0.1012 0.1433
RMS 0.6475 0.0054 0.5910 0.0033 ? ? 0.6057 0.0000 0.5556 0.0514 0.5836 0.0423 0.0205
ROC 0.8923 0.0096 0.8510 0.0000 ? ? 0.8519 0.0364 0.7514 0.1094 0.7968 0.0757 0.0462
PS
F
hi
er
ar
ch
y
SAR 0.7765 0.0073 0.7251 0.0009 ? ? 0.7430 0.0000 0.6770 0.0672 0.7116 0.0482 0.0247
SENTACC 0.7571 0.0045 0.7307 -0.0011 ? ? 0.7399 -0.0007 0.6801 0.0141 0.6889 0.0726 0.0179
PERFSENT 0.4948 0.0069 0.4880 0.0000 ? ? 0.4880 -0.0034 0.4055 0.0206 0.4158 0.1031 0.0254
Table 2: Performance gains from parameter tuning. The left column for each algorithm family is the algorithm?s
performance with default parameter settings. The adjacent ?Parm?? column gives the performance gain from tuning parameters.
For each metric, the best default and tuned performance across all algorithms are italicized and bold-faced, respectively.
5.1 Experiment 1: Parameter Tuning
Experiment 1 measures, for each of the 3 tasks, the
performance of every model on both the tuning and
test data for every metric of interest. Based on tun-
ing set performance, the best default model, the best
model overall, and the best model within each fam-
ily are selected. The best default model is the
highest-scoring model that emerges after comparing
algorithms without doing any parameter tuning. The
best overall model corresponds to ?tuning? both the
algorithm and parameters. The best model in each
family corresponds to ?tuning? the parameters for
that algorithm. Using the test set performances, the
best family models are compared to the correspond-
ing default models to find the gains from parameter
tuning.
Table 2 lists the gains achieved by parameter tun-
ing. Each algorithm column compares the algo-
rithm?s best model to its default model. On the
coreference task, for example, the best KNN model
with respect to BEP shows a 20% improvement (or
gain) over the default KNN model (for a final BEP
score of .5300). The ?Avg ?? column shows the av-
erage gain from parameter tuning for each metric.
For each metric, the best default model is itali-
cized while the best overall model is bold-faced. Re-
ferring again to the coreference BEP row, the best
overall model is a SVM while the best default model
is a bagged decision tree. Recall that these distinc-
tions are based on absolute performance and not
gain. Thus, the best tuned SVM outperforms all
other models on this task and metric.6
Three conclusions can be drawn from Table 2.
First, no algorithm performs the best on all tasks
or on all metrics. For coreference, the best over-
all model is either a bagged tree, a rule learner, or
a SVM, depending on the target metric. Similarly,
for PSF identification the best model depends on the
metric, ranging from a KNN to a SVM. Interest-
ingly, bagged decision trees on the PSF hierarchy
data outperform the other algorithms on all metrics
and seem especially well-suited to the task.
Second, an algorithm?s best-tuned model reliably
yields non-trivial gains over the corresponding de-
fault model. This trend appears to hold regardless of
algorithm, metric, and data set. In 114 of the 154
6Another interesting example is the best overall model for
BEP on the PSF hierarchy task. The baseline (a bagged tree)
outperforms the ?best? model (a different bagged tree) on the
test set even though the best model performed better on the tun-
ing set?otherwise it would not have been selected as the best.
544
cases parameter tuning improves an algorithm?s per-
formance by more than 0.001 (0.1%). In the remain-
ing 40 cases, parameter tuning only hurts 12 times,
and never by more than 0.01.
Third, the best default algorithm is not necessar-
ily the best algorithm after tuning parameters. The
coreference task, in particular, illustrates the poten-
tial problem with using default parameter settings
when judging which algorithm is most suited for
a problem: 7 out of 10 times the best algorithm
changes after parameter tuning.
These results corroborate those found else-
where (Daelemans and Hoste, 2002; Hoste et al,
2002; Hoste, 2005)?parameter settings greatly in-
fluence performance. Further, algorithmic perfor-
mance differences can change when parameters are
changed. Going beyond previous work, our results
also underline the need to consider multiple algo-
rithms for NLL. Ultimately, it is important for re-
searchers to thoroughly explore options for both al-
gorithm and parameter tuning and to report these in
their results.
5.2 Experiment 2: Ensemble Selection
In experiment 2 an ensemble is built to optimize
each target metric. The ensemble?s performance is
compared to that of the best overall model for the
metric. Both the ensemble and the best model are
selected according to tuning set performance.
Table 3 lists the gains from ensemble selection
over the best parameter tuned model. For compar-
ison, the best default and overall performances from
Table 2 are reprinted. For example, the ensemble op-
timized for F1 on the coreference data outperforms
the best bagged tree model by about 1% (and the
best default model by almost 8%).
Disappointingly, ensemble selection does not
consistently improve performance. Indeed, for the
PSF hierarchy task ensemble selection reliably hurts
performance a small amount. For the other two tasks
ensemble selection reliably improves all metrics ex-
cept GACC (a small loss). In other experiments,
however, we optimized F-measure with ? = 1.5 for
the PSF identification task. Ensemble selection hurt
F1.5 by almost 2%, leading us to question the tech-
nique?s reliability for our second data set. Interest-
ingly, the aggregate metrics?metrics that measure
performance by combining multiple predictions?
Metric Best Default Best Tuned? Ens. Sel.?
ACC 0.9861 0.0002 0.0001
APR 0.5373 0.0000 0.0736
BEP 0.6010 0.0000 0.0124
F1 0.5231 0.0664 0.0115
MXE 0.9373 0.0142 0.0035
RMS 0.8882 0.0023 0.0049
ROC 0.9466 -0.0051 0.0120
co
re
fe
re
nc
e
SAR 0.9309 0.0015 0.0032
MUC-F1 0.6691 0.0000 0.0073
B3F1 0.4625 0.0299 0.0077
ACC 0.9886 0.0000 0.0003
APR 0.7697 0.0372 0.0109
BEP 0.6961 0.0385 0.0136
F1 0.6741 0.0062 0.0222
MXE 0.9572 0.0093 0.0029
RMS 0.9000 0.0068 0.0025
ROC 0.9659 0.0188 0.0043
PS
F
id
en
tifi
ca
tio
n
SAR 0.9420 0.0085 0.0021
GACC 0.7962 0.0223 -0.0012
F0.5 0.7811 -0.0054 0.0063
F0.2 0.8171 0.0110 0.0803
ACC 0.8133 0.0000 -0.0028
APR 0.8035 0.0427 -0.0064
BEP 0.7385 -0.0066 0.0056
F1 0.7286 0.0033 -0.0016
MXE 0.6091 0.0166 -0.0012
RMS 0.6475 0.0054 -0.0019
ROC 0.8923 0.0096 -0.0036
PS
F
hi
er
ar
ch
y
SAR 0.7765 0.0073 -0.0015
SENTACC 0.7571 0.0045 0.0024
PERFSENT 0.4948 0.0069 0.0172
Table 3: Impact from tuning and ensemble selection.
Best default shows the performance of the best classifier with
no parameter tuning (i.e. algorithm tuning only). Best tuned?
gives the performance gain from parameter and algorithm tun-
ing. Ens. Sel.? is the performance gain from ensemble selec-
tion over the best tuned model. The best performance for each
metric is marked in bold.
all benefit from ensemble selection, even for the hi-
erarchy task, albeit for small amounts. For our tasks
these comprise a subset of the task-specific perfor-
mance measures: B3F1, MUC-F1, SENTACC, and
PERFSENT.
While we are not surprised that the positive gains
are small,7 we are surprised at how often ensemble
selection hurts performance. As a result, we investi-
gated some of the metrics where ensemble selection
hurts performance and found that ensemble selec-
tion overfits the tuning data. At this time we are not
sure why this overfitting happens for these tasks and
not for the ones used by Caruana et al Preliminary
investigations suggest that having a smaller model
library is a contributing factor (Caruana et al use
libraries containing ? 2000 models). This is con-
sistent with the fact that the task with the largest
model library, coreference, benefits the most from
ensemble selection. Perhaps the reason that ensem-
ble selection consistently improves performance for
7Caruana et al (2004) find the benefit from ensemble selec-
tion is only half as large as the benefit from carefully optimizing
and selecting the best models in the first place.
545
the aggregate metrics is that these metrics are harder
to overfit.
Based on our results, ensemble selection seems
too unreliable for general use in NLL?at least un-
til the model library requirements are better under-
stood. However, ensemble selection is perhaps trust-
worthy enough to optimize metrics that are difficult
to overfit and could not be easily optimized other-
wise ? in our case, the task-specific aggregate per-
formance measures.
6 Related Work
Hoste et al (2002) and Hoste (2005) study the im-
pact of tuning parameters for k-NN and a rule-
learning algorithm on word sense disambiguation
and coreference resolution, respectively, and find
that parameter settings greatly change results. Simi-
lar work by Daelemans and Hoste (2002) shows the
fallacy of comparing algorithm performance without
first tuning parameters. They find that the best algo-
rithm for a task frequently changes after optimizing
parameters. In contrast to our work, these earlier
experiments investigate at most two algorithms and
only measure performance with one metric per task.
7 Conclusion
We evaluate an ensemble selection framework that
enables optimizing classifier performance to arbi-
trary performance metrics without re-training. An
important side benefit of the framework is the fully
automatic production of base-level models, remov-
ing the need for human expertise in choosing algo-
rithms and parameter settings.
Our experiments show that ensemble selection,
compared to simple algorithm and parameter tuning,
reliably improves performance for six of the seven
task-specific metrics and all four ?aggregate? met-
rics, but only benefits all of the metrics for one of
our three data sets. We also find that exploring mul-
tiple algorithms with a variety of settings is impor-
tant for getting the best performance. Tuning pa-
rameter settings results in 0.05% to 14% average
improvements, with most improvements falling be-
tween 2% and 10%. To this end, the ensemble selec-
tion framework can be used as an environment for
automatically choosing the best algorithm and pa-
rameter settings for a given NLP classification task.
More work is needed to understand when ensemble
selection can be safely used for NLL.
Acknowledgements We thank Alex Niculescu-Mizil for
answering questions about ensemble selection and Eric Breck
for sharing his expertise with both PSF data sets. We also thank
the anonymous reviewers and the Cornell NLP group for help-
ful suggestions on paper drafts. This work was supported by
the Advanced Research and Development Activity (ARDA), by
NSF Grant IIS-0208028, and by NSF Grant IIS-0412930.
References
Amit Bagga and Breck Baldwin. 1998. Algorithms for scoring coreference
chains. In Linguistic Coreference Workshop at LREC 1998, pages 563?566,
May.
Eric Breck and Claire Cardie. 2004. Playing the telephone game: Determining
the hierarchical structure of perspective and speech expressions. In COLING
2004, pages 120?126.
Leo Breiman. 1996. Bagging predictors. Machine Learning, 24(2):123?140.
Wray Buntine. 1993. Learning classification trees. In D. J. Hand, editor, Artificial
Intelligence Frontiers in Statistics, pages 182?201. Chapman & Hall, London.
Rich Caruana, Alexandru Niculescu-Mizil, Geoff Crew, and Alex Ksikes. 2004.
Ensemble selection from libraries of models. In ICML.
William W. Cohen. 1995. Fast effective rule induction. In Armand Prieditis and
Stuart Russell, editors, ICML, pages 115?123, Tahoe City, CA, July 9?12.
Morgan Kaufmann.
Walter Daelemans and Ve?ronique Hoste. 2002. Evaluation of machine learning
methods for natural language processing tasks. In LREC 2002, pages 755?
760.
Walter Daelemans, Jakub Zavrel, Ko van der Sloot, and Antal van den Bosch.
2000. TiMBL: Tilburg memory based learner, version 3.0, reference
guide. ILK Technical Report 00-01, Tilburg University. Available from
http://ilk.kub.nl/?ilk/papers/ ilk0001.ps.gz.
Yoav Freund and Robert E. Schapire. 1996. Experiments with a new boosting
algorithm. In ICML, pages 148?156.
Ve?ronique Hoste, Iris Hendrickx, Walter Daelemans, and Antal van den Bosch.
2002. Parameter optimization for machine learning of word sense disam-
biguation. Natural Language Engineering, 8(4):311?325.
Ve?ronique Hoste. 2005. Optimization Issues in Machine Learning of Coreference
Resolution. Ph.D. thesis, University of Antwerp.
Thorsten Joachims. 1999. Making large-scale SVM learning practical. In Bern-
hard Scho?lkopf, Christopher J. C. Burges, and Alexander J. Smola, editors,
Advances in Kernel Methods - Support Vector Learning. MIT Press, Cam-
bridge, USA.
Miroslav Kubat and Stan Matwin. 1997. Addressing the curse of imbalanced
training sets: One-sided selection. In ICML, pages 179?186, San Francisco,
CA. Morgan Kaufmann.
Andrew Kachites McCallum. 2002. MALLET: A machine learning for language
toolkit. http://mallet.cs.umass.edu.
MPQA Corpus. 2002. NRRC MPQA corpus. Available from http://nrrc.
mitre.org/NRRC/Docs Data/MPQA 04/approval time.htm.
Vincent Ng and Claire Cardie. 2002. Improving machine learning approaches to
coreference resolution. In ACL, pages 104?111.
Vincent Ng. 2004. Improving Machine Learning Approaches to Noun Phrase
Coreference Resolution. Ph.D. thesis, Cornell University.
John C. Platt. 2000. Probabilistic outputs for support vector machines and com-
parison to regularized likelihood methods. In Alexander J. Smola, Peter J.
Bartlett, Bernhard Schoelko?pf, and Dale Schuurmans, editors, Advances in
Large-Margin Classifiers, pages 61?74. MIT Press.
Wee Meng Soon, Hwee Tou Ng, and Chung Yong Lim. 2001. A machine learning
approach to coreference resolution of noun phrases. Computational Linguis-
tics, 27(4):521?544.
Marc Vilain, John Burger, John Aberdeen, Dennis Connolly, and Lynette
Hirschman. 1995. A model-theoretic coreference scoring scheme. In Proc.
of the 6th Message Understanding Conference, pages 45?52. Morgan Kauf-
mann.
Janyce Wiebe, Eric Breck, Chris Buckley, Claire Cardie, Paul Davis, Bruce Fraser,
Diane Litman, David Pierce, Ellen Riloff, Theresa Wilson, David Day, and
Mark Maybury. 2003. Recognizing and organizing opinions expressed in the
world press. In Papers from the AAAI Spring Symposium on New Directions
in Question Answering (AAAI tech report SS-03-07). March 24-26, 2003.
Stanford University, Palo Alto, California.
Ian H. Witten and Eibe Frank. 2000. Data Mining: Practical Machine Learning
Tools with Java Implementations. Morgan Kaufmann, San Francisco, CA.
546
Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural Language
Processing (HLT/EMNLP), pages 923?930, Vancouver, October 2005. c?2005 Association for Computational Linguistics
Multi-Perspective Question Answering Using the OpQA Corpus
Veselin Stoyanov and Claire Cardie
Department of Computer Science
Cornell University
Ithaca, NY 14850, USA
{ves,cardie}@cs.cornell.edu
Janyce Wiebe
Department of Computer Science
University of Pittsburgh
Pittsburgh, PA 15260, USA
wiebe@cs.pitt.edu
Abstract
We investigate techniques to support the
answering of opinion-based questions.
We first present the OpQA corpus of opin-
ion questions and answers. Using the cor-
pus, we compare and contrast the proper-
ties of fact and opinion questions and an-
swers. Based on the disparate characteris-
tics of opinion vs. fact answers, we argue
that traditional fact-based QA approaches
may have difficulty in an MPQA setting
without modification. As an initial step
towards the development of MPQA sys-
tems, we investigate the use of machine
learning and rule-based subjectivity and
opinion source filters and show that they
can be used to guide MPQA systems.
1 Introduction
Much progress has been made in recent years in
automatic, open-domain question answering (e.g.,
Voorhees (2001), Voorhees (2002), Voorhees and
Buckland (2003)). The bulk of the research in this
area, however, addresses fact-based questions like:
?When did McDonald?s open its first restaurant??
or ?What is the Kyoto Protocol??. To date, how-
ever, relatively little research been done in the area
of Multi-Perspective Question Answering (MPQA),
which targets questions of the following sort:
? How is Bush?s decision not to ratify the Kyoto Protocol
looked upon by Japan and other US allies?
? How do the Chinese regard the human rights record of the
United States?
In comparison to fact-based question answering
(QA), researchers understand far less about the prop-
erties of questions and answers in MPQA, and have
yet to develop techniques to exploit knowledge of
those properties. As a result, it is unclear whether
approaches that have been successful in the domain
of fact-based QA will work well for MPQA.
We first present the OpQA corpus of opinion ques-
tions and answers. Using the corpus, we compare
and contrast the properties of fact and opinion ques-
tions and answers. We find that text spans identi-
fied as answers to opinion questions: (1) are approx-
imately twice as long as those of fact questions, (2)
are much more likely (37% vs. 9%) to represent par-
tial answers rather than complete answers, (3) vary
much more widely with respect to syntactic cate-
gory ? covering clauses, verb phrases, prepositional
phrases, and noun phrases; in contrast, fact answers
are overwhelming associated with noun phrases, and
(4) are roughly half as likely to correspond to a sin-
gle syntactic constituent type (16-38% vs. 31-53%).
Based on the disparate characteristics of opinion
vs. fact answers, we argue that traditional fact-based
QA approaches may have difficulty in an MPQA
setting without modification. As one such modifi-
cation, we propose that MPQA systems should rely
on natural language processing methods to identify
information about opinions. In experiments in opin-
ion question answering using the OpQA corpus, we
find that filtering potential answers using machine
learning and rule-based NLP opinion filters substan-
tially improves the performance of an end-to-end
MPQA system according to both a mean reciprocal
rank (MRR) measure (0.59 vs. a baseline of 0.42)
923
and a metric that determines the mean rank of the
first correct answer (MRFA) (26.2 vs. a baseline of
61.3). Further, we find that requiring opinion an-
swers to match the requested opinion source (e.g.,
does <source> approve of the Kyoto Protocol) dra-
matically improves the performance of the MPQA
system on the hardest questions in the corpus.
The remainder of the paper is organized as fol-
lows. In the next section we summarize related
work. Section 3 describes the OpQA corpus. Sec-
tion 4 uses the OpQA corpus to identify poten-
tially problematic issues for handling opinion vs.
fact questions. Section 5 briefly describes an opin-
ion annotation scheme used in the experiments. Sec-
tions 6 and 7 explore the use of opinion information
in the design of MPQA systems.
2 Related Work
There is a growing interest in methods for the auto-
matic identification and extraction of opinions, emo-
tions, and sentiments in text. Much of the relevant
research explores sentiment classification, a text cat-
egorization task in which the goal is to assign to
a document either positive (?thumbs up?) or nega-
tive (?thumbs down?) polarity (e.g. Das and Chen
(2001), Pang et al (2002), Turney (2002), Dave et
al. (2003), Pang and Lee (2004)). Other research
has concentrated on analyzing opinions at, or below,
the sentence level. Recent work, for example, indi-
cates that systems can be trained to recognize opin-
ions, their polarity, their source, and their strength
to a reasonable degree of accuracy (e.g. Dave et
al. (2003), Riloff and Wiebe (2003), Bethard et al
(2004), Pang and Lee (2004), Wilson et al (2004),
Yu and Hatzivassiloglou (2003), Wiebe and Riloff
(2005)).
Related work in the area of corpus development
includes Wiebe et al?s (2005) opinion annotation
scheme to identify subjective expressions ? expres-
sions used to express opinions, emotions, sentiments
and other private states in text. Wiebe et al have
applied the annotation scheme to create the MPQA
corpus consisting of 535 documents manually an-
notated for phrase-level expressions of opinion. In
addition, the NIST-sponsored TREC evaluation has
begun to develop data focusing on opinions ? the
2003 Novelty Track features a task that requires sys-
tems to identify opinion-oriented documents w.r.t. a
specific issue (Voorhees and Buckland, 2003).
While all of the above work begins to bridge
the gap between text categorization and question
answering, none of the approaches have been em-
ployed or evaluated in the context of MPQA.
3 OpQA Corpus
To support our research in MPQA, we created the
OpQA corpus of opinion and fact questions and an-
swers. Additional details on the construction of the
corpus as well as results of an interannotator agree-
ment study can be found in Stoyanov et al (2004).
3.1 Documents and Questions
The OpQA corpus consists of 98 documents that ap-
peared in the world press between June 2001 and
May 2002. All documents were taken from the
aforementioned MPQA corpus (Wilson and Wiebe,
2003)1 and are manually annotated with phrase-
level opinion information, following the annotation
scheme of Wiebe et al (2005), which is briefly
summarized in Section 5. The documents cover
four general (and controversial) topics: President
Bush?s alternative to the Kyoto protocol (kyoto); the
US annual human rights report (humanrights); the
2002 coup d?etat in Venezuela (venezuela); and the
2002 elections in Zimbabwe and Mugabe?s reelec-
tion (mugabe). Each topic is covered by between 19
and 33 documents that were identified automatically
via IR methods.
Both fact and opinion questions for each topic
were added to the OpQA corpus by a volunteer not
associated with the current project. The volunteer
was provided with a set of instructions for creat-
ing questions together with two documents on each
topic selected at random. He created between six
and eight questions on each topic, evenly split be-
tween fact and opinion. The 30 questions are given
in Table 1 sorted by topic.
3.2 Answer annotations
Answer annotations were added to the corpus by two
annotators according to a set of annotation instruc-
1The MPQA corpus is available at
http://nrrc.mitre.org/NRRC/publications.htm.
The OpQA corpus is available upon request.
924
Kyoto
1 f What is the Kyoto Protocol about?
2 f When was the Kyoto Protocol adopted?
3 f Who is the president of the Kiko Network?
4 f What is the Kiko Network?
5 o Does the president of the Kiko Network approve of the US action concerning the Kyoto Protocol?
6 o Are the Japanese unanimous in their opinion of Bush?s position on the Kyoto Protocol?
7 o How is Bush?s decision not to ratify the Kyoto Protocol looked upon by Japan and other US allies?
8 o How do European Union countries feel about the US opposition to the Kyoto protocol?
Human Rights
1 f What is the murder rate in the United States?
2 f What country issues an annual report on human rights in the United States?
3 o How do the Chinese regard the human rights record of the United States?
4 f Who is Andrew Welsdan?
5 o What factors influence the way in which the US regards the human rights records of other nations?
6 o Is the US Annual Human Rights Report received with universal approval around the world?
Venezuela
1 f When did Hugo Chavez become President?
2 f Did any prominent Americans plan to visit Venezuela immediately following the 2002 coup?
3 o Did anything surprising happen when Hugo Chavez regained power in Venezuela after he was
removed by a coup?
4 o Did most Venezuelans support the 2002 coup?
5 f Which governmental institutions in Venezuela were dissolved by the leaders of the 2002 coup?
6 o How did ordinary Venezuelans feel about the 2002 coup and subsequent events?
7 o Did America support the Venezuelan foreign policy followed by Chavez?
8 f Who is Vice-President of Venezuela?
Mugabe
1 o What was the American and British reaction to the reelection of Mugabe?
2 f Where did Mugabe vote in the 2002 presidential election?
3 f At which primary school had Mugabe been expected to vote in the 2002 presidential election?
4 f How long has Mugabe headed his country?
5 f Who was expecting Mugabe at Mhofu School for the 2002 election?
6 o What is the basis for the European Union and US critical attitude and adversarial action toward
Mugabe?
7 o What did South Africa want Mugabe to do after the 2002 election?
8 o What is Mugabe?s opinion about the West?s attitude and actions towards the 2002 Zimbabwe elec-
tion?
Table 1: Questions in the OpQA collection by topic.
f in column 1 indicates a fact question; o, an opinion
question.
tions.2 Every text segment that contributes to an
answer to any of the 30 questions is annotated as
an answer. In particular, answer annotations include
segments that constitute a partial answer. Partial an-
swers either (1) lack the specificity needed to consti-
tute a full answer (e.g., ?before May 2004? partially
answers the question When was the Kyoto protocol
ratified? when a specific date is known) or (2) need
to be combined with at least one additional answer
segment to fully answer the question (e.g., the ques-
tion Are the Japanese unanimous in their opposition
of Bush?s position on the Kyoto protocol? is an-
swered only partially by a segment expressing a sin-
gle opinion). In addition, annotators mark the min-
imum answer spans (e.g., ?a Tokyo organization,?
vs. ?a Tokyo organization representing about 150
Japanese groups?).
4 Characteristics of opinion answers
Next, we use the OpQA corpus to analyze and com-
pare the characteristics of fact vs. opinion questions.
Based on our findings, we believe that QA systems
based solely on traditional QA techniques are likely
2The annotation instructions are available
at http://www.cs.cornell.edu/ ves/
Publications/publications.htm.
to be less effective at MPQA than they are at tradi-
tional fact-based QA.
4.1 Traditional QA architectures
Despite the wide variety of approaches implied by
modern QA systems, almost all systems rely on the
following two steps (subsystems), which have em-
pirically proven to be effective:
? IR module. The QA system invokes an IR subsystem that
employs traditional text similarity measures (e.g., tf/idf)
to retrieve and rank document fragments (sentences or
paragraphs) w.r.t. the question (query).
? Linguistic filters. QA systems employ a set of filters
and text processing components to discard some docu-
ment fragments. The following filters have empirically
proven to be effective and are used universally:
Semantic filters prefer an answer segment that matches
the semantic class(es) associated with the question type
(e.g., date or time for when questions; person or organi-
zation for who questions).
Syntactic filters are also configured on the type of ques-
tion. The most common and effective syntactic filters se-
lect a specific constituent (e.g., noun phrase) according to
the question type (e.g., who question).
QA systems typically interleave the above two
subsystems with a variety of different processing
steps of both the question and the answer. The goal
of the processing is to identify text fragments that
contain an answer to the question. Typical QA sys-
tems do not perform any further text processing;
they return the text fragment as it occurred in the
text. 3
4.2 Corpus-based analysis of opinion answers
We hypothesize that QA systems that conform to
this traditional architecture will have difficulty han-
dling opinion questions without non-trivial modifi-
cation. In support of this hypothesis, we provide
statistics from the OpQA corpus to illustrate some of
the characteristics that distinguish answers to opin-
ion vs. fact questions, and discuss their implications
for a traditional QA system architecture.
Answer length. We see in Table 2 that the aver-
age length of opinion answers in the OpQA corpus
3This architecture is seen mainly in QA systems designed
for TREC?s ?factoid? and ?list? QA tracks. Systems competing
in the relatively new ?definition? or ?other? tracks have begun
to introduce new approaches. However, most such systems still
rely on the IR step and return the text fragment as it occurred in
the text.
925
Number of answers Length Number of partials
fact 124 5.12 12 (9.68%)
opinion 415 9.24 154 (37.11%)
Table 2: Number of answers, average answer length
(in tokens), and number of partial answers for
fact/opinion questions.
is 9.24 tokens, almost double that of fact answers.
Unfortunately, longer answers could present prob-
lems for some traditional QA systems. In particu-
lar, some of the more sophisticated algorithms that
perform additional processing steps such as logi-
cal verifiers (Moldovan et al, 2002) may be less ac-
curate or computationally infeasible for longer an-
swers. More importantly, longer answers are likely
to span more than a single syntactic constituent, ren-
dering the syntactic filters, and very likely the se-
mantic filters, less effective.
Partial answers. Table 2 also shows that over 37%
of the opinion answers were marked as partial vs.
9.68% of the fact answers. The implications of par-
tial answers for the traditional QA architecture are
substantial: an MPQA system will require an an-
swer generator to (1) distinguish between partial
and full answers; (2) recognize redundant partial an-
swers; (3) identify which subset of the partial an-
swers, if any, constitutes a full answer; (4) determine
whether additional documents need to be examined
to find a complete answer; and (5) asemble the final
answer from partial pieces of information.
Syntactic constituent of the answer. As discussed
in Section 4.1, traditional QA systems rely heav-
ily on the predicted syntactic and semantic class of
the answer. Based on answer lengths, we specu-
lated that opinion answers are unlikely to span a sin-
gle constituent and/or semantic class. This specula-
tion is confirmed by examining the phrase type as-
sociated with OpQA answers using Abney?s (1996)
CASS partial parser.4 For each question, we count
the number of times an answer segment for the ques-
tion (in the manual annotations) matches each con-
stituent type. We consider four constituent types
? noun phrase (n), verb phrase (v), prepositional
phrase (p), and clause (c) ? and three matching cri-
teria:
4The parser is available from
http://www.vinartus.net/spa/.
Fact OpinionQues- # of Matching Criteria syn Ques- # of Matching Criteria syn
tion answers ex up up/dn type tion answers ex up up/dn type
H 1 1 0 0 0 H 3 15 5 5 5 c
H 2 4 2 2 2 n H 5 24 5 5 10 n
H 4 1 0 0 0 H 6 123 17 23 52 n
K 1 48 13 14 24 n K 5 3 0 0 1
K 2 38 13 13 19 n K 6 34 6 5 12 c
K 3 1 1 1 1 c n K 7 55 9 8 19 c
K 4 2 1 1 1 n K 8 25 4 4 10 v
M 2 3 0 0 1 M 1 74 10 12 29 v
M 3 1 0 0 1 M 6 12 3 5 7 n
M 4 10 2 2 5 n M 7 1 0 0 0
M 5 3 1 1 2 c M 8 3 0 0 1V 1 4 3 3 4 n V 3 1 1 0 1 cV 2 1 1 1 1 n V 4 13 2 2 2 cV 5 3 0 1 1 V 6 9 2 2 5 c nV 8 4 2 4 4 n V 7 23 3 1 5
Cov- 124 39 43 66 Cov- 415 67 70 159
erage 31% 35% 53% erage 16% 17% 38%
Table 3: Syntactic Constituent Type for Answers in
the OpQA Corpus
1. The exact match criterion is satisfied only by answer seg-
ments whose spans exactly correspond to a constituent in
the CASS output.
2. The up criterion considers an answer to match a CASS
constituent if the constituent completely contains the an-
swer and no more than three additional (non-answer) to-
kens.
3. The up/dn criterion considers an answer to match a
CASS constituent if it matches according to the up crite-
rion or if the answer completely contains the constituent
and no more than three additional tokens.
The counts for the analysis of answer segment
syntactic type for fact vs. opinion questions are sum-
marized in Table 3. Results for the 15 fact ques-
tions are shown in the left half of the table, and
for the 15 opinion questions in the right half. The
leftmost column in each half provides the question
topic and number, and the second column indicates
the total number of answer segments annotated for
the question. The next three columns show, for each
of the ex, up, and up/dn matching criteria, respec-
tively, the number of annotated answer segments
that match the majority syntactic type among an-
swer segments for that question/criterion pair. Us-
ing a traditional QA architecture, the MPQA sys-
tem might filter answers based on this majority type.
The syn type column indicates the majority syntac-
tic type using the exact match criterion; two values
in the column indicate a tie for majority syntactic
type, and an empty syntactic type indicates that no
answer exactly matched any of the four constituent
types. With only a few exceptions, the up and up/dn
matching criteria agreed in majority syntactic type.
Results in Table 3 show a significant disparity be-
tween fact and opinion questions. For fact ques-
926
tions, the syntactic type filter would keep 31%, 35%,
or 53% of the correct answers, depending on the
matching criterion. For opinion questions, there is
unfortunately a two-fold reduction in the percentage
of correct answers that would remain after filtering
? only 16%, 17% or 38%, depending on the match-
ing criterion. More importantly, the majority syntac-
tic type among answers for fact questions is almost
always a noun phrase, while no single constituent
type emerges as a useful syntactic filter for opinion
questions (see the syn phrase columns in Table 3).
Finally, because semantic class information is gener-
ally tied to a particular syntactic category, the effec-
tiveness of traditional semantic filters in the MPQA
setting is unclear.
In summary, identifying answers to questions in
an MPQA setting within a traditional QA architec-
ture will be difficult. First, the implicit and explicit
assumptions inherent in standard linguistic filters are
consistent with the characteristics of fact- rather than
opinion-oriented QA. In addition, the presence of
relatively long answers and partial answers will re-
quire a much more complex answer generator than
is typically present in current QA systems.
In Sections 6 and 7, we propose initial steps to-
wards modifying the traditional QA architecture for
use in MPQA. In particular, we propose and evaluate
two types of opinion filters for MPQA: subjectiv-
ity filters and opinion source filters. Both types of
linguistic filters rely on phrase-level and sentence-
level opinion information, which has been manually
annotated for our corpus; the next section briefly de-
scribes the opinion annotation scheme.
5 Manual Opinion Annotations
Documents in our OpQA corpus come from the
larger MPQA corpus, which contains manual opin-
ion annotations. The annotation framework is de-
scribed in detail in (Wiebe et al, 2005). Here we
give a high-level overview.
The annotation framework provides a basis for
subjective expressions: expressions used to express
opinions, emotions, and sentiments. The framework
allows for the annotation of both directly expressed
private states (e.g., afraid in the sentence ?John is
afraid that Sue might fall,?) and opinions expressed
by the choice of words and style of language (e.g.,
it is about time and oppression in the sentence ?It is
about time that we end Saddam?s oppression?). In
addition, the annotations include several attributes,
including the intensity (with possible values low,
medium, high, and extreme) and the source of the
private state. The source of a private state is the per-
son or entity who holds or experiences it.
6 Subjectivity Filters for MPQA Systems
This section describes three subjectivity filters
based on the above opinion annotation scheme. Be-
low (in Section 6.3), the filters are used to remove
fact sentences from consideration when answering
opinion questions, and the OpQA corpus is used to
evaluate their effectiveness.
6.1 Manual Subjectivity Filter
Much previous research on automatic extraction of
opinion information performed classifications at the
sentence level. Therefore, we define sentence-level
opinion classifications in terms of the phrase-level
annotations. For our gold standard of manual opin-
ion classifications (dubbed MANUAL for the rest of
the paper) we will follow Riloff and Wiebe?s (2003)
convention (also used by Wiebe and Riloff (2005))
and consider a sentence to be opinion if it contains
at least one opinion of intensity medium or higher,
and to be fact otherwise.
6.2 Two Automatic Subjectivity Filters
As discussed in section 2, several research efforts
have attempted to perform automatic opinion clas-
sification on the clause and sentence level. We in-
vestigate whether such information can be useful for
MPQA by using the automatic sentence level opin-
ion classifiers of Riloff and Wiebe (2003) and Wiebe
and Riloff (2005).
Riloff and Wiebe (2003) use a bootstrapping al-
gorithm to perform a sentence-based opinion classi-
fication on the MPQA corpus. They use a set of high
precision subjectivity and objectivity clues to iden-
tify subjective and objective sentences. This data
is then used in an algorithm similar to AutoSlog-
TS (Riloff, 1996) to automatically identify a set of
extraction patterns. The acquired patterns are then
used iteratively to identify a larger set of subjective
and objective sentences. In our experiments we use
927
precision recall F
MPQA corpus RULEBASED 90.4 34.2 46.6
NAIVE BAYES 79.4 70.6 74.7
Table 4: Precision, recall, and F-measure for the two
classifiers.
the classifier that was created by the reimplemen-
tation of this bootstrapping process in Wiebe and
Riloff (2005). We will use RULEBASED to denote
the opinion information output by this classifier.
In addition, Wiebe and Riloff used the RULE-
BASED classifier to produce a labeled data set for
training. They trained a Naive Bayes subjectivity
classifier on the labeled set. We will use NAIVE
BAYES to refer to Wiebe and Riloff?s naive Bayes
classifier.5 Table 4 shows the performance of the
two classifiers on the MPQA corpus as reported by
Wiebe and Riloff.
6.3 Experiments
We performed two types of experiments using the
subjectivity filters.
6.3.1 Answer rank experiments
Our hypothesis motivating the first type of exper-
iment is that subjectivity filters can improve the an-
swer identification phase of an MPQA system. We
implement the IR subsystem of a traditional QA sys-
tem, and apply the subjectivity filters to the IR re-
sults. Specifically, for each opinion question in the
corpus 6 , we do the following:
1. Split all documents in our corpus into sentences.
2. Run an information retrieval algorithm7 on the set of all
sentences using the question as the query to obtain a
ranked list of sentences.
3. Apply a subjectivity filter to the ranked list to remove all
fact sentences from the ranked list.
We test each of the MANUAL, RULEBASED, and
NAIVE BAYES subjectivity filters. We compare the
rank of the first answer to each question in the
5Specifically, the one they label Naive Bayes 1.
6We do not evaluate the opinion filters on the 15 fact ques-
tions. Since opinion sentences are defined as containing at least
one opinion of intensity medium or higher, opinion sentences
can contain factual information and sentence-level opinion fil-
ters are not likely to be effective for fact-based QA.
7We use the Lemur toolkit?s standard tf.idf implementation
available from http://www.lemurproject.org/.
Topic Qnum Baseline Manual NaiveBayes Rulebased
Kyoto 5 1 1 1 1
6 5 4 4 3
7 1 1 1 1
8 1 1 1 1
Human 3 1 1 1 1
Rights 5 10 6 7 5
6 1 1 1 1
Venezuela 3 106 81 92 35
4 3 2 3 1
6 1 1 1 1
7 3 3 3 2
Mugabe 1 2 2 2 2
6 7 5 5 4
7 447 291 317 153
8 331 205 217 182
MRR : 0.4244 0.5189 0.5078 0.5856
MRFA: 61.3333 40.3333 43.7333 26.2
Table 5: Results for the subjectivity filters.
ranked list before the filter is applied, with the rank
of the first answer to the question in the ranked list
after the filter is applied.
Results. Results for the opinion filters are compared
to a simple baseline, which performs the informa-
tion retrieval step with no filtering. Table 5 gives the
results on the 15 opinion questions for the baseline
and each of the three subjectivity filters. The table
shows two cumulative measures ? the mean recip-
rocal rank (MRR) 8 and the mean rank of the first
answer (MRFA). 9
Table 5 shows that all three subjectivity filters out-
perform the baseline: for all three filters, the first
answer in the filtered results for all 15 questions is
ranked at least as high as in the baseline. As a result,
the three subjectivity filters outperform the baseline
in both MRR and MRFA. Surprisingly, the best per-
forming subjectivity filter is RULEBASED, surpass-
ing the gold standard MANUAL, both in MRR (0.59
vs. 0.52) and MRFA (40.3 vs. 26.2). Presum-
ably, the improvement in performance comes from
the fact that RULEBASED identifies subjective sen-
tences with the highest precision (and lowest recall).
Thus, the RULEBASED subjectivity filter discards
non-subjective sentences most aggressively.
6.3.2 Answer probability experiments
The second experiment, answer probability, be-
gins to explore whether opinion information can be
8The MRR is computed as the average of 1/r, where r is
the rank of the first answer.
9MRR has been accepted as the standard performance mea-
sure in QA, since MRFA can be strongly affected by outlier
questions. However, the MRR score is dominated by the results
in the high end of the ranking. Thus, MRFA may be more ap-
propriate for our experiments because the filters are an interme-
diate step in the processing, the results of which other MPQA
components may improve.
928
sentence
fact opinion
Manual fact 56 (46.67%) 64 (53.33%)
opinion 42 (10.14%) 372 (89.86%)
question Naive Bayes fact 49 (40.83%) 71 (59.17%)
opinion 57 (13.77%) 357 (86.23%)
Rulebased fact 96 (80.00%) 24 (20.00%)
opinion 184 (44.44%) 230 (55.56%)
Table 6: Answer probability results.
used in an answer generator. This experiment con-
siders correspondences between (1) the classes (i.e.,
opinion or fact) assigned by the subjectivity filters to
the sentences containing answers, and (2) the classes
of the questions the answers are responses to (ac-
cording to the OpQA annotations). That is, we com-
pute the probabilities (where ans = answer):
P(ans is in a C1 sentence | ans is the answer to a C2 ques-
tion) for all four combinations of C1=opinion, fact and
C2=opinion, fact.
Results. Results for the answer probability experi-
ment are given in Table 6. The rows correspond to
the classes of the questions the answers respond to,
and the columns correspond to the classes assigned
by the subjectivity filters to the sentences contain-
ing the answers. The first two rows, for instance,
give the results for the MANUAL criterion. MANUAL
placed 56 of the answers to fact questions in fact
sentences (46.67% of all answers to fact questions)
and 64 (53.33%) of the answers to fact questions in
opinion sentences. Similarly, MANUAL placed 42
(10.14%) of the answers to opinion questions in fact
sentences, and 372 (89.86%) of the answers to opin-
ion questions in opinion sentences.
The answer probability experiment sheds some
light on the subjectivity filter experiments. All three
subjectivity filters place a larger percentage of an-
swers to opinion questions in opinion sentences than
they place in fact sentences. However, the differ-
ent filters exhibit different degrees of discrimination.
Answers to opinion questions are almost always
placed in opinion sentences by MANUAL (89.86%)
and NAIVE BAYES (86.23%). While that aspect of
their performance is excellent, MANUAL and NAIVE
BAYES place more answers to fact questions in opin-
ion rather than fact sentences (though the percent-
ages are in the 50s). This is to be expected, because
MANUAL and NAIVE BAYES are more conservative
and err on the side of classifying sentences as opin-
ions: for MANUAL, the presence of any subjective
expression makes the entire sentence opinion, even
if parts of the sentence are factual; NAIVE BAYES
shows high recall but lower precision in recognizing
opinion sentences (see Table 4). Conversely, RULE-
BASED places 80% of the fact answers in fact sen-
tences and only 56% of the opinion answers in opin-
ion sentences. Again, the lower number of assign-
ments to opinion sentences is to be expected, given
the high precision and low recall of the classifier.
But the net result is that, for RULEBASED, the off-
diagonals are all less than 50%: it places more an-
swers to fact questions in fact rather than opinion
sentences (80%), and more answers to opinion ques-
tions in opinion rather than fact sentences (56%).
This is consistent with its superior performance in
the subjectivity filtering experiment.
In addition to explaining the performance of
the subjectivity filters, the answer rank experiment
shows that the automatic opinion classifiers can be
used directly in an answer generator module. The
two automatic classifiers rely on evidence in the sen-
tence to predict the class (the information extraction
patterns used by RULEBASED and the features used
by NAIVE BAYES). In ongoing work we investigate
ways to use this evidence to extract and summarize
the opinions expressed in text, which is a task simi-
lar to that of an answer generator module.
7 Opinion Source Filters for MPQA
Systems
In addition to subjectivity filters, we also define an
opinion source filter based on the manual opinion
annotations. This filter removes all sentences that
do not have an opinion annotation with a source that
matches the source of the question10. For this filter
we only used the MANUAL source annotations since
we did not have access to automatically extracted
source information. We employ the same Answer
Rank experiment as in 6.3.1, substituting the source
filter for a subjectivity filter.
Results. Results for the source filter are mixed.
The filter outperforms the baseline on some ques-
tions and performs worst on others. As a result the
MRR for the source filter is worse than the base-
10We manually identified the sources of each of the 15 opin-
ion questions.
929
line (0.4633 vs. 0.4244). However, the source fil-
ter exhibits by far the best results using the MRFA
measure, a value of 11.267. The performance im-
provement is due to the filter?s ability to recognize
the answers to the hardest questions, for which the
other filters have the most trouble (questions mu-
gabe 7 and 8). For these questions, the rank of the
first answer improves from 153 to 21, and from 182
to 11, respectively. With the exception of question
venezuela 3, which does not contain a clear source
(and is problematic altogether because there is only
a single answer in the corpus and the question?s
qualification as opinion is not clear) the source filter
always ranked an answer within the first 25 answers.
Thus, source filters can be especially useful in sys-
tems that rely on the presence of an answer within
the first few ranked answer segments and then in-
voke more sophisticated analysis in the additional
processing phase.
8 Conclusions
We began by giving a high-level overview of the
OpQA corpus. Using the corpus, we compared the
characteristics of answers to fact and opinion ques-
tions. Based on the different characteristics, we sur-
mise that traditional QA approaches may not be as
effective for MPQA as they have been for fact-based
QA. Finally, we investigated the use of machine
learning and rule-based opinion filters and showed
that they can be used to guide MPQA systems.
Acknowledgments We would like to thank Di-
ane Litman for her work eliciting the questions for
the OpQA corpus, and the anonymous reviewers
for their helpful comments.This work was supported
by the Advanced Research and Development Activ-
ity (ARDA), by NSF Grants IIS-0208028 and IIS-
0208798, by the Xerox Foundation, and by a NSF
Graduate Research Fellowship to the first author.
References
Steven Abney. 1996. Partial parsing via finite-state cascades.
In Proceedings of the ESSLLI ?96 Robust Parsing Workshop.
S. Bethard, H. Yu, A. Thornton, V. Hativassiloglou, and D. Ju-
rafsky. 2004. Automatic extraction of opinion propositions
and their holders. In 2004 AAAI Spring Symposium on Ex-
ploring Attitude and Affect in Text.
S. Das and M. Chen. 2001. Yahoo for amazon: Extracting mar-
ket sentiment from stock message boards. In Proceedings of
the 8th Asia Pacific Finance Association Annual Conference.
Kushal Dave, Steve Lawrence, and David Pennock. 2003. Min-
ing the peanut gallery: Opinion extraction and semantic clas-
sification of product reviews. In International World Wide
Web Conference, pages 519?528.
D. Moldovan, S. Harabagiu, R. Girju, P. Morarescu, F. Laca-
tusu, A. Novischi, A. Badulescu, and O. Bolohan. 2002.
LCC tools for question answering. In Proceedings of TREC
2002.
Bo Pang and Lillian Lee. 2004. A sentimental education: Sen-
timent analysis using subjectivity summarization based on
minimum cuts. In Proceedings of the ACL, pages 271?278.
Bo Pang, Lillian Lee, and Shivakumar Vaithyanathan. 2002.
Thumbs up? sentiment classification using machine learning
techniques. In Proceedings of EMNLP.
E. Riloff and J. Wiebe. 2003. Learning extraction patterns for
subjective expressions. In Proceesings of EMNLP.
Ellen Riloff. 1996. Automatically generating extraction pat-
terns from untagged text. Proceedings of AAAI.
V. Stoyanov, C. Cardie, J. Wiebe, and D. Litman. 2004. Eval-
uating an opinion annotation scheme using a new Multi-
Perspective Question and Answer corpus. In 2004 AAAI
Spring Symposium on Exploring Attitude and Affect in Text.
Peter Turney. 2002. Thumbs up or thumbs down? Semantic
orientation applied to unsupervised classification of reviews.
In Proceedings of the ACL, pages 417?424.
E. Voorhees and L. Buckland. 2003. Overview of the
TREC 2003 Question Answering Track. In Proceedings of
TREC 12.
Ellen Voorhees. 2001. Overview of the TREC 2001 Question
Answering Track. In Proceedings of TREC 10.
Ellen Voorhees. 2002. Overview of the 2002 Question Answer-
ing Track. In Proceedings of TREC 11.
Janyce Wiebe and Ellen Riloff. 2005. Creating subjective
and objective sentence classifiers from unannotated texts. In
Proceedings of CICLing.
Janyce Wiebe, Theresa Wilson, and Claire Cardie. 2005. An-
notating expressions of opinions and emotions in language.
Language Resources and Evaluation, 1(2).
Theresa Wilson and Janyce Wiebe. 2003. Annotating opinions
in the world press. 4th SIGdial Workshop on Discourse and
Dialogue (SIGdial-03).
T. Wilson, J. Wiebe, and R. Hwa. 2004. Just how mad are you?
Finding strong and weak opinion clauses. In Proceedings of
AAAI.
H. Yu and V. Hatzivassiloglou. 2003. Towards answering opin-
ion questions: Separating facts from opinions and identi-
fying the polarity of opinion sentences. In Proceedings of
EMNLP.
930
Proceedings of HLT/EMNLP 2005 Demonstration Abstracts, pages 34?35,
Vancouver, October 2005.
OpinionFinder: A system for subjectivity analysis
Theresa Wilson?, Paul Hoffmann?, Swapna Somasundaran?, Jason Kessler?,
Janyce Wiebe??, Yejin Choi?, Claire Cardie?, Ellen Riloff?, Siddharth Patwardhan?
?Intelligent Systems Program, University of Pittsburgh, Pittsburgh, PA 15260
?Department of Computer Science, University of Pittsburgh, Pittsburgh, PA 15260
?Department of Computer Science, Cornell University, Ithaca, NY 14853
?School of Computing, University of Utah, Salt Lake City, UT 84112
{twilson,hoffmanp,swapna,jsk44,wiebe}@cs.pitt.edu,
{ychoi,cardie}@cs.cornell.edu, {riloff,sidd}@cs.utah.edu
1 Introduction
OpinionFinder is a system that performs subjectivity
analysis, automatically identifying when opinions,
sentiments, speculations, and other private states are
present in text. Specifically, OpinionFinder aims to
identify subjective sentences and to mark various as-
pects of the subjectivity in these sentences, includ-
ing the source (holder) of the subjectivity and words
that are included in phrases expressing positive or
negative sentiments.
Our goal with OpinionFinder is to develop a sys-
tem capable of supporting other Natural Language
Processing (NLP) applications by providing them
with information about the subjectivity in docu-
ments. Of particular interest are question answering
systems that focus on being able to answer opinion-
oriented questions, such as the following:
How is Bush?s decision not to ratify the
Kyoto Protocol looked upon by Japan and
other US allies?
How do the Chinese regard the human
rights record of the United States?
To answer these types of questions, a system needs
to be able to identify when opinions are expressed in
text and who is expressing them. Other applications
that would benefit from knowledge of subjective lan-
guage include systems that summarize the various
viewpoints in a document or that mine product re-
views. Even typical fact-oriented applications, such
as information extraction, can benefit from subjec-
tivity analysis by filtering out opinionated sentences
(Riloff et al, 2005).
2 OpinionFinder
OpinionFinder runs in two modes, batch and inter-
active. Document processing is largely the same for
both modes. In batch mode, OpinionFinder takes a
list of documents to process. Interactive mode pro-
vides a front-end that allows a user to query on-line
news sources for documents to process.
2.1 System Architecture Overview
OpinionFinder operates as one large pipeline. Con-
ceptually, the pipeline can be divided into two parts.
The first part performs mostly general purpose doc-
ument processing (e.g., tokenization and part-of-
speech tagging). The second part performs the sub-
jectivity analysis. The results of the subjectivity
analysis are returned to the user in the form of
SGML/XML markup of the original documents.
2.2 Document Processing
For general document processing, OpinionFinder
first runs the Sundance partial parser (Riloff and
Phillips, 2004) to provide semantic class tags, iden-
tify Named Entities, and match extraction patterns
that correspond to subjective language (Riloff and
Wiebe, 2003). Next, OpenNLP1 1.1.0 is used to tok-
enize, sentence split, and part-of-speech tag the data,
and the Abney stemmer2 is used to stem. In batch
mode, OpinionFinder parses the data again, this time
to obtain constituency parse trees (Collins, 1997),
which are then converted to dependency parse trees
(Xia and Palmer, 2001). Currently, this stage is only
1http://opennlp.sourceforge.net/
2SCOL version 1g available at http://www.vinartus.net/spa/
34
available for batch mode processing due to the time
required for parsing. Finally, a clue-finder is run to
identify words and phrases from a large subjective
language lexicon.
2.3 Subjectivity Analysis
The subjectivity analysis has four components.
2.3.1 Subjective Sentence Classification
The first component is a Naive Bayes classifier
that distinguishes between subjective and objective
sentences using a variety of lexical and contextual
features (Wiebe and Riloff, 2005; Riloff and Wiebe,
2003). The classifier is trained using subjective and
objective sentences, which are automatically gener-
ated from a large corpus of unannotated data by two
high-precision, rule-based classifiers.
2.3.2 Speech Events and Direct Subjective
Expression Classification
The second component identifies speech events
(e.g., ?said,? ?according to?) and direct subjective
expressions (e.g., ?fears,? ?is happy?). Speech
events include both speaking and writing events.
Direct subjective expressions are words or phrases
where an opinion, emotion, sentiment, etc. is di-
rectly described. A high-precision, rule-based clas-
sifier is used to identify these expressions.
2.3.3 Opinion Source Identification
The third component is a source identifier that
combines a Conditional Random Field sequence
tagging model (Lafferty et al, 2001) and extraction
pattern learning (Riloff, 1996) to identify the sources
of speech events and subjective expressions (Choi
et al, 2005). The source of a speech event is the
speaker; the source of a subjective expression is the
experiencer of the private state. The source identifier
is trained on the MPQA Opinion Corpus3 using a
variety of features. Because the source identifier re-
lies on dependency parse information, it is currently
only available in batch mode.
2.3.4 Sentiment Expression Classification
The final component uses two classifiers to iden-
tify words contained in phrases that express pos-
itive or negative sentiments (Wilson et al, 2005).
3The MPQA Opinion Corpus can be freely obtained at
http://nrrc.mitre.org/NRRC/publications.htm.
The first classifier focuses on identifying sentiment
expressions. The second classifier takes the senti-
ment expressions and identifies those that are pos-
itive and negative. Both classifiers were developed
using BoosTexter (Schapire and Singer, 2000) and
trained on the MPQA Corpus.
3 Related Work
Please see (Wiebe and Riloff, 2005; Choi et al,
2005; Wilson et al, 2005) for discussions of related
work in automatic opinion and sentiment analysis.
4 Acknowledgments
This work was supported by the Advanced Research
and Development Activity (ARDA), by the NSF
under grants IIS-0208028, IIS-0208798 and IIS-
0208985, and by the Xerox Foundation.
References
Y. Choi, C. Cardie, E. Riloff, and S. Patwardhan. 2005. Identi-
fying sources of opinions with conditional random fields and
extraction patterns. In HLT/EMNLP 2005.
M. Collins. 1997. Three generative, lexicalised models for sta-
tistical parsing. In ACL-1997.
J. Lafferty, A. McCallum, and F. Pereira. 2001. Conditional
random fields: Probabilistic models for segmenting and la-
beling sequence data. In ICML-2001.
E. Riloff and W. Phillips. 2004. An Introduction to the Sun-
dance and AutoSlog Systems. Technical Report UUCS-04-
015, School of Computing, University of Utah.
E. Riloff and J. Wiebe. 2003. Learning extraction patterns for
subjective expressions. In EMNLP-2003.
E. Riloff, J. Wiebe, and W. Phillips. 2005. Exploiting sub-
jectivity classification to improve information extraction. In
AAAI-2005.
E. Riloff. 1996. An Empirical Study of Automated Dictionary
Construction for Information Extraction in Three Domains.
Artificial Intelligence, 85:101?134.
R. E. Schapire and Y. Singer. 2000. BoosTexter: A boosting-
based system for text categorization. Machine Learning,
39(2/3):135?168.
J. Wiebe and E. Riloff. 2005. Creating subjective and objec-
tive sentence classifiers from unannotated texts. In CICLing-
2005.
T. Wilson, J. Wiebe, and P. Hoffmann. 2005. Recognizing
contextual polarity in phrase-level sentiment analysis. In
HLT/EMNLP 2005.
F. Xia and M. Palmer. 2001. Converting dependency structures
to phrase structures. In HLT-2001.
35
Weakly Supervised Natural Language Learning Without Redundant Views
Vincent Ng and Claire Cardie
Department of Computer Science
Cornell University
Ithaca, NY 14853-7501
 
yung,cardie  @cs.cornell.edu
Abstract
We investigate single-view algorithms as an al-
ternative to multi-view algorithms for weakly
supervised learning for natural language pro-
cessing tasks without a natural feature split. In
particular, we apply co-training, self-training,
and EM to one such task and find that both self-
training and FS-EM, a new variation of EM that
incorporates feature selection, outperform co-
training and are comparatively less sensitive to
parameter changes.
1 Introduction
Multi-view weakly supervised learning paradigms such
as co-training (Blum and Mitchell, 1998) and co-EM
(Nigam and Ghani, 2000) learn a classification task from
a small set of labeled data and a large pool of unla-
beled data using separate, but redundant, views of the
data (i.e. using disjoint feature subsets to represent the
data). Multi-view learning has been successfully ap-
plied to a number of tasks in natural language processing
(NLP), including text classification (Blum and Mitchell,
1998; Nigam and Ghani, 2000), named entity classifica-
tion (Collins and Singer, 1999), base noun phrase brack-
eting (Pierce and Cardie, 2001), and statistical parsing
(Sarkar, 2001; Steedman et al, 2003).
The theoretical performance guarantees of multi-view
weakly supervised algorithms come with two fairly
strong assumptions on the views. First, each view must
be sufficient to learn the given concept. Second, the views
must be conditionally independent of each other given
the class label. When both conditions are met, Blum and
Mitchell prove that an initial weak learner can be boosted
using unlabeled data.
Unfortunately, finding a set of views that satisfies both
of these conditions is by no means an easy problem. In
addition, recent empirical results by Muslea et al (2002)
and Nigam and Ghani (2000) have shown that multi-view
algorithms are quite sensitive to the two underlying as-
sumptions on the views. Effective view factorization in
multi-view learning paradigms, therefore, remains an im-
portant issue for their successful application. In practice,
views are supplied by users or domain experts, who deter-
mine a natural feature split that is expected to be redun-
dant (i.e. each view is expected to be sufficient to learn
the target concept) and conditionally independent given
the class label.1
We investigate here the application of weakly super-
vised learning algorithms to problems for which no obvi-
ous natural feature split exists and hypothesize that, in
these cases, single-view weakly supervised algorithms
will perform better than their multi-view counterparts.
Motivated, in part, by the results in Mueller et al (2002),
we use the task of noun phrase coreference resolution
for illustration throughout the paper.2 In our experi-
ments, we compare the performance of the Blum and
Mitchell co-training algorithm with that of two com-
monly used single-view algorithms, namely, self-training
and Expectation-Maximization (EM). In comparison to
co-training, self-training achieves substantially superior
performance and is less sensitive to its input parameters.
EM, on the other hand, fails to boost performance, and
we attribute this phenomenon to the presence of redun-
dant features in the underlying generative model. Con-
sequently, we propose a wrapper-based feature selection
method (John et al, 1994) for EM that results in perfor-
mance improvements comparable to that observed with
self-training. Overall, our results suggest that single-view
1Abney (2002) argues that the conditional independence as-
sumption is remarkably strong and is rarely satisfied in real data
sets, showing that a weaker independence assumption suffices.
2Mueller et al (2002) explore a heuristic method for view
factorization for the related problem of anaphora resolution, but
find that co-training shows no performance improvements for
any type of German anaphor except pronouns over a baseline
classifier trained on a small set of labeled data.
                                                               Edmonton, May-June 2003
                                                              Main Papers , pp. 94-101
                                                         Proceedings of HLT-NAACL 2003
weakly supervised learning algorithms are a viable al-
ternative to multi-view algorithms for data sets where a
natural feature split into separate, redundant views is not
available.
The remainder of the paper is organized as follows.
Section 2 presents an overview of the three weakly su-
pervised learning algorithms mentioned previously. In
section 3, we introduce noun phrase coreference resolu-
tion and describe the machine learning framework for the
problem. In section 4, we evaluate the weakly supervised
learning algorithms on the task of coreference resolution.
Section 5 introduces a method for improving the perfor-
mance of weakly supervised EM via feature selection.
We conclude with future work in section 6.
2 Weakly Supervised Algorithms
In this section, we give a high-level description of our im-
plementation of the three weakly supervised algorithms
that we use in our comparison, namely, co-training, self-
training, and EM.
2.1 Co-Training
Co-training (Blum and Mitchell, 1998) is a multi-view
weakly supervised algorithm that trains two classifiers
that can help augment each other?s labeled data using two
separate but redundant views of the data. Each classifier
is trained using one view of the data and predicts the la-
bels for all instances in the data pool, which consists of
a randomly chosen subset of the unlabeled data. Each
then selects its most confident predictions from the pool
and adds the corresponding instances with their predicted
labels to the labeled data while maintaining the class dis-
tribution in the labeled data.
The number of instances to be added to the labeled
data by each classifier at each iteration is limited by a
pre-specified growth size to ensure that only the instances
that have a high probability of being assigned the correct
label are incorporated. The data pool is refilled with in-
stances drawn from the unlabeled data and the process is
repeated for several iterations. During testing, each clas-
sifier makes an independent decision for a test instance
and the decision associated with the higher confidence is
taken to be the final prediction for the instance.
2.2 Self-Training
Self-training is a single-view weakly supervised algo-
rithm that has appeared in various forms in the literature.
The version of the algorithm that we consider here is a
variation of the one presented in Banko and Brill (2001).
Initially, we use bagging (Breiman, 1996) to train a
committee of classifiers using the labeled data. Specifi-
cally, each classifier is trained on a bootstrap sample cre-
ated by randomly sampling instances with replacement
from the labeled data until the size of the bootstrap sam-
ple is equal to that of the labeled data. Then each member
of the committee (or bag) predicts the labels of all unla-
beled data. The algorithm selects an unlabeled instance
for adding to the labeled data if and only if all bags agree
upon its label. This ensures that only the unlabeled in-
stances that have a high probability of being assigned the
correct label will be incorporated into the labeled set. The
above steps are repeated until all unlabeled data is labeled
or a fixed point is reached. Following Breiman (1996),
we perform simple majority voting using the committee
to predict the label of a test instance.
2.3 EM
The use of EM as a single-view weakly supervised clas-
sification algorithm is introduced in Nigam et al (2000).
Like the classic unsupervised EM algorithm (Dempster
et al, 1977), weakly supervised EM assumes a paramet-
ric model of data generation. The labels of the unlabeled
data are treated as missing data. The goal is to find a
model such that the posterior probability of its parame-
ters is locally maximized given both the labeled data and
the unlabeled data.
Initially, the algorithm estimates the model parame-
ters by training a probabilistic classifier on the labeled
instances. Then, in the E-step, all unlabeled data is prob-
abilistically labeled by the classifier. In the M-step, the
parameters of the generative model are re-estimated us-
ing both the initially labeled data and the probabilistically
labeled data to obtain a maximum a posteriori (MAP) hy-
pothesis. The E-step and the M-step are repeated for sev-
eral iterations. The resulting model is then used to make
predictions for the test instances.
3 The Machine Learning Framework for
Coreference Resolution
Noun phrase coreference resolution refers to the problem
of determining which noun phrases (NPs) refer to each
real-world entity mentioned in a document. In this sec-
tion, we give an overview of the coreference resolution
system to which the weakly supervised algorithms de-
scribed in the previous section are applied.
The framework underlying the system is a standard
combination of classification and clustering employed
by supervised learning approaches (e.g. Ng and Cardie
(2002); Soon et al (2001)). Specifically, coreference res-
olution is recast as a classification task, in which a pair
of NPs is classified as co-referring or not based on con-
straints that are learned from an annotated corpus. Train-
ing instances are generated by pairing each NP with each
of its preceding NPs in the document. The classification
associated with a training instance is one of COREFER-
ENT or NOT COREFERENT depending on whether the NPs
Feature Type Feature Description
Lexical PRO STR C if both NPs are pronominal and are the same string; else I.
PN STR C if both NPs are proper names and are the same string; else I.
SOON STR NONPRO C if both NPs are non-pronominal and the string of NP matches that of NP ; else I.
Grammatical PRONOUN 1 Y if NP is a pronoun; else N.
PRONOUN 2 Y if NP is a pronoun; else N.
DEMONSTRATIVE 2 Y if NP starts with a demonstrative such as ?this,? ?that,? ?these,? or ?those;? else N.
BOTH PROPER NOUNS C if both NPs are proper names; NA if exactly one NP is a proper name; else I.
NUMBER C if the NP pair agree in number; I if they disagree; NA if number information for one
or both NPs cannot be determined.
GENDER C if the NP pair agree in gender; I if they disagree; NA if gender information for one or
both NPs cannot be determined.
ANIMACY C if the NPs match in animacy; else I.
APPOSITIVE C if the NPs are in an appositive relationship; else I.
PREDNOM C if the NPs form a predicate nominal construction; else I.
BINDING I if the NPs violate conditions B or C of the Binding Theory; else C.
CONTRAINDICES I if the NPs cannot be co-indexed based on simple heuristics; else C. For instance, two
non-pronominal NPs separated by a preposition cannot be co-indexed.
SPAN I if one NP spans the other; else C.
MAXIMALNP I if both NPs have the same maximal NP projection; else C.
SYNTAX I if the NPs have incompatible values for the BINDING, CONTRAINDICES, SPAN or
MAXIMALNP constraints; else C.
INDEFINITE I if NP is an indefinite and not appositive; else C.
PRONOUN I if NP is a pronoun and NP is not; else C.
EMBEDDED 1 Y if NP is an embedded noun; else N.
TITLE I if one or both of the NPs is a title; else C.
Semantic WNCLASS C if the NPs have the same WordNet semantic class; I if they don?t; NA if the semantic
class information for one or both NPs cannot be determined.
ALIAS C if one NP is an alias of the other; else I.
Positional SENTNUM Distance between the NPs in terms of the number of sentences.
Others PRO RESOLVE C if NP is a pronoun and NP is its antecedent according to a naive pronoun resolution
algorithm; else I.
Table 1: Feature set for the coreference system. The feature set contains relational and non-relational features that are used to
generate an instance representing two NPs, NP and NP , in document 	 , where NP precedes NP . Non-relational features test
some property P of one of the NPs under consideration and take on a value of YES or NO depending on whether P holds. Relational
features test whether some property P holds for the NP pair under consideration and indicate whether the NPs are COMPATIBLE or
INCOMPATIBLE w.r.t. P; a value of NOT APPLICABLE is used when property P does not apply.
co-refer in the text. A separate clustering mechanism then
coordinates the possibly contradictory pairwise classifi-
cations and constructs a partition on the set of NPs.
We perform the experiments in this paper
using our coreference resolution system (see
Ng and Cardie (2002)). For the sake of complete-
ness, we include the descriptions of the 25 features
employed by the system in Table 1. Linguistically,
the features can be divided into five groups: lexical,
grammatical, semantic, positional, and others. However,
we use naive Bayes rather than decision tree induction as
the underlying learning algorithm to train a coreference
classifier, simply because (1) it provides a generative
model assumed by EM and hence facilitates comparison
between different approaches and (2) it is more robust
to the skewed class distributions inherent in coreference
data sets than decision tree learners. When the corefer-
ence system is used within the weakly supervised setting,
a weakly supervised algorithm bootstraps the corefer-
ence classifier from the given labeled and unlabeled data
rather than from a much larger set of labeled instances.
We conclude this section by noting that view factor-
ization is a non-trivial task for coreference resolution.
For many lexical tagging problems such as part-of-speech
tagging, views can be drawn naturally from the left-hand
and right-hand context. For other tasks such as named en-
tity classification, views can be derived from features in-
side and outside the phrase under consideration (Collins
and Singer, 1999). Unfortunately, neither of these op-
tions is possible for coreference resolution. We will ex-
plore several heuristic methods for view factorization in
the next section.
4 Evaluation
In this section, we empirically test our hypothesis that
single-view weakly supervised algorithms can potentially
outperform their multi-view counterparts for problems
without a natural feature split.
4.1 Experimental Setup
To ensure a fair comparison of the weakly supervised
algorithms, the experiments are designed to determine
the best parameter setting of each algorithm (in terms
of its effectiveness to improve performance) for the data
sets we investigate. Specifically, we keep the parame-
ters common to all three weakly supervised algorithms
(i.e. the labeled and unlabeled data) constant and vary the
algorithm-specific parameters, as described below.
Evaluation. We use the MUC-6 (1995) and MUC-7
(1998) coreference data sets for evaluation. The training
set is composed of 30 ?dry run? texts, 1 of which is se-
lected to be the annotated text and the remaining 29 texts
are used as unannotated data. For MUC-6, 3486 training
instances are generated from 84 NPs in the annotated text.
For MUC-7, 3741 training instances are generated from
87 NPs. The unlabeled data is composed of 488173 in-
stances and 478384 instances for the MUC-6 and MUC-7
data sets, respectively. Testing is performed by applying
the bootstrapped coreference classifier and the clustering
algorithm described in section 3 on the 20?30 ?formal
evaluation? texts for each of the MUC-6 and MUC-7 data
sets.
Co-training parameters. The co-training parameters
are set as follows.
Views. We tested three pairs of views. Table 2 re-
produces the 25 features of the coreference system and
shows the views we employ. Specifically, the three view
pairs are generated by the following methods.

 Mueller et al?s heuristic method. Starting from two
empty views, the iterative algorithm selects for each
view the feature whose addition maximizes the per-
formance of the respective view on the labeled data
at each iteration. 3 This method produces the view
pair V1 and V2 in Table 2 for the MUC-6 data set.
A different view pair is produced for MUC-7.

 Random splitting of features into views. Starting
from two empty views, an iterative algorithm that
randomly chooses a feature for each view at each
step is used to split the feature set. The resulting
view pair V3 and V4 is used for both the MUC-6
and MUC-7 data sets.

 Splitting of features according to the feature
type. Specifically, one view comprises the lexico-
syntactic features and the other the remaining ones.
This approach produces the view pair V5 and V6,
which is used for both data sets.
Pool size. We tested pool sizes of 500, 1000, 5000.
Growth size. We tested values of 10, 50, 100, 200, 250.
3Space limitation precludes a detailed description of this
method. See Mueller et al (2002) for details.
Feature V1 V2 V3 V4 V5 V6
PRO STR X X X
PN STR X X X
SOON STR NONPRO X X X
PRONOUN 1 X X X
PRONOUN 2 X X X
DEMONSTRATIVE 2 X X X
BOTH PROPER NOUNS X X X
NUMBER X X X
GENDER X X X
ANIMACY X X X
APPOSITIVE X X X
PREDNOM X X X
BINDING X X X
CONTRAINDICES X X X
SPAN X X X
MAXIMALNP X X X
SYNTAX X X X
INDEFINITE X X X
PRONOUN X X X
EMBEDDED 1 X X X
TITLE X X X
WNCLASS X X X
ALIAS X X X
SENTNUM X X X
PRO RESOLVE X X X
Table 2: Co-training view pairs employed by the corefer-
ence system. Column 1 lists the 25 features shown in Table 1.
Columns 2-7 show three different pairs of views that we have
attempted for co-training coreference classifiers.
Number of co-training iterations. We monitored per-
formance on the test data at every 10 iterations of co-
training and ran the algorithm until performance stabi-
lized.
Self-training parameters. Given the labeled and unla-
beled data, self-training requires only the specification of
the number of bags. We tested all odd number of bags
between 1 and 25.
EM parameters. Given the labeled and unlabeled data,
EM has only one parameter ? the number of iterations.
We ran EM to convergence and kept track of its test set
performance at every iteration.
4.2 Results and Discussion
Results are shown in Table 3, where performance is re-
ported in terms of recall, precision, and F-measure using
the model-theoretic MUC scoring program (Vilain et al,
1995). The baseline coreference system, which is trained
only on the labeled document using naive Bayes, achieves
an F-measure of 55.5 and 43.8 on the MUC-6 and MUC-
7 data sets, respectively.
The results shown in row 2 of Table 3 correspond to
the best F-measure scores achieved by co-training for the
two data sets based on co-training runs that comprise all
of the parameter combinations described in the previous
subsection. The parameter settings with which the best
Experiments MUC-6 MUC-7
Best Parameter Setting R P F Best Parameter Setting R P F
Baseline ? 58.3 52.9 55.5 ? 52.8 37.4 43.8
Co-Training v=V5/V6,g=50,p=5000,i=220 47.5 81.9 60.1 v=V5/V6,g=100,p=500,i=260 40.6 77.6 53.3
Self-Training b=7 54.1 78.6 64.1 b=9 54.6 62.6 58.3
EM i=20 64.8 51.8 57.6 i=2 54.1 40.7 46.4
FS-EM ? 64.2 66.6 65.4 ? 53.3 70.3 60.5
Table 3: Comparative results of co-training, self-training, EM, and FS-EM (to be described in section 5). Recall,
Precision, and F-measure are provided. For co-training, self-training, and EM, the best results (F-measure) achieved by the algo-
rithms and the corresponding parameter settings (with views v, growth size g, pool size p, number of iterations i, and number of
bags b) are shown.
0 100 200 300 400 500 600 700 800 900 1000
30
40
50
60
70
80
90
100
Number of Co?Training Iterations
Sc
or
e
Baseline
Recall
Precision
F?measure
Figure 1: Learning curve for co-training (pool size =
5000, growth size = 50) for the MUC-6 data set.
results are obtained are also shown in the table. To get a
better picture of the behavior of co-training, we present
the learning curve for the co-training run that gives rise
to the best F-measure for the MUC-6 data set in Figure 1.
The horizontal (dotted) line shows the performance of the
baseline system, which achieves an F-measure of 55.5, as
described above. As co-training progresses, F-measure
peaks at iteration 220 and then gradually drops below that
of the baseline after iteration 570.
Although co-training produces substantial improve-
ments over the baseline at its best parameter settings, a
closer examination of our results reveals that they cor-
roborate previous findings: the algorithm is sensitive not
only to the number of iterations, but to other input pa-
rameters such as the pool size and the growth size as well
(Nigam and Ghani, 2000; Pierce and Cardie, 2001). The
lack of a principled method for determining these param-
eters in a weakly supervised setting where labeled data is
scarce remains a serious disadvantage for co-training.
Self-training results are shown in row 3 of Table 3:
self-training performs substantially better than both the
baseline and co-training for both data sets. In contrast
to co-training, however, self-training is relatively insensi-
1 3 5 7 9 11 13 15 17 19 21 23 25
50
55
60
65
70
75
80
Number of Bags
Sc
or
e
Baseline
Recall
Precision
F?measure
Figure 2: Effect of the number of bags on the perfor-
mance of self-training for the MUC-6 data set.
tive to its input parameter. Figure 2 shows the fairly con-
sistent performance of self-training with seven or more
bags for the MUC-6 data set. We observe similar trends
for the MUC-7 data set. These results are consistent with
empirical studies of bagging across a variety of classifi-
cation tasks where seven to 25 bags are deemed sufficient
(Breiman, 1996).
To gain a deeper insight into the behavior of self-
training, we plot the learning curve for self-training using
7 bags in Figure 3, again for the MUC-6 data set. At itera-
tion 0 (i.e. before any unlabeled data is incorporated), the
F-measure score achieved by self-training is higher than
that of the baseline system (58.5 vs. 55.5). The observed
difference is due to voting within the self-training algo-
rithm. Voting has proved to be an effective technique for
improving the accuracy of a classifier when training data
is scarce by reducing the variance of a particular training
corpus (Breiman, 1996). After the first iteration, there
is a rapid increase in F-measure, which is accompanied
by large gains in precision and smaller drops in recall.
These results are consistent with our intuition regarding
self-training: at each iteration the algorithm incorporates
only instances whose label it is most confident about into
0 1 2 3
50
55
60
65
70
75
80
Number of Self?Training Iterations
Sc
or
e
Baseline
Recall
Precision
F?measure
Figure 3: Learning curve for self-training using 7 bags
for the MUC-6 data set.
the labeled data, thereby ensuring that precision will in-
crease. 4
As we can see from Table 3, the recall level achieved
by co-training is much lower than that of self-training.
This is an indication that each co-training view is insuf-
ficient to learn the concept: the feature split limits any
interaction of features in different views that might pro-
duce better recall. Overall, these results provide evidence
that self-training is a better alternative to co-training for
weakly supervised learning for problems such as corefer-
ence resolution where no natural feature split exists.
On the other hand, EM only gives rise to modest per-
formance gains over the baseline system, as we can see
from row 4 of Table 3. The performance of EM depends
in part on the correctness of the underlying generative
model (Nigam et al, 2000), which in our case is naive
Bayes. In this model, an instance with  feature values

,  ,

and class  is created by first choosing
the class with prior probability ffProceedings of NAACL HLT 2007, pages 65?72,
Rochester, NY, April 2007. c?2007 Association for Computational Linguistics
Structured Local Training and Biased Potential Functions for Conditional
Random Fields with Application to Coreference Resolution
Yejin Choi and Claire Cardie
Department of Computer Science
Cornell University
Ithaca, NY 14853
{ychoi,cardie}@cs.cornell.edu
Abstract
Conditional Random Fields (CRFs) have shown
great success for problems involving structured out-
put variables. However, for many real-world NLP
applications, exact maximum-likelihood training is
intractable because computing the global normal-
ization factor even approximately can be extremely
hard. In addition, optimizing likelihood often does
not correlate with maximizing task-specific evalu-
ation measures. In this paper, we present a novel
training procedure, structured local training, that
maximizes likelihood while exploiting the benefits
of global inference during training: hidden vari-
ables are used to capture interactions between lo-
cal inference and global inference. Furthermore,
we introduce biased potential functions that empir-
ically drive CRFs towards performance improve-
ments w.r.t. the preferred evaluation measure for
the learning task. We report promising experimen-
tal results on two coreference data sets using two
task-specific evaluation measures.
1 Introduction
Undirected graphical models such as Conditional
Random Fields (CRFs) (Lafferty et al, 2001) have
shown great success for problems involving struc-
tured output variables (e.g. Wellner et al (2004),
Finkel et al (2005)). For many real-world NLP ap-
plications, however, the required graph structure can
be very complex, and computing the global normal-
ization factor even approximately can be extremely
hard. Previous approaches for training CRFs have
either (1) opted for a training method that no longer
maximizes the likelihood, (e.g. McCallum and Well-
ner (2004), Roth and Yih (2005)) 1, or (2) opted for a
1Both McCallum and Wellner (2004) and Roth and Yih
(2005) used the voted perceptron algorithm (Collins, 2002) to
train intractable CRFs.
simplified graph structure to avoid intractable global
normalization (e.g. Roth and Yih (2005), Wellner et
al. (2004)).
Solutions of the first type replace the computation
of the global normalization factor
?
y p(y|x) with
argmaxy p(y|x) during training, since finding an
argmax of a probability distribution is often an eas-
ier problem than finding the entire probability distri-
bution. Training via the voted perceptron algorithm
(Collins, 2002) or using a max-margin criterion also
correspond to the first option (e.g. McCallum and
Wellner (2004), Finley and Joachims (2005)). But
without the global normalization, the maximum-
likelihood criterion motivated by the maximum en-
tropy principle (Berger et al, 1996) is no longer a
feasible option as an optimization criterion.
The second solution simplifies the graph struc-
ture for training, and applies complex global infer-
ence only for testing. In spite of the discrepancy
between the training model and the testing model,
it has been empirically shown that (1) performing
global inference only during testing can improve
performance (e.g. Finkel et al (2005), Roth and Yih
(2005)), and (2) full-blown global training can of-
ten perform worse due to insufficient training data
(e.g. Punyakanok et al (2005)). Importantly, how-
ever, attempts to reduce the discrepancy between the
training and test models ? by judiciously adding the
effect of global inference to the training ? have pro-
duced substantial performance improvements over
locally trained models (e.g. Cohen and Carvalho
(2005), Sutton and McCallum (2005a)).
In this paper, we present structured local training,
a novel training procedure for maximum-likelihood
65
training of undirected graphical models, such as
CRFs. The procedure maximizes likelihood while
exploiting the benefits of global inference during
training by capturing the interactions between local
inference and global inference via hidden variables.
Furthermore, we introduce biased potential func-
tions that redefine the likelihood for CRFs so that
the performance of CRFs trained under the max-
imum likelihood criterion correlates better empiri-
cally with the preferred evaluation measures such as
F-score and MUC-score.
We focus on the problem of coreference resolu-
tion; however, our approaches are general and can
be extended to other NLP applications with struc-
tured output. Our approaches also extend to non-
conditional graphical models such as Markov Ran-
dom Fields. In experiments on two coreference data
sets, structured local training reduces the error rate
significantly (3.5%) for one coreference data set and
minimally (? 1%) for the other. Experiments using
biased potential functions increase recall uniformly
and significantly for both data sets and both task-
specific evaluation measures. Results for the com-
bination of the two techniques are promising, but
mixed: pairwise F1 increases by 0.8-5.5% for both
data sets; MUC F1 increases by 3.5% for one data
set, but slightly hurts performance for the second
data set.
In ?2, we describe structured local training, and
follow with experimental results in ?3. In ?4, we
describe biased potential functions and follow with
experimental results in ?5. We discuss related work
in ?6.
2 Structured Local Training
2.1 Definitions
For clarity, we define the following terms that we
will use throughout the paper.
? local inference: 2 Inference factored into smaller
independent pieces, without considering the
structure of the output space.
? global inference: Inference applied on the entire
set of output variables, considering the structure
of the output space.
2In this paper, inference refers to the operation of finding the
argmax in particular.
? local training: Training that does not invoke
global inference at each iteration.
? global training: Training that does invoke global
inference at each iteration.
2.2 A Motivating Example for Coreference
Resolution
In this section, we present an example of the coref-
erence resolution problem to motivate our approach.
It has been shown that global inference-based train-
ing for coreference resolution outperforms training
with local inference only (e.g. Finley and Joachims
(2005), McCallum and Wellner (2004)). In particu-
lar, the output of coreference resolution must obey
equivalence relations, and exploiting such structural
constraints on the output space during training can
improve performance. Consider the coreference res-
olution task for the following text.
It was after the passage of this act, that Mary(1)?s attitude
towards Elizabeth(1) became overtly hostile. The deliber-
ations surrounding the act seem to have revived all Mary?s
memories of the humiliations she had suffered at the
hands of Anne Boleyn. At the same time, Elizabeth(2)?s
continuing prevarications over religion confirmed that she
was indeed her mother?s daughter.
In the above text, the ?she? in the last sen-
tence is coreferent with both mentions of
?Elizabeth?. However, when we consider
?she? and ?Elizabeth(1)? in isolation from the
remaining coreference chain, it can be difficult for
a machine learning method to determine whether
the pair is coreferent or not. Indeed, such a
pair may not look very different from the pair
?she? and ?Mary(1)? in terms of feature vectors.
It is much easier, however, to determine that
?she? and ?Elizabeth(2)? are coreferent, or that
?Elizabeth(1)? and ?Elizabeth(2)? are coreferent.
Only by taking the transitive closure of these pair-
wise coreference relations does it become clear that
?she? and ?Elizabeth(1)? are coreferent. In other
words, global training might handle potentially
confusing coreference cases better because it allows
parameter learning (for each pairwise coreference
decision) to be informed by global inference.
We argue that, with appropriate modification to
the learning instances, local training is adequate for
the coreference resolution task. Specifically, we pro-
pose that confusing pairs in the training data ? such
66
as ?she? and ?Elizabeth(1)? ? be learned as not-
coreferent, so long as the global inference step can
fix this error by exploiting the structure of the out-
put space, i.e. by exploiting the equivalence rela-
tions. This is the key idea of structured local train-
ing, which we elaborate formally in the following
section.
2.3 A Hidden-Variable Model
In this section, we present a general description of
structured local training. Let y be a vector of out-
put variables for structured output, and let x be a
vector of input variables. In order to capture the in-
teractions between global inference and local infer-
ence, we introduce hidden variables h, |h| = |y|,
so that the global inference for p(y, h|x) can be fac-
tored into two components using the product rule, as
follows:
p(y, h|x) = p(y|h, x) p(h|x)
= p(y|h) p(h|x)
The second component p(h|x) on the right hand side
corresponds to the local model, for which the infer-
ence factorizes into smaller independent pieces, e.g.
argmaxhp(h|x) = {argmaxh
i
?(h
i
, x)}. And the
first component p(y|h, x) on the right hand side cor-
responds to the global model, whose inference may
not factorize nicely. Further, we assume that y is in-
dependent of x given h, so that p(y|h, x) = p(y|h).
That is to say, h captures sufficient information from
x, so that given h, global inference of y only de-
pends on h. The quantity of p(y|x) then is given by
marginalizing out h as follows:
p(y|x) =
?
h
p(y, h|x)
Intuitively, the hidden variables h represent the lo-
cal decisions that can lead to a good y after global
inference is applied. In the case of coreference reso-
lution, one natural factorization would be that global
inference is a clustering algorithm, and local infer-
ence is a classification decision on each pair of noun
phrases (or mentions).3 In this paper, we assume
3Formally, we define each y
i
? y to be the coreference de-
cision for the ith pair of mentions, and x
i
? x be the input
regarding the ith pair of mentions. Then h
i
corresponds to the
local coreference decision that can lead to a good coreference
decision y
i
after the clustering algorithm has been applied.
that we only parameterize the local model p(h|x),
although it would be possible to extend the parame-
terization to the global model as well, depending on
the particular application under consideration. The
similarity between a pair of mentions is parameter-
ized via log-linear models. However, once we have
the similarity scores extracted via local inference,
the clustering algorithm does not require further pa-
rameterization.
For training, we apply the standard Expectation-
Maximization (EM) algorithm (Dempster et al,
1977) as follows:
? E Step: Compute a distribution
?P (t) = P (h|y, x, ?(t?1))
? M Step: Set ?(t) to ? that maximizes
E
?
P
(t)
[logP (y, h|x, ?)]
By repeatedly applying the above two steps for
t = 1, 2, ..., the value of ? converges to the local
maxima of the conditional log likelihood L(?) =
logP (y|x, ?).
2.4 Application to Coreference Resolution
For y
i
? y (and h
i
? h) in the coreference resolution
task, y
i
= 1 (and h
i
= 1) corresponds to ith pair of
mentions being coreferent, and y
i
= 0 (and h
i
= 0)
corresponds to ith pair being not coreferent.
[Local Model P (h|x)] For the local model, we de-
fine cliques as individual nodes,4 and parameterize
each clique potential as
?(h
i
, x) = ?(h
i
, x
i
) = exp
?
k
?
k
f
k
(h
i
, x
i
)
Let ?(h|x) ?
?
i
?(h
i
, x
i
). Then,
P (h|x) =
?(h, x)
?
h ?(h, x)
Notice that in this model, finding argmaxhP (h|x)
corresponds to simply finding argmax
h
i
?(h
i
, x
i
) in-
dependently for each h
i
? h.
4Each node in the graphical representation of CRFs corre-
sponds to the coreferent decision for each pair of mentions. This
corresponds to the ?Model 3? of McCallum and Wellner (2004).
67
ALGORITHM-1
INPUT: x, true labeling y?, current local model P (h|x)
GOAL: Find the highest confidence labeling y?
such that y? = single-link-clustering(y?)
h? ? argmaxhP (h|x)
h? ? single-link-clustering(h?)
construct a graph G = (V, E), where
E = {h?
i
: h?
i
? h? s.t. y?
i
= 1}
V = {v : v is a NP referred by a h?
i
? E}
with edge cost cost
h
?
i
= ?(h?
i
, x
i
) if h?
i
6= y?
i
with edge cost cost
h
?
i
= 0 if h?
i
= y?
i
find a minimum spanning tree(or forest) M of G
for each h?
i
? h?
if h?
i
= y?
i
y?
i
? h?
i
else if h?
i
? M
y?
i
? 1
else
y?
i
? 0
end for
return y?
Figure 1: Algorithm to find the highest confidence labeling y?
that can be clustered to the true labeling y?
[Global Model P (y|h)] For the global model, we
assume a deterministic clustering algorithm is given.
In particular, we focus on single-link clustering, as it
has been shown to be effective for coreference reso-
lution (e.g. Ng and Cardie (2002)). With single-link
clustering, P (y|h) = 1 if h can be clustered to y,
and P (y|h) = 0 if h cannot be clustered to y.5
[Computation of the E-step] The E-step requires
computation of the distribution of P (h|y, x, ?(t?1)),
which we will simply denote as P (h|y, x), since all
our distributions are implicitly conditioned on the
model parameters ?.
P (h|y, x) =
P (h, y|x)
P (y|x)
? P (y|h) P (h|x)
Notice that when computing P (h|y, x), the denomi-
nator P (y|x) stays as a constant for different values
of h. The E-step requires enumeration of all possible
values of h, but it is intractable with our formulation,
because inference for the global model P (y|h) does
not factor out nicely. Therefore, we must resort to an
5Single-link clustering simply takes the transitive closure,
and does not consider the distance metric. In a pilot study, we
also tried a variant of a stochastic clustering algorithm that takes
into account the distance metric (set as the probabilities from
the local model) for the global model, but the performance was
worse.
ALGORITHM-2
INPUT: x, true labeling y?, current local model P (h|x)
GOAL: Find a high confidence labeling y? that is
close to the true labeling y?
h? ? argmaxhP (h|x)
h? ? single-link-clustering(h?)
for each h?
i
? h?
if h?
i
= y?
i
y?
i
? h?
i
else
y?
i
? y?
i
end for
return y?
Figure 2: Algorithm to find a high confidence labeling y? that
is close to the true labeling y?
approximation method. Neal and Hinton (1998) an-
alyze and motivate various approximate EM training
methods. One popular choice in practice is called
?Viterbi training?, a variant of the EM algorithm,
which has been shown effective in many NLP ap-
plications. Viterbi training approximates the distri-
bution by assigning all probability mass to a single
best assignment. The algorithm for this is shown in
Figure 1.
We propose another approximation option for the
E-step that is given by Figure 2. Intuitively, when
the current local model misses positive coreference
decisions, the first algorithm constructs a y? that is
closest to h? for single-link clustering to recover the
true labeling y?, while the second algorithm con-
structs a y? that is closer to y? by preserving all of
the missing positive coreference decisions. 6
[Computation of M-step] Because P (y|h) is not
parameterized, finding argmax
?
P (y, h|x) reduces
to finding argmax
?
P (h|x), which is standard CRF
training. In order to speed up the training, we start
convex optimization for CRFs using the parame-
ter values ?(t?1) from the previous M-step. For
the very first iteration of EM, we start by setting
P (y?|x) = 1 for E-step, so that the first M-step will
finds argmax
?
P (y?|x).
6In a pilot study, we found that ALGORITHM-2 per-
forms slightly better than ALGORITHM-1. We also tried two
other approximation options, but none performed as well as
ALGORITHM-2. One of them removes the confusing sub-
instances and has the effect of setting a uniform distribution on
those sub-instances. The other computes the actual distribution
on a subset of sub-instances. For brevity, we only present ex-
perimental results using ALGORITHM-2 in this paper.
68
[Inference on the test data] It is intractable to
marginalize out h from P (y, h|x). Therefore, sim-
ilar to the Viterbi-training in the E-step, we approx-
imate the distribution of h by argmaxhP (h|X).
3 Experiments?I
Data set: We evaluate our approach with two
coreference data sets: MUC6 (MUC-6, 1995) and
MPQA7(Wiebe et al, 2005). For the MUC6 data set,
we extract noun phrases (mentions) automatically,
but for MPQA, we assume mentions for corefer-
ence resolution are given as in Stoyanov and Cardie
(2006). For MUC6, we use the standard training/test
data split. For MPQA, we use 150 documents for
training, and 50 documents for testing.
Configuration: We follow Ng and Cardie (2002)
for feature vector construction for each pair of men-
tions,8 and Finley and Joachims (2005) for con-
structing a training/testing instance for each docu-
ment: a training/testing instance consists of all pairs
of mentions in a document. Then, a single pair of
mentions is a sub-instance. We use the Mallet9 im-
plementation of CRFs, and set a Gaussian prior of
1.0 for all experiments. At each M-step, we train
CRFs starting from the parameters from the previous
M-step. We train CRFs up to 200 iterations, but be-
cause we start training CRFs from the previous pa-
rameters, the convergence from the second M-step
becomes much faster. We apply up to 5 EM itera-
tions, and choose best performing ?(t), 2 ? t ? 5
based on the performance on the training data.10
Hypothesis: For the baseline (BASE) we employ
the locally trained model for pairwise decisions
without global inference. Clustering is applied only
at test time, in order to make the assignment on the
output variables coherent. We hypothesize that for
the baseline, maximizing the likelihood for training
will correlate more with the pairwise accuracy of the
7Available at http://nrrc.mitre.org/NRRC/publications.htm.
8In particular, our feature set corresponds to ?All Features?
in Ng and Cardie (2002), and we discretized numeric values.
9Available at http://mallet.cs.umass.edu.
10Selecting ?(t) on a separate tuning data would be better, but
the data for MUC6 in particular is very limited. Notice that we
don?t pick ?1 when reporting the performance of SLT, because
it is identical to the baseline.
MUC6
after clustering before clustering
e % R % P % F % e % R % P % F %
BASE 1.50 59.2 56.2 57.7 1.18 38.0 85.6 52.6
SLT 1.28 49.8 67.3 57.2 1.35 26.4 84.3 40.2
MPQA
after clustering before clustering
e % R % P % F % e % R % P % F %
BASE 9.83 75.8 57.0 65.1 7.05 52.1 83.4 64.1
SLT 6.39 62.1 80.6 70.2 7.39 43.7 90.1 58.9
Table 1: Performance of Structured Local Training: SLT re-
duces error rate (e %) after applying single-link clustering.
incoherent decisions before clustering than the pair-
wise accuracy of the coherent decisions after cluster-
ing. We also hypothesize that by performing struc-
tured local training (SLT), maximizing the likeli-
hood will correlate more with the pairwise accuracy
after clustering.
Results: Experimental results are shown in Ta-
ble 1. We report error rate (error rate = 100 ?
accuracy) on the pairwise decisions (e %), and F1-
score (F %) on the coreferent pairs.11 For compar-
ison, we show numbers from both after and before
single-link clustering is applied. As hypothesized,
the error rate of BASE increases after clustering,
while the error rate of SLT decreases after cluster-
ing. Moreover, the error rate of SLT is considerably
lower than that of BASE after clustering. However,
the F1-score does not correlate with the error rate.
That is, a lower error rate does not always lead to a
higher F1-score, which motivates the Biased Poten-
tial Functions that we introduce in the next section.
Notice that when we compare the precision/recall
breakdown after clustering, SLT has higher precision
and lower recall than BASE.
4 Biased Potential Functions
We introduce biased potential functions for train-
ing CRFs to empirically favor preferred evaluation
measures for the learning task, such as F-score and
MUC-score that have been considered hard for tradi-
11Error rate and F1-score on the coreferent pairs are not ideal
measures for the quality of clustering, however, we show them
here in order to contrast the effect of SLT. We present MUC-
scores for the same experimental settings in Table 3.
69
tional likelihood-based methods to optimize for. In-
tuitively, biased potential functions emphasize those
sub-components of an instance that can be of greater
importance than the rest of an instance.
4.1 Definitions
The conditional probability of P (y|x)12 for CRFs is
given by (Lafferty et al, 2001)
P (y|x) =
?
i
?(C
i
, x)
?
y
?
i
?(C
i
, x)
where ?(C
i
, x) is a potential function defined over
each clique C
i
. Potential functions are typically pa-
rameterized in an exponential form as follows.
?(C
i
, x) = exp
?
k
?
k
f
k
(C
i
, x)
where ?
k
are the parameters and f
k
(?) are fea-
ture indicator functions. Because the Hammersley-
Clifford theorem (1971) for undirected graphical
models holds for any non-negative potential func-
tions, we propose alternative potential functions as
follows.
?(C
i
, x) =
{
??(C
i
, x) if ?(C
i
, x) = true
?(C
i
, x) otherwise
where ? is a non-negative bias factor, and ?(C
i
, x)
is a predicate (or an indicator function) to check cer-
tain properties on (C
i
, x).13 Examples of possible
?(?) would be whether the true assignment for C
i
in the training data contains certain class values, or
whether the current observation indexed by C
i
has
particular characteristics. More specific details will
be given in ?4.2.
Training and testing with biased potential func-
tions is mostly identical to the traditional log-linear
formulations by ?(?) as defined above, except for
small and straightforward modifications to the com-
putation of the likelihood and the derivative of the
likelihood.
12For the local model described in Section 2, y should be
replaced with h. We use y in this section however, as it is a
more conventional notation in general.
13In our problem formulation, cliques are individual nodes,
and potential functions are defined over the observations in-
dexed by the current i only: i.e. ?(C
i
, x) = ?(y
i
, x
i
),
?(C
i
, x) = ?(y
i
, x
i
) and ?(C
i
, x) = ?(y
i
, x
i
).
The key idea for biased potential functions is
nothing new, as it is conceptually similar to in-
stance weighting for problems with non-structured
output (e.g. Aha and Goldstone (1992), Cardie et al
(1997)). However, biased potential functions differ
technically in that they emphasize desired subcom-
ponents without altering the i.i.d. assumption, and
still weight each instance alike. Despite the con-
ceptual simplicity, we are not aware of any previ-
ous work that explored biased potential functions for
problems with structured output.
4.2 Applications to Coreference Resolution
[Bias on Coreferent Pairs] For coreference res-
olution, pairs that are coreferent are in a minority
class14, and biased potential functions can mitigate
this skewed data problem, by amplifying the clique
potentials that correspond to coreferent pairs. We
define ?(y
i
, x
i
) to be true if and only if the true as-
signment for y
i
in the training data is ?coreferent?.
Notice that ?(?) does not depend on what particu-
lar value y
i
might take, but only depends on the true
value of y
i
in the training data. For testing, ?(y
i
, x
i
)
will be always false.15
[Bias on Closer Coreferent Pairs] For corefer-
ence resolution, we hypothesize that coreferent pairs
for closer mentions have more significance, because
they tend to have clearer linguistic clues to deter-
mine coreference. We further hypothesize that by
emphasizing only close coreferent pairs, we can
have our model favor the MUC score. For this, we
define ?(y
i
, x
i
) to be true if and only if x
i
is for a
pair of mentions that are the closest coreferent pair.
5 Experiments?II
Data sets and configurations for experiments are
identical to those used in ?3.
Hypothesis: We hypothesize that using biased po-
tential functions, maximizing the likelihood for
training can correlate better with F1-score or MUC-
score than the pairwise accuracy. In particular,
14Only 1.72% of the pairs are coreferent in the MUC6 data,
and about 12% are coreferent in the MPQA data.
15Notice that ?(y
i
, x
i
) changes the surface of the likelihood
for training, but does not affect the inference of finding the
argmax in our local model. That is, argmax
y
i
?(y
i
, x
i
) =
argmax
y
i
?(y
i
, x
i
) (with y
i
replaced with h
i
).
70
MUC6
pairwise MUC
e % R % P % F % R % P % F %
BASE 1.18 38.0 85.6 52.6 59.0 75.8 66.4
BASIC-P11.5 1.20 38.9 82.1 52.8 64.2 71.8 67.8
BASIC-P13.0 1.32 46.9 71.3 56.6 68.9 64.3 66.5
BASIC-Pa1.5 1.15 44.2 79.9 56.9 62.1 68.7 65.2
BASIC-Pa3.0 1.44 52.5 62.9 57.2 70.9 60.5 65.3
MPQA
pairwise MUC
e % R % P % F % R % P % F %
BASE 7.05 52.1 83.4 64.1 75.6 81.5 78.4
BASIC-P11.5 7.18 54.6 79.6 64.8 77.7 76.5 77.1
BASIC-P13.0 7.22 59.9 75.4 66.8 83.3 71.7 77.1
BASIC-Pa1.5 7.65 59.7 72.2 65.4 79.8 73.2 76.4
BASIC-Pa3.0 8.22 69.2 65.1 67.1 85.8 67.8 75.7
Table 2: Performance of Biased Potential Functions: pairwise
scores are taken before single-link-clustering is applied.
we hypothesize that biasing on every coreferent
pair will correlate more with F1-score, and bias-
ing on close coreferent pairs will correlate more
with MUC-score. In general, we expect that bias-
ing on coreferent pairs will boost recall, potentially
decreasing precision.
Results [BPF]: Experimental results for biased
potential functions, without structured local train-
ing, are shown in Table 2. BASIC-P1? denotes local
training with biased potential on the closest corefer-
ent pairs with bias factor ?, and BASIC-Pa? denotes
local training with biased potential on the all coref-
erent pairs with bias factor ?, where ? = 1.5 or 3.0.
For brevity, we only show pairwise numbers before
applying single-link-clustering.16 As hypothesized,
biased potential functions in general boost recall at
the cost of precision. Also, for a fixed value of
?, BASIC-P1? gives better MUC-F1 than BASIC-
Pa? , and BASIC-Pa? gives better pairwise-F1 than
BASIC-P1? for both data sets.
Results [SLT+BPF]: Experimental results that
combine SLT and BPF are shown in Table 3. Sim-
ilarly as before, SLT-Px? denotes SLT with biased
potential scheme Px, with bias factor ?. For brevity,
16This is because we showed in ?3 that basic local training
does not correlate well with pairwise scores after clustering, and
in order to see the direct effect of biased potential functions, we
examine pairwise numbers before clustering.
MUC6
pairwise MUC
e % R % P % F % R % P % F %
BASE 1.50 59.2 56.2 57.7 59.0 75.8 66.4
SLT 1.28 49.8 67.3 57.2 56.3 77.8 65.3
SLT-P11.5 1.19 52.8 70.6 60.4 59.3 74.6 66.1
SLT-P13.0 1.42 63.5 57.9 60.6 67.5 70.7 69.1
SLT-Pa1.5 1.43 58.6 58.5 58.5* 64.0 73.6 68.5
SLT-Pa3.0 1.71 65.2 50.3 56.8 70.5 69.3 69.9*
MPQA
pairwise MUC
e % R % P % F % R % P % F %
BASE 9.83 75.8 57.0 65.1 75.6 81.5 78.4
SLT 6.39 62.1 80.6 70.2 69.1 88.2 77.5
SLT-P11.5 6.54 64.9 77.4 70.6* 72.2 84.5 77.9*
SLT-P13.0 9.09 77.2 59.6 67.3 78.4 79.5 78.9
SLT-Pa1.5 6.74 65.2 75.7 70.1 72.4 87.2 79.1
SLT-Pa3.0 14.71 78.2 43.9 56.2 80.5 73.8 77.0
Table 3: Performance of Biased Potential Functions with
Structured Local Training: All numbers are taken after single-
link clustering.
we only show numbers after applying single-link-
clustering. Unlike the results shown in Table 2,
for a fixed value of ?, SLT-P1? correlates better
with pairwise-F1, and SLT-Pa? correlates better with
MUC-F1. This indicates that when biased poten-
tial functions are used in conjunction with SLT, the
effect of biased potential functions can be different
from the case without SLT. Comparing F1-scores in
Table 2 and Table 3, we see that the combination of
biased potential functions with SLT improves per-
formance in general. In particular, SLT-P13.0 and
SLT-Pa1.5 consistently improve performance over
BASE on both data sets, for both pairwise-F1 and
MUC-F1. We present performance scores for all
variations of configurations for reference, but we
also mark the particular configuration SLT-Px? (by
?*? on F1-scores) that is chosen when selecting the
configuration based on the performance on the train-
ing data for each performance measure. To con-
clude, structured local training with biased poten-
tial functions bring a substantial improvement for
MUC-F1 score, from 66.4% to 69.9% for MUC6
data set. For pairwise-F1, the performance increase
from 57.7% to 58.5% for MUC6, and from 65.1% to
70.6% for MPQA.17
17Performance on the MPQA data for MUC-F1 is slightly
decreased from 78.4% to 77.9%. Note the MUC scores for the
71
6 Related Work
Structured local training is motivated by recent re-
search that has shown that reducing the discrep-
ancy between the training model and testing model
can improve the performance without incurring the
heavy computational overhead of full-blown global
inference-based training. 18 (e.g. Cohen and Car-
valho (2005), Sutton and McCallum (2005a), Sutton
and McCallum (2005b)). Our work differs in that
(1) we use hidden variables to capture the interac-
tions between local inference and global inference,
(2) we present an application to coreference resolu-
tion, while previous work has shown applications for
variants of sequence tagging. McCallum and Well-
ner (2004) showed a global training approach with
CRFs for coreference resolution, but they used the
voted perceptron algorithm for training, which no
longer maximizes the likelihood. In addition, they
assume that all and only those noun phrases involved
in coreference resolution are given.
The performance of our system on MUC6 data
set is comparable to previously reported systems.
Using the same feature set, Ng and Cardie (2002)
reports 64.5% of MUC-score, while our system
achieved 69.9%. Ng and Cardie (2002) reports
70.4% of MUC-score using hand-selected features.
With an additional feature selection or feature induc-
tion step, the performance of our system might fur-
ther improve. McCallum and Wellner (2004) reports
73.42% of MUC-score on MUC6 data set, but their
experiments assumed perfect identification of all and
only those noun phrases involved in a coreference
relation, thus substantially simplifying the task.
7 Conclusion
We present a novel training procedure, structured
local training, that maximizes likelihood while
exploiting the benefits of global inference during
training. This is achieved by incorporating hidden
variables to capture the interactions between local
MPQA baseline are already quite high to begin with.
18The computational cost for SLT in our experiments were
about twice of the cost for the local training of the baseline. This
is the case because M-step converges very fast from the second
EM iteration, by initializing CRFs using parameters from the
previous M-step. Biased potential functions hardly adds extra
computational cost. In practice, BPFs reduce training time sub-
stantially: we observed that the higher the bias is, the quicker
CRFs converge.
inference and global inference. In addition, we
introduce biased potential functions that allow
CRFs to empirically favor performance measures
such as F1-score or MUC-score. We focused on the
application of coreference resolution in this paper,
but the key ideas of our approaches can be extended
to other applications, and other machine learning
techniques motivated by Markov networks.
Acknowledgments We thank the reviewers as well
as Eric Breck and Ves Stoyanov for their many helpful com-
ments. This work was supported by the Advanced Research and
Development Activity (ARDA), by NSF Grants BCS-0624277,
IIS-0535099, and IIS-0208028, and by gifts from Google and
the Xerox Foundation.
References
D.W. Aha and R.L. Goldstone. 1992. Concept learning and flexible weighting. In
Proc. of the Fourteenth Annual Conference of the Cognitive Science Society.
A. Berger, S.D. Pietra, V.D. Pietra 1996. A Maximum Entropy Approach to
Natural Language Processing. In Computational Linguistics,22.
C. Cardie and N. Howe. 1997. Improving Minority Class Prediction Using Case-
Specific Feature Weights. In ICML.
W.W. Cohen and V. Carvalho. 2005. Stacked Sequential Learning. In IJCAI.
M. Collins. 2002. Discriminative Training Methods for Hidden Markov Models:
Theory and Experiments with Perceptron Algorithms. In EMNLP.
A.P. Dempster, N. M. Laird and D. B. Rubin. 1977. Maximum Likelihood from
Incomplete Data via the EM Algorithm. In Journal of the Loyal Statistical
Society, B.39.
J. Finkel, T. Grenager and C. D. Manning. 2005. Incorporating Non-local Infor-
mation Into Information Extraction Systems By Gibbs Sampling. In ACL.
T. Finley and T. Joachims. 2005. Supervised Clustering with Support Vector
Machines. In ICML.
J. Hammersley and P. Clifford. 1971. Markov fields on finite graphs and lattices.
Unpublished manuscript.
J. Lafferty, A. McCallum and F. Pereira. 2001. Conditional Random Fields:
Probabilistic Models for Segmenting and Labeling Sequence Data. In ICML.
A. McCallum and B. Wellner. 2004. Conditional Models of Identity Uncertainty
with Application to Noun Coreference. In NIPS.
MUC-6 1995. In Proc. of the Sixth Message Understanding Conference (MUC-6)
Morgan Kaufmann.
R. M. Neal and G. E. Hinton. 1998. A view of the EM algorithm that justies
incremental, sparse, and other variants. In Learning in Graphical Models,
Kluwer.
V. Ng and C. Cardie. 2002. Improving Machine Learning Approaches to Coref-
erence Resolution. In ACL.
V. Punyakanok, D. Roth, W. Yih, and D. Zimak 2005. Learning and Inference
over Constrained Output. In IJCAI.
D. Roth and W. Yih. 2005. Integer Linear Programming Inference for Conditional
Random Fields. In ICML.
V. Stoyanov and C. Cardie. 2006. Partially Supervised Coreference Resolution
for Opinion Summarization through Structured Rule Learning. In EMNLP.
C. Sutton and A. McCallum. 2005. Fast, Piecewise Training for Discriminative
Finite-state and Parsing Models. In Technical Report IR-403, University of
Massachusetts.
C. Sutton and A. McCallum. 2005. Piecewise Training for Undirected Models.
In UAI.
B. Wellner, A. McCallum, F. Peng and M. Hay. 2004. An Integrated, Conditional
Model of Information Extraction and Coreference with Application to Citation
Matching. In UAI.
J. Wiebe and T. Wilson and C. Cardie 2005. Annotating Expressions of Opinions
and Emotions in Language. In Language Resources and Evaluation, volume
39, issue 2-3.
72
Improving Machine Learning Approaches to Coreference Resolution
Vincent Ng and Claire Cardie
Department of Computer Science
Cornell University
Ithaca, NY 14853-7501
 
yung,cardie  @cs.cornell.edu
Abstract
We present a noun phrase coreference sys-
tem that extends the work of Soon et
al. (2001) and, to our knowledge, pro-
duces the best results to date on the MUC-
6 and MUC-7 coreference resolution data
sets ? F-measures of 70.4 and 63.4, re-
spectively. Improvements arise from two
sources: extra-linguistic changes to the
learning framework and a large-scale ex-
pansion of the feature set to include more
sophisticated linguistic knowledge.
1 Introduction
Noun phrase coreference resolution refers to the
problem of determining which noun phrases (NPs)
refer to each real-world entity mentioned in a doc-
ument. Machine learning approaches to this prob-
lem have been reasonably successful, operating pri-
marily by recasting the problem as a classification
task (e.g. Aone and Bennett (1995), McCarthy and
Lehnert (1995)). Specifically, a pair of NPs is clas-
sified as co-referring or not based on constraints that
are learned from an annotated corpus. A separate
clustering mechanism then coordinates the possibly
contradictory pairwise classifications and constructs
a partition on the set of NPs. Soon et al (2001),
for example, apply an NP coreference system based
on decision tree induction to two standard coref-
erence resolution data sets (MUC-6, 1995; MUC-
7, 1998), achieving performance comparable to the
best-performing knowledge-based coreference en-
gines. Perhaps surprisingly, this was accomplished
in a decidedly knowledge-lean manner ? the learn-
ing algorithm has access to just 12 surface-level fea-
tures.
This paper presents an NP coreference system that
investigates two types of extensions to the Soon et
al. corpus-based approach. First, we propose and
evaluate three extra-linguistic modifications to the
machine learning framework, which together pro-
vide substantial and statistically significant gains
in coreference resolution precision. Second, in an
attempt to understand whether incorporating addi-
tional knowledge can improve the performance of
a corpus-based coreference resolution system, we
expand the Soon et al feature set from 12 features
to an arguably deeper set of 53. We propose addi-
tional lexical, semantic, and knowledge-based fea-
tures; most notably, however, we propose 26 addi-
tional grammatical features that include a variety of
linguistic constraints and preferences. Although the
use of similar knowledge sources has been explored
in the context of both pronoun resolution (e.g. Lap-
pin and Leass (1994)) and NP coreference resolution
(e.g. Grishman (1995), Lin (1995)), most previous
work treats linguistic constraints as broadly and un-
conditionally applicable hard constraints. Because
sources of linguistic information in a learning-based
system are represented as features, we can, in con-
trast, incorporate them selectively rather than as uni-
versal hard constraints.
Our results using an expanded feature set are
mixed. First, we find that performance drops signifi-
cantly when using the full feature set, even though
the learning algorithms investigated have built-in
feature selection mechanisms. We demonstrate em-
                Computational Linguistics (ACL), Philadelphia, July 2002, pp. 104-111.
                         Proceedings of the 40th Annual Meeting of the Association for
pirically that the degradation in performance can be
attributed, at least in part, to poor performance on
common noun resolution. A manually selected sub-
set of 22?26 features, however, is shown to pro-
vide significant gains in performance when chosen
specifically to improve precision on common noun
resolution. Overall, the learning framework and lin-
guistic knowledge source modifications boost per-
formance of Soon?s learning-based coreference res-
olution approach from an F-measure of 62.6 to 70.4,
and from 60.4 to 63.4 for the MUC-6 and MUC-7
data sets, respectively. To our knowledge, these are
the best results reported to date on these data sets for
the full NP coreference problem.1
The rest of the paper is organized as follows. In
sections 2 and 3, we present the baseline corefer-
ence system and explore extra-linguistic modifica-
tions to the machine learning framework. Section 4
describes and evaluates the expanded feature set. We
conclude with related and future work in Section 5.
2 The Baseline Coreference System
Our baseline coreference system attempts to dupli-
cate both the approach and the knowledge sources
employed in Soon et al (2001). More specifically, it
employs the standard combination of classification
and clustering described above.
Building an NP coreference classifier. We use
the C4.5 decision tree induction system (Quinlan,
1993) to train a classifier that, given a description
of two NPs in a document, NP and NP , decides
whether or not they are coreferent. Each training
instance represents the two NPs under consideration
and consists of the 12 Soon et al features, which
are described in Table 1. Linguistically, the features
can be divided into four groups: lexical, grammati-
cal, semantic, and positional.2 The classification as-
sociated with a training instance is one of COREF-
ERENT or NOT COREFERENT depending on whether
the NPs co-refer in the associated training text. We
follow the procedure employed in Soon et al to cre-
1Results presented in Harabagiu et al (2001) are higher
than those reported here, but assume that all and only the noun
phrases involved in coreference relationships are provided for
analysis by the coreference resolution system. We presume no
preprocessing of the training and test documents.
2In all of the work presented here, NPs are identified, and
features values computed entirely automatically.
ate the training data: we rely on coreference chains
from the MUC answer keys to create (1) a positive
instance for each anaphoric noun phrase, NP , and its
closest preceding antecedent, NP ; and (2) a negative
instance for NP paired with each of the intervening
NPs, NP , NP	 , 


 , NP . This method of neg-
ative instance selection is further described in Soon
et al (2001); it is designed to operate in conjunction
with their method for creating coreference chains,
which is explained next.
Applying the classifier to create coreference
chains. After training, the decision tree is used by
a clustering algorithm to impose a partitioning on all
NPs in the test texts, creating one cluster for each set
of coreferent NPs. As in Soon et al, texts are pro-
cessed from left to right. Each NP encountered, NP ,
is compared in turn to each preceding NP, NP , from
right to left. For each pair, a test instance is created
as during training and is presented to the corefer-
ence classifier, which returns a number between 0
and 1 that indicates the likelihood that the two NPs
are coreferent.3 NP pairs with class values above 0.5
are considered COREFERENT; otherwise the pair is
considered NOT COREFERENT. The process termi-
nates as soon as an antecedent is found for NP or the
beginning of the text is reached.
2.1 Baseline Experiments
We evaluate the Duplicated Soon Baseline sys-
tem using the standard MUC-6 (1995) and MUC-
7 (1998) coreference corpora, training the corefer-
ence classifier on the 30 ?dry run? texts, and ap-
plying the coreference resolution algorithm on the
20?30 ?formal evaluation? texts. The MUC-6 cor-
pus produces a training set of 26455 instances (5.4%
positive) from 4381 NPs and a test set of 28443
instances (5.2% positive) from 4565 NPs. For the
MUC-7 corpus, we obtain a training set of 35895 in-
stances (4.4% positive) from 5270 NPs and a test set
of 22699 instances (3.9% positive) from 3558 NPs.
Results are shown in Table 2 (Duplicated Soon
Baseline) where performance is reported in terms
of recall, precision, and F-measure using the model-
theoretic MUC scoring program (Vilain et al, 1995).
3We convert the binary class value using the smoothed ratio



, where p is the number of positive instances and t is the
total number of instances contained in the corresponding leaf
node.
Feature Type Feature Description
Lexical SOON STR C if, after discarding determiners, the string denoting NP matches that of
NP ; else I.
Grammatical PRONOUN 1* Y if NP is a pronoun; else N.
PRONOUN 2* Y if NP is a pronoun; else N.
DEFINITE 2 Y if NP starts with the word ?the;? else N.
DEMONSTRATIVE 2 Y if NP starts with a demonstrative such as ?this,? ?that,? ?these,? or
?those;? else N.
NUMBER* C if the NP pair agree in number; I if they disagree; NA if number informa-
tion for one or both NPs cannot be determined.
GENDER* C if the NP pair agree in gender; I if they disagree; NA if gender information
for one or both NPs cannot be determined.
BOTH PROPER NOUNS* C if both NPs are proper names; NA if exactly one NP is a proper name;
else I.
APPOSITIVE* C if the NPs are in an appositive relationship; else I.
Semantic WNCLASS* C if the NPs have the same WordNet semantic class; I if they don?t; NA if
the semantic class information for one or both NPs cannot be determined.
ALIAS* C if one NP is an alias of the other; else I.
Positional SENTNUM* Distance between the NPs in terms of the number of sentences.
Table 1: Feature Set for the Duplicated Soon Baseline system. The feature set contains relational and non-relational
features. Non-relational features test some property P of one of the NPs under consideration and take on a value of YES or NO
depending on whether P holds. Relational features test whether some property P holds for the NP pair under consideration and
indicate whether the NPs are COMPATIBLE or INCOMPATIBLE w.r.t. P; a value of NOT APPLICABLE is used when property P does
not apply. *?d features are in the hand-selected feature set (see Section 4) for at least one classifier/data set combination.
The system achieves an F-measure of 66.3 and
61.2 on the MUC-6 and MUC-7 data sets, respec-
tively. Similar, but slightly worse performance
was obtained using RIPPER (Cohen, 1995), an
information-gain-based rule learning system. Both
sets of results are at least as strong as the original
Soon results (row one of Table 2), indicating indi-
rectly that our Baseline system is a reasonable du-
plication of that system.4 In addition, the trees pro-
duced by Soon and by our Duplicated Soon Baseline
are essentially the same, differing only in two places
where the Baseline system imposes additional con-
ditions on coreference.
The primary reason for improvements over the
original Soon system for the MUC-6 data set ap-
pears to be our higher upper bound on recall (93.8%
vs. 89.9%), due to better identification of NPs. For
MUC-7, our improvement stems from increases in
precision, presumably due to more accurate feature
value computation.
4In all of the experiments described in this paper, default
settings for all C4.5 parameters are used. Similarly, all RIPPER
parameters are set to their default value except that classification
rules are induced for both the positive and negative instances.
3 Modifications to the Machine Learning
Framework
This section studies the effect of three changes to
the general machine learning framework employed
by Soon et al with the goal of improving precision
in the resulting coreference resolution systems.
Best-first clustering. Rather than a right-to-left
search from each anaphoric NP for the first coref-
erent NP, we hypothesized that a right-to-left search
for a highly likely antecedent might offer more pre-
cise, if not generally better coreference chains. As
a result, we modify the coreference clustering algo-
rithm to select as the antecedent of NP the NP with
the highest coreference likelihood value from among
preceding NPs with coreference class values above
0.5.
Training set creation. For the proposed best-first
clustering to be successful, however, a different
method for training instance selection would be
needed: rather than generate a positive training ex-
ample for each anaphoric NP and its closest an-
tecedent, we instead generate a positive training ex-
amples for its most confident antecedent. More
specifically, for a non-pronominal NP, we assume
that the most confident antecedent is the closest non-
C4.5 RIPPER
MUC-6 MUC-7 MUC-6 MUC-7
System Variation R P F R P F R P F R P F
Original Soon et al 58.6 67.3 62.6 56.1 65.5 60.4 - - - - - -
Duplicated Soon Baseline 62.4 70.7 66.3 55.2 68.5 61.2 60.8 68.4 64.3 54.0 69.5 60.8
Learning Framework 62.4 73.5 67.5 56.3 71.5 63.0 60.8 75.3 67.2 55.3 73.8 63.2
String Match 60.4 74.4 66.7 54.3 72.1 62.0 58.5 74.9 65.7 48.9 73.2 58.6
Training Instance Selection 61.9 70.3 65.8 55.2 68.3 61.1 61.3 70.4 65.5 54.2 68.8 60.6
Clustering 62.4 70.8 66.3 56.5 69.6 62.3 60.5 68.4 64.2 55.6 70.7 62.2
All Features 70.3 58.3 63.8 65.5 58.2 61.6 67.0 62.2 64.5 61.9 60.6 61.2
Pronouns only ? 66.3 ? ? 62.1 ? ? 71.3 ? ? 62.0 ?
Proper Nouns only ? 84.2 ? ? 77.7 ? ? 85.5 ? ? 75.9 ?
Common Nouns only ? 40.1 ? ? 45.2 ? ? 43.7 ? ? 48.0 ?
Hand-selected Features 64.1 74.9 69.1 57.4 70.8 63.4 64.2 78.0 70.4 55.7 72.8 63.1
Pronouns only ? 67.4 ? ? 54.4 ? ? 77.0 ? ? 60.8 ?
Proper Nouns only ? 93.3 ? ? 86.6 ? ? 95.2 ? ? 88.7 ?
Common Nouns only ? 63.0 ? ? 64.8 ? ? 62.8 ? ? 63.5 ?
Table 2: Results for the MUC-6 and MUC-7 data sets using C4.5 and RIPPER. Recall, Precision, and F-measure
are provided. Results in boldface indicate the best results obtained for a particular data set and classifier combination.
pronominal preceding antecedent. For pronouns,
we assume that the most confident antecedent is sim-
ply its closest preceding antecedent. Negative exam-
ples are generated as in the Baseline system.5
String match feature. Soon?s string match feature
(SOON STR) tests whether the two NPs under con-
sideration are the same string after removing deter-
miners from each. We hypothesized, however, that
splitting this feature into several primitive features,
depending on the type of NP, might give the learn-
ing algorithm additional flexibility in creating coref-
erence rules. Exact string match is likely to be a
better coreference predictor for proper names than
it is for pronouns, for example. Specifically, we
replace the SOON STR feature with three features
? PRO STR, PN STR, and WORDS STR ? which
restrict the application of string matching to pro-
nouns, proper names, and non-pronominal NPs, re-
spectively. (See the first entries in Table 3.) Al-
though similar feature splits might have been con-
sidered for other features (e.g. GENDER and NUM-
BER), only the string match feature was tested here.
Results and discussion. Results on the learning
framework modifications are shown in Table 2 (third
block of results). When used in combination, the
modifications consistently provide statistically sig-
nificant gains in precision over the Baseline system
5This new method of training set creation slightly alters the
class value distribution in the training data: for the MUC-6 cor-
pus, there are now 27654 training instances of which 5.2% are
positive; for the MUC-7 corpus, there are now 37870 training
instances of which 4.2% are positive.
without any loss in recall.6 As a result, we observe
reasonable increases in F-measure for both classi-
fiers and both data sets. When using RIPPER, for
example, performance increases from 64.3 to 67.2
for the MUC-6 data set and from 60.8 to 63.2 for
MUC-7. Similar, but weaker, effects occur when ap-
plying each of the learning framework modifications
to the Baseline system in isolation. (See the indented
Learning Framework results in Table 2.)
Our results provide direct evidence for the claim
(Mitkov, 1997) that the extra-linguistic strategies
employed to combine the available linguistic knowl-
edge sources play an important role in computa-
tional approaches to coreference resolution. In par-
ticular, our results suggest that additional perfor-
mance gains might be obtained by further investi-
gating the interaction between training instance se-
lection, feature selection, and the coreference clus-
tering algorithm.
4 NP Coreference Using Many Features
This section describes the second major extension
to the Soon approach investigated here: we explore
the effect of including 41 additional, potentially use-
ful knowledge sources for the coreference resolu-
tion classifier (Table 3). The features were not de-
rived empirically from the corpus, but were based on
common-sense knowledge and linguistic intuitions
6Chi-square statistical significance tests are applied to
changes in recall and precision throughout the paper. Unless
otherwise noted, reported differences are at the 0.05 level or
higher. The chi-square test is not applicable to F-measure.
regarding coreference. Specifically, we increase the
number of lexical features to nine to allow more
complex NP string matching operations. In addi-
tion, we include four new semantic features to al-
low finer-grained semantic compatibility tests. We
test for ancestor-descendent relationships in Word-
Net (SUBCLASS), for example, and also measure
the WordNet graph-traversal distance (WNDIST) be-
tween NP and NP . Furthermore, we add a new posi-
tional feature that measures the distance in terms of
the number of paragraphs (PARANUM) between the
two NPs.
The most substantial changes to the feature set,
however, occur for grammatical features: we add 26
new features to allow the acquisition of more sophis-
ticated syntactic coreference resolution rules. Four
features simply determine NP type, e.g. are both
NPs definite, or pronouns, or part of a quoted string?
These features allow other tests to be conditioned on
the types of NPs being compared. Similarly, three
new features determine the grammatical role of one
or both of the NPs. Currently, only tests for clausal
subjects are made. Next, eight features encode tra-
ditional linguistic (hard) constraints on coreference.
For example, coreferent NPs must agree both in gen-
der and number (AGREEMENT); cannot SPAN one
another (e.g. ?government? and ?government offi-
cials?); and cannot violate the BINDING constraints.
Still other grammatical features encode general lin-
guistic preferences either for or against coreference.
For example, an indefinite NP (that is not in appo-
sition to an anaphoric NP) is not likely to be coref-
erent with any NP that precedes it (ARTICLE). The
last subset of grammatical features encodes slightly
more complex, but generally non-linguistic heuris-
tics. For instance, the CONTAINS PN feature ef-
fectively disallows coreference between NPs that
contain distinct proper names but are not them-
selves proper names (e.g. ?IBM executives? and
?Microsoft executives?).
Two final features make use of an in-house
naive pronoun resolution algorithm (PRO RESOLVE)
and a rule-based coreference resolution system
(RULE RESOLVE), each of which relies on the origi-
nal and expanded feature sets described above.
Results and discussion. Results using the ex-
panded feature set are shown in the All Features
block of Table 2. These and all subsequent results
also incorporate the learning framework changes
from Section 3. In comparison, we see statistically
significant increases in recall, but much larger de-
creases in precision. As a result, F-measure drops
precipitously for both learning algorithms and both
data sets. A closer examination of the results indi-
cates very poor precision on common nouns in com-
parison to that of pronouns and proper nouns. (See
the indented All Features results in Table 2.7) In
particular, the classifiers acquire a number of low-
precision rules for common noun resolution, pre-
sumably because the current feature set is insuffi-
cient. For instance, a rule induced by RIPPER clas-
sifies two NPs as coreferent if the first NP is a proper
name, the second NP is a definite NP in the subject
position, and the two NPs have the same seman-
tic class and are at most one sentence apart from
each other. This rule covers 38 examples, but has
18 exceptions. In comparison, the Baseline sys-
tem obtains much better precision on common nouns
(i.e. 53.3 for MUC-6/RIPPER and 61.0 for MUC-
7/RIPPER with lower recall in both cases) where the
primary mechanism employed by the classifiers for
common noun resolution is its high-precision string
matching facility. Our results also suggest that data
fragmentation is likely to have contributed to the
drop in performance (i.e. we increased the number
of features without increasing the size of the training
set). For example, the decision tree induced from the
MUC-6 data set using the Soon feature set (Learn-
ing Framework results) has 16 leaves, each of which
contains 1728 instances on average; the tree induced
from the same data set using all of the 53 features,
on the other hand, has 86 leaves with an average of
322 instances per leaf.
Hand-selected feature sets. As a result, we next
evaluate a version of the system that employs man-
ual feature selection: for each classifier/data set
combination, we discard features used primarily to
induce low-precision rules for common noun res-
olution and re-train the coreference classifier using
the reduced feature set. Here, feature selection does
not depend on a separate development corpus and
7For each of the NP-type-specific runs, we measure overall
coreference performance, but restrict NP to be of the specified
type. As a result, recall and F-measure for these runs are not
particularly informative.
L PRO STR* C if both NPs are pronominal and are the same string; else I.
e PN STR* C if both NPs are proper names and are the same string; else I.
x WORDS STR C if both NPs are non-pronominal and are the same string; else I.
i
c
SOON STR NONPRO* C if both NPs are non-pronominal and the string of NP matches that of NP ; else I.
a
l
WORD OVERLAP C if the intersection between the content words in NP and NP is not empty; else I.
MODIFIER C if the prenominal modifiers of one NP are a subset of the prenominal modifiers of the
other; else I.
PN SUBSTR C if both NPs are proper names and one NP is a proper substring (w.r.t. content words
only) of the other; else I.
WORDS SUBSTR C if both NPs are non-pronominal and one NP is a proper substring (w.r.t. content words
only) of the other; else I.
G NP BOTH DEFINITES C if both NPs start with ?the;? I if neither start with ?the;? else NA.
r
a
type BOTH EMBEDDED C if both NPs are prenominal modifiers ; I if neither are prenominal modifiers; else NA.
m
m
BOTH IN QUOTES C if both NPs are part of a quoted string; I if neither are part of a quoted string; else NA.
a BOTH PRONOUNS* C if both NPs are pronouns; I if neither are pronouns, else NA.
t role BOTH SUBJECTS C if both NPs are grammatical subjects; I if neither are subjects; else NA.
i SUBJECT 1* Y if NP is a subject; else N.
c SUBJECT 2 Y if NP is a subject; else N.
a
l
lin-
gui-
AGREEMENT* C if the NPs agree in both gender and number; I if they disagree in both gender and
number; else NA.
stic ANIMACY* C if the NPs match in animacy; else I.
MAXIMALNP* I if both NPs have the same maximal NP projection; else C.
con- PREDNOM* C if the NPs form a predicate nominal construction; else I.
stra- SPAN* I if one NP spans the other; else C.
ints BINDING* I if the NPs violate conditions B or C of the Binding Theory; else C.
CONTRAINDICES* I if the NPs cannot be co-indexed based on simple heuristics; else C. For instance, two
non-pronominal NPs separated by a preposition cannot be co-indexed.
SYNTAX* I if the NPs have incompatible values for the BINDING, CONTRAINDICES, SPAN or
MAXIMALNP constraints; else C.
ling. INDEFINITE* I if NP is an indefinite and not appositive; else C.
prefs PRONOUN I if NP is a pronoun and NP is not; else C.
heur-
istics
CONSTRAINTS* C if the NPs agree in GENDER and NUMBER and do not have incompatible values for
CONTRAINDICES, SPAN, ANIMACY, PRONOUN, and CONTAINS PN; I if the NPs have
incompatible values for any of the above features; else NA.
CONTAINS PN I if both NPs are not proper names but contain proper names that mismatch on every
word; else C.
DEFINITE 1 Y if NP starts with ?the;? else N.
EMBEDDED 1* Y if NP is an embedded noun; else N.
EMBEDDED 2 Y if NP is an embedded noun; else N.
IN QUOTE 1 Y if NP is part of a quoted string; else N.
IN QUOTE 2 Y if NP is part of a quoted string; else N.
PROPER NOUN I if both NPs are proper names, but mismatch on every word; else C.
TITLE* I if one or both of the NPs is a title; else C.
S
e
CLOSEST COMP C if NP is the closest NP preceding NP that has the same semantic class as NP and the
two NPs do not violate any of the linguistic constraints; else I.
m
a
SUBCLASS C if the NPs have different head nouns but have an ancestor-descendent relationship in
WordNet; else I.
n
t
i
WNDIST Distance between NP and NP in WordNet (using the first sense only) when they have
an ancestor-descendent relationship but have different heads; else infinity.
c WNSENSE Sense number in WordNet for which there exists an ancestor-descendent relationship
between the two NPs when they have different heads; else infinity.
P
os
PARANUM Distance between the NPs in terms of the number of paragraphs.
O
t
PRO RESOLVE* C if NP is a pronoun and NP is its antecedent according to a naive pronoun resolution
algorithm; else I.
h
er
RULE RESOLVE C if the NPs are coreferent according to a rule-based coreference resolution algorithm;
else I.
Table 3: Additional features for NP coreference. As before, *?d features are in the hand-selected feature set for at least
one classifier/data set combination.
is guided solely by inspection of the features associ-
ated with low-precision rules induced from the train-
ing data. In current work, we are automating this
feature selection process, which currently employs
a fair amount of user discretion, e.g. to determine a
precision cut-off. Features in the hand-selected set
for at least one of the tested system variations are
*?d in Tables 1 and 3.
In general, we hypothesized that the hand-
selected features would reclaim precision, hopefully
without losing recall. For the most part, the ex-
perimental results support this hypothesis. (See the
Hand-selected Features block in Table 2.) In com-
parison to the All Features version, we see statisti-
cally significant gains in precision and statistically
significant, but much smaller, drops in recall, pro-
ducing systems with better F-measure scores. In
addition, precision on common nouns rises substan-
tially, as expected. Unfortunately, the hand-selected
features precipitate a large drop in precision for pro-
noun resolution for the MUC-7/C4.5 data set. Ad-
ditional analysis is required to determine the reason
for this.
Moreover, the Hand-selected Features produce
the highest scores posted to date for both the MUC-
6 and MUC-7 data sets: F-measure increases w.r.t.
the Baseline system from 64.3 to 70.4 for MUC-
6/RIPPER, and from 61.2 to 63.4 for MUC-7/C4.5.
In one variation (MUC-7/RIPPER), however, the
Hand-selected Features slightly underperforms the
Learning Framework modifications (F-measure of
63.1 vs. 63.2) although changes in recall and pre-
cision are not statistically significant. Overall, our
results indicate that pronoun and especially com-
mon noun resolution remain important challenges
for coreference resolution systems. Somewhat dis-
appointingly, only four of the new grammatical
features corresponding to linguistic constraints and
preferences are selected by the symbolic learning
algorithms investigated: AGREEMENT, ANIMACY,
BINDING, and MAXIMALNP.
Discussion. In an attempt to gain additional in-
sight into the difference in performance between our
system and the original Soon system, we compare
the decision tree induced by each for the MUC-6
ALIAS = C: + (347.0/23.8)
ALIAS = I:
|  SOON_STR_NONPRO = C:
|  |  ANIMACY = NA: - (4.0/2.2)
|  |  ANIMACY = I: + (0.0)
|  |  ANIMACY = C: + (259.0/45.8)
|  SOON_STR_NONPRO = I:
|  |  PRO_STR = C: + (39.0/2.6)
|  |  PRO_STR = I:
|  |  |  PRO_RESOLVE = C:
|  |  |  |  EMBEDDED_1 = Y: - (7.0/3.4)
|  |  |  |  EMBEDDED_1 = N:
|  |  |  |  |  PRONOUN_1 = Y:
|  |  |  |  |  |  ANIMACY = NA: - (6.0/2.3)
|  |  |  |  |  |  ANIMACY = I: - (1.0/0.8)
|  |  |  |  |  |  ANIMACY = C: + (10.0/3.5)
|  |  |  |  |  PRONOUN_1 = N:
|  |  |  |  |  |  MAXIMALNP = C: + (108.0/18.2)
|  |  |  |  |  |  MAXIMALNP = I:
|  |  |  |  |  |  |  WNCLASS = NA: - (5.0/1.2)
|  |  |  |  |  |  |  WNCLASS = I: + (0.0)
|  |  |  |  |  |  |  WNCLASS = C: + (12.0/3.6)
|  |  |  PRO_RESOLVE = I:
|  |  |  |  APPOSITIVE = I: - (26806.0/713.8)
|  |  |  |  APPOSITIVE = C:
|  |  |  |  |  GENDER = NA: + (28.0/2.6)
|  |  |  |  |  GENDER = I: + (5.0/3.2)
|  |  |  |  |  GENDER = C: - (17.0/3.7)
Figure 1: Decision Tree using the Hand-selected
feature set on the MUC-6 data set.
data set.8 For our system, we use the tree induced on
the hand-selected features (Figure 1). The two trees
are fairly different. In particular, our tree makes
use of many of the features that are not present in
the original Soon feature set. The root feature for
Soon, for example, is the general string match fea-
ture (SOON STR); splitting the SOON STR feature
into three primitive features promotes the ALIAS fea-
ture to the root of our tree, on the other hand. In
addition, given two non-pronominal, matching NPs
(SOON STR NONPRO=C), our tree requires an addi-
tional test on ANIMACY before considering the two
NPs coreferent; the Soon tree instead determines
two NPs to be coreferent as long as they are the same
string. Pronoun resolution is also performed quite
differently by the two trees, although both consider
two pronouns coreferent when their strings match.
Finally, intersentential and intrasentential pronomi-
nal references are possible in our system while inter-
sentential pronominal references are largely prohib-
ited by the Soon system.
5 Conclusions
We investigate two methods to improve existing
machine learning approaches to the problem of
8Soon et al (2001) present only the tree learned for the
MUC-6 data set.
noun phrase coreference resolution. First, we pro-
pose three extra-linguistic modifications to the ma-
chine learning framework, which together consis-
tently produce statistically significant gains in pre-
cision and corresponding increases in F-measure.
Our results indicate that coreference resolution sys-
tems can improve by effectively exploiting the in-
teraction between the classification algorithm, train-
ing instance selection, and the clustering algorithm.
We plan to continue investigations along these lines,
developing, for example, a true best-first clustering
coreference framework and exploring a ?supervised
clustering? approach to the problem. In addition,
we provide the learning algorithms with many addi-
tional linguistic knowledge sources for coreference
resolution. Unfortunately, we find that performance
drops significantly when using the full feature set;
we attribute this, at least in part, to the system?s poor
performance on common noun resolution and to data
fragmentation problems that arise with the larger
feature set. Manual feature selection, with an eye
toward eliminating low-precision rules for common
noun resolution, is shown to reliably improve per-
formance over the full feature set and produces the
best results to date on the MUC-6 and MUC-7 coref-
erence data sets ? F-measures of 70.4 and 63.4, re-
spectively. Nevertheless, there is substantial room
for improvement. As noted above, for example, it is
important to automate the precision-oriented feature
selection procedure as well as to investigate other
methods for feature selection. We also plan to in-
vestigate previous work on common noun phrase
interpretation (e.g. Sidner (1979), Harabagiu et al
(2001)) as a means of improving common noun
phrase resolution, which remains a challenge for
state-of-the-art coreference resolution systems.
Acknowledgments
Thanks to three anonymous reviewers for their comments and,
in particular, for suggesting that we investigate data fragmen-
tation issues. This work was supported in part by DARPA
TIDES contract N66001-00-C-8009, and NSF Grants 0081334
and 0074896.
References
C. Aone and S. W. Bennett. 1995. Evaluating Auto-
mated and Manual Acquisition of Anaphora Resolu-
tion Strategies. In Proceedings of the 33rd Annual
Meeting of the Association for Computational Linguis-
tics, pages 122?129.
W. Cohen. 1995. Fast Effective Rule Induction. In Pro-
ceedings of the Twelfth International Conference on
Machine Learning.
R. Grishman. 1995. The NYU System for MUC-6 or
Where?s the Syntax? In Proceedings of the Sixth Mes-
sage Understanding Conference (MUC-6).
S. Harabagiu, R. Bunescu, and S. Maiorano. 2001. Text
and Knowledge Mining for Coreference Resolution.
In Proceedings of the Second Meeting of the North
America Chapter of the Association for Computational
Linguistics (NAACL-2001), pages 55?62.
S. Lappin and H. Leass. 1994. An Algorithm for
Pronominal Anaphora Resolution. Computational
Linguistics, 20(4):535?562.
D. Lin. 1995. University of Manitoba: Description of the
PIE System as Used for MUC-6. In Proceedings of the
Sixth Message Understanding Conference (MUC-6).
J. McCarthy and W. Lehnert. 1995. Using Decision
Trees for Coreference Resolution. In Proceedings of
the Fourteenth International Conference on Artificial
Intelligence, pages 1050?1055.
R. Mitkov. 1997. Factors in anaphora resolution: they
are not the only things that matter. A case study based
on two different approaches. In Proceedings of the
ACL?97/EACL?97 Workshop on Operational Factors
in Practical, Robust Anaphora Resolution.
MUC-6. 1995. Proceedings of the Sixth Message Under-
standing Conference (MUC-6). Morgan Kaufmann,
San Francisco, CA.
MUC-7. 1998. Proceedings of the Seventh Message
Understanding Conference (MUC-7). Morgan Kauf-
mann, San Francisco, CA.
J. R. Quinlan. 1993. C4.5: Programs for Machine
Learning. Morgan Kaufmann, San Mateo, CA.
C. Sidner. 1979. Towards a Computational Theory
of Definite Anaphora Comprehension in English Dis-
course. PhD Thesis, Massachusetts Institute of Tech-
nology.
W. M. Soon, H. T. Ng, and D. C. Y. Lim. 2001. A
Machine Learning Approach to Coreference Resolu-
tion of Noun Phrases. Computational Linguistics,
27(4):521?544.
M. Vilain, J. Burger, J. Aberdeen, D. Connolly, and
L. Hirschman. 1995. A model-theoretic coreference
scoring scheme. In Proceedings of the Sixth Mes-
sage Understanding Conference (MUC-6), pages 45?
52, San Francisco, CA. Morgan Kaufmann.
Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 656?664,
Suntec, Singapore, 2-7 August 2009. c?2009 ACL and AFNLP
Conundrums in Noun Phrase Coreference Resolution:
Making Sense of the State-of-the-Art
Veselin Stoyanov
Cornell University
Ithaca, NY
ves@cs.cornell.edu
Nathan Gilbert
University of Utah
Salt Lake City, UT
ngilbert@cs.utah.edu
Claire Cardie
Cornell University
Ithaca, NY
cardie@cs.cornell.edu
Ellen Riloff
University of Utah
Salt Lake City, UT
riloff@cs.utah.edu
Abstract
We aim to shed light on the state-of-the-art in NP
coreference resolution by teasing apart the differ-
ences in the MUC and ACE task definitions, the as-
sumptions made in evaluation methodologies, and
inherent differences in text corpora. First, we exam-
ine three subproblems that play a role in coreference
resolution: named entity recognition, anaphoric-
ity determination, and coreference element detec-
tion. We measure the impact of each subproblem on
coreference resolution and confirm that certain as-
sumptions regarding these subproblems in the eval-
uation methodology can dramatically simplify the
overall task. Second, we measure the performance
of a state-of-the-art coreference resolver on several
classes of anaphora and use these results to develop
a quantitative measure for estimating coreference
resolution performance on new data sets.
1 Introduction
As is common for many natural language process-
ing problems, the state-of-the-art in noun phrase
(NP) coreference resolution is typically quantified
based on system performance on manually anno-
tated text corpora. In spite of the availability of
several benchmark data sets (e.g. MUC-6 (1995),
ACE NIST (2004)) and their use in many formal
evaluations, as a field we can make surprisingly
few conclusive statements about the state-of-the-
art in NP coreference resolution.
In particular, it remains difficult to assess the ef-
fectiveness of different coreference resolution ap-
proaches, even in relative terms. For example, the
91.5 F-measure reported by McCallum and Well-
ner (2004) was produced by a system using perfect
information for several linguistic subproblems. In
contrast, the 71.3 F-measure reported by Yang et
al. (2003) represents a fully automatic end-to-end
resolver. It is impossible to assess which approach
truly performs best because of the dramatically
different assumptions of each evaluation.
Results vary widely across data sets. Corefer-
ence resolution scores range from 85-90% on the
ACE 2004 and 2005 data sets to a much lower 60-
70% on the MUC 6 and 7 data sets (e.g. Soon et al
(2001) and Yang et al (2003)). What accounts for
these differences? Are they due to properties of
the documents or domains? Or do differences in
the coreference task definitions account for the dif-
ferences in performance? Given a new text collec-
tion and domain, what level of performance should
we expect?
We have little understanding of which aspects
of the coreference resolution problem are handled
well or poorly by state-of-the-art systems. Ex-
cept for some fairly general statements, for exam-
ple that proper names are easier to resolve than
pronouns, which are easier than common nouns,
there has been little analysis of which aspects of
the problem have achieved success and which re-
main elusive.
The goal of this paper is to take initial steps to-
ward making sense of the disparate performance
results reported for NP coreference resolution. For
our investigations, we employ a state-of-the-art
classification-based NP coreference resolver and
focus on the widely used MUC and ACE corefer-
ence resolution data sets.
We hypothesize that performance variation
within and across coreference resolvers is, at least
in part, a function of (1) the (sometimes unstated)
assumptions in evaluation methodologies, and (2)
the relative difficulty of the benchmark text cor-
pora. With these in mind, Section 3 first examines
three subproblems that play an important role in
coreference resolution: named entity recognition,
anaphoricity determination, and coreference ele-
ment detection. We quantitatively measure the im-
pact of each of these subproblems on coreference
resolution performance as a whole. Our results
suggest that the availability of accurate detectors
for anaphoricity or coreference elements could
substantially improve the performance of state-of-
the-art resolvers, while improvements to named
entity recognition likely offer little gains. Our re-
sults also confirm that the assumptions adopted in
656
MUC ACE
Relative Pronouns no yes
Gerunds no yes
Nested non-NP nouns yes no
Nested NEs no GPE & LOC premod
Semantic Types all 7 classes only
Singletons no yes
Table 1: Coreference Definition Differences for MUC and
ACE. (GPE refers to geo-political entities.)
some evaluations dramatically simplify the resolu-
tion task, rendering it an unrealistic surrogate for
the original problem.
In Section 4, we quantify the difficulty of a
text corpus with respect to coreference resolution
by analyzing performance on different resolution
classes. Our goals are twofold: to measure the
level of performance of state-of-the-art corefer-
ence resolvers on different types of anaphora, and
to develop a quantitative measure for estimating
coreference resolution performance on new data
sets. We introduce a coreference performance pre-
diction (CPP) measure and show that it accurately
predicts the performance of our coreference re-
solver. As a side effect of our research, we pro-
vide a new set of much-needed benchmark results
for coreference resolution under common sets of
fully-specified evaluation assumptions.
2 Coreference Task Definitions
This paper studies the six most commonly used
coreference resolution data sets. Two of those are
from the MUC conferences (MUC-6, 1995; MUC-
7, 1997) and four are from the Automatic Con-
tent Evaluation (ACE) Program (NIST, 2004). In
this section, we outline the differences between the
MUC and ACE coreference resolution tasks, and
define terminology for the rest of the paper.
Noun phrase coreference resolution is the pro-
cess of determining whether two noun phrases
(NPs) refer to the same real-world entity or con-
cept. It is related to anaphora resolution: a NP is
said to be anaphoric if it depends on another NP
for interpretation. Consider the following:
John Hall is the new CEO. He starts on Monday.
Here, he is anaphoric because it depends on its an-
tecedent, John Hall, for interpretation. The two
NPs also corefer because each refers to the same
person, JOHN HALL.
As discussed in depth elsewhere (e.g. van
Deemter and Kibble (2000)), the notions of coref-
erence and anaphora are difficult to define pre-
cisely and to operationalize consistently. Further-
more, the connections between them are extremely
complex and go beyond the scope of this paper.
Given these complexities, it is not surprising that
the annotation instructions for the MUC and ACE
data sets reflect different interpretations and sim-
plifications of the general coreference relation. We
outline some of these differences below.
Syntactic Types. To avoid ambiguity, we will
use the term coreference element (CE) to refer
to the set of linguistic expressions that participate
in the coreference relation, as defined for each of
the MUC and ACE tasks.1 At times, it will be im-
portant to distinguish between the CEs that are in-
cluded in the gold standard ? the annotated CEs
? from those that are generated by the corefer-
ence resolution system ? the extracted CEs.
At a high level, both the MUC and ACE eval-
uations define CEs as nouns, pronouns, and noun
phrases. However, the MUC definition excludes
(1) ?nested? named entities (NEs) (e.g. ?Amer-
ica? in ?Bank of America?), (2) relative pronouns,
and (3) gerunds, but allows (4) nested nouns (e.g.
?union? in ?union members?). The ACE defini-
tion, on the other hand, includes relative pronouns
and gerunds, excludes all nested nouns that are not
themselves NPs, and allows premodifier NE men-
tions of geo-political entities and locations, such
as ?Russian? in ?Russian politicians?.
Semantic Types. ACE restricts CEs to entities
that belong to one of seven semantic classes: per-
son, organization, geo-political entity, location, fa-
cility, vehicle, and weapon. MUC has no semantic
restrictions.
Singletons. The MUC data sets include annota-
tions only for CEs that are coreferent with at least
one other CE. ACE, on the other hand, permits
?singleton? CEs, which are not coreferent with
any other CE in the document.
These substantial differences in the task defini-
tions (summarized in Table 1) make it extremely
difficult to compare performance across the MUC
and ACE data sets. In the next section, we take a
closer look at the coreference resolution task, ana-
lyzing the impact of various subtasks irrespective
of the data set differences.
1We define the term CE to be roughly equivalent to (a)
the notion of markable in the MUC coreference resolution
definition and (b) the structures that can be mentions in the
descriptions of ACE.
657
3 Coreference Subtask Analysis
Coreference resolution is a complex task that
requires solving numerous non-trivial subtasks
such as syntactic analysis, semantic class tagging,
pleonastic pronoun identification and antecedent
identification to name a few. This section exam-
ines the role of three such subtasks ? named en-
tity recognition, anaphoricity determination, and
coreference element detection ? in the perfor-
mance of an end-to-end coreference resolution
system. First, however, we describe the corefer-
ence resolver that we use for our study.
3.1 The RECONCILEACL09 Coreference
Resolver
We use the RECONCILE coreference resolution
platform (Stoyanov et al, 2009) to configure a
coreference resolver that performs comparably to
state-of-the-art systems (when evaluated on the
MUC and ACE data sets under comparable as-
sumptions). This system is a classification-based
coreference resolver, modeled after the systems of
Ng and Cardie (2002b) and Bengtson and Roth
(2008). First it classifies pairs of CEs as coreferent
or not coreferent, pairing each identified CE with
all preceding CEs. The CEs are then clustered
into coreference chains2 based on the pairwise de-
cisions. RECONCILE has a pipeline architecture
with four main steps: preprocessing, feature ex-
traction, classification, and clustering. We will
refer to the specific configuration of RECONCILE
used for this paper as RECONCILEACL09.
Preprocessing. The RECONCILEACL09 prepro-
cessor applies a series of language analysis tools
(mostly publicly available software packages) to
the source texts. The OpenNLP toolkit (Baldridge,
J., 2005) performs tokenization, sentence splitting,
and part-of-speech tagging. The Berkeley parser
(Petrov and Klein, 2007) generates phrase struc-
ture parse trees, and the de Marneffe et al (2006)
system produces dependency relations. We em-
ploy the Stanford CRF-based Named Entity Rec-
ognizer (Finkel et al, 2004) for named entity
tagging. With these preprocessing components,
RECONCILEACL09 uses heuristics to correctly ex-
tract approximately 90% of the annotated CEs for
the MUC and ACE data sets.
Feature Set. To achieve roughly state-of-the-
art performance, RECONCILEACL09 employs a
2A coreference chain refers to the set of CEs that refer to
a particular entity.
dataset docs CEs chains CEs/ch tr/tst split
MUC6 60 4232 960 4.4 30/30 (st)
MUC7 50 4297 1081 3.9 30/20 (st)
ACE-2 159 2630 1148 2.3 130/29 (st)
ACE03 105 3106 1340 2.3 74/31
ACE04 128 3037 1332 2.3 90/38
ACE05 81 1991 775 2.6 57/24
Table 2: Dataset characteristics including the number of
documents, annotated CEs, coreference chains, annotated
CEs per chain (average), and number of documents in the
train/test split. We use st to indicate a standard train/test split.
fairly comprehensive set of 61 features introduced
in previous coreference resolution systems (see
Bengtson and Roth (2008)). We briefly summarize
the features here and refer the reader to Stoyanov
et al (2009) for more details.
Lexical (9): String-based comparisons of the two
CEs, such as exact string matching and head noun
matching.
Proximity (5): Sentence and paragraph-based
measures of the distance between two CEs.
Grammatical (28): A wide variety of syntactic
properties of the CEs, either individually or as a
pair. These features are based on part-of-speech
tags, parse trees, or dependency relations. For ex-
ample: one feature indicates whether both CEs are
syntactic subjects; another indicates whether the
CEs are in an appositive construction.
Semantic (19): Capture semantic information
about one or both NPs such as tests for gender and
animacy, semantic compatibility based on Word-
Net, and semantic comparisons of NE types.
Classification and Clustering. We configure
RECONCILEACL09 to use the Averaged Percep-
tron learning algorithm (Freund and Schapire,
1999) and to employ single-link clustering (i.e.
transitive closure) to generate the final partition-
ing.3
3.2 Baseline System Results
Our experiments rely on the MUC and ACE cor-
pora. For ACE, we use only the newswire portion
because it is closest in composition to the MUC
corpora. Statistics for each of the data sets are
shown in Table 2. When available, we use the
standard test/train split. Otherwise, we randomly
split the data into a training and test set following
a 70/30 ratio.
3In trial runs, we investigated alternative classification
and clustering models (e.g. C4.5 decision trees and SVMs;
best-first clustering). The results were comparable.
658
Scoring Algorithms. We evaluate using two
common scoring algorithms4 ? MUC and B3.
The MUC scoring algorithm (Vilain et al, 1995)
computes the F1 score (harmonic mean) of preci-
sion and recall based on the identifcation of unique
coreference links. We use the official MUC scorer
implementation for the two MUC corpora and an
equivalent implementation for ACE.
The B3 algorithm (Bagga and Baldwin, 1998)
computes a precision and recall score for each CE:
precision(ce) = |Rce ?Kce|/|Rce|
recall(ce) = |Rce ?Kce|/|Kce|,
where Rce is the coreference chain to which ce is
assigned in the response (i.e. the system-generated
output) and Kce is the coreference chain that con-
tains ce in the key (i.e. the gold standard). Pre-
cision and recall for a set of documents are com-
puted as the mean over all CEs in the documents
and the F1 score of precision and recall is reported.
B3 Complications. Unlike the MUC score,
which counts links between CEs, B3 presumes
that the gold standard and the system response are
clusterings over the same set of CEs. This, of
course, is not the case when the system automat-
ically identifies the CEs, so the scoring algorithm
requires a mapping between extracted and anno-
tated CEs. We will use the term twin(ce) to refer
to the unique annotated/extracted CE to which the
extracted/annotated CE is matched. We say that
a CE is twinless (has no twin) if no corresponding
CE is identified. A twinless extracted CE signals
that the resolver extracted a spurious CE, while an
annotated CE is twinless when the resolver fails to
extract it.
Unfortunately, it is unclear how the B3 score
should be computed for twinless CEs. Bengtson
and Roth (2008) simply discard twinless CEs, but
this solution is likely too lenient ? it doles no pun-
ishment for mistakes on twinless annotated or ex-
tracted CEs and it would be tricked, for example,
by a system that extracts only the CEs about which
it is most confident.
We propose two different ways to deal with
twinless CEs for B3. One option, B3all, retains
all twinless extracted CEs. It computes the preci-
4We also experimented with the CEAF score (Luo, 2005),
but excluded it due to difficulties dealing with the extracted,
rather than annotated, CEs. CEAF assigns a zero score to
each twinless extracted CE and weights all coreference chains
equally, irrespective of their size. As a result, runs with ex-
tracted CEs exhibit very low CEAF precision, leading to un-
reliable scores.
sion as above when ce has a twin, and computes
the precision as 1/|Rce| if ce is twinless. (Simi-
larly, recall(ce) = 1/|Kce| if ce is twinless.)
The second option, B30, discards twinless
extracted CEs, but penalizes recall by setting
recall(ce) = 0 for all twinless annotated CEs.
Thus, B30 presumes that all twinless extracted
CEs are spurious.
Results. Table 3, box 1 shows the performance
of RECONCILEACL09 using a default (0.5) coref-
erence classifier threshold. The MUC score is
highest for the MUC6 data set, while the four ACE
data sets show much higher B3 scores as com-
pared to the two MUC data sets. The latter occurs
because the ACE data sets include singletons.
The classification threshold, however, can be
gainfully employed to control the trade-off be-
tween precision and recall. This has not tradi-
tionally been done in learning-based coreference
resolution research ? possibly because there is
not much training data available to sacrifice as a
validation set. Nonetheless, we hypothesized that
estimating a threshold from just the training data
might be effective. Our results (BASELINE box
in Table 3) indicate that this indeed works well.5
With the exception of MUC6, results on all data
sets and for all scoring algorithms improve; more-
over, the scores approach those for runs using an
optimal threshold (box 3) for the experiment as de-
termined by using the test set. In all remaining ex-
periments, we learn the threshold from the training
set as in the BASELINE system.
Below, we resume our investigation of the role
of three coreference resolution subtasks and mea-
sure the impact of each on overall performance.
3.3 Named Entities
Previous work has shown that resolving corefer-
ence between proper names is relatively easy (e.g.
Kameyama (1997)) because string matching func-
tions specialized to the type of proper name (e.g.
person vs. location) are quite accurate. Thus, we
would expect a coreference resolution system to
depend critically on its Named Entity (NE) extrac-
tor. On the other hand, state-of-the-art NE taggers
are already quite good, so improving this compo-
nent may not provide much additional gain.
To study the influence of NE recognition,
we replace the system-generated NEs of
5All experiments sample uniformly from 1000 threshold
values.
659
ReconcileACL09 MUC6 MUC7 ACE-2 ACE03 ACE04 ACE05
1. DEFAULT THRESHOLD (0.5)
MUC 70.40 58.20 65.76 66.73 56.75 64.30
B3all 69.91 62.88 77.25 77.56 73.03 72.82
B30 68.55 62.80 76.59 77.27 72.99 72.43
2. BASELINE
MUC 68.50 62.80 65.99 67.87 62.03 67.41
= THRESHOLD ESTIMATION
B3all 70.88 65.86 78.29 79.39 76.50 73.71
B30 68.43 64.57 76.63 77.88 75.41 72.47
3. OPTIMAL THRESHOLD
MUC 71.20 62.90 66.83 68.35 62.11 67.41
B3all 72.31 66.52 78.50 79.41 76.53 74.25
B30 69.49 64.64 76.83 78.27 75.51 72.94
4. BASELINE with
MUC 69.90 - 66.37 70.35 62.88 67.72
perfect NEs
B3all 72.31 - 78.06 80.22 77.01 73.92
B30 67.91 - 76.55 78.35 75.22 72.90
5. BASELINE with
MUC 85.80* 81.10* 76.39 79.68 76.18 79.42
perfect CEs
B3all 76.14 75.88 78.65 80.58 77.79 76.49
B30 76.14 75.88 78.65 80.58 77.79 76.49
6. BASELINE with
MUC 82.20* 71.90* 86.63 85.58 83.33 82.84
anaphoric CEs
B3all 72.52 69.26 80.29 79.71 76.05 74.33
B30 72.52 69.26 80.29 79.71 76.05 74.33
Table 3: Impact of Three Subtasks on Coreference Resolution Performance. A score marked with a * indicates that a 0.5
threshold was used because threshold selection from the training data resulted in an extreme version of the system, i.e. one that
places all CEs into a single coreference chain.
RECONCILEACL09 with gold-standard NEs
and retrain the coreference classifier. Results
for each of the data sets are shown in box 4 of
Table 3. (No gold standard NEs are available for
MUC7.) Comparison to the BASELINE system
(box 2) shows that using gold standard NEs
leads to improvements on all data sets with the
exception of ACE2 and ACE05, on which perfor-
mance is virtually unchanged. The improvements
tend to be small, however, between 0.5 to 3
performance points. We attribute this to two
factors. First, as noted above, although far from
perfect, NE taggers generally perform reasonably
well. Second, only 20 to 25% of the coreference
element resolutions required for these data sets
involve a proper name (see Section 4).
Conclusion #1: Improving the performance of NE tag-
gers is not likely to have a large impact on the performance
of state-of-the-art coreference resolution systems.
3.4 Coreference Element Detection
We expect CE detection to be an important sub-
problem for an end-to-end coreference system.
Results for a system that assumes perfect CEs
are shown in box 5 of Table 3. For these runs,
RECONCILEACL09 uses only the annotated CEs
for both training and testing. Using perfect CEs
solves a large part of the coreference resolution
task: the annotated CEs divulge anaphoricity in-
formation, perfect NP boundaries, and perfect in-
formation regarding the coreference relation de-
fined for the data set.
We see that focusing attention on all and only
the annotated CEs leads to (often substantial) im-
provements in performance on all metrics over
all data sets, especially when measured using the
MUC score.
Conclusion #2: Improving the ability of coreference re-
solvers to identify coreference elements would likely improve
the state-of-the-art immensely ? by 10-20 points in MUC F1
score and from 2-12 F1 points for B3.
This finding explains previously published re-
sults that exhibit striking variability when run with
annotated CEs vs. system-extracted CEs. On the
MUC6 data set, for example, the best published
MUC score using extracted CEs is approximately
71 (Yang et al, 2003), while multiple systems
have produced MUC scores of approximately 85
when using annotated CEs (e.g. Luo et al (2004),
McCallum and Wellner (2004)).
We argue that providing a resolver with the an-
notated CEs is a rather unrealistic evaluation: de-
termining whether an NP is part of an annotated
coreference chain is precisely the job of a corefer-
ence resolver!
Conclusion #3: Assuming the availability of CEs unre-
alistically simplifies the coreference resolution task.
3.5 Anaphoricity Determination
Finally, several coreference systems have suc-
cessfully incorporated anaphoricity determination
660
modules (e.g. Ng and Cardie (2002a) and Bean
and Riloff (2004)). The goal of the module is to
determine whether or not an NP is anaphoric. For
example, pleonastic pronouns (e.g. it is raining)
are special cases that do not require coreference
resolution.
Unfortunately, neither the MUC nor the ACE
data sets include anaphoricity information for all
NPs. Rather, they encode anaphoricity informa-
tion implicitly for annotated CEs: a CE is consid-
ered anaphoric if is not a singleton.6
To study the utility of anaphoricity informa-
tion, we train and test only on the ?anaphoric? ex-
tracted CEs, i.e. the extracted CEs that have an
annotated twin that is not a singleton. Note that
for the MUC datasets all extracted CEs that have
twins are considered anaphoric.
Results for this experiment (box 6 in Table 3)
are similar to the previous experiment using per-
fect CEs: we observe big improvements across the
board. This should not be surprising since the ex-
perimental setting is quite close to that for perfect
CEs: this experiment also presumes knowledge
of when a CE is part of an annotated coreference
chain. Nevertheless, we see that anaphoricity info-
mation is important. First, good anaphoricity iden-
tification should reduce the set of extracted CEs
making it closer to the set of annotated CEs. Sec-
ond, further improvements in MUC score for the
ACE data sets over the runs using perfect CEs (box
5) reveal that accurately determining anaphoric-
ity can lead to substantial improvements in MUC
score. ACE data includes annotations for single-
ton CEs, so knowling whether an annotated CE is
anaphoric divulges additional information.
Conclusion #4: An accurate anaphoricity determina-
tion component can lead to substantial improvement in coref-
erence resolution performance.
4 Resolution Complexity
Different types of anaphora that have to be han-
dled by coreference resolution systems exhibit dif-
ferent properties. In linguistic theory, binding
mechanisms vary for different kinds of syntactic
constituents and structures. And in practice, em-
pirical results have confirmed intuitions that differ-
ent types of anaphora benefit from different clas-
sifier features and exhibit varying degrees of diffi-
culty (Kameyama, 1997). However, performance
6Also, the first element of a coreference chain is usually
non-anaphoric, but we do not consider that issue here.
evaluations rarely include analysis of where state-
of-the-art coreference resolvers perform best and
worst, aside from general conclusions.
In this section, we analyze the behavior of
our coreference resolver on different types of
anaphoric expressions with two goals in mind.
First, we want to deduce the strengths and weak-
nesses of state-of-the-art systems to help direct
future research. Second, we aim to understand
why current coreference resolvers behave so in-
consistently across data sets. Our hypothesis is
that the distribution of different types of anaphoric
expressions in a corpus is a major factor for coref-
erence resolution performance. Our experiments
confirm this hypothesis and we use our empirical
results to create a coreference performance predic-
tion (CPP) measure that successfully estimates the
expected level of performance on novel data sets.
4.1 Resolution Classes
We study the resolution complexity of a text cor-
pus by defining resolution classes. Resolution
classes partition the set of anaphoric CEs accord-
ing to properties of the anaphor and (in some
cases) the antecedent. Previous work has stud-
ied performance differences between pronominal
anaphora, proper names, and common nouns, but
we aim to dig deeper into subclasses of each of
these groups. In particular, we distinguish be-
tween proper and common nouns that can be re-
solved via string matching, versus those that have
no antecedent with a matching string. Intuitively,
we expect that it is easier to resolve the cases
that involve string matching. Similarly, we par-
tition pronominal anaphora into several subcate-
gories that we expect may behave differently. We
define the following nine resolution classes:
Proper Names: Three resolution classes cover
CEs that are named entities (e.g. the PER-
SON, LOCATION, ORGANIZATION and DATE
classes for MUC and ACE) and have a prior ref-
erent7 in the text. These three classes are distin-
guished by the type of antecedent that can be re-
solved against the proper name.
(1) PN-e: a proper name is assigned to this exact string match
class if there is at least one preceding CE in its gold standard
coreference chain that exactly matches it.
(2) PN-p: a proper name is assigned to this partial string
match class if there is at least one preceding CE in its gold
standard chain that has some content words in common.
(3) PN-n: a proper name is assigned to this no string match
7We make a rough, but rarely inaccurate, assumption that
there are no cataphoric expressions in the data.
661
MUC6 MUC7 ACE2 ACE03 ACE04 ACE05 Avg
# % scr # % scr # % scr # % scr # % scr # % scr % scr
PN-e 273 17 .87 249 19 .79 346 24 .94 435 25 .93 267 16 .88 373 31 .92 22 .89
PN-p 157 10 .68 79 6 .59 116 8 .86 178 10 .87 194 11 .71 125 10 .71 9 .74
PN-n 18 1 .18 18 1 .28 85 6 .19 79 4 .15 66 4 .21 89 7 .27 4 .21
CN-e 292 18 .82 276 21 .65 84 6 .40 186 11 .68 165 10 .68 134 11 .79 13 .67
CN-p 229 14 .53 239 18 .49 147 10 .26 168 10 .24 147 9 .40 147 12 .43 12 .39
CN-n 194 12 .27 148 11 .15 152 10 .50 148 8 .90 266 16 .32 121 10 .20 11 .18
1+2Pr 48 3 .70 65 5 .66 122 8 .73 76 4 .73 158 9 .77 51 4 .61 6 .70
G3Pr 160 10 .73 50 4 .79 181 12 .83 237 13 .82 246 14 .84 69 60 .81 10 .80
U3Pr 175 11 .49 142 11 .49 163 11 .45 122 7 .48 153 9 .49 91 7 .49 9 .48
Table 4: Frequencies and scores for each resolution class.
class if no preceding CE in its gold standard chain has any
content words in common with it.
Common NPs: Three analogous string match
classes cover CEs that have a common noun as a
head: (4) CN-e (5) CN-p (6) CN-n.
Pronouns: Three classes cover pronouns:
(7) 1+2Pr: The anaphor is a 1st or 2nd person pronoun.
(8) G3Pr: The anaphor is a gendered 3rd person pronoun
(e.g. ?she?, ?him?).
(9) U3Pr: The anaphor is an ungendered 3rd person pro-
noun.
As noted above, resolution classes are defined for
annotated CEs. We use the twin relationship to
match extracted CEs to annotated CEs and to eval-
uate performance on each resolution class.
4.2 Scoring Resolution Classes
To score each resolution class separately, we de-
fine a new variant of the MUC scorer. We compute
a MUC-RC score (for MUC Resolution Class) for
class C as follows: we assume that all CEs that do
not belong to class C are resolved correctly by tak-
ing the correct clustering for them from the gold
standard. Starting with this correct partial cluster-
ing, we run our classifier on all ordered pairs of
CEs for which the second CE is of class C, es-
sentially asking our coreference resolver to deter-
mine whether each member of class C is corefer-
ent with each of its preceding CEs. We then count
the number of unique correct/incorrect links that
the system introduced on top of the correct par-
tial clustering and compute precision, recall, and
F1 score. This scoring function directly measures
the impact of each resolution class on the overall
MUC score.
4.3 Results
Table 4 shows the results of our resolution class
analysis on the test portions of the six data sets.
The # columns show the frequency counts for each
resolution class, and the % columns show the dis-
tributions of the classes in each corpus (i.e. 17%
MUC6 MUC7 ACE2 ACE03 ACE04 ACE05
0.92 0.95 0.91 0.98 0.97 0.96
Table 5: Correlations of resolution class scores with respect
to the average.
of all resolutions in the MUC6 corpus were in the
PN-e class). The scr columns show the MUC-
RC score for each resolution class. The right-hand
side of Table 4 shows the average distribution and
scores across all data sets.
These scores confirm our expectations about the
relative difficulty of different types of resolutions.
For example, it appears that proper names are eas-
ier to resolve than common nouns; gendered pro-
nouns are easier than 1st and 2nd person pronouns,
which, in turn, are easier than ungendered 3rd per-
son pronouns. Similarly, our intuition is confirmed
that many CEs can be accurately resolved based on
exact string matching, whereas resolving against
antecedents that do not have overlapping strings is
much more difficult. The average scores in Table 4
show that performance varies dramatically across
the resolution classes, but, on the surface, appears
to be relatively consistent across data sets.
None of the data sets performs exactly the same,
of course, so we statistically analyze whether the
behavior of each resolution class is similar across
the data sets. For each data set, we compute the
correlation between the vector of MUC-RC scores
over the resolution classes and the average vec-
tor of MUC-RC scores for the remaining five data
sets. Table 5 contains the results, which show high
correlations (over .90) for all six data sets. These
results indicate that the relative performance of the
resolution classes is consistent across corpora.
4.4 Coreference Performance Prediction
Next, we hypothesize that the distribution of res-
olution classes in a corpus explains (at least par-
tially) why performance varies so much from cor-
662
MUC6 MUC7 ACE2 ACE03 ACE04 ACE05
P 0.59 0.59 0.62 0.65 0.59 0.62
O 0.67 0.61 0.66 0.68 0.62 0.67
Table 6: Predicted (P) vs Observed (O) scores.
pus to corpus. To explore this issue, we create a
Coreference Performance Prediction (CPP) mea-
sure to predict the performance on new data sets.
The CPP measure uses the empirical performance
of each resolution class observed on previous data
sets and forms a predicton based on the make-up
of resolution classes in a new corpus. The distribu-
tion of resolution classes for a new corpus can be
easily determined because the classes can be rec-
ognized superficially by looking only at the strings
that represent each NP.
We compute the CPP score for each of our six
data sets based on the average resolution class per-
formance measured on the other five data sets.
The predicted score for each class is computed as
a weighted sum of the observed scores for each
resolution class (i.e. the mean for the class mea-
sured on the other five data sets) weighted by the
proportion of CEs that belong to the class. The
predicted scores are shown in Table 6 and com-
pared with the MUC scores that are produced by
RECONCILEACL09.8
Our results show that the CPP measure is a
good predictor of coreference resolution perfor-
mance on unseen data sets, with the exception
of one outlier ? the MUC6 data set. In fact,
the correlation between predicted and observed
scores is 0.731 for all data sets and 0.913 exclud-
ing MUC6. RECONCILEACL09?s performance on
MUC6 is better than predicted due to the higher
than average scores for the common noun classes.
We attribute this to the fact that MUC6 includes
annotations for nested nouns, which almost al-
ways fall in the CN-e and CN-p classes. In ad-
dition, many of the features were first created for
the MUC6 data set, so the feature extractors are
likely more accurate than for other data sets.
Overall, results indicate that coreference perfor-
mance is substantially influenced by the mix of
resolution classes found in the data set. Our CPP
measure can be used to produce a good estimate
of the level of performance on a new corpus.
8Observed scores for MUC6 and 7 differ slightly from Ta-
ble 3 because this part of the work did not use the OPTIONAL
field of the key, employed by the official MUC scorer.
5 Related Work
The bulk of the relevant related work is described
in earlier sections, as appropriate. This paper stud-
ies complexity issues for NP coreference resolu-
tion using a ?good?, i.e. near state-of-the-art, sys-
tem. For state-of-the-art performance on the MUC
data sets see, e.g. Yang et al (2003); for state-of-
the-art performance on the ACE data sets see, e.g.
Bengtson and Roth (2008) and Luo (2007). While
other researchers have evaluated NP coreference
resolvers with respect to pronouns vs. proper
nouns vs. common nouns (Ng and Cardie, 2002b),
our analysis focuses on measuring the complexity
of data sets, predicting the performance of coref-
erence systems on new data sets, and quantify-
ing the effect of coreference system subcompo-
nents on overall performance. In the related area
of anaphora resolution, researchers have studied
the influence of subsystems on the overall per-
formance (Mitkov, 2002) as well as defined and
evaluated performance on different classes of pro-
nouns (e.g. Mitkov (2002) and Byron (2001)).
However, due to the significant differences in task
definition, available datasets, and evaluation met-
rics, their conclusions are not directly applicable
to the full coreference task.
Previous work has developed methods to predict
system performance on NLP tasks given data set
characteristics, e.g. Birch et al (2008) does this for
machine translation. Our work looks for the first
time at predicting the performance of NP corefer-
ence resolvers.
6 Conclusions
We examine the state-of-the-art in NP coreference
resolution. We show the relative impact of perfect
NE recognition, perfect anaphoricity information
for coreference elements, and knowledge of all
and only the annotated CEs. We also measure the
performance of state-of-the-art resolvers on sev-
eral classes of anaphora and use these results to
develop a measure that can accurately estimate a
resolver?s performance on new data sets.
Acknowledgments. We gratefully acknowledge
technical contributions from David Buttler and
David Hysom in creating the Reconcile corefer-
ence resolution platform. This research was sup-
ported in part by the Department of Homeland
Security under ONR Grant N0014-07-1-0152 and
Lawrence Livermore National Laboratory subcon-
tract B573245.
663
References
A. Bagga and B. Baldwin. 1998. Algorithms for Scor-
ing Coreference Chains. In In Linguistic Corefer-
ence Workshop at LREC 1998.
Baldridge, J. 2005. The OpenNLP project.
http://opennlp.sourceforge.net/.
D. Bean and E. Riloff. 2004. Unsupervised Learn-
ing of Contextual Role Knowledge for Coreference
Resolution. In Proceedings of the Annual Meeting
of the North American Chapter of the Association
for Computational Linguistics (HLT/NAACL 2004).
Eric Bengtson and Dan Roth. 2008. Understanding
the Value of Features for Coreference Resolution.
In Proceedings of the 2008 Conference on Empiri-
cal Methods in Natural Language Processing, pages
294?303. Association for Computational Linguis-
tics.
Alexandra Birch, Miles Osborne, and Philipp Koehn.
2008. Predicting Success in Machine Translation.
In Proceedings of the 2008 Conference on Empiri-
cal Methods in Natural Language Processing, pages
745?754. Association for Computational Linguis-
tics.
Donna Byron. 2001. The Uncommon Denomina-
tor: A Proposal for Consistent Reporting of Pro-
noun Resolution Results. Computational Linguis-
tics, 27(4):569?578.
Marie-Catherine de Marneffe, Bill MacCartney, and
Christopher D. Manning. 2006. Generating Typed
Dependency Parses from Phrase Structure Parses. In
LREC.
J. Finkel, S. Dingare, H. Nguyen, M. Nissim, and
C. Manning. 2004. Exploiting Context for Biomed-
ical Entity Recognition: From Syntax to the Web. In
Joint Workshop on Natural Language Processing in
Biomedicine and its Applications at COLING 2004.
Yoav Freund and Robert E. Schapire. 1999. Large
Margin Classification Using the Perceptron Algo-
rithm. In Machine Learning, pages 277?296.
Megumi Kameyama. 1997. Recognizing Referential
Links: An Information Extraction Perspective. In
Workshop On Operational Factors In Practical Ro-
bust Anaphora Resolution For Unrestricted Texts.
Xiaoqiang Luo, Abe Ittycheriah, Hongyan Jing, Nanda
Kambhatla, and Salim Roukos. 2004. A
Mention-Synchronous Coreference Resolution Al-
gorithm Based on the Bell Tree. In Proceedings
of the 42nd Annual Meeting of the Association for
Computational Linguistics.
X. Luo. 2005. On Coreference Resolution Perfor-
mance Metrics. In Proceedings of the 2005 Human
Language Technology Conference / Conference on
Empirical Methods in Natural Language Process-
ing.
Xiaoqiang Luo. 2007. Coreference or Not: A Twin
Model for Coreference Resolution. In Proceedings
of the Annual Meeting of the North American Chap-
ter of the Association for Computational Linguistics
(HLT/NAACL 2007).
A. McCallum and B. Wellner. 2004. Conditional Mod-
els of Identity Uncertainty with Application to Noun
Coreference. In 18th Annual Conference on Neural
Information Processing Systems.
Ruslan Mitkov. 2002. Anaphora Resolution. Long-
man, London.
MUC-6. 1995. Coreference Task Definition. In Pro-
ceedings of the Sixth Message Understanding Con-
ference (MUC-6), pages 335?344.
MUC-7. 1997. Coreference Task Definition. In
Proceedings of the Seventh Message Understanding
Conference (MUC-7).
V. Ng and C. Cardie. 2002a. Identifying Anaphoric
and Non-Anaphoric Noun Phrases to Improve
Coreference Resolution. In Proceedings of the 19th
International Conference on Computational Lin-
guistics (COLING 2002).
V. Ng and C. Cardie. 2002b. Improving Machine
Learning Approaches to Coreference Resolution. In
Proceedings of the 40th Annual Meeting of the As-
sociation for Computational Linguistics.
NIST. 2004. The ACE Evaluation Plan.
S. Petrov and D. Klein. 2007. Improved Inference for
Unlexicalized Parsing. In Proceedings of the Annual
Meeting of the North American Chapter of the Asso-
ciation for Computational Linguistics (HLT/NAACL
2007).
W. Soon, H. Ng, and D. Lim. 2001. A Machine
Learning Approach to Coreference of Noun Phrases.
Computational Linguistics, 27(4):521?541.
Veselin Stoyanov, Nathan Gilbert, Claire Cardie, Ellen
Riloff, David Buttler, and David Hysom. 2009.
Reconcile: A Coreference Resolution Research Plat-
form. Computer Science Technical Report, Cornell
University, Ithaca, NY.
Kees van Deemter and Rodger Kibble. 2000. On
Coreferring: Coreference in MUC and Related
Annotation Schemes. Computational Linguistics,
26(4):629?637.
M. Vilain, J. Burger, J. Aberdeen, D. Connolly, and
L. Hirschman. 1995. A Model-Theoretic Corefer-
ence Scoring Theme. In Proceedings of the Sixth
Message Understanding Conference (MUC-6).
Xiaofeng Yang, Guodong Zhou, Jian Su, and
Chew Lim Tan. 2003. Coreference Resolution Us-
ing Competition Learning Approach. In ACL ?03:
Proceedings of the 41st Annual Meeting on Associa-
tion for Computational Linguistics, pages 176?183.
664
Towards Translingual Information Access 
using Portable Information Extraction 
Michael White, Claire Cardie, Chung-hye Han, Nari Kim, # 
Benoit Lavoie, Martha Palmer, Owen Rainbow,* Juntae Yoon 
CoGenTex, Inc. 
Ithaca, NY, USA 
\[mike,benoit.owen\] 
@cogentex.com 
Institute for Research in 
Cognitive Science 
University of Pennsylvania 
Philadelphia, PA, USA 
chunghye@babel, ling. upenn, edu 
\[ nari, mpalmer, j tyoon } 
@linc. cis.upenn.edu 
Dept. of Computer Science 
Cornell University 
Ithaca, NY, USA 
cardie@cs, cornell, edu 
Abstract 
We report on a small study undertaken to 
demonstrate the feasibility of combining 
portable information extraction with MT in 
order to support translingual information 
access. After describing the proposed 
system's usage scenario and system design, 
we describe our investigation of transferring 
information extraction techniques developed 
for English to Korean. We conclude with a 
brief discussion of related MT issues we plan 
to investigate in future work. 
1 Introduction 
In this paper, we report on a small study 
undertaken to demonstrate the feasibility of 
combining portable information extraction with 
MT in order to support ranslingual information 
access. The goal of our proposed system is to 
better enable analysts to perform information 
filtering tasks on foreign language documents. 
This effort was funded by a SBIR Phase I award 
from the U.S. Army Research Lab, and will be 
pursued further under the DARPA TIDES 
initiative. 
Information extraction (IE) systems are 
designed to extract specific types of information 
from natural language texts. In order to achieve 
acceptable accuracy, IE systems need to be 
tuned for a given topic domain. Since this 
domain tuning can be labor intensive, recent IE 
research has focused on developing learning 
algorithms for training IE system components 
(cf. Cardie, 1997, for a survey). To date, 
however, little work has been done on IE 
systems for languages other than English 
(though cf. MUC-5, 1994, and MUC-7, 1998, 
for Japanese IE systems); and, to our knowledge, 
none of the available techniques for the core task 
of learning information extraction patterns have 
been extended or evaluated for multilingual 
information extraction (though again cf. MUC-7, 
1998, where the use of learning techniques for 
the IE subtasks of named entity recognition and 
coreference r solution are described). 
Given this situation, the primary objective of 
our study was to demonstrate he feasibility of 
using portable--i.e., easily trainable--IE 
technology on Korean documents, focusing on 
techniques for learning information extraction 
patterns. Secondary objectives of the study were 
to elaborate the analyst scenario and system 
design. 
2 Analyst Scenario 
Figure 1 illustrates how an intelligence analyst 
might use the proposed system: 
? The analyst selects one or more Korean 
documents in which to search for 
information (this step not shown). 
# Current affiliation: Konan Technology, Inc., Korea, nari@konantech.co.kr 
* Current affiliation: A'IT Labs-Research, Florham Park, NJ, USA, rambow@research.att.com 
31 
Ouery  
Find Report 
Event: Nest !!lg ........... 
sourcn:l . . . . . .  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  ~ ~ 
sate :  I ................. ' ......................... ' ................. i 
Locat Ion: I~u.'~h K..e~.e..a.; ..................................... j I~ 
Part clpant : I .................................................................. i 
Iseun:i~'North Korea" AND "missiles" i 
I 
Response  to  Ouery  
The reports Indicate 2 meetings held In South Korea on the 
issues of North Korea anti missiles: 
Sources Translated Extracts 
Joon,ap~l A ~ ~  
. . . .  I ,4 ~t ln ,  g ~# ,#~=1# o,1 Apf J l  ~ sLYout tP~ I10t 
,!nerF, orea j /ine? ~t~wn Saoul end Tokyo for  the 
Noes / - -  t~Q I ela~gen?? d~/tu~tlons ~uc/t eg Alottl I  Kofgm'~ 
Trans la t ion  o f  Korean  Source  Repor t  
\[Joongang Dally\] 
Korean. Japanese H in i s ters  Discuss NK Po l i cy  
The tmo ministers ~9rsed that any further launching of a 
missile by North Korean would undermine the security of 
~Northeast Asia and the Korea, the United States and Japan 
should take Joint steps against the North Korean missile 
threat. 
}-long requested that Koeura cork to normalize Japan's 
relations with North Korea. rather than cutting channels 
of dialogue bet#men the two countries. 
Koeura said that i f  North Korea continues Its missile 
testing, the Japanese government will definitely stop 
making contributions to KEDO. 
The tee ministers also tentatively agreed that J~anese 
primo minister Kslzo Obuchl should make a state visit  to 
Korea on or around Nerch 20. 
Korean  Source  Repor t  
E t -~ "~I -D lX i '~  ~oo ~ Cll~o" 
oj_a, xd~. ~ ~.\]Ol D IXF~ ~F ~,~FI,,t ~'-9-, ~.~OF ~t~l.~. ~t l  
ud~Otl ~l . : , r t}  ~\]l~i/ ~ol~.-E.II .?-INto ?,,toiSF.~. ~t.-Ol-~ 8-.~01 
~XlI~II= = ~ZISH LDFPI~_ ~C.F~ uH~C3 ~-~-  ~.1-~..~ OF-..It~ 
~01 ~cF.  
x~.~ ~.~OI ~l,.Lt~ EH~=  ~S lO I  ...~CI.~ ~.~_o~ ~It~,/~F 
a~_tOI LO~O KILL= ~0~OPj ~-~/~1 )H~F ~dXl~ 8~9F 
Figure 1 
The analyst selects one or more scenario 
template, to activate in the query. Each 
scenario template corresponds to a specific 
type of event. Available scenario templates 
might include troop movements, acts of 
violence, meetings and negotiathms, 
protests, etc. In Figure 1, the selected event 
is of type meeting (understood broadly). 
The analyst fills in the available slots of the 
selected scenario template in order to restrict 
the search to the information considered to 
be relevant. In Figure 1, the values specified 
in the scenario template indicate that the 
information to f'md is about meetings having 
as location South Korea and as issue North 
Korea and missiles. The analyst also 
32  
specifies what information s/he wants to be 
reported when information matching the 
query is found. In Figure 1, the selected 
boxes under the Report column indicate that 
all information found satisfying the query 
should be reported except for the meeting 
participants. 1 
? Once the analyst submits the query for 
evaluation, the system searches the input 
documents for information matching the 
query. As a result, a hypertext document is 
generated describing the information 
matching the query as well as the source of 
this information. Note that the query 
contains English keywords that are 
automatically translated into Korean prior to 
matching. The extracted information is 
presented in English after being translated 
from Korean. In Figure 1, the generated 
hypertext response indicates two documents 
in the input set that matched the query 
totally or in part. Each summary in the 
response includes just the translations of the 
extracted information that the analyst 
requested to be reported. 
? For each document extract matching the 
analyst query, the analyst can obtain a 
complete machine translation of the Korean 
document where the match was found, and 
where the matched information is 
highlighted. Working with a human 
translator, the analyst can also verify the 
accuracy of the reported information by 
accessing the documents in their original 
language. 
3 System Design 
Figure 2 shows the high-level design of the 
system. It consists of the following components: 
? The User Interface. The browser-based 
interface is for entering queries and 
displaying the resulting presentations. 
? The Portable Information Extractor (PIE) 
component. The PIE component uses the 
While in this example the exclusion of participant 
information in the resulting report is rather artificial, 
in general a scenario template may contain many 
different ypes of information, not all of which are 
likely to interest an analyst at once. 
Extraction Pattem Library - -  which 
contains the set of extraction patterns 
learned in the lab, one set per scenario 
template - -  to extract specific types of 
information from the input Korean 
documents, once parsed. 
? The Ranker component. This component 
ranks the extracted information returned by 
the PIE component according to how well it 
matches the keyword restrictions in the 
query. The MT component's English-to- 
Korean Transfer Lexicon is used to map the 
English keywords to corresponding Korean 
ones. When the match falls below a user- 
? configurable threshold, the extracted 
information is filtered out. 
? The MT component. The MT component 
(cf. Lavoie et al, 2000) translates the 
extracted Korean phrases or sentences into 
corresponding English ones. 
? The Presentation Generator component. 
This component generates well-organized, 
easy-to-read hypertext presentations by 
organizing and formatting the ranked 
extracted information. It uses existing NLG 
components, including the Exemplars text 
planning framework (White and Caldwell, 
1998) and the RealPro syntactic realizer 
(Lavoie and Rainbow, 1997). 
In our feasibility study, the majority of the effort 
went towards developing the PIE component, 
described in the next section. This component 
was implemented in a general way, i.e. in a way 
that we would expect to work beyond the 
specific training/test corpus described below. In 
contrast, we only implemented initial versions of 
the User Interface, Ranker and Presentation 
Generator components, in order to demonstrate 
the system concept; that is, these initial versions 
were only intended.to work with our training/test 
corpus, and will require considerable further 
development prior to reaching operational status. 
For the MT component, we used an early 
version of the lexical transfer-based system 
currently under development in an ongoing 
SBIR Phase II project (cf. Nasr et al, 1997; 
Palmer et al, 1998; Lavoie et al, 2000), though 
with a limited lexicon specifically for translating 
the slot fillers in our training/test corpus. 
33 
Korean Documents 
Parser 
Tagged l 
Korean Documents ( LexiconK?rean 1
~ Syntactic . . . . . .  Eaglish Grammar Structure (English) RealPro 
English Lexicon / ' S~'ntactic Realizer Sentence (English) 
t Parsed Document ~ ::i~i?~'~vii~i? ' .~:Qi~I~:i~-'-iL \[:!::ili:::.:: ~t r~.  :::::::::::::::::::::::: 
Extracted Information \[ 
(Korean) 
Ordered Extracted 
Information(Korean) 
Parsed Document \] Machine "lYanslation I ( 
~l Component (MT) 
Ordered Extracted 
Information (English) 
User Input Data Presentation (E glish) 
Information Extraction 
Query (English) 1 
i : rla0 Inf0rntauonl 
English-Korean 7 
Transfer Lexicon J
Korean-English 
Transfer Lexicon ) 
T 
Miiiiii ii 
? 
Presentation (English) 
End user Document Processing Knowledge base 
component component 
D (C)OTS component 
\[\]Component created in Phase I 
\[\]Component created or improved in Phase II 
Figure 2 
4 Portable Information Extraction 
4.1 Scenario Template and Training/Fest 
Corpus 
For our Phase I feasibility demonstration, we 
chose a minimal scenario template for meeting 
and negotiation events consisting of one or more 
participant slots plus optional date and location 
slots. 2 We then gathered a small corpus of thirty 
articles by searching for articles containing 
"North Korea" and one or more of about 15 
keywords. The first two sentences (with a few 
exceptions) were then annotated with the slots to 
be extracted, leading to a total of 51 sentences 
containing 47 scenario templates and 89 total 
2 In the end, we did not use the 'issue' slot shown in 
Figure 1, as it contained more complex Idlers than 
those that ypically have been handled in IE systems. 
correct slots. Note that in a couple of cases 
more than one template was given for a single 
long sentence. 
When compared to the MUC scenario 
template task, our extraction task was 
considerably simpler, for the following reasons: 
* The answer keys only contained information 
that could be found within a single sentence, 
i.e. the answer keys did not require merging 
information across entences. 
? The answer keys did not require anaphoric 
references to be resolved, and we did not 
deal with conjuncts eparately. 
? We did not attempt o normalize dates or 
remove appositives from NPs. 
4.2 Extraction Pattern Learning 
For our feasibility study, we chose to follow the 
AutoSlog (Lehnert et al, 1992; Riloff, 1993) 
approach to extraction pattern acquisition. In 
this approach, extraction patterns are acquired 
34 
i. E: 
K: 
<target-np>=<subject> <active voice verb> 
<participant> MET 
<target-np>=<subject> <active voice verb> 
<John-i> MANNASSTA 
<John-nom>'MET 
2. E: 
K: 
<target-np>=<subject> <verb> <infinitive> 
<participant> agreed to MEET 
<target-np>=<subject> <verbl-ki- lo> <verb2> 
<John-un> MANNA-ki- lo hapuyhayssta 
<John-nom> MEET-ki- lo agreed 
(-ki: nominalization ending, -io: an adverbial postposition) 
Figure 3 
via a one-shot general-to-specific learning 
algorithm designed specifically for the 
information extraction task. 3 The learning 
algorithm is straightforward and depends only 
on the existence of a (partial) parser and a small 
set of general inguistic patterns that direct the 
creation of specific patterns. As a training 
corpus, it requires a set of texts with noun 
phrases annotated with the slot type to be 
extracted. 
To adapt the AutoSlog approach to Korean, 
we first devised Korean equivalents of the 
English patterns, two of which are shown in 
Figure 3. It turned out that for our corpus, we 
could collapse some of these patterns, though 
some new ones were also needed. In the end we 
used just nine generic patterns. 
Important issues that arose in adapting the 
approach were (1) greater flexibility in word 
order and heavier reliance on morphological 
cues in Korean, and (2) the predominance of 
light verbs (verbs with little semantic ontent of 
their own) and aspectual verbs in the chosen 
domain. We discuss these issues in the next two 
sections. 
4.3 Korean Parser 
We used Yoon's hybrid statistical Korean parser 
(Yoon et al, 1997, 1999; Yoon, 1999) to process 
the input sentences prior to extraction. The 
parser incorporates a POS tagger and 
3 For TIDES, we plan to use more sophisticated 
learning algorithms, as well as active learning 
techniques, such as those described in Thompson et 
al. (1999). 
morphological nalyzer and yields a dependency 
representation as its output? The use of a 
dependency representation e abled us to handle 
the greater flexibility in word order in Korean. 
To facilitate pattern matching, we wrote a 
simple program to convert he parser's output o 
XML form. During the XML conversion, two 
simple heuristics were applied, one to recover 
implicit subjects, and another to correct a 
recurring misanalysis of noun compounds. 
4.4 Trigger Word Filtering and 
Generalization 
In the newswire corpus we looked at, meeting 
events were rarely described with the verb 
'mannata' ('to meet'). Instead, they were 
usually described with a noun that stands for 
'meeting' and a light or aspectual verb, for 
example, 'hoyuy-lul kacta' ('to have a meeting') 
or 'hoyuy-lul machita' ('to finish a meeting'). 
In order to acquire extraction patterns that made 
appropriate use of such collocations, we decided 
to go beyond the AutoSlog approach and 
explicitly group trigger words (such as 'hoyuy') 
into classes, and to likewise group any 
collocations, such as those involving light verbs 
or aspectual verbs. To fmd collocations for the 
trigger words, we reviewed a Korean lexical co- 
occurrence base which was constructed from a 
corpus of 40 million words (Yoon et al, 1997). 
We then used the resulting specification to filter 
the learned patterns to just those containing the 
4 Overall dependency precision is reported to be 
89.4% (Yoon, 1999). 
35 
. - !  
trigger words or trigger word collocations, as 
well as to generalize the patterns to the word 
class level. Because the number of tr:igger 
words is small, this specification can be done 
quickly, and soon pays off in terms of time 
saved in manually filtering the learned patterns. 
4.5 Results 
In testing our approach, we obtained overall 
results of 79% recall and 67% precision in a 
hold-one-out cross validation test. In a cross 
validation test, one repeatedly divides a corpus 
into different raining and test sets, averaging the 
results; in the hold-one-out version, the system 
is tested on a held-out example after being 
trained on the rest. In the IE setting, the recall 
measure is the number of correct slots found 
divided by the total number of correct slots, 
while the precision measure is the number of 
correct slots found divided by the total number 
of slots found. 
While direct comparisons with the MUC 
conference results cannot be made for the 
reasons we gave above, we nevertheless 
consider these results quite promising, as these 
scores exceed the best scores reported at MUC-6 
on the scenario template task. 5 
Table 1: Hold-One-Out Cross Validation 
Slots Recall Precision 
All 79% 67% 
Participant 75% 84% 
Date/Location 86% 54% 
Table2: Hold-One-OutCross Validat~n 
wi~outGeneralizafion 
Slots Recall Precision 
All 61% 64% 
Participant 57% 81% 
Date/Location 67% 52% 
A breakdown by slot is shown in Table 1. We 
may note that precision is low for date and 
location slots because we used a simplistic 
sentence-level merge, rather than dependencies. 
To measure the impact of our approach to 
generalization, we may compare the results in 
5 
http://www.nist.gov/itl/div894/894.02/related_project 
s/tipster/muc.htm 
Table 1 with those shown in Table 2, where 
generalization is not used. As can be seen, the 
generalization step adds substantially to overall 
recall. 
To illustrate the effect of generalization, 
consider the pattern to extract he subject NP of 
the light verb 'kac (hold)' when paired with an 
object NP headed by the noun 'hyepsang 
(negotiation)'. Since this pattern only occurs 
once in our corpus, the slot is not successfully 
extracted in the cross-validation test without 
generalization. However, since this example 
does fall under the more generalized pattern of 
extracting the subject NP of a verb in the light 
verb class when paired with an object NP 
headed by a noun the 'hoytam-hyepsang' class, 
the slot is successfully extracted in the cross- 
validation test using the generalized patterns. 
Cases like these are the source of the 18% boost 
in recall of participant slots, from 57% to 75%. 
5 Discussion 
Our feasibility study has focused our attention 
on several questions concerning the interaction 
of IE and MT, which we hope to pursue under 
the DARPA TIDES initiative. One question is 
the extent o which slot filler translation is more 
practicable than general-purpose MT; one would 
expect to achieve much higher quality on slot 
fillers, as they are typically relatively brief noun 
phrases, and instantiation of a slot implies a 
degree of semantic lassification. On the other 
hand, one might find that higher quality is 
required in order to take translated phrases out 
of their original context. Another question is 
how to automate the construction of bilingual 
lexicons. An important issue here will be how 
to combine information from different sources, 
given that automatically acquired lexical 
information is apt to be less reliable, though 
domain-specific. 
Acknowledgements 
Our thanks go to Richard Kittredge and Tanya 
Korelsky for helpful comments and advice. This 
work was supported by ARL contract DAAD 17- 
99-C-0005. 
36 
References 
Cardie, C. (1997). Empirical Methods in Information 
Extraction. AI Magazine 18(4):65-79. 
Lavoie, B. and Rambow, O. (1997). RealPro - -  A 
fast, portable sentence realizer. In Proceedings of 
the Conference on Applied Natural Language 
Processing (ANLP'97), Washington, DC. 
Lavoie, B., Korelsky, T., and Rambow, O. (2000). A 
Framework for MT and Multilingual NLG Systems 
Based on Uniform Lexico-Structural Processing. 
To appear in Proceedings of the Sixth Conference 
on Applied Natural Language Processing (ANLP- 
2000), Seattle, WA. 
Lehnert, W., Cardie, C., Fisher, D., McCarthy, J., 
Riloff, E., and Soderland, S. (1992). University of 
Massachusetts: Description of the CIRCUS system 
as used in MUC-4. In Proceedings of the Fourth 
Message Understanding Conference (MUC-4), 
pages 282-288, San Mateo, CA. Morgan 
Kaufmann. 
MUC-5 (1994). Proceedings of the Fifth Message 
Understanding Conference (MUC-5). Morgan 
Kaufmann, San Mateo, CA. 
MUC-7 (1998). Proceedings of the Seventh Message 
Understanding Conference (MUC-7). Morgan 
Kaufmann, San Francisco, CA. 
Nasr, A., Rambow, O., Palmer, M., and Rosenzweig, 
J. (1997). Enriching lexical transfer with cross- 
linguistic semantic features. In Proceedings of the 
lnterlingua Workshop at the MT Summit, San 
Diego, CA. 
Palmer, M., Rambow, O., and Nasr, A. (1998). 
Rapid prototyping of domain-specific machine 
translation systems. In Machine Translation and 
the Information Soup - Proceedings of the Third 
Conference of the Association for Machine 
Translation in the Americas AMTA'98, Springer 
Verlag (Lecture Notes in Artificial Intelligence No. 
1529), Berlin. 
Riloff, E. (1993). Automatically constructing a
dictionary for information exlxaction tasks. In 
Proceedings of the Eleventh National Conference 
on Artificial Intelligence, pages 811-816, 
Washington, DC. AAAI Press / MIT Press. 
Thompson, C. A., Califf, M. E., and Mooney, R. J. 
(1999). Active learning for natural language 
parsing and information extraction. In Proceedings 
of the Sixteenth International Machine Learning 
Conference (1CML-99), Bled, Slovenia. 
White, M. and Caldwell, T. (1998). EXEMPLARS: A 
practical, extensible framework for dynamic text 
generation. In Proceedings of the 8th International 
Workshop on Natural Language Generation, 
Niagara-on-the-Lake, Ontario. 
Yoon, J. (1999). Efficient dependency parsing based 
on three types of chunking and lexical association. 
Submitted. 
Yoon, J., Choi, K.-S., and Song, M. (1999). Three 
types of chunking in Korean and dependency 
analysis based on lexical association. In 
Proceedings of lCCPOL. 
Yoon, J., Kim, S., and Song, M. (1997). New parsing 
method using global association table. In 
Proceedings of the 5th International Workshop on 
Parsing Technology. 
37 
 	
Selecting Sentences for Multidocument Summaries using
Randomized Local Search
Michael White
CoGenTex, Inc.
840 Hanshaw Road
Ithaca, NY 14850, USA
mike@cogentex.com
Claire Cardie
Dept. of Computer Science
Cornell University
Ithaca, NY 14850, USA
cardie@cs.cornell.edu
Abstract
We present and evaluate a randomized local
search procedure for selecting sentences to in-
clude in a multidocument summary. The search
favors the inclusion of adjacent sentences while
penalizing the selection of repetitive material,
in order to improve intelligibility without un-
duly affecting informativeness. Sentence simi-
larity is determined using both surface-oriented
measures and semantic groups obtained from
merging the output templates of an information
extraction subsystem. In a comparative evalu-
ation against two DUC-like baselines and three
simpler versions of our system, we found that
our randomized local search method provided
substantial improvements in both content and
intelligibility, while the use of the IE groups also
appeared to contribute a small further improve-
ment in content.
1 Introduction
Improving the intellibility of multidocument
summaries remains a significant challenge.
While most previous approaches to multidoc-
ument summarization have addressed the prob-
lem of reducing repetition, less attention has
been paid to problems of coherence and co-
hesion. In a typical extractive system (e.g.
Goldstein et al (2000)), sentences are selected
for inclusion in the summary one at a time,
with later choices sensitive to their similarity
to earlier ones; the selected sentences are then
ordered either chronologically or by relevance.
The resulting summaries often jump incoher-
ently from topic to topic, and contain broken
cohesive links, such as dangling anaphors or un-
met presuppositions.
Barzilay et al (2001) present an improved
method of ordering sentences in the context
of MultiGen, a multidocument summarizer
that identifies sets of similar sentences, termed
themes, and reformulates their common phrases
as new text. In their approach, topically related
themes are identified and kept together in the
resulting summary, in order to help improve co-
hesion and reduce topic switching.
In this paper, we pursue a related but simpler
idea in an extractive context, namely to favor
the selection of blocks of adjacent sentences in
constructing a multidocument summary. Here,
the challenge is to improve intelligibility with-
out unduly sacrificing informativeness; for ex-
ample, selecting the beginning of the most re-
cent article in a document set will usually pro-
duce a highly intelligible text, but one that is
not very representative of the document set as
a whole.
To manage this tradeoff, we have developed a
randomized local search procedure (cf. Selman
and Kautz (1994)) to select the highest ranking
set of sentences for the summary, where the in-
clusion of adjacent sentences is favored and the
selection of repetitive material is penalized. The
method involves greedily searching for the best
combination of sentences to swap in and out of
the current summary until no more improve-
ments are possible; noise strategies include oc-
casionally adding a sentence to the current sum-
mary, regardless of its score, and restarting the
local search from random starting points for a
fixed number of iterations. In determining sen-
tence similarity, we have used surface-oriented
similarity measures obtained from Columbia?s
SimFinder tool (Hatzivassiloglou et al, 2001),
as well as semantic groups obtained from merg-
ing the output templates of an information ex-
traction (IE) subsystem.
In related work, Marcu (2001) describes an
approach to balancing informativeness and in-
telligibility that also involves searching through
        Philadelphia, July 2002, pp. 9-18.  Association for Computational Linguistics.
          Proceedings of the Workshop on Automatic Summarization (including DUC 2002),
sets of sentences to select. In contrast to
our approach, Marcu employs a beam search
through possible summaries of progressively
greater length, which seems less amenable to
an anytime formulation; this may be an im-
portant practical consideration, since Marcu re-
ports search times in hours, whereas we have
found that less than a minute of searching is
usually effective. In other related work, Lin
and Hovy (2002) suggest pairing extracted sen-
tences with their corresponding lead sentences;
we have not directly compared our search-based
approach to Lin and Hovy?s simpler method.
In order to evaluate our approach, we com-
pared 200-word summaries generated by our
system to those of two baselines that are similar
to those used in DUC 2001 (Harman, 2001), and
to three simpler versions of the system, where
a simple marginal relevance selection procedure
was used instead of the selection search, and/or
the IE groups were ignored. In general, we
found that our randomized local search method
provided substantial improvements in both con-
tent and intelligibility over the DUC-like base-
lines and the simplest variant of our system,
which used marginal relevance selection and no
IE groups (with the exception that the last arti-
cle baseline was always ranked first in intelligi-
bility). The use of the IE groups also appeared
to contribute a small further improvement in
content when used with our selection search.
We discuss these results in greater detail in the
final section of the paper.
2 System Description
We have implemented our randomized local
search method for sentence selection as part
of the RIPTIDES (White et al, 2001) sys-
tem. RIPTIDES combines information extrac-
tion (IE) in the domain of natural disasters and
multidocument summarization to produce hy-
pertext summaries. The hypertext summaries
include a high-level textual overview; tables of
all comparable numeric estimates, organized to
highlight discrepancies; and targeted access to
supporting information from the original arti-
cles. In White et al (2002), we showed that the
hypertext summaries can help to identify dis-
repancies in numeric estimates, and provide a
significantly more complete picture of the avail-
able information than the latest article. The
next subsection walks through a sample hyper-
text summary; it is followed by descriptions of
the IE and Summarizer system components.
2.1 Example
Figure 1 shows a textual overview of the first
dozen or so articles in a corpus of news arti-
cles gathered from the web during the first week
after the January 2001 earthquake in Central
America. Clicking on the magnifying glass icon
brings up the original article in the right frame,
with the extracted sentences highlighted.
The index to the hypertext summary appears
in the left frame of figure 1. Links to the
overview and to the lead sentences of the arti-
cles are followed by links to tables that display
the base level extraction slots for the main event
(here, an earthquake) including its description,
date, location, epicenter and magnitude. Access
to the overall damage estimates appears next,
with separate tables for types of human effects
(e.g. dead, missing) and for object types (e.g.
villages, bridges, houses) with physical effects.
Figure 2 shows the extracted estimates of the
overall death toll. In order to help identify dis-
crepancies, the high and low current estimates
are shown at the top, followed by other cur-
rent estimates and then all extracted estimates.
Heuristics are used to determine which esti-
mates to consider current, taking into account
the source (either news source or attributed
source), specificity (e.g. hundreds vs. at least
200) and confidence level, as indicated by the
presence of hedge words such as perhaps or as-
sumed. The tables also provide links to the orig-
inal articles, allowing the user to quickly and
directly determine the accuracy of any estimate
in the table.
2.2 IE System
The IE system combines existing language tech-
nology components (Bikel et al, 1997; Char-
niak, 1999; Day et al, 1997; Fellbaum, 1998) in
a traditional IE architecture (Cardie, 1997; Gr-
ishman, 1996). Unique features of the system
include a weakly supervised extraction pattern-
learning component, Autoslog-XML, which is
based on Autoslog-TS (Riloff, 1996), but op-
erates in an XML framework and acquires pat-
terns for extracting text elements beyond noun
phrases (e.g. verb groups, adjectives, adverbs,
and single-noun modifiers). In addition, a
Figure 1: Hypertext Summary Overview
Figure 2: Tables of Death Toll Estimates
heuristic-based clustering algorithm organizes
the extracted concepts into output templates
specifically designed to support multi-document
summarization: the IE system, for example, dis-
tinguishes different reports or views of the same
event from multiple sources (White et al, 2001).
Output templates from the IE system for each
text to be covered in the multi-document sum-
mary are provided as input to the summariza-
tion component along with all linguistic anno-
tations accrued in the IE phase.
2.3 Summarizer
The Summarizer operates in three main stages.
In the first stage, the IE output templates are
merged into an event-oriented structure where
comparable facts are semantically grouped. To-
wards the same objective, surface-oriented clus-
tering is used to group sentences from different
documents into clusters that are likely to report
similar content. In the second stage, importance
scores are assigned to the sentences based on the
following indicators: position in document, doc-
ument recency, presence of quotes, average sen-
tence overlap, headline overlap, size of cluster
(if any), size of semantic groups (if any), speci-
ficity of numeric estimates, and whether these
estimates are deemed current. In the third and
final stage, the hypertext summary is generated
from the resulting content pool. Further details
on each stage follow in the paragraphs below;
see White et al (2002) for a more complete de-
scription.
In the analysis stage, we use Columbia?s
SimFinder tool (Hatzivassiloglou et al, 2001) to
obtain surface-oriented similarity measures and
clusters for the sentences in the input articles.
To obtain potentially more accurate partitions
using the IE output, we semantically merge the
extracted slots into comparable groups, i.e. ones
whose members can be examined for discrepan-
cies. This requires distinguishing (i) different
types of damage; (ii) overall damage estimates
vs. those that pertain to a specific locale; and
(iii) damage due to related events, such as previ-
ous quakes in the same area. During this stage,
we also analyze the numeric estimates for speci-
ficity and confidence level, and determine which
estimates to consider current.
In the scoring stage, SimFinder?s similarity
measures and clusters are combined with the
semantic groupings obtained from merging the
IE templates in order to score the input sen-
tences. The scoring of the clusters and seman-
tic groups is based on their size, and the scores
are combined at the sentence level by includ-
ing the score of all semantic groups that con-
tain a phrase extracted from a given sentence.
More precisely, the scores are assigned in three
phases, according to a set of hand-tuned pa-
rameter weights. First, a base score is assigned
to each sentence according to a weighted sum
of the position in document, document recency,
presence of quotes, average sentence overlap,
and headline overlap. The average sentence
overlap is the average of all pairwise sentence
similarity measures; we have found this measure
to be a useful counterpart to sentence position
in reliably identifying salient sentences, with the
other factors playing a lesser role. In the second
scoring phase, the clusters and semantic groups
are assigned a score according to the sum of the
base sentence scores. After normalization in the
third scoring phase, the weighted cluster and
group scores are used to boost the base scores,
thereby favoring sentences from the more im-
portant clusters and semantic groups. Finally,
a small boost is applied for currenten and more
specific numeric estimates.
In the generation stage, the overview is con-
structed by selecting a set of sentences in a
context-sensitive fashion, and then ordering the
blocks of adjacent sentences according to their
importance scores. The summarization scoring
model begins with the sum of the scores for
the candidate sentences, which is then adjusted
to penalize the inclusion of multiple sentences
from the same cluster or semantic group, or sen-
tences whose similarity measure is above a cer-
tain threshold, and to favor the inclusion of ad-
jacent sentences from the same article, in order
to boost intelligibility. A larger bonus is applied
when including a sentence that begins with an
initial pronoun as well as the previous one, and
an even bigger bonus is added when including
a sentence that begins with a strong rhetorical
marker (e.g. however) as well as its predecessor;
corresponding penalties are also used when the
preceding sentence is missing, or when a short
sentence appears without an adjacent one.
To select the sentences for the overview ac-
cording to this scoring model, we use an itera-
tive randomized local search procedure inspired
by Selman and Kautz (1994). Two noise strate-
gies are employed to lessen the problem of lo-
cal maxima in the search space: (i) the local
search is restarted from random starting points,
for a fixed number of iterations, and (ii) during
each local search iteration, greedy steps are in-
terleaved with random steps, where a sentence
is added regardless of its score. In the first local
search iteration, the initial sentence collection
consists of the highest scoring sentences up to
the word limit. In subsequent iterations, the ini-
tial collection is composed of randomly selected
sentences, weighted according to their scores,
up to the word limit. During each local search
iteration, a random step or a greedy step (cho-
sen at random) is repeatedly performed until a
greedy step fails to improve upon the current
collection of sentences. In each greedy step, one
sentence is chosen to add to the collection, and
zero or more (typically one) sentences are cho-
sen to remove from the collection, such that the
word limit is still met, and this combination of
sentences represents the best swap available ac-
cording to the scoring model. After the prede-
termined number of iterations, the best combi-
nation of sentences found during the search is
output; note that the algorithm could easily be
formulated in an anytime fashion as well. From
a practical perspective, we have found that 10
iterations often suffices to find a reasonable col-
lection of sentences, taking well under a minute
on a desktop PC.
Once the overview sentences have been se-
lected, the hypertext summary is generated as a
collection of HTML files, using a series of XSLT
transformations.
2.4 Training and Tuning
For the evaluation below, the IE system was
trained on 12 of 25 texts from topic 89 of the
TDT2 corpus, a set of newswires that describe
the May 1998 earthquake in Afganistan. It
achieves 42% recall and 61% precision when
evaluated on the remaining 13 topic 89 texts.
The parameter settings of the Summarizer were
chosen by hand using the complete TDT2 topic
89 document set as input.
3 Evaluation Method and Results
To select the inputs for the evaluation, we took
five subsets of the articles from TDT2 topic
89 ? all the articles up to the end of days 1
through 5 after the quake. We chose to use
TDT2 topic 89 so that we could assess the im-
pact of the IE quality on the results, given that
we had previously created manual IE annota-
tions for these articles (White et al, 2001).1
1Although our decision to use subsets of TDT2 topic
89 as inputs meant that our training/tuning and test
data overlapped, we do not believe that this choice overly
compromises our results, since ? as will be discussed in
this section and the next ? the impact of the IE groups
For each input document set, we ran the RIP-
TIDES system to produce overview summaries
of 200 words or less. For comparison purposes,
we also ran two baselines, similar to those used
in DUC 2001 (Harman, 2001), and three sim-
pler versions of the system, for a total of six
summary types:
Last The first N sentences of the latest article
in the document set, up to the word limit.
Leads The lead sentences from the latest arti-
cles in the document set, up to the word
limit, listed in chronological order.
MR The top ranking sentences selected accord-
ing to their thresholded marginal relevance,
up to the word limit, listed in chronological
order, using RIPTIDES to score the sen-
tences, except with the IE groups zeroed
out.
MR+IE The MR summarization method, but
with the IE groups included for the RIP-
TIDES sentence scorer.
Search The RIPTIDES overview, except with
the IE groups zeroed out for sentence scor-
ing.
Search+IE The RIPTIDES overview.
The marginal relevance systems (MR and
MR+IE) used a simple selection mechanism
which does not involve search, inspired by the
maximal marginal relevance (MMR) approach
(Goldstein et al, 2000). This selection mech-
anism begins by selecting the top ranking sen-
tence for inclusion, then determines whether to
include the second ranking sentence depending
on whether it is sufficiently dissimilar from the
first one, based on comparing the SimFinder
similarity measure against a hard threshold
(0.85), and likewise for lower ranked sentences,
comparing them against all sentences included
so far, up to the word limit. The selected sen-
tences are then gathered into blocks of adjacent
sentences, and ordered chronologically.2
turned out to be small, while with our selection search,
we ran into a couple of problems on the test data that
did not show up in tuning the parameter settings.
2In trying out the MR systems on all the articles in
TDT2 topic 89, we found chronological ordering to usu-
ally be more coherent than importance ordering.
Content Rank, Simulated IE
Last Leads MR MR+IE Search Search+IE
Day 1 5, 6 5, 5 4, 4 3, 3 1, 1 1, 1
Day 2 5, 4 6, 6 1, 4 4, 3 2, 1 1, 2
Day 3 6, 6 3, 3 3, 3 5, 5 1, 1 1, 1
Day 4 6, 5 3, 6 3, 4 3, 2 2, 2 1, 1
Day 5 6, 6 2, 5 2, 4 2, 2 2, 2 1, 1
Average 5.5 ?0.7 4.4 ?1.5 3.2 ?1.0 3.1 ?1.2 1.6 ?0.7 1.1 ?0.3
Table 1: Content Rankings on TDT2 Topic 89, using Simulated IE. The scores for the two judges
at each time point are separated by commas.
Intelligibility Rank, Simulated IE
Last Leads MR MR+IE Search Search+IE
Day 1 1, 1 5, 2 6, 6 3, 2 2, 4 3, 5
Day 2 1, 1 5, 5 6, 5 4, 4 2, 2 2, 3
Day 3 1, 1 6, 5 5, 6 3, 4 3, 1 2, 1
Day 4 1, 1 6, 6 5, 5 3, 3 3, 4 2, 2
Day 5 1, 1 4, 6 4, 5 4, 3 2, 4 2, 2
Average 1 ?0 5 ?1.2 5.3 ?0.7 3.3 ?0.7 2.7 ?1.1 2.4 ?1.1
Table 2: Intelligibility Rankings on TDT2 Topic 89, using Simulated IE.
Content Rank, Actual IE
Last Leads MR MR+IE Search Search+IE
Day 1 5, 6 5, 5 4, 4 3, 3 2, 1 1, 1
Day 2 5, 4 6, 6 1, 4 3, 3 3, 2 1, 1
Day 3 6, 6 1, 2 1, 2 5, 5 3, 1 3, 2
Day 4 6, 5 4, 6 1, 2 5, 4 1, 1 1, 2
Day 5 6, 6 2, 5 4, 2 4, 4 2, 2 1, 1
Average 5.5 ?0.7 4.2 ?1.9 2.5 ?1.4 3.9 ?0.9 1.8 ?0.8 1.4 ?0.7
Table 3: Content Rankings on TDT2 Topic 89, using Actual IE.
Intelligibility Rank, Actual IE
Last Leads MR MR+IE Search Search+IE
Day 1 1, 1 5, 2 6, 6 3, 2 2, 4 3, 5
Day 2 1, 1 5, 5 6, 5 4, 4 2, 2 2, 2
Day 3 1, 1 6, 4 5, 6 2, 3 2, 2 2, 4
Day 4 1, 1 6, 6 4, 4 4, 3 3, 2 2, 4
Day 5 1, 1 4, 6 4, 5 4, 2 3, 4 2, 3
Average 1 ?0 4.9 ?1.3 5.1 ?0.9 3.1 ?0.9 2.6 ?0.8 2.9 ?1.1
Table 4: Intelligibility Rankings on TDT2 Topic 89, using Actual IE.
TDT2 Topic 89, Simulated IE
1
1.5
2
2.5
3
3.5
4
4.5
5
5.5
6
Las
t
Lea
ds MR
MR
+IE
Sea
rch
Sea
rch
+IE
System
Av
g 
Ra
nk Content
Intelligibility
Figure 3: Average System Rank for Content and Intelligibility on TDT2 Topic 89, using Simulated
IE. The ranks are averaged across two judges and five time points; manual IE annotations were
used with the MR+IE and Search+IE systems.
TDT2 Topic 89, Actual IE
11.522.533.544.555.56
Las
t
Lea
ds MR
MR
+IE
Sea
rch
Sea
rch
+IE
System
Av
g 
Ra
nk Content
Intelligibility
Figure 4: Average System Rank for Content and Intelligibility on TDT2 Topic 89, using Actual
IE. The ranks are averaged across two judges and five time points; actual IE annotations were used
with the MR+IE and Search+IE systems, making all systems fully automatic.
For each of the five time points, we ran the
six systems on two versions of the input docu-
ment sets, one with the manual IE annotations
(simulated IE) and one with the automatic IE
annotations (actual IE). Note that with each of
the first three systems, the output did not differ
from one version of the input to the other, since
these systems did not depend on the IE anno-
tations and did not involve randomized search.
Next, for each document set, we had two judges3
rank the summaries from best to worst, with
3The authors were the judges.
Significant Differences in System Versions
Last Leads MR MR+IE Search Search+IE
Last Int(Sim,Act) Con(Sim,Act) Con(Sim,Act) Con(Sim,Act) Con(Sim,Act)
Int(Sim,Act) Int(Sim,Act) Int(Sim,Act) Int(Sim,Act)
Leads - Int(Sim,Act) Con(Sim,Act) Con(Sim,Act)
Int(Sim,Act) Int(Sim,Act)
MR Int(Sim,Act) Con(Sim) Con(Sim)
Int(Sim,Act) Int(Sim,Act)
MR+IE Con(Act) Con(Sim,Act)
Search -
Table 5: Significant Differences in System Versions based on Pairwise t-Tests. Table entries indicate
statistically significant differences (at the 95% confidence level) in mean Content and Intelligibility
rank on TDT2 Topic 89 texts using Simulated or Actual output from the IE system.
ties allowed, in two categories, content and in-
telligibility. In the case of ties, the tied systems
shared the appropriate ranking; for example, if
two summaries tied for the best content, each
received a rank of 1, with the next best sum-
mary receiving a rank of 3 (cf. Olympic pairs
skating).
The charts in figures 3 and 4 show the sys-
tem rank for content and intelligibility for the
simulated IE and actual IE versions of the doc-
ument sets, respectively, averaged across the
two judges and five time points. Tables 1
through 4 list all the judgements together with
their means and standard deviations.
In general, we found that Search and
Search+IE provided substantial improvements
in both content and intelligibility over Last,
Leads and MR, with the exception that
Last was always ranked first in intelligibility.
Search+IE also appeared to show a small fur-
ther improvement in content.
Determining the significance of the improve-
ments is somewhat complex, due to the small
number of data points and the use of multiple
comparisons. To judge the significance levels,
we calculated pairwise t-tests for all the means
listed in tables 1 through 4, and applied the
Bonferroni adjustment, which is a conservative
way to perform multiple comparisons where the
total chance of error is spread across all com-
parisons.4
4With the total ? equal to 0.05, the Bonferroni ad-
justment provides a 95% confidence level that all the
pairwise judgements are correct. In our case, a total ?
of 0.05 corresponds to an individual ? of 0.0033, which
is difficult to exceed with a small number of data points.
Statisitically significant differences in system
performance at the 95% confidence level appear
in table 5. Turning first to the content rankings,
with the simulated IE output, we found that
both Search and Search+IE scored significantly
higher than Last, Leads and MR. While the dif-
ference between Search and Search+IE was not
significant, only Search+IE achieved a signifi-
cantly higher average rank than MR+IE. With
the actual IE output, Search and Search+IE
again scored significantly higher than Last and
Leads and, although these two systems did not
show a significant improvement over MR, both
systems did improve significantly over Leads
and MR+IE, in contrast to MR.
Turning now to the intelligibility rankings,
with both the simulated and actual IE and, we
found that Search and Search+IE improved sig-
nificantly over Leads and MR. The difference
between Search and Search+IE was not signifi-
cant. Surprisingly, MR+IE scored significantly
higher than MR, and not significantly worse
than Search and Search+IE.
4 Discussion
We were pleased with the substantial improve-
ments in both content and intelligibility that
our randomized local search method provided
over the DUC-like baselines and the simplest
variant of our system, the one using marginal
relevance selection and no IE groups (with the
exception that the last article baseline was al-
ways ranked first in intelligibility). We did not
expect to find that the selection search would
yield substantial improvements over marginal
relevance selection in the content rankings, since
the search method was designed to improve in-
telligibility without unduly affecting content.
At the same time though, we were somewhat
disappointed that the use of the IE groups ap-
peared to only contribute a small further im-
provement in content when used with our selec-
tion search.
It is not entirely clear why our selection
search method led to improvements in the con-
tent rankings when compared to the marginal
relevance variants. One possibility is that the
randomized local search was able to find sen-
tences with greater information density. An-
other possibility is that the use of a hard thresh-
old by the marginal relevance variants led to
some poor sentence selections fairly far down on
the list of ranked sentences; the marginal rele-
vance selection may have worked better had we
used a smaller threshold, or if we had re-ranked
the sentences following each selection according
to a redundancy penalty, rather than simply us-
ing a threshold.
It is also not clear why the IE groups did
not help more with content selection. It may
well be that a more elaborate evaluation, involv-
ing more systems and judgements, would indeed
show that the IE groups yielded significant im-
provements in content rankings. On the other
hand, our results may indicate that shallow ex-
tractive techniques can pick up much the same
information as IE techniques, at least for the
purpose of selecting sentences for generic extrac-
tive summaries. Note that for purposes of dis-
crepancy detection, the ability of IE techniques
to more completely extract relevant phrases is
clearly demonstrated in White et al (2002).
On the intelligibility side, we were surprised
to find that the IE groups led to improvements
in intelligibility when used with marginal rele-
vance selection. One likely explanation for this
improvement is that this system variant jumped
around less from topic to topic than its counter-
part that did not make use of the IE info.
Another question is why the selection search
did not yield further improvements in intelli-
bility. One reason is that the search method
always selected sentences up to the word limit,
even when this yielded highly repetitive sum-
maries ? as was the case with the first two test
sets, which only contained a handful of articles.
Another reason is that the search routine was
prone to selecting a couple of sentences from an
article that was largely off topic, only containing
a brief mention of the quake.
These deficiencies point to possible improve-
ments in the search method: informativeness
could perhaps be balanced with conciseness by
deselecting sentences that do not improve the
overall score; and off-topic sentences could per-
haps be avoided by taking into account the cen-
trality of the document in the sentence scores.
More speculatively, it would be interesting to
extend the approach to work with sub-sentential
units, and to make use of a greater variety of
inter-sentential cohesive links.
Acknowledgments
This work was supported in part by DARPA
TIDES contract N66001-00-C-8009 and NSF
Grants 0081334 and 0074896. We thank
Tanya Korelsky, Daryl McCullough, Vincent
Ng, David Pierce and Kiri Wagstaff for their
help with earlier versions of the system.
References
Regina Barzilay, Noemie Elhadad, and Kath-
leen McKeown. 2001. Sentence Ordering in
Multidocument Summarization. In Proceed-
ings of the First International Conference on
Human Language Technology Research, San
Diego, CA.
D. Bikel, S. Miller, R. Schwartz, and
R. Weischedel. 1997. Nymble: A High-
Performance Learning Name-Finder. In Pro-
ceedings of the Fifth Conference on Applied
Natural Language Processing, pages 194?201,
San Francisco, CA. Morgan Kaufmann.
C. Cardie. 1997. Empirical Methods in Infor-
mation Extraction. AI Magazine, 18(4):65?
79.
Eugene Charniak. 1999. A maximum-entropy-
inspired parser. Technical Report CS99-12,
Brown University.
D. Day, J. Aberdeen, L. Hirschman,
R. Kozierok, P. Robinson, and M. Vi-
lain. 1997. Mixed-Initiative Development
of Language Processing Systems. In Pro-
ceedings of the Fifth Conference on Applied
Natural Language Processing. Association for
Computational Linguistics.
C. Fellbaum. 1998. WordNet: An Electronic
Lexical Database. MIT Press, Cambridge,
MA.
J. Goldstein, V. Mittal, J. Carbonell, and
M. Kantrowitz. 2000. Multi-document sum-
marization by sentence extraction. In Pro-
ceedings of the ANLP/NAACL Workshop on
Automatic Summarization, Seattle, WA.
R. Grishman. 1996. TIPSTER Archi-
tecture Design Document Version 2.2.
Technical report, DARPA. Available at
http://www.tipster.org/.
Donna Harman. 2001. Proceedings of the 2001
Document Understanding Conference (DUC-
2001). NIST.
Vasileios Hatzivassiloglou, Judith L. Klavans,
Melissa L. Holcombe, Regina Barzilay, Min-
Yen Kan, and Kathleen R. McKeown. 2001.
Simfinder: A flexible clustering tool for sum-
marization. In Proceedings of the NAACL
2001 Workshop on Automatic Summariza-
tion, Pittsburgh, PA.
Chin-Yew Lin and Eduard Hovy. 2002. Au-
tomated Multi-document Summarization in
NeATS. In Proceedings of the Second In-
ternational Conference on Human Language
Technology Research, San Diego, CA. To ap-
pear.
Daniel Marcu. 2001. Discourse-Based Sum-
marization in DUC-2001. In Proceedings of
the 2001 Document Understanding Confer-
ence (DUC-2001).
E. Riloff. 1996. Automatically Generating Ex-
traction Patterns from Untagged Text. In
Proceedings of the Thirteenth National Con-
ference on Artificial Intelligence, pages 1044?
1049, Portland, OR. AAAI Press / MIT
Press.
Bart Selman and Henry Kautz. 1994. Noise
Strategies for Improving Local Search. In
Proceedings of AAAI-94.
Michael White, Tanya Korelsky, Claire Cardie,
Vincent Ng, David Pierce, and Kiri Wagstaff.
2001. Multidocument Summarization via In-
formation Extraction. In Proceedings of the
First International Conference on Human
Language Technology Research, San Diego,
CA.
Michael White, Claire Cardie, Vincent Ng, and
Daryl McCullough. 2002. Detecting Discrep-
ancies in Numeric Estimates Using Multidoc-
ument Hypertext Summaries. In Proceedings
of the Second International Conference on
Human Language Technology Research, San
Diego, CA. To appear.
Combining Sample Selection and Error-Driven Pruning for
Machine Learning of Coreference Rules
Vincent Ng and Claire Cardie
Department of Computer Science
Cornell University
Ithaca, NY 14853-7501
 
yung,cardie  @cs.cornell.edu
Abstract
Most machine learning solutions to noun
phrase coreference resolution recast the
problem as a classification task. We ex-
amine three potential problems with this
reformulation, namely, skewed class dis-
tributions, the inclusion of ?hard? training
instances, and the loss of transitivity in-
herent in the original coreference relation.
We show how these problems can be han-
dled via intelligent sample selection and
error-driven pruning of classification rule-
sets. The resulting system achieves an F-
measure of 69.5 and 63.4 on the MUC-
6 and MUC-7 coreference resolution data
sets, respectively, surpassing the perfor-
mance of the best MUC-6 and MUC-7
coreference systems. In particular, the
system outperforms the best-performing
learning-based coreference system to date.
1 Introduction
Noun phrase coreference resolution refers to the
problem of determining which noun phrases (NPs)
refer to each real-world entity mentioned in a doc-
ument. Machine learning approaches to this prob-
lem have been reasonably successful, operating pri-
marily by recasting the problem as a classification
task (e.g. Aone and Bennett (1995), McCarthy and
Lehnert (1995), Soon et al (2001)). Specifically, an
inductive learning algorithm is used to train a classi-
fier that decides whether or not two NPs in a docu-
ment are coreferent. Training data are typically cre-
ated by relying on coreference chains from the train-
ing documents: training instances are generated by
pairing each NP with each of its preceding NPs; in-
stances are labeled as positive if the two NPs are in
the same coreference chain, and labeled as negative
otherwise.1
A separate clustering mechanism then coordinates
the possibly contradictory pairwise coreference clas-
sification decisions and constructs a partition on the
set of NPs with one cluster for each set of corefer-
ent NPs. Although, in principle, any clustering algo-
rithm can be used, most previous work uses a single-
link clustering algorithm to impose coreference par-
titions.2 An implicit assumption in the choice of the
single-link clustering algorithm is that coreference
resolution is viewed as anaphora resolution, i.e. the
goal during clustering is to find an antecedent for
each anaphoric NP in a document.3
Three intrinsic properties of coreference4, how-
ever, make the formulation of the problem as a
classification-based single-link clustering task po-
tentially undesirable:
Coreference is a rare relation. That is, most
NP pairs in a document are not coreferent. Con-
1Two NPs are in the same coreference chain if and only if
they are coreferent.
2One exception is Kehler?s work on probabilistic corefer-
ence (Kehler, 1997), in which he applies Dempster?s Rule of
Combination (Dempster, 1968) to combine all pairwise proba-
bilities of coreference to form a partition.
3In this paper, we consider an NP anaphoric if it is part of a
coreference chain but is not the head of the chain.
4Here, we use the term coreference loosely to refer to either
the problem or the binary relation defined on a set of NPs. The
particular choice should be clear from the context.
                                            Association for Computational Linguistics.
                      Language Processing (EMNLP), Philadelphia, July 2002, pp. 55-62.
                         Proceedings of the Conference on Empirical Methods in Natural
sequently, generating training instances by pairing
each NP with each of its preceding NPs creates
highly skewed class distributions, in which the num-
ber of positive instances is overwhelmed by the
number of negative instances. For example, the stan-
dard MUC-6 and MUC-7 (1995; 1998) coreference
data sets contain only 2% positive instances. Un-
fortunately, learning in the presence of such skewed
class distributions remains an open area of research
in the machine learning community (e.g. Pazzani et
al. (1994), Fawcett (1996), Cardie and Howe (1997),
Kubat and Matwin (1997)).
Coreference is a discourse-level problem with dif-
ferent solutions for different types of NPs. The
interpretation of a pronoun, for example, may be de-
pendent only on its closest antecedent and not on the
rest of the members of the same coreference chain.
Proper name resolution, on the other hand, may be
better served by ignoring locality constraints alto-
gether and relying on string-matching or more so-
phisticated aliasing techniques. Consequently, gen-
erating positive instances from all pairs of NPs from
the same coreference chain can potentially make the
learning task harder: all but a few coreference links
derived from any chain might be hard to identify
based on the available contextual cues.
Coreference is an equivalence relation. Recast-
ing the problem as a classification task precludes en-
forcement of the transitivity constraint. After train-
ing, for example, the classifier might determine that
A is coreferent with B, and B with C, but that A and
C are not coreferent. Hence, the clustering mecha-
nism is needed to coordinate these possibly contra-
dictory pairwise classifications. In addition, because
the coreference classifiers are trained independent of
the clustering algorithm to be used, improvements in
classification accuracy do not guarantee correspond-
ing improvements in clustering-level accuracy, i.e.
overall performance on the coreference resolution
task might not improve.
This paper examines each of the above issues.
First, to address the problem of skewed class dis-
tributions, we apply a technique for negative in-
stance selection similar to that proposed in Soon et
al. (2001). In contrast to results reported there, how-
ever, we show empirically that system performance
increases noticeably in response to negative example
selection, with increases in F-measure of 3-5%.
Second, in an attempt to avoid the inclusion of
?hard? training instances, we present a corpus-based
method for implicit selection of positive instances.
The approach is a fully automated variant of the ex-
ample selection algorithm introduced in Harabagiu
et al (2001). With positive example selection, sys-
tem performance (F-measure) again increases, by
12-14%.
Finally, to more tightly tie the classification- and
clustering-level coreference decisions, we propose
an error-driven rule pruning algorithm that opti-
mizes the coreference classifier ruleset with respect
to the clustering-level coreference scoring function.
Overall, the use of pruning boosts system perfor-
mance from an F-measure of 69.3 to 69.5, and from
57.2 to 63.4 for the MUC-6 and MUC-7 data sets,
respectively, enabling the system to achieve perfor-
mance that surpasses that of the best MUC corefer-
ence systems by 4.6% and 1.6%. In particular, the
system outperforms the best-performing learning-
based coreference system (Soon et al, 2001) by
6.9% and 3.0%.
The remainder of the paper is organized as fol-
lows. In sections 2 and 3, we present the machine
learning framework underlying the baseline corefer-
ence system and examine the effect of negative sam-
ple selection. Section 4 presents our corpus-based
algorithm for selection of positive instances. Section
5 describes and evaluates the error-driven pruning
algorithm. We conclude with future work in section
6.
2 The Machine Learning Framework for
Coreference Resolution
Our machine learning framework for coreference
resolution is a standard combination of classification
and clustering, as described above.
Creating an instance. An instance in our machine
learning framework is a description of two NPs in a
document. More formally, let NP be the  th NP in
document  . An instance formed from NP  and NP 
is denoted by 	
 ffBootstrapping Coreference Classifiers with
Multiple Machine Learning Algorithms
Vincent Ng and Claire Cardie
Department of Computer Science
Cornell University
Ithaca, NY 14853-7501
{yung,cardie}@cs.cornell.edu
Abstract
Successful application of multi-view co-
training algorithms relies on the ability to
factor the available features into views that
are compatible and uncorrelated. This can
potentially preclude their use on problems
such as coreference resolution that lack an
obvious feature split. To bootstrap coref-
erence classifiers, we propose and eval-
uate a single-view weakly supervised al-
gorithm that relies on two different learn-
ing algorithms in lieu of the two different
views required by co-training. In addition,
we investigate a method for ranking un-
labeled instances to be fed back into the
bootstrapping loop as labeled data, aiming
to alleviate the problem of performance
deterioration that is commonly observed
in the course of bootstrapping.
1 Introduction
Co-training (Blum and Mitchell, 1998) is a weakly
supervised paradigm that learns a task from a small
set of labeled data and a large pool of unlabeled
data using separate, but redundant views of the data
(i.e. using disjoint feature subsets to represent the
data). To ensure provable performance guaran-
tees, the co-training algorithm assumes as input a
set of views that satisfies two fairly strict condi-
tions. First, each view must be sufficient for learn-
ing the target concept. Second, the views must be
conditionally independent of each other given the
class. Empirical results on artificial data sets by
Muslea et al (2002) and Nigam and Ghani (2000)
confirm that co-training is sensitive to these assump-
tions. Indeed, although the algorithm has been ap-
plied successfully to natural language processing
(NLP) tasks that have a natural view factorization
(e.g. web page classification (Blum and Mitchell,
1998) and named entity classification (Collins and
Singer, 1999)), there has been little success, and a
number of reported problems, when applying co-
training to NLP data sets for which no natural fea-
ture split has been found (e.g. anaphora resolution
(Mueller et al, 2002)).
As a result, researchers have begun to investigate
co-training procedures that do not require explicit
view factorization. Goldman and Zhou (2000) and
Steedman et al (2003b) use two different learning
algorithms in lieu of the multiple views required by
standard co-training.1 The intuition is that the two
learning algorithms can potentially substitute for the
two views: different learners have different rep-
resentation and search biases and can complement
each other by inducing different hypotheses from the
data. Despite their similarities, the principles under-
lying the Goldman and Zhou and Steedman et al
co-training algorithms are fundamentally different.
In particular, Goldman and Zhou rely on hypothesis
testing to select new instances to add to the labeled
data. On the other hand, Steedman et al use two
learning algorithms that correspond to coarsely dif-
ferent features, thus retaining in spirit the advantages
1Steedman et al (2003b) bootstrap two parsers that use dif-
ferent statistical models via co-training. Hence, the two parsers
can effectively be viewed as two different learning algorithms.
provided by conditionally independent feature splits
in the Blum and Mitchell algorithm.
The goal of this paper is two-fold. First, we
propose a single-view algorithm for bootstrapping
coreference classifiers. Like anaphora resolution,
noun phrase coreference resolution is a problem for
which a natural feature split is not readily available.
In related work (Ng and Cardie, 2003), we com-
pare the performance of the Blum and Mitchell co-
training algorithm with that of two existing single-
view bootstrapping algorithms ? self-training with
bagging (Banko and Brill, 2001) and EM (Nigam et
al., 2000) ? on coreference resolution, and show
that single-view weakly supervised learners are a vi-
able alternative to co-training for the task. This pa-
per instead focuses on developing a single-view al-
gorithm that combines aspects of each of the Gold-
man and Zhou and Steedman et al algorithms.
Second, we investigate a new method that, in-
spired by Steedman et al (2003a), ranks unlabeled
instances to be added to the labeled data in an at-
tempt to alleviate a problem commonly observed in
bootstrapping experiments ? performance deterio-
ration due to the degradation in the quality of the
labeled data as bootstrapping progresses (Pierce and
Cardie, 2001; Riloff and Jones, 1999).
In a set of baseline experiments, we first demon-
strate that multi-view co-training fails to boost the
performance of the coreference system under var-
ious parameter settings. We then show that our
single-view weakly supervised algorithm success-
fully bootstraps the coreference classifiers, boost-
ing the F-measure score by 9-12% on two standard
coreference data sets. Finally, we present experi-
mental results that suggest that our method for rank-
ing instances is more resistant to performance dete-
rioration in the bootstrapping process than Blum and
Mitchell?s ?rank-by-confidence? method.
2 Noun Phrase Coreference Resolution
Noun phrase coreference resolution refers to the
problem of determining which noun phrases (NPs)
refer to each real-world entity mentioned in a doc-
ument.2 In this section, we give an overview of
the coreference resolution system to which the boot-
2Concrete examples of the coreference task can be found in
MUC-6 (1995) and MUC-7 (1998).
strapping algorithms will be applied.
The framework underlying the coreference sys-
tem is a standard combination of classification and
clustering (see Ng and Cardie (2002) for details).
Coreference resolution is first recast as a classifica-
tion task, in which a pair of NPs is classified as co-
referring or not based on constraints that are learned
from an annotated corpus. A separate clustering
mechanism then coordinates the possibly contradic-
tory pairwise classifications and constructs a parti-
tion on the set of NPs. When the system operates
within the weakly supervised setting, a weakly su-
pervised algorithm bootstraps the coreference classi-
fier from the given labeled and unlabeled data rather
than from a much larger set of labeled instances. The
clustering algorithm, however, is not manipulated by
the bootstrapping procedure.
3 Learning Algorithms
We employ naive Bayes and decision list learners
in our single-view, multiple-learner framework for
bootstrapping coreference classifiers. This section
gives an overview of the two learners.
3.1 Naive Bayes
A naive Bayes (NB) classifier is a generative classi-
fier that assigns to a test instance i with feature val-
ues <x
1
, . . ., x
m
> the maximum a posteriori (MAP)
label y?, which is determined as follows:
y? = arg max
y
P (y | i)
= arg max
y
P (y)P (i | y)
= arg max
y
P (y)
m
?
i = 1
P (x
i
| y)
The first equality above follows from the definition
of MAP, the second one from Bayes rule, and the last
one from the conditional independence assumption
of the feature values. We determine the class priors
P(y) and the class densities P(x
i
| y) directly from
the training data using add-one smoothing.
3.2 Decision Lists
Our decision list (DL) algorithm is based on that de-
scribed in Collins and Singer (1999). For each avail-
able feature f
i
and each possible value v
j
of f
i
in the
training data, the learner induces an element of the
Observations Justifications
Many feature-value pairs alone can de-
termine the class value.3 For example,
two NPs cannot be coreferent if they differ
in gender or semantic class.
Decision lists draw a decision boundary based on a single feature-value pair
and can take advantage of this observation directly. On the other hand, naive
Bayes classifiers make a decision based on a combination of features and
thus cannot take advantage of this observation directly.
The class distributions in coreference
data sets are skewed. Specifically, the
fact that most NP pairs in a document are
not coreferent implies that the negative in-
stances grossly outnumber the positives.
Naive Bayes classifiers are fairly resistant to class skewness, which can
only exert its influence on classifier prediction via the class priors. On the
other hand, decision lists suffer from skewed class distributions. Elements
corresponding to the negative class tend to aggregate towards the beginning
of the list, causing the classifier to perform poorly on the minority class.
Many instances contain redundant in-
formation as far as classification is con-
cerned. For example, two NPs may dif-
fer in both gender and semantic class, but
knowing one of these two differences is suf-
ficient for determining the class value.
Both naive Bayes classifiers and decision lists can take advantage of data
redundancy. Frequency counts of feature-value pairs in these classifiers are
updated independently, and thus a single instance can possibly contribute to
the discovery of more than one useful feature-value pair. On the other hand,
some classifiers such as decision trees are not able to take advantage of this
redundancy because of their intrinsic nature of recursive data partitioning.
Table 1: The justifications (shown in the right column) for using naive Bayes and decision list learner as
the underlying learning algorithms for bootstrapping coreference classifiers are based on the corresponding
observations on the coreference task and the features used by the coreference system in the left column.
decision list for each class y. The elements in the list
are sorted in decreasing order of the strength associ-
ated with each element, which is defined as the con-
ditional probability P(y | f
i
= v
j
) and is estimated
based on the training data as follows:
P (y | f
i
= v
j
) =
N (f
i
= v
j
, y) + ?
N (f
i
= v
j
) + k?
N (x) is the frequency of event x in the training
data, ? a smoothing parameter, and k the number
of classes. In this paper, k = 2 and we set ? to 0.01.
A test instance is assigned the class associated with
the first element of the list whose predicate is satis-
fied by the description of the instance.
While generative classifiers estimate class densi-
ties, discriminative classifiers like decision lists fo-
cus on approximating class boundaries. Table 1 pro-
vides the justifications for choosing these two learn-
ers as components in our single-view, multi-learner
bootstrapping algorithm. Based on observations of
the coreference task and the features employed by
our coreference system, the justifications suggest
that the two learners can potentially compensate for
each other?s weaknesses.
4 Multi-View Co-Training
In this section, we describe the Blum and Mitchell
(B&M) multi-view co-training algorithm and apply
it to coreference resolution.
3This justifies the use of a decision list as a potential classi-
fier for bootstrapping. See Yarowsky (1995) for details.
4.1 The Multi-View Co-Training Algorithm
The intuition behind the B&M co-training algorithm
is to train two classifiers that can help augment each
other?s labeled data by exploiting two separate but
redundant views of the data. Specifically, each clas-
sifier is trained using one view of the labeled data
and predicts labels for all instances in the data pool,
which consists of a randomly chosen subset of the
unlabeled data. Each then selects its most confident
predictions, and adds the corresponding instances
with their predicted labels to the labeled data while
maintaining the class distribution in the labeled data.
The number of instances to be added to the la-
beled data by each classifier at each iteration is lim-
ited by a pre-specified growth size to ensure that
only the instances that have a high probability of be-
ing assigned the correct label are incorporated. The
data pool is replenished with instances from the un-
labeled data and the process is repeated.
During testing, each classifier makes an indepen-
dent decision for a test instance. In this paper, the
decision associated with the higher confidence is
taken to be the final prediction for the instance.
4.2 Experimental Setup
One of the goals of the experiments is to enable a
fair comparison of the multi-view algorithm with
our single-view bootstrapping algorithm. Since the
B&M co-training algorithm is sensitive not only to
the views employed but also to other input parame-
MUC-6 MUC-7
Naive Bayes Decision List Naive Bayes Decision List
Experiments R P F R P F R P F R P F
Baseline 50.7 52.6 51.6 17.9 72.0 28.7 40.1 40.2 40.1 32.4 78.3 45.8
Multi-view Co-Training 33.3 90.7 48.7 19.5 71.2 30.6 32.9 76.3 46.0 32.4 78.3 45.8
Single-view Bootstrapping 53.6 79.0 63.9 40.1 83.1 54.1 43.5 73.2 54.6 38.3 75.4 50.8
Self-Training 48.3 63.5 54.9 18.7 70.8 29.6 40.1 40.2 40.1 32.9 78.1 46.3
Table 2: Results of multi-view co-training, single-view bootstrapping, and self-training. Recall, Precision, and
F-measure are provided. Except for the baselines, the best results (F-measure) achieved by the algorithms are shown.
ters such as the pool size and the growth size (Pierce
and Cardie, 2001), we evaluate the algorithm under
different parameter settings, as described below.
Evaluation. We use the MUC-6 (1995) and MUC-
7 (1998) coreference data sets for evaluation. The
training set is composed of 30 ?dry run? texts, from
which 491659 and 482125 NP pair instances are
generated for the MUC-6 and MUC-7 data sets, re-
spectively. Unlike Ng and Cardie (2003) where we
choose one of the dryrun texts (contributing ap-
proximately 3500?3700 instances) form the labeled
data set, however, here we randomly select 1000 in-
stances. The remaining instances are used as un-
labeled data. Testing is performed by applying the
bootstrapped coreference classifier and the cluster-
ing algorithm described in section 2 on the 20?30
?formal evaluation? texts for each of the MUC-6 and
MUC-7 data sets.
Two sets of experiments are conducted, one using
naive Bayes as the underlying supervised learning
algorithm and the other the decision list learner. All
results reported are averages across five runs.
Co-training parameters. The co-training param-
eters are set as follows.
Views. We used three methods to generate the
views from the 25 features used by the coreference
system: Mueller et al?s (2002) greedy method, ran-
dom splitting of features into views, and splitting
of features according to the feature type (i.e. lexico-
syntactic vs. non-lexico-syntactic features).4
Pool size. We tested values of 500, 1000, 5000.
Growth size. We tested values of 10, 50, 100, 200.
4.3 Results and Discussion
Results are shown in Table 2, where performance is
reported in terms of recall, precision, and F-measure
4Space limitation precludes a detailed description of these
methods. See Ng and Cardie (2003) for details.
0 50 100 150 200 250 300 350 400 450 500
20
30
40
50
60
70
80
90
100
Number of Co?Training Iterations
Sc
or
e
Baseline
Recall
Precision
F?measure
Figure 1: Learning curve for co-training (pool size
= 500, growth size = 50, views formed by randomly
splitting the features) for MUC-6.
using the model-theoretic MUC scoring program
(Vilain et al, 1995). The baseline coreference sys-
tem, which is trained only on the initially labeled
data using all of the features, achieves an F-measure
of 51.6 (NB) and 28.7 (DL) on the MUC-6 data set
and 40.1 (NB) and 45.8 (DL) on MUC-7.
The results shown in row 2 of Table 2 correspond
to the best F-measure scores achieved by co-training
across all of the parameter combinations described
in the previous subsection. In comparison to the
baseline, co-training is able to improve system per-
formance in only two of the four classifier/data set
combinations: F-measure increases by 2% and 6%
for MUC-6/DL and MUC-7/NB, respectively. Nev-
ertheless, co-training produces high-precision clas-
sifiers in all four cases (at the expense of recall). In
practical applications in which precision is critical,
the co-training classifiers may be preferable to the
baseline classifiers despite the fact that they achieve
similar F-measure scores.
Figure 1 depicts the learning curve for the co-
training run that gives rise to the best F-measure for
the MUC-6 data set using naive Bayes. The hor-
izontal (dotted) line shows the performance of the
baseline system, as described above. As co-training
progresses, F-measure rises to 48.7 at iteration ten
and gradually drops to and stabilizes at 42.9. We ob-
serve similar performance trends for the other clas-
sifier/data set combinations. The drop in F-measure
is potentially due to the pollution of the labeled data
by mislabeled instances (Pierce and Cardie, 2001).
5 Single-View Bootstrapping
In this section, we describe and evaluate our single-
view, multi-learner bootstrapping algorithm, which
combines ideas from Goldman and Zhou (2000) and
Steedman et al (2003b). We will start by giving an
overview of these two co-training algorithms.
5.1 Related Work
The Goldman and Zhou (G&Z) Algorithm.
This single-view algorithm begins by training two
classifiers on the initially labeled data using two
different learning algorithms; it requires that each
classifier partition the instance space into a set of
equivalence classes (e.g. in a decision tree, each leaf
node defines an equivalence class). Each classi-
fier then considers each equivalence class and uses
hypothesis testing to determine if adding all unla-
beled instances within the equivalence class to the
other classifier?s labeled data will improve the per-
formance of its counterparts. The process is then
repeated until no more instances can be labeled.
The Steedman et al (Ste) Algorithm. This algo-
rithm is a variation of B&M applied to two diverse
statistical parsers. Initially, each parser is trained on
the labeled data. Each then parses and scores all
sentences in the data pool, and then adds the most
confidently parsed sentences to the training data of
the other parser. The parsers are retrained, and the
process is repeated for several iterations.
The algorithm differs from B&M in three main
respects. First, the training data of the two parsers
diverge after the first co-training iteration. Second,
the data pool is flushed and refilled entirely with in-
stances from the unlabeled data after each iteration.
This reduces the possibility of having unreliably la-
beled sentences accumulating in the pool. Finally,
the two parsers, each of which is assumed to hold a
unique ?view? of the data, are effectively two differ-
ent learning algorithms.
5.2 Our Single-View Bootstrapping Algorithm
As mentioned before, our algorithm uses two dif-
ferent learning algorithms to train two classifiers on
the same set of features (i.e. the full feature set).
At each bootstrapping iteration, each classifier la-
bels and scores all instances in the data pool. The
highest scored instances labeled by one classifier are
added to the training data of the other classifier and
vice versa. Since the two classifiers are trained on
the same view, it is important to maintain a separate
training set for each classifier: this reduces the prob-
ability that the two classifiers converge to the same
hypothesis at an early stage and hence implicitly in-
creases the ability to bootstrap. Like Ste, the entire
data pool is replenished with instances drawn from
the unlabeled data after each iteration, and the pro-
cess is repeated. So our algorithm is effectively Ste
applied to coreference resolution ? instead of two
parsing algorithms that correspond to different fea-
tures, we use two learning algorithms, each of which
relies on the same set of features as in G&Z. The
similarities and differences among B&M, G&Z, Ste,
and our algorithm are summarized in Table 3.
5.3 Results and Discussion
We tested different pool sizes and growth sizes as
specified in section 4.2 to determine the best pa-
rameter setting for our algorithm. For both data
sets, the best F-measure score is achieved using a
pool size of 5000 and a growth size of 50. The re-
sults under this parameter setting are given in row
3 of Table 2. In comparison to the baseline, we see
dramatic improvement in F-measure for both clas-
sifiers and both data sets. In addition, we see si-
multaneous gains in recall and precision in all cases
except MUC-7/DL. Furthermore, single-view boot-
strapping beats co-training (in terms of F-measure
scores) by a large margin in all four cases. These
results provide suggestive evidence that single-view,
multi-learner bootstrapping might be a better alter-
native to its multi-view, single-learner counterparts
for coreference resolution.
The bootstrapping run that corresponds to this pa-
rameter setting for the MUC-6 data set using naive
Bayes is shown in Figure 2. Again, we see a ?typi-
Blum and Mitchell Goldman and Zhou Steedman et al Ours
Bootstrapping basis Use different views Use different learners Use different parsers Use different learners
Number of instances
added per iteration
Fixed Variable Fixed Fixed
Training sets for the
two learners/parsers
Same Different Different Different
Data pool flushed af-
ter each iteration
No N/A (No data pool is
used)
Yes Yes
Example selection
method
Highest scored in-
stances
Instances in all
equivalance classes
that are expected to
improve a classifier
Highest scored sen-
tences
Highest scored in-
stances
Table 3: Summary of the major similarities and differences among four bootstrapping schemes: Blum and
Mitchell, Goldman and Zhou, Steedman et al, and ours. Only the relevant dimensions are discussed here.
0 500 1000 1500 2000 2500 3000 3500 4000
40
45
50
55
60
65
70
75
80
85
Number of Co?Training Iterations
Sc
or
e
Baseline
Recall
Precision
F?measure
Figure 2: Learning curve for our single-view boot-
strapping algorithm (pool size = 5000, growth size =
50) for MUC-6.
cal? bootstrapping curve: an initial rise in F-measure
followed by a gradual deterioration. In comparison
to Figure 1, the recall level achieved by co-training is
much lower than that of single-view bootstrapping.
This appears to indicate that each co-training view is
insufficient for learning the target concept: the fea-
ture split limits any interaction of features that can
produce better recall.
Finally, Figure 2 shows that performance in-
creases most rapidly in the first 200 iterations. This
provides indirect evidence that the two classifiers
have acquired different hypotheses from the ini-
tial data and are exchanging information with each
other. To ensure that the classifiers are indeed bene-
fiting from each other, we conducted a self-training
experiment for each classifier separately: at each
self-training iteration, each classifier labels all 5000
instances in the data pool using all available features
and selects the most confidently labeled 50 instances
for addition to its labeled data.5 The best F-measure
scores achieved by self-training are shown in the last
row of Table 2. Overall, self-training only yields
marginal performance gains over the baseline.
Nevertheless, self-training outperforms co-
training in both cases where naive Bayes is used.
While these results seem to suggest that co-training
is inherently handicapped for coreference resolu-
tion, there are two plausible explanations against
this conclusion. First, the fact that self-training has
access to all of the available features may account
for its superior performance to co-training. This is
again partially supported by the fact that the recall
level achieved by co-training is lower than that of
self-training in both cases in which self-training
outperforms co-training. Second, 1000 instances
may simply not be sufficient for co-training to be
effective for this task: in related work (Ng and
Cardie, 2003), we find that starting with 3500?3700
labeled instances instead of 1000 allows co-training
to improve the baseline by 4.6% and 9.5% in
F-measure using naive Bayes classifiers for the
MUC-6 and MUC-7 data sets, respectively.
6 An Alternative Ranking Method
As we have seen before, F-measure scores ulti-
mately decrease as bootstrapping progresses. If the
drop were caused by the degradation in the quality
of the bootstrapped data, then a more ?conservative?
instance selection method than that of B&M would
help alleviate this problem. Our hypothesis is that
selection methods that are based solely on the con-
fidence assigned to an instance by a single classifier
5Note that this is self-training without bagging, unlike the
self-training algorithm discussed in Ng and Cardie (2003).
i1
> i
2
if any of the following is true:
[?(C
1
(i
1
)) = ?(C
2
(i
1
))] ? [?(C
1
(i
2
)) = ?(C
2
(i
2
))]
[?(C
1
(i
1
)) = ?(C
2
(i
1
))] ? [?(C
1
(i
2
)) = ?(C
2
(i
2
))] ? [|C
1
(i
1
) ? C
2
(i
1
)| > |C
1
(i
2
) ? C
2
(i
2
)|]
[?(C
1
(i
1
)) = ?(C
2
(i
1
))] ? [?(C
1
(i
2
)) = ?(C
2
(i
2
))] ? [max(C
1
(i
1
), 1 ? C
1
(i
1
)) > max(C
1
(i
2
), 1 ? C
1
(i
2
))]
Figure 3: The ranking method that a binary classifier C
1
uses to impose a partial ordering on the instances
to be selected and added to the training set of binary classifier C
2
. i
1
and i
2
are arbitrary instances, and ? is
a function that rounds a number to its closest integer.
may be too liberal. In particular, these methods al-
low the addition of instances with opposing labels
to the labeled data; this can potentially result in in-
creased incompatibility between the classifiers.
Consequently, we develop a new procedure for
ranking instances in the data pool. The bootstrap-
ping algorithm then selects the highest ranked in-
stances to add to the labeled data in each iteration.
The method favors instances whose label is agreed
upon by both classifiers (Preference 1). However,
incorporating instances that are confidently labeled
by both classifiers may reduce the probability of
acquiring new information from the data. There-
fore, the method imposes an additional preference
for instances that are confidently labeled by one but
not both (Preference 2). If none of the instances
receives the same label from the classifiers, the
method resorts to the ?rank-by-confidence? method
used by B&M (Preference 3).
More formally, define a binary classifier as a func-
tion that maps an instance to a value that indicates
the probability that it is labeled as positive. Now,
let ? be a function that rounds a number to its near-
est integer. Given two binary classifiers C
1
and C
2
and instances i
1
and i
2
, the ranking method shown in
Figure 3 uses the three preferences described above
to impose a partial ordering on the given instances
for incorporation into C
2
?s labeled data. The method
similarly ranks instances to be added to C
1
?s labeled
data, with the roles of C
1
and C
2
reversed.
Steedman et al (2003a) also investigate instance
selection methods for co-training, but their goal is
primarily to use selection methods as a means to
explore the trade-off between maximizing coverage
and maximizing accuracy.6 In contrast, our focus
6McCallum and Nigam (1998) tackle this idea of balancing
0 500 1000 1500 2000 2500 3000 3500 4000
44
46
48
50
52
54
56
58
60
62
64
Number of Co?Training Iterations
F?
m
ea
su
re
Baseline
Using the B&M ranking method
Using our ranking method
Figure 4: F-measure curves for our single-view
bootstrapping algorithm with different ranking
methods (pool size = 5000, growth size = 50) for
MUC-6.
here is on examining whether a more conservative
ranking method can alleviate the problem of perfor-
mance deterioration. Nevertheless, Preference 2 is
inspired by their Sint-n selection method, which se-
lects an instance if it belongs to the intersection of
the set of the n percent highest scoring instances
of one classifier and the set of the n percent lowest
scoring instances of the other. To our knowledge, no
previous work has examined a ranking method that
combines the three preferences described above.
To compare our ranking procedure with B&M?s
rank-by-confidence method, we repeat the boot-
strapping experiment shown in Figure 2 except that
we replace B&M?s ranking method with ours. The
learning curves generated using the two ranking
methods with naive Bayes for the MUC-6 data set
are shown in Figure 4. The results are consistent
with our intuition regarding the two ranking meth-
accuracy and coverage by combining EM and active learning.
ods. The B&M ranking method is more liberal.
In particular, each classifier always selects the most
confidently labeled instances to add to the other?s la-
beled data at each iteration. If the underlying learn-
ers have indeed induced two different hypotheses
from the data, then each classifier can potentially ac-
quire informative instances from the other and yield
performance improvements very rapidly.
In contrast, our ranking method is more conserva-
tive in that it places more emphasis on maintaining
labeled data accuracy than the B&M method. As
a result, the classifier learns at a slower rate when
compared to that in the B&M case: it is not until iter-
ation 600 that we see a sharp rise in F-measure. Due
to the ?liberal? nature of the B&M method, however,
its performance drops dramatically as bootstrapping
progresses, whereas ours just dips temporarily. This
can potentially be attributed to the more rapid injec-
tion of mislabeled instances into the labeled data in
the B&M case. At iteration 2800, our method starts
to outperform B&M?s. Overall, our ranking method
does not exhibit the performance trend observed
with the B&M method: except for the spike between
iterations 0 and 100, F-measure does not deteriorate
as bootstrapping progresses. Since it is hard to deter-
mine a ?good? stopping point for bootstrapping due
to the paucity of labeled data in a weakly supervised
setting, our ranking method can potentially serve as
an alternative to the B&M method.
7 Conclusions
We have proposed a single-view, multi-learner boot-
strapping algorithm for coreference resolution and
shown empirically that the algorithm is a better al-
ternative to the Blum and Mitchell co-training al-
gorithm for this task for which no natural feature
split has been found. In addition, we have investi-
gated an example ranking method for bootstrapping
that, unlike Blum and Mitchell?s rank-by-confidence
method, can potentially alleviate the problem of per-
formance deterioration due to the pollution of the la-
beled data in the course of bootstrapping.
Acknowledgments
We thank the anonymous reviewers for their invalu-
able and insightful comments. This work was sup-
ported in part by NSF Grant IIS?0208028.
References
Michele Banko and Eric Brill. 2001. Scaling to very very large
corpora for natural language disambiguation. In Proceed-
ings of the ACL/EACL, pages 26?33.
Avrim Blum and Tom Mitchell. 1998. Combining labeled and
unlabeled data with co-training. In Proceedings of COLT,
pages 92?100.
Michael Collins and Yoram Singer. 1999. Unsupervised
models for named entity classification. In Proceedings of
EMNLP/VLC, pages 100?110.
Sally Goldman and Yan Zhou. 2000. Enhancing supervised
learning with unlabeled data. In Proceedings of ICML, pages
327?334.
Andrew McCallum and Kamal Nigam. 1998. Employing EM
and pool-based active learning for text classification. In Pro-
ceedings of ICML, pages 359?367.
MUC-6. 1995. Proceedings of the Sixth Message Understand-
ing Conference (MUC-6).
MUC-7. 1998. Proceedings of the Seventh Message Under-
standing Conference (MUC-7).
Christoph Mueller, Stefan Rapp, and Michael Strube. 2002.
Applying co-training to reference resolution. In Proceedings
of the ACL, pages 352?359.
Ion Muslea, Steven Minton, and Craig Knoblock. 2002. Active
+ Semi-Supervised Learning = Robust Multi-View Learning.
In Proceedings of ICML.
Vincent Ng and Claire Cardie. 2002. Combining sample selec-
tion and error-driven pruning for machine learning of coref-
erence rules. In Proceedings of EMNLP, pages 55?62.
Vincent Ng and Claire Cardie. 2003. Weakly supervised natu-
ral language learning without redundant views. In Proceed-
ings of HLT-NAACL.
Kamal Nigam and Rayid Ghani. 2000. Analyzing the effec-
tiveness and applicability of co-training. In Proceedings of
CIKM, pages 86?93.
Kamal Nigam, Andrew McCallum, Sabastian Thrun, and
Tom Mitchell. 2000. Text classification from labeled
and unlabeled documents using EM. Machine Learning,
39(2/3):103?134.
David Pierce and Claire Cardie. 2001. Limitations of co-
training for natural language learning from large datasets. In
Proceedings of EMNLP, pages 1?9.
Ellen Riloff and Rosie Jones. 1999. Learning dictionaries for
information extraction by multi-level bootstrapping. In Pro-
ceedings of AAAI, pages 474?479.
M. Steedman, R. Hwa, S. Clark, M. Osborne, A. Sarkar,
J. Hockenmaier, P. Ruhlen, S. Baker, and J. Crim. 2003a.
Example selection for bootstrapping statistical parsers. In
Proceedings of HLT-NAACL.
M. Steedman, M. Osborne, A. Sarkar, S. Clark, R. Hwa,
J. Hockenmaier, P. Ruhlen, S. Baker, and J. Crim. 2003b.
Bootstrapping statistical parsers from small datasets. In Pro-
ceedings of the EACL.
M. Vilain, J. Burger, J. Aberdeen, D. Connolly, and
L. Hirschman. 1995. A model-theoretic coreference scoring
scheme. In Proceedings of the Sixth MessageUnderstanding
Conference (MUC-6), pages 45?52.
David Yarowsky. 1995. Unsupervised word sense disambigua-
tion rivaling supervised methods. In Proceedingsof the ACL,
pages 189?196.
Proceedings of the Workshop on Sentiment and Subjectivity in Text, pages 9?14,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Toward Opinion Summarization: Linking the Sources
Veselin Stoyanov and Claire Cardie
Department of Computer Science
Cornell University
Ithaca, NY 14850, USA
{ves,cardie}@cs.cornell.edu
Abstract
We target the problem of linking source
mentions that belong to the same entity
(source coreference resolution), which is
needed for creating opinion summaries. In
this paper we describe how source coref-
erence resolution can be transformed into
standard noun phrase coreference resolu-
tion, apply a state-of-the-art coreference
resolution approach to the transformed
data, and evaluate on an available corpus
of manually annotated opinions.
1 Introduction
Sentiment analysis is concerned with the extrac-
tion and representation of attitudes, evaluations,
opinions, and sentiment from text. The area of
sentiment analysis has been the subject of much
recent research interest driven by two primary mo-
tivations. First, there is a desire to provide appli-
cations that can extract, represent, and allow the
exploration of opinions in the commercial, gov-
ernment, and political domains. Second, effec-
tive sentiment analysis might be used to enhance
and improve existing NLP applications such as in-
formation extraction, question answering, summa-
rization, and clustering (e.g. Riloff et al (2005),
Stoyanov et al (2005)).
Several research efforts (e.g. Riloff and Wiebe
(2003), Bethard et al (2004), Wilson et al (2004),
Yu and Hatzivassiloglou (2003), Wiebe and Riloff
(2005)) have shown that sentiment information
can be extracted at the sentence, clause, or indi-
vidual opinion expression level (fine-grained opin-
ion information). However, little has been done to
develop methods for combining fine-grained opin-
ion information to form a summary representa-
tion in which expressions of opinions from the
same source/target1 are grouped together, multi-
ple opinions from a source toward the same tar-
get are accumulated into an aggregated opinion,
and cumulative statistics are computed for each
source/target. A simple opinion summary2 is
shown in Figure 1. Being able to create opinion
summaries is important both for stand-alone ap-
plications of sentiment analysis as well as for the
potential uses of sentiment analysis as part of other
NLP applications.
In this work we address the dearth of ap-
proaches for summarizing opinion information.
In particular, we focus on the problem of source
coreference resolution, i.e. deciding which source
mentions are associated with opinions that belong
to the same real-world entity. In the example from
Figure 1 performing source coreference resolution
amounts to determining that Stanishev, he, and he
refer to the same real-world entities. Given the
associated opinion expressions and their polarity,
this source coreference information is the critical
knowledge needed to produce the summary of Fig-
ure 1 (although the two target mentions, Bulgaria
and our country, would also need to be identified
as coreferent).
Our work is concerned with fine-grained ex-
pressions of opinions and assumes that a system
can rely on the results of effective opinion and
source extractors such as those described in Riloff
and Wiebe (2003), Bethard et al (2004), Wiebe
and Riloff (2005) and Choi et al (2005). Presented
with sources of opinions, we approach the prob-
lem of source coreference resolution as the closely
1We use source to denote an opinion holder and target to
denote the entity toward which the opinion is directed.
2For simplicity, the example summary does not contain
any source/target statistics or combination of multiple opin-
ions from the same source to the same target.
9
? [Target Delaying of Bulgaria?s accession to the EU] would
be a serious mistake? [Source Bulgarian Prime Minister
Sergey Stanishev] said in an interview for the German daily
Suddeutsche Zeitung. ?[Target Our country] serves as a
model and encourages countries from the region to follow
despite the difficulties?, [Source he] added.
[Target Bulgaria] is criticized by [Source the EU] because of
slow reforms in the judiciary branch, the newspaper notes.
Stanishev was elected prime minister in 2005. Since then,
[Source he] has been a prominent supporter of [Target his
country?s accession to the EU].
Stanishev Accession
EU
Bulgaria
Delaying
+
? ?
+
Figure 1: Example of text containing opinions
(above) and a summary of the opinions (below).
In the text, sources and targets of opinions are
marked and opinion expressions are shown in
italic. In the summary graph, + stands for positive
opinion and - for negative.
related task of noun phrase coreference resolu-
tion. However, source coreference resolution dif-
fers from traditional noun phrase (NP) coreference
resolution in two important aspects discussed in
Section 4. Nevertheless, as a first attempt at source
coreference resolution, we employ a state-of-the-
art machine learning approach to NP coreference
resolution developed by Ng and Cardie (2002).
Using a corpus of manually annotated opinions,
we perform an extensive evaluation and obtain
strong initial results for the task of source coref-
erence resolution.
2 Related Work
Sentiment analysis has been a subject of much re-
cent research. Several efforts have attempted to
automatically extract opinions, emotions, and sen-
timent from text. The problem of sentiment ex-
traction at the document level (sentiment classifi-
cation) has been tackled as a text categorization
task in which the goal is to assign to a document
either positive (?thumbs up?) or negative (?thumbs
down?) polarity (e.g. Das and Chen (2001), Pang
et al (2002), Turney (2002), Dave et al (2003),
Pang and Lee (2004)). In contrast, the problem of
fine-grained opinion extraction has concentrated
on recognizing opinions at the sentence, clause,
or individual opinion expression level. Recent
work has shown that systems can be trained to rec-
ognize opinions, their polarity, and their strength
at a reasonable degree of accuracy (e.g. Dave et
al. (2003), Riloff and Wiebe (2003), Bethard et
al. (2004), Pang and Lee (2004), Wilson et al
(2004), Yu and Hatzivassiloglou (2003), Wiebe
and Riloff (2005)). Additionally, researchers have
been able to effectively identify sources of opin-
ions automatically (Bethard et al, 2004; Choi et
al., 2005; Kim and Hovy, 2005). Finally, Liu et al
(2005) summarize automatically generated opin-
ions about products and develop interface that al-
lows the summaries to be vizualized.
Our work also draws on previous work in the
area of coreference resolution, which is a rela-
tively well studied NLP problem. Coreference
resolution is the problem of deciding what noun
phrases in the text (i.e. mentions) refer to the same
real-world entities (i.e. are coreferent). Generally,
successful approaches have relied machine learn-
ing methods trained on a corpus of documents
annotated with coreference information (such as
the MUC and ACE corpora). Our approach to
source coreference resolution is inspired by the
state-of-the-art performance of the method of Ng
and Cardie (2002).
3 Data set
We begin our discussion by describing the data set
that we use for development and evaluation.
As noted previously, we desire methods that
work with automatically identified opinions and
sources. However, for the purpose of developing
and evaluating our approaches we rely on a corpus
of manually annotated opinions and sources. More
precisely, we rely on the MPQA corpus (Wilson
and Wiebe, 2003)3, which contains 535 manu-
ally annotated documents. Full details about the
corpus and the process of corpus creation can be
found in Wilson and Wiebe (2003); full details
of the opinion annotation scheme can be found in
Wiebe et al (2005). For the purposes of the dis-
cussion in this paper, the following three points
suffice.
First, the corpus is suitable for the domains and
genres that we target ? all documents have oc-
curred in the world press over an 11-month period,
between June 2001 and May 2002. Therefore, the
3The MPQA corpus is available at
http://nrrc.mitre.org/NRRC/publications.htm.
10
corpus is suitable for the political and government
domains as well as a substantial part of the com-
mercial domain. However, a fair portion of the
commercial domain is concerned with opinion ex-
traction from product reviews. Work described in
this paper does not target the genre of reviews,
which appears to differ significantly from news-
paper articles.
Second, all documents are manually annotated
with phrase-level opinion information. The an-
notation scheme of Wiebe et al (2005) includes
phrase level opinions, their sources, as well as
other attributes, which are not utilized by our ap-
proach. Additionally, the annotations contain in-
formation that allows coreference among source
mentions to be recovered.
Finally, the MPQA corpus contains no corefer-
ence information for general NPs (which are not
sources). This might present a problem for tradi-
tional coreference resolution approaches, as dis-
cussed throughout the paper.
4 Source Coreference Resolution
In this Section we define the problem of source
coreference resolution, describe its challenges,
and provide an overview of our general approach.
We define source coreference resolution as the
problem of determining which mentions of opin-
ion sources refer to the same real-world entity.
Source coreference resolution differs from tradi-
tional supervised NP coreference resolution in two
important aspects. First, sources of opinions do
not exactly correspond to the automatic extrac-
tors? notion of noun phrases (NPs). Second, due
mainly to the time-consuming nature of corefer-
ence annotation, NP coreference information is in-
complete in our data set: NP mentions that are not
sources of opinion are not annotated with coref-
erence information (even when they are part of
a chain that contains source NPs)4. In this pa-
per we address the former problem via a heuris-
tic method for mapping sources to NPs and give
statistics for the accuracy of the mapping process.
We then apply state-of-the-art coreference resolu-
tion methods to the NPs to which sources were
4This problem is illustrated in the example of Figure 1
The underlined Stanishev is coreferent with all of the Stan-
ishev references marked as sources, but, because it is used
in an objective sentence rather than as the source of an opin-
ion, the reference would be omitted from the Stanishev source
coreference chain. Unfortunately, this proper noun might be
critical in establishing coreference of the final source refer-
ence he with the other mentions of the source Stanishev.
Single Match Multiple Matches No Match
Total 7811 3461 50
Exact 6242 1303 0
Table 1: Statistics for matching sources to noun
phrases.
mapped (source noun phrases). The latter prob-
lem of developing methods that can work with in-
complete supervisory information is addressed in
a subsequent effort (Stoyanov and Cardie, 2006).
Our general approach to source coreference res-
olution consists of the following steps:
1. Preprocessing: We preprocess the corpus by running
NLP components such as a tokenizer, sentence split-
ter, POS tagger, parser, and a base NP finder. Sub-
sequently, we augment the set of the base NPs found
by the base NP finder with the help of a named en-
tity finder. The preprocessing is done following the NP
coreference work by Ng and Cardie (2002). From the
preprocessing step, we obtain an augmented set of NPs
in the text.
2. Source to noun phrase mapping: The problem
of mapping (manually or automatically annotated)
sources to NPs is not trivial. We map sources to NPs
using a set of heuristics.
3. Coreference resolution: Finally, we restrict our atten-
tion to the source NPs identified in step 2. We extract
a feature vector for every pair of source NPs from the
preprocessed corpus and perform NP coreference reso-
lution.
The next two sections give the details of Steps 2
and 3, respectively. We follow with the results of
an evaluation of our approach in Section 7.
5 Mapping sources to noun phrases
This section describes our method for heuristically
mapping sources to NPs. In the context of source
coreference resolution we consider a noun phrase
to correspond to (or match) a source if the source
and the NP cover the exact same span of text. Un-
fortunately, the annotated sources did not always
match exactly a single automatically extracted NP.
We discovered the following problems:
1. Inexact span match. We discovered that often (in
3777 out of the 11322 source mentions) there is no
noun phrase whose span matches exactly the source al-
though there are noun phrases that overlap the source.
In most cases this is due to the way spans of sources
are marked in the data. For instance, in some cases
determiners are not included in the source span (e.g.
?Venezuelan people? vs. ?the Venezuelan people?). In
other cases, differences are due to mistakes by the NP
extractor (e.g. ?Muslims rulers? was not recognized,
while ?Muslims? and ?rulers? were recognized). Yet in
other cases, manually marked sources do not match the
definition of a noun phrase. This case is described in
more detail next.
11
Measure Overall Method and Instance B3 MUC Positive Identification Actual Pos. Identification
rank parameters selection score Prec. Recall F1 Prec. Recall F1
B3 1 svm C10 ?0.01 none 81.8 71.7 80.2 43.7 56.6 57.5 62.9 60.2
400 5 ripper asc L2 soon2 80.7 72.2 74.5 45.2 56.3 55.1 62.1 58.4
Training MUC Score 1 svm C10 ?0.01 soon1 77.3 74.2 67.4 51.7 58.5 37.8 70.9 49.3
Documents 4 ripper acs L1.5 soon2 78.4 73.6 68.3 49.0 57.0 40.0 69.9 50.9
Positive 1 svm C10 ?0.05 soon1 72.7 73.9 60.0 57.2 58.6 37.8 71.0 49.3
identification 4 ripper acs L1.5 soon1 78.9 73.6 68.8 48.9 57.2 40.0 69.9 50.9
Actual pos. 1 svm C10 ?0.01 none 81.8 71.7 80.2 43.7 56.6 57.5 62.9 60.2
identification 2 ripper asc L4 soon2 73.9 69.9 81.1 40.2 53.9 69.8 52.5 60.0
B3 1 ripper acs L4 none 81.8 67.8 91.4 32.7 48.2 72.0 52.5 60.6
9 svm C10 ?0.01 none 81.4 70.3 81.6 40.8 54.4 58.4 61.6 59.9
200 MUC Score 1 svm C1 ?0.1 soon1 74.8 73.8 63.2 55.2 58.9 32.1 74.4 44.9
Training 5 ripper acs L1 soon1 77.9 0.732 71.4 46.5 56.3 37.7 69.7 48.9
Documents Positive 1 svm C1 ?0.1 soon1 74.8 73.8 63.2 55.2 58.9 32.1 74.4 44.9
identification 4 ripper acs L1 soon1 75.3 72.4 69.1 48.0 56.7 33.3 72.3 45.6
Actual pos. 1 ripper acs L4 none 81.8 67.8 91.4 32.7 48.2 72.0 52.5 60.6
identification 10 svm C10 ?0.01 none 81.4 70.3 81.6 40.8 54.4 58.4 61.6 59.9
Table 2: Performance of the best runs. For SVMs, ? stands for RBF kernel with the shown ? parameter.
2. Multiple NP match. For 3461 of the 11322 source
mentions more than one NP overlaps the source. In
roughly a quarter of these cases the multiple match is
due to the presence of nested NPs (introduced by the
NP augmentation process introduced in Section 3). In
other cases the multiple match is caused by source an-
notations that spanned multiple NPs or included more
than only NPs inside its span. There are three gen-
eral classes of such sources. First, some of the marked
sources are appositives such as ?the country?s new pres-
ident, Eduardo Duhalde?. Second, some sources con-
tain an NP followed by an attached prepositional phrase
such as ?Latin American leaders at a summit meeting in
Costa Rica?. Third, some sources are conjunctions of
NPs such as ?Britain, Canada and Australia?. Treat-
ment of the latter is still a controversial problem in
the context of coreference resolution as it is unclear
whether conjunctions represent entities that are distinct
from the conjuncts. For the purpose of our current work
we do not attempt to address conjunctions.
3. No matching NP. Finally, for 50 of the 11322 sources
there are no overlapping NPs. Half of those (25 to
be exact) included marking of the word ?who? such
as in the sentence ?Carmona named new ministers,
including two military officers who rebelled against
Chavez?. From the other 25, 19 included markings of
non-NPs including question words, qualifiers, and ad-
jectives such as ?many?, ?which?, and ?domestically?.
The remaining six are rare NPs such as ?lash? and
?taskforce? that are mistakenly not recognized by the
NP extractor.
Counts for the different types of matches of
sources to NPs are shown in Table 1. We deter-
mine the match in the problematic cases using a
set of heuristics:
1. If a source matches any NP exactly in span, match that
source to the NP; do this even if multiple NPs overlap
the source ? we are dealing with nested NP?s.
2. If no NP matches matches exactly in span then:
? If a single NP overlaps the source, then map the
source to that NP.Most likely we are dealing with
differently marked spans.
? If multiple NPs overlap the source, determine
whether the set of overlapping NPs include any
non-nested NPs. If all overlapping NPs are
nested with each other, select the NP that is
closer in span to the source ? we are still dealing
with differently marked spans, but now we also
have nested NPs. If there is more than one set
of nested NPs, then most likely the source spans
more than a single NP. In this case we select the
outermost of the last set of nested NPs before any
preposition in the span. We prefer: the outermost
NP because longer NPs contain more informa-
tion; the last NP because it is likely to be the head
NP of a phrase (also handles the case of expla-
nation followed by a proper noun); NP?s before
preposition, because a preposition signals an ex-
planatory prepositional phrase.
3. If no NP overlaps the source, select the last NP before
the source. In half of the cases we are dealing with the
word who, which typically refers to the last preceding
NP.
6 Source coreference resolution as
coreference resolution
Once we isolate the source NPs, we apply corefer-
ence resolution using the standard combination of
classification and single-link clustering (e.g. Soon
et al (2001) and Ng and Cardie (2002)).
We compute a vector of 57 features for every
pair of source noun phrases from the preprocessed
corpus. We use the training set of pairwise in-
stances to train a classifier to predict whether a
source NP pair should be classified as positive (the
NPs refer to the same entity) or negative (different
entities). During testing, we use the trained clas-
sifier to predict whether a source NP pair is pos-
itive and single-link clustering to group together
sources that belong to the same entity.
7 Evaluation
For evaluation we randomly split the MPQA cor-
pus into a training set consisting of 400 documents
12
and a test set consisting of the remaining 135 doc-
uments. We use the same test set for all evalua-
tions, although not all runs were trained on all 400
training documents as discussed below.
The purpose of our evaluation is to create a
strong baseline utilizing the best settings for the
NP coreference approach. As such, we try the
two reportedly best machine learning techniques
for pairwise classification ? RIPPER (for Re-
peated Incremental Pruning to Produce Error Re-
duction) (Cohen, 1995) and support vector ma-
chines (SVMs) in the SVM light implementation
(Joachims, 1998). Additionally, to exclude pos-
sible effects of parameter selection, we try many
different parameter settings for the two classifiers.
For RIPPER we vary the order of classes and the
positive/negative weight ratio. For SVMs we vary
C (the margin tradeoff) and the type and parameter
of the kernel. In total, we use 24 different settings
for RIPPER and 56 for SVM light.
Additionally, Ng and Cardie reported better re-
sults when the training data distribution is bal-
anced through instance selection. For instance
selection they adopt the method of Soon et al
(2001), which selects for each NP the pairs with
the n preceding coreferent instances and all in-
tervening non-coreferent pairs. Following Ng and
Cardie (2002), we perform instance selection with
n = 1 (soon1 in the results) and n = 2 (soon2).
With the three different instance selection algo-
rithms (soon1, soon2, and none), the total number
of settings is 72 for RIPPER and 168 for SVMa.
However, not all SVM runs completed in the time
limit that we set ? 200 min, so we selected half
of the training set (200 documents) at random and
trained all classifiers on that set. We made sure
to run to completion on the full training set those
SVM settings that produced the best results on the
smaller training set.
Table 2 lists the results of the best performing
runs. The upper half of the table gives the re-
sults for the runs that were trained on 400 docu-
ments and the lower half contains the results for
the 200-document training set. We evaluated us-
ing the two widely used performance measures for
coreference resolution ? MUC score (Vilain et al,
1995) and B3 (Bagga and Baldwin, 1998). In ad-
dition, we used performance metrics (precision,
recall and F1) on the identification of the posi-
tive class. We compute the latter in two different
ways ? either by using the pairwise decisions as
the classifiers outputs them or by performing the
clustering of the source NPs and then considering
a pairwise decision to be positive if the two source
NPs belong to the same cluster. The second option
(marked actual in Table 2) should be more repre-
sentative of a good clustering, since coreference
decisions are important only in the context of the
clusters that they create.
Table 2 shows the performance of the best RIP-
PER and SVM runs for each of the four evaluation
metrics. The table also lists the rank for each run
among the rest of the runs.
7.1 Discussion
The absolute B3 and MUC scores for source
coreference resolution are comparable to reported
state-of-the-art results for NP coreference resolu-
tions. Results should be interpreted cautiously,
however, due to the different characteristics of our
data. Our documents contained 35.34 source NPs
per document on average, with coreference chains
consisting of only 2.77 NPs on average. The low
average number of NPs per chain may be produc-
ing artificially high score for the B3 and MUC
scores as the modest results on positive class iden-
tification indicate.
From the relative performance of our runs, we
observe the following trends. First, SVMs trained
on the full training set outperform RIPPER trained
on the same training set as well as the correspond-
ing SVMs trained on the 200-document training
set. The RIPPER runs exhibit the opposite be-
havior ? RIPPER outperforms SVMs on the 200-
document training set and RIPPER runs trained
on the smaller data set exhibit better performance.
Overall, the single best performance is observed
by RIPPER using the smaller training set.
Another interesting observation is that the B3
measure correlates well with good ?actual? perfor-
mance on positive class identification. In contrast,
good MUC performance is associated with runs
that exhibit high recall on the positive class. This
confirms some theoretical concerns that MUC
score does not reward algorithms that recognize
well the absence of links. In addition, the results
confirm our conjecture that ?actual? precision and
recall are more indicative of the true performance
of coreference algorithms.
13
8 Conclusions
As a first step toward opinion summarization we
targeted the problem of source coreference resolu-
tion. We showed that the problem can be tackled
effectively as noun coreference resolution.
One aspect of source coreference resolution that
we do not address is the use of unsupervised infor-
mation. The corpus contains many automatically
identified non-source NPs, which can be used to
benefit source coreference resolution in two ways.
First, a machine learning approach could use the
unlabeled data to estimate the overall distributions.
Second, some links between sources may be real-
ized through a non-source NPs (see the example
of figure 1). As a follow-up to the work described
in this paper we developed a method that utilizes
the unlabeled NPs in the corpus using a structured
rule learner (Stoyanov and Cardie, 2006).
Acknowledgements
The authors would like to thank Vincent Ng and Art Munson
for providing coreference resolution code, members of the
Cornell NLP group (especially Yejin Choi and Art Munson)
for many helpful discussions, and the anonymous reviewers
for their insightful comments. This work was supported by
the Advanced Research and Development Activity (ARDA),
by NSF Grants IIS-0535099 and IIS-0208028, by gifts from
Google and the Xerox Foundation, and by an NSF Graduate
Research Fellowship to the first author.
References
A. Bagga and B. Baldwin. 1998. Entity-based cross-
document coreferencing using the vector space model. In
Proceedings of COLING/ACL.
S. Bethard, H. Yu, A. Thornton, V. Hativassiloglou, and
D. Jurafsky. 2004. Automatic extraction of opinion
propositions and their holders. In 2004 AAAI Spring Sym-
posium on Exploring Attitude and Affect in Text.
Y. Choi, C. Cardie, E. Riloff, and S. Patwardhan. 2005. Iden-
tifying sources of opinions with conditional random fields
and extraction patterns. In Proceedings of EMNLP.
W. Cohen. 1995. Fast effective rule induction. In Proceed-
ings of ICML.
S. Das and M. Chen. 2001. Yahoo for amazon: Extracting
market sentiment from stock message boards. In Proceed-
ings of APFAAC.
K. Dave, S. Lawrence, and D. Pennock. 2003. Mining the
peanut gallery: Opinion extraction and semantic classifi-
cation of product reviews. In Proceedings of IWWWC.
T. Joachims. 1998. Making large-scale support vector
machine learning practical. In A. Smola B. Scho?lkopf,
C. Burges, editor, Advances in Kernel Methods: Support
Vector Machines. MIT Press, Cambridge, MA.
S. Kim and E. Hovy. 2005. Identifying opinion holders for
question answering in opinion texts. In Proceedings of
AAAI Workshop on Question Answering in Restricted Do-
mains.
B. Liu, M. Hu, and J. Cheng. 2005. Opinion observer: An-
alyzing and comparing opinions on the web. In Proceed-
ings of International World Wide Web Conference.
V. Ng and C. Cardie. 2002. Improving machine learning
approaches to coreference resolution. In Proceedings of
ACL.
B. Pang and L. Lee. 2004. A sentimental education: Senti-
ment analysis using subjectivity summarization based on
minimum cuts. In Proceedings of ACL.
B. Pang, L. Lee, and S. Vaithyanathan. 2002. Thumbs
up? Sentiment classification using machine learning tech-
niques. In Proceedings of EMNLP.
E. Riloff and J. Wiebe. 2003. Learning extraction patterns
for subjective expressions. In Proceesings of EMNLP.
E. Riloff, J. Wiebe, and W. Phillips. 2005. Exploiting sub-
jectivity classification to improve information extraction.
In Proceedings of AAAI.
W. Soon, H. Ng, and D. Lim. 2001. A machine learning
approach to coreference resolution of noun phrases. Com-
putational Linguistics, 27(4).
V. Stoyanov and C. Cardie. 2006. Partially supervised
coreference resolution for opinion summarization through
structured rule learning. In Proceedings of EMNLP.
V. Stoyanov, C. Cardie, and J. Wiebe. 2005. Multi-
Perspective question answering using the OpQA corpus.
In Proceedings of EMNLP.
P. Turney. 2002. Thumbs up or thumbs down? Semantic ori-
entation applied to unsupervised classification of reviews.
In Proceedings of ACL.
M. Vilain, J. Burger, J. Aberdeen, D. Connolly, and
L. Hirschman. 1995. A model-theoretic coreference scor-
ing scheme. In Proceedings of MUC-6.
J. Wiebe and E. Riloff. 2005. Creating subjective and objec-
tive sentence classifiers from unannotated texts. In Pro-
ceedings of CICLing.
J. Wiebe, T. Wilson, and C. Cardie. 2005. Annotating ex-
pressions of opinions and emotions in language. Lan-
guage Resources and Evaluation, 1(2).
T. Wilson and J. Wiebe. 2003. Annotating opinions in the
world press. 4th SIGdial Workshop on Discourse and Di-
alogue (SIGdial-03).
T. Wilson, J. Wiebe, and R. Hwa. 2004. Just how mad are
you? Finding strong and weak opinion clauses. In Pro-
ceedings of AAAI.
H. Yu and V. Hatzivassiloglou. 2003. Towards answer-
ing opinion questions: Separating facts from opinions and
identifying the polarity of opinion sentences. In Proceed-
ings of EMNLP.
14
Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing (EMNLP 2006), pages 336?344,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Partially Supervised Coreference Resolution for Opinion Summarization
through Structured Rule Learning
Veselin Stoyanov and Claire Cardie
Department of Computer Science
Cornell University
Ithaca, NY 14850, USA
{ves,cardie}@cs.cornell.edu
Abstract
Combining fine-grained opinion informa-
tion to produce opinion summaries is im-
portant for sentiment analysis applica-
tions. Toward that end, we tackle the
problem of source coreference resolution
? linking together source mentions that re-
fer to the same entity. The partially super-
vised nature of the problem leads us to de-
fine and approach it as the novel problem
of partially supervised clustering. We pro-
pose and evaluate a new algorithm for the
task of source coreference resolution that
outperforms competitive baselines.
1 Introduction
Sentiment analysis is concerned with extracting
attitudes, opinions, evaluations, and sentiment
from text. Work in this area has been motivated
by the desire to provide information analysis ap-
plications in the arenas of government, business,
and politics (e.g. Coglianese (2004)). Addition-
ally, sentiment analysis can augment existing NLP
applications such as question answering, informa-
tion retrieval, summarization, and clustering by
providing information about sentiment (e.g. Stoy-
anov et al (2005), Riloff et al (2005)). To date,
research in the area (see Related Work section)
has focused on the problem of extracting senti-
ment both at the document level (coarse-grained
sentiment information), and at the level of sen-
tences, clauses, or individual expressions (fine-
grained sentiment information).
In contrast, our work concerns the summa-
rization of fine-grained information about opin-
ions. In particular, while recent research ef-
forts have shown that fine-grained opinions (e.g.
Riloff and Wiebe (2003), Bethard et al (2004),
Wiebe and Riloff (2005)) as well as their sources
(e.g. Bethard et al (2004), Choi et al (2005),
Kim and Hovy (2005)) can be extracted auto-
matically, little has been done to create opin-
ion summaries, where opinions from the same
source/target are combined, statistics are com-
puted for each source/target and multiple opinions
from the same source to the same target are ag-
gregated. A simple opinion summary is shown in
figure 1.1 We expect that this type of opinion sum-
mary, based on fine-grained opinion information,
will be important for information analysis applica-
tions in any domain where the analysis of opinions
is critical.
This paper addresses the problem of opinion
summarization by considering the creation of sim-
ple opinion summaries like those of figure 1. We
propose source coreference resolution ? the task
of determining which mentions of opinion sources
refer to the same entity ? as the primary mecha-
nism for identifying the set of opinions attributed
to each real-world source. For this type of sum-
mary, source coreference resolution constitutes an
integral step in the process of generating full opin-
ion summaries. For example, given the opinion
expressions of figure 1, their polarity, and the asso-
ciated opinion sources and targets, the bulk of the
resulting summary can be produced by recogniz-
ing that source mentions ?Zacarias Moussaoui?,
?he?, ?my?, and ?Mr. Moussaoui? all refer to
the same person; and that source mentions ?Mr.
Zerkin? and ?Zerkin? refer to the same person.2
1For simplicity, the example summary does not contain
any source/target statistics.
2In addition, the summary would require the closely re-
lated task of target coreference resolution and a means for ag-
gregating the conflicting opinions from Zerkin toward Mous-
saoui.
336
At first glance, source coreference resolution
appears equivalent to the task of noun phrase
coreference resolution and therefore amenable to
traditional coreference resolution techniques (e.g.
Ng and Cardie (2002), Morton (2000)). We hy-
pothesize in Section 3, however, that the task is
likely to succumb to a better solution by treating
it in the context of a new machine learning set-
ting that we refer to as partially supervised clus-
tering. In particular, due to high coreference an-
notation costs, data sets that are annotated with
opinion information (like ours) do not typically in-
clude supervisory coreference information for all
noun phrases in a document (as would be required
for the application of traditional coreference reso-
lution techniques), but only for noun phrases that
act as opinion sources (or targets).
As a result, we define the task of partially su-
pervised clustering, the goal of which is to learn
a clustering function from a set of partially spec-
ified clustering examples (Section 4). We are not
aware of prior work on the problem of partially
supervised clustering and argue that it differs sub-
stantially from that of semi-supervised clustering.
We propose an algorithm for partially supervised
clustering that extends a rule learner with structure
information and is generally applicable to prob-
lems that fit the partially supervised clustering def-
inition (Section 5). We apply the algorithm to
the source coreference resolution task and evalu-
ate its performance on a standard sentiment analy-
sis data set that includes source coreference chains
(Section 6). We find that our algorithm outper-
forms highly competitive baselines by a consid-
erable margin ? B3 score of 83.2 vs. 81.8 and
67.1 vs. 60.9 F1 score for the identification of
positive source coreference links.
2 Related Work
Work relevant to our problem can be split into
three main areas ? sentiment analysis, traditional
noun phrase coreference resolution, and super-
vised and weakly supervised clustering. Related
work in the former two areas is summarized briefly
below. Supervised and weakly supervised cluster-
ing approaches are discussed in Section 4.
Sentiment analysis. Much of the relevant re-
search in sentiment analysis addresses sentiment
classification, a text categorization task of extract-
ing opinion at the coarse-grained document level.
The goal in sentiment classification is to assign to
[Source Zacarias Moussaoui] [? complained] at length today
about [Target his own lawyer], telling a federal court jury that
[Target he] was [? more interested in achieving fame than sav-
ing Moussaoui?s life].
Mr. Moussaoui said he was appearing on the witness stand to
tell the truth. And one part of the truth, [Source he] said, is that
[Target sending him to prison for life] would be ?[? a greater
punishment] than being sentenced to death.?
?[? [Target You] have put your interest ahead of [Source my]
life],? [Source Mr. Moussaoui] told his court-appointed lawyer
Gerald T. Zerkin.
...
But, [Source Mr. Zerkin] pressed [Target Mr. Moussaoui], was
it [? not true] that he told his lawyers earlier not to involve
any Muslims in the defense, not to present any evidence that
might persuade the jurors to spare his life?
...
[Source Zerkin] seemed to be trying to show the jurors
that while [Target the defendant] is generally [+ an honest
individual], his conduct shows [Target he] is [? not stable
mentally], and thus [? undeserving] of [Target the ultimate
punishment].
Moussaoui
Zerkin
prison for life
ultimate punishment
?
?
?
?/ +
Figure 1: Example text containing opinions
(above) and a summary of the opinions (be-
low). Sources and targets of opinions are brack-
eted; opinion expressions are shown in italics and
bracketed with associated polarity, either positive
(+) or negative (-). The underlined phrase will be
explained later in the paper.
a document either positive (?thumbs up?) or nega-
tive (?thumbs down?) polarity (e.g. Das and Chen
(2001), Pang et al (2002), Turney (2002), Dave
et al (2003)). Other research has concentrated
on analyzing fine-grained opinions at, or below,
the sentence level. Recent work, for example, in-
dicates that systems can be trained to recognize
opinions and their polarity, strength, and sources
to a reasonable degree of accuracy (e.g. Dave et
al. (2003), Riloff and Wiebe (2003), Bethard et
al. (2004), Wilson et al (2004), Yu and Hatzivas-
siloglou (2003), Choi et al (2005), Kim and Hovy
(2005), Wiebe and Riloff (2005)). Our work ex-
tends research on fine-grained opinion extraction
by augmenting the opinions with additional infor-
mation that allows the creation of concise opinion
summaries. In contrast to the opinion extracts pro-
duced by Pang and Lee (2004), our summaries are
not text extracts, but rather explicitly identify and
337
characterize the relations between opinions and
their sources.
Coreference resolution. Coreference resolution
is a relatively well studied NLP problem (e.g.
Morton (2000), Ng and Cardie (2002), Iida et al
(2003), McCallum and Wellner (2003)). Corefer-
ence resolution is defined as the problem of decid-
ing which noun phrases in the text (mentions) re-
fer to the same real world entities (are coreferent).
Generally, successful approaches to coreference
resolution have relied on supervised classification
followed by clustering. For supervised classifica-
tion these approaches learn a pairwise function to
predict whether a pair of noun phrases is corefer-
ent. Subsequently, when making coreference res-
olution decisions on unseen documents, the learnt
pairwise NP coreference classifier is run, followed
by a clustering step to produce the final clusters
(coreference chains) of coreferent NPs. For both
training and testing, coreference resolution algo-
rithms rely on feature vectors for pairs of noun
phrases that encode linguistic information about
the NPs and their local context. Our general ap-
proach to source coreference resolution is inspired
by the state-of-the-art performance of one such ap-
proach to coreference resolution, which relies on a
rule learner and single-link clustering as described
in Ng and Cardie (2002).
3 Source Coreference Resolution
In this section we introduce the problem of source
coreference resolution in the context of opinion
summarization and argue for the need for novel
methods for the task.
The task of source coreference resolution is to
decide which mentions of opinion sources refer to
the same entity. Much like traditional coreference
resolution, we employ a learning approach; how-
ever, our approach differs from traditional coref-
erence resolution in its definition of the learn-
ing task. Motivated by the desire to utilize unla-
beled examples (discussed later), we define train-
ing as an integrated task in which pairwise NP
coreference decisions are learned together with
the clustering function as opposed to treating each
NP pair as a training example. Thus, our train-
ing phase takes as input a set of documents with
manually annotated opinion sources together with
coreference annotations for the sources; it outputs
a classifier that can produce source coreference
chains for previously unseen documents contain-
ing marked (manually or automatically) opinion
sources. More specifically, the source coreference
resolution training phase proceeds through the fol-
lowing steps:
1. Source-to-NP mapping: We preprocess
each document by running a tokenizer, sen-
tence splitter, POS tagger, parser, and an NP
finder. Subsequently, we augment the set of
NPs found by the NP finder with the help of
a system for named entity detection. We then
map the sources to the NPs. Since there is
no one-to-one correspondence, we use a set
of heuristics to create the mapping. More de-
tails about why heuristics are needed and the
process used to map sources to NPs can be
found in Stoyanov and Cardie (2006).
2. Feature vector creation: We extract a fea-
ture vector for every pair of NPs from the pre-
processed corpus. We use the features intro-
duced by Ng and Cardie (2002) for the task
of coreference resolution.
3. Classifier construction: Using the feature
vectors from step 2, we construct a training
set containing one training example per doc-
ument. Each training example consists of the
feature vectors for all pairs of NPs in the doc-
ument, including those that do not map to
sources, together with the available corefer-
ence information for the source noun phrases
(i.e. the noun phrases to which sources are
mapped). The training instances are pro-
vided as input to a learning algorithm (see
Section 5), which constructs a classifier that
can take the instances associated with a new
(previously unseen) document and produce a
clustering over all NPs in the document.
The testing phase employs steps 1 and 2 as de-
scribed above, but replaces step 3 by a straightfor-
ward application of the learnt classifier. Since we
are interested in coreference information only for
the source NPs, we simply discard the non-source
NPs from the resulting clustering.
The approach to source coreference resolution
described here would be identical to traditional
coreference resolution when provided with train-
ing examples containing coreference information
for all NPs. However, opinion corpora in general,
and our corpus in particular, contain no corefer-
ence information about general NPs. Neverthe-
less, after manual sources are mapped to NPs in
338
step 1 above, our approach can rely on the avail-
able coreference information for the source NPs.
Due to the high cost of coreference annotation, we
desire methods that can work in the presence of
only this limited amount of coreference informa-
tion.
A possible workaround the absence of full NP
coreference information is to train a traditional
coreference system only on the labeled part of the
data (indeed that is one of the baselines against
which we compare). However, we believe that
an effective approach to source coreference res-
olution has to utilize the unlabeled noun phrases
because links between sources might be realized
through non-source mentions. This problem is il-
lustrated in figure 1. The underlined Moussaoui
is coreferent with all of the Moussaoui references
marked as sources, but, because it is used in an
objective sentence rather than as the source of
an opinion, the reference would be omitted from
the Moussaoui source chain. Unfortunately, this
proper noun phrase might be critical in establish-
ing the coreference of the final source reference he
with the other mentions of the source Moussaoui.
As mentioned previously, in order to utilize
the unlabeled data, our approach differs from tra-
ditional coreference resolution, which uses NP
pairs as training instances. We instead follow the
framework of supervised clustering (Finley and
Joachims, 2005; Li and Roth, 2005) and consider
each document as a training example. As in super-
vised clustering, this framework has the additional
advantage that the learning algorithm can consider
the clustering algorithm when making decisions
about pairwise classification, which could lead to
improvements in the classifier. In the next section
we describe our approach to classifier construction
for step 3 and compare our problem to traditional
weakly supervised clustering, characterizing it as
an instance of the novel problem of partially su-
pervised clustering.
4 Partially Supervised Clustering
In our desire to perform effective source corefer-
ence resolution we arrive at the following learning
problem ? the learning algorithm is presented with
a set of partially specified examples of clusterings
and acquires a function that can cluster accurately
an unseen set of items, while taking advantage of
the unlabeled information in the examples.
This setting is to be contrasted with semi-
supervised clustering (or clustering with con-
straints), which has received much research at-
tention (e.g. Demiriz et al (1999), Wagstaff and
Cardie (2000), Basu (2005), Davidson and Ravi
(2005)). Semi-supervised clustering can be de-
fined as the problem of clustering a set of items
in the presence of limited supervisory informa-
tion such as pairwise constraints (e.g. two items
must/cannot be in the same cluster) or labeled
points. In contrast to our setting, in the semi-
supervised case there is no training phase ? the
algorithm receives all examples (labeled and un-
labeled) at the same time together with some dis-
tance or cost function and attempts to find a clus-
tering that optimizes a given measure (usually
based on the distance or cost function).
Source coreference resolution might alterna-
tively be approached as a supervised clustering
problem. Traditionally, approaches to supervised
clustering have treated the pairwise link decisions
as a classification problem. These approaches first
learn a distance metric that optimizes the pairwise
decisions; and then follow the pairwise classifica-
tion with a clustering step. However, these tradi-
tional approaches have no obvious way of utilizing
the available unlabeled information.
In contrast, we follow recent approaches to su-
pervised clustering that propose ways to learn
the distance measure in the context of the clus-
tering decisions (Li and Roth, 2005; Finley and
Joachims, 2005; McCallum and Wellner, 2003).
This provides two advantages for the problem of
source coreference resolution. First, it allows the
algorithm to take advantage of the complexity of
the rich structural dependencies introduced by the
clustering problem. Viewed traditionally as a hur-
dle, the structural complexity of clustering may be
beneficial in the partially supervised case. We be-
lieve that provided with a few partially specified
clustering examples, an algorithm might be able
to generalize from the structural dependencies to
infer correctly the whole clustering of the items.
In addition, considering pairwise decisions in the
context of the clustering can arguably lead to more
accurate classifiers.
Unfortunately, none of the supervised cluster-
ing approaches is readily applicable to the partially
supervised case. However, by adapting the for-
mal supervised clustering definition, which we do
next, we can develop approaches to partially su-
pervised clustering that take advantage of the un-
339
labeled portions of the data.
Formal definition. For partially supervised
clustering we extend the formal definition of su-
pervised clustering given by Finley and Joachims
(2005). In the fully supervised setting, an al-
gorithm is given a set S of n training examples
(x1, y1), ..., (xn, yn) ? X ? Y , where X is the
set of all possible sets of items and Y is the set of
all possible clusterings of these sets. For a train-
ing example (x, y), x = {x1, x2, ..., xk} is a set
of k items and y = {y1, y2, ..., yr} is a clustering
of the items in x with each yi ? x. Addition-
ally, each item can be in no more than one cluster
(?i, j.yi ? yj = ?) and in the fully supervised case
each item is in at least one cluster (x = ? yi).
The goal of the learning algorithm is to acquire a
function h : X ? Y that can accurately cluster a
(previously unseen) set of items.
In the context of source coreference resolution
the training set contains one example for each doc-
ument. The items in each training example are the
NPs and the clustering over the items is the equiv-
alence relation defined by the coreference infor-
mation. For source coreference resolution, how-
ever, clustering information is unavailable for the
non-source NPs. Thus, to be able to deal with this
unlabeled component of the data we arrive to the
setting of partially supervised clustering, in which
we relax the condition that each item is in at least
one cluster (x = ? yi) and replace it with the con-
dition x ?
?
yi. The items with no linking infor-
mation (items in x \? yi) constitute the unlabeled
(unsupervised) component of the partially super-
vised clustering.
5 Structured Rule Learner
We develop a novel method for partially super-
vised clustering, which is motivated by the success
of a rule learner (RIPPER) for coreference resolu-
tion (Ng and Cardie, 2002). We extend RIPPER
so that it can learn rules in the context of single-
link clustering, which both suits our task (i.e. pro-
nouns link to their single antecedent) and has ex-
hibited good performance for coreference resolu-
tion (Ng and Cardie, 2002). We begin with a brief
overview of RIPPER followed by a description of
the modifications that we implemented. For ease
of presentation, we assume that we are in the fully
supervised case. We end this section by describing
the changes for the partially supervised case.
procedure StRip(TrainData){
GrowData, PruneData = Split(TrainData);
//Keep instances from the same document together
while(there are positive uncovered instances) {
r = growRule(GrowData);
r = pruneRule(r, PruneData);
DL = relativeDL(Ruleset);
if(DL ? minDL + d bits)
Ruleset.add(r);
Mark examples covered by r as +;
else
exit loop with Ruleset
}
}
procedure growRule(growData){
r = empty rule;
for(every unused feature f){
if (f is nominal feature) {
for(every possible value v of f) {
mark all instances that have values of v for f with +;
compute the transitive closure of the positive instances
//(including instances marked + from previous rules);
compute the infoGain for the future/value combination;
}
} else{ //Numeric feature
create one bag for each feature value and split the instances into bags;
do a forward and a backward pass over the bags keeping a running
clustering and compute the information gain for each value;
}
}
add the future/value pair with the best infoGain to r;
growData = growData - all negative instances;
return r;
}
procedure pruneRule(r, pruneData){
for(all antecedents a in the rule){
apply all antecedents in r up to a to pruneData;
compute the transitive closure of the positive instances;
compute A(a) ? the accuracy of the rule up to antecedent a;
}
Remove all antecedents after the antecedent for which A(a) is maximum.
}
Figure 2: The StRip algorithm. Additions to RIP-
PER are shown in bold.
5.1 The RIPPER Algorithm
RIPPER (for Repeated Incremental Pruning to
Produce Error Reduction) was introduced by Co-
hen (1995) as an extension of an existing rule
induction algorithm. Cohen (1995) showed that
RIPPER produces error rates competitive with
C4.5, while exhibiting better running times. RIP-
PER consists of two phases ? a ruleset is grown
and then optimized.
The ruleset creation phase begins by ran-
domly splitting the training data into a rule-
growing set (2/3 of the training data) and a pruning
set (the remaining 1/3). A rule is then grown on
the former set by repeatedly adding the antecedent
(the feature value test) with the largest information
gain until the accuracy of the rule becomes 1.0 or
there are no remaining potential antecedents. Next
the rule is applied to the pruning data and any rule-
final sequence that reduces the accuracy of the rule
is removed.
The optimization phase uses the full training
340
set to first grow a replacement rule and a revised
rule for each rule in the ruleset. For each rule,
the algorithm then considers the original rule, the
replacement rule, and the revised rule, and keeps
the rule with the smallest description length in the
context of the ruleset. After all rules are con-
sidered, RIPPER attempts to grow residual rules
that cover data not already covered by the rule-
set. Finally, RIPPER deletes any rules from the
ruleset that reduce the overall minimum descrip-
tion length of the data plus the ruleset. RIPPER
performs two rounds of this optimization phase.
5.2 The StRip Algorithm
The property of partially supervised clustering that
we want to explore is the structured nature of the
decisions. That is, each decision of whether two
items (say a and b) belong to the same cluster has
an implication for all items a? that belong to a?s
cluster and all items b? that belong to b?s cluster.
We target modifications to RIPPER that will al-
low StRip (for Structured RIPPER) to learn rules
that produce good clusterings in the context of
single-link clustering. We extend RIPPER so that
every time it makes a decision about a rule, it con-
siders the effect of the rule on the overall clus-
tering of items (as opposed to considering the in-
stances that the rule classifies as positive/negative
in isolation). More precisely, we precede every
computation of rule performance (e.g. informa-
tion gain or description length) by a transitive clo-
sure (i.e. single link clustering) of the data w.r.t. to
the pairwise classifications. Following the transi-
tive closure, all pairs of items that are in the same
cluster are considered covered by the rule for per-
formance computation.
The StRip algorithm is given in figure 2, with
modifications to the original RIPPER algorithm
shown in bold. Due to space limitations the op-
timization stage of the algorithm is omitted. Our
modifications to the optimization stage of RIPPER
are in the spirit of the rest of the StRip algorithm.
Partially supervised case. So far we described
StRip only for the fully supervised case. We
use a very simple modification to handle the par-
tially supervised setting: we exclude the unla-
beled pairs when computing the performance of
the rules. Thus, the unlabeled items do not count
as correct or incorrect classifications when acquir-
ing or pruning a rule, although they do participate
in the transitive closure. Links in the unlabeled
data are inferred entirely through the indirect links
between items in the labeled component that they
introduce. In the example of figure 1, the two
problematic unlabeled links are the link between
the source mention ?he? and the underlined non-
source NP ?Mr. Moussaoui? and the link between
the underlined ?Mr. Moussaoui? to any source
mention of Moussaoui. While StRip will not re-
ward any rule (or rule set) that covers these two
links directly, such rules will be rewarded indi-
rectly since they put the source he in the chain for
the source Moussaoui.
StRip running time. StRip?s running time is
generally comparable to that of RIPPER. We com-
pute transitive closure by using a Union-Find
structure, which runs in time O(log?n), which for
practical purposes can be considered linear (O(n))
3
. However, when computing the best information
gain for a nominal feature, StRip has to make a
pass over the data for each value that the feature
takes, while RIPPER can split the data into bags
and perform the computation in one pass.
6 Evaluation and Results
This section describes the source coreference data
set, the baselines, our implementation of StRip,
and the results of our experiments.
6.1 Data set
For evaluation we use the MPQA corpus (Wiebe
et al, 2005).4 The corpus consists of 535 doc-
uments from the world press. All documents in
the collection are manually annotated with phrase-
level opinion information following the annota-
tion scheme of Wiebe et al (2005). Discussion
of the annotation scheme is beyond the scope of
this paper; for our purposes it suffices to say that
the annotations include the source of each opin-
ion and coreference information for the sources
(e.g. source coreference chains). The corpus con-
tains no additional noun phrase coreference infor-
mation.
For our experiments, we randomly split the data
set into a training set consisting of 400 documents
and a test set consisting of the remaining 135 doc-
uments. We use the same test set for all experi-
3For the transitive closure, n is the number of items in a
document, which is O(
?
k), where k is the number of NP
pairs. Thus, transitive closure is sublinear in the number of
training instances.
4The MPQA corpus is available at
http://nrrc.mitre.org/NRRC/publications.htm.
341
ments, although some learning runs were trained
on 200 training documents (see next Subsection).
The test set contains a total of 4736 source NPs
(average of 35.34 source NPs per document) split
into 1710 total source NP chains (average of 12.76
chains per document) for an average of 2.77 source
NPs per chain.
6.2 Implementation
We implemented the StRip algorithm by modify-
ing JRip ? the java implementation of RIPPER in-
cluded in the WEKA toolkit (Witten and Frank,
2000). The WEKA implementation follows the
original RIPPER specification. We changed the
implementation to incorporate the modifications
suggested by the StRip algorithm; we also mod-
ified the underlying data representations and data
handling techniques for efficiency. Also due to ef-
ficiency considerations, we train StRip only on the
200-document training set.
6.3 Competitive baselines
We compare the results of the new method to three
fully supervised baseline systems, each of which
employs the same traditional coreference resolu-
tion approach. In particular, we use the afore-
mentioned algorithm proposed by Ng and Cardie
(2002), which combines a pairwise NP corefer-
ence classifier with single-link clustering.
For one baseline, we train the coreference reso-
lution algorithm on the MPQA src corpus ? the
labeled portion of the MPQA corpus (i.e. NPs
from the source coreference chains) with unla-
beled instances removed.
The second and third baselines investigate
whether the source coreference resolution task can
benefit from NP coreference resolution training
data from a different domain. Thus, we train the
traditional coreference resolution algorithm on the
MUC6 and MUC7 coreference-annotated corpora5
that contain documents similar in style to those in
the MPQA corpus (e.g. newspaper articles), but
emanate from different domains.
For all baselines we targeted the best possi-
ble systems by trying two pairwise NP classifiers
(RIPPER and an SVM in the SV M light imple-
mentation (Joachims, 1998)), many different pa-
rameter settings for the classifiers, two different
feature sets, two different training set sizes (the
5We train each baseline using both the development set
and the test set from the corresponding MUC corpus.
full training set and a smaller training set consist-
ing of half of the documents selected at random),
and three different instance selection algorithms6.
This variety of classifier and training data settings
was motivated by reported differences in perfor-
mance of coreference resolution approaches w.r.t.
these variations (Ng and Cardie, 2002). More de-
tails on the different parameter settings and in-
stance selection algorithms as well as trends in the
performance of different settings can be found in
Stoyanov and Cardie (2006). In the experiments
below we report the best performance of each of
the two learning algorithms on the MPQA test
data.
6.4 Evaluation
In addition to the baselines described above, we
evaluate StRip both with and without unlabeled
data. That is, we train on the MPQA corpus StRip
using either all NPs or just opinion source NPs.
We use the B3 (Bagga and Baldwin, 1998) eval-
uation measure as well as precision, recall, and
F1 measured on the (positive) pairwise decisions.
B3 is a measure widely used for evaluating coref-
erence resolution algorithms. The measure com-
putes the precision and recall for each NP mention
in a document, and then averages them to produce
combined results for the entire output. More pre-
cisely, given a mention i that has been assigned
to chain ci, the precision for mention i is defined
as the number of correctly identified mentions in
ci divided by the total number of mentions in ci.
Recall for i is defined as the number of correctly
identified mentions in ci divided by the number of
mentions in the gold standard chain for i.
Results are shown in Table 1. The first six
rows of results correspond to the fully supervised
baseline systems trained on different corpora ?
MUC6, MUC7, and MPQA src. The seventh row
of results shows the performance of StRip using
only labeled data. The final row of the table shows
the results for partially supervised learning with
unlabeled data. The table lists results from the best
performing run for each algorithm.
Performance among the baselines trained on the
MUC data is comparable. However, the two base-
line runs trained on the MPQA src corpus (i.e. re-
sults rows five and six) show slightly better perfor-
mance on the B3 metric than the baselines trained
6The goal of the instance selection algorithms is to bal-
ance the data, which contains many more negative than posi-
tive instances
342
ML Framework Training set Classifier B3 precision recall F1
Fully supervised MUC6 SVM 81.2 72.6 52.5 60.9
RIPPER 80.7 57.4 63.5 60.3
MUC7 SVM 81.7 65.6 55.9 60.4
RIPPER 79.7 71.6 48.5 57.9
MPQA src SVM 81.8 57.5 62.9 60.2
RIPPER 81.8 72.0 52.5 60.6
StRip 82.3 76.5 56.1 64.6
Partially supervised MPQA all StRip 83.2 77.1 59.4 67.1
Table 1: Results for Source Coreference. MPQA src stands for the MPQA corpus limited to only source
NPs, while MPQA full contains the unlabeled NPs.
on the MUC data, which indicates that for our
task the similarity of the documents in the train-
ing and test sets appears to be more important
than the presence of complete supervisory infor-
mation. (Improvements over the RIPPER runs
trained on the MUC corpora are statistically sig-
nificant7, while improvements over the SVM runs
are not.)
Table 1 also shows that StRip outperforms the
baselines on both performance metrics. StRip?s
performance is better than the baselines when
trained on MPQA src (improvement not statisti-
cally significant, p > 0.20) and even better when
trained on the full MPQA corpus, which includes
the unlabeled NPs (improvement over the base-
lines and the former StRip run statistically signif-
icant). These results confirm our hypothesis that
StRip improves due to two factors: first, consider-
ing pairwise decisions in the context of the clus-
tering function leads to improvements in the clas-
sifier; and, second, StRip can take advantage of
the unlabeled portion of the data.
StRip?s performance is all the more impressive
considering the strength of the SVM and RIPPER
baselines, which which represent the best runs
across the 336 different parameter settings tested
for SV M light and 144 different settings tested for
RIPPER. In contrast, all four of the StRip runs us-
ing the full MPQA corpus (we vary the loss ratio
for false positive/false negative cost) outperform
those baselines.
7 Future Work
Source coreference resolution is only one aspect
of opinion summarization. Additionally, an opin-
ion summarization system will need to handle
7Statistical significance is measured using both a 2-tailed
paired t-test and the Wilcoxon matched-pairs signed-ranks
test (p < 0.05). The two tests agreed on all significance
judgements, so we will not report them separately.
the closely related task of target coreference res-
olution in order to cluster targets of opinions8
and combine multiple conflicting opinions from a
source to the same targets. Furthermore, a fully
automatic opinion summarizer requires automatic
source and opinion extractors. While we antici-
pate that target coreference resolution will be sub-
ject to error rates similar to those of source coref-
erence resolution, incorporating these imperfect
opinions and sources will further impair the per-
formance of the opinion summarizer. We are not
aware of any measure that can be directly used
to assess the goodness of opinion summaries, but
plan to develop such in future work in conjunc-
tion with the development of methods for creating
opinion summaries completely automatically. The
evaluation metrics will likely have to depend on
the task for which the summaries are used.
A limitation of our approach to partially super-
vised clustering is that we do not directly optimize
for the performance measure (e.g. B3). Other ef-
forts in the area of supervised clustering (Finley
and Joachims, 2005; Li and Roth, 2005) have sug-
gested ways to learn distance measures that can
optimize directly for a desired performance mea-
sure. We plan to investigate algorithms that can di-
rectly optimize for complex measures (such as B3)
for the problem of partially supervised clustering.
Unfortunately, a measure as complex as B3 makes
extending existing approaches far from trivial due
to the difficulty of establishing the connection be-
tween individual pairwise decisions (the distance
metric) and the score of the clustering algorithm.
Acknowledgements
The authors would like to thank Vincent Ng and
Art Munson for providing coreference resolution
8We did not tackle the task of target coreference resolu-
tion in this paper because the MPQA corpus did not contain
target annotations at the time of publication.
343
code, members of the Cornell NLP group (es-
pecially Yejin Choi and Art Munson) for many
helpful discussions, and the anonymous review-
ers for their insightful comments. This work was
supported by the Advanced Research and Devel-
opment Activity (ARDA), by NSF Grants IIS-
0535099 and IIS-0208028, by gifts from Google
and the Xerox Foundation, and by an NSF Gradu-
ate Research Fellowship to the first author.
References
A. Bagga and B. Baldwin. 1998. Entity-based cross-
document coreferencing using the vector space model. In
In Proceedings of COLING/ACL.
S. Basu. 2005. Semi-supervised Clustering: Probabilistic
Models, Algorithms and Experiments. Ph.D. thesis, De-
partment of Computer Sciences, UT at Austin.
S. Bethard, H. Yu, A. Thornton, V. Hativassiloglou, and
D. Jurafsky. 2004. Automatic extraction of opinion
propositions and their holders. In 2004 AAAI Spring Sym-
posium on Exploring Attitude and Affect in Text.
Y. Choi, C. Cardie, E. Riloff, and S. Patwardhan. 2005. Iden-
tifying sources of opinions with conditional random fields
and extraction patterns. In Proceedings of EMNLP.
C. Coglianese. 2004. E-rulemaking: Information technology
and regulatory policy: New directions in digital govern-
ment research. Technical report, Harvard University, J. F.
Kennedy School of Government.
W. Cohen. 1995. Fast effective rule induction. In Proceed-
ings of ICML.
S. Das and M. Chen. 2001. Yahoo for amazon: Extracting
market sentiment from stock message boards. In Proceed-
ings of APFAAC.
K. Dave, S. Lawrence, and D. Pennock. 2003. Mining the
peanut gallery: Opinion extraction and semantic classifi-
cation of product reviews. In Proceedings of IWWWC.
I. Davidson and S. Ravi. 2005. Clustering with constraints:
Feasibility issues and the k-means algorithm. In Proceed-
ings of SDM.
A. Demiriz, K. P. Bennett, and M. J. Embrechts. 1999. Semi-
supervised clustering using genetic algorithms. In Pro-
ceeding of ANNIE.
T. Finley and T. Joachims. 2005. Supervised clustering with
support vector machines. In Proceedings of ICML.
R. Iida, K. Inui, H. Takamura, and Y. Matsumoto. 2003. In-
corporating contextual cues in trainable models for coref-
erence resolution. In Proceedings of the EACL Workshop
on The Computational Treatment of Anaphora.
T. Joachims. 1998. Making large-scale support vector
machine learning practical. In A. Smola B. Scho?lkopf,
C. Burges, editor, Advances in Kernel Methods: Support
Vector Machines. MIT Press, Cambridge, MA.
S. Kim and E. Hovy. 2005. Identifying opinion holders for
question answering in opinion texts. In Proceedings of
AAAI Workshop on Question Answering in Restricted Do-
mains.
X. Li and D. Roth. 2005. Discriminative training of cluster-
ing functions: Theory and experiments with entity identi-
fication. In Proceedings of CoNLL.
A. McCallum and B. Wellner. 2003. Toward conditional
models of identity uncertainty with application to proper
noun coreference. In Proceedings of the IJCAI Workshop
on Information Integration on the Web.
T. Morton. 2000. Coreference for NLP applications. In Pro-
ceedings of ACL.
V. Ng and C. Cardie. 2002. Improving machine learning
approaches to coreference resolution. In In Proceedings
of ACL.
B. Pang and L. Lee. 2004. A sentimental education: Senti-
ment analysis using subjectivity summarization based on
minimum cuts. In Proceedings of ACL.
B. Pang, L. Lee, and S. Vaithyanathan. 2002. Thumbs
up? Sentiment classification using machine learning tech-
niques. In Proceedings of EMNLP.
E. Riloff and J. Wiebe. 2003. Learning extraction patterns
for subjective expressions. In Proceesings of EMNLP.
E. Riloff, J. Wiebe, and W. Phillips. 2005. Exploiting sub-
jectivity classification to improve information extraction.
In Proceedings of AAAI.
V. Stoyanov and C. Cardie. 2006. Toward opinion summa-
rization: Linking the sources. In Proceedings of the ACL
Workshop on Sentiment and Subjectivity in Text.
V. Stoyanov, C. Cardie, and J. Wiebe. 2005. Multi-
Perspective question answering using the OpQA corpus.
In Proceedings of EMNLP.
P. Turney. 2002. Thumbs up or thumbs down? Semantic ori-
entation applied to unsupervised classification of reviews.
In Proceedings of ACL.
K. Wagstaff and C. Cardie. 2000. Clustering with instance-
level constraints. In Proceedings of the 17-th National
Conference on Artificial Intelligence and 12-th Confer-
ence on Innovative Applications of Artificial Intelligence.
J. Wiebe and E. Riloff. 2005. Creating subjective and objec-
tive sentence classifiers from unannotated texts. In Pro-
ceedings of CICLing.
J. Wiebe, T. Wilson, and C. Cardie. 2005. Annotating ex-
pressions of opinions and emotions in language. Lan-
guage Resources and Evaluation, 1(2).
T. Wilson, J. Wiebe, and R. Hwa. 2004. Just how mad are
you? Finding strong and weak opinion clauses. In Pro-
ceedings of AAAI.
I.H. Witten and E. Frank. 2000. Data Mining: Practical ma-
chine learning tools with Java implementations. Morgan
Kaufmann, San Francisco.
H. Yu and V. Hatzivassiloglou. 2003. Towards answer-
ing opinion questions: Separating facts from opinions and
identifying the polarity of opinion sentences. In Proceed-
ings of EMNLP.
344
Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing (EMNLP 2006), pages 431?439,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Joint Extraction of Entities and Relations for Opinion Recognition
Yejin Choi and Eric Breck and Claire Cardie
Department of Computer Science
Cornell University
Ithaca, NY 14853
{ychoi,ebreck,cardie}@cs.cornell.edu
Abstract
We present an approach for the joint ex-
traction of entities and relations in the con-
text of opinion recognition and analysis.
We identify two types of opinion-related
entities ? expressions of opinions and
sources of opinions ? along with the link-
ing relation that exists between them. In-
spired by Roth and Yih (2004), we employ
an integer linear programming approach
to solve the joint opinion recognition task,
and show that global, constraint-based in-
ference can significantly boost the perfor-
mance of both relation extraction and the
extraction of opinion-related entities. Per-
formance further improves when a seman-
tic role labeling system is incorporated.
The resulting system achieves F-measures
of 79 and 69 for entity and relation extrac-
tion, respectively, improving substantially
over prior results in the area.
1 Introduction
Information extraction tasks such as recognizing
entities and relations have long been considered
critical to many domain-specific NLP tasks (e.g.
Mooney and Bunescu (2005), Prager et al (2000),
White et al (2001)). Researchers have further
shown that opinion-oriented information extrac-
tion can provide analogous benefits to a variety of
practical applications including product reputation
tracking (Morinaga et al, 2002), opinion-oriented
question answering (Stoyanov et al, 2005), and
opinion-oriented summarization (e.g. Cardie et
al. (2004), Liu et al (2005)). Moreover, much
progress has been made in the area of opinion ex-
traction: it is possible to identify sources of opin-
ions (i.e. the opinion holders) (e.g. Choi et al
(2005) and Kim and Hovy (2005b)), to determine
the polarity and strength of opinion expressions
(e.g. Wilson et al (2005)), and to recognize propo-
sitional opinions and their sources (e.g. Bethard
et al (2004)) with reasonable accuracy. To date,
however, there has been no effort to simultane-
ously identify arbitrary opinion expressions, their
sources, and the relations between them. Without
progress on the joint extraction of opinion enti-
ties and their relations, the capabilities of opinion-
based applications will remain limited.
Fortunately, research in machine learning has
produced methods for global inference and joint
classification that can help to address this defi-
ciency (e.g. Bunescu and Mooney (2004), Roth
and Yih (2004)). Moreover, it has been shown that
exploiting dependencies among entities and/or re-
lations via global inference not only solves the
joint extraction task, but often boosts performance
on the individual tasks when compared to clas-
sifiers that handle the tasks independently ? for
semantic role labeling (e.g. Punyakanok et al
(2004)), information extraction (e.g. Roth and Yih
(2004)), and sequence tagging (e.g. Sutton et al
(2004)).
In this paper, we present a global inference ap-
proach (Roth and Yih, 2004) to the extraction
of opinion-related entities and relations. In par-
ticular, we aim to identify two types of entities
(i.e. spans of text): entities that express opin-
ions and entities that denote sources of opinions.
More specifically, we use the term opinion expres-
sion to denote all direct expressions of subjectiv-
ity including opinions, emotions, beliefs, senti-
ment, etc., as well as all speech expressions that
introduce subjective propositions; and use the term
source to denote the person or entity (e.g. a re-
431
port) that holds the opinion.1 In addition, we
aim to identify the relations between opinion ex-
pression entities and source entities. That is, for
a given opinion expression Oi and source entity
Sj , we determine whether the relation Li,j def=
(Sj expresses Oi) obtains, i.e. whether Sj is the
source of opinion expression Oi. We refer to this
particular relation as the link relation in the rest
of the paper. Consider, for example, the following
sentences:
S1. [Bush](1) intends(1) to curb the increase in
harmful gas emissions and is counting on(1)
the good will(2) of [US industrialists](2) .
S2. By questioning(3) [the Imam](4)?s edict(4) [the
Islamic Republic of Iran](3) made [the people
of the world](5) understand(5)...
The underlined phrases above are opinion expres-
sions and phrases marked with square brackets are
source entities. The numeric superscripts on en-
tities indicate link relations: a source entity and
an opinion expression with the same number sat-
isfy the link relation. For instance, the source en-
tity ?Bush? and the opinion expression ?intends?
satisfy the link relation, and so do ?Bush? and
?counting on.? Notice that a sentence may con-
tain more than one link relation, and link relations
are not one-to-one mappings between sources and
opinions. Also, the pair of entities in a link rela-
tion may not be the closest entities to each other, as
is the case in the second sentence, between ?ques-
tioning? and ?the Islamic Republic of Iran.?
We expect the extraction of opinion relations to
be critical for many opinion-oriented NLP appli-
cations. For instance, consider the following ques-
tion that might be given to a question-answering
system:
? What is the Imam?s opinion toward the Islamic
Republic of Iran?
Without in-depth opinion analysis, the question-
answering system might mistake example S2 as
relevant to the query, even though S2 exhibits the
opinion of the Islamic Republic of Iran toward
Imam, not the other way around.
Inspired by Roth and Yih (2004), we model
our task as global, constraint-based inference over
separately trained entity and relation classifiers.
In particular, we develop three base classifiers:
two sequence-tagging classifiers for the extraction
1See Wiebe et al (2005) for additional details.
of opinion expressions and sources, and a binary
classifier to identify the link relation. The global
inference procedure is implemented via integer
linear programming (ILP) to produce an optimal
and coherent extraction of entities and relations.
Because many (60%) opinion-source relations
appear as predicate-argument relations, where the
predicate is a verb, we also hypothesize that se-
mantic role labeling (SRL) will be very useful for
our task. We present two baseline methods for
the joint opinion-source recognition task that use
a state-of-the-art SRL system (Punyakanok et al,
2005), and describe two additional methods for in-
corporating SRL into our ILP-based system.
Our experiments show that the global inference
approach not only improves relation extraction
over the base classifier, but does the same for in-
dividual entity extractions. For source extraction
in particular, our system achieves an F-measure of
78.1, significantly outperforming previous results
in this area (Choi et al, 2005), which obtained an
F-measure of 69.4 on the same corpus. In addition,
we achieve an F-measure of 68.9 for link relation
identification and 82.0 for opinion expression ex-
traction; for the latter task, our system achieves
human-level performance.2
2 High-Level Approach and Related
Work
Our system operates in three phases.
Opinion and Source Entity Extraction We
begin by developing two separate token-level
sequence-tagging classifiers for opinion expres-
sion extraction and source extraction, using linear-
chain Conditional Random Fields (CRFs) (Laf-
ferty et al, 2001). The sequence-tagging classi-
fiers are trained using only local syntactic and lex-
ical information to extract each type of entity with-
out knowledge of any nearby or neighboring enti-
ties or relations. We collect n-best sequences from
each sequence tagger in order to boost the recall of
the final system.
Link Relation Classification We also develop
a relation classifier that is trained and tested on
all pairs of opinion and source entities extracted
from the aforementioned n-best opinion expres-
sion and source sequences. The relation classifier
is modeled using Markov order-0 CRFs(Lafferty
2Wiebe et al (2005) reports human annotation agreement
for opinion expression as 82.0 by F1 measure.
432
et al, 2001), which are equivalent to maximum en-
tropy models. It is trained using only local syntac-
tic information potentially useful for connecting a
pair of entities, but has no knowledge of nearby or
neighboring extracted entities and link relations.
Integer Linear Programming Finally, we for-
mulate an integer linear programming problem for
each sentence using the results from the previous
two phases. In particular, we specify a number
of soft and hard constraints among relations and
entities that take into account the confidence val-
ues provided by the supporting entity and relation
classifiers, and that encode a number of heuristics
to ensure coherent output. Given these constraints,
global inference via ILP finds the optimal, coher-
ent set of opinion-source pairs by exploiting mu-
tual dependencies among the entities and relations.
While good performance in entity or relation
extraction can contribute to better performance of
the final system, this is not always the case. Pun-
yakanok et al (2004) notes that, in general, it is
better to have high recall from the classifiers in-
cluded in the ILP formulation. For this reason, it is
not our goal to directly optimize the performance
of our opinion and source entity extraction models
or our relation classifier.
The rest of the paper is organized as follows.
Related work is outlined below. Section 3 de-
scribes the components of the first phase of our
system, the opinion and source extraction classi-
fiers. Section 4 describes the construction of the
link relation classifier for phase two. Section 5
describes the ILP formulation to perform global
inference over the results from the previous two
phases. Experimental results that compare our ILP
approach to a number of baselines are presented in
Section 6. Section 7 describes how SRL can be in-
corporated into our global inference system to fur-
ther improve the performance. Final experimental
results and discussion comprise Section 8.
Related Work The definition of our source-
expresses-opinion task is similar to that of Bethard
et al (2004); however, our definition of opin-
ion and source entities are much more extensive,
going beyond single sentences and propositional
opinion expressions. In particular, we evaluate
our approach with respect to (1) a wide variety
of opinion expressions, (2) explicit and implicit3
sources, (3) multiple opinion-source link relations
3Implicit sources are those that are not explicitly men-
tioned. See Section 8 for more details.
per sentence, and (4) link relations that span more
than one sentence. In addition, the link rela-
tion model explicitly exploits mutual dependen-
cies among entities and relations, while Bethard
et al (2004) does not directly capture the potential
influence among entities.
Kim and Hovy (2005b) and Choi et al (2005)
focus only on the extraction of sources of
opinions, without extracting opinion expressions.
Specifically, Kim and Hovy (2005b) assume a pri-
ori existence of the opinion expressions and ex-
tract a single source for each, while Choi et al
(2005) do not explicitly extract opinion expres-
sions nor link an opinion expression to a source
even though their model implicitly learns approxi-
mations of opinion expressions in order to identify
opinion sources. Other previous research focuses
only on the extraction of opinion expressions (e.g.
Kim and Hovy (2005a), Munson et al (2005) and
Wilson et al (2005)), omitting source identifica-
tion altogether.
There have also been previous efforts to si-
multaneously extract entities and relations by ex-
ploiting their mutual dependencies. Roth and
Yih (2002) formulated global inference using a
Bayesian network, where they captured the influ-
ence between a relation and a pair of entities via
the conditional probability of a relation, given a
pair of entities. This approach however, could not
exploit dependencies between relations. Roth and
Yih (2004) later formulated global inference using
integer linear programming, which is the approach
that we apply here. In contrast to our work, Roth
and Yih (2004) operated in the domain of factual
information extraction rather than opinion extrac-
tion, and assumed that the exact boundaries of en-
tities from the gold standard are known a priori,
which may not be available in practice.
3 Extraction of Opinion and Source
Entities
We develop two separate sequence tagging classi-
fiers for opinion extraction and source extraction,
using linear-chain Conditional Random Fields
(CRFs) (Lafferty et al, 2001). The sequence tag-
ging is encoded as the typical ?BIO? scheme.4
Each training or test instance represents a sen-
tence, encoded as a linear chain of tokens and their
4
?B? is for the token that begins an entity, ?I? is for to-
kens that are inside an entity, and ?O? is for tokens outside an
entity.
433
associated features. Our feature set is based on
that of Choi et al (2005) for source extraction5,
but we include additional lexical and WordNet-
based features. For simplicity, we use the same
features for opinion entity extraction and source
extraction, and let the CRFs learn appropriate fea-
ture weights for each task.
3.1 Entity extraction features
For each token xi, we include the following fea-
tures. For details, see Choi et al (2005).
word: words in a [-4, +4] window centered on xi.
part-of-speech: POS tags in a [-2, +2] window.6
grammatical role: grammatical role (subject, ob-
ject, prepositional phrase types) of xi derived from
a dependency parse.7
dictionary: whether xi is in the opinion expres-
sion dictionary culled from the training data and
augmented by approximately 500 opinion words
from the MPQA Final Report8. Also computed
for tokens in a [-1, +1] window and for xi?s parent
?chunk? in the dependency parse.
semantic class: xi?s semantic class.9
WordNet: the WordNet hypernym of xi.10
4 Relation Classification
We also develop a maximum entropy binary clas-
sifier for opinion-source link relation classifica-
tion. Given an opinion-source pair, Oi-Sj , the re-
lation classifier decides whether the pair exhibits
a valid link relation, Li,j . The relation classifier
focuses only on the syntactic structure and lexical
properties between the two entities of a given pair,
without knowing whether the proposed entities are
correct. Opinion and source entities are taken from
the n-best sequences of the entity extraction mod-
els; therefore, some are invariably incorrect.
From each sentence, we create training and test
instances for all possible opinion-source pairings
that do not overlap: we create an instance for Li,j
only if the span of Oi and Sj do not overlap.
For training, we also filter out instances for
which neither the proposed opinion nor source en-
5We omit only the extraction pattern features.
6Using GATE: http://gate.ac.uk/
7Provided by Rebecca Hwa, based on the Collins parser:
ftp://ftp.cis.upenn.edu/pub/mcollins/PARSER.tar.gz
8https://rrc.mitre.org/pubs/mpqaFinalReport.pdf
9Using SUNDANCE: (http://www.cs.utah.edu/r?iloff/
publications.html#sundance)
10http://wordnet.princeton.edu/
tity overlaps with a correct opinion or source en-
tity per the gold standard. This training instance
filtering helps to avoid confusion between exam-
ples like the following (where entities marked in
bold are the gold standard entities, and entities
in square brackets represent the n-best output se-
quences from the entity extraction classifiers):
(1) [The president] s1 walked away from [the
meeting] o1, [ [revealing] o2 his disap-
pointment] o3 with the deal.
(2) [The monster] s2 walked away, [revealing] o4
a little box hidden underneath.
For these sentences, we construct training in-
stances for L1,1, L1,2, and L1,3, but not L2,4,
which in fact has very similar sentential structure
as L1,2, and hence could confuse the learning al-
gorithm.
4.1 Relation extraction features
The training and test instances for each (potential)
link Li,j (with opinion candidate entity Oi and
source candidate entity Sj) include the following
features.
opinion entity word: the words contained in Oi.
phrase type: the syntactic category of the con-
stituent in which the entity is embedded, e.g. NP
or VP. We encode separate features for Oi and Sj .
grammatical role: the grammatical role of the
constituent in which the entity is embedded.
Grammatical roles are derived from dependency
parse trees, as done for the entity extraction classi-
fiers. We encode separate features for Oi and Sj .
position: a boolean value indicating whether Sj
precedes Oi.
distance: the distance between Oi and Sj in num-
bers of tokens. We use four coarse categories: ad-
jacent, very near, near, far.
dependency path: the path through the depen-
dency tree from the head of Sj to the head of Oi.
For instance, ?subj?verb? or ?subj?verb?obj?.
voice: whether the voice of Oi is passive or active.
syntactic frame: key intra-sentential relations be-
tween Oi and Sj . The syntactic frames that we use
are:
? [E1:role] [distance] [E2:role], where distance
? {adjacent, very near, near, far}, and Ei:role
is the grammatical role of Ei. Either E1 is an
opinion entity and E2 is a source, or vice versa.
? [E1:phrase] [distance] [E2:phrase], where
Ei:phrase is the phrasal type of entity Ei.
434
? [E1:phrase] [E2:headword], where E2 must be
the opinion entity, and E1 must be the source en-
tity (i.e. no lexicalized frames for sources). E1
and E2 can be contiguous.
? [E1:role] [E2:headword], where E2 must be the
opinion entity, and E1 must be the source entity.
? [E1:phrase] NP [E2:phrase] indicates the
presence of specific syntactic patterns, e.g.
?VP NP VP? depending on the possible phrase
types of opinion and source entities. The three
phrases do not need to be contiguous.
? [E1:phrase] VP [E2:phrase] (See above.)
? [E1:phrase] [wh-word] [E2:phrase] (See
above.)
? Src [distance] [x] [distance] Op, where x ?
{by, of, from, for, between, among, and, have,
be, will, not, ], ?, . . . }.
When a syntactic frame is matched to a sen-
tence, the bracketed items should be instantiated
with particular values corresponding to the sen-
tence. Pattern elements without square brackets
are constants. For instance, the syntactic frame
?[E1:phrase] NP [E2:phrase]? may be instantiated
as ?VP NP VP?. Some frames are lexicalized with
respect to the head of an opinion entity to reflect
the fact that different verbs expect source enti-
ties in different argument positions (e.g. SOURCE
blamed TARGET vs. TARGET angered SOURCE).
5 Integer Linear Programming
Approach
As noted in the introduction, we model our task
as global, constraint-based inference over the sep-
arately trained entity and relation classifiers, and
implement the inference procedure as binary in-
teger linear programming (ILP) ((Roth and Yih,
2004), (Punyakanok et al, 2004)). ILP consists
of an objective function which is a dot product
between a vector of variables and a vector of
weights, and a set of equality and inequality con-
straints among variables. Given an objective func-
tion and a set of constraints, LP finds the opti-
mal assignment of values to variables, i.e. one that
minimizes the objective function. In binary ILP,
the assignments to variables must be either 0 or 1.
The variables and constraints defined for the opin-
ion recognition task are summarized in Table 1 and
explained below.
Entity variables and weights For each opinion
entity, we add two variables, Oi and O?i, where
Oi = 1 means to extract the opinion entity, and
Objective function f
=
?
i(woiOi) +
?
i(w?oiO?i)
+
?
j(wsjSj) +
?
j(w?sj S?j)
+
?
i,j(wli,jLi,j) +
?
i(w?li,j L?i,j)
?i, Oi + O?i = 1
?j, Sj + S?j = 1
?i, j, Li,j + L?i,j = 1
?i, Oi =
?
j Li,j
?j, Sj + Aj =
?
i Li,j
?j, Aj ? Sj ? 0
?i, j, i < j, Xi + Xj = 1,X ? {S,O}
Table 1: Binary ILP formulation
O?i = 1 means to discard the opinion entity. To
ensure coherent assignments, we add equality con-
straints ?i, Oi + O?i = 1. The weights woi and
w?oi for Oi and O?i respectively, are computed as
a negative conditional probability of the span of
an entity to be extracted (or suppressed) given the
labelings of the adjacent variables of the CRFs:
woi
def= ?P (xk, xk+1, ..., xl|xk?1, xl+1)
where xk = ?B?
& xm = ?I? for m ? [k + 1, l]
w?oi
def= ?P (xk, xk+1, ..., xl|xk?1, xl+1)
where xm = ?O? for m ? [k, l]
where xi is the value assigned to the random vari-
able of the CRF corresponding to an entity Oi.
Likewise, for each source entity, we add two vari-
ables Sj and S?j and a constraint Sj + S?j = 1. The
weights for source variables are computed in the
same way as opinion entities.
Relation variables and weights For each link
relation, we add two variables Li,j and L?i,j , and
a constraint Li,j + L?i,j = 1. By the definition of
a link, if Li,j = 1, then it is implied that Oi = 1
and Sj = 1. That is, if a link is extracted, then the
pair of entities for the link must be also extracted.
Constraints to ensure this coherency are explained
in the following subsection. The weights for link
variables are based on probabilities from the bi-
nary link classifier.
Constraints for link coherency In our corpus, a
source entity can be linked to more than one opin-
ion entity, but an opinion entity is linked to only
435
one source. Nonetheless, the majority of opinion-
source pairs involve one-to-one mappings, which
we encode as hard and soft constraints as follows:
For each opinion entity, we add an equality con-
straint Oi =
?
j Li,j to enforce that only one
link can emanate from an opinion entity. For each
source entity, we add an equality constraint and an
inequality constraint that together allow a source
to link to at most two opinions: Sj +Aj =
?
i Li,j
and Aj ? Sj ? 0, where Aj is an auxiliary vari-
able, such that its weight is some positive constant
value that suppresses Aj from being assigned to 1.
And Aj can be assigned to 1 only if Sj is already
assigned to 1. It is possible to add more auxiliary
variables to allow more than two opinions to link
to a source, but for our experiments two seemed to
be a reasonable limit.
Constraints for entity coherency When we use
n-best sequences where n > 1, proposed entities
can overlap. Because this should not be the case
in the final result, we add an equality constraint
Xi + Xj = 1, X ? {S,O} for all pairs of entities
with overlapping spans.
Adjustments to weights To balance the preci-
sion and recall, and to take into account the per-
formance of different base classifiers, we apply ad-
justments to weights as follows.
1) We define six coefficients cx and c?x, where
x ? {O,S,L} to modify a group of weights
as follows.
?i, x, wxi := wxi ? cx;
?i, x, w?xi := w?xi ? c?x;
In general, increasing cx will promote recall,
while increasing c?x will promote precision.
Also, setting co > cs will put higher confi-
dence on the opinion extraction classifier than
the source extraction classifier.
2) We also define one constant cA to set the
weights for auxiliary variable Ai. That is,
?i, wAi := cA.
3) Finally, we adjust the confidence of the link
variable based on n-th-best sequences of the en-
tity extraction classifiers as follows.
?i, wLi,j := wLi,j ? d
where d def= 4/(3 + min(m,n)), when Oi is
from an m-th sequence and Sj is from a n-th
sequence.11
11This will smoothly degrade the confidence of a link
based on the entities from higher n-th sequences. Values of d
decrease as 4/4, 4/5, 4/6, 4/7....
6 Experiments?I
We evaluate our system using the NRRC Multi-
Perspective Question Answering (MPQA) corpus
that contains 535 newswire articles that are man-
ually annotated for opinion-related information.
In particular, our gold standard opinion entities
correspond to direct subjective expression anno-
tations and subjective speech event annotations
(i.e. speech events that introduce opinions) in the
MPQA corpus (Wiebe et al, 2005). Gold stan-
dard source entities and link relations can be ex-
tracted from the agent attribute associated with
each opinion entity. We use 135 documents as a
development set and report 10-fold cross valida-
tion results on the remaining 400 documents in all
experiments below.
We evaluate entity and link extraction using
both an overlap and exact matching scheme.12 Be-
cause the exact start and endpoints of the man-
ual annotations are somewhat arbitrary, the over-
lap scheme is more reasonable for our task (Wiebe
et al, 2005). We report results according to both
matching schemes, but focus our discussion on re-
sults obtained using overlap matching.13
We use the Mallet14 implementation of CRFs.
For brevity, we will refer to the opinion extraction
classifier as CRF-OP, the source extraction classi-
fier as CRF-SRC, and the link relation classifier as
CRF-LINK. For ILP, we use Matlab, which pro-
duced the optimal assignment in a matter of few
seconds for each sentence. The weight adjustment
constants defined for ILP are based on the devel-
opment data.15
The link-nearest baselines For baselines, we
first consider a link-nearest heuristic: for each
opinion entity extracted by CRF-OP, the link-
nearest heuristic creates a link relation with the
closest source entity extracted by CRF-SRC. Re-
call that CRF-SRC and CRF-OP extract entities
from n-best sequences. We test the link-nearest
heuristic with n = {1, 2, 10} where larger n will
boost recall at the cost of precision. Results for the
12Given two links L1,1 = (O1, S1) and L2,2 = (O2, S2),
exact matching requires the spans of O1 and O2, and the
spans of S1 and S2, to match exactly, while overlap matching
requires the spans to overlap.
13Wiebe et al (2005) also reports the human annotation
agreement study via the overlap scheme.
14Available at http://mallet.cs.umass.edu
15co = 2.5, c?o = 1.0, cs = 1.5, c?s = 1.0, cL = 2.5, c?L =
2.5, cA = 0.2. Values are picked so as to boost recall while
reasonably suppressing incorrect links.
436
Overlap Match Exact Match
r(%) p(%) f(%) r(%) p(%) f(%)
NEAREST-1 51.6 71.4 59.9 26.2 36.9 30.7
NEAREST-2 60.7 45.8 52.2 29.7 19.0 23.1
NEAREST-10 66.3 20.9 31.7 28.2 00.0 00.0
SRL 59.7 36.3 45.2 32.6 19.3 24.2
SRL+CRF-OP 45.6 83.2 58.9 27.6 49.7 35.5
ILP-1 51.6 80.8 63.0 26.4 42.0 32.4
ILP-10 64.0 72.4 68.0 31.0 34.8 32.8
Table 2: Relation extraction performance
NEAREST-n : link-nearest heuristic w/ n-best
SRL : all V-A0 frames from SRL
SRL+CRF-OP : all V-A0 filtered by CRF-OP
ILP-n : ILP applied to n-best sequences
link-nearest heuristic on the full source-expresses-
opinion relation extraction task are shown in the
first three rows of table 2. NEAREST-1 performs
the best in overlap-match F-measure, reaching
59.9. NEAREST-10 has higher recall (66.3%), but
the precision is really low (20.9%). Performance
of the opinion and source entity classifiers will be
discussed in Section 8.
SRL baselines Next, we consider two base-
lines that use a state-of-the-art SRL system (Pun-
yakanok et al, 2005). In many link relations,
the opinion expression entity is a verb phrase and
the source entity is in an agent argument posi-
tion. Hence our second baseline, SRL, extracts
all verb(V)-agent(A0) frames from the output of
the SRL system and provides an upper bound on
recall (59.7%) for systems that use SRL in isola-
tion for our task. A more sophisticated baseline,
SRL+CRF-OP, extracts only those V-A0 frames
whose verb overlaps with entities extracted by the
opinion expression extractor, CRF-OP. As shown
in table 2, filtering out V-A0 frames that are in-
compatible with the opinion extractor boosts pre-
cision to 83.2%, but the F-measure (58.9) is lower
than that of NEAREST-1.
ILP results The ILP-n system in table 2 de-
notes the results of the ILP approach applied to the
n-best sequences. ILP-10 reaches an F-measure
of 68.0, a significant improvement over the high-
est performing baseline16 , and also a substantial
improvement over ILP-1. Note that the perfor-
mance of NEAREST-10 was much worse than that
16Statistically significant by paired-t test, where p <
0.001.
Overlap Match Exact Match
r(%) p(%) f(%) r(%) p(%) f(%)
ILP-1 51.6 80.8 63.0 26.4 42.0 32.4
ILP-10 64.0 72.4 68.0 31.0 34.8 32.8
ILP+SRL-f -1 51.7 81.5 63.3 26.6 42.5 32.7
ILP+SRL-f -10 65.7 72.4 68.9 31.5 34.3 32.9
ILP+SRL-fc-10 64.0 73.5 68.4 28.4 31.3 29.8
Table 3: Relation extraction with ILP and SRL
ILP-n : ILP applied to n-best sequences
ILP+SRL-f -n : ILP w/ SRL features, n-best
ILP+SRL-fc-n : ILP w/ SRL features,
and SRL constraints, n-best
of NEAREST-1, because the 10-best sequences in-
clude many incorrect entities whereas the corre-
sponding ILP formulation can discard the bad en-
tities by considering dependencies among entities
and relations.17
7 Additional SRL Incorporation
We next explore two approaches for more directly
incorporating SRL into our system.
Extra SRL Features for the Link classifier We
incorporate SRL into the link classifier by adding
extra features based on SRL. We add boolean fea-
tures to check whether the span of an SRL argu-
ment and an entity matches exactly. In addition,
we include syntactic frame features as follows:
? [E1:srl-arg] [E2:srl-arg], where Ei:srl-arg indi-
cates the SRL argument type of entity Ei.
? [E1.srl-arg] [E1:headword] [E2:srl-arg], where
E1 must be an opinion entity, and E2 must be a
source entity.
Extra SRL Constraints for the ILP phase We
also incorporate SRL into the ILP phase of our
system by adding extra constraints based on SRL.
In particular, we assign very high weights for links
that match V-A0 frames generated by SRL, in or-
der to force the extraction of V-A0 frames.
17A potential issue with overlap precision and recall is that
the measures may drastically overestimate the system?s per-
formance as follows: a system predicting a single link rela-
tion whose source and opinion expression both overlap with
every token of a document would achieve 100% overlap pre-
cision and recall. We can ensure this does not happen by mea-
suring the average number of (source, opinion) pairs to which
each correct or predicted pair is aligned (excluding pairs not
aligned at all). In our data, this does not exceed 1.08, (except
for baselines), so we can conclude these evaluation measures
are behaving reasonably.
437
Opinion Source Link
r(%) p(%) f(%) r(%) p(%) f(%) r(%) p(%) f(%)
Before ILP CRF-OP/SRC/LINK with 1 best 76.4 88.4 81.9 67.3 81.9 73.9 60.5 50.5 55.0
merged 10 best 95.7 31.2 47.0 95.3 24.5 38.9 N/A
After ILP ILP-SRL-f -10 75.1 82.9 78.8 80.6 75.7 78.1 65.7 72.4 68.9
ILP-SRL-f -10 ? CRF-OP/SRC with 1 best 82.3 81.7 82.0 81.5 73.4 77.3 N/A
Table 4: Entity extraction performance (by overlap-matching)
8 Experiments?II
Results using SRL are shown in Table 3 (on the
previous page). In the table, ILP+SRL-f denotes
the ILP approach using the link classifier with
the extra SRL ?f ?eatures, and ILP+SRL-fc de-
notes the ILP approach using both the extra SRL
?f ?eatures and the SRL ?c?onstraints. For compar-
ison, the ILP-1 and ILP-10 results from Table 2
are shown in rows 1 and 2.
The F-measure score of ILP+SRL-f -10 is 68.9,
about a 1 point increase from that of ILP-10,
which shows that extra SRL features for the link
classifier further improve the performance over
our previous best results.18 ILP+SRL-fc-10 also
performs better than ILP-10 in F-measure, al-
though it is slightly worse than ILP+SRL-f -10.
This indicates that the link classifier with extra
SRL features already makes good use of the V-A0
frames from the SRL system, so that forcing the
extraction of such frames via extra ILP constraints
only hurts performance by not allowing the extrac-
tion of non-V-A0 pairs in the neighborhood that
could have been better choices.
Contribution of the ILP phase In order to
highlight the contribution of the ILP phase for our
task, we present ?before? and ?after? performance
in Table 4. The first row shows the performance
of the individual CRF-OP, CRF-SRC, and CRF-
LINK classifiers before the ILP phase. Without the
ILP phase, the 1-best sequence generates the best
scores. However, we also present the performance
with merged 10-best entity sequences19 in order
to demonstrate that using 10-best sequences with-
out ILP will only hurt performance. The precision
of the merged 10-best sequences system is very
low, however the recall level is above 95% for both
18Statistically significant by paired-t test, where p <
0.001.
19If an entity Ei extracted by the ith-best sequence over-
laps with an entity Ej extracted by the jth-best sequence,
where i < j, then we discard Ej . If Ei and Ej do not over-
lap, then we extract both entities.
CRF-OP and CRF-SRC, giving an upper bound for
recall for our approach. The third row presents
results after the ILP phase is applied for the 10-
best sequences, and we see that, in addition to the
improved link extraction described in Section 7,
the performance on source extraction is substan-
tially improved, from F-measure of 73.9 to 78.1.
Performance on opinion expression extraction de-
creases from F-measure of 81.9 to 78.8. This de-
crease is largely due to implicit links, which we
will explain below. The fourth row takes the union
of the entities from ILP-SRL-f -10 and the entities
from the best sequences from CRF-OP and CRF-
SRC. This process brings the F-measure of CRF-
OP up to 82.0, with a different precision-recall
break down from those of 1-best sequences with-
out ILP phase. In particular, the recall on opinion
expressions now reaches 82.3%, while maintain-
ing a high precision of 81.7%.
Overlap Match Exact Match
r(%) p(%) f(%) r(%) p(%) f(%)
DEV.CONF 65.7 72.4 68.9 31.5 34.3 32.9
NO.CONF 63.7 76.2 69.4 30.9 36.7 33.5
Table 5: Relation extraction with ILP weight ad-
justment. (All cases using ILP+SRL-f -10)
Effects of ILP weight adjustment Finally, we
show the effect of weight adjustment in the ILP
formulation in Table 5. The DEV.CONF row shows
relation extraction performance using a weight
configuration based from the development data.
In order to see the effect of weight adjustment,
we ran an experiment, NO.CONF, using fixed de-
fault weights.20 Not surprisingly, our weight ad-
justment tuned from the development set is not the
optimal choice for cross-validation set. Neverthe-
less, the weight adjustment helps to balance the
precision and recall, i.e. it improves recall at the
20To be precise, cx = 1.0, c?x = 1.0 for x ? {O, S, L},
but cA = 0.2 is the same as before.
438
cost of precision. The weight adjustment is more
effective when the gap between precision and re-
call is large, as was the case with the development
data.
Implicit links A good portion of errors stem
from the implicit link relation, which our system
did not model directly. An implicit link relation
holds for an opinion entity without an associated
source entity. In this case, the opinion entity is
linked to an implicit source. Consider the follow-
ing example.
? Anti-Soviet hysteria was firmly oppressed.
Notice that opinion expressions such as ?Anti-
Soviet hysteria? and ?firmly oppressed? do not
have associated source entities, because sources of
these opinion expressions are not explicitly men-
tioned in the text. Because our system forces
each opinion to be linked with an explicit source
entity, opinion expressions that do not have ex-
plicit source entities will be dropped during the
global inference phase of our system. Implicit
links amount to 7% of the link relations in our
corpus, so the upper bound for recall for our ILP
system is 93%. In the future we will extend our
system to handle implicit links as well. Note that
we report results against a gold standard that in-
cludes implicit links. Excluding them from the
gold standard, the performance of our final sys-
tem ILP+SRL-f -10 is 72.6% in recall, 72.4% in
precision, and 72.5 in F-measure.
9 Conclusion
This paper presented a global inference approach
to jointly extract entities and relations in the con-
text of opinion oriented information extraction.
The final system achieves performance levels that
are potentially good enough for many practical
NLP applications.
Acknowledgments We thank the reviewers for their
many helpful comments and Vasin Punyakanok for running
our data through his SRL system. This work was sup-
ported by the Advanced Research and Development Activity
(ARDA), by NSF Grants IIS-0535099 and IIS-0208028, and
by gifts from Google and the Xerox Foundation.
References
S. Bethard, H. Yu, A. Thornton, V. Hativassiloglou and
D. Jurafsky 2004. Automatic Extraction of Opin-
ion Propositions and their Holders. In AAAI Spring
Symposium on Exploring Attitude and Affect in Text.
R. Bunescu and R. J. Mooney 2004. Collective In-
formation Extraction with Relational Markov Net-
works. In ACL.
C. Cardie, J. Wiebe, T. Wilson and D. Litman 2004.
Low-Level Annotations and Summary Representa-
tions of Opinions for Multi-Perspective Question
Answering. New Directions in Question Answering.
Y. Choi, C. Cardie, E. Riloff and S. Patwardhan 2005.
Identifying Sources of Opinions with Conditional
Random Fields and Extraction Patterns. In HLT-
EMNLP.
S. Kim and E. Hovy 2005. Automatic Detection of
Opinion Bearing Words and Sentences. In IJCNLP.
S. Kim and E. Hovy 2005. Identifying Opinion
Holders for Question Answering in Opinion Texts.
In AAAI Workshop on Question Answering in Re-
stricted Domains.
J. Lafferty, A. K. McCallum and F. Pereira 2001 Con-
ditional Random Fields: Probabilistic Models for
Segmenting and Labeling Sequence Data. In ICML.
B. Liu, M. Hu and J. Cheng 2005 Opinion Observer:
Analyzing and Comparing Opinions on the Web. In
WWW.
R. J. Mooney and R. Bunescu 2005 Mining Knowl-
edge from Text Using Information Extraction. In
SIGKDD Explorations.
S. Morinaga, K. Yamanishi, K. Tateishi and T.
Fukushima 2002. Mining product reputations on
the Web. In KDD.
M. A. Munson, C. Cardie and R. Caruana. 2005. Opti-
mizing to arbitrary NLP metrics using ensemble se-
lection. In HLT-EMNLP.
J. Prager, E. Brown, A. Coden and D. Radev 2000.
Question-answering by predictive annotation. In SI-
GIR.
V. Punyakanok, D. Roth and W. Yih 2005. General-
ized Inference with Multiple Semantic Role Label-
ing Systems (Shared Task Paper). In CoNLL.
V. Punyakanok, D. Roth, W. Yih and D. Zimak 2004.
Semantic Role Labeling via Integer Linear Program-
ming Inference. In COLING.
D. Roth and W. Yih 2004. A Linear Programming For-
mulation for Global Inference in Natural Language
Tasks. In CoNLL.
D. Roth and W. Yih 2002. Probabilistic Reasoning for
Entity and Relation Recognition. In COLING.
V. Stoyanov, C. Cardie and J. Wiebe 2005. Multi-
Perspective Question Answering Using the OpQA
Corpus. In HLT-EMNLP.
C. Sutton, K. Rohanimanesh and A. K. McCallum
2004. Dynamic Conditional Random Fields: Fac-
torized Probabilistic Models for Labeling and Seg-
menting Sequence Data. In ICML.
M. White, T. Korelsky, C. Cardie, V. Ng, D. Pierce and
K. Wagstaff 2001. Multi-document Summarization
via Information Extraction In HLT.
J. Wiebe and T. Wilson and C. Cardie 2005. Annotat-
ing Expressions of Opinions and Emotions in Lan-
guage. In Language Resources and Evaluation, vol-
ume 39, issue 2-3.
T. Wilson, J. Wiebe and P. Hoffmann 2005. Recogniz-
ing Contextual Polarity in Phrase-Level Sentiment
Analysis. In HLT-EMNLP.
439
Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers,
pages 1660?1669, Dublin, Ireland, August 23-29 2014.
Query-Focused Opinion Summarization for User-Generated Content
Lu Wang
1
Hema Raghavan
2
Claire Cardie
1
Vittorio Castelli
3
1
Department of Computer Science, Cornell University, Ithaca, NY 14853, USA
{luwang, cardie}@cs.cornell.edu
2
LinkedIn, CA, USA
hraghavan@linkedin.com
3
IBM T. J. Watson Research Center, Yorktown Heights, NY 10598, USA
vittorio@us.ibm.com
Abstract
We present a submodular function-based framework for query-focused opinion summarization. Within our
framework, relevance ordering produced by a statistical ranker, and information coverage with respect to
topic distribution and diverse viewpoints are both encoded as submodular functions. Dispersion functions
are utilized to minimize the redundancy. We are the first to evaluate different metrics of text similarity for
submodularity-based summarization methods. By experimenting on community QA and blog summariza-
tion, we show that our system outperforms state-of-the-art approaches in both automatic evaluation and
human evaluation. A human evaluation task is conducted on Amazon Mechanical Turk with scale, and
shows that our systems are able to generate summaries of high overall quality and information diversity.
1 Introduction
Social media forums, such as social networks, blogs, newsgroups, and community question answering
(QA), offer avenues for people to express their opinions as well collect other people?s thoughts on topics
as diverse as health, politics and software (Liu et al., 2008). However, digesting the large amount of
information in long threads on newsgroups, or even knowing which threads to pay attention to, can be
overwhelming. A text-based summary that highlights the diversity of opinions on a given topic can
lighten this information overload. In this work, we design a submodular function-based framework for
opinion summarization on community question answering and blog data.
Question: What is the long term effect of piracy on the music and film industry?
Best Answer: Rising costs for movies and music. ... If they sell less, they need to raise the price to make up for what they lost. The
other thing will be music and movies with less quality. ...
Other Answers:
Ans1: Its bad... really bad. (Just watch this movie and you will find out ... Piracy causes rappers to appear on your computer).
Ans2: By removing the profitability of music & film companies, piracy takes away their motivation to produce new music & movies.
If they can?t protect their copyrights, they can?t continue to do business. ...
Ans4: It is forcing them to rework their business model, which is a good thing. In short, I don?t think the music industry in particular
will ever enjoy the huge profits of the 90?s. ...
Ans6: Please-People in those businesses make millions of dollars as it is!! I don?t think piracy hurts them at all!!!
Figure 1: Example discussion on Yahoo! Answers. Besides the best answer, other answers also contain
relevant information (in italics). For example, the sentence in blue has a contrasting viewpoint compared
to the other answers.
Opinion summarization has previously been applied to restricted domains, such as product reviews (Hu
and Liu, 2004; Lerman et al., 2009) and news (Stoyanov and Cardie, 2006), where the output summary
is either presented in a structured way with respect to each aspect of the product or organized along
contrastive viewpoints. Unlike those works, we address user generated online data: community QA and
blogs. These forums use a substantially less formal language than news articles, and at the same time
address a much broader spectrum of topics than product reviews. As a result, they present new challenges
for automatic summarization. For example, Figure 1 illustrates a sample question from Yahoo! Answers
1
along with the answers from different users. The question receives more than one answer, and one of
them is selected as the ?best answer? by the asker or other participants. In general, answers from other
users also provide relevant information. While community QA successfully pools rich knowledge from
the wisdom of the crowd, users might need to seine through numerous posts to extract the information
This work is licensed under a Creative Commons Attribution 4.0 International Licence. Page numbers and proceedings footer
are added by the organisers. Licence details: http://creativecommons.org/licenses/by/4.0/
1
http://answers.yahoo.com/
1660
they need. Hence, it would be beneficial to summarize answers automatically and present the summaries
to users who ask similar questions in the future. In this work, we aim to return a summary that encapsu-
lates different perspectives for a given opinion question and a set of relevant answers or documents.
In our work we assume that there is a central topic (or query) on which a user is seeking diverse opin-
ions. We predict query-relevance through automatically learned statistical rankers. Our ranking function
not only aims to find sentences that are on the topic of the query but also ones that are ?opinionated?
through the use of several features that indicate subjectivity and sentiment. The relevance score is en-
coded in a submodular function. Diversity is accounted for by a dispersion function that maximizes the
pairwise distance between the pairs of sentences selected.
Our chief contributions are:
(1) We develop a submodular function-based framework for query-focused opinion summarization. To
the best of our knowledge, this is the first time that submodular functions have been used to support
opinion summarization. We test our framework on two tasks: summarizing opinionated sentences in
community QA (Yahoo! Answers) and blogs (TAC-2008 corpus). Human evaluation using Amazon Me-
chanical Turk shows that our system generates the best summary 57.1% of the time. On the other hand,
the best answer picked by Yahoo! users is chosen only 31.9% of the time. We also obtain significant
higher Pyramid F1 score on the blog task as compared to the system of Lin and Bilmes (2011).
(2) Within our summarization framework, the statistically learned sentence relevance is included as part
of our objective function, whereas previous work on submodular summarization (Lin and Bilmes, 2011)
only uses ngram overlap for query relevance. Additionally, we use Latent Dirichlet Allocation (Blei et
al., 2003) to model the topic structure of the sentences, and induce clusterings according to the learned
topics. Therefore, our system is capable of generating summaries with broader topic coverage.
(3) Furthermore, we are the first to study how different metrics for computing text similarity or dis-
similarity affect the quality of submodularity-based summarization methods. We show empirically that
lexical representation-based similarity, such as TFIDF scores, uniformly outperforms semantic similar-
ity computed with WordNet. Moreover, when measuring the summary diversity, topical representation
is marginally better than lexical representation, and both of them beats semantic representation.
2 Related Work
Our work falls in the realm of query-focused summarization, where a user asks a question and the sys-
tem generates a summary of the answers containing pertinent and diverse information. A wide range
of methods have been investigated, where relevance is often estimated through TF-IDF similarity (Car-
bonell and Goldstein, 1998), topic signature words (Lin and Hovy, 2000) or by learning a Bayesian model
over queries and documents (Daum?e and Marcu, 2006). Most work only implicitly penalizes summary
redundancy, e.g. by downweighting the importance of words that are already selected.
Encouraging diversity of a summary has recently been addressed through submodular functions, which
have been applied for multi-document summarization in newswire (Lin and Bilmes, 2011; Sipos et al.,
2012), and comments summarization (Dasgupta et al., 2013). However, these works either ignore the
query information (when available) or else use simple ngram matching between the query and sentences.
In contrast, we propose to optimize an objective function that addresses both relevance and diversity.
Previous work on generating opinion summaries mainly considers product reviews (Hu and Liu, 2004;
Lerman et al., 2009), and formal texts such as news articles (Stoyanov and Cardie, 2006) or editori-
als (Paul et al., 2010). Mostly, there is no query information, and summaries are formulated in a struc-
tured way based on product features or contrastive standpoints. Our work is more related to opinion
summarization on user-generated content, such as community QA. Liu et al. (2008) manually construct
taxonomies for questions in community QA. Summaries are generated by clustering sentences according
to their polarity based on a small dictionary. Tomasoni and Huang (2010) introduce coverage and quality
constraints on the sentences, and utilize an integer linear programming framework to select sentences.
3 Submodular Opinion Summarization
In this section, we describe how query-focused opinion summarization can be addressed by submodular
functions combined with dispersion functions. We first define our problem. Then we introduce the
1661
Basic Features Sentiment Features
- answer position in all answers/sentence position in blog - number/portion of sentiment words from a lexicon (Section 3.2)
- length of the answer/sentence - if contains sentiment words with the same polarity as
- length is less than 5 words sentiment words in query
Query-Sentence Overlap Features Query-Independent Features
- unigram/bigram TF/TFIDF similarity with query - unigram/bigram TFIDF similarity with cluster centroid
- number of key phrases in the query that appear in the - sumBasic score (Nenkova and Vanderwende, 2005)
sentence. A model similar to that described in - number of topic signature words (Lin and Hovy, 2000)
(Luo et al., 2013) was applied to detect key phrases. - JS divergence with cluster
Table 1: Features used for candidate ranking. We use them for ranking answers in both community QA
and blogs.
components of our objective function (Sections 3.1?3.3). The full objective function is presented in
Section 3.4. Lastly, we describe a greedy algorithm with constant factor approximation to the optimal
solution for generating summaries (Section 3.5).
A set of documents or answers to be summarized are first split into a set of individual sentences
V = {s
1
, ? ? ? , s
n
}. Our problem is to select a subset S ? V that maximizes a given objective function
f : 2
V
? R within a length constraint: S
?
= argmax
S?V
f(S), subject to | S |? c. | S | is the length of
the summary S, and c is the length limit.
Definition 1 A function f : 2
V
? R is submodular iff for all s ? V and every S ? S
?
? V , it satisfies
f(S ? {s})? f(S) ? f(S
?
? {s})? f(S
?
).
Previous submodularity-based summarization work assumes this diminishing return property makes
submodular functions a natural fit for summarization and achieves state-of-the-art results on various
datasets. In this paper, we follow the same assumption and work with non-decreasing submodular func-
tions. Nevertheless, they have limitations, one of which is that functions well suited to modeling diversity
are not submodular. Recently, Dasgupta et al. (2013) proved that diversity can nonetheless be encoded
in well-designed dispersion functions which still maintain a constant factor approximation when solved
by a greedy algorithm.
Based on these considerations, we propose an objective function f(S) mainly considering three as-
pects: relevance (Section 3.1), coverage (Section 3.2), and non-redundancy (Section 3.3). Relevance
and coverage are encoded in a non-decreasing submodular function, and non-redundancy is enforced by
maximizing the dispersion function.
3.1 Relevance Function
We first utilize statistical rankers to produce a preference ordering of the candidate answers or sentences.
We choose ListNet (Cao et al., 2007), which has been shown to be effective in many information retrieval
tasks, as our ranker. We use the implementation from Ranklib (Dang, 2011).
Features used in the ranking algorithm are summarized in Table 1. All features are normalized by
standardization. Due to the length limit, we cannot provide the full results on feature evaluation. Never-
theless, we find that ranking candidates by TFIDF similarity or key phrases overlapping with the query
can produce comparable results with using the full feature set (see Section 5).
We take the ranks output by the ranker, and define the relevance of the current summary S as: r(S) =
?
|S|
i
?
rank
?1
i
, where rank
i
is the rank of sentence s
i
in V . For QA answer ranking, sentences from the
same answer have the same ranking. The function r(S) is our first submodular function.
3.2 Coverage Functions
Topic Coverage. This function is designed to capture the idea that a comprehensive opinion sum-
mary should provide thoughts on distinct aspects. Topic models such as Latent Dirichlet Allocation
(LDA) (Blei et al., 2003) and its variants are able to discover hidden topics or aspects of document col-
lections, and thus afford a natural way to cluster texts according to their topics. Recent work (Xie and
Xing, 2013) shows the effectiveness of utilizing topic models for newsgroup document clustering. We
first learn an LDA model from the data, and treat each topic as a cluster. We estimate a sentence-topic
distribution
~
? for each sentence, and assign the sentence to the cluster k corresponding to the mode of the
distribution (i.e., k = argmax
i
?
i
). This naive approach produces comparable clustering performance to
the state-of-the-art according to (Xie and Xing, 2013). T is defined as the clustering induced by our algo-
rithm on the set V . The topic coverage of the current summary S is defined as t(S) =
?
T?T
?
|S ? T |.
1662
From the concavity of the square root it follows that sets S with uniform coverages of topics are preferred
to sets with skewed coverage.
Authorship Coverage. This term encourages the summarization algorithm to select sentences from
different authors. Let A be the clustering induced by the sentence to author relation. In community
QA, sentences from the answers given by the same user belong to the same cluster. Similarly, sentences
from blogs with the same author are in the same cluster. The authorship score is defined as a(S) =
?
A?A
?
|S ?A|.
Polarity Coverage. The polarity score encourages the selection of summaries that cover both positive
and negative opinions. We categorize each sentence simply by counting the number of polarized words
given by our lexicon. A sentence belongs to a positive cluster if it has more positive words than negative
ones, and vice versa. If any negator co-occurs with a sentiment word (e.g. within a window of size 5),
the sentiment is reversed.
2
The polarity clustering P thus have two clusters corresponding to positive
and negative opinions. The score is defined as p(S) =
?
P?P
?
| S ? P |. Our lexicon consists of
MPQA lexicon (Wilson et al., 2005), General Inquirer (Stone et al., 1966), and SentiWordNet (Esuli and
Sebastiani, 2006). Words with conflicting sentiments from different lexicons are removed.
Content Coverage. Similarly to Lin and Bilmes (2011) and Dasgupta et al. (2013), we use the following
function to measure content coverage of the current summary S: c(S) =
?
v?V
min(cov(v, S), ? ?
cov(v, V )), where cov(v, S) =
?
u?S
sim(v, u). We experiment with two types of similarity functions.
One is a Cosine TFIDF similarity score. The other is a WordNet-based semantic similarity score between
pairwise dependency relations from two sentences (Dasgupta et al., 2013). Specifically, sim
Sem
(v, u) =
?
rel
i
?v,rel
j
?u
WN(a
i
, a
j
) ?WN(b
i
, b
j
), where rel
i
= (a
i
, b
i
), rel
j
= (a
j
, b
j
), WN(w
i
, w
j
) is the
shortest path length. All scores are scaled onto [0, 1].
3.3 Dispersion Function
Summaries should contain as little redundant information as possible. We achieve this by adding an
additional term to the objective function, encoded by a dispersion function. Given a set of sentences
S, a complete graph is constructed with each sentence in S as a node. The weight of each edge (u, v)
is their dissimilarity d
?
(u, v). Then the distance between any pair of u and v, d(u, v), is defined as the
total weight of the shortest path connecting u and v.
3
We experiment with two forms of dispersion
function (Dasgupta et al., 2013): (1) h
sum
=
?
u,v?V,u6=v
d(u, v), and (2) h
min
= min
u,v?V,u6=v
d(u, v).
Then we need to define the dissimilarity function d
?
(?, ?). There are different ways to measure the
dissimilarity between sentences (Mihalcea et al., 2006; Agirre et al., 2012). In this work, we experiment
with three types of dissimilarity functions.
Lexical Dissimilarity. This function is based on the well-known Cosine similarity score using TFIDF
weights. Let sim
tfidf
(u, v) be the Cosine similarity between u and v, then we have d
?
Lex
(u, v) =
1? sim
tfidf
(u, v).
Semantic Dissimilarity. This function is based on the semantic meaning embedded in the dependency
relations. d
?
Sem
(u, v) = 1 ? sim
Sem
(v, u), where sim
Sem
(v, u) is the semantic similarity used in
content coverage measurement in Section 3.2.
Topical Dissimilarity. We propose a novel dissimilarity measure based on topic models. Celikyilmaz
et al. (2010) show that estimating the similarity between query and passages by using topic structures
can help improve the retrieval performance. As discussed in the topic coverage in Section 3.2, each
sentence is represented by its sentence-topic distributions estimated by LDA. For candidate sentence u
and v, let their topic distributions be P
u
and P
v
. Then the dissimilarity between u and v can be defined
as: d
?
Topic
(u, v) = JSD(P
u
||P
v
) =
1
2
(
?
i
P
u
(i) log
2
P
u
(i)
P
a
(i)
+
?
i
P
v
(i) log
2
P
v
(i)
P
a
(i)
) where P
a
(i) =
1
2
(P
u
(i) + P
v
(i)).
3.4 Full Objective Function
The objective function takes the interpolation of the submodular functions and dispersion function:
F(S) = r(S) + ?t(S) + ?a(S) + ?p(S) + ?c(S) + ?h(S). (1)
2
There exists a large amount of work on determining the polarity of a sentence (Pang and Lee, 2008) which can be employed
for polarity clustering in this work. We decide to focus on summarization, and estimate sentence polarity through sentiment
word summation (Yu and Hatzivassiloglou, 2003), though we do not distinguish different sentiment words.
3
This definition of distance is used to produce theoretical guarantees for the greedy algorithm described in Section 3.5.
1663
The coefficients ?, ?, ?, ?, ? are non-negative real numbers and can be tuned on a development set.
4
Notice that each summand except h(S) is a non-decreasing, non-negative, and submodular function,
and summation preserves monotonicity, non-negativity, and submodularity. Dispersion function h(s) is
either h
sum
or h
min
as introduced previously.
3.5 Summary Generation via Greedy Algorithm
Generating the summary that maximizes our objective function in Equation 1 is NP-hard (Chandra and
Halld?orsson, 1996). We choose to use a greedy algorithm that guarantees to obtain a constant factor ap-
proximation to the optimal solution (Nemhauser et al., 1978; Dasgupta et al., 2013). Concretely, starting
with an empty set, for each iteration, we add a new sentence so that the current summary achieves the
maximum value of the objective function. In addition to the theoretical guarantee, existing work (Mc-
Donald, 2007) has empirically shown that classical greedy algorithms usually works near-optimally.
4 Experimental Setup
4.1 Opinion Question Identification
We first build a classifier to automatically detect opinion oriented questions in Community QA; questions
in the blog dataset are all opinionated. Our opinion question classifier is trained on two opinion question
datasets: (1) the first, from Li et al. (2008a), contains 646 opinionated and 332 objective questions; (2)
the second dataset, from Amiri et al. (2013), consists of 317 implicit opinion questions, such as ?What
can you do to help environment??, and 317 objective questions. We train a RBF kernel based SVM
classifier to identify opinion questions, which achieves F1 scores of 0.79 and 0.80 on the two datasets
when evaluated using 10-fold cross-validation (the best F1 scores reported are 0.75 and 0.79).
4.2 Datasets
Community QA Summarization: Yahoo! Answers. We use the Yahoo! Answers dataset from Yahoo!
Webscope
TM
program,
5
which contains 3,895,407 questions. We first run the opinion question classifier
to identify the opinion questions. For summarization purpose, we require each question having at least 5
answers, with the average length of answers larger than 20 words. This results in 130,609 questions.
To make a compelling task, we reserve questions with an average length of answers larger than 50
words as our test set for both ranking and summarization; all the other questions are used for training. As
a result, we have 92,109 questions in the training set for learning the statistical ranker, and 38,500 in the
test set. The category distribution of training and test questions (Yahoo! Answers organizes the questions
into predefined categories) are similar. 10,000 questions from the training set are further reserved as the
development set. Each question in the Yahoo! Answers dataset has a user-voted best answer. These best
answers are used to train the statistical ranker that predicts relevance. Separate topic models are learned
for each category, where the category tag is provided by Yahoo! Answer.
Blog Summarization: TAC 2008. We use the TAC 2008 corpus (Dang, 2008), which consists of 25
topics. 23 of them are provided with human labeled nuggets, which TAC used in human evaluation. TAC
also provides snippets (i.e., sentences) that are frequently retrieved by participant systems or identified
as relevant by human annotators. We do not assume those snippets are known to any of our systems.
4.3 Comparisons
For both opinion summarization tasks, we compare with (1) the approach by Dasgupta et al. (2013), and
(2) the systems from Lin and Bilmes (2011) with and without query information. The sentence clustering
process in Lin and Bilmes (2011) is done by using CLUTO (Karypis, 2003). For the implementation of
systems in Lin and Bilmes (2011) and Dasgupta et al. (2013), we always use the parameters reported to
have the best performance in their work.
For cQA summarization, we use the best answer voted by the user as a baseline. Note that this is a
strong baseline since all the other systems are unaware of which answer is the best. For blog summa-
rization, we have three additional baselines ? the best systems in TAC 2008 (Kim et al., 2008; Li et al.,
2008b), top sentences returned by our ranker, a baseline produced by TFIDF similarity and a lexicon
4
The values for the coefficients are 5.0, 1.0, 10.0, 5.0, 10.0 for ?, ?, ?, ?, ?, respectively, as tuned on the development set.
5
http://sandbox.yahoo.com/
1664
(henceforth called TFIDF+Lexicon). In TFIDF+Lexicon, sentences are ranked by the TFIDF similar-
ity with the query, and then sentences with sentiment words are selected in sequence. This baseline aims
to show the performance when we only have access to lexicons without using a learning algorithm.
5 Results
5.1 Evaluating the Ranker
We evaluate our ranker (described in Section 3.1) on the task of best answer prediction. Table 2 compares
the average precision and mean reciprocal rank (MRR) of our method to those of three baselines, (1)
where answers are ranked randomly (Baseline (Random)), (2) by length (Baseline (Length)), and (3)
by Jensen Shannon Divergence (JSD) with all answers. We expect that the best answer is the one that
covers the most information, which is likely to have a smaller JSD. Therefore, we use JSD to rank
answers in the ascending order. Table 2 manifests that our ranker outperforms all the other methods.
Baseline (Random) Baseline (Length) JSD Ranker (ListNet)
Avg Precision 0.1305 0.2834 0.4000 0.5336
MRR 0.3403 0.4889 0.5909 0.6496
Table 2: Performance for best answer prediction. Our ranker outperforms the three baselines.
5.2 Community QA Summarization
Automatic Evaluation. Since human written abstracts are not available for the Yahoo! Answers dataset,
we adopt the Jensen-Shannon divergence (JSD) to measure the summary quality. Intuitively, a smaller
JSD implies that the summary covers more of the content in the answer set. Louis and Nenkova (2013)
report that JSD has a strong negative correlation (Spearman correlation = ?0.737) with the overall
summary quality for multi-document summarization (MDS) on news articles and blogs. Our task is
similar to MDS. Meanwhile, the average JSD of the best answers in our test set is smaller than that of
the other answers (0.39 vs. 0.49), with an average length of 103 words compared with 67 words for the
other answers. Also, on the blog task (Section 5.3), the top two systems by JSD also have the top two
ROUGE scores (a common metric for summarization evaluation when human-constructed summaries
are available). Thus, we conjecture that JSD is a good metric for community QA summaries.
Table 3 (left) shows that our system using a content coverage function based on Cosine using TFIDF
weights, and a dispersion function (h
sum
) based on lexicon dissimilarity and 100 topics, outperforms all
of the compared approaches (paired-t test, p < 0.05). The topic number is tuned on the development set,
and we find that varying the number of topics does not impact performance too much. Meanwhile, both
our system and Dasgupta et al. (2013) produce better JSD scores than the two variants of the Lin and
Bilmes (2011) system, which implies the effectiveness of the dispersion function. We further examine the
effectiveness of each component that contributes to the objective function (Section 3.4), and the results
are shown in Table 3 (right).
Length
100 200
Best answer 0.3858 -
Lin and Bilmes (2011) 0.3398 0.2008
Lin and Bilmes (2011) + q 0.3379 0.1988
Dasgupta et al. (2013) 0.3316 0.1939
Our system 0.3017 0.1758
JSD
100
JSD
200
Rel(evance) 0.3424 0.2053
Rel + Aut(hor) 0.3375 0.2040
Rel + Aut + TM (Topic Models) 0.3366 0.2033
Rel + Aut + TM + Pol(arity) 0.3309 0.1983
Rel + Aut + TM + Pol + Cont(ent Coverage) 0.3102 0.1851
Rel + Aut + TM + Pol + Cont + Disp(ersion) 0.3017 0.1758
Table 3: [Left] Summaries evaluated by Jensen-Shannon divergence (JSD) on Yahoo Answer for sum-
maries of 100 words and 200 words. The average length of the best answer is 102.70. [Right] Value
addition of each component in the objective function. The JSD on each line is statistically significantly
lower than the JSD on the previous (? = 0.05).
Human Evaluation. Human evaluation for Yahoo! Answers is carried out on Amazon Mechanical Turk
6
with carefully designed tasks (or ?HITs?). Turkers are presented summaries from different systems in a
random order, and asked to provide two rankings, one for overall quality and the other for information
diversity. We indicate that informativeness and non-redundancy are desirable for quality; however, Turk-
ers are allowed to consider other desiderata, such as coherence or responsiveness, and write down those
when they submit the answers. Here we believe that ranking the summaries is easier than evaluating each
summary in isolation (Lerman et al., 2009).
6
https://www.mturk.com/mturk/
1665
We randomly select 100 questions from our test set, each of which is evaluated by 4 distinct Turkers
located in United States. 40 HITs are thus created, each containing 10 different questions. Four system
summaries (best answer, Dasgupta et al. (2013), and our system with 100 and 200 words respectively) are
displayed along with one noisy summary (i.e. irrelevant to the question) per question in random order.
7
We reject Turkers? HITs if they rank the noisy summary higher than any other. Two duplicate questions
are added to test intra-annotator agreement. We reject HITs if Turkers produced inconsistent rankings
for both duplicate questions. A total of 137 submissions of which 40 HITs pass the above quality filters.
Turkers of all accepted submissions report themselves as native English speakers. An inter-rater agree-
ment of Fleiss? ? of 0.28 (fair agreement (Landis and Koch, 1977)) is computed for quality ranking and
? is 0.43 (moderate agreement) for diversity ranking. Table 4 shows the percentage of times a particular
method is picked as the best summary, and the macro-/micro-average rank of a method, for both overall
quality and information diversity. Macro-average is computed by first averaging the ranks per question
and then averaging across all questions.
For overall quality, our system with a 200 word limit is selected as the best in 44.6% of the evaluations.
It outperforms the best answer (31.9%) significantly, which suggests that our system summary covers rel-
evant information that is not contained in the best answer. Our system with a length constraint of 100
words is chosen as the best for quality 12.5% times while that of Dasgupta et al. (2013) is chosen 11.0%
of the time. Our system is also voted as the best summary for diversity in 78.7% of the evaluations. More
interestingly, both of our systems, with 100 words and 200 words, outperform the best answer and Das-
gupta et al. (2013) for average ranking (both overall quality and information diversity) significantly by
using Wilcoxon signed-rank test (p < 0.05). When we check the reasons given by Turkers, we found that
people usually prefer our summaries due to ?helpful suggestions that covered many options? or being
?balanced with different opinions?. When Turks prefer the best answers, they mostly stress on coherence
and responsiveness. Sample summaries from all the systems are displayed in Figure 2.
Length of Summary Overall Quality Information Diversity
% Average Rank % Average Rank
Best Macro Micro Best Macro Micro
Best answer 102.70 31.9% 2.68 2.69 9.6% 3.27 3.29
Dasgupta et al. (2013)
100
11.0% 2.84 2.83 5.0% 2.95 2.94
Our system 12.5% 2.50
?
2.50
?
6.7% 2.43
?
2.43
?
Our system 200 44.6% 1.98
?
1.98
?
78.7% 1.35
?
1.34
?
Table 4: Human evaluation on Yahoo! Answer Data. Boldface implies statistically significance com-
pared to other results in the same columns using paired-t test. Both of our systems are ranked higher
(i.e. numbers in bold with
?
) than the best answers voted by Yahoo! users and system summaries from
Dasgupta et al. (2013).
Question: What is the long term effect of piracy on the music and film industry?
Dasgupta et al. (2013) (Qty Rank=2.75 Div. Rank=2.5):
?In short, I don?t think the music industry in particular will ever enjoy the huge profits of the 90?s.
?Please-People in those businesses make millions of dollars as it is !! I don?t think piracy hurts them at all !!!
?The other thing will be music and movies with less quality.
?Its a big gray area, I dont see anything wrong with burning a mix cd or a cd for a friend so long as youre not selling them for profit.
?By removing the profitability of music & film companies, piracy takes away their motivation to produce new music & movies.
Our system (100 words) (Qty Rank=2.25 Div. Rank=2.25):
?Rising costs for movies and music. The other thing will be music and movies with less quality.
?Now, with piracy, there isn?t the willingness to take chances.
?But it?s also like the person put the effort into it and they aren?t getting paid. It?s a big gray area, I don?t see anything wrong with burning a mix cd
or a cd for a friend so long as you?re not selling them for profit.
?It is forcing them to rework their business model, which is a good thing.
Our system (200 words) (Qty. Rank=2.25, Div Rank=1.25):
?Rising costs for movies and music. The other thing will be music and movies with less quality.
?Now, with piracy, there isn?t the willingness to take chances. American Idol is the result of this. .... The real problem here is that the mainstream
music will become even tighter. Record labels will not won?t to go far from what is currently like by the majority.
?I hate when people who have billions of dollars whine about not having more money. But it?s also like the person put the effort into it and they
aren?t getting paid ... I don?t see anything wrong with burning a mix cd or a cd for a friend ....
?It is forcing them to rework their business model, which is a good thing.
?By removing the profitability of music & film companies, piracy takes away their motivation to produce new music & movies.
Figure 2: Sample summaries from Dasgupta et al. (2013), and our systems (100 words and 200 words).
Sentences from separate bullets (?) are partial answers from different users.
7
Note that we aim to compare results with the gold-standard best answers of about 100 words. The evaluation of the
200-word summaries is provided only as an additional data-point.
1666
5.3 Blog Summarization
Automatic Evaluation. We use the ROUGE (Lin and Hovy, 2003) software with standard options to
automatically evaluate summaries with reference to the human labeled nuggets as those are available
for this task. ROUGE-2 measures bigram overlap and ROUGE-SU4 measures the overlap of unigram
and skip-bigram separated by up to four words. We use the ranker trained on Yahoo! data to produce
relevance ordering, and adopt the system parameters from Section 5.2. Table 5 (left) shows that our
system outperforms the best system in TAC?08 with highest ROUGE-2 score (Kim et al., 2008), the two
baselines (TFIDF+Lexicon, and our ranker), Lin and Bilmes (2011), and Dasgupta et al. (2013).
ROUGE-2 ROUGE-SU4 JSD
Best system in TAC?08 0.2923 0.3766 0.3286
TFIDF + Lexicon 0.3069 0.3876 0.2429
Ranker (ListNet) 0.3200 0.3960 0.2293
Lin and Bilmes (2011) 0.2732 0.3582 0.2330
Lin and Bilmes (2011) + q 0.2852 0.3700 0.2349
Dasgupta et al. (2013) 0.2618 0.3500 0.2370
Our system 0.3234 0.3978 0.2258
Pyramid F-score
Best system in TAC?08 0.2225
Lin and Bilmes (2011) 0.2790
Our system 0.3620
Table 5: Results on TAC?08 dataset. [Left] Our system has significant better ROUGE scores than all
the other systems except our ranker (paired-t test, p < 0.05). We also achieve the best JS divergence.
[Right] Human evaluation with Pyramid F-score. Our system significantly outperforms the others.
Human Evaluation. For human evaluation, we use the standard Pyramid F-score used in the TAC?08
opinion summarization track with ? = 3 (Dang, 2008). In the TAC task, systems are allowed to return up
to 7,000 non-white characters for each question. Since the TAC metric favors recall we do not produce
summaries shorter than 7,000 characters. We ask two human judges to evaluate our system along with
the one that got the highest Pyramid F-score in the TAC?08 and Lin and Bilmes (2011). Cohen?s ? for
inter-annotator agreement is 0.68 (substantial). While we did not explicitly evaluate non-redundancy,
both of our judges report that our system summaries contain less redundant information.
5.4 Further Discussion
Yahoo! Answer
DISPERSION
sum
DISPERSION
min
DISSIMI Cont
tfidf
Cont
sem
Cont
tfidf
Cont
sem
Semantic 0.3143 0.324 3 0.3129 0.3232
Topical 0.3101 0.3202 0.3106 0.3209
Lexical 0.3017 0.3147 0.3071 0.3172
TAC 2008
DISPERSION
sum
DISPERSION
min
DISSIMI Cont
tfidf
Cont
sem
Cont
tfidf
Cont
sem
Semantic 0.2216 0.2169 0.2772 0.2579
Topical 0.2128 0.2090 0.3234 0.3056
Lexical 0.2167 0.2129 0.3117 0.3160
Table 6: Effect of different dispersion functions, content coverage, and dissimilarity metrics on our
system. [Left] JSD values for different combinations on Yahoo! data, using LDA with 100 topics.
All systems are significantly different from each other at significance level ? = 0.05. Systems using
summation of distances for dispersion function (h
sum
) uniformly outperform the ones using minimum
distance (h
min
). [Right] ROUGE scores of different choices for TAC 2008 data. All systems use LDA
with 40 topics. The parameters of our systems are adopted from the ones tuned on Yahoo! Answers.
Given that the text similarity metrics and dispersion functions play important roles in the framework,
we further study the effectiveness of different content coverage functions (Cosine using TFIDF vs. Se-
mantic), dispersion functions (h
sum
vs. h
min
), and dissimilarity metrics used in dispersion functions
(Semantic vs. Topical vs. Lexical). Results on Yahoo! Answer (Table 6 (left)) show that systems using
summation of distances for dispersion functions (h
sum
) uniformly outperform the ones using minimum
distance (h
min
). Meanwhile, Cosine using TFIDF is better at measuring content coverage than WordNet-
based semantic measurement, and this may due to the limited coverage of WordNet on verbs. This is also
true for dissimilarity metrics. Results on blog data (Table 6 (right)), however, show that using minimum
distance for dispersion produces better results. This indicates that optimal dispersion function varies by
genre. Topical-based dissimilarity also marginally outperforms the other two metrics in blog data.
6 Conclusion
We propose a submodular function-based opinion summarization framework. Tested on community QA
and blog summarization, our approach outperforms state-of-the-art methods that are also based on sub-
modularity in both automatic evaluation and human evaluation. Our framework is capable of including
statistically learned sentence relevance and encouraging the summary to cover diverse topics. We also
study different metrics on text similarity estimation and their effect on summarization.
1667
References
Eneko Agirre, Daniel Cer, Mona Diab, and Aitor Gonzalez-Agirre. 2012. Semeval-2012 task 6: A pilot on seman-
tic textual similarity. In Volume 2: Proceedings of the Sixth International Workshop on Semantic Evaluation
(SemEval 2012), pages 385?393, Montr?eal, Canada, 7-8 June. Association for Computational Linguistics.
Hadi Amiri, Zheng-Jun Zha, and Tat-Seng Chua. 2013. A pattern matching based model for implicit opinion
question identification. In AAAI. AAAI Press.
David M. Blei, Andrew Y. Ng, and Michael I. Jordan. 2003. Latent dirichlet allocation. J. Mach. Learn. Res.,
3:993?1022, March.
Zhe Cao, Tao Qin, Tie-Yan Liu, Ming-Feng Tsai, and Hang Li. 2007. Learning to rank: From pairwise approach
to listwise approach. In Proceedings of the 24th International Conference on Machine Learning, ICML ?07,
pages 129?136, New York, NY, USA. ACM.
Jaime Carbonell and Jade Goldstein. 1998. The use of mmr, diversity-based reranking for reordering documents
and producing summaries. In Proceedings of the 21st Annual International ACM SIGIR Conference on Research
and Development in Information Retrieval, SIGIR ?98, pages 335?336, New York, NY, USA. ACM.
Asli Celikyilmaz, Dilek Hakkani-Tur, and Gokhan Tur. 2010. Lda based similarity modeling for question answer-
ing. In Proceedings of the NAACL HLT 2010 Workshop on Semantic Search, SS ?10, pages 1?9, Stroudsburg,
PA, USA. Association for Computational Linguistics.
Barun Chandra and Magn?us M. Halld?orsson. 1996. Facility dispersion and remote subgraphs. In Proceedings
of the 5th Scandinavian Workshop on Algorithm Theory, SWAT ?96, pages 53?65, London, UK, UK. Springer-
Verlag.
Hoa Tran Dang. 2008. Overview of the tac 2008 opinion question answering and summarization tasks. In Proc.
TAC 2008.
Van Dang. 2011. RankLib. http://www.cs.umass.edu/?vdang/ranklib.html.
Anirban Dasgupta, Ravi Kumar, and Sujith Ravi. 2013. Summarization through submodularity and dispersion.
In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long
Papers), pages 1014?1022, Sofia, Bulgaria, August. Association for Computational Linguistics.
Hal Daum?e, III and Daniel Marcu. 2006. Bayesian query-focused summarization. In Proceedings of the 21st
International Conference on Computational Linguistics and the 44th Annual Meeting of the Association for
Computational Linguistics, ACL-44, pages 305?312, Stroudsburg, PA, USA. Association for Computational
Linguistics.
Andrea Esuli and Fabrizio Sebastiani. 2006. Sentiwordnet: A publicly available lexical resource for opinion
mining. In In Proceedings of the 5th Conference on Language Resources and Evaluation (LREC06, pages
417?422.
Minqing Hu and Bing Liu. 2004. Mining and summarizing customer reviews. In Proceedings of the Tenth ACM
SIGKDD International Conference on Knowledge Discovery and Data Mining, KDD ?04, pages 168?177, New
York, NY, USA. ACM.
George Karypis. 2003. CLUTO - a clustering toolkit. Technical Report #02-017, November.
Hyun Duk Kim, Dae Hoon Park, V.G.Vinod Vydiswaran, and ChengXiang Zhai. 2008. Opinion summarization
using entity features and probabilistic sentence coherence optimization: Uiuc at tac 2008 opinion summarization
pilot. In Proc. TAC 2008.
J R Landis and G G Koch. 1977. The measurement of observer agreement for categorical data. Biometrics,
33(1):159?174.
Kevin Lerman, Sasha Blair-Goldensohn, and Ryan McDonald. 2009. Sentiment summarization: Evaluating and
learning user preferences. In Proceedings of the 12th Conference of the European Chapter of the Association for
Computational Linguistics, EACL ?09, pages 514?522, Stroudsburg, PA, USA. Association for Computational
Linguistics.
Baoli Li, Yandong Liu, and Eugene Agichtein. 2008a. Cocqa: Co-training over questions and answers with an
application to predicting question subjectivity orientation. In EMNLP, pages 937?946.
Wenjie Li, You Ouyang, Yi Hu, and Furu Wei. 2008b. Polyu at tac 2008. In Proc. TAC 2008.
1668
Hui Lin and Jeff Bilmes. 2011. A class of submodular functions for document summarization. In Proceedings
of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies -
Volume 1, HLT ?11, pages 510?520, Stroudsburg, PA, USA. Association for Computational Linguistics.
Chin-Yew Lin and Eduard Hovy. 2000. The automated acquisition of topic signatures for text summarization.
COLING ?00, pages 495?501, Stroudsburg, PA, USA. Association for Computational Linguistics.
Chin-Yew Lin and Eduard Hovy. 2003. Automatic evaluation of summaries using n-gram co-occurrence statistics.
In Proceedings of the 2003 Conference of the North American Chapter of the Association for Computational
Linguistics on Human Language Technology - Volume 1, pages 71?78.
Yuanjie Liu, Shasha Li, Yunbo Cao, Chin-Yew Lin, Dingyi Han, and Yong Yu. 2008. Understanding and sum-
marizing answers in community-based question answering services. In Proceedings of the 22Nd International
Conference on Computational Linguistics - Volume 1, COLING ?08, pages 497?504, Stroudsburg, PA, USA.
Association for Computational Linguistics.
Annie Louis and Ani Nenkova. 2013. Automatically assessing machine summary content without a gold standard.
Comput. Linguist., 39(2):267?300, June.
Xiaoqiang Luo, Hema Raghavan, Vittorio Castelli, Sameer Maskey, and Radu Florian. 2013. Finding what matters
in questions. In HLT-NAACL, pages 878?887.
Ryan McDonald. 2007. A study of global inference algorithms in multi-document summarization. ECIR?07,
pages 557?564, Berlin, Heidelberg. Springer-Verlag.
Rada Mihalcea, Courtney Corley, and Carlo Strapparava. 2006. Corpus-based and knowledge-based measures of
text semantic similarity. In Proceedings of the 21st National Conference on Artificial Intelligence - Volume 1,
AAAI?06, pages 775?780. AAAI Press.
G. L. Nemhauser, L. A. Wolsey, and M. L. Fisher. 1978. An analysis of approximations for maximizing submod-
ular set functionsI. Mathematical Programming, 14(1):265?294, December.
Ani Nenkova and Lucy Vanderwende. 2005. The impact of frequency on summarization. Microsoft Research,
Redmond, Washington, Tech. Rep. MSR-TR-2005-101.
Bo Pang and Lillian Lee. 2008. Opinion mining and sentiment analysis. Found. Trends Inf. Retr., 2(1-2):1?135,
January.
Michael J. Paul, ChengXiang Zhai, and Roxana Girju. 2010. Summarizing contrastive viewpoints in opinionated
text. In Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, EMNLP
?10, pages 66?76, Stroudsburg, PA, USA. Association for Computational Linguistics.
Ruben Sipos, Pannaga Shivaswamy, and Thorsten Joachims. 2012. Large-margin learning of submodular summa-
rization models. EACL ?12, pages 224?233, Stroudsburg, PA, USA. Association for Computational Linguistics.
Philip J. Stone, Dexter C. Dunphy, Marshall S. Smith, and Daniel M. Ogilvie. 1966. The General Inquirer: A
Computer Approach to Content Analysis. MIT Press, Cambridge, MA.
Veselin Stoyanov and Claire Cardie. 2006. Partially supervised coreference resolution for opinion summarization
through structured rule learning. In Proceedings of the 2006 Conference on Empirical Methods in Natural
Language Processing, EMNLP ?06, pages 336?344, Stroudsburg, PA, USA. Association for Computational
Linguistics.
Mattia Tomasoni and Minlie Huang. 2010. Metadata-aware measures for answer summarization in community
question answering. In Proceedings of the 48th Annual Meeting of the Association for Computational Linguis-
tics, ACL ?10, pages 760?769, Stroudsburg, PA, USA. Association for Computational Linguistics.
Theresa Wilson, Janyce Wiebe, and Paul Hoffmann. 2005. Recognizing contextual polarity in phrase-level senti-
ment analysis. In Proceedings of the Conference on Human Language Technology and Empirical Methods in
Natural Language Processing, HLT ?05, pages 347?354, Stroudsburg, PA, USA. Association for Computational
Linguistics.
Pengtao Xie and Eric Xing. 2013. Integrating document clustering and topic modeling. In Proceedings of the
Twenty-Ninth Conference Annual Conference on Uncertainty in Artificial Intelligence (UAI-13), pages 694?
703, Corvallis, Oregon. AUAI Press.
Hong Yu and Vasileios Hatzivassiloglou. 2003. Towards answering opinion questions: Separating facts from
opinions and identifying the polarity of opinion sentences. In Proceedings of the Conference on Empirical
Methods in Natural Language Processing (EMNLP).
1669
Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 1046?1056,
MIT, Massachusetts, USA, 9-11 October 2010. c?2010 Association for Computational Linguistics
Multi-level Structured Models for Document-level Sentiment Classification
Ainur Yessenalina
Dept. of Computer Science
Cornell University
Ithaca, NY, USA
ainur@cs.cornell.edu
Yisong Yue
Dept. of Computer Science
Cornell University
Ithaca, NY, USA
yyue@cs.cornell.edu
Claire Cardie
Dept. of Computer Science
Cornell University
Ithaca, NY, USA
cardie@cs.cornell.edu
Abstract
In this paper, we investigate structured mod-
els for document-level sentiment classifica-
tion. When predicting the sentiment of a sub-
jective document (e.g., as positive or nega-
tive), it is well known that not all sentences
are equally discriminative or informative. But
identifying the useful sentences automatically
is itself a difficult learning problem. This pa-
per proposes a joint two-level approach for
document-level sentiment classification that
simultaneously extracts useful (i.e., subjec-
tive) sentences and predicts document-level
sentiment based on the extracted sentences.
Unlike previous joint learning methods for
the task, our approach (1) does not rely on
gold standard sentence-level subjectivity an-
notations (which may be expensive to obtain),
and (2) optimizes directly for document-level
performance. Empirical evaluations on movie
reviews and U.S. Congressional floor debates
show improved performance over previous ap-
proaches.
1 Introduction
Sentiment classification is a well-studied and active
research area (Pang and Lee, 2008). One of the main
challenges for document-level sentiment categoriza-
tion is that not every part of the document is equally
informative for inferring the sentiment of the whole
document. Objective statements interleaved with the
subjective statements can be confusing for learning
methods, and subjective statements with conflicting
sentiment further complicate the document catego-
rization task. For example, authors of movie reviews
often devote large sections to (largely objective) de-
scriptions of the plot (Pang and Lee, 2004). In ad-
dition, an overall positive review might still include
some negative opinions about an actor or the plot.
Early research on document-level sentiment clas-
sification employed conventional machine learning
techniques for text categorization (Pang et al, 2002).
These methods, however, assume that documents are
represented via a flat feature vector (e.g., a bag-of-
words). As a result, their ability to identify and ex-
ploit subjectivity (or other useful) information at the
sentence-level is limited.
And although researchers subsequently proposed
methods for incorporating sentence-level subjectiv-
ity information, existing techniques have some un-
desirable properties. First, they typically require
gold standard sentence-level annotations (McDon-
ald et al (2007), Mao and Lebanon (2006)). But
the cost of acquiring such labels can be prohibitive.
Second, some solutions for incorporating sentence-
level information lack mechanisms for controlling
how errors propagate from the subjective sentence
identification subtask to the main document classifi-
cation task (Pang and Lee, 2004). Finally, solutions
that attempt to handle the error propagation problem
have done so by explicitly optimizing for the best
combination of document- and sentence-level clas-
sification accuracy (McDonald et al, 2007). Opti-
mizing for this compromise, when the real goal is
to maximize only the document-level accuracy, can
potentially hurt document-level performance.
In this paper, we propose a joint two-level model
to address the aforementioned concerns. We formu-
late our training objective to directly optimize for
1046
document-level accuracy. Further, we do not require
gold standard sentence-level labels for training. In-
stead, our training method treats sentence-level la-
bels as hidden variables and jointly learns to predict
the document label and those (subjective) sentences
that best ?explain? it, thus controlling the propaga-
tion of incorrect sentence labels. And by directly
optimizing for document-level accuracy, our model
learns to solve the sentence extraction subtask only
to the extent required for accurately classifying doc-
ument sentiment. A software implementation of our
method is also publicly available.1
For the rest of the paper, we will discuss re-
lated work, motivate and describe our model, present
an empirical evaluation on movie reviews and U.S.
Congressional floor debates datasets and close with
discussion and conclusions.
2 Related Work
Pang and Lee (2004) first showed that sentence-
level extraction can improve document-level per-
formance. They used a cascaded approach by
first filtering out objective sentences and perform-
ing subjectivity extractions using a global min-cut
inference. Afterward, the subjective extracts were
converted into inputs for the document-level senti-
ment classifier. One advantage of their approach
is that it avoids the need for explicit subjectiv-
ity annotations. However, like other cascaded ap-
proaches (e.g., Thomas et al (2006), Mao and
Lebanon (2006)), it can be difficult to control how
errors propagate from the sentence-level subtask to
the main document classification task.
Instead of taking a cascaded approach, one can
directly modify the training of flat document clas-
sifiers using lower level information. For instance,
Zaidan et al (2007) used human annotators to mark
the ?annotator rationales?, which are text spans that
support the document?s sentiment label. These an-
notator rationales are then used to formulate addi-
tional constraints during SVM training to ensure that
the resulting document classifier is less confident in
classifying a document that does not contain the ra-
tionale versus the original document. Yessenalina et
al. (2010) extended this approach to use automati-
cally generated rationales.
1http://projects.yisongyue.com/svmsle/
A natural approach to avoid the pitfalls associ-
ated with cascaded methods is to use joint two-
level models that simultaneously solve the sentence-
level and document-level tasks (e.g., McDonald et
al. (2007), Zaidan and Eisner (2008)). Since these
models are trained jointly, the sentence-level pre-
dictions affect the document-level predictions and
vice-versa. However, such approaches typically
require sentence-level annotations during training,
which can be expensive to acquire. Furthermore,
the training objectives are usually formulated as a
compromise between sentence-level and document-
level performance. If the goal is to predict well at the
document-level, then these approaches are solving a
much harder problem that is not exactly aligned with
maximizing document-level accuracy.
Recently, researchers within both Natural Lan-
guage Processing (e.g., Petrov and Klein (2007),
Chang et al (2010), Clarke et al (2010)) and
other fields (e.g., Felzenszwalb et al (2008), Yu
and Joachims (2009)) have analyzed joint multi-
level models (i.e., models that simultaneously solve
the main prediction task along with important sub-
tasks) that are trained using limited or no explicit
lower level annotations. Similar to our approach, the
lower level labels are treated as hidden or latent vari-
ables during training. Although the training process
is non-trivial (and in particular requires a good ini-
tialization of the hidden variables), it avoids the need
for human annotations for the lower level subtasks.
Some researchers have also recently applied hidden
variable models to sentiment analysis, but they were
focused on classifying either phrase-level (Choi and
Cardie, 2008) or sentence-level polarity (Nakagawa
et al, 2010).
3 Extracting Hidden Explanations
In this paper, we take the view that each document
has a subset of sentences that best explains its sen-
timent. Consider the ?annotator rationales? gener-
ated by human judges for the movie reviews dataset
(Zaidan et al, 2007). Each rationale is a text span
that was identified to support (or explain) its parent
document?s sentiment. Thus, these rationales can be
interpreted as (something close to) a ground truth la-
beling of the explanatory segments. Using a dataset
where each document contains only its rationales,
1047
Algorithm 1 Inference Algorithm for (2)
1: Input: x
2: Output: (y, s)
3: s+ ? argmaxs?S(x) ~wT?(x,+1, s)
4: s? ? argmaxs?S(x) ~wT?(x,?1, s)
5: if ~wT?(x,+1, s+) > ~wT?(x,?1, s?) then
6: Return (+1, s+)
7: else
8: Return (?1, s?)
9: end if
cross validation experiments using an SVM classi-
fier yields 97.44% accuracy ? as opposed to 86.33%
accuracy when using the full text of the original doc-
uments. Clearly, extracting the best supporting seg-
ments can offer a tremendous performance boost.
We are interested in settings where human-
extracted explanations such as annotator rationales
might not be readily available, or are imperfect. As
such, we will formulate the set of extracted sen-
tences as latent or hidden variables in our model.
Viewing the extracted sentences as latent variables
will pose no new challenges during prediction, since
the model is expected to predict all labels at test
time. We will leverage recent advances in training
latent variable SVMs (Yu and Joachims, 2009) to ar-
rive at an effective training procedure.
4 Model
In this section, we present a two-level document
classification model. Although our model makes
predictions at both the document and sentence lev-
els, it will be trained (and evaluated) only with re-
spect to document-level performance. We begin
by presenting the feature structure and inference
method. We will then describe a supervised train-
ing algorithm based on structural SVMs, and finally
discuss some extensions and design decisions.
Let x denote a document, y = ?1 denote the sen-
timent (for us, a binary positive or negative polarity)
of a document, and s denote a subset of explanatory
sentences in x. Let ?(x, y, s) denote a joint fea-
ture map that outputs features describing the qual-
ity of predicting sentiment y using explanation s for
document x. We focus on linear models, so given a
(learned) weight vector ~w, we can write the quality
of predicting y (with explanation s) as
F (x, y, s; ~w) = ~wT?(x, y, s), (1)
and a document-level sentiment classifier as
h(x; ~w) = argmax
y=?1
max
s?S(x)
F (x, y, s; ~w), (2)
where S(x) denotes the collection of feasible expla-
nations (e.g., subsets of sentences) for x.
Let xj denote the j-th sentence of x. We propose
the following instantiation of (1),
~wT?(x, y, s) =
1
N(x)
?
j?s
y ? ~wTpol?pol(x
j) + ~wTsubj?subj(x
j), (3)
where the first term in the summation captures the
quality of predicting polarity y on sentences in s,
the second term captures the quality of predicting s
as the subjective sentences, and N(x) is a normaliz-
ing factor (which will be discussed in more detail in
Section 4.3). We represent the weight vector as
~w =
[
~wpol
~wsubj
]
, (4)
and ?pol(xj) and ?subj(xj) denote the polarity and
subjectivity features of sentence xj , respectively.
Note that ?pol and ?subj are disjoint by construc-
tion, i.e., ?Tpol?subj = 0. We will present extensions
in Section 4.5.
For example, suppose ?pol and ?subj were both
bag-of-words feature vectors. Then we might learn
a high weight for the feature corresponding to the
word ?think? in ?subj since that word is indicative
of the sentence being subjective (but not necessarily
indicating positive or negative polarity).
4.1 Making Predictions
Algorithm 1 describes our inference procedure. Re-
call from (2) that our hypothesis function predicts
the sentiment label that maximizes (3). To do this,
we compare the best set of sentences that explains
a positive polarity prediction with the best set that
explains a negative polarity prediction.
We now specify the structure of S(x). In this pa-
per, we use a cardinality constraint,
S(x) = {s ? {1, . . . , |x|} : |s| ? f(|x|)}, (5)
1048
Algorithm 2 Training Algorithm for OP 1
1: Input: {(x1, y1), . . . , (xN , yN )} //training data
2: Input: C //regularization parameter
3: Input: (s1, . . . , sN ) //initial guess
4: ~w ? SSVMSolve(C, {(xi, yi, si)}Ni=1)
5: while ~w not converged do
6: for i = 1, . . . , N do
7: si ? argmaxs?S(xi) ~w
T?(xi, yi, s)
8: end for
9: ~w ? SSVMSolve(C, {(xi, yi, si)}Ni=1)
10: end while
11: Return ~w
where f(|x|) is a function that depends only on the
number of sentences in x. For example, a simple
function is f(|x|) = |x| ? 0.3, indicating that at most
30% of the sentences in x can be subjective.
Using this definition of S(x), we can then com-
pute the best set of subjective sentences for each
possible y by computing the joint subjectivity and
polarity score of each sentence xj in isolation,
y ? ~wTpol?pol(x
j) + ~wTsubj?subj(x
j),
and selecting the top f(|x|) as s (or fewer, if there
are fewer than f(|x|) that have positive joint score).
4.2 Training
For training, we will use an approach based on latent
variable structural SVMs (Yu and Joachims, 2009).
Optimization Problem 1.
min
~w,??0
1
2
?~w?2 +
C
N
N?
i=1
?i (6)
s.t. ?i :
max
s?Si
~wT?(xi, yi, s) ?
max
s??S(xi)
~wT?(xi,?yi, s
?) + 1? ?i (7)
OP 1 optimizes the standard SVM training objec-
tive for binary classification. Each training example
has a corresponding constraint (7), which is quanti-
fied over the best possible explanation of the train-
ing polarity label. Note that we never observe the
true explanation for the training labels; they are the
hidden or latent variables. The hidden variables are
also ignored in the objective function.
As a result, one can interpret OP 1 to be directly
optimizing a trade-off between model complexity
(as measured using the 2-norm) and document-level
classification error in the training set. This has two
main advantages over related training approaches.
First, it solves the multi-level problem jointly as op-
posed to separately, which avoids introducing diffi-
cult to control propagation errors. Second, it does
not require solving the sentence-level task perfectly,
and also does not require precise sentence-level
training labels. In other words, our goal is to learn to
identify the informative (subjective) sentences that
best explain the training labels to the extent required
for good document classification performance.
OP 1 is non-convex because of the constraints (7).
To solve OP 1, we use the combination of the CCCP
algorithm (Yuille and Rangarajan, 2003) with cut-
ting plane training of structural SVMs (Joachims et
al., 2009), as proposed in Yu and Joachims (2009).
Suppose each constraint (7) is replaced by
~wT?(xi, yi, si) ? max
s??S(xi)
~wT?(xi,?yi, s
?)+1??i,
where si is some fixed explanation (e.g., an initial
guess of the best explanation). Then OP 1 reduces
to a standard structural SVM, which can be solved
efficiently (Joachims et al, 2009). Algorithm 2 de-
scribes our training procedure. Starting with an ini-
tial guess si for each training example, the training
procedure alternates between solving an instance of
the resulting structural SVM (called SSVMSolve in
Algorithm 2) using the currently best known expla-
nations si (Line 9), and making a new guess of the
best explanations (Line 7). Yu and Joachims (2009)
showed that this alternating procedure for training
latent variable structural SVMs is an instance of the
CCCP procedure (Yuille and Rangarajan, 2003), and
so is guaranteed to converge to a local optimum.
For our experiments, we do not train until conver-
gence, but instead use performance on a validation
set to choose the halting iteration. Since OP 1 is non-
convex, a good initialization is necessary. To gener-
ate the initial explanations, one can use an off-the-
shelf sentiment classifier such as OpinionFinder2
(Wilson et al, 2005). For some datasets, there ex-
ist documents with annotated sentences, which we
2http://www.cs.pitt.edu/mpqa/
opinionfinderrelease/
1049
can treat either as the ground truth or another (very
good) initial guess of the explanatory sentences.
4.3 Feature Representation
Like any machine learning approach, we must spec-
ify a useful set of features for the? vectors described
above. We will consider two types of features.
Bag-of-words. Perhaps the simplest approach is
to define ? using a bag-of-words feature representa-
tion, with one feature corresponding to each word in
the active lexicon of the corpus. Using such a feature
representation might allow us to learn which words
have high polarity (e.g., ?great?) and which are in-
dicative of subjective sentences (e.g., ?opinion?).
Sentence properties. We can incorporate many
useful features to describe sentence subjectivity. For
example, subjective sentences might densely popu-
late the end of a document, or exhibit spatial co-
herence (so features describing previous sentences
might be useful for classifying the current sentence).
Such features cannot be compactly incorporated into
flat models that ignore the document structure.
For our experiments, we normalize each ?subj
and ?pol to have unit 2-norm.
Joint Feature Normalization. Another design
decision is the choice of normalization N(x) in (3).
Two straightforward choices are N(x) = f(|x|) and
N(x) =
?
f(|x|), where f(|x|) is the size con-
straint as described in (5). In our experiments we
tried both and found the square root normalization
to work better in practice; therefore all the experi-
mental results are reported using N(x) =
?
f(|x|).
The appendix contains an analysis that sheds light
on when square root normalization can be useful.
4.4 Incorporating Proximity Information
As mentioned in Section 4.3, it is possible (and
likely) for subjective sentences to exhibit spatial co-
herence (e.g., they might tend to group together).
To exploit this structure, we will expand the feature
space of ?subj to include both the words of the cur-
rent and previous sentence as follows,
?subj(x, j) =
[
?subj(xj)
?subj(xj?1)
]
.
The corresponding weight vector can be written as
~w?subj =
[
~wsubj
~wprevSubj
]
.
By adding these features, we are essentially assum-
ing that the words of the previous sentence are pre-
dictive of the subjectivity of the current sentence.
Alternative approaches include explicitly ac-
counting for this structure by treating subjective
sentence extraction as a sequence-labeling problem,
such as in McDonald et al (2007). Such struc-
ture formulations can be naturally encoded in the
joint feature map. Note that the inference procedure
in Algorthm 1 is still tractable, since it reduces to
comparing the best sequence of subjective/objective
sentences that explains a positive sentiment versus
the best sequence that explains a negative sentiment.
For this study, we chose not to examine this more
expressive yet more complex structure.
4.5 Extensions
Though our initial model (3) is simple and intuitive,
performance can depend heavily on the quality of
latent variable initialization and the quality of the
feature structure design. Consider the case where
the initialization contains only objective sentences
that do not convey any sentiment. Then all the fea-
tures initially available during training are gener-
ated from these objective sentences and are thus use-
less for sentiment classification. In other words, too
much useful information has been suppressed for
the model to make effective decisions. To hedge
against learning poor models due to using a poor
initialization and/or a suboptimal feature structure,
we now propose extensions that incorporate infor-
mation from the entire document.
We identify the following desirable properties that
any such extended model should satisfy:
(A) The model should be linear.
(B) The model should be trained jointly.
(C) The component that models the entire docu-
ment should influence which sentences are ex-
tracted.
The first property stems from the fact that our ap-
proach relies on linear models. The second property
is desirable since joint training avoids error propaga-
tion that can be difficult to control. The third prop-
erty deals with the information suppression issue.
1050
4.5.1 Regularizing Relative to a Prior
We first consider a model that satisfies properties
(A) and (C). Using the representation in (4), we pro-
pose a training procedure that regularize ~wpol rela-
tive to a prior model. Suppose we have a weight
vector ~w0 which indicated the a priori guess of the
contribution of each corresponding feature, then we
can train our model using OP 2,
Optimization Problem 2.
min
~w,??0
1
2
?~w ? ~w0?
2 +
C
N
N?
i=1
?i
s.t. ?i :
max
s?Si
~wT?(xi, yi, s) ?
max
s??S(xi)
~wT?(xi,?yi, s
?) + 1? ?i
For our experiments, we use
~w0 =
[
~wdoc
0
]
,
where ~wdoc denotes a weight vector trained to clas-
sify the polarity of entire documents. Then one can
interpret OP 2 as enforcing that the polarity weights
~wpol not be too far from ~wdoc. Note that ~w0 must be
available before training. Therefore this approach
does not satisfy property (B).
4.5.2 Extended Feature Space
One simple way to satisfy all three aforemen-
tioned properties is to jointly model not only po-
larity and subjectivity of the extracted sentences,
but also polarity of the entire document. Let ~wdoc
denote the weight vector used to model the polar-
ity of entire document x (so the document polarity
score is then ~wTdoc?pol(x)). We can also incorporate
this weight vector into our structured model to com-
pute a smoothed polarity score of each sentence via
~wTdoc?pol(x
j). Following this intuition, we propose
the following structured model,
~wT?(x, y, s) =
y
N(x)
?
?
?
j?s
(
~wTpol?pol(x
j) + ~wTdoc?pol(x
j)
)
?
?
+
1
N(x)
?
?
?
j?s
~wTsubj?subj(x
j)
?
?+ y ? ~wTdoc?pol(x)
where the weight vector is now
~w =
?
?
~wpol
~wsubj
~wdoc
?
? .
Training this model via OP 1 achieves that ~wdoc is
(1) used to model the polarity of the entire docu-
ment, and (2) used to compute a smoothed estimate
of the polarity of the extracted sentences. This sat-
isfies all three properties (A), (B), and (C), although
other approaches are also possible.
5 Experiments
5.1 Experimental Setup
We evaluate our methods using the Movie Reviews
and U.S. Congressional Floor Debates datasets, fol-
lowing the setup used in previous work for compar-
ison purposes.3
Movie Reviews. We use the movie reviews
dataset from Zaidan et al (2007) that was originally
released by Pang and Lee (2004). This version con-
tains annotated rationales for each review, which we
use to generate an additional initialization during
training (described below). We follow exactly the
experimental setup used in Zaidan et al (2007).4
U.S. Congressional Floor Debates. We also
use the U.S. Congressional floor debates transcripts
from Thomas et al (2006). The data was extracted
from GovTrack (http://govtrack.us), which has all
available transcripts of U.S. floor debates in the
House of Representatives in 2005. As in previ-
ous work, only debates with discussions of ?con-
troversial? bills were considered (where the los-
ing side had at least 20% of the speeches). The
goal is to predict the vote (?yea? or ?nay?) for the
speaker of each speech segment. For our experi-
ments, we evaluate our methods using the speaker-
based speech-segment classification setting as de-
scribed in Thomas et al (2006).5
3Datasets in the required format for SVMsle are available at
http://www.cs.cornell.edu/
?
ainur/data.html
4Since the rationale annotations are available for nine out of
10 folds, we used the 10-th fold as the blind test set. We trained
nine different models on subsets of size eight, used the remain-
ing fold as the validation set, and then measured the average
performance on the final test set.
5In the other setting described in Thomas et al (2006)
(segment-based speech-segment classification), around 39% of
1051
Table 1: Summary of the experimental results for the Movie Reviews (top) and U.S. Congressional Floor Debates
(bottom) datasets using SVMsle, SVMsle w/ Prior and SVMslefs with and without proximity features.
INITIALIZATION SVMsle + Prox.Feat. SVM
sle
+ Prox.Feat. SVMslefs + Prox.Feat.w/ Prior
Random 30% 87.22 85.44 87.61 87.56 89.50 88.22
Last 30% 89.72 ? 88.83 90.50 ? 90.00 ? 91.06 ? 91.22 ?
OpinionFinder 91.28 ? 90.89 ? 91.72 ? 93.22 ? 92.50 ? 92.39 ?
Annot.Rationales 91.61 ? 92.00 ? 92.67 ? 92.00 ? 92.28 ? 93.22 ?
INITIALIZATION SVMsle + Prox.Feat. SVM
sle
+ Prox.Feat. SVMslefs + Prox.Feat.w/ Prior
Random 30% 78.84 73.14 78.49 76.40 77.33 73.84
Last 30% 73.26 73.95 71.51 73.60 67.79 73.37
OpinionFinder 77.33 79.53 77.09 78.60 77.67 77.09
? For Movie Reviews, the SVM baseline accuracy is 88.56%. A ? (or ?) indicates statically significantly better performance than
baseline according to the paired t-test with p < 0.001 (or p < 0.05).
? For U.S. Congressional Floor Debates, the SVM baseline accuracy is 70.00%. Statistical significance cannot be calculated because
the data comes in a single split.
Since our training procedure solves a non-convex
optimization problem, it requires an initial guess of
the explanatory sentences. We use an explanatory
set size (5) of 30% of the number of sentences in
each document, L = d0.3 ? |x|e, with a lower cap of
1. We generate initializations using OpinionFinder
(Wilson et al, 2005), which were shown to be a
reasonable substitute for human annotations in the
Movie Reviews dataset (Yessenalina et al, 2010).6
We consider two additional (baseline) methods
for initialization: using a random set of sentences,
and using the last 30% of sentence in the document.
In the Movie Reviews dataset, we also use sentences
containing human-annotator rationales as a final ini-
tialization option. No such manual annotations are
available for the Congressional Debates.
5.2 Experimental Results
We evaluate three versions of our model: the ini-
tial model (3) which we call SVMsle (SVMs for
Sentiment classification with Latent Explanations),
SVMsle regularized relative to a prior as described in
the documents in the whole dataset contain only 1-3 sentences,
making it an uninteresting setting to analyze with our model.
6We select all sentences whose majority vote of Opinion-
Finder word-level polarities matches the document?s sentiment.
If there are fewer than L sentences, we add sentences starting
from the end of the document. If there are more, we remove
sentences starting from the beginning of the document.
Section 4.5.1 which we refer to as SVMsle w/ Prior,7
and the feature smoothing model described in Sec-
tion 4.5.2 which we call SVMslefs . Due to the diffi-
culty of selecting a good prior, we expect SVMslefs to
exhibit the most robust performance.
Table 1 shows a comparison of our proposed
methods on the two datasets. We observe that
SVMslefs provides both strong and robust perfor-
mance. The performance of SVMsle is generally bet-
ter when trained using a prior than not in the Movie
Reviews dataset. Both extensions appear to hurt
performance in the U.S. Congressional Floor De-
bates dataset. Using OpinionFinder to initialize our
training procedure offers good performance across
both datasets, whereas the baseline initializations
exhibit more erratic performance behavior.8 Unsur-
prisingly, initializing using human annotations (in
the Movie Reviews dataset) can offer further im-
provement. Adding proximity features (as described
in Section 4.4) in general seems to improve perfor-
mance when using a good initialization, and hurts
performance otherwise.
7We either used the same value of C to train both standard
SVM model and SVMsle w/ Prior or used the best standard
SVM model on the validation set to train SVMsle w/ Prior. We
chose the combination that works the best on the validation set.
8Using the random initialization on the U.S. Congressional
Floor Debates dataset offers surprisingly good performance.
1052
Table 2: Comparison of SVMslefs with previous work on
the Movie Reviews dataset. We considered two settings:
when human annotations are available (Annot. Labels),
and when they are unavailable (No Annot. Labels).
METHOD ACC
Baseline SVM 88.56
Annot. Zaidan et al (2007) 92.20
Labels SVMslefs 92.28
SVMslefs + Prox.Feat. 93.22
No Annot. Yessenalina et al (2010) 91.78
Labels SVMslefs 92.50
SVMslefs +Prox.Feat. 92.39
Table 3: Comparison of SVMslefs with previous work on
the U.S. Congressional Floor Debates dataset for the
speaker-based segment classification task.
METHOD ACC
Baseline SVM 70.00
Prior work Thomas et al (2006) 71.28Bansal et al (2008) 75.00
Our work SVM
sle
fs 77.67
SVMslefs + Prox.Feat. 77.09
Tables 2 and 3 show a comparison of SVMslefs with
previous work on the Movie Reviews and U.S. Con-
gressional Floor Debates datasets, respectively. For
the Movie Reviews dataset, we considered two set-
tings: when human annotations are available, and
when they are not (in which case we initialized using
OpinionFinder). For the U.S. Congressional Floor
Debates dataset we used only the latter setting, since
there are no annotations available for this dataset. In
all cases we observe SVMslefs showing improved per-
formance compared to previous results.
Training details. We tried around 10 different
values for C parameter, and selected the final model
based on the validation set. The training proce-
dure alternates between training a standard struc-
tural SVM model and using the subsequent model
to re-label the latent variables. We selected the halt-
ing iteration of the training procedure using the val-
idation set. When initializing using human annota-
tions for the Movie Reviews dataset, the halting iter-
ation is typically the first iteration, whereas the halt-
ing iteration is typically chosen from a later iteration
Figure 1: Overlap of extracted sentences from different
SVMslefs models on the Movie Reviews training set.
Figure 2: Test accuracy on the Movie Reviews dataset for
SVMslefs while varying extraction size.
when initializing using OpinionFinder.
Figure 1 shows the per-iteration overlap of ex-
tracted sentences from SVMslefs models initialized us-
ing OpinionFinder and human annotations on the
Movie Reviews training set. We can see that train-
ing has approximately converged after about 10 it-
erations.9 We can also see that both models itera-
tively learn to extract sentences that are more similar
to each other than their respective initializations (the
overlap between the two initializations is 57%). This
is an indicator that our learning problem, despite be-
ing non-convex and having multiple local optima,
has a reasonably large ?good? region that can be ap-
proached using different initialization methods.
Varying the extraction size. Figure 2 shows how
accuracy on the test set of SVMslefs changes on the
Movie Reviews dataset as a function of varying the
extraction size f(|x|) from (5). We can see that per-
formance changes smoothly10 (and so is robust), and
that one might see further improvement from more
9The number of iterations required to converge is an upper
bound on the number of iterations from which to choose the
halting iteration (based on a validation set).
10The smoothness will depend on the initialization.
1053
Table 4: Example ?yea? speech with Latent Explanations from the U.S. Congressional Floor Debates dataset predicted
by SVMslefs with OpinionFinder initialization. Latent Explanations are preceded by solid circles with numbers denoting
their preference order (1 being most preferred by SVMslefs ). The five least subjective sentences are preceded by circles
with numbers denoting the subjectivity order (1 being least subjective according to SVMslefs ).
? Mr. Speaker, I am proud to stand
on the house floor today to speak in
favor of the Stem Cell Research En-
hancement Act, legislation which will
bring hope to millions of people suffer-
ing from disease in this nation. ? I
want to thank Congresswoman Degette
and Congressman Castle for their tire-
less work in bringing this bill to the
house floor for a vote.
? The discovery of embryonic stem
cells is a major scientific breakthrough.
? Embryonic stem cells have the po-
tential to form any cell type in the
human body. This could have pro-
found implications for diseases such as
Alzheimer?s, Parkinson?s, various forms
of brain and spinal cord disorders, dia-
betes, and many types of cancer. ? Ac-
cording to the Coalition for the Ad-
vancement of Medical Research, there
are at least 58 diseases which could po-
tentially be cured through stem cell re-
search.
That is why more than 200 major
patient groups, scientists, and medical
research groups and 80 Nobel Laure-
ates support the Stem Cell Research En-
hancement Act. ? They know that this
legislation will give us a chance to find
cures to diseases affecting 100 million
Americans.
I want to make clear that I oppose re-
productive cloning, as we all do. I have
voted against it in the past. ? However,
that is vastly different from stem cell re-
search and as an ovarian cancer sur-
vivor, I am not going to stand in the way
of science.
Permitting peer-reviewed Federal
funds to be used for this research,
combined with public oversight of these
activities, is our best assurance that
research will be of the highest quality
and performed with the greatest dignity
and moral responsibility. The policy
President Bush announced in August
2001 has limited access to stem cell
lines and has stalled scientific progress.
As a cancer survivor, I know the des-
peration these families feel as they wait
for a cure. ? This congress must not
stand in the way of that progress. ? We
have an opportunity to change the lives
of millions, and I hope we take it. ? I
urge my colleagues to support this leg-
islation.
careful tuning of the size constraint.
Examining an example prediction. Our pro-
posed methods are not designed to extract inter-
pretable explanations, but examining the extracted
explanations might still yield meaningful informa-
tion. Table 4 contains an example speech from the
U.S. Congressional Floor Debates test set, with La-
tent Explanations found by SVMslefs highlighted in
boldface. This speech was made in support of the
Stem Cell Research Enhancement Act. For com-
parison, Table 4 also shows the five least subjective
sentences according to SVMslefs . Notice that most of
these ?objective? sentences can plausibly belong to
speeches made in opposition to bills that limit stem
cell research funding. That is, they do not clearly in-
dicate the speaker?s stance towards the specific bill
in question. We can thus see that our approach can
indeed learn to infer sentences that are essential to
understanding the document-level sentiment.
6 Discussion
Making good structural assumptions simplifies the
development process. Compared to methods that
modify the training of flat document classifiers (e.g.,
Zaidan et al (2007)), our approach uses fewer pa-
rameters, leading to a more compact and faster train-
ing stage. Compared to methods that use a cascaded
approach (e.g., Pang and Lee (2004)), our approach
is more robust to errors in the lower-level subtask
due to being a joint model.
Introducing latent variables makes the training
procedure more flexible by not requiring lower-level
labels, but does require a good initialization (i.e., a
reasonable substitute for the lower-level labels). We
believe that the widespread availability of off-the-
shelf sentiment lexicons and software, despite being
developed for a different domain, makes this issue
less of a concern, and in fact creates an opportunity
for approaches like ours to have real impact.
One can incorporate many types of sentence-level
information that cannot be directly incorporated into
a flat model. Examples include scores from another
sentence-level classifier (e.g., from Nakagawa et. al
(2010)) or combining phrase-level polarity scores
(e.g., from Choi and Cardie (2008)) for each sen-
tence, or features that describe the position of the
sentence in the document.
Most prior work on the U.S. Congressional Floor
Debates dataset focused on using relationships be-
tween speakers such as agreement (Thomas et al,
2006; Bansal et al, 2008), and used a global min-
cut inference procedure. However, they require all
1054
test instances to be known in advance (i.e., their for-
mulations are transductive). Our method is not lim-
ited to the transductive setting, and instead exploits
a different and complementary structure: the latent
explanation (i.e., only some sentences in the speech
are indicative of the speaker?s vote).
In a sense, the joint feature structure used in
our model is the simplest that could be used. Our
model makes no explicit structural dependencies be-
tween sentences, so the choice of whether to extract
each sentence is essentially made independently of
other sentences in the document. More sophisticated
structures can be used if appropriate. For instance,
one can formulate the sentence extraction task as
a sequence labeling problem similar to (McDonald
et al, 2007), or use a more expressive graphical
model such as in (Pang and Lee, 2004; Thomas et
al., 2006). So long as the global inference proce-
dure is tractable or has a good approximation al-
gorithm, then the training procedure is guaranteed
to converge with rigorous generalization guarantees
(Finley and Joachims, 2008). Since any formulation
of the extraction subtask will suppress information
for the main document-level task, one must take care
to properly incorporate smoothing if necessary.
Another interesting direction is training models to
predict not only sentiment polarity, but also whether
a document is objective. For example, one can pose
a three class problem (?positive?, ?negative?, ?ob-
jective?), where objective documents might not nec-
essarily have a good set of (subjective) explanatory
sentences, similar to (Chang et al, 2010).
7 Conclusion
We have presented latent variable structured mod-
els for the document sentiment classification task.
These models do not rely on sentence-level an-
notations, and are trained jointly (over both the
document and sentence levels) to directly optimize
document-level accuracy. Experiments on two stan-
dard sentiment analysis datasets showed improved
performance over previous results.
Our approach can, in principle, be applied to any
classification task that is well modeled by jointly
solving an extraction subtask. However, as evi-
denced by our experiments, proper training does re-
quire a reasonable initial guess of the extracted ex-
planations, as well as ways to mitigate the risk of
the extraction subtask suppressing too much infor-
mation (such as via feature smoothing).
Acknowledgments
This work was supported in part by National Science
Foundation Grants BCS-0904822, BCS-0624277, IIS-
0535099; by a gift from Google; and by the Department
of Homeland Security under ONR Grant N0014-07-1-
0152. The second author was also supported in part by
a Microsoft Research Graduate Fellowship. The authors
thank Yejin Choi, Thorsten Joachims, Nikos Karampatzi-
akis, Lillian Lee, Chun-Nam Yu, and the anonymous re-
viewers for their helpful comments.
Appendix
Recall that all the ?subj and ?pol vectors have unit 2-
norm, which is assumed here to be desirable. We now
show that using N(x) =
?
f(|x|) achieves a similar
property for ?(x, y, s). We can write the squared 2-norm
of ?(x, y, s) as
|?(x, y, s)|2 =
1
N(x)2
?
?
?
j?s
y ? ?pol(x
j) + ?subj(x
j)
?
?
2
=
1
f(|x|)
?
?
?
?
?
?
j?s
?pol(x
j)
?
?
2
+
?
?
?
j?s
?subj(x
j)
?
?
2
?
?
? ,
where the last equality follows from the fact that
?pol(x
j)T?subj(x
j) = 0,
due to the two vectors using disjoint feature spaces by
construction. The summation of the ?pol(xj) terms is
written as
?
?
?
j?s
?pol(x
j)
?
?
2
=
?
j?s
?
i?s
?pol(x
j)T?pol(x
i)
?
?
j?s
?pol(x
j)T?pol(x
j) (8)
=
?
j?s
1 ? f(|x|),
where (8) follows from the sparsity assumption that
?i 6= j : ?pol(x
j)T?pol(x
i) ? 0.
A similar argument applies for the ?subj(xj) terms.
Thus, by choosing N(x) =
?
f(|x|) the joint feature
vectors ?(x, y, s) will have approximately equal magni-
tude as measured using the 2-norm.
1055
References
Mohit Bansal, Claire Cardie, and Lillian Lee. 2008. The
power of negative thinking: Exploiting label disagree-
ment in the min-cut classification framework. In In-
ternational Conference on Computational Linguistics
(COLING).
Ming-Wei Chang, Dan Goldwasser, Dan Roth, and Vivek
Srikumar. 2010. Discriminative learning over con-
strained latent representations. In Conference of the
North American Chapter of the Association for Com-
putational Linguistics (NAACL).
Yejin Choi and Claire Cardie. 2008. Learning with com-
positional semantics as structural inference for subsen-
tential sentiment analysis. In Empirical Methods in
Natural Language Processing (EMNLP).
James Clarke, Dan Goldwasser, Ming-Wei Chang, and
Dan Roth. 2010. Driving semantic parsing from the
world?s response. In ACL Conference on Natural Lan-
guage Learning (CoNLL), July.
Pedro Felzenszwalb, David McAllester, and Deva Ra-
manan. 2008. A discriminatively trained, multiscale,
deformable part model. In IEEE Conference on Com-
puter Vision and Pattern Recognition (CVPR).
Thomas Finley and Thorsten Joachims. 2008. Train-
ing structural svms when exact inference is intractable.
In International Conference on Machine Learning
(ICML).
Thorsten Joachims, Thomas Finley, and Chun-Nam Yu.
2009. Cutting plane training of structural svms. Ma-
chine Learning, 77(1):27?59.
Yi Mao and Guy Lebanon. 2006. Isotonic conditional
random fields and local sentiment flow. In Neural In-
formation Processing Systems (NIPS).
Ryan McDonald, Kerry Hannan, Tyler Neylon, Mike
Wells, and Jeff Reynar. 2007. Structured models for
fine-to-coarse sentiment analysis. In Annual Meet-
ing of the Association for Computational Linguistics
(ACL).
Tetsuji Nakagawa, Kentaro Inui, and Sadao Kurohashi.
2010. Dependency tree-based sentiment classification
using crfs with hidden variables. In Conference of the
North American Chapter of the Association for Com-
putational Linguistics (NAACL).
Bo Pang and Lillian Lee. 2004. A sentimental education:
Sentiment analysis using subjectivity summarization
based on minimum cuts. In Annual Meeting of the As-
sociation for Computational Linguistics (ACL).
Bo Pang and Lillian Lee. 2008. Opinion mining and
sentiment analysis. Foundations and Trends in Infor-
mation Retrieval, 2(1-2):1?135.
Bo Pang, Lillian Lee, and Shivakumar Vaithyanathan.
2002. Thumbs up? Sentiment classification using ma-
chine learning techniques. In Empirical Methods in
Natural Language Processing (EMNLP).
Slav Petrov and Dan Klein. 2007. Discriminative log-
linear grammars with latent variables. In Neural In-
formation Processing Systems (NIPS).
Matt Thomas, Bo Pang, and Lillian Lee. 2006. Get
out the vote: Determining support or opposition from
Congressional floor-debate transcripts. In Empirical
Methods in Natural Language Processing (EMNLP).
Theresa Wilson, Janyce Wiebe, and Paul Hoffmann.
2005. Recognizing contextual polarity in phrase-level
sentiment analysis. In Empirical Methods in Natural
Language Processing (EMNLP).
Ainur Yessenalina, Yejin Choi, and Claire Cardie. 2010.
Automatically generating annotator rationales to im-
prove sentiment classification. In Annual Meeting of
the Association for Computational Linguistics (ACL).
Chun-Nam Yu and Thorsten Joachims. 2009. Learning
structural svms with latent variables. In International
Conference on Machine Learning (ICML).
Alan L. Yuille and Anand Rangarajan. 2003. The
concave-convex procedure. Neural Computation,
15(4):915?936, April.
Omar F. Zaidan and Jason Eisner. 2008. Modeling an-
notators: a generative approach to learning from an-
notator rationales. In Empirical Methods in Natural
Language Processing (EMNLP).
Omar F. Zaidan, Jason Eisner, and Christine Piatko.
2007. Using ?annotator rationales? to improve ma-
chine learning for text categorization. In Conference
of the North American Chapter of the Association for
Computational Linguistics (NAACL).
1056
Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 172?182,
Edinburgh, Scotland, UK, July 27?31, 2011. c?2011 Association for Computational Linguistics
Compositional Matrix-Space Models for Sentiment Analysis
Ainur Yessenalina
Dept. of Computer Science
Cornell University
Ithaca, NY, 14853
ainur@cs.cornell.edu
Claire Cardie
Dept. of Computer Science
Cornell University
Ithaca, NY, 14853
cardie@cs.cornell.edu
Abstract
We present a general learning-based approach
for phrase-level sentiment analysis that adopts
an ordinal sentiment scale and is explicitly
compositional in nature. Thus, we can model
the compositional effects required for accu-
rate assignment of phrase-level sentiment. For
example, combining an adverb (e.g., ?very?)
with a positive polar adjective (e.g., ?good?)
produces a phrase (?very good?) with in-
creased polarity over the adjective alone. In-
spired by recent work on distributional ap-
proaches to compositionality, we model each
word as a matrix and combine words us-
ing iterated matrix multiplication, which al-
lows for the modeling of both additive and
multiplicative semantic effects. Although the
multiplication-based matrix-space framework
has been shown to be a theoretically ele-
gant way to model composition (Rudolph and
Giesbrecht, 2010), training such models has
to be done carefully: the optimization is non-
convex and requires a good initial starting
point. This paper presents the first such al-
gorithm for learning a matrix-space model for
semantic composition. In the context of the
phrase-level sentiment analysis task, our ex-
perimental results show statistically signifi-
cant improvements in performance over a bag-
of-words model.
1 Introduction
Sentiment analysis has been an active research area
in recent years. Work in the area ranges from iden-
tifying the sentiment of individual words to deter-
mining the sentiment of phrases, sentences and doc-
uments (see Pang and Lee (2008) for a survey). The
bulk of previous research, however, models just pos-
itive vs. negative sentiment, collapsing positive (or
negative) words, phrases and documents of differ-
ing intensities into just one positive (or negative)
class. For word-level sentiment, therefore, these
methods would not recognize a difference in senti-
ment between words like ?good? and ?great?, which
have the same direction of polarity (i.e., positive)
but different intensities. At the phrase level, the
methods will fail to register compositional effects in
sentiment brought about by intensifiers like ?very?,
?absolutely?, ?extremely?, etc. ?Happy? and ?very
happy?, for example, will both be considered sim-
ply ?positive? in sentiment. In real-world settings,
on the other hand, sentiment values extend across a
polarity spectrum ? from very negative, to neutral,
to very positive. Recent research has shown, in par-
ticular, that modeling intensity at the phrase level is
important for real-world natural language process-
ing tasks including question answering and textual
entailment (de Marneffe et al, 2010).
This paper describes a general approach for
phrase-level sentiment analysis that takes these real-
world requirements into account: we adopt a five-
level ordinal sentiment scale and present a learning-
based method that assigns ordinal sentiment scores
to phrases.
Importantly, our approach will also be explicitly
compositional1 in nature so that it can accurately ac-
count for critical interactions among the words in
1The Principle of Compositionality asserts that the meaning
of a complex expression is a function of the meanings of its
constituent expressions and the rules used to combine them.
172
each sentiment-bearing phrase. Consider, for exam-
ple, combining an adverb like ?very? with a polar
adjective like ?good?. ?Good? has an a priori posi-
tive sentiment, so ?very good? should be considered
more positive even though ?very?, on its own, does
not bear sentiment. Combining ?very? with a nega-
tive adjective, like ?bad?, produces a phrase (?very
bad?) that should be characterized as more negative
than the original adjective. Thus, it is convenient
to think of the effect of combining an intensifying
adverb with a polar adjective as being multiplica-
tive in nature, if we assume the adjectives (?good?
and ?bad?) to have positive and a negative sentiment
scores, respectively.
Next, let us consider adverbial negators like ?not?
combined with polar adjectives. When model-
ing only positive and negative labels for sentiment,
negators are generally treated as flipping the polar-
ity of the adjective it modifies (Choi and Cardie,
2008; Nakagawa et al, 2010). However, recent work
(Taboada et al, 2011; Liu and Seneff, 2009) sug-
gests that the effect of the negator when ordinal sen-
timent scores are employed is more akin to damp-
ening the adjective?s polarity rather than flipping it.
For example, if ?perfect? has a strong positive sen-
timent, then the phrase ?not perfect? is still positive,
though to a lesser degree. And while ?not terrible? is
still negative, it is less negative than ?terrible?. For
these cases, it is convenient to view ?not? as shift-
ing polarity to the opposite side of polarity scale by
some value.
There are, of course, more interesting examples of
compositional semantic effects on sentiment: e.g.,
prevent cancer, ease the burden. Here, the verbs
prevent and ease act as content-word negators (Choi
and Cardie, 2008) in that they modify the negative
sentiment of their direct object arguments so that the
phrase as a whole is perceived as somewhat positive.
Nonetheless, the vast majority of methods for
phrase- and sentence-level sentiment analysis do not
tackle the task compositionally: they, instead, em-
ploy a bag-of-words representation and, at best, in-
corporate additional features to account for nega-
tors, intensifiers, and for contextual valence shifters,
which can change the sentiment over neighboring
words (e.g., Polanyi and Zaenen (2004), Wilson et
al. (2005) , Kennedy and Inkpen (2006), Shaikh et
al. (2007)).
One notable exception is Moilanen and Pulman
(2007), who propose a compositional semantic ap-
proach to assign a positive or negative sentiment to
newspaper article titles. However, their knowledge-
based approach presupposes the existence of a sen-
timent lexicon and a set of symbolic compositional
rules.
But learning-based compositional approaches
for sentiment analyis also exist. Choi and
Cardie (2008), for example, propose an algo-
rithm for phrase-based sentiment analysis that learns
proper assignments of intermediate sentiment anal-
ysis decision variables given the a priori (i.e., out
of context) polarity of the words in the phrase and
the (correct) phrase-level polarity. As in Moilianen
and Pulman (2007), semantic inference is based on
(a small set of) hand-written compositional rules. In
contrast, Nakagawa et. al (2010) use a dependency
parse tree to guide the learning of compositional ef-
fects. Each of the above, however, uses a binary
rather than an ordinal sentiment scale.
In contrast, our proposed method for phrase-
level sentiment analysis is inspired by recent work
on distributional approaches to compositionality.
In particular, Baroni and Zamparelli (2010) tackle
adjective-noun compositions using a vector repre-
sentation for nouns and learning a matrix represen-
tation for each adjective. The adjective matrices are
then applied as functions over the meanings of nouns
? via matrix-vector multiplication ? to derive the
meaning of adjective-noun combinations. Rudolph
and Giesbrecht (2010) show theoretically, that mul-
tiplicative matrix-space models are a general case
of vector-space models and furthermore exhibit de-
sirable properties for semantic analysis: they take
into account word order and are algebraically, neuro-
logically and psychologically plausible. This work,
however, does not present an algorithm for learning
such models; nor does it provide empirical evidence
in favor of matrix-space models over vector-space
models.
In the sections below, we propose a learning-
based approach to assign ordinal sentiment scores to
sentiment-bearing phrases using a general composi-
tional matrix-space model of language. In contrast
to previous work, all words are modeled as matri-
ces, independent of their part-of-speech, and com-
positional inference is uniformly modeled as ma-
173
trix multiplication. To predict an ordinal scale sen-
timent value, we employ Ordered Logistic Regres-
sion, introducing a novel training algorithm to ac-
commodate our compositional matrix-space repre-
sentations (Section 2). To our knowledge, this is the
first such algorithm for learning matrix-space mod-
els for semantic composition. We evaluate the ap-
proach on a standard sentiment corpus (Wiebe et al,
2005) (Section 3), making use of its manually anno-
tated phrase-level annotations for polarity and inten-
sity, and compare our approach to the more com-
monly employed bag-of-words model. We show
(Section 4) that our matrix-space model significantly
outperforms a bag-of-words model for the ordinal
scale sentiment prediction task.
2 The Model for Ordinal Scale Sentiment
Prediction
As described above, our task is to predict an ordi-
nal scale sentiment value for a phrase. To this end,
we employ a sentiment scale with five ordinal val-
ues: VERY NEGATIVE, NEGATIVE, NEUTRAL, POS-
ITIVE and VERY POSITIVE. Given a set of phrase-
level training examples with their gold-standard or-
dinal sentiment value, we then use an Ordered Lo-
gistic Regression (OLogReg) model for prediction.
Unfortunately, our matrix-space representation pre-
cludes doing this directly.
We have chosen OLogReg, as opposed to say
PRanking (Crammer and Singer, 2001), because op-
timization of the former is more attractive: the ob-
jective (likelihood) is smooth and the gradients are
continuous. As will become clear shortly, learn-
ing our models is not trivial and it is important to
use sophisticated off-the-shelf optimizers such as L-
BFGS.
For a bag-of-words model, OLogReg learns one
weight for each word and a set of thresholds by max-
imizing the likelihood of the training data. Typically,
this is accomplished by using an optimizer like L-
BFGS whose interface needs the value and gradient
of the likelihood with respect to the parameters at
their current values. In the next subsections, we in-
stantiate OLogReg for our sentiment prediction task
using a matrix-space word model (2.1 and 2.2) and
a bag-of-words model (2.3). The learning formula-
tion of bag-of-words OLogReg is convex therefore
we will get the global optimum; in contrast, the op-
timization problem for matrix-space model is non-
convex, it is important to initialize the model well.
Initialization of the matrix-space model is discussed
in Section 2.4.
2.1 Notation
In the subsequent subsections we will use the
following notation. Let n be the number of phrases
in the training set and let d be the number of words
in the dictionary. Let xi be the i-th phrase and yi
would be the label of xi, where yi takes r different
values yi ? {0, . . . , r ? 1}. Then |xi| will denote
the length of the phrase xi, and the words in i-th
phrase are: xi = xi1, xi2, . . . , xi|xi|; xij , 1 ? j ? |xi|
is the j-th word of i-th phrase; where xij is from the
dictionary: 1 ? xij ? d.
In the case of the bag-of-words model, ?(xi) ?
Rd is the representation of the i-th phrase. ?j(xi)
counts the number of times the j-th word from the
dictionary appears in the i-th phrase. Given a w ?
Rd it assigns a score ?i to a phrase xi by
?i = wT?(xi) =
|xi|?
j=1
wxij (1)
In the case of the matrix-space model the ?(xi) ?
R|xi|?d is the representation of the i-th phrase.
?jk(xi) is 1, if xij is the k-th word in the dictionary,
and zero otherwise. Given u, v ? Rm and a set of
matrices {Wp ? Rm?m}dp=1, one for each word, it
assigns a score ?i to a phrase xi by
?i = uT
?
?
|xi|?
j=1
d?
k=1
Wk?jk(xi)
?
? v
= uT
?
?
|xi|?
j=1
Wxij
?
? v (2)
where ?|xi|j=1 Wxij = Wxi1Wxi2 ? ? ?Wxi|xi| in exactlythis order. We choose to map matrices to the real
numbers by using vectors u and v from Rm?1; so
that ? = uTMv, where M ? Rm?m, which is sen-
sitive to the order of matrices2 , i.e. uTM1M2v 6=
2Care must be taken in choosing way to map matrix to a real
174
uTM2M1v.
Modeling composition. Am?mmatrix, represent-
ing a word, can be considered as a linear function,
mapping from Rm to Rm. Composition of words is
modeled by function composition, in our case com-
position of linear functions, i.e. matrix multipli-
cation. Note, that unlike bag-of-words model, the
matrix-space model takes word order into account,
since matrix multiplication is not commutative op-
eration.
2.2 Ordered Logistic Regression
Now we will describe our objective function for
OLogReg and its derivatives. OLogReg has r ?
1 thresholds (?0, . . . ?r?2), so introducing ??1 =
?? and ?r?1 = ? leads to the unified expression
for posterior probabilities for all values of k:
P (yi = k|x) = P (?k?1 < ?i ? ?k)
= F (?k ? ?i)? F (?k?1 ? ?i)
F (x) is an inverse-logit function
F (x) = e
x
1 + ex
this is its derivative:
dF (x)
dx = F (x)(1? F (x))
Therefore the negative loglikelihood of the training
data will look like the following (Hardin and Hilbe,
2007):
L = ?
n?
i=1
r?1?
k=0
ln(F (?k ? ?i)? F (?k?1 ? ?i))I(yi = k)
where r is the number of ordinal classes, ?i is the
score of i-th phrase, I is the indicator function that
is equal to 1 ? when yi = k, and zero otherwise. We
need to minimize the objective L with respect to the
following constraints:
?k?1 ? ?k, 1 ? k ? r ? 2 (3)
number. For example, one other way to map matrices to the
real numbers is to use the determinant of a matrix; however, the
determinant is not sensitive to the word order: det(M1M2) =
det(M1)det(M2) = det(M2M1); which is not desirable for a
model that needs to account for word order.
(The constraints are similar to the ones in PRank al-
gorithm). For ease of optimization we parametrize
our model via ?0, and ?j , 1 ? j ? r ? 2:
??1 = ??,
?0,
?1 = ?0 + ?1,
?2 = ?0 +
?2
j=1 ?j ,
. . . ,
?r?2 = ?0 +
?r?2
j=1 ?j
?r?1 = ?,
where ?1, . . ., ?r?2 are non-negative values, that rep-
resent how far the corresponding thresholds are from
each other. Then the constraints (3) would be:
?j ? 0, 1 ? j ? r ? 2 (4)
To simplify the equations we can rewrite the nega-
tive loglikelihood as follows:
L = ?
n?
i=1
r?1?
k=0
ln(Aik ?Bik)I(yi = k) (5)
where
Aik =
{
F (?0 +
?k
j=1 ?j ? ?i), if k = 0, . . . , r ? 2
1, if k = r ? 1
Bik =
{
0, if k = 0
F (?0 +
?k?1
j=1 ?j ? ?i), if k = 1, . . . , r ? 1
Let?s introduce Lik = ? ln(Aik ? Bik)I(yi = k)
and then the derivative of Lik with respect to ?0 will
be:
?Lik
??0
= ?[Aik(1?Aik)?Bik(1?Bik)]Aik ?Bik
I(yi = k)
= (Aik + Bik ? 1)I(yi = k)
For j = yi:
?Lik
??j
= ?Aik(1?Aik)Aik ?Bik
I(yi = k)
For all j < yi:
?Lik
??j
= (Aik + Bik ? 1)I(yi = k)
For all j > yi: ?Lik??j = 0.The derivative with respect to the score ?i is:
?Lik
??i
= (?Aik ?Bik + 1)I(yi = k) (6)
175
2.2.1 Matrix-Space Word Model
Here we show the derivatives with respect to a
word. For the OLogReg model with matrix-space
word representations, we have:
?L
?Wxij
= ?L??i
? ??i?Wxij
The expression for ?L??i is given in (6); we will derive
??i
?Wxij
from (2). In the case of the Matrix-Space word
model each word is represented as an m?m affine
matrix W :
W =
(
A b
0 1
)
(7)
We choose the class of affine matrices since for
affine matrices matrix multiplication represents both
operations: linear transformation and translation.
Linear transformation is important for modeling
changes in sentiment - translation is also useful (we
make use of a translation vector during initialization,
see Section 2.4). In this work we consider m ? 3
since we want the matrix A from (7) to represent
rotation and scaling. Applying the affine transfor-
mation W to vector [x, 1]T is equivalent to applying
linear transformation A and translation b to x. 3
Though vectors u and v can be learned together
with word matrices Wj , we choose to fix u and v.
The main intuition behind fixing u and v is to re-
duce the degrees of freedom of the model: differ-
ent assignments of u, v and Wj-s can lead to the
same score ?, i.e. there exist u? v? and W?j-s dif-
ferent from u, v and Wj-s respectively, such that
?(u, v,W ) would be equal to ?(u?, v?, W? ). 4
3
?
A b
0 1
??
x
1
?
=
?
Ax+ b
1
?
where A is a linear transformation, b is a translation vector.
Also the product of affine matrices is an affine matrix.
4The specific choice of u and v leads to an equivalent model
for all u? and v? such that u? = MTu, v? = M?1v, where M is
any invertible transformation (i.e. u?, v? are derived from u,v by
applying linear transformations MT , M?1 respectively):
uTW1W2v = (uTM)(M?1W1M)(M?1W2M)(M?1v)
= u?T W?1W?2v?
The derivative of the phrase ?i with respect to j-th
word Wj would be (for brevity we drop the phrase
index and Wj refers to Wxij and p refers to |xi|):
??i
?Wj
=
(?uTW1W2 . . .Wpv
?Wj
)
=
[
(uTW1 . . .Wj?1)T (Wj+1 . . .Wpv)T
]
=
[
(W Tj?1 . . .W T1 )(uvT )(W Tp . . .W Tj+1)
]
(see Peterson and Pederson(2008)).
In case if a certain word appears multiple times in
the phrase, the derivative with respect to that word
would be a sum of derivatives with respect to each
appearance of a word, while all other appearances
are fixed. For example,
(?uTWW1Wv
?W
)
= u(W1Wv)T + (uTWW1)T vT
where W is a representation of a word that is re-
peated.
So given the expression (6) for ?L??i , the derivativewith respect to each word can be computed. Notice
that the update for the j-th word in a sentence de-
pends on the order words, which is in line with our
desire to account for word order.
2.2.2 Optimization
The goal of training procedure is for the i-th
phrase with p words x1x2 . . . xp to learn word ma-
trices W1, W2, . . . , Wp such that resulting ?i-s will
lead to the lowest negative loglikelihood. The goal
of training procedure is to find word matrices W1,
W2, . . . Wp and thresholds ?0, ?1, . . . ?r?2 such
that the negative loglikelihood is minimized. So,
given the negative loglikelihood and the derivatives
with respect ?0 and ?j-s and word matrices W , we
optimize objective (5) subject to ?j ? 0. We use L-
BFGS-B (Large-scale Bound-constrained Optimiza-
tion) by Byrd et al (1995) as an optimizer.
2.2.3 Regularization in Matrix-Space Model
In order to make sure that the L-BFGS-B updates
do not cause numerical issues we perform the fol-
lowing regularization to the resulting matrices. An
m by m matrix Wj that can be represented as:
Wj =
(
A11 a12
aT21 a22
)
176
where A11 ? Rm?1?m?1, a12, a21 ? Rm?1?1,
a22 ? R. First make the matrix affine by updating
the last row, then the updated matrix will look like:
W?j =
(
A11 a12
0 1
)
It can be proven that such a projection returns the
closest affine matrix in Frobenius norm.
However, we also want to regularize the model to
avoid ill-conditioned matrices. Ill-conditioned ma-
trices represent transformations whose output is very
sensitive to small changes in the input and therefore
they have a similar effect to having large weights
in a bag-of-words model. To perform such a reg-
ularization we ?shrink? the singular values of A11
towards one. More specifically, we first use the
Singular Value Decomposition (SVD) of the A11:
U?V T = A11, where U and V are orthogonal ma-
trices, ? is a matrix with singular values on the diag-
onal. Then we update singular values in the follow-
ing way to get ??: ??ii = ?hii, where h is a parameter
between 0 and 1. If h = 1 then ?ii remains the
same. In the extreme case h = 0 then ?hii = 1. For
intermediate values of h the singular values of A11
would be brought closer to one. Finally, we recom-
pute A?11: A?11 = U ??V T . So, Wj would be :
W?j =
(
A?11 a12
0 1
)
2.2.4 Learning in the Matrix-Space Model
We use Algorithm 1 to learn the matrix-space
model. What essentially happens is that we iter-
ate two steps: optimizing the W matrices using L-
BFGS-B and the projection step. L-BFGS-B returns
a solution that is not necessarily an affine matrix.
After projecting to the space of affine matrices we
start L-BFGS-B from a better initial point. In prac-
tice, the first few iterations lead to large decrease in
negative loglikelihood.
2.3 Bag-Of-Words Model
In the bag-of-words model the score of the i-th
phrase is given in (1). Therefore, the partial deriva-
tive with respect to j-th word in i-th phrase ??i?wxij
is
equal to the number cj of times xji appears in xi, so:
?L
?wxij
= ?L??i
? cj
Algorithm 1 Training Algorithm for Matrix-Space
OLogReg
1: Input: {(x1, y1), . . . , (xn, yn)} //training data
2: Input: h //projection parameter
3: Input: T //number of iterations
4: Input: W , ?0 and ?j //initial values
5: for t = 1, . . . , T do
6: (W , ?0, ?j)=minimize L using L-BFGS-B
7: for i = 1, . . . , d do
8: Wi=Project(Wi, h)
9: end for
10: end for
11: Return W , ?0, ?j
Optimization. We minimize negative loglikelihood
using L-BFGS-B subject to ?j ? 0.
Regularization. To prevent overfitting for bag-of-
words model we regularize w. The L2-regularized
negative loglikelihood will consist of the expression
in (5) and an additional term ?2 ||w||22, where || ? ||2is the L2-norm of a vector. The derivative of the
additional term with respect to w will be:
? ?2 ||w||22
?w = ?w
Hence the partial derivative with respect to wxij willhave an additional term ?wxij .
2.4 Initialization
Initialization of bag-of-words OLogReg. We ini-
tialize the weight for each word with zero and ?0
with a random number and ?j-s with non-negative
random numbers. Since the learning problem for
bag-of-words OLogReg is convex, we will get the
global optimum.
Better Initialization of Matrix-Space Model. Pre-
liminary experiments showed that the Matrix-Space
model needs a good initialization. Initializing with
different random matrices reaches different local
minima and the quality of local minima depends on
initialization. Therefore, it is important to initialize
the model with a good initial point. One way to ini-
tialize the Matrix-Space model is to use the weights
learned by the bag-of-words model. We use the
following intuition for initializing the Matrix-Space
model. As noted in Section 2.2.1 applying trans-
formation A of affine matrix W can model a linear
177
transformation, while vector b represents a transla-
tion. Since matrix-space model can encode a vector-
space model (Rudolph and Giesbrecht, 2010), we
can initialize the matrices to exactly mimic the bag-
of-words model. In order to do that we place the
weight, learned by the bag-of-words model in the
first component of b. Let?s assume that wx1 and wx2
are the weights learned for two distinct words x1 and
x2 respectively. To compute the polarity score of
a phrase x1, x2 the bag-of-words model sums the
weights of these two words: wx1 and wx2 . Now we
want to have the same effect in matrix-space model.
Here we assume m = 3.
Z =
?
?
1 0 wx1
0 1 0
0 0 1
?
?
?
?
1 0 wx2
0 1 0
0 0 1
?
?
=
?
?
1 0 wx1 + wx2
0 1 0
0 0 1
?
?
Finally, there is a step of mapping matrix Z to a
number using u and v, such that ?(Z) = wx1 +
wx2 .We also want vector u and v to be such that:
uT
?
?
1 0 wx1 + wx2
0 1 0
0 0 1
?
? v = wx1 + wx2 (8)
The last equation can help us construct u and v.
We also set u and v to be orthogonal: uT v = 0.
So, we arbitrarily choose two orthogonal vectors for
which equation (8) holds: u = [1,?2, 1]T and v =
[1,?
?
2, 1]T .5
3 Experimental Methodology
For experimental evaluation of the proposed method
we use the publicly available Multi-Perspective
Question Answering (MPQA)6 corpus (Wiebe et al,
2005) version 1.2, which contains 535 newswire
documents that are manually annotated with phrase-
level subjectivity and intensity. We use the
expression-level boundary markings in MPQA to
extract phrases. We evaluate on positive, negative
and neutral opinion expressions that have intensities
5If m > 3, u and v can be set using the same intuition.
6http://www.cs.pitt.edu/mpqa/
Polarity Intensity Ordinal
label
negative high, extreme 0
negative medium 1
neutral high, extreme, medium 2
positive medium 3
positive high, extreme 4
Table 1: Mapping of combination of polarities and inten-
sities from MPQA dataset to our ordinal sentiment scale.
?medium?, ?high? or ?extreme?.7 The schematic
mapping of phrase polarity and intensity values on
ordinal sentimental scale is shown in Table 1.
3.1 Training Details
We perform 10-fold cross-validation on phrases ex-
tracted from the MPQA corpus: eight folds for train-
ing; one as a validation set; and one as test set. In
total there were 8022 phrases. Before training, we
extract lemmas for each word. For evaluation we
use Ranking Loss: 1n
?
i |y?i ? yi|, where y?i is the
prediction.
Choice of dimensionality m. The reported ex-
periments are done by setting m = 3. Preliminary
experiments with higher values of m (5, 20, 50), did
not lead to a better performance and increased the
training time; therefore we did not use those values
in our final experiments.
3.2 Methods
PRank. For each of the folds, we run 500 iterations
of PRank and choose an early stopping iteration us-
ing a model that led to the lowest ranking loss on the
validation set; afterwards report the average perfor-
mance of on a test set.
Bag-of-words OLogReg. To prevent overfitting we
search for the best regularization parameter among
the following values of ?: 10i, from 10?4 to 104.
The lowest negative log-likelihood value on the val-
idation set is attained for8 ? = 0.1. With this value
of ? fixed, the final model is the one with the lowest
negative loglikelihood on the training set.
7We ignored low-intensity phrases similar to (Choi and
Cardie, 2008; Nakagawa et al, 2010).
8We pick single ? that gives best average validation set per-
formance, and then use it to compute the average test set perfor-
mance.
178
Method Ranking loss
PRank 0.7808
Bag-of-words OLogReg 0.6665
Matrix-space OLogReg+RandInit 0.7417
Matrix-space OLogReg+BowInit 0.6375?
Table 2: Ranking loss for vector-space Ordered Logistic
Regression and Matrix-Space Logistic Regression.
? Stands for a significant difference w.r.t. the Bag-Of-
Words OLogReg model with p-value less than 0.001
(p < 0.001)
Matrix-space OLogReg+RandInit. First, we ini-
tialized matrices with with random numbers from
normal distribution N(0, 0.1) and set u and v as in
section 2.4, T is set to 25. We run with two different
random seeds and three different values for the pa-
rameter h: [0.1, 0.5, 0.9] and report the performance
of the model that had the lowest likelihood on the
validation set. The setting of h that lead to the best
model was 0.9.
Matrix-space OLogReg+BowInit. For the matrix-
space models we initialize the model with the out-
put of the regularized Bag-of-words OLogReg as de-
scribed in Section 2.4, T is set to 25. Then we use
the training procedure of Algorithm 1. We consider
three different values for the parameter h [0.1, 0.5,
0.9] and choose as the model with the lowest valida-
tion set negative log-likelihood. The best setting of
h was 0.1.
4 Results and Discussion
We report Ranking Loss for the four models in Ta-
ble 2. The worst performance (denoted by the high-
est ranking loss value) is obtained by PRank, fol-
lowed by matrix-space OLogReg with random ini-
tialization. Bag-of-words OLogReg obtains quite
good performance, and matrix-space OLogReg, ini-
tialized using the bag-of-words model performs the
best, showing statistically significant improvements
over the bag-of-words OLogReg model according to
a paired t-test. .
To see what the bag-of-word and matrix-space
models are learning we performed inference on a
few examples. In Table 3 we show the sentiment
scores of the best performing bag-of-words OLo-
gReg model and the best performing model based
Phrase Matrix-space Bag-of-words
OLogReg+BowInit OLogReg
not -0.83 -0.42
very 0.23 0.04
good 2.81 1.51
very good 3.53 1.55
not good -0.16 1.09
not very good 0.66 1.13
bad -1.67 -1.42
very bad -2.01 -1.38
not bad -0.54 -1.85
not very bad -1.36 -1.80
Table 3: Phrase and the sentiment scores of the phrase for
2 models Matrix-space OLogReg+BowInit and Bag-of-
words OLogReg respectively. Notice that relative rank-
ing order what matters
on matrices Matrix-space OLogReg+BowInit. By
sentiment score, we mean equation (1) of Bag-of-
words OLogReg and equation (2) of Matrix-space
OLogReg+BowInit.
Here we choose two popular adjectives like
?good? and ?bad? that appeared in the training data,
and examine the effect of applying the intensifier
?very? on the sentiment score. As we can see,
the matrix-space model learns a matrix that inten-
sifies both ?bad? and ?good? in the correct sentiment
scale, i.e., ?(good) < ?(very good) and ?(bad) <
?(very bad), while the bag-of-words model gets the
sentiment of ?very bad? wrong: it is more positive
than ?bad?. We also looked at the effect of combin-
ing ?not? with these adjectives. The matrix-space
model correctly encodes the effect of the negator
for both positive and negative adjectives, such that
?(not good) < ?(good) and ?(bad) < ?(not bad).
For the interesting case of applying a negator to a
phrase with an intensifier, ?(not good) should be
less than ?(not very good) and ?(not very bad)
should be less than ?(not bad).9 As shown in Ta-
ble 3, these are predicted correctly by the matrix-
space model, which the matrix-space model gets
right, but the bag-of-words model misses in the case
of ?bad?.
Also notice that since in the matrix-space model
9See the detailed discussion in Taboada et al (2011) and Liu
and Seneff (2009).
179
each word is represented as a function, more specif-
ically a linear operator, and the function composi-
tion defined as matrix multiplication, we can think
of ?not very? being an operator itself, that is a com-
position of operator ?not? and operator ?very?.
5 Related Work
Sentiment Analysis. There has been a lot of
research in determining the sentiment of words
and constructing polarity dictionaries (Hatzivas-
siloglou and McKeown, 1997; Wiebe, 2000; Rao
and Ravichandran, 2009; Mohammad et al, 2009;
Velikovich et al, 2010). Some recent work is try-
ing to identify the degree of sentiment of adjectives
and adverbs from text using co-occurrence statistics.
Work by Taboada et. al (2011) and Liu and Sen-
eff (2009), suggest ways of computing the sentiment
of adjectives from data, and computing the effect
of combining adjective with adverb as multiplica-
tive effect and combining adjective with negation as
additive effect. However these models require the
knowledge of a part of speech of given words and
the list of negators (since the negator is an adjective
as well). In our work we propose a single unified
model for handling all words of any part of speech.
On the other hand, there has been some research
in trying to model compositional effects for senti-
ment at the phrase- and sentence-level. Choi and
Cardie (2008) hand-code compositional rules in or-
der to model compositional effects of combining dif-
ferent words in the phrase. The hand-coded rules
are based on domain knowledge and used to learn
the effects of combining words in the phrase. An-
other recent work that tries to model the compo-
sitional semantics of combining different words is
Nakagawa et. al. (2010), which proposes a model
that learns the effects of combining different words
using phrase/sentence dependency parse trees and an
initial polarity dictionary. They present a learning
method that employs hidden variables for sentiment
classification: given the polarity of a sentence and
the a priori polarities of its words, they learn how
to model the interactions between words with head-
modifier relations in the dependency tree.
Some of the previous work looked at MPQA
phrase-level classification. Wilson et al (2004) tack-
les the problem of classifying clauses according to
their subjective strength but not polarity; Wilson et
al. (2005) classifies phrases according to their po-
larity/sentiment but not strength. Our task is differ-
ent: we classify phrases according to a single ordinal
scale that combines both polarity and strength.
Task of predicting document-level star ratings was
considered in (Pang and Lee, 2005; Goldberg and
Zhu, 2006). In the current work we look at fine-
grained sentiment analysis, more specifically we
study word representations for use in true compo-
sitional semantic settings.
Distributional Semantics and Compositional-
ity. Research in the area of distributional seman-
tics in NLP and Cognitive Science has looked at
different word representations and different ways of
combining words. Mitchell and Lapata (2010) pro-
pose a framework for vector-based semantic com-
position. They define composition as an additive
or multiplicative function of two vectors and show
that compositional approaches generally outperform
non-compositional approaches that treat the phrase
as the union of single lexical items.
Work by Baroni and Zamparelli (2010) models
nouns as vectors in some semantic space and ad-
jectives as matrices. It shows that modeling adjec-
tives as linear transformations and applying those
linear transformations to nouns results in final vec-
tors for adjective-noun compositions that are close
in semantic space to other similar phrases. The
authors argue that modeling adjectives as a linear
transformation is a better idea than using additive
vector-space models. In this work, a separate ma-
trix for each adjective is learned using the Par-
tial Least Squares method in a completely unsuper-
vised way. The recent paper by Rudolph and Gies-
brecht (2010), described in the introduction, argues
for multiplicative matrix-space models. In contrast
to other work in this area, our work is concerned
with a specific dimension of word meaning ? sen-
timent. Our techniques, however, are quite general
and should be applicable to other problems in lexical
semantics.
6 Conclusions and Future work
In the current work we present a novel matrix-space
model for ordinal scale sentiment prediction and an
algorithm for learning such a model. The proposed
180
model learns a matrix for each word; the composi-
tion of words is modeled as iterated matrix multi-
plication. The matrix-space framework with iterated
matrix multiplication defines an elegant framework
for modeling composition; it is also quite general.
We use the matrix-space framework in the context
of sentiment prediction, a domain where interesting
compositional effects can be observed. The main fo-
cus of this work was to study word representations
(represent as a single weight vs. as a matrix) for use
in true compositional semantic settings. One of the
benefits of the proposed approach is that by learn-
ing matrices for words, the model can handle unseen
word compositions (e.g. unseen bigrams) when the
unigrams involved have been seen.
However, it is not trivial to learn a matrix-space
model. Since the final optimization problem is non-
convex, the initialization has to be done carefully.
Here the weights learned in bag-of-words model
come to rescue and provide good initial point for op-
timization procedure. The final model outperforms
the bag-of-words based model, which suggests that
this research direction is very promising.
Though in our model the order of composition is
the same as the word order, we believe that a linguis-
tically informed order of composition can give us
further performance gains. For example, one can use
the output of a dependency parser to guide the order
of composition, similar to Nakagawa et al (2010).
Another possibility for improvement is to use the in-
formation about the scope of negation. In the current
work we assume the scope of negation to be the ex-
pression following the negation; in reality, however,
determining the scope of negation is a complex lin-
guistic phenomenon (Moilanen and Pulman, 2007).
So the proposed model can benefit from identify-
ing the scope of negation, similar to (Councill et al,
2010).
Also we plan to consider other ways to initialize
the matrix-space model. One interesting direction to
explore might be to use non-negative matrix factor-
ization (Lee and Seung, 2001), co-clustering tech-
niques (Dhillon, 2001) to better initialize words that
share similar contexts. The other possible direction
is to use existing sentiment lexicons and employ-
ing a ?curriculum learning? strategy (Bengio et al,
2009; Kumar et al, 2010) for our learning problem.
Acknowledgments
This work was supported in part by National Science
Foundation Grants BCS-0904822, BCS-0624277,
IIS-0968450; and by a gift from Google. We thank
the anonymous reviewers, and David Bindel, Nikos
Karampatziakis, Lillian Lee and Cornell NLP group
for useful suggestions and insightful discussions.
References
Marco Baroni and Roberto Zamparelli. 2010. Nouns
are vectors, adjectives are matrices: representing
adjective-noun constructions in semantic space. In
Proceedings of the 2010 Conference on Empirical
Methods in Natural Language Processing, EMNLP
?10, pages 1183?1193, Morristown, NJ, USA. Asso-
ciation for Computational Linguistics.
Yoshua Bengio, Je?ro?me Louradour, Ronan Collobert, and
Jason Weston. 2009. Curriculum learning. In Pro-
ceedings of the 26th Annual International Conference
on Machine Learning, ICML ?09. ACM.
R. H. Byrd, P. Lu, and J. Nocedal. 1995. A limited
memory algorithm for bound constrained optimiza-
tion. SIAM Journal on Scientific and Statistical Com-
puting, pages 1190?1208.
Yejin Choi and Claire Cardie. 2008. Learning with com-
positional semantics as structural inference for subsen-
tential sentiment analysis. In Empirical Methods in
Natural Language Processing (EMNLP).
Isaac G. Councill, Ryan McDonald, and Leonid Ve-
likovich. 2010. What?s great and what?s not: learn-
ing to classify the scope of negation for improved sen-
timent analysis. In Proceedings of the Workshop on
Negation and Speculation in Natural Language Pro-
cessing, NeSp-NLP ?10, Stroudsburg, PA, USA. As-
sociation for Computational Linguistics.
Koby Crammer and Yoram Singer. 2001. Pranking with
ranking. In Advances in Neural Information Process-
ing Systems 14, pages 641?647. MIT Press.
Marie-Catherine de Marneffe, Christopher D. Manning,
and Christopher Potts. 2010. Was it good? It was
provocative. learning the meaning of scalar adjectives.
In Proceedings of the 48th Annual Meeting of the Asso-
ciation for Computational Linguistics, Uppsala, Swe-
den, July 11?16. ACL.
I. S. Dhillon. 2001. Co-clustering documents and words
using bipartite spectral graph partitioning. In KDD.
Andrew B. Goldberg and Jerry Zhu. 2006. Seeing
stars when there aren?t many stars: Graph-based semi-
supervised learning for sentiment categorization. In
HLT-NAACL Workshop on Textgraphs: Graph-based
Algorithms for Natural Language Processing.
181
James W. Hardin and Joseph Hilbe. 2007. Generalized
Linear Models and Extensions. Stata Press.
Vasileios Hatzivassiloglou and Kathleen R. McKeown.
1997. Predicting the semantic orientation of adjec-
tives. In EACL, pages 174?181.
Alistair Kennedy and Diana Inkpen. 2006. Sentiment
classification of movie reviews using contextual va-
lence shifters. Computational Intelligence, 22(2, Spe-
cial Issue on Sentiment Analysis)):110?125.
M. Pawan Kumar, Benjamin Packer, and Daphne Koller.
2010. Self-paced learning for latent variable models.
In Advances in Neural Information Processing Sys-
tems 23. NIPS.
D. Lee and H. Seung. 2001. Algorithms for non-negative
matrix factorization. In NIPS.
Jingjing Liu and Stephanie Seneff. 2009. Review sen-
timent scoring via a parse-and-paraphrase paradigm.
In Proceedings of the 2009 Conference on Empiri-
cal Methods in Natural Language Processing, pages
161?169, Singapore, August. Association for Compu-
tational Linguistics.
Jeff Mitchell and Mirella Lapata. 2010. Composition in
distributional models of semantics. Cognitive Science,
34(8):1388?1429.
Saif Mohammad, Cody Dunne, and Bonnie Dorr. 2009.
Generating high-coverage semantic orientation lexi-
cons from overtly marked words and a thesaurus.
In Proceedings of the 2009 Conference on Empiri-
cal Methods in Natural Language Processing, pages
599?608, Singapore, August. Association for Compu-
tational Linguistics.
Karo Moilanen and Stephen Pulman. 2007. Sentiment
composition. In Proceedings of Recent Advances in
Natural Language Processing (RANLP 2007), pages
378?382, September 27-29.
Tetsuji Nakagawa, Kentaro Inui, and Sadao Kurohashi.
2010. Dependency tree-based sentiment classification
using crfs with hidden variables. In Conference of the
North American Chapter of the Association for Com-
putational Linguistics (NAACL).
Bo Pang and Lillian Lee. 2005. Seeing stars: Exploiting
class relationships for sentiment categorization with
respect to rating scales. In Proceedings of the ACL,
pages 115?124.
Bo Pang and Lillian Lee. 2008. Opinion mining and
sentiment analysis. Foundations and Trends in Infor-
mation Retrieval, 2(1-2):1?135.
K. B. Petersen and M. S. Pedersen. ?2008?. The Matrix
Cookbook. ?Technical University of Denmark?, ?oct?.
?Version 20081110?.
Livia Polanyi and Annie Zaenen. 2004. Contextual
lexical valence shifters. In Proceedings of the AAAI
Spring Symposium on Exploring Attitude and Affect in
Text: Theories and Applications.
Delip Rao and Deepak Ravichandran. 2009. Semi-
supervised polarity lexicon induction. In Proceedings
of the 12th Conference of the European Chapter of the
ACL (EACL 2009), pages 675?682, Athens, Greece,
March. Association for Computational Linguistics.
Sebastian Rudolph and Eugenie Giesbrecht. 2010. Com-
positional matrix-space models of language. In Pro-
ceedings of the 48th Annual Meeting of the Association
for Computational Linguistics, ACL ?10, pages 907?
916, Morristown, NJ, USA. Association for Computa-
tional Linguistics.
Mostafa Shaikh, Helmut Prendinger, and Ishizuka Mit-
suru. 2007. Assessing sentiment of text by semantic
dependency and contextual valence analysis.
Maite Taboada, Julian Brooke, Milan Tofiloskiy, and
Kimberly Vollz. 2011). Lexicon-based methods for
sentiment analysis. In Computational Linguistics.
Leonid Velikovich, Sasha Blair-Goldensohn, Kerry Han-
nan, and RyanMcDonald. 2010. The viability of web-
derived polarity lexicons. In Human Language Tech-
nologies: The 2010 Annual Conference of the North
American Chapter of the Association for Computa-
tional Linguistics, pages 777?785, Los Angeles, Cal-
ifornia, June. Association for Computational Linguis-
tics.
Janyce Wiebe, Theresa Wilson, and Claire Cardie. 2005.
Annotating expressions of opinions and emotions in
language. Language Resources and Evaluation (for-
merly Computers and the Humanities), 39(2/3):164?
210.
Janyce M. Wiebe. 2000. Learning subjective adjectives
from corpora. In In AAAI, pages 735?740.
Theresa Wilson, Janyce Wiebe, and Rebecca Hwa. 2004.
Just how mad are you? In AAAI. AAAI.
Theresa Wilson, Janyce Wiebe, and Paul Hoffmann.
2005. Recognizing contextual polarity in phrase-level
sentiment analysis. In Empirical Methods in Natural
Language Processing (EMNLP).
182
Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural
Language Learning, pages 1335?1345, Jeju Island, Korea, 12?14 July 2012. c?2012 Association for Computational Linguistics
Extracting Opinion Expressions with
semi-Markov Conditional Random Fields
Bishan Yang
Department of Computer Science
Cornell University
bishan@cs.cornell.edu
Claire Cardie
Department of Computer Science
Cornell University
cardie@cs.cornell.edu
Abstract
Extracting opinion expressions from text is
usually formulated as a token-level sequence
labeling task tackled using Conditional Ran-
dom Fields (CRFs). CRFs, however, do not
readily model potentially useful segment-level
information like syntactic constituent struc-
ture. Thus, we propose a semi-CRF-based ap-
proach to the task that can perform sequence
labeling at the segment level. We extend the
original semi-CRF model (Sarawagi and Co-
hen, 2004) to allow the modeling of arbitrar-
ily long expressions while accounting for their
likely syntactic structure when modeling seg-
ment boundaries. We evaluate performance on
two opinion extraction tasks, and, in contrast
to previous sequence labeling approaches to
the task, explore the usefulness of segment-
level syntactic parse features. Experimental
results demonstrate that our approach outper-
forms state-of-the-art methods for both opin-
ion expression tasks.
1 Introduction
Accurate opinion expression identification is crucial
for tasks that benefit from fine-grained opinion anal-
ysis (Wiebe et al 2005): e.g., it is a first step
in characterizing the sentiment and intensity of the
opinion; it provides a textual anchor for identifying
the opinion holder and the target or topic of an opin-
ion; and these, in turn, form the basis of opinion-
oriented question answering and opinion summa-
rization systems. In this paper, we focus on opin-
ion expressions as defined in Wiebe et al(2005) ?
subjective expressions that denote emotions, senti-
ment, beliefs, opinions, judgments, or other private
states (Quirk et al 1985) in text. These include
direct subjective expressions (DSEs): explicit men-
tions of private states or speech events expressing
private states; and expressive subjective expressions
(ESEs): expressions that indicate sentiment, emo-
tion, etc. without explicitly conveying them. Follow-
ing are two example sentences labeled with DSEs
and ESEs.
(1) The International Committee of the
Red Cross, [as usual][ESE], [has refused to
make any statements][DSE].
(2) The Chief Minister [said][DSE] that [the
demon they have reared will eat up their
own vitals][ESE].
As a type of information extraction task, opinion
expression extraction has been successfully tackled
in the past via sequence tagging methods: Choi et
al. (2006) and Breck et al(2007), for example, ap-
ply conditional random fields (CRFs) (Lafferty et
al., 2001) using sophisticated token-level features.
In token-level sequence labeling, labels are assigned
to single tokens, and the label of each token depends
on the current token and the label of the previous to-
ken (we consider the usual first-order assumption).
Segment-based features ? features that describe a
set of related contiguous tokens, e.g., a phrase or
constituent ? might provide critical information for
identifying opinion expressions; they cannot, how-
ever, be readily and naturally represented in the CRF
model.
1335
Our goal in this work is to extract opinion ex-
pressions at the segment level with semi-Markov
conditional random fields (semi-CRFs). Semi-CRFs
(Sarawagi and Cohen, 2004) are more powerful than
CRFs in that they allow one to construct features
to capture characteristics of the subsequences of a
sentence. They are defined on semi-Markov chains
where labels are attached to segments instead of
tokens and label dependencies are modeled at the
segment-level. Previous work has shown that semi-
CRFs outperform CRFs on named entity recog-
nition (NER) tasks (Sarawagi and Cohen, 2004;
Okanohara et al 2006). However, to the best of
our knowledge, semi-CRF techniques have not been
investigated for opinion expression extraction.
The contribution of this paper is a semi-CRF-
based approach for opinion expression extraction
that leverages parsing information to provide better
modeling of opinion expressions. Specifically, pos-
sible segmentations are generated by taking into ac-
count likely syntactic structure during learning and
inference. As a result, arbitrarily long expressions
can be modeled and their boundaries can be influ-
enced by probable syntactic structure. We also ex-
plore the impact of syntactic features for extracting
opinion expressions.
We evaluate our model on two opinion extrac-
tion tasks: identifying direct subjective expres-
sions (DSEs) and expressive subjective expressions
(ESEs). Experimental results show that our ap-
proach outperforms the state-of-the-art approach for
the task by a large margin. We also identify useful
syntactic features for the task.
2 Related Work
Previous research to extract direct subjective ex-
pressions exists, but is mainly focused on single-
word expressions (Wiebe et al 2005; Wilson et
al., 2005; Munson et al 2005). More recent stud-
ies tackle opinion expression extraction at the ex-
pression level. Breck et al(2007) formulate the
problem as a token-level sequence labeling prob-
lem; their CRF-based approach was shown to sig-
nificantly outperform two subjectivity-clue-based
baselines. Others extend the token-level approach
to jointly identify opinion holders (Choi et al
2006), and to determine the polarity and inten-
sity of the opinion expressions (Choi and Cardie,
2010). Reranking the output of a simple sequence
labeler has been shown to further improve the ex-
traction of opinion expressions (Johansson and Mos-
chitti, 2010; Johansson and Moschitti, 2011); impor-
tantly, their reranking approach relied on features
that encoded syntactic structure. All of the above
approaches, however, are based on token-level se-
quence labeling, which ignores potentially useful
phrase-level information.
Semi-CRFs (Sarawagi and Cohen, 2004) are gen-
eral CRFs that relax the Markovian assumptions to
allow sequence labeling at the segment level. Pre-
vious work has shown that semi-CRFs are supe-
rior to CRFs for NER and Chinese word segmen-
tation (Sarawagi and Cohen, 2004; Okanohara et al
2006; Andrew, 2006). The task of opinion expres-
sion extraction is known to be harder than traditional
NER since subjective expressions exhibit substantial
lexical variation and their recognition requires more
attention to linguistic structure.
Parsing has been leveraged to improve perfor-
mance for numerous natural language tasks. In opin-
ion mining, numerous studies have shown that syn-
tactic parsing features are very helpful for opinion
analysis. A lot of work uses syntactic features to
identify opinion holders and opinion topics (Bethard
et al 2005; Kim and Hovy, 2006; Kobayashi et al
2007; Joshi and Carolyn, 2009; Wu et al 2009;
Choi et al 2005). Jakob et al(2010) recently
employed dependency path features for the extrac-
tion of opinion targets. Johansson and Moschitti
(2010; Johansson and Moschitti (2011) also success-
fully employed syntactic features that indicate de-
pendency relations between opinion expressions for
the task of opinion expression extraction. However,
as their approach is based on the output of a se-
quence labeler, these features cannot be encoded to
help the learning of the sequence labeler.
3 Approach
We formulate the extraction of opinion expres-
sions as a sequence labeling problem. Unlike
previous sequence-labeling approaches to the task
(e.g., Breck et al(2007)), however, we aim to model
segment-level, rather than token-level, information.
As a result, we explore the use of semi-CRFs, which
1336
can assign labels to segments instead of tokens;
hence, features can be defined at the segment level.
For example, features like JX is a verb phraseK can
be easily encoded in the model. In the following
subsections, we first introduce standard semi-CRFs
and then describe our semi-CRF-based approach for
opinion expression extraction.
3.1 Semi-CRFs
In semi-CRFs, each observed sentence x is repre-
sented as a sequence of consecutive segments s =
?s1, ..., sn?, where si is a triple si = (ti, ui, yi), ti
denotes the start position of segment si, ui denotes
the end position, and yi denotes the label of the seg-
ment. Segments are restricted to have positive length
less than or equal to a maximum length of L that has
been seen in the corpus (1 ? ui ? ti + 1 ? L).
Features in semi-CRFs are defined at the seg-
ment level rather than the word level. The fea-
ture function g(i, x, s) is a function of x, the cur-
rent segment si, and the label yi?1 of the previ-
ous segment si?1 (we consider the usual first-order
Markovian assumption). It can also be written as
g(x, ti, ui, yi, yi?1). The conditional probability of
a segmentation s given a sequence x is defined as
p(s|x) =
1
Z(x)
exp
{
?
i
?
k
?kgk(i, x, s)
}
(1)
where
Z(x) =
?
s??S
exp
{
?
i
?
k
?kgk(i, x, s
?)
}
and the set S contains all possible segmentations ob-
tained from segment candidates with length ranging
from 1 to the maximum length L.
The correct segmentation s of a sentence
is defined as a sequence of entity segments
(i.e., the entities to be extracted) and non-
entity segments. For example, the correct
segmentation of sentence (2) in Section 1 is
?(The,NONE),(Chief,NONE),(Minister,NONE),
(said,DSE),(that,NONE),(the demon they have
reared will eat up their own vitals,ESE),(.,NONE)?.
Here, non-entity segments are represented as
unit-length segments.
3.2 Semi-CRF-based Approach for Opinion
Expression Extraction
In this section, we present an extended version of
semi-CRFs in which we can make use of parsing in-
formation in learning entity boundaries and labels
for opinion expression extraction.
As discussed in Section 3.1, the maximum entity
length L is fixed during training to generate segment
candidates in the standard semi-CRFs. In opinion
expression extraction, L is unbounded since opin-
ion expressions may be clauses or whole sentences,
which can be arbitrarily long. Thus, fixing an upper
bound on segment length based on the observed en-
tities may lead to an incorrect removal of segments
during inference. Also note that possible segment
candidates are generated based on the length con-
straint, which means any span of the text consisting
of no more than L words would be considered as
a possible segment. This would lead to the consid-
eration of implausible segments, e.g., ?The Chief?
in sentence (2) is an incorrect segment within the
multi-word expression ?The Chief Minister?.
To address these problems, we propose tech-
niques to incorporate parsing information into the
modeling of segments in semi-CRFs. More specifi-
cally, we construct segment units from the parse tree
of each sentence1, and then build up possible seg-
ment candidates based on those units. In the parse
tree, each leaf phrase or leaf word is considered to be
a segment unit. Each segment unit performs as the
smallest unit in the model (words within a segment
unit will be automatically assigned the same label).
The segment units are highlighted in rectangles in
the parse tree example in Figure 1. As the segment
units are not separable, we avoid implausible seg-
ments, which truncate multi-word expressions. For
example, ?both ridiculous and?, would not be con-
sidered a possible segment in our model.
To generate segment candidates for the model,
we consider meaningful combinations of consecu-
tive segment units. Intuitively, a sentence is made
up of several parts, and each has its own grammati-
cal role or meaning. We define the boundary of these
parts based on the parse tree structure. Specifically,
1We use the Stanford Parser http://nlp.stanford.
edu/software/lex-parser.shtml to generate the
parse trees.
1337
NP VP .
VBD SBAR
S
root
that
.
IN S
NP VP
PRP
I found
DT NNS
the statements
VBP ADJP
are DT JJ JJCC
both riddiculous and odd
Go
G1 G4
G6
 ridiculous and odd
Figure 1: A parse tree example. There are seven segment units in the sentence. The shaded regions correspond to
segment groups, where Gi represents the segment group starting from segment unit Ui.
we consider each segment unit to belong to a mean-
ingful group defined by the span of its parent node.
Two consecutive segment units are considered to be-
long to the same group if the subtrees rooted in their
parent nodes have the same rightmost child. For ex-
ample, in Figure 1, segment units ?are? and ?both
ridiculous and odd? belong to the same group, while
?I? and ?found? belong to different groups.
Algorithm 1 Construction of segment candidates
Input: A training sentence x
Output: A set of segment candidates S
1: Obtain the segment units U = (U1, ..., Um) by
preorder traversal of the parse tree T , each Ui
corresponds to a node in T
2: for i = 1 to m do
3: j ? i? 1
4: while j < m? 1 and
commonGroup(Ui, ..., Uj+1) do
5: j ? j + 1
6: for k = i to j do
7: for t = 0 to j ? k do
8: s? segment(Uk, ..., Uk+t)
9: S ? S ? s
10: Return S
Following this idea, we generate possible seg-
ment candidates by Algorithm 1. Starting from
each segment unit Ui, we first find the rightmost
segment unit Uj that belongs to the same group
as Ui. Function commonGroup(Ui, ..., Uj) re-
turns True if Ui, ..., Uj are within the same group
(the parent nodes of Ui,...,Uj have the same right-
most child in their subtrees), otherwise it returns
False. Then we enumerate all possible combina-
tions of segment units Ui, ..., Uk where i ? k ?
j. segment(Ui, ..., Uj) denotes the segment ob-
tained by concatenating words in the consecutive
segment units Ui,...,Uj . This way, segment can-
didates are generated without constraints on length
and are meaningful for learning entity boundaries.
Based on the generated segment candidates, the
correct segmentation for each training sentence can
be obtained as follows. For opinion expressions
that do not match any segment candidate, we break
them down into smaller segments using a greedy
matching process. Starting from the start position
of the expression, we search for the longest candi-
date that is contained in the expression, add it to
the correct segmentation for the sentence, set the
start position to be the next position, and repeat the
process. Using this process, the correct segmen-
tation of sentence (2) would be s = ?(The Chief
Minister,NONE),(said,DSE),(that,NONE),(the de-
mon they have reared,ESE), (will eat up their own
vitals,ESE),(.)?. Note that here non-entities corre-
spond to segment units instead of single-word seg-
ments in the original semi-CRF model.2
After obtaining the set of possible segment candi-
dates and the correct segmentation s for each train-
ing sentence, the semi-CRF model can be trained.
The goal of learning is to find the optimal parameter
? by maximizing log-likelihood. We use the limited-
2There are cases where words within a segment unit have
different labels. This may be due to errors by the human anno-
tators or the errors in the parser. In such cases, we consider each
word within the segment unit as a segment.
1338
memory BFGS algorithm (Liu and Nocedal, 1989)
for optimization in our implementation, where the
gradient of the log-likelihood L (corresponding to
one instance x) is computed:
?L
??k
=
?
i
gk(x, ti, ui, yi, yi?1)
?
?
s??S
?
y,y?
?
j
gk(x, t
?
j , u
?
j , y, y
?)p(y, y?|x)
(2)
where S is all possible segmentations consisting of
the generated segment candidates, p(y, y?|x) is the
probability of having label y for the current segment
s?j (with boundary (t
?
j , u
?
j)) and label y
? for the pre-
vious segment s?j?1.
We use a forward-backward algorithm to com-
pute the marginal distribution p(y, y?|x) and the nor-
malization factor Z(x) efficiently. For inference we
seek the best segmentation s? = argmaxs p(s|x),
where p(s|x) is defined by Equation 1. We im-
plement efficient inference using an extension of
Viterbi algorithm to segments. In particular, define
V (j, y) as the largest unnormalized probability of
p(s1:j |x) with label y at the ending position j. Then
we have
V (j, y) = max
(i,j)?s:,j
max
y?
?(x, i, j, y, y?)V (i? 1, y?)
where
?(x, i, j, y, y?) = exp
{
?
k
?kgk(x, i, j, y, y
?)
}
and s:,j denotes the set of the generated segment
candidates ending at position j. The best segmen-
tation can be obtained from tracing the path of
maxy V (n, y).
3.3 Features
Here we described the features used in our model.
Very generally, we include CRF-style features that
are segment-level extensions of the token-level fea-
tures. We also include new segment-level features
that can be naturally represented in semi-CRFs but
not CRFs.
For CRF-style features, we consider the string
representation of the current word, its part-of-
speech, and a dictionary-derived feature, which is
based on a subjectivity lexicon provided by Wilson
et al(2005). The lexicon consists of a set of words
that can act as strong or weak cues to subjectivity.
If the current word appears as an entry in the lexi-
con, then a feature strong or weak will be fired if the
entry is of that strength. These features have been
successfully employed in previous work (Breck et
al., 2007). To employ them in our model, we sim-
ply extend the feature definition to the segment level.
For example, a token-level feature Jx is great K will
be extended to a segment-level feature Js contains
great K.
Previous work on semi-CRFs has explored fea-
tures such as the length of the segment, the position
of the segment in the current segmentation (at the be-
ginning or at the end), indicators for the start word
and end word within the segment, and indicators for
words before and after the segment. These features
have been shown useful for the task of NE recogni-
tion (Sarawagi and Cohen, 2004; Okanohara et al
2006). However, we only found the position of the
segment to be helpful for the extraction of opinion
expressions, probably due to the lack of patterns in
the length distribution and word choices of opinion
expressions.
Besides the above features, we design new
segment-level syntactic features to capture the syn-
tactic patterns of opinion expressions. Syntactic pat-
terns are often used to identify useful information in
information extraction tasks. In our task, we found
that the majority of opinion expressions involve verb
phrases.3 For example, ?was encouraged?, ?ex-
pressed goodwill?, ?cannot accept? are all within a
VP constituent. To capture such structural prefer-
ences, we define several syntax-based parse features
for VP-related constituents.4
Let VPROOT denote a VP constituent whose par-
ent node is not VP, and let VPLEAF denote a VP
constituent whose children nodes are non-VP. De-
note the head of VPLEAF as the predicate, and its
next segment unit as the argument. If a segment con-
sists of words in the VP nodes visited by the preorder
3The percentages of opinion expressions involving
VP/NP/PP are 64.13%/18.43%/5.92% for DSEs and
43.22%/24.99%/11.77% for ESEs in the data set we used.
4We also conducted experiments with NP and PP-related
features, and could not find any performance improvement for
the tasks.
1339
traversal from a VPROOT to a VPLEAF, then we re-
fer to it as a verb-cluster segment. If a segment con-
sists of a verb cluster and the argument in VPLEAF,
we consider it as a VP segment. The following fea-
tures are defined for verb-cluster segments and VP
segments.
VPcluster: Indicates whether or not the segment
matches the verb-cluster structure.
VPpred: A feature of the syntactic category and
the word of the head of VPLEAF. The head of
VPLEAF is the predicate of the verb phrase, which
may encode some intention of opinions in the verb
phrase. For example, if ?warned? is the head of
VPLEAF rather than ?informed?, the chance of the
segment being an opinion expression increases.
VParg: A feature of the syntactic category and
the head word of the argument in VPLEAF. For ex-
ample, the noun phrase ?a negative stand? is the ar-
gument of the predicate ?take? in the verb phrase
?take a negative stand?. The argument in the verb
phrase (could be a noun phrase, adjectival phrase or
prepositional phrase) may convey some relevant in-
formation for identifying opinion expressions.
VPsubj: Whether the verb clusters or the argu-
ment in the segment contains an entry from the sub-
jectivity lexicon. For example, the word ?negative?
is in the lexicon, so the segment ?take a negative
stand? has a feature ISVPSUBJ.
4 Experiments
For evaluation, we use the MPQA 1.2 corpus (Wiebe
et al 2005)5, a widely used data set for fine-grained
opinion analysis. It contains 535 news articles, a to-
tal of 11,114 sentences with subjectivity-related an-
notations at the phrase level. We focus on the task
of extracting two types of opinion expressions: di-
rect subjective expressions (DSEs) and expressive
subjective expressions (ESEs). Table 1 shows some
statistics of the corpus. As in prior research that
uses the corpus, we set aside the standard 135 docu-
ments as a development set and use 400 documents
as the evaluation set. All experiments employ 10-
fold cross validation on the evaluation set, and the
average over all runs is reported.
5Available at http://www.cs.pitt.edu/mpqa/.
DSEs ESEs
Sentences with opinions(%) 55.89 57.93
TotalNum 9746 11730
MaxLength 15 40
Length ? 1 (%) 43.38 71.65
Length ? 4 (%) 9.44 35.01
Table 1: Statistics of opinion expressions in the MPQA
Corpus.
4.1 Evaluation Metrics
We use precision, recall, and F-measure to evalu-
ate the quality of the model. Precision is defined
as |C?P ||P | and recall, as
|C?P |
|C| , where C and P are
the sets of correct and predicted expression spans,
respectively. F-measure is computed as 2PRP+R . Be-
cause the boundaries of opinion expressions are hard
to define even for human annotators (Wiebe et al
2005), previous research mainly focused on soft pre-
cision and recall measures for performance evalu-
ation. Breck et al(2007) introduced an overlap
measure, which considers a predicted expression to
be correct if it overlaps with a correct expression.
We refer to this metric as Binary Overlap. Johans-
son and Moschitti (2010) provides a stricter measure
that computes the proportion of overlapping spans:
if a correct expression s overlaps with a predicted
expression s?, the overlap contributes value |s?s
?|
|s?| to
|C ? P | instead of value 1. We refer to this metric
as Proportional Overlap. To compare with previous
work, we present our results according to both met-
rics.
4.2 Baseline Methods
As a baseline, we use the token-level CRF-based ap-
proach of Breck et al(2007) applied to the MPQA
dataset. We employ a very similar, but not iden-
tical set of features: indicators for specific words
at the current location and neighboring words in a
[?4,+4] window, part-of-speech features, and opin-
ion lexicon features for tokens that are contained in
the subjectivity lexicon (see Section 3.3). We do not
include WordNet, Levin?s verb categorization, and
FrameNet features.
We also include two variants of standard CRFs as
baselines: segment-CRF and syntactic-CRF. They
incorporate segmentation information into standard
CRFs without modifying the Markovian assump-
1340
DSE Extraction ESE Extraction
Method Precision Recall F-measure Precision Recall F-measure
CRF 82.83 49.38 61.87 78.56 43.57 56.05
segment-CRF 82.52 51.48 63.41 78.90 44.46 56.88
syntactic-CRF 82.48 49.09 61.55 78.41 43.39 55.95
semi-CRF 66.67 74.13 70.20 71.21 57.41 63.57
new-semi-CRF 67.72?? 74.33 70.87? 73.57??? 57.63 64.74??
semi-CRF(w/ syn) 64.86 74.10 69.17 70.68 56.61 62.87
new-semi-CRF(w/ syn) 70.12??? 74.74? 72.36??? 73.61??? 59.27??? 65.67???
Table 2: Results for extracting opinion expressions with Binary-Overlap metric. (w/ syn) indicates the inclusion of
syntactic parse features VPpre, VParg and VPsubj. Results of new-semi-CRF that are statistically significantly greater
than semi-CRF according to a two-tailed t-test are indicated with ?(p < 0.1), ??(p < 0.05), ???(p < 0.005). T-test
results are also shown for new-semi-CRF(w/ syn) versus semi-CRF(w/ syn).
DSE Extraction ESE Extraction
Method Precision Recall F-measure Precision Recall F-measure
CRF 77.91 46.45 58.20 67.72 37.55 48.31
segment-CRF 77.86 48.58 59.83 68.03 38.34 49.04
syntactic-CRF 77.73 46.27 58.01 67.80 37.60 48.37
semi-CRF 60.38 68.34 64.11 57.30 46.20 51.16
new-semi-CRF 62.50?? 68.59? 65.41? 61.69??? 47.44?? 53.63???
semi-CRF(w/ syn) 58.69 67.80 62.92 57.09 45.63 50.72
new-semi-CRF(w/ syn) 65.52??? 68.91??? 67.17??? 61.66??? 48.77??? 54.47???
Table 3: Results for extracting opinion expressions with Proportional-Overlap metric. Notation is the same as above.
tion. Segment-CRF treats segment units obtained
from the parser as word tokens. For example, in
Figure 1, the segment units the statement and both
ridiculous and odd will be treated as word tokens.
Syntactic-CRF encodes segment-level syntactic in-
formation in a standard token-level CRF as input
features. We consider the VP-related segment fea-
tures introduced in Section 3.3. VPPRE and VPARG
are added to the head word of the corresponding verb
phrase, and VPSUBJ and VPCLUSTER are added to
each token within the corresponding segment.
Another baseline method is the original semi-
CRF model (Sarawagi and Cohen, 2004). To the
best of our knowledge, our work is the first to ex-
plore the use of semi-CRFs on the extraction of
opinion expressions. They are considered to be more
powerful than CRFs since they allow information to
be represented at the expression level. The model
requires an input of the maximum entity length. We
set it to 15 for DSE and 40 for ESE. For segment fea-
tures, we used the same features as in our approach
(see Section 3.3).
4.3 Results
Table 2 and Table 3 show the results of DSE and
ESE extraction using two different metrics. The
standard token-based CRF baseline of Breck et al
(2007) is labeled CRF; the original semi-CRF base-
line is labeled semi-CRF; and our extended semi-
CRF approach is labeled new-semi-CRF. For semi-
CRF and new-semi-CRF, the results were obtained
using two different settings of features: the basic
feature set includes features described in Section 3.3
excluding the segment-level syntactic features. In
the second feature setting (labeled as w/ syn in the
tables), we further augment the basic features with
the syntactic parse features.
Using the basic features, we observe that
semi-CRF-based approaches significantly outper-
form CRF and its two variants segment-CRF and
syntactic-CRF in F-Measure on both DSE and ESE
extraction, and new-semi-CRF achieves the best re-
sults. By simply incorporating the segmentation
prior into the standard CRF, segment-CRF achieves
a slight improvement over standard CRF, but the
results are still worse than those of semi-CRF
and new-semi-CRF. However, adding segment-level
1341
DSE Extraction ESE Extraction
Feature set Precision Recall F-measure Precision Recall F-measure
Basic 67.72 74.33 70.87 73.57 57.63 64.74
Basic+VPpre 70.88 71.44 71.16 73.20 58.20 64.85
Basic+VParg 70.12 74.03 72.02 73.05 58.20 64.79
Basic+VPcluster 70.08 72.94 71.48 73.06 58.45 64.94
Basic+VPsubj 70.04 72.34 71.17 73.31 58.53 65.09
Basic+VPpre+VPsubj 70.91 72.54 71.72 73.61 58.29 65.07
Basic+VParg+VPsubj 70.45 73.53 71.96 74.45 57.80 65.07
Basic+VPpre+VParg+VPsubj 70.12 74.74 72.36 73.61 59.27 65.67
Basic+VPcluster+VPpre+VParg+VPsubj 70.91 72.54 71.72 72.84 58.45 64.86
Table 4: Effect of syntactic features on extracting opinion expressions with Binary-Overlap metric
syntactic features into standard CRF yields slightly
reduced performance. This is not surprising as en-
coding segment-level information into the token-
level CRF is not natural. These experiments in-
dicate that simply encoding segmentation informa-
tion into standard CRF cannot result in large per-
formance gains. The promising F-measure results
obtained by semi-CRF and new-semi-CRF confirm
that relaxing the Markovian assumption on segments
leads to better modeling of opinion expressions. We
can also see that new-semi-CRF consistently outper-
forms the original semi-CRF model. This further
confirms the benefit of taking into account syntactic
parsing information in modeling segments. In Ta-
ble 3, we observe the same general results trend as
in Table 2. The scores are generally lower since the
metric Proportional Overlap is stricter than Binary
Overlap.
We also study the impact of syntactic parse fea-
tures on the semi-Markov CRF models. Here we
consider the combination of VPPRE, VPARG and
VPSUBJ since they turned out to be the most help-
ful features for our tasks. Interestingly, we found
that after incorporating the syntactic parse features,
performance decreases on semi-CRF. This indicates
that syntactic information does not help if learning
and inference take place on segment candidates gen-
erated without accounting for parse information. In
contrast, our approach incorporates syntactic pars-
ing information in modeling segments and meaning-
ful segmentations. We can see in Tables 2 and 3
that adding syntactic features successfully boosts the
performance of our approach.
To further explore the effect of the syntactic fea-
tures, we include the results of our model with dif-
ferent configurations of syntactic features in Table 4
(here we focus on the Binary Overlap metric as
the results with Proportional Overlap demonstrate
a similar conclusion). We can see that using the ba-
sic features and the combination of VPPRE, VPARG
and VPSUBJ yields the best results for both DSE
and ESE extraction. For DSE extraction, combin-
ing these three features improves the precision no-
ticeably from 67.72% to 70.12% while the recall
slightly improves. This indicates that VP-related
structural information is very helpful for modeling
segments as DSEs. However, this trend is not so
clear for ESE extraction. This may be due to the fact
that DSEs often involve verb phrases while ESEs are
represented via a variety of syntactic structures.
Comparison with previous work. In Table 5, we
compare our results to the previous work on opinion
expression extraction (here we also focus on the Bi-
nary Overlap metric due to the similar trend demon-
strated by the Proportional Overlap metric). Breck
et al(2007) presents the state-of-the-art sequence
labeling approach on the tasks of DSE and ESE ex-
traction. Their best results are shown as Breck et
al. Baseline in the table. Johansson and Mos-
chitti (2010) use a reranking technique on the best
k outputs of a sequence labeler to further improve
their sequence labeling results on the task of ex-
tracting DSEs, ESEs and OSEs (Objective Speech
Events) (we don?t consider OSEs here). Results
using our re-implementation of their approach us-
ing SVM struct (Tsochantaridis et al 2004) on the
output of CRF are labeled CRF+Reranking Base-
line in the table. We use the same features and
1342
parameter settings as in their approach. Our ap-
proach+Reranking are results obtained by apply-
ing the reranking step on the output of our new-
semi-CRF approach.
We can see that our approach outperforms the
Breck et alBaseline on both DSE extraction and
ESE extraction in spite of the fact that we do not
use their WordNet, Levin?s verb categorization, and
FrameNet features. The CRF+Reranking Baseline
does provide a performance increase over the the
baseline CRF results, but overall it cannot beat the
other methods since the CRF baseline is very low.
As one might expect, reranking also succeeds in
boosting the performance of new-semi-CRF, achiev-
ing the best performance on F-measure for both DSE
and ESE extraction. Note that the interannotator
agreement results for these two tasks are 75% for
DSE and 72% for ESE using a similar metric to Bi-
nary Overlap. Our results are much closer to these
interannotator scores than previous systems espe-
cially for DSEs.
Task Method F-measure
DSE Extraction
Breck et alBaseline 70.65
CRF+Reranking Baseline 63.87
Our approach 72.36
Our approach+Reranking 73.12
ESE Extraction
Breck et alBaseline 63.43
CRF+Reranking Baseline 58.21
Our approach 65.67
Our approach+Reranking 67.01
Table 5: Comparison of our work with previous work on
opinion expression extraction using the Binary-Overlap
metric
4.4 Discussion
We note that our new-semi-CRF approach outper-
forms the original semi-CRF w.r.t. both precision
and recall, but compared to CRF, our approach
yields a clear improvement on recall but not on pre-
cision. An error analysis helps explain why. We
found that our semi-CRF approach predicted almost
the same number of DSEs as the gold standard la-
bels while CRF only predicted half of them (for ESE
extraction, the trend is similar). With more pre-
dicted entities, the precision is sacrificed but recall is
boosted substantially, and overall we see an increase
in F-measure.
Looking further into the errors, we found sev-
eral mistakes that could potentially be fixed to yield
better a precision score. Some errors were due to
the false prediction of speech events like ?said? or
?told? as DSEs in cases where they actually just in-
troduced statements of fact without expressing any
private state. Adding features to distinguish such
cases should help improve performance. Other er-
rors were due to inadequate modeling of the context
surrounding the expressions. For example, ?enjoy a
relative advantage? was falsely predicted as an ESE.
If incorporating information about the subject of this
verb phrase which is ?products?, this mistake could
be avoided since ?products? cannot hold or express
private state. We also noticed some errors caused
by inaccurate parsing and hope to study ways to ac-
count for these in our approach as future work.
By comparing the extraction results across differ-
ent methods, we see that full parsing provides many
benefits for modeling segment boundaries and im-
proving the prediction precision for opinion expres-
sion extraction. For example, given the sentence, ?...
who are living [a lot better][ESE] ...?, both CRF and
the original semi-CRF extract ?lot better? as an ESE,
while our approach correctly extracts ?a lot better?
as an ESE. And we also found many cases where
the original semi-CRF cannot extract the opinion ex-
pressions while our approach can. Another benefit
of utilizing parsing is to speed up learning and infer-
ence. Although in theory, the computational cost of
parsing is O(g ? n3) where g is the grammar size
and n is the sentence length while the cost of semi-
CRFs is O(K2 ? L? n) where K is the number of
labels and L is the maximum entity length, feature
extraction overhead and the potentially large num-
ber of learning iterations in parameter optimization
may lead to a long training time for semi-CRFs. In
our experiments on the MPQA data set, our machine
with Intel Core 2 Duo CPU and 4GB RAM took 2
hours to fully parse 11,114 sentences using the Stan-
ford Parser, and also 2 hours to train the standard
semi-CRF. With the parsing information, our semi-
CRF-based approach is able to finish training in 15
minutes. As full parsing would be expensive when
the average sentence length is very large, it would be
interesting to study how to utilize parsing with less
cost in our task.
1343
5 Conclusion
In this paper we propose a semi-CRF-based ap-
proach for extracting opinion expressions that takes
into account during learning and inference the struc-
tural information available from syntactic parsing.
Our approach allows opinion expressions to be iden-
tified at the segment level and their boundaries to
be influenced by their probable syntactic structure.
Experimental evaluations show that our model out-
performs the best existing approaches on two opin-
ion extraction tasks. In addition, we identify useful
syntactic parse features for these tasks that have not
been explored in previous work. Our error analysis
indicates that adding additional features that account
for subjectivity cues in the local context might fur-
ther improve the performance. In future work, we
hope to explore better ways of utilizing parsing in-
formation with less cost. Also, we will apply our
model to additional opinion analysis tasks such as
fine-grained opinion summarization and relation ex-
traction.
6 Acknowledgement
This work was supported in part by National Science
Foundation Grants IIS-1111176 and IIS-0968450,
and by a gift from Google. We thank Nikos Karam-
patziakis, Igor Labutov, Veselin Stoyanov, Ainur
Yessenalina and Jason Yosinski for their helpful
comments.
References
Galen Andrew. 2006. A hybrid Markov/semi-Markov
conditional random field for sequence segmentation.
In Proceedings of EMNLP ?06.
Steven Bethard, Hong Yu, Ashley Thornton, Vasileios
Hatzivassiloglou, and Dan Jurafsky. 2005. Extracting
opinion propositions and opinion holders using syn-
tactic and lexical cues. In Shanahan, James G., Yan
Qu, and Janyce Wiebe, editors, Computing Attitude
and Affect in Text: Theory and Applications.
Eric Breck, Yejin Choi, and Claire Cardie. 2007. Identi-
fying expressions of opinion in context. IJCAI?07.
Yejin Choi, Claire Cardie, Ellen Riloff, and Siddharth
Patwardhan. 2005. Identifying sources of opinions
with conditional random fields and extraction patterns.
In Proceedings of HLT ?05.
Yejin Choi, Eric Breck, and Claire Cardie. 2006. Joint
extraction of entities and relations for opinion recog-
nition. In Proceedings of EMNLP ?06.
Yejin Choi and Claire Cardie. 2010. Hierarchical se-
quential learning for extracting opinions and their at-
tributes. In Proceedings of ACL 2010, Short Papers.
Richard Johansson and Alessandro Moschitti. 2010. Syn-
tactic and semantic structure for opinion expression
detection. In Proceedings of CoNLL ?10.
Niklas Jakob and Iryna Gurevych. Extracting opinion tar-
gets in a single- and cross-domain setting with condi-
tional random fields. In Proceedings of EMNLP? 10.
Mahesh Joshi and Penstein-Ros?e Carolyn. 2009. Gen-
eralizing dependency features for opinion mining.
In Proceedings of ACL/IJCNLP 2009, Short Papers
Track.
Dan Klein and Christopher D. Manning. 2003. Accurate
unlexicalized parsing. In Proceedings of ACL ?03.
Soo-Min Kim and Eduard Hovy. 2006. Extracting opin-
ions, opinion holders, and topics expressed in online
news media text. In Proceedings of the ACL Workshop
on Sentiment and Subjectivity in Text.
Nozomi Kobayashi, Kentaro Inui, and Yuji Matsumoto.
2007. Extracting aspect-evaluation and aspect-of rela-
tions in opinion mining. In Proceedings of EMNLP-
CoNLL-2007.
John D. Lafferty, Andrew McCallum, and Fernando C.
N. Pereira. 2001. Conditional Random Fields: Proba-
bilistic Models for Segmenting and Labeling Sequence
Data. In Proceedings of ICML ?01.
Dong C. Liu and Jorge Nocedal. 1989. On the limited
memory BFGS method for large scale optimization.
Mathematical Programming B 45(3): 503-528.
M Arthur Munson, Claire Cardie, and Rich Caruana. Op-
timizing to arbitrary NLP metrics using ensemble se-
lection. In HLT-EMNLP05, 2005.
Daisuke Okanohara, Yusuke Miyao, Yoshimasa Tsu-
ruoka, and Jun?ichi Tsujii. Improving the scalability
of semi-Markov conditional random fields for named
entity recognition. In Proceedings of ACL?06.
Randolph Quirk, Sidney Greenbaum, Geoffrey Leech,
and Jan Svartvik. A comprehensive grammar of the
English language. New York: Longman, 1985.
Richard Johansson and Alessandro Moschitti. Extract-
ing Opinion Expressions and Their Polarities - Explo-
ration of Pipelines and Joint Models. In Proceedings
of ACL ?11, Short Paper.
Ellen Riloff and Janyce M Wiebe. Learning extraction
patterns for subjective expressions. In Proceedings of
EMNLP 2003.
Sunita Sarawagi and William W. Cohen. 2004. Semi-
Markov Conditional Random Fields for Information
Extraction. In Proceedings of NIPS 2004.
1344
Charles Sutton and Andrew McCallum. An Introduc-
tion to Conditional Random Fields. Foundations and
Trends in Machine Learning (FnT ML), 2010.
Janyce Wiebe, Theresa Wilson , and Claire Cardie. 2005.
Annotating expressions of opinions and emotions in
language. Language Resources and Evaluation, vol-
ume 39, issue 2-3, pp. 165-210.
Theresa Wilson, Janyce Wiebe, and Paul Hoffmann.
2005. Recognizing contextual polarity in phrase-level
sentiment analysis. In Proceedings of HLT ?05.
Theresa Wilson, Paul Hoffmann, Swapna Somasun-
daran, Jason Kessler, Janyce Wiebe, Yejin Choi, Claire
Cardie, Ellen Riloff, and Siddharth Patwardhan. Opin-
ionFinder: A system for subjectivity analysis. EMNLP
2005. Demo abstract.
Yuanbin Wu, Qi Zhang, Xuanjing Huang, and Lide Wu.
Phrase dependency parsing for opinion mining. In Pro-
ceedings of EMNLP 2009.
Ioannis Tsochantaridis, Thomas Hofmann, Thorsten
Joachims, and Yasemi Altun. Support Vector Learn-
ing for Interdependent and Structured Output Spaces.
In Proceedings of ICML 2004.
1345
Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1933?1942,
Seattle, Washington, USA, 18-21 October 2013. c?2013 Association for Computational Linguistics
Identifying Manipulated Offerings on Review Portals
Jiwei Li
School of Computer Science
Carnegie Mellon University
Pittsburgh, PA 15213, USA
jiweil@cs.cmu.edu
Myle Ott Claire Cardie
Department of Computer Science
Cornell University
Ithaca, NY 14853, USA
myleott,cardie@cs.cornell.edu
Abstract
Recent work has developed supervised meth-
ods for detecting deceptive opinion spam?
fake reviews written to sound authentic and
deliberately mislead readers. And whereas
past work has focused on identifying individ-
ual fake reviews, this paper aims to identify
offerings (e.g., hotels) that contain fake re-
views. We introduce a semi-supervised man-
ifold ranking algorithm for this task, which
relies on a small set of labeled individual re-
views for training. Then, in the absence of
gold standard labels (at an offering level),
we introduce a novel evaluation procedure
that ranks artificial instances of real offer-
ings, where each artificial offering contains a
known number of injected deceptive reviews.
Experiments on a novel dataset of hotel re-
views show that the proposed method outper-
forms state-of-art learning baselines.
1 Introduction
Consumers increasingly rely on user-generated
online reviews when making purchase deci-
sions (Cone, 2011; Ipsos, 2012). Unfortunately,
the ease of posting content to the Web, potentially
anonymously, combined with the public?s trust and
growing reliance on opinions and other information
found online, create opportunities and incentives for
unscrupulous businesses to post deceptive opinion
spam?fraudulent or fictitious reviews that are
deliberately written to sound authentic, in order to
deceive the reader (Ott et al 2011).
Unlike other kinds of spam, such as
Web (Martinez-Romo and Araujo, 2009; Castillo
et al 2006) and e-mail spam (Chirita et al 2005),
recent work has found that deceptive opinion spam
is neither easily ignored nor easily identified by
human readers (Ott et al 2011). Accordingly, there
is growing interest in developing automatic, usually
learning-based, methods to help users identify
deceptive opinion spam (see Section 2). Even
in fully-supervised settings, however, automatic
methods are imperfect at identifying individual
deceptive reviews, and erroneously labeling genuine
reviews as deceptive may frustrate and alienate
honest reviewers.
An alternative approach, not yet considered in
previous work, is to instead identify those prod-
uct or service offerings where fake reviews appear
with high probability. For example, a hotel manager
may post fake positive reviews to promote their own
hotel, or fake negative reviews to demote a com-
petitor?s hotel. In both cases, rather than identify-
ing these deceptive reviews individually, it may be
preferable to identify the manipulated offering (i.e.,
the hotel) so that review portal operators, such as
TripAdvisor or Yelp, can further investigate the sit-
uation without alienating users.1
Accordingly, this paper addresses the novel task
of identifying manipulated offerings, which we
frame as a ranking problem, where the goal is to rank
offerings by the proportion of their reviews that are
believed to be deceptive. We propose a novel three-
layer graph model, based on manifold ranking (Zhou
et al 2003a; 2003b), to jointly model deceptive lan-
guage at the offering-, review- and term-level. In
particular, rather than treating reviews within the
same offering as independent units, there is a rein-
forcing relationship between offerings and reviews.
1Manipulating online reviews may also have legal conse-
quences. For example, the Federal Trade Commission (FTC)
has updated their guidelines on the use of endorsements and
testimonials in advertising to suggest that posting deceptive re-
views may be unlawful in the United States (FTC, 2009).
1933
Figure 1: Mutual Reinforcement Graph Model for Hotel
Ranking using the Manifold-Ranking Method
Our manifold ranking approach is semi-
supervised in that it requires no supervisory
information at the offering level; rather, it requires
only a small amount of labeled data at a review
level. Intuitively, and as depicted in Figure 1 for
hotel offerings, we represent hotels, reviews and
terms as nodes in a graph, where each hotel is
connected to its reviews, and each review, in turn, is
connected to the terms used within it. The influence
of labeled data is propagated along the graph to
unlabeled data, such that a hotel is considered more
deceptive if it is heavily linked with other deceptive
reviews, and a review, in turn, is more deceptive if it
is generated by a deceptive hotel.
The success of our semi-supervised approach fur-
ther depends on the ability to learn patterns of truth-
ful and deceptive reviews that generalize across re-
views of different offerings. This is challenging, be-
cause reviews often contain offering-specific vocab-
ulary. For example, reviews of hotels in Los Angeles
are more likely to include keywords such as ?beach?,
?sea?, ?sunshine? or ?LA?, while reviews of Juneau
hotels may contain ?glacier?, ?Juneau?, ?bear? or
?aurora borealis.? A hotel review might also men-
tion the hotel?s restaurant or bar by name.
Unfortunately, it is unclear how important (or
detrimental) offering-specific features are when de-
ciding whether a review is fake. Accordingly, we
propose a dimensionality-reduction approach, based
on Latent Dirichlet Allocation (LDA) (Blei et al
2003), to obtain a vector representation of reviews
for the ranking algorithm that generalizes across re-
views of different offerings. Specifically, we train
an LDA-based topic model to view each review as a
mixture of aspect-, city-, hotel- and review-specific
topics (see Section 6). We then reduce the dimen-
sionality of our data (i.e., labeled and unlabeled re-
views) by replacing each review term vector with a
vector that corresponds to its term distribution over
just its aspect-specific topics, i.e., excluding city-,
hotel- and review-specific topics. We find that, com-
pared to models trained either on the full vocabulary,
or trained on standard LDA document-topic vectors,
this representation allows our models to generalize
better across reviews of different offerings.
We evaluate our approach on the task of identi-
fying (ranking) manipulated hotels. In particular, in
the absence of gold standard offering-level labels,
we introduce a novel evaluation procedure for this
task, in which we rank numerous versions of each
hotel, where each hotel version contains a differ-
ent number of injected, known deceptive reviews.
Thus, we expect hotel versions with larger propor-
tions of deceptive reviews to be ranked higher than
those with smaller proportions.
For labeled training data, we use the Ott et
al. (2011) dataset of 800 positive (5-star) reviews of
20 Chicago hotels (400 deceptive and 400 truthful).
For evaluation, we construct a new FOUR-CITIES
dataset, containing 40 deceptive and 40 truthful re-
views for each of eight hotels in four different cities
(640 reviews total), following the procedure out-
lined in Ott et al (2011). We find that our manifold
ranking approach outperforms several state-of-the-
art learning baselines on this task, including trans-
ductive Support Vector Regression. We addition-
ally apply our approach to a large-scale collection
of real-world reviews from TripAdvisor and explore
the resulting ranking.
In the sections below, we discuss related work
(Section 2) and describe the datasets used in this
work (Section 3), the dimensionality-reduction ap-
proach for representing reviews (Section 4), and the
semi-supervised manifold ranking approach (Sec-
tion 5). We then evaluate the methods quantitatively
(Sections 6 and 7) and qualitatively (Section 8).
2 Related Work
A number of recent approaches have focused on
identifying individual fake reviews or users who post
1934
fake reviews. For example, Jindal and Liu (2008)
train machine learning classifiers to identify dupli-
cate (or near duplicate) reviews. Yoo and Gretzel
(2009) gathered 40 truthful and 42 deceptive hotel
reviews and manually compare the psychologically
relevant linguistic differences between them. Lim et
al. (2010) propose an approach based on abnormal
user behavior to predict spam users, without using
any textual features. Ott et al (2011) solicit decep-
tive reviews from workers on Amazon Mechanical
Turk, and built a dataset containing 400 deceptive
and 400 truthful reviews, which they use to train
and evaluate supervised SVM classifiers. Ott et al
(2012) expand upon this work to estimate preva-
lences of deception in a review community. Mukher-
jee et al (2012) study spam produced by groups of
fake reviewers. Li et al (2013) use topic models
to detect differences between deceptive and truthful
topic-word distributions. In contrast, in this work we
aim to identify fake reviews at an offering level.2
LDA Topic Models. LDA topic models (Blei et
al, 2003) have been employed for many NLP tasks
in recent years. Here, we build on earlier work
that uses topic models to (a) separate background
information from information discussing the vari-
ous ?aspects? of products (e.g., Chemudugunta et
al. (2007)) and (b) identify different levels of infor-
mation (e.g., user-specific, location-specific, time-
specific) (Ramage et al, 2009).
Manifold Ranking Algorithm. The manifold-
ranking method (Zhou et al 2003a; Zhou et al
2003b) is a mutual reinforcement ranking approach
initially proposed to rank data points along their un-
derlying manifold structure. It has been widely used
in many different ranking applications, such as sum-
marization (Wan et al 2007; Wan and Yang, 2007).
3 Dataset
In this paper, we train all of our models using the
CHICAGO dataset of Ott et al(2011), which contains
20 deceptive and 20 truthful reviews from each of 20
Chicago hotels (800 reviews total). This dataset is
2Approaches for identifying individual fake reviews may be
applied to our task, for example, by averaging the review-level
predictions for an offering. This averaging approach is one of
our baselines in Section 7.
City Hotels
Chicago W Chicago, Palomar Chicago
New York Hotel Pennsylvania, Waldorf Astoria
Los Angeles
Sheraton Gateway,
The Westin Los Angeles Airport
Houston
Magnolia Hotel,
Crowne Plaza Houston River Oaks
Table 1: Details of our FOUR-CITIES evaluation data.
unique in that it contains known (gold standard) de-
ceptive reviews, solicited through Amazon Mechan-
ical Turk, and is publicly-available.3
Unfortunately, the CHICAGO dataset is limited,
both in size (800 reviews) and scope, in that it only
contains reviews of hotels in one city: Chicago.
Accordingly, in order to perform a more realistic
evaluation for our task, we construct a new dataset,
FOUR-CITIES, that contains 40 deceptive and 40
truthful reviews from each of eight hotels in four dif-
ferent cities (640 reviews total).
We build the FOUR-CITIES dataset using the same
procedure as Ott et al(2011), by creating and di-
viding 320 Mechanical Turk jobs, called Human-
Intelligence Tasks (HITs), evenly across eight of the
most popular hotels in our four chosen cities (see Ta-
ble 1). Each HIT presents a worker with the name of
a hotel and a link to the hotel?s website. Workers are
asked to imagine that they work for the marketing
department of the hotel and that their boss has asked
them to write a fake positive review, as if they were
a customer, to be posted on a travel review website.
Each worker is allowed to submit a single review,
and is paid $1 for an acceptable submission.
Finally, we augment our deceptive FOUR-CITIES
reviews with a matching set of truthful reviews from
TripAdvisor by randomly sampling 40 positive (5-
star) reviews for each of the eight chosen hotels.
While we cannot know for sure that the sampled re-
views are truthful, previous work has suggested that
rates of deception among popular hotels is likely to
be low (Jindal and Liu, 2008; Lim et al 2010).
4 Topic Models for Dimensionality
Reduction
As mentioned in the introduction, we want to learn
patterns of truthful and deceptive reviews that apply
3We use the dataset available at: http://www.cs.
cornell.edu/?myleott/op_spam.
1935
Figure 2: Graphical illustration of the RLDA topic model.
across hotels in different locations. This is challeng-
ing, however, because hotel reviews often contain
specific information about the hotel or city, and it
is unclear whether these features will generalize to
reviews of other hotels.
We therefore investigate an LDA-based
dimensionality-reduction approach (RLDA) to
derive effective vector representations of reviews.
Specifically, we model each document as a bag of
words, generated from a mixture of: (a) ?aspect?
topics (that discuss various dimensions of the
offering); (b) city-specific topics; (c) hotel-specific
topics; (d) review-specific topics;4 and (e) a back-
ground topic. We use this model to reduce the
dimensionality of the review representation in our
training and test sets, by replacing each review?s
term vector with a vector corresponding to the
distribution over only the aspect-based topics, i.e.,
we exclude city, hotel and review-specific topics, as
well as the background topic.
Below we present specific details of our model
(Sections 4.1 and 4.2). The effectiveness of our
dimensionality-reduction approach will be directly
evaluated in Section 6, by comparing the perfor-
mance of various classifiers trained either on the full
vocabulary, or on our reduced feature representation.
4.1 RLDA Model Details
The plate diagram and generative story for our
model are given in Figures 2 and 3, respectively.
Our model has a similar general structure to stan-
dard LDA, but with additional machinery to handle
different levels of information. In particular, in or-
der to model K aspects in a collection of R reviews,
4These will be terms used in just a small number of reviews.
? Draw ?B ? Dir(?)
? For each aspect z = 1, 2, ...,K: draw ?z ? Dir(?)
? For each city c = 1, 2, ..., C: draw ?c ? Dir(?)
? For each hotel h = 1, 2, ..., H: draw ?h ? Dir(?)
? For each review r:
? Draw pir ? Dir(?)
? Draw ?r ? Dir(?)
? Draw ?r ? Dir(?)
? For each word w in d:
? Draw yw ?Multi(pir)
? If yw = 0:
? Draw zw ?Multi(?)
? Draw w ?Multi(?zw )
? If yw = 1: draw w ?Multi(?B)
? If yw = 2: draw w ?Multi(?d)
? If yw = 3: draw w ?Multi(?h)
? If yw = 4: draw w ?Multi(?c)
Figure 3: Generative story for the RLDA topic model.
of H hotels, in C cities, we first draw multinomial
word distributions corresponding to: the background
topic, ?B; aspect topics, ?k for k ? [1,K]; review-
specific topics, ?r for r ? [1, R]; hotel-specific top-
ics, ?h for h ? [1, H]; and city-specific topics, ?c
for c ? [1, C]. Then, for each word w in review
R, we sample a switch variable, y ? [0, 4], indicat-
ing whether w comes from one of the aspect topics
(y = 0), or the background topic (y = 1), review-
specific topic (y = 2), hotel-specific topic (y = 3)
or city-specific topic (y = 4). If the word comes
from one of the aspect topics, then we further sam-
ple the specific aspect topic, zw ? [1,K]. Finally,
we generate the word, w, from the corresponding ?.
4.2 Inference for RLDA
Given the review collection, our goal is to find the
most likely assignment yw (and zw if yw = 0) for
each word, w, in each review. We perform infer-
ence using Gibbs sampling. It is relatively straight-
forward to derive Gibbs sampling equations that al-
low joint sampling of the zw and yw latent variables
for each word token w:
P (yw = 0, Zw = k) =
Nar,?w + ?
Nr,?w + 5?
?
Ckr,?w + ?
?
k C
k
r,?w +K?
?
Ewk + ??
w E
w
k + V ?
,
P (yw = m,m = 1, 2, 3, 4) =
Nmr,?w + ?
Nr,?w + 5?
?
Ewm + ??
w E
w
m + V ?
,
Note that the subscript ?w indicates that the
count for word token w is excluded. Also, Nr
1936
denotes the number of words in review r and
Nar,?w, N
1
r,?w, N
2
r,?w, N
3
r,?w, N
4
r,?w are the number of
words in review r assigned to the aspect, background,
review-specific, hotel-specific and city-specific topics, re-
spectively, excluding the current word. Ckr,?w denotes
the number of words in review r assigned to aspect topic
k. Ewk , E
w
1 , E
w
2 , E
w
3 , E
w
4 denote the number of times that
the word w is assigned to aspect k, the background topic,
review-specific topic r, hotel-specific topic h, and city-
specific topic c, respectively. We set hyperparameter ?
to 1, ? to 0.5, ? to 0.01. We run 200 iterations of Gibbs
sampling until the topic distribution stabilizes. After each
iteration in Gibbs sampling, we obtain:
piir =
N ir + ??
iN
i
r + 5?
?kr =
Ckr + ??
k C
k
r +K?
?wz =
Ewz + ??
w E
w
z + V ?
?wm =
Ewm + ??
w E
w
m + V ?
(1)
Finally, at the end of Gibbs sampling, we filter out
background, document-specific, hotel-specific and
city-specific information, by replacing each docu-
ment?s term vector with a 1?K aspect-topic vector,
~Gr = ??1r , ?
2
r , ? ? ? , ?
K
r ?.
5 Manifold Ranking for Hotels
In this section, we describe our ranking algorithm ?
based on manifold ranking (Zhou et al 2003a; Zhou
et al 2003b) ? that tries to jointly model deceptive
language at the hotel-, review- and term-level.
5.1 Graph Construction
We use a three-layer (hotel layer, review layer and
term layer) mutual reinforcement model (see Fig-
ure 1). Formally, we represent our three-layer graph
as G = ?VH , VR, VT , EHR, ERR, ERT , ETT ?,
where VH = {Hi}
i=NH
i=1 , VR = {R}
i=HR
i=1 and
VT = {Ti}i=Vi=1 correspond to the set of hotels, re-
views and terms respectively. EHR, ERR and ERT
respectively denote the edges between hotels and re-
views, reviews and reviews and reviews and terms.
Each edge is associated with a weight that denotes
the similarity between two nodes.
Let sim(Hi, Rj), where Hi ? VH and Rj ? VR,
denote the edge weight between hotelHi and review
Rj , calculated as follows:
sim(Hi, Rj) =
{
1 if Ri ? Hj
0 if Ri 6? Hj
(2)
Then we get row normalized matrices DHR ?
RNH?NR and DRH ? RNR?NH as follows:
DHR(i, j) =
sim(Hi, Rj)
?
i? sim(Hi? , Rj)
DRH(i, j) =
sim(Hi, Rj)
?
j? sim(Hi, Rj?)
(3)
As described in Section 4.2, each review is rep-
resented with a 1 ? K aspect vector Gr after fil-
tering undesired information. The edge weight be-
tween two reviews is then the cosine similarity,
sim(Ri, Rj), between two reviews and can be cal-
culated as follows:
sim(Ri, Rj) =
?t=K
t=1 G
t
i ?G
t
j
?
?t=K
t=1 G
t2
i ?
?
?t=K
t=1 G
t2
j
(4)
Since the normalization process will make the
review-to-review relation matrix asymmetric, we
adopt the following strategy: let P denote the sim-
ilarity matrix between reviews, where P (i, j) =
sim(Ri, Rj) and M denotes the diagonal matrix
with (i,i)-element equal to the sum of the ith row
of SIM . The normalized matrix between reviews
DRR ? RNR?NR is calculated as follows:
DRR =M
? 12 ? P ?M?
1
2 (5)
sim(Ri, wj) denotes the similarity between re-
view Ri and term wj and is the conditional prob-
ability of word wj given review Ri. If wj ? Rj ,
sim(Ri, wj) is calculated according to Eq. (6) by
integrating out latent parameters ? and pi. Else if
wj 6? Rj , sim(Ri, wj) = 0.
sim(Ri, wj) =
k=K?
k=1
p(z = k|ri)? p(wj |z = k)
+
?
t?{B,h,c,d}
p(wj |yi = t)p(yi = t|ri)
= pi(a)d
k=K?
k=1
?zd ? ?
(wj)
z +
?
t?{B,h,c,d}
pi(t)d ?
(wj)
t
(6)
Similar to Eq. (3), we further get the normalized ma-
trix DRT ? RHR?V and DTR ? RV?HR .
Similarity between terms sim(wi, wj) is given by
the WordNet path-similarity,5 normalized to create
the matrix DV V .
5Path-similarity is based on the shortest path that con-
nects the senses in the ?is-a? (hypernym/hyponym) tax-
onomy. See http://nltk.googlecode.com/svn/
trunk/doc/howto/wordnet.html.
1937
Input: The hotel set VD, review set VR, term
set VT , normalized transition probability matrix
DHR, DRR, DRH , DRT , DTT , DTR.
Output: the ranking vectors SR, SH , ST .
Begin:
1. Initialization: set the score labeled reviews to
+1 or ?1 and other unlabeled reviews 0: S0R =
[+1, ...,+1,?1, ...,?1, 0, ..., 0]. Set S0H and
S0T to 0. Normalize the score vector.
2. update SkR, S
k
H and S
k
T according to Eq. (7).
3. normalize SkR, S
k
H and S
k
T .
4. fix the score of labeled reviews to +1 and ?1.
Go to step (2) until convergence.
Figure 4: Semi-Supervised Reinforcement Ranking.
5.2 Reinforcement Ranking Based on the
Manifold Method
Based on the set of labeled reviews, nodes for truth-
ful reviews (positive) are initialized with a high
score (1) and nodes for deceptive reviews, a low
score (-1). Given the weighted graph, our task is
to assign a score to the each hotel, each term, and
the remaining unlabeled reviews. Let SH , SR and
ST denote the ranking scores of hotels, reviews and
terms, which are updated during each iteration as
follows until convergence6:
?
??
??
Sk+1H = DHR ? S
k
R
Sk+1R = 1DRR ? S
k
R + 2DRH ? S
k
H + 3DRT ? S
k
t
Sk+1T = 4DTT ? S
k
T + 5DTR ? S
k
R
(7)
where 1 + 2 + 3 = 1 and 4 + 5 = 1. (The score
of labeled reviews will be fixed to +1 or ?1.)
6 Learning Generalizable Classifiers
In Section 4, we introduced RLDA to filter out
review-, hotel- and city-specific information from
our vector-based review representation. Here, we
will directly evaluate the effectiveness of RLDA
by comparing the performance of binary deceptive
vs. truthful classifiers trained on three feature sets:
(a) the full vocabulary, encoded as unigrams and
bigrams (N-GRAMS); (b) a reduced-dimensionality
feature space, based on standard LDA (Blei et
al, 2003); and (c) a reduced-dimensionality feature
6Convergence is achieved if the difference between ranking
scores in two consecutive iterations is less than 0.00001.
space, based on our proposed revised LDA approach
(RLDA).
We compare two kinds of classifiers, which are
trained on only the labeled CHICAGO dataset and
tested on the FOUR-CITIES dataset. First, we use
SVMlight (Joachims, 1999) to train linear SVM clas-
sifiers, which have been shown to perform well in
related work (Ott et al 2011). Second, we train a
two-layer manifold classifier, which is a simplified
version of the model presented in Section 5. In this
model, the graph consists of only review and term
layers, and the score of a labeled review is fixed to
1 or -1 in each iteration. After convergence, reviews
with scores greater than 0 are classified as truthful,
and less than 0 as deceptive.
Results and Discussion The results are shown in
Table 2 and show the average accuracy and preci-
sion/recall w.r.t. the truthful (positive) class. We find
that SVM and MANIFOLD are comparable in all six
conditions, and not surprisingly, perform best when
evaluated on reviews from the two Chicago hotels in
our FOUR-CITIES data. However, the N-GRAM and
LDA feature sets perform much worse than RDLA
when evaluation is performed on reviews from the
other three (non-Chicago) cities. This confirms that
classifiers trained on n-gram features overfit to the
training data (CHICAGO) and do not generalize well
to reviews from other cities. In addition, the stan-
dard LDA-based method for dimensionality reduc-
tion is not sufficient for our specific task.
7 Identifying Manipulated Hotels
In this section, we evaluate the performance of our
manifold ranking approach (see Section 5) on the
task of identifying manipulated hotels.
Baselines. We consider several baseline ranking
approaches to compare to our manifold ranking ap-
proach. Like the manifold ranking approach, the
baselines also employ both the CHICAGO dataset (la-
beled) and FOUR-CITIES dataset (without labels).7
For fair comparison, we use identical processing
techniques for each approach. Topic number is set
7While we have not investigated the effects of unlabeled data
in detail, providing additional unlabeled data (beyond the test
set) boosts the manifold ranking performances reported below
by 1-2%.
1938
city feature set
SVM Manifold
Accuracy Precision Recall Accuracy Precision Recall
Chicago
N-GRAMS 0.831 0.844 0.818 0.835 0.844 0.825
LDA 0.833 0.846 0.819 0.817 0.832 0.802
RLDA 0.830 0.838 0.822 0.841 0.819 0.863
Non-Chicago
N-GRAMS 0.728 0.744 0.714 0.733 0.738 0.727
LDA 0.714 0.696 0.732 0.728 0.715 0.741
RLDA 0.791 0.799 0.780 0.801 0.787 0.815
Table 2: Binary classification results showing that n-gram features overfit to the CHICAGO training data. Results
correspond to evaluation on reviews for the two Chicago hotels from FOUR-CITIES and non-Chicago FOUR-CITIES
reviews (six hotels).
to five for all topic-model-based approaches. Each
baseline makes review-level predictions and then
ranks each hotel by the average of those predictions.
? Review-SVR: Uses linear Tranductive Support
Vector Regression with unigram and bigram fea-
tures, similar to Ott et al (2011).
? Review-SVR+LDA (R): Similar to REVIEW-
SVR but uses our revised LDA (RLDA) topic
model for dimensionality reduction (R).
? Two-Layer Manifold (S): A simplified version of
our model where the hotel-level is removed from
the graph. Dimensionality reduction is performed
using standard LDA (S).
? Two-Layer Manifold (R): Similar to TWO-
LAYER MANIFOLD (S) but uses the revised LDA
(RLDA) model for dimensionality reduction.
? Three-layer Manifold (tf-idf): Our three-layer
manifold ranking model, except with each review
represented as a TF-IDF term vector. Review sim-
ilarity is calculated based on the cosine similarity
between these vectors.
Evaluation Method. To evaluate ranking perfor-
mance in the absence of a gold standard set of ma-
nipulated hotels, we rearrange the FOUR-CITIES test
set of 40 truthful and 40 deceptive reviews for each
of eight hotels: we create 41 versions of each hotel,
where each hotel version contains a different num-
ber of injected deceptive reviews, ranging from 0 to
40. For example, the first version of a hotel will have
40 truthful and 0 deceptive reviews, the second ver-
sion 39 truthful and 1 deceptive, and the 41st ver-
sion 0 truthful and 40 deceptive. In total, we gen-
erate 41 ? 8 = 328 versions of hotel reviews. We
expect versions with larger proportions of deceptive
reviews to receive lower scores by the ranking mod-
els (i.e., they are ranked higher/more deceptive).
Metrics. To qualitatively evaluate the ranking re-
sults, we use the Normalized Discounted Cumula-
tive Gain (NDCG), which is commonly used to eval-
uate retrieval algorithms with respect to an ideal
relevance-based ranking. In particular, NDCG re-
wards rankings with the most relevant results at the
top positions (Liu, 2009), which is also our objec-
tive, namely, to rank versions that have higher pro-
portions of deceptive reviews nearer to the top.
Let R(m) denote the relevance score of mth
ranked hotel version. Then, NDCGN is defined as:
NDCGN =
1
IDCGN
m=N?
m=1
2R(m) ? 1
log2(1 +m)
(8)
where IDCGN refers to discounted cumulative gain
(DCG) of the ideal ranking of the top N results. We
define the ideal ranking according to the proportion
of deceptive reviews in different versions, and re-
port NDCG scores for theNth ranked hotel versions
(N = 8 to 321), at intervals of 8 (to account for ties
among the eight hotels).
Results and Discussion. NDCG results are shown
in Figure 5. We observe that our approach (using
2, 5 or 10 topics) generally outperforms the other
approaches. In particular, approaches that use our
RLDA text representation (OUR APPROACH, TWO-
LAYER MANIFOLD (R), and REVIEW-SVR+LDA
(R)), which tries to remove city- and hotel-specific
information, perform better than those that use
the full vocabulary (REVIEW-SVR, TWO-LAYER
MANIFOLD (S), and THREE-LAYER MANIFOLD
(TF-IDF)). This further confirms that our RLDA
dimensionality reduction technique allows models,
1939
Figure 5: NDCGN results for different approaches. K
indicates the number of topics.
trained on limited data, to generalize to reviews of
different hotels and in different locations. We also
find that approaches that model a reinforcing rela-
tionship between hotels and their reviews are bet-
ter than approaches that model reviews as inde-
pendent units, e.g., TWO-LAYER MANIFOLD (R)
vs. REVIEW-SVR+LDA and TWO-LAYER MANI-
FOLD (S) vs. REVIEW-SVR. This confirms our in-
tuition that a hotel is more deceptive if it is con-
nected with many deceptive reviews, and, in turn,
a review is more deceptive if from a deceptive hotel.
8 Qualitative Evaluation
We now present qualitative evaluations for the
RLDA topic model and the manifold ranking model.
Topic Quality. Table 3 gives the top words for
four aspect topics and four city-specific topics in the
RLDA topic model; Table 4 gives the highest and
lowest ranking term weights in our three-layer man-
ifold model. By comparing the first row of topics in
Table 3, corresponding to aspect topics, to the top
words in Table 4, we observe that the learned top-
ics relate to truthful and deceptive classes. For ex-
ample, Topics 1 and 4 share many terms with the
top truthful terms in the manifold model, e.g., spa-
tial terms, such as location, floor and block,
and punctuation, such as (, ), and $. Similarly,
Topics 2 and 7 share many terms with the top de-
ceptive terms in the manifold model, e.g., hotel,
husband, wife, amazing, experience and
recommend. This makes sense, since topic models
have been shown to produce discriminative topics on
Topic1 Topic2 Topic4 Topic7
location hotel ( hotel
$ stay room service
walk staff ) husband
night restaurant park amazing
block friendly bed will
floor room night weekend
quiet recommend shower friendly
nice love view travel
lobby excellent minute experience
breakfast wife pillow friend
NYC Chicago LA Houston
York Chicago los Houston
ny Michigan Angeles downtown
time mile la Texas
square tower lax cab
nyc Illinois shuttling Westside
street avenue hollywood center
empire Rogers plane Northwest
Chinatown river morning st
station Burnham California museum
Wall Goodman downtown mission
Table 3: Top words in topics extracted from RLDA topic
model (see Section 4). The top row presents topic words
from four aspect topics (K = 10) and the bottom row
presents top words from four city-specific topics.
Deceptive Truthful
term score term score
my -1.063 $ 0.964
visit -0.944 location 0.922
we -0.882 ( 0.884
hotel -0.863 ) 0.884
husband -0.828 bathroom 0.842
family -0.824 floor 0.810
amazing -0.782 breakfast 0.784
experience -0.740 bar 0.762
recommend -0.732 block 0.747
wife -0.680 small 0.721
relax -0.678 but 0.720
vacation -0.651 walk 0.707
will -0.651 lobby 0.707
friendly -0.646 quiet 0.684
Table 4: Term scores from our ranking algorithm.
this data in previous work (Li et al, 2013).
With respect to the second row in Table 4, con-
taining top words from city-specific topics, we ob-
serve that each topic does contain primarily city-
specific information. This helps to explain why re-
moving terms associated with these topics resulted
in a better vector representation for reviews.
1940
Figure 6: Hotel Ranking Distribution on TripAdvisor
Figure 7: Proportion of Singletons vs. Hotel Ranking.
Real-world Evaluation. Finally, we apply our
ranking model to a large-scale collection of real-
world reviews from TripAdvisor. We crawl 878,561
reviews from 3,945 hotels in 25 US cities from Tri-
pAdvisor excluding all non-5-star reviews and re-
moving hotels with fewer than 100 reviews. In the
end, we collect 244,810 reviews from 838 hotels.
We apply our manifold ranking model and rank
all 838 hotels. First, we present a histogram of the
resulting manifold ranking scores in Figure 6. We
observe that the distribution reaches a peak around
0.04, which in our quantitative evaluation (Sec-
tion 7) corresponded to a hotel with 34 truthful and
6 deceptive reviews. These results suggest that the
majority of reviews in TripAdvisor are truthful, in
line with previous findings by Ott et al (2011).
Next, we note that previous work has hypothe-
sized that deceptive reviews are more likely to be
posted by first-time review writers, or singleton re-
viewers (Ott et al 2011; Wu et al 2011). Accord-
ingly, if this hypothesis were valid, then manipu-
lated hotels would have an above-average proportion
of singleton reviews. Figure 7 shows a histogram
of the average proportion of singleton reviews, as
a function of the ranking scores produced by our
model. Noting that lower scores correspond to a
higher predicted proportion of deceptive reviews, we
observe that hotels that are ranked as being more de-
ceptive by our model have much higher proportions
of singleton reviews, on average, compared to hotels
ranked as less deceptive.
9 Conclusion
We study the problem of identifying manipulated of-
ferings on review portals and propose a novel three-
layer graph model, based on manifold ranking for
ranking offerings based on the proportion of reviews
expected to be instances of deceptive opinion spam.
Experimental results illustrate the effectiveness of
our model over several learning-based baselines.
Acknowledgments
This work was supported in part by National Sci-
ence Foundation Grant BCS-0904822, a DARPA
Deft grant, as well as a gift from Google. We also
thank the EMNLP reviewers for their helpful com-
ments and advice.
References
David Blei, Ng Andrew and Michael Jordan. Latent
Dirichlet alocation. 2003. In Journal of Machine
Learning Research.
Carlos Castillo, Debora Donato, Luca Becchetti, Paolo
Boldi, Stefano Leonardi, Massimo Santini and Sebas-
tiano Vigna. A reference collection for web spam. In
ACM Sigir Forum. 2006.
Paul-Alexandru Chirita, Jrg Diederich and Wolfgang Ne-
jdl. MailRank: using ranking for spam detection. In
Proceedings of the 14th ACM international conference
on Information and knowledge management. 2005.
Cone. 2011 Online Influence Trend Tracker.
http://www.coneinc.com/negative-reviews-online-
reverse-purchase-decisions. August.
Yajuan Duan, Zhumin Chen, Furu Wei, Ming Zhou and
Heung-Yeung Shum. Twitter Topic Summarization by
Ranking Tweets Using Social Influence and Content
Quality. In Proceedings of 24th International Confer-
ence on Computational Linguistics 2012.
Federal Trade Commission. Guides Concerning Use of
Endorsements and Testimonials in Advertising. In
FTC 16 CFR Part 255. 2009.
1941
Socialogue: Five Stars? Thumbs Up? A+ or
Just Average? URL:http://www.ipsos-na.com/news-
polls/pressrelease.aspx?id=5929g
Nitin Jindal, and Bing Liu. Opinion spam and analysis. In
Proceedings of the 2008 International Conference on
Web Search and Data Mining. 2008.
Nitin Jindal, Bing Liu and Ee-Peng Lim. Finding Unusual
Review Patterns Using Unexpected Rules. In Proceed-
ings of the 19th ACM international conference on In-
formation and knowledge management.2010.
Thorsten Joachims. Making large-scale support vector
machine learning practical. In Advances in kernel
methods.1999.
Fangtao Li, Minlie Huang, Yi Yang and Xiaoyan Zhu.
Learning to identify review Spam. In Proceedings of
the Twenty-Second international joint conference on
Artificial Intelligence. 2011.
Jiwei Li, Claire Cardie and Sujian Li. TopicSpam: a
Topic-Model-Based Approach for Spam Detection. In
Proceedings of the 51th Annual Meeting of the Associ-
ation for Computational Linguis- tics. 2013.
Peng Li, Jing Jiang and Yinglin Wang. Generating tem-
plates of entity summaries with an entity-aspect model
and pattern mining. In Proceedings of the 48th Annual
Meeting of the Association for Computational Linguis-
tics. 2010.
Ee-Peng Lim, Viet-An Nguyen, Nitin Jindal, Bing Liu,
and Hady Wirawan Lauw. Detecting Product Review
Spammers Using Rating Behavior. In Proceedings of
the 19th ACM international conference on Information
and knowledge management. 2010.
Tieyan Liu. Learning to Rank for Information Retrieval.
In Foundations and Trends in Information Retrieval
2009.
Arjun Mukherjee, Bing Liu and Natalie Glance. Spotting
Fake Reviewer Groups in Consumer Reviews . In Pro-
ceedings of the 21st international conference on World
Wide Web. 2012.
Juan Martinez-Romo and Lourdes Araujo. Web spam
identification through language model analysis. In
Proceedings of the 5th international workshop on ad-
versarial information retrieval on the web. 2009.
Myle Ott, Claire Cardie and Jeffrey Hancock. Estimating
the Prevalence of Deception in Online Review Com-
munities. In Proceedings of the 21st international con-
ference on World Wide Web. 2012.
Myle Ott, Yejin Choi, Claire Cardie and Jeffrey Hancock.
Finding Deceptive Opinion Spam by Any Stretch of
the Imagination. In Proceedings of the 49th Annual
Meeting of the Association for Computational Linguis-
tics. 2011.
Daniel Ramage, David Hall, Ramesh Nallapati and
Christopher Manning. Labeled LDA: A supervised
topic model for credit attribution in multi-labeled
corpora. In Proceedings of the 2009 Conference on
Empirical Methods in Natural Language Processing.
2009.
Michal Rosen-zvi, Thomas Griffith, Mark Steyvers and
Padhraic Smyth. The author-topic model for authors
and documents. In Proceedings of the 20th conference
on Uncertainty in artificial intelligence.2004.
Xiaojun Wan and Jianwu Yang. Multi-Document Sum-
marization Using Cluster-Based Link Analysis. In
Proceedings of the 31st annual international ACM SI-
GIR conference on Research and development in in-
formation retrieval. 2008.
Xiaojun Wan, Jianwu Yang and Jianguo Xiao Manifold-
Ranking Based Topic-Focused Multi-Document Sum-
marization. In Proceedings of International Joint Con-
ferences on Artificial Intelligence,2007.
Guan Wang, Sihong Xie, Bing Liu and Philip Yu. Re-
view Graph based Online Store Review Spammer De-
tection. In Proceedings of International Conference of
Data Mining. 2011.
Guangyu Wu, Derek Greene and , Padraig Cunningham.
Merging multiple criteria to identify suspicious re-
views. In Proceedings of the fourth ACM conference
on Recommender systems. 2011.
Kyung-Hyan Yoo and Ulrike Gretzel. Comparison of De-
ceptive and Truthful Travel Reviews. In Information
and Communication Technologies in Tourism. 2009.
Dengyong Zhou, Olivier Bousquet, Thomas Navin and
Jason Weston. Learning with local and global consis-
tency. In Proceedings of Advances in neural informa-
tion processing systems.2003.
Dengyong Zhou, Jason Weston, Arthur Gretton and
Olivier Bousquet. Ranking on data manifolds. In Pro-
ceedings of Advances in neural information processing
systems.2003.
1942
Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 720?728,
October 25-29, 2014, Doha, Qatar.
c?2014 Association for Computational Linguistics
Opinion Mining with
Deep Recurrent Neural Networks
Ozan
?
Irsoy and Claire Cardie
Department of Computer Science
Cornell University
Ithaca, NY, 14853, USA
oirsoy, cardie@cs.cornell.edu
Abstract
Recurrent neural networks (RNNs) are con-
nectionist models of sequential data that are
naturally applicable to the analysis of natural
language. Recently, ?depth in space? ? as
an orthogonal notion to ?depth in time? ? in
RNNs has been investigated by stacking mul-
tiple layers of RNNs and shown empirically
to bring a temporal hierarchy to the architec-
ture. In this work we apply these deep RNNs
to the task of opinion expression extraction
formulated as a token-level sequence-labeling
task. Experimental results show that deep,
narrow RNNs outperform traditional shallow,
wide RNNs with the same number of parame-
ters. Furthermore, our approach outperforms
previous CRF-based baselines, including the
state-of-the-art semi-Markov CRF model, and
does so without access to the powerful opinion
lexicons and syntactic features relied upon by
the semi-CRF, as well as without the standard
layer-by-layer pre-training typically required
of RNN architectures.
1 Introduction
Fine-grained opinion analysis aims to detect the sub-
jective expressions in a text (e.g. ?hate?) and to char-
acterize their intensity (e.g. strong) and sentiment (e.g.
negative) as well as to identify the opinion holder (the
entity expressing the opinion) and the target, or topic,
of the opinion (i.e. what the opinion is about) (Wiebe et
al., 2005). Fine-grained opinion analysis is important
for a variety of NLP tasks including opinion-oriented
question answering and opinion summarization. As a
result, it has been studied extensively in recent years.
In this work, we focus on the detection of opinion ex-
pressions ? both direct subjective expressions (DSEs)
and expressive subjective expressions (ESEs) as de-
fined in Wiebe et al. (2005). DSEs consist of explicit
mentions of private states or speech events expressing
private states; and ESEs consist of expressions that in-
dicate sentiment, emotion, etc., without explicitly con-
veying them. An example sentence shown in Table 1 in
which the DSE ?has refused to make any statements?
explicitly expresses an opinion holder?s attitude and the
The committee , as usual , has
O O O B ESE I ESE O B DSE
refused to make any statements .
I DSE I DSE I DSE I DSE I DSE O
Table 1: An example sentence with labels
ESE ?as usual? indirectly expresses the attitude of the
writer.
Opinion extraction has often been tackled as a se-
quence labeling problem in previous work (e.g. Choi
et al. (2005)). This approach views a sentence as
a sequence of tokens labeled using the conventional
BIO tagging scheme: B indicates the beginning of an
opinion-related expression, I is used for tokens inside
the opinion-related expression, and O indicates tokens
outside any opinion-related class. The example sen-
tence in Table 1 shows the appropriate tags in the BIO
scheme. For instance, the ESE ?as usual? results in the
tags B ESE for ?as? and I ESE for ?usual?.
Variants of conditional random field (CRF) ap-
proaches have been successfully applied to opinion ex-
pression extraction using this token-based view (Choi
et al., 2005; Breck et al., 2007): the state-of-the-art
approach is the semiCRF, which relaxes the Marko-
vian assumption inherent to CRFs and operates at the
phrase level rather than the token level, allowing the in-
corporation of phrase-level features (Yang and Cardie,
2012). The success of the CRF- and semiCRF-based
approaches, however, hinges critically on access to an
appropriate feature set, typically based on constituent
and dependency parse trees, manually crafted opinion
lexicons, named entity taggers and other preprocessing
components (see Yang and Cardie (2012) for an up-to-
date list).
Distributed representation learners provide a differ-
ent approach to learning in which latent features are
modeled as distributed dense vectors of hidden lay-
ers. A recurrent neural network (RNN) is one such
learner that can operate on sequential data of variable
length, which means it can also be applied as a se-
quence labeler. Moreover, bidirectional RNNs incor-
porate information from preceding as well as follow-
ing tokens (Schuster and Paliwal, 1997) while recent
advances in word embedding induction (Collobert and
Weston, 2008; Mnih and Hinton, 2007; Mikolov et
720
al., 2013; Turian et al., 2010) have enabled more ef-
fective training of RNNs by allowing a lower dimen-
sional dense input representation and hence, more com-
pact networks (Mikolov et al., 2010; Mesnil et al.,
2013). Finally, deep recurrent networks, a type of
RNN with multiple stacked hidden layers, are shown
to naturally employ a temporal hierarchy with multi-
ple layers operating at different time scales (Hermans
and Schrauwen, 2013): lower levels capture short term
interactions among words; higher layers reflect inter-
pretations aggregated over longer spans of text. When
applied to natural language sentences, such hierarchies
might better model the multi-scale language effects that
are emblematic of natural languages, as suggested by
previous results (Hermans and Schrauwen, 2013).
Motivated by the recent success of deep architectures
in general and deep recurrent networks in particular, we
explore an application of deep bidirectional RNNs ?
henceforth deep RNNs ? to the task of opinion ex-
pression extraction. For both DSE and ESE detection,
we show that such models outperform conventional,
shallow (uni- and bidirectional) RNNs as well as previ-
ous CRF-based state-of-the-art baselines, including the
semiCRF model.
In the rest of the paper we discuss related work
(Section 2) and describe the architecture and training
methods for recurrent neural networks (RNNs), bidi-
rectional RNNs, and deep (bidirectional) RNNs (Sec-
tion 3). We present experiments using a standard cor-
pus for fine-grained opinion extraction in Section 4.
2 Related Work
Opinion extraction. Early work on fine-grained
opinion extraction focused on recognizing subjective
phrases (Wilson et al., 2005; Munson et al., 2005).
Breck et al. (2007), for example, formulated the prob-
lem as a token-level sequence-labeling problem and ap-
ply a CRF-based approach, which significantly outper-
formed previous baselines. Choi et al. (2005) extended
the sequential prediction approach to jointly identify
opinion holders; Choi and Cardie (2010) jointly de-
tected polarity and intensity along with the opinion ex-
pression. Reranking approaches have also been ex-
plored to improve the performance of a single sequence
labeler (Johansson and Moschitti, 2010; Johansson and
Moschitti, 2011). More recent work relaxes the Marko-
vian assumption of CRFs to capture phrase-level inter-
actions, significantly improving upon the token-level
labeling approach (Yang and Cardie, 2012). In par-
ticular, Yang and Cardie (2013) propose a joint infer-
ence model to jointly detect opinion expressions, opin-
ion holders and targets, as well as the relations among
them, outperforming previous pipelined approaches.
Deep learning. Recurrent neural networks (Elman,
1990) constitute one important class of naturally deep
architecture that has been applied to many sequential
prediction tasks. In the context of NLP, recurrent neu-
ral networks view a sentence as a sequence of tokens
and have been successfully applied to tasks such as lan-
guage modeling (Mikolov et al., 2011) and spoken lan-
guage understanding (Mesnil et al., 2013). Since clas-
sical recurrent neural networks only incorporate infor-
mation from the past (i.e. preceding tokens), bidirec-
tional variants have been proposed to incorporate in-
formation from both the past and the future (i.e. sub-
sequent tokens) (Schuster and Paliwal, 1997). Bidirec-
tionality is especially useful for NLP tasks, since infor-
mation provided by the following tokens is generally
helpful (and sometimes essential) when making a deci-
sion on the current token.
Stacked recurrent neural networks have been pro-
posed as a way of constructing deep RNNs (Schmidhu-
ber, 1992; El Hihi and Bengio, 1995). Careful empir-
ical investigation of this architecture showed that mul-
tiple layers in the stack can operate at different time
scales (Hermans and Schrauwen, 2013). Pascanu et al.
(2013) explore other ways of constructing deep RNNs
that are orthogonal to the concept of stacking layers on
top of each other. In this work, we focus on the stacking
notion of depth.
3 Methodology
This section describes the architecture and training
methods for the deep bidirectional recurrent networks
that we propose for the task of opinion expression min-
ing. Recurrent neural networks are presented in 3.1,
bidirectionality is introduced in 3.2, and deep bidirec-
tional RNNs, in 3.3.
3.1 Recurrent Neural Networks
A recurrent neural network (Elman, 1990) is a class of
neural network that has recurrent connections, which
allow a form of memory. This makes them applica-
ble for sequential prediction tasks with arbitrary spatio-
temporal dimensions. Thus, their structure fits many
NLP tasks, when the interpretation of a single sentence
is viewed as analyzing a sequence of tokens. In this
work, we focus our attention on only Elman-type net-
works (Elman, 1990).
In an Elman-type network, the hidden layer h
t
at
time step t is computed from a nonlinear transforma-
tion of the current input layer x
t
and the previous hid-
den layer h
t?1
. Then, the final output y
t
is computed
using the hidden layer h
t
. One can interpret h
t
as an in-
termediate representation summarizing the past, which
is used to make a final decision on the current input.
More formally, given a sequence of vectors
{x
t
}
t=1..T
, an Elman-type RNN operates by comput-
ing the following memory and output sequences:
h
t
= f(Wx
t
+ V h
t?1
+ b) (1)
y
t
= g(Uh
t
+ c) (2)
where f is a nonlinear function, such as the sigmoid
function and g is the output nonlinearity, such as the
721
Figure 1: Recurrent neural networks. Each black, orange and red node denotes an input, hidden or output layer,
respectively. Solid and dotted lines denote the connections of forward and backward layers, respectively. Top:
Shallow unidirectional (left) and bidirectional (right) RNN. Bottom: 3-layer deep unidirectional (left) and bidirec-
tional (right) RNN.
softmax function. W and V are weight matrices be-
tween the input and hidden layer, and among the hidden
units themselves (connecting the previous intermediate
representation to the current one), respectively, while
U is the output weight matrix. b and c are bias vec-
tors connected to hidden and output units, respectively.
As a base case for the recursion in Equation 1, h
0
is
assumed to be 0.
Training an RNN can be done by optimizing a dis-
criminative objective (e.g. the cross entropy for classifi-
cation tasks) with a gradient-based method. Backprop-
agation through time can be used to efficiently com-
pute the gradients (Werbos, 1990). This method is es-
sentially equivalent to unfolding the network in time
and using backpropagation as in feedforward neural
networks, while sharing the connection weights across
different time steps. The Elman-style RNN is shown in
Figure 1, top left.
3.2 Bidirectionality
Observe that with the above definition of RNNs, we
have information only about the past, when making a
decision on x
t
. This is limiting for most NLP tasks.
As a simple example, consider the two sentences: ?I
did not accept his suggestion? and ?I did not go to
the rodeo?. The first has a DSE phrase (?did not ac-
cept?) and the second does not. However, any such
RNN will assign the same labels for the words ?did?
and ?not? in both sentences, since the preceding se-
quences (past) are the same: the Elman-style unidirec-
tional RNNs lack the representational power to model
this task. A simple way to work around this problem
is to include a fixed-size future context around a single
input vector (token). However, this approach requires
tuning the context size, and ignores future information
from outside of the context window. Another way to
incorporate information about the future is to add bidi-
rectionality to the architecture, referred as the bidirec-
tional RNN (Schuster and Paliwal, 1997):
??
h
t
= f(
??
Wx
t
+
??
V
??
h
t?1
+
??
b ) (3)
??
h
t
= f(
??
Wx
t
+
??
V
??
h
t+1
+
??
b ) (4)
y
t
= g(U
?
??
h
t
+ U
?
??
h
t
+ c) (5)
where
??
W ,
??
V and
??
b are the forward weight matri-
ces and bias vector as before;
??
W ,
??
V and
??
b are their
backward counterparts; U
?
, U
?
are the output ma-
trices; and c is the output bias.
1
Again, we assume
??
h
0
=
??
h
T+1
= 0. In this setting
??
h
t
and
??
h
t
can
be interpreted as a summary of the past, and the future,
respectively, around the time step t. When we make
a decision on an input vector, we employ the two in-
termediate representations
??
h
t
and
??
h
t
of the past and
1
As a convention, we adopt the following notation
throughout the paper: Superscript arrows for vectors disam-
biguate between forward and backward representations. Su-
perscript arrows for matrices denote the resulting vector rep-
resentations (connection outputs), and subscript arrows for
matrices denote incoming vector representations (connection
inputs). We omit subscripts when there is no ambiguity.
722
the future. (See Figure 1, top right.) Therefore in the
bidirectional case, we have perfect information about
the sequence (ignoring the practical difficulties about
capturing long term dependencies, caused by vanishing
gradients), whereas the classical Elman-type network
uses only partial information as described above.
Note that the forward and backward parts of the net-
work are independent of each other until the output
layer when they are combined. This means that during
training, after backpropagating the error terms from the
output layer to the forward and backward hidden lay-
ers, the two parts can be thought of as separate, and
each trained with the classical backpropagation through
time (Werbos, 1990).
3.3 Depth in Space
Recurrent neural networks are often characterized as
having depth in time: when unfolded, they are equiv-
alent to feedforward neural networks with as many
hidden layers as the number tokens in the input se-
quence (with shared connections across multiple layers
of time). However, this notion of depth likely does not
involve hierarchical processing of the data: across dif-
ferent time steps, we repeatedly apply the same trans-
formation to compute the memory contribution of the
input (W ), to compute the response value from the cur-
rent memory (U ) and to compute the next memory vec-
tor from the previous one (V ). Therefore, assuming the
input vectors {x
t
} together lie in the same representa-
tion space, as do the output vectors {y
t
}, hidden rep-
resentations {h
t
} lie in the same space as well. As a
result, they do not necessarily become more and more
abstract, hierarchical representations of one another as
we traverse in time. However in the more conventional,
stacked deep learners (e.g. deep feedforward nets), an
important benefit of depth is the hierarchy among hid-
den representations: every hidden layer conceptually
lies in a different representation space, and constitutes
a more abstract and higher-level representation of the
input (Bengio, 2009).
In order to address these concerns, we investi-
gate deep RNNs, which are constructed by stacking
Elman-type RNNs on top of each other (Hermans and
Schrauwen, 2013). Intuitively, every layer of the deep
RNN treats the memory sequence of the previous layer
as the input sequence, and computes its own memory
representation.
More formally, we have:
??
h
(i)
t
= f(
??
W
(i)
?
??
h
(i?1)
t
+
??
W
(i)
?
??
h
(i?1)
t
+
??
V
(i)
??
h
(i)
t?1
+
??
b
(i)
) (6)
??
h
(i)
t
= f(
??
W
(i)
?
??
h
(i?1)
t
+
??
W
(i)
?
??
h
(i?1)
t
+
??
V
(i)
??
h
(i)
t+1
+
??
b
(i)
) (7)
when i > 1 and
??
h
(1)
t
= f(
??
W
(1)
x
t
+
??
V
(1)
??
h
(1)
t?1
+
??
b
(1)
) (8)
??
h
(1)
t
= f(
??
W
(1)
x
t
+
??
V
(1)
??
h
(1)
t+1
+
??
b
(1)
) (9)
Importantly, note that both forward and backward rep-
resentations are employed when computing the forward
and backward memory of the next layer.
Two alternatives for the output layer computations
are to employ all memory layers or only the last. In
this work we adopt the second approach:
y
t
= g(U
?
??
h
(L)
t
+ U
?
??
h
(L)
t
+ c) (10)
whereL is the number of layers. Intuitively, connecting
the output layer to only the last hidden layer forces the
architecture to capture enough high-level information
at the final layer for producing the appropriate output-
layer decision.
Training a deep RNN can be conceptualized as in-
terleaved applications of the conventional backprop-
agation across multiple layers, and backpropagation
through time within a single layer.
The unidirectional and bidirectional deep RNNs are
depicted in the bottom half of Figure 1.
Hypotheses. In general, we expected that the deep
RNNs would show the most improvement over shal-
low RNNS for ESEs ? phrases that implicitly convey
subjectivity. Existing research has shown that these
are harder to identify than direct expressions of sub-
jectivity (DSEs): they are variable in length and in-
volve terms that, in many (or most) contexts, are neu-
tral with respect to sentiment and subjectivity. As a re-
sult, models that do a better job interpreting the context
should be better at disambiguating subjective vs. non-
subjective uses of phrases involving common words
(e.g. ?as usual?, ?in fact?). Whether or not deep RNNs
would be powerful enough to outperform the state-of-
the-art semiCRF was unclear, especially if the semi-
CRF is given access to the distributed word represen-
tations (embeddings) employed by the deep RNNs. In
addition, the semiCRF has access to parse tree informa-
tion and opinion lexicons, neither of which is available
to the deep RNNs.
4 Experiments
Activation Units. We employ the standard softmax
activation for the output layer: g(x) = e
x
i
/
?
j
e
x
j
.
For the hidden layers we use the rectifier linear ac-
tivation: f(x) = max{0, x}. Experimentally, recti-
fier activation gives better performance, faster conver-
gence, and sparse representations. Previous work also
reported good results when training deep neural net-
works using rectifiers, without a pretraining step (Glo-
rot et al., 2011).
Data. We use the MPQA 1.2 corpus (Wiebe et al.,
2005) (535 news articles, 11,111 sentences) that is
manually annotated with both DSEs and ESEs at the
phrase level. As in previous work, we separate 135
documents as a development set and employ 10-fold
CV over the remaining 400 documents. The develop-
ment set is used during cross validation to do model
selection.
723
Layers |h| Precision Recall F1
Prop. Bin. Prop. Bin. Prop Bin.
Shallow 36 62.24 65.90 65.63* 73.89* 63.83 69.62
Deep 2 29 63.85* 67.23* 65.70* 74.23* 64.70* 70.52*
Deep 3 25 63.53* 67.67* 65.95* 73.87* 64.57* 70.55*
Deep 4 22 64.19* 68.05* 66.01* 73.76* 64.96* 70.69*
Deep 5 21 60.65 61.67 56.83 69.01 58.60 65.06
Shallow 200 62.78 66.28 65.66* 74.00* 64.09 69.85
Deep 2 125 62.92* 66.71* 66.45* 74.70* 64.47 70.36
Deep 3 100 65.56* 69.12* 66.73* 74.69* 66.01* 71.72*
Deep 4 86 61.76 65.64 63.52 72.88* 62.56 69.01
Deep 5 77 61.64 64.90 62.37 72.10 61.93 68.25
Table 2: Experimental evaluation of RNNs for DSE extraction
Layers |h| Precision Recall F1
Prop. Bin. Prop. Bin. Prop Bin.
Shallow 36 51.34 59.54 57.60 72.89* 54.22 65.44
Deep 2 29 51.13 59.94 61.20* 75.37* 55.63* 66.64*
Deep 3 25 53.14* 61.46* 58.01 72.50 55.40* 66.36*
Deep 4 22 51.48 60.59* 59.25* 73.22 54.94 66.15*
Deep 5 21 49.67 58.42 48.98 65.36 49.25 61.61
Shallow 200 52.20* 60.42* 58.11 72.64 54.75 65.75
Deep 2 125 51.75* 60.75* 60.69* 74.39* 55.77* 66.79*
Deep 3 100 52.04* 60.50* 61.71* 76.02* 56.26* 67.18*
Deep 4 86 50.62* 58.41* 53.55 69.99 51.98 63.60
Deep 5 77 49.90* 57.82 52.37 69.13 51.01 62.89
Table 3: Experimental evaluation of RNNs for ESE extraction
Evaluation Metrics. We use precision, recall and F-
measure for performance evaluation. Since the bound-
aries of expressions are hard to define even for human
annotators (Wiebe et al., 2005), we use two soft notions
of the measures: Binary Overlap counts every over-
lapping match between a predicted and true expres-
sion as correct (Breck et al., 2007; Yang and Cardie,
2012), and Proportional Overlap imparts a partial cor-
rectness, proportional to the overlapping amount, to
each match (Johansson and Moschitti, 2010; Yang and
Cardie, 2012). All statistical comparisons are done us-
ing a two-sided paired t-test with a confidence level of
? = .05.
Baselines (CRF and SEMICRF). As baselines, we
use the CRF-based method of Breck et al. (2007)
and the SEMICRF-based method of Yang and Cardie
(2012), which is the state-of-the-art in opinion expres-
sion extraction. Features that the baselines use are
words, part-of-speech tags and membership in a manu-
ally constructed opinion lexicon (within a [-1, +1] con-
text window). Since SEMICRF relaxes the Markovian
assumption and operates at the segment-level instead
of the token-level, it also has access to parse trees of
sentences to generate candidate segments (Yang and
Cardie, 2012).
Word Vectors (+VEC). We also include versions of
the baselines that have access to pre-trained word vec-
tors. In particular, CRF+VEC employs word vectors
as continuous features per every token. Since SEMI-
CRF has phrase-level rather than word-level features,
we simply take the mean of every word vector for a
phrase-level vector representation for SEMICRF+VEC
as suggested in Mikolov et al. (2013).
In all of our experiments, we keep the word vec-
tors fixed (i.e. do not finetune) to reduce the degree
of freedom of our models. We use the publicly avail-
able 300-dimensional word vectors of Mikolov et al.
(2013), trained on part of the Google News dataset
(?100B words). Preliminary experiments with other
word vector representations such as Collobert-Weston
(2008) embeddings or HLBL (Mnih and Hinton, 2007)
provided poorer results (? ?3% difference in propor-
tional and binary F1).
Regularizer. We do not employ any regularization
for smaller networks (?24,000 parameters) because we
have not observed strong overfitting (i.e. the differ-
ence between training and test performance is small).
Larger networks are regularized with the recently pro-
posed dropout technique (Hinton et al., 2012): we ran-
domly set entries of hidden representations to 0 with
a probability called the dropout rate, which is tuned
over the development set. Dropout prevents learned
724
Model Precision Recall F1
Prop. Bin. Prop. Bin. Prop Bin.
DSE CRF 74.96* 82.28* 46.98 52.99 57.74 64.45
semiCRF 61.67 69.41 67.22* 73.08* 64.27 71.15*
CRF +vec 74.97* 82.43* 49.47 55.67 59.59 66.44
semiCRF +vec 66.00 71.98 60.96 68.13 63.30 69.91
Deep RNN 3 100 65.56 69.12 66.73* 74.69* 66.01* 71.72*
ESE CRF 56.08 68.36 42.26 51.84 48.10 58.85
semiCRF 45.64 69.06 58.05 64.15 50.95 66.37*
CRF +vec 57.15* 69.84* 44.67 54.38 50.01 61.01
semiCRF +vec 53.76 70.82* 52.72 61.59 53.10 65.73
Deep RNN 3 100 52.04 60.50 61.71* 76.02* 56.26* 67.18*
Table 4: Comparison of Deep RNNs to state-of-the-art (semi)CRF baselines for DSE and ESE detection
features from co-adapting, and it has been reported
to yield good results when training deep neural net-
works (Krizhevsky et al., 2012; Dahl et al., 2013).
Network Training. We use the standard multiclass
cross-entropy as the objective function when training
the neural networks. We use stochastic gradient de-
scent with momentum with a fixed learning rate (.005)
and a fixed momentum rate (.7). We update weights
after minibatches of 80 sentences. We run 200 epochs
for training. Weights are initialized from small random
uniform noise. We experiment with networks of vari-
ous sizes, however we have the same number of hidden
units across multiple forward and backward hidden lay-
ers of a single RNN. We do not employ a pre-training
step; deep architectures are trained with the supervised
error signal, even though the output layer is connected
to only the final hidden layer. With these configura-
tions, every architecture successfully converges with-
out any oscillatory behavior. Additionally, we employ
early stopping for the neural networks: out of all itera-
tions, the model with the best development set perfor-
mance (Proportional F1) is selected as the final model
to be evaluated.
4.1 Results and Discussion
Bidirectional vs. Unidirectional. Although our fo-
cus is on bidirectional RNNs, we first confirm that the
SHALLOW bidirectional RNN outperforms a (shallow)
unidirectional RNN for both DSE and ESE recogni-
tion. To make the comparison fair, each network has
the same number of total parameters: we use 65 hid-
den units for the unidirectional, and 36 for the bidirec-
tional network, respectively. Results are as expected:
the bidirectional RNN obtains higher F1 scores than the
unidirectional RNN ? 63.83 vs. 60.35 (proportional
overlap) and 69.62 vs. 68.31 (binary overlap) for DSEs;
54.22 vs. 51.51 (proportional) and 65.44 vs. 63.65 (bi-
nary) for ESEs. All differences are statistically signif-
icant at the 0.05 level. Thus, we will not include com-
parisons to the unidirectional RNNs in the remaining
experiments.
Adding Depth. Next, we quantitatively investigate
the effects of adding depth to RNNs. Tables 2
and 3 show the evaluation of RNNs of various depths
and sizes. In both tables, the first group networks
have approximately 24,000 parameters and the second
group networks have approximately 200,000 parame-
ters. Since all RNNs within a group have approxi-
mately the same number of parameters, they grow nar-
rower as they get deeper. Within each group, bold
shows the best result with an asterisk denoting statis-
tically indistinguishable performance with respect to
the best. As noted above, all statistical comparisons
use a two-sided paired t-test with a confidence level of
? = .05.
In both DSE and ESE detection and for larger net-
works (bottom set of results), 3-layer RNNs provide the
best results. For smaller networks (top set of results),
2, 3 and 4-layer RNNs show equally good performance
for certain sizes and metrics and, in general, adding ad-
ditional layers degrades performance. This could be re-
lated to how we train the architectures as well as to the
decrease in width of the networks. In general, we ob-
serve a trend of increasing performance as we increase
the number of layers, until a certain depth.
deepRNNs vs. (semi)CRF. Table 4 shows compari-
son of the best deep RNNs to the previous best results
in the literature. In terms of F-measure, DEEP RNN
performs best for both DSE and ESE detection, achiev-
ing a new state-of-the-art performance for the more
strict proportional overlap measure, which is harder to
improve upon than the binary evaluation metric. SEMI-
CRF, with its very high recall, performs comparably to
the DEEP RNN on the binary metric. Note that RNNs
do not have access to any features other than word vec-
tors.
In general, CRFs exhibit high precision but low re-
call (CRFs have the best precision on both DSE and
ESE detection) while SEMICRFs exhibit a high re-
call, low precision performance. Compared to SEMI-
CRF, the DEEP RNNs produce an even higher recall
but sometimes lower precision for ESE detection. This
suggests that the methods are complementary, and can
725
(1)
The situation obviously remains fluid from hour to hour but it [seems to be] [going in the right direction]
DEEPRNN The situation [obviously] remains fluid from hour to hour but it [seems to be going in the right] direction
SHALLOW The situation [obviously] remains fluid from hour to hour but it [seems to be going in] the right direction
SEMICRF The situation [obviously remains fluid from hour to hour but it seems to be going in the right direction]
(2)
have always said this is a multi-faceted campaign [but equally] we have also said any future military action
[would have to be based on evidence] , ...
DEEPRNN have always said this is a multi-faceted campaign but [equally we] have also said any future military action
[would have to be based on evidence] , ...
SHALLOW have always said this is a multi-faceted [campaign but equally we] have also said any future military action
would have to be based on evidence , ...
SEMICRF have always said this is a multi-faceted campaign but equally we have also said any future military action
would have to be based on evidence , ...
(3)
Ruud Lubbers , the United Nations Commissioner for Refugees , said Afghanistan was [not yet] secure
for aid agencies to operate in and ? [not enough] ? food had been taken into the country .
DEEPRNN Ruud Lubbers , the United Nations Commissioner for Refugees , said Afghanistan was [not yet] secure
for aid agencies to operate in and ? [not enough] ? food had been taken into the country .
SHALLOW Ruud Lubbers , the United Nations Commissioner for Refugees , said Afghanistan was [not yet] secure
for aid agencies to operate in and ? [not enough] ? food had been taken into the country .
SEMICRF Ruud Lubbers , the United Nations Commissioner for Refugees , said Afghanistan was not yet secure
for aid agencies to operate in and ? not enough ? food had been taken into the country .
Figure 2: Examples of output. In each set, the gold-standard annotations are shown in the first line.
potentially be even more powerful when combined in
an ensemble method.
Word vectors. Word vectors help CRFs on both pre-
cision and recall on both tasks. However, SEMICRFs
become more conservative with word vectors, produc-
ing higher precision and lower recall on both tasks.
This sometimes hurts overall F-measure.
Among the (SEMI)CRF-based methods, SEMICRF
obtains the highest F1 score for DSEs and for ESEs
using the softer metric; SEMICRF+VEC performs best
for ESEs according to the stricter proportional overlap
measure.
Network size. Finally, we observe that even small
networks (such as 4-layer deep RNN for DSE and
2-layer deep RNN for ESE) outperform conventional
CRFs. This suggests that with the help of good word
vectors, we can train compact but powerful sequential
neural models.
When examining the output, we see some system-
atic differences between the previously top-performing
SEMICRF and the RNN-based models. (See Figure 2.)
First, SEMICRF often identifies excessively long sub-
jective phrases as in Example 1. Here, none of the mod-
els exactly matches the gold standard, but the RNNs
are much closer. And all three models appear to have
identified an ESE that was mistakenly omitted by the
human annotator ? ?obviously?. At the same time,
the SEMICRF sometimes entirely misses subjective ex-
pressions that the RNNs identify ? this seems to occur
when there are no clear indications of sentiment in the
subjective expression. The latter can be seen in Exam-
ples 2 and 3, in which the SEMICRF does not identify
?but equally?, ?would have to be based on evidence?,
?not yet?, and ?not enough?.
We also observe evidence of the power of the DEEP-
RNN over the SHALLOWRNN in Examples 4 and 5.
(See Figure 3.) In contrast to Figure 2, Figure 3 dis-
tinguishes subjective expressions that are (correctly)
assigned an initial Begin label from those that con-
sist only of Inside labels
2
? the latter are shown in
ALL CAPS and indicate some degree of confusion in
the model that produced them. In Example 4, SHAL-
LOWRNN exhibits some evidence for each ESE ? it
labels one or more tokens as Inside an ESE (?any? and
?time?). But it does not explicitly tag the beginning
of the ESE. DEEPRNN does better, identifying the first
ESE in its entirety (?in any case?) and identifying more
words as being Inside the second ESE (?it is high time).
A similar situation occurs in Example 5.
5 Conclusion
In this paper we have explored an application of deep
recurrent neural networks to the task of sentence-level
opinion expression extraction. We empirically evalu-
ated deep RNNs against conventional, shallow RNNs
that have only a single hidden layer. We also com-
pared our models with previous (semi)CRF-based ap-
proaches.
Experiments showed that deep RNNs outperformed
shallow RNNs on both DSE and ESE extrac-
2
Sequences of I?s are decoded as the associated DSE or
ESE even though they lack the initial B.
726
(4)
[In any case] , [it is high time] that a social debate be organized ...
DEEPRNN [In any case] , it is HIGH TIME that a social debate be organized ...
SHALLOW In ANY case , it is high TIME that a social debate be organized ...
(5)
Mr. Stoiber [has come a long way] from his refusal to [sacrifice himself] for the CDU in an election that
[once looked impossible to win] , through his statement that he would [under no circumstances]
run against the wishes...
DEEPRNN Mr. Stoiber [has come a long way from] his [refusal to sacrifice himself] for the CDU in an election that
[once looked impossible to win] , through his statement that he would [under no circumstances
run against] the wishes...
SHALLOW Mr. Stoiber has come A LONG WAY FROM his refusal to sacrifice himself for the CDU in an election that
[once looked impossible] to win , through his statement that he would under NO CIRCUMSTANCES
run against the wishes...
Figure 3: DEEPRNN Output vs. SHALLOWRNN Output. In each set of examples, the gold-standard annotations
are shown in the first line. Tokens assigned a label of Inside with no preceding Begin tag are shown in ALL CAPS.
tion. Furthermore, deep RNNs outperformed previous
(semi)CRF baselines, achieving new state-of-the-art re-
sults for fine-grained on opinion expression extraction.
We have trained our deep networks without any pre-
training and with only the last hidden layer connected
to the output layer. One potential future direction is
to explore the effects of pre-training on the architec-
ture. Pre-training might help to exploit the additional
representational power available in deeper networks.
Another direction is to investigate the impact of fine-
tuning the word vectors during supervised training.
Additionally, alternative notions of depth that are or-
thogonal to stacking, as in Pascanu et al. (2013) can be
investigated for this task.
Acknowledgments
This work was supported in part by NSF grant IIS-
1314778 and DARPA DEFT Grant FA8750-13-2-0015.
The views and conclusions contained herein are those
of the authors and should not be interpreted as necessar-
ily representing the official policies or endorsements,
either expressed or implied, of NSF, DARPA or the
U.S. Government.
References
Yoshua Bengio. 2009. Learning deep architectures for
ai. Foundations and trends
R
? in Machine Learning,
2(1):1?127.
Eric Breck, Yejin Choi, and Claire Cardie. 2007. Iden-
tifying expressions of opinion in context. In IJCAI,
pages 2683?2688.
Yejin Choi and Claire Cardie. 2010. Hierarchical se-
quential learning for extracting opinions and their at-
tributes. In Proceedings of the ACL 2010 Conference
Short Papers, pages 269?274. Association for Com-
putational Linguistics.
Yejin Choi, Claire Cardie, Ellen Riloff, and Siddharth
Patwardhan. 2005. Identifying sources of opinions
with conditional random fields and extraction pat-
terns. In Proceedings of HLT/EMNLP, pages 355?
362. Association for Computational Linguistics.
Ronan Collobert and Jason Weston. 2008. A unified
architecture for natural language processing: Deep
neural networks with multitask learning. In Pro-
ceedings of the 25th international conference on Ma-
chine learning, pages 160?167. ACM.
George E Dahl, Tara N Sainath, and Geoffrey E Hin-
ton. 2013. Improving deep neural networks for lvcsr
using rectified linear units and dropout. In Acous-
tics, Speech and Signal Processing (ICASSP), 2013
IEEE International Conference on, pages 8609?
8613. IEEE.
Salah El Hihi and Yoshua Bengio. 1995. Hierarchical
recurrent neural networks for long-term dependen-
cies. In Advances in Neural Information Processing
Systems, pages 493?499.
Jeffrey L Elman. 1990. Finding structure in time.
Cognitive science, 14(2):179?211.
Xavier Glorot, Antoine Bordes, and Yoshua Bengio.
2011. Deep sparse rectifier networks. In Proceed-
ings of the 14th International Conference on Arti-
ficial Intelligence and Statistics. JMLR W&CP Vol-
ume, volume 15, pages 315?323.
Michiel Hermans and Benjamin Schrauwen. 2013.
Training and analysing deep recurrent neural net-
works. In Advances in Neural Information Process-
ing Systems, pages 190?198.
Geoffrey E Hinton, Nitish Srivastava, Alex Krizhevsky,
Ilya Sutskever, and Ruslan R Salakhutdinov. 2012.
Improving neural networks by preventing co-
adaptation of feature detectors. arXiv preprint
arXiv:1207.0580.
Richard Johansson and Alessandro Moschitti. 2010.
Syntactic and semantic structure for opinion ex-
pression detection. In Proceedings of the Four-
teenth Conference on Computational Natural Lan-
727
guage Learning, pages 67?76. Association for Com-
putational Linguistics.
Richard Johansson and Alessandro Moschitti. 2011.
Extracting opinion expressions and their polarities:
exploration of pipelines and joint models. In Pro-
ceedings of the 49th Annual Meeting of the Associ-
ation for Computational Linguistics: Human Lan-
guage Technologies: short papers-Volume 2, pages
101?106. Association for Computational Linguis-
tics.
Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hin-
ton. 2012. Imagenet classification with deep convo-
lutional neural networks. In NIPS, volume 1, page 4.
Gr?egoire Mesnil, Xiaodong He, Li Deng, and Yoshua
Bengio. 2013. Investigation of recurrent-neural-
network architectures and learning methods for spo-
ken language understanding. Interspeech.
Tomas Mikolov, Martin Karafi?at, Lukas Burget, Jan
Cernock`y, and Sanjeev Khudanpur. 2010. Recur-
rent neural network based language model. In IN-
TERSPEECH, pages 1045?1048.
Tomas Mikolov, Stefan Kombrink, Lukas Burget,
JH Cernocky, and Sanjeev Khudanpur. 2011.
Extensions of recurrent neural network language
model. In Acoustics, Speech and Signal Processing
(ICASSP), 2011 IEEE International Conference on,
pages 5528?5531. IEEE.
Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Cor-
rado, and Jeff Dean. 2013. Distributed representa-
tions of words and phrases and their compositional-
ity. In Advances in Neural Information Processing
Systems, pages 3111?3119.
Andriy Mnih and Geoffrey Hinton. 2007. Three new
graphical models for statistical language modelling.
In Proceedings of the 24th international conference
on Machine learning, pages 641?648. ACM.
M Arthur Munson, Claire Cardie, and Rich Caruana.
2005. Optimizing to arbitrary nlp metrics using en-
semble selection. In Proceedings of HLT/EMNLP,
pages 539?546. Association for Computational Lin-
guistics.
Razvan Pascanu, C?a?glar G?ulc?ehre, Kyunghyun Cho,
and Yoshua Bengio. 2013. How to construct
deep recurrent neural networks. arXiv preprint
arXiv:1312.6026.
J?urgen Schmidhuber. 1992. Learning complex, ex-
tended sequences using the principle of history com-
pression. Neural Computation, 4(2):234?242.
Mike Schuster and Kuldip K Paliwal. 1997. Bidirec-
tional recurrent neural networks. Signal Processing,
IEEE Transactions on, 45(11):2673?2681.
Joseph Turian, Lev Ratinov, and Yoshua Bengio. 2010.
Word representations: a simple and general method
for semi-supervised learning. In Proceedings of the
48th Annual Meeting of the Association for Compu-
tational Linguistics, pages 384?394. Association for
Computational Linguistics.
Paul J Werbos. 1990. Backpropagation through time:
what it does and how to do it. Proceedings of the
IEEE, 78(10):1550?1560.
Janyce Wiebe, Theresa Wilson, and Claire Cardie.
2005. Annotating expressions of opinions and emo-
tions in language. Language resources and evalua-
tion, 39(2-3):165?210.
Theresa Wilson, Janyce Wiebe, and Paul Hoff-
mann. 2005. Recognizing contextual polarity in
phrase-level sentiment analysis. In Proceedings of
HLT/EMNLP, pages 347?354. Association for Com-
putational Linguistics.
Bishan Yang and Claire Cardie. 2012. Extracting
opinion expressions with semi-markov conditional
random fields. In Proceedings of the 2012 Joint
Conference on Empirical Methods in Natural Lan-
guage Processing and Computational Natural Lan-
guage Learning, pages 1335?1345. Association for
Computational Linguistics.
Bishan Yang and Claire Cardie. 2013. Joint inference
for fine-grained opinion extraction. In Proceedings
of ACL.
728
Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1997?2007,
October 25-29, 2014, Doha, Qatar.
c
?2014 Association for Computational Linguistics
Major Life Event Extraction from Twitter based on
Congratulations/Condolences Speech Acts
Jiwei Li
1
, Alan Ritter
2
, Claire Cardie
3
and Eduard Hovy
4
1
Computer Science Department, Stanford University, Stanford, CA 94305, USA
2
Department of Computer Science and Engineering, the Ohio State University, OH 43210, USA
3
Computer Science Department, Cornell University, Ithaca, NY 14853, USA
4
Language Technology Institute, Carnegie Mellon University, PA 15213, USA
jiweil@stanford.edu ritter.1492@osu.edu
cardie@cs.cornell.edu ehovy@andrew.cmu.edu
Abstract
Social media websites provide a platform
for anyone to describe significant events
taking place in their lives in realtime.
Currently, the majority of personal news
and life events are published in a tex-
tual format, motivating information ex-
traction systems that can provide a struc-
tured representations of major life events
(weddings, graduation, etc. . . ). This pa-
per demonstrates the feasibility of accu-
rately extracting major life events. Our
system extracts a fine-grained description
of users? life events based on their pub-
lished tweets. We are optimistic that our
system can help Twitter users more easily
grasp information from users they take in-
terest in following and also facilitate many
downstream applications, for example re-
altime friend recommendation.
1 Introduction
Social networking websites such as Facebook and
Twitter have recently challenged mainstream me-
dia as the freshest source of information on im-
portant news events. In addition to an important
source for breaking news, social media presents a
unique source of information on private events, for
example a friend?s engagement or college gradua-
tion (examples are presented in Figure 1). While
a significant amount of previous work has inves-
tigated event extraction from Twitter (e.g., (Rit-
ter et al., 2012; Diao et al., 2012)), existing ap-
proaches mostly focus on public bursty event ex-
traction, and little progress has been made towards
the problem of automatically extracting the major
life events of ordinary users.
A system which can automatically extract ma-
jor life events and generate fine-grained descrip-
tions as in Figure 1 will not only help Twitter
users with the problem of information overload by
summarizing important events taking place in their
friends lives, but could also facilitate downstream
applications such as friend recommendation (e.g.,
friend recommendation in realtime to people who
were just admitted into the same university, get
the same jobs or internships), targeted online ad-
vertising (e.g., recommend baby care products to
newly expecting mothers, or wedding services to
new couples), information extraction, etc.
Before getting started, we first identify a num-
ber of key challenges in extracting significant life
events from user-generated text, which account the
reason for the lack of previous work in this area:
Challenge 1: Ambiguous Definition for Ma-
jor Life Events Major life event identification
is an open-domain problem. While many types of
events (e.g., marriage, engagement, finding a new
job, giving birth) are universally agreed to be im-
portant, it is difficult to robustly predefine a list of
characteristics for important life events on which
algorithms can rely for extraction or classification.
Challenge 2: Noisiness of Twitter Data: The
user-generated text found in social media websites
such as Twitter is extremely noisy. The language
used to describe life events is highly varied and
ambiguous and social media users frequently dis-
cuss public news and mundane events from their
daily lives, for instance what they ate for lunch.
Even for a predefined life event category, such
as marriage, it is still difficult to accurately iden-
tify mentions. For instance, a search for the
keyphrase ?get married? using Twitter Search
1
re-
sults in a large number of returned results that do
not correspond to a personal event:
? I want to get married once. No divorce & no
cheating, just us two till the end.
(error: wishes)
1
https://twitter.com/search?q=
get
?
married
1997
Figure 1: Examples of users mentioning personal life events on Twitter.
? Can Adam Sandler and Drew Barrymore just
drop the pretense and get married already?
(error: somebody else)
? I got married and had kids on purpose
(error: past)
Challenge 3: the Lack of Training Data Col-
lecting sufficient training data in this task for ma-
chine learning models is difficult for a number of
reasons: (1) A traditional, supervised learning ap-
proach, requires explicit annotation guidelines for
labeling, though it is difficult to know which cat-
egories are most representative in the data apriori.
(2) Unlike public events which are easily identi-
fied based on message volume, significant private
events are only mentioned by one or several users
directly involved in the event. Many important cat-
egories are relatively infrequent, so even a large
annotated dataset may contain just a few or no ex-
amples of these categories, making classification
difficult.
In this paper, we present a pipelined system that
addresses these challenges and extracts a struc-
tured representation of individual life events based
on users? Twitter feeds. We exploit the insight to
automatically gather large volumes of major life
events which can be used as training examples for
machine learning models. Although personal life
events are difficult to identify using traditional
approaches due to their highly diverse nature, we
noticed that users? followers often directly reply
to such messages with CONGRATULATIONS or
CONDOLENCES speech acts, for example:
User1: I got accepted into Harvard !
User2: Congratulations !
These speech acts are easy to identify with high
precision because the possible ways to express
them are relatively constrained. Instead of directly
inspecting tweets to determine whether they corre-
spond to major life events, we start by identifying
replies corresponding to CONGRATULATIONS or
CONDOLENCES, and then retrieve the message
they are in response to, which we assume refer to
important life events.
The proposed system automatically identifies
major life events and then extracts correspondent
event properties. Through the proposed system,
we demonstrate that it is feasible to automatically
reconstruct a detailed list of individual life events
based on users? Twitter streams. We hope that
work presented in this paper will facilitate down-
stream applications and encourage follow-up work
on this task.
2 System Overview
An overview of the components of the system is
presented in Figure 2. Pipeline1 first identifies
the major life event category the input tweet talks
about and filters out the irrelevant tweets and will
be described in Section 4. Next, Pipeline2, as,
demonstrated in Section 5, identifies whether the
speaker is directly involved in the life event. Fi-
nally, Pipeline3 extracts the property of event and
will be illustrated in Section 6.
Section 3 serves as the preparing step for the
pipelined system, describing how we collect train-
ing data in large-scale. The experimental evalua-
tion regarding each pipeline of the system is pre-
sented in the corresponding section (i.e., Section
4,5,6) and the end-to-end evaluation will be pre-
1998
Figure 2: System Overview. Blue: original input tweets. Red: filtered out tweets. Magenta: life event
category. Green: life event property. Pipeline 1 identifies the life category the input tweet talks about
(e.g., marriage, graduation) and filter out irrelevant tweets (e.g., I had beef stick for lunch). Pipeline 2
identifies whether the speaker is directly involved in the event. It will preserve self-reported information
(i.e. ?I got married?) and filtered out unrelated tweets (e.g., ?my friend Chris got married?). Pipeline
3 extracts the property of event (e.g. to whom the speaker married or the speaker admitted by which
university).
sented in Section 7.
3 Personal Life Event Clustering
In this section, we describe how we identify com-
mon categories of major life events by leverag-
ing large quantities of unlabeled data and obtain
a collection of tweets corresponding to each type
of identified event.
3.1 Response based Life Event Detection
While not all major life events will elicit CON-
GRATULATIONS or CONDOLENCES from a user?s
followers, this technique allows us to collect large
volumes of high-precision personal life events
which can be used to train models to recognize the
diverse categories of major life events discussed
by social media users.
3.2 Life Event Clustering
Based on the above intuition, we develop an ap-
proach to obtain a list of individual life event clus-
ters. We first define a small set of seed responses
which capture common CONGRATULATIONS and
CONDOLENCES, including the phrases: ?Congrat-
ulations?, ?Congrats?, ?Sorry to hear that?, ?Awe-
some?, and gather tweets that were observed with
seed responses. Next, an LDA (Blei et al., 2003)
2
based topic model is used to cluster the gathered
2
Topic Number is set to 120.
tweets to automatically identify important cate-
gories of major life events in an unsupervised way.
In our approach, we model the whole conversation
dialogue as a document
3
with the response seeds
(e.g., congratulation) masked out. We furthermore
associate each sentence with a single topic, fol-
lowing strategies adopted by (Ritter et al., 2010;
Gruber et al., 2007). We limit the words in our
document collection to verbs and nouns which
we found to lead to clearer topic representations,
and used collapsed Gibbs Sampling for inference
(Griffiths and Steyvers, 2004).
Next one of the authors manually inspected the
resulting major life event types inferred by the
model, and manually assigned them labels such
as ?getting a job?, ?graduation? or ?marriage?
and discarded incoherent topics
4
. Our methodol-
ogy is inspired by (Ritter et al., 2012) that uses
a LDA-CLUSTERING+HUMAN-IDENTIFICATION
strategy to identify public events from Twitter.
Similar strategies have been widely used in un-
supervised information extraction (Bejan et al.,
2009; Yao et al., 2011) and selectional preference
3
Each whole conversation usually contains multiple
tweets and users.
4
While we applied manual labeling and coherence eval-
uation in this work, an interesting direction for future work
is automatically labeling major life event categories follow-
ing previous work on labeling topics in traditional document-
based topic models (Mimno et al., 2011; Newman et al.,
2010).
1999
Figure 3: Illustration of bootstrapping process.
Input: Reply seed list E = {e}, Tweet conversation col-
lection T = {t}, Retrieved Tweets Collection D = ?.
Identified topic list L=?
Begin
While not stopping:
1. For unprocessed conversation t ? T
if t contains reply e ? E,
? add t to D: D = D + t.
? remove t from T : T = T ? t
2. Run streaming LDA (Yao et al., 2009) on newly added
tweets in D.
3. Manually Identify meaningful/trash topics, giving label
to meaningful topics.
4. Add newly detected meaningful topic l to L.
5. For conversation t belonging to trash topics
? remove t from D: D = D ? t
6. Harvest more tweets based on topic distribution.
7. Manually identify top 20 responses to tweets harvested
from Step 6.
8. Add meaningful responses to E.
End
Output: Identified topic list L. Tweet collection D.
Figure 4: Bootstrapping Algorithm for Response-
based Life event identification.
modeling (Kozareva and Hovy, 2010a; Roberts
and Harabagiu, 2011).
Conversation data was extracted from the CMU
Twitter Warehouse of 2011 which contains a total
number of 10% of all published tweets in that year.
3.3 Expanding dataset using Bootstrapping
While our seed patterns for identifying mes-
sages expressing CONGRATULATIONS and CON-
DOLENCES are very high precision, they don?t
cover all the possible ways these speech acts
can be expressed. We therefore adopt a semi-
supervised bootstrapping approach to expand our
reply seeds and event-related tweets. Our boot-
strapping approach is related to previous work
on semi-supervised information harvesting (e.g.,
(Kozareva and Hovy, 2010b; Davidov et al.,
2007)). To preserve the labeled topics from the
first iteration, we apply a streaming approach to
inference (Yao et al., 2009) over unlabeled tweets
(those which did not match one of the response
Figure 5: Illustration of data retrieved in each step
of bootstrapping.
congratulations (cong, congrats); (that?s) fantastic; (so) cool;
(I?m) (very) sorry to hear that; (that?s) great (good) new;
awesome; what a pity; have fun; great; that sucks; too
bad; (that?s) unfortunate; how sad; fabulous; (that?s)
terrific; (that?s) (so) wonderful; my deepest condolences;
Table 1: Responses retrieved from Bootstrapping.
seeds). We collect responses to the newly added
tweets, then select the top 20 frequent replies
5
.
Next we manually inspect and filter the top ranked
replies, and use them to harvest more tweets. This
process is then repeated with another round of
inference in LDA including manual labeling of
newly inferred topics, etc... An illustration of our
approach is presented in Figure 3 and the details
are presented in Figure 4. The algorithm outputs
a collection of personal life topics L, and a collec-
tion of retrieved tweets D. Each tweet d ? D is
associated with a life event topic l, l ? L.
We repeat the bootstrapping process for 4 iter-
ations and end up with 30 different CONGRATU-
LATIONS and CONDOLENCES patterns (shown in
Table 1) and 42 coherent event types which refer to
significant life events (statistics for harvested data
from each step is shown in Figure 5). We show
examples of the mined topics with correspondent
human labels in Table 3, grouped according to a
specific kind of resemblance.
3.4 Summary and Discussion
The objective of this section is (1) identifying a
category of life events (2) identifying tweets asso-
ciated with each event type which can be used as
candidates for latter self reported personal infor-
mation and life event category identification.
We understand that the event list retrieved from
our approach based on replies in the conversation
is far from covering all types of personal events
(especially the less frequent life events). But our
5
We only treat the first sentence that responds to the be-
ginning of the conversation as replies.
2000
Life Event Proportion
Birthday 9.78
Job 8.39
Wedding
Engagement
7.24
Award 6.20
Sports 6.08
Anniversary 5.44
Give Birth 4.28
Graduate 3.86
Death 3.80
Admission 3.54
Interview
Internship
3.44
Moving 3.26
Travel 3.24
Illness 2.45
Life Event Proportion
Vacation 2.24
Relationship 2.16
Exams 2.02
Election 1.85
New Car 1.65
Running 1.42
Surgery 1.20
Lawsuit 0.64
Acting 0.50
Research 0.48
Essay 0.35
Lost Weight 0.35
Publishing 0.28
Song 0.22
OTHER 15.31
Table 2: List of automatically discovered life event
types with percentage (%) of data covered.
list is still able to cover a large proportion of IM-
PORTANT and COMMON life events. Our latter
work is focused on given a random tweet, identi-
fying whether it corresponds to one of the 42 types
of life events in our list.
Another thing worth noting here is that, while
current section is not focused on self-reported in-
formation identification, we have already obtained
a relatively clean set of data with a large pro-
portion of non self-reported information related
tweets being screened: people do not usually re-
spond to non self-reported information with com-
monly used replies, or in other words, with replies
that will pass our next step human test
6
. These non
self-reported tweets would therefore be excluded
from training data.
4 Life Event Identification
In this section, we focused on deciding whether a
given tweet corresponds to one of the 42 prede-
fined life events.
Our training dataset consists of approximately
72,000 tweets from 42 different categories of life
events inferred by our topic model as described
in Section 3. We used the top 25% of tweets for
which our model assigned highest probability to
each topic. For sparsely populated topics we used
the top 50% of tweets to ensure sufficient cover-
age.
We further collected a random sample of about
10 million tweets from Twitter API
7
as non-life
6
For example, people don?t normally respond to ?I want
to get married once? (example in Challenge 2, Section 1)
with ?Congratulations?.
7
https://dev.twitter.com/
Human Label Top words
Wedding
&engagement
wedding, love, ring, engagement,
engaged, bride, video, marrying
Relationship
Begin
boyfriend, girlfriend, date, check,
relationship, see, look
Anniversary anniversary, years, year, married,
celebrating, wife, celebrate, love
Relation End/
Devoice
relationship, ended, hurt, hate, de-
voice, blessings, single
Graduation graduation, school, college, gradu-
ate, graduating, year, grad
Admission admitted, university, admission, ac-
cepted, college, offer, school
Exam passed, exam, test, school,
semester, finished, exams,
midterms
Research research, presentation, journalism,
paper, conference, go, writing
Essay & Thesis essay, thesis, reading, statement,
dissertation, complete, project
Job job, accepted, announce, join, join-
ing, offer, starting, announced,
work
Interview& In-
ternship
interview, position, accepted, in-
ternship, offered, start, work
Moving house, moving, move, city, home,
car, place, apartment, town, leaving
Travel leave, leaving, flight, home, miss,
house, airport, packing, morning
Vacation vocation, family, trip, country, go,
flying, visited, holiday, Hawaii
Winning Award won, award, support, awards, win-
ning, honor, scholarship, prize
Election/
Promotion/
Nomination
president, elected, run, nominated,
named, promotion, cel, selected,
business, vote
Publishing book, sold, writing, finished, read,
copy, review, release, books, cover
Contract signed, contract, deal, agreements,
agreed, produce, dollar, meeting
song/ video/ al-
bum release
video, song, album, check, show,
see, making, radio, love
Acting play, role, acting, drama, played,
series, movie, actor, theater
Death dies, passed, cancer, family, hospi-
tal, dad, grandma, mom, grandpa
Give Birth baby, born, boy, pregnant, girl, lbs,
name, son, world, daughter, birth
Illness ill, hospital, feeling, sick, cold, flu,
getting, fever, doctors, cough
Surgery surgery, got, test, emergency, blood,
tumor, stomachs, hospital, pain,
brain
Sports win, game, team, season, fans,
played, winning, football, luck
Running run, race, finished, race, marathon,
ran, miles, running, finish, goal
New Car car, buy, bought, cars, get, drive,
pick, seat, color, dollar, meet
Lost Weight weight, lost, week, pounds, loss,
weeks, gym, exercise, running
Birthday birthday, come, celebrate, party,
friends, dinner, tonight, friend
Lawsuit sue, sued, file, lawsuit, lawyer, dol-
lars, illegal, court, jury.
Table 3: Example event types with top words dis-
covered by our model.
2001
event examples and trained a 43-class maximum
entropy classifier based on the following features:
? Word: The sequence of words in the tweet.
? NER: Named entity Tag.
? Dictionary: Word matching a dictionaries of
the top 40 words for each life event category
(automatically inferred by the topic model).
The feature value is the term?s probability
generated by correspondent event.
? Window: If a dictionary term exists, left and
right context words within a window of 3
words and their part-of-speech tags.
Name entity tag is assigned from Ritter et al?s
Twitter NER system (Ritter et al., 2011). Part-of-
Speech tags are assigned based on Twitter POS
package (Owoputi et al., 2013) developed by
CMU ARK Lab. Dictionary and Window are
constructed based on the topic-term distribution
obtained from the previous section.
The average precision and recall are shown in
Table 4. And as we can observe, the dictionary
(with probability) contributes a lot to the perfor-
mance and by taking into account a more compre-
hensive set of information around the key word,
classifier on All feature setting generate signifi-
cantly better performance, with 0.382 prevision
and 0.48 recall, which is acceptable considering
(1) This is is a 43-way classification with much
more negative data than positive (2) Some types of
events are very close to each other (e.g., Leaving
and Vocation). Note that recall is valued more than
precision here as false-positive examples will be
further screened in self-reported information iden-
tification process in the following section.
Feature Setting Precision Recall
Word+NER 0.204 0.326
Word+NER+Dictionary 0.362 0.433
All 0.382 0.487
Table 4: Average Performance of Multi-Class
Classifier on Different Feature Settings. Negative
examples (non important event type) are not con-
sidered.
5 Self-Reported Information
Identification
Although a message might refer to a topic cor-
responding to a life event such as marriage, the
event still might be one in which the speaker is
not directly involved. In this section we describe
the self reported event identification portion of our
pipeline, which takes output from Section 4 and
further identifies whether each tweet refers to an
event directly involving the user who publishes it.
Direct labeling of randomly sampled Twitter
messages is infeasible for the following reasons:
(1) Class imbalance: self-reported events are rela-
tively rare in randomly sampled Twitter messages.
(2) A large proportion of self-reported information
refers to mundane, everyday topics (e.g., ?I just
finished dinner!?). Fortunately, many of the tweets
retrieved from Section 3 consist of self-reported
information and describe major life events. The
candidates for annotation are therefore largely nar-
rowed down.
We manually annotated 800 positive examples
of self-reported events distributed across the event
categories identified in Section 3. We ensured
good coverage by first randomly sampling 10 ex-
amples from each category, the remainder were
sampled from the class distribution in the data.
Negative examples of self-reported information
consisted of a combination of examples from the
original dataset
8
and randomly sampled messages
gathered by searching for the top terms in each of
the pre-identified topics using the Twitter Search
interface
9
. Due to great varieties of negative sce-
narios, the negative dataset constitutes about 2500
tweets.
5.1 Features
Identifying self-reported tweet requires sophisti-
cated feature engineering. Let u denote the term
within the tweet that gets the highest possibility
generated by the correspondent topic. We experi-
mented with combinations of the following types
of features (results are presented in Table ??):
? Bigram: Bigrams within each tweet (punctu-
ation included).
? Window: A window of k ? {0, 1, 2} words
adjacent to u and their part-of-speech tags.
? Tense: A binary feature indicating past tense
identified in by the presence of past tense
verb (VBD).
? Factuality: Factuality denotes whether one
expression is presented as corresponding to
real situations in the world (Saur?? and Puste-
jovsky, 2007). We use Stanford PragBank
10
,
8
Most tweets in the bootstrapping output are positive.
9
The majority of results returned by Twitter Search are
negative examples.
10
http://compprag.christopherpotts.net/
factbank.html
2002
an extension of FactBank (Saur?? and Puste-
jovsky, 2009) which contains a list of modal
words such as ?might?, ?will?, ?want to?
etc
11
.
? I: Whether the subject of the tweet is first per-
son singular.
? Dependency: If the subject is first person
singular and the u is a verb, the dependency
path between the subject and u (or non-
dependency).
Tweet dependency paths were obtained from
(Kong et al., 2014). As the tweet parser we use
only supports one-to-one dependency path iden-
tification but no dependency properties, Depen-
dency is a binary feature. The subject of each
tweet is determined by the dependency link to the
root of the tweet from the parser.
Among the features we explore, Word encodes
the general information within the tweet. Win-
dow addresses the information around topic key
word. The rest of the features specifically address
each of the negative situations described in Chal-
lenge 2, Section 1: Tense captures past event de-
scription, Factuality filters out wishes or imagi-
nation, I and Dependency correspond to whether
the described event involves the speaker. We built
a linear SVM classifier using SVM
light
package
(Joachims, 1999).
5.2 Evaluation
Feature Setting Acc Pre Rec
Bigram+Window 0.76 0.47 0.44
Bigram+Window
+Tense+Factuality
0.77 0.47 0.46
all 0.82 0.51 0.48
Table 5: Performance for self-report information
identification regarding different feature settings.
We report performance on the task of identi-
fying self-reported information in this subsection.
We employ 5-fold cross validation and report Ac-
curacy (Accu), Prevision (Prec) and Recall (Rec)
regarding different feature settings. The Tense,
Factuality, I and Dependency features positively
contribute to performance respectively and the
best performance is obtained when all types of fea-
tures are included.
11
Due to the colloquial property of tweets, we also intro-
duced terms such as ?gonna?, ?wanna?, ?bona?.
precision recall F1
0.82 0.86 0.84
Table 7: Performance for identifying properties.
6 Event Property Extraction
Thus far we have described how to automatically
identify tweets referring to major life events. In
addition, it is desirable to extract important prop-
erties of the event, for example the name of the
university the speaker was admitted to (See Figure
1). In this section we take a supervised approach to
event property extraction, based on manually an-
notated data for a handfull of the major life event
categories automatically identified by our system.
While this approach is unlikely to scale to the di-
versity of important personal events Twitter users
are discussing, our experiments demonstrate that
event property extraction is indeed feasible.
We cast the problem of event property extrac-
tion as a sequence labeling task, using Conditional
Random Fields (Lafferty et al., 2001) for learning
and inference. To make best use of the labeled
data, we trained a unified CRF model for closely
related event categories which often share proper-
ties; the full list is presented in Table 6 and we
labeled 300 tweets in total. Features we used in-
clude:
? word token, capitalization, POS
? left and right context words within a window
of 3 and the correspondent part-of-speech
tags
? word shape, NER
? a gazetteer of universities and employers bor-
rowed from NELL
12
.
We use 5-fold cross-validation and report results
in Table 7.
7 End-to-End Experiment
The evaluation for each part of our system has
been demonstrated in the corresponding section.
We now present a real-world evaluation: to what
degree can our trained system automatically iden-
tify life events in real world.
7.1 Dataset
We constructed a gold-standard life event dataset
using annotators from Amazon?s Mechanical Turk
(Snow et al., 2008) using 2 approaches:
12
http://rtw.ml.cmu.edu/rtw/kbbrowser/
2003
Life Event Property
(a) Acceptance, Graduation Name of University/College
(b) Wedding, Engagement, Falling love Name of Spouse/ partner/ bf/ gf
(c) Getting a job, interview, internship Name of Enterprise
(d) Moving to New Places, Trip, Vocation, Leaving Place, Origin, Destination
(e) Winning Award Name of Award, Prize
Table 6: Labeling Event Property.
? Ask Twitter users to label their own tweets
(Participants include friends, colleagues of
the authors and Turkers from Amazon Me-
chanical Turk
13
).
? Ask Turkers to label other people?s tweets.
For option 1, we asked participants to directly la-
bel their own published tweets. For option 2, for
each tweet, we employed 2 Turkers. Due to the
ambiguity in defining life events, the value co-
hen?s kappa
14
as a measure of inter-rater agree-
ment is 0.54; this does not show significant inter-
annotator agreement. The authors examined dis-
agreements and also verified all positively labeled
tweets. The resulting dataset contains around 900
positive tweets and about 60,000 negative tweets.
To demonstrate the advantage of leveraging
large quantities of unlabeled data, the first base-
line we investigate is a Supervised model which is
trained on the manually annotated labeled dataset,
and evaluated using 5 fold cross validation. Our
Supervised baseline consists of a linear SVM
classifier using bag of words, NER and POS fea-
tures. We also tested a second baseline that
combines Supervised algorithm with an our self-
reported information classifier, denoted as Super-
vised+Self.
Results are reported in Table 8; as we can ob-
serve, the fully supervised approach is not suitable
for this task with only one digit F1 score. The
explanations are as follows: (1) the labeled data
can only cover a small proportion of life events
(2) supervised learning does not separate impor-
tant event categories and will therefore classify
any tweet with highly weighted features (e.g., the
mention of ?I? or ?marriage?) as positive. By us-
ing an additional self-reported information classi-
fier in Supervised+Self, we get a significant boost
in precision with a minor recall loss.
13
https://www.mturk.com/mturk/welcome
14
http://en.wikipedia.org/wiki/Cohen?s_
kappa
Approach Precision Recall
Our approach 0.62 0.48
Supervised 0.13 0.20
Supervised+Self 0.25 0.18
Table 8: Performance for different approaches for
identifying life events in real world.
Approach Precision Recall
Step 1 0.65 0.36
Step 2 0.64 0.43
Step 3 0.62 0.48
Table 9: Performance for different steps of boot-
strapping for identifying life events in real world.
Another interesting question is to what degree
the bootstrapping contributes to the final results.
We keep the self-reported information classifier
fixed (though it?s based the ultimate identified
data source), and train the personal event classifier
based on topic distributions identified from each
of the three steps of bootstrapping
15
. Precision
and recall at various stages of bootstrapping are
presented in Table 9. As bootstrapping continues,
the precision remains roughly constant, but recall
increases as more life events and CONGRATULA-
TIONS and CONDOLENCES are discovered.
8 Related Work
Our work is related to three lines of NLP re-
searches. (1) user-level information extraction on
social media (2) public event extraction on social
media. (3) Data harvesting in Information Extrac-
tion, each of which contains large amount of re-
lated work, to which we can not do fully justice.
User Information Extraction from Twitter
Some early approaches towards understanding
user level information on social media is focused
on user profile/attribute prediction (e.g.,(Ciot et
al., 2013)) user-specific content extraction (Diao
15
which are 24, 38, 42-class classifiers, where 24, 38, 42
denoted the number of topics discovered in each step of boot-
strapping (see Figure 5).
2004
et al., 2012; Diao and Jiang, 2013; Li et al., 2014)
or user personalization (Low et al., 2011) identifi-
cation.
The problem of user life event extraction was
first studied by Li and Cardie?s (2014). They at-
tempted to construct a chronological timeline for
Twitter users from their published tweets based on
two criterion: a personal event should be personal
and time-specific. Their system does not explic-
itly identify a global category of life events (and
tweets discussing correspondent event) but identi-
fies the topics/events that are personal and time-
specific to a given user using an unsupervised ap-
proach, which helps them avoids the nuisance of
explicit definition for life event characteristics and
acquisition of labeled data. However, their sys-
tem has the short-coming that each personal topic
needs to be adequately discussed by the user and
their followers in order to be detected
16
.
Public Event Extraction from Twitter Twitter
serves as a good source for event detection owing
to its real time nature and large number of users.
These approaches include identifying bursty pub-
lic topics (e.g.,(Diao et al., 2012)), topic evolution
(Becker et al., 2011) or disaster outbreak (Sakaki
et al., 2010; Li and Cardie, 2013) by spotting the
increase/decrease of word frequency. Some other
approaches are focused on generating a structured
representation of events (Ritter et al., 2012; Ben-
son et al., 2011).
Data Acquisition in Information Extraction
Our work is also related with semi-supervised data
harvesting approaches, the key idea of which is
that some patterns are learned based on seeds.
They are then used to find additional terms, which
are subsequently used as new seeds in the patterns
to search for additional new patterns (Kozareva
and Hovy, 2010b; Davidov et al., 2007; Riloff
et al., 1999; Igo and Riloff, 2009; Kozareva et
al., 2008). Also related approaches are distant or
weakly supervision (Mintz et al., 2009; Craven et
al., 1999; Hoffmann et al., 2011) that rely on avail-
able structured data sources as a weak source of
supervision for pattern extraction from related text
corpora.
16
The reason is that topic models use word frequency for
topic modeling.
9 Conclusion and Discussion
In this paper, we propose a pipelined system for
major life event extraction from Twitter. Experi-
mental results show that our model is able to ex-
tract a wide variety of major life events.
The key strategy adopted in this work is to ob-
tain a relatively clean training dataset from large
quantity of Twitter data by relying on minimum
efforts of human supervision, and sometimes is at
the sacrifice of recall. To achieve this goal, we rely
on a couple of restrictions and manual screenings,
such as relying on replies, LDA topic identifica-
tion and seed screening. Each part of system de-
pends on the early steps. For example, topic clus-
tering in Section 3 not only offers training data for
event identification in Section 4, but prepares the
training data for self-information identification in
Section 5. .
We acknowledge that our approach is not
perfect due to the following ways: (1) The system
is only capable of discovering a few categories
of life events with many others left unidentified.
(2) Each step of the system will induce errors and
negatively affected the following parts. (3) Some
parts of evaluations are not comprehensive due
to the lack of gold-standard data. (4) Among all
pipelines, event property identification in Section
6 still requires full supervision in CRF model,
making it hard to scale to every event type
17
.
How to address these aspects and generate a more
accurate, comprehensive and fine-grained life
event list for Twitter users constitute our further
work.
Acknowledgements
A special thanks is owned to Myle Ott for sug-
gestions on bootstrapping procedure in data har-
vesting. The authors want to thank Noah Smith,
Chris Dyer and Alok Kothari for useful com-
ments, discussions and suggestions regarding dif-
ferent steps of the system and evaluations. We
thank Lingpeng Kong and members of Noah?s
ARK group at CMU for providing the tweet de-
pendency parser. All data used in this work is ex-
tracted from CMU Twitter Warehouse maintained
by Brendan O?Connor, to whom we want to ex-
press our gratitude.
17
We view weakly supervised life event property extrac-
tion as an interesting direction for future work.
2005
References
Hila Becker, Mor Naaman, and Luis Gravano. 2011.
Beyond trending topics: Real-world event identifi-
cation on twitter. ICWSM, 11:438?441.
Cosmin Adrian Bejan, Matthew Titsworth, Andrew
Hickl, and Sanda M Harabagiu. 2009. Nonparamet-
ric bayesian models for unsupervised event corefer-
ence resolution. In NIPS, pages 73?81.
Edward Benson, Aria Haghighi, and Regina Barzilay.
2011. Event discovery in social media feeds. In
Proceedings of the 49th Annual Meeting of the Asso-
ciation for Computational Linguistics: Human Lan-
guage Technologies-Volume 1, pages 389?398. As-
sociation for Computational Linguistics.
David M Blei, Andrew Y Ng, and Michael I Jordan.
2003. Latent dirichlet allocation. the Journal of ma-
chine Learning research, 3:993?1022.
Morgane Ciot, Morgan Sonderegger, and Derek Ruths.
2013. Gender inference of twitter users in non-
english contexts. In Proceedings of the 2013 Con-
ference on Empirical Methods in Natural Language
Processing, Seattle, Wash, pages 18?21.
Mark Craven, Johan Kumlien, et al. 1999. Construct-
ing biological knowledge bases by extracting infor-
mation from text sources. In ISMB, volume 1999,
pages 77?86.
Dmitry Davidov, Ari Rappoport, and Moshe Koppel.
2007. Fully unsupervised discovery of concept-
specific relationships by web mining. In Annual
Meeting-Association For Computational Linguis-
tics, volume 45, page 232.
Qiming Diao and Jing Jiang. 2013. A unified model
for topics, events and users on twitter. In Proceed-
ings of the 2013 Conference on Empirical Methods
in Natural Language Processing, pages 1869?1879.
Qiming Diao, Jing Jiang, Feida Zhu, and Ee-Peng
Lim. 2012. Finding bursty topics from microblogs.
In Proceedings of the 50th Annual Meeting of the
Association for Computational Linguistics: Long
Papers-Volume 1, pages 536?544. Association for
Computational Linguistics.
Thomas L Griffiths and Mark Steyvers. 2004. Find-
ing scientific topics. Proceedings of the National
academy of Sciences of the United States of Amer-
ica, 101(Suppl 1):5228?5235.
Amit Gruber, Yair Weiss, and Michal Rosen-Zvi.
2007. Hidden topic markov models. In Inter-
national Conference on Artificial Intelligence and
Statistics, pages 163?170.
Raphael Hoffmann, Congle Zhang, Xiao Ling, Luke
Zettlemoyer, and Daniel S Weld. 2011. Knowledge-
based weak supervision for information extraction
of overlapping relations. In Proceedings of the 49th
Annual Meeting of the Association for Computa-
tional Linguistics: Human Language Technologies-
Volume 1, pages 541?550. Association for Compu-
tational Linguistics.
Sean P Igo and Ellen Riloff. 2009. Corpus-based se-
mantic lexicon induction with web-based corrobora-
tion. In Proceedings of the Workshop on Unsuper-
vised and Minimally Supervised Learning of Lexical
Semantics, pages 18?26. Association for Computa-
tional Linguistics.
Thorsten Joachims. 1999. Making large scale svm
learning practical.
Lingpeng Kong, Nathan Schneider, Swabha
Swayamdipta, Archna Bhatia, Chris Dyer, and
Noah Smith. 2014. A dependency parser for tweets.
In EMNLP.
Zornitsa Kozareva and Eduard Hovy. 2010a. Learn-
ing arguments and supertypes of semantic relations
using recursive patterns. In Proceedings of the
48th Annual Meeting of the Association for Compu-
tational Linguistics, pages 1482?1491. Association
for Computational Linguistics.
Zornitsa Kozareva and Eduard Hovy. 2010b. Not
all seeds are equal: Measuring the quality of text
mining seeds. In Human Language Technologies:
The 2010 Annual Conference of the North American
Chapter of the Association for Computational Lin-
guistics, pages 618?626. Association for Computa-
tional Linguistics.
Zornitsa Kozareva, Ellen Riloff, and Eduard H Hovy.
2008. Semantic class learning from the web with
hyponym pattern linkage graphs. In ACL, volume 8,
pages 1048?1056.
John Lafferty, Andrew McCallum, and Fernando CN
Pereira. 2001. Conditional random fields: Prob-
abilistic models for segmenting and labeling se-
quence data.
Jiwei Li and Claire Cardie. 2013. Early stage
influenza detection from twitter. arXiv preprint
arXiv:1309.7340.
Jiwei Li and Claire Cardie. 2014. Timeline generation:
Tracking individuals on twitter. WWW, 2014.
Jiwei Li, Alan Ritter, and Eduard Hovy. 2014.
Weakly supervised user profile extraction from twit-
ter. ACL.
Yucheng Low, Deepak Agarwal, and Alexander J
Smola. 2011. Multiple domain user personaliza-
tion. In Proceedings of the 17th ACM SIGKDD in-
ternational conference on Knowledge discovery and
data mining, pages 123?131. ACM.
David Mimno, Hanna M Wallach, Edmund Talley,
Miriam Leenders, and Andrew McCallum. 2011.
Optimizing semantic coherence in topic models. In
2006
Proceedings of the Conference on Empirical Meth-
ods in Natural Language Processing, pages 262?
272. Association for Computational Linguistics.
Mike Mintz, Steven Bills, Rion Snow, and Dan Ju-
rafsky. 2009. Distant supervision for relation ex-
traction without labeled data. In Proceedings of the
Joint Conference of the 47th Annual Meeting of the
ACL and the 4th International Joint Conference on
Natural Language Processing of the AFNLP: Vol-
ume 2-Volume 2, pages 1003?1011. Association for
Computational Linguistics.
David Newman, Jey Han Lau, Karl Grieser, and Tim-
othy Baldwin. 2010. Automatic evaluation of
topic coherence. In Human Language Technologies:
The 2010 Annual Conference of the North American
Chapter of the Association for Computational Lin-
guistics, pages 100?108. Association for Computa-
tional Linguistics.
Olutobi Owoputi, Brendan O?Connor, Chris Dyer,
Kevin Gimpel, Nathan Schneider, and Noah A
Smith. 2013. Improved part-of-speech tagging for
online conversational text with word clusters. In
Proceedings of NAACL-HLT, pages 380?390.
Ellen Riloff, Rosie Jones, et al. 1999. Learning dic-
tionaries for information extraction by multi-level
bootstrapping. In AAAI/IAAI, pages 474?479.
Alan Ritter, Colin Cherry, and Bill Dolan. 2010. Un-
supervised modeling of twitter conversations.
Alan Ritter, Sam Clark, Oren Etzioni, et al. 2011.
Named entity recognition in tweets: an experimental
study. In Proceedings of the Conference on Empiri-
cal Methods in Natural Language Processing, pages
1524?1534. Association for Computational Linguis-
tics.
Alan Ritter, Oren Etzioni, Sam Clark, et al. 2012.
Open domain event extraction from twitter. In Pro-
ceedings of the 18th ACM SIGKDD international
conference on Knowledge discovery and data min-
ing, pages 1104?1112. ACM.
Kirk Roberts and Sanda M Harabagiu. 2011. Unsuper-
vised learning of selectional restrictions and detec-
tion of argument coercions. In Proceedings of the
Conference on Empirical Methods in Natural Lan-
guage Processing, pages 980?990. Association for
Computational Linguistics.
Takeshi Sakaki, Makoto Okazaki, and Yutaka Matsuo.
2010. Earthquake shakes twitter users: real-time
event detection by social sensors. In Proceedings
of the 19th international conference on World wide
web, pages 851?860. ACM.
Roser Saur?? and James Pustejovsky. 2007. Deter-
mining modality and factuality for text entailment.
In Semantic Computing, 2007. ICSC 2007. Interna-
tional Conference on, pages 509?516. IEEE.
Roser Saur?? and James Pustejovsky. 2009. Factbank:
A corpus annotated with event factuality. Language
resources and evaluation, 43(3):227?268.
Rion Snow, Brendan O?Connor, Daniel Jurafsky, and
Andrew Y Ng. 2008. Cheap and fast?but is it
good?: evaluating non-expert annotations for natu-
ral language tasks. In Proceedings of the conference
on empirical methods in natural language process-
ing, pages 254?263. Association for Computational
Linguistics.
Limin Yao, David Mimno, and Andrew McCallum.
2009. Efficient methods for topic model inference
on streaming document collections.
Limin Yao, Aria Haghighi, Sebastian Riedel, and An-
drew McCallum. 2011. Structured relation discov-
ery using generative models. In Proceedings of the
Conference on Empirical Methods in Natural Lan-
guage Processing, pages 1456?1466. Association
for Computational Linguistics.
2007
Proceedings of NAACL-HLT 2013, pages 497?501,
Atlanta, Georgia, 9?14 June 2013. c?2013 Association for Computational Linguistics
Negative Deceptive Opinion Spam
Myle Ott Claire Cardie
Department of Computer Science
Cornell University
Ithaca, NY 14853
{myleott,cardie}@cs.cornell.edu
Jeffrey T. Hancock
Department of Communication
Cornell University
Ithaca, NY 14853
jeff.hancock@cornell.edu
Abstract
The rising influence of user-generated online
reviews (Cone, 2011) has led to growing in-
centive for businesses to solicit and manufac-
ture DECEPTIVE OPINION SPAM?fictitious
reviews that have been deliberately written to
sound authentic and deceive the reader. Re-
cently, Ott et al (2011) have introduced an
opinion spam dataset containing gold standard
deceptive positive hotel reviews. However, the
complementary problem of negative deceptive
opinion spam, intended to slander competitive
offerings, remains largely unstudied. Follow-
ing an approach similar to Ott et al (2011), in
this work we create and study the first dataset
of deceptive opinion spam with negative sen-
timent reviews. Based on this dataset, we find
that standard n-gram text categorization tech-
niques can detect negative deceptive opinion
spam with performance far surpassing that of
human judges. Finally, in conjunction with
the aforementioned positive review dataset,
we consider the possible interactions between
sentiment and deception, and present initial
results that encourage further exploration of
this relationship.
1 Introduction
Consumer?s purchase decisions are increasingly in-
fluenced by user-generated online reviews of prod-
ucts and services (Cone, 2011). Accordingly,
there is a growing incentive for businesses to so-
licit and manufacture DECEPTIVE OPINION SPAM?
fictitious reviews that have been deliberately writ-
ten to sound authentic and deceive the reader (Ott et
al., 2011). For example, Ott et al (2012) has esti-
mated that between 1% and 6% of positive hotel re-
views appear to be deceptive, suggesting that some
hotels may be posting fake positive reviews in order
to hype their own offerings.
In this work we distinguish between two kinds of
deceptive opinion spam, depending on the sentiment
expressed in the review. In particular, reviews in-
tended to promote or hype an offering, and which
therefore express a positive sentiment towards the
offering, are called positive deceptive opinion spam.
In contrast, reviews intended to disparage or slander
competitive offerings, and which therefore express a
negative sentiment towards the offering, are called
negative deceptive opinion spam. While previous
related work (Ott et al, 2011; Ott et al, 2012) has
explored characteristics of positive deceptive opin-
ion spam, the complementary problem of negative
deceptive opinion spam remains largely unstudied.
Following the framework of Ott et al (2011), we
use Amazon?s Mechanical Turk service to produce
the first publicly available1 dataset of negative de-
ceptive opinion spam, containing 400 gold standard
deceptive negative reviews of 20 popular Chicago
hotels. To validate the credibility of our decep-
tive reviews, we show that human deception detec-
tion performance on the negative reviews is low, in
agreement with decades of traditional deception de-
tection research (Bond and DePaulo, 2006). We then
show that standard n-gram text categorization tech-
niques can be used to detect negative deceptive opin-
ion spam with approximately 86% accuracy ? far
1Dataset available at: http://www.cs.cornell.
edu/?myleott/op_spam.
497
surpassing that of the human judges.
In conjunction with Ott et al (2011)?s positive de-
ceptive opinion spam dataset, we then explore the
interaction between sentiment and deception with
respect to three types of language features: (1)
changes in first-person singular use, often attributed
to psychological distancing (Newman et al, 2003),
(2) decreased spatial awareness and more narrative
form, consistent with theories of reality monitor-
ing (Johnson and Raye, 1981) and imaginative writ-
ing (Biber et al, 1999; Rayson et al, 2001), and (3)
increased negative emotion terms, often attributed to
leakage cues (Ekman and Friesen, 1969), but per-
haps better explained in our case as an exaggeration
of the underlying review sentiment.
2 Dataset
One of the biggest challenges facing studies of de-
ception is obtaining labeled data. Recently, Ott et
al. (2011) have proposed an approach for generat-
ing positive deceptive opinion spam using Amazon?s
popular Mechanical Turk crowdsourcing service. In
this section we discuss our efforts to extend Ott et
al. (2011)?s dataset to additionally include negative
deceptive opinion spam.
2.1 Deceptive Reviews from Mechanical Turk
Deceptive negative reviews are gathered from Me-
chanical Turk using the same procedure as Ott et
al. (2011). In particular, we create and divide 400
HITs evenly across the 20 most popular hotels in
Chicago, such that we obtain 20 reviews for each
hotel. We allow workers to complete only a single
HIT each, so that each review is written by a unique
worker.2 We further require workers to be located
in the United States and to have an average past ap-
proval rating of at least 90%. We allow a maximum
of 30 minutes to complete the HIT, and reward ac-
cepted submissions with one US dollar ($1).
Each HIT instructs a worker to imagine that they
work for the marketing department of a hotel, and
that their manager has asked them to write a fake
negative review of a competitor?s hotel to be posted
online. Accompanying each HIT is the name and
2While Mechanical Turk does not provide a convenient
mechanism for ensuring the uniqueness of workers, this con-
straint can be enforced with Javascript. The script is available
at: http://uniqueturker.myleott.com.
URL of the hotel for which the fake negative re-
view is to be written, and instructions that: (1) work-
ers should not complete more than one similar HIT,
(2) submissions must be of sufficient quality, i.e.,
written for the correct hotel, legible, reasonable in
length,3 and not plagiarized,4 and, (3) the HIT is for
academic research purposes.
Submissions are manually inspected to ensure
that they are written for the correct hotel and to
ensure that they convey a generally negative senti-
ment.5 The average accepted review length was 178
words, higher than for the positive reviews gathered
by Ott et al (2011), who report an average review
length of 116 words.
2.2 Truthful Reviews from the Web
Negative (1- or 2-star) truthful reviews are mined
from six popular online review communities: Expe-
dia, Hotels.com, Orbitz, Priceline, TripAdvisor, and
Yelp. While reviews mined from these communities
cannot be considered gold standard truthful, recent
work (Mayzlin et al, 2012; Ott et al, 2012) suggests
that deception rates among travel review portals is
reasonably small.
Following Ott et al (2011), we sample a subset
of the available truthful reviews so that we retain an
equal number of truthful and deceptive reviews (20
each) for each hotel. However, because the truthful
reviews are on average longer than our deceptive re-
views, we sample the truthful reviews according to
a log-normal distribution fit to the lengths of our de-
ceptive reviews, similarly to Ott et al (2011).6
3 Deception Detection Performance
In this section we report the deception detection per-
formance of three human judges (Section 3.1) and
supervised n-gram Support Vector Machine (SVM)
classifiers (Section 3.2).
3We define ?reasonable length? to be ? 150 characters.
4We use http://plagiarisma.net to determine
whether or not a review is plagiarized.
5We discarded and replaced approximately 2% of the sub-
missions, where it was clear that the worker had misread the
instructions and instead written a deceptive positive review.
6We use the R package GAMLSS (Rigby and Stasinopou-
los, 2005) to fit a log-normal distribution (left truncated at 150
characters) to the lengths of the deceptive reviews.
498
TRUTHFUL DECEPTIVE
Accuracy P R F P R F
HUMAN
JUDGE 1 65.0% 65.0 65.0 65.0 65.0 65.0 65.0
JUDGE 2 61.9% 63.0 57.5 60.1 60.9 66.3 63.5
JUDGE 3 57.5% 57.3 58.8 58.0 57.7 56.3 57.0
META
MAJORITY 69.4% 70.1 67.5 68.8 68.7 71.3 69.9
SKEPTIC 58.1% 78.3 22.5 35.0 54.7 93.8 69.1
Table 1: Deception detection performance, incl. (P)recision, (R)ecall, and (F)1-score, for three human judges and two
meta-judges on a set of 160 negative reviews. The largest value in each column is indicated with boldface.
3.1 Human Performance
Recent large-scale meta-analyses have shown hu-
man deception detection performance is low, with
accuracies often not much better than chance (Bond
and DePaulo, 2006). Indeed, Ott et al (2011) found
that two out of three human judges were unable to
perform statistically significantly better than chance
(at the p < 0.05 level) at detecting positive decep-
tive opinion spam. Nevertheless, it is important to
subject our reviews to human judgments to validate
their convincingness. In particular, if human detec-
tion performance is found to be very high, then it
would cast doubt on the usefulness of the Mechan-
ical Turk approach for soliciting gold standard de-
ceptive opinion spam.
Following Ott et al (2011), we asked three vol-
unteer undergraduate university students to read and
make assessments on a subset of the negative review
dataset described in Section 2. Specifically, we ran-
domized all 40 deceptive and truthful reviews from
each of four hotels (160 reviews total). We then
asked the volunteers to read each review and mark
whether they believed it to be truthful or deceptive.
Performance for the three human judges appears
in Table 1. We additionally show the deception de-
tection performance of two meta-judges that aggre-
gate the assessments of the individual human judges:
(1) the MAJORITY meta-judge predicts deceptive
when at least two out of three human judges predict
deceptive (and truthful otherwise), and (2) the SKEP-
TIC meta-judge predicts deceptive when at least one
out of three human judges predicts deceptive (and
truthful otherwise).
A two-tailed binomial test suggests that JUDGE 1
and JUDGE 2 both perform better than chance (p =
0.0002, 0.003, respectively), while JUDGE 3 fails to
reject the null hypothesis of performing at-chance
(p = 0.07). However, while the best human judge
is accurate 65% of the time, inter-annotator agree-
ment computed using Fleiss? kappa is only slight
at 0.07 (Landis and Koch, 1977). Furthermore,
based on Cohen?s kappa, the highest pairwise inter-
annotator agreement is only 0.26, between JUDGE
1 and JUDGE 2. These low agreements suggest
that while the judges may perform statistically better
than chance, they are identifying different reviews
as deceptive, i.e., few reviews are consistently iden-
tified as deceptive.
3.2 Automated Classifier Performance
Standard n-gram?based text categorization tech-
niques have been shown to be effective at detect-
ing deception in text (Jindal and Liu, 2008; Mihal-
cea and Strapparava, 2009; Ott et al, 2011; Feng et
al., 2012). Following Ott et al (2011), we evaluate
the performance of linear Support Vector Machine
(SVM) classifiers trained with unigram and bigram
term-frequency features on our novel negative de-
ceptive opinion spam dataset. We employ the same
5-fold stratified cross-validation (CV) procedure as
Ott et al (2011), whereby for each cross-validation
iteration we train our model on all reviews for 16
hotels, and test our model on all reviews for the re-
maining 4 hotels. The SVM cost parameter, C, is
tuned by nested cross-validation on the training data.
Results appear in Table 2. Each row lists the sen-
timent of the train and test reviews, where ?Cross
Val.? corresponds to the cross-validation procedure
described above, and ?Held Out? corresponds to
classifiers trained on reviews of one sentiment and
tested on the other. The results suggest that n-gram?
based SVM classifiers can detect negative decep-
tive opinion spam in a balanced dataset with perfor-
mance far surpassing that of untrained human judges
(see Section 3.1). Furthermore, our results show that
499
TRUTHFUL DECEPTIVE
Train Sentiment Test Sentiment Accuracy P R F P R F
POSITIVE POSITIVE (800 reviews, Cross Val.) 89.3% 89.6 88.8 89.2 88.9 89.8 89.3
(800 reviews) NEGATIVE (800 reviews, Held Out) 75.1% 69.0 91.3 78.6 87.1 59.0 70.3
NEGATIVE POSITIVE (800 reviews, Held Out) 81.4% 76.3 91.0 83.0 88.9 71.8 79.4
(800 reviews) NEGATIVE (800 reviews, Cross Val.) 86.0% 86.4 85.5 85.9 85.6 86.5 86.1
COMBINED POSITIVE (800 reviews, Cross Val.) 88.4% 87.7 89.3 88.5 89.1 87.5 88.3
(1600 reviews) NEGATIVE (800 reviews, Cross Val.) 86.0% 85.3 87.0 86.1 86.7 85.0 85.9
Table 2: Automated classifier performance for different train and test sets, incl. (P)recision, (R)ecall and (F)1-score.
classifiers trained and tested on reviews of differ-
ent sentiments perform worse, despite having more
training data,7 than classifiers trained and tested on
reviews of the same sentiment. This suggests that
cues to deception differ depending on the sentiment
of the text (see Section 4).
Interestingly, we find that training on the com-
bined sentiment dataset results in performance that
is comparable to that of the ?same sentiment? classi-
fiers (88.4% vs. 89.3% accuracy for positive reviews
and 86.0% vs. 86.0% accuracy for negative reviews).
This is explainable in part by the increased training
set size (1,280 vs. 640 reviews per 4 training folds).
4 Interaction of Sentiment and Deception
An important question is how language features op-
erate in our fake negative reviews compared with the
fake positive reviews of Ott et al (2011). For exam-
ple, fake positive reviews included less spatial lan-
guage (e.g., floor, small, location, etc.) because in-
dividuals who had not actually experienced the ho-
tel simply had less spatial detail available for their
review (Johnson and Raye, 1981). This was also the
case for our negative reviews, with less spatial lan-
guage observed for fake negative reviews relative to
truthful. Likewise, our fake negative reviews had
more verbs relative to nouns than truthful, suggest-
ing a more narrative style that is indicative of imag-
inative writing (Biber et al, 1999; Rayson et al,
2001), a pattern also observed by Ott et al (2011).
There were, however, several important differ-
ences in the deceptive language of fake negative rel-
ative to fake positive reviews. First, as might be
expected, negative emotion terms were more fre-
7?Cross Val.? classifiers are effectively trained on 80% of
the data and tested on the remaining 20%, whereas ?Held Out?
classifiers are trained and tested on 100% of each data.
quent, according to LIWC (Pennebaker et al, 2007),
in our fake negative reviews than in the fake posi-
tive reviews. But, importantly, the fake negative re-
viewers over-produced negative emotion terms (e.g.,
terrible, disappointed) relative to the truthful re-
views in the same way that fake positive reviewers
over-produced positive emotion terms (e.g., elegant,
luxurious). Combined, these data suggest that the
more frequent negative emotion terms in the present
dataset are not the result of ?leakage cues? that re-
veal the emotional distress of lying (Ekman and
Friesen, 1969). Instead, the differences suggest that
fake hotel reviewers exaggerate the sentiment they
are trying to convey relative to similarly-valenced
truthful reviews.
Second, the effect of deception on the pattern of
pronoun frequency was not the same across posi-
tive and negative reviews. In particular, while first
person singular pronouns were produced more fre-
quently in fake reviews than truthful, consistent with
the case for positive reviews, the increase was di-
minished in the negative reviews examined here. In
the positive reviews reported by Ott et al (2011),
the rate of first person singular in fake reviews
(M=4.36%, SD=2.96%) was twice the rate observed
in truthful reviews (M=2.18%, SD=2.04%). In con-
trast, the rate of first person singular in the deceptive
negative reviews (M=4.47%, SD=2.83%) was only
57% greater than for truthful reviews (M=2.85%,
SD=2.23%). These results suggest that the empha-
sis on the self, perhaps as a strategy of convinc-
ing the reader that the author had actually been to
the hotel, is not as evident in the fake negative re-
views, perhaps because the negative tone of the re-
views caused the reviewers to psychologically dis-
tance themselves from their negative statements, a
phenomenon observed in several other deception
studies, e.g., Hancock et al (2008).
500
5 Conclusion
We have created the first publicly-available corpus
of gold standard negative deceptive opinion spam,
containing 400 reviews of 20 Chicago hotels, which
we have used to compare the deception detection ca-
pabilities of untrained human judges and standard
n-gram?based Support Vector Machine classifiers.
Our results demonstrate that while human deception
detection performance is greater for negative rather
than positive deceptive opinion spam, the best detec-
tion performance is still achieved through automated
classifiers, with approximately 86% accuracy.
We have additionally explored, albeit briefly, the
relationship between sentiment and deception by
utilizing Ott et al (2011)?s positive deceptive opin-
ion spam dataset in conjunction with our own. In
particular, we have identified several features of lan-
guage that seem to remain consistent across senti-
ment, such as decreased awareness of spatial details
and exaggerated language. We have also identified
other features that vary with the sentiment, such as
first person singular use, although further work is re-
quired to determine if these differences may be ex-
ploited to improve deception detection performance.
Indeed, future work may wish to jointly model sen-
timent and deception in order to better determine the
effect each has on language use.
Acknowledgments
This work was supported in part by NSF Grant BCS-
0904822, a DARPA Deft grant, the Jack Kent Cooke
Foundation, and by a gift from Google. We also thank
the three Cornell undergraduate volunteer judges, as well
as the NAACL reviewers for their insightful comments,
suggestions and advice on various aspects of this work.
References
D. Biber, S. Johansson, G. Leech, S. Conrad, E. Finegan,
and R. Quirk. 1999. Longman grammar of spoken and
written English, volume 2. MIT Press.
C.F. Bond and B.M. DePaulo. 2006. Accuracy of de-
ception judgments. Personality and Social Psychology
Review, 10(3):214.
Cone. 2011. 2011 Online Influence Trend Tracker. On-
line: http://www.coneinc.com/negative-
reviews-online-reverse-purchase-
decisions, August.
P. Ekman and W.V. Friesen. 1969. Nonverbal leakage
and clues to deception. Psychiatry, 32(1):88.
Song Feng, Ritwik Banerjee, and Yejin Choi. 2012. Syn-
tactic stylometry for deception detection. In Proceed-
ings of the 50th Annual Meeting of the Association
for Computational Linguistics: Short Papers-Volume
2, pages 171?175. Association for Computational Lin-
guistics.
J.T. Hancock, L.E. Curry, S. Goorha, and M. Woodworth.
2008. On lying and being lied to: A linguistic anal-
ysis of deception in computer-mediated communica-
tion. Discourse Processes, 45(1):1?23.
N. Jindal and B. Liu. 2008. Opinion spam and analysis.
In Proceedings of the international conference on Web
search and web data mining, pages 219?230. ACM.
M.K. Johnson and C.L. Raye. 1981. Reality monitoring.
Psychological Review, 88(1):67?85.
J.R. Landis and G.G. Koch. 1977. The measurement of
observer agreement for categorical data. Biometrics,
33(1):159.
Dina Mayzlin, Yaniv Dover, and Judith A Chevalier.
2012. Promotional reviews: An empirical investiga-
tion of online review manipulation. Technical report,
National Bureau of Economic Research.
R. Mihalcea and C. Strapparava. 2009. The lie detector:
Explorations in the automatic recognition of deceptive
language. In Proceedings of the ACL-IJCNLP 2009
Conference Short Papers, pages 309?312. Association
for Computational Linguistics.
M.L. Newman, J.W. Pennebaker, D.S. Berry, and J.M.
Richards. 2003. Lying words: Predicting deception
from linguistic styles. Personality and Social Psychol-
ogy Bulletin, 29(5):665.
M. Ott, Y. Choi, C. Cardie, and J.T. Hancock. 2011.
Finding deceptive opinion spam by any stretch of the
imagination. In Proceedings of the 49th Annual Meet-
ing of the Association for Computational Linguistics:
Human Language Technologies-Volume 1, pages 309?
319. Association for Computational Linguistics.
Myle Ott, Claire Cardie, and Jeff Hancock. 2012. Es-
timating the prevalence of deception in online review
communities. In Proceedings of the 21st international
conference on World Wide Web, pages 201?210. ACM.
J.W. Pennebaker, C.K. Chung, M. Ireland, A. Gonzales,
and R.J. Booth. 2007. The development and psycho-
metric properties of LIWC2007. Austin, TX: LIWC
(www.liwc.net).
P. Rayson, A. Wilson, and G. Leech. 2001. Grammatical
word class variation within the British National Cor-
pus sampler. Language and Computers, 36(1):295?
306.
R. A. Rigby and D. M. Stasinopoulos. 2005. Generalized
additive models for location, scale and shape,(with dis-
cussion). Applied Statistics, 54:507?554.
501
Proceedings of the ACL 2010 Conference Short Papers, pages 156?161,
Uppsala, Sweden, 11-16 July 2010. c?2010 Association for Computational Linguistics
Coreference Resolution with Reconcile
Veselin Stoyanov
Center for Language
and Speech Processing
Johns Hopkins Univ.
Baltimore, MD
ves@cs.jhu.edu
Claire Cardie
Department of
Computer Science
Cornell University
Ithaca, NY
cardie@cs.cornell.edu
Nathan Gilbert
Ellen Riloff
School of Computing
University of Utah
Salt Lake City, UT
ngilbert@cs.utah.edu
riloff@cs.utah.edu
David Buttler
David Hysom
Lawrence Livermore
National Laboratory
Livermore, CA
buttler1@llnl.gov
hysom1@llnl.gov
Abstract
Despite the existence of several noun phrase coref-
erence resolution data sets as well as several for-
mal evaluations on the task, it remains frustratingly
difficult to compare results across different corefer-
ence resolution systems. This is due to the high cost
of implementing a complete end-to-end coreference
resolution system, which often forces researchers
to substitute available gold-standard information in
lieu of implementing a module that would compute
that information. Unfortunately, this leads to incon-
sistent and often unrealistic evaluation scenarios.
With the aim to facilitate consistent and realis-
tic experimental evaluations in coreference resolu-
tion, we present Reconcile, an infrastructure for the
development of learning-based noun phrase (NP)
coreference resolution systems. Reconcile is de-
signed to facilitate the rapid creation of corefer-
ence resolution systems, easy implementation of
new feature sets and approaches to coreference res-
olution, and empirical evaluation of coreference re-
solvers across a variety of benchmark data sets and
standard scoring metrics. We describe Reconcile
and present experimental results showing that Rec-
oncile can be used to create a coreference resolver
that achieves performance comparable to state-of-
the-art systems on six benchmark data sets.
1 Introduction
Noun phrase coreference resolution (or simply
coreference resolution) is the problem of identi-
fying all noun phrases (NPs) that refer to the same
entity in a text. The problem of coreference res-
olution is fundamental in the field of natural lan-
guage processing (NLP) because of its usefulness
for other NLP tasks, as well as the theoretical in-
terest in understanding the computational mech-
anisms involved in government, binding and lin-
guistic reference.
Several formal evaluations have been conducted
for the coreference resolution task (e.g., MUC-6
(1995), ACE NIST (2004)), and the data sets cre-
ated for these evaluations have become standard
benchmarks in the field (e.g., MUC and ACE data
sets). However, it is still frustratingly difficult to
compare results across different coreference res-
olution systems. Reported coreference resolu-
tion scores vary wildly across data sets, evaluation
metrics, and system configurations.
We believe that one root cause of these dispar-
ities is the high cost of implementing an end-to-
end coreference resolution system. Coreference
resolution is a complex problem, and successful
systems must tackle a variety of non-trivial sub-
problems that are central to the coreference task ?
e.g., mention/markable detection, anaphor identi-
fication ? and that require substantial implemen-
tation efforts. As a result, many researchers ex-
ploit gold-standard annotations, when available, as
a substitute for component technologies to solve
these subproblems. For example, many published
research results use gold standard annotations to
identify NPs (substituting for mention/markable
detection), to distinguish anaphoric NPs from non-
anaphoric NPs (substituting for anaphoricity de-
termination), to identify named entities (substitut-
ing for named entity recognition), and to identify
the semantic types of NPs (substituting for seman-
tic class identification). Unfortunately, the use of
gold standard annotations for key/critical compo-
nent technologies leads to an unrealistic evalua-
tion setting, and makes it impossible to directly
compare results against coreference resolvers that
solve all of these subproblems from scratch.
Comparison of coreference resolvers is further
hindered by the use of several competing (and
non-trivial) evaluation measures, and data sets that
have substantially different task definitions and
annotation formats. Additionally, coreference res-
olution is a pervasive problem in NLP and many
NLP applications could benefit from an effective
coreference resolver that can be easily configured
and customized.
To address these issues, we have created a plat-
form for coreference resolution, called Reconcile,
that can serve as a software infrastructure to sup-
port the creation of, experimentation with, and
evaluation of coreference resolvers. Reconcile
was designed with the following seven desiderata
in mind:
? implement the basic underlying software ar-
156
chitecture of contemporary state-of-the-art
learning-based coreference resolution sys-
tems;
? support experimentation on most of the stan-
dard coreference resolution data sets;
? implement most popular coreference resolu-
tion scoring metrics;
? exhibit state-of-the-art coreference resolution
performance (i.e., it can be configured to cre-
ate a resolver that achieves performance close
to the best reported results);
? can be easily extended with new methods and
features;
? is relatively fast and easy to configure and
run;
? has a set of pre-built resolvers that can be
used as black-box coreference resolution sys-
tems.
While several other coreference resolution sys-
tems are publicly available (e.g., Poesio and
Kabadjov (2004), Qiu et al (2004) and Versley et
al. (2008)), none meets all seven of these desider-
ata (see Related Work). Reconcile is a modular
software platform that abstracts the basic archi-
tecture of most contemporary supervised learning-
based coreference resolution systems (e.g., Soon
et al (2001), Ng and Cardie (2002), Bengtson and
Roth (2008)) and achieves performance compara-
ble to the state-of-the-art on several benchmark
data sets. Additionally, Reconcile can be eas-
ily reconfigured to use different algorithms, fea-
tures, preprocessing elements, evaluation settings
and metrics.
In the rest of this paper, we review related work
(Section 2), describe Reconcile?s organization and
components (Section 3) and show experimental re-
sults for Reconcile on six data sets and two evalu-
ation metrics (Section 4).
2 Related Work
Several coreference resolution systems are cur-
rently publicly available. JavaRap (Qiu et al,
2004) is an implementation of the Lappin and
Leass? (1994) Resolution of Anaphora Procedure
(RAP). JavaRap resolves only pronouns and, thus,
it is not directly comparable to Reconcile. GuiTaR
(Poesio and Kabadjov, 2004) and BART (Versley
et al, 2008) (which can be considered a succes-
sor of GuiTaR) are both modular systems that tar-
get the full coreference resolution task. As such,
both systems come close to meeting the majority
of the desiderata set forth in Section 1. BART,
in particular, can be considered an alternative to
Reconcile, although we believe that Reconcile?s
approach is more flexible than BART?s. In addi-
tion, the architecture and system components of
Reconcile (including a comprehensive set of fea-
tures that draw on the expertise of state-of-the-art
supervised learning approaches, such as Bengtson
and Roth (2008)) result in performance closer to
the state-of-the-art.
Coreference resolution has received much re-
search attention, resulting in an array of ap-
proaches, algorithms and features. Reconcile
is modeled after typical supervised learning ap-
proaches to coreference resolution (e.g. the archi-
tecture introduced by Soon et al (2001)) because
of the popularity and relatively good performance
of these systems.
However, there have been other approaches
to coreference resolution, including unsupervised
and semi-supervised approaches (e.g. Haghighi
and Klein (2007)), structured approaches (e.g.
McCallum and Wellner (2004) and Finley and
Joachims (2005)), competition approaches (e.g.
Yang et al (2003)) and a bell-tree search approach
(Luo et al (2004)). Most of these approaches rely
on some notion of pairwise feature-based similar-
ity and can be directly implemented in Reconcile.
3 System Description
Reconcile was designed to be a research testbed
capable of implementing most current approaches
to coreference resolution. Reconcile is written in
Java, to be portable across platforms, and was de-
signed to be easily reconfigurable with respect to
subcomponents, feature sets, parameter settings,
etc.
Reconcile?s architecture is illustrated in Figure
1. For simplicity, Figure 1 shows Reconcile?s op-
eration during the classification phase (i.e., assum-
ing that a trained classifier is present).
The basic architecture of the system includes
five major steps. Starting with a corpus of docu-
ments together with a manually annotated corefer-
ence resolution answer key1, Reconcile performs
1Only required during training.
157
Figure 1: The Reconcile classification architecture.
the following steps, in order:
1. Preprocessing. All documents are passed
through a series of (external) linguistic pro-
cessors such as tokenizers, part-of-speech
taggers, syntactic parsers, etc. These com-
ponents produce annotations of the text. Ta-
ble 1 lists the preprocessors currently inter-
faced in Reconcile. Note that Reconcile in-
cludes several in-house NP detectors, that
conform to the different data sets? defini-
tions of what constitutes a NP (e.g., MUC
vs. ACE). All of the extractors utilize a syn-
tactic parse of the text and the output of a
Named Entity (NE) extractor, but extract dif-
ferent constructs as specialized in the corre-
sponding definition. The NP extractors suc-
cessfully recognize about 95% of the NPs in
the MUC and ACE gold standards.
2. Feature generation. Using annotations pro-
duced during preprocessing, Reconcile pro-
duces feature vectors for pairs of NPs. For
example, a feature might denote whether the
two NPs agree in number, or whether they
have any words in common. Reconcile in-
cludes over 80 features, inspired by other suc-
cessful coreference resolution systems such
as Soon et al (2001) and Ng and Cardie
(2002).
3. Classification. Reconcile learns a classifier
that operates on feature vectors representing
Task Systems
Sentence UIUC (CC Group, 2009)
splitter OpenNLP (Baldridge, J., 2005)
Tokenizer OpenNLP (Baldridge, J., 2005)
POS OpenNLP (Baldridge, J., 2005)
Tagger + the two parsers below
Parser Stanford (Klein and Manning, 2003)
Berkeley (Petrov and Klein, 2007)
Dep. parser Stanford (Klein and Manning, 2003)
NE OpenNLP (Baldridge, J., 2005)
Recognizer Stanford (Finkel et al, 2005)
NP Detector In-house
Table 1: Preprocessing components available in
Reconcile.
pairs of NPs and it is trained to assign a score
indicating the likelihood that the NPs in the
pair are coreferent.
4. Clustering. A clustering algorithm consoli-
dates the predictions output by the classifier
and forms the final set of coreference clusters
(chains).2
5. Scoring. Finally, during testing Reconcile
runs scoring algorithms that compare the
chains produced by the system to the gold-
standard chains in the answer key.
Each of the five steps above can invoke differ-
ent components. Reconcile?s modularity makes it
2Some structured coreference resolution algorithms (e.g.,
McCallum and Wellner (2004) and Finley and Joachims
(2005)) combine the classification and clustering steps above.
Reconcile can easily accommodate this modification.
158
Step Available modules
Classification various learners in the Weka toolkit
libSVM (Chang and Lin, 2001)
SVMlight (Joachims, 2002)
Clustering Single-link
Best-First
Most Recent First
Scoring MUC score (Vilain et al, 1995)
B3 score (Bagga and Baldwin, 1998)
CEAF score (Luo, 2005)
Table 2: Available implementations for different
modules available in Reconcile.
easy for new components to be implemented and
existing ones to be removed or replaced. Recon-
cile?s standard distribution comes with a compre-
hensive set of implemented components ? those
available for steps 2?5 are shown in Table 2. Rec-
oncile contains over 38,000 lines of original Java
code. Only about 15% of the code is concerned
with running existing components in the prepro-
cessing step, while the rest deals with NP extrac-
tion, implementations of features, clustering algo-
rithms and scorers. More details about Recon-
cile?s architecture and available components and
features can be found in Stoyanov et al (2010).
4 Evaluation
4.1 Data Sets
Reconcile incorporates the six most commonly
used coreference resolution data sets, two from the
MUC conferences (MUC-6, 1995; MUC-7, 1997)
and four from the ACE Program (NIST, 2004).
For ACE, we incorporate only the newswire por-
tion. When available, Reconcile employs the stan-
dard test/train split. Otherwise, we randomly split
the data into a training and test set following a
70/30 ratio. Performance is evaluated according
to the B3 and MUC scoring metrics.
4.2 The Reconcile2010 Configuration
Reconcile can be easily configured with differ-
ent algorithms for markable detection, anaphoric-
ity determination, feature extraction, etc., and run
against several scoring metrics. For the purpose of
this sample evaluation, we create only one partic-
ular instantiation of Reconcile, which we will call
Reconcile2010 to differentiate it from the general
platform. Reconcile2010 is configured using the
following components:
1. Preprocessing
(a) Sentence Splitter: OpenNLP
(b) Tokenizer: OpenNLP
(c) POS Tagger: OpenNLP
(d) Parser: Berkeley
(e) Named Entity Recognizer: Stanford
2. Feature Set - A hand-selected subset of 60 out of the
more than 80 features available. The features were se-
lected to include most of the features from Soon et al
Soon et al (2001), Ng and Cardie (2002) and Bengtson
and Roth (2008).
3. Classifier - Averaged Perceptron
4. Clustering - Single-link - Positive decision threshold
was tuned by cross validation of the training set.
4.3 Experimental Results
The first two rows of Table 3 show the perfor-
mance of Reconcile2010. For all data sets, B3
scores are higher than MUC scores. The MUC
score is highest for the MUC6 data set, while B3
scores are higher for the ACE data sets as com-
pared to the MUC data sets.
Due to the difficulties outlined in Section 1,
results for Reconcile presented here are directly
comparable only to a limited number of scores
reported in the literature. The bottom three
rows of Table 3 list these comparable scores,
which show that Reconcile2010 exhibits state-of-
the-art performance for supervised learning-based
coreference resolvers. A more detailed study of
Reconcile-based coreference resolution systems
in different evaluation scenarios can be found in
Stoyanov et al (2009).
5 Conclusions
Reconcile is a general architecture for coreference
resolution that can be used to easily create various
coreference resolvers. Reconcile provides broad
support for experimentation in coreference reso-
lution, including implementation of the basic ar-
chitecture of contemporary state-of-the-art coref-
erence systems and a variety of individual mod-
ules employed in these systems. Additionally,
Reconcile handles all of the formatting and scor-
ing peculiarities of the most widely used coref-
erence resolution data sets (those created as part
of the MUC and ACE conferences) and, thus,
allows for easy implementation and evaluation
across these data sets. We hope that Reconcile
will support experimental research in coreference
resolution and provide a state-of-the-art corefer-
ence resolver for both researchers and application
developers. We believe that in this way Recon-
cile will facilitate meaningful and consistent com-
parisons of coreference resolution systems. The
full Reconcile release is available for download at
http://www.cs.utah.edu/nlp/reconcile/.
159
System Score Data sets
MUC6 MUC7 ACE-2 ACE03 ACE04 ACE05
Reconcile2010
MUC 68.50 62.80 65.99 67.87 62.03 67.41
B3 70.88 65.86 78.29 79.39 76.50 73.71
Soon et al (2001) MUC 62.6 60.4 ? ? ? ?
Ng and Cardie (2002) MUC 70.4 63.4 ? ? ? ?
Yang et al (2003) MUC 71.3 60.2 ? ? ? ?
Table 3: Scores for Reconcile on six data sets and scores for comparable coreference systems.
Acknowledgments
This research was supported in part by the Na-
tional Science Foundation under Grant # 0937060
to the Computing Research Association for the
CIFellows Project, Lawrence Livermore National
Laboratory subcontract B573245, Department of
Homeland Security Grant N0014-07-1-0152, and
Air Force Contract FA8750-09-C-0172 under the
DARPA Machine Reading Program.
The authors would like to thank the anonymous
reviewers for their useful comments.
References
A. Bagga and B. Baldwin. 1998. Algorithms for scoring
coreference chains. In Linguistic Coreference Workshop
at the Language Resources and Evaluation Conference.
Baldridge, J. 2005. The OpenNLP project.
http://opennlp.sourceforge.net/.
E. Bengtson and D. Roth. 2008. Understanding the value of
features for coreference resolution. In Proceedings of the
2008 Conference on Empirical Methods in Natural Lan-
guage Processing (EMNLP).
CC Group. 2009. Sentence Segmentation Tool.
http://l2r.cs.uiuc.edu/ cogcomp/atool.php?tkey=SS.
C. Chang and C. Lin. 2001. LIBSVM: a Li-
brary for Support Vector Machines. Available at
http://www.csie.ntu.edu.tw/cjlin/libsvm.
J. Finkel, T. Grenager, and C. Manning. 2005. Incorporating
Non-local Information into Information Extraction Sys-
tems by Gibbs Sampling. In Proceedings of the 21st In-
ternational Conference on Computational Linguistics and
44th Annual Meeting of the ACL.
T. Finley and T. Joachims. 2005. Supervised clustering with
support vector machines. In Proceedings of the Twenty-
second International Conference on Machine Learning
(ICML 2005).
A. Haghighi and D. Klein. 2007. Unsupervised Coreference
Resolution in a Nonparametric Bayesian Model. In Pro-
ceedings of the 45th Annual Meeting of the ACL.
T. Joachims. 2002. SVMLight, http://svmlight.joachims.org.
D. Klein and C. Manning. 2003. Fast Exact Inference with
a Factored Model for Natural Language Parsing. In Ad-
vances in Neural Information Processing (NIPS 2003).
S. Lappin and H. Leass. 1994. An algorithm for pronom-
inal anaphora resolution. Computational Linguistics,
20(4):535?561.
X. Luo, A. Ittycheriah, H. Jing, N. Kambhatla, and
S. Roukos. 2004. A mention-synchronous coreference
resolution algorithm based on the bell tree. In Proceed-
ings of the 42nd Annual Meeting of the ACL.
X. Luo. 2005. On Coreference Resolution Performance
Metrics. In Proceedings of Human Language Technology
Conference and Conference on Empirical Methods in Nat-
ural Language Processing (HLT/EMNLP).
A. McCallum and B. Wellner. 2004. Conditional Models
of Identity Uncertainty with Application to Noun Coref-
erence. In Advances in Neural Information Processing
(NIPS 2004).
MUC-6. 1995. Coreference Task Definition. In Proceedings
of the Sixth Message Understanding Conference (MUC-
6).
MUC-7. 1997. Coreference Task Definition. In Proceed-
ings of the Seventh Message Understanding Conference
(MUC-7).
V. Ng and C. Cardie. 2002. Improving Machine Learning
Approaches to Coreference Resolution. In Proceedings of
the 40th Annual Meeting of the ACL.
NIST. 2004. The ACE Evaluation Plan. NIST.
S. Petrov and D. Klein. 2007. Improved Inference for Un-
lexicalized Parsing. In Proceedings of the Joint Meeting
of the Human Language Technology Conference and the
North American Chapter of the Association for Computa-
tional Linguistics (HLT-NAACL 2007).
M. Poesio and M. Kabadjov. 2004. A general-purpose,
off-the-shelf anaphora resolution module: implementation
and preliminary evaluation. In Proceedings of the Lan-
guage Resources and Evaluation Conference.
L. Qiu, M.-Y. Kan, and T.-S. Chua. 2004. A public reference
implementation of the rap anaphora resolution algorithm.
In Proceedings of the Language Resources and Evaluation
Conference.
W. Soon, H. Ng, and D. Lim. 2001. A Machine Learning Ap-
proach to Coreference of Noun Phrases. Computational
Linguistics, 27(4):521?541.
V. Stoyanov, N. Gilbert, C. Cardie, and E. Riloff. 2009. Co-
nundrums in noun phrase coreference resolution: Mak-
ing sense of the state-of-the-art. In Proceedings of
ACL/IJCNLP.
160
V. Stoyanov, C. Cardie, N. Gilbert, E. Riloff, D. Buttler, and
D. Hysom. 2010. Reconcile: A coreference resolution
research platform. Technical report, Cornell University.
Y. Versley, S. Ponzetto, M. Poesio, V. Eidelman, A. Jern,
J. Smith, X. Yang, and A. Moschitti. 2008. BART: A
modular toolkit for coreference resolution. In Proceed-
ings of the Language Resources and Evaluation Confer-
ence.
M. Vilain, J. Burger, J. Aberdeen, D. Connolly, and
L. Hirschman. 1995. A Model-Theoretic Coreference
Scoring Theme. In Proceedings of the Sixth Message Un-
derstanding Conference (MUC-6).
X. Yang, G. Zhou, J. Su, and C. Tan. 2003. Coreference
resolution using competition learning approach. In Pro-
ceedings of the 41st Annual Meeting of the ACL.
161
Proceedings of the ACL 2010 Conference Short Papers, pages 269?274,
Uppsala, Sweden, 11-16 July 2010. c?2010 Association for Computational Linguistics
Hierarchical Sequential Learning for Extracting Opinions and their
Attributes
Yejin Choi and Claire Cardie
Department of Computer Science
Cornell University
Ithaca, NY 14853
{ychoi,cardie}@cs.cornell.edu
Abstract
Automatic opinion recognition involves a
number of related tasks, such as identi-
fying the boundaries of opinion expres-
sion, determining their polarity, and de-
termining their intensity. Although much
progress has been made in this area, ex-
isting research typically treats each of the
above tasks in isolation. In this paper,
we apply a hierarchical parameter shar-
ing technique using Conditional Random
Fields for fine-grained opinion analysis,
jointly detecting the boundaries of opinion
expressions as well as determining two of
their key attributes ? polarity and inten-
sity. Our experimental results show that
our proposed approach improves the per-
formance over a baseline that does not
exploit hierarchical structure among the
classes. In addition, we find that the joint
approach outperforms a baseline that is
based on cascading two separate compo-
nents.
1 Introduction
Automatic opinion recognition involves a number
of related tasks, such as identifying expressions of
opinion (e.g. Kim and Hovy (2005), Popescu and
Etzioni (2005), Breck et al (2007)), determining
their polarity (e.g. Hu and Liu (2004), Kim and
Hovy (2004), Wilson et al (2005)), and determin-
ing their strength, or intensity (e.g. Popescu and
Etzioni (2005), Wilson et al (2006)). Most pre-
vious work treats each subtask in isolation: opin-
ion expression extraction (i.e. detecting the bound-
aries of opinion expressions) and opinion attribute
classification (e.g. determining values for polar-
ity and intensity) are tackled as separate steps in
opinion recognition systems. Unfortunately, er-
rors from individual components will propagate in
systems with cascaded component architectures,
causing performance degradation in the end-to-
end system (e.g. Finkel et al (2006)) ? in our
case, in the end-to-end opinion recognition sys-
tem.
In this paper, we apply a hierarchical param-
eter sharing technique (e.g., Cai and Hofmann
(2004), Zhao et al (2008)) using Conditional Ran-
dom Fields (CRFs) (Lafferty et al, 2001) to fine-
grained opinion analysis. In particular, we aim to
jointly identify the boundaries of opinion expres-
sions as well as to determine two of their key at-
tributes ? polarity and intensity.
Experimental results show that our proposed ap-
proach improves the performance over the base-
line that does not exploit the hierarchical structure
among the classes. In addition, we find that the
joint approach outperforms a baseline that is based
on cascading two separate systems.
2 Hierarchical Sequential Learning
We define the problem of joint extraction of opin-
ion expressions and their attributes as a sequence
tagging task as follows. Given a sequence of to-
kens, x = x1 ... xn, we predict a sequence of
labels, y = y1 ... yn, where yi ? {0, ..., 9} are
defined as conjunctive values of polarity labels
and intensity labels, as shown in Table 1. Then
the conditional probability p(y|x) for linear-chain
CRFs is given as (Lafferty et al, 2001)
P (y|x) = 1
Zx
exp
?
i
(
? f(yi, x, i)+?? f ?(yi?1, yi, x, i)
)
where Zx is the normalization factor.
In order to apply a hierarchical parameter shar-
ing technique (e.g., Cai and Hofmann (2004),
Zhao et al (2008)), we extend parameters as fol-
lows.
269
 
  

	 

  

  
 


	 




 
 

	 

 

 
       
	 

    
Figure 1: The hierarchical structure of classes for opinion expressions with polarity (positive, neutral,
negative) and intensity (high, medium, low)
LABEL 0 1 2 3 4 5 6 7 8 9
POLARITY none positive positive positive neutral neutral neutral negative negative negative
INTENSITY none high medium low high medium low high medium low
Table 1: Labels for Opinion Extraction with Polarity and Intensity
? f(yi, x, i) = ?? gO(?, x, i) (1)
+ ?? gP(?, x, i)
+ ?? gS(?, x, i)
?? f ?(yi?1, yi, x, i) = ???,?? g
?
O(?, ??, x, i)
+ ???,?? g
?
P(?, ??, x, i)
+ ???,?? g
?
S(?, ??, x, i)
where gO and g?O are feature vectors defined for
Opinion extraction, gP and g?P are feature vectors
defined for Polarity extraction, and gS and g?S are
feature vectors defined for Strength extraction, and
?, ?? ? {OPINION, NO-OPINION}
?, ?? ? {POSITIVE, NEGATIVE, NEUTRAL, NO-POLARITY}
?, ?? ? {HIGH, MEDIUM, LOW, NO-INTENSITY}
For instance, if yi = 1, then
? f(1, x, i) = ?OPINION gO(OPINION, x, i)
+ ?POSITIVE gP(POSITVE, x, i)
+ ?HIGH gS(HIGH, x, i)
If yi?1 = 0, yi = 4, then
?? f ?(0, 4, x, i)
= ??NO-OPINION,OPINION g
?
O(NO-OPINION, OPINION, x, i)
+ ??NO-POLARITY, NEUTRAL g
?
P(NO-POLARITY, NEUTRAL, x, i)
+ ??NO-INTENSITY, HIGH g
?
S(NO-INTENSITY, HIGH, x, i)
This hierarchical construction of feature and
weight vectors allows similar labels to share the
same subcomponents of feature and weight vec-
tors. For instance, all ? f(yi, x, i) such that
yi ? {1, 2, 3} will share the same compo-
nent ?POSITIVE gP(POSITVE, x, i). Note that there
can be other variations of hierarchical construc-
tion. For instance, one can add ?? gI(?, x, i)
and ??
?,??
g?I(?, ??, x, i) to Equation (1) for ? ?
{0, 1, ..., 9}, in order to allow more individualized
learning for each label.
Notice also that the number of sets of param-
eters constructed by Equation (1) is significantly
smaller than the number of sets of parameters that
are needed without the hierarchy. The former re-
quires (2+ 4+4)+ (2? 2+4? 4+ 4? 4) = 46
sets of parameters, but the latter requires (10) +
(10 ? 10) = 110 sets of parameters. Because a
combination of a polarity component and an in-
tensity component can distinguish each label, it is
not necessary to define a separate set of parameters
for each label.
3 Features
We first introduce definitions of key terms that will
be used to describe features.
? PRIOR-POLARITY & PRIOR-INTENSITY:
We obtain these prior-attributes from the polar-
ity lexicon populated by Wilson et al (2005).
? EXP-POLARITY, EXP-INTENSITY & EXP-SPAN:
Words in a given opinion expression often do
not share the same prior-attributes. Such dis-
continuous distribution of features can make
it harder to learn the desired opinion expres-
sion boundaries. Therefore, we try to obtain
expression-level attributes (EXP-POLARITY and
EXP-INTENSITY) using simple heuristics. In or-
der to derive EXP-POLARITY, we perform simple
270
voting. If there is a word with a negation effect,
such as ?never?, ?not?, ?hardly?, ?against?, then
we flip the polarity. For EXP-INTENSITY, we use
the highest PRIOR-INTENSITY in the span. The text
span with the same expression-level attributes
are referred to as EXP-SPAN.
3.1 Per-Token Features
Per-token features are defined in the form of
gO(?, x, i), gP(?, x, i) and gS(?, x, i). The do-
mains of ?, ?, ? are as given in Section 3.
Common Per-Token Features
Following features are common for all class labels.
The notation ? indicates conjunctive operation of
two values.
? PART-OF-SPEECH(xi):
based on GATE (Cunningham et al, 2002).
? WORD(xi), WORD(xi?1), WORD(xi+1)
? WORDNET-HYPERNYM(xi):
based on WordNet (Miller, 1995).
? OPINION-LEXICON(xi):
based on opinion lexicon (Wiebe et al, 2002).
? SHALLOW-PARSER(xi):
based on CASS partial parser (Abney, 1996).
? PRIOR-POLARITY(xi) ? PRIOR-INTENSITY(xi)
? EXP-POLARITY(xi) ? EXP-INTENSITY(xi)
? EXP-POLARITY(xi) ? EXP-INTENSITY(xi) ?
STEM(xi)
? EXP-SPAN(xi):
boolean to indicate whether xi is in an EXP-SPAN.
? DISTANCE-TO-EXP-SPAN(xi): 0, 1, 2, 3+.
? EXP-POLARITY(xi) ? EXP-INTENSITY(xi) ?
EXP-SPAN(xi)
Polarity Per-Token Features
These features are included only for gO(?, x, i)
and gP(?, x, i), which are the feature functions
corresponding to the polarity-based classes.
? PRIOR-POLARITY(xi), EXP-POLARITY((xi)
? STEM(xi) ? EXP-POLARITY(xi)
? COUNT-OF-Polarity:
where Polarity ? {positive, neutral, negative}.
This feature encodes the number of positive,
neutral, and negative EXP-POLARITY words re-
spectively, in the current sentence.
? STEM(xi) ? COUNT-OF-Polarity
? EXP-POLARITY(xi) ? COUNT-OF-Polarity
? EXP-SPAN(xi) and EXP-POLARITY(xi)
? DISTANCE-TO-EXP-SPAN(xi) ? EXP-POLARITY(xp)
Intensity Per-Token Features
These features are included only for gO(?, x, i)
and gS(?, x, i), which are the feature functions cor-
responding to the intensity-based classes.
? PRIOR-INTENSITY(xi), EXP-INTENSITY(xi)
? STEM(xi) ? EXP-INTENSITY(xi)
? COUNT-OF-STRONG, COUNT-OF-WEAK:
the number of strong and weak EXP-INTENSITY
words in the current sentence.
? INTENSIFIER(xi): whether xi is an intensifier,
such as ?extremely?, ?highly?, ?really?.
? STRONGMODAL(xi): whether xi is a strong modal
verb, such as ?must?, ?can?, ?will?.
? WEAKMODAL(xi): whether xi is a weak modal
verb, such as ?may?, ?could?, ?would?.
? DIMINISHER(xi): whether xi is a diminisher, such
as ?little?, ?somewhat?, ?less?.
? PRECEDED-BY-? (xi),
PRECEDED-BY-? (xi) ? EXP-INTENSITY(xi):
where ? ? { INTENSIFIER, STRONGMODAL, WEAK-
MODAL, DIMINISHER}
? ? (xi) ? EXP-INTENSITY(xi),
? (xi) ? EXP-INTENSITY(xi?1),
? (xi?1) ? EXP-INTENSITY(xi+1)
? EXP-SPAN(xi) ? EXP-INTENSITY(xi)
? DISTANCE-TO-EXP-SPAN(xi) ? EXP-INTENSITY(xp)
3.2 Transition Features
Transition features are employed to help with
boundary extraction as follows:
Polarity Transition Features
Polarity transition features are features that are
used only for g?O(?, ??, x, i) and g?P(?, ??, x, i).
? PART-OF-SPEECH(xi) ? PART-OF-SPEECH(xi+1) ?
EXP-POLARITY(xi)
? EXP-POLARITY(xi) ? EXP-POLARITY(xi+1)
Intensity Transition Features
Intensity transition features are features that are
used only for g?O(?, ??, x, i) and g?S(?, ??, x, i).
? PART-OF-SPEECH(xi) ? PART-OF-SPEECH(xi+1) ?
EXP-INTENSITY(xi)
? EXP-INTENSITY(xi) ? EXP-INTENSITY(xi+1)
4 Evaluation
We evaluate our system using the Multi-
Perspective Question Answering (MPQA) cor-
pus1. Our gold standard opinion expressions cor-
1The MPQA corpus can be obtained at
http://nrrc.mitre.org/NRRC/publications.htm.
271
Positive Neutral Negative
Method Description r(%) p(%) f(%) r(%) p(%) f(%) r(%) p(%) f(%)
Polarity-Only ? Intensity-Only (BASELINE1) 29.6 65.7 40.8 26.5 69.1 38.3 35.5 77.0 48.6
Joint without Hierarchy (BASELINE2) 30.7 65.7 41.9 29.9 66.5 41.2 37.3 77.1 50.3
Joint with Hierarchy 31.8 67.1 43.1 31.9 66.6 43.1 40.4 76.2 52.8
Table 2: Performance of Opinion Extraction with Correct Polarity Attribute
High Medium Low
Method Description r(%) p(%) f(%) r(%) p(%) f(%) r(%) p(%) f(%)
Polarity-Only ? Intensity-Only (BASELINE1) 26.4 58.3 36.3 29.7 59.0 39.6 15.4 60.3 24.5
Joint without Hierarchy (BASELINE2) 29.7 54.2 38.4 28.0 57.4 37.6 18.8 55.0 28.0
Joint with Hierarchy 27.1 55.2 36.3 32.0 56.5 40.9 21.1 56.3 30.7
Table 3: Performance of Opinion Extraction with Correct Intensity Attribute
Method Description r(%) p(%) f(%)
Polar-Only ? Intensity-Only 43.3 92.0 58.9
Joint without Hierarchy 46.0 88.4 60.5
Joint with Hierarchy 48.0 87.8 62.0
Table 4: Performance of Opinion Extraction
respond to direct subjective expression and expres-
sive subjective element (Wiebe et al, 2005).2
Our implementation of hierarchical sequential
learning is based on the Mallet (McCallum, 2002)
code for CRFs. In all experiments, we use a Gaus-
sian prior of 1.0 for regularization. We use 135
documents for development, and test on a dif-
ferent set of 400 documents using 10-fold cross-
validation. We investigate three options for jointly
extracting opinion expressions with their attributes
as follows:
[Baseline-1] Polarity-Only ? Intensity-Only:
For this baseline, we train two separate sequence
tagging CRFs: one that extracts opinion expres-
sions only with the polarity attribute (using com-
mon features and polarity extraction features in
Section 3), and another that extracts opinion ex-
pressions only with the intensity attribute (using
common features and intensity extraction features
in Section 3). We then combine the results from
two separate CRFs by collecting all opinion en-
tities extracted by both sequence taggers.3 This
2Only 1.5% of the polarity annotations correspond to
both; hence, we merge both into the neutral. Similarly, for
gold standard intensity, we merge extremely high into high.
3We collect all entities whose portions of text spans are
extracted by both models.
baseline effectively represents a cascaded compo-
nent approach.
[Baseline-2] Joint without Hierarchy: Here
we use simple linear-chain CRFs without exploit-
ing the class hierarchy for the opinion recognition
task. We use the tags shown in Table 1.
Joint with Hierarchy: Finally, we test the hi-
erarchical sequential learning approach elaborated
in Section 3.
4.1 Evaluation Results
We evaluate all experiments at the opinion entity
level, i.e. at the level of each opinion expression
rather than at the token level. We use three evalua-
tion metrics: recall, precision, and F-measure with
equally weighted recall and precision.
Table 4 shows the performance of opinion ex-
traction without matching any attribute. That is, an
extracted opinion entity is counted as correct if it
overlaps4 with a gold standard opinion expression,
without checking the correctness of its attributes.
Table 2 and 3 show the performance of opinion
extraction with the correct polarity and intensity
respectively.
From all of these evaluation criteria, JOINT WITH
4Overlap matching is a reasonable choice as the annotator
agreement study is also based on overlap matching (Wiebe
et al, 2005). One might wonder whether the overlap match-
ing scheme could allow a degenerative case where extracting
the entire test dataset as one giant opinion expression would
yield 100% recall and precision. Because each sentence cor-
responds to a different test instance in our model, and because
some sentences do not contain any opinion expression in the
dataset, such degenerative case is not possible in our experi-
ments.
272
HIERARCHY performs the best, and the least effec-
tive one is BASELINE-1, which cascades two sepa-
rately trained models. It is interesting that the sim-
ple sequential tagging approach even without ex-
ploiting the hierarchy (BASELINE-2) performs better
than the cascaded approach (BASELINE-1).
When evaluating with respect to the polarity at-
tribute, the performance of the negative class is
substantially higher than the that of other classes.
This is not surprising as there is approximately
twice as much data for the negative class. When
evaluating with respect to the intensity attribute,
the performance of the LOW class is substantially
lower than that of other classes. This result reflects
the fact that it is inherently harder to distinguish
an opinion expression with low intensity from no
opinion. In general, we observe that determining
correct intensity attributes is a much harder task
than determining correct polarity attributes.
In order to have a sense of upper bound, we
also report the individual performance of two sep-
arately trained models used for BASELINE-1: for the
Polarity-Only model that extracts opinion bound-
aries only with polarity attribute, the F-scores with
respect to the positive, neutral, negative classes are
46.7, 47.5, 57.0, respectively. For the Intensity-
Only model, the F-scores with respect to the high,
medium, low classes are 37.1, 40.8, 26.6, respec-
tively. Remind that neither of these models alone
fully solve the joint task of extracting boundaries
as well as determining two attributions simultane-
ously. As a result, when conjoining the results
from the two models (BASELINE-1), the final per-
formance drops substantially.
We conclude from our experiments that the sim-
ple joint sequential tagging approach even with-
out exploiting the hierarchy brings a better perfor-
mance than combining two separately developed
systems. In addition, our hierarchical joint se-
quential learning approach brings a further perfor-
mance gain over the simple joint sequential tag-
ging method.
5 Related Work
Although there have been much research for fine-
grained opinion analysis (e.g., Hu and Liu (2004),
Wilson et al (2005), Wilson et al (2006), Choi
and Claire (2008), Wilson et al (2009)),5 none is
5For instance, the results of Wilson et al (2005) is not
comparable even for our Polarity-Only model used inside
BASELINE-1, because Wilson et al (2005) does not operate
directly comparable to our results; much of previ-
ous work studies only a subset of what we tackle
in this paper. However, as shown in Section 4.1,
when we train the learning models only for a sub-
set of the tasks, we can achieve a better perfor-
mance instantly by making the problem simpler.
Our work differs from most of previous work in
that we investigate how solving multiple related
tasks affects performance on sub-tasks.
The hierarchical parameter sharing technique
used in this paper has been previously used by
Zhao et al (2008) for opinion analysis. However,
Zhao et al (2008) employs this technique only to
classify sentence-level attributes (polarity and in-
tensity), without involving a much harder task of
detecting boundaries of sub-sentential entities.
6 Conclusion
We applied a hierarchical parameter sharing tech-
nique using Conditional Random Fields for fine-
grained opinion analysis. Our proposed approach
jointly extract opinion expressions from unstruc-
tured text and determine their attributes ? polar-
ity and intensity. Empirical results indicate that
the simple joint sequential tagging approach even
without exploiting the hierarchy brings a better
performance than combining two separately de-
veloped systems. In addition, we found that the
hierarchical joint sequential learning approach im-
proves the performance over the simple joint se-
quential tagging method.
Acknowledgments
This work was supported in part by National
Science Foundation Grants BCS-0904822, BCS-
0624277, IIS-0535099 and by the Department of
Homeland Security under ONR Grant N0014-07-
1-0152. We thank the reviewers and Ainur Yesse-
nalina for many helpful comments.
References
S. Abney. 1996. Partial parsing via finite-state cas-
cades. In Journal of Natural Language Engineering,
2(4).
E. Breck, Y. Choi and C. Cardie. 2007. Identifying
Expressions of Opinion in Context. In IJCAI.
on the entire corpus as unstructured input. Instead, Wilson
et al (2005) evaluate only on known words that are in their
opinion lexicon. Furthermore, Wilson et al (2005) simplifies
the problem by combining neutral opinions and no opinions
into the same class, while our system distinguishes the two.
273
L. Cai and T. Hofmann. 2004. Hierarchical docu-
ment categorization with support vector machines.
In CIKM.
Y. Choi and C. Cardie. 2008. Learning with Composi-
tional Semantics as Structural Inference for Subsen-
tential Sentiment Analysis. In EMNLP.
H. Cunningham, D. Maynard, K. Bontcheva and V.
Tablan. 2002. GATE: A Framework and Graphical
Development Environment for Robust NLP Tools
and Applications. In ACL.
J. R. Finkel, C. D. Manning and A. Y. Ng. 2006.
Solving the Problem of Cascading Errors: Approx-
imate Bayesian Inference for Linguistic Annotation
Pipelines. In EMNLP.
M. Hu and B. Liu. 2004. Mining and Summarizing
Customer Reviews. In KDD.
S. Kim and E. Hovy. 2004. Determining the sentiment
of opinions. In COLING.
S. Kim and E. Hovy. 2005. Automatic Detection of
Opinion Bearing Words and Sentences. In Com-
panion Volume to the Proceedings of the Second In-
ternational Joint Conference on Natural Language
Processing (IJCNLP-05).
J. Lafferty, A. McCallum and F. Pereira. 2001. Condi-
tional Random Fields: Probabilistic Models for Seg-
menting and Labeling Sequence Data. In ICML.
A. McCallum. 2002. MALLET: A Machine Learning
for Language Toolkit. http://mallet.cs.umass.edu.
G. A. Miller. 1995. WordNet: a lexical database for
English. In Communications of the ACM, 38(11).
Ana-Maria Popescu and O. Etzioni. 2005. Extracting
Product Features and Opinions from Reviews. In
HLT-EMNLP.
J. Wiebe, E. Breck, C. Buckley, C. Cardie, P. Davis,
B. Fraser, D. Litman, D. Pierce, E. Riloff and T.
Wilson. 2002. Summer Workshop on Multiple-
Perspective Question Answering: Final Report. In
NRRC.
J. Wiebe and T. Wilson and C. Cardie 2005. Annotat-
ing Expressions of Opinions and Emotions in Lan-
guage. In Language Resources and Evaluation, vol-
ume 39, issue 2-3.
T. Wilson, J. Wiebe and P. Hoffmann. 2005. Recogniz-
ing Contextual Polarity in Phrase-Level Sentiment
Analysis. In HLT-EMNLP.
T. Wilson, J. Wiebe and R. Hwa. 2006. Recognizing
strong and weak opinion clauses. In Computational
Intelligence. 22 (2): 73-99.
T.Wilson, J. Wiebe and P. Hoffmann. 2009. Recogniz-
ing Contextual Polarity: an exploration of features
for phrase-level sentiment analysis. Computational
Linguistics 35(3).
J. Zhao, K. Liu and G. Wang. 2008. Adding Redun-
dant Features for CRFs-based Sentence Sentiment
Classification. In EMNLP.
274
Proceedings of the ACL 2010 Conference Short Papers, pages 336?341,
Uppsala, Sweden, 11-16 July 2010. c?2010 Association for Computational Linguistics
Automatically generating annotator rationales
to improve sentiment classification
Ainur Yessenalina Yejin Choi Claire Cardie
Department of Computer Science, Cornell University, Ithaca NY, 14853 USA
{ainur, ychoi, cardie}@cs.cornell.edu
Abstract
One of the central challenges in sentiment-
based text categorization is that not ev-
ery portion of a document is equally in-
formative for inferring the overall senti-
ment of the document. Previous research
has shown that enriching the sentiment la-
bels with human annotators? ?rationales?
can produce substantial improvements in
categorization performance (Zaidan et al,
2007). We explore methods to auto-
matically generate annotator rationales for
document-level sentiment classification.
Rather unexpectedly, we find the automat-
ically generated rationales just as helpful
as human rationales.
1 Introduction
One of the central challenges in sentiment-based
text categorization is that not every portion of
a given document is equally informative for in-
ferring its overall sentiment (e.g., Pang and Lee
(2004)). Zaidan et al (2007) address this prob-
lem by asking human annotators to mark (at least
some of) the relevant text spans that support each
document-level sentiment decision. The text spans
of these ?rationales? are then used to construct ad-
ditional training examples that can guide the learn-
ing algorithm toward better categorizationmodels.
But could we perhaps enjoy the performance
gains of rationale-enhanced learningmodels with-
out any additional human effort whatsoever (be-
yond the document-level sentiment label)? We hy-
pothesize that in the area of sentiment analysis,
where there has been a great deal of recent re-
search attentiongiven to various aspects of the task
(Pang and Lee, 2008), this might be possible: us-
ing existing resources for sentiment analysis, we
might be able to construct annotator rationales au-
tomatically.
In this paper, we explore a number of methods
to automatically generate rationales for document-
level sentiment classification. In particular, we in-
vestigate the use of off-the-shelf sentiment analy-
sis components and lexicons for this purpose. Our
approaches for generating annotator rationales can
be viewed as mostly unsupervised in that we do not
require manually annotated rationales for training.
Rather unexpectedly, our empirical results show
that automatically generated rationales (91.78%)
are just as good as human rationales (91.61%) for
document-level sentiment classification of movie
reviews. In addition, complementing the hu-
man annotator rationales with automatic rationales
boosts the performance even further for this do-
main, achieving 92.5% accuracy. We further eval-
uate our rationale-generation approaches on prod-
uct review data for which human rationales are not
available: here we find that even randomly gener-
ated rationales can improve the classification accu-
racy although rationales generated from sentiment
resources are not as effective as for movie reviews.
The rest of the paper is organized as follows.
We first briefly summarize the SVM-based learn-
ing approach of Zaidan et al (2007) that allows the
incorporation of rationales (Section 2). We next
introduce three methods for the automatic gener-
ation of rationales (Section 3). The experimental
results are presented in Section 4, followed by re-
lated work (Section 5) and conclusions (Section
6).
2 Contrastive Learning with SVMs
Zaidan et al (2007) first introduced the notion of
annotator rationales ? text spans highlighted by
human annotators as support or evidence for each
document-level sentiment decision. These ratio-
nales, of course, are only useful if the sentiment
categorization algorithm can be extended to ex-
ploit the rationales effectively. With this in mind,
Zaidan et al (2007) propose the following con-
336
trastive learning extension to the standard SVM
learning algorithm.
Let ~xi be movie review i, and let {~rij} be the
set of annotator rationales that support the posi-
tive or negative sentiment decision for ~xi. For each
such rationale~rij in the set, construct a contrastive
training example ~vij , by removing the text span
associated with the rationale ~rij from the original
review ~xi. Intuitively, the contrastive example ~vij
should not be as informative to the learning algo-
rithm as the original review ~xi, since one of the
supporting regions identified by the human anno-
tator has been deleted. That is, the correct learned
model should be less confident of its classifica-
tion of a contrastive example vs. the corresponding
original example, and the classification boundary
of the model should be modified accordingly.
Zaidan et al (2007) formulate exactly this intu-
ition as SVM constraints as follows:
(?i, j) : yi (~w~xi ? ~w~vij) ? ?(1 ? ?ij)
where yi ? {?1,+1} is the negative/positive sen-
timent label of document i, ~w is the weight vector,
? ? 0 controls the size of the margin between the
original examples and the contrastive examples,
and ?ij are the associated slack variables. After
some re-writing of the equations, the resulting ob-
jective function and constraints for the SVM are as
follows:
1
2 ||~w||
2 + C
?
i
?i + Ccontrast
?
ij
?ij (1)
subject to constraints:
(?i) : yi ~w ? ~xi ? 1 ? ?i, ?i ? 0
(?i, j) : yi ~w ? ~xij ? 1 ? ?ij ?ij ? 0
where ?i and ?ij are the slack variables for ~xi
(the original examples) and ~xij (~xij are named as
pseudo examples and defined as ~xij = ~xi?~vij? ), re-
spectively. Intuitively, the pseudo examples (~xij)
represent the difference between the original ex-
amples (~xi) and the contrastive examples (~vij),
weighted by a parameter ?. C and Ccontrast are
parameters to control the trade-offs between train-
ing errors and margins for the original examples ~xi
and pseudo examples ~xij respectively. As noted in
Zaidan et al (2007),Ccontrast values are generally
smaller than C for noisy rationales.
In the work described below, we similarly em-
ploy Zaidan et al?s (2007) contrastive learning
method to incorporate rationales for document-
level sentiment categorization.
3 Automatically Generating Rationales
Our goal in the current work, is to generate anno-
tator rationales automatically. For this, we rely on
the following two assumptions:
(1) Regions marked as annotator rationales are
more subjective than unmarked regions.
(2) The sentiment of each annotator rationale co-
incides with the document-level sentiment.
Note that assumption 1 was not observed in the
Zaidan et al (2007) work: annotators were asked
only to mark a few rationales, leaving other (also
subjective) rationale sections unmarked.
And at first glance, assumption (2) might seem
too obvious. But it is important to include as there
can be subjective regions with seemingly conflict-
ing sentiment in the same document (Pang et al,
2002). For instance, an author for a movie re-
view might express a positive sentiment toward
the movie, while also discussing a negative sen-
timent toward one of the fictional characters ap-
pearing in the movie. This implies that not all sub-
jective regions will be relevant for the document-
level sentiment classification ? rather only those
regions whose polarity matches that of the docu-
ment should be considered.
In order to extract regions that satisfy the above
assumptions, we first look for subjective regions
in each document, then filter out those regions that
exhibit a sentiment value (i.e., polarity) that con-
flicts with polarity of the document. Assumption
2 is important as there can be subjective regions
with seemingly conflicting sentiment in the same
document (Pang et al, 2002).
Because our ultimate goal is to reduce human
annotation effort as much as possible, we do not
employ supervised learning methods to directly
learn to identify good rationales from human-
annotated rationales. Instead, we opt for methods
that make use of only the document-level senti-
ment and off-the-shelf utilities that were trained
for slightly different sentiment classification tasks
using a corpus from a different domain and of a
different genre. Although such utilities might not
be optimal for our task, we hoped that these ba-
sic resources from the research community would
constitute an adequate source of sentiment infor-
mation for our purposes.
We next describe three methods for the auto-
matic acquisition of rationales.
337
3.1 Contextual Polarity Classification
The first approach employs OpinionFinder (Wil-
son et al, 2005a), an off-the-shelf opinion anal-
ysis utility.1 In particular, OpinionFinder identi-
fies phrases expressing positive or negative opin-
ions. Because OpinionFinder models the task as
a word-based classification problem rather than a
sequence tagging task, most of the identified opin-
ion phrases consist of a single word. In general,
such short text spans cannot fully incorporate the
contextual information relevant to the detection of
subjective language (Wilson et al, 2005a). There-
fore, we conjecture that good rationales should ex-
tend beyond short phrases.2 For simplicity, we
choose to extend OpinionFinder phrases to sen-
tence boundaries.
In addition, to be consistentwith our second op-
erating assumption, we keep only those sentences
whose polarity coincides with the document-level
polarity. In sentences where OpinionFindermarks
multiple opinion words with opposite polarities
we perform a simple voting ? if words with pos-
itive (or negative) polarity dominate, then we con-
sider the entire sentence as positive (or negative).
We ignore sentences with a tie. Each selected sen-
tence is considered as a separate rationale.
3.2 Polarity Lexicons
Unfortunately, domain shift as well as task mis-
match could be a problem with any opinion util-
ity based on supervised learning.3 Therefore, we
next consider an approach that does not rely on su-
pervised learning techniques but instead explores
the use of a manually constructed polarity lexicon.
In particular, we use the lexicon constructed for
Wilson et al (2005b), which contains about 8000
words. Each entry is assigned one of three polarity
values: positive, negative, neutral. We construct
rationales from the polarity lexicon for every in-
stance of positive and negative words in the lexi-
con that appear in the training corpus.
As in the OpinionFinder rationales, we extend
the words found by the PolarityLexicon approach
to sentence boundaries to incorporate potentially
1Available at www.cs.pitt.edu/mpqa/opinionfinderrelease/.
2This conjecture is indirectly confirmed by the fact that
human-annotated rationales are rarely a single word.
3It is worthwhile to note that OpinionFinder is trained on a
newswire corpus whose prevailing sentiment is known to be
negative (Wiebe et al, 2005). Furthermore, OpinionFinder
is trained for a task (word-level sentiment classification) that
is different from marking annotator rationales (sequence tag-
ging or text segmentation).
relevant contextual information. We retain as ra-
tionales only those sentences whose polarity co-
incides with the document-level polarity as deter-
mined via the voting scheme of Section 3.1.
3.3 Random Selection
Finally, we generate annotator rationales ran-
domly, selecting 25% of the sentences from each
document4 and treating each as a separate ratio-
nale.
3.4 Comparison of Automatic vs.
Human-annotated Rationales
Before evaluating the performance of the au-
tomatically generated rationales, we summarize
in Table 1 the differences between automatic
vs. human-generated rationales. All computa-
tions were performed on the same movie review
dataset of Pang and Lee (2004) used in Zaidan et
al. (2007). Note, that the Zaidan et al (2007) an-
notation guidelines did not insist that annotators
mark all rationales, only that some were marked
for each document. Nevertheless, we report pre-
cision, recall, and F-score based on overlap with
the human-annotated rationales of Zaidan et al
(2007), so as to demonstrate the degree to which
the proposed approaches align with human intu-
ition. Overlap measures were also employed by
Zaidan et al (2007).
As shown in Table 1, the annotator rationales
found by OpinionFinder (F-score 49.5%) and the
PolarityLexicon approach (F-score 52.6%) match
the human rationales much better than those found
by random selection (F-score 27.3%).
As expected, OpinionFinder?s positive ratio-
nales match the human rationales at a significantly
lower level (F-score 31.9%) than negative ratio-
nales (59.5%). This is due to the fact that Opinion-
Finder is trained on a dataset biased toward nega-
tive sentiment (see Section 3.1 - 3.2). In contrast,
all other approaches show a balanced performance
for positive and negative rationales vs. human ra-
tionales.
4 Experiments
For our contrastive learning experiments we use
SVM light (Joachims, 1999). We evaluate the use-
fulness of automatically generated rationales on
4We chose the value of 25% to match the percentage of
sentences per document, on average, that contain human-
annotated rationales in our dataset (24.7%).
338
% of sentences Precision Recall F-Score
Method selected ALL POS NEG ALL POS NEG ALL POS NEG
OPINIONFINDER 22.8% 54.9 56.1 54.6 45.1 22.3 65.3 49.5 31.9 59.5
POLARITYLEXICON 38.7% 45.2 42.7 48.5 63.0 71.8 55.0 52.6 53.5 51.6
RANDOM 25.0% 28.9 26.0 31.8 25.9 24.9 26.7 27.3 25.5 29.0
Table 1: Comparison of Automatic vs. Human-annotated Rationales.
five different datasets. The first is the movie re-
view data of Pang and Lee (2004), which was
manually annotated with rationales by Zaidan et
al. (2007)5; the remaining are four product re-
view datasets from Blitzer et al (2007).6 Only
the movie review dataset contains human annota-
tor rationales. We replicate the same feature set
and experimental set-up as in Zaidan et al (2007)
to facilitate comparison with their work.7
The contrastive learning method introduced in
Zaidan et al (2007) requires three parameters: (C,
?, Ccontrast). To set the parameters, we use a grid
search with step 0.1 for the range of values of each
parameter around the point (1,1,1). In total, we try
around 3000 different parameter triplets for each
type of rationales.
4.1 Experiments with the Movie Review Data
We follow Zaidan et al (2007) for the training/test
data splits. The top half of Table 2 shows the
performance of a system trained with no anno-
tator rationales vs. two variations of human an-
notator rationales. HUMANR treats each rationale
in the same way as Zaidan et al (2007). HU-
MANR@SENTENCE extends the human annotator
rationales to sentence boundaries, and then treats
each such sentence as a separate rationale. As
shown in Table 2, we get alost the same per-
formance from these two variations (91.33% and
91.61%).8 This result demonstrates that locking
rationales to sentence boundaries was a reasonable
5Available at http://www.cs.jhu.edu/?ozaidan/rationales/.
6http://www.cs.jhu.edu/?mdredze/datasets/sentiment/.
7We use binary unigram features corresponding to the un-
stemmed words or punctuation marks with count greater or
equal to 4 in the full 2000 documents, then we normalize the
examples to the unit length. When computing the pseudo ex-
amples ~xij = ~xi?~vij? we first compute (~xi ? ~vij) using the
binary representation. As a result, features (unigrams) that
appeared in both vectors will be zeroed out in the resulting
vector. We then normalize the resulting vector to a unit vec-
tor.
8The performance of HUMANR reported by Zaidan et al
(2007) is 92.2% which lies between the performance we get
(91.61%) and the oracle accuracy we get if we knew the best
parameters for the test set (92.67%).
Method Accuracy
NORATIONALES 88.56
HUMANR 91.61?
HUMANR@SENTENCE 91.33? ?
OPINIONFINDER 91.78? ?
POLARITYLEXICON 91.39? ?
RANDOM 90.00?
OPINIONFINDER+HUMANR@SENTENCE 92.50? 4
Table 2: Experimental results for the movie
review data.
? The numbers marked with ? (or ?) are statistically
significantly better than NORATIONALES according to a
paired t-test with p < 0.001 (or p < 0.01).
? The numbers marked with 4 are statistically significantly
better than HUMANR according to a paired t-test with
p < 0.01.
? The numbers marked with ? are not statistically signifi-
cantly worse than HUMANR according to a paired t-test with
p > 0.1.
choice.
Among the approaches that make use of only
automatic rationales (bottom half of Table 2), the
best is OPINIONFINDER, reaching 91.78% accu-
racy. This result is slightly better than results
exploiting human rationales (91.33-91.61%), al-
though the difference is not statistically signifi-
cant. This result demonstrates that automatically
generated rationales are just as good as human
rationales in improving document-level sentiment
classification. Similarly strong results are ob-
tained from the POLARITYLEXICON as well.
Rather unexpectedly, RANDOM also achieves
statistically significant improvement over NORA-
TIONALES (90.0% vs. 88.56%). However, notice
that the performance of RANDOM is statistically
significantly lower than those based on human ra-
tionales (91.33-91.61%).
In our experiments so far, we observed that
some of the automatic rationales are just as
good as human rationales in improving the
document-level sentiment classification. Could
we perhaps achieve an even better result if we
combine the automatic rationales with human
339
rationales? The answer is yes! The accuracy
of OPINIONFINDER+HUMANR@SENTENCE
reaches 92.50%, which is statistically signifi-
cantly better than HUMANR (91.61%). In other
words, not only can our automatically generated
rationales replace human rationales, but they can
also improve upon human rationales when they
are available.
4.2 Experiments with the Product Reviews
We next evaluate our approaches on datasets for
which human annotator rationales do not exist.
For this, we use some of the product review data
from Blitzer et al (2007): reviews for Books,
DVDs, Videos and Kitchen appliances. Each
dataset contains 1000 positive and 1000 negative
reviews. The reviews, however, are substantially
shorter than those in the movie review dataset:
the average number of sentences in each review
is 9.20/9.13/8.12/6.37 respectively vs. 30.86 for
the movie reviews. We perform 10-fold cross-
validation, where 8 folds are used for training, 1
fold for tuning parameters, and 1 fold for testing.
Table 3 shows the results. Rationale-based
methods perform statistically significantly bet-
ter than NORATIONALES for all but the Kitchen
dataset. An interesting trend in product re-
view datasets is that RANDOM rationales are just
as good as other more sophisticated rationales.
We suspect that this is because product reviews
are generally shorter and more focused than the
movie reviews, thereby any randomly selected
sentence is likely to be a good rationale. Quantita-
tively, subjective sentences in the product reviews
amount to 78% (McDonald et al, 2007), while
subjective sentences in the movie review dataset
are only about 25% (Mao and Lebanon, 2006).
4.3 Examples of Annotator Rationales
In this section, we examine an example to com-
pare the automatically generated rationales (using
OPINIONFINDER) with human annotator ratio-
nales for the movie review data. In the following
positive document snippet, automatic rationales
are underlined, while human-annotated ratio-
nales are in bold face.
...But a little niceness goes a long way these days, and
there?s no denying the entertainment value of that thing
you do! It?s just about impossible to hate. It?s an
inoffensive, enjoyable piece ofnostalgia that is sure to leave
audiences smiling and humming, if not singing, ?that thing
you do!? ?quite possibly for days...
Method Books DVDs Videos Kitchen
NORATIONALES 80.20 80.95 82.40 87.40
OPINIONFINDER 81.65? 82.35? 84.00? 88.40
POLARITYLEXICON 82.75? 82.85? 84.55? 87.90
RANDOM 82.05? 82.10? 84.15? 88.00
Table 3: Experimental results for subset of
Product Review data
? The numbers marked with ? (or ?) are statistically
significantly better than NORATIONALES according to a
paired t-test with p < 0.05 (or p < 0.08).
Notice that, although OPINIONFINDER misses
some human rationales, it avoids the inclusion of
?impossible to hate?, which contains only negative
terms and is likely to be confusing for the con-
trastive learner.
5 Related Work
In broad terms, constructing annotator rationales
automatically and using them to formulate con-
trastive examples can be viewed as learning with
prior knowledge (e.g., Schapire et al (2002), Wu
and Srihari (2004)). In our task, the prior knowl-
edge corresponds to our operating assumptions
given in Section 3. Those assumptions can be
loosely connected to recognizing and exploiting
discourse structure (e.g., Pang and Lee (2004),
Taboada et al (2009)). Our automatically gener-
ated rationales can be potentially combined with
other learning frameworks that can exploit anno-
tator rationales, such as Zaidan and Eisner (2008).
6 Conclusions
In this paper, we explore methods to automatically
generate annotator rationales for document-level
sentiment classification. Our study is motivated
by the desire to retain the performance gains of
rationale-enhanced learning models while elimi-
nating the need for additional human annotation
effort. By employing existing resources for sen-
timent analysis, we can create automatic annota-
tor rationales that are as good as human annotator
rationales in improving document-level sentiment
classification.
Acknowledgments
We thank anonymous reviewers for their comments. This
work was supported in part by National Science Founda-
tion Grants BCS-0904822, BCS-0624277, IIS-0535099 and
by the Department of Homeland Security under ONR Grant
N0014-07-1-0152.
340
References
John Blitzer, Mark Dredze, and Fernando Pereira. 2007. Bi-
ographies, bollywood, boom-boxes and blenders: Domain
adaptation for sentiment classification. In Proceedings of
the 45th Annual Meeting of the Association of Computa-
tional Linguistics, pages 440?447, Prague, Czech Repub-
lic, June. Association for Computational Linguistics.
Thorsten Joachims. 1999. Making large-scale support vector
machine learning practical. pages 169?184.
Yi Mao and Guy Lebanon. 2006. Sequential models for sen-
timent prediction. In Proceedings of the ICML Workshop:
Learning in Structured Output Spaces Open Problems in
Statistical Relational Learning Statistical Network Analy-
sis: Models, Issues and New Directions.
Ryan McDonald, Kerry Hannan, Tyler Neylon, Mike Wells,
and Jeff Reynar. 2007. Structured models for fine-to-
coarse sentiment analysis. In Proceedings of the 45th
Annual Meeting of the Association of Computational Lin-
guistics, pages 432?439, Prague, Czech Republic, June.
Association for Computational Linguistics.
Bo Pang and Lillian Lee. 2004. A sentimental education:
sentiment analysis using subjectivity summarization based
on minimum cuts. In ACL ?04: Proceedings of the 42nd
Annual Meeting on Association for Computational Lin-
guistics, page 271, Morristown, NJ, USA. Association for
Computational Linguistics.
Bo Pang and Lillian Lee. 2008. Opinion mining and senti-
ment analysis. Found. Trends Inf. Retr., 2(1-2):1?135.
Bo Pang, Lillian Lee, and Shivakumar Vaithyanathan. 2002.
Thumbs up?: sentiment classification using machine
learning techniques. In EMNLP ?02: Proceedings of the
ACL-02 conference on Empirical methods in natural lan-
guage processing, pages 79?86, Morristown, NJ, USA.
Association for Computational Linguistics.
Robert E. Schapire, Marie Rochery, Mazin G. Rahim, and
Narendra Gupta. 2002. Incorporating prior knowledge
into boosting. In ICML ?02: Proceedings of the Nine-
teenth International Conference on Machine Learning,
pages 538?545, San Francisco, CA, USA. Morgan Kauf-
mann Publishers Inc.
Maite Taboada, Julian Brooke, and Manfred Stede. 2009.
Genre-based paragraph classification for sentiment anal-
ysis. In Proceedings of the SIGDIAL 2009 Conference,
pages 62?70, London, UK, September. Association for
Computational Linguistics.
Janyce Wiebe, Theresa Wilson, and Claire Cardie. 2005.
Annotating expressions of opinions and emotions in lan-
guage. Language Resources and Evaluation, 1(2):0.
Theresa Wilson, Paul Hoffmann, Swapna Somasundaran, Ja-
son Kessler, Janyce Wiebe, Yejin Choi, Claire Cardie,
Ellen Riloff, and Siddharth Patwardhan. 2005a. Opinion-
finder: a system for subjectivity analysis. In Proceedings
of HLT/EMNLP on Interactive Demonstrations, pages 34?
35, Morristown, NJ, USA. Association for Computational
Linguistics.
Theresa Wilson, Janyce Wiebe, and Paul Hoffmann. 2005b.
Recognizing contextual polarity in phrase-level sentiment
analysis. In HLT-EMNLP ?05: Proceedings of the con-
ference on Human Language Technology and Empirical
Methods in Natural Language Processing, pages 347?
354, Morristown, NJ, USA. Association for Computa-
tional Linguistics.
Xiaoyun Wu and Rohini Srihari. 2004. Incorporating
prior knowledgewith weighted margin support vector ma-
chines. In KDD ?04: Proceedings of the tenth ACM
SIGKDD international conference on Knowledge discov-
ery and data mining, pages 326?333, New York, NY,
USA. ACM.
Omar F. Zaidan and Jason Eisner. 2008. Modeling anno-
tators: a generative approach to learning from annotator
rationales. In EMNLP ?08: Proceedings of the Confer-
ence on Empirical Methods in Natural LanguageProcess-
ing, pages 31?40, Morristown, NJ, USA. Association for
Computational Linguistics.
Omar F. Zaidan, Jason Eisner, and Christine Piatko. 2007.
Using ?annotator rationales? to improve machine learning
for text categorization. In NAACLHLT 2007; Proceedings
of the Main Conference, pages 260?267, April.
341
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 309?319,
Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational Linguistics
Finding Deceptive Opinion Spam by Any Stretch of the Imagination
Myle Ott Yejin Choi Claire Cardie
Department of Computer Science
Cornell University
Ithaca, NY 14853
{myleott,ychoi,cardie}@cs.cornell.edu
Jeffrey T. Hancock
Department of Communication
Cornell University
Ithaca, NY 14853
jth34@cornell.edu
Abstract
Consumers increasingly rate, review and re-
search products online (Jansen, 2010; Litvin
et al, 2008). Consequently, websites con-
taining consumer reviews are becoming tar-
gets of opinion spam. While recent work
has focused primarily on manually identifi-
able instances of opinion spam, in this work
we study deceptive opinion spam?fictitious
opinions that have been deliberately written to
sound authentic. Integrating work from psy-
chology and computational linguistics, we de-
velop and compare three approaches to detect-
ing deceptive opinion spam, and ultimately
develop a classifier that is nearly 90% accurate
on our gold-standard opinion spam dataset.
Based on feature analysis of our learned mod-
els, we additionally make several theoretical
contributions, including revealing a relation-
ship between deceptive opinions and imagina-
tive writing.
1 Introduction
With the ever-increasing popularity of review web-
sites that feature user-generated opinions (e.g.,
TripAdvisor1 and Yelp2), there comes an increasing
potential for monetary gain through opinion spam?
inappropriate or fraudulent reviews. Opinion spam
can range from annoying self-promotion of an un-
related website or blog to deliberate review fraud,
as in the recent case3 of a Belkin employee who
1http://tripadvisor.com
2http://yelp.com
3http://news.cnet.com/8301-1001_
3-10145399-92.html
hired people to write positive reviews for an other-
wise poorly reviewed product.4
While other kinds of spam have received consid-
erable computational attention, regrettably there has
been little work to date (see Section 2) on opinion
spam detection. Furthermore, most previous work in
the area has focused on the detection of DISRUPTIVE
OPINION SPAM?uncontroversial instances of spam
that are easily identified by a human reader, e.g., ad-
vertisements, questions, and other irrelevant or non-
opinion text (Jindal and Liu, 2008). And while the
presence of disruptive opinion spam is certainly a
nuisance, the risk it poses to the user is minimal,
since the user can always choose to ignore it.
We focus here on a potentially more insidi-
ous type of opinion spam: DECEPTIVE OPINION
SPAM?fictitious opinions that have been deliber-
ately written to sound authentic, in order to deceive
the reader. For example, one of the following two
hotel reviews is truthful and the other is deceptive
opinion spam:
1. I have stayed at many hotels traveling for both business
and pleasure and I can honestly stay that The James is
tops. The service at the hotel is first class. The rooms
are modern and very comfortable. The location is per-
fect within walking distance to all of the great sights and
restaurants. Highly recommend to both business trav-
ellers and couples.
2. My husband and I stayed at the James Chicago Hotel
for our anniversary. This place is fantastic! We knew
as soon as we arrived we made the right choice! The
rooms are BEAUTIFUL and the staff very attentive and
wonderful!! The area of the hotel is great, since I love
to shop I couldn?t ask for more!! We will definatly be
4It is also possible for opinion spam to be negative, poten-
tially in order to sully the reputation of a competitor.
309
back to Chicago and we will for sure be back to the James
Chicago.
Typically, these deceptive opinions are neither
easily ignored nor even identifiable by a human
reader;5 consequently, there are few good sources
of labeled data for this research. Indeed, in the ab-
sence of gold-standard data, related studies (see Sec-
tion 2) have been forced to utilize ad hoc procedures
for evaluation. In contrast, one contribution of the
work presented here is the creation of the first large-
scale, publicly available6 dataset for deceptive opin-
ion spam research, containing 400 truthful and 400
gold-standard deceptive reviews.
To obtain a deeper understanding of the nature of
deceptive opinion spam, we explore the relative util-
ity of three potentially complementary framings of
our problem. Specifically, we view the task as: (a)
a standard text categorization task, in which we use
n-gram?based classifiers to label opinions as either
deceptive or truthful (Joachims, 1998; Sebastiani,
2002); (b) an instance of psycholinguistic decep-
tion detection, in which we expect deceptive state-
ments to exemplify the psychological effects of ly-
ing, such as increased negative emotion and psycho-
logical distancing (Hancock et al, 2008; Newman et
al., 2003); and, (c) a problem of genre identification,
in which we view deceptive and truthful writing as
sub-genres of imaginative and informative writing,
respectively (Biber et al, 1999; Rayson et al, 2001).
We compare the performance of each approach
on our novel dataset. Particularly, we find that ma-
chine learning classifiers trained on features tradi-
tionally employed in (a) psychological studies of
deception and (b) genre identification are both out-
performed at statistically significant levels by n-
gram?based text categorization techniques. Notably,
a combined classifier with both n-gram and psy-
chological deception features achieves nearly 90%
cross-validated accuracy on this task. In contrast,
we find deceptive opinion spam detection to be well
beyond the capabilities of most human judges, who
perform roughly at-chance?a finding that is consis-
tent with decades of traditional deception detection
research (Bond and DePaulo, 2006).
5The second example review is deceptive opinion spam.
6Available by request at: http://www.cs.cornell.
edu/?myleott/op_spam
Additionally, we make several theoretical con-
tributions based on an examination of the feature
weights learned by our machine learning classifiers.
Specifically, we shed light on an ongoing debate in
the deception literature regarding the importance of
considering the context and motivation of a decep-
tion, rather than simply identifying a universal set
of deception cues. We also present findings that are
consistent with recent work highlighting the difficul-
ties that liars have encoding spatial information (Vrij
et al, 2009). Lastly, our study of deceptive opinion
spam detection as a genre identification problem re-
veals relationships between deceptive opinions and
imaginative writing, and between truthful opinions
and informative writing.
The rest of this paper is organized as follows: in
Section 2, we summarize related work; in Section 3,
we explain our methodology for gathering data and
evaluate human performance; in Section 4, we de-
scribe the features and classifiers employed by our
three automated detection approaches; in Section 5,
we present and discuss experimental results; finally,
conclusions and directions for future work are given
in Section 6.
2 Related Work
Spam has historically been studied in the contexts of
e-mail (Drucker et al, 2002), and the Web (Gyo?ngyi
et al, 2004; Ntoulas et al, 2006). Recently, re-
searchers have began to look at opinion spam as
well (Jindal and Liu, 2008; Wu et al, 2010; Yoo
and Gretzel, 2009).
Jindal and Liu (2008) find that opinion spam is
both widespread and different in nature from either
e-mail or Web spam. Using product review data,
and in the absence of gold-standard deceptive opin-
ions, they train models using features based on the
review text, reviewer, and product, to distinguish
between duplicate opinions7 (considered deceptive
spam) and non-duplicate opinions (considered truth-
ful). Wu et al (2010) propose an alternative strategy
for detecting deceptive opinion spam in the absence
7Duplicate (or near-duplicate) opinions are opinions that ap-
pear more than once in the corpus with the same (or similar)
text. While these opinions are likely to be deceptive, they are
unlikely to be representative of deceptive opinion spam in gen-
eral. Moreover, they are potentially detectable via off-the-shelf
plagiarism detection software.
310
of gold-standard data, based on the distortion of pop-
ularity rankings. Both of these heuristic evaluation
approaches are unnecessary in our work, since we
compare gold-standard deceptive and truthful opin-
ions.
Yoo and Gretzel (2009) gather 40 truthful and 42
deceptive hotel reviews and, using a standard statis-
tical test, manually compare the psychologically rel-
evant linguistic differences between them. In con-
trast, we create a much larger dataset of 800 opin-
ions that we use to develop and evaluate automated
deception classifiers.
Research has also been conducted on the re-
lated task of psycholinguistic deception detection.
Newman et al (2003), and later Mihalcea and
Strapparava (2009), ask participants to give both
their true and untrue views on personal issues
(e.g., their stance on the death penalty). Zhou et
al. (2004; 2008) consider computer-mediated decep-
tion in role-playing games designed to be played
over instant messaging and e-mail. However, while
these studies compare n-gram?based deception clas-
sifiers to a random guess baseline of 50%, we addi-
tionally evaluate and compare two other computa-
tional approaches (described in Section 4), as well
as the performance of human judges (described in
Section 3.3).
Lastly, automatic approaches to determining re-
view quality have been studied?directly (Weimer
et al, 2007), and in the contexts of helpful-
ness (Danescu-Niculescu-Mizil et al, 2009; Kim et
al., 2006; O?Mahony and Smyth, 2009) and credibil-
ity (Weerkamp and De Rijke, 2008). Unfortunately,
most measures of quality employed in those works
are based exclusively on human judgments, which
we find in Section 3 to be poorly calibrated to de-
tecting deceptive opinion spam.
3 Dataset Construction and Human
Performance
While truthful opinions are ubiquitous online, de-
ceptive opinions are difficult to obtain without re-
sorting to heuristic methods (Jindal and Liu, 2008;
Wu et al, 2010). In this section, we report our ef-
forts to gather (and validate with human judgments)
the first publicly available opinion spam dataset with
gold-standard deceptive opinions.
Following the work of Yoo and Gretzel (2009), we
compare truthful and deceptive positive reviews for
hotels found on TripAdvisor. Specifically, we mine
all 5-star truthful reviews from the 20 most popular
hotels on TripAdvisor8 in the Chicago area.9 De-
ceptive opinions are gathered for those same 20 ho-
tels using Amazon Mechanical Turk10 (AMT). Be-
low, we provide details of the collection methodolo-
gies for deceptive (Section 3.1) and truthful opinions
(Section 3.2). Ultimately, we collect 20 truthful and
20 deceptive opinions for each of the 20 chosen ho-
tels (800 opinions total).
3.1 Deceptive opinions via Mechanical Turk
Crowdsourcing services such as AMT have made
large-scale data annotation and collection efforts fi-
nancially affordable by granting anyone with ba-
sic programming skills access to a marketplace of
anonymous online workers (known as Turkers) will-
ing to complete small tasks.
To solicit gold-standard deceptive opinion spam
using AMT, we create a pool of 400 Human-
Intelligence Tasks (HITs) and allocate them evenly
across our 20 chosen hotels. To ensure that opin-
ions are written by unique authors, we allow only a
single submission per Turker. We also restrict our
task to Turkers who are located in the United States,
and who maintain an approval rating of at least 90%.
Turkers are allowed a maximum of 30 minutes to
work on the HIT, and are paid one US dollar for an
accepted submission.
Each HIT presents the Turker with the name and
website of a hotel. The HIT instructions ask the
Turker to assume that they work for the hotel?s mar-
keting department, and to pretend that their boss
wants them to write a fake review (as if they were
a customer) to be posted on a travel review website;
additionally, the review needs to sound realistic and
portray the hotel in a positive light. A disclaimer
8TripAdvisor utilizes a proprietary ranking system to assess
hotel popularity. We chose the 20 hotels with the greatest num-
ber of reviews, irrespective of the TripAdvisor ranking.
9It has been hypothesized that popular offerings are less
likely to become targets of deceptive opinion spam, since the
relative impact of the spam in such cases is small (Jindal and
Liu, 2008; Lim et al, 2010). By considering only the most
popular hotels, we hope to minimize the risk of mining opinion
spam and labeling it as truthful.
10http://mturk.com
311
Time spent t (minutes)
All submissions
count: 400
tmin: 0.08, tmax: 29.78
t?: 8.06, s: 6.32
Length ` (words)
All submissions
`min: 25, `max: 425
?`: 115.75, s: 61.30
Time spent t < 1
count: 47
`min: 39, `max: 407
?`: 113.94, s: 66.24
Time spent t ? 1
count: 353
`min: 25, `max: 425
?`: 115.99, s: 60.71
Table 1: Descriptive statistics for 400 deceptive opinion
spam submissions gathered using AMT. s corresponds to
the sample standard deviation.
indicates that any submission found to be of insuffi-
cient quality (e.g., written for the wrong hotel, unin-
telligible, unreasonably short,11 plagiarized,12 etc.)
will be rejected.
It took approximately 14 days to collect 400 sat-
isfactory deceptive opinions. Descriptive statistics
appear in Table 1. Submissions vary quite dramati-
cally both in length, and time spent on the task. Par-
ticularly, nearly 12% of the submissions were com-
pleted in under one minute. Surprisingly, an inde-
pendent two-tailed t-test between the mean length of
these submissions (?`t<1) and the other submissions
(?`t?1) reveals no significant difference (p = 0.83).
We suspect that these ?quick? users may have started
working prior to having formally accepted the HIT,
presumably to circumvent the imposed time limit.
Indeed, the quickest submission took just 5 seconds
and contained 114 words.
3.2 Truthful opinions from TripAdvisor
For truthful opinions, we mine all 6,977 reviews
from the 20 most popular Chicago hotels on
TripAdvisor. From these we eliminate:
? 3,130 non-5-star reviews;
? 41 non-English reviews;13
? 75 reviews with fewer than 150 characters
since, by construction, deceptive opinions are
11A submission is considered unreasonably short if it con-
tains fewer than 150 characters.
12Submissions are individually checked for plagiarism at
http://plagiarisma.net.
13Language is determined using http://tagthe.net.
at least 150 characters long (see footnote 11 in
Section 3.1);
? 1,607 reviews written by first-time authors?
new users who have not previously posted an
opinion on TripAdvisor?since these opinions
are more likely to contain opinion spam, which
would reduce the integrity of our truthful re-
view data (Wu et al, 2010).
Finally, we balance the number of truthful and
deceptive opinions by selecting 400 of the remain-
ing 2,124 truthful reviews, such that the document
lengths of the selected truthful reviews are similarly
distributed to those of the deceptive reviews. Work
by Serrano et al (2009) suggests that a log-normal
distribution is appropriate for modeling document
lengths. Thus, for each of the 20 chosen hotels, we
select 20 truthful reviews from a log-normal (left-
truncated at 150 characters) distribution fit to the
lengths of the deceptive reviews.14 Combined with
the 400 deceptive reviews gathered in Section 3.1
this yields our final dataset of 800 reviews.
3.3 Human performance
Assessing human deception detection performance
is important for several reasons. First, there are few
other baselines for our classification task; indeed, re-
lated studies (Jindal and Liu, 2008; Mihalcea and
Strapparava, 2009) have only considered a random
guess baseline. Second, assessing human perfor-
mance is necessary to validate the deceptive opin-
ions gathered in Section 3.1. If human performance
is low, then our deceptive opinions are convincing,
and therefore, deserving of further attention.
Our initial approach to assessing human perfor-
mance on this task was with Mechanical Turk. Un-
fortunately, we found that some Turkers selected
among the choices seemingly at random, presum-
ably to maximize their hourly earnings by obviating
the need to read the review. While a similar effect
has been observed previously (Akkaya et al, 2010),
there remains no universal solution.
Instead, we solicit the help of three volunteer un-
dergraduate university students to make judgments
on a subset of our data. This balanced subset, cor-
responding to the first fold of our cross-validation
14We use the R package GAMLSS (Rigby and Stasinopoulos,
2005) to fit the left-truncated log-normal distribution.
312
TRUTHFUL DECEPTIVE
Accuracy P R F P R F
HUMAN
JUDGE 1 61.9% 57.9 87.5 69.7 74.4 36.3 48.7
JUDGE 2 56.9% 53.9 95.0 68.8 78.9 18.8 30.3
JUDGE 3 53.1% 52.3 70.0 59.9 54.7 36.3 43.6
META
MAJORITY 58.1% 54.8 92.5 68.8 76.0 23.8 36.2
SKEPTIC 60.6% 60.8 60.0 60.4 60.5 61.3 60.9
Table 2: Performance of three human judges and two meta-judges on a subset of 160 opinions, corresponding to the
first fold of our cross-validation experiments in Section 5. Boldface indicates the largest value for each column.
experiments described in Section 5, contains all 40
reviews from each of four randomly chosen hotels.
Unlike the Turkers, our student volunteers are not
offered a monetary reward. Consequently, we con-
sider their judgements to be more honest than those
obtained via AMT.
Additionally, to test the extent to which the in-
dividual human judges are biased, we evaluate the
performance of two virtual meta-judges. Specifi-
cally, the MAJORITY meta-judge predicts ?decep-
tive? when at least two out of three human judges
believe the review to be deceptive, and the SKEP-
TIC meta-judge predicts ?deceptive? when any hu-
man judge believes the review to be deceptive.
Human and meta-judge performance is given in
Table 2. It is clear from the results that human
judges are not particularly effective at this task. In-
deed, a two-tailed binomial test fails to reject the
null hypothesis that JUDGE 2 and JUDGE 3 per-
form at-chance (p = 0.003, 0.10, 0.48 for the three
judges, respectively). Furthermore, all three judges
suffer from truth-bias (Vrij, 2008), a common find-
ing in deception detection research in which hu-
man judges are more likely to classify an opinion
as truthful than deceptive. In fact, JUDGE 2 clas-
sified fewer than 12% of the opinions as decep-
tive! Interestingly, this bias is effectively smoothed
by the SKEPTIC meta-judge, which produces nearly
perfectly class-balanced predictions. A subsequent
reevaluation of human performance on this task sug-
gests that the truth-bias can be reduced if judges
are given the class-proportions in advance, although
such prior knowledge is unrealistic; and ultimately,
performance remains similar to that of Table 2.
Inter-annotator agreement among the three
judges, computed using Fleiss? kappa, is 0.11.
While there is no precise rule for interpreting
kappa scores, Landis and Koch (1977) suggest
that scores in the range (0.00, 0.20] correspond
to ?slight agreement? between annotators. The
largest pairwise Cohen?s kappa is 0.12, between
JUDGE 2 and JUDGE 3?a value far below generally
accepted pairwise agreement levels. We suspect
that agreement among our human judges is so
low precisely because humans are poor judges of
deception (Vrij, 2008), and therefore they perform
nearly at-chance respective to one another.
4 Automated Approaches to Deceptive
Opinion Spam Detection
We consider three automated approaches to detect-
ing deceptive opinion spam, each of which utilizes
classifiers (described in Section 4.4) trained on the
dataset of Section 3. The features employed by each
strategy are outlined here.
4.1 Genre identification
Work in computational linguistics has shown that
the frequency distribution of part-of-speech (POS)
tags in a text is often dependent on the genre of the
text (Biber et al, 1999; Rayson et al, 2001). In our
genre identification approach to deceptive opinion
spam detection, we test if such a relationship exists
for truthful and deceptive reviews by constructing,
for each review, features based on the frequencies of
each POS tag.15 These features are also intended to
provide a good baseline with which to compare our
other automated approaches.
4.2 Psycholinguistic deception detection
The Linguistic Inquiry and Word Count (LIWC)
software (Pennebaker et al, 2007) is a popular au-
tomated text analysis tool used widely in the so-
cial sciences. It has been used to detect personality
15We use the Stanford Parser (Klein and Manning, 2003) to
obtain the relative POS frequencies.
313
traits (Mairesse et al, 2007), to study tutoring dy-
namics (Cade et al, 2010), and, most relevantly, to
analyze deception (Hancock et al, 2008; Mihalcea
and Strapparava, 2009; Vrij et al, 2007).
While LIWC does not include a text classifier, we
can create one with features derived from the LIWC
output. In particular, LIWC counts and groups
the number of instances of nearly 4,500 keywords
into 80 psychologically meaningful dimensions. We
construct one feature for each of the 80 LIWC di-
mensions, which can be summarized broadly under
the following four categories:
1. Linguistic processes: Functional aspects of text
(e.g., the average number of words per sen-
tence, the rate of misspelling, swearing, etc.)
2. Psychological processes: Includes all social,
emotional, cognitive, perceptual and biological
processes, as well as anything related to time or
space.
3. Personal concerns: Any references to work,
leisure, money, religion, etc.
4. Spoken categories: Primarily filler and agree-
ment words.
While other features have been considered in past
deception detection work, notably those of Zhou et
al. (2004), early experiments found LIWC features
to perform best. Indeed, the LIWC2007 software
used in our experiments subsumes most of the fea-
tures introduced in other work. Thus, we focus our
psycholinguistic approach to deception detection on
LIWC-based features.
4.3 Text categorization
In contrast to the other strategies just discussed,
our text categorization approach to deception de-
tection allows us to model both content and con-
text with n-gram features. Specifically, we consider
the following three n-gram feature sets, with the
corresponding features lowercased and unstemmed:
UNIGRAMS, BIGRAMS+, TRIGRAMS+, where the
superscript + indicates that the feature set subsumes
the preceding feature set.
4.4 Classifiers
Features from the three approaches just introduced
are used to train Na??ve Bayes and Support Vector
Machine classifiers, both of which have performed
well in related work (Jindal and Liu, 2008; Mihalcea
and Strapparava, 2009; Zhou et al, 2008).
For a document ~x, with label y, the Na??ve Bayes
(NB) classifier gives us the following decision rule:
y? = arg max
c
Pr(y = c) ? Pr(~x | y = c) (1)
When the class prior is uniform, for example
when the classes are balanced (as in our case), (1)
can be simplified to the maximum likelihood classi-
fier (Peng and Schuurmans, 2003):
y? = arg max
c
Pr(~x | y = c) (2)
Under (2), both the NB classifier used by Mihal-
cea and Strapparava (2009) and the language model
classifier used by Zhou et al (2008) are equivalent.
Thus, following Zhou et al (2008), we use the SRI
Language Modeling Toolkit (Stolcke, 2002) to esti-
mate individual language models, Pr(~x | y = c),
for truthful and deceptive opinions. We consider
all three n-gram feature sets, namely UNIGRAMS,
BIGRAMS+, and TRIGRAMS+, with corresponding
language models smoothed using the interpolated
Kneser-Ney method (Chen and Goodman, 1996).
We also train Support Vector Machine (SVM)
classifiers, which find a high-dimensional separating
hyperplane between two groups of data. To simplify
feature analysis in Section 5, we restrict our evalu-
ation to linear SVMs, which learn a weight vector
~w and bias term b, such that a document ~x can be
classified by:
y? = sign(~w ? ~x + b) (3)
We use SVMlight (Joachims, 1999) to train our
linear SVM models on all three approaches and
feature sets described above, namely POS, LIWC,
UNIGRAMS, BIGRAMS+, and TRIGRAMS+. We also
evaluate every combination of these features, but
for brevity include only LIWC+BIGRAMS+, which
performs best. Following standard practice, doc-
ument vectors are normalized to unit-length. For
LIWC+BIGRAMS+, we unit-length normalize LIWC
and BIGRAMS+ features individually before com-
bining them.
314
TRUTHFUL DECEPTIVE
Approach Features Accuracy P R F P R F
GENRE IDENTIFICATION POSSVM 73.0% 75.3 68.5 71.7 71.1 77.5 74.2
PSYCHOLINGUISTIC
LIWCSVM 76.8% 77.2 76.0 76.6 76.4 77.5 76.9
DECEPTION DETECTION
TEXT CATEGORIZATION
UNIGRAMSSVM 88.4% 89.9 86.5 88.2 87.0 90.3 88.6
BIGRAMS+SVM 89.6% 90.1 89.0 89.6 89.1 90.3 89.7
LIWC+BIGRAMS+SVM 89.8% 89.8 89.8 89.8 89.8 89.8 89.8
TRIGRAMS+SVM 89.0% 89.0 89.0 89.0 89.0 89.0 89.0
UNIGRAMSNB 88.4% 92.5 83.5 87.8 85.0 93.3 88.9
BIGRAMS+NB 88.9% 89.8 87.8 88.7 88.0 90.0 89.0
TRIGRAMS+NB 87.6% 87.7 87.5 87.6 87.5 87.8 87.6
HUMAN / META
JUDGE 1 61.9% 57.9 87.5 69.7 74.4 36.3 48.7
JUDGE 2 56.9% 53.9 95.0 68.8 78.9 18.8 30.3
SKEPTIC 60.6% 60.8 60.0 60.4 60.5 61.3 60.9
Table 3: Automated classifier performance for three approaches based on nested 5-fold cross-validation experiments.
Reported precision, recall and F-score are computed using a micro-average, i.e., from the aggregate true positive, false
positive and false negative rates, as suggested by Forman and Scholz (2009). Human performance is repeated here for
JUDGE 1, JUDGE 2 and the SKEPTIC meta-judge, although they cannot be directly compared since the 160-opinion
subset on which they are assessed only corresponds to the first cross-validation fold.
5 Results and Discussion
The deception detection strategies described in Sec-
tion 4 are evaluated using a 5-fold nested cross-
validation (CV) procedure (Quadrianto et al, 2009),
where model parameters are selected for each test
fold based on standard CV experiments on the train-
ing folds. Folds are selected so that each contains all
reviews from four hotels; thus, learned models are
always evaluated on reviews from unseen hotels.
Results appear in Table 3. We observe that auto-
mated classifiers outperform human judges for every
metric, except truthful recall where JUDGE 2 per-
forms best.16 However, this is expected given that
untrained humans often focus on unreliable cues to
deception (Vrij, 2008). For example, one study ex-
amining deception in online dating found that hu-
mans perform at-chance detecting deceptive pro-
files because they rely on text-based cues that are
unrelated to deception, such as second-person pro-
nouns (Toma and Hancock, In Press).
Among the automated classifiers, baseline per-
formance is given by the simple genre identifica-
tion approach (POSSVM) proposed in Section 4.1.
Surprisingly, we find that even this simple auto-
16As mentioned in Section 3.3, JUDGE 2 classified fewer than
12% of opinions as deceptive. While achieving 95% truthful re-
call, this judge?s corresponding precision was not significantly
better than chance (two-tailed binomial p = 0.4).
mated classifier outperforms most human judges
(one-tailed sign test p = 0.06, 0.01, 0.001 for the
three judges, respectively, on the first fold). This
result is best explained by theories of reality mon-
itoring (Johnson and Raye, 1981), which suggest
that truthful and deceptive opinions might be clas-
sified into informative and imaginative genres, re-
spectively. Work by Rayson et al (2001) has found
strong distributional differences between informa-
tive and imaginative writing, namely that the former
typically consists of more nouns, adjectives, prepo-
sitions, determiners, and coordinating conjunctions,
while the latter consists of more verbs,17 adverbs,18
pronouns, and pre-determiners. Indeed, we find that
the weights learned by POSSVM (found in Table 4)
are largely in agreement with these findings, no-
tably except for adjective and adverb superlatives,
the latter of which was found to be an exception by
Rayson et al (2001). However, that deceptive opin-
ions contain more superlatives is not unexpected,
since deceptive writing (but not necessarily imagi-
native writing in general) often contains exaggerated
language (Buller and Burgoon, 1996; Hancock et al,
2008).
Both remaining automated approaches to detect-
ing deceptive opinion spam outperform the simple
17Past participle verbs were an exception.
18Superlative adverbs were an exception.
315
TRUTHFUL/INFORMATIVE DECEPTIVE/IMAGINATIVE
Category Variant Weight Category Variant Weight
NOUNS
Singular 0.008
VERBS
Base -0.057
Plural 0.002 Past tense 0.041
Proper, singular -0.041 Present participle -0.089
Proper, plural 0.091 Singular, present -0.031
ADJECTIVES
General 0.002 Third person
0.026
Comparative 0.058 singular, present
Superlative -0.164 Modal -0.063
PREPOSITIONS General 0.064
ADVERBS
General 0.001
DETERMINERS General 0.009 Comparative -0.035
COORD. CONJ. General 0.094
PRONOUNS
Personal -0.098
VERBS Past participle 0.053 Possessive -0.303
ADVERBS Superlative -0.094 PRE-DETERMINERS General 0.017
Table 4: Average feature weights learned by POSSVM. Based on work by Rayson et al (2001), we expect weights on
the left to be positive (predictive of truthful opinions), and weights on the right to be negative (predictive of deceptive
opinions). Boldface entries are at odds with these expectations. We report average feature weights of unit-normalized
weight vectors, rather than raw weights vectors, to account for potential differences in magnitude between the folds.
genre identification baseline just discussed. Specifi-
cally, the psycholinguistic approach (LIWCSVM) pro-
posed in Section 4.2 performs 3.8% more accurately
(one-tailed sign test p = 0.02), and the standard text
categorization approach proposed in Section 4.3 per-
forms between 14.6% and 16.6% more accurately.
However, best performance overall is achieved by
combining features from these two approaches. Par-
ticularly, the combined model LIWC+BIGRAMS+SVM
is 89.8% accurate at detecting deceptive opinion
spam.19
Surprisingly, models trained only on
UNIGRAMS?the simplest n-gram feature set?
outperform all non?text-categorization approaches,
and models trained on BIGRAMS+ perform even
better (one-tailed sign test p = 0.07). This suggests
that a universal set of keyword-based deception
cues (e.g., LIWC) is not the best approach to de-
tecting deception, and a context-sensitive approach
(e.g., BIGRAMS+) might be necessary to achieve
state-of-the-art deception detection performance.
To better understand the models learned by these
automated approaches, we report in Table 5 the top
15 highest weighted features for each class (truthful
and deceptive) as learned by LIWC+BIGRAMS+SVM
and LIWCSVM. In agreement with theories of reality
monitoring (Johnson and Raye, 1981), we observe
that truthful opinions tend to include more sensorial
and concrete language than deceptive opinions; in
19The result is not significantly better than BIGRAMS+SVM.
LIWC+BIGRAMS+SVM LIWCSVM
TRUTHFUL DECEPTIVE TRUTHFUL DECEPTIVE
- chicago hear i
... my number family
on hotel allpunct perspron
location , and negemo see
) luxury dash pronoun
allpunctLIWC experience exclusive leisure
floor hilton we exclampunct
( business sexual sixletters
the hotel vacation period posemo
bathroom i otherpunct comma
small spa space cause
helpful looking human auxverb
$ while past future
hotel . husband inhibition perceptual
other my husband assent feel
Table 5: Top 15 highest weighted truthful and deceptive
features learned by LIWC+BIGRAMS+SVM and LIWCSVM.
Ambiguous features are subscripted to indicate the source
of the feature. LIWC features correspond to groups
of keywords as explained in Section 4.2; more details
about LIWC and the LIWC categories are available at
http://liwc.net.
particular, truthful opinions are more specific about
spatial configurations (e.g., small, bathroom, on, lo-
cation). This finding is also supported by recent
work by Vrij et al (2009) suggesting that liars have
considerable difficultly encoding spatial information
into their lies. Accordingly, we observe an increased
focus in deceptive opinions on aspects external to
the hotel being reviewed (e.g., husband, business,
316
vacation).
We also acknowledge several findings that, on the
surface, are in contrast to previous psycholinguistic
studies of deception (Hancock et al, 2008; Newman
et al, 2003). For instance, while deception is often
associated with negative emotion terms, our decep-
tive reviews have more positive and fewer negative
emotion terms. This pattern makes sense when one
considers the goal of our deceivers, namely to create
a positive review (Buller and Burgoon, 1996).
Deception has also previously been associated
with decreased usage of first person singular, an ef-
fect attributed to psychological distancing (Newman
et al, 2003). In contrast, we find increased first
person singular to be among the largest indicators
of deception, which we speculate is due to our de-
ceivers attempting to enhance the credibility of their
reviews by emphasizing their own presence in the
review. Additional work is required, but these find-
ings further suggest the importance of moving be-
yond a universal set of deceptive language features
(e.g., LIWC) by considering both the contextual (e.g.,
BIGRAMS+) and motivational parameters underly-
ing a deception as well.
6 Conclusion and Future Work
In this work we have developed the first large-scale
dataset containing gold-standard deceptive opinion
spam. With it, we have shown that the detection
of deceptive opinion spam is well beyond the ca-
pabilities of human judges, most of whom perform
roughly at-chance. Accordingly, we have introduced
three automated approaches to deceptive opinion
spam detection, based on insights coming from re-
search in computational linguistics and psychology.
We find that while standard n-gram?based text cate-
gorization is the best individual detection approach,
a combination approach using psycholinguistically-
motivated features and n-gram features can perform
slightly better.
Finally, we have made several theoretical con-
tributions. Specifically, our findings suggest the
importance of considering both the context (e.g.,
BIGRAMS+) and motivations underlying a decep-
tion, rather than strictly adhering to a universal set
of deception cues (e.g., LIWC). We have also pre-
sented results based on the feature weights learned
by our classifiers that illustrate the difficulties faced
by liars in encoding spatial information. Lastly, we
have discovered a plausible relationship between de-
ceptive opinion spam and imaginative writing, based
on POS distributional similarities.
Possible directions for future work include an ex-
tended evaluation of the methods proposed in this
work to both negative opinions, as well as opinions
coming from other domains. Many additional ap-
proaches to detecting deceptive opinion spam are
also possible, and a focus on approaches with high
deceptive precision might be useful for production
environments.
Acknowledgments
This work was supported in part by National
Science Foundation Grants BCS-0624277, BCS-
0904822, HSD-0624267, IIS-0968450, and NSCC-
0904822, as well as a gift from Google, and the
Jack Kent Cooke Foundation. We also thank, al-
phabetically, Rachel Boochever, Cristian Danescu-
Niculescu-Mizil, Alicia Granstein, Ulrike Gretzel,
Danielle Kirshenblat, Lillian Lee, Bin Lu, Jack
Newton, Melissa Sackler, Mark Thomas, and Angie
Yoo, as well as members of the Cornell NLP sem-
inar group and the ACL reviewers for their insight-
ful comments, suggestions and advice on various as-
pects of this work.
References
C. Akkaya, A. Conrad, J. Wiebe, and R. Mihalcea. 2010.
Amazon mechanical turk for subjectivity word sense
disambiguation. In Proceedings of the NAACL HLT
2010 Workshop on Creating Speech and Language
Data with Amazons Mechanical Turk, Los Angeles,
pages 195?203.
D. Biber, S. Johansson, G. Leech, S. Conrad, E. Finegan,
and R. Quirk. 1999. Longman grammar of spoken and
written English, volume 2. MIT Press.
C.F. Bond and B.M. DePaulo. 2006. Accuracy of de-
ception judgments. Personality and Social Psychology
Review, 10(3):214.
D.B. Buller and J.K. Burgoon. 1996. Interpersonal
deception theory. Communication Theory, 6(3):203?
242.
W.L. Cade, B.A. Lehman, and A. Olney. 2010. An ex-
ploration of off topic conversation. In Human Lan-
guage Technologies: The 2010 Annual Conference of
317
the North American Chapter of the Association for
Computational Linguistics, pages 669?672. Associa-
tion for Computational Linguistics.
S.F. Chen and J. Goodman. 1996. An empirical study of
smoothing techniques for language modeling. In Pro-
ceedings of the 34th annual meeting on Association
for Computational Linguistics, pages 310?318. Asso-
ciation for Computational Linguistics.
C. Danescu-Niculescu-Mizil, G. Kossinets, J. Kleinberg,
and L. Lee. 2009. How opinions are received by on-
line communities: a case study on amazon.com help-
fulness votes. In Proceedings of the 18th international
conference on World wide web, pages 141?150. ACM.
H. Drucker, D. Wu, and V.N. Vapnik. 2002. Support
vector machines for spam categorization. Neural Net-
works, IEEE Transactions on, 10(5):1048?1054.
G. Forman and M. Scholz. 2009. Apples-to-Apples in
Cross-Validation Studies: Pitfalls in Classifier Perfor-
mance Measurement. ACM SIGKDD Explorations,
12(1):49?57.
Z. Gyo?ngyi, H. Garcia-Molina, and J. Pedersen. 2004.
Combating web spam with trustrank. In Proceedings
of the Thirtieth international conference on Very large
data bases-Volume 30, pages 576?587. VLDB Endow-
ment.
J.T. Hancock, L.E. Curry, S. Goorha, and M. Woodworth.
2008. On lying and being lied to: A linguistic anal-
ysis of deception in computer-mediated communica-
tion. Discourse Processes, 45(1):1?23.
J. Jansen. 2010. Online product research. Pew Internet
& American Life Project Report.
N. Jindal and B. Liu. 2008. Opinion spam and analysis.
In Proceedings of the international conference on Web
search and web data mining, pages 219?230. ACM.
T. Joachims. 1998. Text categorization with support vec-
tor machines: Learning with many relevant features.
Machine Learning: ECML-98, pages 137?142.
T. Joachims. 1999. Making large-scale support vec-
tor machine learning practical. In Advances in kernel
methods, page 184. MIT Press.
M.K. Johnson and C.L. Raye. 1981. Reality monitoring.
Psychological Review, 88(1):67?85.
S.M. Kim, P. Pantel, T. Chklovski, and M. Pennacchiotti.
2006. Automatically assessing review helpfulness.
In Proceedings of the 2006 Conference on Empirical
Methods in Natural Language Processing, pages 423?
430. Association for Computational Linguistics.
D. Klein and C.D. Manning. 2003. Accurate unlexical-
ized parsing. In Proceedings of the 41st Annual Meet-
ing on Association for Computational Linguistics-
Volume 1, pages 423?430. Association for Computa-
tional Linguistics.
J.R. Landis and G.G. Koch. 1977. The measurement of
observer agreement for categorical data. Biometrics,
33(1):159.
E.P. Lim, V.A. Nguyen, N. Jindal, B. Liu, and H.W.
Lauw. 2010. Detecting product review spammers us-
ing rating behaviors. In Proceedings of the 19th ACM
international conference on Information and knowl-
edge management, pages 939?948. ACM.
S.W. Litvin, R.E. Goldsmith, and B. Pan. 2008. Elec-
tronic word-of-mouth in hospitality and tourism man-
agement. Tourism management, 29(3):458?468.
F. Mairesse, M.A. Walker, M.R. Mehl, and R.K. Moore.
2007. Using linguistic cues for the automatic recogni-
tion of personality in conversation and text. Journal of
Artificial Intelligence Research, 30(1):457?500.
R. Mihalcea and C. Strapparava. 2009. The lie detector:
Explorations in the automatic recognition of deceptive
language. In Proceedings of the ACL-IJCNLP 2009
Conference Short Papers, pages 309?312. Association
for Computational Linguistics.
M.L. Newman, J.W. Pennebaker, D.S. Berry, and J.M.
Richards. 2003. Lying words: Predicting deception
from linguistic styles. Personality and Social Psychol-
ogy Bulletin, 29(5):665.
A. Ntoulas, M. Najork, M. Manasse, and D. Fetterly.
2006. Detecting spam web pages through content
analysis. In Proceedings of the 15th international con-
ference on World Wide Web, pages 83?92. ACM.
M.P. O?Mahony and B. Smyth. 2009. Learning to rec-
ommend helpful hotel reviews. In Proceedings of
the third ACM conference on Recommender systems,
pages 305?308. ACM.
F. Peng and D. Schuurmans. 2003. Combining naive
Bayes and n-gram language models for text classifica-
tion. Advances in Information Retrieval, pages 547?
547.
J.W. Pennebaker, C.K. Chung, M. Ireland, A. Gonzales,
and R.J. Booth. 2007. The development and psycho-
metric properties of LIWC2007. Austin, TX, LIWC.
Net.
N. Quadrianto, A.J. Smola, T.S. Caetano, and Q.V.
Le. 2009. Estimating labels from label proportions.
The Journal of Machine Learning Research, 10:2349?
2374.
P. Rayson, A. Wilson, and G. Leech. 2001. Grammatical
word class variation within the British National Cor-
pus sampler. Language and Computers, 36(1):295?
306.
R.A. Rigby and D.M. Stasinopoulos. 2005. Generalized
additive models for location, scale and shape. Jour-
nal of the Royal Statistical Society: Series C (Applied
Statistics), 54(3):507?554.
318
F. Sebastiani. 2002. Machine learning in automated
text categorization. ACM computing surveys (CSUR),
34(1):1?47.
M.A?. Serrano, A. Flammini, and F. Menczer. 2009.
Modeling statistical properties of written text. PloS
one, 4(4):5372.
A. Stolcke. 2002. SRILM-an extensible language mod-
eling toolkit. In Seventh International Conference on
Spoken Language Processing, volume 3, pages 901?
904. Citeseer.
C. Toma and J.T. Hancock. In Press. What Lies Beneath:
The Linguistic Traces of Deception in Online Dating
Profiles. Journal of Communication.
A. Vrij, S. Mann, S. Kristen, and R.P. Fisher. 2007. Cues
to deception and ability to detect lies as a function
of police interview styles. Law and human behavior,
31(5):499?518.
A. Vrij, S. Leal, P.A. Granhag, S. Mann, R.P. Fisher,
J. Hillman, and K. Sperry. 2009. Outsmarting the
liars: The benefit of asking unanticipated questions.
Law and human behavior, 33(2):159?166.
A. Vrij. 2008. Detecting lies and deceit: Pitfalls and
opportunities. Wiley-Interscience.
W. Weerkamp and M. De Rijke. 2008. Credibility im-
proves topical blog post retrieval. ACL-08: HLT,
pages 923?931.
M. Weimer, I. Gurevych, and M. Mu?hlha?user. 2007. Au-
tomatically assessing the post quality in online discus-
sions on software. In Proceedings of the 45th An-
nual Meeting of the ACL on Interactive Poster and
Demonstration Sessions, pages 125?128. Association
for Computational Linguistics.
G. Wu, D. Greene, B. Smyth, and P. Cunningham. 2010.
Distortion as a validation criterion in the identification
of suspicious reviews. Technical report, UCD-CSI-
2010-04, University College Dublin.
K.H. Yoo and U. Gretzel. 2009. Comparison of De-
ceptive and Truthful Travel Reviews. Information and
Communication Technologies in Tourism 2009, pages
37?47.
L. Zhou, J.K. Burgoon, D.P. Twitchell, T. Qin, and J.F.
Nunamaker Jr. 2004. A comparison of classifica-
tion methods for predicting deception in computer-
mediated communication. Journal of Management In-
formation Systems, 20(4):139?166.
L. Zhou, Y. Shi, and D. Zhang. 2008. A Statistical Lan-
guage Modeling Approach to Online Deception De-
tection. IEEE Transactions on Knowledge and Data
Engineering, 20(8):1077?1081.
319
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 320?330,
Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational Linguistics
 
 
Joint Bilingual Sentiment Classification with Unlabeled Parallel Corpora 
 
Bin Lu1,3*, Chenhao Tan2, Claire Cardie2 and Benjamin K. Tsou3,1 
1 Department of Chinese, Translation and Linguistics, City University of Hong Kong, Hong Kong 
2 Department of Computer Science, Cornell University, Ithaca, NY, USA 
3 Research Centre on Linguistics and Language Information Sciences,  
Hong Kong Institute of Education, Hong Kong  
lubin2010@gmail.com, {chenhao, cardie}@cs.cornell.edu, btsou99@gmail.com 
 
 
Abstract 
Most previous work on multilingual sentiment 
analysis has focused on methods to adapt 
sentiment resources from resource-rich 
languages to resource-poor languages. We 
present a novel approach for joint bilingual 
sentiment classification at the sentence level 
that augments available labeled data in each 
language with unlabeled parallel data. We rely 
on the intuition that the sentiment labels for 
parallel sentences should be similar and present 
a model that jointly learns improved mono-
lingual sentiment classifiers for each language. 
Experiments on multiple data sets show that the 
proposed approach (1) outperforms the mono-
lingual baselines, significantly improving the 
accuracy for both languages by 3.44%-8.12%; 
(2) outperforms two standard approaches for 
leveraging unlabeled data; and (3) produces 
(albeit smaller) performance gains when 
employing pseudo-parallel data from machine 
translation engines. 
1 Introduction 
The field of sentiment analysis has quickly 
attracted the attention of researchers and 
practitioners alike (e.g. Pang et al, 2002; Turney, 
2002; Hu and Liu, 2004; Wiebe et al, 2005; Breck 
et al, 2007; Pang and Lee, 2008). 1 Indeed, 
sentiment analysis systems, which mine opinions 
from textual sources (e.g. news, blogs, and 
reviews), can be used in a wide variety of 
                                                          
*The work was conducted when the first author was visiting 
Cornell University. 
applications, including interpreting product 
reviews, opinion retrieval and political polling.  
Not surprisingly, most methods for sentiment 
classification are supervised learning techniques, 
which require training data annotated with the 
appropriate sentiment labels (e.g. document-level 
or sentence-level positive vs. negative polarity).  
This data is difficult and costly to obtain, and must 
be acquired separately for each language under 
consideration.  
Previous work in multilingual sentiment analysis 
has therefore focused on methods to adapt 
sentiment resources (e.g. lexicons) from resource-
rich languages (typically English) to other 
languages, with the goal of transferring sentiment 
or subjectivity analysis capabilities from English to 
other languages (e.g. Mihalcea et al (2007); Banea 
et al (2008; 2010); Wan (2008; 2009); 
Prettenhofer and Stein (2010)). In recent years, 
however, sentiment-labeled data is gradually 
becoming available for languages other than 
English (e.g. Seki et al (2007; 2008); Nakagawa et 
al. (2010); Schulz et al (2010)). In addition, there 
is still much room for improvement in existing 
monolingual (including English) sentiment 
classifiers, especially at the sentence level (Pang 
and Lee, 2008).  
This paper tackles the task of bilingual 
sentiment analysis. In contrast to previous work, 
we (1) assume that some amount of sentiment-
labeled data is available for the language pair 
under study, and (2) investigate methods to 
simultaneously improve sentiment classification 
for both languages. Given the labeled data in each 
language, we propose an approach that exploits an 
unlabeled parallel corpus with the following 
320
  
intuition: two sentences or documents that are 
parallel (i.e. translations of one another) should 
exhibit the same sentiment ? their sentiment 
labels (e.g. polarity, subjectivity, intensity) should 
be similar. The proposed maximum entropy-based 
EM approach jointly learns two monolingual 
sentiment classifiers by treating the sentiment 
labels in the unlabeled parallel text as unobserved 
latent variables, and maximizes the regularized 
joint likelihood of the language-specific labeled 
data together with the inferred sentiment labels of 
the parallel text.  Although our approach should be 
applicable at the document-level and for additional 
sentiment tasks, we focus on sentence-level 
polarity classification in this work. 
We evaluate our approach for English and 
Chinese on two dataset combinations (see Section 
4) and find that the proposed approach outperforms 
the monolingual baselines (i.e. maximum entropy 
and SVM classifiers) as well as two alternative 
methods for leveraging unlabeled data 
(transductive SVMs (Joachims, 1999b) and co-
training (Blum and Mitchell, 1998)).  Accuracy is 
significantly improved for both languages, by 
3.44%-8.12%. We furthermore find that 
improvements, albeit smaller, are obtained when 
the parallel data is replaced with a pseudo-parallel 
(i.e. automatically translated) corpus. To our 
knowledge, this is the first multilingual sentiment 
analysis study to focus on methods for 
simultaneously improving sentiment classification 
for a pair of languages based on unlabeled data 
rather than resource adaptation from one language 
to another.  
The rest of the paper is organized as follows. 
Section 2 introduces related work. In Section 3, the 
proposed joint model is described. Sections 4 and 
5, respectively, provide the experimental setup and 
results; the conclusion (Section 6) follows. 
2 Related Work 
Multilingual Sentiment Analysis.  There is a 
growing body of work on multilingual sentiment 
analysis. Most approaches focus on resource 
adaptation from one language (usually English) to 
other languages with few sentiment resources. 
Mihalcea et al (2007), for example, generate 
subjectivity analysis resources in a new language 
from English sentiment resources by leveraging a 
bilingual dictionary or a parallel corpus. Banea et 
al. (2008; 2010) instead automatically translate the 
English resources using automatic machine 
translation engines for subjectivity classification. 
Prettenhofer and Stein (2010) investigate cross-
lingual sentiment classification from the 
perspective of domain adaptation based on 
structural correspondence learning (Blitzer et al, 
2006). 
Approaches that do not explicitly involve 
resource adaptation include Wan (2009), which 
uses co-training (Blum and Mitchell, 1998) with 
English vs. Chinese features comprising the two 
independent ?views? to exploit unlabeled Chinese 
data and a labeled English corpus and thereby 
improves Chinese sentiment classification. 
Another notable approach is the work of Boyd-
Graber and Resnik (2010), which presents a 
generative model --- supervised multilingual latent 
Dirichlet alocation --- that jointly models topics 
that are consistent across languages, and employs 
them to better predict sentiment ratings. 
Unlike the methods described above, we focus 
on simultaneously improving the performance of 
sentiment classification in a pair of languages by 
developing a model that relies on sentiment-
labeled data in each language as well as unlabeled 
parallel text for the language pair. 
Semi-supervised Learning.  Another line of 
related work is semi-supervised learning, which 
combines labeled and unlabeled data to improve 
the performance of the task of interest (Zhu and 
Goldberg, 2009). Among the popular semi-
supervised methods (e.g. EM on Na?ve Bayes 
(Nigam et al, 2000), co-training (Blum and 
Mitchell, 1998), transductive SVMs (Joachims, 
1999b), and co-regularization (Sindhwani et al, 
2005; Amini et al, 2010)), our approach employs 
the EM algorithm, extending it to the bilingual 
case based on maximum entropy. We compare to 
co-training and transductive SVMs in Section 5. 
Multilingual NLP for Other Tasks. Finally, 
there exists related work using bilingual resources 
to help other NLP tasks, such as word sense 
disambiguation (e.g. Ido and Itai (1994)), parsing 
(e.g. Burkett and Klein (2008); Zhao et al (2009); 
Burkett et al (2010)), information retrieval (Gao et 
al., 2009), named entity detection (Burkett et al, 
2010); topic extraction (e.g. Zhang et al, 2010), 
text classification (e.g. Amini et al, 2010), and 
hyponym-relation acquisition (e.g. Oh et al, 2009). 
321
  
In these cases, multilingual models increase 
performance because different languages contain 
different ambiguities and therefore present 
complementary views on the shared underlying 
labels.  Our work shares a similar motivation. 
3 A Joint Model with Unlabeled Parallel 
Text 
We propose a maximum entropy-based statistical 
model. Maximum entropy (MaxEnt) models1 have 
been widely used in many NLP tasks (Berger et al, 
1996; Ratnaparkhi, 1997; Smith, 2006). The 
models assign the conditional probability of the 
label   given the observation   as follows: 
          
 
 
                                 (1) 
where    is a real-valued vector of feature weights 
and    is a feature function that maps pairs       to 
a nonnegative real-valued feature vector. Each 
feature has an associated parameter,   , which is 
called its weight; and   is the corresponding 
normalization factor.  
Maximum likelihood parameter estimation 
(training) for such a model, with a set of labeled 
examples            
  , amounts to solving the 
following optimization problem: 
  
 
                   
 
                        (2) 
3.1 Problem Definition 
Given two languages    and   , suppose we have 
two distinct (i.e. not parallel) sets of sentiment-
labeled data,    and     written in    and     
respectively. In addition, we have unlabeled (w.r.t. 
sentiment) bilingual (in    and   ) parallel data   
that are defined as follows. 
               
    
     
     
               
    
     
    
     
    
       
     
   
   
 
   
where               denotes the polarity of 
the  -th instance    (positive or negative);    and    
are respectively the numbers of labeled instances 
in    and   ;   
   and   
   are parallel instances in    
and   , respectively (i.e. they are supposed to be 
                                                          
1They are sometimes referred to as log-linear models, but also 
known as exponential models, generalized linear models, or 
logistic regression. 
translations of one another), whose labels   
   and 
  
   are unobserved, but according to the intuition 
outlined in Section 1, should be similar.  
Given the input data        and  , our task is to 
jointly learn two monolingual sentiment classifiers 
? one for    and one for   . With MaxEnt, we 
learn from the input data:  
                   
 
     
 
  
where    
   and     
 
 are the vectors of feature weights 
for    and   , respectively (for brevity we denote 
them as    and    in the remaining sections). In this 
study, we focus on sentence-level sentiment 
classification, i.e. each    is a sentence, and   
   and 
  
   are parallel sentences. 
3.2 The Joint Model  
Given the problem definition above, we now 
present a novel model to exploit the 
correspondence of parallel sentences in unlabeled 
bilingual text. The model maximizes the following 
joint likelihood with respect to    and   : 
                                        
    
    
    
    
         
        
    
     
  
   
 
     
     
     
     
     
         
 
   (3) 
where          denotes    or   ; the first term on 
the right-hand side is the likelihood of labeled data 
for both    and   ; and the second term is the 
likelihood of the unlabeled parallel data  .  
If we assume that parallel sentences are perfect 
translations, the two sentences in each pair should 
have the same polarity label, which gives us:   
    
     
     
     
          
     
    
          
    
        
                          (4) 
where   
  is the unobserved class label for the  -th 
instance in the unlabeled data. This probability 
directly models the sentiment label agreement 
between   
   and   
  . 
However, there could be considerable noise in 
real-world parallel data, i.e. the sentence pairs may 
be noisily parallel (or even comparable) instead of 
fully parallel (Munteanu and Marcu, 2005). In such 
noisy cases, the labels (positive or negative) could 
be different for the two monolingual sentences in a 
sentence pair. Although we do not know the exact 
probability that a sentence pair exhibits the same 
label, we can approximate it using their translation 
322
  
probabilities, which can be computed using word 
alignment toolkits such as Giza++ (Och and Ney, 
2003) or the Berkeley word aligner (Liang et al, 
2006). The intuition here is that if the translation 
probability of two sentences is high, the probability 
that they have the same sentiment label should be 
high as well. Therefore, by considering the noise in 
parallel data, we get: 
    
     
     
     
           
          
    
         
    
            
              
    
             
               (5)                 
where       is the translation probability of the  -th 
sentence pair in  ;2    is the opposite of   ; the first 
term models the probability that   
   and   
   have 
the same label; and the second term models the 
probability that they have different labels.  
By further considering the weight to ascribe to 
the unlabeled data vs. the labeled data (and the 
weight for the L2-norm regularization), we get the 
following regularized joint log likelihood to be 
maximized: 
                                    
 
    
         
    
    
    
         
  
 
      
  
         (6) 
where the first term on the right-hand side is the 
log likelihood of the labeled data from both    and 
    the second is the log likelihood of the 
unlabeled parallel data  , multiplied by     , a 
constant that controls the contribution of the 
unlabeled data; and      is a regularization 
constant that penalizes model complexity or large 
feature weights. When    is 0, the algorithm 
ignores the unlabeled data and degenerates to two 
MaxEnt models trained on only the labeled data. 
3.3 The EM Algorithm on MaxEnt 
To solve the optimization problem for the model, 
we need to jointly estimate the optimal parameters 
for the two monolingual classifiers by finding: 
   
    
                                      (7) 
This can be done with an EM algorithm, whose 
steps are summarized in Algorithm 1. First, the 
MaxEnt parameters,    and   , are estimated from 
                                                          
2The probability should be rescaled within the range of [0, 1], 
where 0.5 means that we are completely unsure if the 
sentences are translations of each other or not, and only those 
translation pairs with a probability larger than 0.5 are 
meaningful for our purpose. 
just the labeled data. Then, in the E-step, the 
classifiers, based on current values of     and   , 
compute          for each labeled example and 
assign probabilistically-weighted class labels to 
each unlabeled example. Next, in the M-step, the 
parameters,    and   , are updated using both the 
original labeled data (   and   ) and the newly 
labeled data  . These last two steps are iterated 
until convergence or a predefined iteration limit  . 
Algorithm 1. The MaxEnt-based EM Algorithm for 
Multilingual Sentiment Classification 
Input: Labeled data    and    
Unlabeled parallel data   
Output: 
Two monolingual MaxEnt classifiers with 
parameters   
  and   
 , respectively 
1. Train two initial monolingual models 
Train and initialize   
   
 and   
   
 on the labeled data 
2. Jointly optimize two monolingual models 
for     to   do // T: number of iterations 
       E-Step: 
Compute         for each example in    ,    and   
based on   
     
 and   
     
; 
Compute the expectation of the log likelihood with 
respect to       ; 
M-Step: 
Find    
   
 and   
   
 by maximizing the regularized 
joint log likelihood; 
Convergence: 
 If the increase of the joint log likelihood is 
sufficiently small, break; 
      end for  
3. Output    
  as   
   
s, and   
  as    
   
  
In the M-step, we can optimize the regularized 
joint log likelihood using any gradient-based 
optimization technique (Malouf, 2002). The 
gradient for Equation 3 based on Equation 4 is 
shown in Appendix A; those for Equations 5 and 6 
can be derived similarly. In our experiments, we 
use the L-BFGS algorithm (Liu et al, 1989) and 
run EM until the change in regularized joint log 
likelihood is less than 1e-5 or we reach 100 
iterations.3 
                                                          
3Since the EM-based algorithm may find a local maximum of 
the objective function, the initialization of the parameters is 
important. Our experiments show that an effective maximum 
can usually be found by initializing the parameters with those 
learned from the labeled data; performance would be much 
worse if we initialize all the parameters to 0 or 1. 
323
  
3.4 Pseudo-Parallel Labeled and Unlabeled 
Data 
We also consider the case where a parallel corpus 
is not available: to obtain a pseudo-parallel corpus 
  (i.e. sentences in one language with their 
corresponding automatic translations), we use an 
automatic machine translation system (e.g. Google 
machine translation 4 ) to translate unlabeled in-
domain data from    to    or vice versa. 
Since previous work (Banea et al, 2008; 2010; 
Wan, 2009) has shown that it could be useful to 
automatically translate the labeled data from the 
source language into the target language, we can 
further incorporate such translated labeled data into 
the joint model by adding the following component 
into Equation 6: 
           
    
      
  
   
 
                  (8) 
where    is the alternative class of  ,   
   is the 
automatically translated example from   
 ; and  
     is a constant that controls the weight of the 
translated labeled data. 
4 Experimental Setup 
4.1 Data Sets and Preprocessing 
The following labeled datasets are used in our 
experiments. 
MPQA (Labeled English Data): The Multi-
Perspective Question Answering (MPQA) corpus 
(Wiebe et al, 2005) consists of newswire 
documents manually annotated with phrase-level 
subjectivity information. We extract all sentences 
containing strong (i.e. intensity is medium or 
higher), sentiment-bearing (i.e. polarity is positive 
or negative) expressions following Choi and 
Cardie (2008). Sentences with both positive and 
negative strong expressions are then discarded, and 
the polarity of each remaining sentence is set to 
that of its sentiment-bearing expression(s). 
NTCIR-EN (Labeled English Data) and 
NTCIR-CH (Labeled Chinese Data): The 
NTCIR Opinion Analysis task (Seki et al, 2007; 
2008) provides sentiment-labeled news data in 
Chinese, Japanese and English. Only those 
sentences with a polarity label (positive or 
negative) agreed to by at least two annotators are 
extracted. We use the Chinese data from NTCIR-6 
                                                          
4http://translate.google.com/ 
as our Chinese labeled data. Since far fewer 
sentences in the English data pass the annotator 
agreement filter, we combine the English data from 
NTCIR-6 and NTCIR-7. The Chinese sentences 
are segmented using the Stanford Chinese word 
segmenter (Tseng et al, 2005). 
The number of sentences in each of these 
datasets is shown in Table 1. In our experiments, 
we evaluate two settings of the data: (1) 
MPQA+NTCIR-CH, and (2) NTCIR-EN+NTCIR-
CH. In each setting, the English labeled data 
constitutes    and the Chinese labeled data,   .  
 MPQA NTCIR-EN NTCIR-CH 
Positive 1,471 (30%) 528 (30%) 2,378 (55%) 
Negative 3,487 (70%) 1,209 (70%) 1,916 (45%) 
Total 4,958 1,737 4,294 
Table 1: Sentence Counts for the Labeled Data 
Unlabeled Parallel Text and its Preprocessing. 
For the unlabeled parallel text, we use the ISI 
Chinese-English parallel corpus (Munteanu and 
Marcu, 2005), which was extracted automatically 
from news articles published by Xinhua News 
Agency in the Chinese Gigaword (2nd Edition) and 
English Gigaword (2nd Edition) collections. 
Because sentence pairs in the ISI corpus are quite 
noisy, we rely on Giza++ (Och and Ney, 2003) to 
obtain a new translation probability for each 
sentence pair, and select the 100,000 pairs with the 
highest translation probabilities.5  
We also try to remove neutral sentences from 
the parallel data since they can introduce noise into 
our model, which deals only with positive and 
negative examples. To do this, we train a single 
classifier from the combined Chinese and English 
labeled data for each data setting above by 
concatenating the original English and Chinese 
feature sets. We then classify each unlabeled 
sentence pair by combining the two sentences in 
each pair into one. We choose the most confidently 
predicted 10,000 positive and 10,000 negative 
pairs to constitute the unlabeled parallel corpus   
for each data setting. 
                                                          
5We removed sentence pairs with an original confidence score 
(given in the corpus) smaller than 0.98, and also removed the 
pairs that are too long (more than 60 characters in one 
sentence) to facilitate Giza++. We first obtain translation 
probabilities for both directions (i.e. Chinese to English and 
English to Chinese) with Giza++, take the log of the product 
of those two probabilities, and then divide it by the sum of 
lengths of the two sentences in each pair.  
324
  
4.2 Baseline Methods 
In our experiments, the proposed joint model is 
compared with the following baseline methods. 
MaxEnt: This method learns a MaxEnt 
classifier for each language given the monolingual 
labeled data; the unlabeled data is not used.  
SVM: This method learns an SVM classifier for 
each language given the monolingual labeled data; 
the unlabeled data is not used. SVM-light 
(Joachims, 1999a) is used for all the SVM-related 
experiments. 
Monolingual TSVM (TSVM-M): This method 
learns two transductive SVM (TSVM) classifiers 
given the monolingual labeled data and the 
monolingual unlabeled data for each language.  
Bilingual TSVM (TSVM-B): This method 
learns one TSVM classifier given the labeled 
training data in two languages together with the 
unlabeled sentences by combining the two 
sentences in each unlabeled pair into one. We 
expect this method to perform better than TSVM-
M since the combined (bilingual) unlabeled 
sentences could be more helpful than the unlabeled 
monolingual sentences. 
Co-Training with SVMs (Co-SVM): This 
method applies SVM-based co-training given both 
the labeled training data and the unlabeled parallel 
data following Wan (2009). First, two monolingual 
SVM classifiers are built based on only the 
corresponding labeled data, and then they are 
bootstrapped by adding the most confident 
predicted examples from the unlabeled data into 
the training set. We run bootstrapping for 100 
iterations. In each iteration, we select the most 
confidently predicted 50 positive and 50 negative 
sentences from each of the two classifiers, and take 
the union of the resulting 200 sentence pairs as the 
newly labeled training data. (Examples with 
conflicting labels within the pair are not included.) 
5 Results and Analysis 
In our experiments, the methods are tested in the 
two data settings with the corresponding unlabeled 
parallel corpus as mentioned in Section 4.6 We use 
                                                          
6 The results reported in this section employ Equation 4. 
Preliminary experiments showed that Equation 5 does not 
significantly improve the performance in our case, which is 
reasonable since we choose only sentence pairs with the 
highest translation probabilities to be our unlabeled data (see 
Section 4.1).      
5-fold cross-validation and report average accuracy 
(also MicroF1 in this case) and MacroF1 scores. 
Unigrams are used as binary features for all 
models, as Pang et al (2002) showed that binary 
features perform better than frequency features for 
sentiment classification. The weights for unlabeled 
data and regularization,    and   , are set to 1 
unless otherwise stated. Later, we will show that 
the proposed approach performs well with a wide 
range of parameter values.7 
5.1 Method Comparison 
We first compare the proposed joint model (Joint) 
with the baselines in Table 2. As seen from the 
table, the proposed approach outperforms all five 
baseline methods in terms of both accuracy and 
MacroF1 for both English and Chinese and in both 
of the data settings. 8  By making use of the 
unlabeled parallel data, our proposed approach 
improves the accuracy, compared to MaxEnt, by 
8.12% (or 33.27% error reduction) on English and 
3.44% (or 16.92% error reduction) on Chinese in 
the first setting, and by 5.07% (or 19.67% error 
reduction) on English and 3.87% (or 19.4% error 
reduction) on Chinese in the second setting. 
 Among the baselines, the best is Co-SVM; 
TSVMs do not always improve performance using 
the unlabeled data compared to the standalone 
SVM; and TSVM-B outperforms TSVM-M except 
for Chinese in the second setting. The MPQA data 
is more difficult in general compared to the NTCIR 
data. Without unlabeled parallel data, the 
performance on the Chinese data is better than on 
the English data, which is consistent with results 
reported in NTCIR-6 (Seki et al, 2007).  
Overall, the unlabeled parallel data improves 
classification accuracy for both languages when 
using our proposed joint model and Co-SVM. The 
joint model makes better use of the unlabeled 
parallel data than Co-SVM or TSVMs presumably 
because of its attempt to jointly optimize the two 
monolingual models via soft (probabilistic) 
assignments of the unlabeled instances to classes in 
each iteration, instead of the hard assignments in 
Co-SVM and TSVMs. Although English sentiment 
                                                          
7The code is at http://sites.google.com/site/lubin2010. 
8 Significance is tested using paired t-tests with  <0.05: ? 
denotes statistical significance compared to the corresponding 
performance of MaxEnt; * denotes statistical significance 
compared to SVM; and 
?
 denotes statistical significance 
compared to Co-SVM. 
325
  
classification alone is more difficult than Chinese 
for our datasets, we obtain greater performance 
gains for English by exploiting unlabeled parallel 
data as well as the Chinese labeled data.  
5.2 Varying the Weight and Amount of 
Unlabeled Data 
Figure 1 shows the accuracy curve of the proposed 
approach for the two data settings when varying 
the weight for the unlabeled data,   , from 0 to 1. 
When    is set to 0, the joint model degenerates to 
two MaxEnt models trained with only the labeled 
data.  
We can see that the performance gains for the 
proposed approach are quite remarkable even when 
   is set to 0.1; performance is largely stable after 
   reaches 0.4. Although MPQA is more difficult 
in general compared to the NTCIR data, we still 
see steady improvements in performance with 
unlabeled parallel data. Overall, the proposed 
approach performs quite well for a wide range of 
parameter values of   .  
Figure 2 shows the accuracy curve of the 
proposed approach for the two data settings when 
varying the amount of unlabeled data from 0 to 
20,000 instances. We see that the performance of 
the proposed approach improves steadily by adding 
more and more unlabeled data. However, even 
with only 2,000 unlabeled sentence pairs, the 
proposed approach still produces large 
performance gains.  
5.3 Results on Pseudo-Parallel Unlabeled 
Data 
As discussed in Section 3.4, we generate pseudo-
parallel data by translating the monolingual 
sentences in each setting using Google?s machine 
translation system. Figures 3 and 4 show the 
performance of our model using the pseudo-
parallel data versus the real parallel data, in the two 
settings, respectively. The EN->CH pseudo-
parallel data consists of the English unlabeled data 
and its automatic Chinese translation, and vice 
versa. 
Although not as significant as those with parallel 
data, we can still obtain improvements using the 
pseudo-parallel data, especially in the first setting. 
The difference between using parallel versus 
pseudo-parallel data is around 2-4% in Figures 3 
and 4, which is reasonable since the quality of the 
pseudo-parallel data is not as good as that of the 
parallel data. Therefore, the performance using 
pseudo-parallel data is better with a small weight 
(e.g.   = 0.1) in some cases.  
 
Setting 1: NTCIR-EN+NTCIR-CH Setting 2: MPQA+NTCIR-CH 
Accuracy MacroF1 Accuracy MacroF1 
English Chinese English Chinese English Chinese English Chinese 
MaxEnt 75.59 79.67 66.61* 79.34 74.22 79.67 65.09* 79.34 
SVM 76.34 81.02 61.12 80.75? 76.74? 81.02 61.35 80.75? 
TSVM-M 73.46 80.21 55.33 79.99 72.89 81.14 52.82 79.99 
TSVM-B 78.36 81.60? 65.53 81.42 76.42? 78.51 61.66 78.32 
Co-SVM 82.44?* 82.79? 72.61?* 82.67?* 78.18?* 82.63?* 68.03?* 82.51?* 
Joint 83.71?* 83.11?* 75.89?*? 82.97?* 79.29?*? 83.54?* 72.58?*? 83.37?* 
Table 2: Comparison of Results 
       
Figure 1. Accuracy vs. Weight of Unlabeled Data                Figure 2. Accuracy vs. Amount of Unlabeled Data 
 
0 0.2 0.4 0.6 0.8 1
72
74
76
78
80
82
84
86
Weight of Unlabeled Data
A
c
c
u
r
a
c
y
(
%
)
 
 
English on NTCIR-EN+NTCIR-CH
Chinese on NTCIR-EN+NTCIR-CH
English  on MPQA+NTCIR-CH
Chinese on MPQA+NTCIR-CH
0 0.5 1 1.5 2
72
74
76
78
80
82
84
86
Size of Unlabeled Data
A
c
c
u
r
a
c
y
 
(
%
)
 
 
English on NTCIR-EN+NTCIR-CH
Chinese on NTCIR-EN+NTCIR-CH
English  on MPQA+NTCIR-CH
Chinese on MPQA+NTCIR-CH
326
  
5.4 Adding Pseudo-Parallel Labeled Data 
In this section, we investigate how adding 
automatically translated labeled data might 
influence the performance as mentioned in Section 
3.4. We use only the translated labeled data to train 
classifiers, and then directly classify the test data. 
The average accuracies in setting 1 are 66.61% and 
63.11% on English and Chinese, respectively; 
while the accuracies in setting 2 are 58.43% and 
54.07% on English and Chinese, respectively. This 
result is reasonable because of the language gap 
between the original language and the translated 
language. In addition, the class distributions of the 
English labeled data and the Chinese are quite 
different (30% vs. 55% for positive as shown in 
Table 1).  
Figures 5 and 6 show the accuracies when 
varying the weight of the translated labeled data vs. 
the labeled data, with and without the unlabeled 
parallel data. From Figure 5 for setting 1, we can 
see that the translated data can be helpful given the 
labeled data and even the unlabeled data, as long as 
   is small; while in Figure 6, the translated data 
decreases the performance in most cases for setting 
2. One possible reason is that in the first data 
setting, the NTCIR English data covers the same 
topics as the NTCIR Chinese data and thus direct 
translation is helpful, while the English and 
Chinese topics are quite different in the second 
data setting, and thus direct translation hurts the 
performance given the existing labeled data in each 
language. 
5.5 Discussion 
To further understand what contributions our 
proposed approach makes to the performance gain, 
we look inside the parameters in the MaxEnt 
models learned before and after adding the parallel 
unlabeled data. Table 3 shows the features in the 
model learned from the labeled data that have the 
largest weight change after adding the parallel data;  
     
Figure 3. Accuracy with Pseudo-Parallel Unlabeled           Figure 4. Accuracy with Pseudo-Parallel Unlabeled 
 Data in Setting 1                                                         Data in Setting 2 
        
Figure 5. Accuracy with Pseudo-Parallel Labeled              Figure 6. Accuracy with Pseudo-Parallel Labeled  
Data in Setting 1                                                      Data in Setting 2 
 
 
0 0.2 0.4 0.6 0.8 1
74
76
78
80
82
84
86
Weight of Unlabeled Data
A
c
c
u
r
a
c
y
(
%
)
 
 
English on Parallel Data
Chinese on Parallel Data
English on EN->CH Pseudo-Parallel Data
Chinese on EN->CH Pseudo-Parallel Data
English on CH->EN Pseudo-Parallel Data
Chinese on CH->EN Pseudo-Parallel Data
0 0.2 0.4 0.6 0.8 1
65
70
75
80
85
Weight of Unlabeled Data
A
c
c
u
r
a
c
y
(
%
)
 
 
English on Parallel Data
Chinese on Parallel Data
English on EN->CH Pseudo-Parallel Data
Chinese on EN->CH Pseudo-Parallel Data
English on CH->EN Pseudo-Parallel Data
Chinese on CH->EN Pseudo-Parallel Data
0 0.2 0.4 0.6 0.8 1
70
72
74
76
78
80
82
84
6
Weight of Translated Labeled Data
A
c
c
u
r
a
c
y
(
%
)
 
 
English w/o Unlabeled Data
Chinese w/o Unlabeled Data
English with Unlabeled Data
Chinese with Unlabeled Data
0 0.2 0.4 0.6 0.8 1
68
70
72
74
76
78
8
82
84
86
Weight of Translated Labeled Data
A
c
c
u
r
a
c
y
(
%
)
 
 
English w/o Unlabeled Data
Chinese w/o Unlabeled Data
English with Unlabeled Data
Chinese with Unlabeled Data
327
  
Positive Negative 
Word Weight Word Weight 
friendly 0.701 german 0.783 
principles 0.684 arduous 0.531 
hopes 0.630 oppose 0.511 
hoped 0.553 administrations 0.431 
cooperative 0.552 oau9 0.408 
Table 4. New Features Learned from Unlabeled Data 
and Table 4 shows the newly learned features from 
the unlabeled data with the largest weights. 
From Table 3 10  we can see that the weight 
changes of the original features are quite 
reasonable, e.g. the top words in the positive class 
are obviously positive and the proposed approach 
gives them higher weights. The new features also 
seem reasonable given the knowledge that the 
labeled and unlabeled data includes negative news 
about for specific topics (e.g. Germany, Taiwan),. 
We also examine the process of joint training by 
checking the performance on test data and the 
agreement of the two monolingual models on the 
unlabeled parallel data in both settings. The 
average agreement across 5 folds is 85.06% and 
73.87% in settings 1 and 2, respectively, before the 
joint training, and increases to 100% and 99.89%, 
respectively, after 100 iterations of joint training. 
Although the average agreement has already 
increased to 99.50% and 99.02% in settings 1 and 
2, respectively, after 30 iterations, the performance 
on the test set steadily improves in both settings 
until around 50-60 iterations, and then becomes 
relatively stable after that. 
Examination of those sentence pairs in setting 2 
for which the two monolingual models still 
                                                          
9
This is an abbreviation for the Organization of African Unity. 
10The features and weights in Tables 3 and 4 are extracted 
from the English model in the first fold of setting 1. 
disagree after 100 iterations of joint training often 
produces sentences that are not quite parallel, e.g.: 
English: The two sides attach great importance to 
international cooperation on protection and promotion of 
human rights. 
Chinese: ????,????????????????,???
??????????????(Both sides agree that double 
standards on the issue of human rights are to be avoided, and 
are opposed to using pressure on human rights issues in 
international relations.) 
Since the two sentences discuss human rights 
from very different perspectives, it is reasonable 
that the two monolingual models will classify them 
with different polarities (i.e. positive for the 
English sentence and negative for the Chinese 
sentence) even after joint training.  
6 Conclusion 
In this paper, we study bilingual sentiment 
classification and propose a joint model to 
simultaneously learn better monolingual sentiment 
classifiers for each language by exploiting an 
unlabeled parallel corpus together with the labeled 
data available for each language. Our experiments 
show that the proposed approach can significantly 
improve sentiment classification for both 
languages. Moreover, the proposed approach 
continues to produce (albeit smaller) performance 
gains when employing pseudo-parallel data from 
machine translation engines. 
In future work, we would like to apply the joint 
learning idea to other learning frameworks (e.g. 
SVMs), and to extend the proposed model to 
handle word-level parallel information, e.g. 
bilingual dictionaries or word alignment 
information. Another issue is to investigate how to 
improve multilingual sentiment analysis by 
exploiting comparable corpora. 
Acknowledgments 
We thank Shuo Chen, Long Jiang, Thorsten 
Joachims, Lillian Lee, Myle Ott, Yan Song, 
Xiaojun Wan, Ainur Yessenalina, Jingbo Zhu and 
the anonymous reviewers for many useful 
comments and discussion. This work was 
supported in part by National Science Foundation 
Grants BCS-0904822, BCS-0624277, IIS-
0968450; and by a gift from Google. Chenhao Tan 
is supported by NSF (DMS-0808864), ONR (YIP-
N000140910911), and a grant from Microsoft.  
  
 Word 
Weight 
Before After Change 
Positive 
important 0.452 1.659 1.207 
cooperation 0.325 1.492 1.167 
support 0.533 1.483 0.950 
importance 0.450 1.193 0.742 
agreed 0.347 1.061 0.714 
Negative 
difficulties 0.018 0.663 0.645 
not 0.202 0.844 0.641 
never 0.245 0.879 0.634 
germany 0.035 0.664 0.629 
taiwan 0.590 1.216 0.626 
Table 3. Original Features with Largest Weight Change 
328
  
References 
Massih-Reza Amini, Cyril Goutte, and Nicolas Usunier. 
2010. Combining coregularization and consensus-
based self-training for multilingual text 
categorization. In Proceeding of SIGIR?10. 
Carmen Banea, Rada Mihalcea, and Janyce Wiebe. 
2010. Multilingual subjectivity: Are more languages 
better? In Proceedings of COLING?10. 
Carmen Banea, Rada Mihalcea, Janyce Wiebe, and 
Samer Hassan. 2008. Multilingual subjectivity 
analysis using machine translation. In Proceedings of 
EMNLP?08. 
Adam L. Berger, Stephen A. Della Pietra and Vincent J. 
Della Pietra. 1996. A maximum entropy approach to 
natural language processing. Computational 
Linguistics, 22(1). 
John Blitzer, Ryan McDonald, and Fernando Pereira. 
2006. Domain adaptation with structural correspond-
dence learning. In Proceedings of EMNLP?06. 
Avrim Blum and Tom Mitchell. 1998. Combining 
labeled and unlabeled data with co-training. In 
Proceedings of COLT?98. 
Jordan Boyd-Graber and Philip Resnik. 2010. Holistic 
sentiment analysis across languages: Multilingual 
supervised Latent Dirichlet Allocation. In 
Proceedings of EMNLP?10. 
Eric Breck, Yejin Choi, and Claire Cardie. 2007. 
Identifying expressions of opinion in context. In 
Proceedings of IJCAI?07.  
David Burkett, Slav Petrov, John Blitzer, and Dan 
Klein. 2010. Learning better monolingual models 
with unannotated bilingual text. In Proceedings of 
CoNLL?10. 
David Burkett and Dan Klein. 2008. Two languages are 
better than one (for syntactic parsing). In 
Proceedings of EMNLP?08. 
Yejin Choi and Claire Cardie. 2008. Learning with 
compositional semantics as structural inference for 
subsentential sentiment analysis. In Proceedings of 
EMNLP?08. 
Wei Gao, John Blitzer, Ming Zhou, and Kam-Fai Wong. 
2009. Exploiting bilingual information to improve 
web search. In Proceedings of ACL/IJCNLP?09. 
Minqing Hu and Bing Liu. 2004. Mining opinion 
features in customer reviews. In Proceedings of 
AAAI?04. 
Ido Dagan, and Alon Itai. 1994. Word sense 
disambiguation using a second language monolingual 
corpus, Computational Linguistics, 20(4): 563-596. 
Thorsten Joachims. 1999a. Making Large-Scale SVM 
Learning Practical. In: Advances in Kernel Methods - 
Support Vector Learning, B. Sch?lkopf, C. Burges, 
and A. Smola (ed.), MIT Press. 
Thorsten Joachims. 1999b. Transductive inference for 
text classification using support vector machines. In 
Proceedings of ICML?99. 
Percy Liang, Ben Taskar, and Dan Klein. 2006. 
Alignment by agreement. In Proceedings of 
NAACL?06. 
Dong C. Liu and  Jorge Nocedal. 1989. On the limited 
memory BFGS method for large scale optimization. 
Mathematical Programming, (45): 503?528. 
Robert Malouf. 2002. A comparison of algorithms for 
maximum entropy parameter estimation. In 
Proceedings of CoNLL?02. 
Rada Mihalcea, Carmen Banea, and Janyce Wiebe. 
2007. Learning multilingual subjective language via 
cross-lingual projections. In Proceedings of ACL?07. 
Dragos S. Munteanu and Daniel Marcu. 2005. 
Improving machine translation performance by 
exploiting non-parallel corpora. Computational 
Linguistics, 31(4): 477?504. 
Tetsuji Nakagawa, Kentaro Inui, and Sadao Kurohashi. 
2010. Dependency tree-based sentiment classification 
using CRFs with hidden variables. In Proceedings of 
NAACL/HLT ?10. 
Kamal Nigam, Andrew K. Mccallum, Sebastian Thrun, 
and Tom Mitchell. 2000. Text classification from 
labeled and unlabeled documents using EM. Machine 
Learning, 39(2): 103?134. 
Franz J. Och and Hermann Ney. 2003. A systematic 
comparison of various statistical alignment models. 
Computational Linguistics, 29(1): 19-51. 
Bo Pang and Lillian Lee. 2008. Opinion mining and 
sentiment analysis, Foundations and Trends in 
Information Retrieval, Now Publishers. 
Bo Pang, Lillian Lee, and Shivakumar Vaithyanathan. 
2002. Thumbs up? Sentiment classification using 
machine learning techniques. In Proceedings of 
EMNLP?02. 
Peter  Prettenhofer and  Benno Stein. 2010. Cross-
language text classification using structural 
correspondence learning. In Proceedings of ACL?10. 
Adwait Ratnaparkhi. 1997. A simple introduction to 
maximum entropy models for natural language 
processing. Technical Report 97-08, University of 
Pennsylvania. 
329
  
Julia M. Schulz, Christa Womser-Hacker, and Thomas 
Mandl. 2010. Multilingual corpus development for 
opinion mining. In Proceedings of LREC?10. 
Yohei Seki, David Kirk Evans, Lun-Wei Ku, Le Sun, 
Hsin-His Chen, and Noriko Kando. 2008. Overview 
of multilingual opinion analysis task at NTCIR-7. In 
Proceedings of the NTCIR-7 Workshop.  
Yohei Seki, David K. Evans, Lun-Wei Ku, Le Sun, 
Hsin-His Chen, Noriko Kando, and Chin-Yew Lin. 
2007. Overview of opinion analysis pilot task at 
NTCIR-6. In Proceedings of the NTCIR-6 Workshop. 
Vikas Sindhwani, Partha Niyogi, and Mikhail Belkin. 
2005. A co-regularization approach to semi-
supervised learning with multiple views. In 
Proceedings of ICML?05. 
Noah A. Smith. 2006. Novel estimation methods for 
unsupervised discovery of latent structure in natural 
language text. Ph.D. thesis, Department of Computer 
Science, Johns Hopkins University. 
Huihsin Tseng, Pichuan Chang, Galen Andrew, Daniel 
Jurafsky and Christopher Manning. 2005. A 
conditional random field word segmenter. In 
Proeedings of the 4th SIGHAN Workshop. 
Peter D. Turney. 2002. Thumbs up or thumbs down? 
Semantic orientation applied to unsupervised 
classification of reviews, In Proceedings of ACL?02. 
Xiaojun Wan. 2008. Using Bilingual Knowledge and 
Ensemble Techniques for Unsupervised Chinese 
Sentiment Analysis. In Proceedings of  EMNLP?08. 
Xiaojun Wan. 2009. Co-training for cross-lingual 
sentiment classification. In Proceedings of 
ACL/AFNLP?09. 
Janyce Wiebe, Theresa Wilson, and Claire Cardie. 
2005. Annotating expressions of opinions and 
emotions in language. Language Resources and 
Evaluation, 39(2- 3): 165-210. 
Duo Zhang, Qiaozhu Mei, and ChengXiang Zhai. 2010. 
Cross-lingual latent topic extraction, In Proceedings 
of ACL?10. 
Hai Zhao, Yan Song, Chunyu Kit, and Guodong Zhou. 
2009. Cross language dependency parsing using a 
bilingual lexicon. In Proceedings of 
ACL/IJCNLP?09.  
Xiaojin Zhu and Andrew B. Goldberg. 2009. 
Introduction to Semi-Supervised Learning. Morgan 
& Claypool Publishers. 
Appendix A. Equation Deduction 
In this appendix, we derive the gradient for the objective 
function in Equation 3, which is used in parameter 
estimation. As mentioned in Section 3.3, the parameters 
can be learned by finding: 
   
    
         
      
                 
       
      
                    
       
      
                           
         
     
     
     
         
 
        
Since the first term on the right-hand side is just the 
expression for the standard MaxEnt problem, we will 
focus on the gradient for the second term, and denote 
       
     
     
     
          as ( ). 
Let         denote    or   , and   
  be the  th weight 
in the vector   . For brevity, we drop the   in the above 
notation, and write   
  to denote   
  . Then the partial 
derivative of (*) based on Equation 4 with respect to   
  
is as follows: 
    
   
  
     
    
     
 
   
     
    
       
 
     
    
         
    
       
 
                              (1) 
Further, we obtain: 
 
   
     
    
      
 
   
 
             
   
   
              
   
   
  
 
                       
 
             
   
   
              
   
   
  
 
  
    
    
     
 
             
   
   
                      
   
   
  
  
               
    
     
    
    
       
     
    
        
    
    
        
    
       
    
    
       (2) 
Merge (2) into (1), we get: 
    
   
  
 
     
    
         
    
       
 
      
    
         
    
          
   
    
    
        
    
       
    
    
         
      
    
         
    
       
    
    
        
     
    
       
    
    
             
    
    
    
       
    
         
    
          
    
           
  
330
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 1384?1394,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
A Sentence Compression Based Framework to Query-Focused
Multi-Document Summarization
Lu Wang1 Hema Raghavan2 Vittorio Castelli2 Radu Florian2 Claire Cardie1
1Department of Computer Science, Cornell University, Ithaca, NY 14853, USA
{luwang, cardie}@cs.cornell.edu
2IBM T. J. Watson Research Center, Yorktown Heights, NY 10598, USA
{hraghav, vittorio, raduf}@us.ibm.com
Abstract
We consider the problem of using sentence
compression techniques to facilitate query-
focused multi-document summarization. We
present a sentence-compression-based frame-
work for the task, and design a series of
learning-based compression models built on
parse trees. An innovative beam search de-
coder is proposed to efficiently find highly
probable compressions. Under this frame-
work, we show how to integrate various in-
dicative metrics such as linguistic motivation
and query relevance into the compression pro-
cess by deriving a novel formulation of a com-
pression scoring function. Our best model
achieves statistically significant improvement
over the state-of-the-art systems on several
metrics (e.g. 8.0% and 5.4% improvements in
ROUGE-2 respectively) for the DUC 2006 and
2007 summarization task.
1 Introduction
The explosion of the Internet clearly warrants
the development of techniques for organizing and
presenting information to users in an effective
way. Query-focused multi-document summariza-
tion (MDS) methods have been proposed as one
such technique and have attracted significant at-
tention in recent years. The goal of query-focused
MDS is to synthesize a brief (often fixed-length)
and well-organized summary from a set of topic-
related documents that answer a complex ques-
tion or address a topic statement. The result-
ing summaries, in turn, can support a number of
information analysis applications including open-
ended question answering, recommender systems,
and summarization of search engine results. As
further evidence of its importance, the Document
Understanding Conference (DUC) has used query-
focused MDS as its main task since 2004 to foster
new research on automatic summarization in the
context of users? needs.
To date, most top-performing systems for
multi-document summarization?whether query-
specific or not?remain largely extractive: their
summaries are comprised exclusively of sen-
tences selected directly from the documents
to be summarized (Erkan and Radev, 2004;
Haghighi and Vanderwende, 2009; Celikyilmaz
and Hakkani-Tu?r, 2011). Despite their simplicity,
extractive approaches have some disadvantages.
First, lengthy sentences that are partly relevant
are either excluded from the summary or (if se-
lected) can block the selection of other important
sentences, due to summary length constraints.
In addition, when people write summaries, they
tend to abstract the content and seldom use
entire sentences taken verbatim from the original
documents. In news articles, for example, most
sentences are lengthy and contain both potentially
useful information for a summary as well as un-
necessary details that are better omitted. Consider
the following DUC query as input for a MDS
system:1 ?In what ways have stolen artworks
been recovered? How often are suspects arrested
or prosecuted for the thefts?? One manually gen-
erated summary includes the following sentence
but removes the bracketed words in gray:
A man suspected of stealing a million-dollar collection
of [hundreds of ancient] Nepalese and Tibetan art objects in
New York [11 years ago] was arrested [Thursday at his South
Los Angeles home, where he had been hiding the antiquities,
police said].
In this example, the compressed sentence is rela-
1From DUC 2005, query for topic d422g.
1384
tively more succinct and readable than the origi-
nal (e.g. in terms of Flesch-Kincaid Reading Ease
Score (Kincaid et al, 1975)). Likewise, removing
information irrelevant to the query (e.g. ?11 years
ago?, ?police said?) is crucial for query-focused
MDS.
Sentence compression techniques (Knight and
Marcu, 2000; Clarke and Lapata, 2008) are the
standard for producing a compact and grammat-
ical version of a sentence while preserving rel-
evance, and prior research (e.g. Lin (2003)) has
demonstrated their potential usefulness for generic
document summarization. Similarly, strides have
been made to incorporate sentence compression
into query-focused MDS systems (Zajic et al,
2006). Most attempts, however, fail to produce
better results than those of the best systems built
on pure extraction-based approaches that use no
sentence compression.
In this paper we investigate the role of sentence
compression techniques for query-focused MDS.
We extend existing work in the area first by inves-
tigating the role of learning-based sentence com-
pression techniques. In addition, we design three
types of approaches to sentence-compression?
rule-based, sequence-based and tree-based?and
examine them within our compression-based
framework for query-specific MDS. Our top-
performing sentence compression algorithm in-
corporates measures of query relevance, con-
tent importance, redundancy and language qual-
ity, among others. Our tree-based methods rely on
a scoring function that allows for easy and flexi-
ble tailoring of sentence compression to the sum-
marization task, ultimately resulting in significant
improvements for MDS, while at the same time
remaining competitive with existing methods in
terms of sentence compression, as discussed next.
We evaluate the summarization models on
the standard Document Understanding Confer-
ence (DUC) 2006 and 2007 corpora 2 for query-
focused MDS and find that all of our compression-
based summarization models achieve statistically
significantly better performance than the best
DUC 2006 systems. Our best-performing sys-
tem yields an 11.02 ROUGE-2 score (Lin and
Hovy, 2003), a 8.0% improvement over the best
reported score (10.2 (Davis et al, 2012)) on the
2We believe that we can easily adapt our system for tasks
(e.g. TAC-08?s opinion summarization or TAC-09?s update
summarization) or domains (e.g. web pages or wikipedia
pages). We reserve that for future work.
DUC 2006 dataset, and an 13.49 ROUGE-2, a
5.4% improvement over the best score in DUC
2007 (12.8 (Davis et al, 2012)). We also ob-
serve substantial improvements over previous sys-
tems w.r.t. the manual Pyramid (Nenkova and
Passonneau, 2004) evaluation measure (26.4 vs.
22.9 (Jagarlamudi et al, 2006)); human annota-
tors furthermore rate our system-generated sum-
maries as having less redundancy and compara-
ble quality w.r.t. other linguistic quality metrics.
With these results we believe we are the first
to successfully show that sentence compression
can provide statistically significant improvements
over pure extraction-based approaches for query-
focused MDS.
2 Related Work
Existing research on query-focused multi-
document summarization (MDS) largely relies
on extractive approaches, where systems usually
take as input a set of documents and select
the top relevant sentences for inclusion in the
final summary. A wide range of methods have
been employed for this task. For unsupervised
methods, sentence importance can be estimated
by calculating topic signature words (Lin and
Hovy, 2000; Conroy et al, 2006), combining
query similarity and document centrality within
a graph-based model (Otterbacher et al, 2005),
or using a Bayesian model with sophisticated
inference (Daume? and Marcu, 2006). Davis et
al. (2012) first learn the term weights by Latent
Semantic Analysis, and then greedily select
sentences that cover the maximum combined
weights. Supervised approaches have mainly
focused on applying discriminative learning for
ranking sentences (Fuentes et al, 2007). Lin and
Bilmes (2011) use a class of carefully designed
submodular functions to reward the diversity of
the summaries and select sentences greedily.
Our work is more related to the less studied
area of sentence compression as applied to (sin-
gle) document summarization. Zajic et al (2006)
tackle the query-focused MDS problem using a
compress-first strategy: they develop heuristics to
generate multiple alternative compressions of all
sentences in the original document; these then be-
come the candidates for extraction. This approach,
however, does not outperform some extraction-
based approaches. A similar idea has been stud-
ied for MDS (Lin, 2003; Gillick and Favre, 2009),
1385
but limited improvement is observed over extrac-
tive baselines with simple compression rules. Fi-
nally, although learning-based compression meth-
ods are promising (Martins and Smith, 2009;
Berg-Kirkpatrick et al, 2011), it is unclear how
well they handle issues of redundancy.
Our research is also inspired by probabilis-
tic sentence-compression approaches, such as the
noisy-channel model (Knight and Marcu, 2000;
Turner and Charniak, 2005), and its extension via
synchronous context-free grammars (SCFG) (Aho
and Ullman, 1969; Lewis and Stearns, 1968) for
robust probability estimation (Galley and McKe-
own, 2007). Rather than attempt to derive a new
parse tree like Knight and Marcu (2000) and Gal-
ley and McKeown (2007), we learn to safely re-
move a set of constituents in our parse tree-based
compression model while preserving grammati-
cal structure and essential content. Sentence-level
compression has also been examined via a dis-
criminative model McDonald (2006), and Clarke
and Lapata (2008) also incorporate discourse in-
formation by using integer linear programming.
3 The Framework
We now present our query-focused MDS frame-
work consisting of three steps: Sentence Rank-
ing, Sentence Compression and Post-processing.
First, sentence ranking determines the importance
of each sentence given the query. Then, a sen-
tence compressor iteratively generates the most
likely succinct versions of the ranked sentences,
which are cumulatively added to the summary, un-
til a length limit is reached. Finally, the post-
processing stage applies coreference resolution
and sentence reordering to build the summary.
Sentence Ranking. This stage aims to rank sen-
tences in order of relevance to the query. Un-
surprisingly, ranking algorithms have been suc-
cessfully applied to this task. We experimented
with two of them ? Support Vector Regres-
sion (SVR) (Mozer et al, 1997) and Lamb-
daMART (Burges et al, 2007). The former
has been used previously for MDS (Ouyang et
al., 2011). LambdaMart on the other hand has
shown considerable success in information re-
trieval tasks (Burges, 2010); we are the first to
apply it to summarization. For training, we use
40 topics (i.e. queries) from the DUC 2005 cor-
pus (Dang, 2005) along with their manually gener-
ated abstracts. As in previous work (Shen and Li,
Basic Features
relative/absolute position
is among the first 1/3/5 sentences?
number of words (with/without stopwords)
number of words more than 5/10 (with/without stopwords)
Query-Relevant Features
unigram/bigram/skip bigram (at most four words apart) overlap
unigram/bigram TF/TF-IDF similarity
mention overlap
subject/object/indirect object overlap
semantic role overlap
relation overlap
Query-Independent Features
average/total unigram/bigram IDF/TF-IDF
unigram/bigram TF/TF-IDF similarity with the centroid of the cluster
average/sum of sumBasic/SumFocus (Toutanova et al, 2007)
average/sum of mutual information
average/sum of number of topic signature words (Lin and Hovy, 2000)
basic/improved sentence scorers from Conroy et al (2006)
Content Features
contains verb/web link/phone number?
contains/portion of words between parentheses
Table 1: Sentence-level features for sentence ranking.
2011; Ouyang et al, 2011), we use the ROUGE-
2 score, which measures bigram overlap between
a sentence and the abstracts, as the objective for
regression.
While space limitations preclude a longer dis-
cussion of the full feature set (ref. Table 1), we
describe next the query-relevant features used for
sentence ranking as these are the most impor-
tant for our summarization setting. The goal of
this feature subset is to determine the similarity
between the query and each candidate sentence.
When computing similarity, we remove stopwords
as well as the words ?discuss, describe, specify,
explain, identify, include, involve, note? that are
adopted and extended from Conroy et al (2006).
Then we conduct simple query expansion based
on the title of the topic and cross-document coref-
erence resolution. Specifically, we first add the
words from the topic title to the query. And for
each mention in the query, we add other mentions
within the set of documents that corefer with this
mention. Finally, we compute two versions of the
features?one based on the original query and an-
other on the expanded one. We also derive the
semantic role overlap and relation instance over-
lap between the query and each sentence. Cross-
document coreference resolution, semantic role la-
beling and relation extraction are accomplished
via the methods described in Section 5.
Sentence Compression. As the main focus of
this paper, we propose three types of compression
methods, described in detail in Section 4 below.
Post-processing. Post-processing performs
coreference resolution and sentence ordering.
1386
Basic Features Syntactic Tree Features
first 1/3/5 tokens (toks)? POS tag
last 1/3/5 toks? parent/grandparent label
first letter/all letters capitalized? leftmost child of parent?
is negation? second leftmost child of parent?
is stopword? is headword?
Dependency Tree Features in NP/VP/ADVP/ADJP chunk?
dependency relation (dep rel) Semantic Features
parent/grandparent dep rel is a predicate?
is the root? semantic role label
has a depth larger than 3/5?
Rule-Based Features
For each rule in Table 2 , we construct a corresponding feature to
indicate whether the token is identified by the rule.
Table 3: Token-level features for sequence-based com-
pression.
We replace each pronoun with its referent unless
they appear in the same sentence. For sentence
ordering, each compressed sentence is assigned
to the most similar (tf-idf) query sentence. Then
a Chronological Ordering algorithm (Barzilay et
al., 2002) sorts the sentences for each query based
first on the time stamp, and then the position in
the source document.
4 Sentence Compression
Sentence compression is typically formulated as
the problem of removing secondary information
from a sentence while maintaining its grammati-
cality and semantic structure (Knight and Marcu,
2000; McDonald, 2006; Galley and McKeown,
2007; Clarke and Lapata, 2008). We leave other
rewrite operations, such as paraphrasing and re-
ordering, for future work. Below we describe
the sentence compression approaches developed
in this research: RULE-BASED COMPRESSION,
SEQUENCE-BASED COMPRESSION, and TREE-
BASED COMPRESSION.
4.1 Rule-based Compression
Turner and Charniak (2005) have shown that ap-
plying hand-crafted rules for trimming sentences
can improve both content and linguistic qual-
ity. Our rule-based approach extends existing
work (Conroy et al, 2006; Toutanova et al, 2007)
to create the linguistically-motivated compression
rules of Table 2. To avoid ill-formed output, we
disallow compressions of more than 10 words by
each rule.
4.2 Sequence-based Compression
As in McDonald (2006) and Clarke and Lapata
(2008), our sequence-based compression model
makes a binary ?keep-or-delete? decision for each
word in the sentence. In contrast, however, we
Figure 1: Diagram of tree-based compression. The
nodes to be dropped are grayed out. In this example,
the root of the gray subtree (a ?PP?) would be labeled
REMOVE. Its siblings and parent are labeled RETAIN
and PARTIAL, respectively. The trimmed tree is real-
ized as ?Malaria causes millions of deaths.?
view compression as a sequential tagging problem
and make use of linear-chain Conditional Ran-
dom Fields (CRFs) (Lafferty et al, 2001) to se-
lect the most likely compression. We represent
each sentence as a sequence of tokens, X =
x0x1 . . . xn, and generate a sequence of labels,
Y = y0y1 . . . yn, that encode which tokens are
kept, using a BIO label format: {B-RETAIN de-
notes the beginning of a retained sequence, I-
RETAIN indicates tokens ?inside? the retained se-
quence, O marks tokens to be removed}.
The CRF model is built using the features
shown in Table 3. ?Dependency Tree Features?
encode the grammatical relations in which each
word is involved as a dependent. For the ?Syntac-
tic Tree?, ?Dependency Tree? and ?Rule-Based?
features, we also include features for the two
words that precede and the two that follow the cur-
rent word. Detailed descriptions of the training
data and experimental setup are in Section 5.
During inference, we find the maximally likely
sequence Y according to a CRF with parameter
? (Y = argmaxY ? P (Y ?|X; ?)), while simulta-
neously enforcing the rules of Table 2 to reduce
the hypothesis space and encourage grammatical
compression. To do this, we encode these rules as
features for each token, and whenever these fea-
ture functions fire, we restrict the possible label
for that token to ?O?.
4.3 Tree-based Compression
Our tree-based compression methods are in line
with syntax-driven approaches (Galley and McK-
eown, 2007), where operations are carried out
on parse tree constituents. Unlike previous
work (Knight and Marcu, 2000; Galley and McK-
eown, 2007), we do not produce a new parse tree,
1387
Rule Example
Header [MOSCOW , October 19 ( Xinhua ) ?] Russian federal troops Tuesday continued...
Relative dates ...Centers for Disease Control confirmed [Tuesday] that there was...
Intra-sentential attribution ...fueling the La Nina weather phenomenon, [the U.N. weather agency said].
Lead adverbials [Interestingly], while the Democrats tend to talk about...
Noun appositives Wayne County Prosecutor [John O?Hara] wanted to send a message...
Nonrestrictive relative clause Putin, [who was born on October 7, 1952 in Leningrad], was elected in the presidential election...
Adverbial clausal modifiers [Starting in 1998], California will require 2 per cent of a manufacturer...
(Lead sentence) [Given the short time], car makers see electric vehicles as...
Within Parentheses ...to Christian home schoolers in the early 1990s [(www.homecomputermarket.com)].
Table 2: Linguistically-motivated rules for sentence compression. The grayed-out words in brackets are removed.
but focus on learning to identify the proper set of
constituents to be removed. In particular, when a
node is dropped from the tree, all words it sub-
sumes will be deleted from the sentence.
Formally, given a parse tree T of the sentence
to be compressed and a tree traversal algorithm,
T can be presented as a list of ordered constituent
nodes, T = t0t1 . . . tm. Our objective is to find a
set of labels, L = l0l1 . . . lm, where li ? {RETAIN,
REMOVE, PARTIAL}. RETAIN (RET) and RE-
MOVE (REM) denote whether the node ti is re-
tained or removed. PARTIAL (PAR) means ti is
partly removed, i.e. at least one child subtree of ti
is dropped.
Labels are identified, in order, according to the
tree traversal algorithm. Every node label needs
to be compatible with the labeling history: given
a node ti, and a set of labels l0 . . . li?1 predicted
for nodes t0 . . . ti?1, li =RET or li =REM is com-
patible with the history when all children of ti are
labeled as RET or REM, respectively; li =PAR is
compatible when ti has at least two descendents
tj and tk (j < i and k < i), one of which is
RETained and the other, REMoved. As such, the
root of the gray subtree in Figure 1 is labeled as
REM; its left siblings as RET; its parent as PAR.
As the space of possible compressions is expo-
nential in the number of leaves in the parse tree,
instead of looking for the globally optimal solu-
tion, we use beam search to find a set of highly
likely compressions and employ a language model
trained on a large corpus for evaluation.
A Beam Search Decoder. The beam search de-
coder (see Algorithm 1) takes as input the sen-
tence?s parse tree T = t0t1 . . . tm, an order-
ing O for traversing T (e.g. postorder) as a se-
quence of nodes in T , the set L of possible
node labels, a scoring function S for evaluat-
ing each sentence compression hypothesis, and
a beam size N . Specifically, O is a permuta-
tion on the set {0, 1, . . . ,m}?each element an
index onto T . Following O, T is re-ordered as
tO0tO1 . . . tOm , and the decoder considers each or-
dered constituent tOi in turn. In iteration i, all
existing sentence compression hypotheses are ex-
panded by one node, tOi , labeling it with all com-
patible labels. The new hypotheses (usually sub-
sentences) are ranked by the scorer S and the top
N are preserved to be extended in the next itera-
tion. See Figure 2 for an example.
Input : parse tree T , ordering O = O0O1 . . . Om,
L ={RET, REM, PAR}, hypothesis scorer S,
beam size N
Output: N best compressions
stack? ? (empty set);
foreach node tOi in T = tO0 . . . tOm doif i == 0 (first node visited) then
foreach label lO0 in L donewHypothesis h? ? [lO0 ];put h? into Stack;
end
else
newStack? ? (empty set);
foreach hypothesis h in stack do
foreach label lOi in L doif lOi is compatible thennewHypothesis h? ? h + [lOi ];put h? into newStack;
end
end
end
stack? newStack;
end
Apply S to sort hypotheses in stack in descending
order;
Keep the N best hypotheses in stack;
end
Algorithm 1: Beam search decoder.
Our BASIC Tree-based Compression in-
stantiates the beam search decoder with
postorder traversal and a hypothesis scorer
that takes a possible sentence compression?
a sequence of nodes (e.g. tO0 . . . tOk ) and
their labels (e.g. lO0 . . . lOk )?and returns?k
j=1 logP (lOj |tOj ) (denoted later as
ScoreBasic). The probability is estimated by
a Maximum Entropy classifier (Berger et al,
1388
Figure 2: Example of beam search decoding. For
postorder traversal, the three nodes are visited in a
bottom-up order. The associated compression hypothe-
ses (boxed) are ranked based on the scores in parenthe-
ses. Beam scores for other nodes are omitted.
Basic Features Syntactic Tree Features
projection falls w/in first 1/3/5 toks?? constituent label
projection falls w/in last 1/3/5 toks?? parent left/right sibling label
subsumes first 1/3/5 toks?? grandparent left/right sibling label
subsumes last 1/3/5 toks?? is leftmost child of parent?
number of words larger than 5/10?? is second leftmost child of parent?
is leaf node?? is head node of parent?
is root of parsing tree?? label of its head node
has word with first letter capitalized? has a depth greater than 3/5/10?
has word with all letters capitalized? Dependency Tree Features
has negation? dep rel of head node?
has stopwords? dep rel of parent?s head node?
Semantic Features dep rel of grandparent?s head node?
the head node has predicate? contain root of dep tree??
semantic roles of head node has a depth larger than 3/5??
Rule-Based Features
For each rule in Table 2 , we construct a corresponding feature to indicate
whether the token is identified by the rule.
Table 4: Constituent-level features for tree-based com-
pression. ? or ? denote features that are concatenated
with every Syntactic Tree feature to compose a new
one.
1996) trained at the constituent level using the
features in Table 4. We also apply the rules of
Table 2 during the decoding process. Concretely,
if the words subsumed by a node are identified
by any rule, we only consider REM as the node?s
label.
Given the N -best compressions from the de-
coder, we evaluate the yield of the trimmed trees
using a language model trained on the Giga-
word (Graff, 2003) corpus and return the compres-
sion with the highest probability. Thus, the de-
coder is quite flexible ? its learned scoring func-
tion allows us to incorporate features salient for
sentence compression while its language model
guarantees the linguistic quality of the compressed
string. In the sections below we consider addi-
tional improvements.
4.3.1 Improving Beam Search
CONTEXT-aware search is based on the intu-
ition that predictions on preceding context can
be leveraged to facilitate the prediction of the
current node. For example, parent nodes with
children that have all been removed (retained)
should have a label of REM (RET). In light of
this, we encode these contextual predictions as
additional features of S, that is, ALL-CHILDREN-
REMOVED/RETAINED, ANY-LEFTSIBLING-
REMOVED/RETAINED/PARTLY REMOVED,
LABEL-OF-LEFT-SIBLING/HEAD-NODE.
HEAD-driven search modifies the BASIC pos-
torder tree traversal by visiting the head node first
at each level, leaving other orders unchanged. In
a nutshell, if the head node is dropped, then its
modifiers need not be preserved. We adopt the
same features as CONTEXT-aware search, but re-
move those involving left siblings. We also add
one more feature: LABEL-OF-THE-HEAD-NODE-
IT-MODIFIES.
4.3.2 Task-Specific Sentence Compression
The current scorer ScoreBasic is still fairly naive
in that it focuses only on features of the sen-
tence to be compressed. However extra-sentential
knowledge can also be important for query-
focused MDS. For example, information regard-
ing relevance to the query might lead the de-
coder to produce compressions better suited for
the summary. Towards this goal, we construct
a compression scoring function?the multi-scorer
(MULTI)?that allows the incorporation of mul-
tiple task-specific scorers. Given a hypothesis at
any stage of decoding, which yields a sequence of
words W = w0w1...wj , we propose the following
component scorers.
Query Relevance. Query information ought to
guide the compressor to identify the relevant con-
tent. The query Q is expanded as described in
Section 3. Let |W ? Q| denote the number of
unique overlapping words betweenW andQ, then
scoreq = |W ?Q|/|W |.
Importance. A query-independent impor-
tance score is defined as the average Sum-
Basic (Toutanova et al, 2007) value in W ,
i.e. scoreim =?ji=1 SumBasic(wi)/|W |.
Language Model. We let scorelm be the proba-
bility of W computed by a language model.
Cross-Sentence Redundancy. To encourage di-
versified content, we define a redundancy score to
discount replicated content: scorered = 1? |W ?
C|/|W |, whereC is the words already selected for
the summary.
1389
The multi-scorer is defined as a linear
combination of the component scorers: Let
~? = (?0, . . . , ?4), 0 ? ?i ? 1, ????score =
(scoreBasic, scoreq, scoreim, scorelm, scorered),
S = scoremulti = ~? ? ????score (1)
The parameters ~? are tuned on a held-out tuning
set by grid search. We linearly normalize the score
of each metric, where the minimum and maximum
values are estimated from the tuning data.
5 Experimental Setup
We evaluate our methods on the DUC 2005, 2006
and 2007 datasets (Dang, 2005; Dang, 2006;
Dang, 2007), each of which is a collection of
newswire articles. 50 complex queries (topics) are
provided for DUC 2005 and 2006, 35 are collected
for DUC 2007 main task. Relevant documents for
each query are provided along with 4 to 9 human
MDS abstracts. The task is to generate a summary
within 250 words to address the query. We split
DUC 2005 into two parts: 40 topics to train the
sentence ranking models, and 10 for ranking algo-
rithm selection and parameter tuning for the multi-
scorer. DUC 2006 and DUC 2007 are reserved as
held out test sets.
Sentence Compression. The dataset
from Clarke and Lapata (2008) is used to
train the CRF and MaxEnt classifiers (Section 4).
It includes 82 newswire articles with one manually
produced compression aligned to each sentence.
Preprocessing. Documents are processed by a
full NLP pipeline, including token and sentence
segmentation, parsing, semantic role labeling,
and an information extraction pipeline consist-
ing of mention detection, NP coreference, cross-
document resolution, and relation detection (Flo-
rian et al, 2004; Luo et al, 2004; Luo and Zitouni,
2005).
Learning for Sentence Ranking and Compres-
sion. We use Weka (Hall et al, 2009) to train a
support vector regressor and experiment with var-
ious rankers in RankLib (Dang, 2011)3. As Lamb-
daMART has an edge over other rankers on the
held-out dataset, we selected it to produce ranked
sentences for further processing. For sequence-
based compression using CRFs, we employ Mal-
let (McCallum, 2002) and integrate the Table 2
rules during inference. NLTK (Bird et al, 2009)
3Default parameters are used. If an algorithm needs a val-
idation set, we use 10 out of 40 topics.
MaxEnt classifiers are used for tree-based com-
pression. Beam size is fixed at 2000.4 Sen-
tence compressions are evaluated by a 5-gram lan-
guage model trained on Gigaword (Graff, 2003)
by SRILM (Stolcke, 2002).
6 Results
The results in Table 5 use the official ROUGE soft-
ware with standard options5 and report ROUGE-
2 (R-2) (measures bigram overlap) and ROUGE-
SU4 (R-SU4) (measures unigram and skip-bigram
separated by up to four words). We compare our
sentence-compression-based methods to the best
performing systems based on ROUGE in DUC
2006 and 2007 (Jagarlamudi et al, 2006; Pingali
et al, 2007), system by Davis et al (2012) that
report the best R-2 score on DUC 2006 and 2007
thus far, and to the purely extractive methods of
SVR and LambdaMART.
Our sentence-compression-based systems
(marked with ?) show statistically significant
improvements over pure extractive summarization
for both R-2 and R-SU4 (paired t-test, p < 0.01).
This means our systems can effectively remove
redundancy within the summary through compres-
sion. Furthermore, our HEAD-driven beam search
method with MULTI-scorer beats all systems on
DUC 20066 and all systems on DUC 2007 except
the best system in terms of R-2 (p < 0.01). Its
R-SU4 score is also significantly (p < 0.01)
better than extractive methods, rule-based and
sequence-based compression methods on both
DUC 2006 and 2007. Moreover, our systems with
learning-based compression have considerable
compression rates, indicating their capability to
remove superfluous words as well as improve
summary quality.
Human Evaluation. The Pyramid (Nenkova
and Passonneau, 2004) evaluation was developed
to manually assess how many relevant facts or
Summarization Content Units (SCUs) are cap-
tured by system summaries. We ask a professional
annotator (who is not one of the authors, is highly
experienced in annotating for various NLP tasks,
and is fluent in English) to carry out a Pyramid
evaluation on 10 randomly selected topics from
4We looked at various beam sizes on the heldout data, and
observed that the performance peaks around this value.
5ROUGE-1.5.5.pl -n 4 -w 1.2 -m -2 4 -u -c 95 -r 1000 -f
A -p 0.5 -t 0 -a -d
6The system output from Davis et al (2012) is not avail-
able, so significance tests are not conducted on it.
1390
DUC 2006 DUC 2007
System C Rate R-2 R-SU4 C Rate R-2 R-SU4
Best DUC system ? 9.56 15.53 ? 12.62 17.90
Davis et al (2012) ? 10.2 15.2 ? 12.8 17.5
SVR 100% 7.78 13.02 100% 9.53 14.69
LambdaMART 100% 9.84 14.63 100% 12.34 15.62
Rule-based 78.99% 10.62 ?? 15.73 ? 78.11% 13.18? 18.15?
Sequence 76.34% 10.49 ? 15.60 ? 77.20% 13.25? 18.23?
Tree (BASIC + ScoreBasic) 70.48% 10.49 ? 15.86 ? 69.27% 13.00? 18.29?
Tree (CONTEXT + ScoreBasic) 65.21% 10.55 ?? 16.10 ? 63.44% 12.75 18.07?
Tree (HEAD + ScoreBasic) 66.70% 10.66 ?? 16.18 ? 65.05% 12.93 18.15?
Tree (HEAD + MULTI) 70.20% 11.02 ?? 16.25 ? 73.40% 13.49? 18.46?
Table 5: Query-focused MDS performance comparison: C Rate or compression rate is the proportion of words
preserved. R-2 (ROUGE-2) and R-SU4 (ROUGE-SU4) scores are multiplied by 100. ??? indicates that data is
unavailable. BASIC, CONTEXT and HEAD represent the basic beam search decoder, context-aware and head-driven
search extensions respectively. ScoreBasic and MULTI refer to the type of scorer used. Statistically significant
improvements (p < 0.01) over the best system in DUC 06 and 07 are marked with ?. ? indicates statistical
significance (p < 0.01) over extractive approaches (SVR or LambdaMART). HEAD + MULTI outperforms all the
other extract- and compression-based systems in R-2.
System Pyr Gra Non-Red Ref Foc Coh
Best DUC system (ROUGE) 22.9?8.2 3.5?0.9 3.5?1.0 3.5?1.1 3.6?1.0 2.9?1.1
Best DUC system (LQ) ? 4.0?0.8 4.2?0.7 3.8?0.7 3.6?0.9 3.4?0.9
Our System 26.4?10.3 3.0?0.9 4.0?1.1 3.6?1.0 3.4?0.9 2.8?1.0
Table 6: Human evaluation on our multi-scorer based system, Jagarlamudi et al (2006) (Best DUC system
(ROUGE)), and Lacatusu et al (2006) (Best DUC system (LQ)). Our system can synthesize more relevant content
according to Pyramid (?100). We also examine linguistic quality (LQ) in Grammaticality (Gra), Non-redundancy
(Non-Red), Referential clarity (Ref), Focus (Foc), and Structure and Coherence (Coh) like Dang (2006), each rated
from 1 (very poor) to 5 (very good). Our system has better non-redundancy than Jagarlamudi et al (2006) and is
comparable to Jagarlamudi et al (2006) and Lacatusu et al (2006) in other metrics except grammaticality.
the DUC 2006 task with gold-standard SCU an-
notation in abstracts. The Pyramid score (see Ta-
ble 6) is re-calculated for the system with best
ROUGE scores in DUC 2006 (Jagarlamudi et al,
2006) along with our system by the same annota-
tor to make a meaningful comparison.
We further evaluate the linguistic quality (LQ)
of the summaries for the same 10 topics in ac-
cordance with the measurement in Dang (2006).
Four native speakers who are undergraduate stu-
dents in computer science (none are authors) per-
formed the task, We compare our system based
on HEAD-driven beam search with MULTI-scorer
to the best systems in DUC 2006 achieving top
ROUGE scores (Jagarlamudi et al, 2006) (Best
DUC system (ROUGE)) and top linguistic quality
scores (Lacatusu et al, 2006) (Best DUC system
(LQ))7. The average score and standard deviation
for each metric is displayed in Table 6. Our sys-
tem achieves a higher Pyramid score, an indication
that it captures more of the salient facts. We also
7Lacatusu et al (2006) obtain the best scores in three lin-
guistic quality metrics (i.e. grammaticality, focus, structure
and coherence), and overall responsiveness on DUC 2006.
attain better non-redundancy than Jagarlamudi et
al. (2006), meaning that human raters perceive
less replicative content in our summaries. Scores
for other metrics are comparable to Jagarlamudi
et al (2006) and Lacatusu et al (2006), which
either uses minimal non-learning-based compres-
sion rules or is a pure extractive system. However,
our compression system sometimes generates less
grammatical sentences, and those are mostly due
to parsing errors. For example, parsing a clause
starting with a past tense verb as an adverbial
clausal modifier can lead to an ill-formed com-
pression. Those issues can be addressed by an-
alyzing k-best parse trees and we leave it in the
future work. A sample summary from our multi-
scorer based system is in Figure 3.
Sentence Compression Evaluation. We
also evaluate sentence compression separately
on (Clarke and Lapata, 2008), adopting the same
partitions as (Martins and Smith, 2009), i.e. 1, 188
sentences for training and 441 for testing. Our
compression models are compared with Hedge
Trimmer (Dorr et al, 2003), a discriminative
model proposed by McDonald (2006) and a
1391
System C Rate Uni-Prec Uni-Rec Uni-F1 Rel-F1
HedgeTrimmer 57.64% 0.72 0.65 0.64 0.50
McDonald (2006) 70.95% 0.77 0.78 0.77 0.55
Martins and Smith (2009) 71.35% 0.77 0.78 0.77 0.56
Rule-based 87.65% 0.74 0.91 0.80 0.63
Sequence 70.79% 0.77 0.80 0.76 0.58
Tree (BASIC) 69.65% 0.77 0.79 0.75 0.56
Tree (CONTEXT) 67.01% 0.79 0.78 0.76 0.57
Tree (HEAD) 68.06% 0.79 0.80 0.77 0.59
Table 7: Sentence compression comparison. The true c rate is 69.06% for the test set. Tree-based approaches
all use single-scorer. Our context-aware and head-driven tree-based approaches outperform all the other systems
significantly (p < 0.01) in precision (Uni-Prec) without sacrificing the recalls (i.e. there is no statistically signifi-
cant difference between our models and McDonald (2006) / M & S (2009) with p > 0.05). Italicized numbers for
unigram F1 (Uni-F1) are statistically indistinguishable (p > 0.05). Our head-driven tree-based approach also pro-
duces significantly better grammatical relations F1 scores (Rel-F1) than all the other systems except the rule-based
method (p < 0.01).
Topic D0626H: How were the bombings of the US em-
bassies in Kenya and Tanzania conducted? What terror-
ist groups and individuals were responsible? How and
where were the attacks planned?
WASHINGTON, August 13 (Xinhua) ? President Bill
Clinton Thursday condemned terrorist bomb attacks at
U.S. embassies in Kenya and Tanzania and vowed to find
the bombers and bring them to justice. Clinton met with
his top aides Wednesday in the White House to assess the
situation following the twin bombings at U.S. embassies
in Kenya and Tanzania, which have killed more than 250
people and injured over 5,000, most of them Kenyans and
Tanzanians. Local sources said the plan to bomb U.S. em-
bassies in Kenya and Tanzania took three months to com-
plete and bombers destined for Kenya were dispatched
through Somali and Rwanda. FBI Director Louis Freeh,
Attorney General Janet Reno and other senior U.S. gov-
ernment officials will hold a news conference at 1 p.m.
EDT (1700GMT) at FBI headquarters in Washington ?to
announce developments in the investigation of the bomb-
ings of the U.S. embassies in Kenya and Tanzania,? the
FBI said in a statement. ...
Figure 3: Part of the summary generated by the multi-
scorer based summarizer for topic D0626H (DUC
2006). Grayed out words are removed. Query-
irrelevant phrases, such as temporal information or
source of the news, have been removed.
dependency-tree based compressor (Martins and
Smith, 2009)8. We adopt the metrics in Martins
and Smith (2009) to measure the unigram-level
macro precision, recall, and F1-measure with
respect to human annotated compression. In
addition, we also compute the F1 scores of
grammatical relations which are annotated by
RASP (Briscoe and Carroll, 2002) according
to Clarke and Lapata (2008).
In Table 7, our context-aware and head-driven
tree-based compression systems show statistically
significantly (p < 0.01) higher precisions (Uni-
8Thanks to Andre? F.T. Martins for system outputs.
Prec) than all the other systems, without decreas-
ing the recalls (Uni-Rec) significantly (p > 0.05)
based on a paired t-test. Unigram F1 scores (Uni-
F1) in italics indicate that the corresponding sys-
tems are not statistically distinguishable (p >
0.05). For grammatical relation evaluation, our
head-driven tree-based system obtains statistically
significantly (p < 0.01) better F1 score (Rel-F1
than all the other systems except the rule-based
system).
7 Conclusion
We have presented a framework for query-focused
multi-document summarization based on sentence
compression. We propose three types of com-
pression approaches. Our tree-based compres-
sion method can easily incorporate measures of
query relevance, content importance, redundancy
and language quality into the compression pro-
cess. By testing on a standard dataset using the
automatic metric ROUGE, our models show sub-
stantial improvement over pure extraction-based
methods and state-of-the-art systems. Our best
system also yields better results for human eval-
uation based on Pyramid and achieves comparable
linguistic quality scores.
Acknowledgments
This work was supported in part by National Sci-
ence Foundation Grant IIS-0968450 and a gift
from Boeing. We thank Ding-Jung Han, Young-
Suk Lee, Xiaoqiang Luo, Sameer Maskey, Myle
Ott, Salim Roukos, Yiye Ruan, Ming Tan, Todd
Ward, Bowen Zhou, and the ACL reviewers for
valuable suggestions and advice on various as-
pects of this work.
1392
References
Alfred V. Aho and Jeffrey D. Ullman. 1969. Syntax directed
translations and the pushdown assembler. J. Comput. Syst.
Sci., 3(1):37?56.
Regina Barzilay, Noemie Elhadad, and Kathleen R. McKe-
own. 2002. Inferring strategies for sentence ordering in
multidocument news summarization. J. Artif. Int. Res.,
17(1):35?55, August.
Taylor Berg-Kirkpatrick, Dan Gillick, and Dan Klein. 2011.
Jointly learning to extract and compress. ACL ?11, pages
481?490, Stroudsburg, PA, USA. Association for Compu-
tational Linguistics.
Adam L. Berger, Vincent J. Della Pietra, and Stephen
A. Della Pietra. 1996. A maximum entropy ap-
proach to natural language processing. Comput. Linguist.,
22(1):39?71, March.
Steven Bird, Ewan Klein, and Edward Loper. 2009. Natural
Language Processing with Python. O?Reilly Media.
T. Briscoe and J. Carroll. 2002. Robust accurate statistical
annotation of general text.
Christopher J.C. Burges, Robert Ragno, and Quoc Viet Le.
2007. Learning to rank with nonsmooth cost functions. In
B. Scho?lkopf, J. Platt, and T. Hoffman, editors, Advances
in Neural Information Processing Systems 19, pages 193?
200. MIT Press, Cambridge, MA.
Christopher J. C. Burges. 2010. From RankNet to Lamb-
daRank to LambdaMART: An overview. Technical report,
Microsoft Research.
Asli Celikyilmaz and Dilek Hakkani-Tu?r. 2011. Discovery
of topically coherent sentences for extractive summariza-
tion. ACL ?11, pages 491?499, Stroudsburg, PA, USA.
Association for Computational Linguistics.
James Clarke and Mirella Lapata. 2008. Global inference
for sentence compression an integer linear programming
approach. J. Artif. Int. Res., 31(1):399?429, March.
John M. Conroy, Judith D. Schlesinger, Dianne P. O?Leary,
and Jade Goldstein, 2006. Back to Basics: CLASSY 2006.
U.S. National Inst. of Standards and Technology.
Hoa T. Dang. 2005. Overview of DUC 2005. In Document
Understanding Conference.
Hoa Tran Dang. 2006. Overview of DUC 2006. In
Proc. Document Understanding Workshop, page 10 pages.
NIST.
Hoa T. Dang. 2007. Overview of DUC 2007. In Document
Understanding Conference.
Van Dang. 2011. RankLib. Online.
Hal Daume?, III and Daniel Marcu. 2006. Bayesian
query-focused summarization. ACL ?06, pages 305?312,
Stroudsburg, PA, USA. Association for Computational
Linguistics.
Sashka T. Davis, John M. Conroy, and Judith D. Schlesinger.
2012. Occams - an optimal combinatorial covering algo-
rithm for multi-document summarization. In ICDM Work-
shops, pages 454?463.
Bonnie J Dorr, David Zajic, and Richard Schwartz. 2003.
Hedge trimmer: a parse-and-trim approach to headline
generation. In Proceedings of the HLT-NAACL 03 on
Text summarization workshop - Volume 5, HLT-NAACL-
DUC ?03, pages 1 ? 8, Stroudsburg, PA, USA. Association
for Computational Linguistics, Association for Computa-
tional Linguistics.
Gu?nes Erkan and Dragomir R. Radev. 2004. Lexrank: graph-
based lexical centrality as salience in text summarization.
J. Artif. Int. Res., 22(1):457?479, December.
Radu Florian, Hany Hassan, Abraham Ittycheriah, Hongyan
Jing, Nanda Kambhatla, Xiaoqiang Luo, Nicolas Nicolov,
and Salim Roukos. 2004. A statistical model for multilin-
gual entity detection and tracking. In HLT-NAACL, pages
1?8.
Maria Fuentes, Enrique Alfonseca, and Horacio Rodr??guez.
2007. Support vector machines for query-focused sum-
marization trained and evaluated on pyramid data. In Pro-
ceedings of the 45th Annual Meeting of the ACL on In-
teractive Poster and Demonstration Sessions, ACL ?07,
pages 57?60, Stroudsburg, PA, USA. Association for
Computational Linguistics.
Michel Galley and Kathleen McKeown. 2007. Lexicalized
Markov grammars for sentence compression. NAACL
?07, pages 180?187, Rochester, New York, April. Asso-
ciation for Computational Linguistics.
Dan Gillick and Benoit Favre. 2009. A scalable global model
for summarization. In Proceedings of the Workshop on In-
teger Linear Programming for Natural Langauge Process-
ing, ILP ?09, pages 10?18, Stroudsburg, PA, USA. Asso-
ciation for Computational Linguistics.
David Graff. 2003. English Gigaword.
Aria Haghighi and Lucy Vanderwende. 2009. Explor-
ing content models for multi-document summarization.
NAACL ?09, pages 362?370, Stroudsburg, PA, USA. As-
sociation for Computational Linguistics.
Mark Hall, Eibe Frank, Geoffrey Holmes, Bernhard
Pfahringer, Peter Reutemann, and Ian H. Witten. 2009.
The weka data mining software: an update. SIGKDD Ex-
plor. Newsl., 11(1):10?18, November.
Jagadeesh Jagarlamudi, Prasad Pingali, and Vasudeva Varma,
2006. Query Independent Sentence Scoring approach to
DUC 2006.
J. Peter Kincaid, Robert P. Fishburne, Richard L. Rogers, and
Brad S. Chissom. 1975. Derivation of New Readability
Formulas (Automated Readability Index, Fog Count and
Flesch Reading Ease Formula) for Navy Enlisted Person-
nel. Technical report, February.
Kevin Knight and Daniel Marcu. 2000. Statistics-based sum-
marization - step one: Sentence compression. AAAI ?00,
pages 703?710. AAAI Press.
Finley Lacatusu, Andrew Hickl, Kirk Roberts, Ying Shi,
Jeremy Bensley, Bryan Rink, Patrick Wang, and Lara Tay-
lor, 2006. LCCs gistexter at duc 2006: Multi-strategy
multi-document summarization.
John D. Lafferty, Andrew McCallum, and Fernando C. N.
Pereira. 2001. Conditional random fields: Probabilistic
models for segmenting and labeling sequence data. In
1393
Proceedings of the Eighteenth International Conference
on Machine Learning, ICML ?01, pages 282?289, San
Francisco, CA, USA. Morgan Kaufmann Publishers Inc.
P. M. Lewis, II and R. E. Stearns. 1968. Syntax-directed
transduction. J. ACM, 15(3):465?488, July.
Hui Lin and Jeff Bilmes. 2011. A class of submodular func-
tions for document summarization. In Proceedings of the
49th Annual Meeting of the Association for Computational
Linguistics: Human Language Technologies - Volume 1,
HLT ?11, pages 510?520, Stroudsburg, PA, USA. Associ-
ation for Computational Linguistics.
Chin-Yew Lin and Eduard Hovy. 2000. The automated ac-
quisition of topic signatures for text summarization. In
Proceedings of the 18th conference on Computational
linguistics - Volume 1, COLING ?00, pages 495?501,
Stroudsburg, PA, USA. Association for Computational
Linguistics.
Chin-Yew Lin and Eduard Hovy. 2003. Automatic eval-
uation of summaries using n-gram co-occurrence statis-
tics. In Proceedings of the 2003 Conference of the North
American Chapter of the Association for Computational
Linguistics on Human Language Technology - Volume 1,
pages 71?78.
Chin-Yew Lin. 2003. Improving summarization perfor-
mance by sentence compression: a pilot study. In Pro-
ceedings of the sixth international workshop on Informa-
tion retrieval with Asian languages - Volume 11, AsianIR
?03, pages 1?8, Stroudsburg, PA, USA. Association for
Computational Linguistics.
Xiaoqiang Luo and Imed Zitouni. 2005. Multi-lingual
coreference resolution with syntactic features. In
HLT/EMNLP.
Xiaoqiang Luo, Abraham Ittycheriah, Hongyan Jing, Nanda
Kambhatla, and Salim Roukos. 2004. A mention-
synchronous coreference resolution algorithm based on
the bell tree. In ACL, pages 135?142.
Andre? F. T. Martins and Noah A. Smith. 2009. Summariza-
tion with a joint model for sentence extraction and com-
pression. In Proceedings of the Workshop on Integer Lin-
ear Programming for Natural Langauge Processing, ILP
?09, pages 1?9, Stroudsburg, PA, USA. Association for
Computational Linguistics.
Andrew Kachites McCallum. 2002. Mallet: A machine
learning for language toolkit. http://mallet.cs.umass.edu.
Ryan McDonald. 2006. Discriminative Sentence Compres-
sion with Soft Syntactic Constraints. In Proceedings of
the 11th?EACL, Trento, Italy, April.
Michael Mozer, Michael I. Jordan, and Thomas Petsche, ed-
itors. 1997. Advances in Neural Information Processing
Systems 9, NIPS, Denver, CO, USA, December 2-5, 1996.
MIT Press.
Ani Nenkova and Rebecca Passonneau. 2004. Evaluating
content selection in summarization: The pyramid method.
In Daniel Marcu Susan Dumais and Salim Roukos, edi-
tors, HLT-NAACL 2004: Main Proceedings, pages 145?
152, Boston, Massachusetts, USA, May 2 - May 7. Asso-
ciation for Computational Linguistics.
Jahna Otterbacher, Gu?nes? Erkan, and Dragomir R. Radev.
2005. Using random walks for question-focused sentence
retrieval. In Proceedings of the conference on Human
Language Technology and Empirical Methods in Natural
Language Processing, HLT ?05, pages 915?922, Strouds-
burg, PA, USA. Association for Computational Linguis-
tics.
You Ouyang, Wenjie Li, Sujian Li, and Qin Lu. 2011.
Applying regression models to query-focused multi-
document summarization. Inf. Process. Manage.,
47(2):227?237, March.
Prasad Pingali, Rahul K, and Vasudeva Varma, 2007. IIIT
Hyderabad at DUC 2007. U.S. National Inst. of Standards
and Technology.
Chao Shen and Tao Li. 2011. Learning to rank for query-
focused multi-document summarization. In Diane J.
Cook, Jian Pei, Wei Wang 0010, Osmar R. Zaane, and
Xindong Wu, editors, ICDM, pages 626?634. IEEE.
Andreas Stolcke. 2002. SRILM ? an extensible language
modeling toolkit. In Proceedings of ICSLP, volume 2,
pages 901?904, Denver, USA.
Kristina Toutanova, Chris Brockett, Michael Gamon, Ja-
gadeesh Jagarlamudi, Hisami Suzuki, and Lucy Vander-
wende. 2007. The PYTHY Summarization System: Mi-
crosoft Research at DUC 2007. In Proc. of DUC.
Jenine Turner and Eugene Charniak. 2005. Supervised and
unsupervised learning for sentence compression. ACL
?05, pages 290?297, Stroudsburg, PA, USA. Association
for Computational Linguistics.
David Zajic, Bonnie J Dorr, Jimmy Lin, and R. Schwartz.
2006. Sentence compression as a component of a multi-
document summarization system. Proceedings of the
2006 Document Understanding Workshop, New York.
1394
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 1395?1405,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
Domain-Independent Abstract Generation
for Focused Meeting Summarization
Lu Wang
Department of Computer Science
Cornell University
Ithaca, NY 14853
luwang@cs.cornell.edu
Claire Cardie
Department of Computer Science
Cornell University
Ithaca, NY 14853
cardie@cs.cornell.edu
Abstract
We address the challenge of generating natu-
ral language abstractive summaries for spoken
meetings in a domain-independent fashion.
We apply Multiple-Sequence Alignment to in-
duce abstract generation templates that can be
used for different domains. An Overgenerate-
and-Rank strategy is utilized to produce and
rank candidate abstracts. Experiments us-
ing in-domain and out-of-domain training on
disparate corpora show that our system uni-
formly outperforms state-of-the-art supervised
extract-based approaches. In addition, human
judges rate our system summaries significantly
higher than compared systems in fluency and
overall quality.
1 Introduction
Meetings are a common way to collaborate,
share information and exchange opinions. Con-
sequently, automatically generated meeting sum-
maries could be of great value to people and busi-
nesses alike by providing quick access to the es-
sential content of past meetings. Focused meet-
ing summaries have been proposed as particularly
useful; in contrast to summaries of a meeting as
a whole, they refer to summaries of a specific as-
pect of a meeting, such as the DECISIONS reached,
PROBLEMS discussed, PROGRESS made or AC-
TION ITEMS that emerged (Carenini et al, 2011).
Our goal is to provide an automatic summariza-
tion system that can generate abstract-style fo-
cused meeting summaries to help users digest the
vast amount of meeting content in an easy manner.
Existing meeting summarization systems re-
main largely extractive: their summaries are com-
prised exclusively of patchworks of utterances se-
lected directly from the meetings to be summa-
rized (Riedhammer et al, 2010; Bui et al, 2009;
Xie et al, 2008). Although relatively easy to con-
struct, extractive approaches fall short of produc-
ing concise and readable summaries, largely due
C: Looking at what we?ve got, we we want an LCD dis-
play with a spinning wheel.
B: You have to have some push-buttons, don?t you?
C: Just spinning and not scrolling, I would say.
B: I think the spinning wheel is definitely very now.
A: but since LCDs seems to be uh a definite yes,
C: We?re having push-buttons on the outside
C: and then on the inside an LCD with spinning wheel,
Decision Abstract (Summary):
The remote will have push buttons outside, and an LCD
and spinning wheel inside.
A: and um I?m not sure about the buttons being in the
shape of fruit though.
D: Maybe make it like fruity colours or something.
C: The power button could be like a big apple or some-
thing.
D: Um like I?m just thinking bright colours.
Problem Abstract (Summary):
How to incorporate a fruit and vegetable theme into the
remote.
Figure 1: Clips from the AMI meeting corpus (Mc-
cowan et al, 2005). A, B, C and D refer to distinct
speakers. Also shown is the gold-standard (manual)
abstract (summary) for the decision and the problem.
to the noisy, fragmented, ungrammatical and un-
structured text of meeting transcripts (Murray et
al., 2010b; Liu and Liu, 2009).
In contrast, human-written meeting summaries
are typically in the form of abstracts ? distilla-
tions of the original conversation written in new
language. A user study from Murray et al (2010b)
showed that people demonstrate a strong prefer-
ence for abstractive summaries over extracts when
the text to be summarized is conversational. Con-
sider, for example, the two types of focused sum-
mary along with their associated dialogue snippets
in Figure 1. We can see that extracts are likely to
include unnecessary and noisy information from
the meeting transcripts. On the contrary, the man-
ually composed summaries (abstracts) are more
compact and readable, and are written in a dis-
tinctly non-conversational style.
1395
To address the limitations of extract-based sum-
maries, we propose a complete and fully automatic
domain-independent abstract generation frame-
work for focused meeting summarization. Fol-
lowing existing language generation research (An-
geli et al, 2010; Konstas and Lapata, 2012), we
first perform content selection: given the dia-
logue acts relevant to one element of the meet-
ing (e.g. a single decision or problem), we train
a classifier to identify summary-worthy phrases.
Next, we develop an ?overgenerate-and-rank?
strategy (Walker et al, 2001; Heilman and Smith,
2010) for surface realization, which generates and
ranks candidate sentences for the abstract. Af-
ter redundancy reduction, the full meeting abstract
can thus comprise the focused summary for each
meeting element. As described in subsequent sec-
tions, the generation framework allows us to iden-
tify and reformulate the important information for
the focused summary. Our contributions are as fol-
lows:
? To the best of our knowledge, our system is
the first fully automatic system to generate
natural language abstracts for spoken meet-
ings.
? We present a novel template extraction al-
gorithm, based on Multiple Sequence Align-
ment (MSA) (Durbin et al, 1998), to induce
domain-independent templates that guide ab-
stract generation. MSA is commonly used
in bioinformatics to identify equivalent frag-
ments of DNAs (Durbin et al, 1998) and
has also been employed for learning para-
phrases (Barzilay and Lee, 2003).
? Although our framework requires labeled
training data for each type of focused sum-
mary (decisions, problems, etc.), we also
make initial tries for domain adaptation so
that our summarization method does not need
human-written abstracts for each new meet-
ing domain (e.g. faculty meetings, theater
group meetings, project group meetings).
We instantiate the abstract generation frame-
work on two corpora from disparate domains
? the AMI Meeting Corpus (Mccowan et al,
2005) and ICSI Meeting Corpus (Janin et al,
2003) ? and produce systems to generate fo-
cused summaries with regard to four types of
meeting elements: DECISIONs, PROBLEMs, AC-
TION ITEMSs, and PROGRESS. Automatic eval-
uation (using ROUGE (Lin and Hovy, 2003) and
BLEU (Papineni et al, 2002)) against manually
generated focused summaries shows that our sum-
marizers uniformly and statistically significantly
outperform two baseline systems as well as a
state-of-the-art supervised extraction-based sys-
tem. Human evaluation also indicates that the
abstractive summaries produced by our systems
are more linguistically appealing than those of
the utterance-level extraction-based system, pre-
ferring them over summaries from the extraction-
based system of comparable semantic correctness
(62.3% vs. 37.7%).
Finally, we examine the generality of our model
across domains for two types of focused summa-
rization ? decisions and problems ? by train-
ing the summarizer on out-of-domain data (i.e. the
AMI corpus for use on the ICSI meeting data,
and vice versa). The resulting systems yield re-
sults comparable to those from the same system
trained on in-domain data, and statistically signif-
icantly outperform supervised extractive summa-
rization approaches trained on in-domain data.
2 Related Work
Most research on spoken dialogue summariza-
tion attempts to generate summaries for full dia-
logues (Carenini et al, 2011). Only recently has
the task of focused summarization been studied.
Supervised methods are investigated to identify
key phrases or utterances for inclusion in the de-
cision summary (Ferna?ndez et al, 2008; Bui et
al., 2009). Based on Ferna?ndez et al (2008), a
relation representation is proposed by Wang and
Cardie (2012) to form structured summaries; we
adopt this representation here for content selec-
tion.
Our research is also in line with generating ab-
stractive summaries for conversations. Extrac-
tive approaches (Murray et al, 2005; Xie et al,
2008; Galley, 2006) have been investigated exten-
sively in conversation summarization. Murray et
al. (2010a) present an abstraction system consist-
ing of interpretation and transformation steps. Ut-
terances are mapped to a simple conversation on-
tology in the interpretation step according to their
type, such as a decision or problem. Then an in-
teger linear programming approach is employed
to select the utterances that cover more entities as
1396
Dialogue Acts: 
C: Looking at what we've got, 
we we want [an LCD display 
with a spinning wheel]. 
B: You have to have some 
push-buttons, don't you? 
C: Just spinning and not 
scrolling , I would say . 
B: I think the spinning wheel is 
definitely very now. 
A: but since LCDs seems to be 
uh a definite yes, 
C: We're having push-buttons 
[on the outside] 
C: and then on the inside an 
LCD with spinning wheel, 
Relation Instances: 
<want, an LCD display with a spinning 
wheel> 
<an LCD display, with a spinning 
wheel> 
<have, some push-buttons> 
<having, push-buttons on the outside> 
<push-buttons, on the outside> 
<an LCD, with spinning wheel> 
? (other possibilities) 
<want, an LCD display with a spinning wheel> 
? The team will want an LCD display with a 
spinning wheel. 
? The team with work with an LCD display 
with a spinning wheel. 
? The group decide to use an LCD display with 
a spinning wheel. 
? (other possibilities) 
<push-buttons, on the outside> 
? Push-buttons are going to be on the outside. 
? Push-buttons on the outside will be used. 
? There will be push-buttons on the outside. 
? (other possibilities) 
One-Best 
Abstract: 
The group decide to 
use an LCD display 
with a spinning 
wheel. 
One-Best 
Abstract: 
There will be push-
buttons on the 
outside. 
Final Summary: 
The group decide to 
use an LCD display with 
a spinning wheel. 
There will be push-
buttons on the outside. 
Learned Templates 
? (all possible abstracts per relation 
instance) 
Relation 
Extraction 
Content Selection 
Template 
Filling 
Statistical 
Ranking 
Surface Realization 
? (one-best abstract 
per relation instance) 
Post-
Selection 
Figure 2: The abstract generation framework. It takes as input a cluster of meeting-item-specific dialogue acts,
from which one focused summary is constructed. Sample relation instances are denoted in bold (The indicators
are further italicized and the arguments are in [brackets]). Summary-worthy relation instances are identified by
content selection module (see Section 4) and then filled into the learned templates individually. A statistical ranker
subsequently selects one best abstract per relation instance (see Section 5.2). The post-selection component reduces
the redundancy and outputs the final summary (see Section 5.3).
determined by an external ontology. Liu and Liu
(2009) apply sentence compression on extracted
summary utterances. Though some of the unnec-
essary words are dropped, the resulting compres-
sions can still be ungrammatical and unstructured.
This work is also broadly related to ex-
pert system-based language generation (Reiter
and Dale, 2000) and concept-to-text generation
tasks (Angeli et al, 2010; Konstas and Lapata,
2012), where the generation process is decom-
posed into content selection (or text planning) and
surface realization. For instance, Angeli et al
(2010) learn from structured database records and
parallel textual descriptions. They generate texts
based on a series of decisions made to select the
records, fields, and proper templates for render-
ing. Those techniques that are tailored to specific
domains (e.g. weather forecasts or sportcastings)
cannot be directly applied to the conversational
data, as their input is well-structured and the tem-
plates learned are domain-specific.
3 Framework
Our domain-independent abstract generation
framework produces a summarizer that gener-
ates a grammatical abstract from a cluster of
meeting-element-related dialogue acts (DAs) ?
all utterances associated with a single decision,
problem, action item or progress step of interest.
Note that identifying these DA clusters is a diffi-
cult task in itself (Bui et al, 2009). Accordingly,
our experiments evaluate two conditions ? one
in which we assume that they are perfectly iden-
tified, and one in which we identify the clusters
automatically.
The summarizer consists of two major compo-
nents and is depicted in Figure 2. Given the DA
cluster to be summarized, the Content Selection
module identifies a set of summary-worthy rela-
tion instances represented as indicator-argument
pairs (i.e. these constitute a finer-grained represen-
tation than DAs). The Surface Realization compo-
nent then generates a short summary in three steps.
In the first step, each relation instance is filled into
templates with disparate structures that are learned
automatically from the training set (Template Fill-
ing). A statistical ranker then selects one best ab-
stract per relation instance (Statistical Ranking).
Finally, selected abstracts are processed for redun-
dancy removal in Post-Selection. Detailed descrip-
tions for each individual step are provided in Sec-
tions 4 and 5.
4 Content Selection
Phrase-based content selection approaches have
been shown to support better meeting sum-
maries (Ferna?ndez et al, 2008). Therefore, we
chose a content selection representation of a finer
granularity than an utterance: we identify relation
instances that can both effectively detect the cru-
cial content and incorporate enough syntactic in-
formation to facilitate the downstream surface re-
alization.
More specifically, our relation instances are
based on information extraction methods that
identify a lexical indicator (or trigger) that evokes
a relation of interest and then employ syntac-
tic information, often in conjunction with se-
mantic constraints, to find the argument con-
stituent(or target phrase) to be extracted. Rela-
1397
tion instances, then, are represented by indicator-
argument pairs (Chen et al, 2011). For example,
in the DA cluster of Figure 2, ?want, an LCD dis-
play with a spinning wheel? and ?push-buttons, on
the outside? are two relation instances.
Relation Instance Extraction We adopt and
extend the syntactic constraints from Wang and
Cardie (2012) to identify all relation instances in
the input utterances; the summary-worthy ones
will be selected by a discriminative classifier.
Constituent and dependency parses are obtained
by the Stanford parser (Klein and Manning, 2003).
Both the indicator and argument take the form of
constituents in the parse tree. We restrict the el-
igible indicator to be a noun or verb; the eligi-
ble arguments is a noun phrase (NP), prepositional
phrase (PP) or adjectival phrase (ADJP). A valid
indicator-argument pair should have at least one
content word and satisfy one of the following con-
straints:
? When the indicator is a noun, the argument
has to be a modifier or complement of the in-
dicator.
? When the indicator is a verb, the argument
has to be the subject or the object if it is an
NP, or a modifier or complement of the indi-
cator if it is a PP/ADJP.
We view relation extraction as a binary classifi-
cation problem rather than a clustering task (Chen
et al, 2011). All relation instances can be cate-
gorized as summary-worthy or not, but only the
summary-worthy ones are used for abstract gen-
eration. A discriminative classifier is trained for
this purpose based on Support Vector Machines
(SVMs) (Joachims, 1998) with an RBF kernel.
For training data construction, we consider a re-
lation instance to be a positive example if it shares
any content word with its corresponding abstracts,
and a negative example otherwise. The features
used are shown in Table 1.
5 Surface Realization
In this section, we describe surface realization,
which renders the relation instances into natural
language abstracts. This process begins with tem-
plate extraction (Section 5.1). Once the templates
are learned, the relation instances from Section 4
are filled into the templates to generate an abstract
(see Section 5.2). Redundancy handling is dis-
cussed in Section 5.3.
Basic Features
number of words/content words
portion of content words/stopwords
number of content words in indicator/argument
number of content words that are also in previous DA
indicator/argument only contains stopword?
number of new nouns
Content Features
has capitalized word?
has proper noun?
TF/IDF/TFIDF min/max/average
Discourse Features
main speaker or not?
is in an adjacency pair (AP)?
is in the source/target of the AP?
number of source/target DA in the AP
is the target of the AP a positive/negative/neutral response?
is the source of the AP a question?
Syntax Features
indicator/argument constituent tag
dependency relation of indicator and argument
Table 1: Features for content selection. Most are
adapted from previous work (Galley, 2006; Xie et al,
2008; Wang and Cardie, 2012). Every basic or con-
tent feature is concatenated with the constituent tags of
indicator and argument to compose a new one. Main
speakers include the most talkative speaker (who has
said the most words) and other speakers whose word
count is more than 20% of the most talkative one (Xie
et al, 2008). Adjacency pair (AP) (Galley, 2006) is
an important conversational analysis concept; each AP
consists of a source utterance and a target utterance pro-
duced by different speakers.
5.1 Template Extraction
Sentence Clustering. Template extraction starts
with clustering the sentences that constitute the
manually generated abstracts in the training data
according to their lexical and structural similarity.
From each cluster, multiple-sequence alignment
techniques are employed to capture the recurring
patterns.
Intuitively, desirable templates are those that
can be applied in different domains to generate
the same type of focused summary (e.g. decision
or problem summaries). We do not want sen-
tences to be clustered only because they describe
the same domain-specific details (e.g. they are all
about ?data collection?), which will lead to frag-
mented templates that are not reusable for new do-
mains. We therefore replace all appearances of
dates, numbers, and proper names with generic la-
bels. We also replace words that appear in both
the abstract and supporting dialogue acts by a la-
bel indicating its phrase type. For any noun phrase
with its head word abstracted, the whole phrase is
also replaced with ?NP?.
1398
        
start They 
The group were not sure whether to VP NP 
use 
NP should include end 
how much would cost to make 
1) The group were not sure whether to [include]VP [a recharger for the remote]NP . 2) The group were not sure whether to use [plastic and rubber or titanium for the case]NP . 3) The group were not sure whether [the remote control]NP should include [functions for controlling video]NP . 4) They were not sure how much [a recharger]NP would cost to make . ? (Kther abstracts) 
1) The group were not sure whether to VP NP .  2) The group were not sure whether to use NP .  3) The group were not sure whether NP should include NP .  4) They were not sure how much NP would cost to make . 
Generic Label Replacement + Clustering 
Template Examples:  Fine T1: The group were not sure whether to SLOTVP NP . (1, 2)  Fine T2: The group were not sure whether NP SLOTVP SLOTVP NP . (3)  Fine T3: SLOTNP were not sure SLOTWHADJP SLOTWHADJP NP SLOTVP SLOTVP SLOTVP SLOTVP SLOTVP . (4)  Coarse T1: SLOTNP SLOTNP were not sure SLOTSBAR SLOTVP SLOTVP SLOTNP . (1, 2)  Coarse T2: SLOTNP SLOTNP were not sure SLOTSBAR SLOTNP SLOTVP SLOTVP SLOTNP . (3)  Coarse T3: SLOTNP were not sure SLOTWHADJP SLOTWHADJP SLOTNP SLOTVP SLOTVP SLOTVP SLOTVP . (4) 
Template Induction 
MSA 
Figure 3: Example of template extraction by Multiple-
Sequence Alignment for problem abstracts from AMI.
Backbone nodes shared by at least 50% sentences are
shaded. The grammatical errors exist in the original
abstracts.
Following Barzilay and Lee (2003), we ap-
proach the sentence clustering task by hierarchical
complete-link clustering with a similarity metric
based on word n-gram overlap (n = 1, 2, 3). Clus-
ters with fewer than three abstracts are removed1.
Learning the Templates via MSA. For learn-
ing the structural patterns among the abstracts,
Multiple-Sequence Alignment (MSA) is first com-
puted for each cluster. MSA takes as input multi-
ple sentences and one scoring function to measure
the similarity between any two words. For inser-
tions or deletions, a gap cost is also added. MSA
can thus find the best way to align the sequences
with insertions or deletions in accordance with the
scorer. However, computing an optimal MSA is
NP-complete (Wang and Jiang, 1994), thus we
implement an approximate algorithm (Needleman
and Wunsch, 1970) that iteratively aligns two se-
quences each time and treats the resulting align-
ment as a new sequence2. Figure 3 demonstrates
an MSA computed from a sample cluster of ab-
1Clustering stops when the similarity between any pair-
wise clusters is below 5. This is applied to every type of sum-
marization. We tune the parameter on a small held-out devel-
opment set by manually evaluating the induced templates. No
significant change is observed within a small range.
2We adopt the scoring function for MSA from Barzilay
and Lee (2003), where aligning two identical words scores
1, inserting a gap scores ?0.01, and aligning two different
words scores ?0.5.
stracts. The MSA is represented in the form of
word lattice, from which we can detect the struc-
tural similarities shared by the sentences.
To transform the resulting MSAs into templates,
we need to decide whether a word in the sentence
should be retained to comprise the template or ab-
stracted. The backbone nodes in an MSA are iden-
tified as the ones shared by more than 50%3 of the
cluster?s sentences (shaded in gray in Figure 3).
We then create a FINE template for each sentence
by abstracting the non-backbone words, i.e. re-
placing each of those words with a generic token
(last step in Figure 3). We also create a COARSE
template that only preserves the nodes shared by
all of the cluster?s sentences. By using the op-
erations above, domain-independent patterns are
thus identified and domain-specific details are re-
moved.
Note that we do not explicitly evaluate the qual-
ity of the learned templates, which would require
a significant amount of manual evaluation. In-
stead, they are evaluated extrinsically. We encode
the templates as features (Angeli et al, 2010) that
could be selected or ignored in the succeeding ab-
stract ranking model.
5.2 Template Filling
An Overgenerate-and-Rank Approach. Since
filling the relation instances into templates of dis-
tinct structures may result in abstracts of vary-
ing quality, we rank the abstracts based on the
features of the template, the transformation con-
ducted, and the generated abstract. This is realized
by the Overgenerate-and-Rank strategy (Walker et
al., 2001; Heilman and Smith, 2010). It takes as
input a set of relation instances (from the same
cluster) R = {?indi, argi?}Ni=1 that are produced
by content selection component, a set of templates
T = {tj}Mj=1 that are represented as parsing trees,
a transformation function F (described below),
and a statistical ranker S for ranking the generated
abstracts, for which we defer description later in
this Section.
For each ?indi, argi?, the overgenerate-and-
rank approach fills it into each template in T by
applying F to generate all possible abstracts. Then
the ranker S selects the best abstract absi. Post-
selection is conducted on the abstracts {absi}Ni=1
to form the final summary.
3See Barzilay and Lee (2003) for a detailed discussion
about the choice of 50% according to pigeonhole principle.
1399
The transformation function F models the
constituent-level transformations of relation in-
stances and their mappings to the parse trees of
templates. With the intuition that people will reuse
the relation instances from the transcripts albeit
not necessarily in their original form to write the
abstracts, we consider three major types of map-
ping operations for the indicator or argument in the
source pair, namely, Full-Constituent Mapping,
Sub-Constituent Mapping, and Removal. Full-
Constituent Mapping denotes that a source con-
stituent is mapped directly to a target constituent
of the template parse tree with the same tag. Sub-
Constituent Mapping encodes more complex and
flexible transformations in that a sub-constituent
of the source is mapped to a target constituent
with the same tag. This operation applies when
the source has a tag of PP or ADJP, in which case
its sub-constituent, if any, with a tag of NP, VP or
ADJP can be mapped to the target constituent with
the same tag. For instance, an argument ?with a
spinning wheel? (PP) can be mapped to an NP in a
template because it has a sub-constituent ?a spin-
ning wheel? (NP). Removal means a source is not
mapped to any constituent in the template.
Formally, F is defined as:
F (?indsrc, argsrc?, t) =
{?indtrank , argtrank , indtark , argtark ?}Kk=1
where ?indsrc, argsrc? ? R is a relation in-
stance (source pair); t ? T is a template; indtrank
and argtrank is the transformed pair of indsrc and
argsrc; indtark and argtark are constituents in t, and
they compose one target pair for ?indsrc, argsrc?.
We require that indsrc and argsrc are not removed
at the same time. Moreover, for valid indtark and
argtark , the words subsumed by them should be all
abstracted in the template, and they do not overlap
in the parse tree.
To obtain the realized abstract, we traverse the
parse tree of the filled template in pre-order. The
words subsumed by the leaf nodes are thus col-
lected sequentially.
Learning a Statistical Ranker. We utilize a dis-
criminative ranker based on Support Vector Re-
gression (SVR) (Smola and Scho?lkopf, 2004) to
rank the generated abstracts. Given the train-
ing data that includes clusters of gold-standard
summary-worthy relation instances, associated ab-
stracts they support, and the parallel templates for
each abstract, training samples for the ranker are
Basic Features
number of words in indsrc/argsrc
number of new nouns in indsrc/argsrc
indtrank /argtrank only has stopword?number of new nouns in indtrank /argtrankStructure Features
constituent tag of indsrc/argsrc
constituent tag of indsrc with constituent tag of indtar
constituent tag of argsrc with constituent tag of argtar
transformation of indsrc/argsrc combined with constituent tag
dependency relation of indsrc and argsrc
dependency relation of indtar and argtar
above 2 features have same value?
Template Features
template type (fine/coarse)
realized template (e.g. ?the group decided to?)
number of words in template
the template has verb?
Realization Features
realization has verb?
realization starts with verb?
realization has adjacent verbs/NPs?
indsrc precedes/succeeds argsrc?
indtar precedes/succeeds argtar?
above 2 features have same value?
Language Model Features
log pLM (first word in indtrank |previous 1/2 words)
log pLM (realization)
log pLM (first word in argtrank |previous 1/2 words)
log pLM (realization)/length
log pLM (next word | last 1/2 words in indtrank )
log pLM (next word | last 1/2 words in argtrank )
Table 2: Features for abstracts ranking. The language
model features are based on a 5-gram language model
trained on Gigaword (Graff, 2003) by SRILM (Stolcke,
2002).
constructed according to the transformation func-
tion F mentioned above. Each sample is repre-
sented as:
(?indsrc, argsrc?, ?indtrank , argtrank , indtark , argtark ?, t, a)
where ?indsrc, argsrc? is the source pair,
?indtrank , argtrank ? is the transformed pair,
?indtark , argtark ? is the target pair in template t,
and a is the abstract parallel to t.
We first find ?indtar,absk , argtar,absk ?, whichis the corresponding constituent pair of
?indtark , argtark ? in a. Then we identify
the summary-worthy words subsumed by
?indtrank , argtrank ? that also appear in a. If those
words are all subsumed by ?indtar,absk , argtar,absk ?,then it is considered to be a positive sample, and
a negative sample otherwise. Table 2 displays the
features used in abstract ranking.
5.3 Post-Selection: Redundancy Handling.
Post-selection aims to maximize the information
coverage and minimize the redundancy of the
summary. Given the generated abstracts A =
1400
Input : relation instances R = {?indi, argi?}Ni=1,
generated abstracts A = {absi}Ni=1, objective
function f , cost function C
Output: final abstract G
G? ? (empty set);
U ? A;
while U 6= ? do
abs? arg maxabsi?U f(A,G?absi)?f(A,G)C(absi) ;if f(A,G ? abs)? f(A,G) ? 0 then
G? G ? abs;
end
U ? U \ abs;
end
Algorithm 1: Greedy algorithm for post-
selection to generate the final summary.
{absi}Ni=1, we use a greedy algorithm (Lin and
Bilmes, 2010) to select a subsetA?, whereA? ? A,
to form the final summary. We define wij as
the unigram similarity between abstracts absi and
absj , C(absi) as the number of words in absi. We
employ the following objective function:
f(A,G) =?absi?A\G
?
absj?G wi,j , G ? AAlgorithm 1 sequentially finds an abstract with
the greatest ratio of objective function gain to
length, and add it to the summary if the gain is
non-negative.
6 Experimental Setup
Corpora. Two disparate corpora are used for
evaluation. The AMI meeting corpus (Mccowan
et al, 2005) contains 139 scenario-driven meet-
ings, where groups of four people participate in
a series of four meetings for a fictitious project of
designing remote control. The ICSI meeting cor-
pus (Janin et al, 2003) consists of 75 naturally oc-
curring meetings, each of them has 4 to 10 par-
ticipants. Compared to the fabricated topics in
AMI, the conversations in ICSI tend to be special-
ized and technical, e.g. discussion about speech
and language technology. We use 57 meetings in
ICSI and 139 meetings in AMI that include a short
(usually one-sentence), manually constructed ab-
stract summarizing each important output for ev-
ery meeting. Decision and problem summaries are
annotated for both corpora. AMI has extra ac-
tion item summaries, and ICSI has progress sum-
maries. The set of dialogue acts that support each
abstract are annotated as such.
System Inputs. We consider two system input
settings. In the True Clusterings setting, we
use the annotations to create perfect partitions of
the DAs for input to the system; in the System
Figure 4: Content selection evaluation by using
ROUGE-SU4 (multiplied by 100). SVM-DA and
SVM-TOKEN denotes for supervised extract-based
methods with SVMs on utterance- and token-level.
Summaries for decision, problem, action item, and
progress are generated and evaluated for AMI and ICSI
(with names in parentheses). X-axis shows the number
of meetings used for training.
Clusterings setting, we employ a hierarchical ag-
glomerative clustering algorithm used for this task
in (Wang and Cardie, 2011). DAs are grouped ac-
cording to a classifier trained beforehand.
Baselines and Comparisons. We compare our
system with (1) two unsupervised baselines, (2)
two supervised extractive approaches, and (3) an
oracle derived from the gold standard abstracts.
Baselines. As in Riedhammer et al (2010), the
LONGEST DA in each cluster is selected as the
summary. The second baseline picks the clus-
ter prototype (i.e. the DA with the largest TF-
IDF similarity with the cluster centroid) as the
summary according to Wang and Cardie (2011).
Although it is possible that important content is
spread over multiple DAs, both baselines allow
us to determine summary quality when summaries
are restricted to a single utterance.
Supervised Learning. We also compare our
approach to two supervised extractive sum-
marization methods ? Support Vector Ma-
chines (Joachims, 1998) trained with the same fea-
1401
tures as our system (see Table 1) to identify the im-
portant DAs (no syntax features) (Xie et al, 2008;
Sandu et al, 2010) or tokens (Ferna?ndez et al,
2008) to include into the summary4.
Oracle. We compute an oracle consisting of the
words from the DA cluster that also appear in the
associated abstract to reflect the gap between the
best possible extracts and the human abstracts.
7 Results
Content Selection Evaluation. We first employ
ROUGE (Lin and Hovy, 2003) to evaluate the
content selection component with respect to the
human written abstracts. ROUGE computes the
ngram overlapping between the system summaries
with the reference summaries, and has been used
for both text and speech summarization (Dang,
2005; Xie et al, 2008). We report ROUGE-2 (R-
2) and ROUGE-SU4 (R-SU4) that are shown to
correlate with human evaluation reasonably well.
In AMI, four meetings of different functions are
carried out in each group5. 35 meetings for ?con-
ceptual design? are randomly selected for testing.
For ICSI, we reserve 12 meetings for testing.
The R-SU4 scores for each system are displayed
in Figure 4 and show that our system uniformly
outperforms the baselines and supervised systems.
The learning curve of our system is relatively flat,
which means not many training meetings are re-
quired to reach a usable performance level.
Note that the ROUGE scores are relative low
when the reference summaries are human ab-
stracts, even for evaluation among abstracts pro-
duced by different annotators (Dang, 2005). The
intrinsic difference of styles between dialogue and
human abstract further lowers the scores. But the
trend is still respected among the systems.
Abstract Generation Evaluation. To evaluate
the full abstract generation system, the BLEU
score (Papineni et al, 2002) (the precision of uni-
grams and bigrams with a brevity penalty) is com-
puted with human abstracts as reference. BLEU
has a fairly good agreement with human judge-
ment and has been used to evaluate a variety of
language generation systems (Angeli et al, 2010;
Konstas and Lapata, 2012).
4We use SVMlight (Joachims, 1999) with RBF kernel by
default parameters for SVM-based classifiers and regressor.
5The four types of meetings in AMI are: project kick-off
(35 meetings), functional design (35 meetings), conceptual
design (35 meetings), and detailed design (34 meetings).
Figure 5: Full abstract generation system evaluation
by using BLEU (multiplied by 100). SVM-DA de-
notes for supervised extractive methods with SVMs on
utterance-level.
We are not aware of any existing work gen-
erating abstractive summaries for conversations.
Therefore, we compare our full system against
a supervised utterance-level extractive method
based on SVMs along with the baselines. The
BLEU scores in Figure 5 show that our system im-
proves the scores consistently over the baselines
and the SVM-based approach.
Domain Adaptation Evaluation. We further
examine our system in domain adaptation sce-
narios for decision and problem summarization,
where we train the system on AMI for use on ICSI,
and vice versa. Table 3 indicates that, with both
true clusterings and system clusterings, our sys-
tem trained on out-of-domain data achieves com-
parable performance with the same system trained
on in-domain data. In most experiments, it also
significantly outperforms the baselines and the
extract-based approaches (p < 0.05).
Human Evaluation. We randomly select 15 de-
cision and 15 problem DA clusters (true cluster-
ings). We evaluate fluency (is the text gram-
matical?) and semantic correctness (does the
summary convey the gist of the DAs in the clus-
ter?) for OUR SYSTEM trained on IN-domain data
1402
System (True Clusterings) AMI Decision ICSI Decision AMI Problem ICSI Problem
R-2 R-SU4 BLEU R-2 R-SU4 BLEU R-2 R-SU4 BLEU R-2 R-SU4 BLEU
CENTROID DA 1.3 3.0 7.7 1.8 3.5 3.8 1.0 2.7 4.2 1.0 2.3 2.8
LONGEST DA 1.6 3.3 7.0 2.8 4.7 6.5 1.0 3.0 3.6 1.2 3.4 4.6
SVM-DA (IN) 3.4 4.7 9.7 3.4 4.5 5.7 1.4 2.4 5.0 1.6 3.4 3.4
SVM-DA (OUT) 2.7 4.2 6.6 3.1 4.2 4.6 1.4 2.2 2.5 1.3 3.0 4.6
OUR SYSTEM (IN) 4.5 6.2 11.6 4.9 7.1 10.0 3.1 4.8 7.2 4.0 5.9 6.0
OUR SYSTEM (OUT) 4.6 6.1 10.3 4.8 6.4 7.8 3.5 4.7 6.2 3.0 5.5 5.3
ORACLE 7.5 12.0 22.8 9.9 14.9 20.2 6.6 11.3 18.9 6.4 12.6 13.0
System (System Clusterings) AMI Decision ICSI Decision AMI Problem ICSI Problem
R-2 R-SU4 BLEU R-2 R-SU4 BLEU R-2 R-SU4 BLEU R-2 R-SU4 BLEU
CENTROID DA 1.4 3.3 3.8 1.4 2.1 2.0 0.8 2.8 2.9 0.9 2.3 1.8
LONGEST DA 1.4 3.3 5.7 1.7 3.4 5.5 0.8 3.2 4.1 0.9 3.4 4.4
SVM-DA (IN) 2.6 4.6 10.5 3.5 6.5 7.1 1.8 3.7 4.9 1.8 4.0 4.6
SVM-DA (OUT) 3.4 5.8 10.3 2.7 4.8 6.3 2.1 3.8 4.3 1.5 3.8 3.5
OUR SYSTEM (IN) 3.5 5.4 11.7 4.4 7.4 9.1 3.3 4.6 9.5 2.3 4.2 7.4
OUR SYSTEM (OUT) 3.9 6.4 11.4 4.1 5.1 8.4 3.6 5.6 8.9 1.8 4.0 6.8
ORACLE 6.4 12.0 15.1 8.2 15.2 17.6 6.5 13.0 20.9 5.5 11.9 15.5
Table 3: Domain adaptation evaluation. Systems trained on out-of-domain data are denoted with ?(OUT)?, oth-
erwise with ?(IN)?. ROUGE and BLEU scores are multiplied by 100. Our systems that statistically significantly
outperform all the other approaches (except ORACLE) are in bold (p < 0.05, paired t-test). The numbers in italics
show the significant improvement over the baselines by our systems.
System Fluency Semantic Length
Mean S.D. Mean S.D.
OUR SYSTEM (IN) 3.67 0.85 3.27 1.03 23.65
OUR SYSTEM (OUT) 3.58 0.90 3.25 1.16 24.17
SVM-DA (IN) 3.36 0.84 3.44 1.26 38.83
Table 4: Human evaluation results of Fluency and Se-
mantic correctness for the generated abstracts. The rat-
ings are on 1 (worst) to 5 (best) scale. The average
Length of the abstracts for each system is also listed.
and OUT-of-domain data, and for the utterance-
level extraction system (SVM-DA) trained on in-
domain data. Each cluster of DAs along with three
randomly ordered summaries are presented to the
judges. Five native speaking Ph.D. students (none
are authors) performed the task.
We carry out an one-way Analysis of Variance
which shows significant differences in score as a
function of system (p < 0.05, paired t-test). Re-
sults in Table 4 demonstrate that our system sum-
maries are significantly more compact and fluent
than the extract-based method (p < 0.05) while
semantic correctness is comparable.
The judges also rank the three summaries in
terms of the overall quality in content, concise-
ness and grammaticality. An inter-rater agreement
of Fleiss?s ? = 0.45 (moderate agreement (Landis
and Koch, 1977)) was computed. Judges selected
our system as the best system in 62.3% scenarios
(IN-DOMAIN: 35.6%, OUT-OF-DOMAIN: 26.7%).
Sample summaries are exhibited in Figure 6.
8 Conclusion
We presented a domain-independent abstract gen-
eration framework for focused meeting summa-
rization. Experimental results on two disparate
meeting corpora show that our system can uni-
Decision Summary:
Human: The remote will have push buttons outside, and
an LCD and spinning wheel inside.
Our System (In): The group decide to use an LCD dis-
play with a spinning wheel. There will be push-buttons on
the outside.
Our System (Out): LCD display is going to be with a
spinning wheel. It is necessary having push-buttons on
the outside.
SVM-DA: Looking at what we?ve got, we we want an
LCD display with a spinning wheel. Just spinning and not
scrolling, I would say. I think the spinning wheel is defi-
nitely very now. We?re having push-buttons on the outside
Problem Summary:
Human: How to incorporate a fruit and vegetable theme
into the remote.
Our System (In): Whether to include the shape of fruit.
The team had to thinking bright colors.
Our System (Out): It is unclear that the buttons being in
the shape of fruit.
SVM-DA: and um Im not sure about the buttons being in
the shape of fruit though.
Figure 6: Sample decision and problem sum-
maries generated by various systems for examples
in Figure 1.
formly outperform the state-of-the-art supervised
extraction-based systems in both automatic and
manual evaluation. Our system also exhibits an
ability to train on out-of-domain data to generate
abstracts for a new target domain.
9 Acknowledgments
This work was supported in part by National Sci-
ence Foundation Grant IIS-0968450 and a gift
from Boeing. We thank Moontae Lee, Myle Ott,
Yiye Ruan, Chenhao Tan, and the ACL reviewers
for valuable suggestions and advice on various as-
pects of this work.
1403
References
Gabor Angeli, Percy Liang, and Dan Klein. 2010. A
simple domain-independent probabilistic approach
to generation. In Proceedings of the 2010 Confer-
ence on Empirical Methods in Natural Language
Processing, EMNLP ?10, pages 502?512, Strouds-
burg, PA, USA. Association for Computational Lin-
guistics.
Regina Barzilay and Lillian Lee. 2003. Learn-
ing to paraphrase: an unsupervised approach using
multiple-sequence alignment. In Proceedings of the
2003 Conference of the North American Chapter
of the Association for Computational Linguistics on
Human Language Technology - Volume 1, NAACL
?03, pages 16?23, Stroudsburg, PA, USA. Associa-
tion for Computational Linguistics.
Trung H. Bui, Matthew Frampton, John Dowding, and
Stanley Peters. 2009. Extracting decisions from
multi-party dialogue using directed graphical mod-
els and semantic similarity. In Proceedings of the
SIGDIAL 2009 Conference: The 10th Annual Meet-
ing of the Special Interest Group on Discourse and
Dialogue, SIGDIAL ?09, pages 235?243, Strouds-
burg, PA, USA. Association for Computational Lin-
guistics.
Giuseppe Carenini, Gabriel Murray, and Raymond Ng.
2011. Methods for Mining and Summarizing Text
Conversations. Morgan & Claypool Publishers.
Harr Chen, Edward Benson, Tahira Naseem, and
Regina Barzilay. 2011. In-domain relation discov-
ery with meta-constraints via posterior regulariza-
tion. In Proceedings of the 49th Annual Meeting of
the Association for Computational Linguistics: Hu-
man Language Technologies - Volume 1, HLT ?11,
pages 530?540, Stroudsburg, PA, USA. Association
for Computational Linguistics.
Hoa T. Dang. 2005. Overview of DUC 2005. In Doc-
ument Understanding Conference.
Richard Durbin, Sean R. Eddy, Anders Krogh, and
Graeme Mitchison. 1998. Biological Sequence
Analysis: Probabilistic Models of Proteins and Nu-
cleic Acids. Cambridge University Press, July.
Raquel Ferna?ndez, Matthew Frampton, John Dowding,
Anish Adukuzhiyil, Patrick Ehlen, and Stanley Pe-
ters. 2008. Identifying relevant phrases to sum-
marize decisions in spoken meetings. In INTER-
SPEECH, pages 78?81.
Michel Galley. 2006. A skip-chain conditional random
field for ranking meeting utterances by importance.
In Proceedings of the 2006 Conference on Empirical
Methods in Natural Language Processing, EMNLP
?06, pages 364?372, Stroudsburg, PA, USA. Associ-
ation for Computational Linguistics.
David Graff. 2003. English Gigaword.
Michael Heilman and Noah A. Smith. 2010. Good
question! statistical ranking for question generation.
In Human Language Technologies: The 2010 An-
nual Conference of the North American Chapter of
the Association for Computational Linguistics, HLT
?10, pages 609?617, Stroudsburg, PA, USA. Associ-
ation for Computational Linguistics.
A. Janin, D. Baron, J. Edwards, D. Ellis, D. Gelbart,
N. Morgan, B. Peskin, T. Pfau, E. Shriberg, A. Stol-
cke, and C. Wooters. 2003. The icsi meeting corpus.
volume 1, pages I?364?I?367 vol.1.
Thorsten Joachims. 1998. Text categorization with
suport vector machines: Learning with many rele-
vant features. In Proceedings of the 10th European
Conference onMachine Learning, ECML ?98, pages
137?142, London, UK, UK. Springer-Verlag.
Thorsten Joachims. 1999. Advances in kernel meth-
ods. chapter Making large-scale support vector ma-
chine learning practical, pages 169?184. MIT Press,
Cambridge, MA, USA.
Dan Klein and Christopher D. Manning. 2003. Ac-
curate unlexicalized parsing. In Proceedings of the
41st Annual Meeting on Association for Computa-
tional Linguistics - Volume 1, ACL ?03, pages 423?
430, Stroudsburg, PA, USA. Association for Com-
putational Linguistics.
Ioannis Konstas and Mirella Lapata. 2012. Concept-
to-text generation via discriminative reranking. In
Proceedings of the 50th Annual Meeting of the Asso-
ciation for Computational Linguistics: Long Papers
- Volume 1, ACL ?12, pages 369?378, Stroudsburg,
PA, USA. Association for Computational Linguis-
tics.
J R Landis and G G Koch. 1977. The measurement of
observer agreement for categorical data. Biometrics,
33(1):159?174.
Hui Lin and Jeff Bilmes. 2010. Multi-document sum-
marization via budgeted maximization of submod-
ular functions. In Human Language Technologies:
The 2010 Annual Conference of the North American
Chapter of the Association for Computational Lin-
guistics, HLT ?10, pages 912?920, Stroudsburg, PA,
USA. Association for Computational Linguistics.
Chin-Yew Lin and Eduard Hovy. 2003. Auto-
matic evaluation of summaries using n-gram co-
occurrence statistics. In Proceedings of the 2003
Conference of the North American Chapter of the
Association for Computational Linguistics on Hu-
man Language Technology - Volume 1, pages 71?78.
Fei Liu and Yang Liu. 2009. From extractive to ab-
stractive meeting summaries: can it be done by sen-
tence compression? In Proceedings of the ACL-
IJCNLP 2009 Conference Short Papers, ACLShort
?09, pages 261?264, Stroudsburg, PA, USA. Associ-
ation for Computational Linguistics.
1404
I. Mccowan, G. Lathoud, M. Lincoln, A. Lisowska,
W. Post, D. Reidsma, and P. Wellner. 2005. The ami
meeting corpus. In In: Proceedings Measuring Be-
havior 2005, 5th International Conference on Meth-
ods and Techniques in Behavioral Research. L.P.J.J.
Noldus, F. Grieco, L.W.S. Loijens and P.H. Zimmer-
man (Eds.), Wageningen: Noldus Information Tech-
nology.
Gabriel Murray, Steve Renals, and Jean Carletta. 2005.
Extractive summarization of meeting recordings. In
INTERSPEECH, pages 593?596.
Gabriel Murray, Giuseppe Carenini, and Raymond Ng.
2010a. Interpretation and transformation for ab-
stracting conversations. In Human Language Tech-
nologies: The 2010 Annual Conference of the North
American Chapter of the Association for Com-
putational Linguistics, HLT ?10, pages 894?902,
Stroudsburg, PA, USA. Association for Computa-
tional Linguistics.
Gabriel Murray, Giuseppe Carenini, and Raymond T.
Ng. 2010b. Generating and validating abstracts of
meeting conversations: a user study. In INLG.
S. B. Needleman and C. D. Wunsch. 1970. A general
method applicable to the search for similarities in
the amino acid sequence of two proteins. Journal of
molecular biology, 48(3):443?453, March.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a method for automatic eval-
uation of machine translation. In Proceedings of
the 40th Annual Meeting on Association for Com-
putational Linguistics, ACL ?02, pages 311?318,
Stroudsburg, PA, USA. Association for Computa-
tional Linguistics.
Ehud Reiter and Robert Dale. 2000. Building natural
language generation systems. Cambridge Univer-
sity Press, New York, NY, USA.
Korbinian Riedhammer, Benoit Favre, and Dilek
Hakkani-Tu?r. 2010. Long story short - global unsu-
pervised models for keyphrase based meeting sum-
marization. Speech Commun., 52(10):801?815, Oc-
tober.
Oana Sandu, Giuseppe Carenini, Gabriel Murray, and
Raymond Ng. 2010. Domain adaptation to sum-
marize human conversations. In Proceedings of the
2010 Workshop on Domain Adaptation for Natural
Language Processing, DANLP 2010, pages 16?22,
Stroudsburg, PA, USA. Association for Computa-
tional Linguistics.
Alex J. Smola and Bernhard Scho?lkopf. 2004. A tu-
torial on support vector regression. Statistics and
Computing, 14(3):199?222, August.
Andreas Stolcke. 2002. SRILM ? an extensible lan-
guage modeling toolkit. In Proceedings of ICSLP,
volume 2, pages 901?904, Denver, USA.
Marilyn A. Walker, Owen Rambow, and Monica Ro-
gati. 2001. Spot: a trainable sentence planner.
In Proceedings of the second meeting of the North
American Chapter of the Association for Com-
putational Linguistics on Language technologies,
NAACL ?01, pages 1?8, Stroudsburg, PA, USA. As-
sociation for Computational Linguistics.
Lu Wang and Claire Cardie. 2011. Summarizing de-
cisions in spoken meetings. In Proceedings of the
Workshop on Automatic Summarization for Different
Genres, Media, and Languages, WASDGML ?11,
pages 16?24, Stroudsburg, PA, USA. Association
for Computational Linguistics.
Lu Wang and Claire Cardie. 2012. Focused meet-
ing summarization via unsupervised relation extrac-
tion. In Proceedings of the 13th Annual Meeting of
the Special Interest Group on Discourse and Dia-
logue, SIGDIAL ?12, pages 304?313, Stroudsburg,
PA, USA. Association for Computational Linguis-
tics.
Lusheng Wang and Tao Jiang. 1994. On the complex-
ity of multiple sequence alignment. Journal of Com-
putational Biology, 1(4):337?348.
Shasha Xie, Yang Liu, and Hui Lin. 2008. Evaluating
the effectiveness of features and sampling in extrac-
tive meeting summarization. In in Proc. of IEEE
Spoken Language Technology (SLT.
1405
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 1640?1649,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
Joint Inference for Fine-grained Opinion Extraction
Bishan Yang
Department of Computer Science
Cornell University
bishan@cs.cornell.edu
Claire Cardie
Department of Computer Science
Cornell University
cardie@cs.cornell.edu
Abstract
This paper addresses the task of fine-
grained opinion extraction ? the identi-
fication of opinion-related entities: the
opinion expressions, the opinion hold-
ers, and the targets of the opinions, and
the relations between opinion expressions
and their targets and holders. Most ex-
isting approaches tackle the extraction
of opinion entities and opinion relations
in a pipelined manner, where the inter-
dependencies among different extraction
stages are not captured. We propose a joint
inference model that leverages knowledge
from predictors that optimize subtasks
of opinion extraction, and seeks a glob-
ally optimal solution. Experimental re-
sults demonstrate that our joint inference
approach significantly outperforms tradi-
tional pipeline methods and baselines that
tackle subtasks in isolation for the problem
of opinion extraction.
1 Introduction
Fine-grained opinion analysis is concerned with
identifying opinions in text at the expression level;
this includes identifying the subjective (i.e., opin-
ion) expression itself, the opinion holder and the
target of the opinion (Wiebe et al, 2005). The
task has received increasing attention as many nat-
ural language processing applications would ben-
efit from the ability to identify text spans that cor-
respond to these key components of opinions. In
question-answering systems, for example, users
may submit questions in the form ?What does en-
tity A think about target B??; opinion-oriented
summarization systems also need to recognize
opinions and their targets and holders.
In this paper, we address the task of identifying
opinion-related entities and opinion relations. We
consider three types of opinion entities: opinion
expressions or direct subjective expressions as de-
fined in Wiebe et al (2005) ? expressions that ex-
plicitly indicate emotions, sentiment, opinions or
other private states (Quirk et al, 1985) or speech
events expressing private states; opinion targets
? expressions that indicate what the opinion is
about; and opinion holders ? mentions of whom
or what the opinion is from. Consider the follow-
ing examples in which opinion expressions (O) are
underlined and targets (T) and holders (H) of the
opinion are bracketed.
S1: [The workers][H1,2] were irked[O1]
by [the government report][T1] and
were worried[O2] as they went about
their daily chores.
S2: From the very start it could be
predicted[O1] that on the subject of
economic globalization, [the developed
states][T1,2] were going to come across
fierce opposition[O2].
The numeric subscripts denote linking relations,
one of IS-ABOUT or IS-FROM. In S1, for in-
stance, opinion expression ?were irked? (O1) IS-
ABOUT ?the government report? (T1). Note that
the IS-ABOUT relation can contain an empty tar-
get (e.g. ?were worried? in S1); similarly for IS-
FROM w.r.t. the opinion holder (e.g. ?predicted? in
S2). We also allow an opinion entity to be involved
in multiple relations (e.g. ?the developed states? in
S2).
Not surprisingly, fine-grained opinion extrac-
tion is a challenging task due to the complexity
and variety of the language used to express opin-
ions and their components (Pang and Lee, 2008).
Nevertheless, much progress has been made in ex-
tracting opinion information from text. Sequence
labeling models have been successfully employed
to identify opinion expressions (e.g. (Breck et al,
1640
2007; Yang and Cardie, 2012)) and relation ex-
traction techniques have been proposed to extract
opinion holders and targets based on their link-
ing relations to the opinion expressions (e.g. Kim
and Hovy (2006), Kobayashi et al (2007)). How-
ever, most existing work treats the extraction of
different opinion entities and opinion relations in a
pipelined manner: the interaction between differ-
ent extraction tasks is not modeled jointly and er-
ror propagation is not considered. One exception
is Choi et al (2006), which proposed an ILP ap-
proach to jointly identify opinion holders, opinion
expressions and their IS-FROM linking relations,
and demonstrated the effectiveness of joint infer-
ence. Their ILP formulation, however, does not
handle implicit linking relations, i.e. opinion ex-
pressions with no explicit opinion holder; nor does
it consider IS-ABOUT relations.
In this paper, we present a model that jointly
identifies opinion-related entities, including opin-
ion expressions, opinion targets and opinion hold-
ers as well as the associated opinion linking rela-
tions, IS-ABOUT and IS-FROM. For each type of
opinion relation, we allow implicit (i.e. empty) ar-
guments for cases when the opinion holder or tar-
get is not explicitly expressed in text. We model
entity identification as a sequence tagging prob-
lem and relation extraction as binary classifica-
tion. A joint inference framework is proposed to
jointly optimize the predictors for different sub-
problems with constraints that enforce global con-
sistency. We hypothesize that the ambiguity in
the extraction results will be reduced and thus,
performance increased. For example, uncertainty
w.r.t. the spans of opinion entities can adversely
affect the prediction of opinion relations; and evi-
dence of opinion relations might provide clues to
guide the accurate extraction of opinion entities.
We evaluate our approach using a standard cor-
pus for fine-grained opinion analysis (the MPQA
corpus (Wiebe et al, 2005)) and demonstrate that
our model outperforms by a significant margin tra-
ditional baselines that do not employ joint infer-
ence for extracting opinion entities and different
types of opinion relations.
2 Related Work
Significant research effort has been invested into
fine-grained opinion extraction for open-domain
text such as news articles (Wiebe et al, 2005; Wil-
son et al, 2009). Many techniques were proposed
to identify the text spans for opinion expressions
(e.g. (Breck et al, 2007; Johansson and Moschitti,
2010b; Yang and Cardie, 2012)), opinion hold-
ers (e.g. (Choi et al, 2005)) and topics of opin-
ions (Stoyanov and Cardie, 2008). Some consider
extracting opinion targets/holders along with their
relation to the opinion expressions. Kim and Hovy
(2006) identifies opinion holders and targets by us-
ing their semantic roles related to opinion words.
Ruppenhofer et al (2008) argued that semantic
role labeling is not sufficient for identifying opin-
ion holders and targets. Johansson and Moschitti
(2010a) extract opinion expressions and holders
by applying reranking on top of sequence label-
ing methods. Kobayashi et al (2007) considered
extracting ?aspect-evaluation? relations (relations
between opinion expressions and targets) by iden-
tifying opinion expressions first and then search-
ing for the most likely target for each opinion ex-
pression via a binary relation classifier. All these
methods extract opinion arguments and opinion
relations in separate stages instead of extracting
them jointly.
Most similar to our method is Choi et al (2006),
which jointly extracts opinion expressions, hold-
ers and their IS-FROM relations using an ILP ap-
proach. In contrast, our approach (1) also consid-
ers the IS-ABOUT relation which is arguably more
complex due to the larger variety in the syntac-
tic structure exhibited by opinion expressions and
their targets, (2) handles implicit opinion relations
(opinion expressions without any associated argu-
ment), and (3) uses a simpler ILP formulation.
There has also been substantial interest in opin-
ion extraction from product reviews (Liu, 2012).
Most existing approaches focus on the extrac-
tion of opinion targets and their associated opin-
ion expressions and usually employ a pipeline
architecture: generate candidates of opinion ex-
pressions and opinion targets first, and then use
rule-based or machine-learning-based approaches
to identify potential relations between opinions
and targets (Hu and Liu, 2004; Wu et al, 2009;
Liu et al, 2012). In addition to pipeline ap-
proaches, bootstrapping-based approaches were
proposed (Qiu et al, 2009; Qiu et al, 2011; Zhang
et al, 2010) to identify opinion expressions and
targets iteratively; however, they suffer from the
problem of error propagation.
There is much work demonstrating the bene-
fit of performing global inference. Roth and Yih
1641
(2004) proposed a global inference approach in the
formulation of a linear program (LP) and applied
it to the task of extracting named entities and re-
lations simultaneously. Their problem is similar
to ours ? the difference is that Roth and Yih Roth
and Yih (2004) assume that named entity spans are
known a priori and only their labels need to be as-
signed. Joint inference has also been applied to
semantic role labeling (Punyakanok et al, 2008;
Srikumar and Roth, 2011; Das et al, 2012), where
the goal is to jointly identify semantic arguments
for given lexical predicates. The problem is con-
ceptually similar to identifying opinion arguments
for opinion expressions, however, we do not as-
sume prior knowledge of opinion expressions (un-
like in SRL, where predicates are given).
3 Model
As proposed in Section 1, we consider the task of
jointly identifying opinion entities and opinion re-
lations. Specifically, given a sentence, our goal is
to identify spans of opinion expressions, opinion
arguments (targets and holders) and their associ-
ated linking relations. Training data consists of
text with manually annotated opinion expression
and argument spans, each with a list of relation
ids specifying the linking relation between opin-
ion expressions and their arguments.
In this section, we will describe how we model
opinion entity identification and opinion relation
extraction, and how we combine them in a joint
inference model.
3.1 Opinion Entity Identification
We formulate the task of opinion entity identifica-
tion as a sequence labeling problem and employ
conditional random fields (CRFs) (Lafferty et al,
2001) to learn the probability of a sequence as-
signment y for a given sentence x. Through in-
ference we can find the best sequence assignment
for sentence x and recover the opinion entities ac-
cording to the standard ?IOB? encoding scheme.
We consider four entity labels: D,T,H,N , where
D denotes opinion expressions, T denotes opinion
targets, H denotes opinion holders and N denotes
?NONE? entities.
We define potential function fiz that gives the
probability of assigning a span i with entity label
z, and the probability is estimated based on the
learned parameters from CRFs. Formally, given
a within-sentence span i = (a, b), where a is the
starting position and b is the end position, and la-
bel z ? {D,T,H}, we have
fiz = p(ya = Bz,ya+1 = Iz, ...,
yb = Iz,yb+1 6= Iz|x)
fiN = p(ya = O, ...,yb = O|x)
These probabilities can be efficiently computed
using the forward-backward algorithm.
3.2 Opinion Relation Extraction
We consider extracting the IS-ABOUT and IS-
FROM opinion relations. In the following we will
not distinguish these two relations, since they can
both be characterized as relations between opinion
expressions and opinion arguments, and the meth-
ods for relation extraction are the same.
We treat the relation extraction problem as a
combination of two binary classification prob-
lems: opinion-arg classification, which decides
whether a pair consisting of an opinion candidate o
and an argument candidate a forms a relation; and
opinion-implicit-arg classification, which decides
whether an opinion candidate o is linked to an im-
plicit argument, i.e. no argument is mentioned. We
define a potential function r to capture the strength
of association between an opinion candidate o and
an argument candidate a,
roa = p(y = 1|x)? p(y = 0|x)
where p(y = 1|x) and p(y = 0|x) are the logistic
regression estimates of the positive and negative
relations. Similarly, we define potential ro? to de-
note the confidence of predicting opinion span o
associated with an implicit argument.
3.2.1 Opinion-Arg Relations
For opinion-arg classification, we construct can-
didates of opinion expressions and opinion argu-
ments and consider each pair of an opinion can-
didate and an argument candidate as a potential
opinion relation. Conceptually, all possible sub-
sequences in the sentence are candidates. To filter
out candidates that are less reasonable, we con-
sider the opinion expressions and arguments ob-
tained from the n-best predictions by CRFs1. We
also employ syntactic patterns from dependency
1We randomly split the training data into 10 parts and ob-
tained the 50-best CRF predictions on each part for the gen-
eration of candidates. We also experimented with candidates
generated from more CRF predictions, but did not find any
performance improvement for the task.
1642
trees to generate candidates. Specifically, we se-
lected the most common patterns of the shortest
dependency paths2 between an opinion candidate
o and an argument candidate a in our dataset, and
include all pairs of candidates that satisfy at least
one dependency pattern. For the IS-ABOUT rela-
tion, the top three patterns are (1) o ?dobj a, (2)
o ?ccomp x ?nsubj a (x is a word in the path that is
not covered by either o nor a), (3) o ?ccomp a; for
the IS-FROM relation, the top three patterns are (1)
o ?nsubj a, (2) o ?poss a, (3) o ?ccomp x ?nsubj a.
Note that generating candidates this way will
give us a large number of negative examples. Sim-
ilar to the preprocessing approach in (Choi et al,
2006), we filter pairs of opinion and argument can-
didates that do not overlap with any gold standard
relation in our training data.
Many features we use are common features
in the SRL tasks (Punyakanok et al, 2008)
due to the similarity of opinion relations to the
predicate-argument relations in SRL (Ruppen-
hofer et al, 2008; Choi et al, 2006). In general,
the features aim to capture (a) local properties of
the candidate opinion expressions and arguments
and (b) syntactic and semantic attributes of their
relation.
Words and POS tags: the words contained in the
candidate and their POS tags.
Lexicon: For each word in the candidate, we
include its WordNet hypernyms and its strength
of subjectivity in the Subjectivity Lexicon3
(e.g. weaksubj, strongsubj).
Phrase type: the syntactic category of the deepest
constituent that covers the candidate in the parse
tree, e.g. NP, VP.
Semantic frames: For each verb in the opinion
candidate, we include its frame types according to
FrameNet4.
Distance: the relative distance (number of words)
between the opinion and argument candidates.
Dependency Path: the shortest path in the
dependency tree between the opinion candidate
and the target candidate, e.g. ccomp?nsubj?. We
also include word types and POS types in the
paths, e.g. opinion?ccompsuffering?nsubjpatient,
2We use the Stanford Parser to generate parse trees and
dependency graphs.
3http://mpqa.cs.pitt.edu/lexicons/
subj_lexicon/
4https://framenet.icsi.berkeley.edu/
fndrupal/
NN?ccompVBG?nsubjNN. The dependency path
has been shown to be very useful in extracting
opinion expressions and opinion holders (Johans-
son and Moschitti, 2010a).
3.2.2 Opinion-Implicit-Arg Relations
When the opinion-arg relation classifier predicts
that there is no suitable argument for the opinion
expression candidate, it does not capture the possi-
bility that an opinion candidate may associate with
an implicit argument. To incorporate knowledge
of implicit relations, we build an opinion-implicit-
arg classifier to identify an opinion candidate with
an implicit argument based on its own properties
and context information.
For training, we consider all gold-standard
opinion expressions as training examples ?
including those with implicit arguments ? as
positive examples and those associated with
explicit arguments as negative examples. For
features, we use words, POS tags, phrase types,
lexicon and semantic frames (see Section 3.2.1
for details) to capture the properties of the opinion
expression, and also features that capture the
context of the opinion expression:
Neighboring constituents: The words and gram-
matical roles of neighboring constituents of the
opinion expression in the parse tree ? the left and
right sibling of the deepest constituent containing
the opinion expression in the parse tree.
Parent Constituent: The grammatical role of
the parent constituent of the deepest constituent
containing the opinion expression.
Dependency Argument: The word types and
POS types of the arguments of the dependency
patterns in which the opinion expression is
involved. We consider the same dependency
patterns that are used to generate candidates for
opinion-arg classification.
3.3 Joint Inference
The inference goal is to find the optimal prediction
for both opinion entity identification and opinion
relation extraction. For a given sentence, we de-
note O as a set of opinion candidates, Ak as a set
of argument candidates, where k denotes the type
of opinion relation ? IS-ABOUT or IS-FROM ?
and S as a set of within-sentence spans that cover
all of the opinion candidates and argument can-
1643
didates. We introduce binary variable xiz , where
xiz = 1 means span i is associated with label z.
We also introduce binary variable uij for every
pair of opinion candidate i and argument candidate
j, where uij = 1 means i forms an opinion rela-
tion with j, and binary variable vik for every opin-
ion candidate i in relation type k, where vik = 1
means i associates with an implicit argument in
relation k. Given the binary variables xiy, uij , vik,
it is easy to recover the entity and relation assign-
ment by checking which spans are labeled as opin-
ion entities, and which opinion span and argument
span form an opinion relation.
The objective function is defined as a linear
combination of the potentials from different pre-
dictors with a parameter ? to balance the contribu-
tion of two components: opinion entity identifica-
tion and opinion relation extraction.
argmax
x,u,v
?
?
i?S
?
z
fizxiz
+ (1? ?)
?
k
?
i?O
?
??
j?Ak
rijuij + ri?vik
?
?
(1)
It is subject to the following linear constraints:
Constraint 1: Uniqueness. For each span i, we
must assign one and only one label z, where z ?
{H,D, T,N}.
?
z
xiz = 1
Constraint 2: Non-overlapping. If two spans i and
j overlap, then at most one of the spans can be
assigned to a non-NONE entity label: H,D, T .
?
z 6=N
xiz +
?
z 6=N
xjz ? 1
Constraint 3: Consistency between the opinion-
arg and opinion-implicit-arg classifiers. For an
opinion candidate i, if it is predicted to have an
implicit argument in relation k, vik = 1, then no
argument candidate should form a relation with i.
If vik = 0, then there exists some argument can-
didate j ? Ak such that uij = 1. We introduce
two auxiliary binary variables aik and bik to limit
the maximum number of relations associated with
each opinion candidate to be less than or equal to
three5. When vik = 1, aik and bik have to be 0.
?
j?Ak
uij = 1? vik + aik + bik
aik ? 1? vik, bik ? 1? vik
Constraint 4: Consistency between opinion-arg
classifier and opinion entity extractor. Suppose
an argument candidate j in relation k is assigned
an argument label by the entity extractor, that is
xjz = 1 (z = T for IS-ABOUT relation and z = H
for IS-FROM relation), then there exists some opin-
ion candidates that associate with j. Similar to
constraint 3, we introduce auxiliary binary vari-
ables cj and dj to enforce that an argument j links
to at most three opinion expressions. If xjz = 0,
then no relations should be extracted for j.
?
i?O
uij = xjz + cjk + djk
cjk ? xjz, djk ? xjz
Constraint 5: Consistency between the opinion-
implicit-arg classifier and opinion entity extractor.
When an opinion candidate i is predicted to asso-
ciate with an implicit argument in relation k, that
is vik = 1, then we allow xiD to be either 1 or
0 depending on the confidence of labeling i as an
opinion expression. When vik = 0, there exisits
some opinion argument associated with the opin-
ion candidate, and we enforce xiD = 1, which
means the entity extractor agrees to label i as an
opinion expression.
vik + xiD ? 1
Note that in our ILP formulation, the label
assignment for a candidate span involves one
multiple-choice decision among different opinion
entity labels and the ?NONE? entity label. The
scores of different label assignments are compara-
ble for the same span since they come from one
entity extraction model. This makes our ILP for-
mulation advantageous over the ILP formulation
proposed in Choi et al (2006), which needs m bi-
nary decisions for a candidate span, wherem is the
number of types of opinion entities, and the score
for each possible label assignment is obtained by
5It is possible to add more auxiliary variables to allow
more than three arguments to link to an opinion expression,
but this rarely happens in our experiments. For the IS-FROM
relation, we set aik = 0, bik = 0 since an opinion expression
usually has only one holder.
1644
the sum of raw scores from m independent extrac-
tion models. This design choice also allows us
to easily deal with multiple types of opinion ar-
guments and opinion relations.
4 Experiments
For evaluation, we used version 2.0 of the MPQA
corpus (Wiebe et al, 2005; Wilson, 2008), a
widely used data set for fine-grained opinion anal-
ysis.6 We considered the subset of 482 docu-
ments7 that contain attitude and target annotations.
There are a total of 9,471 sentences with opinion-
related labels at the phrase level. We set aside 132
documents as a development set and use 350 doc-
uments as the evaluation set. All experiments em-
ploy 10-fold cross validation on the evaluation set;
the average over the 10 runs is reported.
Our gold standard opinion expressions, opinion
targets and opinion holders correspond to the di-
rect subjective annotations, target annotations and
agent annotations, respectively. The IS-FROM re-
lation is obtained from the agent attribute of each
opinion expression. The IS-ABOUT relation is ob-
tained from the attitude annotations: each opinion
expression is annotated with attitude frames and
each attitude frame is associated with a list of tar-
gets. The relations may overlap: for example, in
the following sentence, the target of relation 1 con-
tains relation 2.
[John]H1 is happyO1 because [[he]H2
lovesO2 [being at Enderly Park]T2]T1 .
We discard relations that contain sub-relations be-
cause we believe that identifying the sub-relations
usually is sufficient to recover the discarded rela-
tions. (Prediction of overlapping relations is con-
sidered as future work.) In the example above, we
will identify (loves, being at Enderly Park) as an
IS-ABOUT relation and happy as an opinion ex-
pression associated with an implicit target. Table 1
shows some statistics of the corpus.
We adopted the evaluation metrics for entity and
relation extraction from Choi et al (2006), which
include precision, recall, and F1-measure accord-
ing to overlap and exact matching metrics.8 We
6Available at http://www.cs.pitt.edu/mpqa/.
7349 news articles from the original MPQA corpus, 84
Wall Street Journal articles (Xbank), and 48 articles from the
American National Corpus.
8Overlap matching considers two spans to match if they
overlap, while exact matching requires two spans to be ex-
actly the same.
Opinion Target Holder
TotalNum 5849 4676 4244
Opinion-arg Relations Implicit Relations
IS-ABOUT 4823 1302
IS-FROM 4662 1187
Table 1: Data Statistics of the MPQA Corpus.
will focus our discussion on results obtained us-
ing overlap matching, since the exact boundaries
of opinion entities are hard to define even for hu-
man annotators (Wiebe et al, 2005).
We trained CRFs for opinion entity identifica-
tion using the following features: indicators for
words, POS tags, and lexicon features (the sub-
jectivity strength of the word in the Subjectivity
Lexicon). All features are computed for the cur-
rent token and tokens in a [?1,+1] window. We
used L2-regularization; the regularization param-
eter was tuned using the development set. We
trained the classifiers for relation extraction using
L1-regularized logistic regression with default pa-
rameters using the LIBLINEAR (Fan et al, 2008)
package. For joint inference, we used GLPK9 to
provide the optimal ILP solution. The parameter
? was tuned using the development set.
4.1 Baseline Methods
We compare our approach to several pipeline base-
lines. Each extracts opinion entities first using
the same CRF employed in our approach, and
then predicts opinion relations on the opinion en-
tity candidates obtained from the CRF prediction.
Three relation extraction techniques were used in
the baselines:
? Adj: Inspired by the adjacency rule used
in Hu and Liu (2004), it links each argu-
ment candidate to its nearest opinion candi-
date. Arguments that do not link to any opin-
ion candidate are discarded. This is also used
as a strong baseline in Choi et al (2006).
? Syn: Links pairs of opinion and argument
candidates that present prominent syntactic
patterns. (We consider the syntactic patterns
listed in Section 3.2.1.) Previous work also
demonstrates the effectiveness of syntactic
information in opinion extraction (Johansson
and Moschitti, 2012).
9http://www.gnu.org/software/glpk/
1645
Opinion Expression Opinion Target Opinion Holder
Method P R F1 P R F1 P R F1
CRF 82.21 66.15 73.31 73.22 48.58 58.41 72.32 49.09 58.48
CRF+Adj 82.21 66.15 73.31 80.87 42.31 55.56 75.24 48.48 58.97
CRF+Syn 82.21 66.15 73.31 81.87 30.36 44.29 78.97 40.20 53.28
CRF+RE 83.02 48.99 61.62 85.07 22.01 34.97 78.13 40.40 53.26
Joint-Model 71.16 77.85 74.35? 75.18 57.12 64.92?? 67.01 66.46 66.73??
CRF 66.60 52.57 58.76 44.44 29.60 35.54 65.18 44.24 52.71
CRF+Adj 66.60 52.57 58.76 49.10 25.81 33.83 68.03 43.84 53.32
CRF+Syn 66.60 52.57 58.76 50.26 18.41 26.94 74.60 37.98 50.33
CRF+RE 69.27 40.09 50.79 60.45 15.37 24.51 75 38.79 51.13
Joint-Model 57.39 62.40 59.79? 49.15 38.33 43.07?? 62.73 62.22 62.47??
Table 2: Performance on opinion entity extraction using overlap and exact matching metrics (the top table uses overlap and
the bottom table uses exact). Two-tailed t-test results are shown on F1 measure for our method compared to the other baselines
(statistical significance is indicated with ?(p < 0.05), ??(p < 0.005)).
IS-ABOUT IS-FROM
Method P R F1 P R F1
CRF+Adj 73.65 37.34 49.55 70.22 41.58 52.23
CRF+Syn 76.21 28.28 41.25 77.48 36.63 49.74
CRF+RE 78.26 20.33 32.28 74.81 37.55 50.00
CRF+Adj-merged-10-best 25.05 61.18 35.55 30.28 62.82 40.87
CRF+Syn-merged-10-best 41.60 45.66 43.53 48.08 54.03 50.88
CRF+RE-merged-10-best 51.60 33.09 40.32 47.73 54.40 50.84
Joint-Model 64.38 51.20 57.04?? 64.97 58.61 61.63??
Table 3: Performance on opinion relation extraction using the overlap metric.
? RE: Predicts opinion relations by employ-
ing the opinion-arg classifier and opinion-
implicit-arg classifier. First, the opinion-arg
classifier identifies pairs of opinion and argu-
ment candidates that form valid opinion rela-
tions, and then the opinion-implicit-arg clas-
sifier is used on the remaining opinion candi-
dates to further identify opinion expressions
without explicit arguments.
We report results using opinion entity candi-
dates from the best CRF output and from the
merged 10-best CRF output.10 The motivation of
merging the 10-best output is to increase recall for
the pipeline methods.
5 Results
Table 2 shows the results of opinion entity identi-
fication using both overlap and exact metrics. We
compare our approach with the pipeline baselines
and CRF (the first step of the pipeline). We can
see that our joint inference approach significantly
outperforms all the baselines in F1 measure on ex-
tracting all types of opinion entities. In general,
10It is similar to the merged 10-best baseline in Choi et
al. (2006). If an entity Ei extracted by the ith-best sequence
overlaps with an entity Ej extracted by the jth-best sequence,
where i ? j, then we discard Ej . If Ei and Ej do not over-
lap, then we consider both entities.
by adding the relation extraction step, the pipeline
baselines are able to improve precision over the
CRF but fail at recall. CRF+Syn and CRF+Adj
provide the same performance as CRF, since the
relation extraction step only affects the results of
opinion arguments. By incorporating syntactic
information, CRF+Syn provides better precision
than CRF+Adj on extracting arguments at the ex-
pense of recall. This indicates that using simple
syntactic rules would mistakenly filter many cor-
rect relations. By using binary classifiers to pre-
dict relations, CRF+RE produces high precision
on opinion and target extraction but also results in
very low recall. Using the exact metric, we ob-
serve the same general trend in the results as the
overlap metric. The scores are lower since the
metric is much stricter.
Table 3 shows the results of opinion relation ex-
traction using the overlap metric. We compare our
approach with pipelined baselines in two settings:
one employs relation extraction on 1-best output
of CRF (top half of table) and the other employs
the merged 10-best output of CRF (bottom half of
table). We can see that in general, using merged
10-best CRF outputs boosts the recall while sac-
rificing precision. This is expected since merging
the 10-best CRF outputs favors candidates that are
1646
IS-ABOUT Relation Extraction IS-FROM Relation Extraction
Method P R F1 P R F1
ILP-W/O-ENTITY 49.10 40.48 44.38 44.77 58.24 50.63
ILP-W-SINGLE-RE 63.88 49.35 55.68 53.64 65.02 58.78
ILP-W/O-IMPLICIT-RE 62.00 44.73 51.97 73.23 51.28 60.32
Joint-Model 64.38 51.20 57.04?? 64.97 58.61 61.63?
Table 4: Comparison between our approach and ILP baselines that omit some potentials in our approach.
believed to be more accurate by the CRF predictor.
If CRF makes mistakes, the mistakes will propa-
gate to the relation extraction step. The poor per-
formance on precision further confirms the error
propagation problem in the pipeline approaches.
In contrast, our joint-inference method success-
fully boosts the recall while maintaining reason-
able precision. This demonstrates that joint infer-
ence can effectively leverage the advantage of in-
dividual predictors and limit error propagation.
To demonstrate the effectiveness of different
potentials in our joint inference model, we con-
sider three variants of our ILP formulation that
omit some potentials in the joint inference: one
is ILP-W/O-ENTITY, which extracts opinion rela-
tions without integrating information from opin-
ion entity identification; one is ILP-W-SINGLE-RE,
which focuses on extracting a single opinion re-
lation and ignores the information from the other
relation; the third one is ILP-W/O-IMPLICIT-RE,
which omits the potential for opinion-implicit-arg
relation and assumes every opinion expression is
linked to an explicit argument. The objective func-
tion of ILP-W/O-ENTITY can be represented as
argmax
u
?
k
?
i?O
?
j?Ak
rijuij (2)
which is subject to constraints on uij to enforce
relations to not overlap and limit the maximum
number of relations that can be extracted for each
opinion expression and each argument. For ILP-
W-SINGLE-RE, we simply remove the variables as-
sociated with one opinion relation in the objective
function (1) and constraints. The formulation of
ILP-W/O-IMPLICIT-RE removes the variables as-
sociated with potential ri in the objective function
and corresponding constraints. It can be viewed
as an extension to the ILP approach in Choi et al
(2006) that includes opinion targets and uses sim-
pler ILP formulation with only one parameter and
fewer binary variables and constraints to represent
entity label assignments 11.
11We compared the proposed ILP formulation with the ILP
Table 4 shows the results of these methods on
opinion relation extraction. We can see that with-
out the knowledge of the entity extractor, ILP-
W/O-ENTITY provides poor performance on both
relation extraction tasks. This confirms the effec-
tiveness of leveraging knowledge from entity ex-
tractor and relation extractor. The improvement
yielded by our approach over ILP-W-SINGLE-RE
demonstrates the benefit of jointly optimizing dif-
ferent types of opinion relations. Our approach
also outperforms ILP-W/O-IMPLICIT-RE, which
does not take into account implicit relations. The
results demonstrate that incorporating knowledge
of implicit opinion relations is important.
6 Discussion
We note that the joint inference model yields a
clear improvement on recall but not on precision
compared to the CRF-based baselines. Analyz-
ing the errors, we found that the joint model ex-
tracts comparable number of opinion entities com-
pared to the gold standard, while the CRF-based
baselines extract significantly fewer opinion enti-
ties (around 60% of the number of entities in the
gold standard). With more extracted opinion enti-
ties, the precision is sacriced but recall is boosted
substantially, and overall we see an increase in
F-measure. We also found that a good portion
of errors were made because the generated candi-
dates failed to cover the correct solutions. Recall
that the joint model finds the global optimal solu-
tion over a set of opinion entity and relation can-
didates, which are obtained from the n-best CRF
predictions and constituents in the parse tree that
satisfy certain syntactic patterns. It is possible
that the generated candidates do not contain the
gold standard answers. For example, our model
failed to identify the IS-ABOUT relation (offers,
general aid) from the following sentence Powell
had contacted ... and received offersO1 of [gen-
formulation in Choi et al (2006) on extracting opinion hold-
ers, opinion expressions and IS-FROM relations, and showed
that the proposed ILP formulation performs better on all three
extraction tasks.
1647
eral aid]T1 ... because both the CRF predictor and
syntactic heuristics fail to capture (offers, general
aid) as a potential relation candidate. By applying
simple heuristics such as treating all verbs or verb
phrases as opinion candidates would not help be-
cause it would introduce a large number of nega-
tive candidates and lower the accuracy of relation
extraction (only 52% of the opinion expressions
are verbs or verb phrases and 64% of the opinion
targets are noun or noun phrases in the corpus we
used). Therefore a more effective candidate gen-
eration method is needed to allow more candidates
while limiting the number of negative candidates.
We also observed incorrect parsing to be a cause of
error. We hope to study ways to account for such
errors in our approach as future work.
For computational time, our ILP formulation
can be solved very efficiently using advanced ILP
solvers. In our experiment, using GLPK?s branch-
and-cut solver took 0.2 seconds to produce opti-
mal ILP solutions for 1000 sentences on a machine
with Intel Core 2 Duo CPU and 4GB RAM.
7 Conclusion
In this paper we propose a joint inference ap-
proach for extracting opinion-related entities and
opinion relations. We decompose the task into
different subproblems, and jointly optimize them
using constraints that aim to encourage their con-
sistency and reduce prediction uncertainty. We
show that our approach can effectively integrate
knowledge from different predictors and achieve
significant improvements in overall performance
for opinion extraction. For future work, we plan to
extend our model to handle more complex opinion
relations, e.g. nesting or cross-sentential relations.
This can be potentially addressed by incorporat-
ing more powerful predictors and more complex
linguistic constraints.
Acknowledgments
This work was supported in part by DARPA-BAA-
12-47 DEFT grant 12475008 and NSF grant BCS-
0904822. We thank Igor Labutov for helpful dis-
cussion and suggestions, Ainur Yessenalina for
early discussion of the work, as well as the reviews
for helpful comments.
References
E. Breck, Y. Choi, and C. Cardie. 2007. Identifying
expressions of opinion in context. In Proceedings of
the 20th international joint conference on Artifical
intelligence, pages 2683?2688. Morgan Kaufmann
Publishers Inc.
Yejin Choi, Claire Cardie, Ellen Riloff, and Siddharth
Patwardhan. 2005. Identifying sources of opin-
ions with conditional random fields and extraction
patterns. In Proceedings of the conference on Hu-
man Language Technology and Empirical Methods
in Natural Language Processing, pages 355?362.
Association for Computational Linguistics.
Y. Choi, E. Breck, and C. Cardie. 2006. Joint ex-
traction of entities and relations for opinion recog-
nition. In Proceedings of the 2006 Conference on
Empirical Methods in Natural Language Process-
ing, pages 431?439. Association for Computational
Linguistics.
D. Das, A.F.T. Martins, and N.A. Smith. 2012. An
exact dual decomposition algorithm for shallow se-
mantic parsing with constraints. Proceedings of*
SEM.[ii, 10, 50].
Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh, Xiang-
Rui Wang, and Chih-Jen Lin. 2008. Liblinear: A
library for large linear classification. The Journal of
Machine Learning Research, 9:1871?1874.
M. Hu and B. Liu. 2004. Mining opinion features
in customer reviews. In Proceedings of the Na-
tional Conference on Artificial Intelligence, pages
755?760. Menlo Park, CA; Cambridge, MA; Lon-
don; AAAI Press; MIT Press; 1999.
Richard Johansson and Alessandro Moschitti. 2010a.
Reranking models in fine-grained opinion analysis.
In Proceedings of the 23rd International Conference
on Computational Linguistics, pages 519?527. As-
sociation for Computational Linguistics.
Richard Johansson and Alessandro Moschitti. 2010b.
Syntactic and semantic structure for opinion ex-
pression detection. In Proceedings of the Four-
teenth Conference on Computational Natural Lan-
guage Learning, pages 67?76. Association for Com-
putational Linguistics.
Richard Johansson and Alessandro Moschitti. 2012.
Relational features in fine-grained opinion analysis.
Soo-Min Kim and Eduard Hovy. 2006. Extracting
opinions, opinion holders, and topics expressed in
online news media text. In Proceedings of the Work-
shop on Sentiment and Subjectivity in Text, pages 1?
8. Association for Computational Linguistics.
N. Kobayashi, K. Inui, and Y. Matsumoto. 2007.
Extracting aspect-evaluation and aspect-of relations
in opinion mining. In Proceedings of the 2007
Joint Conference on Empirical Methods in Natural
Language Processing and Computational Natural
1648
Language Learning (EMNLP-CoNLL), pages 1065?
1074.
John Lafferty, Andrew McCallum, and Fernando CN
Pereira. 2001. Conditional random fields: Prob-
abilistic models for segmenting and labeling se-
quence data.
K. Liu, L. Xu, and J. Zhao. 2012. Opinion target
extraction using word-based translation model. In
Proceedings of the conference on Empirical Meth-
ods in Natural Language Processing. Association
for Computational Linguistics.
Bing Liu. 2012. Sentiment analysis and opinion min-
ing. Synthesis Lectures on Human Language Tech-
nologies, 5(1):1?167.
Bo Pang and Lillian Lee. 2008. Opinion mining and
sentiment analysis. Now Pub.
V. Punyakanok, D. Roth, and W. Yih. 2008. The im-
portance of syntactic parsing and inference in se-
mantic role labeling. Computational Linguistics,
34(2):257?287.
Guang Qiu, Bing Liu, Jiajun Bu, and Chun Chen.
2009. Expanding domain sentiment lexicon through
double propagation. In Proceedings of the 21st
international jont conference on Artifical intelli-
gence, pages 1199?1204. Morgan Kaufmann Pub-
lishers Inc.
G. Qiu, B. Liu, J. Bu, and C. Chen. 2011. Opinion
word expansion and target extraction through double
propagation. Computational linguistics, 37(1):9?
27.
R. Quirk, S. Greenbaum, G. Leech, J. Svartvik, and
D. Crystal. 1985. A comprehensive grammar of
the English language, volume 397. Cambridge Univ
Press.
D. Roth and W. Yih. 2004. A linear programming
formulation for global inference in natural language
tasks. Defense Technical Information Center.
J. Ruppenhofer, S. Somasundaran, and J. Wiebe. 2008.
Finding the sources and targets of subjective expres-
sions. In Proceedings of LREC.
Vivek Srikumar and Dan Roth. 2011. A joint model
for extended semantic role labeling. In Proceedings
of the Conference on Empirical Methods in Natural
Language Processing, pages 129?139. Association
for Computational Linguistics.
V. Stoyanov and C. Cardie. 2008. Topic identification
for fine-grained opinion analysis. In Proceedings
of the 22nd International Conference on Computa-
tional Linguistics-Volume 1, pages 817?824. Asso-
ciation for Computational Linguistics.
J. Wiebe, T. Wilson, and C. Cardie. 2005. Annotating
expressions of opinions and emotions in language.
Language Resources and Evaluation, 39(2):165?
210.
Theresa Wilson, Janyce Wiebe, and Paul Hoffmann.
2009. Recognizing contextual polarity: An explo-
ration of features for phrase-level sentiment analy-
sis. Computational linguistics, 35(3):399?433.
Theresa Wilson. 2008. Fine-Grained Subjectivity
Analysis. Ph.D. thesis, Ph. D. thesis, University of
Pittsburgh. Intelligent Systems Program.
Y. Wu, Q. Zhang, X. Huang, and L. Wu. 2009. Phrase
dependency parsing for opinion mining. In Proceed-
ings of the 2009 Conference on Empirical Methods
in Natural Language Processing: Volume 3-Volume
3, pages 1533?1541. Association for Computational
Linguistics.
B. Yang and C. Cardie. 2012. Extracting opinion
expressions with semi-markov conditional random
fields. In Proceedings of the conference on Empiri-
cal Methods in Natural Language Processing. Asso-
ciation for Computational Linguistics.
Lei Zhang, Bing Liu, Suk Hwan Lim, and Eamonn
O?Brien-Strain. 2010. Extracting and ranking prod-
uct features in opinion documents. In Proceedings
of the 23rd International Conference on Computa-
tional Linguistics: Posters, pages 1462?1470. Asso-
ciation for Computational Linguistics.
1649
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 217?221,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
TopicSpam: a Topic-Model-Based Approach for Spam Detection
Jiwei Li , Claire Cardie
School of Computer Science
Cornell University
Ithaca, NY, 14853
jl3226@cornell.edu
cardie@cs.cornell.edu
Sujian Li
Laboratory of Computational Linguistics
Peking University
Bejing, P.R.China, 150001
lisujian@pku.edu.cn
Abstract
Product reviews are now widely used by
individuals and organizations for decision
making (Litvin et al, 2008; Jansen, 2010).
And because of the profits at stake, peo-
ple have been known to try to game the
system by writing fake reviews to promote
target products. As a result, the task of de-
ceptive review detection has been gaining
increasing attention. In this paper, we pro-
pose a generative LDA-based topic mod-
eling approach for fake review detection.
Our model can aptly detect the subtle dif-
ferences between deceptive reviews and
truthful ones and achieves about 95% ac-
curacy on review spam datasets, outper-
forming existing baselines by a large mar-
gin.
1 Introduction
Consumers rely increasingly on user-generated
online reviews to make purchase decisions. Pos-
itive opinions can result in significant financial
gains. This gives rise to deceptive opinion spam
(Ott et al, 2011; Jindal et al, 2008), fake reviews
written to sound authentic and deliberately mis-
lead readers. Previous research has shown that
humans have difficulty distinguishing fake from
truthful reviews, operating for the most part at
chance (Ott et al, 2011). Consider, for example,
the following two hotel reviews. One is truthful
and the other is deceptive1:
1. My husband and I stayed for two nights at the Hilton
Chicago. We were very pleased with the accommoda-
tions and enjoyed the service every minute of it! The
bedrooms are immaculate, and the linens are very soft.
We also appreciated the free wifi, as we could stay in
touch with friends while staying in Chicago. The bath-
room was quite spacious, and I loved the smell of the
shampoo they provided. Their service was amazing,
1The first example is a deceptive review.
and we absolutely loved the beautiful indoor pool. I
would recommend staying here to anyone.
2. We stayed at the Sheraton by Navy Pier the first week-
end of November. The view from both rooms was spec-
tacular (as you can tell from the picture attached). They
also left a plate of cookies and treats in the kids room
upon check-in made us all feel very special. The hotel
is central to both Navy Pier and Michigan Ave. so we
walked, trolleyed, and cabbed all around the area. We
ate the breakfast buffet on both mornings and thought
it was pretty good. The eggs were a little runny. Our
six year old ate free and our two eleven year old were
$14 (instead of the adult $20). The rooms were clean,
the concierge and reception staff were both friendly
and helpful...we will definitely visit this Sheraton again
when we stay in Chicago next time.
Because of the difficulty of recognizing deceptive
opinions, there has been a widespread and growing
interest in developing automatic, usually learning-
based methods to help users identify deceptive re-
views (Ott et al, 2011; Jindal et al, 2008; Jindal
et al, 2010; Li et al, 2011; Lim et al, 2011; Wang
et al, 2011).
The state-of-the-art approach treats the task of
spam detection as a text categorization prob-
lem and was first introduced by Jindal and Liu
(2009) who trained a supervised classifier to dis-
tinguish duplicated reviews (assumed deceptive)
from original ones (assumed truthful). Since then,
many supervised approaches have been proposed
for spam detection. Ott et al (2011) employed
standard word and part-of-speech (POS) n-gram
features for supervised learning and built a gold?
standard opinion dataset of 800 reviews. Lim et
al. (2010) proposed the inclusion of user behavior-
based features and found that behavior abnormali-
ties of reviewers could predict spammers, without
using any textual features. Li et al (2011) care-
fully explored review-related features based on
content and sentiment, training a semi-supervised
classifier for opinion spam detection. However,
the disadvantages of standard supervised learning
methods are obvious. First, they do not gener-
ally provide readers with a clear probabilistic pre-
217
diction of how likely a review is to be deceptive
vs. truthful. Furthermore, identifying features that
provide direct evidence against deceptive reviews
is always a hard problem.
LDA topic models (Blei et al, 2003) have
widely been used for their ability to model latent
topics in document collection. In LDA, each docu-
ment is presented as a mixture distribution of top-
ics and each topic is presented as a mixture distri-
bution of words. Researchers also integrated dif-
ferent levels of information into LDA topic mod-
els to model the specific knowledge that they are
interested in, such as user-specific information
(Rosen-zvi et al, 2006), document-specific infor-
mation (Li et al, 2010) and time-specific infor-
mation (Diao et al, 2012). Ramage et al (2009)
developed a Labeled LDA model to define a one-
to-one correspondence between LDA latent topics
and tags. Chemudugunta et al (2008) illustrated
that by considering background information and
document-specific information, we can largely im-
prove the performance of topic modeling.
In this paper, we propose a Bayesian approach
called TopicSpam for deceptive review detection.
Our approach, which is a variation of Latent
Dirichlet Allocation (LDA) (Blei et al, 2003),
aims to detect the subtle differences between the
topic-word distributions of deceptive reviews vs.
truthful ones. In addition, our model can give
a clear probabilistic prediction on how likely a
review should be treated as deceptive or truth-
ful. Performance is tested on dataset from Ott et
al.(2011) that contains 800 reviews of 20 Chicago
hotels. Our model achieves more than 94% accu-
racy on that dataset.
2 TopicSpam
We are presented with four subsets of ho-
tel reviews, M = {Mi}i=4i=1, representing
deceptive train, truthful train, deceptive test
and truthful test data, respectively. Each re-
view r is comprised of a number of words r =
{wt}t=nrt=1 . Input for the TopicSpam algorithm is
the datasets M ; output is the label (deceptive,
truthful) for each review inM3 andM4. V denotes
vocabulary size.
2.1 Details of TopicSpam
In TopicSpam, each document is modeled as a
bag of words, which are assumed to be gener-
ated from a mixture of latent topics. Each word
is associated with a latent variable that specifies
Figure 1: Graphical Model for TopicSpam
the topic from which it is generated. Words in a
document are assumed to be conditionally inde-
pendent given the hidden topics. A general back-
ground distribution ?B and hotel-specific distri-
butions ?Hj (j = 1, ..., 20) are first introduced
to capture the background information and hotel-
specific information. To capture the difference
between deceptive reviews and truthful reviews,
TopicSpam also learns a deceptive topic distribu-
tion ?D and truthful topic distribution ?T . The
generative model of TopicSpam is shown as fol-
lows:
? For a training review in r1j ? M1, words are
originated from one of the three different top-
ics: ?B , ?Hj and ?D.
? For a training review in r2j ? M2, words are
originated from one of the three different top-
ics: ?B , ?Hj and ?T .
? For a test review in rmj ? Mm,m = 3, 4,
words are originated from one of the four dif-
ferent topics: ?B , ?Hj ?D and ?T .
The generation process of TopicSpam is shown
in Figure 1 and the corresponding graphical
model is illustrated in Figure 2. We use
? = (?G, ?Hi , ?D, ?T ) to represent the asym-
metric priors for topic-word distribution genera-
tion. In our experiments, we set ?G = 0.1,
and ?Hi = ?D = ?T = 0.01. The intu-
ition for the asymmetric priors is that there should
be more words assigned to the background topic.
? = [?B, ?Hi , ?D, ?T ] denotes the priors for
the document-level topic distribution in the LDA
model. We set ?B = 2 and ?T = ?D = ?Hi = 1,
reflecting the intuition that more words in each
document should cover the background topic.
2.2 Inference
We adopt the collapsed Gibbs sampling strategy to
infer the latent parameters in TopicSpam. In Gibbs
218
1. sample ?G ? Dir(?G)
2. sample ?D ? Dir(?D)
3. sample ?T ? Dir(?T )
4. for each hotel j ? [1, N ]: sample ?Hj ? ?H
5. for each review r
if i=1: sample ?r ? Dir(?B, ?Hj , ?D)
if i=2: sample ?r ? Dir(?B, ?Hj , ?T )
if i=3: sample ?r ? Dir(?B, ?Hj , ?D, ?T )
if i=4: sample ?r ? Dir(?B, ?Hj , ?D, ?T )
for each word w in R
sample z ? ?r sample w ? ?z
Figure 2: Generative Model for TopicSpam
sampling, for each word w in review r, we need
to calculate P (zw|w, z?w, ?, ?) in each iteration,
where z?w denotes the topic assignments for all
words except that of the current word zw.
P (zw = m|z?w, i, j, ?, ?)
Nmr + ?m?
m?(Nm
?
r + ??m)
? E
w
m + ?m?V
w? Ewm + V ?m
(1)
where Nmr denotes the number of times that topic
m appears in current review r and Ewm denotes the
number of times that word w is assigned to topic
m. After each sampling iteration, the latent pa-
rameters can be estimated using the following for-
mulas:
?mr =
Nmr + ?m?
m?(Nm
?
r + ?m)
?(w)m =
Ewm + ?m?
w? Ew
?
m + V ?m(2)
2.3 Labeling the Test Data
For each review r in the test data, let NDr denote
the number of words generated from the decep-
tive topic and NTr , the number of words generated
from the truthful topic. The decision for whether a
review is deceptive or truthful is made as follows:
? if NDr > NTr , r is deceptive.
? if NDr < NTr , r is truthful.
? if NDr = NTr , it is hard to decide.
Let P(D) denote the probability that r is deceptive
and P(T) denote the probability that r is truthful.
P (D) = N
D
r
NDr +NTr
P (T ) = N
T
r
NDr +NTr
(3)
3 Experiments
3.1 System Description
Our experiments are conducted on the dataset
from Ott et al(2011), which contains reviews of
the 20 most popular hotels on TripAdvisor in the
Chicago areas. There are 20 truthful and 20 decep-
tive reviews for each of the chosen hotels (800 re-
views total). Deceptive reviews are gathered using
Amazon Mechanical Turk2. In our experiments,
we adopt the same 5-fold cross-validation strat-
egy as in Ott et al, using the same data partitions.
Words are stemmed using PorterStemmer3.
3.2 Baselines
We employ a number of techniques as baselines:
TopicTD: A topic-modeling approach that only
considers two topics: deceptive and truthful.
Words in deceptive train are all generated from
the deceptive topic and words in truthful train
are generated from the truthful topic. Test docu-
ments are presented with a mixture of the decep-
tive and truthful topics.
TopicTDB: A topic-modeling approach that
only considers background, deceptive and truthful
information.
SVM-Unigram: Using SVMlight(Joachims,
1999) to train linear SVM models on unigram fea-
tures.
SVM-Bigram: Using SVMlight(Joachims,
1999) to train linear SVM models on bigram fea-
tures.
SVM-Unigram-Removal1: In SVM-Unigram-
Removal, we first train TopicSpam. Then words
generated from hotel-specific topics are removed.
We use the remaining words as features in SVM-
light.
SVM-Unigram-Removal2: Same as SVM-
Unigram-removal-1 but removing all background
words and hotel-specific words.
Experimental results are shown in Table 14.
As we can see, the accuracy of TopicSpam is
0.948, outperforming TopicTD by 6.4%. This il-
lustrates the effectiveness of modeling background
and hotel-specific information for the opinion
spam detection problem. We also see that Top-
icSpam slightly outperforms TopicTDB, which
2https://www.mturk.com/mturk/.
3http://tartarus.org/martin/PorterStemmer/
4Reviews with NDr = NTr are regarded as incorrectly
classified by TopicSpam.
219
Approach Accuracy T-P T-R T-F D-P D-R D-F
TopicSpam 0.948 0.954 0.942 0.944 0.941 0.952 0.946
TopicTD 0.888 0.901 0.878 0.889 0.875 0.897 0.886
TopicTDB 0.931 0.938 0.926 0.932 0.925 0.937 0.930
SVM-Unigram 0.884 0.899 0.865 0.882 0.870 0.903 0.886
SVM-Bigram 0.896 0.901 0.890 0.896 0.891 0.903 0.897
SVM-Unigram-Removal1 0.895 0.906 0.889 0.898 0.887 0.907 0.898
SVM-Unigram-Removal2 0.822 0.852 0.806 0.829 0.793 0.840 0.817
Table 1: Performance for different approaches based on nested 5-fold cross-validation experiments.
neglects hotel-specific information. By check-
ing the results of Gibbs sampling, we find that
this is because only a small number of words
are generated by the hotel-specific topics. Top-
icTD and SVM-Unigram get comparative accu-
racy rates. This can be explained by the fact
that both models use unigram frequency as fea-
tures for the classifier or topic distribution train-
ing. SVM-Unigram-Removal1 is also slightly
better than SVM-Unigram. In SVM-Unigram-
removal1, hotel-specific words are removed for
classifier training. So the first-step LDA model
can be viewed as a feature selection process for the
SVM, giving rise to better results. We can also see
that the performance of SVM-Unigram-removal2
is worse than other baselines. This can be ex-
plained as follows: for example, word ?my? has
large probability to be generated from the back-
ground topic. However it can also be generated by
deceptive topic occasionaly but can hardly be gen-
erated from the truthful topic. So the removal of
these words results in the loss of useful informa-
tion, and leads to low accuracy rate.
Our topic-modeling approach uses word fre-
quency as features and does not involve any fea-
ture selection process. Here we present the re-
sults of the sample reviews from Section 1. Stop
words are labeled in black, background topics (B)
in blue, hotel specific topics (H) in orange, de-
ceptive topics (D) in red and truthful topic (T) in
green.
1. My husband and I stayed for two nights at the Hilton
Chicago. We were very pleased with the accommoda-
tions and enjoyed the service every minute of it! The
bedrooms are immaculate,and the linens are very soft.
We also appreciated the free wifi, as we could stay in
touch with friends while staying in Chicago. The bath-
room was quite spacious, and I loved the smell of the
shampoo they provided not like most hotel shampoos.
Their service was amazing,and we absolutely loved the
beautiful indoor pool. I would recommend staying here
to anyone.
[B,H,D,T]=[41,6,10,1] p(D)=0.909 P(T)=0.091
2. We stayed at the Sheraton by Navy Pier the first week-
end of November. The view from both rooms was spec-
tacular (as you can tell from the picture attached). They
also left a plate of cookies and treats in the kids room
upon check-in made us all feel very special. The ho-
tel is central to both Navy Pier and Michigan Ave. so
we walked, trolleyed, and cabbed all around the area.
We ate the breakfast buffet both mornings and thought
it was pretty good. The eggs were a little runny. Our
six year old ate free and our two eleven year old were
$14 ( instead of the adult $20) The rooms were clean,
the concierge and reception staff were both friendly
and helpful...we will definitely visit this Sheraton again
when we?re in Chicago next time.
[B,H,D,T]=[80,15,3,18] p(D)=0.143 P(T)=0.857
background deceptive truthful Hilton
hotel hotel room Hilton
stay my ) palmer
we chicago ( millennium
room will but lockwood
! room $ park
Chicago very bathroom lobby
my visit location line
great husband night valet
I city walk shampoo
very experience park dog
Omni Amalfi Sheraton James
Omni Amalfi tower James
pool breakfast Sheraton service
plasma view pool spa
sundeck floor river bar
chocolate bathroom lake upgrade
indoor cocktail navy primehouse
request morning indoor design
pillow wine shower overlook
suitable great kid romantic
area room theater home
Table 2: Top words in different topics from Topic-
Spam
4 Conclusion
In this paper, we propose a novel topic model
for deceptive opinion spam detection. Our model
achieves an accuracy of 94.8%, demonstrating its
effectiveness on the task.
5 Acknowledgements
We thank Myle Ott for his insightful comments and sugges-
tions. This work was supported in part by NSF Grant BCS-
0904822, a DARPA Deft grant, and by a gift from Google.
220
References
David Blei, Andrew Ng and Micheal Jordan. Latent
Dirichlet alocation. 2003. In Journal of Machine
Learning Research.
Carlos Castillo, Debora Donato, Luca Becchetti, Paolo
Boldi, Stefano Leonardi Massimo Santini, and Se-
bastiano Vigna. A reference collection for web
spam. In Proceedings of annual international ACM
SIGIR conference on Research and development in
information retrieval, 2006.
Chaltanya Chemudugunta, Padhraic Smyth and Mark
Steyers. Modeling General and Specific Aspects of
Documents with a Probabilistic Topic Model.. In
Advances in Neural Information Processing Systems
19: Proceedings of the 2006 Conference.
Paul-Alexandru Chirita, Jorg Diederich, and Wolfgang
Nejdl. MailRank: using ranking for spam detection.
In Proceedings of ACM international conference on
Information and knowledge management. 2005.
Harris Drucke, Donghui Wu, and Vladimir Vapnik.
2002. Support vector machines for spam categoriza-
tion. In Neural Networks.
Qiming Diao, Jing Jiang, Feida Zhu and Ee-Peng Lim.
In Proceeding of the 50th Annual Meeting of the As-
sociation for Computational Linguistics. 2012
Thorsten Joachims. 1999. Making large-scale support
vector machine learning practical. In Advances in
kernel methods.
Jack Jansen. 2010. Online product research. In Pew In-
ternet and American Life Project Report.
Nitin Jindal, and Bing Liu. Opinion spam and analysis.
2008. In Proceedings of the international conference
on Web search and web data mining
Nitin Jindal, Bing Liu, and Ee-Peng Lim. Finding
Unusual Review Patterns Using Unexpected Rules.
2010. In Proceedings of the 19th ACM international
conference on Information and knowledge manage-
ment
Pranam Kolari, Akshay Java, Tim Finin, Tim Oates and
Anupam Joshi. Detecting Spam Blogs: A Machine
Learning Approach. In Proceedings of Association
for the Advancement of Artificial Intelligence. 2006.
Peng Li, Jing Jiang and Yinglin Wang. 2010. Gener-
ating templates of entity summaries with an entity-
aspect model and pattern mining. In Proceedings of
the 48th Annual Meeting of the Association for Com-
putational Linguistics.
Fangtao Li, Minlie Huang, Yi Yang, and Xiaoyan Zhu.
Learning to identify review Spam. 2011. In Proceed-
ings of the Twenty-Second international joint confer-
ence on Artificial Intelligence.
Ee-Peng Lim, Viet-An Nguyen, Nitin Jindal, Bing Liu,
and Hady Wirawan Lauw. Detecting Product Re-
view Spammers Using Rating Behavior. 2010. In
Proceedings of the 19th ACM international confer-
ence on Information and knowledge management.
Stephen Litvina, Ronald Goldsmithb and Bing Pana.
2008. Electronic word-of-mouth in hospitality
and tourism management. Tourism management,
29(3):458468.
Juan Martinez-Romo and Lourdes Araujo. Web Spam
Identification Through Language Model Analysis.
In AIRWeb. 2009.
Arjun Mukherjee, Bing Liu and Natalie Glance. Spot-
ting Fake Reviewer Groups in Consumer Reviews.
In Proceedings of the 18th international conference
on World wide web, 2012.
Alexandros Ntoulas, Marc Najork, Mark Manasse and
Dennis Fetterly. Detecting Spam Web Pages through
Content Analysis. In Proceedings of international
conference on World Wide Web 2006
Myle Ott, Yejin Choi, Claire Cardie and Jeffrey Han-
cock. Finding deceptive opinion spam by any stretch
of the imagination. 2011. In Proceedings of the 49th
Annual Meeting of the Association for Computa-
tional Linguistics: Human Language Technologies
Bo Pang and Lillian Lee. Opinion mining and senti-
ment analysis. In Found. Trends Inf. Retr.
Daniel Ramage, David Hall, Ramesh Nallapati and
Christopher D. Manning. Labeled LDA: a super-
vised topic model for credit attribution in multi-
labeled corpora. 2009. In Proceedings of the 2009
Conference on Empirical Methods in Natural Lan-
guage Processing 2009.
Michal Rosen-zvi, Thomas Griffith, Mark Steyvers and
Padhraic Smyth. The author-topic model for authors
and documents. In Proceedings of the 20th confer-
ence on Uncertainty in artificial intelligence.
Guan Wang, Sihong Xie, Bing Liu and Philip Yu. Re-
view Graph based Online Store Review Spammer
Detection. 2011. In Proceedings of 11th Interna-
tional Conference of Data Mining.
Baoning Wu, Vinay Goel and Brian Davison. Topical
TrustRank: using topicality to combat Web spam.
In Proceedings of international conference on World
Wide Web 2006 .
Kyang Yoo and Ulrike Gretzel. 2009. Compari-
son of Deceptive and Truthful Travel Reviews.
InInformation and Communication Technologies in
Tourism 2009.
221
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 325?335,
Baltimore, Maryland, USA, June 23-25 2014.
c
?2014 Association for Computational Linguistics
Context-aware Learning for Sentence-level Sentiment Analysis
with Posterior Regularization
Bishan Yang
Department of Computer Science
Cornell University
bishan@cs.cornell.edu
Claire Cardie
Department of Computer Science
Cornell University
cardie@cs.cornell.edu
Abstract
This paper proposes a novel context-aware
method for analyzing sentiment at the
level of individual sentences. Most ex-
isting machine learning approaches suf-
fer from limitations in the modeling of
complex linguistic structures across sen-
tences and often fail to capture non-
local contextual cues that are important
for sentiment interpretation. In contrast,
our approach allows structured modeling
of sentiment while taking into account
both local and global contextual infor-
mation. Specifically, we encode intu-
itive lexical and discourse knowledge as
expressive constraints and integrate them
into the learning of conditional random
field models via posterior regularization.
The context-aware constraints provide ad-
ditional power to the CRF model and can
guide semi-supervised learning when la-
beled data is limited. Experiments on
standard product review datasets show that
our method outperforms the state-of-the-
art methods in both the supervised and
semi-supervised settings.
1 Introduction
The ability to extract sentiment from text is cru-
cial for many opinion-mining applications such as
opinion summarization, opinion question answer-
ing and opinion retrieval. Accordingly, extract-
ing sentiment at the fine-grained level (e.g. at the
sentence- or phrase-level) has received increasing
attention recently due to its challenging nature and
its importance in supporting these opinion analysis
tasks (Pang and Lee, 2008).
In this paper, we focus on the task of sentence-
level sentiment classification in online reviews.
Typical approaches to the task employ supervised
machine learning algorithms with rich features
and take into account the interactions between
words to handle compositional effects such as po-
larity reversal (e.g. (Nakagawa et al, 2010;
Socher et al, 2013)). Still, their methods can en-
counter difficulty when the sentence on its own
does not contain strong enough sentiment signals
(due to the lack of statistical evidence or the re-
quirement for background knowledge). Consider
the following review for example,
1. Hearing the music in real stereo is a true reve-
lation. 2. You can feel that the music is no longer
constrained by the mono recording. 3. In fact, it
is more like the players are performing on a stage
in front of you ...
Existing feature-based classifiers may be effective
in identifying the positive sentiment of the first
sentence due to the use of the word revelation,
but they could be less effective in the last two sen-
tences due to the lack of explicit sentiment signals.
However, if we examine these sentences within the
discourse context, we can see that: the second sen-
tence expresses sentiment towards the same aspect
? the music ? as the first sentence; the third sen-
tence expands the second sentence with the dis-
course connective In fact. These discourse-level
relations help indicate that sentence 2 and 3 are
likely to have positive sentiment as well.
The importance of discourse for sentiment anal-
ysis has become increasingly recognized. Most
existing work considers discourse relations be-
tween adjacent sentences or clauses and incor-
porates them as constraints (Kanayama and Na-
sukawa, 2006; Zhou et al, 2011) or features in
classifiers Trivedi and Eisenstein (2013; Lazari-
dou et al (2013). Very little work has explored
long-distance discourse relations for sentiment
analysis. Somasundaran et al (2008) defines
coreference relations on opinion targets and ap-
plies them to constrain the polarity of sentences.
325
However, the discourse relations were obtained
from fine-grained annotations and implemented as
hard constraints on polarity.
Obtaining sentiment labels at the fine-grained
level is costly. Semi-supervised techniques have
been proposed for sentence-level sentiment classi-
fication (T?ackstr?om and McDonald, 2011a; Qu et
al., 2012). However, they rely on a large amount
of document-level sentiment labels that may not
be naturally available in many domains.
In this paper, we propose a sentence-level senti-
ment classification method that can (1) incorporate
rich discourse information at both local and global
levels; (2) encode discourse knowledge as soft
constraints during learning; (3) make use of un-
labeled data to enhance learning. Specifically, we
use the Conditional Random Field (CRF) model
as the learner for sentence-level sentiment classi-
fication, and incorporate rich discourse and lexi-
cal knowledge as soft constraints into the learn-
ing of CRF parameters via Posterior Regulariza-
tion (PR) (Ganchev et al, 2010). As a framework
for structured learning with constraints, PR has
been successfully applied to many structural NLP
tasks (Ganchev et al, 2009; Ganchev et al, 2010;
Ganchev and Das, 2013). Our work is the first to
explore PR for sentiment analysis. Unlike most
previous work, we explore a rich set of structural
constraints that cannot be naturally encoded in the
feature-label form, and show that such constraints
can improve the performance of the CRF model.
We evaluate our approach on the sentence-
level sentiment classification task using two stan-
dard product review datasets. Experimental re-
sults show that our model outperforms state-of-
the-art methods in both the supervised and semi-
supervised settings. We also show that dis-
course knowledge is highly useful for improving
sentence-level sentiment classification.
2 Related Work
There has been a large amount of work on sen-
timent analysis at various levels of granular-
ity (Pang and Lee, 2008). In this paper, we focus
on the study of sentence-level sentiment classifi-
cation. Existing machine learning approaches for
the task can be classified based on the use of two
ideas. The first idea is to exploit sentiment sig-
nals at the sentence level by learning the relevance
of sentiment and words while taking into account
the context in which they occur: Nakagawa et
al. (2010) uses tree-CRF to model word interac-
tions based on dependency tree structures; Choi
and Cardie (2008) applies compositional inference
rules to handle polarity reversal; Socher et al
(2011) and Socher et al (2013) compute composi-
tional vector representations for words and phrases
and use them as features in a classifier.
The second idea is to exploit sentiment signals
at the inter-sentential level. Polanyi and Zaenen
(2006) argue that discourse structure is important
in polarity classification. Various attempts have
been made to incorporate discourse relations into
sentiment analysis: Pang and Lee (2004) explored
the consistency of subjectivity between neighbor-
ing sentences; Mao and Lebanon (2007),McDon-
ald et al (2007), and T?ackstr?om and McDonald
(2011a) developed structured learning models to
capture sentiment dependencies between adjacent
sentences; Kanayama and Nasukawa (2006) and
Zhou et al (2011) use discourse relations to con-
strain two text segments to have either the same
polarity or opposite polarities; Trivedi and Eisen-
stein (2013) and Lazaridou et al (2013) encode
the discourse connectors as model features in su-
pervised classifiers. Very little work has explored
long-distance discourse relations. Somasundaran
et al (2008) define opinion target relations and ap-
ply them to constrain the polarity of text segments
annotated with target relations. Recently, Zhang
et al (2013) explored the use of explanatory dis-
course relations as soft constraints in a Markov
Logic Network framework for extracting subjec-
tive text segments.
Leveraging both ideas, our approach exploits
sentiment signals from both intra-sentential and
inter-sentential context. It has the advantages of
utilizing rich discourse knowledge at different lev-
els of context and encoding it as soft constraints
during learning.
Our approach is also semi-supervised. Com-
pared to the existing work on semi-supervised
learning for sentence-level sentiment classification
(T?ackstr?om and McDonald, 2011a; T?ackstr?om and
McDonald, 2011b; Qu et al, 2012), our work
does not rely on a large amount of coarse-grained
(document-level) labeled data, instead, distant
supervision mainly comes from linguistically-
motivated constraints.
Our work also relates to the study of posterior
regularization (PR) (Ganchev et al, 2010). PR has
been successfully applied to many structured NLP
326
tasks such as dependency parsing, information ex-
traction and cross-lingual learning tasks (Ganchev
et al, 2009; Bellare et al, 2009; Ganchev et al,
2010; Ganchev and Das, 2013). Most previous
work using PR mainly experiments with feature-
label constraints. In contrast, we explore a rich
set of linguistically-motivated constraints which
cannot be naturally formulated in the feature-label
form. We also show that constraints derived from
the discourse context can be highly useful for dis-
ambiguating sentence-level sentiment.
3 Approach
In this section, we present the details of our pro-
posed approach. We formulate the sentence-level
sentiment classification task as a sequence label-
ing problem. The inputs to the model are sentence-
segmented documents annotated with sentence-
level sentiment labels (positive, negative or neu-
tral) along with a set of unlabeled documents.
During prediction, the model outputs sentiment la-
bels for a sequence of sentences in the test docu-
ment. We utilize conditional random fields and use
Posterior Regularization (PR) to learn their param-
eters with a rich set of context-aware constraints.
In what follows, we first briefly describe the
framework of Posterior Regularization. Then we
introduce the context-aware constraints derived
based on intuitive discourse and lexical knowl-
edge. Finally we describe how to perform learning
and inference with these constraints.
3.1 Posterior Regularization
PR is a framework for structured learning with
constraints (Ganchev et al, 2010). In this work,
we apply PR in the context of CRFs for sentence-
level sentiment classification.
Denote x as a sequence of sentences within a
document and y as a vector of sentiment labels
associated with x. The CRF model the following
conditional probabilities:
p
?
(y|x) =
exp(? ? f(x,y))
Z
?
(x)
where f(x,y) are the model features, ? are the
model parameters, and Z
?
(x) =
?
y
exp(? ?
f(x,y)) is a normalization constant. The objec-
tive function for a standard CRF is to maximize
the log-likelihood over a collection of labeled doc-
uments plus a regularization term:
max
?
L(?) = max
?
?
(x,y)
log p
?
(y|x)?
||?||
2
2
2?
2
PR makes the assumption that the labeled data
we have is not enough for learning good model
parameters, but we have a set of constraints on the
posterior distribution of the labels. We can define
the set of desirable posterior distrbutions as
Q = {q(Y) : E
q
[?(X,Y)] = b} (1)
where ? is a constraint function, b is a vector of
desired values of the expectations of the constraint
functions under the distribution q
1
. Note that the
distribution q is defined over a collection of un-
labeled documents where the constraint functions
apply, and we assume independence between doc-
uments.
The PR objective can be written as the origi-
nal model objective penalized with a regulariza-
tion term, which minimizes the KL-divergence be-
tween the desired model posteriors and the learned
model posteriors with an L2 penalty
2
for the con-
straint violations.
max
?
L(?)?min
q?Q
{KL(q(Y)||p
?
(Y|X))
+ ?||E
q
[?(X,Y)]? b||
2
2
}
(2)
The objective can be optimized by an EM-like
scheme that iteratively solves the minimization
problem and the maximization problem. Solving
the minimization problem is equivalent to solving
its dual since the objective is convex. The dual
problem is
argmax
?
? ? b? logZ
?
(X)?
1
4?
||?||
2
2
(3)
We optimize the objective function 2 using
stochastic projected gradient, and compute the
learning rate using AdaGrad (Duchi et al, 2010).
3.2 Context-aware Posterior Constraints
We develop a rich set of context-aware poste-
rior constraints for sentence-level sentiment anal-
ysis by exploiting lexical and discourse knowl-
edge. Specifically, we construct the lexical con-
straints by extracting sentiment-bearing patterns
1
In general, inequality constraints can also be used. We
focus on the equality constraints since we found them to ex-
press the sentiment-relevant constraints well.
2
Other convex functions can be used for the penalty. We
use L2 norm because it works well in practice. ? is a regular-
ization constant
327
within sentences and construct the discourse-level
constraints by extracting discourse relations that
indicate sentiment coherence or sentiment changes
both within and across sentences. Each constraint
can be formulated as equality between the expec-
tation of a constraint function value and a desired
value set by prior knowledge. The equality is not
strictly enforced (due to the regularization in the
PR objective 2). Therefore all the constraints are
applied as soft constraints. Table 1 provides in-
tuitive description and examples for all the con-
straints used in our model.
Lexical Patterns The existence of a polarity-
carrying word alone may not correctly indicate the
polarity of the sentence, as the polarity can be re-
versed by other polarity-reversing words. We ex-
tract lexical patterns that consist of polar words
and negators
3
, and apply the heuristics based on
compositional semantics (Choi and Cardie, 2008)
to assign a sentiment value to each pattern.
We encode the extracted lexical patterns along
with their sentiment values as feature-label con-
straints. The constraint function can be written as
?
w
(x, y) =
?
i
f
w
(x
i
, y
i
)
where f
w
(x
i
, y
i
) is a feature function which has
value 1 when sentence x
i
contains the lexical pat-
tern w and its sentiment label y
i
equals to the ex-
pected sentiment value and has value 0 otherwise.
The constraint expectation value is set to be the
prior probability of associating w with its senti-
ment value. Note that sentences with neutral senti-
ment can also contain such lexical patterns. There-
fore we allow the lexical patterns to be assigned a
neutral sentiment with a prior probability r
0
(we
compute this value as the empirical probability of
neutral sentiment in the training documents). Us-
ing the polarity indicated by lexical patterns to
constrain the sentiment of sentences is quite ag-
gressive. Therefore we only consider lexical pat-
terns that are strongly discriminative (many opin-
ion words in the lexicon only indicate sentiment
with weak strength). The selected lexical patterns
include a handful of seed patterns (such as ?pros?
and ?cons?) and the lexical patterns that have high
precision (larger then 0.9) of predicting sentiment
in the training data.
3
The polar words are identified using the MPQA lexicon
and the negators are identified using a handful of seed words
extended by the General Inquirer dictionary and WordNet as
described in (Choi and Cardie, 2008).
Discourse Connectives. Lexical patterns can
be limited in capturing contextual information
since they only look at interactions between words
within an expression. To capture context at the
clause or sentence level, we consider discourse
connectives, which are cue phrases or words that
indicate discourse relations between adjacent sen-
tences or clauses. To identify discourse connec-
tives, we apply a discourse tagger trained on the
Penn Discourse Treebank (Prasad et al, 2008)
4
to our data. Discourse connectives are tagged with
four senses: Expansion, Contingency, Compari-
son, Temporal.
Discourse connectives can operate at both intra-
sentential and inter-sentential level. For example,
the word ?although? is often used to connect two
polar clauses within a sentence, while the word
?however? is often used to at the beginning of
the sentence to connect two polar sentences. It
is important to distinguish these two types of dis-
course connectives. We consider a discourse con-
nective to be intra-sentential if it has the Com-
parison sense and connects two polar clauses with
opposite polarities (determined by the lexical pat-
terns). We construct a feature-label constraint for
each intra-sentential discourse connective and set
its expected sentiment value to be neutral.
Unlike the intra-sentential discourse connec-
tives, the inter-sentential discourse connectives
can indicate sentiment transitions between sen-
tences. Intuitively, discourse connectives with
the senses of Expansion (e.g. also, for example,
furthermore) and Contingency (e.g. as a result,
hence, because) are likely to indicate sentiment
coherence; discourse connectives with the sense
of Comparison (e.g. but, however, nevertheless)
are likely to indicate sentiment changes. This in-
tuition is reasonable but it assumes the two sen-
tences connected by the discourse connective are
both polar sentences. In general, discourse con-
nectives can also be used to connect non-polar
(neutral) sentences. Thus it is hard to directly
constrain the posterior expectation for each type
of sentiment transitions using inter-sentential dis-
course connectives.
Instead, we impose constraints on the model
posteriors by reducing constraint violations. We
4
http://www.cis.upenn.edu/
?
epitler/
discourse.html
328
Types Description and Examples Inter-sentential
Lexical patterns
The sentence containing a polar lexical pattern w tends to have the polarity
indicated by w. Example lexical patterns are annoying, hate, amazing, not dis-
appointed, no concerns, favorite, recommend.
Discourse Connectives
(clause)
The sentence containing a discourse connective cwhich connects its two clauses
that have opposite polarities indicated by the lexical patterns tends to have neu-
tral sentiment. Example connectives are while, although, though, but.
Discourse Connectives
(sentence)
Two adjacent sentences which are connected by a discourse connective c tends
to have the same polarity if c indicates a Expansion or Contingency relation,
e.g. also, for example, in fact, because ; opposite polarities if c indicates a
Comparison relation, e.g. otherwise, nevertheless, however.
X
Coreference
The sentences which contain coreferential entities appeared as targets of opinion
expressions tend to have the same polarity.
X
Listing patterns
A series of sentences connected via a listing tend to have the same polarity.
X
Global labels
The sentence-level polarity tends to be consistent with the document-level po-
larity.
X
Table 1: Summarization of Posterior Constraints for Sentence-level Sentiment Classification
define the following constraint function:
?
c,s
(x, y) =
?
i
f
c,s
(x
i
, y
i
, y
i?1
)
where c denotes a discourse connective, s indi-
cates its sense, and f
c,s
is a penalty function that
takes value 1.0 when y
i
and y
i?1
form a contradic-
tory sentiment transition, that is, y
i
6=
polar
y
i?1
if
s ? {Expansion,Contingency}, or y
i
=
polar
y
i?1
if s = Comparison. The desired value for the con-
straint expectation is set to 0 so that the model is
encouraged to have less constraint violations.
Opinion Coreference Sentences in a discourse
can be linked by many types of coherence rela-
tions (Jurafsky et al, 2000). Coreference is one
of the commonly used relations in written text.
In this work, we explore coreference in the con-
text of sentence-level sentiment analysis. We con-
sider a set of polar sentences to be linked by the
opinion coreference relation if they contain core-
ferring opinion-related entities. For example, the
following sentences express opinions towards ?the
speaker phone?, ?The speaker phone? and ?it? re-
spectively. As these opinion targets are corefer-
ential (referring to the same entity ?the speaker
phone?), they are linked by the opinion corefer-
ence relation
5
.
My favorite features are the speaker
phone and the radio. The speaker
phone is very functional. I use it in
the car, very audible even with freeway
noise.
5
In general, the opinion-related entities include both the
opinion targets and the opinion holders. In this work, we
only consider the targets since we experiment with single-
author product reviews. The opinion holders can be included
in a similar way as the opinion targets.
Our coreference relations indicated by opinion
targets overlap with the same target relation intro-
duced in (Somasundaran et al, 2009). The dif-
ferences are: (1) we encode the coreference re-
lations as soft constraints during learning instead
of applying them as hard constraints during infer-
ence time; (2) our constraints can apply to both
polar and non-polar sentences; (3) our identifica-
tion of coreference relations is automatic without
any fine-grained annotations for opinion targets.
To extract coreferential opinion targets, we ap-
ply Stanford?s coreference system (Lee et al,
2013) to extract coreferential mentions in the doc-
ument, and then apply a set of syntactic rules to
identify opinion targets from the extracted men-
tions. The syntactic rules correspond to the
shortest dependency paths between an opinion
word and an extracted mention. We consider
the 10 most frequent dependency paths in the
training data. Example dependency paths include
nsubj(opinion, mention), nobj(opinion, mention),
and amod(mention, opinion).
For sentences connected by the opinion coref-
erence relation, we expect their sentiment to be
consistent. To encode this intuition, we define the
following constraint function:
?
coref
(x, y) =
?
i,ant(i)=j,j?0
f
coref
(x
i
, x
j
, y
i
, y
j
)
where ant(i) denotes the index of the sentence
which contains an antecedent target of the target
mentioned in sentence i (the antecedent relations
over pairs of opinion targets can be constructed
using the coreference resolver), and f
coref
is a
penalty function which takes value 1.0 when the
expected sentiment coherency is violated, that is,
y
i
6=
polar
y
j
. Similar to the inter-sentential dis-
329
course connectives, modeling opinion coreference
via constraint violations allows the model to han-
dle neutral sentiment. The expected value of the
constraint functions is set to 0.
Listing Patterns Another type of coherence re-
lations we observe in online reviews is listing,
where a reviewer expresses his/her opinions by
listing a series of statements followed by a se-
quence of numbers. For example, ?1. It?s smaller
than the ipod mini .... 2. It has a removable battery
....?. We expect sentences connected by a listing
to have consistent sentiment. We implement this
constraint in the same form as the coreference con-
straint (the antecedent assignments are constructed
from the numberings).
Global Sentiment Previous studies have
demonstrated the value of document-level sen-
timent in guiding the semi-supervised learning
of sentence-level sentiment (T?ackstr?om and
McDonald, 2011b; Qu et al, 2012). In this work,
we also take into account this information and
encode it as posterior constraints. Note that these
constraints are not necessary for our model and
can be applied when the document-level sentiment
labels are naturally available.
Based on an analysis of the Amazon review
data, we observe that sentence-level sentiment
usually doesn?t conflict with the document-level
sentiment in terms of polarity. For example, the
proportion of negative sentences in the positive
documents is very small compared to the propor-
tion of positive sentences. To encode this intuition,
we define the following constraint function:
?
g
(x, y) =
n
?
i
?(y
i
6=
polar
g)/n
where g ? {positive, negative} denotes the sen-
timent value of a polar document, n is the total
number of sentences in x, and ? is an indicator
function. We hope the expectation of the con-
straint function takes a small value. In our experi-
ments, we set the expected value to be the empiri-
cal estimate of the probability of ?conflicting? sen-
timent in polar documents using the training data.
3.3 Training and Inference
During training, we need to compute the constraint
expectations and the feature expectations under
the auxiliary distribution q at each gradient step.
We can derive q by solving the dual problem in 3:
q(y|x) =
exp(? ? f(x,y) + ? ? ?(x,y))
Z
?,?
(X)
(4)
where Z
?,?
(X) is a normalization constant. Most
of our constraints can be factorized in the same
way as factorizing the model features in the first-
order CRF model, and we can compute the expec-
tations under q very efficiently using the forward-
backward algorithm. However, some of our dis-
course constraints (opinion coreference and list-
ing) can break the tractable structure of the model.
For constraints with higher-order structures, we
use Gibbs Sampling (Geman and Geman, 1984) to
approximate the expectations. Given a sequence
x, we sample a label y
i
at each position i by com-
puting the unnormalized conditional probabilities
p(y
i
= l|y
?i
) ? exp(? ? f(x,y
i
= l,y
?i
) + ? ?
?(x,y
i
= l,y
?i
)) and renormalizing them. Since
the possible label assignments only differ at posi-
tion i, we can make the computation efficient by
maintaining the structure of the coreference clus-
ters and precomputing the constraint function for
different types of violations.
During inference, we find the best label assign-
ment by computing argmax
y
q(y|x). For doc-
uments where the higher-order constraints apply,
we use the same Gibbs sampler as described above
to infer the most likely label assignment, other-
wise, we use the Viterbi algorithm.
4 Experiments
We experimented with two product review
datasets for sentence-level sentiment classifica-
tion: the Customer Review (CR) data (Hu and Liu,
2004)
6
which contains 638 reviews of 14 prod-
ucts such as cameras and cell phones, and the
Multi-domain Amazon (MD) data from the test set
of T?ackstr?om and McDonald (2011a) which con-
tains 294 reivews from 5 different domains. As in
Qu et al (2012), we chose the books, electronics
and music domains for evaluation. Each domain
also comes with 33,000 extra reviews with only
document-level sentiment labels.
We evaluated our method in two settings: su-
pervised and semi-supervised. In the supervised
setting, we treated the test data as unlabeled data
and performed transductive learning. In the semi-
supervised setting, our unlabeled data consists of
6
Available at http://www.cs.uic.edu/
?
liub/
FBS/sentiment-analysis.html.
330
both the available unlabeled data and the test data.
For each domain in the MD dataset, we made
use of no more than 100 unlabeled documents in
which our posterior constraints apply. We adopted
the evaluation schemes used in previous work: 10-
fold cross validation for the CR dataset and 3-fold
cross validation for the MD dataset. We also report
both two-way classification (positive vs. negative)
and three-way classification results (positive, neg-
ative or neutral). We use accuracy as the per-
formance measure. In our tables, boldface num-
bers are statistically significant by paired t-test for
p < 0.05 against the best baseline developed in
this paper
7
.
We trained our model using a CRF incorpo-
rated with the proposed posterior constraints. For
the CRF features, we include the tokens, the part-
of-speech tags, the prior polarities of lexical pat-
terns indicated by the opinion lexicon and the
negator lexicon, the number of positive and neg-
ative tokens and the output of the vote-flip algo-
rithm (Choi and Cardie, 2009). In addition, we in-
clude the discourse connectives as local or transi-
tion features and the document-level sentiment la-
bels as features (only available in the MD dataset).
We set the CRF regularization parameter ? = 1
and set the posterior regularization parameter ?
and ? (a trade-off parameter we introduce to bal-
ance the supervised objective and the posterior
regularizer in 2) by using grid search
8
. For
approximation inference with higher-order con-
straints, we perform 2000 Gibbs sampling itera-
tions where the first 1000 iterations are burn-in it-
erations. To make the results more stable, we con-
struct three Markov chains that run in parallel, and
select the sample with the largest objective value.
All posterior constraints were developed using
the training data on each training fold. For the MD
dataset, we also used the dvd domain as additional
labeled data for developing the constraints.
Baselines. We compared our method to a num-
ber of baselines: (1) CRF: CRF with the same set
of model features as in our method. (2) CRF-
INF: CRF augmented with inference constraints.
We can incorporate the proposed constraints (con-
straints derived from lexical patterns and discourse
connectives) as hard constraints into CRF during
7
Significance test was not conducted over the previous
methods as we do not have their results for each fold.
8
We conducted 10-fold cross-validation on each training
fold with the parameter space: ? : [0.01, 0.05, 0.1, 0.5, 1.0]
and ? : [0.1, 0.5, 1.0, 5.0, 10.0].
Methods CR MD
CRF 81.1 67.0
CRF-inf
lex
80.9 66.4
CRF-inf
disc
81.1 67.2
PR
lex
81.8 69.7
PR 82.7 70.6
Previous work
TreeCRF (Nakagawa et al, 2010) 81.4 -
Dropout LR (Wang and Manning, 2013) 82.1 -
Table 2: Accuracy results (%) for supervised sen-
timent classification (two-way)
Books Electronics Music Avg
VoteFlip 44.6 45.0 47.8 45.8
DocOracle 53.6 50.5 63.0 55.7
CRF 57.4 57.5 61.8 58.9
CRF-inf
lex
56.7 56.4 60.4 57.8
CRF-inf
disc
57.2 57.6 62.1 59.0
PR
lex
60.3 59.9 63.2 61.1
PR 61.6 61.0 64.4 62.3
Previous work
HCRF 55.9 61.0 58.7 58.5
MEM 59.7 59.6 63.8 61.0
Table 3: Accuracy results (%) for semi-supervised
sentiment classification (three-way) on the MD
dataset
inference by manually setting ? in equation 4 to a
large value,
9
. When ? is large enough, it is equiva-
lent to adding hard constraints to the viterbi infer-
ence. To better understand the different effects of
lexical and discourse constraints, we report results
for applying only the lexical constraints (CRF-
INF
lex
) as well as results for applying only the
discourse constraints (CRF-INF
disc
). (3) PR
lex
:
a variant of our PR model which only applies the
lexical constraints. For the three-way classifica-
tion task on the MD dataset, we also implemented
the following baselines: (4) VOTEFLIP: a rule-
based algorithm that leverages the positive, nega-
tive and neutral cues along with the effect of nega-
tion to determine the sentence sentiment (Choi
and Cardie, 2009). (5) DOCORACLE: assigns
each sentence the label of its corresponding doc-
ument.
4.1 Results
We first report results on a binary (positive or neg-
ative) sentence-level sentiment classification task.
For this task, we used the supervised setting and
performed transductive learning for our model.
Table 2 shows the accuracy results. We can see
9
We set ? to 1000 for the lexical constraints and -1000 to
the discourse connective constraints in the experiments
331
Books Electronics Music
pos/neg/neu pos/neg/neu pos/neg/neu
VoteFlip 43/42/47 45/46/44 50/46/46
DocOracle 54/60/49 57/54/42 72/65/52
CRF 47/51/64 60/61/52 67/60/58
CRF-inf
lex
46/52/63 59/61/50 65/59/57
CRF-inf
disc
47/51/64 60/61/52 67/61/59
PR
lex
50/56/66 64/63/53 67/64/59
PR 52/56/68 64/66/53 69/65/60
Table 4: F1 scores for each sentiment cate-
gory (positive, negative and neutral) for semi-
supervised sentiment classification on the MD
dataset
that PR significantly outperforms all other base-
lines in both the CR dataset and the MD dataset
(average accuracy across domains is reported).
The poor performance of CRF-INF
lex
indicates
that directly applying lexical constraints as hard
constraints during inference could only hurt the
performance. CRF-INF
disc
slightly outperforms
CRF but the improvement is not significant. In
contrast, both PR
lex
and PR significantly outper-
form CRF, which implies that incorporating lex-
ical and discourse constraints as posterior con-
straints is much more effective. The superior per-
formance of PR over PR
lex
further suggests that
the proper use of discourse information can signif-
icantly improve accuracy for sentence-level senti-
ment classification.
We also analyzed the model?s performance on a
three-way sentiment classification task. By intro-
ducing the ?neutral? category, the sentiment clas-
sification problem becomes harder. Table 4 shows
the results in terms of accuracy for each domain
in the MD dataset. We can see that both PR and
PR
lex
significantly outperform all other baselines
in all domains. The rule-based baseline VOTE-
FLIP gave the weakest performance because it has
no prediction power on sentences with no opinion
words. DOCORACLE performs much better than
VOTEFLIP and performs especially well on the
Music domain. This indicates that the document-
level sentiment is a very strong indicator of the
sentence-level sentiment label. For the CRF base-
line and its invariants, we observe a similar per-
formance trend as in the two-way classification
task: there is nearly no performance improve-
ment from applying the lexical and discourse-
connective-based constraints during CRF infer-
ence. In contrast, both PR
lex
and PR provide
substantial improvements over CRF. This con-
firms that encoding lexical and discourse knowl-
edge as posterior constraints allows the feature-
based model to gain additional learning power
for sentence-level sentiment prediction. In par-
ticular, incorporating discourse constraints leads
to consistent improvements to our model. This
demonstrates that our modeling of discourse in-
formation is effective and that taking into account
the discourse context is important for improving
sentence-level sentiment analysis. We also com-
pare our results to the previously published results
on the same dataset. HCRF (T?ackstr?om and Mc-
Donald, 2011a) and MEM (Qu et al, 2012) are
two state-of-the-art semi-supervised methods for
sentence-level sentiment classification. We can
see that our best model PR gives the best results
in most categories.
Table 4 shows the results in terms of F1 scores
for each sentiment category (positive, negative and
neutral). We can see that the PR models are able to
provide improvements over all the sentiment cate-
gories compared to all the baselines in general. We
observe that the DOCORACLE baseline provides
very strong F1 scores on the positive and nega-
tive categories especially in the Books and Mu-
sic domains, but very poor F1 on the neutral cate-
gory. This is because it over-predicts the polar sen-
tences in the polar documents, and predicts no po-
lar sentences in the neutral documents. In contrast,
our PR models provide more balanced F1 scores
among all the sentiment categories. Compared to
the CRF baseline and its variants, we found that
the PR models can greatly improve the precision
of predicting positive and negative sentences, re-
sulting in a significant improvement on the pos-
itive/negative F1 scores. However, the improve-
ment on the neutral category is modest. A plausi-
ble explanation is that most of our constraints fo-
cus on discriminating polar sentences. They can
help reduce the errors of misclassifying polar sen-
tences, but the model needs more constraints in
order to distinguish neutral sentences from polar
sentences. We plan to address this issue in future
work.
4.2 Discussion
We analyze the errors to better understand the mer-
its and limitations of the PR model. We found
that the PR model is able to correct many CRF
errors caused by the lack of labeled data. The first
row in Table 5 shows an example of such errors.
332
Example Sentences CRF PR
Example 1: ?neg? If I could, I would like to return it or exchange
for something better.?/neg?
?neu? ? X
Example 2: ?neg? Things I wasn?t a fan of ? the ending was to
cutesy for my taste.?/neg? ?neg? Also, all of the side characters
(particularly the mom, vee, and the teacher) were incredibly flat
and stereotypical to me.?/neg?
?neu? ?pos? ? X
Example 3: ?neg? I also have excessive noise when I talk and
have phone in my pocket while walking.?/neg? ?neu? But other
models are no better.?/neu?
?neg? ?pos? ? ?neg? ?pos? ?
Table 5: Example sentences where PR succeeds and fails to correct the mistakes of CRF
The lexical features return and exchange may
be good indicators of negative sentiment for the
sentence. However, with limited labeled data, the
CRF learner can only associate very weak senti-
ment signals to these features. In contrast, the PR
model is able to associate stronger sentiment sig-
nals to these features by leveraging unlabeled data
for indirect supervision. A simple lexicon-based
constraint during inference time may also correct
this case. However, hard-constraint baselines can
hardly improve the performance in general be-
cause the contributions of different constraints are
not learned and their combination may not lead to
better predictions. This is also demonstrated by
the limited performance of CRF-INF in our exper-
iments.
We also found that the discourse constraints
play an important role in improving the sentiment
prediction. The lexical constraints alone are of-
ten not sufficient since their coverage is limited by
the sentiment lexicon and they can only constrain
sentiment locally. On the contrary, discourse con-
straints are not dependent on sentiment lexicons,
and more importantly, they can provide sentiment
preferences on multiple sentences at the same
time. When combining discourse constraints with
features from different sentences, the PR model
becomes more powerful in disambiguating senti-
ment. The second example in Table 5 shows that
the PR model learned with discourse constraints
correctly predicts the sentiment of two sentences
where no lexical constraints apply.
However, discourse constraints are not always
helpful. One reason is that they do not constrain
the neutral sentiment. As a result they could not
help disambiguate neutral sentiment from polar
sentiment, such as the third example in Table 5.
This is also a problem for most of our lexical con-
straints. In general, it is hard to learn reliable indi-
cators for the neutral sentiment. In the MD dataset,
a neutral label may be given because the sentence
contains mixed sentiment or no sentiment or it is
off-topic. We plan to explore more refined con-
straints that can deal with the neutral sentiment in
future work. Another limitation of the discourse
constraints is that they could be affected by the er-
rors of the discourse parser and the coreference re-
solver. A potential way to address this issue is to
learn discourse constraints jointly with sentiment.
We plan to study this in future research.
5 Conclusion
In this paper, we propose a context-aware ap-
proach for learning sentence-level sentiment. Our
approach incorporates intuitive lexical and dis-
course knowledge as expressive constraints while
training a conditional random field model via pos-
terior regularization. We explore a rich set of
context-aware constraints at both intra- and inter-
sentential levels, and demonstrate their effective-
ness in the analysis of sentence-level sentiment.
While we focus on the sentence-level task, our ap-
proach can be easily extended to handle sentiment
analysis at finer levels of granularity. Our exper-
iments show that our model achieves better accu-
racy than existing supervised and semi-supervised
models for the sentence-level sentiment classifica-
tion task.
Acknowledgments
This work was supported in part by DARPA-BAA-
12-47 DEFT grant #12475008 and NSF grant
BCS-0904822. We thank Igor Labutov for help-
ful discussion and suggestions; Oscar T?ackstr?om
and Lizhen Qu for providing their Amazon review
datasets; and the anonymous reviewers for helpful
comments and suggestions.
References
Kedar Bellare, Gregory Druck, and Andrew McCal-
lum. 2009. Alternating projections for learning
333
with expectation constraints. In Proceedings of the
Twenty-Fifth Conference on Uncertainty in Artificial
Intelligence, pages 43?50. AUAI Press.
Yejin Choi and Claire Cardie. 2008. Learning with
compositional semantics as structural inference for
subsentential sentiment analysis. In Proceedings of
the Conference on Empirical Methods in Natural
Language Processing, pages 793?801. Association
for Computational Linguistics.
Yejin Choi and Claire Cardie. 2009. Adapting a po-
larity lexicon using integer linear programming for
domain-specific sentiment classification. In Pro-
ceedings of the 2009 Conference on Empirical Meth-
ods in Natural Language Processing: Volume 2-
Volume 2, pages 590?598. Association for Compu-
tational Linguistics.
John Duchi, Elad Hazan, and Yoram Singer. 2010.
Adaptive subgradient methods for online learning
and stochastic optimization. Journal of Machine
Learning Research, 12:2121?2159.
Kuzman Ganchev and Dipanjan Das. 2013. Cross-
lingual discriminative learning of sequence models
with posterior regularization.
Kuzman Ganchev, Jennifer Gillenwater, and Ben
Taskar. 2009. Dependency grammar induction via
bitext projection constraints. In Proceedings of the
ACL-IJCNLP, pages 369?377.
Kuzman Ganchev, Joao Grac?a, Jennifer Gillenwater,
and Ben Taskar. 2010. Posterior regularization for
structured latent variable models. The Journal of
Machine Learning Research, 99:2001?2049.
Stuart Geman and Donald Geman. 1984. Stochas-
tic relaxation, gibbs distributions, and the bayesian
restoration of images. Pattern Analysis and Machine
Intelligence, IEEE Transactions on, (6):721?741.
Minqing Hu and Bing Liu. 2004. Mining and summa-
rizing customer reviews. In Proceedings of the tenth
ACM SIGKDD international conference on Knowl-
edge discovery and data mining, pages 168?177.
ACM.
Dan Jurafsky, James H Martin, Andrew Kehler, Keith
Vander Linden, and Nigel Ward. 2000. Speech
and language processing: An introduction to natu-
ral language processing, computational linguistics,
and speech recognition, volume 2. MIT Press.
Hiroshi Kanayama and Tetsuya Nasukawa. 2006.
Fully automatic lexicon expansion for domain-
oriented sentiment analysis. In Proceedings of the
2006 Conference on Empirical Methods in Natural
Language Processing, pages 355?363. Association
for Computational Linguistics.
Angeliki Lazaridou, Ivan Titov, and Caroline
Sporleder. 2013. A bayesian model for joint
unsupervised induction of sentiment, aspect and
discourse representations. In To Appear in Proceed-
ings of the 51th Annual Meeting of the Association
for Computational Linguistics, Sofia, Bulgaria,
August. Association for Computational Linguistics.
Heeyoung Lee, Angel Chang, Yves Peirsman,
Nathanael Chambers, Mihai Surdeanu, and Dan Ju-
rafsky. 2013. Deterministic coreference resolution
based on entity-centric, precision-ranked rules.
Yi Mao and Guy Lebanon. 2007. Isotonic conditional
random fields and local sentiment flow. Advances in
neural information processing systems, 19:961.
Ryan McDonald, Kerry Hannan, Tyler Neylon, Mike
Wells, and Jeff Reynar. 2007. Structured mod-
els for fine-to-coarse sentiment analysis. In An-
nual Meeting-Association For Computational Lin-
guistics, volume 45, page 432.
Tetsuji Nakagawa, Kentaro Inui, and Sadao Kurohashi.
2010. Dependency tree-based sentiment classifica-
tion using crfs with hidden variables. In Human
Language Technologies: The 2010 Annual Confer-
ence of the North American Chapter of the Associa-
tion for Computational Linguistics, pages 786?794.
Association for Computational Linguistics.
Bo Pang and Lillian Lee. 2004. A sentimental educa-
tion: Sentiment analysis using subjectivity summa-
rization based on minimum cuts. In Proceedings of
the 42nd annual meeting on Association for Compu-
tational Linguistics, page 271. Association for Com-
putational Linguistics.
Bo Pang and Lillian Lee. 2008. Opinion mining and
sentiment analysis. Now Pub.
Livia Polanyi and Annie Zaenen. 2006. Contextual
valence shifters. In Computing attitude and affect in
text: Theory and applications, pages 1?10. Springer.
Rashmi Prasad, Nikhil Dinesh, Alan Lee, Eleni Milt-
sakaki, Livio Robaldo, Aravind K Joshi, and Bon-
nie L Webber. 2008. The penn discourse treebank
2.0. In LREC. Citeseer.
Lizhen Qu, Rainer Gemulla, and Gerhard Weikum.
2012. A weakly supervised model for sentence-level
semantic orientation analysis with multiple experts.
In Proceedings of the 2012 Joint Conference on
Empirical Methods in Natural Language Process-
ing and Computational Natural Language Learning,
pages 149?159. Association for Computational Lin-
guistics.
Richard Socher, Jeffrey Pennington, Eric H Huang,
Andrew Y Ng, and Christopher D Manning. 2011.
Semi-supervised recursive autoencoders for predict-
ing sentiment distributions. In Proceedings of the
Conference on Empirical Methods in Natural Lan-
guage Processing, pages 151?161. Association for
Computational Linguistics.
334
Richard Socher, Alex Perelygin, Jean Y Wu, Jason
Chuang, Christopher D Manning, Andrew Y Ng,
and Christopher Potts. 2013. Recursive deep mod-
els for semantic compositionality over a sentiment
treebank. In Proceedings of EMNLP.
Swapna Somasundaran, Janyce Wiebe, and Josef Rup-
penhofer. 2008. Discourse level opinion interpre-
tation. In Proceedings of the 22nd International
Conference on Computational Linguistics-Volume 1,
pages 801?808. Association for Computational Lin-
guistics.
Swapna Somasundaran, Galileo Namata, Janyce
Wiebe, and Lise Getoor. 2009. Supervised and
unsupervised methods in employing discourse rela-
tions for improving opinion polarity classification.
In Proceedings of the 2009 Conference on Empirical
Methods in Natural Language Processing: Volume
1-Volume 1, pages 170?179. Association for Com-
putational Linguistics.
Oscar T?ackstr?om and Ryan McDonald. 2011a. Dis-
covering fine-grained sentiment with latent variable
structured prediction models. In Advances in Infor-
mation Retrieval, pages 368?374. Springer.
Oscar T?ackstr?om and Ryan McDonald. 2011b. Semi-
supervised latent variable models for sentence-level
sentiment analysis.
Rakshit Trivedi and Jacob Eisenstein. 2013. Discourse
connectors for latent subjectivity in sentiment analy-
sis. In Proceedings of NAACL-HLT, pages 808?813.
Sida Wang and Christopher Manning. 2013. Fast
dropout training. In Proceedings of the 30th Inter-
national Conference on Machine Learning (ICML-
13), pages 118?126.
Qi Zhang, Jin Qian, Huan Chen, Jihua Kang, and Xu-
anjing Huang. 2013. Discourse level explanatory
relation extraction from product reviews using first-
order logic.
Lanjun Zhou, Binyang Li, Wei Gao, Zhongyu Wei,
and Kam-Fai Wong. 2011. Unsupervised discovery
of discourse relations for eliminating intra-sentence
polarity ambiguities. In Proceedings of the Con-
ference on Empirical Methods in Natural Language
Processing, pages 162?171. Association for Com-
putational Linguistics.
335
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 1566?1576,
Baltimore, Maryland, USA, June 23-25 2014.
c
?2014 Association for Computational Linguistics
Towards a General Rule for Identifying Deceptive Opinion Spam
Jiwei Li
1
, Myle Ott
2
, Claire Cardie
2
, Eduard Hovy
1
1
Language Technology Institute, Carnegie Mellon University, Pittsburgh, P.A. 15213, USA
2
Department of Computer Science, Cornell University, Ithaca, N.Y., 14853, USA
bdlijiwei@gmail.com, myleott@cs.cornell.edu
cardie@cs.cornell.edu, ehovy@andrew.cmu.edu
Abstract
Consumers? purchase decisions are in-
creasingly influenced by user-generated
online reviews. Accordingly, there has
been growing concern about the poten-
tial for posting deceptive opinion spam?
fictitious reviews that have been deliber-
ately written to sound authentic, to de-
ceive the reader. In this paper, we ex-
plore generalized approaches for identify-
ing online deceptive opinion spam based
on a new gold standard dataset, which is
comprised of data from three different do-
mains (i.e. Hotel, Restaurant, Doctor),
each of which contains three types of re-
views, i.e. customer generated truthful re-
views, Turker generated deceptive reviews
and employee (domain-expert) generated
deceptive reviews. Our approach tries to
capture the general difference of language
usage between deceptive and truthful re-
views, which we hope will help customers
when making purchase decisions and re-
view portal operators, such as TripAdvisor
or Yelp, investigate possible fraudulent ac-
tivity on their sites.
1
1 Introduction
Consumers increasingly rely on user-generated
online reviews when making purchase deci-
sion (Cone, 2011; Ipsos, 2012). Unfortunately,
the ease of posting content to the Web, poten-
tially anonymously, creates opportunities and in-
centives for unscrupulous businesses to post de-
ceptive opinion spam?fictitious reviews that are
deliberately written to sound authentic, in order to
deceive the reader.
2
Accordingly, there appears
1
Dataset available by request from the first author.
2
Manipulating online reviews may also have legal conse-
quences. For example, the Federal Trade Commission (FTC)
to be widespread and growing concern among
both businesses and the public about this poten-
tial abuse (Meyer, 2009; Miller, 2009; Streitfeld,
2012; Topping, 2010; Ott, 2013).
Existing approaches for spam detection are usu-
ally focused on developing supervised learning-
based algorithms to help users identify decep-
tive opinion spam, which are highly dependent
upon high-quality gold-standard labeled data (Jin-
dal and Liu, 2008; Jindal et al, 2010; Lim et al,
2010; Wang et al, 2011; Wu et al, 2010). Stud-
ies in the literature rely on a couple of approaches
for obtaining labeled data, which usually fall into
two categories. The first relies on the judge-
ments of human annotators (Jindal et al, 2010;
Mukherjee et al, 2012). However, recent stud-
ies show that deceptive opinion spam is not eas-
ily identified by human readers (Ott et al, 2011).
An alternative approach, as introduced by Ott et
al. (2011), crowdsourced deceptive reviews using
Amazon Mechanical Turk.
3
A couple of follow-up
works have been introduced based on Ott et al?s
dataset, including estimating prevalence of decep-
tion in online reviews (Ott et al, 2012), identifica-
tion of negative deceptive opinion spam (Ott et al,
2013), and identifying manipulated offerings (Li
et al, 2013b).
Despite the advantages of soliciting deceptive
gold-standard material from Turkers (it is easy,
large-scale, and affordable), it is unclear whether
Turkers are representative of the general popula-
tion that generate fake reviews, or in other words,
Ott et al?s data set may correspond to only one
type of online deceptive opinion spam ? fake re-
views generated by people who have never been
to offerings or experienced the entities. Specifi-
cally, according to their findings (Ott et al, 2011;
has updated their guidelines on the use of endorsements and
testimonials in advertising to suggest that posting deceptive
reviews may be unlawful in the United States (FTC, 2009).
3
http://www.mturk.com
1566
Li et al, 2013a), truthful hotel reviews encode
more spatial details, characterized by terms such
as ?bathroom? and ?location?, while deceptive re-
views talk about general concepts such as why or
with whom they went to the hotel. However, a
hotel can instead solicit fake reviews from their
employees or customers who possess substantial
domain knowledge to write fake reviews and en-
code more spatial details in their lies. Indeed,
cases have been reported where hotel owners bribe
guests in return for good reviews on TripAdvi-
sor
4
, or companies ordered employees to pretend
they were satisfied customers and write glowing
reviews of its face-lift procedure on Web sites.
5
The domain knowledge possessed by domain ex-
perts enables them to craft reviews that are much
more difficult for classifiers to detect, compared to
the crowdsourced fake reviews.
Additionally, existing supervised algorithms in
the literature are usually narrowed to one spe-
cific domain and heavily rely on domain-specific
vocabulary. For example, classifiers assign high
weights to domain-specific terms such as ?hotel?,
?rooms?, or even the name of the hotels such as
?Hilton? when trained on reviews on hotels. It
is unclear whether these classifiers will perform
well at detecting deception in other domains, e.g.,
Restaurant or Doctor reviews. Even in a single do-
main, e.g., Hotel, classifiers trained from reviews
of one city (e.g., Chicago) may not be effective if
directly applied to reviews from other cities (e.g.,
New York City) (Li et al, 2013b). In the exam-
ples in Table 1, we trained a linear SVM clas-
sifier on Ott?s Chicago-hotel dataset on unigram
features and tested it on a couple of different do-
mains (the details of data acquisition are illustrated
in Section 3). Good performance is obtained on
Chicago-hotel reviews (Ott et al, 2011), but not as
good on New York City ones. The performance is
reasonable in Restaurant reviews due to the many
shared properties among restaurants and hotels,
but suffers in Doctor settings.
In this paper, we try to obtain a deeper under-
standing of the general nature of deceptive opin-
ion spam. One contribution of the work presented
here is the creation of the cross-domain (i.e., Ho-
tel, Restaurant and Doctor) gold-standard dataset.
4
http://www.dailymail.co.uk/travel/article-
2013391/Tripadvisor-Hotel-owners-bribe-guests-return-
good-reviews.html
5
http://www.nytimes.com/2009/07/15/
technology/internet/15lift.html?_r=0
Accuracy Precision Recall F1
NYC-Hotel 0.799 0.794 0.758 0.766
Chicago-Restaurant 0.785 0.813 0.742 0.778
Doctor 0.550 0.537 0.725 0.617
Table 1: SVM performance on datasets for a clas-
sifier trained on Chicago hotel review based on
Unigram feature.
In contrast to existing work (Ott et al, 2011; Li et
al., 2013b), our new gold standard includes three
types of reviews: domain expert deceptive opinion
spam (Employee), crowdsourced deceptive opin-
ion spam (Turker), and truthful Customer reviews
(Customer). In addition, some of domains contain
both positive (P) and negative (N) reviews.
6
To explore the general rule of deceptive opinion
spam, we extended SAGE Model (Eisenstein et
al., 2011), a bayesian generative approach that can
capture the multiple generative facets (i.e., decep-
tive vs truthful, positive vs negative, experienced
vs non-experienced, hotel vs restaurant vs doctor)
in the text collection. We find that more general
features, such as LIWC and POS, are more robust
when modeled using SAGE, compared with just
bag-of-words.
We additionally make theoretical contributions
that may shed light on a longstanding debate in the
literature about deception. For example, in con-
trast to existing findings that highlight the lack of
spatial detail in deceptive reviews (Ott et al, 2011;
Li et al, 2013b), we find that a lack of spatial de-
tail may not be a universal cue to deception, since
it does not apply to fake reviews written by domain
experts. Instead, our finding suggest that other lin-
guistic features may offer more robust cues to de-
ceptive opinion spam, such as overly highlighted
sentiment in the review or the overuse of first-
person singular pronouns.
The rest of this paper is organized as follows.
In Section 2, we briefly go over related work. We
describe the creation of our data set in Section 3
and present our model in Section 4. Experimental
results are shown in Section 5. We present anal-
ysis of general cues to deception in Section 6 and
conclude this paper in Section 7.
6
For example, a hotel manager could hire people to write
positive reviews to increase the reputation of his own hotel
or post negative ones to degrade his competitors. Identify-
ing positive/negative opinion spam is explored in (Ott et al,
2011; Ott et al, 2013)
1567
2 Related Work
Spam has been historically studied in the contexts
of Web text (Gy?ongyi et al, 2004; Ntoulas et al,
2006) or email (Drucker et al, 1999). Recently
there has been increasing concern about deceptive
opinion spam (Jindal and Liu, 2008; Ott et al,
2011; Wu et al, 2010; Mukherjee et al, 2013b;
Wang et al, 2012).
Jindal and Liu (2008) first studied the deceptive
opinion problem and trained models using features
based on the review text, reviewer, and product
to identify duplicate opinions, i.e., opinions that
appear more than once in the corpus with simi-
lar contexts. Wu et al (2010) propose an alter-
native strategy to detect deceptive opinion spam
in the absence of a gold standard. Yoo and Gret-
zel (2009) gathered 40 truthful and 42 deceptive
hotel reviews and manually compare the linguis-
tic differences between them. Ott et al created
a gold-standard collection by employing Turkers
to write fake reviews, and follow-up research was
based on their data (Ott et al, 2012; Ott et al,
2013; Li et al, 2013b; Feng and Hirst, 2013). For
example, Song et al (2012) looked into syntactic
features from Context Free Grammar parse trees
to improve the classifier performance. A step fur-
ther, Feng and Hirst (2013) make use of degree
of compatibility between the personal experiment
and a collection of reference reviews about the
same product rather than simple textual features.
In addition to exploring text or linguistic fea-
tures in deception, some existing work looks
into customers? behavior to identify deception
(Mukherjee et al, 2013a). For example, Mukher-
jee et al (2011; 2012) delved into group behavior
to identify group of reviewers who work collabo-
ratively to write fake reviews. Qian and Liu (2013)
identified multiple user IDs that are generated by
the same author, as these authors are more likely
to generate deceptive reviews.
In the psychological literature, researchers have
looked into possible linguistic cues to deception
(Newman et al, 2003), such as decreased spatial
detail, which is consistent with theories of reality
monitoring (Johnson and Raye, 1981), increased
negative emotion terms (Newman et al, 2003), or
the writing style difference between informative
(truthful) and imaginative (deceptive) writings in
(Rayson et al, 2001). The former typically con-
sists of more nouns, adjectives, prepositions, de-
terminers, and coordinating conjunctions, while
the latter consists of more verbs, adverbs, pro-
nouns, and pre-determiners.
SAGE (Sparse Additive Generative Model):
SAGE is an generative bayesian approach in-
troduced by Eisenstein et al (2011), which
can be viewed as an combination of topic mod-
els (Blei et al, 2003) and generalized additive
models (Hastie and Tibshirani, 1990). Unlike
other derivatives of topic models, SAGE drops
the Dirichlet-multinomial assumption and adopts
a Laplacian prior, triggering sparsity in topic-word
distribution. The reason why SAGE is tailored for
our task is that SAGE constructs multi-faceted la-
tent variable models by simply adding together the
component vectors rather than incorporating mul-
tiple switching latent variables in multiple facets.
3 Dataset Construction
In this section, we report our efforts to gather gold-
standard opinion spam datasets. Our datasets con-
tain the following domains, namely Hotel, Restau-
rant, and Doctor.
3.1 Turker set, using Mechanical Turk
Crowdsourcing services such as AMT greatly fa-
cilitate large-scale data annotation and collection
efforts. Anyone with basic programming skills can
create Human Intelligence Tasks (HITs) and ac-
cess a marketplace of anonymous online workers
(Turkers) willing to complete the tasks. We bor-
rowed some rules used by Ott et al to create their
dataset, such as restricting task to Turkers located
in the United States, and who maintain an approval
rating of at least 90%.
Hotel-Turker : We directly borrowed datasets
from Ott
7
and Li.
8
Restaurant-Turker : We gathered 20 positive
(P) deceptive reviews for each of 10 of the most
popular restaurants in Chicago, for a total of 200
positive deceptive restaurant reviews.
Doctor-Turker : We gathered a total number of
200 positive reviews from Turkers.
3.2 Employee set, by domain experts
We seek deceptive opinion spam written by people
with expert-level domain knowledge. It is not ap-
propriate to use crowdsourcing to obtain this data,
7
http://myleott.com/op_spam/
8
http://www.cs.cmu.edu/
?
jiweil/html/
four_city.html
1568
Turker Expert Customer
Hotel (P/N) 400/400 140/140 400/400
Restaurant (P/N) 200/0 120/0 200/200
Doctor (P/N) 200/0 32/0 200/0
Table 2: Statistics for our dataset.
so instead we solicit reviews written by employees
in each domain.
Hotel-Employee: We asked two hotel employ-
ees from each of seven hotels (14 employees to-
tal) each to write 10 deceptive positive-sentiment
reviews of their own hotel, and 10 deceptive
negative-sentiment reviews of their biggest local
competitor?s hotel. In total, we obtained 280 de-
ceptive reviews of 14 hotels, including a balanced
mix of positive- and negative-sentiment reviews.
Restaurant-Employee: We asked employees
from selected restaurants (a waiter/waitress or
cook) to each write positive-sentiment reviews of
their restaurant.
Doctor-Employee: We asked real doctors to
write positive fake reviews about themselves. In
total we obtained 32 reviews from 15 doctors.
3.3 Customer set from Actual Customers
Hotel-Customer: We borrowed from Ott et al?s
dataset.
Restaurant/Doctor-Customer: We solicited
data by matching a set of truthful reviews as Ott
et al did in collecting truthful hotel reviews.
3.4 Summary for Data Creation
Statistics for our data set is presented in Table 2.
Due to the difficulty in obtaining gold-standard
data in the literature, there is no doubt that our data
set is not perfect. Some parts are missing, some
are unbalanced, participants in the survey may not
be representative of the general population. How-
ever, as far as we know, this is the most compre-
hensive dataset for deceptive opinion spam so far,
and may to some extent shed insights on the nature
of online deception.
4 Feature-based Additive Model
In this section, we briefly describe our model.
Since mathematics are not the main theme of this
paper, we omit the exact details for inference,
which can be found in (Eisenstein et al, 2011).
Before describing the model in detail, we note
the following advantages of the SAGE model, and
our reasons for using it in this paper:
1. the ?additive? nature of SAGE allows a better
understanding of which features contribute
most to each type of deceptive review and
how much each such feature contributes to
the final decision jointly. If we instead use
SVM, for example, we would have to train
classifiers one by one (due to the distinct fea-
tures from different sources) to draw con-
clusions regarding the differences between
Turker vs Expert vs truthful reviews, positive
expert vs negative expert reviews, or reviews
from different domains. This would not only
become intractable, but would make the con-
clusions less clear.
2. For cross-domain classification task, standard
machine learning approaches may suffer due
to domain-specific properties (See Section
5.2).
4.1 Model
In SAGE, each termw is drawn from a distribution
proportional to exp(m
(w)
+ ?
(T )(w)
y
d
+ ?
(A)(w)
z
n
+
?
(I)(w)
y
d
,z
n
), where m
(w)
is the observed background
term frequency, ?
y
d
, ?
z
n
and ?
y
d
,z
n
denote the log
frequency deviation representing topic z
n
, facet
y
d
, and the second-order interaction part respec-
tively. Superscripts T ,A and I respectively denote
the index of the topic, facet, and second-order in-
teraction. In our task, we adapt the SAGE model
as follows:
Y = {y
Sentiment
? {positive, negative},
y
Domain
? {hotel, restaurant, doctor},
y
Source
? {employee, turker, customer}}
We model three ??s, one for each type of y. Let
i, j, k denote the index of the different types of y,
so that each term w is drawn as follows:
P (w|i, j, k) ? exp(m
(w)
+ ?
(i)(w)
y
Sentiment
+?
(j)(w)
y
Domain
+ ?
(k)(w)
y
Scource
+ higher order)
where the higher order parts denote the interac-
tions between different facets.
In our approach each document-level feature f
is drawn from the following distribution:
P (f |i, j, k) ? exp(m
(f)
+ ?
(i)(f)
y
Sentiment
+ ?
(j)(f)
y
Domain
+ ?
(k)(f)
y
Scource
+ higher order)
(1)
1569
where m
(f)
can be interpreted as the background
value of feature f . For each review d, the proba-
bility that it is drawn from facets with index i, j, k
is as follows:
P (d|i, j, k) =
?
f?d
P (f |i, j, k)
?
w?d
P (w|i, j, k) (2)
In the training process, parameters ?
(w)
y
and ?
(f)
y
are to be learned by maximizing the posterior
distribution following the original SAGE training
procedure. For prediction, we estimate y
Source
for
each document given all or part of ?
(w)
y
and ?
(f)
y
as follows:
y
Source
=
argmax
y
?
Source
P (d|y
?
Source
, y
Sentiment
, y
Domain
),
where we assume y
Sentiment
and y
Domain
are
given for each document d. Note that we as-
sume conditional independence between features
and words given y, similar to other topic mod-
els (Blei et al, 2003). Notably, our revised SAGE
model degenerates into a model similar to Gen-
eralized Additive Model (Hastie and Tibshirani,
1990) when word features are not considered.
5 Experiments
In this section, we report our experimental results.
We first restrict experiments to the within-domain
task and see what features most characterize the
deceptive reviews, and how. We later extend it to
cross domains to explore a more general classifier
of deceptive opinion spam.
5.1 Intra-Domain Classification
We explore the effect of both domain experts
and crowdsourcing workers on intra-domain de-
ception. Specifically, we reframe it as a intra-
domain multi-class classification task, where
given the labeled training data from one domain,
we learn a classifier to classify reviews accord-
ing to their source, i.e., Employee, Turker and
Customer. Since the machine learning classi-
fier is trained and tested within the same domain,
?
(j)(w)
y
Domain
and ?
(i)(f)
y
Domain
are not considered here.
We use a One-Versus-Rest (OvR) scheme, in
which we train m classifiers using SAGE, such
that each classifier f
i
, for i ? [1,m], is trained to
distinguish between class i on the one hand, and
all classes except i on the other. To make an m-
way decision, we then choose the class c with the
most confident prediction. OvR approaches have
been shown to produce state-of-art performance
compared to other multi-class approaches such as
Multinomial Naive Bayes or One-Versus-One clas-
sification scheme. We train the OvR classifier on
three sets of features, LIWC, Unigram, and POS.
9
Multi-class classification results are given at Ta-
ble 3. We report both OvR performance and the
performance of three One-versus-One binary clas-
sifiers, trained to distinguish between each pair
of classes. In particular, the three-class classifier
is around 65% accurate at distinguishing between
Employee, Customer, and Turker for each of the
domains using Unigram, significantly higher than
random guess. We also observe that each of the
three One-versus-One binary classifications per-
forms significantly better than chance, suggesting
that Employee, Customer, and Turker are in fact
three different classes. In particular, the two-class
classifier is around 0.76 accurate in distinguish-
ing between Turker and Employee reviews, de-
spite both kinds of reviews being deceptive opin-
ion spam.
Best performance is achieved on Unigram fea-
tures, constantly outperforming LIWC and POS
features in both three-class and two-class settings
in the hotel domain. Similar results are observed
for restaurant and doctor domains and details are
excluded for brevity. This suggests that a universal
set of keyword-based deception cues (e.g., LIWC)
is not the best approach for Intra-Domain Classifi-
cation. Similar results were also reported in previ-
ous work (Ott et al, 2012; Ott, 2013).
5.2 Cross-domain Classification
In this subsection, we frame our problem as a
domain adaptation task (Pan and Yang, 2010).
Again, we explore 3 feature sets: LIWC, Uni-
gram and POS. We train a classifier on hotel re-
views, and evaluate the performance on other do-
mains. For simplicity, we focus on truthful (Cus-
tomer) versus deceptive (Turker) binary classifi-
cation rather than a multi-class classification.
We report results from SAGE and SVM
10
in Ta-
ble 4. We first observe that classifiers trained on
hotel reviews apply well in the restaurant domain,
which is reasonable due to the many shared prop-
9
Part-of-speech tags were assigned based on Stan-
ford Parser http://nlp.stanford.edu/software/
lex-parser.shtml
10
We use SVMlight (Joachims, 1999) to train our linear
SVM classifiers
1570
Domain Setting Features
Customer Employee Turker
A P R P R P R
Hotel
Three-Class
Unigram 0.664 0.678 0.669 0.589 0.610 0.641 0.582
LIWC 0.602 0.617 0.613 0.541 0.598 0.590 0.511
POS 0.517 0.532 0.669 0.481 0.479 0.482 0.416
Customer vs Turker
Unigram 0.818 0.812 0.840 - - 0.820 0.809
LIWC 0.764 0.774 0.771 - - 0.723 0.749
POS 0.729 0.748 0.692 - - 0.707 0.759
Customer vs Employee
Unigram 0.799 0.832 0.784 0.804 0.820 - -
LIWC 0.732 0.746 0.751 0.714 0.722 - -
POS 0.728 0.713 0.742 0.707 0.754 - -
Employee vs Turker
Unigram 0.762 - - 0.786 0.806 0.826 0.794
LIWC 0.720 - - 0.728 0.726 0.698 0.739
POS 0.701 - - 0.688 0.710 0.701 0.697
Restaurant
Three-Class
Unigram
0.647 0.692 0.725 0.625 0.648 0.686 0.702
Customer vs Turker 0.817 0.842 0.816 - - 0.804 0.812
Customer vs Employee 0.785 0.790 0.814 0.769 0.826 - -
Employee vs Turker 0.774 - - 0.784 0.804 0.802 0.763
Doctor Customer vs Turker 0.745 0.772 0.701 - - 0.752 0.718
Table 3: Within-domain multi-class classifier performance.
Model Features Domain A P R F1 Domain A P R F1
SVM
Unigram Restaurant 0.785 0.813 0.742 0.778 Doctor 0.550 0.537 0.725 0.617
LIWC Restaurant 0.745 0.692 0.840 0.759 Doctor 0.521 0.512 0.965 0.669
POS Restaurant 0.735 0.697 0.815 0.751 Doctor 0.540 0.521 0.975 0.679
SAGE
Unigram Restaurant 0.770 0.793 0.750 0.784 Doctor 0.520 0.547 0.705 0.616
LIWC Restaurant 0.742 0.728 0.749 0.738 Doctor 0.647 0.650 0.608 0.628
POS Restaurant 0.746 0.732 0.687 0.701 Doctor 0.634 0.623 0.682 0.651
Table 4: Classifier performance in cross-domain adaptation.
erties among restaurants and hotels. Among three
types of features, Unigram still performs best.
POS and LIWC features are also robust across do-
mains.
In the doctor domain, we observe that models
trained on Unigram features from the hotels do-
main do not generalize well to doctor reviews, and
the performance is a little bit better than random
guess with only 0.55 accuracy. For SVM, models
trained on POS and LIWC features achieve even
lower accuracy than Unigram. POS and LIWC
features obtain around 0.5 precision and 1.0 re-
call, indicating that all doctor reviews are classi-
fied as deceptive by the classifier. One plausible
explanation could be doctor reviews generally en-
code some type of positive-weighted (deceptive)
features more than hotel reviews and these types
of features dominate the decision making proce-
dures, leading all reviews to be classified as de-
ceptive.
Tables 5 and 6 give the top weighted LIWC and
POS features. We observe that many features are
indeed shared among doctor and hotel domains.
Notably, POS features are more robust than LIWC
as more shared features are observed. As domain
specific properties will be considered in the in-
teraction part (?
LIWC
domain
and ?
POS
domain
) of the addi-
LIWC (hotel) LIWC (doctor)
deceptive truthful deceptive truthful
i AllPct Sixletters present
family number past AllPct
pronoun hear work social
Sixletters we health shehe
see space i number
posemo dash friend time
certain human posemo we
leisure exclusive feel you
future past perceptual negemo
perceptual home leisure Period
feel otherpunct insight relativ
comma negemo comma ingest
cause dash future money
Table 5: Top weighted LIWC features for Turker
vs Customer in Doctor and Hotel reviews. Blue
denotes shared positive (deceptive) features and
red denotes negative (truthful) features.
tive model, SAGE achieve much better results than
SVM, and is around 0.65 accurate in the cross-
domain task.
6 General Linguistic Cues of Deceptive
Opinion Spam
In this section, we examine a number of general
POS and LIWC features that may shed light on
a general rule for identifying deceptive opinion
1571
Figure 1: Visualization of the ? for POS features: Horizontal axes correspond to the values ? and are
NORMALIZED from the log-frequency function.
POS (hotel) POS (doctor)
deceptive truthful deceptive truthful
PRP$ CD VBD CD
PRP RRB NNP VBZ
VB LRB VB VBP
TO CC TO FW
NNP NNS VBG RRB
VBG RP PRP$ LRB
MD VBN JJS RB
VBP IN JJ LS
RB EX WRB PDT
JJS VBZ PRP VBN
Table 6: Top weighted POS features for Turker vs
Customer in Doctor and Hotel reviews. Blue de-
notes shared positive (deceptive) features and red
denotes negative (truthful) features.
spam. Our modified SAGE model provides us
with a tailored tool for this analysis. Specifically,
each feature f is associated with a background
valuem
f
. For each facetA, ?
f
A
, presents the facet-
specific preference value for feature f . Note that
sentiments are separated into positive and negative
dimensions, which is necessary because hotel em-
ployee authors wrote positive-sentiment reviews
when reviewing their own hotels, and negative-
sentiment reviews when reviewing their competi-
tors? hotels.
6.1 POS features
Early findings in the literature (Rayson et al,
2001; Buller and Burgoon, 1996; Biber et al,
1999) found that informative (truthful) writings
typically consist of more nouns, adjectives, prepo-
sitions, determiners, and coordinating conjunc-
tions, while imaginative (deceptive) writing con-
sist of more verbs, adverbs, pronouns, and pre-
determiners (with a few exceptions). Our find-
ings with POS features are largely in agreement
with these findings when distinguishing between
Turker and Customer reviews, but are violated in
the Employee set.
We present the eight types of POS features in
Figure 1, namely, N (Noun), JJ (Adjective), IN
(Preposition or subordinating conjunction) and DT
(Determiner), V (Verb), RB (Adverb), PRP (Pro-
nouns, both personal and possessive) and PDT
(Pre-Determiner).
From Figures 1(a)(b)(e)(f), we observe that with
the exception of PDT, the word frequency of
which is too small to draw a conclusion, Turker
and Customer reviews exhibit linguistic patterns in
agreement with previous findings in the literature,
where truthful reviews (Customer) tend to include
more N, JJ, IN and DT, while deceptive writings
tend to encode more V, RB and PRP.
However, in the case of the Employee-Positive
dataset, which is equally deceptive, most of these
rules are violated. Notably, reviews from the
Employee-Positive set did not encode fewer N, JJ
and DT terms, as expected (see Figures 1(a)(c)).
Instead, they encode even more N, JJ and DT
vocabularies than truthful reviews from the Cus-
tomer reviews. Also, fewer V and RB are found
in Employee-Positive reviews compared with Cus-
tomer reviews (see Figures 1(e)(g)).
One explanation for these observations is that
informative (truthful) writing tends to be more in-
troductory and descriptive, encoding more con-
crete details, when compared with imaginary writ-
ings. As domain experts possess considerable
knowledge of their own offerings, they highlight
1572
Figure 2: Visualization of the ? for LIWC features: Horizontal axes correspond to the values ? and are
normalized from the log-frequency function.
the details and their lies may be even more in-
formative and descriptive than those generated by
real customers! This explains why Employee-
Positive contains more N, IN and DT. Meanwhile,
as domain experts are engaged more in talking
about the details, they inevitably overlook other
information, possibly leading to fewer V and RB.
For Employee-Positive reviews, shown in Fig-
ures 1(d)(h), it turns out that domain experts do
not compensate for their lack of prior experience
when writing negative reviews for competitors? of-
ferings, as we will see again with LIWC features
in the next subsection.
6.2 LIWC features
We explore 3 LIWC categories (from left to right
in subfigures of Figure 2): sentiment (neg emo and
pos emo), spatial detail (space), and first-person
singular pronouns (first-person).
Space: Note that spatial details are more spe-
cific in the Hotel and Restaurant domains,
which is reflected in the high positive value of
?
Hotel,space
domain
(see Figure 2(g)) and negative value
of ?
Doctor,space
domain
(see Figure 2(h)). It illustrates how
domain-specific details can be predictive of decep-
tive text. Similarly predictive LIWC features are
home for the Hotel domain, ingest for the Restau-
rant domain, and health and body for the Doctor
domain.
In Figure 2(i)(j)(k)(l), we can easily see that
both actual customers and domain experts encode
more spatial details in their reviews (positive value
of ?), which is in agreement with our expectation.
This further demonstrates that a lack of spatial de-
tails would not be a general cue for deception.
Moreover, it appears that general domain expertise
does not compensate for the lack of prior experi-
ence when writing deceptive negative reviews for
competitors? hotels, as demonstrated by the lack
of spatial details in the negative-sentiment reviews
by employees shown in Figure 2(k).
Sentiment: According to our findings, the pres-
ence of sentiment is a general cue to deceptive
opinion spam, as observed when comparing Fig-
ure 2(b) to Figure 2(c) and (d). Participants, both
Employees and Turkers, tend to exaggerate senti-
ment, and include more sentiment-related vocabu-
laries in their lies. In other words, positive decep-
tive reviews were generally more positive and neg-
ative deceptive reviews were more negative in sen-
timent when compared with the truthful reviews
generated by actual customers. A similar pattern
can also be observed when comparing Figure 2(i)
to Figure 2(j).
1573
First-Person Singular Pronouns: The litera-
ture also associates deception with decreased us-
age of first-person singular pronouns, an effect at-
tributed to psychological distancing, whereby de-
ceivers talk less about themselves due either to a
lack of personal experience, or to detach them-
selves from the lie (Newman et al, 2003; Zhou
et al, 2004; Buller et al, 1996; Knapp and Co-
maden, 1979). However, according to our find-
ings, we find the opposite to hold. Increased first
person singular is an apparent indicator of decep-
tion, when comparing Figure 2(b) to 2(c) and 2(e).
We suspect that this relates to an effect observed
in previous studies of deception, where liars inad-
vertently undermine their lies by overemphasizing
aspects of their deception that they believe reflect
credibility (Bond and DePaulo, 2006; DePaulo et
al., 2003). One interpretation for this phenomenon
would be that deceivers try to overemphasize their
physical presence because they believe that this in-
creases their credibility.
7 Conclusion and Discussion
In this work, we have developed a multi-domain
large-scale dataset containing gold-standard de-
ceptive opinion spam. It includes reviews of Ho-
tels, Restaurants and Doctors, generated through
crowdsourcing and domain experts. We study this
data using SAGE, which enables us to make ob-
servations about the respects in which truthful and
deceptive text differs. Our model includes sev-
eral domain-independent features that shed light
on these differences, which further allows us to
formulate some general rules for recognizing de-
ceptive opinion spam.
We also acknowledge several important caveats
to this work. By soliciting fake reviews from par-
ticipants, including crowd workers and domain
experts, we have found that is possible to de-
tect fake reviews with above-chance accuracy, and
have used our models to explore several psycho-
logical theories of deception. However, it is still
very difficult to estimate the practical impact of
such methods, as it is very challenging to obtain
gold-standard data in the real world. Moreover,
by soliciting deceptive opinion spam in an arti-
ficial environment, we are endorsing the decep-
tion, which may influence the cues that we ob-
serve (Feeley and others, 1998; Frank and Ekman,
1997; Newman et al, 2003; Ott, 2013). Finally, it
may be possible to train people to tell more con-
vincing lies. Many of the characteristics regard-
ing fake review generation might be overcome by
well-trained fake review writers, which would re-
sults in opinion spam that is harder for detect. Fu-
ture work may wish to consider some of these ad-
ditional challenges.
8 Acknowledgement
We thank Wenjie Li and Xun Wang for useful dis-
cussions and suggestions. This work was sup-
ported in part by National Science Foundation
Grant BCS-0904822, a DARPA Deft grant, as well
as a gift from Google. We also thank the ACL re-
viewers for their helpful comments and advice.
References
Douglas Biber, Stig Johansson, Geoffrey Leech, Su-
san Conrad, Edward Finegan, and Randolph Quirk.
1999. Longman grammar of spoken and written En-
glish, volume 2. MIT Press.
David Blei, Andrew Ng, and Michael Jordan. 2003.
Latent dirichlet alocation. the Journal of machine
Learning research, 3:993?1022.
Charles Bond and Bella DePaulo. 2006. Accuracy of
deception judgments. Personality and Social Psy-
chology Review, 10(3):214?234.
David B Buller and Judee K Burgoon. 1996. Inter-
personal deception theory. Communication theory,
6(3):203?242.
David B Buller, Judee K Burgoon, Aileen Buslig, and
James Roiger. 1996. Testing interpersonal decep-
tion theory: The language of interpersonal decep-
tion. Communication theory, 6(3):268?289.
Paul-Alexandru Chirita, J?org Diederich, and Wolfgang
Nejdl. 2005. Mailrank: using ranking for spam
detection. In Proceedings of the 14th ACM inter-
national conference on Information and knowledge
management, pages 373?380. ACM.
Cone. 2011. 2011 Online Influence Trend Tracker.
http://www.coneinc.com/negative-reviews-online-
reverse-purchase-decisions, August.
Bella DePaulo, James Lindsay, Brian Malone, Laura
Muhlenbruck, Kelly Charlton, and Harris Cooper.
2003. Cues to deception. Psychological bulletin,
129(1):74.
Harris Drucker, Donghui Wu, and Vladimir Vapnik.
1999. Support vector machines for spam catego-
rization. Neural Networks, IEEE Transactions on,
10(5):1048?1054.
1574
Jacob Eisenstein, Amr Ahmed, and Eric P Xing. 2011.
Sparse additive generative models of text. In Pro-
ceedings of the 28th International Conference on
Machine Learning (ICML-11), pages 1041?1048.
Thomas Feeley. 1998. The behavioral correlates of
sanctioned and unsanctioned deceptive communica-
tion. Journal of Nonverbal Behavior, 22(3):189?
204.
Vanessa Feng and Graeme Hirst. 2013. Detecting de-
ceptive opinions with profile compatibility. In Pro-
ceedings of the 6th International Joint Conference
on Natural Language Processing, Nagoya, Japan,
pages 14?18.
Song Feng, Ritwik Banerjee, and Yejin Choi. 2012.
Syntactic stylometry for deception detection. In
Proceedings of the 50th Annual Meeting of the
Association for Computational Linguistics: Short
Papers-Volume 2, pages 171?175. Association for
Computational Linguistics.
Mark Frank and Paul Ekman. 1997. The ability to de-
tect deceit generalizes across different types of high-
stake lies. Journal of personality and social psychol-
ogy, 72(6):1429.
Zolt?an Gy?ongyi, Hector Garcia-Molina, and Jan Ped-
ersen. 2004. Combating web spam with trustrank.
In Proceedings of the Thirtieth international con-
ference on Very large data bases-Volume 30, pages
576?587. VLDB Endowment.
Trevor J Hastie and Robert J Tibshirani. 1990. Gener-
alized additive models, volume 43. CRC Press.
Ipsos. 2012. Socialogue: Five Stars? Thumbs Up? A+
or Just Average? http://www.ipsos-na.com/news-
polls/pressrelease.aspx?id=5929.
Nitin Jindal and Bing Liu. 2008. Opinion spam and
analysis. In Proceedings of the international con-
ference on Web search and web data mining, pages
219?230. ACM.
Nitin Jindal, Bing Liu, and Ee-Peng Lim. 2010. Find-
ing unusual review patterns using unexpected rules.
In Proceedings of the 19th ACM international con-
ference on Information and knowledge management,
pages 1549?1552. ACM.
Thorsten Joachims. 1999. Making large scale svm
learning practical.
Marcia K Johnson and Carol L Raye. 1981. Reality
monitoring. Psychological review, 88(1):67.
Mark Knapp and Mark Comaden. 1979. Telling it like
it isn?t: A review of theory and research on decep-
tive communications. Human Communication Re-
search, 5(3):270?285.
Jiwei Li, Claire Cardie, and Sujian Li. 2013a. Top-
icspam: a topic-model-based approach for spam de-
tection. In Proceedings of the 51th Annual Meeting
of the Association for Computational Linguis-tics.
Jiwei Li, Myle Ott, and Claire Cardie. 2013b. Iden-
tifying manipulated offerings on review portals. In
Proceedings of the 2013 Conference on Empirical
Methods in Natural Language Processing, Seattle,
Wash, pages 18?21.
Ee-Peng Lim, Viet-An Nguyen, Nitin Jindal, Bing Liu,
and Hady Wirawan Lauw. 2010. Detecting prod-
uct review spammers using rating behaviors. In Pro-
ceedings of the 19th ACM international conference
on Information and knowledge management, pages
939?948. ACM.
Juan Martinez-Romo and Lourdes Araujo. 2009. Web
spam identification through language model analy-
sis. In Proceedings of the 5th International Work-
shop on Adversarial Information Retrieval on the
Web, pages 21?28. ACM.
David Meyer. 2009. Fake reviews prompt belkin apol-
ogy.
Claire Miller. 2009. Company settles case of reviews
it faked. New York Times.
Arjun Mukherjee, Bing Liu, Junhui Wang, Natalie
Glance, and Nitin Jindal. 2011. Detecting group
review spam. In Proceedings of the 20th interna-
tional conference companion on World wide web,
pages 93?94. ACM.
Arjun Mukherjee, Bing Liu, and Natalie Glance. 2012.
Spotting fake reviewer groups in consumer reviews.
In Proceedings of the 21st international conference
on World Wide Web, pages 191?200. ACM.
Arjun Mukherjee, Abhinav Kumar, Bing Liu, Junhui
Wang, Meichun Hsu, Malu Castellanos, and Riddhi-
man Ghosh. 2013a. Spotting opinion spammers us-
ing behavioral footprints. In Proceedings of the 19th
ACM SIGKDD international conference on Knowl-
edge discovery and data mining, pages 632?640.
ACM.
Arjun Mukherjee, Vivek Venkataraman, Bing Liu, and
Natalie Glance. 2013b. What yelp fake review fil-
ter might be doing. In Seventh International AAAI
Conference on Weblogs and Social Media.
Matthew L Newman, James W Pennebaker, Diane S
Berry, and Jane M Richards. 2003. Lying words:
Predicting deception from linguistic styles. Person-
ality and social psychology bulletin, 29(5):665?675.
Alexandros Ntoulas, Marc Najork, Mark Manasse, and
Dennis Fetterly. 2006. Detecting spam web pages
through content analysis. In Proceedings of the 15th
international conference on World Wide Web, pages
83?92. ACM.
Myle Ott, Yejin Choi, Claire Cardie, and Jeffrey T.
Hancock. 2011. Finding deceptive opinion spam
by any stretch of the imagination. In Proceedings
of the 49th Annual Meeting of the Association for
Computational Linguistics: Human Language Tech-
nologies, pages 309?319.
1575
Myle Ott, Claire Cardie, and Jeff Hancock. 2012. Esti-
mating the prevalence of deception in online review
communities. In Proceedings of the 21st interna-
tional conference on World Wide Web, pages 201?
210. ACM.
Myle Ott, Claire Cardie, and Jeffrey T. Hancock. 2013.
Negative deceptive opinion spam. In Proceedings of
the 2013 Conference of the North American Chap-
ter of the Association for Computational Linguistics:
Human Language Technologies, Short Papers, At-
lanta, Georgia, USA, June. Association for Compu-
tational Linguistics.
Myle Ott. 2013. Computational lingustic models of
deceptive opinion spam. PHD, thesis.
Sinno Pan and Qiang Yang. 2010. A survey on transfer
learning. Knowledge and Data Engineering, IEEE
Transactions on, 22(10):1345?1359.
Tieyun Qian and Bing Liu. 2013. Identifying multiple
userids of the same author. In Proceedings of the
2013 Conference on Empirical Methods in Natural
Language Processing, Seattle, Wash, pages 18?21.
Paul Rayson, Andrew Wilson, and Geoffrey Leech.
2001. Grammatical word class variation within
the british national corpus sampler. Language and
Computers, 36(1):295?306.
David Streitfeld. 2012. For 2 a star, an online retailer
gets 5-star product reviews. New York Times., 26.
Alexandra Topping. 2010. Historian orlando figes
agrees to pay damages for fake reviews. The
Guardian., 16.
Guan Wang, Sihong Xie, Bing Liu, and Philip Yu.
2011. Review graph based online store review
spammer detection. In Data Mining (ICDM),
2011 IEEE 11th International Conference on, pages
1242?1247. IEEE.
Guan Wang, Sihong Xie, Bing Liu, and Philip Yu.
2012. Identify online store review spammers via so-
cial review graph. ACM Transactions on Intelligent
Systems and Technology (TIST), 3(4):61.
Guangyu Wu, Derek Greene, Barry Smyth, and P?adraig
Cunningham. 2010. Distortion as a validation cri-
terion in the identification of suspicious reviews. In
Proceedings of the First Workshop on Social Media
Analytics, pages 10?13. ACM.
Kyung-Hyan Yoo and Ulrike Gretzel. 2009. Com-
parison of deceptive and truthful travel reviews.
In Information and communication technologies in
tourism 2009, pages 37?47. Springer.
Lina Zhou, Judee K Burgoon, Douglas P Twitchell,
Tiantian Qin, and Jay F Nunamaker Jr. 2004. A
comparison of classification methods for predict-
ing deception in computer-mediated communica-
tion. Journal of Management Information Systems,
20(4):139?166.
1576
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 693?699,
Baltimore, Maryland, USA, June 23-25 2014.
c
?2014 Association for Computational Linguistics
A Piece of My Mind: A Sentiment Analysis Approach
for Online Dispute Detection
Lu Wang
Department of Computer Science
Cornell University
Ithaca, NY 14853
luwang@cs.cornell.edu
Claire Cardie
Department of Computer Science
Cornell University
Ithaca, NY 14853
cardie@cs.cornell.edu
Abstract
We investigate the novel task of online dis-
pute detection and propose a sentiment analy-
sis solution to the problem: we aim to identify
the sequence of sentence-level sentiments ex-
pressed during a discussion and to use them
as features in a classifier that predicts the
DISPUTE/NON-DISPUTE label for the dis-
cussion as a whole. We evaluate dispute de-
tection approaches on a newly created corpus
of Wikipedia Talk page disputes and find that
classifiers that rely on our sentiment tagging
features outperform those that do not. The best
model achieves a very promising F1 score of
0.78 and an accuracy of 0.80.
1 Introduction
As the web has grown in popularity and scope, so
has the promise of collaborative information en-
vironments for the joint creation and exchange of
knowledge (Jones and Rafaeli, 2000; Sack, 2005).
Wikipedia, a wiki-based online encyclopedia, is
arguably the best example: its distributed edit-
ing environment allows readers to collaborate as
content editors and has facilitated the production
of over four billion articles
1
of surprisingly high
quality (Giles, 2005) in English alone since its de-
but in 2001.
Existing studies of collaborative knowledge
systems have shown, however, that the quality of
the generated content (e.g. an encyclopedia arti-
cle) is highly correlated with the effectiveness of
the online collaboration (Kittur and Kraut, 2008;
Kraut and Resnick, 2012); fruitful collaboration,
in turn, inevitably requires dealing with the dis-
putes and conflicts that arise (Kittur et al, 2007).
Unfortunately, human monitoring of the often
massive social media and collaboration sites to de-
tect, much less mediate, disputes is not feasible.
1
http://en.wikipedia.org
In this work, we investigate the heretofore novel
task of dispute detection in online discussions.
Previous work in this general area has analyzed
dispute-laden content to discover features corre-
lated with conflicts and disputes (Kittur et al,
2007). Research focused primarily on cues de-
rived from the edit history of the jointly created
content (e.g. the number of revisions, their tem-
poral density (Kittur et al, 2007; Yasseri et al,
2012)) and relied on small numbers of manually
selected discussions known to involve disputes. In
contrast, we investigate methods for the automatic
detection, i.e. prediction, of discussions involving
disputes. We are also interested in understanding
whether, and which, linguistic features of the dis-
cussion are important for dispute detection.
Drawing inspiration from studies of human me-
diation of online conflicts (e.g. Billings and Watts
(2010), Kittur et al (2007), Kraut and Resnick
(2012)), we hypothesize that effective methods
for dispute detection should take into account the
sentiment and opinions expressed by participants
in the collaborative endeavor. As a result, we
propose a sentiment analysis approach for online
dispute detection that identifies the sequence of
sentence-level sentiments (i.e. very negative, neg-
ative, neutral, positive, very positive) expressed
during the discussion and uses them as features
in a classifier that predicts the DISPUTE/NON-
DISPUTE label for the discussion as a whole. Con-
sider, for example, the snippet in Figure 1 from the
Wikipedia Talk page for the article on Philadel-
phia; it discusses the choice of a picture for the
article?s ?infobox?. The sequence of almost exclu-
sively negative statements provides evidence of a
dispute in this portion of the discussion.
Unfortunately, sentence-level sentiment tagging
for this domain is challenging in its own right
due to the less formal, often ungrammatical, lan-
guage and the dynamic nature of online conver-
sations. ?Really, grow up? (segment 3) should
693
1-Emy111: I think everyone is forgetting that my previous image was the
lead image for well over a year! ...
> Massimo: I?m sorry to say so, but it is grossly over processed...
2-Emy111: i?m glad you paid more money for a camera than I did. con-
grats... i appreciate your constructive criticism. thank you.
> Massimo: I just want to have the best picture as a lead for the article ...
3-Emy111: Wow, I am really enjoying this photography debate... [so don?t
make assumptions you know nothing about.]
NN
[Really, grow up.]
N
[If you
all want to complain about Photoshop editing, lets all go buy medium for-
mat film cameras, shoot film, and scan it, so no manipulation is possible.]
O
[Sound good?]
NN
> Massimo: ... I do feel it is a pity, that you turned out to be a sore loser...
Figure 1: From the Wikipedia Talk page for the article
?Philadelphia?. Omitted sentences are indicated by ellipsis.
Names of editors are in bold. The start of each set of related
turns is numbered; ?>? is an indicator for the reply structure.
presumably be tagged as a negative sentence as
should the sarcastic sentences ?Sounds good?? (in
the same turn) and ?congrats? and ?thank you?
(in segment 2). We expect that these, and other,
examples will be difficult for the sentence-level
classifier unless the discourse context of each sen-
tence is considered. Previous research on senti-
ment prediction for online discussions, however,
focuses on turn-level predictions (Hahn et al,
2006; Yin et al, 2012).
2
As the first work that
predicts sentence-level sentiment for online dis-
cussions, we investigate isotonic Conditional Ran-
dom Fields (CRFs) (Mao and Lebanon, 2007) for
the sentiment-tagging task as they preserve the ad-
vantages of the popular CRF-based sequential tag-
ging models (Lafferty et al, 2001) while provid-
ing an efficient mechanism for encoding domain
knowledge ? in our case, a sentiment lexicon ?
through isotonic constraints on model parameters.
We evaluate our dispute detection approach us-
ing a newly created corpus of discussions from
Wikipedia Talk pages (3609 disputes, 3609 non-
disputes).
3
We find that classifiers that employ the
learned sentiment features outperform others that
do not. The best model achieves a very promis-
ing F1 score of 0.78 and an accuracy of 0.80 on
the Wikipedia dispute corpus. To the best of our
knowledge, this represents the first computational
approach to automatically identify online disputes
on a dataset of scale.
Additional Related Work. Sentiment analysis
has been utilized as a key enabling technique in
a number of conversation-based applications. Pre-
vious work mainly studies the attitudes in spoken
2
A notable exception is Hassan et al (2010), which identi-
fies sentences containing ?attitudes? (e.g. opinions), but does
not distinguish them w.r.t. sentiment. Context information is
also not considered.
3
The talk page associated with each article records con-
versations among editors about the article content and allows
editors to discuss the writing process, e.g. planning and orga-
nizing the content.
meetings (Galley et al, 2004; Hahn et al, 2006) or
broadcast conversations (Wang et al, 2011) using
variants of Conditional Random Fields (Lafferty et
al., 2001) and predicts sentiment at the turn-level,
while our predictions are made for each sentence.
2 Data Construction: A Dispute Corpus
We construct the first dispute detection corpus to
date; it consists of dispute and non-dispute discus-
sions from Wikipedia Talk pages.
Step 1: Get Talk Pages of Disputed Articles.
Wikipedia articles are edited by different editors.
If an article is observed to have disputes on its
talk page, editors can assign dispute tags to the
article to flag it for attention. In this research, we
are interested in talk pages whose corresponding
articles are labeled with the following tags:
DISPUTED, TOTALLYDISPUTED, DISPUTED-
SECTION, TOTALLYDISPUTED-SECTION, POV.
The tags indicate that an article is disputed, or the
neutrality of the article is disputed (POV).
We use the 2013-03-04 Wikipedia data dump,
and extract talk pages for articles that are labeled
with dispute tags by checking the revision history.
This results in 19,071 talk pages.
Step 2: Get Discussions with Disputes. Dis-
pute tags can also be added to talk pages them-
selves. Therefore, in addition to the tags men-
tioned above, we also consider the ?Request for
Comment? (RFC) tag on talk pages. According to
Wikipedia
4
, RFC is used to request outside opin-
ions concerning the disputes.
3609 discussions are collected with dispute
tags found in the revision history. We further
classify dispute discussions into three subcate-
gories: CONTROVERSY, REQUEST FOR COM-
MENT (RFC), and RESOLVED based on the tags
found in discussions (see Table 1). The numbers
of discussions for the three types are 42, 3484, and
105, respectively. Note that dispute tags only ap-
pear in a small number of articles and talk pages.
There may exist other discussions with disputes.
Dispute Subcategory Wikipedia Tags on Talk pages
Controversy CONTROVERSIAL, TOTALLYDISPUTED,
DISPUTED, CALM TALK, POV
Request for Comment RFC
Resolved Any tag from above + RESOLVED
Table 1: Subcategory for disputes with corresponding tags.
Note that each discussion in the RESOLVED class has more
than one tag.
Step 3: Get Discussions without Disputes. Like-
wise, we collect non-dispute discussions from
4
http://en.wikipedia.org/wiki/Wikipedia:
Requests_for_comment
694
pages that are never tagged with disputes. We con-
sider non-dispute discussions with at least 3 dis-
tinct speakers and 10 turns. 3609 discussions are
randomly selected with this criterion. The average
turn numbers for dispute and non-dispute discus-
sions are 45.03 and 22.95, respectively.
3 Sentence-level Sentiment Prediction
This section describes our sentence-level senti-
ment tagger, from which we construct features for
dispute detection (Section 4).
Consider a discussion comprised of sequential
turns; each turn consists of a sequence of sen-
tences. Our model takes as input the sentences
x = {x
1
, ? ? ? , x
n
} from a single turn, and out-
puts the corresponding sequence of sentiment la-
bels y = {y
1
, ? ? ? , y
n
}, where y
i
? O,O =
{NN,N,O,P,PP}. The labels in O represent
very negative (NN), negative (N), neutral (O), pos-
itive (P), and very positive (PP), respectively.
Given that traditional Conditional Random
Fields (CRFs) (Lafferty et al, 2001) ignore the or-
dinal relations among sentiment labels, we choose
isotonic CRFs (Mao and Lebanon, 2007) for
sentence-level sentiment analysis as they can en-
force monotonicity constraints on the parameters
consistent with the ordinal structure and domain
knowledge (e.g. word-level sentiment conveyed
via a lexicon). Concretely, we take a lexiconM =
M
p
?M
n
, whereM
p
andM
n
are two sets of fea-
tures (usually words) identified as strongly associ-
ated with positive and negative sentiment. Assume
?
??,w?
encodes the weight between label ? and
feature w, for each feature w ? M
p
; then the iso-
tonic CRF enforces ? ? ?
?
? ?
??,w?
? ?
??
?
,w?
.
For example, when ?totally agree? is observed in
training, parameter ?
?PP,totally agree?
is likely to
increase. Similar constraints are defined onM
n
.
Our lexicon is built by combining MPQA (Wil-
son et al, 2005), General Inquirer (Stone et al,
1966), and SentiWordNet (Esuli and Sebastiani,
2006) lexicons. Words with contradictory senti-
ments are removed. We use the features in Table 2
for sentiment prediction.
Syntactic/Semantic Features. We have two ver-
sions of dependency relation features, the origi-
nal form and a form that generalizes a word to its
POS tag, e.g. ?nsubj(wrong, you)? is generalized
to ?nsubj(ADJ, you)? and ?nsubj(wrong, PRP)?.
Discourse Features. We extract the initial uni-
gram, bigram, and trigram of each utterance as dis-
Lexical Features Syntactic/Semantic Features
- unigram/bigram - unigram with POS tag
- number of words all uppercased - dependency relation
- number of words Conversation Features
Discourse Features - quote overlap with target
- initial uni-/bi-/tri-gram - TFIDF similarity with target
- repeated punctuations (remove quote first)
- hedging phrases collected from Sentiment Features
Farkas et al (2010) - connective + sentiment words
- number of negators - sentiment dependency relation
- sentiment words
Table 2: Features used in sentence-level sentiment predic-
tion. Numerical features are first normalized by standardiza-
tion, then binned into 5 categories.
course features (Hirschberg and Litman, 1993).
Sentiment Features. We gather connectives from
the Penn Discourse TreeBank (Rashmi Prasad and
Webber, 2008) and combine them with any senti-
ment word that precedes or follows it as new fea-
tures. Sentiment dependency relations are the de-
pendency relations that include a sentiment word.
We replace those words with their polarity equiv-
alents. For example, relation ?nsubj(wrong, you)?
becomes ?nsubj(SentiWord
neg
, you)?.
4 Online Dispute Detection
4.1 Training A Sentiment Classifier
Dataset. We train the sentiment classifier using
the Authority and Alignment in Wikipedia Discus-
sions (AAWD) corpus (Bender et al, 2011) on a 5-
point scale (i.e. NN, N, O, P, PP). AAWD consists
of 221 English Wikipedia discussions with posi-
tive and negative alignment annotations. Annota-
tors either label each sentence as positive, negative
or neutral, or label the full turn. For instances that
have only a turn-level label, we assume all sen-
tences have the same label as the turn. We further
transform the labels into the five sentiment labels.
Sentences annotated as being a positive alignment
by at least two annotators are treated as very posi-
tive (PP). If a sentence is only selected as positive
by one annotator or obtains the label via turn-level
annotation, it is positive (P). Very negative (NN)
and negative (N) are collected in the same way.
All others are neutral (O). Among all 16,501 sen-
tences in AAWD, 1,930 and 1,102 are labeled as
NN and N. 532 and 99 of them are PP and P. The
other 12,648 are considered neutral.
Evaluation. To evaluate the performance of the
sentiment tagger, we compare to two baselines.
(1) Baseline (Polarity): a sentence is predicted as
positive if it has more positive words than nega-
tive words, or negative if more negative words are
observed. Otherwise, it is neutral. (2) Baseline
(Distance) is extended from (Hassan et al, 2010).
Each sentiment word is associated with the closest
695
Pos Neg Neutral
Baseline (Polarity) 22.53 38.61 66.45
Baseline (Distance) 33.75 55.79 88.97
SVM (3-way) 44.62 52.56 80.84
CRF (3-way) 56.28 56.37 89.41
CRF (5-way) 58.39 56.30 90.10
isotonic CRF 68.18 62.53 88.87
Table 3: F1 scores for positive and negative alignment on
Wikipedia Talk pages (AAWD) using 5-fold cross-validation.
In each column, bold entries (if any) are statistically signif-
icantly higher than all the rest. We also compare with an
SVM and linear CRF trained with three classes (3-way). Our
model based on the isotonic CRF produces significantly bet-
ter results than all the other systems.
second person pronoun, and a surface distance is
computed. An SVM classifier (Joachims, 1999) is
trained using features of the sentiment words and
minimum/maximum/average of the distances.
We also compare with two state-of-the-art
methods that are used in sentiment prediction for
conversations: (1) an SVM (RBF kernel) that is
employed for identifying sentiment-bearing sen-
tences (Hassan et al, 2010), and (dis)agreement
detection (Yin et al, 2012) in online debates; (2)
a Linear CRF for (dis)agreement identification in
broadcast conversations (Wang et al, 2011).
We evaluate the systems using standard F1 on
classes of positive, negative, and neutral, where
samples predicted as PP and P are positive align-
ment, and samples tagged as NN and N are neg-
ative alignment. Table 3 describes the main re-
sults on the AAWD dataset: our isotonic CRF
based system significantly outperforms the alter-
natives for positive and negative alignment detec-
tion (paired-t test, p < 0.05).
4.2 Dispute Detection
We model dispute detection as a standard bi-
nary classification task, and investigate four major
types of features as described below.
Lexical Features. We first collect unigram and
bigram features for each discussion.
Topic Features. Articles on specific topics, such
as politics or religions, tend to arouse more dis-
putes. We thus extract the category informa-
tion of the corresponding article for each talk page.
We further utilize unigrams and bigrams of
the category as topic features.
Discussion Features. This type of feature aims
to capture the structure of the discussion. Intu-
itively, the more turns or the more participants
a discussion has, the more likely there is a
dispute. Meanwhile, participants tend to produce
longer utterances when they make arguments.
We choose number of turns, number
of participants, average number of
words in each turn as features. In addi-
tion, the frequency of revisions made during the
discussion has been shown to be good indicator
for controversial articles (Vuong et al, 2008), that
are presumably prone to have disputes. Therefore,
we encode the number of revisions that
happened during the discussion as a feature.
Sentiment Features. This set of features en-
code the sentiment distribution and transition in
the discussion. We train our sentiment tagging
model on the full AAWD dataset, and run it on
the Wikipedia dispute corpus.
Given that consistent negative senti-
ment flow usually indicates an ongoing
dispute, we first extract features from
sentiment distribution in the form
of number/probability of sentiment
per type. We also estimate the sentiment
transition probability P (S
t
? S
t+1
) from
our predictions, where S
t
and S
t+1
are sentiment
labels for the current sentence and the next. We
then have features as number/portion of
sentiment transitions per type.
Features described above mostly depict the
global sentiment flow in the discussions. We fur-
ther construct a local version of them, since sen-
timent distribution may change as discussion pro-
ceeds. For example, less positive sentiment can be
observed as dispute being escalated. We thus split
each discussion into three equal length stages, and
create sentiment distribution and transition fea-
tures for each stage.
Prec Rec F1 Acc
Baseline (Random) 50.00 50.00 50.00 50.00
Baseline (All dispute) 50.00 100.00 66.67 50.00
Logistic Regression 74.76 72.29 73.50 73.94
SVM
Linear
69.81 71.90 70.84 70.41
SVM
RBF
77.38 79.14 78.25 80.00
Table 4: Dispute detection results on Wikipedia Talk pages.
The numbers are multiplied by 100. The items in bold are sta-
tistically significantly higher than others in the same column
(paired-t test, p < 0.05). SVM with the RBF kernel achieves
the best performance in precision, F1, and accuracy.
Results and Error Analysis. We experiment with
logistic regression, SVM with linear and RBF ker-
nels, which are effective methods in multiple text
categorization tasks (Joachims, 1999; Zhang and
J. Oles, 2001). We normalize the features by stan-
dardization and conduct a 5-fold cross-validation.
Two baselines are listed: (1) labels are randomly
assigned; (2) all discussions have disputes.
Main results for different classifiers are dis-
played in Table 4. All learning based methods
696
T1 T2 T3T4T5 T6 T7 T8 T9 T10 T11 T12 T13T14 T15 T16 T17 T182
10
12Sentimen
t
A B C D EF
Sentiment Flow in Discussion with Unresolved Dispute
Sample sentences (sentiment in parentheses)
A: no, I sincerely plead with you... (N) If not, you are just wasting my
time. (NN)
B: I believe Sweet?s proposal... is quite silly. (NN)
C: Tell you what. (NN) If you can get two other editors to agree... I will
shut up and sit down. (NN)
D: But some idiot forging your signature claimed that doing so would
violate. (NN)... Please go have some morning coffee. (O)
E: And I don?t like coffee. (NN) Good luck to you. (NN)
F: Was that all? (NN)... I think that you are in error... (N)
T1 T2 T3 T4 T5 T6 T7 T8 T9 T10 T11 T12 T13 T142
10
12Sentimen
t
A B
C
D
E FSentiment Flow in Discussion with Resolved Dispute A: So far so confusing. (NN)...B: ... I can not see a rationale for the landrace having its own article...(N) With Turkish Van being a miserable stub, there?s no such rationale for
forking off a new article... (NN)...
C: I?ve also copied your post immediately above to that article?s talk page
since it is a great ?nutshell? summary. (PP)
D: Err.. how can the opposite be true... (N)
E: Thanks for this, though I have to say some of the facts floating around
this discussion are wrong. (P)
F: Great. (PP) Let?s make sure the article is clear on this. (O)
Figure 2: Sentiment flow for a discussion with unresolved dispute about the definition of ?white people? (top) and a dis-
cussion with resolved dispute on merging articles about van cat (bottom). The labels {NN,N,O,P,PP} are mapped to
{?2,?1, 0, 1, 2} in sequence. Sentiment values are convolved by Gaussian smoothing kernel, and cubic-spline interpolation is
then conducted. Different speakers are represented by curves of different colors. Dashed vertical lines delimit turns. Represen-
tative sentences are labeled with letters and their sentiment labels are shown on the right. For unresolved dispute (top), we see
that negative sentiment exists throughout the discussion. Whereas, for the resolved dispute (bottom), less negative sentiment is
observed at the end of the discussion; participants also show appreciation after the problem is solved (e.g. E and F in the plot).
Prec Rec F1 Acc
Lexical (Lex) 75.86 34.66 47.58 61.82
Topic (Top) 68.44 71.46 69.92 69.26
Discussion (Dis) 69.73 76.14 72.79 71.54
Sentiment (Senti
g+l
) 72.54 69.52 71.00 71.60
Top + Dis 68.49 71.79 70.10 69.38
Top + Dis + Senti
g
77.39 78.36 77.87 77.74
Top + Dis + Senti
g+l
77.38 79.14 78.25 80.00
Lex + Top + Dis + Senti
g+l
78.38 75.12 76.71 77.20
Table 5: Dispute detection results with different feature
sets by SVM with RBF kernel. The numbers are multi-
plied by 100. Senti
g
represents global sentiment features, and
Senti
g+l
includes both global and local features. The number
in bold is statistically significantly higher than other numbers
in the same column (paired-t test, p < 0.05), and the italic
entry has the highest absolute value.
outperform the two baselines, and among them,
SVM with the RBF kernel achieves the best F1
score and accuracy (0.78 and 0.80). Experimental
results with various combinations of features sets
are displayed in Table 5. As it can be seen, senti-
ment features obtains the best accuracy among the
four types of features. A combination of topic, dis-
cussion, and sentiment features achieves the best
performance on recall, F1, and accuracy. Specif-
ically, the accuracy is significantly higher than all
the other systems (paired-t test, p < 0.05).
After a closer look at the results, we find two
main reasons for incorrect predictions. Firstly,
sentiment prediction errors get propagated into
dispute detection. Due to the limitation of ex-
isting general-purpose lexicons, some opinionated
dialog-specific terms are hard to catch. For exam-
ple, ?I told you over and over again...? strongly
suggests a negative sentiment, but no single word
shows negative connotation. Constructing a lexi-
con tuned for conversational text may improve the
performance. Secondly, some dispute discussions
are harder to detect than the others due to differ-
ent dialog structures. For instance, the recalls for
dispute discussions of ?controversy?, ?RFC?, and
?resolved? are 0.78, 0.79, and 0.86 respectively.
We intend to design models that are able to cap-
ture dialog structures in the future work.
Sentiment Flow Visualization. We visualize the
sentiment flow of two disputed discussions in Fig-
ure 2. The plots reveal persistent negative sen-
timent in unresolved disputes (top). For the re-
solved dispute (bottom), participants show grati-
tude when the problem is settled.
5 Conclusion
We present a sentiment analysis-based approach
to online dispute detection. We create a large-
scale dispute corpus from Wikipedia Talk pages to
study the problem. A sentiment prediction model
based on isotonic CRFs is proposed to output sen-
timent labels at the sentence-level. Experiments
on our dispute corpus also demonstrate that clas-
sifiers trained with sentiment tagging features out-
perform others that do not.
Acknowledgments We heartily thank the Cornell
NLP Group, the reviewers, and Yiye Ruan for
helpful comments. We also thank Emily Ben-
der and Mari Ostendorf for providing the AAWD
dataset. This work was supported in part by NSF
grants IIS-0968450 and IIS-1314778, and DARPA
DEFT Grant FA8750-13-2-0015. The views and
conclusions contained herein are those of the au-
thors and should not be interpreted as necessarily
representing the official policies or endorsements,
either expressed or implied, of NSF, DARPA or
the U.S. Government.
697
References
Emily M. Bender, Jonathan T. Morgan, Meghan Ox-
ley, Mark Zachry, Brian Hutchinson, Alex Marin,
Bin Zhang, and Mari Ostendorf. 2011. Anno-
tating social acts: Authority claims and alignment
moves in wikipedia talk pages. In Proceedings of
the Workshop on Languages in Social Media, LSM
?11, pages 48?57, Stroudsburg, PA, USA. Associa-
tion for Computational Linguistics.
Matt Billings and Leon Adam Watts. 2010. Under-
standing dispute resolution online: using text to re-
flect personal and substantive issues in conflict. In
Elizabeth D. Mynatt, Don Schoner, Geraldine Fitz-
patrick, Scott E. Hudson, W. Keith Edwards, and
Tom Rodden, editors, CHI, pages 1447?1456. ACM.
Andrea Esuli and Fabrizio Sebastiani. 2006. Sen-
tiwordnet: A publicly available lexical resource
for opinion mining. In In Proceedings of the 5th
Conference on Language Resources and Evaluation
(LREC06), pages 417?422.
Rich?ard Farkas, Veronika Vincze, Gy?orgy M?ora, J?anos
Csirik, and Gy?orgy Szarvas. 2010. The conll-2010
shared task: Learning to detect hedges and their
scope in natural language text. In Proceedings of
the Fourteenth Conference on Computational Natu-
ral Language Learning ? Shared Task, CoNLL ?10:
Shared Task, pages 1?12, Stroudsburg, PA, USA.
Association for Computational Linguistics.
Michel Galley, Kathleen McKeown, Julia Hirschberg,
and Elizabeth Shriberg. 2004. Identifying agree-
ment and disagreement in conversational speech:
use of Bayesian networks to model pragmatic de-
pendencies. In ACL ?04: Proceedings of the 42nd
Annual Meeting on Association for Computational
Linguistics, pages 669+, Morristown, NJ, USA. As-
sociation for Computational Linguistics.
Jim Giles. 2005. Internet encyclopaedias go head to
head. Nature, 438(7070):900?901.
Sangyun Hahn, Richard Ladner, and Mari Ostendorf.
2006. Agreement/disagreement classification: Ex-
ploiting unlabeled data using contrast classifiers.
In Proceedings of the Human Language Technol-
ogy Conference of the NAACL, Companion Volume:
Short Papers, pages 53?56, New York City, USA,
June. Association for Computational Linguistics.
Ahmed Hassan, Vahed Qazvinian, and Dragomir
Radev. 2010. What?s with the attitude?: Identify-
ing sentences with attitude in online discussions. In
Proceedings of the 2010 Conference on Empirical
Methods in Natural Language Processing, EMNLP
?10, pages 1245?1255, Stroudsburg, PA, USA. As-
sociation for Computational Linguistics.
Julia Hirschberg and Diane Litman. 1993. Empirical
studies on the disambiguation of cue phrases. Com-
put. Linguist., 19(3):501?530, September.
Thorsten Joachims. 1999. Advances in kernel meth-
ods. chapter Making Large-scale Support Vector
Machine Learning Practical, pages 169?184. MIT
Press, Cambridge, MA, USA.
Quentin Jones and Sheizaf Rafaeli. 2000. Time to
split, virtually: discourse architecture and commu-
nity building create vibrant virtual publics. Elec-
tronic Markets, 10:214?223.
Aniket Kittur and Robert E. Kraut. 2008. Harness-
ing the wisdom of crowds in wikipedia: Quality
through coordination. In Proceedings of the 2008
ACM Conference on Computer Supported Coopera-
tive Work, CSCW ?08, pages 37?46, New York, NY,
USA. ACM.
Aniket Kittur, Bongwon Suh, Bryan A. Pendleton, and
Ed H. Chi. 2007. He says, she says: Conflict and
coordination in wikipedia. In Proceedings of the
SIGCHI Conference on Human Factors in Comput-
ing Systems, CHI ?07, pages 453?462, New York,
NY, USA. ACM.
Robert E. Kraut and Paul Resnick. 2012. Building suc-
cessful online communities: Evidence-based social
design. MIT Press, Cambridge, MA.
John D. Lafferty, Andrew McCallum, and Fernando
C. N. Pereira. 2001. Conditional random fields:
Probabilistic models for segmenting and labeling se-
quence data. In Proceedings of the Eighteenth Inter-
national Conference on Machine Learning, ICML
?01, pages 282?289, San Francisco, CA, USA. Mor-
gan Kaufmann Publishers Inc.
Yi Mao and Guy Lebanon. 2007. Isotonic conditional
random fields and local sentiment flow. In Advances
in Neural Information Processing Systems.
Alan Lee Eleni Miltsakaki Livio Robaldo Ar-
avind Joshi Rashmi Prasad, Nikhil Dinesh and Bon-
nie Webber. 2008. The penn discourse tree-
bank 2.0. In Proceedings of the Sixth Interna-
tional Conference on Language Resources and Eval-
uation (LREC?08), Marrakech, Morocco, may. Eu-
ropean Language Resources Association (ELRA).
http://www.lrec-conf.org/proceedings/lrec2008/.
Warren Sack. 2005. Digital formations: It and new
architectures in the global realm. chapter Discourse
architecture and very large-scale conversation, pages
242?282. Princeton University Press, Princeton, NJ
USA.
Philip J. Stone, Dexter C. Dunphy, Marshall S. Smith,
and Daniel M. Ogilvie. 1966. The General In-
quirer: A Computer Approach to Content Analysis.
MIT Press, Cambridge, MA.
Ba-Quy Vuong, Ee-Peng Lim, Aixin Sun, Minh-Tam
Le, Hady Wirawan Lauw, and Kuiyu Chang. 2008.
On ranking controversies in wikipedia: Models and
evaluation. In Proceedings of the 2008 Interna-
tional Conference on Web Search and Data Mining,
WSDM ?08, pages 171?182, New York, NY, USA.
ACM.
698
Wen Wang, Sibel Yaman, Kristin Precoda, Colleen
Richey, and Geoffrey Raymond. 2011. Detection of
agreement and disagreement in broadcast conversa-
tions. In Proceedings of the 49th Annual Meeting of
the Association for Computational Linguistics: Hu-
man Language Technologies: Short Papers - Volume
2, HLT ?11, pages 374?378, Stroudsburg, PA, USA.
Association for Computational Linguistics.
Theresa Wilson, Janyce Wiebe, and Paul Hoffmann.
2005. Recognizing contextual polarity in phrase-
level sentiment analysis. In Proceedings of the Con-
ference on Human Language Technology and Em-
pirical Methods in Natural Language Processing,
HLT ?05, pages 347?354, Stroudsburg, PA, USA.
Association for Computational Linguistics.
Taha Yasseri, R?obert Sumi, Andr?as Rung, Andr?as Kor-
nai, and J?anos Kert?esz. 2012. Dynamics of conflicts
in wikipedia. CoRR, abs/1202.3643.
Jie Yin, Paul Thomas, Nalin Narang, and Cecile Paris.
2012. Unifying local and global agreement and
disagreement classification in online debates. In
Proceedings of the 3rd Workshop in Computational
Approaches to Subjectivity and Sentiment Analysis,
WASSA ?12, pages 61?69, Stroudsburg, PA, USA.
Association for Computational Linguistics.
Tong Zhang and Frank J. Oles. 2001. Text categoriza-
tion based on regularized linear classification meth-
ods. Inf. Retr., 4(1):5?31, April.
699
Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 1: Proceedings of the Main Conference
and the Shared Task, pages 221?228, Atlanta, Georgia, June 13-14, 2013. c?2013 Association for Computational Linguistics
CPN-CORE: A Text Semantic Similarity System Infused
with Opinion Knowledge
Carmen Banea[?, Yoonjung Choi?, Lingjia Deng?, Samer Hassan?, Michael Mohler?
Bishan Yang
?
, Claire Cardie
?
, Rada Mihalcea[?, Janyce Wiebe?
[University of North Texas
Denton, TX
?University of Pittsburgh
Pittsburgh, PA
?Google Inc.
Mountain View, CA
?Language Computer Corp.
Richardson, TX
?
Cornell University
Ithaca, NY
Abstract
This article provides a detailed overview of the
CPN text-to-text similarity system that we par-
ticipated with in the Semantic Textual Similar-
ity task evaluations hosted at *SEM 2013. In
addition to more traditional components, such
as knowledge-based and corpus-based met-
rics leveraged in a machine learning frame-
work, we also use opinion analysis features to
achieve a stronger semantic representation of
textual units. While the evaluation datasets are
not designed to test the similarity of opinions,
as a component of textual similarity, nonethe-
less, our system variations ranked number 38,
39 and 45 among the 88 participating systems.
1 Introduction
Measures of text similarity have been used for a long
time in applications in natural language processing
and related areas. One of the earliest applications
of text similarity is perhaps the vector-space model
used in information retrieval, where the document
most relevant to an input query is determined by
ranking documents in a collection in reversed or-
der of their angular distance with the given query
(Salton and Lesk, 1971). Text similarity has also
been used for relevance feedback and text classifi-
cation (Rocchio, 1971), word sense disambiguation
(Lesk, 1986; Schutze, 1998), and extractive summa-
rization (Salton et al, 1997), in the automatic evalu-
ation of machine translation (Papineni et al, 2002),
?carmen.banea@gmail.com
? rada@cs.unt.edu
text summarization (Lin and Hovy, 2003), text co-
herence (Lapata and Barzilay, 2005) and in plagia-
rism detection (Nawab et al, 2011).
Earlier work on this task has primarily focused on
simple lexical matching methods, which produce a
similarity score based on the number of lexical units
that occur in both input segments. Improvements
to this simple method have considered stemming,
stopword removal, part-of-speech tagging, longest
subsequence matching, as well as various weight-
ing and normalization factors (Salton and Buckley,
1997). While successful to a certain degree, these
lexical similarity methods cannot always identify the
semantic similarity of texts. For instance, there is an
obvious similarity between the text segments ?she
owns a dog? and ?she has an animal,? yet these
methods will mostly fail to identify it.
More recently, researchers have started to con-
sider the possibility of combining the large number
of word-to-word semantic similarity measures (e.g.,
(Jiang and Conrath, 1997; Leacock and Chodorow,
1998; Lin, 1998; Resnik, 1995)) within a semantic
similarity method that works for entire texts. The
methods proposed to date in this direction mainly
consist of either bipartite-graph matching strate-
gies that aggregate word-to-word similarity into a
text similarity score (Mihalcea et al, 2006; Islam
and Inkpen, 2009; Hassan and Mihalcea, 2011;
Mohler et al, 2011), or data-driven methods that
perform component-wise additions of semantic vec-
tor representations as obtained with corpus mea-
sures such as latent semantic analysis (Landauer et
al., 1997), explicit semantic analysis (Gabrilovich
and Markovitch, 2007), or salient semantic analysis
221
(Hassan and Mihalcea, 2011).
In this paper, we describe the system variations
with which we participated in the *SEM 2013 task
on semantic textual similarity (Agirre et al, 2013).
The system builds upon our earlier work on corpus-
based and knowledge-based methods of text seman-
tic similarity (Mihalcea et al, 2006; Hassan and
Mihalcea, 2011; Mohler et al, 2011; Banea et al,
2012), while also incorporating opinion aware fea-
tures. Our observation is that text is not only similar
on a semantic level, but also with respect to opin-
ions. Let us consider the following text segments:
?she owns a dog? and ?I believe she owns a dog.?
The question then becomes how similar these text
fragments truly are. Current systems will consider
the two sentences semantically equivalent, yet to a
human, they are not. A belief is not equivalent to a
fact (and for the case in point, the person may very
well have a cat or some other pet), and this should
consequently lower the relatedness score. For this
reason, we advocate that STS systems should also
consider the opinions expressed and their equiva-
lence. While the *SEM STS task is not formulated
to evaluate this type of similarity, we complement
more traditional corpus and knowledge-based meth-
ods with opinion aware features, and use them in
a meta-learning framework in an arguably first at-
tempt at incorporating this type of information to in-
fer text-to-text similarity.
2 Related Work
Over the past years, the research community has
focused on computing semantic relatedness using
methods that are either knowledge-based or corpus-
based. Knowledge-based methods derive a measure
of relatedness by utilizing lexical resources and on-
tologies such as WordNet (Miller, 1995) to measure
definitional overlap, term distance within a graph-
ical taxonomy, or term depth in the taxonomy as
a measure of specificity. We explore several of
these measures in depth in Section 3.3.1. On the
other side, corpus-based measures such as Latent
Semantic Analysis (LSA) (Landauer et al, 1997),
Explicit Semantic Analysis (ESA) (Gabrilovich
and Markovitch, 2007), Salient Semantic Analysis
(SSA) (Hassan and Mihalcea, 2011), Pointwise Mu-
tual Information (PMI) (Church and Hanks, 1990),
PMI-IR (Turney, 2001), Second Order PMI (Islam
and Inkpen, 2006), Hyperspace Analogues to Lan-
guage (Burgess et al, 1998) and distributional simi-
larity (Lin, 1998) employ probabilistic approaches
to decode the semantics of words. They consist
of unsupervised methods that utilize the contextual
information and patterns observed in raw text to
build semantic profiles of words. Unlike knowledge-
based methods, which suffer from limited coverage,
corpus-based measures are able to induce a similar-
ity between any given two words, as long as they
appear in the very large corpus used as training.
3 Semantic Textual Similarity System
3.1 Task Setup
The STS task consists of labeling one sentence pair
at a time, based on the semantic similarity existent
between its two component sentences. Human as-
signed similarity scores range from 0 (no relation)
to 5 (semantivally equivalent). The *SEM 2013 STS
task did not provide additional labeled data to the
training and testing sets released as part of the STS
task hosted at SEMEVAL 2012 (Agirre et al, 2012);
our system variations were trained on SEMEVAL
2012 data.
The test sets (Agirre et al, 2013) consist of
text pairs extracted from headlines (headlines,
750 pairs), sense definitions from WordNet and
OntoNotes (OnWN, 561 pairs), sense definitions
from WordNet and FrameNet (FNWN, 189 pairs),
and data used in the evaluation of machine transla-
tion systems (SMT, 750 pairs).
3.2 Resources
Various subparts of our framework use several re-
sources that are described in more detail below.
Wikipedia1 is the most comprehensive encyclo-
pedia to date, and it is an open collaborative effort
hosted on-line. Its basic entry is an article which in
addition to describing an entity or an event also con-
tains hyperlinks to other pages within or outside of
Wikipedia. This structure (articles and hyperlinks)
is directly exploited by semantic similarity methods
such as ESA (Gabrilovich and Markovitch, 2007),
or SSA (Hassan and Mihalcea, 2011)2.
1www.wikipedia.org
2In the experiments reported in this paper, all the corpus-
based methods are trained on the English Wikipedia download
from October 2008.
222
WordNet (Miller, 1995) is a manually crafted lex-
ical resource that maintains semantic relationships
such as synonymy, antonymy, hypernymy, etc., be-
tween basic units of meaning, or synsets. These rela-
tionships are employed by various knowledge-based
methods to derive semantic similarity.
The MPQA corpus (Wiebe and Riloff, 2005) is
a newswire data set that was manually annotated
at the expression level for opinion-related content.
Some of the features derived by our opinion extrac-
tion models were based on training on this corpus.
3.3 Features
Our system variations derive the similarity score of a
given sentence-pair by integrating information from
knowledge, corpus, and opinion-based sources3.
3.3.1 Knowledge-Based Features
Following prior work from our group (Mihalcea
et al, 2006; Mohler and Mihalcea, 2009), we em-
ploy several WordNet-based similarity metrics for
the task of sentence-level similarity. Briefly, for each
open-class word in one of the input texts, we com-
pute the maximum semantic similarity4 that can be
obtained by pairing it with any open-class word in
the other input text. All the word-to-word similarity
scores obtained in this way are summed and normal-
ized to the length of the two input texts. We provide
below a short description for each of the similarity
metrics employed by this system.
The shortest path (Path) similarity is equal to:
Simpath =
1
length
(1)
where length is the length of the shortest path be-
tween two concepts using node-counting.
The Leacock & Chodorow (Leacock and
Chodorow, 1998) (LCH) metric is equal to:
Simlch = ? log
length
2 ?D
(2)
where length is the length of the shortest path be-
tween two concepts using node-counting, and D is
the maximum depth of the taxonomy.
The Lesk (Lesk) similarity of two concepts is de-
fined as a function of the overlap between the cor-
responding definitions, as provided by a dictionary.
3The abbreviation in italics accompanying each method al-
lows for cross-referencing with the results listed in Table 2.
4We use the WordNet::Similarity package (Pedersen et al,
2004).
It is based on an algorithm proposed by Lesk (1986)
as a solution for word sense disambiguation.
The Wu & Palmer (Wu and Palmer, 1994) (WUP )
similarity metric measures the depth of two given
concepts in the WordNet taxonomy, and the depth
of the least common subsumer (LCS), and combines
these figures into a similarity score:
Simwup =
2 ? depth(LCS)
depth(concept1) + depth(concept2)
(3)
The measure introduced by Resnik (Resnik, 1995)
(RES) returns the information content (IC) of the
LCS of two concepts:
Simres = IC(LCS) (4)
where IC is defined as:
IC(c) = ? logP (c) (5)
and P (c) is the probability of encountering an in-
stance of concept c in a large corpus.
The measure introduced by Lin (Lin, 1998) (Lin)
builds on Resnik?s measure of similarity, and adds
a normalization factor consisting of the information
content of the two input concepts:
Simlin =
2 ? IC(LCS)
IC(concept1) + IC(concept2)
(6)
We also consider the Jiang & Conrath (Jiang and
Conrath, 1997) (JCN ) measure of similarity:
Simjnc =
1
IC(concept1) + IC(concept2)? 2 ? IC(LCS)
(7)
3.3.2 Corpus Based Features
While most of the corpus-based methods induce
semantic profiles in a word-space, where the seman-
tic profile of a word is expressed in terms of its co-
occurrence with other words, LSA, ESA and SSA
rely on a concept-space representation, thus express-
ing a word?s semantic profile in terms of the im-
plicit (LSA), explicit (ESA), or salient (SSA) con-
cepts. This departure from the sparse word-space to
a denser, richer, and unambiguous concept-space re-
solves one of the fundamental problems in semantic
relatedness, namely the vocabulary mismatch.
Latent Semantic Analysis (LSA) (Landauer et al,
1997). In LSA, term-context associations are cap-
tured by means of a dimensionality reduction op-
erated by a singular value decomposition (SVD)
223
on the term-by-context matrix T, where the ma-
trix is induced from a large corpus. This reduc-
tion entails the abstraction of meaning by collaps-
ing similar contexts and discounting noisy and ir-
relevant ones, hence transforming the real world
term-context space into a word-latent-concept space
which achieves a much deeper and concrete seman-
tic representation of words5.
Random Projection (RP ) (Dasgupta, 1999). In RP,
a high dimensional space is projected onto a lower
dimensional one, using a randomly generated ma-
trix. (Bingham and Mannila, 2001) show that unlike
LSA or principal component analysis (PCA), RP
is computationally efficient for large corpora, while
also retaining accurate vector similarity and yielding
comparable results.
Explicit Semantic Analysis (ESA) (Gabrilovich
and Markovitch, 2007). ESA uses encyclopedic
knowledge in an information retrieval framework to
generate a semantic interpretation of words. It relies
on the distribution of words inside Wikipedia arti-
cles, thus building a semantic representation for a
given word using a word-document association.
Salient Semantic Analysis (SSA) (Hassan and Mi-
halcea, 2011). SSA incorporates a similar seman-
tic abstraction as ESA, yet it uses salient con-
cepts gathered from encyclopedic knowledge, where
a ?concept? represents an unambiguous expression
which affords an encyclopedic definition. Saliency
in this case is determined based on the word being
hyperlinked in context, implying that it is highly rel-
evant to the given text.
In order to determine the similarity of two text
fragments, we employ two variations: the typical
cosine similarity (cos) and a best alignment strat-
egy (align), which we explain in more detail in
the paragraph below. Both variations were paired
with the ESA, and SSA systems resulting in four
similarity scores that were used as features by our
meta-system, namely ESAcos, ESAalign, SSAcos,
and SSAalign; in addition, we also used BOWcos,
LSAcos, and RPcos.
Best Alignment Strategy (align). Let Ta and Tb be
two text fragments of size a and b respectively. After
removing all stopwords, we first determine the num-
5We use the LSA implementation available at code.
google.com/p/semanticvectors/.
ber of shared terms (?) between Ta and Tb. Second,
we calculate the semantic relatedness of all possible
pairings between non-shared terms in Ta and Tb. We
further filter these possible combinations by creating
a list ? which holds the strongest semantic pairings
between the fragments? terms, such that each term
can only belong to one and only one pair.
Sim(Ta, Tb) =
(? +
?|?|
i=1 ?i)? (2ab)
a+ b
(8)
where ?i is the similarity score for the ith pairing.
3.3.3 Opinion Aware Features
We design opinion-aware features to capture sen-
tence similarity on the subjectivity level based on the
output of three subjectivity analysis systems. Intu-
itively, two sentences are similar in terms of sub-
jectivity if there exists similar opinion expressions
which also share similar opinion holders.
OpinionFinder (Wilson et al, 2005) is a publicly
available opinion extraction model that annotates the
subjectivity of new text based on the presence (or
absence) of words or phrases in a large lexicon. The
system consists of a two step process, by feeding
the sentences identified as subjective or objective
by a rule-based high-precision classifier to a high-
recall classifier that iteratively learns from the re-
maining corpus. For each sentence in a STS pair,
the two classifiers provide two predictions; a subjec-
tivity similarity score (SUBJSL) is computed as fol-
lows. If both sentences are classified as subjective
or objective, the score is 1; if one is subjective and
the other one is objective, the score is -1; otherwise
it is 0. We also make use of the output of the sub-
jective expression identifier in OpinionFinder. We
first record how many expressions the two sentences
have: feature NUMEX1 and NUMEX2. Then we
compare how many tokens these expressions share
and we normalize by the total number of expressions
(feature EXPR).
We compute the difference between the probabil-
ities of the two sentences being subjective (SUBJD-
IFF), by employing a logistic regression classifier
using LIBLINEAR (Fan et al, 2008) trained on the
MPQA corpus. The smaller the difference, the more
similar the sentences are in terms of subjectivity.
We also employ features produced by the opinion-
extraction model of Yang and Cardie (Yang and
Cardie, 2012), which is better suited to process ex-
224
pressions of arbitrary length. Specifically, for each
sentence, we extract subjective expressions and gen-
erate the following features. SUBJCNT is a binary
feature which is equal to 1 if both sentences con-
tain a subjective expression. DSEALGN marks the
number of shared words between subjective expres-
sions in two sentences, while DSESIM represents
their similarity beyond the word level. We repre-
sent the subjective expressions in each sentence as
a feature vector, containing unigrams extracted from
the expressions, their part-of-speech, their WordNet
hypernyms and their subjectivity label6, and com-
pute the cosine similarity between the feature vec-
tors. The holder of the opinion expressions is ex-
tracted with the aid of a dependency parser7. In most
cases, the opinion holder and the opinion expression
are related by the dependency relation subj. This re-
lation is used to expand the verb dependents in the
opinion expression and identify the opinion holder
or AGENT.
3.4 Meta-learning
Each metric described above provides one individ-
ual score for every sentence-pair in both the train-
ing and test set. These scores then serve as in-
put to a meta-learner, which adjusts their impor-
tance, and thus their bearing on the overall similar-
ity score predicted by the system. We experimented
with regression and decision tree based algorithms
by performing 10-fold cross validation on the 2012
training data; these types of learners are particularly
well suited to maintain the ordinality of the seman-
tic similarity scores (i.e. a score of 4.5 is closer
to either 4 or 5, implying that the two sentences
are mostly or fully equivalent, while also being far
further away from 0, implying no semantic relat-
edness between the two sentences). We obtained
consistent results when using support vector regres-
sion with polynomial kernel (Drucker et al, 1997;
Smola and Schoelkopf, 1998) (SV R) and random
subspace meta-classification with tree learners (Ho,
1998) (RandSubspace)8.
We submitted three system variations based
on the training corpus (first word in the sys-
6Label is based on the OpinionFinder subjectivity lexicon
(Wiebe et al, 2005).
7nlp.stanford.edu/software/
8Included with the Weka framework (Hall et al, 2009); we
used the default values for both algorithms.
System FNWN headlines OnWN SMT Mean
comb.RandSubSpace 0.331 0.677 0.514 0.337 0.494
comb.SVR 0.362 0.669 0.510 0.341 0.494
indv.RandSubspace 0.331 0.677 0.548 0.277 0.483
baseline-tokencos 0.215 0.540 0.283 0.286 0.364
Table 1: Evaluation results (Agirre et al, 2013).
tem name) or the learning methodology (second
word) used: comb.RandSubspace, comb.SV R and
indv.RandSubspace. For comb, training was per-
formed on the merged version of the entire 2012 SE-
MEVAL dataset. For indv, predictions for OnWN
and SMT test data were based on training on
matching OnWN and SMT 9 data from 2012, pre-
dictions for the other test sets were computed using
the combined version (comb).
4 Results and Discussion
Table 2 lists the correlations obtained between
the scores assigned by each one of the features
we used and the scores assigned by the human
judges. It is interesting to note that overall, corpus-
based measures are stronger performers compared to
knowledge-based measures. The top contenders in
the former group are ESAalign, SSAalign, LSAcos,
and RPcos, indicating that these methods are able to
leverage a significant amount of semantic informa-
tion from text. While LSAcos achieves high corre-
lations on many of the datasets, replacing the singu-
lar value decomposition operation by random pro-
jection to a lower-dimension space (RP ) achieves
competitive results while also being computation-
ally efficient. This observation is in line with prior
literature (Bingham and Mannila, 2001). Among
the knowledge-based methods, JCN and Path
achieve high performance on more than five of the
datasets. In some cases, particularly on the 2013
test data, the shortest path method (Path) peforms
better or on par with the performance attained by
other knowledge-based measures, despite its com-
putational simplicity. While opinion-based mea-
sures do not exhibit the same high correlation, we
should remember that none of the datasets displays
consistent opinion content, nor were they anno-
tated with this aspect in mind, in order for this in-
formation to be properly leveraged and evaluated.
9The SMT training set is a combination of SMTeuroparl
(in this paper abbreviated as SMTep) and SMTnews data.
225
Train 2012 Test 2012 Test 2013
Feature SMTep MSRpar MSRvid SMTep MSRpar MSRvid OnWN SMTnews FNWN headlines OnWN SMT
Knowledge-based measures
JCN 0.51 0.49 0.63 0.48 0.48 0.64 0.62 0.28 0.38 0.72 0.71 0.34
LCH 0.45 0.48 0.49 0.47 0.49 0.54 0.54 0.3 0.39 0.69 0.69 0.32
Lesk 0.5 0.48 0.59 0.5 0.47 0.63 0.64 0.4 0.4 0.71 0.7 0.33
Lin 0.48 0.49 0.54 0.48 0.48 0.56 0.57 0.27 0.28 0.65 0.66 0.3
Path 0.5 0.49 0.62 0.48 0.49 0.65 0.62 0.35 0.43 0.72 0.73 0.34
RES 0.48 0.47 0.55 0.49 0.47 0.6 0.62 0.33 0.28 0.64 0.7 0.31
WUP 0.42 0.46 0.38 0.44 0.48 0.42 0.48 0.26 0.19 0.55 0.6 0.25
Corpus-based measures
BOW cos 0.51 0.47 0.69 0.32 0.44 0.71 0.66 0.37 0.34 0.68 0.52 0.32
ESA cos 0.53 0.34 0.71 0.44 0.3 0.77 0.63 0.44 0.34 0.55 0.35 0.27
ESA align 0.55 0.56 0.75 0.49 0.52 0.78 0.69 0.38 0.46 0.71 0.47 0.34
SSA cos 0.4 0.34 0.63 0.4 0.22 0.71 0.6 0.42 0.35 0.48 0.47 0.26
SSA align 0.54 0.56 0.74 0.49 0.51 0.77 0.68 0.38 0.44 0.69 0.46 0.34
LSA cos 0.65 0.48 0.76 0.36 0.45 0.79 0.67 0.45 0.25 0.63 0.61 0.32
RP cos 0.6 0.49 0.78 0.46 0.43 0.79 0.7 0.45 0.38 0.68 0.57 0.34
Opinion-aware measures
AGENT 0.16 0.15 0.05 0.11 0.12 0.03 n/a -0.01 n/a 0.08 -0.04 0.11
DSEALGN 0.18 0.2 0.11 0.05 0.11 0.11 0.07 0.06 -0.1 0.08 0.13 0.1
DSESIM 0.12 0.15 0.05 0.1 0.08 0.07 0.04 0.08 0.05 0.08 0.04 0.08
EXPR 0.17 0.19 0.06 0.18 0.18 0.02 0.07 0 0.13 0.08 0.18 0.17
NUMEX1 0.12 0.22 -0.03 0.07 0.16 -0.05 -0.01 -0.01 -0.01 -0.03 0.08 0.1
NUMEX2 -0.25 0.19 0.01 0.06 0.14 -0.03 0.01 0.06 0.09 -0.05 0.03 0.11
SUBJCNT 0.14 0.19 0.01 0.09 0.07 0.03 0.02 0.08 0.05 0.05 0.05 0.09
SUBJDIFF -0.07 -0.07 -0.17 -0.27 -0.13 -0.22 -0.17 -0.12 -0.04 -0.12 -0.2 -0.12
SUBJSL 0.15 -0.11 0.07 0.23 0.01 0.07 0.11 -0.08 0.15 0.07 -0.03 0
Table 2: Correlation of individual features for the training and test sets with the gold standard.
Nonetheless, we notice several promising features,
such as DSEALIGN and EXPR. Lower cor-
relations seem to be associated with shorter spans
of text, since when averaging all opinion-based cor-
relations per dataset, MSRvid (x2), OnWN (x2),
and headlines display the lowest average correla-
tion, ranging from 0 to 0.03. This matches the
expectation that opinionated content can be easier
identified in longer contexts, as additional subjective
elements amount to a stronger prediction. The other
seven datasets consist of longer spans of text; they
display an average opinion-based correlation be-
tween 0.07 and 0.12, with the exception of FNWN
and SMTnews at 0.04 and 0.01, respectively.
Our systems performed well, ranking 38, 39 and
45 among the 88 competing systems in *SEM 2013
(see Table 1), with the best being comb.SVR and
comb.RandSubspace, both with a mean correlation
of 0.494. We noticed from our participation in
SEMEVAL 2012 (Banea et al, 2012), that training
and testing on the same type of data achieves the
best results; this receives further support when con-
sidering the performance of the indv.RandSubspace
variation on the OnWN data10, which exhibits a
10The SMT test data is not part of the same corpus as either
0.034 correlation increase over our next best sys-
tem (comb.RandSubspace). While we do surpass the
bag-of-words cosine baseline (baseline-tokencos)
computed by the task organizers by a 0.13 differ-
ence in correlation, we fall short by 0.124 from the
performance of the best system in the STS task.
5 Conclusions
To participate in the STS *SEM 2013 task, we con-
structed a meta-learner framework that combines
traditional knowledge and corpus-based methods,
while also introducing novel opinion analysis based
metrics. While the *SEM data is not particularly
suited for evaluating the performance of opinion fea-
tures, this is nonetheless a first step toward conduct-
ing text similarity research while also considering
the subjective dimension of text. Our system varia-
tions ranked 38, 39 and 45 among the 88 participat-
ing systems.
Acknowledgments
This material is based in part upon work sup-
ported by the National Science Foundation CA-
REER award #0747340 and IIS awards #1018613,
SMTep or SMTnews.
226
#0208798 and #0916046. This work was sup-
ported in part by DARPA-BAA-12-47 DEFT grant
#12475008. Any opinions, findings, and conclu-
sions or recommendations expressed in this material
are those of the authors and do not necessarily reflect
the views of the National Science Foundation or the
Defense Advanced Research Projects Agency.
References
E. Agirre, D. Cer, M. Diab, and A. Gonzalez. 2012.
Semeval-2012 task 6: A pilot on semantic textual sim-
ilarity. In Proceedings of the 6th International Work-
shop on Semantic Evaluation (SemEval 2012), in con-
junction with the First Joint Conference on Lexical and
Computational Semantics (*SEM 2012).
E. Agirre, D. Cer, M. Diab, A. Gonzalez-Agirre, and W.
Guo. 2013. *SEM 2013 Shared Task: Semantic Tex-
tual Similarity, including a Pilot on Typed-Similarity.
In Proceedings of the Second Joint Conference on Lex-
ical and Computational Semantics (*SEM 2013), At-
lanta, GA, USA.
C. Banea, S. Hassan, M. Mohler, and R. Mihalcea. 2012.
UNT: A supervised synergistic approach to seman-
tic text similarity. In Proceedings of the First Joint
Conference on Lexical and Computational Semantics
(*SEM 2012), pages 635?642, Montreal, Canada.
E. Bingham and H. Mannila. 2001. Random projection
in dimensionality reduction: applications to image and
text data. In Proceedings of the seventh ACM SIGKDD
international conference on Knowledge discovery and
data mining (KDD 2001), pages 245?250, San Fran-
cisco, CA, USA.
C. Burgess, K. Livesay, and K. Lund. 1998. Explorations
in context space: words, sentences, discourse. Dis-
course Processes, 25(2):211?257.
K. Church and P. Hanks. 1990. Word association norms,
mutual information, and lexicography. Computational
Linguistics, 16(1):22?29.
S. Dasgupta. 1999. Learning mixtures of Gaussians. In
40th Annual Symposium on Foundations of Computer
Science (FOCS 1999), pages 634?644, New York, NY,
USA.
H. Drucker, C. J. Burges, L. Kaufman, A. Smola, and
Vladimir Vapnik. 1997. Support vector regression
machines. Advances in Neural Information Process-
ing Systems, 9:155?161.
R. Fan, K. Chang, C. Hsieh, X. Wang, and C. Lin. 2008.
Liblinear: A library for large linear classification. The
Journal of Machine Learning Research, 9:1871?1874.
E. Gabrilovich and S. Markovitch. 2007. Comput-
ing semantic relatedness using Wikipedia-based ex-
plicit semantic analysis. In Proceedings of the 20th
AAAI International Conference on Artificial Intelli-
gence (AAAI?07), pages 1606?1611, Hyderabad, In-
dia.
M. Hall, E. Frank, G. Holmes, B. Pfahringer, P. Reute-
mann, and Ian H. Witten. 2009. The WEKA data
mining software: An update. SIGKDD Explorations,
11(1).
S. Hassan and R. Mihalcea. 2011. Measuring semantic
relatedness using salient encyclopedic concepts. Arti-
ficial Intelligence, Special Issue.
T. K. Ho. 1998. The Random Subspace Method for
Constructing Decision Forests. IEEE Transactions on
Pattern Analysis and Machine Intelligence, 20(8):832?
844.
A. Islam and D. Inkpen. 2006. Second order co-
occurrence PMI for determining the semantic similar-
ity of words. In Proceedings of the 5th Conference on
Language Resources and Evaluation (LREC 06), vol-
ume 2, pages 1033?1038, Genoa, Italy, July.
A. Islam and D. Inkpen. 2009. Semantic Similarity of
Short Texts. In Nicolas Nicolov, Galia Angelova, and
Ruslan Mitkov, editors, Recent Advances in Natural
Language Processing V, volume 309 of Current Issues
in Linguistic Theory, pages 227?236. John Benjamins,
Amsterdam & Philadelphia.
J. J. Jiang and D. W. Conrath. 1997. Semantic similarity
based on corpus statistics and lexical taxonomy. In
International Conference Research on Computational
Linguistics (ROCLING X), pages 9008+, September.
T. K. Landauer, T. K. L, D. Laham, B. Rehder, and M.
E. Schreiner. 1997. How well can passage meaning
be derived without using word order? a comparison of
latent semantic analysis and humans.
M. Lapata and R. Barzilay. 2005. Automatic evaluation
of text coherence: Models and representations. In Pro-
ceedings of the 19th International Joint Conference on
Artificial Intelligence, Edinburgh.
C. Leacock and M. Chodorow. 1998. Combining local
context and WordNet similarity for word sense identi-
fication. In WordNet: An Electronic Lexical Database,
pages 305?332.
M. Lesk. 1986. Automatic sense disambiguation us-
ing machine readable dictionaries: how to tell a pine
cone from an ice cream cone. In SIGDOC ?86: Pro-
ceedings of the 5th annual international conference on
Systems documentation, pages 24?26, New York, NY,
USA. ACM.
C. Lin and E. Hovy. 2003. Automatic evaluation of sum-
maries using n-gram co-occurrence statistics. In Pro-
ceedings of Human Language Technology Conference
(HLT-NAACL 2003), Edmonton, Canada, May.
D. Lin. 1998. An information-theoretic definition of
similarity. In Proceedings of the Fifteenth Interna-
227
tional Conference on Machine Learning, pages 296?
304, Madison, Wisconsin.
R. Mihalcea, C. Corley, and C. Strapparava. 2006.
Corpus-based and knowledge-based measures of text
semantic similarity. In Proceedings of the American
Association for Artificial Intelligence (AAAI 2006),
pages 775?780, Boston, MA, US.
G. A. Miller. 1995. WordNet: a Lexical database for
English. Communications of the Association for Com-
puting Machinery, 38(11):39?41.
M. Mohler and R. Mihalcea. 2009. Text-to-text seman-
tic similarity for automatic short answer grading. In
Proceedings of the European Association for Compu-
tational Linguistics (EACL 2009), Athens, Greece.
M. Mohler, R. Bunescu, and R. Mihalcea. 2011. Learn-
ing to grade short answer questions using semantic
similarity measures and dependency graph alignments.
In Proceedings of the Association for Computational
Linguistics ? Human Language Technologies (ACL-
HLT 2011), Portland, Oregon, USA.
R. M. A. Nawab, M. Stevenson, and P. Clough. 2011.
External plagiarism detection using information re-
trieval and sequence alignment: Notebook for PAN at
CLEF 2011. In Proceedings of the 5th International
Workshop on Uncovering Plagiarism, Authorship, and
Social Software Misuse (PAN 2011).
K. Papineni, S. Roukos, T. Ward, and W. Zhu. 2002.
Bleu: a method for automatic evaluation of machine
translation. In Proceedings of the 40th Annual Meet-
ing of the Association for Computational Linguistics,
pages 311?318, Philadelphia, PA.
T. Pedersen, S. Patwardhan, and J. Michelizzi. 2004.
WordNet:: Similarity-Measuring the Relatedness of
Concepts. Proceedings of the National Conference on
Artificial Intelligence, pages 1024?1025.
P. Resnik. 1995. Using information content to evaluate
semantic similarity in a taxonomy. In In Proceedings
of the 14th International Joint Conference on Artificial
Intelligence, pages 448?453.
J. Rocchio, 1971. Relevance feedback in information re-
trieval. Prentice Hall, Ing. Englewood Cliffs, New Jer-
sey.
G. Salton and C. Buckley. 1997. Term weighting ap-
proaches in automatic text retrieval. In Readings in
Information Retrieval. Morgan Kaufmann Publishers,
San Francisco, CA.
G. Salton and M. Lesk, 1971. The SMART Retrieval Sys-
tem: Experiments in Automatic Document Processing,
chapter Computer evaluation of indexing and text pro-
cessing. Prentice Hall, Ing. Englewood Cliffs, New
Jersey.
G. Salton, A. Singhal, M. Mitra, and C. Buckley. 1997.
Automatic text structuring and summarization. Infor-
mation Processing and Management, 2(32).
H. Schutze. 1998. Automatic word sense discrimination.
Computational Linguistics, 24(1):97?124.
A. Smola and B. Schoelkopf. 1998. A tutorial on sup-
port vector regression. NeuroCOLT2 Technical Re-
port NC2-TR-1998-030.
P. D. Turney. 2001. Mining the Web for Synonyms:
PMI-IR versus LSA on TOEFL. In Proceedings of
the 12th European Conference on Machine Learning
(ECML?01), pages 491?502, Freiburg, Germany.
J. Wiebe and E. Riloff. 2005. Creating subjective and
objective sentence classifiers from unannotated texts.
In Proceedings of the 6th international conference on
Computational Linguistics and Intelligent Text Pro-
cessing (CICLing 2005), pages 486?497, Mexico City,
Mexico.
J. Wiebe, T. Wilson, and C. Cardie. 2005. Annotating ex-
pressions of opinions and emotions in language. Lan-
guage Resources and Evaluation, 39(2-3):165?210.
T. Wilson, P. Hoffmann, S. Somasundaran, J. Kessler,
Janyce Wiebe, Yejin Choi, Claire Cardie, Ellen Riloff,
and Siddharth Patwardhan. 2005. OpinionFinder:
A system for subjectivity analysis. In Proceedings
of HLT/EMNLP on Interactive Demonstrations, pages
34?35, Vancouver, BC, Canada.
Z. Wu and M. Palmer. 1994. Verbs semantics and lexical
selection. In Proceedings of the 32nd annual meeting
on Association for Computational Linguistics, pages
133?-138, Las Cruces, New Mexico.
B. Yang and C. Cardie. 2012. Extracting opinion expres-
sions with semi-markov conditional random fields. In
Proceedings of the conference on Empirical Meth-
ods in Natural Language Processing. Association for
Computational Linguistics.
228
Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 81?91,
Dublin, Ireland, August 23-24, 2014.
SemEval-2014 Task 10: Multilingual Semantic Textual Similarity
Eneko Agirre
a
, Carmen Banea
b?
, Claire Cardie
c
, Daniel Cer
d
, Mona Diab
e?
Aitor Gonzalez-Agirre
a
, Weiwei Guo
f
, Rada Mihalcea
b
, German Rigau
a
, Janyce Wiebe
g
a
University of the Basque Country
Basque Country, Spain
b
University of Michigan
Ann Arbor, MI
c
Cornell University
Ithaca, NY
d
Google Inc.
Mountain View, CA
e
George Washington University
Washington, DC
f
Columbia University
New York, NY
g
University of Pittsburgh
Pittsburgh, PA
Abstract
In Semantic Textual Similarity, systems
rate the degree of semantic equivalence
between two text snippets. This year,
the participants were challenged with new
data sets for English, as well as the in-
troduction of Spanish, as a new language
in which to assess semantic similarity.
For the English subtask, we exposed the
systems to a diversity of testing scenar-
ios, by preparing additional OntoNotes-
WordNet sense mappings and news head-
lines, as well as introducing new gen-
res, including image descriptions, DEFT
discussion forums, DEFT newswire, and
tweet-newswire headline mappings. For
Spanish, since, to our knowledge, this is
the first time that official evaluations are
conducted, we used well-formed text, by
featuring sentences extracted from ency-
clopedic content and newswire. The an-
notations for both tasks leveraged crowd-
sourcing. The Spanish subtask engaged 9
teams participating with 22 system runs,
and the English subtask attracted 15 teams
with 38 system runs.
1 Introduction and motivation
Given two snippets of text, Semantic Textual Sim-
ilarity (STS) captures the notion that some texts
are more similar than others, measuring their de-
gree of semantic equivalence. Textual similar-
ity can range from complete unrelatedness to ex-
act semantic equivalence, and a graded similar-
ity intuitively captures the notion of intermediate
?
carmennb@umich.edu, mtdiab@gwu.edu
This work is licensed under a Creative Commons At-
tribution 4.0 International Licence. Page numbers and pro-
ceedings footer are added by the organisers. Licence details:
http://creativecommons.org/licenses/by/4.0/
shades of similarity, as pairs of text may differ
from some minor nuanced aspects of meaning, to
relatively important semantic differences, to shar-
ing only some details, or to simply being related
to the same topic (cf. Section 2).
One of the goals of the STS task is to create a
unified framework for combining several seman-
tic components that otherwise have historically
tended to be evaluated independently and with-
out characterization of impact on NLP applica-
tions. By providing such a framework, STS al-
lows for an extrinsic evaluation of these modules.
Moreover, such an STS framework itself could in
turn be evaluated intrinsically and extrinsically as
a grey/black box within various NLP applications
such as Machine Translation (MT), Summariza-
tion, Generation, Question Answering (QA), etc.
STS is related to both Textual Entailment (TE)
and Paraphrasing, but differs in a number of ways
and it is more directly applicable to a number of
NLP tasks. STS is different from TE inasmuch
as it assumes bidirectional graded equivalence be-
tween the pair of textual snippets. In the case of
TE the equivalence is directional, e.g. a car is a
vehicle, but a vehicle is not necessarily a car. STS
also differs from both TE and Paraphrasing (in as
far as both tasks have been defined to date in the
literature) in that, rather than being a binary yes/no
decision (e.g. a vehicle is not a car), we define
STS to be a graded similarity notion (e.g. a ve-
hicle and a car are more similar than a wave and
a car). A quantifiable graded bidirectional notion
of textual similarity is useful for a myriad of NLP
tasks such as MT evaluation, information extrac-
tion, question answering, summarization, etc.
In 2012 we held the first pilot task at SemEval
2012, as part of the *SEM 2012 conference, with
great success: 35 teams participated with 88 sys-
tem runs (Agirre et al., 2012). In addition, we held
81
year dataset pairs source
2012 MSRpar 1500 newswire
2012 MSRvid 1500 videos
2012 OnWN 750 glosses
2012 SMTnews 750 MT eval.
2012 SMTeuroparl 750 MT eval.
2013 HDL 750 newswire
2013 FNWN 189 glosses
2013 OnWN 561 glosses
2013 SMT 750 MT eval.
2014 HDL 750 newswire headlines
2014 OnWN 750 glosses
2014 Deft-forum 450 forum posts
2014 Deft-news 300 news summary
2014 Images 750 image descriptions
2014 Tweet-news 750 tweet-news pairs
Table 2: English subtask: Summary of train (2012
and 2013) and test (2014) datasets.
a DARPA sponsored workshop at Columbia Uni-
versity.
1
In 2013, STS was selected as the offi-
cial Shared Task of the *SEM 2013 conference,
with two subtasks: The Core task, which is sim-
ilar to the 2012 task; and a Pilot task on Typed-
similarity between semi-structured records. The
Core task attracted 34 participants with 89 runs,
and the Typed-similarity task attracted 6 teams
with 14 runs.
For STS 2014 we defined two subtasks: En-
glish and Spanish. For the English subtask we pro-
vided five test datasets: two datasets that extend
already released genres (the OntoNotes-WordNet
sense mappings and news headlines) and three
new genres: image descriptions, DEFT discus-
sion forum data and newswire, as well as tweet-
newswire headline mappings. Participants could
use all datasets released in 2012 and 2013 as train-
ing data. The Spanish subtask introduced two di-
verse datasets on different genres, namely ency-
clopedic descriptions extracted from the Spanish
Wikipedia and contemporary Spanish newswire.
For the Spanish subtask, the participants had ac-
cess to a limited amount of labeled data, consist-
ing of 65 sentence pairs, which they could use for
training.
2 Task Description
2.1 English Subtask
The English dataset comprises pairs of news head-
lines (HDL), pairs of glosses (OnWN), image de-
scriptions (Images), DEFT-related discussion fo-
rums (Deft-forum) and news (Deft-news), and
1
http://www.cs.columbia.edu/
?
weiwei/
workshop/
tweet comments and newswire headline mappings
(Tweets).
For HDL, we used naturally occurring news
headlines gathered by the Europe Media Moni-
tor (EMM) engine (Best et al., 2005) from sev-
eral different news sources. EMM clusters to-
gether related news. Our goal was to generate
a balanced data set across the different similar-
ity ranges, hence we built two sets of headline
pairs: (i) a set where the pairs come from the same
EMM cluster, (ii) and another set where the head-
lines come from a different EMM cluster, then
we computed the string similarity between those
pairs. Accordingly, we sampled 375 headline pairs
of headlines that occur in the same EMM cluster,
aiming for pairs equally distributed between min-
imal and maximal similarity using simple string
similarity. We sampled other 375 pairs from the
different EMM cluster in the same manner.
For OnWN, we used the sense definition pairs
of OntoNotes (Hovy et al., 2006) and WordNet
(Fellbaum, 1998). Different from previous tasks,
the two definition sentences in a pair belong to dif-
ferent senses. We sampled 750 pairs based on a
string similarity ranging from 0.5 to 1.
The Images data set is a subset of the PAS-
CAL VOC-2008 data set (Rashtchian et al., 2010),
which consists of 1,000 images and has been used
by a number of image description systems. It was
also sampled from string similarity values between
0.6 and 1.
Deft-forum and Deft-news are from DEFT
data.
2
Deft-forum contains the forum post sen-
tences, and Deft-news are news summaries. We
selected 450 pairs for Deft-forum and 300 pairs for
Deft-news. They are sampled evenly from string
similarities falling in the interval 0.6 to 1.
The Tweets data set contains tweet-news pairs
selected from the corpus released in (Guo et al.,
2013), where each pair contains a sentence that
pertains to the news title, while the other one rep-
resents a Twitter comment on that particular news.
They are evenly sampled from string similarity
values between 0.5 and 1.
Table 1 shows the explanations and values as-
sociated with each score between 5 and 0. As
in prior years, we used Amazon Mechanical Turk
(AMT)
3
to crowdsource the annotation of the En-
glish pairs.
4
Annotators are presented with the
2
LDC2013E19, LDC2012E54
3
www.mturk.com
4
For STS 2013, we used CrowdFlower as a front-end to
82
Score English Spanish
5/4 The two sentences are completely equivalent, as they mean the same thing.
The bird is bathing in the sink.
Birdie is washing itself in the water basin.
El p?ajaro se esta ba?nando en el lavabo.
El p?ajaro se est?a lavando en el aguamanil.
4 The two sentences are mostly equivalent, but some unimportant details differ.
In May 2010, the troops attempted to invade
Kabul.
The US army invaded Kabul on May 7th last
year, 2010.
3 The two sentences are roughly equivalent, but some important information differs/missing.
John said he is considered a witness but not a
suspect.
?He is not a suspect anymore.? John said.
John dijo que ?el es considerado como testigo, y
no como sospechoso.
?
?
El ya no es un sospechoso,? John dijo.
2 The two sentences are not equivalent, but share some details.
They flew out of the nest in groups.
They flew into the nest together.
Ellos volaron del nido en grupos.
Volaron hacia el nido juntos.
1 The two sentences are not equivalent, but are on the same topic.
The woman is playing the violin.
The young lady enjoys listening to the guitar.
La mujer est?a tocando el viol??n.
La joven disfruta escuchar la guitarra.
0 The two sentences are completely dissimilar.
John went horse back riding at dawn with a
whole group of friends.
Sunrise at dawn is a magnificent view to take
in if you wake up early enough for it.
Al amanecer, Juan se fue a montar a caballo
con un grupo de amigos.
La salida del sol al amanecer es una magn??fica
vista que puede presenciar si usted se despierta
lo suficientemente temprano para verla.
Table 1: Similarity scores with explanations and examples for the English and Spanish subtasks, where
the sentences in Spanish are translations of the English ones.
A similarity score of 5 in English is mirrored by a maximum score of 4 in Spanish; the definitions pertaining to scores 3 and 4
in English were collapsed under a score of 3 in Spanish, with the definition ?The two sentences are mostly equivalent, but some
details differ.?
detailed instructions provided in Figure 1, and
are asked to label each STS sentence pair on our
six point scale, selecting from a dropdown box.
Five sentence pairs are presented to each annota-
tor at once, per human intelligence task (HIT), at
a payrate of $0.20; we collect five separate anno-
tations per sentence pair. Annotators were only el-
igible to work on the task if they had the Mechan-
ical Turk Master Qualification. This is a special
Amazon Mechanical Turk, since it provides numerous useful
tools to assist in running a successful annotation project using
crowdsourcing, such as support for hidden ?golden? questions
that can be used both to train annotators and to automatically
stop people who repeatedly make mistakes from contribut-
ing to the task. However, in 2013, CrowdFlower dropped
Amazon Mechanical Turk as an annotation source. When we
tried running pairs for STS 2014 on CrowdFlower using the
same templates that were successfully used for the 2013 task,
we found that we obtained significantly degraded annotation
quality, with an average Pearson (AMT provider vs. rest of
AMT providers) of only 22.8%. In contrast, when we ran the
task for 2014 on AMT, we obtained a one-vs-rest annotation
of 73.6%.
qualification conferred by AMT (using a priority
statistical model) to annotators who consistently
maintain a very high level of quality across a vari-
ety of tasks from numerous requesters). Access to
these skilled workers entails a 20% surcharge.
To monitor the quality of the annotations, we
use the gold dataset of 105 pairs that were manu-
ally annotated by the task organizers during STS
2013. We include one of these gold pairs in each
set of five sentence pairs, where the gold pairs are
indistinguishable from the rest. Unlike when we
ran on CrowdFlower for STS 2013, the gold pairs
are not used for training purposes, nor are workers
automatically banned from the task if they make
too many mistakes on annotating them. Rather, the
gold pairs are only used to help in identifying and
removing the data associated with poorly perform-
ing annotators. With few exceptions, 90% of the
answers from each individual annotator fall within
+/-1 of the answers selected by the organizers for
83
Figure 1: Annotation instructions for English subtask.
the gold dataset.
The distribution of scores obtained from the
AMT providers in the Deft-forum, Deft-news,
OnWN and tweet-news datasets is roughly uni-
form across the different grades of similarity, al-
though the scores are slightly higher for tweet-
news. Compared to the other data sets, the scores
for OnWN, were more bimodal, ranging between
4.6 to 5 and 0 to 0.4, when compared to middle
values (2.6-3.4).
In order to assess the annotation quality, we
measure the correlation of each annotator with the
average of the rest of the annotators, and then aver-
age the results. This approach to estimate the qual-
ity is identical to the method used for evaluations
(see Section 3), and it can thus be considered as
the upper bound of the systems. The inter-tagger
correlation for each English dataset is as follows:
? HDL: 79.4%
? OnWN: 67.2%
? Deft-forum: 58.6%
? Deft-news: 70.7%
? Images: 83.6%
? Tweets-news: 74.4%
The correlation figures are generally high (over
70%), with the exception of the OnWN and Deft
datasets, which score 67.2% and 58.6%, respec-
tively. The reason for the low inter-tagger correla-
tion on OnWN compared to the higher correlations
in previous years is that we only used unmapped
sense definitions, i.e., the two sentences in a pair
belong to two different senses. For the Deft-forum
dataset, we found that similarity values tend to be
lower than in the other datasets, and more annota-
tion disagreements happen in these low similarity
values.
2.2 Spanish Subtask
The Spanish subtask follows a setup similar to the
English subtask, except that the similarity scores
were adapted to fit a range from 0 to 4 (see Table
1). We thought that the distinction between a score
of 3 and 4 for the English task will pose more dif-
ficulty for us in conveying into Spanish, as the sole
difference between the two lies in how the annota-
tors perceive the importance of additional details
or missing information with respect to the core se-
mantic interpretation of the pair. As this aspect en-
tails a subjective judgement, and since it is the first
time that a Spanish STS evaluation is organized,
we casted the annotation guidelines into straight-
forward and unambiguous instructions, and thus
opted to use a similarity range from 0 to 4.
Prior to the evaluation window, we released 65
Spanish sentence pairs for trial / training. In or-
der to evaluate system performance under differ-
84
ent scenarios, we developed two test datasets, one
extracted from the Spanish Wikipedia
5
(December
2013 dump) and one from contemporary news ar-
ticles collected from media in Spanish (February
2014).
2.2.1 Spanish Wikipedia
The Wikipedia dump was processed using the
Parse::MediaWikiDump Perl library. We removed
all titles, html tags, wiki tags and hyperlinks
(keeping only the surface forms). Each article was
split into paragraphs, where the first paragraph
was considered to be the article?s abstract, while
the remaining ones were deemed to be its content.
Each of these were split into sentences using the
Perl library Uplug::PreProcess::SentDetect, and
only the sentences longer than eight words were
used. We iteratively computed the lexical simi-
larity
6
between every sentence in the abstract and
every sentence in the content, and retained those
pairs whose sentence length ratio was higher than
0.5, and their similarity scored over 0.35.
The final set of sentence pairs was split into five
bins, and their scores normalized to range from
0 to 1. The more interesting and difficult pairs
were found, perhaps not surprisingly, in bins 0 and
1, where synonyms/short paraphrases where more
frequent. An example extracted from those bins,
where the text in italics highlights the differences
between the two sentences:
? ?America? es el segundo continente m?as
grande del planeta, despu?es de Asia.
?America? is the second largest continent in the world,
following Asia.
? America corresponde a la segunda masa de
tierra m?as grande del planeta, luego de Asia.
America is the second largest land mass on the planet,
after Asia.
The Spanish verb ?Es? maps to (En:
7
is), ?cor-
responde a? (En: corresponds to), the phrase ?el
segundo continente? (En: the second continent) is
equivalent to ?la segunda masa de tierra? (the sec-
ond land mass), and ?despues? (En: following) to
?luego? (En: after). Despite the difference in vo-
cabulary choice, the two sentences are paraphrases
of each other.
From the candidate pairs, we manually selected
324 sentence pairs, in order to ensure a diverse
5
es.wikipedia.org
6
Algorithm based on the Linux diff command (Algo-
rithm::Diff Perl module).
7
?En? stands for English.
and challenging set. This set was annotated in two
ways, first by two graduate students in Computer
Science who are native speakers of Spanish, and
second by using AMT.
The AMT framework was set up to contain
seven sentence pairs per HIT, where six of them
were part of the test dataset, while one was used
for control. AMT providers were eligible to com-
plete a task if they had more than 500 accepted
HITs, with 90%+ acceptance rate.
8
We paid $0.30
per HIT, and each HIT was annotated by five AMT
providers. We sought to ensure that only Spanish
speaking annotators would complete the HITs by
providing all the information related to the task (its
title, abstract, description, guidelines and exam-
ples), as well as the control pair in Spanish only.
The participants were instructed to label the pairs
on a scale from 0 to 4 (see Table 1). Each sentence
pair was followed by a comment text box, which
the AMT providers used to provide the topic of the
sentences, corrections, etc.
The two students achieved a Pearson correla-
tion of 0.6974 on the Wikipedia dataset. To see
how their judgement compares to the crowd wis-
dom, we averaged the AMT scores for each pair,
and computed their correlation with our annota-
tors, obtaining 0.824 and 0.742, respectively. Sur-
prisingly enough, both these correlation values are
higher than the correlation among the annotators
themselves. When averaging the annotator scores
and comparing them with the AMT providers?
average score per pair, the correlation becomes
0.8546, indicating that the task is well defined,
and that the annotations contributed by the AMT
providers are of satisfactory quality. Given these
scores, the gold standard was annotated using the
average AMT provider judgement per pair.
2.2.2 Spanish News
The second Spanish dataset was extracted from
news articles published in Spanish language me-
dia from around the world in February 2014. The
hyperlinks to the articles were obtained by pars-
ing the ?International? page of Spanish Google
News,
9
which aggregates or clusters in real time
articles describing a particular event from a di-
verse pool of news sites, where each grouping
8
Initially, Amazon had automatically upgraded our anno-
tation task to require Master level providers (as those partici-
pating in the English annotations), yet after approximately 4
days, no HIT had been completed.
9
news.google.es
85
is labeled with the title of one of the predomi-
nant articles. By leveraging these clusters of links
pointing to the sites where the articles were orig-
inally published, we are able to gather raw text
that has a high probability of containing seman-
tically similar sentences. We encountered several
difficulties while mining the articles, ranging from
each article having its own formatting depend-
ing on the source site, to advertisements, cookie
requirements, to encoding for Spanish diacritics.
We used the lynx text-based browser,
10
which was
able to standardize the raw articles to a degree.
The output of the browser was processed using a
rule based approach taking into account continu-
ous text span length, ratio of symbols and num-
bers to the text, etc., in order to determine when
a paragraph is part of the article content. After
that, a second pass over the predictions corrected
mislabeled paragraphs if they were preceded and
followed by paragraphs identified as content. All
the content pertaining to articles on the same event
was joined, sentence split, and diff pairwise simi-
larities were computed. The set of candidate sen-
tences followed the same requirements as for the
Wikipedia dataset, namely length ratio higher than
0.5 and similarity score over 0.35. From these, we
manually extracted 480 sentence pairs which were
deemed to pose a challenge to an automated sys-
tem.
Due to the high correlations obtained between
the AMT providers? scores and the annotators?
scores on Wikipedia, the news dataset was only
annotated using AMT, following exactly the same
task setup as for Wikipedia.
3 Evaluation
Evaluation of STS is still an open issue.
STS experiments have traditionally used Pearson
product-moment correlation between the system
scores and the GS scores, or, alternatively, Spear-
man rank order correlation. In addition, we also
need a method to aggregate the results from each
dataset into an overall score. The analysis per-
formed in (Agirre and Amig?o, In prep) shows that
Pearson and averaging across datasets are the best
suited combination in general. In particular, Pear-
son is more informative than Spearman, in that
Spearman only takes the rank differences into ac-
count, while Pearson does account for value dif-
ferences as well. The study also showed that other
10
lynx.browser.org
alternatives need to be considered, depending on
the requirements of the target application.
We leave application-dependent evaluations for
future work, and focus on average Pearson correla-
tion. When averaging, we weight each individual
correlation by the size of the dataset. In order to
compute statistical significance among system re-
sults, we use a one-tailed parametric test based on
Fisher?s z-transformation (Press et al., 2002, equa-
tion 14.5.10). In addition, English subtask partic-
ipants could provide an optional confidence mea-
sure between 0 and 100 for each of their predic-
tions. Team RTM-DCU is the only one who has
provided these, and the evaluation of their runs us-
ing weighted Pearson (Pozzi et al., 2012) is listed
at the end of Table 3.
Participants
11
could take part in the shared task
with a maximum of 3 system runs per subtask.
3.1 English Subtask
In order to provide a simple word overlap baseline
(Baseline-tokencos), we tokenize the input sen-
tences splitting on white spaces, and then repre-
sent each sentence as a vector in the multidimen-
sional token space. Each dimension has 1 if the to-
ken is present in the sentence, 0 otherwise. Vector
similarity is computed using the cosine similarity
metric.
We also run the freely available system, Take-
Lab (
?
Sari?c et al., 2012), which yielded state of the
art performance in STS 2012 and strong results
out-of-the-box in 2013.
12
15 teams participated in the English subtask,
submitting 38 system runs. One team submitted
the results past the deadline, as explicitly marked
in Table 3. After the submission deadline expired,
the organizers published the gold standard and par-
ticipant submissions on the task website, in order
to ensure a transparent evaluation process.
Table 3 shows the results of the English sub-
task, with runs listed in alphabetical order. The
correlation in each dataset is given, followed
11
Participating teams: Bielefeld SC (McCrae et al.,
2013), BUAP (Vilari?no et al., 2014), DLS@CU (Sultan et
al., 2014b), FBK-TR (Vo et al., 2014), IBM EG (no in-
formation), LIPN (Buscaldi et al., 2014), Meerkat Mafia
(Kashyap et al., 2014), NTNU (Lynum et al., 2014), RTM-
DCU (Bic?ici and Way, 2014), SemantiKLUE (Proisi et al.,
2014), StanfordNLP (Socher et al., 2014), TeamZ (Gupta,
2014), UMCC DLSI SemSim (Chavez et al., 2014), UNAL-
NLP (Jimenez et al., 2014), UNED (Martinez-Romo et al.,
2011), UoW (Rios, 2014).
12
Code is available at http://ixa2.si.ehu.es/
stswiki
86
Run Name deft deft Headl images OnWN tweet Weighted mean Rank
forum news news
Baseline-tokencos 0.353 0.596 0.510 0.513 0.406 0.654 0.507 -
TakeLab 0.333 0.716 0.720 0.742 0.793 0.650 0.678 -
Bielefeld SC-run1 0.211 0.432 0.321 0.368 0.367 0.415 0.354 32
Bielefeld SC-run2 0.211 0.431 0.311 0.356 0.361 0.409 0.347 33
BUAP-EN-run1 0.456 0.686 0.689 0.697 0.654 0.771 0.671 19
DLS@CU-run1 0.483 0.766 0.765 0.821 0.723 0.764 0.734 7
DLS@CU-run2 0.483 0.766 0.765 0.821 0.859 0.764 0.761 1
FBK-TR-run1 0.322 0.523 0.547 0.601 0.661 0.462 0.535 25
FBK-TR-run2 0.167 0.421 0.485 0.521 0.572 0.359 0.441 28
FBK-TR-run3 0.305 0.405 0.471 0.489 0.551 0.438 0.459 27
IBM EG-run1 0.474 0.743 0.737 0.801 0.760 0.730 0.722 8
IBM EG-run2 0.464 0.641 0.710 0.747 0.732 0.696 0.684 15
LIPN-run1 0.454 0.640 0.653 0.809 - 0.551 0.508 26
LIPN-run2 0.084 - - - - - 0.010 35
Meerkat Mafia-Hulk 0.449 0.785 0.757 0.790 0.787 0.757 0.735 6
Meerkat Mafia-pairingWords 0.471 0.763 0.760 0.801 0.875 0.779 0.761 2
Meerkat Mafia-SuperSaiyan 0.492 0.771 0.767 0.768 0.802 0.765 0.741 5
NTNU-run1 0.437 0.714 0.722 0.800 0.835 0.411 0.663 20
NTNU-run2 0.508 0.766 0.753 0.813 0.777 0.792 0.749 4
NTNU-run3 0.531 0.781 0.784 0.834 0.850 0.675 0.755 3
SemantiKLUE-run1 0.337 0.608 0.728 0.783 0.848 0.632 0.687 14
SemantiKLUE-run2 0.349 0.643 0.733 0.773 0.855 0.640 0.694 13
StanfordNLP-run1 0.319 0.635 0.636 0.758 0.627 0.669 0.627 22
StanfordNLP-run2 0.304 0.679 0.621 0.715 0.625 0.636 0.610 24
StanfordNLP-run3 0.342 0.650 0.602 0.754 0.609 0.638 0.614 23
UMCC DLSI SemSim-run1 0.475 0.662 0.632 0.742 0.813 0.675 0.682 16
UMCC DLSI SemSim-run2 0.469 0.662 0.625 0.739 0.814 0.654 0.676 18
UMCC DLSI SemSim-run3 0.283 0.385 0.267 0.436 0.603 0.278 0.381 30
UNAL-NLP-run1 0.504 0.721 0.762 0.807 0.782 0.614 0.711 12
UNAL-NLP-run2 0.383 0.730 0.765 0.771 0.827 0.403 0.657 21
UNAL-NLP-run3 0.461 0.722 0.761 0.778 0.843 0.658 0.721 9
UNED-run22 p np 0.104 0.315 0.037 0.324 0.509 0.490 0.310 34
UNED-runS5K 10 np 0.118 0.506 0.057 0.498 0.488 0.579 0.379 31
UNED-runS5K 3 np 0.094 0.564 0.018 0.607 0.577 0.670 0.431 29
UoW-run1 0.342 0.751 0.754 0.776 0.799 0.737 0.714 11
UoW-run2 0.342 0.587 0.754 0.788 0.799 0.628 0.682 17
UoW-run3 0.342 0.763 0.754 0.788 0.799 0.753 0.721 10
?RTM-DCU-run1 0.434 0.697 0.620 0.699 0.806 0.688 0.671
?RTM-DCU-run2 0.397 0.681 0.613 0.666 0.799 0.669 0.651
?RTM-DCU-run3 0.308 0.556 0.630 0.647 0.800 0.553 0.608
?RTM-DCU-run1 0.418 0.685 0.622 0.698 0.833 0.687 0.673
?RTM-DCU-run2 0.383 0.674 0.609 0.663 0.826 0.669 0.653
?RTM-DCU-run3 0.273 0.553 0.633 0.644 0.825 0.568 0.611
Table 3: English evaluation results. Results at the top correspond to out-of-the-box systems. Results at
the bottom correspond to results using the confidence score.
Notes: ?-? for not submitted, ??? for post-deadline submission.
87
by the mean correlation (the official measure),
and the rank of the run. The highest correla-
tions are for OnWN (87.5%, by Meerkat Mafia)
and images (83.4%, by NTNU), followed by
Tweets (79.2%, by NTNU), HEADL (78.4%, by
NTNU) and deft news and forums (78.1% and
53.1%, respectively, by NTNU). Compared to the
inter-annotator agreement correlation, the ranking
among datasets is very similar, with the exception
of OnWN, as it gets the best score but has very low
agreement. One possible reason is that the partic-
ipants used previously available data. The results
of the best 4 top system runs are significantly dif-
ferent (p-value < 0.05) from the 5th top scoring
system run and below. The top 4 systems did not
show statistical significant variation among them.
Only three runs (cf. lower rows in Table 3) in-
cluded non-uniform confidence scores, barely af-
fecting their ranking.
Interestingly, the two top performing systems
on the English STS sub-task are both unsuper-
vised. DLS@CU (Sultan et al., 2014b) presents
an unsupervised algorithm which predicts the STS
score based on the proportion of word alignments
in the two sentences. Two related words are
aligned depending on how similar the two words
are, and also on how similar the contexts of the
words are in the respective sentences (Sultan et al.,
2014a). Meerkat Mafia pairingWords (Kashyap
et al., 2014) also follows a fully unsupervised ap-
proach. The authors train LSA on an English cor-
pus of three billion words using a sliding window
approach, resulting in a vocabulary size of 29,000
words associated with 300 dimensions. They ac-
count for named entities and out-of-vocabulary
words by leveraging external resources such as
DBpedia
13
and Wordnik.
14
In Spanish, the sys-
tem equivalent to this run ranked second following
a cross-lingual approach, by applying the English
system to the translated version of the dataset (see
3.2).
The Table also shows the results of TakeLab,
which was trained with all datasets from previ-
ous years. TakeLab would rank 18th, ten absolute
points below the best system, a smaller difference
than in 2013.
13
dbpedia.org
14
wordnick.com
3.2 Spanish Subtask
The Spanish subtask attracted 9 teams with 22
participating systems, out of which 16 were su-
pervised and 6 unsupervised. The participants
were from both Spanish (Colombia, Cuba, Mex-
ico, Spain), and non-Spanish speaking countries
(two teams from France, Germany, Ireland, UK,
US). The evaluation results appear in Table 4.
The top ranking system is the 2nd run of
UMCC DLSI SemSim (Chavez et al., 2014),
which achieves a weighted correlation of 0.807. It
entails a cross-lingual approach, as it leverages a
SVM-based English framework, by mapping the
Spanish words to their English equivalent using
the most common sense in WordNet 3.0. The clas-
sifier uses a combination of features, such as those
derived from traditional knowledge-based ((Lea-
cock and Chodorow, 1998; Wu and Palmer, 1994;
Lin, 1998), and others) and corpus-based metrics
(LSA (Landauer et al., 1997)), paired with lexi-
cal features (such as Dice-Similarity, Euclidean-
distance, etc.). It is trained on a cumulative En-
glish STS dataset comprising train and test data
released as part of tasks in SemEval2012 (Agirre
et al., 2012) and *Sem 2013 (Agirre et al., 2013),
as well as training data available from tasks 1 and
10 in SemEval 2014. Interestingly enough, run 2
of the system performs better than run 1, despite
the fact that it uses half the features, and focuses
on string based similarity measures only. This dif-
ference between runs is noticed on the Wikipedia
dataset only, and it amounts to 4% Pearson corre-
lation. While the system had a robust performance
on the Spanish subtask, for English, its overall
rank was 16, 18, and 33, respectively.
Coming in close at only 0.3% difference, is
Meerkat-Mafia PairingAvg (run 2) (Kashyap et
al., 2014), which also follows a cross-lingual ap-
proach, by applying the system the team devel-
oped for the English subtask to the translated ver-
sion of the datasets (see 3.1). The interesting as-
pect of their work is that in their first submission
(run 1), they only consider the similarity result-
ing from the sentence pair translation through the
Google Translate service.
15
In the second run,
they expand each sentence to 20 possible combi-
nations by accounting for the multiple translation
meanings of a given word, and considering the av-
erage similarity of all resulting pairs. While the
first run achieves a weighted correlation of 73.8%,
15
translate.google.com
88
Run Name System type Wikipedia News Weighted mean Rank
Bielefeld-SC-run1 unsupervised* 0.263 0.554 0.437 22
Bielefeld-SC-run2 unsupervised* 0.265 0.555 0.438 21
BUAP-run1 supervised 0.550 0.679 0.627 17
BUAP-run2 unsupervised 0.640 0.764 0.714 14
RTM-DCU-run1 supervised 0.422 0.700 0.588 18
RTM-DCU-run2 supervised 0.369 0.625 0.522 20
RTM-DCU-run3 supervised 0.424 0.641 0.554 19
LIPN-run1 supervised 0.652 0.826 0.756 11
LIPN-run2 supervised 0.716 0.832 0.785 6
LIPN-run3 supervised 0.716 0.809 0.771 10
Meerkat-Mafia-run1 unsupervised 0.668 0.785 0.738 13
Meerkat-Mafia-run2 unsupervised 0.743 0.845 0.804 2
Meerkat-Mafia-run3 supervised 0.738 0.822 0.788 5
TeamZ-run1 supervised 0.610 0.717 0.674 15
TeamZ-run2 supervised 0.604 0.710 0.667 16
UMCC-DLSI-run1 supervised 0.741 0.825 0.791 4
UMCC-DLSI-run2 supervised 0.7802 0.825 0.807 1
UNAL-NLP-run1 weakly supervised 0.7803 0.815 0.801 3
UNAL-NLP-run2 supervised 0.757 0.783 0.772 9
UNAL-NLP-run3 supervised 0.689 0.796 0.753 12
UoW-run1 supervised 0.748 0.800 0.779 7
UoW-run2 supervised 0.748 0.800 0.779 8
Table 4: Spanish evaluation results in terms of Pearson correlation.
the second one performs significantly better at
80.4%, indicating that the additional context may
also include multiple instances of accurate trans-
lations, hence significantly impacting the overall
similarity score. In English, the system equiva-
lent to run 2 in Spanish, namely Meerkat Mafia-
pairingWords, achieves a competitive ranked per-
formance across all six datasets, ranking second,
at an order of 10
?4
distance from the top sys-
tem. This supports the claim that, despite its unsu-
pervised nature, the system is quite versatile and
highly competitive with the top performing super-
vised frameworks, and that it may achieve an even
higher performance in Spanish if accurate sen-
tence translations were provided.
Overall, most systems were cross-lingual, rely-
ing on different translation approaches, such as 1)
translating the test data into English (as the two
systems above), and then exporting the score ob-
tained for the English sentences back to Spanish,
or 2) performing automatic translation of the En-
glish training data, and learning a classifier di-
rectly in Spanish. (Buscaldi et al., 2014) supple-
mented their training dataset with human annota-
tions conducted in Spanish, using definition pairs
extracted from a Spanish dictionary. A different
angle was explored by (Rios, 2014), who proposed
a multilingual framework using transfer learning
across English and Spanish by training on tradi-
tional lexical, knowledge-based and corpus-based
features. The semantic similarity task was ap-
proached from a monolingual perspective as well
(Gupta, 2014), by focusing on Spanish resources,
such as the trial data we released as part of the
subtask, and the Spanish WordNet;
16
these were
leveraged using meta-learning over variations of
overlap-based metrics. Following the same line,
(Bic?ici and Way, 2014) pursued language inde-
pendent methods, who avoided relying on task or
domain specific information through the usage of
referential translation machines. This approach
models textual semantic similarity as a decision in
terms of translation quality between two datasets
(in our case Spanish STS trial and test data) given
relevant examples from an in-language reference
corpus.
In comparison to the correlations obtained in the
English subtask, where the highest weighted mean
was 76.1%, for Spanish, we obtained 80.7%, prob-
ably due to the more formal nature of the datasets,
since Wikipedia and news articles employ mostly
well formed and grammatically correct sentences,
and we selected all snippets to be longer than 8
words. The overall correlation scores obtained for
English were hurt by the deft-forum data, which
scored significantly lower (at a maximum corre-
lation of 50.8%), when compared to all the other
datasets whose correlation was higher than 70%.
The OnWN data was most similar to our test sets,
and it attained a maximum of 85.9%.
16
grial.uab.es/descarregues.php
89
4 Conclusion
This year?s STS task comprised a multilingual
flair, by introducing Spanish datasets alongside the
English ones. In English, the datasets sought to ex-
pose the participating teams to more diverse sce-
narios compared to the previous years, by intro-
ducing image descriptions, forum and newswire
genre, and tweet-newswire headline mappings.
For Spanish, two datasets were developed consist-
ing of encyclopedic and newswire text acquired
from Spanish sources. Overall, the English sub-
task attracted 15 teams (with 38 system varia-
tions), while the Spanish subtask had 9 teams
(with 22 system runs). Most teams from the Span-
ish subtask have also submitted runs for the En-
glish evaluations.
Acknowledgments
The authors are grateful to Ver?onica P?erez-Rosas
and Vanessa Loza for their help with the anno-
tations for the Spanish subtask. This material is
based in part upon work supported by National
Science Foundation CAREER award #1361274
and IIS award #1018613, by DARPA-BAA-12-
47 DEFT grant #12475008, and by MINECO
CHIST-ERA READERS and SKATER projects
(PCIN-2013-002-C02-01, TIN2012-38584-C06-
02). Aitor Gonzalez Agirre is supported by a doc-
toral grant from MINECO. Any opinions, find-
ings, and conclusions or recommendations ex-
pressed in this material are those of the authors and
do not necessarily reflect the views of the National
Science Foundation or the Defense Advanced Re-
search Projects Agency.
References
Eneko Agirre and Enrique Amig?o. In prep. Exploring
evaluation measures for semantic textual similarity.
In Unpublished manuscript.
Eneko Agirre, Daniel Cer, Mona Diab, and Aitor
Gonzalez-Agirre. 2012. Semeval-2012 task 6: A
pilot on semantic textual similarity. In *SEM 2012:
The First Joint Conference on Lexical and Compu-
tational Semantics ? Volume 1: Proceedings of the
main conference and the shared task, and Volume 2:
Proceedings of the Sixth International Workshop on
Semantic Evaluation (SemEval 2012), pages 385?
393, Montr?eal, Canada, 7-8 June.
Eneko Agirre, Daniel Cer, Mona Diab, Aitor Gonzalez-
Agirre, and Weiwei Guo. 2013. *SEM 2013 Shared
Task: Semantic textual similarity, including a pi-
lot on typed-similarity. In The Second Joint Con-
ference on Lexical and Computational Semantics
(*SEM 2013), pages 32?43.
Clive Best, Erik van der Goot, Ken Blackler, Tefilo
Garcia, and David Horby. 2005. Europe me-
dia monitor - system description. In EUR Report
22173-En, Ispra, Italy.
Ergun Bic?ici and Andy Way. 2014. RTM-DCU: Ref-
erential translation machines for semantic similarity.
In Proceedings of the 8th International Workshop on
Semantic Evaluation (SemEval-2014), Dublin, Ire-
land.
Davide Buscaldi, Jorge J. Garcia Flores, Joseph Le
Roux, Nadi Tomeh, and Belem Priego Sanchez.
2014. LIPN: Introducing a new geographical con-
text similarity measure and a statistical similarity
measure based on the Bhattacharyya coefficient. In
Proceedings of the 8th International Workshop on
Semantic Evaluation (SemEval-2014), Dublin, Ire-
land.
Alexander Chavez, Hector Davila, Yoan Gutierrez,
Antonio Fernandez-Orquin, Andr?es Montoyo, and
Rafael Munoz. 2014. UMCC DLSI SemSim: Mul-
tilingual system for measuring semantic textual sim-
ilarity. In Proceedings of the 8th International
Workshop on Semantic Evaluation (SemEval-2014),
Dublin, Ireland.
Christiane Fellbaum. 1998. WordNet - An electronic
lexical database. MIT Press.
Weiwei Guo, Hao Li, Heng Ji, and Mona Diab. 2013.
Linking tweets to news: A framework to enrich on-
line short text data in social media. In Proceedings
of the 51th Annual Meeting of the Association for
Computational Linguistics, pages 239?249.
Anubhav Gupta. 2014. TeamZ: Measuring semantic
textual similarity for Spanish using an overlap-based
approach. In Proceedings of the 8th International
Workshop on Semantic Evaluation (SemEval-2014),
Dublin, Ireland.
Eduard Hovy, Mitchell Marcus, Martha Palmer,
Lance Ramshaw, and Ralph Weischedel. 2006.
OntoNotes: The 90% solution. In Proceedings of
the Human Language Technology Conference of the
North American Chapter of the ACL, pages 57?60.
Sergio Jimenez, George Due?nas, Julia Baquero, and
Alexander Gelbukh. 2014. UNAL-NLP: Combin-
ing soft cardinality features for semantic textual sim-
ilarity, relatedness and entailment. In Proceedings
of the 8th International Workshop on Semantic Eval-
uation (SemEval-2014), Dublin, Ireland.
Abhay Kashyap, Lushan Han, Roberto Yus, Jennifer
Sleeman, Taneeya Satyapanich, Sunil Gandhi, and
Tim Finin. 2014. Meerkat Mafia: Multilingual and
cross-level semantic textual similarity systems. In
Proceedings of the 8th International Workshop on
90
Semantic Evaluation (SemEval-2014), Dublin, Ire-
land.
Thomas K. Landauer, Darrell Laham, Bob Rehder, and
M. E. Schreiner. 1997. How well can passage mean-
ing be derived without using word order? A compar-
ison of latent semantic analysis and humans. Cogni-
tive Science.
Claudia Leacock and Martin Chodorow. 1998. Com-
bining local context and WordNet similarity for
word sense identification. In WordNet: An Elec-
tronic Lexical Database, pages 305?332.
Dekang Lin. 1998. An information-theoretic defini-
tion of similarity. In Proceedings of the Fifteenth In-
ternational Conference on Machine Learning, pages
296?304, Madison, Wisconsin.
Andr?e Lynum, Partha Pakray, Bj?orn Gamb?ack, and
Sergio Jimenez. 2014. NTNU: Measuring se-
mantic similarity with sublexical feature represen-
tations and soft cardinality. In Proceedings of the
8th International Workshop on Semantic Evaluation
(SemEval-2014), Dublin, Ireland.
Juan Martinez-Romo, Lourdes Araujo, Javier Borge-
Holthoefer, Alex Arenas, Jos?e A. Capit?an, and
Jos?e A. Cuesta. 2011. Disentangling categori-
cal relationships through a graph of co-occurrences.
Phys. Rev. E, 84:046108, Oct.
John P. McCrae, Philipp Cimiano, and Roman Klinger.
2013. Orthonormal explicit topic analysis for cross-
lingual document matching. In Proceedings of the
2013 Conference on Empirical Methods in Natu-
ral Language Processing, pages 1732?1740, Seattle,
Washington, USA.
Francesco Pozzi, Tiziana Di Matteo, and Tomaso Aste.
2012. Exponential smoothing weighted correla-
tions. The European Physical Journal B, 85(6).
William H. Press, Saul A. Teukolsky, William T. Vet-
terling, and Brian P. Flannery. 2002. Numerical
recipes: The art of scientific computing V 2.10 with
Linux or single-screen license. Cambridge Univer-
sity Press.
Thomas Proisi, Stefan Evert, Paul Greiner, and Besim
Kabashi. 2014. SemantiKLUE: Robust semantic
similarity at multiple levels using maximum weight
matching. In Proceedings of the 8th International
Workshop on Semantic Evaluation (SemEval-2014),
Dublin, Ireland.
Cyrus Rashtchian, Peter Young, Micah Hodosh, and
Julia Hockenmaier. 2010. Collecting image annota-
tions using Amazon?s Mechanical Turk. In Proceed-
ings of the NAACL HLT 2010 Workshop on Creating
Speech and Language Data with Amazon?s Mechan-
ical Turk, CSLDAMT ?10, pages 139?147, Strouds-
burg, PA, USA.
Miguel Rios. 2014. UoW: Multi-task learning Gaus-
sian process for semantic textual similarity. In Pro-
ceedings of the 8th International Workshop on Se-
mantic Evaluation (SemEval-2014), Dublin, Ireland.
Richard Socher, Andrej Karpathy, Quoc V. Le, Christo-
pher D. Manning, and Andrew Y. Ng. 2014.
Grounded compositional semantics for finding and
describing images with sentences. Transactions
of the Association for Computational Linguistics,
pages 207?218.
Md Arafat Sultan, Steven Bethard, and Tamara Sum-
ner. 2014a. Back to basics for monolingual align-
ment: Exploiting word similarity and contextual ev-
idence. Transactions of the Association for Compu-
tational Linguistics, 2:219?230.
Md Arafat Sultan, Steven Bethard, and Tamara Sum-
ner. 2014b. DLS@CU: Sentence similarity from
word aligment. In Proceedings of the 8th Interna-
tional Workshop on Semantic Evaluation (SemEval-
2014), Dublin, Ireland.
Darnes Vilari?no, David Pinto, Sa?ul Le?on, Mireya To-
var, and Beatriz Beltr?an. 2014. BUAP: Evaluating
features for multilingual and cross-level semantic
textual similarity. In Proceedings of the 8th Interna-
tional Workshop on Semantic Evaluation (SemEval-
2014), Dublin, Ireland.
Ngoc Phuoc An Vo, Tommaso Caselli, and Octavian
Popescu. 2014. FBK-TR: Applying SVM with
multiple linguistic features for cross-level semantic
similarity. In Proceedings of the 8th International
Workshop on Semantic Evaluation (SemEval-2014),
Dublin, Ireland.
Frane
?
Sari?c, Goran Glava?s, Mladen Karan, Jan
?
Snajder,
and Bojana Dalbelo Ba?si?c. 2012. Takelab: Sys-
tems for measuring semantic text similarity. In Pro-
ceedings of the Sixth International Workshop on Se-
mantic Evaluation (SemEval 2012), pages 441?448,
Montr?eal, Canada, 7-8 June.
Zhibiao Wu and Martha Palmer. 1994. Verbs seman-
tics and lexical selection. In Proceedings of the 32nd
annual meeting on Association for Computational
Linguistics, pages 133?138, Las Cruces, New Mex-
ico.
91
Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 560?565,
Dublin, Ireland, August 23-24, 2014.
SimCompass: Using Deep Learning Word Embeddings
to Assess Cross-level Similarity
Carmen Banea, Di Chen,
Rada Mihalcea
?
University of Michigan
Ann Arbor, MI
Claire Cardie
Cornell University
Ithaca, NY
Janyce Wiebe
University of Pittsburgh
Pittsburgh, PA
Abstract
This article presents our team?s partici-
pating system at SemEval-2014 Task 3.
Using a meta-learning framework, we
experiment with traditional knowledge-
based metrics, as well as novel corpus-
based measures based on deep learning
paradigms, paired with varying degrees of
context expansion. The framework en-
abled us to reach the highest overall per-
formance among all competing systems.
1 Introduction
Semantic textual similarity is one of the key
components behind a multitude of natural lan-
guage processing applications, such as informa-
tion retrieval (Salton and Lesk, 1971), relevance
feedback and text classification (Rocchio, 1971),
word sense disambiguation (Lesk, 1986; Schutze,
1998), summarization (Salton et al., 1997; Lin
and Hovy, 2003), automatic evaluation of machine
translation (Papineni et al., 2002), plagiarism de-
tection (Nawab et al., 2011), and more.
To date, semantic similarity research has pri-
marily focused on comparing text snippets of simi-
lar length (see the semantic textual similarity tasks
organized during *Sem 2013 (Agirre et al., 2013)
and SemEval 2012 (Agirre et al., 2012)). Yet,
as new challenges emerge, such as augmenting a
knowledge-base with textual evidence, assessing
similarity across different context granularities is
gaining traction. The SemEval Cross-level seman-
tic similarity task is aimed at this latter scenario,
and is described in more details in the task paper
(Jurgens et al., 2014).
?
{carmennb,chenditc,mihalcea}@umich.edu
This work is licensed under a Creative Commons At-
tribution 4.0 International Licence. Page numbers and pro-
ceedings footer are added by the organisers. Licence details:
http://creativecommons.org/licenses/by/4.0/
2 Related Work
Over the past years, the research community has
focused on computing semantic relatedness us-
ing methods that are either knowledge-based or
corpus-based. Knowledge-based methods derive
a measure of relatedness by utilizing lexical re-
sources and ontologies such as WordNet (Miller,
1995) or Roget (Rog, 1995) to measure defi-
nitional overlap, term distance within a graph-
ical taxonomy, or term depth in the taxonomy
as a measure of specificity. There are many
knowledge-based measures that were proposed in
the past, e.g., (Leacock and Chodorow, 1998;
Lesk, 1986; Resnik, 1995; Jiang and Conrath,
1997; Lin, 1998; Jarmasz and Szpakowicz, 2003;
Hughes and Ramage, 2007).
On the other side, corpus-based measures such
as Latent Semantic Analysis (LSA) (Landauer
and Dumais, 1997), Explicit Semantic Analy-
sis (ESA) (Gabrilovich and Markovitch, 2007),
Salient Semantic Analysis (SSA) (Hassan and
Mihalcea, 2011), Pointwise Mutual Informa-
tion (PMI) (Church and Hanks, 1990), PMI-IR
(Turney, 2001), Second Order PMI (Islam and
Inkpen, 2006), Hyperspace Analogues to Lan-
guage (Burgess et al., 1998) and distributional
similarity (Lin, 1998) employ probabilistic ap-
proaches to decode the semantics of words. They
consist of unsupervised methods that utilize the
contextual information and patterns observed in
raw text to build semantic profiles of words. Un-
like knowledge-based methods, which suffer from
limited coverage, corpus-based measures are able
to induce the similarity between any two words, as
long as they appear in the corpus used for training.
3 System Description
3.1 Generic Features
Our system employs both knowledge and corpus-
based measures as detailed below.
560
Knowledge-based features
Knowledge-based metrics were shown to provide
high correlation scores with the goldstandard in
text similarity tasks (Agirre et al., 2012; Agirre et
al., 2013). We used three WordNet-based simi-
larity measures that employ information content.
We chose these metrics because they are able to
incorporate external information derived from a
large corpus: Resnik (Resnik, 1995) (RES), Lin
(Lin, 1998) (LIN ), and Jiang & Conrath (Jiang
and Conrath, 1997) (JCN ).
Corpus based features
Our corpus based features are derived from a
deep learning vector space model that is able to
?understand? word meaning without human in-
put. Distributed word embeddings are learned us-
ing a skip-gram recurrent neural net architecture
running over a large raw corpus (Mikolov et al.,
2013b; Mikolov et al., 2013a). A primary advan-
tage of such a model is that, by breaking away
from the typical n-gram model that sees individual
units with no relationship to each other, it is able to
generalize and produce word vectors that are simi-
lar for related words, thus encoding linguistic reg-
ularities and patterns (Mikolov et al., 2013b). For
example, vec(Madrid)-vec(Spain)+vec(France) is
closer to vec(Paris) than any other word vec-
tor (Mikolov et al., 2013a). We used the pre-
trained Google News word2vec model (WTV )
built over a 100 billion words corpus, and con-
taining 3 million 300-dimension vectors for words
and phrases. The model is distributed with the
word2vec toolkit.
1
Since the methods outlined above provide similar-
ity scores at the sense or word level, we derive text
level metrics by employing two methods.
VectorSum. We add the vectors corresponding to
the non-stopwords tokens in bag of words (BOW)
A and B, resulting in vectors V
A
and V
B
, respec-
tively. The assumption is that these vectors are
able to capture the semantic meaning associated
with the contexts, enabling us to gauge their relat-
edness using cosine similarity.
Align. Given two BOW A and B as input, we
compare them using a word-alignment-based sim-
ilarity measure (Mihalcea et al., 2006). We calcu-
late the pairwise similarity between the words in
A and B, and match each word in A with its most
similar counterpart in B. For corpus-based fea-
1
https://code.google.com/p/word2vec/
tures, the similarity measure represents the aver-
age over these scores, while for knowledge-based
measures, we consider the top 40% ranking pairs.
We use the DKPro Similarity package (B?ar et
al., 2013) to compute knowledge-based metrics,
and the word2vec implementation from the Gen-
sim toolkit (Rehurek and Sojka, 2010).
3.2 Feature Variations
Since our system participated in all four lexical
levels evaluations, we describe below the modifi-
cations pertaining to each.
word2sense. At the word2sense level, we em-
ploy both knowledge and corpus-based features.
Since the information available in each pair is ex-
tremely limited (only a word and a sense key)
we infuse contextual information by drawing on
WordNet (Miller, 1995). In WordNet, the sense
of each word is encapsulated in a uniquely iden-
tifiable synset, consisting of the definition (gloss),
usage examples and its synonyms. We can derive
three variations (where the word and sense com-
ponents are represented by BOW A and B, respec-
tively): a) no expansion (A={word}, B={sense}),
b) expand right (R) (A={word}, B={sense gloss
& example}), c) expand left (L) & right (R)
(A={word glosses & examples}, B={sense gloss
& example}). After applying the Align method,
we obtain measures JNC, LIN , RES and
WTV 1; VectorSum results in WTV 2.
phrase2word. As this lexical level also suf-
fers from low context, we adapt the above vari-
ations, where the phrase and word components
are represented by BOW A and BOW B, re-
spectively. Thus, we have: a) no expan-
sion (A={phrase}, B={word}), b) expand R
(A={phrase}, B={word glosses and examples}),
c) expand L & R (A={phrase glosses & exam-
ples}, B={word glosses and examples}). We ex-
tract the same measures as for word2sense.
sentence2phrase. For this variation, we use only
corpus based measures; BOW A represents the
sentence component, B, the phrase. Since there is
sufficient context available, we follow the no ex-
pansion variation, and obtain metrics WTV 1 (by
applying Align) and WTV 2 (using VectorSum).
paragraph2sentence. At this level, due to the
long context that entails one-to-many mappings
between the words in the sentence and those in
the paragraph, we use a text clustering technique
prior to calculating the features? weights.
561
a) no clustering. We use only corpus based mea-
sures, where the paragraph represents BOW A,
and the sentence represents BOW B. Then we ap-
ply Align and VectorSum, resulting in WTV 1 and
WTV 2, respectively.
b) paragraph centroids extraction. Since the
longer text contains more information compared
to the shorter one, we extract k topic vectors after
K-means clustering the left context.
2
These cen-
troids are able to model topics permeating across
sentences, and by comparing them with the word
vectors pertaining to the short text, we seek to cap-
ture how much of the information is covered in the
shorter text. Each word is paired with the centroid
that it is closest to, and the average is computed
over these scores, resulting in WTV 3.
c) sentence centroids extraction. Under a dif-
ferent scenario, assuming that one sentence cov-
ers only a few strongly expressed topics, unlike
a paragraph that may digress and introduce unre-
lated noise, we apply clustering on the short text.
The centroids thus obtained are able to capture
the essence of the sentence, so when compared to
every word in the paragraph, we can gauge how
much of the short text is reflected in the longer
one. Each centroid is paired with the word that it is
most similar to, and we average these scores, thus
obtaining WTV 4. In a way, methods b) and c)
provide a macro, respectively micro view of how
the topics are reflected across the two spans of text.
3.3 Meta-learning
The measures of similarity described above pro-
vide a single score per each long text - short text
pair in the training and test data. These scores then
become features for a meta-learner, which is able
to optimize their impact on the prediction process.
We experimented with multiple regression algo-
rithms by conducting 10 fold cross-validation on
the training data. The strongest performer across
all lexical levels was Gaussian processes with a
radial basis function (RBF) kernel. Gaussian pro-
cesses regression is an efficient probabilistic pre-
diction framework that assumes a Gaussian pro-
cess prior on the unobservable (latent) functions
and a likelihood function that accounts for noise.
An individual classifier
3
was trained for each lex-
ical level and applied to the test data sets.
2
Implementation provided in the Scikit library (Pedregosa
et al., 2011), where k is set to 3.
3
Implementation available in the WEKA machine learn-
ing software (Hall et al., 2009) using the default parameters.
4 Evaluations & Discussion
Our system participated in all cross-level subtasks
under the name SimCompass, competing with 37
other systems developed by 20 teams.
Figure 1 highlights the Pearson correlations at
the four lexical levels between the gold standard
and each similarity measure introduced in Section
3, as well as the predictions ensuing as a result
of meta-learning. The left and right histograms in
each subfigure present the scores obtained on the
train and test data, respectively.
In the case of word2sense train data, we no-
tice that expanding the context provides additional
information and improves the correlation results.
For corpus-based measures, the correlations are
stronger when the expansion involves only the
right side of the tuple, namely the sense. We
notice an increase of 0.04 correlation points for
WTV1 and 0.09 for WTV2. As soon as the word
is expanded as well, the context incorporates too
much noise, and the correlation levels drop. In
the case of knowledge-based measures, expanding
the context does not seem to impact the results.
However, these trends do not carry out to the test
data, where the corpus-based features without ex-
pansion reach a correlation higher than 0.3, while
the knowledge-based features score significantly
lower (by 0.16). Once all these measures are used
as features in a meta learner (All) using Gaus-
sian processes regression (GP), the correlation in-
creases over the level attained by the best perform-
ing individual feature, reaching 0.45 on the train
data and 0.36 on the test data. SimCompass ranks
second in this subtask?s evaluations, falling short
of the leading system by 0.025 correlation points.
Turning now to the phrase2word subfigure, we
notice that the context already carries sufficient
information, and expanding it causes the perfor-
mance to drop (the more extensive the expan-
sion, the steeper the drop). Unlike the scenario
encountered for word2sense, the trend observed
here on the training data also gets mirrored in the
test data. Same as before, knowledge-based mea-
sures have a significantly lower performance, but
deep learning-based features based on word2vec
(WTV) only show a correlation variation by at
most 0.05, proving their robustness. Leveraging
all the features in a meta-learning framework en-
ables the system to predict stronger scores for both
the train and the test data (0.48 and 0.42, respec-
tively). Actually, for this variation, SimCompass
562
 0
 0.1
 0.2
 0.3
 0.4
 0.5
w
o
rd
2s
en
se
Train Test
 0
 0.1
 0.2
 0.3
 0.4
 0.5
BL JNC
LIN
RES
W
TV
1
W
TV
2
G
P
JN
C
LIN
RES
W
TV
1
W
TV
2
G
P
ph
ra
se
2w
or
d
No expansion Expansion R Expansion L&R All
 0
 0.2
 0.4
 0.6
 0.8
s
e
n
te
n
ce
2p
hr
as
e
Train Test
BL WTV
1
W
TV
2
W
TV
3
W
TV
4
G
P
W
TV
1
W
TV
2
W
TV
3
W
TV
4
G
P
 0
 0.2
 0.4
 0.6
 0.8
pa
ra
gr
ap
h2
se
nt
en
ce
Figure 1: Pearson correlation of individual measures on the train and test data sets. As these measures be-
come features in a regression algorithm (GP), prediction correlations are included as well. BL represents
the baseline computed by the organizers.
obtains the highest score among all competing sys-
tems, surpassing the second best by 0.10.
Noticing that expansion is not helpful when suf-
ficient context is available, for the next variations
we use the original tuples. Also, due to the re-
duced impact of knowledge-based features on the
training outcome, we only focus on deep learning
features (WTV1, WTV2, WTV3, WTV4).
Shifting to sentence2phrase, WTV2 (con-
structed using VectorSum) is the top perform-
ing feature, surpassing the baseline by 0.19,
and attaining 0.69 and 0.73 on the train and
test sets, respectively. Despite also considering
a lower performing feature (WTV1), the meta-
learner maintains high scores, surpassing the cor-
relation achieved on the train data by 0.04 (from
0.70 to 0.74). In this variation, our system ranks
fifth, at 0.035 from the top system.
For the paragraph2sentence variation, due to
the availability of longer contexts, we introduce
WTV3 and WTV4 that are based on clustering the
left and the right sides of the tuple, respectively.
WTV2 fares slightly better than WTV3 and WTV4.
WTV1 surpasses the baseline this time, leaving its
mark on the decision process. When training the
GP learner on all features, we obtain 0.78 correla-
tion on the train data, and 0.81 on test data, 0.10
higher than those attained by the individual fea-
tures alone. SimCompass ranks seventh in perfor-
mance on this subtask, at 0.026 from the first.
Considering the overall system performance,
SimCompass is remarkably versatile, ranking
among the top at each lexical level, and taking the
first place in the SemEval Task 3 overall evalu-
ation with respect to both Pearson (0.58 average
correlation) and Spearman correlations.
5 Conclusion
We described SimCompass, the system we partic-
ipated with at SemEval-2014 Task 3. Our exper-
iments suggest that traditional knowledge-based
features are cornered by novel corpus-based word
meaning representations, such as word2vec, which
emerge as efficient and strong performers under
a variety of scenarios. We also explored whether
context expansion is beneficial to the cross-level
similarity task, and remarked that only when the
context is particularly short, this enrichment is vi-
able. However, in a meta-learning framework, the
information permeating from a set of similarity
measures exposed to varying context expansions
can attain a higher performance than possible with
individual signals. Overall, our system ranked first
among 21 teams and 38 systems.
Acknowledgments
This material is based in part upon work sup-
ported by National Science Foundation CAREER
award #1361274 and IIS award #1018613 and
by DARPA-BAA-12-47 DEFT grant #12475008.
Any opinions, findings, and conclusions or recom-
mendations expressed in this material are those of
the authors and do not necessarily reflect the views
563
of the National Science Foundation or the Defense
Advanced Research Projects Agency.
References
Eneko Agirre, Daniel Cer, Mona Diab, and Aitor
Gonzalez-Agirre. 2012. SemEval-2012 Task 6: A
pilot on semantic textual similarity. In Proceedings
of the 6th International Workshop on Semantic Eval-
uation (SemEval 2012), in conjunction with the First
Joint Conference on Lexical and Computational Se-
mantics (*SEM 2012), Montreal, Canada.
Eneko Agirre, Daniel Cer, Mona Diab, Aitor Gonzalez-
Agirre, and Weiwei Guo. 2013. *SEM 2013 Shared
Task: Semantic Textual Similarity, including a Pi-
lot on Typed-Similarity. In The Second Joint Con-
ference on Lexical and Computational Semantics
(*SEM 2013).
Daniel B?ar, Torsten Zesch, and Iryna Gurevych. 2013.
DKPro Similarity: An open source framework for
text similarity. In Proceedings of the 51st Annual
Meeting of the Association for Computational Lin-
guistics: System Demonstrations, pages 121?126,
Sofia, Bulgaria.
Curt Burgess, Kay Livesay, and Kevin Lund. 1998.
Explorations in context space: words, sentences,
discourse. Discourse Processes, 25(2):211?257.
Kenneth Church and Patrick Hanks. 1990. Word as-
sociation norms, mutual information, and lexicogra-
phy. Computational Linguistics, 16(1):22?29.
Evgeniy Gabrilovich and Shaul Markovitch. 2007.
Computing semantic relatedness using Wikipedia-
based explicit semantic analysis. In Proceedings of
the 20th International Joint Conference on Artificial
Intelligence, pages 1606?1611, Hyderabad, India.
Mark Hall, Eibe Frank, Geoffrey Holmes, Bernhard
Pfahringer, Peter Reutemann, and Ian H. Witten.
2009. The WEKA Data Mining Software: An Up-
date. SIGKDD Explorations, 11(1).
Samer Hassan and Rada Mihalcea. 2011. Measuring
semantic relatedness using salient encyclopedic con-
cepts. Artificial Intelligence, Special Issue.
Thad Hughes and Daniel Ramage. 2007. Lexical se-
mantic knowledge with random graph walks. In
Proceedings of the Conference on Empirical Meth-
ods in Natural Language Processing, Prague, Czech
Republic.
Aminul Islam and Diana Zaiu Inkpen. 2006. Second
order co-occurrence PMI for determining the seman-
tic similarity of words. In Proceedings of the Fifth
Conference on Language Resources and Evaluation,
volume 2, pages 1033?1038, Genoa, Italy, July.
Mario Jarmasz and Stan Szpakowicz. 2003. Roget?s
thesaurus and semantic similarity. In Proceedings
of the conference on Recent Advances in Natural
Language Processing RANLP-2003, Borovetz, Bul-
garia, September.
Jay J. Jiang and David W. Conrath. 1997. Semantic
similarity based on corpus statistics and lexical tax-
onomy. In Proceeding of the International Confer-
ence Research on Computational Linguistics (RO-
CLING X), Taiwan.
David Jurgens, Mohammad Taher Pilehvar, and
Roberto Navigli. 2014. SemEval-2014 Task 3:
Cross-Level Semantic Similarity. In Proceedings of
the 8th International Workshop on Semantic Evalu-
ation (SemEval-2014), Dublin, Ireland.
Thomas K. Landauer and Susan T. Dumais. 1997.
A solution to plato?s problem: The latent semantic
analysis theory of acquisition, induction, and repre-
sentation of knowledge. Psychological Review, 104.
Claudia Leacock and Martin Chodorow. 1998. Com-
bining local context and WordNet sense similarity
for word sense identification. In WordNet, An Elec-
tronic Lexical Database. The MIT Press.
Michael E. Lesk. 1986. Automatic sense disambigua-
tion using machine readable dictionaries: How to
tell a pine cone from an ice cream cone. In Pro-
ceedings of the SIGDOC Conference 1986, Toronto,
June.
Chin-Yew Lin and Eduard Hovy. 2003. Auto-
matic evaluation of summaries using n-gram co-
occurrence statistics. In Proceedings of Human Lan-
guage Technology Conference (HLT-NAACL 2003),
Edmonton, Canada, May.
Dekang Lin. 1998. An information-theoretic defini-
tion of similarity. In Proceedings of the Fifteenth In-
ternational Conference on Machine Learning, pages
296?304, Madison, Wisconsin.
Rada Mihalcea, Courtney Corley, and Carlo Strappa-
rava. 2006. Corpus-based and knowledge-based
measures of text semantic similarity. In American
Association for Artificial Intelligence (AAAI-2006),
Boston, MA.
Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey
Dean. 2013a. Distributed Representations of Words
and Phrases and their Compositionality . In NIPS,
pages 3111?3119.
Tomas Mikolov, Wen-tau Yih, and Geoffrey Zweig.
2013b. Linguistic regularities in continuous space
word representations. In NAACL HLT, pages 746?
751, Atlanta, GA, USA.
George A. Miller. 1995. WordNet: a Lexical database
for English. Communications of the Association for
Computing Machinery, 38(11):39?41.
Rao Muhammad Adeel Nawab, Mark Stevenson, and
Paul Clough. 2011. External plagiarism detection
using information retrieval and sequence alignment:
564
Notebook for PAN at CLEF 2011. In Proceedings
of the 5th International Workshop on Uncovering
Plagiarism, Authorship, and Social Software Misuse
(PAN 2011).
Kishore. Papineni, Salim Roukos, Todd Ward, and
Wei-Jing Zhu. 2002. Bleu: A method for automatic
evaluation of machine translation. In Proceedings of
the 40th Annual Meeting of the Association for Com-
putational Linguistics, pages 311?318, Philadelphia,
PA.
Fabian Pedregosa, Ga?el Varoquaux, Alexandre Gram-
fort, Vincent Michel, Bertrand Thirion, Olivier
Grisel, Mathieu Blondel, Peter Prettenhofer, Ron
Weiss, Vincent Dubourg, Jake Vanderplas, Alexan-
dre Passos, David Cournapeau, Matthieu Brucher,
Matthieu Perrot, and
?
Edouard Duchesnay. 2011.
Scikit-learn: Machine learning in Python. The Jour-
nal of Machine Learning Research, 12:2825?2830.
Radim Rehurek and Petr Sojka. 2010. Software frame-
work for topic modelling with large corpora. In Pro-
ceedings of the LREC 2010 Workshop on New Chal-
lenges for NLP Frameworks, pages 45?50, Valletta,
Malta, May. ELRA.
Philip Resnik. 1995. Using information content to
evaluate semantic similarity in a taxonomy. In Pro-
ceedings of the 14th International Joint Conference
on Artificial Intelligence, pages 448?453, Montreal,
Quebec, Canada. Morgan Kaufmann Publishers Inc.
J. Rocchio, 1971. Relevance feedback in information
retrieval. Prentice Hall, Ing. Englewood Cliffs, New
Jersey.
1995. Roget?s II: The New Thesaurus. Houghton Mif-
flin.
Gerard Salton and Michael E. Lesk, 1971. The SMART
Retrieval System: Experiments in Automatic Doc-
ument Processing, chapter Computer evaluation of
indexing and text processing. Prentice Hall, Ing. En-
glewood Cliffs, New Jersey.
Gerard Salton, Amit Singhal, Mandar Mitra, and Chris
Buckley. 1997. Automatic text structuring and sum-
marization. Information Processing and Manage-
ment, 2(32).
Hinrich Schutze. 1998. Automatic word sense dis-
crimination. Computational Linguistics, 24(1):97?
124.
Peter D. Turney. 2001. Mining the Web for Synonyms:
PMI-IR versus LSA on TOEFL. In Proceedings of
the 12th European Conference on Machine Learning
(ECML?01), pages 491?502, Freiburg, Germany.
565
Proceedings of the Workshop on Automatic Summarization for Different Genres, Media, and Languages, pages 16?24,
Portland, Oregon, June 23, 2011. c?2011 Association for Computational Linguistics
Summarizing Decisions in Spoken Meetings
Lu Wang
Department of Computer Science
Cornell University
Ithaca, NY 14853
luwang@cs.cornell.edu
Claire Cardie
Department of Computer Science
Cornell University
Ithaca, NY 14853
cardie@cs.cornell.edu
Abstract
This paper addresses the problem of summa-
rizing decisions in spoken meetings: our goal
is to produce a concise decision abstract for
each meeting decision. We explore and com-
pare token-level and dialogue act-level au-
tomatic summarization methods using both
unsupervised and supervised learning frame-
works. In the supervised summarization set-
ting, and given true clusterings of decision-
related utterances, we find that token-level
summaries that employ discourse context can
approach an upper bound for decision ab-
stracts derived directly from dialogue acts.
In the unsupervised summarization setting,we
find that summaries based on unsupervised
partitioning of decision-related utterances per-
form comparably to those based on partitions
generated using supervised techniques (0.22
ROUGE-F1 using LDA-based topic models
vs. 0.23 using SVMs).
1 Introduction
Meetings are a common way for people to share in-
formation and discuss problems. And an effective
meeting always leads to concrete decisions. As a re-
sult, it would be useful to develop automatic meth-
ods that summarize not the entire meeting dialogue,
but just the important decisions made. In particular,
decision summaries would allow participants to re-
view decisions from previous meetings as they pre-
pare for an upcoming meeting. For those who did
not participate in the earlier meetings, decision sum-
maries might provide one type of efficient overview
of the meeting contents. For managers, decision
summaries could act as a concise record of the idea
generation process.
While there has been some previous work in
summarizing meetings and conversations, very lit-
tle work has focused on decision summarization:
Ferna?ndez et al (2008a) and Bui et al (2009) in-
vestigate the use of a semantic parser and machine
learning methods for phrase- and token-level deci-
sion summarization. We believe our work is the first
to explore and compare token-level and dialogue
act-level approaches ? using both unsupervised and
supervised learning methods ? for summarizing de-
cisions in meetings.
C: Just spinning and not scrolling , I would say . (1)
C: But if you?ve got a [disfmarker] if if you?ve got a flipped
thing , effectively it?s something that?s curved on one side
and flat on the other side , but you folded it in half . (2)
D: the case would be rubber and the the buttons , (3)
B: I think the spinning wheel is definitely very now . (1)
B: and then make the colour of the main remote [vocal-
sound] the colour like vegetable colours , do you know ? (4)
B: I mean I suppose vegetable colours would be orange
and green and some reds and um maybe purple (4)
A: but since LCDs seems to be uh a definite yes , (1)
A: Flat on the top . (2)
Decision Abstracts (Summary)
DECISION 1: The remote will have an LCD and spinning
wheel inside.
DECISION 2: The case will be flat on top and curved on
the bottom.
DECISION 3: The remote control and its buttons will be
made of rubber.
DECISION 4: The remote will resemble a vegetable and
be in bright vegetable colors.
Table 1: A clip of a meeting from the AMI meeting cor-
pus (Carletta et al, 2005). A, B, C and D refer to distinct
speakers; the numbers in parentheses indicate the asso-
ciated meeting decision: DECISION 1, 2, 3 or 4. Also
shown is the gold-standard (manual) abstract (summary)
for each decision.
16
Consider the sample dialogue snippet in Table 1,
which is part of the AMI meeting corpus (Carletta et
al., 2005). The Table lists only decision-related di-
alogue acts (DRDAs) ? utterances associated with
at least one decision made in the meeting.1 The DR-
DAs are ordered by time; intervening utterances are
not shown. DRDAs are important because they con-
tain critical information for decision summary con-
struction.
Table 1 clearly shows some challenges for deci-
sion summarization for spoken meetings beyond the
disfluencies, high word error rates, absence of punc-
tuation, interruptions and hesitations due to speech.
First, different decisions can be discussed more or
less concurrently; as a result, the utterances asso-
ciated with a single decision are not contiguous in
the dialogue. In Table 1, the dialogue acts (hence-
forth, DAs) concerning DECISION 1, for exam-
ple, are interleaved with DAs for other decisions.
Second, some decision-related DAs contribute more
than others to the associated decision. In compos-
ing the summary for DECISION 1, for example, we
might safely ignore the first DA for DECISION 1. Fi-
nally, more so than for standard text summarization,
purely extract-based summaries are not likely to be
easily interpretable: DRDAs often contain text that
is irrelevant to the decision and many will only be
understandable if analyzed in the context of the sur-
rounding utterances.
In this paper, we study methods for decision sum-
marization for spoken meetings. We assume that
all decision-related DAs have been identified and
aim to produce a summary for the meeting in the
form of concise decision abstracts (see Table 1), one
for each decision made. In response to the chal-
lenges described above, we propose a summariza-
tion framework that includes:
Clustering of decision-related DAs. Here we aim to
partition the decision-related utterances (DRDAs)
according to the decisions each supports. This step
is similar in spirit to many standard text summariza-
tion techniques (Salton et al, 1997) that begin by
grouping sentences according to semantic similar-
ity.
Summarization at the DA-level. We select just the im-
portant DRDAs in each cluster. Our goal is to elimi-
nate redundant and less informative utterances. The
1These are similar, but not completely equivalent, to the de-
cision dialogue acts (DDAs) of Bui et al (2009), Ferna?ndez et
al. (2008a), Frampton et al (2009). The latter refer to all DAs
that appear in a decision discussion even if they do NOT support
any particular decision.
selected DRDAs are then concatenated to form the
decision summary.
Optional token-level summarization of the selected
DRDAs. Methods are employed to capture con-
cisely the gist of each decision, discarding any
distracting text.
Incorporation of the discourse context as needed.
We hypothesize that this will produce more
interpretable summaries.
More specifically, we compare both unsupervised
(TFIDF (Salton et al, 1997) and LDA topic mod-
eling (Blei et al, 2003)) and (pairwise) supervised
clustering procedures (using SVMs and MaxEnt) for
partitioning DRDAs according to the decision each
supports. We also investigate unsupervised methods
and supervised learning for decision summarization
at both the DA and token level, with and without the
incorporation of discourse context. During training,
the supervised decision summarizers are told which
DRDAs for each decision are the most informative
for constructing the decision abstract.
Our experiments employ the aforementioned
AMI meeting corpus: we compare our decision
summaries to the manually generated decision ab-
stracts for each meeting and evaluate performance
using the ROUGE-1 (Lin and Hovy, 2003) text sum-
marization evaluation metric.
In the supervised summarization setting, our ex-
periments demonstrate that with true clusterings of
decision-related DAs, token-level summaries that
employ limited discourse context can approach an
upper bound for summaries extracted directly from
DRDAs2 ? 0.4387 ROUGE-F1 vs. 0.5333. When
using system-generated DRDA clusterings, the DA-
level summaries always dominate token-level meth-
ods in terms of performance.
For the unsupervised summarization setting, we
investigate the use of both unsupervised and su-
pervised methods for the initial DRDA clustering
step. We find that summaries based on unsupervised
clusterings perform comparably to those generated
using supervised techniques (0.2214 ROUGE-F1
using LDA-based topic models vs. 0.2349 using
SVMs). As in the supervised summarization setting,
we observe that including additional discourse con-
text boosts performance only for token-level sum-
maries.
2The upper bound measures the vocabulary overlap of each
gold-standard decision summary with the complete text of all of
its associated DRDAs.
17
2 Related Work
There exists much previous research on automatic
text summarization using corpus-based, knowledge-
based or statistical methods (Mani, 1999; Marcu,
2000). Dialogue summarization methods, how-
ever, generally try to account for the special char-
acteristics of speech. Among early work in
this subarea, Zechner (2002) investigates speech
summarization based on maximal marginal rele-
vance (MMR) and cross-speaker linking of infor-
mation. Popular supervised methods for summa-
rizing speech ? including maximum entropy, con-
ditional random fields (CRFs), and support vector
machines (SVMs) ? are investigated in Buist et al
(2004), Xie et al (2008) and Galley (2006). Tech-
niques for determining semantic similarity are used
for selecting relevant utterances in Gurevych and
Strube (2004).
Studies in Banerjee et al (2005) show that de-
cisions are considered to be one of the most im-
portant outputs of meetings. And in recent years,
there has been much research on detecting decision-
related DAs. Hsueh and Moore (2008), for exam-
ple, propose maximum entropy classification tech-
niques to identify DRDAs in meetings; Ferna?ndez
et al (2008b) develop a model of decision-making
dialogue structure and detect decision DAs based on
it; and Frampton et al (2009) implement a real-time
decision detection system.
Ferna?ndez et al (2008a) and Bui et al (2009),
however, might be the most relevant previous work
to ours. The systems in both papers run an open-
domain semantic parser on meeting transcriptions
to produce multiple short fragments, and then em-
ploy machine learning methods to select the phrases
or words that comprise the decision summary. Al-
though their task is also decision summarization,
their gold-standard summaries consist of manually
annotated words from the meeting while we judge
performance using manually constructed decision
abstracts as the gold standard. The latter are more
readable, but often use a vocabulary different from
that of the associated decision-related utterances in
the meeting.
Our work differs from all of the above in that we
(1) incorporate a clustering step to partition DRDAs
according to the decision each supports; (2) generate
decision summaries at both the DA- and token-level;
and (3) investigate the role of discourse context for
decision summarization.
In the following sections, we investigate methods
for clustering DRDAs (Section 3) and generating
DA-level and token-level decision summaries (Sec-
tion 4). In each case, we evaluate the methods using
the AMI meeting corpus.
3 Clustering Decision-Related Dialogue
Acts
We design a preprocessing step that facilitates deci-
sion summarization by clustering all of the decision-
related dialogue acts according to the decision(s) it
supports. Because it is not clear how many deci-
sions are made in a meeting, we use a hierarchi-
cal agglomerative clustering algorithm (rather than
techniques that require a priori knowledge of the
number of clusters) and choose the proper stopping
conditions. In particular, we employ average-link
methods: at each iteration, we merge the two clus-
ters with the maximum average pairwise similarity
among their DRDAs. In the following subsections,
we introduce unsupervised and supervised methods
for measuring the pairwise DRDA similarity.
3.1 DRDA Similarity: Unsupervised Methods
We consider two unsupervised similarity measures
? one based on the TF-IDF score from the Infor-
mation Retrieval research community, and a second
based on Latent Dirichlet Allocation topic models.
TF-IDF similarity. TF-IDF similarity metrics
have worked well as a measure of document simi-
larity. As a result, we employ it as one metric for
measuring the similarity of two DRDAs. Suppose
there are L distinct word types in the corpus. We
treat each decision-related dialgue act DAi as a
document, and represent it as an L-dimensional
feature vector
???
FVi = (xi1, xi2, ..., xiL), where xik
is word wk?s tf ? idf score for DAi. Then the
(average-link) similarity of cluster Cm and cluster
Cn, Sim TFIDF (Cm, Cn), is defined as :
1
| Cm | ? | Cn |
?
DAi?Cm
DAj?Cn
???
FVi ?
???
FVj
?
???
FVi ??
???
FVj ?
LDA topic models. In recent years, topic models
have become a popular technique for discovering the
latent structure of ?topics? or ?concepts? in a cor-
pus. Here we use the Latent Dirichlet Allocation
(LDA) topic models of Blei et al (2003) ? unsuper-
18
Features
number of overlapping words
proportion of the number of overlapping words to the le-
ngth of shorter DA
TF-IDF similarity
whether the DAs are in an adjacency pair (see 4.3)
time difference of pairwise DAs
relative dialogue position of pairwise DAs
whether the two DAs have the same DA type
number of overlapping words in the contexts (see 4.2)
Table 2: Features for Pairwise Supervised Clustering
vised probabilistic generative models that estimate
the properties of multinomial observations. In our
setting, LDA-based topic models provide a soft clus-
tering of the DRDAs according to the topics they
discuss.3 To determine the similarity of two DR-
DAs, we effectively measure the similarity of their
term-based topic distributions.
To train an LDA-based topic model for our task4,
we treat each DRDA as an individual document.
After training, each DRDA, DAi, is assigned a
topic distribution
??
?i according to the learned model.
Thus, we can define the similarity of cluster Cm and
cluster Cn, Sim LDA(Cm, Cn), as :
1
| Cm | ? | Cn |
?
DAi?Cm
DAj?Cn
??
?i ?
??
?j
3.2 DRDA Similarity: Supervised Techniques
In addition to unsupervised methods for clustering
DRDAs, we also explore an approach based on Pair-
wise Supervised Learning: we develop a classifier
that determines whether or not a pair of DRDAs sup-
ports the same decision. So each training and test
example is a feature vector that is a function of two
DRDAs: for DAi and DAj , the feature vector is
???
FVij = f(DAi, DAj) = {fv1ij , fv
2
ij , ..., fv
k
ij}. Ta-
ble 2 gives a full list of features that are used. Be-
cause the annotations for the time information and
dialogue type of DAs are available from the cor-
pus, we employ features including time difference
of pairwise DAs, relative position5 and whether they
3We cannot easily associate each topic with a decision be-
cause the number of decisions is not known a priori.
4Parameter estimation and inference done by GibbsLDA++.
5Here is the definition for the relative position of pairwise
DAs. Suppose there are N DAs in one meeting ordered by time,
have the same DA type.
We employ Support Vector Machines (SVMs)
and Maximum Entropy (MaxEnt) as our learning
methods, because SVMs are shown to be effective
in text categorization (Joachims, 1998) and Max-
Ent has been applied in many natural language
processing tasks (Berger et al, 1996). Given an
???
FVij , for SVMs, we utilize the decision value of
wT ?
???
FVij + b as the similarity, where w is the
weight vector and b is the bias. For MaxEnt, we
make use of the probability of P (SameDecision |
???
FVij) as the similarity value.
3.3 Experiments
Corpus. We use the AMI meeting Corpus (Car-
letta et al, 2005), a freely available corpus of multi-
party meetings that contains a wide range of anno-
tations. The 129 scenario-driven meetings involve
four participants playing different roles on a de-
sign team. A short (usually one-sentence) abstract
is included that describes each decision, action, or
problem discussed in the meeting; and each DA is
linked to the abstracts it supports. We use the manu-
ally constructed decision abstracts as gold-standard
summaries and assume that all decision-related DAs
have been identified (but not linked to the decision(s)
it supports).
Baselines. Two clustering baselines are utilized
for comparison. One baseline places all decision-
related DAs for the meeting into a single partition
(ALLINONEGROUP). The second uses the text seg-
mentation software of Choi (2000) to partition the
decision-related DAs (ordered according to time)
into several topic-based groups (CHOISEGMENT).
Experimental Setup and Evaluation. Results for
pairwise supervised clustering were obtained using
3-fold cross-validation. In the current work, stop-
ping conditions for hierarchical agglomerative clus-
tering are selected manually: For the TF-IDF and
topic model approaches, we stop when the similar-
ity measure reaches 0.035 and 0.015, respectively;
For the SVM and MaxEnt versions, we use 0 and
0.45, respectively. We use the Mallet implementa-
tion for MaxEnt and the SVMlight implementation
of SVMs.
Our evaluation metrics include b3 (also called B-
cubed) (Bagga and Baldwin, 1998), which is a com-
DAi is the ith DA and DAj is positioned at j. So the relative
position of DAi and DAj is
|i?j|
N .
19
B-cubed Pairwise VOI
PRECISION RECALL F1 PRECISION RECALL F1
Baselines
AllInOneGroup 0.2854 1.0000 0.4441 0.1823 1.0000 0.3083 2.2279
ChoiSegment 0.4235 0.9657 0.5888 0.2390 0.8493 0.3730 1.8061
Unsupervised Methods
TFIDF 0.6840 0.6686 0.6762 0.3281 0.3004 0.3137 1.6604
LDA topic models 0.8265 0.6432 0.7235 0.4588 0.2980 0.3613 1.4203
Pairwise Supervised Methods
SVM 0.7593 0.7466 0.7529 0.5474 0.4821 0.5127 1.2239
MaxEnt 0.6999 0.7948 0.7443 0.4858 0.5704 0.5247 1.2726
Table 3: Results for Clustering Decision-Related DAs According to the Decision Each Supports
mon measure employed in noun phrase coreference
resolution research; a pairwise scorer that measures
correctness for every pair of DRDAs; and a variation
of information (VOI) scorer (Meila?, 2007), which
measures the difference between the distributions of
the true clustering and system generated clustering.
As space is limited, we refer the readers to the orig-
inal papers for more details. For b3 scorer and pair-
wise scorer, higher results represent better perfor-
mance; for VOI, lower is better.6
Results. The results in Table 3 show first that all
of the proposed clustering methods outperform the
baselines. Among the unsupervised methods, the
LDA topic modeling is preferred to TFIDF. For the
supervised methods, SVMs and MaxEnt produce
comparable results.
4 Decision Summarization
In this section, we turn to decision summarization ?
extracting a short description of each decision based
on the decision-related DAs in each cluster. We in-
vestigate options for constructing an extract-based
summary that consists of a single DRDA and an
abstract-based summary comprised of keywords that
describe the decision. For both types of summary,
we employ standard techniques from text summa-
rization, but also explore the use of dialogue-specific
features and the use of discourse context.
4.1 DA-Level Summarization Based on Unsu-
pervised Methods
We make use of two unsupervised methods to sum-
marize the DRDAs in each ?decision cluster?. The
first method simply returns the longest DRDA in the
6The MUC scorer is popular in coreference evaluation, but it
is flawed in measuring the singleton clusters which is prevalent
in the AMI corpus. So we do not use it in this work.
Lexical Features
unigram/bigram
length of the DA
contain digits?
has overlapping words with next DA?
next DA is a positive feedback?
Structural Features
relative position in the meeting?(beginning, ending, or else)
in an AP?
if in an AP, AP type
if in an AP, the other part is decision-related?
if in an AP, is the source part or target part?
if in an AP and is source part, target is positive feedback?
if in an AP and is target part, source is a question?
Discourse Features
relative position to ?WRAP UP? or ?RECAP?
Other Features
DA type
speaker role
topic
Table 4: Features Used in DA-Level Summarization
cluster as the summary (LONGEST DA). The sec-
ond approach returns the decision cluster prototype,
i.e., the DRDA with the largest TF-IDF similar-
ity with the cluster centroid (PROTOTYPE DA). Al-
though important decision-related information may
be spread over multiple DRDAs, both unsupervised
methods allow us to determine summary quality
when summaries are restricted to a single utterance.
4.2 DA-Level and Token-Level Summarization
Using Supervised Learning
Because the AMI corpus contains a decision abstract
for each decision made in the meeting, we can use
this supervisory information to train classifiers that
can identify informative DRDAs (for DA-level sum-
maries) or informative tokens (for token-level sum-
maries).
20
Lexical Features
current token/current token and next token
length of the DA
is digit?
appearing in next DA?
next DA is a positive feedback?
Structural Features
see Table 3
Grammatical Features
part-of-speech
phrase type (VP/NP/PP)
dependency relations
Other Features
speaker role
topic
Table 5: Features Used in Token-Level Summarization
PREC REC F1
True Clusterings
Longest DA 0.3655 0.4077 0.3545
Prototype DA 0.3626 0.4140 0.3539
System Clusterings
using LDA
Longest DA 0.3623 0.1892 0.2214
Prototype DA 0.3669 0.1887 0.2212
using SVMs
Longest DA 0.3719 0.1261 0.1682
Prototype DA 0.3816 0.1264 0.1700
No Clustering
Longest DA 0.1039 0.1382 0.1080
Prototype DA 0.1350 0.1209 0.1138
Upper Bound 0.8970 0.4089 0.5333
Table 6: Results for ROUGE-1: Decision Summary Gen-
eration Using Unsupervised Methods
Dialogue Act-based Summarization. Previous
research (e.g., Murray et al (2005), Galley
(2006), Gurevych and Strube (2004)) has shown
that DRDA-level extractive summarization can be
effective when viewed as a binary classification task.
To implement this approach, we assume that the
DRDA to be extracted for the summary is the one
with the largest vocabulary overlap with the cluster?s
gold-standard decision abstract. This DA-level sum-
marization method has an advantage that the sum-
mary maintains good readability without a natural
language generation component.
Token-based Summarization. As shown in Table
1, some decision-related DAs contain many useless
words when compared with the gold-standard ab-
stracts. As a result, we propose a method for token-
level decision summarization that focuses on iden-
tifying critical keywords from the cluster?s DRDAs.
We follow the method of Ferna?ndez et al (2008a),
but use a larger set of features and different learning
methods.
Adding Discourse Context. For each of the su-
pervised DA- and token-based summarization meth-
ods, we also investigate the role of the discourse
context. Specifically, we augment the DRDA clus-
terings with additional (not decision-related) DAs
from the meeting dialogue: for each decision par-
tition, we include the DA with the highest TF-IDF
similarity with the centroid of the partition. We
will investigate the possible effects of this additional
context on summary quality.
In the next subsection, we describe the features
used for supervised learning of DA- and token-based
decision summaries.
4.3 Dialogue Cues for Decision Summarization
Different from text, dialogues have some notable
features that we expect to be useful for finding in-
formative, decision-related utterances. This section
describes some of the dialogue-based features em-
ployed in our classifiers. The full lists of features
are shown in Table 4 and Table 5.
Structural Information: Adjacency Pairs. An
Adjacency Pair (AP) is an important conversational
analysis concept; APs are considered the fundamen-
tal unit of conversational organization (Schegloff
and Sacks, 1973). In the AMI corpus, an AP pair
consists of a source utterance and a target utterance,
produced by different speakers. The source pre-
cedes the target but they are not necessarily adja-
cent. We include features to indicate whether or not
two DAs are APs indicating QUESTION+ANSWER
or POSITIVE FEEDBACK. For these features, we use
the gold-standard AP annotations. We also include
one feature that checks membership in a small set
of words to decide whether a DA contains positive
feedback (e.g., ?yeah?, ?yes?).
Discourse Information: Review and Closing In-
dicator. Another pragmatic cue for dialogue dis-
cussion is terms like ?wrap up? or ?recap?, indicat-
ing that speakers will review the key meeting con-
tent. We include the distance between these indica-
tors and DAs as a feature.
Grammatical Information: Dependency Relation
Between Words. For token-level summarization,
we make use of the grammatical relationships in
the DAs. As in Bui et al (2009) and Ferna?ndez
21
CRFs SVMs
PRECISION RECALL F1 PRECISION RECALL F1
True Clusterings
DA 0.3922 0.4449 0.3789 0.3661 0.4695 0.3727
Token 0.5055 0.2453 0.3033 0.4953 0.3788 0.3963
DA+Context 0.3753 0.4372 0.3678 0.3595 0.4449 0.3640
Token+Context 0.5682 0.2825 0.3454 0.6213 0.3868 0.4387
System Clusterings
using LDA
DA 0.3087 0.1663 0.1935 0.3391 0.2097 0.2349
Token 0.3379 0.0911 0.1307 0.3760 0.1427 0.1843
DA+Context 0.3305 0.1748 0.2041 0.2903 0.1869 0.2068
Token+Context 0.4557 0.1198 0.1727 0.4882 0.1486 0.2056
using SVMs
DA 0.3508 0.1884 0.2197 0.3592 0.2026 0.2348
Token 0.2807 0.04968 0.0777 0.3607 0.0885 0.1246
DA+Context 0.3583 0.1891 0.2221 0.3418 0.1892 0.2213
Token+Context 0.4891 0.0822 0.1288 0.4873 0.0914 0.1393
No Clustering
DA 0.08673 0.1957 0.0993 0.0707 0.1979 0.0916
Token 0.1906 0.0625 0.0868 0.1890 0.3068 0.2057
Table 7: Results for ROUGE-1: Summary Generation Using Supervised Learning
et al (2008a), we design features that encode (a)
basic predicate-argument structures involving major
phrase types (S, VP, NP, and PP) and (b) additional
typed dependencies from Marneffe et al (2006). We
use the Stanford Parser.
5 Experiments
Experiments based on supervised learning are per-
formed using 3-fold cross-validation. We train two
different types of classifiers for identifying infor-
mative DAs or tokens: Conditional Random Fields
(CRFs) (via Mallet) and Support Vector Machines
(SVMs) (via SVMlight).
We remove function words from DAs before us-
ing them as the input of our systems. The AMI deci-
sion abstracts are the gold-standard summaries. We
use the ROUGE (Lin and Hovy, 2003) evaluation
measure. ROUGE is a recall-based method that can
identify systems producing succinct and descriptive
summaries.7
Results and Analysis. Results for the unsuper-
vised and supervised summarization methods are
shown in Tables 6 and 7, respectively. In the tables,
TRUE CLUSTERINGS means that we apply our meth-
ods on the gold-standard DRDA clusterings. SYS-
TEM CLUSTERINGS use clusterings obtained from
the methods introduced in Section 4; we show re-
7We use the stemming option of the ROUGE software at
http://berouge.com/.
sults only using the best unsupervised (USING LDA)
and supervised (USING SVMS) DRDA clustering
techniques.
Both Table 6 and 7 show that some attempt to
cluster DRDAs improves the summarization results
vs. NO CLUSTERING. In Table 6, there is no signif-
icant difference between the results obtained from
the LONGEST DA and PROTOTYPE DA for any ex-
periment setting. This is because the longest DA is
often selected as the prototype. An UPPER BOUND
result is listed for comparison: for each decision
cluster, this system selects all words from the DR-
DAs that are part of the decision abstract (discarding
duplicates).
Table 7 presents the results for supervised sum-
marization. Rows starting with DA or TOKEN indi-
cate results at the DA- or token-level. The +CON-
TEXT rows show results when discourse context is
included.8 We see that: (1) SVMs have a superior or
comparable summarization performance vs. CRFs
on every task. (2) Token-level summaries perform
better than DA-level summaries only using TRUE
CLUSTERINGS and the SVM-based summarizer. (3)
Discourse context generally improves token-level
summaries but not DA-level summaries.9 (4) DRDA
8In our experiments, we choose the top 20 relevant DAs as
context.
9We do not extract words from the discourse context and
experiments where we tried this were unsuccessful.
22
clusterings produced by (unsupervised) LDA lead to
summaries that are quite comparable in quality to
those generated from DRDA clusterings produced
by SVMs (supervised). From Table 6, we see that
F1 is 0.2214 when choosing longest DAs from LDA-
generated clusterings, which is comparable with the
F1s of 0.1935 and 0.2349, attained when employing
CRF and SVMs on the same clusterings.
The results in Table 7 are achieved by compar-
ing abstracts having function words with system-
generated summaries without function words. To re-
duce the vocabulary difference as much as possible,
we also ran experiments that remove function words
from the gold-standard abstracts, but no significant
difference is observed.10
Finally, we considered comparing our systems to
the earlier similar work of (Ferna?ndez et al, 2008a)
and (Bui et al, 2009), but found that it would
be quite difficult because they employ a different
notion from DRDAs which is Decision Dialogue
Acts(DDAs). In addition, they manually annotate
words from their DDAs as the gold-standard sum-
mary, guaranteeing that their decision summaries
employ the same vocabulary as the DDAs. We in-
stead use the actual decision abstracts from the AMI
corpus.
5.1 Sample Decision Summaries
Here we show sample summaries produced using
our methods (Table 8). We pick one of the clus-
terings generated by LDA consisting of four DAs
which support two decisions and take SVMs as
the supervised summarization method. We remove
function words and special markers like ?[disf-
marker]? from the DAs.
The outputs indicate that either the longest DA or
prototype DA contains part of the decisions in this
?mixed? cluster. Adding discourse context refines
the summaries at both the DA- and token-levels.
6 Conclusion
In this work, we explore methods for producing de-
cision summaries from spoken meetings at both the
DA-level and the token-level. We show that clus-
10Given abstracts without function words, and using the clus-
terings generated by LDA and employ CRF on DA- and token-
level summarization, we get F1s of 0.1954 and 0.1329, which
is marginally better than the corresponding 0.1935 and 0.1307
in Table 7. Similarly, if SVMs are employed in the same cases,
we get F1s of 0.2367 and 0.1861 instead of 0.2349 and 0.1843.
All of the other results obtain negligible minor increases in F1.
DA (1): um of course , as [disfmarker] we , we?ve already
talked about the personal face plates in this meeting , (a)
DA (2): and I?d like to stick to that . (a)
DA (3): Well , I guess plastic and coated in rubber . (b)
DA (4): So the actual remote would be hard plastic and
the casings rubber . (b)
Decision (a): Will use personal face plates.
Decision (b): Case will be plastic and coated in rubber.
Longest DA:
talked about personal face plates in meeting
Prototype DA:
actual remote hard plastic casings rubber
DA-level:
talked about personal face plates in meeting, like to
stick to, guess plastic and coated in rubber,
actual remote hard plastic casings rubber
Token-level:
actual remote plastic casings rubber
DA-level and Discourse Context:
talked about personal face plates in meeting, guess plastic
and coated in rubber, actual remote hard plastic casings
rubber
Token-level and Discourse Context:
remote plastic rubber
Table 8: Sample system outputs by different methods are
in the third cell (methods? names are in bold). First cell
contains four DAs. (a) or (b) refers to the decision that
DA supports, which is listed in the second cell.
tering DRDAs before identifying informative con-
tent to extract can improve summarization quality.
We also find that unsupervised clustering of DR-
DAs (using LDA-based topic models) can produce
summaries of comparable quality to those gener-
ated from supervised DRDA clustering. Token-level
summarization methods can be boosted by adding
discourse context and outperform DA-level summa-
rization when true DRDA clusterings are available;
otherwise, DA-level summarization methods offer
better performance.
Acknowledgments. This work was supported in part
by National Science Foundation Grants IIS-0535099 and
IIS-0968450, and by a gift from Google.
References
Amit Bagga and Breck Baldwin. 1998. Algorithms for
scoring coreference chains. In In The First Interna-
tional Conference on Language Resources and Eval-
uation Workshop on Linguistics Coreference, pages
563?566.
Satanjeev Banerjee, Carolyn Penstein Rose?, and Alexan-
der I. Rudnicky. 2005. The necessity of a meet-
23
ing recording and playback system, and the benefit of
topic-level annotations to meeting browsing. In IN-
TERACT, pages 643?656.
Adam L. Berger, Vincent J. Della Pietra, and Stephen
A. Della Pietra. 1996. A maximum entropy approach
to natural language processing. Comput. Linguist.,
22:39?71, March.
David M. Blei, Andrew Y. Ng, and Michael I. Jordan.
2003. Latent dirichlet alocation. J. Mach. Learn.
Res., 3:993?1022, March.
Trung H. Bui, Matthew Frampton, John Dowding, and
Stanley Peters. 2009. Extracting decisions from
multi-party dialogue using directed graphical models
and semantic similarity. In Proceedings of the SIG-
DIAL 2009 Conference, pages 235?243.
Anne Hendrik Buist, Wessel Kraaij, and Stephan Raaij-
makers. 2004. Automatic summarization of meeting
data: A feasibility study. In in Proc. Meeting of Com-
putational Linguistics in the Netherlands (CLIN.
Jean Carletta, Simone Ashby, Sebastien Bourban,
Mike Flynn, Thomas Hain, Jaroslav Kadlec, Vasilis
Karaiskos, Wessel Kraaij, Melissa Kronenthal, Guil-
laume Lathoud, Mike Lincoln, Agnes Lisowska, and
Mccowan Wilfried Post Dennis Reidsma. 2005. The
ami meeting corpus: A pre-announcement. In In Proc.
MLMI, pages 28?39.
Freddy Y. Y. Choi. 2000. Advances in domain inde-
pendent linear text segmentation. In Proceedings of
the 1st North American chapter of the Association for
Computational Linguistics conference, pages 26?33.
Raquel Ferna?ndez, Matthew Frampton, John Dowding,
Anish Adukuzhiyil, Patrick Ehlen, and Stanley Peters.
2008a. Identifying relevant phrases to summarize de-
cisions in spoken meetings. INTERSPEECH-2008,
pages 78?81.
Raquel Ferna?ndez, Matthew Frampton, Patrick Ehlen,
Matthew Purver, and Stanley Peters. 2008b. Mod-
elling and detecting decisions in multi-party dialogue.
In Proceedings of the 9th SIGdial Workshop on Dis-
course and Dialogue, pages 156?163.
Matthew Frampton, Jia Huang, Trung Huu Bui, and Stan-
ley Peters. 2009. Real-time decision detection in
multi-party dialogue. In Proceedings of the 2009 Con-
ference on Empirical Methods in Natural Language
Processing: Volume 3 - Volume 3, pages 1133?1141.
Michel Galley. 2006. A skip-chain conditional random
field for ranking meeting utterances by importance.
In Proceedings of the 2006 Conference on Empirical
Methods in Natural Language Processing, pages 364?
372.
Iryna Gurevych and Michael Strube. 2004. Semantic
similarity applied to spoken dialogue summarization.
In Proceedings of the 20th international conference on
Computational Linguistics.
Pei-Yun Hsueh and Johanna D. Moore. 2008. Automatic
decision detection in meeting speech. In Proceedings
of the 4th international conference on Machine learn-
ing for multimodal interaction, pages 168?179.
Thorsten Joachims. 1998. Text categorization with Sup-
port Vector Machines: Learning with many relevant
features. In Claire Ne?dellec and Ce?line Rouveirol,
editors, Machine Learning: ECML-98, volume 1398,
chapter 19, pages 137?142. Berlin/Heidelberg.
Chin-Yew Lin and Eduard Hovy. 2003. Automatic evalu-
ation of summaries using n-gram co-occurrence statis-
tics. In Proceedings of the 2003 Conference of the
North American Chapter of the Association for Com-
putational Linguistics on Human Language Technol-
ogy - Volume 1, pages 71?78.
Inderjeet Mani. 1999. Advances in Automatic Text Sum-
marization. MIT Press, Cambridge, MA, USA.
Daniel Marcu. 2000. The Theory and Practice of Dis-
course Parsing and Summarization. MIT Press, Cam-
bridge, MA, USA.
M. Marneffe, B. Maccartney, and C. Manning. 2006.
Generating Typed Dependency Parses from Phrase
Structure Parses. In Proceedings of LREC-06, pages
449?454.
Marina Meila?. 2007. Comparing clusterings?an infor-
mation based distance. J. Multivar. Anal., 98:873?895,
May.
Gabriel Murray, Steve Renals, and Jean Carletta. 2005.
Extractive summarization of meeting recordings. In
in Proceedings of the 9th European Conference on
Speech Communication and Technology, pages 593?
596.
Gerard Salton, Amit Singhal, Mandar Mitra, and Chris
Buckley. 1997. Automatic text structuring and
summarization. Inf. Process. Manage., 33:193?207,
March.
E. A. Schegloff and H. Sacks. 1973. Opening up clos-
ings. Semiotica, 8(4):289?327.
Shasha Xie, Yang Liu, and Hui Lin. 2008. Evaluating
the effectiveness of features and sampling in extractive
meeting summarization. In in Proc. of IEEE Spoken
Language Technology (SLT.
Klaus Zechner. 2002. Automatic summarization of
open-domain multiparty dialogues in diverse genres.
Comput. Linguist., 28:447?485, December.
24
Proceedings of the 15th Conference on Computational Natural Language Learning: Shared Task, pages 122?126,
Portland, Oregon, 23-24 June 2011. c?2011 Association for Computational Linguistics
Reconciling OntoNotes: Unrestricted Coreference Resolution in
OntoNotes with Reconcile
Veselin Stoyanov
CLSP
Johns Hopkins University
Baltimore, MD
Uday Babbar and Pracheer Gupta and Claire Cardie
Department of Computer Science
Cornell University
Ithaca, NY
Abstract
This paper describes our entry to the 2011 CoNLL
closed task (Pradhan et al, 2011) on modeling un-
restricted coreference in OntoNotes. Our system is
based on the Reconcile coreference resolution re-
search platform. Reconcile is a general software in-
frastructure for the development of learning-based
noun phrase (NP) coreference resolution systems.
Our entry for the CoNLL closed task is a configura-
tion of Reconcile intended to do well on OntoNotes
data. This paper describes our configuration of Rec-
oncile as well as the changes that we had to imple-
ment to integrate with the OntoNotes task definition
and data formats. We also present and discuss the
performance of our system under different testing
conditions on a withheld validation set.
1 Introduction
Noun phrase (NP) coreference resolution is one of
the fundamental tasks of the field of Natural Lan-
guage Processing (NLP). Recently, the creation of
the OntoNotes corpus (Pradhan et al, 2007) has
provided researchers with a large standard data
collection with which to create and empirically
compare coreference resolution systems.
Reconcile (Stoyanov et al, 2010b) is a general
coreference resolution research platform that aims
to abstract the architecture of different learning-
based coreference systems and to provide infras-
tructure for their quick implementation. Recon-
cile is distributed with several state-of-the art NLP
components and a set of optimized feature imple-
mentations. We decided to adapt Reconcile for
the OntoNotes corpus and enter it in the 2011
CoNLL shared task with three goals in mind: (i) to
compare the architecture and components of Rec-
oncile with other state-of-the-art coreference sys-
tems, (ii) to implement and provide the capabil-
ity of running Reconcile on the OntoNotes corpus,
and, (iii) to provide a baseline for future algorithm
implementations in Reconcile that evaluate on the
OntoNotes corpus.
Although Reconcile can be easily adapted to
new corpora, doing so requires introducing new
components. More precisely, the system has to
be modified to be consistent with the specific def-
inition of the coreference task embodied in the
OntoNotes annotation instructions. Additionally,
different corpora use different data formats, so the
system needs to implement capabilities for dealing
with these new formats. Finally, Reconcile can be
configured with different features and components
to create an instantiation that models well the par-
ticular data.
In this paper we describe, ReconcileCoNLL,
our entry to the 2011 CoNLL shared task based on
the Reconcile research platform. We begin by de-
scribing the general Reconcile architecture (Sec-
tion 2), then describe the changes that we incor-
porated in order to enable Reconcile to work on
OntoNotes data (Sections 3 and 4). Finally, we
describe our experimental set up and results from
running ReconcileCoNLL under different condi-
tions (Section 5).
2 Overview of Reconcile
In this section we give a high-level overview of the
Reconcile platform. We refer the reader for more
details to Stoyanov et al (2010a) and Stoyanov et
al. (2010b). Results from running a Reconcile-
based coreference resolution system on different
corpora can be found in Stoyanov et al (2009).
Reconcile was developed to be a coreference
resolution research platform that allows for quick
implementation of coreference resolution systems.
The platform abstracts the major processing steps
(components) of current state-of-the-art learning-
based coreference resolution systems. A descrip-
tion of the steps and the available components can
be found in the referenced papers.
3 The ReconcileCoNLL System
To participate in the 2011 CoNLL shared task, we
configured Reconcile to conform to the OntoNotes
general coreference resolution task. We will use
the name ReconcileCoNLL, to refer to this par-
ticular instantiation of the general Reconcile plat-
form. The remainder of this section describe the
changes required to enable ReconcileCoNLL to
run (accurately) on OntoNotes data.
122
ReconcileCoNLL employs the same basic
pipelined architecture as Reconcile. We describe
the specific components used in each step.
1. Preprocessing. Documents in the OntoNotes
corpus are manually (or semi-automatically) an-
notated with many types of linguistic information.
This information includes tokens, part-of-speech
tags, and named entity information as well as a
constituent syntactic parse of the text. For the pur-
pose of participating in the shared task, we rely on
these manual annotations, when available. Thus,
we do not run most of the standard Reconcile pre-
processing components. One type of information
not provided in the OntoNotes corpus is a depen-
dency parse. Several of Reconcile?s features rely
on a dependency parse of the text. Thus, we ran
the Stanford dependency parser (Klein and Man-
ning, 2003), which performs a constituent parse
and uses rules to convert to a dependency format.1
Two additional changes to the preprocessing
step were necessary for running on the OntoNotes
data. The first is the implementation of compo-
nents that can convert data from the OntoNotes
format to the Reconcile internal format. The sec-
ond is adaptation of the Coreference Element (CE)
extractor to conform to the OntoNotes definition
of what can constitute a CE. Our implementations
for these two tasks are briefly described in Sec-
tions 4.1 and 4.2, respectively.
2. Feature generation. ReconcileCoNLL was
configured with 61 features that have proven suc-
cessful for coreference resolution on other data
sets. Due to the lack of time we performed
no feature engineering or selection specific to
OntoNotes. We used a new component for gener-
ating the pairwise CEs that comprise training and
test instances, which we dub SMARTPG (for smart
pair generator). This is described in Section 4.3.
3. Classification. We train a linear classifier us-
ing the averaged perceptron algorithm (Freund and
Schapire, 1999). We use a subset of 750 randomly
selected documents for training, since training on
the entire set required too much memory.2 As a
result, we had ample validation data for tuning
thresholds, etc.
1A better approach would be to use the rules to create the
dependency parse from the manual constituent parse. We de-
cided against this approach due to implementation overhead.
2It is easy to address the memory issue in the on-line per-
ceptron setting, but in the interest of time we chose to reduce
the size of the training data. Training on the set of 750 docu-
ments is done efficiently in memory by allocating 4GB to the
Java virtual machine.
4. Clustering. We use Reconcile?s single-link
clustering algorithm. In other words, we compute
the transitive closure of the positive pairwise pre-
dictions. Note that what constitutes a positive pre-
diction depends on a threshold set for the classifier
from the previous step. This clustering threshold
is optimized using validation data. More details
about the influence of the validation process can
be found in Section 5.
5. Scoring. The 2011 CoNLL shared task pro-
vides a scorer that computes a set of commonly
used coreference resolution evaluation metrics.
We report results using this scorer in Section 5.
However, we used the Reconcile-internal versions
of scorers to optimize the threshold. This was
done for pragmatic reasons ? time pressure pre-
vented us from incorporating the CoNLL scorer in
the system. We also report the Reconcile-internal
scores in the experiment section.
This concludes the high-level description of
the ReconcileCoNLL system. Next, we describe
in more detail the main changes implemented to
adapt to the OntoNotes data.
4 Adapting to OntoNotes
The first two subsection below describe the two
main tasks that need to be addressed when running
Reconcile on a new data set: annotation conver-
sion and CE extraction. The third subsection de-
scribes the new Smart CE Pairwise instance gen-
erator ? a general component that can be used for
any coreference data set.
4.1 Annotation Conversion
There are fundamental differences between the an-
notation format used by OntoNotes and that used
internally by Reconcile. While OntoNotes relies
on token-based representations, Reconcile uses a
stand-off bytespan annotation. A significant part
of the development of ReconcileCoNLL was de-
voted to conversion of the OntoNotes manual to-
ken, parse, named-entity and coreference annota-
tions. In general, we prefer the stand-off bytespan
format because it allows the reference text of the
document to remain unchanged while annotation
layers are added as needed.
4.2 Coreference Element Extraction
The definition of what can constitute an element
participating in the coreference relation (i.e., a
Coreference Element or CE) depends on the par-
ticular dataset. Optimizing the CE extraction com-
123
Optimized Thres- B- CEAF MUC
Metric hold Cubed
BCubed 0.4470 0.7112 0.1622 0.6094
CEAF 0.4542 0.7054 0.1650 0.6141
MUC 0.4578 0.7031 0.1638 0.6148
Table 1: Reconcile-internal scores for different
thresholds. The table lists the best threshold for
the validation data and results using that threshold.
Pair Gen. BCubed CEAFe MUC
SMARTPG 0.6993 0.1634 0.6126
All Pairs 0.6990 0.1603 0.6095
Table 3: Influence of different pair generators.
ponent for the particular task definition can result
in dramatic improvements in performance. An ac-
curate implementation limits the number of ele-
ments that the coreference system needs to con-
sider while keeping the recall high.
The CE extractor that we implemented for
OntoNotes extends the existing Reconcile ACE05
CE extractor (ACE05, 2005) via the following
modifications:
Named Entities: We exclude named entities of
type CARDINAL NUMBER, MONEY and NORP,
the latter of which captures nationality, religion,
political and other entities.
Possessives: In the OntoNotes corpus, posses-
sives are included as coreference elements, while
in ACE they are not.
ReconcileCoNLL ignores the fact that verbs can
also be CEs for the OntoNotes coreference task as
this change would have constituted a significant
implementation effort.
Overall, our CE extractor achieves recall of over
96%, extracting roughly twice the number of CEs
in the answer key (precision is about 50%). High
recall is desirable for the CE extractor at the cost of
precision since the job of the coreference system is
to further narrow down the set of anaphoric CEs.
4.3 Smart Pair Generator
Like most current coreference resolution systems,
at the heart of Reconcile lies a pairwise classifier.
The job of the classifier is to decide whether or not
two CEs are coreferent or not. We use the term
pair generation to refer to the process of creating
the CE pairs that the classifier considers. The most
straightforward way of generating pairs is by enu-
merating all possible unique combinations. This
approach has two undesirable properties ? it re-
quires time in the order of O(n2) for a given doc-
ument (where n is the number of CEs in the docu-
ment) and it produces highly imbalanced data sets
with the number of positive instances (i.e., coref-
erent CEs) being a small fraction of the number of
negative instances. The latter issue has been ad-
dressed by a technique named instance generation
(Soon et al, 2001): during training, each CE is
matched with the first preceding CE with which it
corefers and all other CEs that reside in between
the two. During testing, a CE is compared to all
preceding CEs until a coreferent CE is found or
the beginning of the document is reached. This
technique reduces class imbalance, but it has the
same worst-case runtime complexity of O(n2).
We employ a new type of pair generation that
aims to address both the class imbalance and
improves the worst-case runtime. We will use
SMARTPG to refer to this component. Our pair
generator relies on linguistic intuitions and is
based on the type of each CE. For a given CE,
we use a rule-based algorithm to guess its type.
Based on the type, we restrict the scope of possi-
ble antecedents to which the CE can refer in the
following way:
Proper Name (Named Entity): A proper name
is compared against all proper names in the 20 pre-
ceding sentences. In addition, it is compared to all
other CEs in the two preceding sentences.
Definite noun phrase: Compared to all CEs in
the six preceding sentences.
Common noun phrase: Compared to all CEs
in the two preceding sentences.
Pronoun: Compared to all CEs in the two pre-
ceding sentences unless it is a first person pronoun.
First person pronouns are additionally compared
to first person pronouns in the preceding 20 sen-
tences.
During development, we used SMARTPG
on coreference resolution corpora other than
OntoNotes and determined that the pair generator
tends to lead to more accurate results. It also has
runtime linear in the number of CEs in a docu-
ment, which leads to a sizable reduction in run-
ning time for large documents. Training files gen-
erated by SMARTPG also tend to be more bal-
anced. Finally, by omitting pairs that are un-
likely to be coreferent, SMARTPG produces much
smaller training sets. This leads to faster learning
and allows us to train on more documents.
124
Optimized Metric Threshold BCubed CEAFe MUC BLANC CEAFm Combined
BCubed 0.4470 0.6651 0.4134 0.6156 0.6581 0.5249 0.5647
CEAF 0.4542 0.6886 0.4336 0.6206 0.7012 0.5512 0.5809
MUC 0.4578 0.6938 0.4353 0.6215 0.7108 0.5552 0.5835
Table 2: CoNLL scores for different thresholds on validation data.
CoNLL Official Test Scores BCubed CEAFe MUC BLANC CEAFm Combined
Closed Task 0.6144 0.3588 0.5843 0.6088 0.4608 0.5192
Gold Mentions 0.6248 0.3664 0.6154 0.6296 0.4808 0.5355
Table 4: Official CoNLL 2011 test scores. Combined score is the average of MUC, BCubed and CEAFe.
5 Experiments
In this section we present and discuss the results
for ReconcileCoNLLwhen trained and evaluated
on OntoNotes data. For all experiments, we train
on a set of 750 randomly selected documents from
the OntoNotes corpus. We use another 674 ran-
domly selected documents for validation. We re-
port scores using the scorers implemented inter-
nally in Reconcile as well as the scorers supplied
by the CoNLL shared task.
In the rest of the section, we describe our results
when controlling two aspects of the system ? the
threshold of the pairwise CE classifier, which is
tuned on training data, and the method used for
pair generation. We conclude by presenting the
official results for the CoNLL shared task.
Influence of Classifier Threshold As previ-
ously mentioned, the threshold above which the
decision of the classifier is considered positive
provides us with a knob that controls the preci-
sion/recall trade-off. Reconcile includes a mod-
ule that can automatically search for a threshold
value that optimizes a particular evaluation met-
ric. Results using three Reconcile-internal scor-
ers (BCubed, CEAF, MUC) are shown in Table
1. First, we see that the threshold that optimizes
performance on the validation data also exhibits
the best results on the test data. The same does
not hold when using the CoNLL scorer for test-
ing, however: as Table 2 shows, the best results
for almost all of the CoNLL scores are achieved at
the threshold that optimizes the Reconcile-internal
MUC score. Note that we did not optimize thresh-
olds for the external scorer in the name of sav-
ing implementation effort. Unfortunately, the re-
sults that we submitted for the official evaluations
were for the suboptimal threshold that optimizes
Reconcile-internal BCubed score.
Influence of Pair Generation Strategies Next,
we evaluate the performance of SMARTPG pair
generators. We run the same system set-up as
above substituting the pair generation module. Re-
sults (using the internal scorer), displayed in Table
3, show our SMARTPG performs identically to the
generator producing all pairs, while it runs in time
linear in the number of CEs.
Official Scores for the CoNLL 2011 Shared
Task Table 4 summarizes the official scores of
ReconcileCoNLL on the CoNLL shared task. Sur-
prisingly, the scores are substationally lower than
the scores on our held-out training set. So far, we
have no explanation for these differences in perfor-
mance. We also observe that using gold-standard
instead of system-extracted CEs leads to improve-
ment in score of about point and a half.
The official score places us 8th out of 21 sys-
tems on the closed task. We note that because
of the threshold optimization mix-up we suffered
about 2 points in combined score performance.
Realistically our system should score around 0.54
placing us 5th or 6th on the task.
6 Conclusions
In this paper, we presented ReconcileCoNLL, our
system for the 2011 CoNLL shared task based on
the Reconcile research platform. We described
the overall Reconcile platform, our configuration
for the CoNLL task and the changes that we im-
plemented specific to the task. We presented the
results of an empirical evaluation performed on
held-out training data. We discovered that results
for our system on this data are quite different from
the official score that our system achieved.
Acknowledgments
This material is based upon work supported by
the National Science Foundation under Grant #
0937060 to the Computing Research Association
for the CIFellows Project.
125
References
ACE05. 2005. NIST ACE evaluation website. In
http://www.nist.gov/speech/tests/ace/2005.
Yoav Freund and Robert E. Schapire. 1999. Large
margin classification using the perceptron algorithm.
In Machine Learning, pages 277?296.
D. Klein and C. Manning. 2003. Fast Exact Inference
with a Factored Model for Natural Language Pars-
ing. In Advances in Neural Information Processing
(NIPS 2003).
Sameer S. Pradhan, Lance Ramshaw, Ralph
Weischedel, Jessica MacBride, and Linnea Micci-
ulla. 2007. Unrestricted coreference: Identifying
entities and events in ontonotes. In Proceedings
of the International Conference on Semantic
Computing.
Sameer Pradhan, Lance Ramshaw, Mitchell Marcus,
Martha Palmer, Ralph Weischedel, and Nianwen
Xue. 2011. Conll-2011 shared task: Modeling un-
restricted coreference in ontonotes. In Proceedings
of the Fifteenth Conference on Computational Nat-
ural Language Learning (CoNLL 2011), Portland,
Oregon, June.
W. Soon, H. Ng, and D. Lim. 2001. A Machine
Learning Approach to Coreference of Noun Phrases.
Computational Linguistics, 27(4):521?541.
V. Stoyanov, N. Gilbert, C. Cardie, and E. Riloff. 2009.
Conundrums in noun phrase coreference resolution:
Making sense of the state-of-the-art. In Proceedings
of ACL/IJCNLP.
V. Stoyanov, C. Cardie, N. Gilbert, E. Riloff, D. But-
tler, and D. Hysom. 2010a. Reconcile: A corefer-
ence resolution research platform. Technical report,
Cornell University.
Veselin Stoyanov, Claire Cardie, Nathan Gilbert, Ellen
Riloff, David Buttler, and David Hysom. 2010b.
Coreference resolution with reconcile. In Proceed-
ings of the ACL 2010.
126
Proceedings of the EACL 2012 Workshop on Computational Approaches to Deception Detection, pages 23?30,
Avignon, France, April 23 - 27 2012. c?2012 Association for Computational Linguistics
In Search of a Gold Standard in Studies of Deception
Stephanie Gokhman1, Jeff Hancock1,3, Poornima Prabhu2, Myle Ott2, Claire Cardie2,3
Departments of Communication1, Computer Science2, and Information Science3
Cornell University, Ithaca, NY 14853
{sbg94,jth34,pmp67,mao37,ctc9}@cornell.edu
Abstract
In this study, we explore several popular
techniques for obtaining corpora for decep-
tion research. Through a survey of tra-
ditional as well as non-gold standard cre-
ation approaches, we identify advantages
and limitations of these techniques for web-
based deception detection and offer crowd-
sourcing as a novel avenue toward achiev-
ing a gold standard corpus. Through an in-
depth case study of online hotel reviews,
we demonstrate the implementation of this
crowdsourcing technique and illustrate its
applicability to a broad array of online re-
views.
1 Introduction
Leading deception researchers have recently ar-
gued that verbal cues are the most promising indi-
cators for detecting deception (Vrij, 2008) while
lamenting the fact that the majority of previous
research has focused on nonverbal cues. At the
same time, increasing amounts of language are
being digitized and stored on computers and the
Internet ? from email, Twitter and online dating
profiles to legal testimony and corporate commu-
nication. With the recent advances in natural lan-
guage processing that have enhanced our ability
to analyze language, researchers now have an op-
portunity to similarly advance our understanding
of deception.
One of the crucial components of this enter-
prise, as recognized by the call for papers for the
present workshop, is the need to develop corpora
for developing and testing models of deception.
To date there has not been any systematic ap-
proach for corpus creation within the deception
field. In the present study, we first provide an
overview of traditional approaches for this task
(Section 2) and discuss recent deception detec-
tion methods that rely on non-gold standard cor-
pora (Section 3). Section 4 introduces novel ap-
proaches for corpus creation that employ crowd-
sourcing and argues that these have several ad-
vantages over traditional and non-gold standard
approaches. Finally, we describe an in-depth
case study of how these techniques can be im-
plemented to study deceptive online hotel reviews
(Section 5).
2 Traditional Approaches
The deception literature involves a number of
widely used traditional methods for gathering
deceptive and truthful statements. We classify
these according to whether they are sanctioned,
in which the experimenter supplies instructions to
individuals to lie or not lie, or unsanctioned ap-
proaches, in which the participant lies of his or
her own accord.
2.1 Sanctioned Deception
The vast majority of studies examining deception
employ some form of the sanctioned lie method.
A common example is recruiting participants for a
study on deception and randomly assigning them
to a lie or truth condition. A classic example of
this kind of procedure is the original study by Ek-
man and Friesen (1969), in which nurses were
required to watch pleasant or highly disturbing
movie clips. The nurses were instructed to indi-
cate that they were watching a pleasing movie,
which required the nurses watching the disturbing
clips to lie about their current emotional state.
In another example, Newman et. al. (2003) ask
23
participants about their beliefs concerning a given
topic, such as abortion, and then instruct partici-
pants to convince a partner that they hold the op-
posite belief.
Another form of sanctioned deception is to in-
struct participants to engage in some form of
mock crime and then ask them to lie about it. For
example, in one study (Porter and Yuille, 1996),
participants were asked to take an item, such as
a wallet, from a room and then lie about it after-
wards. The mock crime approach improves the
ecological validity of the deception, and makes it
the case that the person actually did in fact act a
certain way that they then must deny.
2.1.1 Advantages and Limitations
The advantages are obvious for these sanc-
tioned lie approaches. The researcher has large
degrees of experimental control over what the par-
ticipant lies about and when, which allows for
careful comparison across the deceptive and non-
deceptive accounts. Another advantage is the rel-
ative ease of instructing participants to lie vs. try-
ing to identify actual (but unknown) lies in a dia-
logue.
The limitations for this approach, however, are
also obvious. In asking participants to lie, the
researcher is essentially giving permission to the
person to lie. This should affect the partici-
pant?s behavior as the lie is being conducted at
the behest of a power figure, essentially acting
out their deception. Indeed, a number of schol-
ars have pointed out this problem (Frank and Ek-
man, 1997), and have suggested that unless high
stakes are employed the paradigm produces data
that does not replicate any typical lying situation.
High stakes refers to the potential for punishment
if the lie is detected or reward if the lie goes unde-
tected. Perhaps because of the difficulty in creat-
ing high-stakes deception scenarios, to date there
are few corpora involving high-stakes lies.
2.2 Unsanctioned Deception
Unsanctioned lies are those that are told without
any explicit instruction or permission from the re-
searcher. These kinds of lies have been collected
in a number of ways.
2.2.1 Diary studies and surveys
Two related methods for collecting information
about unsanctioned lies are diary studies and sur-
vey studies. In diary studies participants are asked
on an ongoing basis (e.g., every night) to recall
lies that they told over a given period (e.g., a day,
a week) (DePaulo et al, 1996; Hancock et al,
2004). Similarly, recent studies have asked par-
ticipants in national surveys how often they have
lied in the last 24 hours (Serota et al, 2010).
One important feature of these approaches is
that the lies have already taken place, and thus
they do not share the same limitations as sanc-
tioned lies. There are several drawbacks, how-
ever, especially given the current goal to collect
deception corpora. First, both diary studies and
survey approaches require self-reported recall of
deception. Several biases are likely to affect the
results, including under-reporting of deception in
order to reduce embarrassment and difficult-to-
remember deceptions that have occurred over the
time period. More importantly, this kind of ap-
proach does not lend itself to collecting the actual
language of the lie, for incorporation into a cor-
pus: people have a poor memory for conversation
recall (Stafford and Sharkey, 1987).
2.2.2 Retrospective Identification
One method for getting around the memory
limitations for natural discourse is to record the
discourse and ask participants to later identify any
deceptions in their discourse. For instance, one
study (Feldman and Happ, 2002) asked partici-
pants to meet another individual and talk for ten
minutes. After the discussion, participants were
asked to examine the videotape of the discussion
and indicated any times in which they were de-
ceptive. More recently, others have used the ret-
rospective identification technique on mediated
communication, such as SMS, which produces
an automatic record of the conversation that can
be reviewed for deception (Hancock, 2009). Be-
cause this approach preserves a record that the
participant can use to identify the deception, this
technique can generate data for linguistic analy-
sis. However, an important limitation, as with the
diary and survey data, is that the researcher must
assume that the participant is being truthful about
their deception reporting.
2.2.3 Cheating Procedures
The last form of unsanctioned lying involves
incentivizing participants to first cheat on a task
and to then lie when asked about the cheating be-
havior. Levine et al (2010) have recently used
24
this approach, which involved students perform-
ing a trivia quiz. During the quiz, an opportunity
to cheat arises where some of the students will
take the opportunity. At this point, they have not
yet lied, but, after the quiz is over, all students
are asked whether they cheated by an interviewer
who does not know if they cheated or not. While
most of the cheaters admit to cheating, a small
fraction of the cheaters deny cheating. This sub-
set of cheating denials represents real deception.
The advantages to this approach are three-
fold: (1) the deception is unsanctioned, (2) it
does not involve self-report, and (3) the decep-
tions have objective ground-truth. Unfortunately,
these kinds of experiments are extremely effort-
intensive given the number of deceptions pro-
duced. Only a tiny fraction of the participants
typically end up cheating and subsequently lying
about the cheating.
2.2.4 Limitations
While these techniques have been useful in
many psychology experiments, in which assess-
ing deception detection has been the priority
rather than corpus creation, they are not very
feasible when considering obtaining corpora for
large-scale settings, e.g., the web. Furthermore,
the techniques are limited in the kinds of con-
texts that can be created. For instance, in many
cases, e.g., deliberate posting of fake online re-
views, subjects can be both highly incentivized
to lie and highly concerned with getting caught.
One could imagine surveying hotel owners as to
whether they have ever posted a fake review?but
it would seem unlikely that any owner would ever
admit to having done so.
3 Non-gold Standard Approaches
Recently, alternative approaches have emerged to
study deception in the absence of gold standard
deceptive data. These approaches can typically
be broken up into three distinct types. In Sec-
tion 3.1, we discuss approaches to deception cor-
pus creation that rely on the manual annotation of
deceptive instances in the data. In Section 3.2, we
discuss approaches that rely on heuristic methods
for deriving approximate, but non-gold standard
deception labels. In Section 3.3, we discuss a re-
cent approach that uses assumptions about the ef-
fects of deception to identify examples of decep-
tion in the data. We will refer to the latter as the
unlabeled approach to deception corpus creation.
3.1 Manual Annotations of Deception
In Section 2.2, we discussed diary and self-report
methods of obtaining gold standard labels of de-
ception. Recently, work studying deceptive (fake)
online reviews has suggested using manual anno-
tations of deception, given by third-party human
judges.
Lim et al (2010) study deceptive product re-
views found on Amazon.com. They develop a
sophisticated software interface for manually la-
beling reviews as deceptive or truthful. The inter-
face allows annotators to view all of each user?s
reviews, ranked according to dimensions poten-
tially of importance to identifying deception, e.g.,
whether the review is duplicated, whether the re-
viewer has authored many reviews in a single day
with identical high or low ratings, etc.
Wu et al (2010a) also study deceptive online
reviews of TripAdvisor hotels, manually labeling
a set of reviews according to ?suspiciousness.?
This manually labeled dataset is then used to val-
idate eight proposed characteristics of deceptive
hotels. The proposed characteristics include fea-
tures based on the number of reviews written, e.g.,
by first-time reviewers, as well as the review rat-
ings, especially as they compare to other ratings
of the same hotel.
Li et al (2011) study deceptive product reviews
found on Epinions.com. Based on user-provided
helpfulness ratings, they first draw a subsample of
reviews such that the majority are considered to
be unhelpful. They then manually label this sub-
sample according to whether or not each review
seems to be fake.
3.1.1 Limitations
Manual annotation of deception is problematic
for a number of reasons. First, many of the same
challenges that face manual annotation efforts in
other domains also applies to annotations of de-
ception. For example, manual annotations can be
expensive to obtain, especially in large-scale set-
tings, e.g., the web.
Most seriously however, is that human abil-
ity to detect deception is notoriously poor (Bond
and DePaulo, 2006). Indeed, recent studies have
confirmed that human agreement and deception
detection performance is often no better than
chance (Ott et al, 2011); this is especially the
25
case when considering the overtrusting nature of
most human judges, a phenomenon referred to in
the psychological deception literature as a truth
bias (Vrij, 2008).
3.2 Heuristically Labeled
Work by Jindal and Liu (2008) studying the char-
acteristics of untruthful (deceptive) Amazon.com
reviews, has instead developed an approach for
heuristically assigning approximate labels of de-
ceptiveness, based on a set of assumptions spe-
cific to their domain. In particular, after re-
moving certain types of irrelevant ?reviews,? e.g.,
questions, advertisements, etc., they determine
whether each review has been duplicated, i.e.,
whether the review?s text heavily overlaps with
the text of other reviews in the same corpus. Then,
they simply label all discovered duplicate reviews
as untruthful.
Heuristic labeling approaches do not produce a
true gold-standard corpus, but for some domains
may offer an acceptable approximation. How-
ever, as with other non-gold standard approaches,
certain behaviors might have other causes, e.g.,
duplication could be accidental, and just because
something is duplicated does not make the origi-
nal (first) post deceptive. Indeed, in cases where
the original review is truthful, its duplication is
not a good example of deceptive reviews written
from scratch.
3.3 Unlabeled
Rather than develop heuristic labeling ap-
proaches, Wu et al (2010b) propose a novel strat-
egy for evaluating hypotheses about deceptive ho-
tel reviews found on TripAdvisor.com, based on
distortions of popularity rankings. Specifically,
they test the Proportion of Positive Singletons and
Concentration of Positive Singletons hypotheses
of Wu et al (2010a) (Section 3.1), but instead of
using manually-derived labels they evaluate their
hypotheses by the corresponding (distortion) ef-
fect they have on the hotel rankings.
Unlabeled approaches rely on assumptions
about the effects of the deception. For example,
the approach utilized by Wu et al (2010b) observ-
ing distortion effects on hotel rankings, relies on
the assumption that the goal of deceivers in the
online hotel review setting is to increase a hotel?s
ranking. And while this may be true for positive
hotel reviews, it is likely to be very untrue for fake
negative reviews intended to defame a competitor.
Indeed, great care must be taken in making such
assumptions in unlabeled approaches to studies of
deception.
4 Crowdsourcing Approaches
As with traditional sanctioned deception ap-
proaches (see Section 2.1), one way of obtain-
ing gold standard labels is to simply create gold
standard deceptive content. Crowdsourcing plat-
forms are a particularly compelling space to pro-
duce such deceptive content: they connect people
who request the completion of small tasks with
workers who will carry out the tasks. Crowd-
sourcing platforms that solicit small copywriting
tasks include Clickworker, Amazon?s Mechanical
Turk, Fiverr, and Worth1000. Craigslist, while not
a crowdsourcing platform, also promotes similar
solicitations for writing. In the case of fake online
reviews (see Section 5), and by leveraging plat-
forms such as Mechanical Turk, we can often gen-
erate gold standard deceptive content in contexts
very similar to those observed in practice.
Mihalcea and Strapparava (2009) were among
the first to use Mechanical Turk to collect decep-
tive and truthful opinions ? personal stances on
issues such as abortion and the death penalty. In
particular, for a given topic, they solicited one
truthful and one deceptive stance from each Me-
chanical Turk participant.
Ott et al (2011) have also used Mechanical
Turk to produce gold standard deceptive content.
In particular, they use Mechanical Turk to gener-
ate a dataset of 400 positive (5-star), gold stan-
dard deceptive hotel reviews. These were com-
bined with 400 (positive) truthful reviews cov-
ering the same set of hotels and used to train a
learning-based classifier that could distinguish de-
ceptive vs. truthful positive reviews at 90% accu-
racy levels. The truthful reviews were mined di-
rectly from a well-known hotel review site. The
Ott et al (2011) approach for collecting the gold
standard deceptive reviews is the subject of the
case study below.
5 Case Study: Crowdsourcing Deceptive
Reviews
To illustrate in more detail how crowdsourcing
techniques can be implemented to create gold
standard data sets for the study of deception, we
26
draw from the Ott et al (2011) approach that
crowdsources the collection of deceptive positive
hotel reviews using Mechanical Turk. The key
assumptions of the approach are as follows:
? We desire a balanced data set, i.e., equal
numbers of truthful and deceptive reviews.
This is so that statistical analyses of the data
set won?t be biased towards either type of re-
view.
? The truthful and deceptive reviews should
cover the same set of entities. If the two
sets of reviews cover different entities (e.g.,
different hotels), then the language that dis-
tinguishes truthful from deceptive reviews
might be attributed to the differing entities
under discussion rather than to the legiti-
macy of the review.
? The resulting data set should be of a rea-
sonable size. Ott et al (2011) found that
a dataset of 800 total reviews (400 truthful,
400 deceptive) was adequate for their goal
of training a learning-based classifier.
? The truthful and deceptive reviews should
exhibit the same valence, i.e., sentiment.
If the truthful reviews gathered from the on-
line site are positive reviews, the deceptive
reviews should be positive as well.
? More generally, the deceptive reviews
should be generated under the same ba-
sic guidelines as governs the generation
of truthful reviews. E.g., they should have
the same length constraints, the same quality
constraints, etc.
Step 1: Identify the set of entities to be cov-
ered in the truthful reviews. In order to de-
fine a set of desirable reviews, a master database,
provided by the review site itself, is mined to
identify the most commented (most popular) en-
tities. These are a good source of truthful re-
views. In particular, previous work has hypoth-
esized that popular offerings are less likely to
be targeted by spam (Jindal and Liu, 2008), and
therefore reviews for those entities are less likely
to be deceptive?enabling those reviews to later
comprise the truthful review corpus. The review
site database typically divides the entity set into
subcategories that differ across contexts: in the
case of hotel reviews the subcategories might re-
fer to cities, or in the case of doctor reviews
subcategories might refer to specialties. To en-
sure that enough reviews of the entity can be col-
lected, it may be important to select subcategories
that themselves are popular. The study of Ott et
al. (2011), for example, focused on reviews of ho-
tels in Chicago, IL, gathering positive (i.e., 5-star)
reviews for the 20 most popular hotels.
Step 2: Develop the crowdsourcing prompt.
Once a set of entities has been identified for the
deceptive reviews (Step 1), the prompt for Me-
chanical Turk is developed. This begins with a
survey of other solicitations for reviews within the
same subcategory through searching Mechanical
Turk, Craigslist, and other online resources. Us-
ing those solicitations as reference, a scenario can
then be developed that will be used in the prompt
to achieve the appropriate (in our case, positive)
valence. The result is a prompt that mimics the
vocabulary and tone that ?Turkers? (i.e., the work-
ers on Mechanical Turk) may find familiar and de-
sirable.
For example, the prompt of Ott et al (2011)
read: Imagine you work for the marketing depart-
ment of a hotel. Your boss asks you to write a fake
review for the hotel (as if you were a customer) to
be posted on a travel review website. The review
needs to sound realistic and portray the hotel in
a positive light. Look at their website if you are
not familiar with the hotel. (A link to the website
was provided.)
Step 3: Attach appropriate warnings to the
crowdsource solicitation. It is important that
warnings are attached to the solicitation to avoid
gathering (and paying for) reviews that would
invalidate the review set for the research. For
example, because each review should be written
by a different person, the warning might disallow
coders from performing multiple reviews; forbid
any form of plagiarism; require that reviews be
?on topic,? coherent, etc. Finally, the prompt
may inform the Turker that this exercise is for
academic purposes only and will not be posted
online, however, if such a notice is presented
before the review is written and submitted, the
resulting lie may be overly sanctioned.
27
Step 4: Incorporate into the solicitation a
means for gathering additional data. Append
to the end of the solicitation some mechanism
(e.g., Mechanical Turk allows for a series of ra-
dio buttons) to input basic information about age,
gender, or education of the coder. This allows for
post-hoc understanding of the demographic of the
participating Turkers. Ott et al (2011) also sup-
ply a space for comments by the workers, with an
added incentive of a potential bonus for particu-
larly helpful comments. Ott et al (2011) found
this last step critical to the iterative process for
providing insights from coders on inconsistencies,
technical difficulties, and other unforeseen prob-
lems that arise in the piloting phase.
Step 5: Gather the deceptive reviews in
batches. The solicitation is then published in a
small pilot test batch. In Ott et al (2011), each pi-
lot requested ten (10) reviews from unique work-
ers. Once the pilot run is complete, the results
are evaluated, with particular attention to the com-
ments, and is then iterated upon in small batches
of 10 until there are no technical complaints and
the results are of desired experiment quality.
Once this quality is achieved, the solicitation is
then published as a full run, generating 400 re-
views by unique workers. The results are man-
ually evaluated and cleaned to ensure all reviews
are valid, then filtered for plagiarism. The result-
ing set of gold standard online deceptive spam is
then used to train the algorithm for deceptive pos-
itive reviews.
5.1 Handling Plagiarism
One of the main challenges facing crowdsourced
deceptive content is identifying plagiarism. For
example, when a worker on Mechanical Turk
is asked to write a deceptive hotel review, that
worker may copy an available review from var-
ious sources on the Internet (e.g., TripAdvisor).
These plagiarized reviews lead to flaws in our
gold standard. Hence there arises a need to detect
such reviews and separate them from the entire
review set.
One way to address this challenge is to do
a manual check of the reviews, one-by-one, us-
ing online plagiarism detection web services, e.g.,
plagiarisma.net or searchenginereports.net. The
manual process is taxing, especially when there
are reviews in large numbers (as large as 400) to
be processed. This illustrates a need to have a
tool which automates the detection of plagiarized
content in Turker submissions. There are several
plagiarism detection softwares which are widely
available in the market. Most of them maintain
a database of content against which to check for
plagiarism. The input content is checked against
these databases and the content is stored in the
same database at the end of the process. Such
tools are an appropriate fit for detecting plagia-
rized content in term papers, course assignments,
journals etc. However, online reviews define a
separate need which checks for plagiarism against
the content available on the web. Hence the avail-
able software offerings are not adequate.
We implemented a command line tool using the
Yahoo! BOSS API, which is used to query sen-
tences on the web. Each of the review files is
parsed to read as individual sentences. Each sen-
tence is passed as a query input to the API. We
introduce the parameters, n and m, defined as:
1. Any sentence which is greater than n words
is considered to be a ?long sentence? in the
application usage. If the sentence is a ?long
sentence? and the Yahoo! BOSS API returns
no result, we query again using the first n
words of the sentence. Here n is a config-
urable parameter, and in our experiments we
configured n = 10.
2. A sentence that is commonly used on the
web can return many matches, even if it was
not plagiarized. Thus, we introduce another
parameter, m, such that if the number of
search results returned by the Yahoo! BOSS
API is greater than m, then the sentence is
considered common and is ignored. Our ob-
servations indicate that such frequently used
sentences are likely to be short. For exam-
ple: ?We are tired,? ?No room,? etc. For our
usage we configured m = 30.
We consider a sentence to be plagiarized if the
total number of results returned by the Yahoo!
BOSS API is less than m. Hence each sentence
is assigned a score as follows:
? If the total number of results is greater than
m: assign a score of 0
? If the total number of results is less than or
equal to m: assign a score of 1
28
We then divide the sum of the sentence scores in a
review by the total number of sentences to obtain
the ratio of the number of matches to total num-
ber of sentences. We use this ratio to determine
whether or not a review was plagiarized.
6 Discussion and Conclusion
We have discussed several techniques for creating
and labeling deceptive content, including tradi-
tional, non-gold standard, and crowdsourced ap-
proaches. We have also given an illustrative in-
depth look at how one might use crowdsourcing
services such as Mechanical Turk to solicit decep-
tive hotel reviews.
While we argue that the crowdsourcing ap-
proach to creating deceptive statements has
tremendous potential, there remain a number of
important limitations, some shared by the pre-
vious traditional methods laid out above. First,
workers are given ?permission? to lie, so these
lies are sanctioned and have the same concerns
as the traditional sanctioned methods, including
the concern that the workers are just play-acting
rather than lying. Other unique limitations in-
clude the current state of knowledge about work-
ers. In a laboratory setting we can fairly tightly
measure and control for gender, race, and even
socioeconomic status, but this is not the case for
the Amazon Turkers, who potentially make up a
much more diverse population.
Despite these issues we believe that the ap-
proach has much to offer. First, and perhaps most
importantly, the deceptions are being solicited in
exactly the manner real-world deceptions are ini-
tiated. This is important in that the deception task,
though sanctioned, is precisely the same task that
a real-world deceiver might use, e.g., to collect
fake hotel reviews for themselves. Second, this
approach is extremely cost effective in terms of
the time and finances required to create custom
deception settings that fit a specific context. Here
we looked at creating fake hotel reviews, but we
can easily apply this approach to other types of
reviews, including reviews of medical profession-
als, restaurants, and products.
Acknowledgments
This work was supported in part by National Sci-
ence Foundation Grant NSCC-0904913, and the
Jack Kent Cooke Foundation. We also thank the
EACL reviewers for their insightful comments,
suggestions and advice on various aspects of this
work.
References
C.F. Bond and B.M. DePaulo. 2006. Accuracy of de-
ception judgments. Personality and Social Psychol-
ogy Review, 10(3):214.
B.M. DePaulo, D.A. Kashy, S.E. Kirkendol, M.M.
Wyer, and J.A. Epstein. 1996. Lying in everyday
life. Journal of personality and social psychology,
70(5):979.
P. Ekman and W. V. Friesen. 1969. Nonverbal Leak-
age And Clues To Deception, volume 32.
Forrest J. A. Feldman, R. S. and B. R. Happ. 2002.
Self-presentation and verbal deception: Do self-
presenters lie more? Basic and Applied Social Psy-
chology, 24:163?170.
M.G. Frank and P. Ekman. 1997. The Ability To De-
tect Deceit Generalizes Across Different Types of
High-Stake Lies. Journal of Personality and Social
Psychology, 72:1429?1439.
J.T. Hancock, J. Thom-Santelli, and T. Ritchie. 2004.
Deception and design: The impact of communi-
cation technology on lying behavior. In Proceed-
ings of the SIGCHI conference on Human factors in
computing systems, pages 129?134. ACM.
J.T. Hancock. 2009. Digital Deception: The Practice
of Lying in the Digital Age. Deception: Methods,
Contexts and Consequences, pages 109?120.
N. Jindal and B. Liu. 2008. Opinion spam and analy-
sis. In Proceedings of the international conference
on Web search and web data mining, pages 219?
230. ACM.
Kim R. K. Levine, T. R. and J. P. Blair. 2010.
(In)accuracy at detecting true and false confessions
and denials: An initial test of a projected motive
model of veracity judgments. Human Communica-
tion Research, 36:81?101.
F. Li, M. Huang, Y. Yang, and X. Zhu. 2011. Learning
to identify review spam. In Twenty-Second Interna-
tional Joint Conference on Artificial Intelligence.
E.P. Lim, V.A. Nguyen, N. Jindal, B. Liu, and H.W.
Lauw. 2010. Detecting product review spammers
using rating behaviors. In Proceedings of the 19th
ACM international conference on Information and
knowledge management, pages 939?948. ACM.
R. Mihalcea and C. Strapparava. 2009. The lie de-
tector: Explorations in the automatic recognition of
deceptive language. In Proceedings of the ACL-
IJCNLP 2009 Conference Short Papers, pages 309?
312. Association for Computational Linguistics.
M.L. Newman, J.W. Pennebaker, D.S. Berry, and J.M.
Richards. 2003. Lying words: Predicting decep-
tion from linguistic styles. Personality and Social
Psychology Bulletin, 29(5):665.
29
M. Ott, Y. Choi, C. Cardie, and J.T. Hancock. 2011.
Finding deceptive opinion spam by any stretch of
the imagination. In Proceedings of the 49th Annual
Meeting of the Association for Computational Lin-
guistics: Human Language Technologies-Volume 1,
pages 309?319. Association for Computational Lin-
guistics.
S. Porter and J.C. Yuille. 1996. The language of de-
ceit: An investigation of the verbal clues to decep-
tion in the interrogation context. Law and Human
Behavior, 20:443?458.
K.B. Serota, T.R. Levine, and F.J. Boster. 2010.
The prevalence of lying in america: Three studies
of self-reported lies. Human Communication Re-
search, 36(1):2?25.
Burggraf C. S. Stafford, L. and W.F. Sharkey. 1987.
Conversational Memory The Effects of Time, Re-
call, Mode, and Memory Expectancies on Remem-
brances of Natural Conversations. Human Commu-
nication Research, 14:203?229.
A. Vrij. 2008. Detecting lies and deceit: Pitfalls and
opportunities. Wiley-Interscience.
G. Wu, D. Greene, B. Smyth, and P. Cunningham.
2010a. Distortion as a validation criterion in the
identification of suspicious reviews. In Proceedings
of the First Workshop on Social Media Analytics,
pages 10?13. ACM.
G. Wu, D. Greene, B. Smyth, and P. Cunningham.
2010b. Distortion as a validation criterion in
the identification of suspicious reviews. Techni-
cal report, UCD-CSI-2010-04, University College
Dublin.
30
Proceedings of the 13th Annual Meeting of the Special Interest Group on Discourse and Dialogue (SIGDIAL), pages 40?49,
Seoul, South Korea, 5-6 July 2012. c?2012 Association for Computational Linguistics
Unsupervised Topic Modeling Approaches to Decision Summarization in
Spoken Meetings
Lu Wang
Department of Computer Science
Cornell University
Ithaca, NY 14853
luwang@cs.cornell.edu
Claire Cardie
Department of Computer Science
Cornell University
Ithaca, NY 14853
cardie@cs.cornell.edu
Abstract
We present a token-level decision summariza-
tion framework that utilizes the latent topic
structures of utterances to identify ?summary-
worthy? words. Concretely, a series of
unsupervised topic models is explored and
experimental results show that fine-grained
topic models, which discover topics at the
utterance-level rather than the document-level,
can better identify the gist of the decision-
making process. Moreover, our proposed
token-level summarization approach, which
is able to remove redundancies within utter-
ances, outperforms existing utterance ranking
based summarization methods. Finally, con-
text information is also investigated to add ad-
ditional relevant information to the summary.
1 Introduction
Meetings are an important way for information shar-
ing and collaboration, where people can discuss
problems and make concrete decisions. Not sur-
prisingly, there is an increasing interest in develop-
ing methods for extractive summarization for meet-
ings and conversations (Zechner, 2002; Maskey and
Hirschberg, 2005; Galley, 2006; Lin and Chen,
2010; Murray et al, 2010a). Carenini et al (2011)
describe the specific need for focused summaries of
meetings, i.e., summaries of a particular aspect of a
meeting rather than of the meeting as a whole. For
example, the decisions made, the action items that
emerged and the problems arised are all important
outcomes of meetings. In particular, decision sum-
maries would allow participants to review decisions
from previous meetings and understand the related
topics quickly, which facilitates preparation for the
upcoming meetings.
A:We decided our target group is the focus on who can
afford it , (1)
B:Uh I?m kinda liking the idea of latex , if if spongy is
the in thing . (2)
B:what I?ve seen , just not related to this , but of latex
cases before , is that [vocalsound] there?s uh like a hard
plastic inside , and it?s just covered with the latex . (2)
C:Um [disfmarker] And I think if we wanna keep our costs
down , we should just go for pushbuttons , (3)
D:but if it?s gonna be in a latex type thing and that?s
gonna look cool , then that?s probably gonna have a
bigger impact than the scroll wheel . (2)
A:we?re gonna go with um type pushbuttons , (3)
A:So we?re gonna have like a menu button , (4)
C:uh volume , favourite channels , uh and menu . (4)
A:Pre-set channels (4)
Decision Abstracts (Summary)
DECISION 1: The target group comprises of individuals
who can afford the product.
DECISION 2: The remote will have a latex case.
DECISION 3: The remote will have pushbuttons.
DECISION 4: The remote will have a power button, volume
buttons, channel preset buttons, and a menu button.
Figure 1: A clip of a meeting from the AMI meeting cor-
pus (Carletta et al, 2005). A, B, C and D refer to distinct
speakers; the numbers in parentheses indicate the associated
meeting decision: DECISION 1, 2, 3 or 4. Also shown is the
gold-standard (manual) abstract (summary) for each decision.
Meeting conversation is intrinsically different
from well-written text, as meetings may not be well
organized and most utterances have low density of
salient content. Therefore, multiple problems need
to be addressed for speech summarization. Consider
the sample dialogue snippet in Figure 1 from the
AMI meeting corpus (Carletta et al, 2005). Only
decision-related dialogue acts (DRDAs) ? utter-
40
ances at least one decision made in the meeting1 ?
are listed and ordered by time. Each DRDA is la-
beled numerically according to the decision it sup-
ports; so the second and third utterances (in bold)
support DECISION 2, as do the fifth utterance in the
snippet. Manually constructed decision abstracts for
each decision are shown at the bottom of the figure.
Besides the prevalent dialogue phenomena (such
as ?Uh I?m kinda liking? in Figure 1), disfluencies
and off-topic expressions, we notice that single ut-
terance is usually not informative enough to form
a decision. For instance, no single DRDA associ-
ated with DECISION 4 corresponds all that well with
its decision abstract: ?pushbuttons?, ?menu button?
and ?Pre-set channels? are mentioned in separate
DAs. As a result, extractive summarization methods
that select individual utterance to form the summary
will perform poorly.
Furthermore, it is difficult to identify the core
topic when multiple topics are discussed in one ut-
terance. For example, all of the bold DRDAs sup-
porting DECISION 2 contain the word ?latex?. How-
ever, the last DA in bold also mentions ?bigger im-
pact? and ?the scroll wheel?, which are not specifi-
cally relevant for DECISION 2. Though this problem
can be approached by training a classifier to identify
the relevant phrases and ignore the irrelevant ones
or dialogue phenomena, it needs expensive human
annotation and is limited to the specific domain.
Note also that for DECISION 4, the ?power but-
ton? is not specified in any of the listed DRDAs
supporting it. By looking at the transcript, we find
?power button? mentioned in one of the preceding,
but not decision-related DAs. Consequently another
challenge would be to add complementary knowl-
edge when the DRDAs cannot provide complete in-
formation.
Therefore, we need a summarization approach
that is tolerant of dialogue phenomena, can deter-
mine the key semantic content and is easily trans-
ferable between domains. Recently, topic model-
ing approaches have been investigated and achieved
state-of-the-art results in multi-document summa-
rization (Haghighi and Vanderwende, 2009; Celiky-
1These DRDAs are annotated in the AMI corpus and usually
contain the decision content. They are similar, but not com-
pletely equivalent, to the decision dialogue acts (DDAs) of Bui
et al (2009), Ferna?ndez et al (2008), Frampton et al (2009).
ilmaz and Hakkani-Tur, 2010). Thus, topic mod-
els appear to be a better ref for document simi-
larity w.r.t. semantic concepts than simple literal
word matching. However, very little work has in-
vestigated its role in spoken document summariza-
tion (Chen and Chen, 2008; Hazen, 2011), and much
less conducted comparisons among topic modeling
approaches for focused summarization in meetings.
In contrast to previous work, we study the un-
supervised token-level decision summarization in
meetings by identifying a concise set of key words
or phrases, which can either be output as a com-
pact summary or be a starting point to generate ab-
stractive summaries. This paper addresses problems
mentioned above and make contributions as follows:
? As a step towards creating the abstractive sum-
maries that people prefer when dealing with spo-
ken language (Murray et al, 2010b), we propose a
token-level rather than sentence-level framework
for identifying components of the summary. Ex-
perimental results show that, compared to the sen-
tence ranking based summarization algorithms,
our token-level summarization framework can bet-
ter identify the summary-worthy words and re-
move the redundancies.
? Rather than employing supervised learning meth-
ods that rely on costly manual annotation, we ex-
plore and evaluate topic modeling approaches of
different granularities for the unsupervised deci-
sion summarization at both the token-level and di-
alogue act-level. We investigate three topic mod-
els ? Local LDA (LocalLDA) (Brody and El-
hadad, 2010), Multi-grain LDA (MG-LDA) (Titov
and McDonald, 2008) and Segmented Topic
Model (STM) (Du et al, 2010) ? which can uti-
lize the latent topic structure on utterance level
instead of document level. Under our proposed
token-level summarization framework, three fine-
grained models outperform the basic LDA model
and two extractive baselines that select the longest
and the most representative utterance for each de-
cision, respectively. (ROUGE-SU4 F score of
14.82% for STM vs. 13.58% and 13.46% for
the baselines, given the perfect clusterings of DR-
DAs.)
? In line with prior research that explore the role of
context for utterance-based extractive summariza-
41
tion (Murray and Renals, 2007), we investigate the
role of context in our token-level summarization
framework. For the given clusters of DRDAs, We
study two types of context information ? the DAs
preceding and succeeding a DRDA and DAs of
high TF-IDF similarity with a DRDA. We also in-
vestigate two ways to select relevant words from
the context DA. Experimental results show that
two types of context have comparable effect, but
selecting words from the dominant topic of the
center DRDA performs better than from the dom-
inant topic of the context DA. Moreover, by lever-
aging context, the recall exceeds the provided up-
perbound?s recall (ROUGE-1 recall: 48.10% vs.
45.05% for upperbound by using DRDA only) al-
though the F scores decrease after adding context
information. Finally, we show that when the true
DRDA clusterings are not available, adding con-
text can improve both the recall and F score.
2 Related Work
Speech and dialogue summarization has become im-
portant in recent years as the number of multime-
dia resources containing speech has grown. A pri-
mary goal for most speech summarization systems
is to account for the special characteristics of di-
alogue. Early work in this area investigated su-
pervised learning methods, including maximum en-
tropy, conditional random fields (CRFs), and sup-
port vector machines (SVMs) (Buist et al, 2004;
Galley, 2006; Xie et al, 2008). For unsupervised
methods, maximal marginal relevance (MMR) is in-
vestigated in (Zechner, 2002) and (Xie and Liu,
2010). Gillick et al (2009) introduce a concept-
based global optimization framework by using in-
teger linear programming (ILP).
Only in very recent works has decision sum-
marization been addressed in (Ferna?ndez et al,
2008), (Bui et al, 2009) and (Wang and Cardie,
2011). (Ferna?ndez et al, 2008) and (Bui et al, 2009)
utilize semantic parser to identify candidate phrases
for decision summaries and employ SVM to rank
those phrases. They also train HMM and SVM
directly on a set of decision-related dialogue acts
on token level and use the classifiers to identify
summary-worthy words. Wang and Cardie (2011)
provide an exploration on supervised and unsuper-
vised learning for decision summarization on both
utterance- and token- level.
Our work also arises out of applying topic mod-
els to text summarization (Bhandari et al, 2008;
Haghighi and Vanderwende, 2009; Celikyilmaz and
Hakkani-Tur, 2010; Celikyilmaz and Hakkani-Tur,
2010). Mostly, the sentences are ranked according to
importance based on latent topic structures, and top
ones are selected as the summary. There are some
works for applying document-level topic models to
speech summarization (Kong and shan Leek, 2006;
Chen and Chen, 2008; Hazen, 2011). Different from
their work, we further investigate the topic models of
fine granularity on sentence level and leverage con-
text information for decision summarization task.
Most existing approaches for speech summariza-
tion result in a selection of utterances from the dia-
logue, which cannot remove the redundancy within
utterances. To eliminate the superfluous words, our
work is also inspired by keyphrase extraction of
meetings (Liu et al, 2009; Liu et al, 2011) and
keyphrase based summarization (Riedhammer et al,
2010). However, a small set of keyphrases are not
enough to concretely display the content. Instead of
only picking up keyphrases, our work identifies all
of the summary-worthy words and phrases, and re-
moves redundancies within utterances.
3 Summarization Frameworks
In this section, we first present our proposed token-
level decision summarization framework ? Dom-
Sum ? which utilizes latent topic structure in ut-
terances to extract words from Dominant Topic (see
details in Section 3.1) to form Summaries. In Sec-
tion 3.2, we describe four existing sentence scor-
ing metrics denoted as OneTopic, MultiTopic, TMM-
Sum and KLSum which are also based on latent topic
distributions. We adopt them to the utterance-level
summarization for comparison in Section 6.
3.1 Token-level Summarization Framework
Domsum takes as input the clusters of DRDAs (with
or without additional context DAs), the topic distri-
bution for each DA and the word distribution for
each topic. The output is a set of topic-coherent
summary-worthy words which can be used directly
as the summary or to further generate abstractive
summary. We introduce DomSum in two steps ac-
cording to its input: taking clusters of DRDAs as the
input and with additional context information.
42
DRDAs Only. Given clusters of DRDAs, we use
Algorithm 1 to produce the token-level summary for
each cluster. Generally, Algorithm 1 chooses the
topic with the highest probability as the dominant
topic given the dialogue act (DA). Then it collects
the words with a high joint probability with the dom-
inant topic from that DA.
Input : Cluster C = {DAi}, P (Tj |DAi), P (wk|Tj)
Output: Summary
Summary? ? (empty set)
foreach DAi in C do
DomTopic? maxTj P (Tj |DAi) (*)Candidate? ?
foreach word wk inDAi do
SampleTopic? maxTj P (wk|Tj)P (Tj |DAi)
if DomTopic == SampleTopic then
Candidate? Union(Candidate, wk)
end
end
Summary? Union(Summary, Candidate)
end
Algorithm 1: DomSum ? The token-level sum-
marization framework. DomSum takes as input the
clusters of DRDAs and related probability distribu-
tions.
Leveraging Context. For each DRDA (denoted as
?center DA?), we study two types of context infor-
mation (denoted as ?context DAs?). One is adjacent
DAs, i.e., immediately preceding and succeeding
DAs, the other is the DAs having top TF-IDF simi-
larities with the center DA. Context DAs are added
into the cluster the corresponding center DA in.
We also study two criteria of word selection from
the context DAs. For each context DA, we can take
the words appearing in the dominant topic of ei-
ther this context DA or its center DRDA. We will
show in Section 6.1 that the latter performs better
as it produces more topic-coherent summaries. Al-
gorithm 1 can be easily modified to leverage context
DAs by updating the input clusters and assigning the
proper dominant topic for each DA accordingly ?
this changes the step (?) in Algorithm 1.
3.2 Utterance-level Summarization Metrics
We also adopt four sentence scoring metrics based
on the latent topic structure for extractive summa-
rization. Though they are developed on different
topic models, given the desired topic distributions as
input, they can rank the utterances according to their
importance and provide utterance-level summaries
for comparison.
OneTopic and MultiTopic. In (Bhandari et al,
2008), several sentence scoring functions are intro-
duced based on Probabilistic Latent Semantic Index-
ing. We adopt two metrics, which are OneTopic
and MultiTopic. For OneTopic, topic T with high-
est probability P (T ) is picked as the central topic
per cluster C. The score for DA in C is:
P (DA|T ) =
?
w?DA P (T |DA,w)?
DA??C,w?DA? P (T |DA?, w)
,
MultiTopic modifies OneTopic by taking all of the
topics into consideration. Given a cluster C, DA in
C is scored as:
?
T
P (DA|T )P (T ) =
?
T
?
w?DA P (T |DA,w)?
DA??C,w?DA? P (T |DA?, w)
P (T )
TMMSum. Chen and Chen (2008) propose a Top-
ical Mixture Model (TMM) for speech summariza-
tion, where each dialogue act is modeled as a TMM
for generating the document. TMM is shown to
provide better utterance-level extractive summaries
for spoken documents than other conventional unsu-
pervised approaches, such as Vector Space Model
(VSM) (Gong and Liu, 2001), Latent Semantic
Analysis (LSA) (Gong and Liu, 2001) and Max-
imum Marginal Relevance (MMR) (Murray et al,
2005). The importance of a sentence S can be mea-
sured by its generative probability P (D|S), where
D is the document S belongs to. In our experiments,
one decision is made per cluster of DAs. So we
adopt their scoring metric to compute the generative
probability of the cluster C for each DA:
P (C|DA) =
?
wi?C
?
Tj
P (wi|Tj)P (Tj |DA),
KLSum. Kullback-Lieber (KL) divergence is ex-
plored for summarization in (Haghighi and Vander-
wende, 2009) and (Lin et al, 2010), where it is used
to measure the distance of distributions between the
document and the summary. For a cluster C of DAs,
given a length limit ?, a set of DAs S is selected as:
S? = arg min
S:|S|<?
KL(PC ||PS) = arg min
S:|S|<?
?
Ti
P (Ti|C)log
P (Ti|C)
P (Ti|S)
4 Topic Models
In this section, we briefly describe the three fine-
grained topic models employed to compute the la-
tent topic distributions on utterance level in the
43
meetings. According to the input of Algorithm 1,
we are interested in estimating the topic distribution
for each DA P (T |DA) and the word distribution
for each topic P (w|T ). For MG-LDA, P (T |DA)
is computed as the expectation of local topic distri-
butions with respect to the window distribution.
4.1 Local LDA
Local LDA (LocalLDA) (Brody and Elhadad, 2010)
uses almost the same probabilistic generative model
as Latent Dirichlet Allocation (LDA) (Blei et al,
2003), except that it treats each sentence as a sepa-
rate document2. Each DA d is generated as follows:
1. For each topic k:
(a) Choose word distribution: ?k ? Dir(?)
2. For each DA d:
(a) Choose topic distribution: ?d ? Dir(?)
(b) For each word w in DA d:
i. Choose topic: zd,w ? ?d
ii. choose word: w ? ?zd,w
4.2 Multi-grain LDA
Multi-grain LDA (MG-LDA) (Titov and McDonald,
2008) can model both the meeting specific topics
(e.g. the design of a remote control) and various con-
crete aspects (e.g. the cost or the functionality). The
generative process is:
1. Choose a global topic distribution: ?glm ? Dir(?gl)
2. For each sliding window v of size T :
(a) Choose local topic distribution: ?locm,v ? Dir(?loc)
(b) Choose granularity mixture: pim,v ? Beta(?mix)
3. For each DA d:
(a) choose window distribution: ?m,d ? Dir(?)
4. For each word w in DA d of meeting m:
(a) Choose sliding window: vm,w ? ?m,d
(b) Choose granularity: rm,w ? pim,vm,w
(c) If rm,w = gl, choose global topic: zm,w ? ?glm
(d) If rm,w = loc, choose local topic: zm,w ? ?locm,vm,w
(e) Choose word w from the word distribution: ?rm,wzm,w
4.3 Segmented Topic Model
The last model we utilize is Segmented Topic Model
(STM) (Du et al, 2010), which jointly models
document- and sentence-level latent topics using
a two-parameter Poisson Dirichlet Process (PDP).
Given parameters ?, ?,? and PDP parameters a, b,
the generative process is:
1. Choose distribution of topics: ?m ? Dir(?)
2. For each dialogue act d:
2For the generative process of LDA, the DAs in the same
meeting make up the document, so ?each DA? is changed to
?each meeting? in LocalLDA?s generative process.
(a) Choose distribution of topics: ?d ? PDP (?m, a, b)
3. For each word w in dialogue act d:
(a) Choose topic: zm,w ? ?d
(b) Choose word: w ? ?zm,w
5 Experimental Setup
The Corpus. We evaluate our approach on the
AMI meeting corpus (Carletta et al, 2005) that con-
sists of 140 multi-party meetings. The 129 scenario-
driven meetings involve four participants playing
different roles on a design team. A short (usually
one-sentence) abstract is manually constructed to
summarize each decision discussed in the meeting
and used as gold-standard summaries in our experi-
ments.
System Inputs. Our summarization system re-
quires as input a partitioning of the DRDAs accord-
ing to the decision(s) that each supports (i.e., one
cluster of DRDAs per decision). As mentioned ear-
lier, we assume for all experiments that the DRDAs
for each meeting have been identified. For evalua-
tion we consider two system input settings. In the
True Clusterings setting, we use the AMI annota-
tions to create perfect partitionings of the DRDAs
as the input; in the System Clusterings setting, we
employ a hierarchical agglomerative clustering algo-
rithm used for this task in previous work (Wang and
Cardie, 2011). The Wang and Cardie (2011) cluster-
ing method groups DRDAs according to their LDA
topic distribution similarity. As better approaches
for DRDA clustering become available, they could
be employed instead.
Evaluation Metric. To evaluate the performance
of various summarization approaches, we use the
widely accepted ROUGE (Lin and Hovy, 2003) met-
rics. We use the stemming option of the ROUGE
software at http://berouge.com/ and remove
stopwords from both the system and gold-standard
summaries, same as Riedhammer et al (2010) do.
Inference and Hyperparameters We use the im-
plementation from (Lu et al, 2011) for the three
topic models in Section 4. The collapsed Gibbs
Sampling approach (Griffiths and Steyvers, 2004) is
exploited for inference. Hyperparameters are cho-
sen according to (Brody and Elhadad, 2010), (Titov
and McDonald, 2008) and (Du et al, 2010). In LDA
and LocalLDA, ? and ? are both set to 0.1 . For
MG-LDA, ?gl, ?loc and ?mix are set to 0.1; ? is 0.1
44
and the window size T is 3. And the number of lo-
cal topic is set as the same number of global topic as
discussed in (Titov and McDonald, 2008). In STM,
?, a and b are set to 0.5, 0.1 and 1, respectively.
5.1 Baselines and Comparisons
We compare our token-level summarization frame-
work based on the fine-grained topic models to (1)
two unsupervised baselines, (2) token-level summa-
rization by LDA, (3) utterance-level summarization
by Topical Mixture Model (TMM) (Chen and Chen,
2008), (4) utterance-level summarization based on
the fine-grained topic models using existing metrics
(Section 3.2), (5) two supervised methods, and (6)
an upperbound derived from the AMI gold standard
decision abstracts. (1) and (6) are described below,
others will be discussed in Section 6.
The LONGEST DA Baseline. As in (Riedhammer
et al, 2010) and (Wang and Cardie, 2011), this base-
line simply selects the longest DRDA in each cluster
as the summary. Thus, it performs utterance-level
decision summarization. This baseline and the next
allow us to determine summary quality when sum-
maries are restricted to a single utterance.
The PROTOTYPE DA Baseline. Following Wang
and Cardie (2011), the second baseline selects the
decision cluster prototype (i.e., the DRDA with the
largest TF-IDF similarity with the cluster centroid)
as the summary.
Upperbound. We also compute an upperbound
that reflects the gap between the best possible ex-
tractive summaries and the human-written abstracts
according to the ROUGE score: for each cluster of
DRDAs, we select the words that also appear in the
associated decision abstract.
6 Results and Discussion
6.1 True Clusterings
How do fine-grained topic models compare to ba-
sic topic models or baselines? Figure 2 demon-
strates that by using the DomSum token-level sum-
marization framework, the three fine-grained topic
models uniformly outperform the two non-trivial
baselines and TMM (Chen and Chen, 2008) (reim-
plemented by us) that generates utterance-level sum-
maries. Moreover, the fine-grained models also beat
basic LDA under the same DomSum token-level
summarization framework. This shows the fine-
2 3 4 5 6 7 8 9 106
7
8
9
10
11
12
13
14
15
#Topics
ROU
GE?S
U4 F
 (%)
Comparison with Baselines, TMM and LDA
 
 
LocalLDAMG?LDASTMLDATMMBASELINE 1BASELINE 2
Figure 2: With true clusterings of DRDAs as the input, we use
DomSum to compare the performance of LocalLDA, MGLDA
and STM against two baselines, LDA and TMM. ?# topic? in-
dicates the number of topics for the model. For MGLDA, ?#
topic? is the number of local topics.
2 3 4 5 6 7 8 9 1013
13.2
13.4
13.6
13.8
14
14.2
14.4
14.6
14.8
15
#Topic    
ROU
GE?S
U4 F 
(%)
Summarization from DRDAs by Different Metrics Based on STM (DRDA only)
 
 
DomSumOneTopicMultiTopicTMMSumKLSum
Figure 3: With true clusterings of DRDAs as the input, Dom-
Sum is compared with four DA-level summarization metrics us-
ing topic distributions from STM. Results from LocalLDA and
MGLDA are similar so they are not displayed.
grained topic models that discover topic structures
on utterance-level better identify gist information.
Can the proposed token-level summarization
framework better identify important words and
remove redundancies than utterance selection
methods? Figure 3 demonstrates the comparison
results for our DomSum token-level summarization
framework with four existing utterance scoring met-
rics discussed in Section 3.2, namely OneTopic,
MultiTopic, TMMSum and KLSum. The utterance
with highest score is extracted to form the summary.
LocalLDA and STM are utilized to compute the in-
put distributions, i.e., P (T |DA) and P (w|T ). From
Figure 3, DomSum yields the best F scores which
45
2 3 4 5 6 7 8 9 107.5
8
8.5
9
9.5
10
10.5
11
#Topic             
ROU
GE?
SU4
 F (%
)
Leveraging Context by STM
 
 Adj+DomSum(Multi)
Adj+DomSum(One)
TFIDF+DomSum(Multi)
TFIDF+DomSum(One)
Figure 4: Under DomSum framework, two types of context
information are added: Adjacent DA (?Adj?) and DAs with high
TFIDF similarities (?TFIDF?). For each context DA, selecting
words from the dominant topic of center DA (?One?) or the
current context DA (?Multi?) are investigated.
2 3 4 5 6 7 8 9 107.5
8
8.5
9
9.5
10
10.5
11
11.5
12
12.5
#Topic              
ROU
GE?
SU4
 F (%
)
Summarization by Different Metrics (adding Context)
 
 LocalLDA+DomSum(One)STM+DomSum(One)LocalLDA+OneTopicSTM+OneTopicLocalLDA+MultiTopicSTM+MultiTopic
Figure 5: By using adjacent DAs as context, DomSum is com-
pared with two DA-level summarization metrics: OneTopic and
MultiTopic. For DomSum, the words of context DA from dom-
inant topic of the center DA (?One?) is selected; For OneTopic
and MultiTopic, three top ranked DAs are selected.
shows that the token-level summarization approach
is more effective than utterance-level methods.
Which way is better for leveraging context infor-
mation? We explore two types of context infor-
mation. For adjacent content (Adj in Figure 4), 5
DAs immediately preceding and 5 DAs succeeding
the center DRDA are selected. For TF-IDF context
(TFIDF in Figure 4), 10 DAs of highest TF-IDF sim-
ilarity with the center DRDA are taken. We also
explore two ways to extract summary-worthy words
from the context DA ? selecting words from the
dominant topic of either the center DA (denoted as
?One? in parentheses in Figure 4) or the current con-
text DA (denoted as ?multi? in parentheses in Fig-
True Clusterings
R-1 R-2 R-SU4
PREC REC F1 F1 F1
Baselines
Longest DA 34.06 31.28 32.61 12.03 13.58
Prototype DA 40.72 28.21 33.32 12.18 13.46
Supervised
Methods
CRF 52.89 26.77 35.53 11.48 14.03
SVM 43.24 37.92 40.39 12.78 16.24
Our Approach
5 topics
LocalLDA 35.18 38.92 36.95 12.33 14.74
+ context 17.26 45.34 25.00 8.40 11.05
STM 34.06 41.30 37.32 12.42 14.82
+ context 15.60 48.10 23.56 8.16 9.98
10 topics
LocalLDA 36.20 36.81 36.50 12.04 14.34
+ context 21.82 41.57 28.62 9.61 12.24
STM 34.15 40.83 37.19 12.40 14.56
+ context 17.87 46.57 25.82 8.89 10.97
Upperbound 100.00 45.05 62.12 33.27 34.89
Table 1: ROUGE-1 (R-1), ROUGE-2 (R-2) and ROUGE-SU4
(R-SU4) scores for our proposed token-level summarization ap-
proaches along with two baselines, supervised methods and the
Upperbound (only using DRDAs). ? all use True Clusterings
ure 4). Figure 4 indicates that the two types of con-
text information do not have significant difference,
while selecting the words from the dominant topic
of the center DA results in better ROUGE-SU4 F
scores. Notice that compared with Figure 3, the re-
sults in Figure 4 have lower F scores when using the
true clusterings of DRDAs. This is because context
DAs bring in relevant words as well as noisy infor-
mation. We will show in Section 6.2 that when true
clusterings are not available, the context information
can boost both recall and F score.
How do the token-level summarization frame-
work compared to utterance selection methods
for leveraging context? We also compare the
ability of leveraging context of DomSum to utter-
ance scoring metrics, i.e., OneTopic and MultiTopic.
5 DAs preceding and 5 DAs succeeding the center
DA are added as context information. For context
DA under DomSum, we select words from the dom-
inant topic of the center DA (denoted as ?One? in
parentheses in Figure 5). For OneTopic and Mul-
tiTopic, the top 3 DAs are extracted as the sum-
mary. Figure 5 demonstrates the combination of Lo-
calLDA and STM with each of the metrics. Dom-
Sum, as a token-level summarization metrics, domi-
nates other two metrics in leveraging context.
46
System Clusterings
R-1 R-2 R-SU4
PREC REC F1 F1 F1
Baselines
Longest DA 17.06 11.64 13.84 2.76 3.34
Prototype DA 18.14 10.11 12.98 2.84 3.09
Supervised
Methods
CRF 46.97 15.25 23.02 6.09 9.11
SVM 39.05 18.45 25.06 6.11 9.82
Our Approach
5 topics
LocalLDA 25.57 16.57 20.11 4.03 5.87
+ context 20.68 25.96 23.02 3.09 4.48
STM 24.15 17.82 20.51 4.03 5.69
+ context 20.64 30.03 24.47 3.59 4.76
10 topics
LocalLDA 25.98 15.94 19.76 3.59 4.41
+ context 23.98 21.92 22.90 3.45 4.10
STM 26.32 19.14 22.16 4.07 5.88
+ context 22.50 28.40 25.11 3.43 4.15
Table 2: ROUGE-1 (R-1), ROUGE-2 (R-2) and ROUGE-SU4
(R-SU4) scores for our proposed token-level summarization ap-
proaches, compared with two baselines and supervised meth-
ods. ? all use System Clusterings
How do our approach perform when compared
with supervised learning approaches? For a bet-
ter comparison, we also provide summarization
results by using supervised systems along with
an upperbound. We use Support Vector Ma-
chines (Joachims, 1998) with RBF kernel and order-
1 Conditional Random Fields (Lafferty et al, 2001)
? trained with the same features as (Wang and
Cardie, 2011) to identify the summary-worthy to-
kens to include in the abstract. A three-fold cross
validation is conducted for both methods. ROUGE-
1, ROUGE-2 and ROUGE-SU4 scores are listed in
Table 1. From Table 1, our token-level summa-
rization approaches based on LocalLDA and STM
are shown to outperform the baselines and even the
CRF. Meanwhile, by adding context information,
both LocalLDA and STM can get better ROUGE-1
recall than the supervised methods, even higher than
the provided upperbound which is computed by only
using DRDAs. This shows the DomSum framework
can leverage context to compensate the summaries.
6.2 System Clusterings
Results using the System Clusterings (Table 2)
present similar findings, though all of the system and
baseline scores are lower. By adding context infor-
mation, the token-level summarization approaches
based on fine-grained topic models compare favor-
DRDA (1): I think if we can if we can include them at not too
much extra cost, then I?d put them in,
DRDA (2): Uh um we we?re definitely going in for voice
recognition as well as LCDs, mm.
DRDA (3): So we?ve basically worked out that we?re going
with a simple battery,
context DA (1):So it?s advanced integrated circuits?
context DA (2):the advanced chip
context DA (3): and a curved on one side case which is folded
in on itself , um made out of rubber
Decision Abstract: It will have voice recognition, use a simple
battery, and contain an advanced chip.
Longest DA & Prototype DA: Uh um we we?re definitely going
in for voice recognition as well as LCDs, mm.
TMM: I think if we can if we can include them at not too much
extra cost, then I?d put them in,
SVM: cost voice recognition simple battery
CRF: voice recognition battery
STM: extra cost, definitely going voice recognition LCDs,
simple battery
STM + context: cost, company, advanced integrated circuits, going
voice recognition, simple battery, advanced chip, curved case rubber
Table 3: Sample system outputs by different methods are in the
third cell (methods? names are in bold). First cell contains three
DRDAs supporting the decision in the second cell and three ad-
jacent DAs of them.
ably to the supervised methods in F scores, and also
get the best ROUGE-1 recalls.
6.3 Sample System Summaries
To better exemplify the summaries generated by
different systems, sample output for each method
is shown in Table 3. We see from the table that
utterance-level extractive summaries (Longest DA,
Prototype DA, TMM) make more coherent but still
far from concise and compact abstracts. On the other
hand, the supervised methods (SVM, CRF) that pro-
duce token-level extracts better identify the overall
content of the decision abstract. Unfortunately, they
require human annotation in the training phase. In
comparison, the output of fine-grained topic models
can cover the most useful information.
7 Conclusion
We propose a token-level summarization framework
based on topic models and show that modeling topic
structure at the utterance-level is better at identify-
ing relevant words and phrases than document-level
models. The role of context is also studied and
shown to be able to identify additional summary-
worthy words.
Acknowledgments This work was supported in part by
National Science Foundation Grants IIS-0968450 and
IIS-1111176, and by a gift from Google.
47
References
Harendra Bhandari, Takahiko Ito, Masashi Shimbo, and
Yuji Matsumoto. 2008. Generic text summarization
using probabilistic latent semantic indexing. In Pro-
ceedings of IJCNLP, pages 133?140.
David M. Blei, Andrew Y. Ng, and Michael I. Jordan.
2003. Latent dirichlet alocation. J. Mach. Learn.
Res., 3:993?1022, March.
Samuel Brody and Noemie Elhadad. 2010. An unsu-
pervised aspect-sentiment model for online reviews.
In Human Language Technologies: The 2010 Annual
Conference of the North American Chapter of the
Association for Computational Linguistics, HLT ?10,
pages 804?812, Stroudsburg, PA, USA. Association
for Computational Linguistics.
Trung H. Bui, Matthew Frampton, John Dowding, and
Stanley Peters. 2009. Extracting decisions from
multi-party dialogue using directed graphical models
and semantic similarity. In Proceedings of the SIG-
DIAL 2009 Conference, pages 235?243.
Anne Hendrik Buist, Wessel Kraaij, and Stephan Raaij-
makers. 2004. Automatic summarization of meeting
data: A feasibility study. In Proc. Meeting of Compu-
tational Linguistics in the Netherlands (CLIN).
Giuseppe Carenini, Gabriel Murray, and Raymond Ng.
2011. Methods for Mining and Summarizing Text Con-
versations. Morgan & Claypool Publishers.
Jean Carletta, Simone Ashby, Sebastien Bourban,
Mike Flynn, Thomas Hain, Jaroslav Kadlec, Vasilis
Karaiskos, Wessel Kraaij, Melissa Kronenthal, Guil-
laume Lathoud, Mike Lincoln, Agnes Lisowska, and
Mccowan Wilfried Post Dennis Reidsma. 2005. The
ami meeting corpus: A pre-announcement. In In Proc.
MLMI, pages 28?39.
Asli Celikyilmaz and Dilek Hakkani-Tur. 2010. A hy-
brid hierarchical model for multi-document summa-
rization. In Proceedings of the 48th Annual Meeting
of the Association for Computational Linguistics, ACL
?10, pages 815?824, Stroudsburg, PA, USA. Associa-
tion for Computational Linguistics.
Berlin Chen and Yi-Ting Chen. 2008. Extractive
spoken document summarization for information re-
trieval. Pattern Recogn. Lett., 29:426?437, March.
Lan Du, Wray Buntine, and Huidong Jin. 2010. A
segmented topic model based on the two-parameter
poisson-dirichlet process. Mach. Learn., 81:5?19, Oc-
tober.
Raquel Ferna?ndez, Matthew Frampton, John Dowding,
Anish Adukuzhiyil, Patrick Ehlen, and Stanley Peters.
2008. Identifying relevant phrases to summarize de-
cisions in spoken meetings. INTERSPEECH-2008,
pages 78?81.
Matthew Frampton, Jia Huang, Trung Huu Bui, and Stan-
ley Peters. 2009. Real-time decision detection in
multi-party dialogue. In Proceedings of the 2009 Con-
ference on Empirical Methods in Natural Language
Processing: Volume 3 - Volume 3, pages 1133?1141.
Michel Galley. 2006. A skip-chain conditional random
field for ranking meeting utterances by importance.
In Proceedings of the 2006 Conference on Empirical
Methods in Natural Language Processing, pages 364?
372.
Dan Gillick, Korbinian Riedhammer, Benoit Favre, and
Dilek Hakkani-Tur. 2009. A global optimization
framework for meeting summarization. In Proceed-
ings of the 2009 IEEE International Conference on
Acoustics, Speech and Signal Processing, ICASSP
?09, pages 4769?4772. IEEE Computer Society.
Yihong Gong and Xin Liu. 2001. Generic text summa-
rization using relevance measure and latent semantic
analysis. In Proceedings of the 24th annual interna-
tional ACM SIGIR conference on Research and devel-
opment in information retrieval, SIGIR ?01, pages 19?
25, New York, NY, USA. ACM.
T. L. Griffiths and M. Steyvers. 2004. Finding scien-
tific topics. Proceedings of the National Academy of
Sciences, 101(Suppl. 1):5228?5235, April.
Aria Haghighi and Lucy Vanderwende. 2009. Exploring
content models for multi-document summarization. In
Proceedings of Human Language Technologies: The
2009 Annual Conference of the North American Chap-
ter of the Association for Computational Linguistics,
NAACL ?09, pages 362?370. Association for Compu-
tational Linguistics.
Timothy J. Hazen. 2011. Latent topic modeling for au-
dio corpus summarization. In INTERSPEECH, pages
913?916.
Thorsten Joachims. 1998. Text categorization with Sup-
port Vector Machines: Learning with many relevant
features. In Claire Ne?dellec and Ce?line Rouveirol,
editors, Machine Learning: ECML-98, volume 1398,
chapter 19, pages 137?142. Berlin/Heidelberg.
Sheng-Yi Kong and Lin shan Leek. 2006. Improved spo-
ken document summarization using probabilistic latent
semantic analysis (plsa). In Proceedings of the 2006
IEEE International Conference on Acoustics, Speech
and Signal Processing, ICASSP ?06.
John D. Lafferty, Andrew McCallum, and Fernando C. N.
Pereira. 2001. Conditional random fields: Proba-
bilistic models for segmenting and labeling sequence
data. In Proceedings of the Eighteenth International
Conference on Machine Learning, ICML ?01, pages
282?289, San Francisco, CA, USA. Morgan Kauf-
mann Publishers Inc.
48
Shih-Hsiang Lin and Berlin Chen. 2010. A risk mini-
mization framework for extractive speech summariza-
tion. In Proceedings of the 48th Annual Meeting of the
Association for Computational Linguistics, ACL ?10,
pages 79?87. Association for Computational Linguis-
tics.
Chin-Yew Lin and Eduard Hovy. 2003. Automatic evalu-
ation of summaries using n-gram co-occurrence statis-
tics. In Proceedings of the 2003 Conference of the
North American Chapter of the Association for Com-
putational Linguistics on Human Language Technol-
ogy - Volume 1, pages 71?78.
S.-H. Lin, Y.-M. Yeh, and B. Chen. 2010. Leveraging
kullback-leibler divergence measures and information-
rich cues for speech summarization.
Feifan Liu, Deana Pennell, Fei Liu, and Yang Liu. 2009.
Unsupervised approaches for automatic keyword ex-
traction using meeting transcripts. In Proceedings
of Human Language Technologies: The 2009 Annual
Conference of the North American Chapter of the As-
sociation for Computational Linguistics, NAACL ?09,
pages 620?628, Stroudsburg, PA, USA. Association
for Computational Linguistics.
Fei Liu, Feifan Liu, and Yang Liu. 2011. A supervised
framework for keyword extraction from meeting tran-
scripts. IEEE Transactions on Audio, Speech & Lan-
guage Processing, 19(3):538?548.
Bin Lu, Myle Ott, Claire Cardie, and Benjamin Tsou.
2011. Multi-aspect sentiment analysis with topic mod-
els. In Workshop on Sentiment Elicitation from Natu-
ral Text for Information Retrieval and Extraction.
Sameer Maskey and Julia Hirschberg. 2005. Comparing
Lexical, Acoustic/Prosodic, Structural and Discourse
Features for Speech Summarization. In Proc. Euro-
pean Conference on Speech Communication and Tech-
nology (Eurospeech).
Gabriel Murray and Steve Renals. 2007. Towards on-
line speech summarization. In INTERSPEECH, pages
2785?2788.
Gabriel Murray, Steve Renals, and Jean Carletta. 2005.
Extractive summarization of meeting recordings. In
in Proceedings of the 9th European Conference on
Speech Communication and Technology, pages 593?
596.
Gabriel Murray, Giuseppe Carenini, and Raymond Ng.
2010a. Interpretation and transformation for abstract-
ing conversations. In Human Language Technologies:
The 2010 Annual Conference of the North American
Chapter of the Association for Computational Linguis-
tics, HLT ?10, pages 894?902, Stroudsburg, PA, USA.
Association for Computational Linguistics.
Gabriel Murray, Giuseppe Carenini, and Raymond T. Ng.
2010b. Generating and validating abstracts of meeting
conversations: a user study. In INLG?10.
Korbinian Riedhammer, Benoit Favre, and Dilek
Hakkani-Tu?r. 2010. Long story short - global unsu-
pervised models for keyphrase based meeting summa-
rization. Speech Commun., 52(10):801?815, October.
Ivan Titov and Ryan McDonald. 2008. Modeling online
reviews with multi-grain topic models. In Proceed-
ing of the 17th international conference on World Wide
Web, WWW ?08, pages 111?120. ACM.
Lu Wang and Claire Cardie. 2011. Summarizing deci-
sions in spoken meetings. In Proceedings of the Work-
shop on Automatic Summarization for Different Gen-
res, Media, and Languages, pages 16?24, Portland,
Oregon, June. Association for Computational Linguis-
tics.
Shasha Xie and Yang Liu. 2010. Using confusion net-
works for speech summarization. In Human Language
Technologies: The 2010 Annual Conference of the
North American Chapter of the Association for Com-
putational Linguistics, HLT ?10, pages 46?54. Associ-
ation for Computational Linguistics.
Shasha Xie, Yang Liu, and Hui Lin. 2008. Evaluating
the effectiveness of features and sampling in extrac-
tive meeting summarization. In Proc. of IEEE Spoken
Language Technology (SLT).
Klaus Zechner. 2002. Automatic summarization of
open-domain multiparty dialogues in diverse genres.
Comput. Linguist., 28:447?485, December.
49
Proceedings of the 13th Annual Meeting of the Special Interest Group on Discourse and Dialogue (SIGDIAL), pages 108?112,
Seoul, South Korea, 5-6 July 2012. c?2012 Association for Computational Linguistics
Improving Implicit Discourse Relation Recognition
Through Feature Set Optimization
Joonsuk Park
Department of Computer Science
Cornell University
Ithaca, NY, USA
jpark@cs.cornell.edu
Claire Cardie
Department of Computer Science
Cornell University
Ithaca, NY, USA
cardie@cs.cornell.edu
Abstract
We provide a systematic study of previously
proposed features for implicit discourse re-
lation identification, identifying new feature
combinations that optimize F1-score. The re-
sulting classifiers achieve the best F1-scores
to date for the four top-level discourse rela-
tion classes of the Penn Discourse Tree Bank:
COMPARISON, CONTINGENCY, EXPAN-
SION, and TEMPORAL. We further identify
factors for feature extraction that can have a
major impact on performance and determine
that some features originally proposed for the
task no longer provide performance gains in
light of more powerful, recently discovered
features. Our results constitute a new set of
baselines for future studies of implicit dis-
course relation identification.
1 Introduction
The ability to recognize the discourse relations that
exist between arbitrary text spans is crucial for un-
derstanding a given text. Indeed, a number of natu-
ral language processing (NLP) applications rely on it
? e.g., question answering, text summarization, and
textual entailment. Fortunately, explicit discourse
relations ? discourse relations marked by explicit
connectives ? have been shown to be easily identi-
fied by automatic means (Pitler et al, 2008): each
such connective is generally strongly coupled with
a particular relation. The connective ?because?, for
example, serves as a prominent cue for the CONTIN-
GENCY relation.
The identification of implicit discourse relations
? where such connectives are absent ? is much
harder. It has been the subject of much recent re-
search since the release of the Penn Discourse Tree-
bank 2.0 (PDTB) (Prasad et al, 2008), which anno-
tates relations between adjacent text spans in Wall
Street Journal (WSJ) articles, while clearly distin-
guishing implicit from explicit discourse relations.1
Recent studies, for example, explored the utility of
various classes of features for the task, including
linguistically informed features, context, constituent
and dependency parse features, and features that en-
code entity information or rely on language mod-
els (Pitler et al, 2009; Lin et al, 2009; Louis et al,
2010; Zhou et al, 2010).
To date, however, there has not been a systematic
study of combinations of these features for implicit
discourse relation identification. In addition, the re-
sults of existing studies are often difficult to compare
because of differences in data set creation, feature
set choice, or experimental methodology.
This paper provides a systematic study of previ-
ously proposed features for implicit discourse re-
lation identification and identifies feature combina-
tions that optimize F1-score using forward selection
(John et al, 1994). We report the performance of our
binary (one vs. rest) classifiers on the PDTB data
set for its four top-level discourse relation classes:
COMPARISON, CONTINGENCY, EXPANSION, and
TEMPORAL. In each case, the resulting classifiers
achieve the best F1-scores for the PDTB to date. We
1Research on implicit discourse relation recognition prior to
the release of the PDTB instead relied on synthetic data cre-
ated by removing explicit connectives from explicit discourse
relation instances (Marcu and Echihabi, 2002), but the trained
classifiers do not perform as well on real-world data (Blair-
Goldensohn et al, 2007).
108
further identify factors for feature extraction that can
have a major impact performance, including stem-
ming and lexicon look-up. Finally, by document-
ing an easily replicable experimental methodology
and making public the code for feature extraction2,
we hope to provide a new set of baselines for future
studies of implicit discourse relation identification.
2 Data
The experiments are conducted on the PDTB
(Prasad et al, 2008), which provides discourse rela-
tion annotations between adjacent text spans in WSJ
articles. Each training and test instance represents
one such pair of text spans and is classified in the
PDTB w.r.t. its relation type and relation sense.
In the work reported here, we use the relation
type to distinguish examples of explicit vs. implicit
discourse relations. In particular, we consider all in-
stances with a relation type other than explicit as
implicit relations since they lack an explicit con-
nective between the text spans. The relation sense
determines the relation that exists between its text
span arguments as one of: COMPARISON, CONTIN-
GENCY, EXPANSION, and TEMPORAL. For exam-
ple, the following shows an explicit CONTINGENCY
relation between argument1 (arg1) and argument2
(arg2), denoted via the connective ?because?:
The federal government suspended sales of
U.S. savings bonds because Congress hasn?t
listed the ceiling on government debt.
The four relation senses comprise the target classes
for our classifiers.
A notable feature of the PDTB is that the anno-
tation is done on the same corpus as Penn Tree-
bank (Marcus et al, 1993), which provides parse
trees and part-of-speech (POS) tags. This enables
the use of gold standard parse information for some
features, e.g., the production rules feature, one of
the most effective features proposed to date.
3 Features
Below are brief descriptions of features whose ef-
ficacy have been empirically determined in prior
works3, along with the rationales behind them:
2These are available from http://www.joonsuk.org.
3Word Pairs (Marcu and Echihabi, 2002). First-Last-First3
(Wellner et al, 2006). Polarity, Verbs, Inquirer Tags, Modality,
Context (Pitler et al, 2009). Production Rules (Lin et al, 2009).
Word Pairs (cross product of unigrams: arg1 ?
arg2) ? A few of these word pairs may capture in-
formation revealing the discourse relation of the tar-
get spans. For instance, rain-wet can hint at CON-
TINGENCY.
First-Last-First3 (the first, last, and first three
words of each argument) ? The words in this range
may be expressions that function as connectives for
certain relations.
Polarity (the count of words in arg1 and arg2, re-
spectively, that hold negated vs. non-negated posi-
tive, negative, and neutral sentiment) according to
the MPQA corpus (Wilson et al, 2005)) ? The
change in sentiment from arg1 to arg2 could be a
good indication of COMPARISON.
Inquirer Tags (negated and non-negated fine-
grained semantic classification tags for the verbs in
each argument and their cross product) ? The tags
are drawn from the General Inquirer Lexicon (Stone
et al, 1966)4, which provides word level relations
that might be propagated to the target spans? dis-
course relation, e.g., rise:fall.
Verbs (count of pairs of verbs from arg1 and arg2
belonging to the same Levin English Verb Class
(Levin and Somers, 1993)5, the average lengths of
verb phrases as well as their cross product, and the
POS of the main verb from each argument) ? Levin
Verb classes provide a means of clustering verbs
according to their meanings and behaviors. Also,
longer verb phrases might correlate with CONTIN-
GENCY, indicating a justification.
Modality (three features denoting the presence of
modal verbs in arg1, arg2, or both) ? Modal verbs
often appear in CONTINGENCY relations.
Context (the connective and the sense of the im-
mediately preceding and following relations (if ex-
plicit), and a feature denoting if arg1 starts a para-
graph) ? Certain relations co-occur.
Production Rules (three features denoting the pres-
ence of syntactic productions in arg1, arg2 or both,
based on all pairs of parent-children nodes in the ar-
gument parse trees) ? The syntactic structure of an
argument can influence that of the other argument as
4http://www.wjh.harvard.edu/ inquirer/inqdict.txt
5http://www-personal.umich.edu/ jlawler/levin.html
109
well as its relation type.
4 Experiments
We aim to identify the optimal subsets of the afore-
mentioned features for each of the four top-level
PDTB discourse relation senses: COMPARISON,
CONTINGENCY, EXPANSION, and TEMPORAL. In
order to provide a meaningful comparison with ex-
isting work, we carefully follow the experiment
setup of Pitler et al (2009), the origin of the ma-
jority of the features under consideration:
First, sections 0-2 and 21-22 of the PDTB are
used as the validation and test set, respectively.
Then, we randomly down-sample sections 2-20 to
construct training sets for each of the classifiers,
where each set has the same number of positive and
negative instances with respect to the target rela-
tion. Since the composition of the corresponding
training set has a noticeable impact on the classifier
performance we select a down-sampled training set
for each classifier through cross validation. All in-
stances of non-explicit relation senses are used; the
ENTREL type is considered as having the EXPAN-
SION sense.6
Second, Naive Bayes is used not only to duplicate
the Pitler et al (2009) setting, but also because it
equaled or outperformed other learning algorithms,
such as SVM and MaxEnt, in preliminary experi-
ments, while requiring a significantly shorter train-
ing time.7
Prior to the feature selection experiments, the best
preprocessing methods for feature extraction are de-
termined through cross validation. We consider sim-
ple lowercasing, Porter Stemming, PTB-style tok-
enization8, and hand-crafted rules for matching to-
kens to entries in the polarity and General Inquirer
lexicons.
Then, feature selection is performed via forward
selection, in which we start with the single best-
performing feature and, in each iteration, add the
feature that improves the F1-score the most, until
no significant improvement can be made. Once the
6Some prior work uses a different experimental setting. For
instance, Zhou et al (2010) only considers two of the non-
explicit relations, namely Implicit and NoRel.
7We use classifiers from the nltk package (Bird, 2006).
8Stanford Parser (Klein and Manning, 2003).
optimal feature set for each relation sense is deter-
mined by testing on the validation set, we retrain
each classifier using the entire training set and re-
port final performance on the test set.
5 Results and Analysis
Table 5 indicates the performance achieved by em-
ploying the feature set found to be optimal for each
relation sense via forward selection, along with the
performance of the individual features that consti-
tute the ideal subset. The two bottom rows show the
results reported in two previous papers with the most
similar experiment methodology as ours. The no-
table efficacy of the production rules feature, yield-
ing the best or the second best result across all re-
lation senses w.r.t. both F1-score and accuracy, con-
firms the finding of Zhou et al (2010). In contrast
to their work, however, combining existing features
enhances the performance. Below, we discuss the
primary observations gleaned from the experiments.
Word pairs as features. Starting with earlier works
that proposed them as features (Marcu and Echihabi,
2002), some form of word pairs has generally been
part of feature sets for implicit discourse relation
recognition. According to our research, however,
these features provide little or no additional gain,
once other features are employed. This seems sensi-
ble, since we now have a clearer idea of the types of
information important for the task and have devel-
oped a variety of feature types, each of which aims
to represent these specific aspects of the discourse
relation arguments. Thus, general features like word
pairs may no longer have a role to play for implicit
discourse relation identification.
Preprocessing. Preprocessing turned out to impact
the classifier performance immensely, especially for
features like polarity and inquirer tags that rely on
information retrieved from a lexicon. For these fea-
tures, if a match for a given word is not found in the
lexicon, no information is passed on to the classifier.
As an example, consider the General Inquirer lex-
icon. Most of its verb entries are present tense singu-
lar in form; thus, without stemming, dictionary look
up fails for a large portion of the verbs. In our case,
the F1-score increases by roughly 10% after stem-
ming.
Further tuning is possible by a few hand-written
110
Feature Type COMP. vs Rest CONT. vs Rest EXP. vs Rest TEMP. vs RestF1 Acc. F1 Acc. F1 Acc. F1 Acc.
1. Polarity 16.49 46.82 28.47 61.39 64.20 56.80 13.58 50.69
2. First-Last-First3 22.54 53.05 37.64 66.71 62.27 56.40 15.24 51.81
3. Inquirer Tags 18.07 82.14 34.88 69.60 77.76 66.38 21.65 80.04
4. Verbs 18.05 55.29 23.61 78.33 68.33 58.37 18.11 58.44
5. Production Rules 30.04 75.84 47.80 71.90 77.64 69.60 20.96 63.36
Best Combination 2 & 4 & 5 2 & 4 & 5 1 & 3 & 4 & 5 1 & 3 & 531.32 74.66 49.82 72.09 79.22 69.14 26.57 79.32
Pitler ?09 (Best) 21.96 56.59 47.13 67.30 76.42 63.62 16.76 63.49
Zhou ?10 (Best)* 31.79 58.22 47.16 48.96 70.11 54.54 20.30 55.48
* The experiments are conducted under a slightly different setting, as described in Section 4.
Table 1: Summary of Classifier Performance. 4-way classifiers have been tested as well, but their performance is not
as good as that of the binary classifiers shown here. One major difference is that it is harder to balance the number of
instances across all the classes when training 4-way classifiers.
rules to guide lexicon lookup. The word supplied,
for instance, becomes suppli after stemming, which
still fails to match the lexicon entry supply, unless
adjusted accordingly.
Binning. An additional finding regards features
that capture numeric, rather than binary, informa-
tion, such as polarity. Since this feature encodes the
counts of each type of sentiment word (with respect
to each argument and their cross product), and Naive
Bayes can only interpret binary features, we first em-
ployed a binning mechanism with each bin covering
a single value. For instance, if arg1 consists of three
positive words, we included arg1pos1, arg1pos2 and
arg1pos3 as features instead of just arg1pos3.
The rationale behind binning is that it captures
the proximity of related instances. Imagine having
three instances each with one, two, and three pos-
itive words in arg1, respectively. Without binning,
the features added are simply arg1pos1, arg1pos2,
arg1pos3, respectively. From the perspective of the
classifier, the third instance is no more similar to the
second instance than it is to the first instance, even
though having three positive words is clearly closer
to having two positive words than having one posi-
tive word. With binning, this proximity is captured
by the fact that the first instance has just one fea-
ture in common with the third instance, whereas the
second instance has two.
Binning, however, significantly degrades perfor-
mance on most of the classification tasks. One pos-
sible explanation is that these features function as an
abstraction of certain lexical patterns, rather than di-
rectly capturing similarities among instances of the
same class.
6 Conclusion
We employ a simple greedy feature selection ap-
proach to identify subsets of known features for
implicit discourse relation identification that yield
the best performance to date w.r.t. F1-score on the
PDTB data set. We also identify aspects of feature
set extraction and representation that are crucial for
obtaining state-of-the-art performance. Possible fu-
ture work includes evaluating the performance with-
out using the gold standard parses. This will give a
better idea of how the features that rely on parser
output will perform on real-world data where no
gold standard parsing information is available. In
this way, we can ensure that findings in this area of
research bring practical gains to the community.
Acknowledgments
We would like to thank Annie Louis and Yu Xu for help-
ing us reimplement the systems from Louis et al (2010)
and Zhou et al (2010), respectively. We also thank the
anonymous reviewers for their helpful feedback. This
work was supported in part by National Science Founda-
tion Grants IIS-1111176 and IIS-0968450, and by a gift
from Google.
111
References
Steven Bird. 2006. Nltk: the natural language toolkit. In
Proceedings of the COLING/ACL on Interactive pre-
sentation sessions, COLING-ACL ?06, pages 69?72,
Stroudsburg, PA, USA. Association for Computational
Linguistics.
Sasha Blair-Goldensohn, Kathleen McKeown, and Owen
Rambow. 2007. Building and refining rhetorical-
semantic relation models. In HLT-NAACL, pages 428?
435.
G. John, R. Kohavi, and K. Pfleger. 1994. Irrelevant
Features and the Subset Selection Problem. In W. Co-
hen and H. Hirsh, editors, Proceedings of the Eleventh
International Conference on Machine Learning, pages
121?129. Morgan Kaufmann.
Dan Klein and Christopher D. Manning. 2003. Accu-
rate unlexicalized parsing. In IN PROCEEDINGS OF
THE 41ST ANNUAL MEETING OF THE ASSOCIA-
TION FOR COMPUTATIONAL LINGUISTICS, pages
423?430.
Beth Levin and Harold Somers. 1993. English Verb
Classes and Alternations: A Preliminary Investiga-
tion. University Of Chicago Press.
Ziheng Lin, Min-Yen Kan, and Hwee Tou Ng. 2009.
Recognizing implicit discourse relations in the penn
discourse treebank. In EMNLP, pages 343?351.
Annie Louis, Aravind K. Joshi, Rashmi Prasad, and Ani
Nenkova. 2010. Using entity features to classify im-
plicit discourse relations. In SIGDIAL Conference,
pages 59?62.
Daniel Marcu and Abdessamad Echihabi. 2002. An
unsupervised approach to recognizing discourse rela-
tions. In ACL, pages 368?375.
M. Marcus, M. Marcinkiewicz, and B. Santorini. 1993.
Building a Large Annotated Corpus of English:
The Penn Treebank. Computational Linguistics,
19(2):313?330.
Emily Pitler, Mridhula Raghupathy, Hena Mehta, Ani
Nenkovak, Alan Lee, and Aravind K. Joshi. 2008.
Easily identifiable discourse relations. In COLING
(Posters), pages 87?90.
Emily Pitler, Annie Louis, and Ani Nenkova. 2009. Au-
tomatic sense prediction for implicit discourse rela-
tions in text. In ACL/AFNLP, pages 683?691.
Rashmi Prasad, Nikhil Dinesh, Alan Lee, Eleni Milt-
sakaki, Livio Robaldo, Aravind Joshi, and Bon-
nie Webber. 2008. The penn discourse tree-
bank 2.0. In Bente Maegaard Joseph Mariani Jan
Odjik Stelios Piperidis Daniel Tapias Nicoletta Cal-
zolari (Conference Chair), Khalid Choukri, editor,
Proceedings of the Sixth International Conference
on Language Resources and Evaluation (LREC?08),
Marrakech, Morocco, may. European Language
Resources Association (ELRA). http://www.lrec-
conf.org/proceedings/lrec2008/.
P J Stone, D C Dunphy, M S Smith, and D M Ogilvie.
1966. The General Inquirer: A Computer Approach
to Content Analysis, volume 08. MIT Press.
Ben Wellner, Lisa Ferro, Warren R. Greiff, and Lynette
Hirschman. 2006. Reading comprehension tests for
computer-based understanding evaluation. Natural
Language Engineering, 12(4):305?334.
Theresa Wilson, Janyce Wiebe, and Paul Hoffmann.
2005. Recognizing contextual polarity in phrase-level
sentiment analysis. In HLT/EMNLP.
Zhi-Min Zhou, Yu Xu, Zheng-Yu Niu, Man Lan, Jian Su,
and Chew Lim Tan. 2010. Predicting discourse con-
nectives for implicit discourse relation recognition. In
COLING (Posters), pages 1507?1514.
112
Proceedings of the 13th Annual Meeting of the Special Interest Group on Discourse and Dialogue (SIGDIAL), pages 304?313,
Seoul, South Korea, 5-6 July 2012. c?2012 Association for Computational Linguistics
Focused Meeting Summarization via Unsupervised Relation Extraction
Lu Wang
Department of Computer Science
Cornell University
Ithaca, NY 14853
luwang@cs.cornell.edu
Claire Cardie
Department of Computer Science
Cornell University
Ithaca, NY 14853
cardie@cs.cornell.edu
Abstract
We present a novel unsupervised framework
for focused meeting summarization that views
the problem as an instance of relation extrac-
tion. We adapt an existing in-domain rela-
tion learner (Chen et al, 2011) by exploit-
ing a set of task-specific constraints and fea-
tures. We evaluate the approach on a decision
summarization task and show that it outper-
forms unsupervised utterance-level extractive
summarization baselines as well as an exist-
ing generic relation-extraction-based summa-
rization method. Moreover, our approach pro-
duces summaries competitive with those gen-
erated by supervised methods in terms of the
standard ROUGE score.
1 Introduction
For better or worse, meetings play an integral role
in most of our daily lives ? they let us share infor-
mation and collaborate with others to solve a prob-
lem, to generate ideas, and to weigh options. Not
surprisingly then, there is growing interest in devel-
oping automatic methods for meeting summariza-
tion (e.g., Zechner (2002), Maskey and Hirschberg
(2005), Galley (2006), Lin and Chen (2010), Mur-
ray et al (2010a)). This paper tackles the task of fo-
cused meeting summarization , i.e., generating sum-
maries of a particular aspect of a meeting rather than
of the meeting as a whole (Carenini et al, 2011).
For example, one might want a summary of just the
DECISIONS made during the meeting, the ACTION
ITEMS that emerged, the IDEAS discussed, or the
HYPOTHESES put forth, etc.
Consider, for example, the task of summarizing
the decisions in the dialogue snippet in Figure 1. The
figure shows only the decision-related dialogue acts
(DRDAs) ? utterances associated with one or more
decisions.1 Each DRDA is labeled numerically ac-
cording to the decision it supports; so the first two
utterances support DECISION 1 as do the final two
utterances in the snippet. Manually constructed de-
cision abstracts for each decision are shown at the
bottom of the figure.2 These constitute the decision-
focused summary for the snippet.
Notice that many portions of the DRDAs are not
relevant to the decision itself: they often begin with
phrases that identify the utterance within the dis-
course as potentially introducing a decision (e.g.,
?Maybe that could be?, ?It seems like you?re gonna
have?), but do not themselves describe the decision.
We will refer to this portion of a DRDA (underlined
in Figure 1) as the Decision Cue.
Moreover, the decision cue is generally directly
followed by the actual Decision Content (e.g., ?be a
little apple?, ?have rubber cases?). Decision Content
phrases are denoted in Figure 1 via italics and square
brackets. Importantly, it is just the decision content
portion of the utterance that should be considered for
incorporation into the focused summary.
1These are similar, but not completely equivalent, to the de-
cision dialogue acts (DDAs) of (Bui et al, 2009), (Ferna?ndez et
al., 2008), (Frampton et al, 2009).
2Murray et al (2010b) show that users much prefer abstrac-
tive summaries over extracts when the text to be summarized
is a conversation. In particular, extractive summaries drawn
from group conversations can be confusing to the reader with-
out additional context; and the noisy, error-prone, disfluent text
of speech transcripts is likely to result in extractive summaries
with low readability.
304
C: Say the standby button is quite kinda separate from all the
other functions. (1)
C: Maybe that could be [a little apple]. (1)
C: It seems like you?re gonna have [rubber cases], as well as
[buttons]. (2)
A: [Rubber buttons] require [rubber case]. (2)
A: You could have [your company badge] and [logo]. (3)
A: I mean a lot of um computers for instance like like on the one
you?ve got there, it actually has a sort of um [stick on badge]. (3)
C: Shall we go [for single curve], just to compromise? (2)
B: We?ll go [for single curve], yeah. (2)
C: And the rubber push buttons, rubber case. (2)
D: And then are we going for sort of [one button] shaped
[like a fruit]. <vocalsound> Or veg. (1)
D: Could be [a red apple], yeah. (1)
Decision Abstracts (Summary)
DECISION 1: The group decided to make the standby button
in the shape of an apple.
DECISION 2: The remote will also feature a rubber case and
rubber buttons, and a single-curved design.
DECISION 3: The remote will feature the company logo,
possibly in a sticker form.
Figure 1: Clip from the AMI meeting corpus (Carletta et al,
2005). A, B, C and D refer to distinct speakers; the numbers
in parentheses indicate the associated meeting decision: DECI-
SION 1, 2 or 3. Also shown is the gold-standard (manual) ab-
stract (summary) for each decision. Colors indicate overlapping
vocabulary between utterances and the summary. Underlining,
italics, and [bracketing] are decscribed in the running text.
This paper presents an unsupervised framework
for focused meeting summarization that supports the
generation of abstractive summaries. (Note that we
do not currently generate actual abstracts, but rather
aim to identify those Content phrases that should
comprise the abstract.) In contrast to existing ap-
proaches to focused meeting summarization (e.g.,
Purver et al (2007), Ferna?ndez et al (2008), Bui et
al. (2009)), we view the problem as an information
extraction task and hypothesize that existing meth-
ods for domain-specific relation extraction can be
modified to identify salient phrases for use in gener-
ating abstractive summaries.
Very generally, information extraction methods
identify a lexical ?trigger? or ?indicator? that evokes
a relation of interest and then employ syntactic in-
formation, often in conjunction with semantic con-
straints, to find the ?target phrase? or ?argument
constituent? to be extracted. Relation instances,
then, are represented by indicator-argument pairs
(Chen et al, 2011).
Figure 1 shows some possible indicator-argument
pairs for identifying the Decision Content phrases
in the dialogue sample. Content indicator words
are shown in italics; the Decision Content target
phrases are the arguments. For example, in the
fourth DRDA, ?require? is the indicator, and ?rub-
ber buttons? and ?rubber case? are both arguments.
Although not shown in Figure 1, it is also possible
to identify relations that correspond to the Decision
Cue phrases.3
Specifically, we focus on the task of decision sum-
marization and, as in previous work in meeting sum-
marization (e.g., Ferna?ndez et al (2008), Wang and
Cardie (2011)), assume that all decision-related ut-
terances (DRDAs) have been identified. We adapt
the unsupervised relation learning approach of Chen
et al (2011) to separately identify relations asso-
ciated with decision cues vs. the decision content
within DRDAs by defining a new set of task-specific
constraints and features to take the place of the
domain-specific constraints and features of the orig-
inal model. Output of the system is a set of extracted
indicator-argument decision content relations (see
the ?OUR METHOD? sample summary of Table 6)
that can be used as the basis of the decision abstract.
We evaluate the approach (using the AMI cor-
pus (Carletta et al, 2005)) under two input set-
tings ? in the True Clusterings setting, we assume
that the DRDAs for each meeting have been per-
fectly grouped according to the decision(s) each sup-
ports; in the System Clusterings setting, an auto-
mated system performs the DRDA-decision pairing.
The results show that the relation-based summariza-
tion approach outperforms two extractive summa-
rization baselines that select the longest and the most
representative utterance for each decision, respec-
tively. (ROUGE-1 F score of 37.47% vs. 32.61%
and 33.32% for the baselines given the True Cluster-
ings of DRDAs.) Moreover, our approach performs
admirably in comparison to two supervised learning
alternatives (scores of 35.61% and 40.87%) that aim
to identify the important tokens to include in the de-
cision abstract given the DRDA clusterings. In con-
trast to our approach which is transferable to differ-
ent domains or tasks, these methods would require
labeled data for retraining for each new meeting cor-
pus.
3Consider, for example, the phrases underlined in the sixth
and seventh DRDAs. ?I mean? and ?shall we? are two typical
Decision Cue phrases where ?mean? and ?shall? are possible
indicators with ?I? and ?we? as their arguments, respectively.
305
Finally, in order to compare our approach to an-
other relation-based summarization technique, we
modify the multi-document summarization system
of Hachey (2009) to the single-document meeting
scenario. Here again, our proposed approach per-
forms better (37.47% vs. 34.69%). Experiments un-
der the System Clusterings setting produce the same
overall results, albeit with lower scores for all of the
systems and baselines.
In the remainder of the paper, we review related
work in Section 2 and give a high-level description
of the relation-based approach to focused summa-
rization in Section 3. Sections 4, 5 and 6 present the
modifications to the Chen et al (2011) relation ex-
traction model required for its instantiation for the
meeting summarization task. Sections 7 and 8 pro-
vide our experimental setup and results.
2 Related Work
Most research on spoken dialogue summariza-
tion attempts to generate summaries for full dia-
logues (Carenini et al, 2011). Only recently, how-
ever, has the task of focused summarization, and de-
cision summarization, in particular, been addressed.
Ferna?ndez et al (2008) and Bui et al (2009) em-
ploy supervised learning methods to rank phrases
or words for inclusion in the decision summary.
In comparison, Ferna?ndez et al (2008) find that
the phrase-based approach yields better recall than
token-based methods, concluding that phrases have
the potential to support better summaries. Input to
their system, however, is narrowed down (manually)
from the full set of DRDAs to the subset that is use-
ful for summarization. In addition, they evaluate
their system w.r.t. informative phrases or words that
have been manually annotated within this DRDA
subset. We are instead interested in comparing our
extracted relations to the abstractive summaries.
In contrast to our phrase-based approach, we pre-
viously explored a collection of supervised and un-
supervised learning methods for utterance-level (i.e.,
dialogue act) and token-level decision summariza-
tion (Wang and Cardie, 2011). We adopt here the
two unsupervised baselines (utterance-level sum-
maries) from that work for use in our evaluation.
We further employ their supervised summarization
methods as comparison points for token-level sum-
marization, adding additional features for consis-
tency with the other approaches in the evaluation.
Murray et al (2010a) develop an integer linear pro-
gramming approach for focused summarization at
the utterance-level, selecting sentences that cover
more of the entities mentioned in the meeting as de-
termined through the use of an external ontology.
The most relevant previous work is Hachey
(2009), which uses relational representations to fa-
cilitate sentence-ranking for multi-document sum-
marization. The method utilizes generic relation ex-
traction to represent the concepts in the documents
as relation instances; summaries are generated based
on a set cover algorithm that selects a subset of
the sentences that best cover the weighted concepts.
Thus, the goal of Hachey?s approach is sentence ex-
traction rather than phrase extraction. Although his
relation extraction method, like ours (see Section
4), is probabilistic and unsupervised (he uses Latent
Dirichelt Allocation (Blei et al, 2003)), the relations
are limited to pairs of named-entities, which is not
appropriate for our decision summarization setting.
Nevertheless, we will adapt his approach for com-
parison with our relation-based summarization tech-
nique and include it for evaluation.
3 Focused Summarization as Relation Ex-
traction
Given the DRDAs for each meeting grouped (not
necessarily correctly) according to the decisions
they support, we put each cluster of DRDAs (or-
dered according to time within the cluster) into one
?decision document?. The goal will be to pro-
duce one decision abstract for each such decision
document. We obtain constituent and dependency
parses using the Stanford parser (Klein and Man-
ning, 2003; de Marneffe et al, 2006). With the cor-
pus of constituent-parsed decision documents as the
input, we will use and modify Chen et al (2011)?s
system to identify decision cue relations and deci-
sion content relations for each cluster.4 (Section 6
will make clear how the learned decision cue rela-
tions will be used to identify decision content re-
lations.) The salient decision content relation in-
stances will be returned as decision summary com-
4Other unsupervised relation learning methods might also
be appropriate (e.g., Open IE (Banko et al, 2007)), but they
generally model relations between pairs of entities and group
relations only according to lexical similarity.
306
ponents.
Designed for in-domain relation discovery from
standard written texts (e.g., newswire), however, the
Chen et al (2011) system cannot be applied to our
task directly. In our setting, for example, neither the
number of relations nor the relation types is known
in advance.
In the following sections, we describe the modi-
fications needed for the spoken meeting genre and
decision-focused summarization task. In particular,
Chen et al (2011) provide two mechanisms that al-
low for this type of tailoring: the feature set used to
cluster potential relation instances into groups/types,
and a set of global constraints that characterize the
general qualities (e.g., syntactic form, prevalence,
discourse behavior) of a good relation for the task.
4 Model
In this section, we describe the Chen et al (2011)
probabilistic relation learning model used for both
Decision Cue and Decision Content relation extrac-
tion. The parameter estimation and constraint en-
coding through posterior inference are presented in
Section 5.
The relation learning model takes as input clus-
ters of DRDAs, sorted according to utterance time
and concatenated into one decision document. We
assume one decision will be made per document.
The goal for the model is to explain how the de-
cision documents are generated from the latent re-
lation variables. The posterior regularization tech-
nique (Section 5) biases inference to adhere to the
declarative constraints on relation instances. In gen-
eral, instead of extracting relation instances strictly
satisfying a set of human-written rules, features and
constraints are designed to allow the model to reveal
diverse relation types and to ensure that the identi-
fied relation instances are coherent and meaningful.
For each decision document, we select the relation
instance with highest probability for each relation
type and concatenate them to form the decision sum-
mary.
We restrict the eligible indicators to be a noun or
verb, and eligible arguments to be a noun phrase
(NP), prepositional phrase (PP) or clause introduced
by ?to? (S). Given a pre-specified number of relation
types K, the model employs a set of features ?i(w)
and ?a(x) (see Section 6) to describe the indicator
?0?k,? |?i|i i ?k,?bi i ?k,? |?a|a a ?k,?ba a ?k K
?0
?i(w)|?i| W ?a(x)|?a|X
sd,k
zd,kad,kid,k K D
Figure 2: Graphical model representation for the relation
learning model. D is the number of decision documents (each
decision document consists of a cluster of DRDAs). K is the
number of relation types. W and X represent the number of in-
dicators and arguments in the decision document. |?i| and |?a|
are the number of features for indicator and argument.
word w and argument constituent x. Each relation
type k is associated with a set of feature distributions
?k and a location distribution ?k. ?k include four pa-
rameter vectors: ?ik for indicator words, ?bik for non-
indicator words, ?ak for argument constituents, and
?bak for non-argument constituents. Each decision
document is divided into L equal-length segments
and the location parameter vector ?k describes the
probability of relation k arising from each segment.
The plate diagram for the model is shown in Fig-
ure 2. The generative process and likelihood of the
model are shown in Appendix A.
5 Parameter Estimation and Inference via
Posterior Regularization
In order to specify global preferences for the rela-
tion instances (e.g. the syntactic structure of the ex-
pressions), we impose inequality constraints on ex-
pectations of the posterior distributions during infer-
ence (Graca et al, 2008).
5.1 Variational inference with Constraints
Suppose we are interested in estimating the posterior
distribution p(?, z|x) of a model in general, where
?, z and x are parameters to estimate, latent vari-
ables and observations, respectively. We aim to find
a distribution q(?, z) ? Q that minimizes the KL-
divergence to the true posterior
KL(q(?, z)?p(?, z|x)) (1)
307
A mean-field assumption is made for variational
inference, where q(?, z) = q(?)q(z). Then we can
minimize Equation 1 by performing coordinate de-
scent on q(?) and q(z). Now we intend to have fine-
level control on the posteriors to induce meaningful
semantic parts. For instance, we would like most of
the extracted relation instances to satisfy a set of pre-
defined syntactic patterns. As presented in (Graca et
al., 2008), a general way to put constraints on pos-
terior q is through bounding expectations of given
functions: Eq[f(z)] ? b, where f(z) is a determin-
istic function of z, and b is a pre-specified threshold.
For instance, define f(z) as a function to count the
number of generated relation instances that meet the
pre-defined syntactic patterns, then most of the ex-
tracted relation instances will have the desired syn-
tactic structures.
By using the mean-field assumption, the model in
Section 4 is factorized as
q(?, ?, z, i, a) =
K?
k=1
q(?k; ??k)q(?ik; ??ik)q(?bik ; ??bik )q(?ak ??ak)q(?bak ; ??bak )
?
D?
d=1
q(zd,k, id,k, ad,k; c?d,k) (2)
The constraints are encoded in the inequalities
Eq[f(z, i, a)] ? b or Eq[f(z, i, a)] ? b, and affect
the inference as described above. Updates for the
parameters are discussed in Appendix B.
5.2 Task-Specific Constraints.
We define four types of constraints for the decision
relation extraction model.
Syntactic Constraints. Syntactic constraints are
widely used for information extraction (IE) systems
(Snow et al, 2005; Banko and Etzioni, 2008), as it
has been shown that most relations are expressed via
a small number of common syntactic patterns. For
each relation type, we require at least 80%5 of the
induced relation instances in expectation to match
one of the following syntactic patterns:
? The indicator is a verb and the argument is a noun
phrase. The headword of the argument is the direct
object of the indicator or the nominal subject of the
indicator.
5Experiments show that this threshold is suitable for deci-
sion relation extraction, so we adopt it from (Chen et al, 2011).
? The indicator is a verb and the argument is a prepo-
sitional phrase or a clause starting with ?to?. The
indicator and the argument have the same parent in
the constituent parsing tree.
? The indicator is a noun and is the headword of a
noun phrase, and the argument is a prepositional
phrase. The noun phrase with the indicator as its
headword and the argument have the same parent in
the constituent parsing tree.
For relation k, let f(zk, ik, ak) count the number
of induced indicator ik and argument ak pairs that
match one of the patterns above, and b is set to 0.8D,
whereD is the number of decision documents. Then
the syntactic constraint is encoded in the inequality
Eq[f(zk, ik, ak)] ? b.
Prevalence Constraints. The prevalence con-
straint is enforced on the number of times a relation
is instantiated, in order to guarantee that every rela-
tion has enough instantiations across the corpus and
is task-relevant. Again, we require each relation to
have induced instances in at least 80% of decision
documents.
Occurrence Constraints. Diversity of relation
types is enforced through occurrence constraints. In
particular, for each decision document, we restrict
each word to trigger at most two relation types as in-
dicator and occur at most twice as part of a relation?s
argument in expectation. An entire span of argument
constituent can appear in at most one relation type.
Discourse Constraints. The discourse constraint
captures the insight that the final decision on an is-
sue is generally made, or at least restated, at the end
of the decision-related discussion. As each decision
document is divided into four equal parts, we re-
strict 50% of the relation instances to be from the
last quarter of the decision documents.
6 Features
Table 1 lists the features we use for discovering
both the decision cue relations and decision con-
tent relations. We start with a collection of domain-
independent BASIC FEATURES shown to be use-
ful in relation extraction (Banko and Etzioni, 2008;
Chen et al, 2011). Then we add MEETING FEA-
TURES, STRUCTURAL FEATURES and SEMANTIC
FEATURES that have been found to be good pre-
dictors for decision detection (Hsueh and Moore,
2007) or meeting and decision summarization (Gal-
308
Basic Features
unigram (stemmed)
part-of-speech (POS)
constituent label (NP, VP, S/SBAR (start with ?to?))
dependency label
Meeting Features
Dialogue Act (DA) type
speaker role
topic
Structural Features (Galley, 2006) (Wang and Cardie, 2011)
in an Adjacency Pair (AP)?
if in an AP, AP type
if in an AP, the other part is decision-related?
if in an AP, the source part or target part?
if in an AP and is source part, is the target positive feedback?
if in an AP and is target part, is the source a question?
Semantic Features (from WordNet) (Miller, 1995)
first Synset of head word with the given POS
first hypernym path for the first synset of head word
Other Features (only for Argument)
number of words (without stopwords)
has capitalized word or not
has proper noun or not
Table 1: Features for Decision Cue and Decision Content re-
lation extraction. All features, except the last type of features,
are used for both the indicator and argument. (An Adjacency
Pair (AP) is an important conversational analysis concept (Schegloff
and Sacks, 1973). In the AMI corpus, an AP pair consists of a source
utterance and a target utterance, produced by different speakers.)
ley, 2006; Murray and Carenini, 2008; Ferna?ndez et
al., 2008; Wang and Cardie, 2011). Features em-
ployed only for argument?s are listed in the last cat-
egory in Table 1.
After applying the features in Table 1 and the
global constraints from Section 5 in preliminary ex-
periments, we found that the extracted relation in-
stances are mostly derived from decision cue rela-
tions. Sample decision cue relations and instances
are displayed in Table 2 and are not necessarily sur-
prising: previous research (Hsueh and Moore, 2007)
has observed the important role of personal pro-
nouns, such as ?we? and ?I?, in decision-making ex-
pressions. Notably, the decision cue is always fol-
lowed by the decision content. As a result, we in-
clude two additional features (see Table 3) that rely
on the cues to identify the decision content. Finally,
we disallow content relation instances with an argu-
ment containing just a personal pronoun.
7 Experiment Setup
The Corpus. We evaluate our approach on the
AMI meeting corpus (Carletta et al, 2005) that con-
sists of 140 multi-party meetings with a wide range
Decision Cue Relations Relation Instances
Group Wrap-up / Recap we have, we are, we say, we want
Personal Explanation I mean, I think, I guess, I (would) say
Suggestion do we, we (could/should) do
Final Decision it is (gonna), it will, we will
Table 2: Sample Decision Cue relation instances. The words
in parentheses are filled for illustration purposes, while they are
not part of the relation instances.
Discourse Features
clause position (first, second, other)
position to the first decision cue relation if any (before, after)
Table 3: Additional features for Decision Content relation ex-
traction, inspired by Decision Cue relations. Both indicator and
argument use those features.
of annotations. The 129 scenario-driven meetings
involve four participants playing different roles on
a design team. Importantly, the corpus includes a
short (usually one-sentence), manually constructed
abstract summarizing each decision discussed in the
meeting. In addition, all of the dialogue acts that
support (i.e., are relevant to) each decision are an-
notated as such. We use the manually constructed
decision abstracts as gold-standard summaries.
System Inputs. We consider two system input set-
tings. In the True Clusterings setting, we use
the AMI annotations to create perfect partitionings
of the DRDAs for input to the summarization sys-
tem; in the System Clusterings setting, we em-
ploy a hierarchical agglomerative clustering algo-
rithm used for this task in previous work (Wang and
Cardie, 2011). The Wang and Cardie (2011) cluster-
ing method groups DRDAs according to their LDA
topic distribution similarity. As better approaches
for DRDA clustering become available, they could
be employed instead.
Evaluation Metrics. We use the widely accepted
ROUGE (Lin and Hovy, 2003) evaluation measure.
We adopt the ROUGE-1 and ROUGE-SU4 met-
rics from (Hachey, 2009), and also use ROUGE-
2. We choose the stemming option of the ROUGE
software at http://berouge.com/ and remove
stopwords from both the system and gold-standard
summaries.
Training and Parameters. The Dirichlet hyper-
parameters are set to 0.1 for the priors. When train-
ing the model, ten random restarts are performed
and each run stops when reaching a convergence
threshold (10?5). Then we select the posterior with
309
the lowest final free energy. For the parameters
used in posterior constraints, we either adopt them
from (Chen et al, 2011) or choose them arbitrarily
without tuning in the spirit of making the approach
domain-independent.
We compare our decision summarization ap-
proach with (1) two unsupervised baselines, (2)
the unsupervised relation-based approach of Hachey
(2009), (3) two supervised methods, and (4) an up-
perbound derived from the gold standard decision
abstracts.
The LONGEST DA Baseline. As in Riedhammer
et al (2010) and Wang and Cardie (2011), this base-
line simply selects the longest DRDA in each clus-
ter as the summary. Thus, this baseline performs
utterance-level decision summarization. Although
it?s possible that decision content is spread over mul-
tiple DRDAs in the cluster, this baseline and the next
allow us to determine summary quality when sum-
maries are restricted to a single utterance.
The PROTOTYPE DA Baseline. Following Wang
and Cardie (2011), the second baseline selects the
decision cluster prototype (i.e., the DRDA with the
largest TF-IDF similarity with the cluster centroid)
as the summary.
The Generic Relation Extraction (GRE) Method
of Hachey (2009). Hachey (2009) presents
a generic relation extraction (GRE) for multi-
document summarization. Informative sentences
are extracted to form summaries instead of relation
instances. Relation types are discovered by Latent
Dirichlet Allocation, such that a probability is
output for each relation instance given a topic
(equivalent to relation). Their relation instances are
named entity(NE)-mention pairs conforming to a
set of pre-specified rules. For comparison, we use
these same rules to select noun-mention pairs rather
than NE-mention pairs, which is better suited to
meetings, which do not contain many NEs.6
6Because an approximate set cover algorithm is used in
GRE, one decision-related dialogue act (DRDA) is extracted
each time until the summary reaches the desired length. We run
two sets of experiments using this GRE system with different
output summaries ? one selects one entire DRDA as the final
summary (as Hachey (2009) does), and another one outputs the
relation instances with highest probability conditional on each
relation type. We find that the first set of experiments gets better
True Clusterings
R-1 R-2 R-SU4
PREC REC F1 F1 F1
Baselines
Longest DA 34.06 31.28 32.61 12.03 13.58
Prototype DA 40.72 28.21 33.32 12.18 13.46
GRE
5 topics 38.51 30.66 34.13 11.44 13.54
10 topics 39.39 31.01 34.69 11.28 13.42
15 topics 38.00 29.83 33.41 11.40 12.80
20 topics 37.24 30.13 33.30 10.89 12.95
Supervised Methods
CRF 53.95 26.57 35.61 11.52 14.07
SVM 42.30 41.49 40.87 12.91 16.29
Our Method
5 Relations 39.33 35.12 37.10 12.05 14.29
10 Relations 37.94 37.03 37.47 12.20 14.59
15 Relations 37.36 37.43 37.39 11.47 14.00
20 Relations 37.27 37.64 37.45 11.40 13.90
Upperbound 100.00 45.05 62.12 33.27 34.89
Table 4: ROUGE-1 (R-1), ROUGE-2 (R-2) and ROUGE-
SU4 (R-SU4) scores for summaries produced by the baselines,
GRE (Hachey, 2009)?s best results, the supervised methods, our
method and an upperbound ? all with perfect/true DRDA clus-
terings.
Supervised Learning (SVMs and CRFs). We
also compare our approach to two supervised learn-
ing methods ? Support Vector Machines (Joachims,
1998) with RBF kernel and order-1 Conditional
Random Fields (Lafferty et al, 2001) ? trained us-
ing the same features as our system (see Tables 1
and 3) to identify the important tokens to include in
the decision abstract. Three-fold cross validation is
conducted for both methods.
Upperbound. We also compute an upperbound
that reflects the gap between the best possible ex-
tractive summaries and the human-written abstracts
according to the ROUGE score: for each cluster of
DRDAs, we select the words that also appear in the
associated decision abstract.
8 Results and Discussion
Table 4 illustrates that, using True (DRDA) Clus-
terings our method outperforms the two baselines
and the generic relation extraction (GRE) based sys-
tem in terms of F score in ROUGE-1 and ROUGE-
SU4 with varied numbers of relations. Note that for
GRE based approach, we only list out their best re-
sults for utterance-level summarization. If using the
salient relation instances identified by GRE as the
summaries, the ROUGE results will be significantly
performance than the second, so we only report the best results
for their system in this paper.
310
System Clusterings
R-1 R-2 R-SU4
PREC REC F1 F1 F1
Baselines
Longest DA 17.06 11.64 13.84 2.76 3.34
Prototype DA 18.14 10.11 12.98 2.84 3.09
GRE
5 topics 17.10 9.76 12.40 3.03 3.41
10 topics 16.28 10.03 12.35 3.00 3.36
15 topics 16.54 10.90 13.04 2.84 3.28
20 topics 17.25 8.99 11.80 2.90 3.23
Supervised Methods
CRF 47.36 15.34 23.18 6.12 9.21
SVM 39.50 18.49 25.19 6.15 9.86
Our Method
5 Relations 16.12 18.93 17.41 3.31 5.56
10 Relations 16.27 18.93 17.50 3.32 5.69
15 Relations 16.42 19.14 17.68 3.47 5.75
20 Relations 16.75 18.25 17.47 3.33 5.64
Table 5: ROUGE-1 (R-1), ROUGE-2 (R-2) and ROUGE-
SU4 (R-SU4) scores for summaries produced by the baselines,
GRE (Hachey, 2009)?s best results, the supervised methods and
our method ? all with system clusterings.
lower. When measured by ROUGE-2, our method
still have better or comparable performances than
other unsupervised methods. Moreover, our sys-
tem achieves F scores in between those of the su-
pervised learning methods, performing better than
the CRF in both recall and F score. The recall score
for the upperbound in ROUGE-1, on the other hand,
indicates that there is still a wide gap between the
extractive summaries and human-written abstracts:
without additional lexical information (e.g., seman-
tic class information, ontologies) or a real language
generation component, recall appears to be a bottle-
neck for extractive summarization methods that se-
lect content only from decision-related dialogue acts
(DRDAs).
Results using the System Clusterings (Table 5)
are comparable, although all of the system and base-
line scores are much lower. Supervised methods get
the best F scores largely due to their high precision;
but our method attains the best recall in ROUGE-1.
Discussion. To better exemplify the summaries
generated by different systems, sample output for
each method is shown in Table 6. The GRE system
uses an approximate algorithm for set cover extrac-
tion, we list the first three selected DRDA in order.
We see from the table that utterance-level extractive
summaries (Longest DA, Prototype DA, GRE) make
more coherent but still far from concise and compact
DRDA (1): Uh the batteries, uh we also thought about that already,
DRDA (2): uh will be chargeable with uh uh an option for a
mount station
DRDA (3): Maybe it?s better to to include rechargeable batteries
DRDA (4): We already decided that on the previous meeting.
DRDA (5): which you can recharge through the docking station.
DRDA (6): normal plain batteries you can buy at the supermarket
or retail shop. Yeah.
Decision Abstract: The remote will use rechargeable batteries
which recharge in a docking station.
Longest DA & Prototype DA: normal plain batteries you can
buy at the supermarket or retail shop. Yeah.
GRE: 1st: normal plain batteries you can buy at the supermarket
or retail shop. Yeah.
2nd: which you can recharge through the docking station.
3rd: uh will be chargeable with uh uh an option for a mount station
SVM: batteries include rechargeable batteries decided recharge
docking station
CRF: chargeable station rechargeable batteries
Our Method: <option, for a mount station>,
<include, rechargeable batteries>,
<decided, that on the previous meeting>,
<recharge, through the docking station>,
<buy, normal plain batteries>
Table 6: Sample system outputs by different methods are in
the third cell (methods? names are in bold). First cell contains
the six DRDAs supporting the decision abstracted in the second
cell.
abstracts. On the other hand, the supervised methods
(SVM, CRF) that produce token-level extracts better
identify the overall content of the decision abstract.
Unfortunately, they require human annotation in the
training phase; in addition, the output is ungrammat-
ical and lacks coherence. In comparison, our sys-
tem presents the decision summary in the form of
phrase-based relations that provide a relatively com-
prehensive expression.
9 Conclusions
We present a novel framework for focused meet-
ing summarization based on unsupervised relation
extraction. Our approach is shown to outperform
unsupervised utterance-level extractive summariza-
tion baselines as well as an existing generic relation-
extraction-based summarization method. Our ap-
proach also produces summaries competitive with
those generated by supervised methods in terms of
the standard ROUGE score. Overall, we find that
relation-based methods for focused summarization
have potential as a technique for supporting the gen-
eration of abstractive decision summaries.
Acknowledgments This work was supported in part by
National Science Foundation Grants IIS-0968450 and
IIS-1111176, and by a gift from Google.
311
References
Michele Banko and Oren Etzioni. 2008. The Tradeoffs
Between Open and Traditional Relation Extraction. In
Proceedings of ACL-08: HLT, Columbus, Ohio.
Michele Banko, Michael J Cafarella, Stephen Soderl,
Matt Broadhead, and Oren Etzioni. 2007. Open in-
formation extraction from the web. In In IJCAI, pages
2670?2676.
David M. Blei, Andrew Y. Ng, and Michael I. Jordan.
2003. Latent dirichlet alocation. J. Mach. Learn.
Res., 3:993?1022, March.
Trung H. Bui, Matthew Frampton, John Dowding, and
Stanley Peters. 2009. Extracting decisions from
multi-party dialogue using directed graphical models
and semantic similarity. In Proceedings of the SIG-
DIAL 2009 Conference, pages 235?243.
Giuseppe Carenini, Gabriel Murray, and Raymond Ng.
2011. Methods for Mining and Summarizing Text Con-
versations. Morgan & Claypool Publishers.
Jean Carletta, Simone Ashby, Sebastien Bourban,
Mike Flynn, Thomas Hain, Jaroslav Kadlec, Vasilis
Karaiskos, Wessel Kraaij, Melissa Kronenthal, Guil-
laume Lathoud, Mike Lincoln, Agnes Lisowska, and
Mccowan Wilfried Post Dennis Reidsma. 2005. The
ami meeting corpus: A pre-announcement. In In Proc.
MLMI, pages 28?39.
Harr Chen, Edward Benson, Tahira Naseem, and Regina
Barzilay. 2011. In-domain relation discovery with
meta-constraints via posterior regularization. In Pro-
ceedings of the 49th Annual Meeting of the Associa-
tion for Computational Linguistics: Human Language
Technologies - Volume 1, HLT ?11, pages 530?540,
Stroudsburg, PA, USA. Association for Computational
Linguistics.
Marie-Catherine de Marneffe, Bill MacCartney, and
Christopher D. Manning. 2006. Generating typed de-
pendency parses from phrase structure trees. In LREC.
Raquel Ferna?ndez, Matthew Frampton, John Dowding,
Anish Adukuzhiyil, Patrick Ehlen, and Stanley Peters.
2008. Identifying relevant phrases to summarize de-
cisions in spoken meetings. INTERSPEECH-2008,
pages 78?81.
Matthew Frampton, Jia Huang, Trung Huu Bui, and Stan-
ley Peters. 2009. Real-time decision detection in
multi-party dialogue. In Proceedings of the 2009 Con-
ference on Empirical Methods in Natural Language
Processing: Volume 3 - Volume 3, pages 1133?1141.
Michel Galley. 2006. A skip-chain conditional random
field for ranking meeting utterances by importance.
In Proceedings of the 2006 Conference on Empirical
Methods in Natural Language Processing, pages 364?
372.
Joao Graca, Kuzman Ganchev, and Ben Taskar. 2008.
Expectation maximization and posterior constraints.
In J.C. Platt, D. Koller, Y. Singer, and S. Roweis, edi-
tors, Advances in Neural Information Processing Sys-
tems 20, pages 569?576. MIT Press, Cambridge, MA.
Ben Hachey. 2009. Multi-document summarisation us-
ing generic relation extraction. In Proceedings of the
2009 Conference on Empirical Methods in Natural
Language Processing: Volume 1 - Volume 1, EMNLP
?09, pages 420?429, Stroudsburg, PA, USA. Associa-
tion for Computational Linguistics.
Pei-yun Hsueh and Johanna Moore. 2007. What deci-
sions have you made: Automatic decision detection in
conversational speech. In In NAACL/HLT 2007.
Thorsten Joachims. 1998. Text categorization with Sup-
port Vector Machines: Learning with many relevant
features. In Claire Ne?dellec and Ce?line Rouveirol,
editors, Machine Learning: ECML-98, volume 1398,
chapter 19, pages 137?142. Berlin/Heidelberg.
Dan Klein and Christopher D. Manning. 2003. Accurate
unlexicalized parsing. In Proceedings of the 41st An-
nual Meeting on Association for Computational Lin-
guistics - Volume 1, ACL ?03, pages 423?430, Strouds-
burg, PA, USA. Association for Computational Lin-
guistics.
John D. Lafferty, Andrew McCallum, and Fernando C. N.
Pereira. 2001. Conditional random fields: Proba-
bilistic models for segmenting and labeling sequence
data. In Proceedings of the Eighteenth International
Conference on Machine Learning, ICML ?01, pages
282?289, San Francisco, CA, USA. Morgan Kauf-
mann Publishers Inc.
Shih-Hsiang Lin and Berlin Chen. 2010. A risk mini-
mization framework for extractive speech summariza-
tion. In Proceedings of the 48th Annual Meeting of the
Association for Computational Linguistics, ACL ?10,
pages 79?87, Stroudsburg, PA, USA. Association for
Computational Linguistics.
Chin-Yew Lin and Eduard Hovy. 2003. Automatic evalu-
ation of summaries using n-gram co-occurrence statis-
tics. In Proceedings of the 2003 Conference of the
North American Chapter of the Association for Com-
putational Linguistics on Human Language Technol-
ogy - Volume 1, pages 71?78.
Sameer Maskey and Julia Hirschberg. 2005. Comparing
Lexical, Acoustic/Prosodic, Structural and Discourse
Features for Speech Summarization. In Proc. Euro-
pean Conference on Speech Communication and Tech-
nology (Eurospeech).
George A. Miller. 1995. Wordnet: a lexical database for
english. Commun. ACM, 38:39?41, November.
Gabriel Murray and Giuseppe Carenini. 2008. Summa-
rizing spoken and written conversations. In Proceed-
312
ings of the Conference on Empirical Methods in Natu-
ral Language Processing, pages 773?782.
Gabriel Murray, Giuseppe Carenini, and Raymond Ng.
2010a. Interpretation and transformation for abstract-
ing conversations. In Human Language Technologies:
The 2010 Annual Conference of the North American
Chapter of the Association for Computational Linguis-
tics, HLT ?10, pages 894?902, Stroudsburg, PA, USA.
Association for Computational Linguistics.
Gabriel Murray, Giuseppe Carenini, and Raymond T. Ng.
2010b. Generating and validating abstracts of meeting
conversations: a user study. In INLG?10.
Matthew Purver, John Dowding, John Niekrasz, Patrick
Ehlen, Sharareh Noorbaloochi, and Stanley Peters.
2007. Detecting and summarizing action items in
multi-party dialogue. In in Proceedings of the 8th SIG-
dial Workshop on Discourse and Dialogue.
Korbinian Riedhammer, Benoit Favre, and Dilek
Hakkani-Tu?r. 2010. Long story short - global unsu-
pervised models for keyphrase based meeting summa-
rization. Speech Commun., 52(10):801?815, October.
E. A. Schegloff and H. Sacks. 1973. Opening up clos-
ings. Semiotica, 8(4):289?327.
Rion Snow, Daniel Jurafsky, and Andrew Y. Ng. 2005.
Learning Syntactic Patterns for Automatic Hypernym
Discovery. In Lawrence K. Saul, Yair Weiss, and
Le?on Bottou, editors, Advances in Neural Information
Processing Systems 17, pages 1297?1304. MIT Press,
Cambridge, MA.
Lu Wang and Claire Cardie. 2011. Summarizing deci-
sions in spoken meetings. In Proceedings of the Work-
shop on Automatic Summarization for Different Gen-
res, Media, and Languages, pages 16?24, Portland,
Oregon, June. Association for Computational Linguis-
tics.
Klaus Zechner. 2002. Automatic summarization of
open-domain multiparty dialogues in diverse genres.
Comput. Linguist., 28:447?485, December.
Appendix A Generative Process
The entire generative process is as follows (?Dir?
and ?Mult? refer to the Dirichlet distribution and
multinomial distribution):
1. For each relation type k:
(a) For each indicator feature ?i, draw feature distribu-
tions ?ik,?i , ?bik,?i ? Dir(?0)
(b) For each argument feature ?a, draw feature distri-
butions ?ak,?a , ?bak,?a ? Dir(?0)
(c) Draw location distribution ?k ? Dir(?0)
2. For each relation type k and decision document d:
(a) Select decision document segment sd,k ?
Mult(?k)
(b) Select DRDA zd,k uniformly from segment sd,k,
and indicator id,k and argument constituent ad,k
uniformly from DRDA zd,k
3. For each indicator word w in every decision document d:
(a) For each indicator feature ?i(w) ?
Mult( 1Z?Kk=1?k,?i), where ?k,?i is ?ik,?i if
id,k = w and ?bik,?i otherwise. Z is thenormalization factor.
4. For each argument constituent x in every decision docu-
ment d:
(a) For each indicator feature ?a(x) ?
Mult( 1Z?Kk=1?k,?a), where ?k,?a is ?ak,?a
if ad,k = x and ?bak,?a otherwise. Z is the
normalization factor.
Given ?0 and ?0, The joint distribution of a set of
feature parameters ?, the location distributions ?, a
set of DRDAs z, and the selected indicators i and
arguments a is:
P (?, ?, z, i, a; ?0, ?0) =
K?
k=1
P (?ik; ?0)P (?bik ; ?0)P (?ak |?0)P (?bak ; ?0)P (?k;?0)
? (
D?
d=1
P (id,k; zd,k)P (ad,k; zd,k)P (zd,k; sd,k)P (sd,k;?k)
? (P (w = id,k; ?ik)
?
w 6=id,k
P (w; ?bik ))
? (P (x = ad,k; ?ak)
?
x 6=ad,k
P (x; ?bak )))
Appendix B Updates for the Parameters
The constraints put on the posterior will only affect
the update for q(z). For q(?), the update is
q(?) = argmin
q(?)
KL(q(?)?q?(?)), (3)
where q?(?) ? expEq(z)[log p(?, z, x)], and q(?)
is updated to q?(?). For q(z), the update is
q(z) = argmin
q(z)
KL(q(z)?q?(z))
s.t. Eq(z)[fc(z)] ? bc, ?c ? C (4)
where q?(z) ? expEq(?)[log p(?, z, x)]. Equa-
tion 4 is easily solved via the dual (Graca et al,
2008) (Chen et al, 2011).
313
Proceedings of the First Workshop on Argumentation Mining, pages 29?38,
Baltimore, Maryland USA, June 26, 2014.
c?2014 Association for Computational Linguistics
Identifying Appropriate Support for Propositions in Online User
Comments
Joonsuk Park
Department of Computer Science
Cornell University
Ithaca, NY, USA
jpark@cs.cornell.edu
Claire Cardie
Department of Computer Science
Cornell University
Ithaca, NY, USA
cardie@cs.cornell.edu
Abstract
The ability to analyze the adequacy of sup-
porting information is necessary for deter-
mining the strength of an argument.
1
This
is especially the case for online user com-
ments, which often consist of arguments
lacking proper substantiation and reason-
ing. Thus, we develop a framework for
automatically classifying each proposition
as UNVERIFIABLE, VERIFIABLE NON-
EXPERIENTIAL, or VERIFIABLE EXPE-
RIENTIAL
2
, where the appropriate type of
support is reason, evidence, and optional
evidence, respectively
3
. Once the exist-
ing support for propositions are identi-
fied, this classification can provide an es-
timate of how adequately the arguments
have been supported. We build a gold-
standard dataset of 9,476 sentences and
clauses from 1,047 comments submitted
to an eRulemaking platform and find that
Support Vector Machine (SVM) classifiers
trained with n-grams and additional fea-
tures capturing the verifiability and expe-
rientiality exhibit statistically significant
improvement over the unigram baseline,
achieving a macro-averaged F
1
of 68.99%.
1 Introduction
Argumentation mining is a relatively new field
focusing on identifying and extracting argumen-
tative structures in documents. An argument is
typically defined as a conclusion with supporting
1
In this work, even unsupported propositions are consider
part of an argument. Not disregarding such implicit argu-
ments allows us to discuss the types of support that can fur-
ther be provided to strengthen the argument, as a form of as-
sessment.
2
Verifiable Experiential propositions are verifiable propo-
sitions about personal state or experience. See Table 1 for
examples.
3
We are assuming that there is no background knowledge
that eliminates the need of support.
premises, which can be conclusions of other argu-
ments themselves (Toulmin, 1958; Toulmin et al.,
1979; Pollock, 1987). To date, much of the argu-
mentation mining research has been conducted on
domains like news articles, parliamentary records
and legal documents, where the documents con-
tain well-formed explicit arguments, i.e. proposi-
tions with supporting reasons and evidence present
in the text (Moens et al., 2007; Palau and Moens,
2009; Wyner et al., 2010; Feng and Hirst, 2011;
Ashley and Walker, 2013).
Unlike documents written by professionals, on-
line user comments often contain arguments with
inappropriate or missing justification. One way
to deal with such implicit arguments is to sim-
ply disregard them and focus on extracting ar-
guments containing proper support (Villalba and
Saint-Dizier, 2012; Cabrio and Villata, 2012).
However, recognizing such propositions as part
of an argument,
4
and determining the appropriate
types of support can be useful for assessing the ad-
equacy of the supporting information, and in turn,
the strength of the whole argument. Consider the
following examples:
How much does a small carton of
milk cost?
1
More children should drink
milk
2
, because children who drink milk
everyday are taller than those who
don?t
3
. Children would want to drink
milk, anyway
4
.
Firstly, Sentence 1 does not need any support,
nor is it part of an argument. Next, Proposition 2
is an unverifiable proposition because it cannot be
proved with objective evidence, due to the value
judgement. Instead, it can be supported by a rea-
son explaining why it may be true. If the rea-
son, Proposition 3, were not true, the whole ar-
4
Not all sentences in user comments are part of an argu-
ment, e.g. questions and greetings. We address this in Sec-
tion 4.1
29
gument would fall apart, giving little weight to
Proposition 2. Thus, an objective evidence sup-
porting Proposition 3, which is a verifiable propo-
sition, could be provided to strengthen the argu-
ment. Lastly, as Proposition 4 is unverifiable, we
cannot expect an objective evidence that proves it,
but a reason as its support. Note that providing
a reason why Proposition 3 might be true is not
as effective as substantiating it with a proof, but
is still better than having no support. This shows
that not only the presence, but also the type of sup-
porting information affects the strength of the ar-
gument.
Examining each proposition in this way, i.e.
with respect to its verifiability, provides a means
to determine the desirable types of support, if
any, and enables the analysis of the arguments
in terms of the adequacy of their support. Thus,
we propose the task of classifying each proposi-
tion (the elementary unit of argumentation in this
work) in an argument as UNVERIFIABLE, VERI-
FIABLE PUBLIC, or VERIFIABLE PRIVATE, where
the appropriate type of support is reason, evidence,
and optional evidence, respectively. To perform
the experiments, we annotate 9,476 sentences and
clauses from 1,047 comments extracted from an
eRulemaking platform.
In the remainder of the paper, we describe the
annotation scheme and a newly created dataset
(Section 2), propose a supervised learning ap-
proach to the task (Section 3), evaluate the ap-
proach (Section 4), and survey related work (Sec-
tion 5). We find that Support Vector Machines
(SVM) classifiers trained with n-grams and other
features to capture the verifiability and experien-
tiality exhibit statistically significant improvement
over the unigram baseline, achieving a macro-
averaged F
1
score of 68.99%.
2 Data
We have collected and manually annotated sen-
tences and (independent) clauses from user com-
ments extracted from an eRulemaking website,
Regulation Room
5
. Rulemaking is the process by
which U.S. government agencies make new reg-
ulations and enact public policy; its digital coun-
terpart ? eRulemaking ? moves the process to
online platforms (see, e.g. (Park et al., 2012)).
By providing platforms in which the public can
discuss regulations that interest them, government
5
http://www.regulationroom.org
agencies hope to enlist the expertise and experi-
ence of participants to create better regulations.
In many rulemaking scenarios, agencies are, in
fact, required to obtain feedback from the pub-
lic on the proposed regulation as well as to ad-
dress all substantive questions, criticisms or sug-
gestions that are raised (Lubbers, 2006). In this
way, public comments can produce changes in the
final rule (Hochschild and Danielson, 1998) that,
in turn, can affect millions of lives. It is crucial,
therefore, for rule makers to be able to identify
credible comments from those submitted.
Regulation Room is an experimental web-
site operated by Cornell eRulemaking Initiative
(CeRI)
6
to promote public participation in the
rulemaking process, help users write more infor-
mative comments and build collective knowledge
via active discussions guided by human moder-
ators. Regulation Room hosts actual regulations
from government agencies, such as the U.S. De-
partment of Transportation.
For our research, we collected and manually an-
notated 9,476 propositions from 1,047 user com-
ments from two recent rules: Airline Passenger
Rights (serving peanuts on the plane, tarmac de-
lay contingency plan, oversales of tickets, baggage
fees and other airline traveller rights) and Home
Mortgage Consumer Protection (loss mitigation,
accounting error resolution, etc.).
2.1 Annotation Scheme
To start, we collected 1,147 comments and ran-
domly selected 100 of them to devise an annota-
tion scheme for identifying appropriate types of
support for propositions and to train annotators.
Initially, we allowed the annotators to define the
span for a propositions, leading to various compli-
cations and a low inter-annotator reliability. Thus,
we introduced an additional step in which com-
ments were manually sliced into propositions (or
non-propositional sentences) before being given to
the annotators. A proposition or sentence found
this way was split further if it consisted of two or
more independent clauses. The sliced comments
were then coded by two annotators into the fol-
lowing four disjoint classes (See Figure 1 for an
overview):
Verifiable Proposition [Experiential(VERIF
EXP
)
and Non-experiential(VERIF
NON
)]. A proposi-
tion is verifiable if it contains an objective asser-
6
http://www.lawschool.cornell.edu/ceri/
30
Figure 1: Flow chart for annotation (It refers to the sentence (or clause) being annotated)
# proposition
V
E
R
I
F
E
X
P
1 I?ve been a physician for 20 years.
2 My son has hypolycemia.
3 They flew me to NY in February.
4 The flight attendant yelled at the passengers.
V
E
R
I
F
N
O
N
5 They can have inhalation reactions.
6 since they serve them to the whole plane.
7 Peanuts do not kill people.
8 Clearly, peanuts do not kill people.
9 I believe peanuts do not kill people.
10 The governor said that he enjoyed it.
11 food allergies are rare
12 food allergies are seen in less than 20% of the
population
U
N
V
E
R
I
F
13 Again, keep it simple.
14 Banning peanuts will reduce deaths.
15 I enjoy having peanuts on the plane.
16 others are of uncertain significance
17 banning peanuts is a slippery slope
N
O
N
A
R
G
18 Who is in charge of this?
19 I have two comments
20 http://www.someurl.com
21 Thanks for allowing me to comment.
22 - Mike
Table 1: Example Sentences.
* Italics is used to illustrate core clause (Section 3.2).
tion, where objective means ?expressing or deal-
ing with facts or conditions as perceived without
distortion by personal feelings, prejudices, or in-
terpretations.?
7
Such assertions have truth values
that can be proved or disproved with objective ev-
idence
8
:
Consider the examples from Table 1. propo-
sitions 1 through 7 are clearly verifiable because
they only contain objective assertions. proposi-
tions 8 and 9 show that adding subjective expres-
sions such as ?Clearly? (e.g. sentence 8) or ?I be-
lieve that? (e.g. sentence 9) to an objectively veri-
fiable proposition (e.g. sentence 7) does not affect
the verifiability of the proposition. Sentence 10 is
considered verifiable because whether or not the
7
http://www.merriam-webster.com/
8
The correctness of the assertion or the availability of the
objective evidence does not matter.
governor said ?he enjoyed the peanuts? can be ver-
ified with objective evidence, even though whether
he really does or not cannot be verified.
For the purpose of identifying an appropriate
type of support, we employ a rather lenient no-
tion of objectivity: an assertion is objectively veri-
fiable if the domain of comparison is free of inter-
pretation. For instance, sentence 11 is regarded as
objectively verifiable, because it is clear, i.e. it is
not open for interpretation, that percentage of the
population is the metric under comparison even
though the threshold is purely subjective
9
. The
rationale is that this type of proposition can be
sufficiently substantiated with objective evidence
(e.g. published statistics showing the percentage
of people suffering from food allergies). Another
way to think about it is that sentence 11 is a loose
way of saying a (more obviously) verifiable sen-
tence 12, where the commenter neglected to men-
tion the threshold. This is fundamentally different
from propositions 13 through 16 for which objec-
tive evidence cannot exist
10
.
A verifiable proposition can further be dis-
tinguished as experiential or not, depending on
whether the proposition is about the writer?s per-
sonal state or experience (VERIF
EXP
) or some-
thing non-experiential (VERIF
NON
). This dif-
ference determines whether objective evidence is
mandatory or optional with respect to the credibil-
ity of the comment. Evidence is optional when the
evidence contains private information or is prac-
tically impossible to be provided: While proposi-
tions 1 through 3 can be proved with pictures of
official documents, for instance, the commenters
may not want to provide them for privacy rea-
sons. Also, the website interface may not al-
9
One may think anything less frequent than the average is
rare and another may have more stricter notion.
10
Objective evidence may exist for propositions that pro-
vide reasons for propositions 13 through 16.
31
Regulation VERIF
NON
VERIF
EXP
UNVERIF Subtotal NONARG Total # of Comments
APR 1106 851 4413 6370 522 6892 820
HMCP 251 416 1733 2400 186 2586 227
Total 1357 1267 6146 8770 708 9476 1047
Table 2: Class Distribution Over Sentences and Clauses
low pictures to be uploaded in comment section,
which is the case with most websites. sentence 4
is practically impossible to prove unless the com-
menter happened to have recorded the conversa-
tion, and the website interface allows multimedia
files to be uploaded. This is different from propo-
sitions 5 through 12, which should be (if valid, that
is) based on non-experiential knowledge the com-
menter acquired through objective evidence avail-
able to the public.
In certain domains, VERIF
EXP
propositions?
sometimes referred to as anectotal evidence?
provide the novel knowledge that readers are seek-
ing. In eRulemaking, for instance, agencies ac-
cept a wide variety of comments from the pub-
lic, including accounts of personal experience with
the problems or conditions the new regulation pro-
poses to address. If these accounts are relevant and
plausible, the agencies may use them, even if they
include no independent substantiation. Taking it
to an extreme, even if the ?experience? is fake, the
?experience? and opinions based on them are valu-
able to the agencies as long as the ?experience? is
realistic.
Unverifiable Proposition (UNVERIF). A propo-
sition is unverifiable if it cannot be proved with ob-
jective evidence. UNVERIF propositions are typi-
cally opinions, suggestions, judgements, or asser-
tions about what will happen in the future. (See
propositions 13 through 17.) Assertions about the
future are typically unverifiable, because there is
no direct evidence that something will happen. A
very prominent exception is a prediction based on
a policy of organizations, i.e. ?The store will be
open this Sunday.? where the policy serves as a di-
rect evidence.
Non-Argumentative (NONARG). A sentence or
clause is in this category if it is not a proposition,
i.e. it cannot be verified with objective evidence
and no supporting reason is required for the pur-
pose of improving the comment quality. Exam-
ples include question, greeting, citation, and URL.
(See sentences 18 through 21.)
2.2 Annotation Results
The resulting distribution of classes is shown in
Table 2. Note that even though we employed
a rather lenient definition of objective proposi-
tions, the distribution is highly skewed towards
UNVERIF propositions. This is expected because
the comments are written by people who want to
express their opinions about a regulation. Also,
NONARG sentences comprise about 7% of the
data, suggesting that most comment propositions
need to be supported with a reason or evidence for
maximal credibility.
The inter-coder reliability checked on 30% of
the data is moderate, yielding an Unweighted Co-
hen?s ? of 0.73. Most of the disagreement oc-
curred in propositions like ?Airlines have to pro-
vide compensation for both fees and lost bags? in
which it is not clear from the context whether it
is an opinion (UNVERIF) or a law (VERIF
NON
).
Also, opinions that may be verifiable (e.g. ?The
problems with passenger experience are not de-
pendant on aircraft size!?) seem to cause disagree-
ment among annotators.
3 Proposition Type Classification
3.1 Learning Algorithm
To classify each proposition in an argument as
VERIF
NON
, VERIF
EXP
, or UNVERIF, we train
multiclass Support Vector Machines (SVM) as for-
mulated by Crammer and Singer (2002), and later
extended by Keerthi et al.(2008). We use the Lib-
Linear (Fan et al., 2008) implementation. We ex-
perimented with other multiclass SVM approaches
such as 1-vs-all and 1-vs-1 (all-vs-all), but the dif-
ferences were statistically insignificant, consistent
with Hsu and Lin?s (2002) empirical comparison
of these methods. Thus, we only report the per-
formance of the Crammer and Singer version of
Multiclass SVM.
3.2 Features
The features are binary-valued, and the feature
vector for each data point is normalized to have
the unit length: ?Presence? features are binary
features indicating whether the given feature is
present in the proposition or not; ?Count? features
32
are numeric counts of the occurrence of each fea-
ture is converted to a set of three binary features
each denoting 0, 1 and 2 or more occurrences.
We first tried a binning method with each digit
as its own interval, resulting in binary features of
the form featCnt
n
, but the three-interval approach
proved to be better empirically, and is consistent
with the approach by Riloff and Shoen (1995).
The features can be grouped into three cate-
gories by purpose: Verifiability-specific (VER),
Experientiality-specific (EXP) and Basic Features,
each designed to capture the given proposition?s
verifiability, experientiality, and both, respec-
tively. Now we discuss the features in more detail.
3.2.1 Basic Features
N-gram Presence A set of binary features de-
note whether a given unigram or bigram occurs
in the proposition. The intuition is that by ex-
amining the occurrence of words or phrases in
VERIF
NON
, VERIF
EXP
, and UNVERIF propo-
sitions, the classes that have close ties to certain
words and phrases can be identified. For instance,
when a proposition contains the word happy, the
proposition tends to be UNVERIF. From this ob-
servation, we can speculate that happy is highly
associated with UNVERIF, and went, VERIF
EXP
.
n-gram presence, rather than the raw or normal-
ized frequency is chosen for its superior perfor-
mance (O?Keefe and Koprinska, 2009).
Core Clause Tag (CCT) To correctly classify
propositions with main or subordinate clauses that
do not affect the verifiability of the proposition
(e.g. propositions 8 through 10 in Table 1, respec-
tively), it is necessary to distinguish features that
appear in the main clause from those that appear in
the subordinate clause. Thus, we employ an auxil-
iary feature that adds clausal information to other
features by tagging them as either core or acces-
sory clause.
Let?s consider propositions 7, 9 and 10 in Ta-
ble 1: In all three examples, the core clause is ital-
icized. In single clause cases like proposition 7,
the entire proposition is the core clause. However,
for proposition 9, the core clause is the subordi-
nate clause introduced by the main clause, i.e. ?I
believe? should be ignored, since the verifiability
of ?peanuts do not kill people? is not dependent on
it. It is the opposite for proposition 10: the main
clause ?The governor said? is the core clause, and
the rest need not be considered. The reason is that
?said? is a speech event, and it is possible to objec-
tively verify whether or not the governor verbally
expressed his appreciation of peanuts.
To realize this intuition, we use syntactic parse
trees generated by the Stanford Parser (De Marn-
effe et al., 2006). In particular, Penn Treebank
2 Tags contain a clause-level tag SBAR denoting
a ?clause introduced by a subordinating conjunc-
tion? (Marcus et al., 1993). The ?that? clause in
proposition 10 spans a subtree rooted by SBAR,
whose left-most child has a lexical value ?that.?
Similarly, the subordinate (non-italicized) clause
in proposition 9 falls in a subtree rooted by SBAR,
whose only child is S. Once the main clause of a
given proposition is identified, all features set off
by the clause are tagged as ?core? and the rest are
tagged as ?accessory.? If a speech event is present,
the tags are flipped.
3.2.2 Verifiability-specific Features (VER)
Parts-of-Speech (POS) Count Rayson et
al. (2001) have shown that the POS distribution
is distinct in imaginative vs. informative writing.
We expect this feature to distinguish UNVERIF
propositions from the rest.
Sentiment Clue Count Wilson et al. (2005) pro-
vides a subjectivity clue lexicon, which is a list of
words with sentiment strength tags, either strong
or weak, along with additional information, such
as the sentiment polarity, Part-of-Speech Count
(POS), etc. We suspect that propositions contain-
ing more sentiment words is more likely to be UN-
VERIF.
Speech Event Count We use the 50 most frequent
Objective-speech-event text anchors crawled from
the MPQA 2.0 corpus (Wilson and Wiebe, 2005)
as a speech event lexicon. The speech event text
anchors refer to words like ?stated? and ?wrote?
that introduce written or spoken propositions at-
tributed to a source. propositions containing
speech events such as proposition 10 in Table 1
are generally VERIF
NON
or VERIF
EXP
, since
whether the attributed source has indeed made the
proposition he allegedly made is objectively veri-
fiable regardless of the subjectivity of the proposi-
tion itself.
Imperative Expression Count Imperatives, i.e.
commands, are generally UNVERIF (e.g. ?Do the
homework now!? that is, we expect there to be
no objective evidence proving that the homework
should be done right away.), unless the sentence
is a law or general procedure (e.g. ?The library
should allow you to check out books.? where the
33
context makes it clear that the writer is claiming
that the library lends out books.) This feature de-
notes whether the proposition begins with a verb
or contains the following: must, should, need to,
have to, ought to.
Emotion Expression Count These features tar-
get specific tokens ?!?, and ?...? as well as fully
capitalized word tokens to capture the emotion in
text. The rationale is that expression of emotion is
likely to be more prevalent in UNVERIF proposi-
tions.
3.2.3 Experientiality-specific Features (EXP)
Tense Count propositions written in past tense
are rarely VERIF
NON
, because even in the case
that the statment is verifiable, they are likely to be
the commenter?s past experience, i.e. VERIF
EXP
.
Future tense are typically UNVERIF because
propositions about what will happen in the fu-
ture are often unverifiable with objective evidence,
with exception being propositions like predictions
based on policy of organizations, i.e. ?Fedex will
deliver on Sunday.? To take advantage of these ob-
servations, three binary features capture each of
three tenses: past, present, and future.
Person Count First person narratives can suggest
that the proposition is UNVERIF or VERIF
EXP
,
except for rare cases like ?We, the passengers,...?
in which the first person pronoun refers to a large
body of individuals. This intuition is captured by
having binary features for: 1st, 2nd and 3rd per-
son.
4 Experiments
4.1 Methodology
A Note on Argument Detection A natural first
step in argumentation mining is to determine
which portions of the given document comprise
an argument. It can also be framed as a binary
classification task in which each proposition in the
document needs to be classified as either argumen-
tative or not. Some authors choose to skip this
step (Feng and Hirst, 2011), while others make
use of various classifiers to achieve high level of
accuracy, as Palau and Moens achieved over 70%
accuracy on Araucaria and ECHR corpus (Reed
and Moens, 2008; Palau and Moens, 2009).
As we have discussed in Section 1, our setup
is a bit unique in that we also consider implicit
arguments, where propositions are not supported
with explicit reason or evidence, as argumentative.
As a result, only about 7%(
NONARG
TOTAL
in Table 2) of
our entire dataset is marked as non-argumentative,
most of which consists of questions and greetings.
By simply searching for specific unigrams, such
as ??? and ?thank?, we achieve over 99% F
1
score
in determining which propositions are part of an
argument.
The remaining experiments were done without
non-argumentative propositions, i.e. NONARG in
Table 2.
Experimental Setup We first randomly selected
292 comments as held-out test set, resulting in the
distribution shown in Table 4. Then, VERIF
NON
and VERIF
EXP
in the training set were oversam-
pled so that the classes are equally distributed.
During training, five fold cross-validation was
done on the training set to tune the C parameter
to 32. Because the micro-averaged F
1
score can
be easily boosted on datasets with highly skewed
class distribution, we optimize for the macro-
averaged F
1
score.
Preprocessing was kept at a minimal level: cap-
ital letters were lowercased after counting fully
capitalized words, and numbers were converted to
a NUM token.
VERIF
NON
VERIF
EXP
UNVERIF Total
Train 987 900 4459 6346
Test 370 367 1687 2424
Total 1357 1267 6146 8770
Table 4: # of propositions in Train and Test Set
4.2 Results & Analysis
Table 3 shows a summary of the classification re-
sults. The best overall performance is achieved
by combining all features (UNI+BI+VER+EXP),
yielding 68.99% macro-averaged F
1
, where the
gain over the baseline is statistically significant
according to the bootstrap method with 10,000
samples (Efron and Tibshirani, 1994; Berg-
Kirkpatrick et al., 2012).
Core Clause Tag (CCT) We do not report the
performance of employing feature sets with Core
Clause Tag (CCT) in Table 3, because the effect
of CCT on each of the six sets of features is sta-
tistically insignificant. This is surprising at first,
given the strong motivation for distinguishing the
core clause from auxiliary clause, as addressed in
the previous section: Subordinate clauses like ?I
believe? should not cause the entire proposition to
be classified as UNVERIF, and clauses like ?He
said? should serve as a queue for VERIF
NON
or
VERIF
EXP
, even if an unverifiable clause follows
34
Feature Set
UNVERIF vs All VERIF
NON
vs All VERIF
EXP
vs All Average F
1
Pre. Rec. F
1
Pre. Rec. F
1
Pre. Rec. F
1
Macro Micro
UNI(base)
85.24 79.43 82.23 42.57 51.89 46.77 61.10 66.76 63.80 64.27 73.31
UNI+BI
82.14 89.69* 85.75* 51.67* 37.57 43.51 73.48* 62.67 67.65* 65.63 77.64*
VER
88.52* 52.10 65.60 28.41 61.35* 38.84 42.41 73.02* 53.65 52.70 56.68
EXP
82.42 4.45 8.44 20.92 76.49* 32.85 31.02 82.83* 45.14 28.81 27.31
VER+EXP
89.40* 49.50 63.72 29.25 71.62* 41.54 50.00 79.56* 61.41 55.55 57.43
UNI+BI+
VER+EXP
86.86* 83.05* 84.91* 49.88* 55.14 52.37* 66.67* 73.02* 69.70* 68.99* 77.27*
Table 3: Three class classification results in % (Crammer & Singer?s Multiclass SVMs)
Precision, recall, and F
1
scores are computed with respect to each one-vs-all classification problem for evaluation purposes,
though a single machine is built for the multi-class classification problem, instead of 3 one-vs-all classifiers. The star (*)
indicates that the given result is statistically significantly better than the unigram baseline.
Fts UNI UNI
CCT
U
N
V
E
R
I
F
+
should, whatever, respon-
sibility
should
C
, should
A
,
understand
C
-
previous, solve, florida,
exposed, reacted, reply,
kinds
exposed
C
, solve
C
,
NUM
C
, florida
C
,
reacted
C
, pool
C
, owed
C
V
E
R
I
F
N
O
N
+
impacted, NUM, solve,
cars, pull, kinds, congress
impacted
C
, solve
C
,
cars
C
, NUM
C
, pool
C
,
writing
C
, death
C
, link
C
-
should, seems, comments should
C
, comments
C
V
E
R
I
F
E
X
P
+
owed, consumed, saw, ex-
pert, interesting, him, re-
acted, refinance
owed
C
, consumed
C
,
expert
C
, reacted
C
,
happened
C
, interesting
C
-
impacted, wo impacted
C
, wo
C
,
concern
C
, died
C
Table 5: Most Informative Features for UNI and UNI
CCT
10 Unigrams with the largest weight (magnitude) with
respect to each class ( + : positive weight / - : negative
weight).
it. Our conjecture turned out to be wrong, mainly
because such distinction can be made for only a
small subset of the data: For instance, over 83%
of the unigrams are tagged as core in the UNI fea-
ture set. Thus, most of the important features for
feature sets with CCT end up being features with
core tag, and the important features for feature sets
with and without CCT are practically the same, as
shown in Table 5, resulting in statistically insignif-
icant performance differences.
Informative Features The most informative fea-
Feature Set UNI+BI+VER+EXP
UNVERIF
+
should,StrSentClue
>2
, VB
>2
-
StrSentClue
0
, VBD
>2
, air, since, no one, al-
lergic, not an
VERIF
NON
+
die, death, reaction, person, allergen, air-
borne, no one, allergies
-
PER
1st
, should
VERIF
EXP
+
VBD
>2
, PER
1st
, i have, his, he, him, time !
-
VBZ
>2
, PER
2nd
Table 6: Most Informative Features for UNI+BI+VER+EXP
10 Features with the largest weight (magnitude) with re-
spect to each class ( + : positive weight / - : negative
weight).
tures reported in Table 6 exhibit interesting differ-
ences among the three classes: Sentiment bearing
words, i.e. ?should? and strong sentiment clues,
are good indicators of UNVERIF, whereas person
and tense information is crucial for VERIF
EXP
.
As expected, the strong indicators of UNVERIF
and VERIF
EXP
, namely ?should? and PER
1st
are
negatively associated with VERIF
NON
. It is in-
triguing to see that the heavily weighted features
of VERIF
NON
are non-verb content words, unlike
those of the other classes. One explanation for this
is that VERIF
NON
are rarely indicated by specific
cues; instead, a good sign of VERIF
NON
is the
absences of cues for the other classes, which are
often function words and verbs. What is remain-
ing, then, are non-verb content words. Also, cer-
tain content words seem to be more likely to bring
about factual discussions. For instance, technical
terms like?allergen? and ?airborne,? appear in ver-
ifiable non-experiential propositions as ?The FDA
requires labeling for the following 8 allergens.?
Non-n-gram Features Table 3 clearly shows that
the three non-n-gram features, VER, EXP, and
VER+EXP, do not perform as well as the n-gram
features. But still, the performance is impressive,
given the drastic difference in the dimensionality
of the features: Even the combined feature set,
VER+EXP, consists of only about 100 features,
when there are over 8,000 unigrams and close to
70,000 bigrams. In other words, the non-n-gram
features are effectively capturing characteristics
of each class. This is very promising, since this
shows that a better understanding of the types of
proposition can potentially lead to a more concise
set of features with equal, or even better, perfor-
mance.
Also notice that VER outperforms EXP for the
most part, even with respect to VERIF
NON
vs All
and VERIF
EXP
vs All, except for recall. This is in-
35
triguing, because VER are mostly from subjectiv-
ity detection domain, intended to capture the sub-
jectivity of words in the propositions leveraging
on pre-built lexia. Simply considering subjectivity
of words should provide no means of distinguish-
ing VERIF
NON
from VERIF
EXP
. One of the rea-
sons for VER?s superior performance over EXP is
that EXP by itself is inadequate for the classifi-
cation task: EXP consists of only 6 (or 12 with
CCT) features denoting the person and tense infor-
mation. Another reason is that VER, in a limited
fashion, does encode experientiality: For instance,
past tense propositions can be identified with the
existence of VBD(verb, past tense) and VBN(verb,
past participle).
5 Related Work
Argumentation Mining The primary goal of ar-
gumentation mining has been to identify and ex-
tract argumentative structures present in docu-
ments, which are often written by profession-
als (Moens et al., 2007; Wyner et al., 2010; Feng
and Hirst, 2011; Ashley and Walker, 2013). In cer-
tain cases, the specific document structure allows
additional means of identify arguments (Mochales
and Moens, 2008). Even the work on online text
data, which are less rigid in structure and often
contain insufficiently supported propositions, fo-
cus on the extraction of arguments (Villalba and
Saint-Dizier, 2012; Cabrio and Villata, 2012). We,
however, are interested in the assessment of the
argumentative structure, potentially providing rec-
ommendations to readers and feedback to the writ-
ers. Thus it is crucial that we also process unsub-
stantiated propositions, which we consider as im-
plicit arguments. Our approach should be valu-
able for processing documents like online user
comment where arguments may not have adequate
support and an automatic means of analysis can be
useful.
Subjectivity Detection Work to distinguish sub-
jective from objective propositions (e.g.(Wiebe
and Riloff, 2005)), often a subtask for sentiment
analysis (Pang and Lee, 2008), is relevant to our
work since we are concerned with the objective
verifiability of propositions. In particular, previ-
ous work attempts to detect certain types of sub-
jective proposition: Conrad et al. (2012) iden-
tify arguing subjectivity propositions and tag them
with argument labels in order to cluster argument
paraphrases. Others incorporate this task as a com-
ponent for solving related problems, such as an-
swering opinion-based questions and determining
the writer?s political stance (Somasundaran et al.,
2007; Somasundaran and Wiebe, 2010). Similarly,
Rosenthal and McKeown (2012) identify opinion-
ated propositions expressing beliefs, leveraging
from previous work in sentiment analysis and be-
lief tagging. While the class of subjective propo-
sitions in subjectivity detection strictly contains
UNVERIF propositions, it also partially overlaps
with the VERIF
EXP
and VERIF
NON
classes of
our work: We want to identify verifiable assertions
within propositions, rather than determine the sub-
jectivity of the proposition as a whole (e.g. propo-
sition 8 in Table 1 is classified as a VERIF
NON
,
though ?Clearly? is subjective.). We also distin-
guish two types of verifiable propositions, which
is necessary for the purpose of identifying appro-
priate types of support.
6 Conclusions and Future Work
We have proposed a novel task of automatically
classifying each proposition as UNVERIFIABLE,
VERIFIABLE NONEXPERIENTIAL, or VERIFI-
ABLE EXPERIENTIAL, where the appropriate type
of support is reason, evidence, and optional evi-
dence, respectively. This classification, once the
existing support relations among propositions are
identified, can provide an estimate of how well the
arguments are supported. We find that Support
Vector Machines (SVM) classifiers trained with
n-grams and other features to capture the verifi-
ability and experientiality exhibit statistically sig-
nificant improvement over the unigram baseline,
achieving a macro-averaged F
1
score of 68.99%.
In the process, we have built a gold-standard
dataset of 9,476 propositions from 1,047 com-
ments submitted to an eRulemaking platform.
One immediate avenue for future work is to in-
corporate the identification of relations among the
propositions in an argument to the system to ana-
lyze the adequacy of the supporting information in
the argument. This, in turn, can be used to recom-
mend comments to readers and provide feedback
to writers so that they can construct better argu-
ments.
Acknowledgments
This work was supported in part by NSF grants
IIS-1111176 and IIS?1314778. We thank our
annotators, Pamela Ijeoma Amaechi and Simon
Boehme, as well as the Cornell NLP Group and
the reviewers for helpful comments.
36
References
Kevin D. Ashley and Vern R. Walker. 2013. From in-
formation retrieval (ir) to argument retrieval (ar) for
legal cases: Report on a baseline study. In JURIX,
pages 29?38.
Taylor Berg-Kirkpatrick, David Burkett, and Dan
Klein. 2012. An empirical investigation of statis-
tical significance in nlp. In Proceedings of the 2012
Joint Conference on Empirical Methods in Natu-
ral Language Processing and Computational Natu-
ral Language Learning, EMNLP-CoNLL ?12, pages
995?1005, Stroudsburg, PA, USA. Association for
Computational Linguistics.
Elena Cabrio and Serena Villata. 2012. Combin-
ing textual entailment and argumentation theory for
supporting online debates interactions. In Proceed-
ings of the 50th Annual Meeting of the Association
for Computational Linguistics (Volume 2: Short Pa-
pers), pages 208?212, Jeju Island, Korea, July. As-
sociation for Computational Linguistics.
Alexander Conrad, Janyce Wiebe, Hwa, and Rebecca.
2012. Recognizing arguing subjectivity and argu-
ment tags. In Proceedings of the Workshop on
Extra-Propositional Aspects of Meaning in Com-
putational Linguistics, ExProM ?12, pages 80?88,
Stroudsburg, PA, USA. Association for Computa-
tional Linguistics.
Koby Crammer and Yoram Singer. 2002. On the algo-
rithmic implementation of multiclass kernel-based
vector machines. J. Mach. Learn. Res., 2:265?292,
March.
Marie-Catherine De Marneffe, Bill Maccartney, and
Christopher D. Manning. 2006. Generating typed
dependency parses from phrase structure parses. In
In Proc. Intl Conf. on Language Resources and Eval-
uation (LREC, pages 449?454.
B. Efron and R.J. Tibshirani. 1994. An Introduction to
the Bootstrap. Chapman & Hall/CRC Monographs
on Statistics & Applied Probability. Taylor & Fran-
cis.
Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh, Xiang-
Rui Wang, and Chih-Jen Lin. 2008. Liblinear: A li-
brary for large linear classification. J. Mach. Learn.
Res., 9:1871?1874, June.
Vanessa Wei Feng and Graeme Hirst. 2011. Classi-
fying arguments by scheme. In Proceedings of the
49th Annual Meeting of the Association for Com-
putational Linguistics: Human Language Technolo-
gies - Volume 1, HLT ?11, pages 987?996, Strouds-
burg, PA, USA. Association for Computational Lin-
guistics.
Jennifer L. Hochschild and Michael Danielson, 1998.
Can We Desegregate Public Schools and Subsi-
dized Housing? Lessons from the Sorry History of
Yonkers, New York, chapter 2, pages 23?44. Uni-
versity Press of Kansas, Lawrence KS, edited by
clarence stone edition.
Chih-Wei Hsu and Chih-Jen Lin. 2002. A comparison
of methods for multiclass support vector machines.
Trans. Neur. Netw., 13(2):415?425, March.
S. Sathiya Keerthi, S. Sundararajan, Kai-Wei Chang,
Cho-Jui Hsieh, and Chih-Jen Lin. 2008. A sequen-
tial dual method for large scale multi-class linear
svms. In Proceedings of the 14th ACM SIGKDD in-
ternational conference on Knowledge discovery and
data mining, KDD ?08, pages 408?416, New York,
NY, USA. ACM.
Jeffrey S. Lubbers. 2006. A Guide to Federal Agency
Rulemaking. American Bar Association Chicago,
4th ed. edition.
Mitchell P. Marcus, Beatrice Santorini, and Mary Ann
Marcinkiewicz. 1993. Building a large annotated
corpus of english: The penn treebank. COMPUTA-
TIONAL LINGUISTICS, 19(2):313?330.
Raquel Mochales and Marie-Francine Moens. 2008.
Study on the structure of argumentation in case law.
In Proceedings of the 2008 Conference on Legal
Knowledge and Information Systems: JURIX 2008:
The Twenty-First Annual Conference, pages 11?20,
Amsterdam, The Netherlands, The Netherlands. IOS
Press.
Marie-Francine Moens, Erik Boiy, Raquel Mochales
Palau, and Chris Reed. 2007. Automatic detection
of arguments in legal texts. In Proceedings of the
11th International Conference on Artificial Intelli-
gence and Law, ICAIL ?07, pages 225?230, New
York, NY, USA. ACM.
Tim O?Keefe and Irena Koprinska. 2009. Feature se-
lection and weighting methods in sentiment analy-
sis. In Proceedings of the 14th Australasian Docu-
ment Computing Symposium.
Raquel Mochales Palau and Marie-Francine Moens.
2009. Argumentation mining: The detection, classi-
fication and structure of arguments in text. In Pro-
ceedings of the 12th International Conference on Ar-
tificial Intelligence and Law, ICAIL ?09, pages 98?
107, New York, NY, USA. ACM.
Bo Pang and Lillian Lee. 2008. Opinion mining and
sentiment analysis. Found. Trends Inf. Retr., 2(1-
2):1?135, January.
Joonsuk Park, Sally Klingel, Claire Cardie, Mary
Newhart, Cynthia Farina, and Joan-Josep Vallb?e.
2012. Facilitative moderation for online participa-
tion in erulemaking. In Proceedings of the 13th An-
nual International Conference on Digital Govern-
ment Research, dg.o ?12, pages 173?182, New York,
NY, USA. ACM.
John L. Pollock. 1987. Defeasible reasoning. Cogni-
tive Science, 11:481?518.
Paul Rayson, Andrew Wilson, and Geoffrey Leech.
2001. Grammatical word class variation within the
british national corpus sampler. Language and Com-
puters.
37
Raquel Mochales Palau Rowe Glenn Reed, Chris and
Marie-Francine Moens. 2008. Language resources
for studying argument. In Proceedings of the 6th
conference on language resources and evaluation -
LREC 2008, pages 91?100. ELRA.
Ellen Riloff and Jay Shoen. 1995. Automatically
acquiring conceptual patterns without an annotated
corpus. In In Proceedings of the Third Workshop on
Very Large Corpora, pages 148?161.
Sara Rosenthal and Kathleen McKeown. 2012. De-
tecting opinionated claims in online discussions. In
ICSC, pages 30?37.
Swapna Somasundaran and Janyce Wiebe. 2010. Rec-
ognizing stances in ideological on-line debates. In
Proceedings of the NAACL HLT 2010 Workshop on
Computational Approaches to Analysis and Gener-
ation of Emotion in Text, CAAGET ?10, pages 116?
124, Stroudsburg, PA, USA. Association for Com-
putational Linguistics.
Swapna Somasundaran, Josef Ruppenhofer, and Janyce
Wiebe. 2007. Detecting arguing and sentiment in
meetings. In Proceedings of the SIGdial Workshop
on Discourse and Dialogue.
Stephen E. Toulmin, Richard Rieke, and Allan Janik.
1979. An Introduction to Reasoning. Macmillan
Publishing Company.
S.E. Toulmin. 1958. The Uses of Argument. Cam-
bridge University Press.
Maria Paz Garcia Villalba and Patrick Saint-Dizier.
2012. Some facets of argument mining for opinion
analysis. In COMMA, pages 23?34.
Janyce Wiebe and Ellen Riloff. 2005. Creating subjec-
tive and objective sentence classifiers from unanno-
tated texts. In In CICLing2005, pages 486?497.
Theresa Wilson and Janyce Wiebe. 2005. Annotat-
ing attributions and private states. In Proceedings of
ACL Workshop on Frontiers in Corpus Annotation
II: Pie in the Sky.
Theresa Wilson. 2005. Recognizing contextual po-
larity in phrase-level sentiment analysis. In In Pro-
ceedings of HLT-EMNLP, pages 347?354.
Adam Wyner, Raquel Mochales-Palau, Marie-Francine
Moens, and David Milward. 2010. Semantic pro-
cessing of legal texts. chapter Approaches to Text
Mining Arguments from Legal Cases, pages 60?79.
Springer-Verlag, Berlin, Heidelberg.
38
Proceedings of the ACL 2014 Workshop on Language Technologies and Computational Social Science, pages 5?7,
Baltimore, Maryland, USA, June 26, 2014.
c
?2014 Association for Computational Linguistics
Overview of the 2014 NLP Unshared Task in PoliInformatics
Noah A. Smith
?
Claire Cardie
?
Anne L. Washington
?
John D. Wilkerson
?
?
School of Computer Science, Carnegie Mellon University, Pittsburgh, PA 15213, USA
?
Department of Computer Science, Cornell University, Ithaca, NY 14853, USA
?
School of Public Policy, George Mason University, Arlington, VA 22201, USA
?
Department of Political Science, University of Washington, Seattle, WA 98195, USA
?
Corresponding author: nasmith@cs.cmu.edu
Abstract
We describe a research activity carried
out during January?April 2014, seeking to
increase engagement between the natural
language processing research community
and social science scholars. In this activ-
ity, participants were offered a corpus of
text relevant to the 2007?8 financial cri-
sis and an open-ended prompt. Their re-
sponses took the form of a short paper
and an optional demonstration, to which a
panel of judges will respond with the goal
of identifying efforts with the greatest po-
tential for future interdisciplinary collabo-
ration.
1 Introduction
In recent years, numerous interdisciplinary re-
search meetings have sought to bring together
computer scientists with expertise in automated
text data analysis and scholars with substan-
tive interests that might make use of text data.
The latter group has included political scientists,
economists, and communications scholars. An
NSF Research Coordination Network grant to en-
courage research using open government data was
awarded to co-authors Washington and Wilker-
son in 2013. The network for Political Informat-
ics, or PoliInformatics, brought together a steering
committee from diverse research backgrounds that
convened in February 2013. At that meeting, a
substantive focus on the 2007?8 nancial crisis was
selected.
Drawing inspiration from the ?shared task?
model that has been successful in the natural lan-
guage processing community, we designed a re-
search competition for computer scientists. In a
shared task, a gold-standard dataset is created in
advance of the competition, inputs and outputs are
defined by the organizers, typically creating a su-
pervised learning setup with held-out data used for
evaluation. Constraints on the resources that may
be used are typically set in place as well, to fo-
cus the energies of participants on a core prob-
lem, and the official evaluation scores are pub-
lished, usually as open-source software. Final sys-
tems (or system output) is submitted by a dead-
line and judged automatically against the gold-
standard. Participants report on their systems in
short papers, typically presented at a meeting as-
sociated with a conference or workshop.
With neither a clear definition of what the fi-
nal outcome might be, nor the resources to create
the necessary gold-standard data, we developed a
more open-ended competition. A text corpus was
collected and made available, and a prompt was
offered. Participants were given freedom in how
to respond; competition entries took the form of
short research papers and optional demonstrations
of the results of the projects. Rather than an ob-
jective score, a panel of judges organized by the
PoliInformatics steering committee offered public
reviews of the work, with an emphasis on poten-
tial for future interdisciplinary research efforts that
might stem from these preliminary projects.
2 Setup
The prompts offered to participants were:
Who was the financial crisis? We seek to un-
derstand the participants in the lawmaking and
regulatory processes that formed the government?s
response to the crisis: the individuals, indus-
tries, and professionals targeted by those poli-
cies; the agencies and organizations responsi-
ble for implementing them; and the lobbyists,
witnesses, advocates, and politicians who were
actively involved?and the connections among
them.
What was the financial crisis? We seek to un-
derstand the cause(s) of the crisis, proposals for
reform, advocates for those proposals, arguments
5
for and against, policies ultimately adopted by the
government, and the impact of those policies.
The set of datasets made available is listed in
Table 1. Several additional datasets were sug-
gested on the website,
1
but were not part of the
official data.
3 Response
Forty teams initially registered to participate in the
unshared task; ten submitted papers. The teams
came from a variety of institutions spread across
six countries. Half of the teams included links to
online demonstrations or browsable system out-
put. At this writing, the papers are under review
by the panel of judges. We provide a very brief
summary of the contributions of each team.
3.1 Who was the financial crisis?
Bordea et al. (2014) inferred importance and hi-
erarchy of topics along with expertise mining to
find which participants in the discourse might be
experts (e.g., Paul Volcker and ?proprietary trad-
ing?) based on FOMC, FCIC, and Congressional
hearing and report data.
Baerg et al. (2014) considered transcripts of the
FOMC, developing a method for scaling the pref-
erences of its members with respect to inflation
(hawks to doves); the method incorporates auto-
matic dimensionality reduction and expert topic
interpretation.
Zirn et al. (2014) also focused on the transcripts,
distinguishing between position-taking statements
and shorter ?discussion elements? that express
agreement or disagreement rather than substance,
and used this analysis to quantify similarity among
FOMC members and take first steps toward extrac-
tion of sub-dialogues among them.
Bourreau and Poibeau (2014) focused on the
FCIC report and the two Congressional reports,
identifying named entities and then visualizing
correlations among mentions both statically (as
networks) and dynamically. Clark et al. (2014)
considered Congressional hearings, applying a
reasoning model that integrates analysis of social
roles and relationships with analysis of individ-
ual beliefs in hope of detecting opinion shifts and
signs of influence.
With an eye toward substantive hypotheses
about dependencies among banks? access to
1
https://sites.google.com/site/
unsharedtask2014
bailout funds relating to underlying social con-
nections, Morales et al. (2014) automatically ex-
tracted a social network from the corpus alongside
structured data in Freebase.
3.2 What was the financial crisis?
Miller and McCoy (2014) considered FOMC tran-
scripts, applying topic models for dimensionality
reduction and viewing topic proportions as time
series.
In a study of the TARP, Dodd-Frank, and the
health reform bills, Li et al. (2014) explored the
ideas expressed in those bills, applying models of
text reuse from bills introduced in the 110th and
111th Congresses.
Wang et al. (2014) implemented a query-
focused summarization system for FOMC and
FCIC meeting transcripts and Congressional hear-
ings, incorporating topic and expertise measures
into the score, and queried the corpus with candi-
date causes for the crisis, derived from Wikipedia
(e.g., ?subprime lending? and ?growth housing
bubble?).
Kleinnijenhuis et al. (2014) considered Con-
gressional hearings alongside news text from the
United States and the United Kingdom, carrying
out keyword analysis to compare and measure di-
rectional effects between the two, on different di-
mensions.
4 Conclusion
The unshared task was successful in attracting the
interest of forty participants working on ten teams.
A highly diverse range of activities ensued, each
of which is being reviewed at this writing by a
panel of judges. Reviews and final outcomes will
be posted at the https://sites.google.
com/site/unsharedtask2014 as soon as
they are available, and a presentation summariz-
ing the competition will be part of the ACL 2014
Workshop on Language Technologies and Com-
putational Social Science.
Acknowledgments
We thank the participants and judges for their time
and effort. This activity was supported in part by
NSF grants 1243917 and 1054319.
6
? Federal Open Market Committee (FOMC):
? Meeting transcripts are only made available five years after each meeting date. (The 2008 transcripts came available
around the time of the activity and were kindly made available by participant William Li.)
? Meeting minutes are available for all meetings to date.
? Federal Crisis Inquiry Commission (FCIC; an independent commission created by Congress to investigate the causes of
the crisis):
? Report
? Transcript of the first public hearing
? Congressional reports:
? Senate Committee on Homeland Security and Governmental Affairs: ?Wall Street and the financial crisis: anatomy
of a financial collapse?
? House Committee on Financial Services: ?The stock market plunge: what happened and what is next??
? Congressional bills:
? Troubled Assets Relief Program, 2008 (TARP)
? Dodd-Frank Wall Street Reform and Consumer Protection Act (2010)
? American Recovery and Reinvestment Act of 2009 (Stimulus)
? Housing and Economic Recovery Act of 2008
? Public Company Accounting Reform and Investor Protection Act of 2002 (Sarbanes-Oxley)
? Financial Services Modernization Act of 1999 (Gramm-Leach-Bliley)
? In addition to the above financial reform bills, the text of all versions of all Congressional bills introduced in the
110th and 111th Congresses
? Congressional hearings, segmented into turns:
? Monetary policy (26)
? TARP (12)
? Dodd-Frank (61)
? Other selected committee hearings relating to financial reform (15)
Table 1: Text datasets made available to unshared task participants. These can be downloaded at https://sites.google.
com/site/unsharedtask2014.
References
Nicole Rae Baerg, Will Lowe, Simone Ponzetto,
Heiner Stuckenschmidt, and C?acilia Zirn. 2014. Es-
timating central bank preferences.
Georgeta Bordea, Kartik Asooja, Paul Buitelaar, and
Leona O?Brien. 2014. Gaining insights into the
global financial crisis using Saffron.
Pierre Bourreau and Thierry Poibeau. 2014. Map-
ping the economic crisis: Some preliminary inves-
tigations.
Micah Clark, Adam Dalton, Tomas By, Yorick Wilks,
Samira Shaikh, Ching-Sheng Lin, and Tomek Strza-
lkowski. 2014. Influence and belief in Congres-
sional hearings.
Jan Kleinnijenhuis, Wouter van Atteveldt, and Antske
Fokkens. 2014. Chicken or egg? the reciprocal in-
fluence of press and politics.
William P. Li, David Larochelle, and Andrew W. Lo.
2014. Estimating policy trajectories during the fi-
nancial crisis.
John E. Miller and Kathleen F. McCoy. 2014. Chang-
ing focus of the FOMC through the financial crisis.
Michelle Morales, David Brizan, Hussein Ghaly,
Thomas Hauner, Min Ma, and Andrew Rosenberg.
2014. Application of social network analysis in the
estimation of bank financial strength during the fi-
nancial crisis.
Lu Wang, Parvaz Mahdabi, Joonsuk Park, Dinesh Pu-
ranam, Bishan Yang, and Claire Cardie. 2014.
Cornell expert aided query-focused summarization
(CEAQS): A summarization framework to PoliIn-
formatics.
C?acilia Zirn, Michael Sch?afer, Simone Paolo Ponzetto,
and Michael Strube. 2014. Exploring structural fea-
tures for position analysis in political discussions.
7
Proceedings of the 5th Workshop on Computational Approaches to Subjectivity, Sentiment and Social Media Analysis, pages 97?106,
Baltimore, Maryland, USA. June 27, 2014.
c?2014 Association for Computational Linguistics
Improving Agreement and Disagreement Identification
in Online Discussions with A Socially-Tuned Sentiment Lexicon
Lu Wang
Department of Computer Science
Cornell University
Ithaca, NY 14853
luwang@cs.cornell.edu
Claire Cardie
Department of Computer Science
Cornell University
Ithaca, NY 14853
cardie@cs.cornell.edu
Abstract
We study the problem of agreement and
disagreement detection in online discus-
sions. An isotonic Conditional Random
Fields (isotonic CRF) based sequential
model is proposed to make predictions
on sentence- or segment-level. We auto-
matically construct a socially-tuned lex-
icon that is bootstrapped from existing
general-purpose sentiment lexicons to fur-
ther improve the performance. We eval-
uate our agreement and disagreement tag-
ging model on two disparate online discus-
sion corpora ? Wikipedia Talk pages and
online debates. Our model is shown to
outperform the state-of-the-art approaches
in both datasets. For example, the iso-
tonic CRF model achieves F1 scores of
0.74 and 0.67 for agreement and disagree-
ment detection, when a linear chain CRF
obtains 0.58 and 0.56 for the discussions
on Wikipedia Talk pages.
1 Introduction
We are in an era where people can easily voice and
exchange their opinions on the internet through
forums or social media. Mining public opinion
and the social interactions from online discus-
sions is an important task, which has a wide range
of applications. For example, by analyzing the
users? attitude in forum posts on social and po-
litical problems, it is able to identify ideological
stance (Somasundaran and Wiebe, 2009) and user
relations (Qiu et al., 2013), and thus further dis-
cover subgroups (Hassan et al., 2012; Abu-Jbara
et al., 2012) with similar ideological viewpoint.
Meanwhile, catching the sentiment in the conver-
sation can help detect online disputes, reveal popu-
lar or controversial topics, and potentially disclose
the public opinion formation process.
In this work, we study the problem of agreement
and disagreement identification in online discus-
sions. Sentence-level agreement and disagreement
detection for this domain is challenging in its own
right due to the dynamic nature of online conversa-
tions, and the less formal, and usually very emo-
tional language used. As an example, consider a
snippet of discussion from Wikipedia Talk page
for article ?Iraq War? where editors argue on the
correctness of the information in the opening para-
graph (Figure 1). ?So what?? should presumably
be tagged as a negative sentence as should the sen-
tence ?If you?re going to troll, do us all a favor
and stick to the guidelines.?. We hypothesize that
these, and other, examples will be difficult for the
tagger unless the context surrounding each sen-
tence is considered and in the absence of a sen-
timent lexicon tuned for conversational text (Ding
et al., 2008; Choi and Cardie, 2009).
As a result, we investigate isotonic Condi-
tional Random Fields (isotonic CRF) (Mao and
Lebanon, 2007) for the sentiment tagging task
since they preserve the advantages of the popu-
lar CRF sequential tagging models (Lafferty et
al., 2001) while providing an efficient mechanism
to encode domain knowledge ? in our case, a
sentiment lexicon ? through isotonic constraints
on the model parameters. In particular, we boot-
strap the construction of a sentiment lexicon from
Wikipedia talk pages using the lexical items in ex-
isting general-purpose sentiment lexicons as seeds
and in conjunction with an existing label propaga-
tion algorithm (Zhu and Ghahramani, 2002).
1
To summarize, our chief contributions include:
(1) We propose an agreement and disagree-
ment identification model based on isotonic Con-
ditional Random Fields (Mao and Lebanon, 2007)
to identify users? attitude in online discussion.
Our predictions that are made on the sentence-
1
Our online discussion lexicon (Section 4) will be made
publicly available.
97
Zer0faults: So questions comments feedback welcome.
Other views etc. I just hope we can remove the assertations
that WMD?s were in fact the sole reason for the US invasion,
considering that HJ Res 114 covers many many reasons.
>Mr. Tibbs: So basically what you want to do is remove all
mention of the cassus belli of the Iraq War and try to create
the false impression that this military action was as inevitable
as the sunrise.
[NN ]
No. Just because things didn?t turn out the
way the Bush administration wanted doesn?t give you license
to rewrite history.
[NN ]
...
>>MONGO: Regardless, the article is an antiwar propa-
ganda tool.
[NN ]
...
>>>Mr. Tibbs: So what?
[NN ]
That wasn?t the cassus
belli and trying to give that impression After the Fact is
Untrue.
[NN ]
Hell, the reason it wasn?t the cassus belli is be-
cause there are dictators in Africa that make Saddam look like
a pussycat...
>>Haizum: Start using the proper format or it?s over for your
comments.
[N ]
If you?re going to troll, do us all a favor and
stick to the guidelines.
[N ]
...
Tmorton166: Hi, I wonder if, as an outsider to this debate I
can put my word in here. I considered mediating this discus-
sion however I?d prefer just to comment and leave it at that :).
I agree mostly with what Zer0faults is saying
[PP ]
. ...
>Mr. Tibbs: Here?s the problem with that.
[NN ]
It?s not about
publicity or press coverage. It?s about the fact that the Iraq
disarmament crisis set off the 2003 Invasion of Iraq. ... And
theres a huge problem with rewriting the intro as if the Iraq
disarmament crisis never happened.
[NN ]
>>Tmorton166: ... To suggest in the opening paragraph that
the ONLY reason for the war was WMD?s is wrong - because
it simply isn?t.
[NN ]
However I agree that the emphasis needs
to be on the armaments crisis because it was the reason sold
to the public and the major one used to justify the invasion but
it needs to acknowledge that there was at least 12 reasons for
the war as well.
[PP ]
...
Figure 1: Example discussion from wikipedia talk page
for article ?Iraq War?, where editors discuss about the cor-
rectness of the information in the opening paragraph. We
only show some sentences that are relevant for demonstra-
tion. Other sentences are omitted by ellipsis. Names of ed-
itors are in bold. ?>? is an indicator for the reply structure,
where turns starting with > are response for most previous
turn that with one less >. We use ?NN?, ?N?, and ?PP? to in-
dicate ?strongly disagree?, ?disagree?, and ?strongly agree?.
Sentences in blue are examples whose sentiment is hard to
detect by an existing lexicon.
or segment-level, are able to discover fine-grained
sentiment flow within each turn, which can be fur-
ther applied in other applications, such as dispute
detection or argumentation structure analysis. We
employ two existing online discussion data sets:
the Authority and Alignment in Wikipedia Dis-
cussions (AAWD) corpus of Bender et al. (2011)
(Wikipedia talk pages) and the Internet Argu-
ment Corpus (IAC) of Walker et al. (2012a). Ex-
perimental results show that our model signifi-
cantly outperforms state-of-the-art methods on the
AAWD data (our F1 scores are 0.74 and 0.67 for
agreement and disagreement, vs. 0.58 and 0.56 for
the linear chain CRF approach) and IAC data (our
F1 scores are 0.61 and 0.78 for agreement and dis-
agreement, vs. 0.28 and 0.73 for SVM).
(2) Furthermore, we construct a new senti-
ment lexicon for online discussion. We show
that the learned lexicon significantly improves per-
formance over systems that use existing general-
purpose lexicons (i.e. MPQA lexicon (Wilson et
al., 2005), General Inquirer (Stone et al., 1966),
and SentiWordNet (Esuli and Sebastiani, 2006)).
Our lexicon is constructed from a very large-scale
discussion corpus based on Wikipedia talk page,
where previous work (Somasundaran and Wiebe,
2010) for constructing online discussion lexicon
relies on human annotations derived from limited
number of conversations.
In the remainder of the paper, we describe first
the related work (Section 2). Then we intro-
duce the sentence-level agreement and disagree-
ment identification model (Section 3) as well as
the label propagation algorithm for lexicon con-
struction (Section 4). After explain the experimen-
tal setup, we display the results and provide further
analysis in Section 6.
2 Related Work
Sentiment analysis has been utilized as a key en-
abling technique in a number of conversation-
based applications. Previous work mainly stud-
ies the attitudes in spoken meetings (Galley et al.,
2004; Hahn et al., 2006) or broadcast conversa-
tions (Wang et al., 2011) using Conditional Ran-
dom Fields (CRF) (Lafferty et al., 2001). Galley
et al. (2004) employ Conditional Markov models
to detect if discussants reach at an agreement in
spoken meetings. Each state in their model is an
individual turn and prediction is made on the turn-
level. In the same spirit, Wang et al. (2011) also
propose a sequential model based on CRF for de-
tecting agreements and disagreements in broadcast
conversations, where they primarily show the ef-
ficiency of prosodic features. While we also ex-
ploit a sequential model extended from CRFs, our
predictions are made for each sentence or segment
rather than at the turn-level. Moreover, we experi-
ment with online discussion datasets that exhibit
a more realistic distribution of disagreement vs.
agreement, where much more disagreement is ob-
served due to its function and the relation between
the participants. This renders the detection prob-
lem more challenging.
Only recently, agreement and disagreement de-
tection is studied for online discussion, especially
98
for online debate. Abbott et al. (2011) investi-
gate different types of features based on depen-
dency relations as well as manually-labeled fea-
tures, such as if the participants are nice, nasty,
or sarcastic, and respect or insult the target par-
ticipants. Automatically inducing those features
from human annotation are challenging itself, so
it would be difficult to reproduce their work on
new datasets. We use only automatically gener-
ated features. Using the same dataset, Misra and
Walker (2013) study the effectiveness of topic-
independent features, e.g. discourse cues indicat-
ing agreement or negative opinion. Those cues,
which serve a similar purpose as a sentiment lex-
icon, are also constructed manually. In our work,
we create an online discussion lexicon automat-
ically and construct sentiment features based on
the lexicon. Also targeting online debate, Yin et
al. (2012) train a logistic regression classifier with
features aggregating posts from the same partici-
pant to predict the sentiment for each individual
post. This approach works only when the speaker
has enough posts on each topic, which is not ap-
plicable to newcomers. Hassan et al. (2010) focus
on predicting the attitude of participants towards
each other. They relate the sentiment words to
the second person pronoun, which produces strong
baselines. We also adopt their baselines in our
work. Although there are available datasets with
(dis)agreement annotated on Wikipedia talk pages,
we are not aware of any published work that uti-
lizes these annotations. Dialogue act recognition
on talk pages (Ferschke et al., 2012) might be the
most related.
While detecting agreement and disagreement in
conversations is useful on its own, it is also a key
component for related tasks, such as stance pre-
diction (Thomas et al., 2006; Somasundaran and
Wiebe, 2009; Walker et al., 2012b) and subgroup
detection (Hassan et al., 2012; Abu-Jbara et al.,
2012). For instance, Thomas et al. (2006) train an
agreement detection classifier with Support Vec-
tor Machines on congressional floor-debate tran-
scripts to determine whether the speeches repre-
sent support of or opposition to the proposed leg-
islation. Somasundaran and Wiebe (2009) design
various sentiment constraints for inclusion in an
integer linear programming framework for stance
classification. For subgroup detection, Abu-Jbara
et al. (2012) uses the polarity of the expressions in
the discussions and partition discussants into sub-
groups based on the intuition that people in the
same group should mostly agree with each other.
Though those work highly relies on the compo-
nent of agreement and disagreement detection, the
evaluation is always performed on the ultimate ap-
plication only.
3 The Model
We first give a brief overview on isotonic Con-
ditional Random Fields (isotonic CRF) (Mao and
Lebanon, 2007), which is used as the backbone
approach for our sentence- or segment-level agree-
ment and disagreement detection model. We defer
the explanation of online discussion lexicon con-
struction in Section 4.
3.1 Problem Description
Consider a discussion comprised of sequential
turns uttered by the participants; each turn con-
sists of a sequence of text units, where each unit
can be a sentence or a segment of several sen-
tences. Our model takes as input the text units
x = {x
1
, ? ? ? , x
n
} in the same turn, and outputs
a sequence of sentiment labels y = {y
1
, ? ? ? , y
n
},
where y
i
? O,O = {NN,N,O,P,PP}. The la-
bels in O represent strongly disagree (NN), dis-
agree (N), neutral (O), agree (P), strongly agree
(PP), respectively. In addition, elements in the
partially ordered set O possess an ordinal relation
?. Here, we differentiate agreement and disagree-
ment with different intensity, because the output
of our classifier can be used for other applications,
such as dispute detection, where ?strongly dis-
agree? (e.g. NN) plays an important role. Mean-
while, fine-grained sentiment labels potentially
provide richer context information for the sequen-
tial model employed for this task.
3.2 Isotonic Conditional Random Fields
Conditional Random Fields (CRF) have been suc-
cessfully applied in numerous sequential labeling
tasks (Lafferty et al., 2001). Given a sequence
of utterances or segments x = {x
1
, ? ? ? , x
n
}, ac-
cording to linear-chain CRF, the probability of the
labels y for x is given by:
p(y|x) =
1
Z(x)
exp(
?
i
?
?,?
?
??,??
f
??,??
(y
i?1
, y
i
)
+
?
i
?
?,w
?
??,w?
g
??,w?
(y
i
, x
i
))
(1)
99
f??,??
(y
i?1
, y
i
) and g
??,w?
(y
i
, x
i
) are feature
functions. Given that y
i?1
, y
i
, x
i
take values of
?, ?, w, the functions are indexed by pairs ??, ??
and ??,w?. ?
??,??
, ?
??,w?
are the parameters.
CRF, as defined above, is not appropriate for or-
dinal data like sentiment, because it ignores the
ordinal relation among sentiment labels. Isotonic
Conditional Random Fields (isotonic CRF) are
proposed by Mao and Lebanon (2007) to enforce a
set of monotonicity constraints on the parameters
that are consistent with the ordinal structure and
domain knowledge (in our case, a sentiment lexi-
con automatically constructed from online discus-
sions).
Given a lexiconM = M
p
?M
n
, whereM
p
and M
n
are two sets of features (usually words)
identified as strongly associated with positive sen-
timent and negative sentiment. The constraints are
encoded as below. For each feature w ? M
p
, iso-
tonic CRF enforces ? ? ?
?
? ?
??,w?
? ?
??
?
,w?
.
Intuitively, the parameters ?
??,w?
are intimately
tied to the model probabilities. When a feature
such as ?totally agree? is observed in the training
data, the feature parameter for ?
?PP,totally agree?
is
likely to increase. Similar constraints are also de-
fined onM
n
. In this work, we boostrap the con-
struction of an online discussion sentiment lexicon
used asM in the isotonic CRF (see Section 4).
The parameters can be found by maximizing the
likelihood subject to the monotonicity constraints.
We adopt the re-parameterization from Mao and
Lebanon (2007) for a simpler optimization prob-
lem, and refer the readers to Mao and Lebanon
(2007) for more details.
2
3.3 Features
The features used in sentiment prediction are listed
in Table 1. Features with numerical values are first
normalized by standardization, then binned into 5
categories.
Syntactic/Semantic Features. Dependency re-
lations have been shown to be effective for various
sentiment prediction tasks (Joshi and Penstein-
Ros?e, 2009; Somasundaran and Wiebe, 2009;
Hassan et al., 2010; Abu-Jbara et al., 2012). We
have two versions of dependency relation as fea-
tures, one being the original form, another gen-
2
The full implementation is based on MALLET (McCal-
lum, 2002). We thank Yi Mao for sharing the implementation
of the core learning algorithm.
Lexical Features
- unigram/bigram
- num of words all uppercased
- num of words
Discourse Features
- initial uni-/bi-/trigram
- repeated punctuations
- hedging (Farkas et al., 2010)
- number of negators
Syntactic/Semantic Features
- unigram with POS tag
- dependency relation
Conversation Features
- quote overlap with target
- TFIDF similarity with target (remove quote first)
Sentiment Features
- connective + sentiment words
- sentiment dependency relation
- sentiment words
Table 1: Features used in sentiment prediction.
eralizing a word to its POS tag in turn. For in-
stance, ?nsubj(wrong, you)? is generlized as the
?nsubj(ADJ, you)? and ?nsubj(wrong, PRP)?. We
use Stanford parser (de Marneffe et al., 2006) to
obtain parse trees and dependency relations.
Discourse Features. Previous work (Hirschberg
and Litman, 1993; Abbott et al., 2011) suggests
that discourse markers, such as what?, actually,
may have their use for expressing opinions. We
extract the initial unigram, bigram, and trigram of
each utterance as discourse features (Hirschberg
and Litman, 1993). Hedge words are collected
from the CoNLL-2012 shared task (Farkas et al.,
2010).
Conversation Features. Conversation features
encode some useful information regarding the
similarity between the current utterance(s) and the
sentences uttered by the target participant. TFIDF
similarity is computed. We also check if the cur-
rent utterance(s) quotes target sentences and com-
pute its length.
Sentiment Features. We gather connectives
from Penn Discourse TreeBank (Rashmi Prasad
and Webber, 2008) and combine them with any
sentiment word that precedes or follows it as
new features. Sentiment dependency relations are
the subset of dependency relations with sentiment
words. We replace those words with their polarity
equivalents. For example, relation ?nsubj(wrong,
you)? becomes ?nsubj(SentiWord
neg
, you)?.
100
POSITIVE
please elaborate, nod, await response, from experiences, anti-war, profits, promises of, is undisputed,
royalty, sunlight, conclusively, badges, prophecies, in vivo, tesla, pioneer, published material, from god,
plea for, lend itself, geek, intuition, morning, anti SentiWord
neg
, connected closely, Rel(undertake,
to), intelligibility, Rel(articles, detailed), of noting, for brevity, Rel(believer, am), endorsements, testable,
source carefully
NEGATIVE
: (, TOT, ?!!, in contrast, ought to, whatever, Rel(nothing, you), anyway, Rel(crap, your), by facts, pur-
porting, disproven, Rel(judgement, our), Rel(demonstrating, you), opt for, subdue to, disinformation,
tornado, heroin, Rel(newbies, the), Rel (intentional, is), pretext, watergate, folly, perjury, Rel(lock, ar-
ticle), contrast with, poke to, censoring information, partisanship, insurrection, bigot, Rel(informative,
less), clowns, Rel(feeling, mixed), never-ending
Table 2: Example terms and relations from our online discussion lexicon. We choose for display terms
that do not contain any seed word.
4 Online Discussion Sentiment Lexicon
Construction
So far as we know, there is no lexicon available
for online discussions. Thus, we create from a
large-scale corpus via label propagation. The la-
bel propagation algorithm, proposed by Zhu and
Ghahramani (2002), is a semi-supervised learning
method. In general, it takes as input a set of seed
samples (e.g. sentiment words in our case), and
the similarity between pairwise samples, then it-
eratively assigns values to the unlabeled samples
(see Algorithm 1). The construction of graph G is
discussed in Section 4.1. Sample sentiment words
in the new lexicon are listed in Table 2.
Input : G = (V,E), w
ij
? [0, 1], positive
seed words P , negative seed words
N , number of iterations T
Output: {y
i
}
|V |?1
i=0
y
i
= 1.0, ?v
i
? P
y
i
= ?1.0, ?v
i
? N
y
i
= 0.0, ?v
i
/? P ?N
for t = 1 ? ? ?T do
y
i
=
?
(v
i
,v
j
)?E
w
ij
?y
j
?
(v
i
,v
j
)?E
w
ij
, ?v
i
? V
y
i
= 1.0, ?v
i
? P
y
i
= ?1.0, ?v
i
? N
end
Algorithm 1: The label propagation algo-
rithm (Zhu and Ghahramani, 2002) used for
constructing online discussion lexicon.
4.1 Graph Construction
Node Set V . Traditional lexicons, like General
Inquirer (Stone et al., 1966), usually consist of po-
larized unigrams. As we mentioned in Section 1,
unigrams lack the capability of capturing the sen-
timent conveyed in online discussions. Instead,
bigrams, dependency relations, and even punctu-
ation can serve as supplement to the unigrams.
Therefore, we consider four types of text units as
nodes in the graph: unigrams, bigrams, depen-
dency relations, sentiment dependency relations.
Sentiment dependency relations are described in
Section 3.3. We replace all relation names with a
general label. Text units that appear in at least 10
discussions are retained as nodes to reduce noise.
Edge Set E. As Velikovich et al. (2010) and
Feng et al. (2013) notice, a dense graph with a
large number of nodes is susceptible to propagat-
ing noise, and will not scale well. We thus adopt
the algorithm in Feng et al. (2013) to construct
a sparsely connected graph. For each text unit t,
we first compute its representation vector ~a using
Pairwise Mutual Information scores with respect
to the top 50 co-occuring text units. We define
?co-occur? as text units appearing in the same sen-
tence. An edge is created between two text units
t
0
and t
1
only if they ever co-occur. The similar-
ity between t
0
and t
1
is calculated as the Cosine
similarity between ~a
0
and ~a
1
.
Seed Words. The seed sentiment are collected
from three existing lexicons: MPQA lexicon, Gen-
eral Inquirer, and SentiWordNet. Each word in
SentiWordNet is associated with a positive score
and a negative score; words with a polarity score
101
larger than 0.7 are retained. We remove words
with conflicting sentiments.
4.2 Data
The graph is constructed based on Wikipedia talk
pages. We download the 2013-03-04 Wikipedia
data dump, which contains 4,412,582 talk pages.
Since we are interested in conversational lan-
guages, we filter out talk pages with fewer than
5 participants. This results in a dataset of 20,884
talk pages, from which the graph is constructed.
5 Experimental Setup
5.1 Datasets
Wikipedia Talk pages. The first dataset we use
is Authority and Alignment in Wikipedia Dis-
cussions (AAWD) corpus (Bender et al., 2011).
AAWD consists of 221 English Wikipedia discus-
sions with agreement and disagreement annota-
tions.
3
The annotation of AAWD is made at utterance-
or turn-level, where a turn is defined as continu-
ous body of text uttered by the same participant.
Annotators either label each utterance as agree-
ment, disagreement or neutral, and select the cor-
responding spans of text, or label the full turn.
Each turn is annotated by two or three people. To
induce an utterance-level label for instances that
have only a turn-level label, we assume they have
the same label as the turn.
To train our sentiment model, we further trans-
form agreement and disagreement labels (i.e. 3-
way) into the 5-way labels. For utterances that
are annotated as agreement and have the text
span specified by at least two annotators, they are
treated as ?strongly agree? (PP). If an utterance is
only selected as agreement by one annotator or it
gets the label by turn-level annotation, it is ?agree?
(P). ?Strongly disagree? (NN) and ?disagree? (N)
are collected in the same way from disagreement
label. All others are neutral (O). In total, we have
16,501 utterances. 1,930 and 1,102 utterances are
labeled as ?NN? and ?N?. 532 and 99 of them are
?PP? and ?P?. All other 12,648 are neutral sam-
ples.
4
3
Bender et al. (2011) originally use positive alignment
and negative alignment to indicate two types of social moves.
They define those alignment moves as ?agreeing or disagree-
ing? with the target. We thus use agreement and disagreement
instead of positive and negative alignment in this work.
4
345 samples with both positive and negative labels are
treated as neutral.
Online Debate. The second dataset is the Inter-
net Argument Corpus (IAC) (Walker et al., 2012a)
collected from an online debate forum. Each dis-
cussion in IAC consists of multiple posts, where
we treat each post as a turn. Most posts (72.3%)
contain quoted content from the posts they target
at or other resources. A post can have more than
one quote, which naturally break the post into mul-
tiple segments. 1,806 discussions are annotated
with agreement and disagreement on the segment-
level from -5 to 5, with -5 as strongly disagree and
5 as strongly agree. We first compute the average
score for each segment among different annotators
and transform the score into sentiment label in the
following way. We treat [?5,?3] as NN (1595
segments), (?3,?1] as N (4548 segments), [1, 3)
as P (911 samples), [3, 5] as PP (199), all others as
O (290 segments).
In the test phase, utterances or segments pre-
dicted with NN or N are treated as disagreement;
the ones predicted as PP or P are agreement; O is
neutral.
5.2 Comparison
We compare with two baselines. (1) Baseline (Po-
larity) is based on counting the sentiment words
from our lexicon. An utterance or segment is
predicted as agreement if it contains more posi-
tive words than negative words, or disagreement
if more negative words are observed. Other-
wise, it is neutral. (2) Baseline (Distance) is ex-
tended from (Hassan et al., 2010). Each sentiment
word is associated with the closest second per-
son pronoun, and a surface distance can be com-
puted between them. A classifier based on Sup-
port Vector Machines (Joachims, 1999) (SVM) is
trained with the features of sentiment words, min-
imum/maximum/average of the distances.
We also compare with two state-of-the-art
methods that are widely used in sentiment predic-
tion for conversations. The first one is an RBF
kernel SVM based approach, which has been used
for sentiment prediction (Hassan et al., 2010), and
(dis)agreement detection (Yin et al., 2012) in on-
line debates. The second is linear chain CRF,
which has been utilized for (dis)agreement iden-
tification in broadcast conversations (Wang et al.,
2011).
102
Strict F1 Soft F1
Agree Disagree Neutral Agree Disagree Neutral
Baseline (Polarity) 14.56 25.70 64.04 22.53 38.61 66.45
Baseline (Distance) 8.08 20.68 84.87 33.75 55.79 88.97
SVM (3-way) 26.76 35.79 77.39 44.62 52.56 80.84
+ downsampling 21.60 36.32 72.11 31.86 49.58 74.92
CRF (3-way) 20.99 23.85 85.28 56.28 56.37 89.41
CRF (5-way) 20.47 19.42 85.86 58.39 56.30 90.10
+ downsampling 24.26 31.28 77.12 47.30 46.24 80.18
isotonic CRF 24.32 21.95 86.26 68.18 62.53 88.87
+ downsampling 29.62 34.17 80.97 55.38 53.00 84.56
+ new lexicon 46.01 51.49 87.40 74.47 67.02 90.56
+ new lexicon + downsampling 47.90 49.61 81.60 64.97 58.97 84.04
Table 3: Strict and soft F1 scores for agreement and disagreement detection on Wikipedia talk pages
(AAWD). All the numbers are multiplied by 100. In each column, bold entries (if any) are statistically
significantly higher than all the rest, and the italic entry has the highest absolute value. Our model based
on the isotonic CRF with the new lexicon produces significantly better results than all the other systems
for agreement and disagreement detection. Downsampling, however, is not always helpful.
6 Results
In this section, we first show the experimental re-
sults on sentence- and segment-level agreement
and disagreement detection in two types of online
discussions ? Wikipedia Talk pages and online de-
bates. Then we provide more detailed analysis for
the features used in our model. Furthermore, we
discuss several types of errors made in the model.
6.1 Wikipedia Talk Pages
We evaluate the systems by standard F1 score on
each of the three categories: agreement, disagree-
ment, and neutral. For AAWD, we compute two
versions of F1 scores. Strict F1 is computed
against the true labels. For soft F1, if a sentence
is never labeled by any annotator on the sentence-
level and adopts its agreement/disagreement label
from the turn-level annotation, then it is treated as
a true positive when predicted as neutral.
Table 3 demonstrates our main results on the
Wikipedia Talk pages (AAWD dataset). With-
out downsampling, our isotonic CRF based sys-
tems with the new lexicon significantly outper-
form the compared approaches for agreement and
disagreement detection according to the paired-
t test (p < 0.05). We also perform downsam-
pling by removing the turns only containing neu-
tral utterances. However, it does not always help
with performance. We suspect that, with less neu-
tral samples in the training data, the classifier is
less likely to make neutral predictions, which thus
decreases true positive predictions. For strict F-
scores on agreement/disagreement, downsampling
Agree Disagree Neu
Baseline (Polarity) 3.33 5.96 65.61
Baseline (Distance) 1.65 5.07 85.41
SVM (3-way) 25.62 69.10 31.47
+ new lexicon features 28.35 72.58 34.53
CRF (3-way) 29.46 74.81 31.93
CRF (5-way) 24.54 69.31 39.60
+ new lexicon features 28.85 71.81 39.14
isotonic CRF 53.40 76.77 44.10
+ new lexicon 61.49 77.80 51.43
Table 4: F1 scores for agreement and disagree-
ment detection on online debate (IAC). All the
numbers are multiplied by 100. In each column,
bold entries (if any) are statistically significantly
higher than all the rest, and the italic entry has the
highest absolute value except baselines. We have
two main observations: 1) Both of our models
based on isotonic CRF significantly outperform
other systems for agreement and disagreement de-
tection. 2) By adding the new lexicon, either as
features or constraints in isotonic CRF, all systems
achieve better F1 scores.
has mixed effect, but mostly we get slightly better
performance.
6.2 Online Debates
Similarly, F1 scores for agreement, disagreement
and neutral for online debates (IAC dataset) are
displayed in Table 4. Both of our systems based
on isotonic CRF achieve significantly better F1
scores than the comparison. Especially, our sys-
tem with the new lexicon produces the best results.
For SVM and linear-chain CRF based systems, we
also add new sentiment features constructed from
the new lexicon as described in Section 3.3. We
103
can see that those sentiment features also boost the
performance for both of the compared approaches.
6.3 Feature Evaluation
Moreover, we evaluate the effectiveness of fea-
tures by adding one type of features each time.
The results are listed in Table 5. As it can be seen,
the performance gets improved incrementally with
every new set of features.
We also utilize ?
2
-test to highlight some of
the salient features on the two datasets. We can
see from Table 6 that, for online debates (IAC),
some features are highly topic related, such as ?the
male? or ?the scientist?. This observation concurs
with the conclusion in Misra and Walker (2013)
that features with topic information are indicative
for agreement and disagreement detection.
AAWD Agree Disagree Neu
Lex 40.77 52.90 79.65
Lex + Syn 68.18 63.91 88.87
Lex + Syn + Disc 70.93 63.69 89.32
Lex + Syn + Disc + Con 71.27 63.72 89.60
Lex + Syn + Disc + Con + Sent 74.47 67.02 90.56
IAC Agree Disagree Neu
Lex 56.65 75.35 45.72
Lex + Syn 54.16 75.13 46.12
Lex + Syn + Disc 54.27 76.41 47.60
Lex + Syn + Disc + Con 55.31 77.25 48.87
Lex + Syn + Disc + Con + Sent 61.49 77.80 51.43
Table 5: Results on Wikipedia talk page
(AAWD) (with soft F1 score) and online de-
bate (IAC) with different feature sets (i.e Lexical,
Syntacitc/Semantic, Discourse, Conversation, and
Sentiment features) by using isotonic CRF. The
numbers in bold are statistically significantly
higher than the numbers above it (paired-t test,
p < 0.05).
6.4 Error Analysis
After a closer look at the data, we found two ma-
jor types of errors. Firstly, people express dis-
agreement not only by using opinionated words,
but also by providing contradictory example. This
needs a deeper understanding of the semantic in-
formation embedded in the text. Techniques like
textual entailment can be used in the further work.
Secondly, a sequence of sentences with sarcasm is
hard to detect. For instance, ?Bravo, my friends!
Bravo! Goebbles would be proud of your abilities
to whitewash information.? We observe terms like
?Bravo?, ?friends?, and ?be proud of? that are in-
dicators for positive sentiment; however, they are
AAWD
POSITIVE: agree, nsubj (agree, I), nsubj (right,
you), Rel (Sentiment
pos
, I), thanks, amod (idea,
good), nsubj(glad, I), good point, concur, happy
with, advmod (good, pretty), suggestion
Hedge
NEGATIVE: you, your, nsubj (negative, you),
numberOfNegator, don?t, nsubj (disagree, I),
actually
SentInitial
, please stop
SentInitial
, what
?
SentInitial
, should
Hedge
IAC
POSITIVE: amod (conclusion, logical), Rel (agree,
on), Rel (have, justified), Rel (work, out), one
might
SentInitial
, to confirm
Hedge
, women
NEGATIVE: their kind, the male, the female, the
scientist, according to, is stated, poss (understand-
ing, my), hell
SentInitial
, whatever
SentInitial
Table 6: Relevant features by ?
2
test on AAWD
and IAC.
in sarcastic tone. We believe a model that is able
to detect sarcasm would further improve the per-
formance.
7 Conclusion
We present an agreement and disagreement detec-
tion model based on isotonic CRFs that outputs
labels at the sentence- or segment-level. We boot-
strap the construction of a sentiment lexicon for
online discussions, encoding it in the form of do-
main knowledge for the isotonic CRF learner. Our
sentiment-tagging model is shown to outperform
the state-of-the-art approaches on both Wikipedia
Talk pages and online debates.
Acknowledgments We heartily thank the Cornell
NLP Group and the reviewers for helpful com-
ments. This work was supported in part by NSF
grants IIS-0968450 and IIS-1314778, and DARPA
DEFT Grant FA8750-13-2-0015. The views and
conclusions contained herein are those of the au-
thors and should not be interpreted as necessarily
representing the official policies or endorsements,
either expressed or implied, of NSF, DARPA or
the U.S. Government.
References
Rob Abbott, Marilyn Walker, Pranav Anand, Jean E.
Fox Tree, Robeson Bowmani, and Joseph King. 2011.
How can you say such things?!?: Recognizing disagree-
ment in informal political argument. In Proceedings of
the Workshop on Languages in Social Media, LSM ?11,
pages 2?11, Stroudsburg, PA, USA. Association for Com-
putational Linguistics.
Amjad Abu-Jbara, Mona Diab, Pradeep Dasigi, and
104
Dragomir Radev. 2012. Subgroup detection in ideo-
logical discussions. In Proceedings of the 50th Annual
Meeting of the Association for Computational Linguis-
tics: Long Papers - Volume 1, ACL ?12, pages 399?409,
Stroudsburg, PA, USA. Association for Computational
Linguistics.
Emily M. Bender, Jonathan T. Morgan, Meghan Oxley, Mark
Zachry, Brian Hutchinson, Alex Marin, Bin Zhang, and
Mari Ostendorf. 2011. Annotating social acts: Author-
ity claims and alignment moves in wikipedia talk pages.
In Proceedings of the Workshop on Languages in Social
Media, LSM ?11, pages 48?57, Stroudsburg, PA, USA.
Association for Computational Linguistics.
Yejin Choi and Claire Cardie. 2009. Adapting a polarity lex-
icon using integer linear programming for domain-specific
sentiment classification. In Proceedings of the 2009 Con-
ference on Empirical Methods in Natural Language Pro-
cessing: Volume 2 - Volume 2, EMNLP ?09, pages 590?
598, Stroudsburg, PA, USA. Association for Computa-
tional Linguistics.
Marie-Catherine de Marneffe, Bill MacCartney, and Christo-
pher D. Manning. 2006. Generating typed dependency
parses from phrase structure trees. In LREC.
Xiaowen Ding, Bing Liu, and Philip S. Yu. 2008. A holistic
lexicon-based approach to opinion mining. In Proceed-
ings of the 2008 International Conference on Web Search
and Data Mining, WSDM ?08, pages 231?240, New York,
NY, USA. ACM.
Andrea Esuli and Fabrizio Sebastiani. 2006. Sentiwordnet:
A publicly available lexical resource for opinion mining.
In In Proceedings of the 5th Conference on Language Re-
sources and Evaluation (LREC06, pages 417?422.
Rich?ard Farkas, Veronika Vincze, Gy?orgy M?ora, J?anos
Csirik, and Gy?orgy Szarvas. 2010. The conll-2010 shared
task: Learning to detect hedges and their scope in nat-
ural language text. In Proceedings of the Fourteenth
Conference on Computational Natural Language Learn-
ing ? Shared Task, CoNLL ?10: Shared Task, pages 1?
12, Stroudsburg, PA, USA. Association for Computational
Linguistics.
Song Feng, Jun Seok Kang, Polina Kuznetsova, and Yejin
Choi. 2013. Connotation lexicon: A dash of sentiment
beneath the surface meaning. In ACL, pages 1774?1784.
The Association for Computer Linguistics.
Oliver Ferschke, Iryna Gurevych, and Yevgen Chebotar.
2012. Behind the article: Recognizing dialog acts in
wikipedia talk pages. In Proceedings of the 13th Con-
ference of the European Chapter of the Association for
Computational Linguistics, EACL ?12, pages 777?786,
Stroudsburg, PA, USA. Association for Computational
Linguistics.
Michel Galley, Kathleen McKeown, Julia Hirschberg, and
Elizabeth Shriberg. 2004. Identifying agreement and dis-
agreement in conversational speech: use of Bayesian net-
works to model pragmatic dependencies. In ACL ?04:
Proceedings of the 42nd Annual Meeting on Association
for Computational Linguistics, pages 669+, Morristown,
NJ, USA. Association for Computational Linguistics.
Sangyun Hahn, Richard Ladner, and Mari Ostendorf. 2006.
Agreement/disagreement classification: Exploiting unla-
beled data using contrast classifiers. In Proceedings of the
Human Language Technology Conference of the NAACL,
Companion Volume: Short Papers, pages 53?56, New
York City, USA, June. Association for Computational Lin-
guistics.
Ahmed Hassan, Vahed Qazvinian, and Dragomir Radev.
2010. What?s with the attitude?: Identifying sentences
with attitude in online discussions. In Proceedings of
the 2010 Conference on Empirical Methods in Natural
Language Processing, EMNLP ?10, pages 1245?1255,
Stroudsburg, PA, USA. Association for Computational
Linguistics.
Ahmed Hassan, Amjad Abu-Jbara, and Dragomir Radev.
2012. Detecting subgroups in online discussions by mod-
eling positive and negative relations among participants.
In Proceedings of the 2012 Joint Conference on Empiri-
cal Methods in Natural Language Processing and Com-
putational Natural Language Learning, EMNLP-CoNLL
?12, pages 59?70, Stroudsburg, PA, USA. Association for
Computational Linguistics.
Julia Hirschberg and Diane Litman. 1993. Empirical studies
on the disambiguation of cue phrases. Comput. Linguist.,
19(3):501?530, September.
Thorsten Joachims. 1999. Advances in kernel meth-
ods. chapter Making Large-scale Support Vector Machine
Learning Practical, pages 169?184. MIT Press, Cam-
bridge, MA, USA.
Mahesh Joshi and Carolyn Penstein-Ros?e. 2009. Generaliz-
ing dependency features for opinion mining. In Proceed-
ings of the ACL-IJCNLP 2009 Conference Short Papers,
ACLShort ?09, pages 313?316, Stroudsburg, PA, USA.
Association for Computational Linguistics.
John D. Lafferty, Andrew McCallum, and Fernando C. N.
Pereira. 2001. Conditional random fields: Probabilistic
models for segmenting and labeling sequence data. In
Proceedings of the Eighteenth International Conference
on Machine Learning, ICML ?01, pages 282?289, San
Francisco, CA, USA. Morgan Kaufmann Publishers Inc.
Yi Mao and Guy Lebanon. 2007. Isotonic conditional ran-
dom fields and local sentiment flow. In Advances in Neu-
ral Information Processing Systems.
Andrew Kachites McCallum. 2002. Mallet: A machine
learning for language toolkit. http://mallet.cs.umass.edu.
Amita Misra and Marilyn Walker. 2013. Topic independent
identification of agreement and disagreement in social me-
dia dialogue. In Proceedings of the SIGDIAL 2013 Con-
ference, pages 41?50, Metz, France, August. Association
for Computational Linguistics.
Minghui Qiu, Liu Yang, and Jing Jiang. 2013. Mining user
relations from online discussions using sentiment analy-
sis and probabilistic matrix factorization. In Proceedings
of the 2013 Conference of the North American Chapter
of the Association for Computational Linguistics: Human
Language Technologies, pages 401?410, Atlanta, Georgia,
June. Association for Computational Linguistics.
Alan Lee Eleni Miltsakaki Livio Robaldo Aravind Joshi
Rashmi Prasad, Nikhil Dinesh and Bonnie Webber.
2008. The penn discourse treebank 2.0. In Bente
Maegaard Joseph Mariani Jan Odijk Stelios Piperidis
Daniel Tapias Nicoletta Calzolari (Conference Chair),
105
Khalid Choukri, editor, Proceedings of the Sixth Interna-
tional Conference on Language Resources and Evaluation
(LREC?08), Marrakech, Morocco, may. European Lan-
guage Resources Association (ELRA). http://www.lrec-
conf.org/proceedings/lrec2008/.
Swapna Somasundaran and Janyce Wiebe. 2009. Recogniz-
ing stances in online debates. In Proceedings of the Joint
Conference of the 47th Annual Meeting of the ACL and the
4th International Joint Conference on Natural Language
Processing of the AFNLP: Volume 1 - Volume 1, ACL ?09,
pages 226?234, Stroudsburg, PA, USA. Association for
Computational Linguistics.
Swapna Somasundaran and Janyce Wiebe. 2010. Recogniz-
ing stances in ideological on-line debates. In Proceedings
of the NAACL HLT 2010 Workshop on Computational Ap-
proaches to Analysis and Generation of Emotion in Text,
CAAGET ?10, pages 116?124, Stroudsburg, PA, USA.
Association for Computational Linguistics.
Philip J. Stone, Dexter C. Dunphy, Marshall S. Smith, and
Daniel M. Ogilvie. 1966. The General Inquirer: A Com-
puter Approach to Content Analysis. MIT Press, Cam-
bridge, MA.
Matt Thomas, Bo Pang, and Lillian Lee. 2006. Get out
the vote: Determining support or opposition from con-
gressional floor-debate transcripts. In Proceedings of the
2006 Conference on Empirical Methods in Natural Lan-
guage Processing, EMNLP ?06, pages 327?335, Strouds-
burg, PA, USA. Association for Computational Linguis-
tics.
Leonid Velikovich, Sasha Blair-Goldensohn, Kerry Hannan,
and Ryan McDonald. 2010. The viability of web-derived
polarity lexicons. In Human Language Technologies: The
2010 Annual Conference of the North American Chap-
ter of the Association for Computational Linguistics, HLT
?10, pages 777?785, Stroudsburg, PA, USA. Association
for Computational Linguistics.
Marilyn Walker, Jean Fox Tree, Pranav Anand, Rob Abbott,
and Joseph King. 2012a. A corpus for research on de-
liberation and debate. In Proceedings of the Eight Inter-
national Conference on Language Resources and Evalu-
ation (LREC?12), Istanbul, Turkey, may. European Lan-
guage Resources Association (ELRA).
Marilyn A. Walker, Pranav Anand, Rob Abbott, and Ricky
Grant. 2012b. Stance classification using dialogic prop-
erties of persuasion. In HLT-NAACL, pages 592?596. The
Association for Computational Linguistics.
Wen Wang, Sibel Yaman, Kristin Precoda, Colleen Richey,
and Geoffrey Raymond. 2011. Detection of agreement
and disagreement in broadcast conversations. In Proceed-
ings of the 49th Annual Meeting of the Association for
Computational Linguistics: Human Language Technolo-
gies: Short Papers - Volume 2, HLT ?11, pages 374?
378, Stroudsburg, PA, USA. Association for Computa-
tional Linguistics.
Theresa Wilson, Janyce Wiebe, and Paul Hoffmann. 2005.
Recognizing contextual polarity in phrase-level sentiment
analysis. In Proceedings of the Conference on Human
Language Technology and Empirical Methods in Natural
Language Processing, HLT ?05, pages 347?354, Strouds-
burg, PA, USA. Association for Computational Linguis-
tics.
Jie Yin, Paul Thomas, Nalin Narang, and Cecile Paris. 2012.
Unifying local and global agreement and disagreement
classification in online debates. In Proceedings of the
3rd Workshop in Computational Approaches to Subjec-
tivity and Sentiment Analysis, WASSA ?12, pages 61?
69, Stroudsburg, PA, USA. Association for Computational
Linguistics.
Xiaojin Zhu and Zoubin Ghahramani. 2002. Learning from
labeled and unlabeled data with label propagation. In
Technical Report CMU-CALD-02-107.
106
Proceedings of the Joint Workshop on Social Dynamics and Personal Attributes in Social Media, pages 17?27,
Baltimore, Maryland USA, 27 June 2014.
c
?2014 Association for Computational Linguistics
The Enrollment Effect: A Study of Amazon?s Vine Program
Dinesh Puranam
Samuel Curtis Johnson
Graduate School of Management
Cornell University
dp457@cornell.edu
Claire Cardie
Department of Computer Science
Department of Information Science
Cornell University
cardie@cs.cornell.edu
Abstract
Do rewards from retailers such as free
products and recognition in the form of
status badges
1
influence the recipient?s be-
havior? We present a novel application
of natural language processing to detect
differences in consumer behavior due to
such rewards. Specifically, we investigate
the ?Enrollment? effect, i.e. whether re-
ceiving products for free affect how con-
sumer reviews are written. Using data
from Amazon?s Vine program, we con-
duct a detailed analysis to detect stylis-
tic differences in product reviews written
by reviewers before and after enrollment
in the Vine program. Our analysis sug-
gests that the ?Enrollment? effect exists.
Further, we are able to characterize the
effect on syntactic and semantic dimen-
sions. This work has implications for re-
searchers, firms and consumer advocates
studying the influence of user-generated
content as these changes in style could po-
tentially influence consumer decisions.
1 Introduction
In 2007 Amazon introduced its Vine program
2
.
According to Amazon, ?Amazon invites cus-
tomers to become Vine Voices based on their re-
viewer rank, which is a reflection of the quality
and helpfulness of their reviews as judged by other
Amazon customers. Amazon provides Vine mem-
bers with free products that have been submitted
to the program by participating vendors. Vine re-
views are the ?independent opinions of the Vine
1
A status badge is a special identification usually placed
next to a username in online content.
2
http://blog.librarything.com/main/2007/08/amazon-
vine-and-early-reviewers/
Voices.?
3
There could be potential concerns as to
whether this enrollment affects the way reviews
are written, introducing, for example, a positive
bias.
4
In this work, we investigate whether enroll-
ment in the Vine program results in changes in
the linguistic style used in reviews. We investi-
gate this by looking at reviews by individuals be-
fore and after enrollment in the program. Follow-
ing Feng et al. (2012) and Bergsma et al. (2012),
we conduct a stylometric analysis using a number
of syntactic and semantic features to detect differ-
ences in style. We believe that detecting changes
in consumer behavior due to intervention by a firm
is a novel natural language processing task. Our
approach offers a framework for analyzing text to
detect these changes. This work is relevant for
social scientists and consumer advocates as re-
search suggests that product reviews are influen-
tial (Chevalier and Mayzlin, 2006) and changes in
style could potentially influence consumer deci-
sions.
2 Related Work
Our work lies at the intersection of research in
four broad areas ? Product Reviews, Product
Sampling, Status and Stylometry.
Product Reviews Product reviews have re-
ceived considerable attention in multiple disci-
plines including Marketing, Computer Science
and Information Science. Research has addressed
questions such as the influence of product reviews
on product sales and on brands (Gopinath et al.
(2014); Chevalier and Mayzlin (2006)), detection
of deceptive reviews (Ott et al., 2011) and senti-
ment summarization (Titov and McDonald, 2008).
3
http://www.amazon.com/gp/vine/help, words italicized
by authors.
4
http://www.npr.org/blogs/money/2013/10/29/241372607/top-
reviewers-on-amazon-get-tons-of-free-stuff.
17
This list is by no means comprehensive, but it is
indicative of the extensive work in this domain.
Product Sampling Here, consumers receive
products for free ? as a marketing tactic. This
is also a well-studied phenomenon. Research in
this area has indicated that consumers value free
products (Shampanier et al. (2007); Palmeira and
Srivastava (2013)); that product sampling affects
brand sales (Bawa and Shoemaker, 2004) and that
sampling influences consumer behavior (Wadhwa
et al., 2008).
Status Research shows that status can influ-
ence writing style. Danescu-Niculescu-Mizil et al.
(2012) study discussions among Wikipedia edi-
tors and transcripts of oral arguments before the
U.S. Supreme Court and show how variations
in linguistic style can provide information about
power differences within social groups.
Stylometry focuses on the recognition of style
elements to identify authors (Rosen-Zvi et al.,
2004), detect genders and even determine the
venue where an academic paper was presented
(Bergsma et al., 2012).
Our work draws from each of these research
areas and in turn hopes to make a contribution to
each in return. Our primary objective is to es-
tablish a framework to detect behavioral change
due to a decision by a firm (in this case enroll-
ment to the Vine program characterized by free
products and Vine membership status) by analyz-
ing product reviews. Further, we hope to under-
stand the dimensions on which this behavior may
have changed. Consequently, we pursue a novel
stylometric task. This type of work is especially
important when the traditional numerical measure
(rating) suggests there is no difference in the re-
view pre and post-enrollment (see Section 4).
3 Data & Pre-processing Steps
We gathered all reviews by the top 10,000 review-
ers ranked by Amazon as of September, 2012.
These rankings are partly driven by helpfulness
and recency of reviews
5
. The data collected in-
cludes the review text, review title, rating as-
signed, date posted, product URL, product price,
whether the reviewed product was received for
free via the Vine program (also referred to as
5
http://www.amazon.com/review/guidelines/top-
reviewers.html/
?Vine Review?), ?helpfulness? votes and badges
received by the reviewer .
We collected a total of 2,464,141 reviews of
which 282,913 reviews were for products received
for free via the Vine program. These reviews cov-
ered a total of 9,982 reviewers
6
of which 3,566
were members of the Vine program. Approxi-
mately half the reviews belonged to Vine mem-
bers. We eliminated reviews that did not have a
rating. We further excluded reviews where the re-
view text was less than 20 words in length. We
were left with 1,189,704 reviews by Vine mem-
bers.
The date of enrollment to the Vine program
for each reviewer is not explicitly available. We
infer the date of enrollment in the following man-
ner. We sort in ascending order all the ?Vine
Reviews? for each reviewer by posted date. We
assume the earliest posted date for a ?Vine re-
view? is the enrollment date. This is an important
assumption, as potentially reviewers could have
moved in and out of the program at varying points
of time. Reviewers can be moved out of the pro-
gram for reasons such as not posting a ?Vine Re-
view? within 30 days of receipt of the product. In
our data set we found 47,510 ?Vine Reviews? by
163 reviewers who were not actively on the Vine
program
7
. We can view these reviewers as having
been dropped from the Vine program. Given the
small volume of this type of reviews and review-
ers, our assumption on date of enrollment appears
reasonable.
Member
Type
Free/
Paid
Enrollment
Timing
Review
Count
Non Vine Paid NA 1,169,561
Non Vine Free NA 47,510
Vine Paid Post 452,729
Vine Paid Pre 503,688
Vine Free Post 233,287
Table 1: Data Summary
4 Enrollment Effect
This research seeks to answer the question: does
enrollment in the Vine program change the writ-
ing styles of reviewers. One naive theory is that
6
During the crawling, ranks changed resulting in fewer
than 10,000 reviewers in our data set.
7
As these reviewers were not enrolled to Amazon?s Vine
Program as of September, 2012, they are excluded from our
analysis.
18
perhaps receiving products for free and receiving
status badges will result in Vine members post-
ing more positive reviews. Interestingly, the av-
erage rating for reviews by Vine members posted
before enrollment is 4.22 and after enrollment is
4.21 and this difference is not statistically signif-
icant. In contrast, the length of reviews signifi-
cantly increased from 251 words prior to enroll-
ment to 306 words post-enrollment. Natural lan-
guage techniques are the only option to further
investigate possible effects of enrollment. Con-
sequently we focus on the review text posted by
Vine members.
4.1 Approach
Following Ashok et al. (2013) and Bergsma et al.
(2012) we construct features that represent writ-
ing style from each review (discussed in more de-
tail in the next section). We incorporate these fea-
tures in a classification algorithm that attempts to
classify each review as having been written pre or
post-enrollment to the Vine program. We report
whether the difference in accuracy for this clas-
sifier vs. a majority vote classification is statisti-
cally significant or not. In order to detect differ-
ences in style pre and post-enrollment, we need to
address certain confounding factors ? Reviewer
Specificity , Product Specificity and Time Speci-
ficity.
Reviewer Specificity It may be possible that
certain users post more reviews post-enrollment
than pre-enrollment. Consequently the classifier
may simply end up learning the differences in
style between reviewers. To avoid this, we con-
struct a balanced sample where we randomly se-
lect 25 reviews for each reviewer prior to and post-
enrollment (see Table 2). This also sets our base-
line accuracy at 50%.
Product Specificity As the program started in
2007, the post-enrollment reviews are likely to
predominantly contain products released in after
2007. This might result in the classifier simply
learning the differences between products (say I
Phone vs Palm). Given our focus on style, we
do not use word tokens as such - thus avoiding
the use of product specific features. However, for
some products, the product specific details may
result in the use of specific syntactic structures.
We assume this is not a significant contributor to
the prediction performance. A post-hoc analysis
of the top features supports this assumption. A
second source of change in writing style could be
due to simply whether the product was bought or
received for free. We exclude ?Vine Reviews?
8
to
eliminate this confounding factor.
Time Specificity A similar concern as Product
Specificity exists for date references. By focusing
on syntactic and semantic style, we avoid the use
of time specific features.
Another concern is that perhaps post enroll-
ment, reviewers receive writing guidelines from
Amazon. This does not appear to be the case, as
the writing guidelines
9
appear to be for all mem-
bers.We now turn to the extraction of style fea-
tures.
Data Type Number of
Reviews
Number of
Reviewers
Training 113,250 2,265
Test 2,500 50
Table 2: Experiment Data
4.2 Feature Extraction
We consider three different features ? ?Bag of
words/ unigrams?, ?Parse Tree Based Features?
and an umbrella category consisting of genre and
semantic features (see Section 4.2.3).
4.2.1 Bag of Words
Bag of Words/Unigrams (UNIGRAMS) Uni-
grams have often been found to be effective pre-
dictive features (Joachims, 2001). In our context,
this serves as a competitive baseline for the clas-
sification task.
4.2.2 Parse Tree Based Features
Following Feng et al. (2012) and Ashok et al.
(2013) we use Probabilistic Context Free Gram-
mar (PCFG) to construct a parse tree for each sen-
tence. We then generate features from this parse
tree and aggregate features to a review level.
All Production Rules (?) This set of features
include all production rule features for each re-
view, including the leaves of the parse tree for
8
Reviews where product was received for free via the
Vine program.
9
http://www.amazon.com/gp/community-help/customer-
reviews-guidelines
19
each sentence in the review. This effectively rep-
resents a combination of production rules and un-
igrams as features and represents an additional
competitive baseline.
Non Terminal Production Rules (?
N
) This ex-
cludes the leaves and hence restricts the feature
set to non-terminal production rules. This allows
us to investigate purely syntactic features from the
text.
Phrasal/ Clausal Nodes (PHR/CLSL) We also
investigate features that incorporate phrasal or
clausal nodes of the parse trees. Please see Table
5 and Table 6 for examples of these features.
Parse Tree Measures (PTM) We construct a
set of measures for each sentence based on the
parse tree. These measures are maximum height
of parse tree, maximum width of the parse tree and
the number of sentences in each review.
Latent Dirichlet Allocation (LDA) We also
apply Latent Dirichlet Allocation (Blei et al.,
2003) to the production rules extracted from the
Probabilistic Context Free Grammar. We use the
topics generated as features in our prediction task.
Our objective was to determine whether certain
co-occurring production rules offered better clas-
sification accuracy. Our implementation includes
hyper-parameter optimization via maximum like-
lihood. The number of topics is selected by maxi-
mizing the pairwise cosine distance amongst top-
ics. We used the Stanford Parser (Klein and Man-
ning, 2003) to parse each of the reviews and the
Natural Language Toolkit (NLTK) (Bird et al.,
2009) to post process the results.
4.2.3 Genre and Semantic Features
Style Metrics (STYLE ) This includes three dis-
tinct types of metrics. Character Based - This
includes counts of uppercased letters, number of
letters, number of spaces and number of vow-
els. Word Based - This includes measures such as
number of short words (3 characters or less ), long
words (8 characters or less), average word length
and number of different words. Syntax Based -
This includes measures such as number of peri-
ods, commas, common conjunctions, interroga-
tives, prepositions, pronouns and verbs.
Parts of Speech (POS) features have often been
surprisingly effective in tasks such as predicting
deception (Ott et al., 2011). Consequently we test
this feature set as well.
Domain-independent Dictionary We make
use of the Linguistic Inquiry and Word Count
(LIWC) categorization (Tausczik and Pen-
nebaker, 2010). One key advantage of this
categorization is that it is domain independent
and emphasizes psycho-linguistic cues. We run
two variants of this set of features. The first
(LIWC ALL) includes all the categories ? both
sub-ordinate and super-ordinate categories. The
second (LIWC SUB CATEG.) only includes the
sub-ordinate categories, thus ensuring the features
are mutually exclusive.
Subjectivity Measures (OPINION) We measure
number of subjective, objective and other (neither
subjective nor objective) sentences in each review.
We use the ?OpinionFinder System? (Wiebe et al.,
2005) to classify each sentence with these mea-
sures. We aggregate the count of subjective, ob-
jective and other sentences at the review level and
use these aggregates as features.
10
We also re-
port results on experiments where multiple feature
types are included simultaneously in the model.
5 Experimental Methodology
All experiments use the Fan et al. (2008) im-
plementation of linear Support Vector Machines
(Vapnik, 1998). The linear specification allows
us to infer feature importance. We learn the
penalty parameter via grid search using 5 fold
cross-validation and report performance on a held-
out balanced sample of reviews from 50 randomly
selected users (all of whom were excluded from
the training set) from the group of reviewers with
at least 25 reviews in pre and post enrollment peri-
ods. While reporting the results, for some features
we report the threshold (Thr) value set to exclude
the least frequent features. These thresholds were
also learned via the 5 fold cross validation pro-
cess. Finally, text features can be binarized, mean
centered and/or normalized. Each of these options
were also selected via 5 fold cross validation.
6 Results & Analysis
All of the feature sets perform statistically better
11
than a majority vote (50%).
Baselines Unsurprisingly, the feature set con-
taining all production rules (?) yields the best ac-
10
One drawback is that the classifiers are trained on sen-
tences from the MPQA corpus. Domain specificity is likely
to yield poorer classification performance on our data.
11
as indicated by a paired t-test at p=0.05 on the held out
sample
20
Baselines
Style Features Feature
Count
Accuracy
UNIGRAMS 796,826 60.9 %
? (Thr =50) 29,362 62.0 %
By Feature Type
Style Features Feature
Count
Accuracy
?
N
(Thr=200) 2,730 59.2 %
PHR/CLSL 23 57.4 %
PTM 3 55.8 %
LDA 200 54.0 %
STYLE 26 57.6 %
POS 45 57.5 %
LIWC ALL 76 59.8 %
LIWC SUB CATEG. 67 60.3 %
OPINION 3 56.3 %
Feature Combinations
Style Features Feature
Count
Accuracy
?
N
(THR=200) + STYLE 2,756 57.9 %
?
N
(THR=200) + OPINION 2,733 56.2 %
PHR/CLSL + OPINION 26 58.0 %
PHR/CLSL + STYLE 49 57.5 %
LIWC + STYLE 93 60.2 %
LIWC + PHR/CLSL 90 60.2 %
LIWC + ?
N
(Thr=200) 2,797 59.1 %
LIWC + OPINION 70 60.3 %
PTM + OPINION 6 57.2 %
STYLE + OPINION 29 58.7 %
STYLE + PTM 29 57.4 %
LIWC +STYLE+PHR/CLSL 116 60.1 %
Table 3: Experiment Results
curacy (62.0 %). Unfortunately, as expected, the
top features all included terminal production rules
that signal time or product specificity. For ex-
ample in the pre-enrollment reviews the top 10
features for ? include NNP ? ?Update?, CD ?
?2006?, NNP ? ?XP? and NNP ? ?Palm?. In
the post-enrollment reviews the top 10 features in-
clude CD ? ?2012?,CD ? ?2011?, NN ? ?iPad?
and NN ? ?iPhone?. We observe the same issue
with the UNIGRAMS feature set. This supports our
contention that the analysis should restrict itself
to style and domain-independent features. The
best performing style feature set is LIWC SUB
CATEG. followed by Non Terminal Production
Rules (?
N
). OPINION is the most parsimonious
feature set that performs significantly better than
a majority vote.
Non Terminal Production Rules (?
N
) Table 7
presents the top Non Terminal Production Rules.
We observe the following: First, pre-enrollment
reviews have noun phrases(NP) that contain fewer
leaf nodes than in the post-enrollment reviews.
This appears to be due to the inclusion of de-
terminers (DT), adjectives (JJ), comparative ad-
jectives (JJR), personal pronouns (PRP $) or
simply more nouns (NN). This might indicate
that topics are discussed with more specifics
in post-enrollment reviews. Second, clauses(S)
begin with action oriented verb phrases (VP)
in the pre-enrollment reviews. In contrast in
the post-enrollment reviews clauses connect two
clauses using coordinating conjunctions(CC) or
prepositions(IN). One possibility is that review-
ers are offering more detail/concepts per sen-
tence (where each clause is a detail/concept) in
the post-enrollment reviews. Finally, we ob-
serve that pre-enrollment reviews include adjec-
tival phrases (ADJP) connect to superlative ad-
verbs (RBS)which convey certainty. We will re-
visit this finding when we review the results from
the LIWC model below.
Phrasal/Clausal (PHR./CLSL.) Tables 5 and 6
suggest that post-enrollment reviews emphasize
information using descriptive phrases ? adjecti-
val phrases (ADJP) and adverbial phrases (ADVP)
? and quantifier phrases (QP). Pre-enrollment re-
views appear to have more complex clause struc-
tures (SBAR, SINV, SQ, SBARQ - see table 5 for
definitions).
Parse Tree Metrics (PTM) The three features
used are number of sentences, maximum height
of parse tree and the maximum width of the parse
tree, listed here in descending order of importance
for the post-enrollment reviews. As mentioned
earlier in section 4 the average review length is
higher in the post-enrollment reviews so the find-
ing that the number of sentences predict post-
enrollment reviews is consistent. Maximum tree
width predicts the pre-enrollment reviews. This
flat structure indicates a more complex communi-
cation structure.
Latent Dirichlet Allocation (LDA) This
model did not perform very well, being statis-
tically marginally better than majority vote. As
mentioned before, we selected the number of
topics by maximizing the average cosine distance
amongst topics. Even with 200 topics, this
measure was 0.39, suggesting that the topics were
themselves not well separated. In the limit, each
topic would be a non-terminal production rule.
This is the same as Non Terminal Production
Rules (?
N
) feature set discussed earlier in this
section.
21
Predicts PRE Enrollment
?number of different words?, ?uppercase?, ?alphas?,
?vowels? , ?short words?, ?words per sentence?, ?to be
words? , ?punctuation symbols?, ?long words?, ?common
prepositions?
Predicts POST Enrollment
?average word length?, ?spaces?, ?verbs are?, ?chars per
sentence? , ?verbs be?, ?common conjunctions?, ?verbs
were?, ?personal pronouns? , ?verbs was?, ?verbs am?
Table 4: Style Metrics: Top Features
Style (STYLE) Table 4 presents the top features
for this feature set. The features suggest that
reviewers used a more varied vocabulary (num-
ber of different words), more words per sentence
(words per sentence) and more long words (long
words) in pre-enrollment than in post-enrollment
reviews. This might indicate that sentences in
the pre-enrollment reviews were longer and more
complex. Interestingly, the average word length
did go up in the post-enrollment reviews as did
the characters per sentence. In addition, more per-
sonal pronouns and conjunctions are used ? a
finding replicated in the model using LIWC fea-
tures (see below).
Parts of Speech (POS) The top features for
post-enrollment are commas, periods, compara-
tive adjectives, verb phrases and coordinating con-
junctions. The top features for pre-enrollment are
nouns, noun phrases, determiners , prepositions
and superlative adverbs. These results are more
difficult to interpret though the use of comparative
adjectives suggest more comparisons between dif-
ferent objects in the post enrollment reviews.
LIWC SUB CATEG. The top 10 LIWC fea-
tures are shown in Table 8. LIWC features are cat-
egories that are contained in broader categories.
For example POSEMO (see Table 8, first feature for
?Predicts POST enrollment?) refers to the class of
positive emotion words. POSEMO itself is con-
tained in a category called ?Affective Features?
which in turn is classified as a Psychological Pro-
cess (abbreviated to Pscyh.). The analysis of
the categories of features is in itself interesting.
Psych./ Cognitive Features occur higher up in fea-
tures predictive of pre-enrollment reviews than in
the features predictive of post-enrollment reviews.
?Psych./ Affective Features? occurs as a top fea-
ture for the post-enrollment reviews. The ac-
tual feature from the ?Psych./ Affective Features?
category is POSEMO suggesting that the positive
emotion is more strongly conveyed in the post-
enrollment reviews than in the pre-enrollment re-
views. Interestingly the corresponding negative
feature NEGEMO is in the top 10 features predict-
ing the pre-enrollment reviews. This is especially
intriguing since the average rating for reviews in
the pre and post-enrollment reviews is the same
(see 4). We were concerned that possibly our sam-
pling had induced a bias in the ratings. But the av-
erage ratings in our sample are 4.18 and 4.19 pre
and post-enrollment respectively (difference is not
statistically significant).
FUNCTION WORDS occur extensively in the
post-enrollment reviews. We also observe that
inclusive (INCL) and exclusive (EXCL) terms are
used more in the post-enrollment reviews. Its pos-
sible that reviewers are seeking to be more bal-
anced. Products are described in personal (I),
perceptual (FEEL) and relativistic (SPACE) terms.
Pre-enrollment reviews discuss personal concerns
(LEISURE, RELIG) , indicate a level of certainty
(CERTAIN) and opinions are presented in terms of
thought process (INSIGHT). Interestingly, the pre-
enrollment reviews address the reader (YOU).
Opinions (OPINION) Features predicting post-
enrollment are number of objective sentences,
number of subjective sentences and finally num-
ber of other (neither subjective nor objective) sen-
tences. This suggests that reviewers try to write
somewhat more objectively in the post-enrollment
reviews.
Feature Combinations With the exception of
the combinations STYLE + OPINION , PHR/CLSL
+OPINION and PTM + OPINION which improve
on either feature set used alone, none of the
other combinations improved performance over
all component feature sets modeled individually.
Overall, none of the combinations improved over
LIWC SUB CATEG. Hence we do not delve fur-
ther into features from these models.
Summary Overall pre-enrollment reviews are
more complex (complex clauses, wide parse trees,
varied vocabulary, more words per sentence), have
fewer concepts per sentence, contain negative
emotions, addresses the reader directly and are
more certain. Post-enrollment reviews are longer,
more descriptive, contain comparisons, contain
quantifiers, have more positive emotion and de-
scribe the product experience in physical and per-
sonal terms.
22
Predicts PRE Enrollment
1 NP (Noun Phrase) 6 LST (List marker.
Includes surrounding
punctuation)
EXAMPLE EXAMPLE
NP
NN
person
DT
another
(3)
2 SBAR (Clause introduced by
a (possibly empty) subordinat-
ing conjunction)
7 VP (Verb Phrase)
EXAMPLE EXAMPLE
SBAR
S
VP
(...)
NP
(...)
IN
If
VP
NP
NN
person
DT
another
VBN
loved
3 SQ ( Inverted yes/no ques-
tion, or main clause of a wh-
question, following the wh-
phrase in SBARQ)
8 PRN (Parenthetical)
EXAMPLE EXAMPLE
SQ
VP
VB
matter
NP
PRP
it
VBZ
does
(p. 73)
4 NAC (Not a Constituent; used
to show the scope of certain
prenominal modifiers within an
NP)
9 SINV ( Inverted
declarative sentence,
i.e. one in which the
subject follows the
tensed verb or modal)
EXAMPLE EXAMPLE
NAC
?
?
NN
My
JJ
Oh
PRP;
My
?
?
SINV
.
.
VP
(...)
NP
PRP
it
VBD
did
CC
Nor
5 SBARQ (Direct question in-
troduced by a wh-word or a wh-
phrase)
10 NX (Used within
certain complex NPs to
mark the head of the
NP)
EXAMPLE EXAMPLE
SBARQ
.
?
SQ
VP
VBG
thinking
NP
PRP
you
VBD
were
WHNP
WP
what
,
,
S
VP
PRT
RP
on
VB
Come
NX
NNP
Love
NNP
Of
NNP
Nature
Table 5: Phr/Clsl: Top Features PRE
Predicts POST Enrollment
1 S (Simple declarative
clause)
6 FRAG (Fragment)
EXAMPLE EXAMPLE
S
NP
NNS
items
DT
the
PDT
all
RB
almost
FRAG
.
.
SBAR
S
VP
VP
VP
PP
NP
PP
NP
NNP
War
NNP
Cold
DT
the
IN
of
NP
NN
midst
DT
the
IN
in
VBN
sounded
VB
have
MD
must
NP
PRP
it
WHADVP
WRB
how
ADVP
RB
especially
2 ADJP ( Adjective
Phrase)
7 QP (Quantifier Phrase)
EXAMPLE EXAMPLE
ADJP
JJ
different
RB
so
RB
yet
QP
RB
just
IN
than
JJR
more
3 PRT (Particle. Cat-
egory for words that
should be tagged RP)
8 WHNP (Wh-noun Phrase)
EXAMPLE EXAMPLE
PRT
RP
up
WHNP
WDT
that
4 ADVP (Adverb
Phrase)
9 UCP (Unlike Coordinated
Phrase)
EXAMPLE EXAMPLE
ADVP
RBR
earlier
NP
NNS
years
CD
four
UCP
VP
VBG
gloating
ADVP
RB
just
CC
or
ADJP
JJ
true
5 X (Unknown, uncer-
tain, or unbracketable)
10 CONJP (Conjunction
Phrase)
EXAMPLE EXAMPLE
X
In
CONJP
IN
than
RB
rather
Table 6: Phr/Clsl: Top Features POST
23
Predicts PRE Enrollment
Feature Examples
ROOT? S (1) And nearly every sin-
gle item seemed cute and
usable to me. (2) Look
closely, (...) overwhelming
personal and cultural up-
heaval.
NP? NNP NNP (1) Tim Bess (2) Jennifer
Fitch
PP? IN NP (1) for its psychological and
emotional richness (2) of
loyalty
NP? DT NN (1) the price (2) a book
NP? NNP POS (1) Frost ?s (2) Clough ?s
ADJP? RBS JJ (1) most assuredly (2) most
entertaining
WHNP?WP (1) who (2) what
NP? NNP (1) Blessed (2) India
PP? TO NP (1) to the crime (2) to me
S? VP (1) linking Pye to the crime
scene (2) Gripping due to
(...)
Predicts POST Enrollment
Feature Examples
S? S , IN S . (1) It is functionally the
same as Apple?s 10 watt
charger which outputs 2.1
A , so it is also suitable for
charging the iPad. (2) It has
3 levels of trays that spread
as you open the box, so you
can easily access contents
in all trays.
S? IN NP VP . (1) So I don?t think the
investment in graphics (...)
enjoyability in the game.
(2) So we decided to try it
again this year.
ROOT? NP (1) Some kind of (...) disor-
der ? (2) Proper Alignment
and Posture; This segment
(...) .
S? S CC S . (1) Mage and Takumo (...)
but lacking in depth.(2) The
light feature is great and it
powers off (...).
NP? PRP$ NNP NN (1) your Alpine yodeling
(2) my MacBook Pro
S? VP . (1) Enough negativity. (2)
Suffice it to say that (...) .
NP? DT JJR NN (1) a better future (2) a
slower flow
NP? DT JJ , JJ NN (1) an immediate , visceral
reaction (2)a roots-based,
singer-songwriter effort
NP ? DT NNP NNP NNP
NNP
(1) the Post-Total Body
Weight Training (2) The
Gunfighter DVD Gregory
Peck
WHADVP?WRB RB (1) How far (2) how well
Table 7: ?
N
: Top Features (PCFG Non Terminal)
Predicts PRE Enrollment
Feature Category Examples
leisure Personal Concerns Cook, chat, movie
verb Function words Walk, want, see
certain Psych./Cognitive
Processes
always, never
insight Psych./Cognitive
Processes
think, know, con-
sider
negemo Psych./Affective Pro-
cesses
Hurt, ugly, nasty
exclam Exclamation !
period Period .
you Function words 2
nd
person , you,
your
preps Function words to, with, above
relig Personal Concerns 2
nd
synagogue, sa-
cred
Predicts POST Enrollment
Feature Category Examples
posemo Psych./Affective Pro-
cesses
Love, nice, sweet
article Function words a, an, the
i Function words 1
st
person singular.
space Psych./Relativity Down, in, thin
ingest Psych./Biological Pro-
cesses
Dish, eat, pizza
ipron Function words Impersonal Pro-
nouns, it its ,
those
incl Psych./Cognitive
Processes
Inclusive, and, with
, include
conj Function words and, but, whereas
excl Psych./Cognitive
Processes
Exclusive but,
without, exclude
feel Psych./Perceptual Pro-
cesses
feels , touch
Table 8: LIWC Sub Category : Top Features
These reviews are are specific, balanced and
contain more objective sentences as well.
Discussion on Readability One possibility is
that the ?Enrollment? effect leads to reviewers
writing more readable reviews. To test this hy-
pothesis we performed a paired t-test between
readability scores for pre and post-enrollment re-
views. Table 9 suggests that indeed this is the
case. Flesch Reading Ease is the only measure
where a higher score indicates simpler text. For
the rest of the measures a higher score implies
more complex text. All of the measures are within
the average readability range and the magnitude
of the differences are small. Nevertheless, these
differences are statistically significant
12
with one
exception lending support to the idea that ?Enroll-
ment? effect might lead to reviewers writing more
readable reviews.
12
The cell size for each class is 57,875, making the modest
difference in magnitude statistically significant.
24
Reading
Measure /Cite
Pre
Mean
Post
Mean
t
Value
ARI /(Senter and Smith,
1967)
9.16 9.15 (0.45)
Coleman Liau /(Coleman
and Liau, 1975)
8.76 8.68 (6.39)*
Flesch Kincaid /(Kincaid
et al., 1975)
8.75 8.71 (2.19)*
Flesch Reading Ease /(Kin-
caid et al., 1975)
65.63 66.18 6.61*
Gunning Fog /(Gunning,
1952)
11.75 11.70 (2.18)*
LIX /(Anderson, 1983) 38.24 38.07 (2.89)*
RIX /(Anderson, 1983) 3.74 3.71 (3.05)*
SMOG /(McLaughlin,
1969)
10.59 10.56 (2.56)*
* Significant at 5% level
Table 9: Readability Measures
7 Discussion
So far we have ignored the possibility that writ-
ing styles of reviewers may simply continuously
evolve with experience and we are simply detect-
ing a difference due to this underlying trend.
13
To address this question we investigated the sub-
periods within the the pre and post enrollment pe-
riods.
We split the post enrollment period (i.e. from
date of enrollment to the date the most recent re-
view was posted) further into two equal time pe-
riods for each reviewer. As before, we learn a
classifier to discriminate between the sub periods.
Interestingly the classifier performed the same as
chance at p=0.05 (Test Accuracy= 51.0%).
14 15
However a similar analysis in the pre-enrollment
period results in a test set accuracy of 63.3% (sig-
nificant at p=0.05). So there is a change in writing
style within the pre-enrollment period, but there is
no continued change post-enrollment. This is not
consistent with the continuous style evolution hy-
pothesis. One account would be that Amazon en-
rolls reviewers whose styles have stabilized. This
remains a possibility as Amazon actively selects
the members (and we are not aware of the specific
rules used by Amazon). The trends (see Figure 1
13
Ideally, if a) the enrollment date had been the same for
all reviewers and b) the enrollment was random, we would
have a clean experimental framework to detect whether a
similar trend exists for non-vine reviewers. Unfortunately,
this is not the case.
14
We report the results only on POS for conciseness. The
other feature sets performed similarly.
15
As before the test sample includes 50 users. However we
sampled only 10 reviews in each sub period. Corresponding
down sampled performance for Pre vs Post enrollment accu-
racy is 57.5% (significant at p=0.05)using POS features.
)suggest that there are changes right upto the en-
rollment dateand some levelling out in the post en-
rollment period , providing some evidence against
this hypothesis.
Figure 1: Feature Trends
Train
Size
Test
Size
Accuracy
Within Pre-
Enrollment
44,800 1000 63.3%
Within Post-
Enrollment
59,250 1000 51.0%
Pre vs Post Enroll.
Down Sampled
53,840 1000 57.5%
Table 10: Sub Period Results
8 Conclusion
We view this work as a first step toward inves-
tigating this phenomenon further. In particular,
we plan to test the robustness of our results w.r.t.
product specificity, to investigate stylistic differ-
ences (a) between reviews for purchased products
versus for products received for free amongst Vine
members and (b) between reviews by Vine review-
ers and non-Vine reviewers. Another line of in-
quiry involves decomposing the ?Enrollment? ef-
fect into a reputation/status effect (the influence of
the status badge - Vine membership) and a product
sampling effect (the influence of receiving goods
for free). Finally, investigating the temporal dy-
namics of style for these reviewers might prove in-
teresting as would determining whether these sub-
tle differences in style affect the readers and influ-
ence purchase decisions.
9 Acknowledgements
We would like to thank our reviewers for insight-
ful comments that we sought to address here. We
would also like to thank Myle Ott for generously
sharing the data.
25
References
Jonathan Anderson. Lix and rix: Variations on a
little-known readability index. Journal of Read-
ing, pages 490?496, 1983.
Vikas Ganjigunte Ashok, Song Feng, and Yejin
Choi. Success with style: Using writing style
to predict the success of novels. Poetry, 580(9):
70, 2013.
Kapil Bawa and Robert Shoemaker. The ef-
fects of free sample promotions on incremen-
tal brand sales. Marketing Science, 23(3):345?
363, 2004.
Shane Bergsma, Matt Post, and David Yarowsky.
Stylometric analysis of scientific articles. In
Proceedings of the 2012 Conference of the
North American Chapter of the Association for
Computational Linguistics: Human Language
Technologies, pages 327?337. Association for
Computational Linguistics, 2012.
Steven Bird, Ewan Klein, and Edward Loper.
Natural Language Processing with Python.
O?Reilly Media, 2009.
David M Blei, Andrew Y Ng, and Michael I Jor-
dan. Latent dirichlet allocation. the Journal of
machine Learning research, 3:993?1022, 2003.
Judith A Chevalier and Dina Mayzlin. The ef-
fect of word of mouth on sales: Online book
reviews. Journal of marketing research, 43(3):
345?354, 2006.
Meri Coleman and TL Liau. A computer readabil-
ity formula designed for machine scoring. Jour-
nal of Applied Psychology, 60(2):283, 1975.
Cristian Danescu-Niculescu-Mizil, Lillian Lee,
Bo Pang, and Jon Kleinberg. Echoes of power:
Language effects and power differences in so-
cial interaction. In Proceedings of the 21st
international conference on World Wide Web,
pages 699?708. ACM, 2012.
Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh,
Xiang-Rui Wang, and Chih-Jen Lin. Liblin-
ear: A library for large linear classification.
The Journal of Machine Learning Research, 9:
1871?1874, 2008.
Song Feng, Ritwik Banerjee, and Yejin Choi.
Characterizing stylistic elements in syntactic
structure. In Proceedings of the 2012 Joint Con-
ference on Empirical Methods in Natural Lan-
guage Processing and Computational Natural
Language Learning, pages 1522?1533. Associ-
ation for Computational Linguistics, 2012.
Shyam Gopinath, Jacquelyn S Thomas, and Lak-
shman Krishnamurthi. Investigating the rela-
tionship between the content of online word
of mouth, advertising, and brand performance.
Marketing Science, 2014.
Robert Gunning. Technique of clear writing.
1952.
Thorsten Joachims. A statistical learning learn-
ing model of text classification for support vec-
tor machines. In Proceedings of the 24th an-
nual international ACM SIGIR conference on
Research and development in information re-
trieval, pages 128?136. ACM, 2001.
J Peter Kincaid, Robert P Fishburne Jr, Richard L
Rogers, and Brad S Chissom. Derivation of new
readability formulas (automated readability in-
dex, fog count and flesch reading ease formula)
for navy enlisted personnel. Technical report,
DTIC Document, 1975.
Dan Klein and Christopher D Manning. Accurate
unlexicalized parsing. In Proceedings of the
41st Annual Meeting on Association for Com-
putational Linguistics-Volume 1, pages 423?
430. Association for Computational Linguis-
tics, 2003.
G Harry McLaughlin. Smog grading: A new read-
ability formula. Journal of reading, 12(8):639?
646, 1969.
Myle Ott, Yejin Choi, Claire Cardie, and Jeffrey T
Hancock. Finding deceptive opinion spam by
any stretch of the imagination. In Proceed-
ings of the 49th Annual Meeting of the Asso-
ciation for Computational Linguistics: Human
Language Technologies-Volume 1, pages 309?
319. Association for Computational Linguis-
tics, 2011.
Mauricio M Palmeira and Joydeep Srivastava.
Free offer 6= cheap product: A selective acces-
sibility account on the valuation of free offers.
Journal of Consumer Research, 40(4):644?656,
2013.
Michal Rosen-Zvi, Thomas Griffiths, Mark
Steyvers, and Padhraic Smyth. The author-
topic model for authors and documents. In Pro-
ceedings of the 20th conference on Uncertainty
in artificial intelligence, pages 487?494. AUAI
Press, 2004.
26
RJ Senter and EA Smith. Automated readabil-
ity index. Technical report, DTIC Document,
1967.
Kristina Shampanier, Nina Mazar, and Dan
Ariely. Zero as a special price: The true value of
free products. Marketing Science, 26(6):742?
757, 2007.
Yla R Tausczik and James W Pennebaker. The
psychological meaning of words: Liwc and
computerized text analysis methods. Journal of
Language and Social Psychology, 29(1):24?54,
2010.
Ivan Titov and Ryan McDonald. A joint model
of text and aspect ratings for sentiment sum-
marization. In Proceedings of ACL-08: HLT,
pages 308?316, Columbus, Ohio, June 2008.
Association for Computational Linguistics.
Vladimir N. Vapnik. Statistical Learning Theory.
Wiley-Interscience, 1998.
Monica Wadhwa, Baba Shiv, and Stephen M
Nowlis. A bite to whet the reward appetite:
The influence of sampling on reward-seeking
behaviors. Journal of Marketing Research, 45
(4):403?413, 2008.
Janyce Wiebe, Theresa Wilson, and Claire Cardie.
Annotating expressions of opinions and emo-
tions in language. Language resources and
evaluation, 39(2-3):165?210, 2005.
27
