A Trigger Language Model-based IR System 
ZHANG Jun-lin   SUN Le   QU Wei-min   SUN Yu-fang  
 
Open System & Chinese Information Processing Center 
Institute of Software, The Chinese Academy of Sciences 
P.O.BOX 8718,Beijing 100080 
junlin01@iscas.cn
 
Abstract 
Language model based IR system proposed in 
recent 5 years has introduced the language 
model approach in the speech recognition 
area into the IR community and improves the 
performance of the IR system effectively. 
However, the assumption that all the indexed 
words are irrelative behind the method is not 
the truth. Though statistical MT approach 
alleviates the situation by taking the 
synonymy factor into account, it never helps 
to judge the different meanings of the same 
word in varied context. In this paper we 
propose the trigger language model based IR 
system to resolve the problem. Firstly we 
compute the mutual information of the words 
from training corpus and then design the 
algorithm to get the triggered words of the 
query in order to fix down the topic of query 
more clearly. We introduce the relative 
parameters into the document language model 
to form the trigger language model based IR 
system. Experiments show that the 
performance of trigger language model based 
IR system has been improved greatly. The 
precision of trigger language model increased 
12% and recall increased nearly 10.8% 
compared with Ponte language model 
method. 
 
1 Introduction 
 
Using language models for information 
retrieval has been studied extensively 
recently(Jin et al2002 Lafferty and Zhai 2001 
Srikanth and Srihari 2002  Lavrenko and Croft 
2001 Liu and Croft 2002). The basic idea is to 
compute the conditional probability P(Q|D), i.e. 
the probability of generating a query Q given the 
observation of a document D. Several different 
methods have been applied to compute this 
conditional probability. In most approaches, the 
computation is conceptually decomposed into 
two distinct steps: (1) Estimating a document 
language model; (2) Computing the query 
likelihood using the estimated document model 
based on some query model. For example, Ponte 
and Croft emphasized the first step, and used 
several heuristics to smooth the Maximum 
Likelihood of the document language model, and 
assumed that the query is generated under a 
multivariate Bernoulli model (Ponte and Croft 
1998). The BBN method (Miller et al1999) 
emphasized the second step and used a two-state 
hidden Markov model as the basis for generating 
queries, which, in effect, is to smooth the MLE 
with linear interpolation, a strategy also adopted 
in Hiemstra and Kraaij (Hiemstra and  Kraaij 
1999). In Zhai and Lafferty (Zhai and  Lafferty 
2001), it has been found that the retrieval 
performance is affected by both the estimation 
accuracy of document language models and the 
appropriate modeling of the query, and a two 
stage smoothing method was suggested to 
explicitly address these two distinct steps. 
It?s not hard to see that the unigram 
language model IR method contains the 
following assumption: Each word appearing in 
the document set and query has nothing to do 
with any other word. Obviously this assumption 
is not true in reality. Though statistical MT 
approach (Berger and  Lafferty 1999 ) alleviates 
the situation by taking the synonymy factor into 
account, it never helps to judge the different 
meanings of the same word in varied context. In 
this paper we propose the trigger language model 
based IR system to resolve the problem. Though 
the basic idea of using the triggered words to 
improve the performance of language model was 
proposed by Raymond almost 10 years ago 
(Raymond et al1993), Our method adopts a 
different approach for other objectivity in the IR 
field. Firstly we compute the mutual information 
of the words from training corpus and then 
design the algorithm to get the triggered words of 
the query in order to fix down the topic of query 
more clearly. We introduce the relative 
parameters into the document language model to 
form the trigger language model based IR system. 
Experiments show that the performance of trigger 
language model based IR system has been 
improved greatly.   
In what follows, Section 2 describes trigger 
language model based IR system in detail. 
Section 3 is our evaluation about the model. 
Finally, Section 4 summarizes the work in this 
paper. 
2 Trigger Language Model based IR 
System 
 
2.1 Inter-relationship of Indexing Words 
  
In order to find out the inter-relationship of 
words in some specific context, we consider the 
co-occurring times of different words within 
fixed sized text window of the document. When 
the co-occurring time is large enough, we think 
that relationship is meaningful. Mutual 
Information is a common tool to be applied under 
this situation. So we compute the mutual 
information as following: 
 
)()()1(
),,(
)()(
)1(
),,(
),(
baw
wwba
w
b
w
a
ww
wba
ba
wNwNL
NLwwN
N
wN
N
wN
LN
LwwN
ww
???
?=
???
?
???
?????
?
???
?
??=?
             
(1) 
  
where  denotes the size of the vocabulary, 
 is the co-occurring times of 
word  and  within   sized window 
in training set.  is the count of the word 
 appearing in the training set and  is 
the count of word  appearing in the training 
set. 
wN
, wb L ),( a wwN
aw
aw
bw
(N
wL
)aw
bw
)( bwN
We use the corpus provided by IR task of 
NTCIR2 (NTCIR 2002) as the training set to 
compute the mutual information of words. This 
corpus contains nearly 100 thousands news 
articles encoding in BIG5 charset. We think the 
mutual information which is larger than 25 is 
meaningful. Considering the stop words in 
document or query are useless to represent the 
content, we remove 200 highest frequent words 
from the document before computation. Table 1 
shows some examples with higher mutual 
information. 
 
? ? 
(test) 
? ? ? (alphabet):1895 
??(rail):1353  
? ? (delimitation):758 
? ? ? ?
(windtunnel):473   
???(meter):421   
?? (test paper):403  
? ?
(missile) 
? ? ? ?
(antiaircraft):1063  
??(develop):708 
? ?? (long-range):472 
???(anti-tank):354 
?? 
(bribe) 
? ? (tax dodging):3462 
????(jobbery):2603 
????(FBI):1041 
???(voter):730 
??(zhanjiang):478 
???(Utah):427  
? ? ?
(truculency)
????(scrutator):710 
? ? ? ? (long-range 
missile):497  
????(terrorism):457 
? ? (biochemistry):390 
??(equipoise):327 
??(plague):334   
???(Bagdad):325   
 
      Table 1. Examples of Mutual Information 
2.2 Algorithm of Triggered Words by 
Query  
Generally speaking, a word always 
represents many different meanings and its exact 
meaning adopted in specific topic can be 
determined by the co-occurring words in its 
context. Different meaning of a word often lead 
to the different vocabulary set of related word. 
In order to find out the exact meaning of the 
words contained by the query in IR system, we 
design the algorithm to compute the triggered 
vocabularies of query. It is just these triggered 
words that show the exact meaning of the words 
in query in some specific context and help fix 
down the topic of query more clearly. The basic 
idea behind the algorithm is as following: By 
computing the mutual information, we can derive 
the relative words of a query word. All these 
words mean the semantically related vocabularies 
of the query word under different contexts. We 
propose that if the intersection of the derived 
related words of different words in query is not 
null, the words in the intersection is useful to 
judge the exact meaning of the words in query. 
At the same time, the more times an intersection 
word appears in related vocabulary set of 
different query word, the higher the weight of 
this word to fix down the topic of the query is. So 
we design the following algorithm to compute 
the triggered vocabulary set of query:     
 
Algorithm 1:Triggered vocabularies by query 
Input: Vocabulary set I of query word and its 
co-occurring words after removing the stop 
words in the query. 
},......,,......,,,{ 2211 ><><><><= nnii SqSqSqSqI
Output: Triggered vocabulary set T. 
 
Setp 1. Initialize the set ?=T . 
Setp 2. for(i =2;i<=n;i++) 
{ 
for(j=1;j<= ;j++) inC
{ 
2.1get the different 
combination },......,,,{ ,,2,2,1,1, ><><><= ijijjjjjj SqSqSqL
i
 
which contains  elements from set I ; 
2.2 if any vocabulary set )1(, ikS kj <=<  
in  contains no element, then we turn to 2.4 , 
otherwise we turn to 2.3; 
jL
2.3 Compute the intersection  of all 
vocabulary set  in . 
Here
jiT ,
< mw
)1(, ikS kj <=<
,......,, 221 ><> w
jL
>i },,{ 1, <=ji wT ??? ,
where
2
log i
w =? , ( iw <==<1 ). w?  is the 
word  weight decided by the length of ; jL
jiTT ,?=
w?
q
,,{ 11 ,,......, 22 <>>< mww ???
+? )|( jij dqp
><><=
=
other
ww
dq
q
ji
)
>m
)( i
cs
q
<wm,,,,{ 2211 ??
},.... )(Qli q
?
2.4 T , adopting the higher word 
weight during the merging process; 
} 
}  
Step 3. Output the triggered vocabulary set T ;    
      
2.3 Similarity Computation of Query and 
Document 
We use the similar strategy with Ponte 
language model method (Ponte and Croft 1998) 
to compute the similarity between the query and 
the document. That is, we firstly construct the 
simple language model according to the 
statistical information of vocabulary and then 
compute the generative probability of the query. 
The difference is that the trigger language model 
method takes the context information of a word 
into account. So we compute the triggered words 
set of query  according to algorithm 1.This 
way we get the triggered vocabulary set  
}><= mq wT . 
This set contains the words triggered by query 
and it is these triggered words that determine the 
exact meaning of the vocabularies in query 
among the several optional choices. This helps 
fix down the topic of query more clearly.   
Introducing the triggered words factor into the 
document language model, we can form the 
trigger language model based information 
retrieval system. 
The similarity of query and document can 
be computed as following: 
? ?
= =
=
)(
1
)(
1
()|(
Ql
i
dl
j
d
tf
CMQP   (2) 
??
??
?
???= Tddqdqp jjijji
0
}),......()(
1
)|( ?  (3) 
 
(1) ,......,{ 21 qqqQ =  denotes query 
and is the length of the query;  )(Ql
(2) denotes the trigger language model of 
document ;  
dM
d
(3) denotes a 
document in document set and is the length 
of the document; 
},....,....,{ )(21 dlj ddddd =
(l )d
(4) 
)(
)(
dl
df
C jj =
jd
 is the weight parameter of 
words  in a document. Here  means 
the account of the words  appearing in the 
document. 
)( jdf
jd
(5)  denotes the probability of  
being triggered by the document word .When 
2 words are same, the probability equals 1. If 
they are different and the word  belongs to 
the triggered vocabulary set of query, the 
probability equals the according parameter in the 
,otherwise the probability is 0? 
)|( ji dqp iq
jd
jd
qT
(6)
cs
qtf i )(
)iq
iq
is used for data smoothing; here 
 denotes times of query word  
appearing in document set and   denotes the 
total length of documents which contains the 
word . 
(tf iq
cs
 
3 Experiment Results  
3.1 Corpus 
 
The corpus we used to evaluate the 
performance of our proposed trigger language 
model IR system is the document set offered by 
the traditional Chinese Document set of NTCIR3 
for the IR task. The corpus consists of 381681 
news articles from Hong Kong and Taiwan with 
varied topics. After the word segmentation, the 
document set contains 150700953 words. Among 
them,127519 different words are the entries of 
the vocabulary. The average length of each 
document is 394. 
The 50 queries offered by NTCIR3 IR task 
are contained in a XML file and each query 
consists of following elements: Topic 
Number(NUM),Topic Title(TITLE),Topic 
question(DESC),Topic Narrative(NARR) and 
Topic Concepts(CONC). In order to make it 
easer to compare the performance of the different 
IR methods, we adopt the Topic Question field as 
the query and regard the top 1000 retrieval 
documents as the standard result of the 
experiment.    
 
3.2 Analysis of Experiment Results 
We design 3 relative experiments to 
evaluate the trigger language model IR method: 
vector space model, Ponte language model based 
method and the trigger language model approach. 
Precision and recall are two main evaluation 
parameters. As for the trigger model IR method, 
the optimal size of the text window is 20 content 
words and the mutual information over 25 is 
regarded as the meaningful information. 
Experiment results can be seen in table 2. 
The data of column %  in table 2 shows 
the performance improvement of Ponte language 
model compared with vector space model. The 
data tells us that the precision of language model 
based method increased 10% and recall increased 
nearly 13.7%. The data of column %?  in table 
2 shows the performance improvement of trigger 
language model compared with Ponte language 
model method. From the data we can see that the 
precision of trigger language model increased 
12% and recall increased nearly 10.8%. We can 
draw the conclusion that the trigger language 
model has improved the performance greatly. 
The performance comparison can be showed 
more clearly in figure 1. 
1?
2
    
 
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0 0.2 0.4 0.6 0.8 1
Recall
Pr
ec
is
io
n
tfidf
Ponte
Language
Model
Trigger
Language
Model
 
 Figure 1. Precision-Recall of 3 methods 
 
  Tfidf Lm(ponte) Trigger lm % 1?  %  2?
Relevant: 3284 3284 3284 ----
- 
----
- 
Rel.ret: 1843 2096 2322 13.7 10.8 
Precision:      
0. 00 
0. 10 
0. 20 
0. 30 
0. 40 
0. 50 
0. 60 
0. 70 
0. 80 
0. 90 
1. 00 
Avg: 
0. 6016 
0. 4607 
0. 3812 
0. 3336 
0. 2738 
0. 2495 
0. 2179 
0. 1566 
0. 0978 
0. 0389 
0. 0019 
0.2377 
0.6109 
0.4844 
0.4123 
0.3757 
0.3255 
0.2854 
0.2313 
0.1716 
0.1041 
0.0474 
0.0025 
0.2610 
0. 7537 
0. 5314 
0. 4541 
0. 4094 
0. 3648 
0. 3237 
0. 2538 
0. 2011 
0. 1153 
0. 0435 
0. 0055 
0. 2933 
+2 
+5 
+8 
+12 
+18 
+14 
+6 
+9 
+6 
+21 
+31 
+10 
+23 
+10 
+10 
+9 
+12 
+13 
+9 
+17 
+10 
-8 
+120 
+12 
 
Table 2. Experiment results
 
4  Conclusion  
Language model based IR system proposed 
in recent 5 years has introduced the language 
model approach in the speech recognition area 
into the IR community and improves the 
performance of the IR system effectively. 
However, the assumption that all the indexed 
words are irrelative behind the method is not the 
truth. Though statistical MT approach alleviates 
the situation by taking the synonymy factor into 
account, it never helps to judge the different 
meanings of the same word in varied context. In 
this paper we propose the trigger language model 
based IR system to resolve the problem. . Firstly 
we compute the mutual information of the words 
from training corpus and then design the 
algorithm to get the triggered words of the query 
in order to fix down the topic of query more 
clearly. We introduce the relative parameters into 
the document language model to form the trigger 
language model based IR system. Experiments 
show that the performance of trigger language 
model based IR system has been improved 
greatly. 
Acknowledgement 
This work is supported by Beijing New Star Plan 
of Technology & Science(NO.H020820790130) 
and the National Science Fund of China under 
contact 60203007. 
References  
Berger A. and  Lafferty J. (1999). Information 
retrieval as statistical translation. In Proceedings of 
SIGIR ?99. pp. 222-229. 
 
Jin R., Hauptmann A.G.  and Zhai C.(2002) Title 
Language Model for Information Retrieval. In 
Proceedings of the 2002 ACM SIGIR Conference 
on Research and Development in Information 
Retrieval. 
 
Hiemstra D. and  Kraaij W. (1999), Twenty-One at 
TREC-7: ad-hoc and cross-language track, In 
Proceedings of the seventh Text Retrieval 
Conference TREC-7, NIST Special Publication 
500-242, pages 227-238, 1999. 
 
Lafferty J. and Zhai. C. (2001) Document language 
models,query models and risk minimization for 
information retrieval. In Proceedings of the 24th 
ACM SIGIR Conference,pp.111-119. 
 
Lavrenko,V., and Croft,W.B.(2001) .Relevance based 
language models. In Proceedings of the 24th ACM 
SIGIR Conference.pp.120-127. 
 
Liu,X. and Croft,W.B.(2002).Passage Retrieval 
Based on Language Models. In Proceedings of the 
11th  International Conference on Information and 
Knowledge Management. Pp.375-382 
 
Miller D.,  Leek T.  and Schwartz  R. M. (1999). 
A hidden Markov model information retrieval 
system. Proceedings of SIGIR?1999, pp. 214-222. . 
 
NTCIR Workshop 
(research.nii.ac.jp/ntcir/index-en.html) 
  
Ponte J. and Croft W. B. (1998). A language 
modeling approach to information retrieval. In 
Proceedings of SIGIR? 1998, pp. 275-281. 
 
Raymond Lau, Roni Rosenfeld and Salim 
Roukos(1993) Trigger-based Language Models: A 
Maximum Entropy Apporach. Proceedings ICASSP 
'93, Minneapolis, MN, pp. II-45 - II-48. 
 
Srikanth M. and  Srihari. R(2002). Biterm 
Language Models for Document Retrieval. In 
Proceedings of the 2002 ACM SIGIR Conference 
on Research and Development in Information 
Retrieval.  
 
Zhai C. and Lafferty J.(2001). A study of smoothing 
methods for language models applied to ad hoc 
information retrieval. In Proceeding of SIGIR?01, 
2001, pp. 334-342. 
 
 
 
Constructing of a Large-Scale Chinese-English  
Parallel Corpus 
 
Le Sun, Song Xue, Weimin Qu, Xiaofeng Wang,Yufang Sun 
Chinese Information Processing Center 
Institute of Software, Chinese Academy of Sciences 
Beijing 100080, P. R. China 
lesun, bradxue, qwm, wxf, yfsun@sonata.iscas.ac.cn 
 
Abstract  
This paper describes the constructing of a 
large-scale (above 500,000 pair sentences) 
Chinese-English parallel corpus. The current 
status of Chinese corpora is overviewed with 
the emphasis on parallel corpus. The XML 
coding principles for Chinese?English 
parallel corpus are discussed. The sentence 
alignment algorithm used in this project is 
described with a computer-aided checking 
processing. Finally, we show the design of 
the concordance of the parallel corpus and 
the prospect to further development. 
Introduction 
With the development of the corpus linguistics, 
more and more language resources have been 
established and used in language engineering 
research and applications. As we all know, there 
are different kinds of corpora for different kinds 
applications. For example, the Chinese 
Part-Of-Speech annotation corpus used to train 
program for Chinese word segmentation and 
POS tag, the Chinese tree bank used to Chinese 
syntax study, and so on.  
 
In this paper the constructing of a large-scale 
Chinese-English parallel corpus, which is totally 
above 500,000 pair sentences and the first year 
task is 100,000 pair sentences, is described. The 
applications of the large-scale Chinese-English 
parallel corpus put emphasis on the sentence 
template extracting for EBMT (Example-Based 
Machine Translation) and translation model 
training for SBMT (Statistical-Based Machine 
Translation). The latent applications may include 
the bilingual lexicon extraction, special term or 
phase extraction, bilingual teaching, 
Chinese-English contrastive study, etc.  
 
Numerous corpus data gathering efforts exit all 
of the world. The rapid multiplication of such 
efforts has made it critical to create a set of 
standards for encoding corpora. CES (Corpus 
Encoding Standard), which is conformant to the 
TEI Guideline for Electronic Text Encoding and 
Interchange of the Text Encoding Initiative (TEI 
2002), has been adopted by many corpus-based 
work. The XML Corpus Encoding Standard 
(XCES) is a part of the Guideline developed by 
the Expert Advisory Group on Language 
Engineering Standards (Ide, N., Bonhomme, P., 
Romary, L. 2000). The coding of our 
Chinese-English Parallel Corpus is in broad 
agreement with the TEI Guideline for electronic 
texts. 
 
In the following section, we first present a brief 
review of the current status of Chinese corpora 
with the emphasis on parallel corpus. Then the 
XML coding principles for Chinese?English 
parallel corpus are discussed in detail. Following 
this is the sentence alignment algorithm used in 
this project with a computer-aided checking 
processing. Finally, we show the design of the 
concordance of the parallel corpus and the 
prospect to further development. 
1 Chinese Corpus Project Overview 
The Chinese Corpus constructing work started in 
1920?s, See Zhiwei Feng (2001). The 
machine-readable corpora established in 1980?s 
are listed as following: 
 Chinese Modern Literature Corpus 
(1979), 5.27 Million Chinese 
Characters, WuHan University; 
 Modern Chinese Corpus (1983), 20 
Million Chinese Characters, Beijing 
University of Aeronautics and 
Astronautics; 
 Middle School Chinese Book 
Corpus (1983), 1.06 Million Chinese 
Characters, Beijing Normal University; 
 Modern Chinese Word Frequency 
Corpus (1983), 1.82 million Chinese 
characters, Beijing Language & 
Culture University. 
 
The first national large-scale Chinese corpus 
project is proposed in 1991 by State Language 
Commission in China. The Chinese texts used in 
this corpus are selected carefully under the 
condition of times, genre, and field. Now the 
corpus is about 20 million Chinese characters. 
 
From 1992, there are several large-scale Chinese 
corpus constructed by different institutes. The 
most noticeable in them is the Chinese POS 
annotation corpus accomplished by Institute of 
Computational Linguistics, Peking University, 
with the cooperation with Fujitsu Company. The 
content of this corpus is people?s daily, one of 
the most popular newspapers in China. The 
Chinese texts are segmented and added POS tag 
with high precision. The total Chinese 
Characters are about 27 million.  
 
There are several Chinese corpora in Tsinghua 
University also. The corpus, which is used for 
Chinese segmentation study, includes 100 
million Chinese characters. The Hua Yu corpus 
(2 million Chinese characters) is a POS tagged 
field-balance corpus. And the 10 percent of this 
corpus has been used for constructing Chinese 
tree bank. 
 
These are also other valuable Chinese corpora 
established in ShanXi University, Harbin 
technical University, ShangHai Normal 
University, City University of Hong Kong, 
Taiwan Academia Sinica, University of 
Pennsylvania and so on. Please refer to Zhiwei 
Feng (2001) for detail.  
 
In October 2001, a national corpus project, that 
is, national 863 project about Chinese 
Information Processing Platform, is launched. 
It?s a cooperation project between five institutes 
in China, including Institute of Software, 
Chinese Academy of Sciences, Institute of 
Computational Linguistics, Peking University, 
Tsinghua University, Nanjing University and 
Institute of Language, State Language 
Commission. The content of corpora and 
intended scale in this project are showed in table 
1 in detail. The large-scale Chinese-English 
parallel corpus described in this paper is one of 
the scheming corpora in this project. 
 
The multilingual corpus is important for 
computational linguistics research and 
contrastive linguistics study. So there are many 
multilingual corpus have been established or 
being developed in many institutes in China 
mainland. The table 2 shows the 
Chinese-English parallel corpus had been 
constructed in Mainland China. There are also 
some bilingual corpora about other language pair, 
such as Chinese-Japanese, Chinese-German, etc.
 
 
Sub-Project Name Responsible Institute First-Year  Scale 
Scheming
 Scale 
Chinese Balance Corpus State Language Commission 70 MCC 150 MCC
Chinese-English Parallel Corpus  IOS, Chinese Academy of Science 100 TS 500 TS 
Chinese POS Annotation Corpus ICL, Peking University 7 MCC 30 MCC 
Chinese Tree Bank Tsinghua University 15 TS 60 TS 
Chinese Concept Dictionary ICL, Peking University 20 TC 60 TC 
Chinese Semantic Knowledge Base Tsinghua University 8 TW 24TW 
Table 1 The 863 Chinese corpus project 
 
MCC: Million Chinese Character       TS: Thousand Sentence 
TC: Thousand Concept                 TW: Thousand Word 
Institute  Corpus Describing  
 Scale 
ICL, Peking University Sentence & Phrase Alignment 5 TS 
Harbin Institute of Technology  Sentence, Phrase, Word Alignment Above 5 TS 
State Language Commission Computer Science and Plato Unknown 
Beijing Foreign Studies University Literary, Science and Civilization in 
China 
Unknown 
Northeastern University Sentence & Phrase Alignment Unknown 
IOS, Chinese Academy of Science Sentence Alignment  8 TS 
Table 2 The Chinese-English parallel corpus in Mainland 
 
It has been noticed by many scholars that we 
should build a principle for sharing language 
resource in research work and to avoid the waste 
in time and effort in repeated construction. 
 
2 Resource Collection 
Unlike single linguistic resource, the parallel 
resource for special language pair is limited no 
matter what language pair is. Although the 
Chinese and English both are most popular 
language in the world, we still encounter much 
difficult in obtaining parallel corpus resource 
from Internet for following reasons: 
There are seldom web pages in China 
provide the same content in English 
pages and in Chinese pages; 
The English news in web are translated 
freely other than literally with many 
content omission; 
Some bilingual texts are restricted and 
used only to member.  
 
After two years efforts, there are totally about 
16,000KB untagged Chinese-English parallel 
texts in hand. The genres of the resource we 
collected are showed in table 3. 
 
 
Chinese Genre About Percent
News 10% 
Literature 30% 
Government 
Report 25% 
Sciences & 
Technology 35% 
Table 3 The genre in parallel texts 
3 Coding 
3.1 General Principles 
The coding of the parallel corpus is in broad 
agreement with the TEI Guideline for electronic 
texts. The eXtendible Make-up Language (XML) 
is used for the text coding. Textual features are 
marked by tags enclosed within angle brackets. 
For example, a title is marked by start tag <title> 
and an end tag </title >. Every element has some 
attributes to identifier of the element.  
 
The document type definition (DTD) for the 
texts in the corpus may differs in some respects 
from the TEI model. The general principle for 
coding are based on following consideration: 
Comply with TEI guide lines on the 
whole; 
Define the tag with clear meaning used 
by most people in china; 
Only used the attributes which can be 
easily and automatically get from source 
texts, except the alignment link, which is 
the key attribute in this corpus and 
several steps are used to keep high 
precise (See section 4 for detail); 
Try to keep all the interim resource in 
hand in case information loses, such as, 
the title tag in HTML files. 
 
The overall structure of a Chinese-English 
Parallel corpus is shown by this example: 
<article id=?UH001?> 
<Header type =?Unix Handbook?> 
</Header> 
<text> 
</text> 
</article> 
There are two main parts in a text: a header and 
the main text. Every text has an unique identifier 
that is, article id, in this case UH001 (indicating 
text 001 of the Unix Handbook) 
3.2 The header 
Each text is described by a header, which has 
four parts in accordance with the TEI guidelines: 
a file description, an encoding description, a 
profile description, and a revision description. 
The file description gives bibliographical 
information on the source text. The elements 
include title, author, www address (If the text is 
obtain from Internet), etc. The encoding 
description in our corpus is very brief, only the 
project name and the DTD file name are listed.  
 
The country or region use the language is 
indicated in the profile description. The 
description under <language> used in our corpus 
is in terms of labels like: Mainland Chinese 
(MaC), Hong Kong Chinese (HKC), Taiwan 
Chinese (TwC), Singapore Chinese (SiC), 
American English (AmE), British English (BrE), 
Canadian English (CaE), etc. 
 
Another tag used in the profile description is 
<textclass>. According to the parallel resource 
in hand, the texts are grouped into 4 genres (as 
show in table 3), such as, News , Literature, 
Science & Technical?Government Report. 
A series of changes are listed in the revise 
description and specified the change, the date of 
the change, the person responsible for the 
change, and the nature of the change. 
3.3 Text Units and Alignment Unit 
The corpus texts are segmented according to the 
natural units, such as: chapter, paragraph, 
sentence (S-unit), and word. The English words 
are simply marked by spacing as in ordinary 
written text. The Chinese words are not 
indicated by space in order to avoid the segment 
error. 
 
An ID is given to every paragraph to indicate the 
relative position in whole chapter. The sentence 
is called S-unit, the same as Johansson, Ebeling 
and Oksefjell (1999) to underline that they are 
not necessarily sentences in a grammatical sense. 
 
The sentence alignment type between Chinese 
S-unit and English S-unit maybe 1:1, 2:1, 3:1, 
1:2, 1:3,2:2, 3:2, 2:3. Links between parallel 
texts are showed by attributes of S-Alignment. 
One of the Chinese alignment unit (it may 
beyond one S-unit) are linked with the 
correspondence English alignment unit.   
3.4 Sample Text 
A sample text of our Chinese-English parallel 
corpus is showed in figure 1.  
 
 
Figure1 Sample Text  
4 Sentence Alignment 
4.1 Algorithm Overview 
The key attribute in this corpus is alignment link, 
which connect the one or more Chinese sentence 
with one or more correspond English sentence. 
In order to keep high precise in sentence 
alignment, several steps are used with the human 
and computer cooperation. 
 
The first step to extract structural information for 
parallel corpus is paragraph alignment and 
sentence alignment, that is noting which 
paragraph and sentence in one language 
correspond to which paragraph and sentence in 
another language.  
 
This problem has been studied by many 
researchers and a number of quite encouraging 
results have been reported. However, almost all 
bilingual corpora used in research are clear 
(nearly without sentence omission or insertion) 
and literal translation bilingual texts. The 
performance tends to deteriorate significantly 
when these approaches are applied to noisy 
complex corpora (with sentence omission or 
insertion, less literal translation). 
 
There are basically three kinds of approaches on 
sentence alignment: the length-based approach 
(Gale & Church 1991 and Brown et al 1991), 
the lexical approach (key & Roscheisen 1993), 
and the combination of them (Chen 1993, Wu 
1994 and Langlais 1998, etc.). 
 
The first published algorithms for aligning 
sentences in parallel texts are length-based 
approach proposed by Gale & Church (1991) 
and Brow et al(1991). Based on the observation 
that short sentences tend to be translated as short 
sentences and long sentences as long sentences, 
they calculate the most likely sentence 
correspondences as a function of the relative 
length of the candidates. The basic approach of 
Brow et al is similar to Gale and Church, but 
works by comparing sentence length in words 
rather than characters. While the idea is simple, 
the models can still be quite effective when used 
to clear and literal translated corpora. Once the 
algorithm had accidentally mis-aligned a pair 
sentence, it tends to be unable to correct itself 
and get back on track before the end of the 
paragraph. Use alone, length-based alignment 
algorithms are therefore neither very robust nor 
reliable. 
 
Kay & Roscheisen (1993) use a partial 
alignment of lexical items induce a maximum 
likelihood at sentence level. The method is 
reliable but time consuming.  
 
Chen (1993) combines the length-based 
approach and lexicon-based approach together. 
A translation model is used to estimate the cost 
of a certain alignment, and the best alignment is 
found by using dynamic programming as the 
length-based method. The method is robust, fast 
enough to be practical and more accurate than 
previous methods. 
 
The first sentence alignment model used to align 
English-Chinese bilingual texts is proposed by 
Wu (1994). For lack of cognates in 
English-Chinese, he used lexical cues to add the 
robust of his model. 
 
All of these works are test on nearly clear and 
literal translation bilingual corpora. 
 
There are seldom papers related to paragraph 
alignment. It's believed by most of the 
researchers that the paragraph alignment is an 
easier task than sentence alignment. Gale & 
Church (1991) suggest that the same 
length-based algorithm can be used to align 
paragraph also. 
 
4.2 The Alignment Steps 
Sentence alignment algorithm of our system can 
be outlined as follows: 
 
Step 1: Align sentence by the improved 
length-based algorithm.(Desicibed in Sun 
etc. 1999) 
Step 2: A lexicon checking process is added to 
judge all the alignment results in step 1. 
A score is given to every alignment pair 
(A Chinese word segmentation system is 
used in this process to find Chinese 
word). 
Step 3: The alignments whose score above a 
threshold C1 are judged as correct 
alignment. Remove these correct 
alignments from bilingual texts temporally. 
Step 4: The rest parts are aligned again by length 
based approach. 
Step 5: Repeat step 2, the score of every 
lignment is showed as a reference to human 
checking. 
 
4.3 Computer-Aided Checking 
It's obviously difficult to increase greatly the 
accuracy and robust of sentence alignment only by 
length based approach. So a lexicon checking 
process is added to our system. The alignment 
results obtained by length based approach are 
checked by an English-Chinese lexicon. A score SA 
is given to every alignment sentence pair. The 
score SA is calculated by following idea, that is, 
the twice number of correctly matched English 
words and Chinese words to the sum of number 
of English and Chinese words. In figure 2, the 
interface for human checking is showed in order 
to processes the noise Chinese-English parallel 
resource. 
 
4.4 Experiment Results 
 
We tested our alignment algorithm with part of a 
computer handbook (Sco Unix handbook). There 
are about 4681 English sentences and 4430 
Chinese sentences in this computer handbook 
after filter noisy figures and tables. The detail 
experiment result of automatic sentence 
alignment is show in table 4. The total precision 
is about 95%.  
 
 
Figure 2 Interface for Human Checking 
 
Class of 
Alignment 
No. of 
Aligned Sentence 
Pair 
No. of 
Correct 
Sentence Pair 
No. of 
Error 
Sentence Pair 
 
Precision 
1:1 2992 2957 35 98.83% 
1:2 238 211 27 88.66% 
2:1 414 352 62 85.02% 
2:2 113 97 16 85.84% 
1:3 35 24 11 68.57% 
3:1 75 49 26 65.33% 
2:3 13 6 7 46.15% 
3:2 22 16 6 72.72% 
3:3 6 3 3 50.00% 
0:1 3 2 1 66.67% 
1:0 7 4 3 75.00% 
Total 3918 3721 197 94.97% 
Table 4 The detail experiment result of automatic sentence alignment 
5 Bilingual Concordance Design 
We also designed a bilingual concordance tool 
used for discovering facts during the translation 
between Chinese and English. Besides a listing 
of the keywords with the contexts in which they 
appear, the correspondence translation sentence 
also be presented in this tool. The options may 
include bilingual concordances, sorting in a 
variety of orders, and producing basic text 
statistics. The intended interface is showed in 
figure 3.  
  
 
Figure 3 The Interface for bilingual Concordance 
 
6 Conclusion & Further Prospects 
In this paper, we introduce the developing project, 
that is, the constructing of a large-scale (above 
500,000 pair sentences) Chinese-English parallel 
corpus. The current status of Chinese corpora is 
overviewed with the emphasis on parallel corpus. 
The XML coding principles for Chinese?English 
parallel corpus are discussed. The sentence 
alignment algorithm used in this project is 
described with a computer-aided checking 
processing in order to processes the noise 
Chinese-English parallel resource.. We show the 
design of the bilingual concordance for the 
parallel corpus, also.  
 
As a beginning project, there is still much room 
for further development. The parallel resource is 
relative rare, so the new ways, such as, data 
exchange with other researcher institute and 
translation company, should be launched to 
obtain more parallel resource which can be used 
to research society. The coding principle should 
be adjusted in real work. A coding rule in more 
detail should form in near future. We also intend 
to add the option for recommendation the 
correspondence translation word for input 
keywords in concordance tool.   
 
Acknowledgements 
This work is supported by China 863 project 
(Grant No. 2001AA114040) and the National 
Science Fund of China under contact 69983009. 
Our thanks go to all the project members from 
five institutes for discussion and the anonymous 
reviewers for kind suggestions. . 
 
References  
Catherine N. Bal (1997), Tutorial: 
Concordances and Corpora, 
http://www.georgetown.edu/cball 
/corpora/tutorial.html  
D. Wu, (1994) Aligning a Parallel 
English-Chinese Corpus Statistically with 
Lexical Criteria, In Proceedings of the 32nd 
Annual Meeting of the Association for 
Computational Linguistics (ACL'94), 
pp.80-87 
I. D. Melamed. (1996)  Automatic Detection of 
Omissions in Transaltions, In Proceedings of  
the 16th International Conference on 
Computational Linguistics, Copenhagen, 
Denmark 
ISLE, International Standards for Language 
Engineering 
http://www.ilc.pi.cnr.it/EAGLES96/isle/ISLE
_Home_Page.htm 
J.S. Chang and M. H. Chen (1997)  An 
alignment method for noisy parallel corpora 
based on image processing techniques, In 
Proceedings of the 35th Meeting of the 
Association for Computational Linguistics, 
Madrid, pp. 297-304 
Kay M., and Roscheisen M. (1993). 
Text-Translation Alignment, Computational 
Linguistics, 19/1,pp.121-142 
Le Sun , Lin Du, Yufang Sun, Jin Youbin (1999) 
Sentence Alignment of English-Chinese 
Complex Bilingual Corpora. Proceeding of 
the workshop MAL'99, 135-139 
N. Ide, L. Romary (2001). A Common 
Framework for Syntactic Annotation 
Proceedings of ACL'2001, Toulouse, 298-305 
N. Ide, L. Romary, (2000) XML Support for 
Annotated Language Resources. Proceedings 
of the Workshop on Web-based Language 
Documentation and Description, 
Philadelphia, 148-153. 
N. Ide, P. Bonhomme,, L. Romary (2000). 
XCES: An XML-based Standard for 
Linguistic Corpora.. Proceedings of the 
Second Language Resources and Evaluation 
Conference (LREC), Athens, Greece, 825-30. 
P. F. Brown, J. C. Lai, and R. L. Mercer (1991) 
Aligning Sentences in Parallel Corpora, In 
Proceedings of the 29th Annual Meeting of 
the Association for Computational 
Linguistics (ACL'91), pp.169-176. 
P. Fung, and K. W. Church (1994)  K-vec: A 
New Approach for Aligning Parallel Texts, In 
Proceedings of the 15th International 
Conference on Computational Linguistics 
(COLING'94), Tokyo, Japan, pp. 1096-1102, 
Ph. Langlais, M. Simard, J. Veronis, 
S.Armstong, P. Bonhomme, F. Debili, P. 
Isabelle, E. Souissi, and P. Theron. (1998)  
Arcade: A cooperative research project on 
parallel text alignment evaluation. In First 
International Conference on Language 
Resources and Evaluation, Granada, Spain. 
S. F. Chen, (1993) Aligning Sentences in 
Bilingual Corpora Using Lexical Information. 
In Proceedings of the 31th Annual Meeting 
of the Association for Computational 
Linguistics, pp. 9-16 
Shiwen Yu, Xuefeng Zhu, Hui Wang, Yunyun 
Zhang (1998), The Grammatical 
Knowledge-base of Contemporary Chinese: 
A complete Specification. Tsinghua 
University Publishers 
Stig Johansson, Jarle Ebeling, Signe Oksefjell 
(1999), English-Norwegian Parallel 
Corpus:Manual, 
http://www.hf.uio.no/iba/prosjekt/ 
TEI (2002) The XML Version of the TEI Guidelines   http://www.hcu.ox.ac.uk/TEI/Guidelines/ 
W. A. Gale, and K. W. Church (1991) A Program 
for Aligning Sentences in Bilingual Corpora, 
In Proceedings of the 29th Annual Meeting of 
the Association for Computational Linguistics 
(ACL'91), pp. 177-184 
Zhiwei Feng (2001), The History and Current 
status of Chinese Corpus Research, 
International Conference on Chinese 
Computing ICCC2001, pp. 1-15 (In Chinese)  
 
