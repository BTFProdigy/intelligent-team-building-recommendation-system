Support Vector Machines Applied to the Classification of Semantic Relations
in Nominalized Noun Phrases
Roxana Girju
Computer Science Department
Baylor University
Waco, Texas
girju@cs.baylor.edu
Ana-Maria Giuglea, Marian Olteanu,
Ovidiu Fortu, Orest Bolohan, and
Dan Moldovan
Department of Computing Science
University of Texas at Dallas
Dallas, Texas
moldovan@utdallas.edu
Abstract
The discovery of semantic relations in text
plays an important role in many NLP appli-
cations. This paper presents a method for the
automatic classification of semantic relations
in nominalized noun phrases. Nominalizations
represent a subclass of NP constructions in
which either the head or the modifier noun is
derived from a verb while the other noun is an
argument of this verb. Especially designed fea-
tures are extracted automatically and used in a
Support Vector Machine learning model. The
paper presents preliminary results for the se-
mantic classification of the most representative
NP patterns using four distinct learning mod-
els.
1 Introduction
1.1 Problem description
The automatic identification of semantic relations in text
has become increasingly important in Information Ex-
traction, Question Answering, Summarization, Text Un-
derstanding, and other NLP applications. This paper dis-
cusses the automatic labeling of semantic relations in
nominalized noun phrases (NPs) using a support vector
machines learning algorithm.
Based on the classification provided by the New Web-
ster?s Grammar Guide (Semmelmeyer and Bolander
1992) and our observations of noun phrase patterns on
large text collections, the most frequently occurring NP
level constructions are: (1) Compound Nominals consist-
ing of two consecutive nouns (eg pump drainage - an IN-
STRUMENT relation), (2) Adjective Noun constructions
where the adjectival modifier is derived from a noun (eg
parental refusal - AGENT), (3) Genitives (eg tone of con-
versation - a PROPERTY relation), (4) Adjective phrases
in which the modifier noun is expressed by a preposi-
tional phrase which functions as an adjective (eg amuse-
ment in the park - a LOCATION relation), and (5) Adjec-
tive clauses where the head noun is modified by a relative
clause (eg the man who was driving the car - an AGENT
relation between man and driving).
1.2 Previous work on the discovery of semantic
relations
The development of large semantically annotated cor-
pora, such as Penn Treebank2 and, more recently, Prop-
Bank (Kingsbury, et al 2002), as well as semantic
knowledge bases, such as FrameNet (Baker, Fillmore,
and Lowe 1998), have stimulated a high interest in the
automatic acquisition of semantic relations, and espe-
cially of semantic roles. In the last few years, many re-
searchers (Blaheta and Charniak 2000), (Gildea and Ju-
rafsky 2002), (Gildea and Palmer 2002), (Pradhan et
al. 2003) have focused on the automatic prediction of se-
mantic roles using statistical techniques. These statistical
techniques operate on the output of probabilistic parsers
and take advantage of the characteristic features of the
semantic roles that are then employed in a learning algo-
rithm.
While these systems focus on verb-argument semantic
relations, called semantic roles, in this paper we inves-
tigate predicate-argument semantic relations in nominal-
ized noun phrases and present a method for their auto-
matic detection in open-text.
1.3 Approach
We approach the problem top-down, namely identify and
study first the characteristics or feature vectors of each
noun phrase linguistic pattern and then develop models
for their semantic classification. The distribution of the
semantic relations is studied across different NP patterns
and the similarities and differences among resulting se-
mantic spaces are analyzed. A thorough understanding
of the syntactic and semantic characteristics of NPs pro-
vides valuable insights into defining the most representa-
tive feature vectors that ultimately drive the discriminat-
ing learning models.
An important characteristic of this work is that it re-
lies heavily on state-of-the-art natural language process-
ing and machine learning methods. Prior to the discovery
of semantic relations, the text is syntactically parsed with
Charniak?s parser (Charniak 2001) and words are seman-
tically disambiguated and mapped into their appropriate
WordNet senses. The word sense disambiguation is done
manually for training and automatically for testing with
a state-of-the-art WSD module, an improved version of
a system with which we have participated successfully
in Senseval 2 and which has an accuracy of 81% when
disambiguating nouns in open-domain. The discovery of
semantic relations is based on learning lexical, syntactic,
semantic and contextual constraints that effectively iden-
tify the most probable relation for each NP construction
considered.
2 Semantic Relations in Nominalized Noun
Phrases
In this paper we study the behavior of semantic relations
at the noun phrase level when one of the nouns is nom-
inalized. The following NP level constructions are con-
sidered: complex nominals, genitives, adjective phrases,
and adjective clauses.
Complex Nominals
Levi (Levi 1979) defines complex nominals (CNs) as ex-
pressions that have a head noun preceded by one or more
modifying nouns, or by adjectives derived from nouns
(usually called denominal adjectives). Each sequence of
nouns, or possibly adjectives and nouns, has a particular
meaning as a whole carrying an implicit semantic rela-
tion; for example, ?parental refusal? (AGENT).
The main tasks are the recognition, and the interpre-
tation of complex nominals. The recognition task deals
with the identification of CN constructions in text, while
the interpretation of CNs focuses on the detection and
classification of a comprehensive set of semantic rela-
tions between the noun constituents.
Genitives
In English there are two kinds of genitives; in one, the
modifier is morphologically linked to the possessive clitic
?s and precedes the head noun (s-genitive e.g. ?John?s
conclusion?), and in the second one the modifier is syn-
tactically marked by the preposition of and follows the
head noun (of-genitive, e.g. ?declaration of indepen-
dence?).
Adjective Phrases are prepositional phrases attached to
nouns and act as adjectives (cf. (Semmelmeyer and
Bolander 1992)). Prepositions play an important role
both syntactically and semantically ( (Dorr 1997). Prepo-
sitional constructions can encode various semantic re-
lations, their interpretations being provided most of the
time by the underlying context. For instance, the preposi-
tion ?with? can encode different semantic relations: (1) It
was the girl with blue eyes (MERONYMY), (2) The baby
with the red ribbon is cute (POSSESSION), (3) The woman
with triplets received a lot of attention (KINSHIP).
The conclusion for us is that in addition to the nouns se-
mantic classes, the preposition and the context play im-
portant roles here.
Adjective Clauses are subordinate clauses attached to
nouns (cf. (Semmelmeyer and Bolander 1992)). Often
they are introduced by a relative pronoun/adverb (ie that,
which, who, whom, whose, where) as in the following ex-
amples: (1) Here is the book which I am reading (book
is the THEME of reading) (2) The man who was driving
the car was a spy (man is the AGENT of driving). Adjec-
tive clauses are inherently verb-argument structures, thus
their interpretation consists of detecting the semantic role
between the head noun and the main verb in the relative
clause. This is addressed below.
3 Nominalizations and Mapping of NPs
into Grammatical Role Structures
3.1 Nominalizations
A further analysis of various examples of noun - noun
pairs encoded by the first three major types of NP-level
constructions shows the need for a different taxonomy
based on the syntactic and grammatical roles the con-
stituents have in relation to each other. The criterion in
this classification splits the noun - noun examples (re-
spectively, adjective - noun examples in complex nom-
inals) into nominalizations and non-nominalizations.
Nominalizations represent a particular subclass of NP
constructions that in general have ?a systematic corre-
spondence with a clause structure? (Quirk et al1985).
The head or modifier noun is derived from a verb while
the other noun (the modifier, or respectively, the head) is
interpreted as an argument of this verb. For example, the
noun phrase ?car owner? corresponds to ?he owns a car?.
The head noun owner is morphologically related to the
verb own. Otherwise said, the interpretation of this class
of NPs is reduced to the automatic detection and inter-
pretation of semantic roles mapped on the corresponding
verb-argument structure.
As in (Hull and Gomez 1996), in this paper we use
the term nominalization to refer only to those senses of
the nominalized nouns which are derived from verbs.
For example, the noun ?decoration? has three senses in
WordNet 2.0: an ornament (#1), a medal (#2), and the act
of decorating (#3). Only the last sense is a nominaliza-
tion. However, there are more complex situations when
the underlying verb has more than one sense that refers to
an action/event. This is the case of ?examination? which
has five senses of which four are action-related. In this
case, the selection of the correct sense is provided by the
context.
We are interested in answering the following ques-
tions: (1) What is the best set of features that can capture
the meaning of noun - noun nominalization pairs for each
NP-level construction? and (2) What is the semantic be-
havior of nominalization constructions across NP levels?
3.2 Taxonomy of nominalizations
Deverbal vs verbal noun.
(Quirk et al1985) generally classify nominalizations
based on the morphological formation of the nominal-
ized noun. They distinguish between deverbal nouns, i.e.
those derived from the underlying verb through word for-
mation; e.g., ?student examination?, and verbal nouns,
i.e. those derived from the verb by adding the gerund
suffix ?-ing?; e.g.: ?cleaning woman?. Most of the time,
verbal nouns are derived from verbs which don?t have a
deverbal correspondent.
Table 1 shows the mapping of the first three major syn-
tactic NP constructions to the grammatical role level. By
analyzing a large corpus, we have observed that Quirk?s
grammatical roles shown in Table 1 are not uniformly dis-
tributed over the types of NP-constructions. For example,
the ?
 	 
Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the ACL, pages 929?936,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Semantic Role Labeling via FrameNet, VerbNet and PropBank
Ana-Maria Giuglea and Alessandro Moschitti
Department of Computer Science
University of Rome ?Tor Vergata?
Rome, Italy
agiuglea@gmail.com
moschitti@info.uniroma2.it
Abstract
This article describes a robust seman-
tic parser that uses a broad knowledge
base created by interconnecting three ma-
jor resources: FrameNet, VerbNet and
PropBank. The FrameNet corpus con-
tains the examples annotated with seman-
tic roles whereas the VerbNet lexicon pro-
vides the knowledge about the syntac-
tic behavior of the verbs. We connect
VerbNet and FrameNet by mapping the
FrameNet frames to the VerbNet Intersec-
tive Levin classes. The PropBank corpus,
which is tightly connected to the VerbNet
lexicon, is used to increase the verb cov-
erage and also to test the effectiveness of
our approach. The results indicate that our
model is an interesting step towards the
design of more robust semantic parsers.
1 Introduction
During the last years a noticeable effort has been
devoted to the design of lexical resources that
can provide the training ground for automatic se-
mantic role labelers. Unfortunately, most of the
systems developed until now are confined to the
scope of the resource used for training. A very
recent example in this sense was provided by the
CONLL 2005 shared task (Carreras and Ma`rquez,
2005) on PropBank (PB) (Kingsbury and Palmer,
2002) role labeling. The systems that participated
in the task were trained on the Wall Street Jour-
nal corpus (WSJ) and tested on portions of WSJ
and Brown corpora. While the best F-measure
recorded on WSJ was 80%, on the Brown cor-
pus, the F-measure dropped below 70%. The
most significant causes for this performance decay
were highly ambiguous and unseen predicates (i.e.
predicates that do not have training examples).
The same problem was again highlighted by the
results obtained with and without the frame infor-
mation in the Senseval-3 competition (Litkowski,
2004) of FrameNet (Johnson et al, 2003) role la-
beling task. When such information is not used
by the systems, the performance decreases by 10
percent points. This is quite intuitive as the se-
mantics of many roles strongly depends on the fo-
cused frame. Thus, we cannot expect a good per-
formance on new domains in which this informa-
tion is not available.
A solution to this problem is the automatic
frame detection. Unfortunately, our preliminary
experiments showed that given a FrameNet (FN)
predicate-argument structure, the task of identify-
ing the associated frame can be performed with
very good results when the verb predicates have
enough training examples, but becomes very chal-
lenging otherwise. The predicates belonging to
new application domains (i.e. not yet included in
FN) are especially problematic since there is no
training data available.
Therefore, we should rely on a semantic context
alternative to the frame (Giuglea and Moschitti,
2004). Such context should have a wide coverage
and should be easily derivable from FN data. A
very good candidate seems to be the Intersective
Levin class (ILC) (Dang et al, 1998) that can be
found as well in other predicate resources like PB
and VerbNet (VN) (Kipper et al, 2000).
In this paper we have investigated the above
claim by designing a semi-automatic algorithm
that assigns ILCs to FN verb predicates and by
carrying out several semantic role labeling (SRL)
experiments in which we replace the frame with
the ILC information. We used support vector ma-
929
chines (Vapnik, 1995) with (a) polynomial ker-
nels to learn the semantic role classification and
(b) Tree Kernels (Moschitti, 2004) for learning
both frame and ILC classification. Tree kernels
were applied to the syntactic trees that encode the
subcategorization structures of verbs. This means
that, although FN contains three types of predi-
cates (nouns, adjectives and verbs), we only con-
centrated on the verb predicates and their roles.
The results show that: (1) ILC can be derived
with high accuracy for both FN and Probank and
(2) ILC can replace the frame feature with almost
no loss in the accuracy of the SRL systems. At the
same time, ILC provides better predicate coverage
as it can also be learned from other corpora (e.g.
PB).
In the remainder of this paper, Section 2 sum-
marizes previous work done on FN automatic role
detection. It also explains in more detail why mod-
els based exclusively on this corpus are not suit-
able for free-text parsing. Section 3 focuses on VN
and PB and how they can enhance the robustness
of our semantic parser. Section 4 describes the
mapping between frames and ILCs whereas Sec-
tion 5 presents the experiments that support our
thesis. Finally, Section 6 summarizes the conclu-
sions.
2 Automatic Semantic Role Labeling
One of the goals of the FN project is to design a
linguistic ontology that can be used for the auto-
matic processing of semantic information. The as-
sociated hierarchy contains an extensive semantic
analysis of verbs, nouns, adjectives and situations
in which they are used, called frames. The basic
assumption on which the frames are built is that
each word evokes a particular situation with spe-
cific participants (Fillmore, 1968). The word that
evokes a particular frame is called target word or
predicate and can be an adjective, noun or verb.
The participant entities are defined using semantic
roles and they are called frame elements.
Several models have been developed for the
automatic detection of the frame elements based
on the FN corpus (Gildea and Jurafsky, 2002;
Thompson et al, 2003; Litkowski, 2004). While
the algorithms used vary, almost all the previous
studies divide the task into: 1) the identification of
the verb arguments to be labeled and 2) the tag-
ging of each argument with a role. Also, most
of the models agree on the core features as be-
ing: Predicate, Headword, Phrase Type, Govern-
ing Category, Position, Voice and Path. These are
the initial features adopted by Gildea and Jurafsky
(2002) (henceforth G&J) for both frame element
identification and role classification.
One difference among previous machine-
learning models is whether they used the frame in-
formation or not. The impact of the frame feature
over unseen predicates and words is particularly
interesting for us. The results obtained by G&J
provide some interesting insights in this direction.
In one of their experiments, they used the frame to
generalize from predicates seen in the training data
to unseen predicates, which belonged to the same
frame. The overall performance increased show-
ing that when no training data is available for a
target word we can use data from the same frame.
Other studies suggest that the frame is cru-
cial when trying to eliminate the major sources
of errors. In their error analysis, (Thompson et
al., 2003) pinpoints that the verb arguments with
headwords that are rare in a particular frame but
not rare over the whole corpus are especially hard
to classify. For these cases the frame is very im-
portant because it provides the context informa-
tion needed to distinguish between different word
senses.
Overall, the experiments presented in G&J?s
study correlated with the results obtained in the
Senseval-3 competition show that the frame fea-
ture increases the performance and decreases the
amount of annotated examples needed in training
(i.e. frame usage improves the generalization abil-
ity of the learning algorithm). On the other hand,
the results obtained without the frame information
are very poor.
These results show that having broader frame
coverage is very important for robust semantic
parsing. Unfortunately, the 321 frames that con-
tain at least one verb predicate cover only a small
fraction of the English verb lexicon and of the
possible domains. Also from these 321 frames
only 100 were considered to have enough training
data and were used in Senseval-3 (see (Litkowski,
2004) for more details).
Our approach for solving such problems in-
volves the usage of a frame-like feature, namely
the Intersective Levin class (ILC). We show that
the ILC can replace the frame with almost no loss
in performance. At the same time, ILC provides
better coverage as it can be learned also from other
930
corpora (e.g. PB).
The next section provides the theoretical sup-
port for the unified usage of FN, VN and PB, ex-
plaining why and how it is possible to link them.
3 Linking FrameNet to VerbNet and
PropBank
In general, predicates belonging to the same FN
frame have a coherent syntactic behavior that is
also different from predicates pertaining to other
frames (G&J). This finding is consistent with the-
ories of linking that claim that the syntactic behav-
ior of a verb can be predicted from its semantics
(Levin, 1993). This insight justifies the attempt to
use ILCs instead of the frame feature when clas-
sifying FN semantic roles (Giuglea and Moschitti,
2004).
The main advantage of using Levin classes
comes from the fact that other resources like PB
and the VN lexicon contain this kind of informa-
tion. Thus, we can train an ILC classifier also on
the PB corpus, considerably increasing the verb
knowledge base at our disposal. Another advan-
tage derives from the syntactic criteria that were
applied in defining the Levin?s clusters. As shown
later in this article, the syntactic nature of these
classes makes them easier to classify than frames
when using only syntactic and lexical features.
More precisely, Levin?s clusters are formed ac-
cording to diathesis alternation criteria which are
variations in the way verbal arguments are gram-
matically expressed when a specific semantic phe-
nomenon arises. For example, two different types
of diathesis alternations are the following:
(a) Middle Alternation
[Subject, Agent The butcher] cuts [Direct
Object, Patient the meat].
[Subject, Patient The meat] cuts easily.
(b) Causative/inchoative Alternation
[Subject, Agent Janet] broke [Direct Object,
Patient the cup].
[Subject, Patient The cup] broke.
In both cases, what is alternating is the grammati-
cal function that the Patient role takes when chang-
ing from the transitive use of the verb to the intran-
sitive one. The semantic phenomenon accompa-
nying these types of alternations is the change of
focus from the entity performing the action to the
theme of the event.
Levin documented 79 alternations which con-
stitute the building blocks for the verb classes.
Although alternations are chosen as the primary
means for identifying the classes, additional prop-
erties related to subcategorization, morphology
and extended meanings of verbs are taken into ac-
count as well. Thus, from a syntactic point of
view, the verbs in one Levin class have a regu-
lar behavior, different from the verbs pertaining to
other classes. Also, the classes are semantically
coherent and all verbs belonging to one class share
the same participant roles.
This constraint of having the same semantic
roles is further ensured inside the VN lexicon
which is constructed based on a more refined ver-
sion of the Levin?s classification, called Intersec-
tive Levin classes (ILCs) (Dang et al, 1998). The
lexicon provides a regular association between the
syntactic and semantic properties of each of the
described classes. It also provides information
about the syntactic frames (alternations) in which
the verbs participate and the set of possible seman-
tic roles.
One corpus associated with the VN lexicon is
PB. The annotation scheme of PB ensures that
the verbs belonging to the same Levin class share
similarly labeled arguments. Inside one ILC, to
one argument corresponds one semantic role num-
bered sequentially from ARG0 to ARG5. The ad-
junct roles are labeled ARGM.
Levin classes were constructed based on regu-
larities exhibited at grammatical level and the re-
sulting clusters were shown to be semantically co-
herent. As opposed, the FN frames were built on
semantic bases, by putting together verbs, nouns
and adjectives that evoke the same situations. Al-
though different in conception, the FN verb clus-
ters and VN verb clusters have common proper-
ties1:
1. Different syntactic properties between dis-
tinct verb clusters (as proven by the experi-
ments in G&J)
2. A shared set of possible semantic roles for all
verbs pertaining to the same cluster.
Having these insights, we have assigned a corre-
spondent VN class not to each verb predicate but
rather to each frame. In doing this we have ap-
plied the simplifying assumption that a frame has a
1See section 4.4 for more details
931
unique corresponding Levin class. Thus, we have
created a one-to-many mapping between the ILCs
and the frames. In order to create a pair ?FN frame,
VN class?, our mapping algorithm checks both the
syntactic and semantic consistency by comparing
the role frequency distributions on different syn-
tactic positions for the two candidates. The algo-
rithm is described in detail in the next section.
4 Mapping FrameNet frames to VerbNet
classes
The mapping algorithm consists of three steps: (a)
we link the frames and ILCs that have the largest
number of verbs in common and we create a set of
pairs ?FN frame, VN class? (see Table 1); (b) we
refine the pairs obtained in the previous step based
on diathesis alternation criteria, i.e. the verbs per-
taining to the FN frame have to undergo the same
diathesis alternation that characterize the corre-
sponding VN class (see Table 2) and (c) we man-
ually check the resulting mapping.
4.1 The mapping algorithm
Given a frame, F , we choose as candidate for the
mapping the ILC, C, that has the largest number of
verbs in common with it (see Table 1, line (I)). If
the number is greater or equal than three we form
a pair ?F , C? that will be tested in the second step
of the algorithm. Only the frames that have more
than 3 verb lexical units are candidates for this step
(frames with less than 3 members cannot pass con-
dition (II)). This excludes a number of 60 frames
that will be subsequently manually mapped.
In order to assign a VN class to a frame, we
have to verify that the verbs belonging to the FN
frame participate in the same diathesis alternation
criteria used to define the VN class. Thus, the
pairs ?F,C? formed in step 1 of the mapping al-
gorithm have to undergo a validation step that ver-
ifies the similarity between the enclosed FN frame
and VN class. This validation process has several
sub-steps:
First, we make use of the property (2) of the
Levin classes and FN frames presented in the pre-
vious section. According to this property, all verbs
pertaining to one frame or ILC have the same par-
ticipant roles. Thus, a first test of compatibility
between a frame and a Levin class is that they
share the same participant roles. As FN is anno-
tated with frame-specific semantic roles, we man-
ually mapped these roles into the VN set of the-
INPUT
V N = {C|C is a V erbNet class}
V N Class C = {v|c is a verb of C}
FN = {F |F is a FrameNet frame}
FN frame F = {v|v is a verb of F}
OUTPUT
Pairs = {?F, C? |F ? FN,C ? V N : F maps to C }
COMPUTE PAIRS:
Let Pairs = ?
for each F ? FN
(I) compute C? = argmaxC?V N |F ? C|
(II) if |F ? C?| ? 3 then Pairs = Pairs ? ?F,C??
Table 1: Linking FrameNet frames and VerbNet
classes.
TR = {?i : ?i is the i? th theta role of VerbNet }
for each ?F, C? ? Pairs
??AF = ?o1, .., on?, oi = #??i, F, pos =adjacent???DF = ?o1, .., on?, oi = #??i, F, pos =distant???AC = ?o1, .., on?, oi = #??i, C, pos =adjacent???DC = ?o1, .., on?, oi = #??i, C, pos =distant?
ScoreF,C = 23 ?
??AF ???AC???
?????AF
???
????
???
?????AC
???
???
+ 13 ?
??DF ???DC???
?????DF
???
????
???
?????DC
???
???
Table 2: Mapping algorithm - refining step.
matic roles. Given a frame, we assigned thematic
roles to all frame elements that are associated with
verbal predicates. For example the Speaker, Ad-
dressee, Message and Topic roles from the Telling
frame were respectively mapped into the Agent,
Recipient, Theme and Topic theta roles.
Second, we build a frequency distribution of
VN thematic roles on different syntactic positions.
Based on our observation and previous studies
(Merlo and Stevenson, 2001), we assume that each
ILC has a distinct frequency distribution of roles
on different grammatical slots. As we do not have
matching grammatical functions in FN and VN,
we approximate that subjects and direct objects
are more likely to appear on positions adjacent
to the predicate, while indirect objects appear on
more distant positions. The same intuition is suc-
cessfully used by G&J to design the Position fea-
ture.
For each thematic role ?i we acquired from VN
and FN data the frequencies with which ?i appears
on an adjacent A or distant D positions in a given
frame or VN class (i.e. #??i , class, position?).
Therefore, for each frame and class, we obtain two
vectors with thematic role frequencies correspond-
ing respectively to the adjacent and distant posi-
tions (see Table 2). We compute a score for each
932
 Score No. of Frames 
Not 
mapped Correct 
Overall 
Correct 
[0,0.5] 118 48.3% 82.5% 
(0.5,0.75] 69 0 84% 
(0.75,1] 72 0 100% 
89.6% 
 
Table 3: Results of the mapping algorithm.
pair ?F,C? using the normalized scalar product.
The core arguments, which tend to occupy adja-
cent positions, show a minor syntactic variability
and are more reliable than adjunct roles. To ac-
count for this in the overall score, we multiply the
adjacent and the distant scores by 2/3 and 1/3, re-
spectively. This limits the impact of adjunct roles
like Temporal and Location.
The above frequency vectors are computed for
FN directly from the corpus of predicate-argument
structure examples associated with each frame.
The examples associated with the VN lexicon are
extracted from the PB corpus. In order to do this
we apply a preprocessing step in which each la-
bel Arg0..5 is replaced with its corresponding the-
matic role given the ILC of the predicate. We
assign the same roles to the adjuncts all over PB
as they are general for all verb classes. The only
exception is ARGM-DIR that can correspond to
Source, Goal or Path. We assign different roles to
this adjunct based on the prepositions. We ignore
some adjuncts like ARGM-ADV or ARGM-DIS
because they cannot bear a thematic role.
4.2 Mapping Results
We found that only 133 VN classes have corre-
spondents among FN frames. Moreover, from the
frames mapped with an automatic score smaller
than 0.5 almost a half did not match any of the
existing VN classes2. A summary of the results
is depicted in Table 3. The first column contains
the automatic score provided by the mapping al-
gorithm when comparing frames with ILCs. The
second column contains the number of frames for
each score interval. The third column contains the
percentage of frames that did not have a corre-
sponding VN class and finally the fourth and fifth
columns contain the accuracy of the mapping al-
gorithm for each interval score and for the whole
task, respectively.
We mention that there are 3,672 distinct verb
senses in PB and 2,351 distinct verb senses in
2The automatic mapping is improved by manually assign-
ing the FN frames of the pairs that receive a score lower than
0.5.
FN. Only 501 verb senses are in common between
the two corpora which means 13.64% of PB and
21.31% of FN. Thus, by training an ILC classifier
on both PB and FN we extend the number of avail-
able verb senses to 5,522.
4.3 Discussion
In the literature, other studies compared the Levin
classes with the FN frames, e.g. (Baker and Rup-
penhofer, 2002; Giuglea and Moschitti, 2004; Shi
and Mihalcea, 2005). Their findings suggest that
although the two set of clusters are roughly equiv-
alent there are also several types of mismatches:
1. Levin classes that are narrower than the cor-
responding frames,
2. Levin classes that are broader that the corre-
sponding frames and
3. Overlapping groups.
For our task, point 2 does not pose a problem.
Points 1 and 3 however suggest that there are cases
in which to one FN frame corresponds more than
one Levin class. By investigating such cases, we
noted that the mapping algorithm consistently as-
signs scores below 75% to cases that match prob-
lem 1 (two Levin classes inside one frame) and
below 50% to cases that match problem 3 (more
than two Levin classes inside one frame). Thus,
to increase the accuracy of our results, a first step
should be to assign independently an ILC to each
of the verbs pertaining to frames with score lower
than 0.75%.
Nevertheless the current results are encourag-
ing as they show that the algorithm is achieving its
purpose by successfully detecting syntactic inco-
herences that can be subsequently corrected man-
ually. Also, in the next section we will show that
our current mapping achieves very good results,
giving evidence for the effectiveness of the Levin
class feature.
5 Experiments
In the previous sections we have presented the
algorithm for annotating the verb predicates of
FrameNet (FN) with Intersective Levin classes
(ILCs). In order to show the effectiveness of this
annotation and of the ILCs in general we have per-
formed several experiments.
First, we trained (1) an ILC multiclassifier from
FN, (2) an ILC multiclassifier from PB and (3) a
933
  
 Run 
51.3.2 
Cooking 
45.3 
Characterize 
29.2 
Other_cos 
45.4 
Say 
37.7 
Correspond 
36.1 Multiclassifier 
PB #Train Instances 
PB #Test Instances 
262 
5 
6 
5 
2,945 
134 
2,207 
149 
9,707 
608 
259 
20 
52,172 
2,742 
PB Results 75 33.33 96.3 97.24 100 88.89 92.96 
FN #Train Instances 
FN #Test Instances 
5,381 
1,343 
138 
35 
765 
40 
721 
184 
1,860 
1,343 
557 
111 
46,734 
11,650 
FN Results 96.36 72.73 95.73 92.43 94.43 78.23 92.63 
Table 4: F1s of some individual ILC classifiers and the overall multiclassifier accuracy (180 classes on
PB and 133 on FN).
 
 Body_part Crime Degree Agent Multiclassifier 
FN #Train Instances 
FN #Test Instances 
1,511 
356 
39 
5 
765 
187 
6,441 
1,643 
102,724 
25,615 
LF+Gold Frame 90.91 88.89 70.51 93.87 90.8 
LF+Gold ILC 90.80 88.89 71.52 92.01 88.23 
LF+Automatic Frame 84.87 88.89 70.10 87.73 85.64 
LF+Automatic ILC 85.08 88.89 69.62 87.74 84.45 
LF 79.76 75.00 64.17 80.82 80.99 
 
Table 5: F1s of some individual FN role classifiers and the overall multiclassifier accuracy (454 roles).
frame multiclassifier from FN. We compared the
results obtained when trying to classify the VN
class with the results obtained when classifying
frame. We show that ILCs are easier to detect than
FN frames.
Our second set of experiments regards the auto-
matic labeling of FN semantic roles on FN corpus
when using as features: gold frame, gold ILC, au-
tomatically detected frame and automatically de-
tected ILC. We show that in all situations in which
the VN class feature is used, the accuracy loss,
compared to the usage of the frame feature, is neg-
ligible. This suggests that the ILC can success-
fully replace the frame feature for the task of se-
mantic role labeling.
Another set of experiments regards the gener-
alization property of the ILC. We show the impact
of this feature when very few training data is avail-
able and its evolution when adding more and more
training examples. We again perform the exper-
iments for: gold frame, gold ILC, automatically
detected frame and automatically detected ILC.
Finally, we simulate the difficulty of free text
by annotating PB with FN semantic roles. We
used PB because it covers a different set of ver-
bal predicates and also because it is very different
from FN at the level of vocabulary and sometimes
even syntax. These characteristics make PB a dif-
ficult testbed for the semantic role models trained
on FN.
In the following section we present the results
obtained for each of the experiments mentioned
above.
5.1 Experimental setup
The corpora available for the experiments were PB
and FN. PB contains about 54,900 predicates and
gold parse trees. We used sections from 02 to 22
(52,172 predicates) to train the ILC classifiers and
Section 23 (2,742 predicates) for testing purposes.
The number of ILCs is 180 in PB and 133 on FN,
i.e. the classes that we were able to map.
For the experiments on FN corpus, we extracted
58,384 sentences from the 319 frames that contain
at least one verb annotation. There are 128,339
argument instances of 454 semantic roles. In our
evaluation we use only verbal predicates. More-
over, as there is no fixed split between training and
testing, we randomly selected 20% of sentences
for testing and 80% for training. The sentences
were processed using Charniak?s parser (Char-
niak, 2000) to generate parse trees automatically.
The classification models were implemented by
means of the SVM-light-TK software available at
http://ai-nlp.info.uniroma2.it/moschitti
which encodes tree kernels in the SVM-light
software (Joachims, 1999). We used the default
parameters. The classification performance was
evaluated using the F1 measure for the individual
role and ILC classifiers and the accuracy for the
multiclassifiers.
934
5.2 Automatic VerbNet class vs. automatic
FrameNet frame detection
In these experiments, we classify ILCs on PB and
frames on FN. For the training stage we use SVMs
with Tree Kernels.
The main idea of tree kernels is the modeling
of a KT (T1,T2) function which computes the num-
ber of common substructures between two trees T1
and T2. Thus, we can train SVMs with structures
drawn directly from the syntactic parse tree of the
sentence. The kernel that we employed in our ex-
periments is based on the SCF structure devised
in (Moschitti, 2004). We slightly modified SCF
by adding the headwords of the arguments, useful
for representing the selectional preferences (more
details are given in (Giuglea and Moschitti, 2006).
For frame detection on FN, we trained our clas-
sifier on 46,734 training instances and tested on
11,650 testing instances, obtaining an accuracy of
91.11%. For ILC detection the results are depicted
in Table 4. The first six columns report the F1
measure of some verb class classifiers whereas the
last column shows the global multiclassifier accu-
racy. We note that ILC detection is more accurate
than the frame detection on both FN and PB. Ad-
ditionally, the ILC results on PB are similar with
those obtained for the ILCs on FN. This suggests
that the training corpus does not have a major in-
fluence. Also, the SCF-based tree kernel seems to
be robust in what concerns the quality of the parse
trees. The performance decay is very small on FN
that uses automatic parse trees with respect to PB
that contains gold parse trees.
5.3 Automatic semantic role labeling on
FrameNet
In the experiments involving semantic role label-
ing, we used SVMs with polynomial kernels. We
adopted the standard features developed for se-
mantic role detection by Gildea and Jurafsky (see
Section 2). Also, we considered some of the fea-
tures designed by (Pradhan et al, 2005): First and
Last Word/POS in Constituent, Subcategorization,
Head Word of Prepositional Phrases and the Syn-
tactic Frame feature from (Xue and Palmer, 2004).
For the rest of the paper, we will refer to these fea-
tures as being literature features (LF). The results
obtained when using the literature features alone
or in conjunction with the gold frame feature, gold
ILC, automatically detected frame feature and au-
tomatically detected ILC are depicted in Table 5.
30
40
50
60
70
80
90
10 20 30 40 50 60 70 80 90 100
% Training Data
Acc
ura
cy
LF+ILC
LF
LF+Automatic ILC Trained on PB
LF+Automatic ILC Trained on FN
Figure 1: Semantic role learning curve.
The first four columns report the F1 measure
of some role classifiers whereas the last column
shows the global multiclassifier accuracy. The first
row contains the number of training and testing in-
stances and each of the other rows contains the
performance obtained for different feature com-
binations. The results are reported for the label-
ing task as the argument-boundary detection task
is not affected by the frame-like features (G&J).
We note that automatic frame produces an accu-
racy very close to the one obtained with automatic
ILC suggesting that this is a very good candidate
for replacing the frame feature. Also, both auto-
matic features are very effective and they decrease
the error rate by 20%.
To test the impact of ILC on SRL with different
amount of training data, we additionally draw the
learning curves with respect to different features:
LF, LF+ (gold) ILC, LF+automatic ILC trained on
PB and LF+automatic ILC trained on FN. As can
be noted, the automatic ILC information provided
by the ILC classifiers (trained on FN or PB) per-
forms almost as good as the gold ILC.
5.4 Annotating PB with FN semantic roles
To show that our approach can be suitable for
semantic role free-text annotation, we have au-
tomatically classified PB sentences3 with the FN
semantic-role classifiers. In order to measure
the quality of the annotation, we randomly se-
lected 100 sentences and manually verified them.
We measured the performance obtained with and
without the automatic ILC feature. The sentences
contained 189 arguments from which 35 were in-
correct when ILC was used compared to 72 incor-
rect in the absence of this feature, i.e. an accu-
racy of 81% with ILC versus 62% without it. This
demonstrates the importance of the ILC feature
3The results reported are only for role classification.
935
outside the scope of FN where the frame feature
is not available.
6 Conclusions
In this paper we have shown that the ILC feature
can successfully replace the FN frame feature. By
doing that we could interconnect FN to VN and
PB obtaining better verb coverage and a more ro-
bust semantic parser. Our good results show that
we have defined an effective framework which is
a promising step toward the design of more robust
semantic parsers.
In the future, we intend to measure the effec-
tiveness of our system by testing FN SRL on a
larger portion of PB or on other corpora containing
a larger verb set.
References
Collin Baker and Josef Ruppenhofer. 2002. Framenets
frames vs. levins verb classes. In 28th Annual Meet-
ing of the Berkeley Linguistics Society.
Xavier Carreras and Llu??s Ma`rquez. 2005. Introduc-
tion to the CoNLL-2005 shared task: Semantic role
labeling. In Proceedings of CoNLL-2005.
Eugene Charniak. 2000. A maximum-entropy-
inspired parser. In Proceedings of NACL00, Seattle,
Washington.
Hoa Trang Dang, Karin Kipper, Martha Palmer, and
Joseph Rosenzweig. 1998. Investigating regular
sense extensions based on intersective levin classes.
In Coling-ACL98.
Charles J. Fillmore. 1968. The case for case. In Uni-
versals in Linguistic Theory.
Daniel Gildea and Daniel Jurafsky. 2002. Automatic
labeling of semantic roles. Computational Linguis-
tic.
Ana-Maria Giuglea and Alessandro Moschitti. 2004.
Knowledge discovering using FrameNet, VerbNet
and PropBank. In Proceedings of Workshop on On-
tology and Knowledge Discovering at ECML 2004,
Pisa, Italy.
Ana-Maria Giuglea and Alessandro Moschitti. 2006.
Shallow semantic parsing based on FrameNet, Verb-
Net and PropBank. In Proceedings of the 17th Euro-
pean Conference on Artificial Intelligence, Riva del
Garda, Italy.
T. Joachims. 1999. Making large-scale SVM learning
practical. In B. Scho?lkopf, C. Burges, and A. Smola,
editors, Advances in Kernel Methods - Support Vec-
tor Learning.
Christopher Johnson, Miriam Petruck, Collin Baker,
Michael Ellsworth, Josef Ruppenhofer, and Charles
Fillmore. 2003. Framenet: Theory and practice.
Berkeley, California.
Paul Kingsbury and Martha Palmer. 2002. From Tree-
bank to PropBank. In LREC02).
Karin Kipper, Hoa Trang Dang, and Martha Palmer.
2000. Class-based construction of a verb lexicon.
In AAAI00.
Beth Levin. 1993. English Verb Classes and Alterna-
tions A Preliminary Investigation. Chicago: Univer-
sity of Chicago Press.
Kenneth Litkowski. 2004. Senseval-3 task automatic
labeling of semantic roles. In Senseval-3.
Paola Merlo and Suzanne Stevenson. 2001. Automatic
verb classification based on statistical distribution of
argument structure. CL Journal.
Alessandro Moschitti. 2004. A study on convolution
kernels for shallow semantic parsing. In ACL04,
Barcelona, Spain.
Sameer Pradhan, Kadri Hacioglu, Valeri Krugler,
Wayne Ward, James H. Martin, and Daniel Jurafsky.
2005. Support vector learning for semantic argu-
ment classification. Machine Learning Journal.
Lei Shi and Rada Mihalcea. 2005. Putting pieces to-
gether: Combining FrameNet, VerbNet and Word-
Net for robust semantic parsing. In Proceedings of
Cicling 2005, Mexico.
Cynthia A. Thompson, Roger Levy, and Christopher
Manning. 2003. A generative model for semantic
role labeling. In 14th European Conference on Ma-
chine Learning.
V. Vapnik. 1995. The Nature of Statistical Learning
Theory. Springer.
Nianwen Xue and Martha Palmer. 2004. Calibrating
features for semantic role labeling. In Proceedings
of EMNLP 2004, Barcelona, Spain. Association for
Computational Linguistics.
936
Proceedings of the 9th Conference on Computational Natural Language Learning (CoNLL),
pages 201?204, Ann Arbor, June 2005. c?2005 Association for Computational Linguistics
Hierarchical Semantic Role Labeling
Alessandro Moschitti?
moschitti@info.uniroma2.it
? DISP - University of Rome ?Tor Vergata?, Rome, Italy
? ITC-Irst, ? DIT - University of Trento, Povo, Trento, Italy
Ana-Maria Giuglea?
ana-maria.giuglea@topex.ro
Bonaventura Coppola??
coppolab@itc.it
Roberto Basili?
basili@info.uniroma2.it
Abstract
We present a four-step hierarchical SRL
strategy which generalizes the classical
two-level approach (boundary detection
and classification). To achieve this, we
have split the classification step by group-
ing together roles which share linguistic
properties (e.g. Core Roles versus Ad-
juncts). The results show that the non-
optimized hierarchical approach is com-
putationally more efficient than the tradi-
tional systems and it preserves their accu-
racy.
1 Introduction
For accomplishing the CoNLL 2005 Shared Task
on Semantic Role Labeling (Carreras and Ma`rquez,
2005), we capitalized on our experience on the se-
mantic shallow parsing by extending our system,
widely experimented on PropBank and FrameNet
(Giuglea and Moschitti, 2004) data, with a two-
step boundary detection and a hierarchical argument
classification strategy.
Currently, the system can work in both basic and
enhanced configuration. Given the parse tree of an
input sentence, the basic system applies (1) a bound-
ary classifier to select the nodes associated with cor-
rect arguments and (2) a multi-class labeler to assign
the role type. For such models, we used some of the
linear (e.g. (Gildea and Jurasfky, 2002; Pradhan et
al., 2005)) and structural (Moschitti, 2004) features
developed in previous studies.
In the enhanced configuration, the boundary an-
notation is subdivided in two steps: a first pass in
which we label argument boundary and a second
pass in which we apply a simple heuristic to elimi-
nate the argument overlaps. We have also tried some
strategies to learn such heuristics automatically. In
order to do this we used a tree kernel to classify the
subtrees associated with correct predicate argument
structures (see (Moschitti et al, 2005)). The ratio-
nale behind such an attempt was to exploit the cor-
relation among potential arguments.
Also, the role labeler is divided into two steps:
(1) we assign to the arguments one out of four possi-
ble class labels: Core Roles, Adjuncts, Continuation
Arguments and Co-referring Arguments, and (2) in
each of the above class we apply the set of its spe-
cific classifiers, e.g. A0,..,A5 within the Core Role
class. As such grouping is relatively new, the tradi-
tional features may not be sufficient to characterize
each class. Thus, to generate a large set of features
automatically, we again applied tree kernels.
Since our SRL system exploits the PropBank for-
malism for internal data representation, we devel-
oped ad-hoc procedures to convert back and forth
to the CoNLL Shared Task format. This conversion
step gave us useful information about the amount
and the nature of the parsing errors. Also, we could
measure the frequency of the mismatches between
syntax and role annotation.
In the remainder of this paper, Section 2 describes
the basic system configuration whereas Section 3 il-
lustrates its enhanced properties and the hierarchical
structure. Section 4 describes the experimental set-
ting and the results. Finally, Section 5 summarizes
201
our conclusions.
2 The Basic Semantic Role Labeler
In the last years, several machine learning ap-
proaches have been developed for automatic role la-
beling, e.g. (Gildea and Jurasfky, 2002; Pradhan
et al, 2005). Their common characteristic is the
adoption of flat feature representations for predicate-
argument structures. Our basic system is similar to
the one proposed in (Pradhan et al, 2005) and it is
described hereafter.
We divided the predicate argument labeling in two
subtasks: (a) the detection of the arguments related
to a target, i.e. all the compounding words of such
argument, and (b) the classification of the argument
type, e.g. A0 or AM. To learn both tasks we used the
following algorithm:
1. Given a sentence from the training-set, generate
a full syntactic parse-tree;
2. Let P and A be respectively the set of predicates
and the set of parse-tree nodes (i.e. the potential ar-
guments);
3. For each pair <p, a> ? P ?A:
- extract the feature representation set, Fp,a;
- if the subtree rooted in a covers exactly the
words of one argument of p, put Fp,a in T+
(positive examples), otherwise put it in T?
(negative examples).
We trained the SVM boundary classifier on T+ and
T? sets and the role labeler i on the T+i , i.e. its pos-
itive examples and T?i , i.e. its negative examples,
where T+ = T+i ? T?i , according to the ONE-vs.-
ALL scheme. To implement the multi-class clas-
sifiers we select the argument associated with the
maximum among the SVM scores.
To represent the Fp,a pairs we used the following
features:
- the Phrase Type, Predicate Word, Head Word,
Governing Category, Position and Voice defined in
(Gildea and Jurasfky, 2002);
- the Partial Path, Compressed Path, No Direction
Path, Constituent Tree Distance, Head Word POS,
First and Last Word/POS in Constituent, SubCate-
gorization and Head Word of Prepositional Phrases
proposed in (Pradhan et al, 2005); and
- the Syntactic Frame designed in (Xue and Palmer,
2004).
Figure 1: Architecture of the Hierarchical Semantic Role La-
beler.
3 Hierarchical Semantic Role Labeler
Having two phases for argument labeling provides
two main advantages: (1) the efficiency is increased
as the negative boundary examples, which are al-
most all parse-tree nodes, are used with one clas-
sifier only (i.e. the boundary classifier), and (2) as
arguments share common features that do not occur
in the non-arguments, a preliminary classification
between arguments and non-arguments advantages
the boundary detection of roles with fewer training
examples (e.g. A4). Moreover, it may be simpler
to classify the type of roles when the not-argument
nodes are absent.
Following this idea, we generalized the above two
level strategy to a four-step role labeling by group-
ing together the arguments sharing similar proper-
ties. Figure 1, shows the hierarchy employed for ar-
gument classification:
During the first phase, we select the parse tree
nodes which are likely predicate arguments. An
SVM with moderately high recall is applied for such
purpose.
In the second phase, a simple heuristic which se-
lects non-overlapping nodes from those derived in
the previous step is applied. Two nodes n1 and n2
do not overlap if n1 is not ancestor of n2 and vicev-
ersa. Our heuristic simply eliminates the nodes that
cause the highest number of overlaps. We have also
studied how to train an overlap resolver by means of
tree kernels; the promising approach and results can
be found in (Moschitti et al, 2005).
In the third phase, we classify the detected argu-
ments in the following four classes: AX, i.e. Core
202
Arguments, AM, i.e. Adjuncts, CX, i.e. Continua-
tion Arguments and RX, i.e. the Co-referring Argu-
ments. The above classification relies on linguistic
reasons. For example Core arguments class contains
the arguments specific to the verb frames while Ad-
junct Arguments class contains arguments that are
shared across all verb frames.
In the fourth phase, we classify the members
within the classes of the previous level, e.g. A0 vs.
A1, ..., A5.
4 The Experiments
We experimented our approach with the CoNLL
2005 Shared Task standard dataset, i.e. the Pen-
nTree Bank, where sections from 02 to 21 are used
as training set, Section 24 as development set (Dev)
and Section 23 as the test set (WSJ). Additionally,
the Brown corpus? sentences were also used as the
test set (Brown). As input for our feature extractor
we used only the Charniak?s parses with their POSs.
The evaluations were carried out with the SVM-
light-TK software (Moschitti, 2004) available at
http://ai-nlp.info.uniroma2.it/moschitti/
which encodes the tree kernels in the SVM-light
software (Joachims, 1999). We used the default
polynomial kernel (degree=3) for the linear feature
representations and the tree kernels for the structural
feature processing.
As our feature extraction module was designed
to work on the PropBank project annotation format
(i.e. the prop.txt index file), we needed to generate
it from the CoNLL data. Each PropBank annota-
tion refers to a parse tree node which exactly cov-
ers the target argument but when using automatic
parses such node may not exist. For example, on
the CoNLL Charniak?s parses, (sections 02-21 and
24), we discovered that this problem affects 10,293
out of the 241,121 arguments (4.3%) and 9,741 sen-
tences out of 87,257 (11.5%). We have found out
that most of the errors are due to wrong parsing at-
tachments. This observation suggests that the capa-
bility of discriminating between correct and incor-
rect parse trees is a key issue in the boundary de-
tection phase and it must be properly taken into ac-
count.
4.1 Basic System Evaluation
For the boundary classifier we used a SVM with
the polynomial kernel of degree 3. We set the reg-
ularization parameter, c, to 1 and the cost factor,
j to 7 (to have a slightly higher recall). To re-
duce the learning time, we applied a simple heuristic
which removes the nodes covering the target predi-
cate node. From the initial 4,683,777 nodes (of sec-
tions 02-21), the heuristic removed 1,503,100 nodes
with a loss of 2.6% of the total arguments. How-
ever, as we started the experiments in late, we used
only the 992,819 nodes from the sections 02-08. The
classifier took about two days and half to converge
on a 64 bits machine (2.4 GHz and 4Gb Ram).
The multiclassifier was built with 52 binary ar-
gument classifiers. Their training on all arguments
from sec 02-21, (i.e. 242,957), required about a half
day on a machine with 8 processors (32 bits, 1.7
GHz and overll 4Gb Ram).
We run the role multiclassifier on the output of the
boundary classifier. The results on the Dev, WSJ and
Brown test data are shown in Table 1. Note that, the
overlapping nodes cause the generation of overlap-
ping constituents in the sentence annotation. This
prevents us to use the CoNLL evaluator. Thus, we
used the overlap resolution algorithm also for the ba-
sic system.
4.2 Hierarchical Role Labeling Evaluation
As the first two phases of the hierarchical labeler are
identical to the basic system, we focused on the last
two phases. We carried out our studies over the Gold
Standard boundaries in the presence of arguments
that do not have a perfect-covering node in the Char-
niak trees.
To accomplish the third phase, we re-organized
the flat arguments into the AX, AM, CX and RX
classes and we built a single multi-classifier. For
the fourth phase, we built a multi-classifier for each
of the above classes: only the examples related to
the target class were used, e.g. the AX mutliclas-
sifier was designed with the A0,..,A5 ONE-vs-ALL
binary classifiers.
In rows 2 and 3, Table 2 shows the numbers of
training and development set instances. Row 4 con-
tains the F1 of the binary classifiers of the third
phase whereas Row 5 reports the F1 of the result-
ing multi-classifier. Row 6 presents the F1s of the
multi-classifiers of the fourth phase.
Row 7 illustrates the F1 measure of the fourth
phase classifier applied to the third phase output. Fi-
203
Precision Recall F?=1
Development 74.95% 73.10% 74.01
Test WSJ 76.55% 75.24% 75.89
Test Brown 65.92% 61.83% 63.81
Test WSJ+Brown 75.19% 73.45% 74.31
Test WSJ Precision Recall F?=1
Overall 76.55% 75.24% 75.89
A0 81.05% 84.37% 82.67
A1 77.21% 74.12% 75.63
A2 67.02% 68.11% 67.56
A3 69.63% 54.34% 61.04
A4 74.75% 72.55% 73.63
A5 100.00% 40.00% 57.14
AM-ADV 55.23% 55.34% 55.28
AM-CAU 66.07% 50.68% 57.36
AM-DIR 50.62% 48.24% 49.40
AM-DIS 77.71% 78.44% 78.07
AM-EXT 68.00% 53.12% 59.65
AM-LOC 59.02% 63.09% 60.99
AM-MNR 67.67% 52.33% 59.02
AM-MOD 98.65% 92.56% 95.51
AM-NEG 97.37% 96.52% 96.94
AM-PNC 42.28% 45.22% 43.70
AM-PRD 0.00% 0.00% 0.00
AM-REC 0.00% 0.00% 0.00
AM-TMP 81.90% 74.52% 78.03
R-A0 79.50% 84.82% 82.07
R-A1 62.23% 75.00% 68.02
R-A2 100.00% 31.25% 47.62
R-A3 0.00% 0.00% 0.00
R-A4 0.00% 0.00% 0.00
R-AM-ADV 0.00% 0.00% 0.00
R-AM-CAU 100.00% 50.00% 66.67
R-AM-EXT 100.00% 100.00% 100.00
R-AM-LOC 85.71% 85.71% 85.71
R-AM-MNR 22.22% 33.33% 26.67
R-AM-TMP 67.69% 84.62% 75.21
V 97.34% 97.30% 97.32
Table 1: Overall results (top) and detailed results on
the WSJ test (bottom).
nally, in Row 8, we report the F1 of the basic system
on the gold boundary nodes. We note that the basic
system shows a slightly higher F1 but is less compu-
tational efficient than the hierarchical approach.
5 Final Remarks
In this paper we analyzed the impact of a hierarchi-
cal categorization on the semantic role labeling task.
The results show that such approach produces an ac-
curacy similar to the flat systems with a higher ef-
ficiency. Moreover, some preliminary experiments
show that each node of the hierarchy requires differ-
ent features to optimize the associated multiclassi-
fier. For example, we found that the SCF tree kernel
(Moschitti, 2004) improves the AX multiclassifier
AX AM CX RX
# train. examples 172,457 59,473 2,954 7,928
# devel. examples 5,930 2,132 105 284
Phase III: binary class. 97.29 97.35 70.86 93.15
Phase III 95.99
Phase IV 92.50 85.88 91.43 91.55
Phase III & IV 88.15
Basic System 88.61
Table 2: Hierarchical Semantic Role Labeler Results
whereas the PAF tree kernel seems more suited for
the classification within the other classes, e.g. AM.
Future work on the optimization of each phase is
needed to study the potential accuracy limits of the
proposed hierarchical approach.
Acknowledgements
We wish to thank Daniele Pighin for his valuable
support in the development of the SRL system.
References
Xavier Carreras and Llu??s Ma`rquez. 2005. Introduction to the
CoNLL-2005 Shared Task: Semantic Role Labeling. In pro-
ceedings of CoNLL?05.
Daniel Gildea and Daniel Jurasfky. 2002. Automatic labeling
of semantic roles. Computational Linguistic.
Ana-Maria Giuglea and Alessandro Moschitti. 2004. Knowl-
edge Discovering using FrameNet, VerbNet and PropBank.
In proceedings of the Workshop on Ontology and Knowledge
Discovering at ECML?04, Pisa, Italy.
T. Joachims. 1999. Making large-scale SVM learning practical.
In B. Scho?lkopf, C. Burges, and A. Smola, editors, Advances
in Kernel Methods - Support Vector Learning.
Alessandro Moschitti, Bonaventura Coppola, Daniele Pighin,
and Roberto Basili. 2005. Engineering of syntactic features
for shallow semantic parsing. In proceedings of the Feature
Engineering Workshop at ACL?05, Ann Arbor, USA.
Alessandro Moschitti. 2004. A study on convolution kernel
for shallow semantic parsing. In proceedings of ACL-2004,
Barcelona, Spain.
Sameer Pradhan, Kadri Hacioglu, Valeri Krugler, Wayne Ward,
James H. Martin, and Daniel Jurafsky. 2005. Support vector
learning for semantic argument classification. to appear in
Machine Learning Journal.
Nianwen Xue and Martha Palmer. 2004. Calibrating features
for semantic role labeling. In Proceedings of EMNLP?04,
Barcelona, Spain.
204
Towards Free-text Semantic Parsing: A Unified Framework Based on  
FrameNet, VerbNet and PropBank 
Ana-Maria Giuglea and Alessandro Moschitti 
University of Rome ?Tor Vergata?,  
Rome, Italy 
ana-maria.giuglea@topex.ro  
moschitti@info.uniroma2.it 
Abstract 
This article describes a robust semantic 
parser that uses a broad knowledge base 
created by interconnecting three major 
resources: FrameNet, VerbNet and 
PropBank. The FrameNet corpus con-
tains the examples annotated with se-
mantic roles whereas the VerbNet lexi-
con provides the knowledge about the 
syntactic behavior of the verbs. We 
connect VerbNet and FrameNet by 
mapping the FrameNet frames to the 
VerbNet Intersective Levin classes. The 
PropBank corpus, which is tightly con-
nected to the VerbNet lexicon, is used to 
increase the verb coverage and also to 
test the effectiveness of our approach. 
The results indicate that our model is an 
interesting step towards the design of 
free-text semantic parsers. 
1 Introduction 
During the last years a noticeable effort has been 
devoted to the design of lexical resources that 
can provide the training ground for automatic 
semantic role labelers. Unfortunately, most of the 
systems developed until now are confined to the 
scope of the resource that they use during the 
learning stage.  A very recent example in this 
sense was provided by the CONLL 2005 Shared 
Task on PropBank (Kingsbury and Palmer, 
2002) role labeling (Carreras and M?rquez, 
2005). While the best F-measure recorded on a 
test set selected from the training corpus (WSJ) 
was 80%, on the Brown corpus, the F-measure 
dropped below 70%. The most significant causes 
for this performance decay were highly ambigu-
ous and unseen predicates (i.e. predicates that do 
not have training examples, unseen in the train-
ing set). 
On the FrameNet (Johnson et al, 2003) role 
labeling task, the Senseval-3 competition (Lit-
kowski, 2004) registered similar results (~80%) 
by using the gold frame information as a given 
feature. No tests were performed outside Frame-
Net. In this paper, we show that when the frame 
feature is not used, the performance decay on 
different corpora reaches 30 points. Thus, the 
context knowledge provided by the frame is very 
important and a free-text semantic parser using 
FrameNet roles depends on the accurate auto-
matic detection of this information.  
In order to test the feasibility of such a task, 
we have trained an SVM (Support Vector Ma-
chine) Tree Kernel model for the automatic ac-
quisition of the frame information. Although Fra-
meNet contains three types of predicates (nouns, 
adjectives and verbs), we concentrated on the 
verb predicates and the roles associated with 
them. Therefore, we considered only the frames 
that have at least one verb lexical unit. Our 
experiments show that given a FrameNet 
predicate-argument structure, the task of identi-
fying the originating frame can be performed 
with very good results when the verb predicates 
have enough training examples, but becomes 
very challenging otherwise. The predicates not 
yet included in FrameNet and the predicates be-
longing to new application domains (that require 
new frames) are especially problematic as for 
them there is no available training data.  
We have thus studied new means of captur-
ing the semantic context, other than the frame, 
which can be easily annotated on FrameNet and 
are available on a larger scale (i.e. have a better 
coverage). A very good candidate seems to be 
the Intersective Levin classes (Dang et al, 1998) 
that can be found as well in other predicate re-
sources like PropBank and VerbNet (Kipper et 
al., 2000).  Thus, we have designed a semi-
automatic algorithm for assigning an Intersective 
Levin class to each FrameNet verb predicate. 
78
The algorithm creates a mapping between Fra-
meNet frames and the Intersective Levin classes. 
By doing that we could connect FrameNet to 
VerbNet and PropBank and obtain an increased 
training set for the Intersective Levin class. This 
leads to better verb coverage and a more robust 
semantic parser. The newly created knowledge 
base allows us to surpass the shortcomings that 
arise when FrameNet, VerbNet and PropBank 
are used separately while, at the same time, we 
benefit from the extensive research involving 
each of them (Pradhan et al, 2004; Gildea and 
Jurafsky, 2002; Moschitti, 2004). 
We mention that there are 3,672 distinct 
verb senses1 in PropBank and 2,351 distinct verb 
senses in FrameNet. Only 501 verb senses are in 
common between the two corpora which mean 
13.64% of PropBank and 21.31% of FrameNet. 
Thus, by training an Intersective Levin class 
classifier on both PropBank and FrameNet we 
extend the number of available verb senses to 
5,522.
In the remainder of this paper, Section 2 
summarizes previous work done on FrameNet 
automatic role detection. It also explains in more 
detail why models based exclusively on this cor-
pus are not suitable for free-text parsing. Section 
3 focuses on VerbNet and PropBank and how 
they can enhance the robustness of our semantic 
parser. Section 4 describes the mapping between 
frames and Intersective Levin classes whereas 
Section 5 presents the experiments that support 
our thesis. Finally, Section 6 summarizes the 
conclusions. 
2 Automatic semantic role detection on 
FrameNet 
One of the goals of the FrameNet project is to 
design a linguistic ontology that can be used for 
automatic processing of semantic information. 
This hierarchy contains an extensive semantic 
analysis of verbs, nouns, adjectives and situa-
tions in which they are used, called frames. The 
basic assumption on which the frames are built is 
that each word evokes a particular situation with 
specific participants (Fillmore, 1968). The situa-
tions can be fairly simple depicting the entities 
involved and the roles they play or can be very 
complex and in this case they are called scenar-
ios. The word that evokes a particular frame is 
called target word or predicate and can be an 
                                                
1 A verb sense is an Intersective Levin class in which 
the verb is listed. 
adjective, noun or verb. The participant entities 
are defined using semantic roles and they are 
called frame elements.
Several models have been developed for the 
automatic detection of the frame elements based 
on the FrameNet corpus (Gildea and Jurafsky, 
2002; Thompson et al, 2003; Litkowski, 2004). 
While the algorithms used vary, almost all the 
previous studies divide the task into 1) the identi-
fication of the verb arguments to be labeled and 
2) the tagging of each argument with a role. 
Also, most of the models agree on the core fea-
tures as being: Predicate, Headword, Phrase 
Type, Governing Category, Position, Voice and 
Path. These are the initial features adopted by 
Gildea and Jurafsky (2002) (henceforth G&J) for 
both frame element identification and role classi-
fication.  
A difference among the previous machine-
learning models is whether the frame information 
was used as gold feature. Of particular interest 
for us is the impact of the frame over unseen 
predicates and unseen words in general.  The 
results obtained by G&J are relevant in this 
sense; especially, the experiment that uses the 
frame to generalize from predicates seen in the 
training data to other predicates (i.e. when no 
data is available for a target word, G&J use data 
from the corresponding frame). The overall per-
formance induced by the frame usage increased. 
Other studies suggest that the frame is cru-
cial when trying to eliminate the major sources 
of errors. In their error analysis, (Thompson et 
al., 2003) pinpoints that the verb arguments with 
headwords that are ?rare? in a particular frame 
but not rare over the whole corpus are especially 
hard to classify. For these cases the frame is very 
important because it provides the context infor-
mation needed to distinguish between different 
word senses. 
Overall, the experiments presented in G&J?s 
study correlated with the results obtained in the 
Senseval-3 competition show that the frame fea-
ture increases the performance and decreases the 
amount of annotated examples needed in training 
(i.e. frame usage improves the generalization 
ability of the learning algorithm). On the other 
hand the results obtained without the frame in-
formation are very poor.  
This behavior suggests that predicates in the 
same frame behave similarly in terms of their 
argument structure and that they differ with re-
spect to other frames. From this perspective, hav-
ing a broader verb knowledge base becomes of 
major importance for free-text semantic parsing. 
79
Unfortunately, the 321 frames that contain at 
least one verb predicate cover only a small frac-
tion of the English verb lexicon and of possible 
domains. Also from these 321 frames only 100 
were considered to have enough training data 
and were used in Senseval-3 (see Litkowski, 
2004 for more details). 
Our approach for solving such problems in-
volves the usage of a frame-like feature, namely 
the Intersective Levin class. We show that the 
Levin class is similar in many aspects to the 
frame and can replace it with almost no loss in 
performance. At the same time, Levin class pro-
vides better coverage as it can be learned also 
from other corpora (i.e. PropBank). We annotate 
FrameNet with Intersective Levin classes by us-
ing a mapping algorithm that exploits current 
theories of linking. Our extensive experimenta-
tion shows the validity of our technique and its 
effectiveness on corpora different from Frame-
Net. The next section provides the theoretical 
support for the unified usage of FrameNet, 
VerbNet and PropBank, explaining why and how 
is possible to link them. 
3 Linking FrameNet to VerbNet and 
PropBank 
In general, predicates belonging to the same 
FrameNet frame have a coherent syntactic be-
havior that is also different from predicates per-
taining to other frames (G&J). This finding is 
consistent with theories of linking that claim that 
the syntactic behavior of a verb can be predicted 
from its semantics (Levin 1993, Levin and Rap-
paport Hovav, 1996). This insight determined us 
to study the impact of using a feature based on 
Intersective Levin classes instead of the frame 
feature when classifying FrameNet semantic 
roles. The main advantage of using Levin classes 
comes from the fact that other resources like 
PropBank and the VerbNet lexicon contain this 
kind of information. Thus, we can train a Levin 
class classifier also on the PropBank corpus, 
considerably increasing the verb knowledge base 
at our disposal. Another advantage derives from 
the syntactic criteria that were applied in defin-
ing the Levin clusters. As shown later in this ar-
ticle, the syntactic nature of these classes makes 
them easier to classify than frames, when using 
only syntactic and lexical features. 
More precisely, the Levin clusters are 
formed according to diathesis alternation criteria 
which are variations in the way verbal arguments 
are grammatically expressed when a specific se-
mantic phenomenon arises. For example, two 
different types of diathesis alternations are the 
following: 
(a) Middle Alternation
[Subject, Agent The butcher] cuts [Direct Object, Patient the meat]. 
[Subject, Patient The meat] cuts easily.
(b) Causative/inchoative Alternation
[Subject, Agent Janet] broke [Direct Object, Patient the cup]. 
[Subject, Patient The cup] broke.
In both cases, what is alternating is the 
grammatical function that the Patient role takes 
when changing from the transitive use of the 
verb to the intransitive one. The semantic phe-
nomenon accompanying these types of alterna-
tions is the change of focus from the entity per-
forming the action to the theme of the event.  
Levin documented 79 alternations which 
constitute the building blocks for the verb 
classes. Although alternations are chosen as the 
primary means for identifying the classes, addi-
tional properties related to subcategorization, 
morphology and extended meanings of verbs are 
taken into account as well. Thus, from a syntactic 
point of view, the verbs in one Levin class have a 
regular behavior, different from the verbs per-
taining to other classes. Also, the classes are se-
mantically coherent and all verbs belonging to 
one class share the same participant roles. 
This constraint of having the same semantic 
roles is further ensured inside the VerbNet lexi-
con that is constructed based on a more refined 
version of the Levin classification called Inter-
sective Levin classes (Dang et al, 1998). The 
lexicon provides a regular association between 
the syntactic and semantic properties of each of 
the described classes. It also provides informa-
tion about the syntactic frames (alternations) in 
which the verbs participate and the set of possi-
ble semantic roles.   
One corpus associated with the VerbNet 
lexicon is PropBank. The annotation scheme of 
PropBank ensures that the verbs belonging to the 
same Levin class share similarly labeled argu-
ments. Inside one Intersective Levin class, to one 
argument corresponds one semantic role num-
bered sequentially from Arg0 to Arg5. Higher 
numbered argument labels are less consistent and 
assigned per-verb basis.  
The Levin classes were constructed based on 
regularities exhibited at grammatical level and 
the resulting clusters were shown to be semanti-
cally coherent. As opposed, the FrameNet frames 
were build on semantic bases, by putting together 
verbs, nouns and adjectives that evoke the same 
situations. Although different in conception, the 
80
FrameNet verb clusters and VerbNet verb clus-
ters have common properties2: 
(1) Coherent syntactic behavior of verbs inside one 
cluster,  
(2) Different syntactic properties between any two 
distinct verb clusters,  
(3) Shared set of possible semantic roles for all verbs 
pertaining to the same cluster.  
Having these insights, we have assigned a corre-
spondent VerbNet class not to each verb predi-
cate but rather to each frame. In doing this we 
have applied the simplifying assumption that a 
frame has a unique corresponding Levin class. 
Thus, we have created a one-to-many mapping 
between the Intersective Levin classes and the 
frames. In order to create a pair ?FrameNet 
frame, VerbNet class?, our mapping algorithm 
checks both the syntactic and semantic consis-
tency by comparing the role frequency distribu-
tions on different syntactic positions for the two 
candidates. The algorithm is described in detail 
in the next section. 
4 Mapping FrameNet frames to 
VerbNet classes 
The mapping algorithm consists of three steps: 
(a) we link the frames and Intersective Levin 
verb classes that have the largest number of 
verbs in common and we create a set of pairs 
?FrameNet frame, VerbNet class? (see Figure 1); 
(b) we refine the pairs obtained in the previous 
step based on diathesis alternation criteria, i.e. 
the verbs pertaining to the FrameNet frame have 
to undergo the same diathesis alternation that 
characterize the corresponding VerbNet class 
(see Figure 2) and (c) we manually check and 
correct the resulting mapping. In the next sec-
tions we will explain in more detail each step of 
the mapping algorithm. 
4.1 Linking frames and Intersective Levin 
classes based on common verbs 
During the first phase of the algorithm, given a 
frame, we compute its intersection with each 
VerbNet class. We choose as candidate for the 
mapping the Intersective Levin class that has the 
largest number of verbs in common with the 
given frame (Figure 1, line (I)). If the size of the 
intersection between the FrameNet frame and the 
candidate VerbNet class is bigger than or equal 
                                                
2 For FrameNet, properties 1 and 2 are true for most 
of the frames but not for all. See section 4.4 for more 
details.  
to 3 elements then we form a pair ?FrameNet 
frame, VerbNet class? that qualifies for the 
second step of the algorithm.  
Only the frames that have more than three 
verb lexical units are candidates for this step 
(frames with less than 3 members cannot pass 
condition (II)). This excludes a number of 60 
frames that will subsequently be mapped 
manually.
Figure 1. Linking FrameNet frames and VerbNet 
classes 
4.2 Refining the mapping based on verb 
alternations 
In order to assign a VerbNet class to a frame, we 
have to check that the verbs belonging to that 
frame respect the diathesis alternation criteria 
used to define the VerbNet class. Thus, the pairs 
?FrameNet frame, VerbNet class? formed in step 
(I) of the mapping algorithm have to undergo a 
validation step that verifies the similarity be-
tween the enclosed FrameNet frame and VerbNet 
class. This validation process has several sub-
steps. 
First, we make use of the property (3) of the 
Levin classes and FrameNet frames presented in 
the previous section. According to this property, 
all verbs pertaining to one frame or Levin class 
have the same participant roles. Thus, a first test 
of compatibility between a frame and a Levin 
class is that they share the same participant roles. 
As FrameNet is annotated with frame-specific 
semantic roles we manually mapped these roles 
into the VerbNet set of thematic roles. Given a 
frame, we assigned thematic roles to all frame 
elements that are associated with verbal predi-
cates. For example the roles Speaker, Addressee, 
Message and Topic from the Telling frame were 
respectively mapped into Agent, Recipient, 
Theme and Topic.
)({ }
( )**
*
,3)(
maxarg)(
:,|,
}|{
}|{
}|{
}|{
CFPairsPairsthenCFifII
                            CFCcomputeI
FNFeachfor
PairsLet
:PAIRSCOMPUTE
CtomappedisFVNCFNFCFPairs
OUTPUT
FofverbaisvvFFrameFN
frameFrameNetaisFFFN
CofverbaisvvCClassVN
classVerbNetaisCCVN
INPUT
VNC
?=??
?=
?
?=
??=
=
=
=
=
?
81
)(
||||||||3
1
||||||||3
2
),,(#),,..,(
),,(#),,..,(
),,(#),,..,(
),,(#),,..,(
,
}:{
,
1
1
1
1
CF
CF
CF
CF
CF
iin
C
iin
C
iin
F
iin
F
th
ii
DSTDST
DSTDST
ADJADJ
ADJADJScore
positionCowhereooDST
positionCowhereooADJ
positionFowhereooDST
positionFowhereooADJ
PairsCFeachfor
a role setrbNet thete of theVe theta rolis the iTR
?
?+
?
?=
===
===
===
===
?
=
??
distant  
adjacent  
distant  
adjacent  
  
?
?
?
?
??
Second, we build a frequency distribution of 
VerbNet thematic roles on different syntactic 
position. Based on our observation and previous 
studies (Merlo and Stevenson, 2001), we assume 
that each Levin class has a distinct frequency 
distribution of roles on different grammatical 
slots. As we do not have matching grammatical 
function in FrameNet and VerbNet, we approxi-
mate that subjects and direct objects are more 
likely to appear on positions adjacent to the 
predicate, while indirect objects appear on more 
distant positions. The same intuition is used suc-
cessfully by G&J in the design of the Position
feature. 
We will acquire from the corpus, for each 
thematic role ?i, the frequencies with which it 
appears on an adjacent (ADJ) or distant (DST) 
position in a given frame or VerbNet class (i.e. 
#(?i, class, position)). Therefore, for each frame 
and class, we obtain two vectors with thematic 
role frequencies corresponding respectively to 
the adjacent and distant positions (see Figure 2). 
We compute a score for each pair ?FrameNet 
frame, VerbNet class? using the normalized sca-
lar product. We give a bigger weight to the adja-
cent dot product multiplying its score by 2/3 with 
respect to the distant dot product that is multi-
plied by 1/3. We do this to minimize the impact 
that adjunct roles like Temporal and Location 
(that appear mostly on the distant positions) 
could have on the final outcome.  
Figure 2. Mapping algorithm ? refining step 
The above frequency vectors are computed 
for FrameNet directly from the corpus of predi-
cate-argument structure examples associated 
with each frame. The examples associated with 
the VerbNet lexicon are extracted from the 
PropBank corpus.  In order to do this we apply a 
preprocessing step in which each label ARG0..N 
is replaced with its corresponding thematic role 
given the Intersective Levin class of the predi-
cate. We assign the same roles to the adjuncts all 
over PropBank as they are general for all verb 
classes. The only exception is ARGM-DIR that 
can correspond to Source, Goal or Path. We as-
sign different roles to this adjunct based on the 
prepositions. We ignore some adjuncts like 
ARGM-ADV or ARGM-DIS because they can-
not bear a thematic role. 
4.3 Mapping Results 
We found that only 133 VerbNet classes have 
correspondents among FrameNet frames. Also, 
from the frames mapped with an automatic score 
smaller than 0.5 points almost a half did not 
match any of the existing VerbNet classes3. A 
summary of the results is depicted in Table 1. 
The first column contains the automatic score 
provided by the mapping algorithm when com-
paring frames with Intersective Levin classes. 
The second column contains the number of 
frames for each score interval. The third column 
contains the percentage of frames, per each score 
interval, that did not have a corresponding 
VerbNet class and finally the forth column con-
tains the accuracy of the mapping algorithm.  
Score No. of Frames 
Not 
mapped Correct 
Overall 
Correct 
[0,0.5] 118 48.3% 82.5% 
(0.5,0.75] 69 0 84% 
(0.75,1] 72 0 100% 
89.6% 
Table 1. Results of the mapping algorithm 
4.4 Discussion 
In the literature, other studies compared the 
Levin classes to the FrameNet frames (Baker and 
Ruppenhofer, 2002). Their findings suggest that 
although the two set of clusters are roughly 
equivalent  there are also several types of 
mistmaches: 1) Levin classes that are narrower 
than  the corresponding frames, 2) Levin classes 
that are broader that the corresponding frames 
and 3) overlapping groupings. For our task, point 
2 does not pose a problem. Points 1 and 3 
however suggest that there are cases in which to 
one FrameNet frame corresponds more than one 
Levin class. By investigating such cases we 
noted that the mapping algorithm consistently 
assigns scores below 75% to cases that match 
problem 1 (two Levin classes inside one frame) 
and below 50% to cases that match problem 3 
(more than two Levin classes inside one frame). 
Thus, in order to increase the accuracy of our 
results a first step should be to assign an 
                                                
3 The automatic mapping  can be improved by manu-
ally assigning the FrameNet frames of the pairs that 
receive a score lower than 0.5. 
82
Intersective Levin class to each of the verbs 
pertaining to frames with score lower than 0.75. 
Nevertheless the current results are encouraging 
as they show that the algorithm is achiving its 
purpose by successfully detecting syntactic 
incoherencies that can be subsequently corrected 
manually. Also, in the next section we will show 
that our current mapping achieves very good 
results, giving evidence for  the effectivenes of 
the Levin class feature.  
5 Experiments 
In the previous section we have presented the 
algorithm for annotating the verb predicates of 
FrameNet with Intersective Levin classes. In or-
der to show the effectiveness of this annotation 
and of the Intersective Levin class in general we 
have performed several experiments. 
First, we trained (1) an ILC multiclassifier 
from FrameNet, (2) an ILC multiclassifier from 
PropBank and (3) a frame multiclassifier from 
FrameNet. We compared the results obtained 
when trying to classify the VerbNet class with 
the results obtained when classifying frame. We 
show that Intersective Levin classes are easier to 
detect than FrameNet frames.  
Our second set of experiments regards the 
automatic labeling of FrameNet semantic roles 
on FrameNet corpus when using as features: gold 
frame, gold Intersective Levin class, automati-
cally detected frame and automatically detected 
Intersective Levin class. We show that in all 
situations in which the VerbNet class feature is 
used, the accuracy loss, compared to the usage of 
the frame feature, is negligible. We thus show 
that the Intersective Levin class can successfully 
replace the frame feature for the task of semantic 
role labeling.  
Another set of experiments regards the gen-
eralization property of the Intersective Levin 
class. We show the impact of this feature when 
very few training data is available and its evolu-
tion when adding more and more training exam-
ples. We again perform the experiments for: gold 
frame, gold Intersective Levin class, automati-
cally detected frame and automatically detected 
Intersective Levin class.  
Finally, we simulate the difficulty of free 
text by annotating PropBank with FrameNet se-
mantic roles. We use PropBank because it is dif-
ferent from FrameNet from a domain point of 
view. This characteristic makes PropBank a dif-
ficult test bed for semantic role models trained 
on FrameNet.  
In the following section we present the re-
sults obtained for each of the experiments men-
tioned above. 
5.1 Experimental setup 
The corpora available for the experiments were 
PropBank and FrameNet. PropBank contains 
about 54,900 sentences and gold parse trees. We 
used sections from 02 to 22 (52,172 sentences) to 
train the Intersective Levin class classifiers and 
section 23 (2,742 sentences) for testing purposes. 
For the experiments on FrameNet corpus we 
extracted 58,384 sentences from the 319 frames 
that contain at least one verb annotation. There 
are 128,339 argument instances of 454 semantic 
roles. Only verbs are selected to be predicates in 
our evaluations. Moreover, as there is no fixed 
split between training and testing, we randomly 
selected 20% of sentences for testing and 80% 
for training. The sentences were processed using 
Charniak?s parser (Charniak, 2000) to generate 
parse trees automatically. 
For classification, we used the SVM-light-
TK software available at http://ai-nlp. 
info.uniroma2.it/moschitti which en-
codes tree kernels in the SVM-light software 
(Joachims, 1999). The classification performance 
was evaluated using the F1 measure for the sin-
gle-argument classifiers and the accuracy for the 
multiclassifiers. 
5.2 Automatic VerbNet vs. automatic Fra-
meNet frame detection 
In these experiments we classify Intersective 
Levin classes (ILC) on PropBank (PB) and 
FrameNet (FN) and frame on FrameNet. For the 
training stage we use SVMs with Tree Kernels. 
The main idea of tree kernels is the modeling 
of a KT(T1,T2) function which computes the 
number of common substructures between two 
trees T1 and T2. Thus, we can train SVMs with 
structures drawn directly from the syntactic parse 
tree of the sentence.  
The kernel that we employed in our 
experiments is based on the SCF structure 
devised in (Moschitti, 2004). We slightly 
modified SCF by adding the headwords of the 
arguments, useful for representing the selectional 
preferences.
  For frame detection on FrameNet, we trained 
our classifier on 46,734 training instances and 
tested on 11,650 testing instances, obtaining an 
accuracy of 91.11%. For ILC detection the 
results are depicted in Table  2. The first six 
columns report the F1 measure of some verb 
83
class classifiers whereas the last column shows 
the global multiclassifier accuracy.  
We note that ILC detection is performed better 
than frame detection on both FrameNet and 
PropBank. Also, the results obtained on ILC on 
PropBank are similar with the ones obtained on 
ILC on FrameNet. This suggests that the training 
corpus does not have a major influence. Also, the 
SCF-based tree kernel seems to be robust in what 
concerns the quality of the parse trees. The 
performance decay is very small on FrameNet 
that uses automatic parse trees with respect to 
PropBank that contains gold parse trees. These 
properties suggest that ILC are very suitable for 
free text.  
Table 2 . F1 and accuracy of the argument classifiers and the overall multiclassifier for Intersective Levin class  
5.3 Automatic semantic role labeling on 
FrameNet 
In the experiments involving semantic role 
labelling, we used a SVM with a polynomial 
kernel. We adopted the standard features 
developed for semantic role detection by Gildea 
and Jurafsky (see Section 2). Also, we 
considered some of the features designed by 
(Pradhan et al, 2004): First and Last Word/POS 
in Constituent, Subcategorization, Head Word of 
Prepositional Phrases and the Syntactic Frame
feature from (Xue and Palmer, 2004). For the 
rest of the paper we will refer to these features as 
being literature features (LF). The results 
obtained when using the literature features alone 
or in conjunction with the gold frame feature, 
gold ILC, automatically detected frame feature 
and automatically detected ILC are depicted in 
Table 3. The first four columns report the F1
measure of some role classifiers whereas the last 
column shows the global multiclassifier 
accuracy. The first row contains the number of 
training and testing instances and each of the 
other rows contains the performance obtained for 
different feature combinations. The results are 
reported for the labeling task as the argument-
boundary detection task is not affected by the 
frame-like features (G&J). 
We note that automatic frame results are 
very similar to automatic ILC results suggesting 
that ILC feature is a very good candidate for 
replacing the frame feature. Also, both automatic 
features are very effective, decreasing the error 
rate of 20%. 
 Body_part Crime Degree Agent Multiclassifier 
FN #Train Instances 
FN #Test Instances 
1,511 
356 
39 
5 
765 
187 
6,441 
1,643 
102,724 
25,615 
LF+Gold Frame 90.91 88.89 70.51 93.87 90.8 
LF+Gold ILC 90.80 88.89 71.52 92.01 88.23 
LF+Automatic Frame 84.87 88.89 70.10 87.73 85.64 
LF+Automatic ILC 85.08 88.89 69.62 87.74 84.45 
LF 79.76 75.00 64.17 80.82 80.99 
Table 3. F1 and accuracy of the argument classifiers and the overall multiclassifier for  
FrameNet semantic roles 
5.4 Semantic role learning curve when us-
ing Intersective Levin classes 
The next set of experiments show the impact of 
the ILC feature on semantic role labelling when 
few training data is available (Figure 3). As can 
be noted, the automatic ILC features (i.e. derived 
with classifers trained on FrameNet or PB) 
produce accuracy almost as good as the gold ILC 
one. Another observation is that the SRL 
classifiers are not saturated and more training 
examples would improve their accuracy. 
 run-
51.3.2 
cooking-
45.3 
characterize-
29.2 
other_cos-
45.4 
say-
37.7 
correspond-
36.1 Multiclassifier 
PB #Train Instances 
PB #Test Instances 
262 
5 
6 
5 
2,945 
134 
2,207 
149 
9,707 
608 
259 
20 
52,172 
2,742 
PB Results 75 33.33 96.3 97.24 100 88.89 92.96 
FN #Train Instances 
FN #Test Instances 
5,381 
1,343 
138 
35 
765 
40 
721 
184 
1,860 
1,343 
557 
111 
46,734 
11,650 
FN Results 96.36 72.73 95.73 92.43 94.43 78.23 92.63 
84
30
40
50
60
70
80
90
10 20 30 40 50 60 70 80 90 100
% Training Data
A
cc
ur
ac
y 
   
 --
LF+ILC
LF
LF+Automatic ILC Trained on PB
LF+Automatic ILC Trained on FN
Figure 3. Semantic Role learning curve 
5.5 Annotating PropBank with FrameNet 
semantic roles 
To show that our approach can be suitable for 
semantic role free-text annotation, we have 
automatically classified PropBank sentences with 
the FrameNet semantic-role classifiers. In order 
to measure the quality of the annotation, we ran-
domly selected 100 sentences and manually veri-
fied them. We measured the performance ob-
tained with and without the automatic ILC fea-
ture. The sentences contained 189 arguments 
from which 35 were incorrect when ILC was 
used compared to 72 incorrect in the absence of 
this feature. This corresponds to an accuracy of 
81% with Intersective Levin class versus 62% 
without it.  
6 Conclusions 
In this paper we have shown that the Intersective 
Levin class feature can successfully replace the 
FrameNet frame feature. By doing that we could 
interconnect FrameNet to VerbNet and Prop-
Bank obtaining better verb coverage and a more 
robust semantic parser. Our good results show 
that we have defined an effective framework 
which is a promising step toward the design of 
free-text semantic parsers.  
In the future, we intend to measure the effective-
ness of our system by testing on larger, more 
comprehensive corpora and without relying on 
any manual annotation. 
Reference 
Collin Baker and Josef Ruppenhofer. 2002. Frame-
Net?s frames vs. Levin?s verb classes. 28th Annual 
Meeting of the Berkeley Linguistics Society. 
Xavier Carreras and Llu?s M?rquez. 2005. Introduc-
tion to the CoNLL-2005 Shared Task: Semantic 
Role Labeling. CONLL?05. 
Eugene Charniak. 2000. A Maximum-Entropy-
Inspired Parser. ANLP?00 
Hoa Trang Dang, Karin Kipper, Martha Palmer and 
Joseph Rosenzweig. 1998. Investigating regular 
sense extensions based on Intersective Levin 
classes. Coling-ACL?98. 
Charles Fillmore. 1968. The case for case. Universals 
in Linguistic Theory. 
 Daniel Gildea and Daniel Jurafsky. 2002. Automatic 
labeling of semantic roles. CL Journal. 
Christopher Johnson, Miriam Petruck, Collin Baker, 
Michael Ellsworth, Josef Ruppenhofer, and Charles 
Fillmore. 2003. FrameNet: Theory and Practice. 
Berkeley, California. 
Paul Kingsbury, Martha Palmer. 2002. From Tree-
Bank to PropBank. LREC?02. 
Karin Kipper, Hoa Trang Dang and Martha Palmer. 
2000. Class-based construction of a verb lexicon. 
AAAI?00. 
 Beth Levin. 1993. English Verb Classes and Alterna-
tions A Preliminary Investigation. Chicago: Uni-
versity of Chicago Press. 
Kenneth Litkowski. 2004. Senseval-3 task automatic 
labeling of semantic roles. Senseval-3. 
Paola Merlo and Suzanne Stevenson. 2001. Auto-
matic verb classification based on statistical distri-
bution of argument structure. CL Journal. 
Alessandro Moschitti. 2004. A study on convolution 
kernel for shallow semantic parsing. ACL?04. 
Sameer Pradhan, Kadri Hacioglu, Valeri Krugler, 
Wayne Ward, James H. Martin, and Daniel Juraf-
sky. 2004. Support vector learning for semantic ar-
gument classification. Machine Learning Journal. 
Cynthia A. Thompson, Roger Levy, and Christopher 
Manning. 2003. A Generative Model for FrameNet 
Semantic Role Labeling. ECML?03. 
Thorsten Joachims. 1999. Making large-scale SVM 
learning practical.. Advances in Kernel Methods - 
Support Vector Learning. 
Nianwen Xue and Martha Palmer. 2004. Calibrating 
features for semantic role labeling. EMNLP?04. 
85
