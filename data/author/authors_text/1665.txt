Proceedings of the Third Linguistic Annotation Workshop, ACL-IJCNLP 2009, pages 10?18,
Suntec, Singapore, 6-7 August 2009. c?2009 ACL and AFNLP
Complex Linguistic Annotation ? No Easy Way Out!  A Case from Bangla and Hindi POS Labeling Tasks  Sandipan Dandapat1         Priyanka Biswas1       Monojit Choudhury  Kalika Bali Dublin City University       LDCIL         Microsoft Research Labs India Ireland                   CIIL-Mysore, India               Bangalore, India E-mail:  sdandapat@computing.dcu.ie, biswas.priyanka@gmail.com, monojitc@microsoft.com, kalikab@microsoft.com   Abstract 
Alternative paths to linguistic annotation, such as those utilizing games or exploiting the web users, are becoming popular in recent times owing to their very high benefit-to-cost ratios. In this paper, however, we report a case study on POS annotation for Bangla and Hindi, where we observe that reliable linguis-tic annotation requires not only expert anno-tators, but also a great deal of supervision. For our hierarchical POS annotation scheme, we find that close supervision and training is necessary at every level of the hierarchy, or equivalently, complexity of the tagset. Never-theless, an intelligent annotation tool can sig-nificantly accelerate the annotation process and increase the inter-annotator agreement for both expert and non-expert annotators. These findings lead us to believe that reliable annotation requiring deep linguistic knowl-edge (e.g., POS, chunking, Treebank, seman-tic role labeling) requires expertise and su-pervision. The focus, therefore, should be on design and development of appropriate anno-tation tools equipped with machine learning based predictive modules that can signifi-cantly boost the productivity of the annota-tors.1  1 Introduction Access to reliable annotated data is the first hur-dle encountered in most NLP tasks be it at the level of Parts-of-Speech (POS) tagging or a more complex discourse level annotation. The per-formance of the machine learning approaches which have become de rigueur for most NLP tasks are dependent on accurately annotated large datasets. Creation of such databases is, hence, a highly resource intensive task both in terms of time and expertise.                                                  1 This work has been done during the authors? internship at Microsoft Research Lab India. 
While the cost of an annotation task can be characterized by the number of man-hours and the level of expertise required, the productivity or the benefit can be measured in terms of the reliability and usability of the end-product, i.e., the annotated dataset. It is thus no surprise that considerable effort has gone into developing techniques and tools that can effectively boost the benefit-to-cost ratio of the annotation proc-ess. These include, but are not limited to: (a) exploiting the reach of the web to reduce the effort required for annotation (see, e.g., Snow et al (2008) and references therein) (b) smartly designed User Interfaces for aiding the annotators (see, e.g., Eryigit (2007); Koutsis et al (2007); Reidsma et al (2004)) (c) using supervised learning to bootstrap a small annotated dataset to automatically la-bel a larger corpus and getting it corrected by human annotators (see, e.g., Tomanek et al (2007); Wu et al (2007)) (d) Active Learning (Ringger et al 2007) where only those data-points which are directly re-levant for training are presented for manual annotation. Methods exploiting the web-users for linguis-tic annotation are particularly popular these days, presumably because of the success of the ESP-Game (von Ahn and Dabbish, 2004) and its suc-cessors in image annotation. A more recent study by (Snow et al, 2008) shows that annotated data obtained from non-expert anonymous web-users is as good as those obtained from experts. How-ever, unlike the game model, here the task is dis-tributed among non-experts through an Internet portal such as Amazon Mechanical Turk, and the users are paid for their annotations.  This might lead to an impression that the ex-pert knowledge is dispensable for NLP annota-tion tasks. However, while these approaches may work for more simple tasks like those described in (Snow et al, 2008), most NLP related annota-tion tasks such as POS tagging, chunking, se-mantic role labeling, Treebank annotation and 
10
discourse level tagging, require expertise in the relevant linguistic area. In this work, we present a case study of POS annotation in Bangla and Hindi using a hierarchical tagset, where we ob-serve that reliable linguistic annotation requires not only expert annotators, but also a great deal of supervision. A generic user interface for facili-tating the task of hierarchical word level linguis-tic annotation was designed and experiments conducted to measure the inter-annotator agreement (IA) and annotation time. It is ob-served that the tool can significantly accelerate the annotation process and increase the IA. The productivity of the annotation process is further enhanced through bootstrapping, whereby a little amount of manually annotated data is used to train an automatic POS tagger. The annotators are then asked to edit the data already tagged by the automatic tagger using an appropriate user interface.      However, the most significant observation to emerge from these experiments is that irrespec-tive of the complexity of the annotation task (see Sec. 2 for definition), language, design of the user interface and the accuracy of the automatic POS tagger used during bootstrapping, the pro-ductivity and reliability of the expert annotators working under close supervision of the dataset designer is higher than that of non-experts or those working without expert-supervision. This leads us to believe that among the four aforemen-tioned approaches for improving the benefit-to-cost ratio of the annotation tasks, solution (a) does not seem to be the right choice for involved linguistic annotations; rather, approaches (b), (c) and (d) show more promise. The paper is organized as follows: Section 2 provides a brief introduction to IL-POST ? a hi-erarchical POS Tag framework for Indian Lan-guages which is used for defining the specific annotation tasks used for the experiments. The design and features of the data annotation tool are described in Section 3. Section 4 presents the experiments conducted for POS labeling task of Bangla and Hindi while the results of these ex-periments are discussed in Section 5. The con-clusions are presented in Section 6. 2 IL-POST IL-POST is a POS-tagset framework for Indian Languages, which has been designed to cover the morphosyntactic details of Indian Languages (Baskaran et al 2008).  It  supports  a  three-level 
 Figure 1: A schematic of IL-POST framework  hierarchy of Categories, Types  and  Attributes that provides a systematic method to annotate language specific categories without disregarding the shared traits of the Indian languages. This allows the framework to offer flexibility, cross-linguistic compatibility and reusability across several languages and applications. An important consequence of its hierarchical structure and decomposable tags is that it allows users to spec-ify the morpho-syntactic information applicable at the desired granularity according to the spe-cific language and task. The complete framework supports 11 categories at the top level with 32 types at the second level to represent the main POS categories and their sub-types. Further, 18 morphological attributes or features are associ-ated with the types. The framework can thus, be used to derive a flat tagset of only 11 categories or a complex three level tagset of several thou-sand tags depending on the language and/or ap-plication. Figure 1 shows a schematic of the IL-POST framework. The current framework has been used to derive maximally specified tagsets for Bangla and Hindi (see Baskaran et al (2008) for the descriptions of the tagsets), which have been used to design the experiments presented in this paper. 3 Annotation Tool  Though a number of POS annotation tools are available none are readily suitable for hierarchi-cal tagging. The tools from other domains (like discourse annotation, for example) that use hier-archical tagsets require considerable customiza-tion for the task described here. Thus, in order to facilitate the task of word-level linguistic annota-tion for complex tagsets we developed a generic annotation tool. The annotation tool can be cus-tomized to work for any tagset that has up to 
11
 Figure 2: The basic Interface Window and Controls. See the text for details.  three levels of hierarchy and for any word level linguistic annotation task, such as Named Entity annotation and Chunk boundary labeling. In this section we describe the design of the user inter-face and other features of the annotation tool. 3.1 Interface Design Principles The annotation scheme followed for linguistic data creation is heavily dependent on the end-application the data will cater to. Moreover, an-notations are often performed by trained linguists who, in the Indian context, are either novice or intermittent users of computer. These observa-tions led us to adopt the following principles: (1) customizability of the interface to any word level annotation task; (2) mouse driven selection of tags for faster and less erroneous annotation; and (3) display of all possible choices at every stage of the task to reduce memorization overload.   3.2 Basic Interface Figure 2 depicts the basic interface of the annota-tion tool 3.2.1 Automatic Handling Apart from the surface controls, the interface also supports automatic selection facility that highlights the next unlabeled word that needs to be annotated. After loading the task (i.e., a sen-tence) it automatically highlights the first unla-beled word. Once a tag is assigned to the high-lighted word, the next unlabeled word is auto-matically selected. However, the automatic se-lection module can be stopped by selecting a par-ticular word through a mouse click. 3.2.2 Handling Hierarchical Annotation The first two levels of the IL-POST hierarchy are displayed (on a right mouse click) as a two level 
context menu. This is illustrated in Fig. 3(a). On selection of the category and type by left clicks, a window is dynamically generated for the as-signment of the attribute values, i.e., the third level of the hierarchy. A drop down box is asso-ciated with each attribute for selecting the appro-priate values. This is shown in Fig. 3(b). The default values for each of the attributes are set based on the frequency of occurrence of the val-ues in a general corpus. This further reduces the time of tag assignment. When the user clicks ?OK? on the attribute assignment window, the system automatically generates the tag as per the user?s selection and displays it in the Text-box just after the selected word.  3.3 Edit Mode Annotation While performing the annotation task, human annotators need to label every word of a sen-tence. Instead of annotating every word from scratch, we incorporate machine intelligence to automatically label every word in a sentence. Suppose that we have an automatic POS tag pre-diction module that does a fairly accurate job. In that case, the task of annotation would mean ed-iting the pre-assigned tags to the words. We hy-pothesize that such an editing based annotation task that incorporates some intelligence in the form of a tagger will be much faster than purely manual annotation, provided that the pre-assigned tags are ?sufficiently accurate?. Thus, human annotators only need to edit a particular word whenever machine assigns an incorrect tag making the process faster. We also make certain changes to the basic interface for facilitating easy editing.  In particular, when the corpus is loaded using the interface, the predicted tags are shown for each word and the first category-type is high-lighted automatically. The user can navigate 
12
                                                       (a)                                 (b)  Figure 3: Annotation at a) Category-Type level, b) Attribute level  to the next or pervious editable positions (Cate-gory-Type or Attributes) by using the Shift and the Ctrl keys respectively. The user may edit a particular pre-assigned tag by making a right mouse click and choosing from the usual context menus or attribute editing window.  The user also has the provision to choose an editable loca-tion by left mouse-click. 3.3.1 Automatic POS Tagger We developed a statistical POS tagger based on Cyclic Dependency Network (Toutanova et al, 2003) as an initial annotator for the Edit mode annotation. The tagger was trained for Bangla and Hindi on the data that was created during the first phase of annotation (i.e. annotation from scratch). We developed taggers for both Cate-gory+Type level (CT) and Category+Type+ At-tribute level (CTA). We also developed two ver-sions of the same tagger with high and low accu-racies for each level of the annotation by control-ling the amount of training data.  As we shall see in Sec. 4 and 5, the different versions of the tag-ger at various levels of the hierarchy and accu-racy will help us to understand the relation be-tween the Edit mode annotation, and the com-plexity of the tagset and the accuracy of the tag-ger used for initial annotation. The taggers were trained on 1457 sentences (approximately 20,000 words) for Bangla and 2366 sentences (approxi-mately 45,000 words) for Hindi. The taggers were tested on 256 sentences (~ 3,500 words) for Bangla and 591 sentences for Hindi, which are disjoint from the training corpus. The evaluation of a hierarchical tagset is non-trivial because the error in the machine tagged data with respect to the gold standard should take into account the level of the hierarchy where the mismatch be-tween the two takes place. Clearly, mismatch at the category or type level should incur a higher 
penalty than one at the level of the attributes. If for a word, there is a mismatch between the type assigned by the machine and that present in the gold standard, then it is assumed to be a full error (equivalent to 1 unit). On the other hand, if the type assigned is correct, then the error is 0.5 times the fraction of attributes that do not agree with the gold standard.  Table 1 reports the accuracies of the various taggers. Note that the attributes in IL-POST cor-respond to morphological features. Unlike Bangla, we do not have access to a morphologi-cal analyzer for Hindi to predict the attributes during the POS tagging at the CTA level. There-fore, the tagging accuracy in the CTA level for Hindi is lower than that of Bangla even though the amount of training data used in Hindi is much higher than that in Bangla.  4 Experiments The objective of the current work is to study the cognitive load associated with the task of linguis-tic annotation, more specifically, POS annota-tion. Cognitive load relates to the higher level of processing required by the working memory of an annotator when more learning is to be done in a shorter time. Hence, a higher cognitive load implies more time required for annotation and higher error rates. The time required for annota-tion can be readily measured by keeping track of the time taken by the annotators while tagging a sentence. The timer facility provided with the annotation tool helps us keep track of the annota-tion time. Measuring the error rate is slightly trickier as we do not have any ground truth (gold standard) against which we can measure the ac-curacy of the manual annotators. Therefore, we measure the IA, which should be high if the error rate is low. Details of the evaluation metrics are discussed in the next section. 
13
 Table 1: Tagging accuracy in % for Bangla and Hindi  The cognitive load of the annotation task is de-pendent on the complexity of the tagset, (un)availability of an appropriate annotation tool and bootstrapping facility. Therefore, in order to quantify the effect of these factors on the annota-tion task, annotation experiments are conducted under eight different settings. Four experiments are done for annotation at the Category+Type (CT) level. These are: ? CT-AT: without using annotation tool, i.e., using any standard text editor2. ? CT+AT: with the help of the basic anno-tation tool.  ? CT+ATL: with the help of the annota-tion tool in the edit mode, where the POS tagger used has low accuracy. ? CT+ATH: in the edit mode where the POS tagger used has a high accuracy. Similarly, four experiments are conducted at the Category+Type+Attribute (CTA) level, which are named following the same convention: CTA-AT, CTA+AT, CTA+ATL, CTA+ATH.  4.1 Subjects The reliability of annotation is dependent on the expertise of the annotators. In order to analyze the effect of annotator expertise, we chose sub-jects with various levels of expertise and pro-vided them different amount of training and su-pervision during the annotation experiments.  The experiments for Bangla have been con-ducted with 4 users (henceforth referred to as B1, B2, B3 and B4), all of whom are trained linguists having at least a post-graduate degree in linguis-tics. Two of them, namely B1 and B2, were pro-vided rigorous training in-house before the anno-tation task. During the training phase the tagset and the annotation guidelines were explained to them in detail. This was followed by 3-4 rounds of trial annotation tasks, during which the anno-                                                2 The experiments without the tool were also conducted using the basic interface, where the annotator has to type in the tag strings; the function of the tool here is limited to loading the corpus and the timer.  
tators were asked to annotate a set of 10-15 sen-tences and they were given feedback regarding the correctness of their annotations as judged by other human experts. For B1 and B2, the experi-ments were conducted in-house and under close supervision of the designers of the tagset and the tool, as well as a senior research linguist.   The other two annotators, B3 and B4, were provided with the data, the required annotation tools and the experimental setup, annotation guidelines and the tool usage guidelines, and the task were described in another document. Thus, the annotators were self-trained as far as the tool usage and the annotation scheme were con-cerned. They were asked to return the annotated data (and the time logs that are automatically generated during the annotation) at the end of all the experiments. This situation is similar to that of linguistic annotation using the Internet users, where the annotators are self-trained and work under no supervision. However, unlike ordinary Internet users, our subjects are trained linguists.    Experiments in Hindi were conducted with two users (henceforth referred to as H1 and H2), both of whom are trained linguists. As in the case of B1 and B2, the experiments were conducted under close supervision of a senior linguist, but H1 and H2 were self-trained in the use of the tool.   The tasks were randomized to minimize the effect of familiarity with the task as well as the tool. 4.2 Data The annotators were asked to annotate approxi-mately 2000 words for CT+AT and CTA+AT experiments and around 1000 words for CT-AT and CTA-AT experiments. The edit mode ex-periments (CT+ATL, CT+ATH, CTA+ATL and CTA+ATH) have been conducted on approxi-mately 1000 words. The amount of data was de-cided based primarily on the time constraints for the experiments. For all the experiments in a par-ticular language, 25-35% of the data was com-mon between every pair of annotators. These common sets have been used to measure the IA. However, there was no single complete set common to all the annotators. In order to meas-ure the influence of the pre-assigned labels on the judgment of the annotators, some amount of data was kept common between CTA+AT and CTA+ATL/H experiments for every annotator. 
CT CTA Language High Low High Low Bangla 81.43 66.73 76.98 64.52 Hindi 87.66 67.85 69.53 57.90 
14
  
 (a)        (b) Figure 4: Mean annotation time (in sec per word) for different users at (a) CT and (b) CTA levels  Mean Time (in Sec) Level -AT +AT +ATL +ATH CT 6.3 5.0 (20.7) 2.6 (59.4) 2.5 (59.8) CTA 15.2 10.9 (28.1) 5.2 (66.0) 4.8 (68.3) Table 2: Mean annotation time for Bangla ex-periments (%reduction in time with respect to ?AT is given within parentheses). 5 Analysis of Results In this section we report the observations from our annotation experiments and analyze those to identify trends and their underlying reasons.  5.1 Mean Annotation Time We measure the mean annotation time by com-puting the average time required to annotate a word for a sentence and then average it over all sentences for a given experiment by a specific annotator. Fig. 4 shows the mean annotation time (in seconds per word) for the different experi-ments by the different annotators. It is evident that complex annotation task (i.e., CTA level) takes much more time compared to a simple one (i.e., CT level). We also note that the tool effec-tively reduces the annotation time for most of the subjects. There is some variation in time (for ex-ample, B3) where the subject took longer to get accustomed to the annotation tool. As expected, the annotation process is accelerated by boot-strapping. In fact, the higher the accuracy of the automatic tagger, the faster is the annotation. Table 2 presents the mean time averaged over the six subjects for the 8 experiments in Bangla along with the %reduction in the time with re-spect to the case when no tool is present (i.e., ?-AT?). We observe that (a) the tool is more effec-tive for complex annotation, (b) on average, an-notation at the CTA level take twice the time of their CT level counterparts, and (c) bootstrapping   
IA (in %) Level -AT +AT +ATL +ATH CT 68.9 79.2 (15.0) 77.2 (12.2) 89.9 (30.6) CTA 51.4 72.5 (41.0) 79.3 (54.2) 83.4 (62.1) Table 3: Average IA for Bangla experiments (%increase in IA with respect to ?AT is given within parentheses).  can significantly accelerate the annotation proc-ess.  We also note that experts working under close supervision (B1 and B2) are in general faster than self-trained annotators (B3 and B4). 5.2 Inter-annotator Agreement Inter-annotator agreement (IA) is a very good indicator of the reliability of an annotated data. A high IA denotes that at least two annotators agree on the annotation and therefore, the probability that the annotation is erroneous is very small. There are various ways to quantify the IA rang-ing from a very simple percentage agreement to more complex measures such as the kappa statis-tics (Cohen, 1960; Geertzen and Bunt, 2006). For a hierarchical tagset the measurement of IA is non-trivial because the extent of disagreement should take into account the level of the hierar-chy where the mismatch between two annotators takes place. Here we use percentage agreement which takes into consideration the level of hier-archy where the disagreement between the two annotators takes place. For example, the differ-ence in IA at the category level between say, a Noun and a Nominal Modifier, versus the differ-ence at the number attribute level between singu-lar and plural. The extent of agreement for each of the tags is computed in the same way as we have evaluated our POS tagger (Sec.3.2.1). We have also measured the Cohen?s Kappa (Cohen, 1960) for the CT level experiments. Its behavior is similar to that of percentage agreement. 
15
 (a)      (b) Figure 5: Pair-wise IA (in %) at (a) CT and (b) CTA levels  Fig. 5 shows the pair-wise percentage IA for the eight experiments and Table 3 summarizes the %increase in IA due to the use of tool/bootstrapping with respect to the ?-AT? ex-periments at CT and CTA levels. We observe the following basic trends: (a) IA is consistently lower for a complex annotation (CTA) task than a simpler one (CT),  (b)  use  of  annotation  tool  helps in improvement of the IA, more so for the CTA level experiments, (c) bootstrapping helps in further improvement in IA, especially when the POS tagger used has high accuracy, and (d) IA between the trained subjects (B1 and B2) is always higher than the other pairs. IA is dependent on several factors such as the ambiguity in the tagset, inherently ambiguous cases, underspecified or ambiguously specified annotation guidelines, and errors due to careless-ness of the annotator. However, manual inspec-tion reveals that the factor which results in very low IA in ?-AT? case that the tool helps improve significantly is the typographical errors made by the annotators while using a standard text editor for annotation (e.g., NC mistyped as MC). This is more prominent in the CTA level experiments, where typing the string of attributes in the wrong order or missing out on some attributes, which are very common when annotation tool is not used, lead to a very low IA. Thus, memorization has a huge overload during the annotation proc-ess, especially for complex annotation schemes, which the annotation tool can effectively handle. In fact, more than 50% errors in CTA level are due to the above phenomenon. The analysis of other factors that lower the IA is discussed in Sec. 5.4. We would like to emphasize the fact that al-though the absolute time difference between the trained and un-trained users reduces when the  tool and/or bootstrapping is used, the IA does not decrease significantly in case of the untrained users for the complex annotation task.   
 Subjects Level Tagger B1 B2 B3 B4 Low 89.6 89.8 74.2 81.8 CT High 90.8 90.1 64.8 77.8 Low 85.4 85.1 68.2 76.1 CTA High 86.4 85.4 59.1 73.4 Table 4: Percentage agreement between the edit and the normal mode annotations (for Bangla).   5.3 Machine Influence We have seen that the IA increases in the edit mode experiments. This apparent positive result might be an unacceptable artifact of machine influence, which is to say that the annotators, whenever in confusion, might blindly agree with the pre-assigned labels.  In order to understand the influence of the pre-assigned labels on the annotators, we calculate the percentage agree-ment for a subject between the data annotated from scratch using the tool (+AT) and that in the edit mode (+ATL and +ATH). The results are summarized in Table 4.  The low agreement between the data anno-tated under the two modes for the untrained an-notators (B3 and B4) shows that there is a strong influence of pre-assigned labels for these users. Untrained annotators have lower agreement while using a high accuracy initial POS tagger compared to the case when a low accuracy POS tagger is used. This is because the high accuracy tagger assigns an erroneous label mainly for the highly ambiguous cases where a larger context is required to disambiguate. These cases are also difficult for human annotators to verify and un-trained annotators tend to miss these cases during edit mode experiments. The trained annotators show a consistent performance. Nevertheless, there is still some influence of the pre-assigned labels. 
16
5.4 Error Patterns In order to understand the reasons of disagree-ment between the annotators, we analyze the confusion matrix for different pairs of users for the various experimental scenarios. We observe that the causes of disagreement are primarily of three kinds: (1) unspecified and/or ambiguous guidelines, (2) ignorance about the guidelines, and (3) inherent ambiguities present in the sen-tences. We have found that a large number of the errors are due to type (1). For example, in attrib-ute level annotation, for every attribute two spe-cial values are ?0? (denotes ?not applicable for the particular lexical item?) and ?x? (denotes ?undecided or doubtful to the annotator?). How-ever, we find that both trained and untrained an-notators have their own distinct patterns of as-signing ?0? or ?x?. Later we made this point clearer with examples and enumerated possible cases of ?0? and ?x? tags. This was very helpful in improving the IA.   A major portion of the errors made by the un-trained users are due to type (2).  For example, it was clearly mentioned in the annotation guide-lines that if a borrowed/foreign word is written in the native script, then it has to be tagged accord-ing to its normal morpho-syntactic function in the sentence. However, if a word is typed in for-eign script, then it has to be tagged as a foreign word.  However, none of the untrained annota-tors adhered to these rules strictly.  Finally, there are instances which are inher-ently ambiguous. For example, in noun-noun compounds, a common confusion is whether the first noun is to be tagged as a nouns or an adjec-tive. These kinds of confusions are evenly dis-tributed over all the users and at every level of annotation. One important fact that we arrive at through the analysis of the confusion matrices is that the trained annotators working under close supervi-sion have few and consistent error patterns over all the experiments, whereas the untrained anno-tators exhibit no consistent and clearly definable error patterns. This is not surprising because the training helps the annotators to understand the task and the annotation scheme clearly; on the other hand, constant supervision helps clarifying doubts arising during annotation.  6 Conclusion In this paper we reported our observations for POS annotation experiments for Bangla and Hindi using the IL-POST annotation scheme un-
der various scenarios. Experiments in Tamil and Sanskrit are planned in the future. We argue that the observations from the various experiments make a case for the need of training and supervision for the annotators as well as the use of appropriate annotation interfaces and techniques such as bootstrapping. The results are indicative in nature and need to be validated with larger number of annotators. We summarize our salient contributions/conclusions: ? The generic tool described here for complex and hierarchical word level annotation is ef-fective in accelerating the annotation task as well as improving the IA. Thus, the tool helps reducing the cognitive load associated with annotation. ? Bootstrapping, whereby POS tags are pre-assigned by an automatic tagger and human annotators are required to edit the incorrect labels, further accelerates the task, at the risk of slight influence of the pre-assigned labels. ? Although with the help of the tool and tech-niques such as bootstrapping we are able to bring down the time required by untrained annotators to the level of their trained coun-terparts, the IA, and hence the reliability of the annotated data for the former is always poorer. Hence, training and supervision is very important for reliable linguistic annota-tion. We would like to emphasize the last point be-cause recently it is being argued that Internet and other game based techniques can be effectively used for gathering annotated data for NLP. While this may be suitable for certain types of annota-tions, such as word sense, lexical similarity or affect (see Snow et al (2008) for details), we argue that many mainstream linguistic annotation tasks such as POS, chunk, semantic roles and Treebank annotations call for expertise, training and close supervision. We believe that there is no easy way out to this kind of complex linguistic annotations, though smartly designed annotation interfaces and methods such as bootstrapping and active learning can significantly improve the productivity and reliability, and therefore, should be explored and exploited in future. Acknowledgements We would like to thank the annotators Dripta Piplai, Anumitra Ghosh Dastidar, Narayan Choudhary and Maansi Sharma. We would also like to thank Prof. Girish Nath Jha for his help in conducting the experiments. 
17
References  S. Baskaran, K. Bali, M. Choudhury, T. Bhattacharya, P. Bhattacharyya, G. N. Jha, S. Rajendran, K. Sa-ravanan, L. Sobha and K.V. Subbarao. 2008. A Common Parts-of-Speech Tagset Framework for Indian Languages. In Proc. of LREC 2008. J.  Cohen. 1960. A coefficient of agreement for nomi-nal scales. Educational and Psychological Meas-urement, 20 (1):37-46 J. Geertzen, and H. Bunt. 2006.  Measuring annotator agreement in a complex hierarchical dialogue act annotation scheme. In Proc. of the Workshop on Discourse and Dialogue, ACL 2006, pp. 126-133. G. Eryigit. 2007. ITU Treebank annotation tool. In Proc. of Linguistic Annotation Workshop, ACL 2007, pp. 117-120. I. Koutsis, G. Markopoulos, and G. Mikros. 2007. Episimiotis: A Multilingual Tool for Hierarchical Annotation of Texts. In Corpus Linguistics, 2007. D. Reidsma, N. Jovanovi, and D. Hofs. 2004. Design-ing annotation tools based on the properties of an-notation problems. Report, Centre for Telematics and Information Technology, 2004. E. Ringger, P. McClanahan, R. Haertel, G. Busby, M. Carmen, J. Carroll, K. Seppi, and D. Lonsdale. 2007. Active Learning for Part-of-Speech Tagging: Accelerating Corpus Annotation. In Proc. of Lin-guistic Annotation Workshop, ACL 2007, pp.  101-108. R. Snow, B. O?Connor, D. Jurafsky and A. Y. Ng. 2008. Cheap and Fast ? But is it Good? Evaluat-ing Non-Expert Annotations for Natural Language Tasks. In Proc of EMNLP-08 K. Tomanek, J. Wermter, and U. Hahn. 2007. Effi-cient annotation with the Jena ANotation Environ-ment (JANE). In Proc. of Linguistic Annotation Workshop, ACL 2007, pp.  9-16. L. von Ahn and L. Dabbish. 2004. Labeling Images with a Computer Game. In ACM Conference on Human Factors in Computing Systems, CHI 2004. Y. Wu, P. Jin, T. Guo and S. Yu. 2007. Building Chi-nese sense annotated corpus with the help of soft-ware tools. In Proc. of Linguistic Annotation Workshop, ACL 2007, pp. 125-131.  
18
Proceedings of the 2009 Named Entities Workshop, ACL-IJCNLP 2009, pages 104?107,
Suntec, Singapore, 7 August 2009. c?2009 ACL and AFNLP
English?Hindi Transliteration Using Context-Informed PB-SMT:    
the DCU System for NEWS 2009 
Rejwanul Haque, Sandipan Dandapat, Ankit Kumar Srivastava,  
Sudip Kumar Naskar and Andy Way 
CNGL, School of Computing 
Dublin City University, Dublin 9, Ireland 
{rhaque,sdandapat,snaskar,asrivastava,away}@computing.dcu.ie 
 
Abstract 
This paper presents English?Hindi translit-
eration in the NEWS 2009 Machine Translit-
eration Shared Task adding source context 
modeling into state-of-the-art log-linear 
phrase-based statistical machine translation 
(PB-SMT). Source context features enable us 
to exploit source similarity in addition to tar-
get similarity, as modelled by the language 
model. We use a memory-based classification 
framework that enables efficient estimation of 
these features while avoiding data sparseness 
problems.We carried out experiments both at 
character and transliteration unit (TU) level. 
Position-dependent source context features 
produce significant improvements in terms of 
all evaluation metrics. 
1 Introduction 
Machine Transliteration is of key importance in 
many cross-lingual natural language processing 
applications, such as information retrieval, ques-
tion answering and machine translation (MT). 
There are numerous ways of performing auto-
matic transliteration, such as noisy channel mod-
els (Knight and Graehl, 1998), joint source chan-
nel models (Li et al, 2004), decision-tree models 
(Kang and Choi, 2000) and statistical MT models 
(Matthews, 2007). 
For the shared task, we built our machine 
transliteration system based on phrase-based sta-
tistical MT (PB-SMT) (Koehn et al, 2003) using 
Moses (Koehn et al, 2007).  We adapt PB-SMT 
models for transliteration by translating charac-
ters rather than words as in character-level trans-
lation systems (Lepage & Denoual, 2006). How-
ever, we go a step further from the basic PB-
SMT model by using source-language context 
features (Stroppa et al, 2007). We also create 
translation models by constraining the character-
level segmentations, i.e. treating a consonant-
vowel cluster as one transliteration unit.  
The remainder of the paper is organized as fol-
lows. In section 2 we give a brief overview of 
PB-SMT. Section 3 describes how context-
informed features are incorporated into state-of-
art log-linear PB-SMT. Section 4 includes the 
results obtained, together with some analysis. 
Section 5 concludes the paper. 
2 Log-Linear PB-SMT  
Translation is modelled in PB-SMT as a decision 
process, in which the translation Ie1 = e1 . . .  eI of 
a source sentence Jf1 = f1 . . . fJ is chosen to 
maximize (1): 
)1()().|(maxarg)|(maxarg 111
,
11
, 11
IIJ
eI
JI
eI
ePefPfeP
II
?  
where )|( 11
IJ efP  and )( 1
IeP  denote respec-
tively the translation model and the target lan-
guage model (Brown et al, 1993). In log-linear 
phrase-based SMT, the posterior probability 
)|( 11
JI feP  is directly modelled as a (log-linear) 
combination of features (Och and Ney, 2002), 
that usually comprise M translational features, 
and the language model, as in (2): 
?
?
?
m
m
KIJ
mm
JI sefhfeP
1
11111 ),,()|(log ?   
                             )(log 1
I
LM eP??                  (2) 
where k
K sss ...11 ?  denotes a segmentation of the 
source and target sentences respectively into the 
sequences of phrases )?,...,?( 1 kee  and )
?,...,?( 1 kff  
such that (we set i0 = 0) (3): 
,1 Kk ???  sk = (ik ; bk, jk), 
          
kk iik
eee ...? 11 ??? , 
                      
kk jbk
fff ...? ?                              (3) 
The translational features involved depend 
only on a pair of source/target phrases and do not 
take into account any context of these phrases. 
This means that each feature mh   in (2) can be 
rewritten as in (4): 
104
?
?
?
K
k
kkkm
KIJ
m sefhsefh
1
111 ),?,?(?),,(           (4) 
where mh? is a feature that applies to a single 
phrase-pair. Thus (2) can be rewritten as: 
? ??
? ??
?
K
k
K
k
kkkkkkm
m
m
m sefhsefh
1 11
),?,?(?),?,?(??        (5) 
where, m
m
m
mhh ??
1
?
?
? ? . In this context, the transla-
tion process amounts to: (i) choosing a segmen-
tation of the source sentence, (ii) translating each 
source phrase. 
3 Source Context Features in Log-
Linear PB-SMT 
The context of a source phrase kf?  is defined as 
the sequence before and after a focus phrase kf?  
=
kk ji
ff ... . Source context features (Stroppa et 
al., 2007) include the direct left and right context 
words (in our case, character/TU instead of word) 
of length l (resp. lii kk ff ?? ...1  and ljj kk ff ?? ...1 ) of 
a given focus phrase kf? = kk ji ff ... . A window of 
size 2l+1 features including the focus phrase is 
formed. Thus lexical contextual information (CI) 
can be described as in (6): 
CI = }...,...{ 11 ljjili kkkk ffff ????                    (6) 
As in (Haque et al, 2009), we considered a 
context window of ?1 and ?2 (i.e. l=1, 2) for our 
experiments. 
One natural way of expressing a context-
informed feature is as the conditional probability 
of the target phrase given the source phrase and 
its context information, as in (7): 
mh? ( kf? ,CI( kf? ), ke? , sk) = log P( ke? | kf? , CI( kf? ))  (7) 
3.1 Memory-Based Classification 
As (Stroppa et al, 2007) point out, directly esti-
mating P( ke? | kf? , CI( kf? )) using relative fre-
quencies is problematic. Indeed, Zens and Ney 
(2004) showed that the estimation of P( ke? | kf? ) 
using relative frequencies results in the overesti-
mation of the probabilities of long phrases, so 
smoothing factors in the form of lexical-based 
features are often used to counteract this bias 
(Foster et al, 2006). In the case of context-
informed features, since the context is also taken 
into account, this estimation problem can only 
become worse. To avoid such problems, in this 
work we use three memory-based classifiers: 
IGTree, IB1 and TRIBL 1  (Daelemans et al, 
2005). When predicting a target phrase given a 
source phrase and its context, the source phrase 
is intuitively the feature with the highest predic-
tion power; in all our experiments, it is the fea-
ture with the highest gain ratio (GR).  
In order to build the set of examples required 
to train the classifier, we modify the standard 
phrase-extraction method of (Koehn et al, 2003) 
to extract the context of the source phrases at the 
same time as the phrases themselves. Importantly, 
therefore, the context extraction comes at no ex-
tra cost.  
We refer interested readers to (Stroppa et al, 
2007) and (Haque et al, 2009) as well as the ref-
erences therein for more details of how Memory-
Based Learning (MBL) is used for classification 
of source examples for use in the log-linear MT 
framework. 
3.2 Implementation Issues 
We split named entities (NE) into characters. We 
break NEs into transliteration units (TU), which 
bear close resemblance to syllables. We split 
English NEs into TUs having C*V* pattern and 
Hindi NEs are divided into TUs having Ch+M 
pattern (M: Hindi Matra / vowel modifier, Ch: 
Characters other than Matras). We carry out ex-
periments on both character-level (C-L) and TU-
level (TU-L) data. We use a 5-gram language 
model for all our experiments. The Moses PB-
SMT system serves as our baseline system. 
The distribution of target phrases given a 
source phrase and its contextual information is 
normalised to estimate P( ke? | kf? ,CI( kf? )). There-
fore our expected feature is derived as in (8): 
mblh? = log P( ke? | kf? ,CI( kf? ))                         (8) 
As for the standard phrase-based approach, 
their weights are optimized using Minimum Er-
ror Rate Training (MERT) of (Och, 2003) for 
each of the experiments. 
As (Stroppa et al, 2007) point out, PB-SMT 
decoders such as Pharaoh (Koehn, 2004) or 
Moses (Koehn, 2007) rely on a static phrase-
table represented as a list of aligned phrases ac-
companied with several features. Since these fea-
                                               
1 An implementation of IGTree, IB1 and TRIBL is available 
in the TiMBL software package (http://ilk.uvt.nl/timbl). 
 
105
tures do not express the context in which those 
phrases occur, no context information is kept in 
the phrase-table, and there is no way to recover 
this information from the phrase-table. 
In order to take into account the context-
informed features for use with such decoders, the 
devset and testset that need to be translated are 
pre-processed. Each token appearing in the test-
set and devset is assigned a unique id. First we 
prepare the phrase table using the training data. 
Then we generate all possible phrases from the 
devset and testset. These devset and testset 
phrases are then searched for in the phrase table, 
and if found, then the phrase along with its con-
textual information is given to MBL for classifi-
cation. MBL produces class distributions accord-
ing to the maximum-match of the features con-
tained in the source phrase. We derive new 
scores from this class distribution and merge 
them with the initial information contained in the 
phrase table to take into account our feature 
functions ( mblh? ) in the log-linear model (2). 
In this way we create a dynamic phrase table 
containing both the standard and the context-
informed features. The new phrase table contains 
the source phrase (represented by the sequence 
of ids of the words composing the phrase), target 
phrase and the new score. 
Similarly, replacing all the words by their ids 
in the development set, we perform MERT using 
our new phrase table to optimize the feature 
weights. We translate the test set (words repre-
sented by ids) using our new phrase table. 
4 Results and Analysis 
We used 10,000 NEs from the NEWS 2009 Eng-
lish?Hindi training data (Kumaran and Kellner, 
2007) for the standard submission, and the addi-
tional English?Hindi parallel person names data 
(105,905 distinct name pairs) of the Election 
Commission of India2 for the non-standard sub-
missions. In addition to the baseline Moses sys-
tem, we carried out three different set of experi-
ments on IGTree, IB1 and TRIBL. Each of these 
experiments was carried out on both the standard 
data and the combined larger data, both at char-
acter level and the TU level, and considering 
?1/?2 tokens as context. For each experiment, 
we produce the 10-best distinct hypotheses. The 
results are shown in Table 1. 
We observed that many of the (unseen) TUs in 
the testset remain untranslated in TU-L systems 
                                               
2 http://www.eci.gov.in/DevForum/Fullname.asp 
due to the problems of data sparseness. When-
ever a TU-L system fails to translate a TU, we 
fallback on the corresponding C-L system to 
translate the TU as a post-processing step. 
The accuracy of the TU-L baseline system 
(0.391) is much higher compared to the C-L 
baseline system (0.290) on standard dataset. Fur-
thermore, contextual modelling of the source 
language gives an accuracy of 0.416 and 0.399 
for TU-L system and C-L system respectively. 
Similar trends are observed in case of larger 
dataset. However, the highest accuracy (0.445) 
has been achieved with the TU-L system using 
the larger dataset. 
5 Conclusion 
In this work, we employed source context model-
ing into the state-of-the-art log-linear PB-SMT 
for the English?Hindi transliteration task. We 
have shown that taking source context into ac-
count substantially improve the system perform-
ance (an improvement of 43.44% and 26.42% 
respectively for standard and larger datasets). 
IGTree performs best for TU-L systems while 
TRIBL seems to perform better for C-L systems 
on both standard and non-standard datasets. 
Acknowledgements 
We would like to thank Antal van den Bosch for 
his input on the use of memory based classifiers. 
We are grateful to SFI (http://www.sfi.ie) for 
generously sponsoring this research under grant 
07/CE/I1142. 
References  
Adimugan Kumaran and Tobias Kellner. A generic 
framework for machine transliteration. Proc. of the 
30th SIGIR, 2007. 
Byung-Ju Kang and Key-Sun Choi. Automatic trans-
literation and back-transliteration by decision tree 
learning. 2000. Proc. of LREC-2000, Athens, 
Greece, pp. 1135-1141. 
David Matthews. 2007. Machine Transliteration of 
Proper Names. Master's Thesis, University of Ed-
inburgh, Edinburgh, United Kingdom. 
Franz Och and Hermann Ney. 2002. Discriminative 
training and maximum entropy models for statisti-
cal machine translation. Proc. of ACL 2002, Phila-
delphia, PA, pp. 295?302. 
George Foster, Roland Kuhn, and Howard Johnson. 
2006. Phrasetable smoothing for statistical machine 
translation.  Proc. of EMNLP-2006, Sydney, Aus-
tralia, pp. 53-61. 
106
Table1: Experimental Results (S/B ? Standard / Big data, S*? TM on Standard data, but LM on Big data, 
C/TU ? Character / TU level, SD? Standard submission, NSD? Non-standard submission). Better results with 
bold faces have not been submitted in the NEWS 2009 Machine Transliteration Shared Task. 
Haizhou Li, Zhang Min and Su Jian. 2004. A joint 
source-channel model for machine translitera-
tion. Proc. of ACL 2004, Barcelona, Spain, 
pp.159-166. 
Kevin Knight and Jonathan Graehl. 1998. Machine 
Transliteration. Computational Linguistics, 
24(4):559-612. 
Nicolas Stroppa, Antal van den Bosch and Andy 
Way. 2007. Exploiting Source Similarity for 
SMT using Context-Informed Features. Proc. of  
TMI-2007, Sk?vde, Sweden, pp. 231-240. 
Peter F. Brown, S. A. D. Pietra, V. J. D. Pietra and 
R. L. Mercer. 1993. The mathematics of statisti-
cal machine translation: parameter estimation. 
Computational Linguistics 19 (2), pp. 263-311. 
Philipp Koehn, F. J. Och, and D. Marcu. 2003. Sta-
tistical phrase-based translation. Proc. of HLT-
NAACL 2003, Edmonton, Canada, pp. 48-54. 
Philipp Koehn. 2004. Pharaoh: a beam search de-
coder for phrase-based statistical machine trans-
lation models. Machine translation: from real 
users to research: Proc. of AMTA 2004, Berlin: 
Springer Verlag, 2004, pp. 115-124. 
Philipp Koehn, H. Hoang, A. Birch, C. Callison-
Burch, M. Federico, N. Bertoldi, B. Cowan, W. 
Shen, C. Moran, R. Zens, C. Dyer,  O. Bojar, A. 
Constantin and E. Herbst. 2007. Moses: open 
source toolkit for statistical machine translation. 
Proc. of ACL, Prague, Czech Republic, pp. 177-
180. 
Rejwanul Haque, Sudip Kumar Naskar, Yanjun Ma 
and Andy Way. 2009. Using Supertags as Source 
Language Context in SMT. Proc. of EAMT-09, 
Barcelona, Spain, pp. 234-241. 
Richard Zens and Hermann Ney. 2004. Improve-
ments in phrase-based statistical machine trans-
lation. Proc. of HLT/NAACL 2004, Boston, MA, 
pp. 257?264. 
Walter Daelemans & Antal van den Bosch. 2005. 
Memory-based language processing. Cambridge, 
UK, Cambridge University Press. 
Yves Lepage and Etienne Denoual. 2006. Objective 
evaluation of the analogy-based machine transla-
tion system ALEPH. Proc. of the 12th Annual 
Meeting of the Association of NLP, pp. 873-876. 
 S/B C/TU Context ACC M-F-Sc MRR MAP_ref MAP_10 MAP_sys 
C 0 .290 .814 .393 .286 .131 .131  
S TU 0 .391 .850 .483 .384 .160 .160 
C 0 .352 .830 .463 .346 .156 .156 
 
Baseline 
Moses  
B TU 0 .407 .853 .500 .402 .165 .165 
?1 .391 .858 .501 .384 .166 .166  
C ?2 .386 .860 .479 .379 .155 .155 
?1 .406 .858 .466 .398 .178 .178 
 
S 
 
TU ?2 .359 .838 .402 .349 .165 .165 
?1 .431 .865 .534 .423 .177 .177  
C ?2 (NSD1) .420 .867 .519 .413 .170 .170 
?1 .437 .863 .507 .429 .191 .191 
 
 
 
 
IB1  
B 
 
TU ?2 .427 .862 .487 .418 .194 .194 
?1 .372 .849 .482 .366 .160 .160  
C ?2 .371 .847 .476 .364 .156 .156 
?1 .412 .859 .486 .404 .164 .164 
 
S 
 
TU ?2 .416 .860 .493 .409 .166 .166 
?1 .413 .855 .518 .406 .173 .173  
C ?2 (NSD2) .407 .856 .507 .399 .168 .168 
?1 .445 .864 .527 .440 .176 .176 
 
 
 
 
IGTree  
B 
 
TU ?2 .427 .861 .516 .422 .173 .173 
?1 .382 .854 .493 .375 .164 .164  
C ?2 (SD) .399 .863 .488 .392 .157 .157 
?1 .408 .858 .474 .400 .181 .181 
 
S 
 
TU ?2 .395 .857 .453 .385 .182 .182 
?1 .439 .866 .543 .430 .179 .179  
C ?2 (NSD3) .421 .864 .519 .415 .171 .171 
?1 .444 .863 .512 .436 .193 .193 
 
 
 
 
TRIBL  
B 
 
TU ?2 .439 .865 .497 .430 .197 .197 
 S* C ?2 (NSD4) .419 .868 .464 .419 .338 .338 
107
Proceedings of the 12th Conference of the European Chapter of the ACL, pages 121?129,
Athens, Greece, 30 March ? 3 April 2009. c?2009 Association for Computational Linguistics
Large-Coverage Root Lexicon Extraction for Hindi
Cohan Sujay Carlos Monojit Choudhury Sandipan Dandapat
Microsoft Research India
monojitc@microsoft.com
Abstract
This paper describes a method using mor-
phological rules and heuristics, for the au-
tomatic extraction of large-coverage lexi-
cons of stems and root word-forms from
a raw text corpus. We cast the problem
of high-coverage lexicon extraction as one
of stemming followed by root word-form
selection. We examine the use of POS
tagging to improve precision and recall of
stemming and thereby the coverage of the
lexicon. We present accuracy, precision
and recall scores for the system on a Hindi
corpus.
1 Introduction
Large-coverage morphological lexicons are an es-
sential component of morphological analysers.
Morphological analysers find application in lan-
guage processing systems for tasks like tagging,
parsing and machine translation. While raw text
is an abundant and easily accessible linguistic re-
source, high-coverage morphological lexicons are
scarce or unavailable in Hindi as in many other
languages (Cle?ment et al, 2004). Thus, the devel-
opment of better algorithms for the extraction of
morphological lexicons from raw text corpora is a
task of considerable importance.
A root word-form lexicon is an intermediate
stage in the creation of a morphological lexicon.
In this paper, we consider the problem of extract-
ing a large-coverage root word-form lexicon for
the Hindi language, a highly inflectional and mod-
erately agglutinative Indo-European language spo-
ken widely in South Asia.
Since a POS tagger, another basic tool, was
available along with POS tagged data to train it,
and since the error patterns indicated that POS tag-
ging could greatly improve the accuracy of the lex-
icon, we used the POS tagger in our experiments
on lexicon extraction.
Previous work in morphological lexicon extrac-
tion from a raw corpus often does not achieve very
high precision and recall (de Lima, 1998; Oliver
and Tadic?, 2004). In some previous work the pro-
cess of lexicon extraction involves incremental or
post-construction manual validation of the entire
lexicon (Cle?ment et al, 2004; Sagot, 2005; Fors-
berg et al, 2006; Sagot et al, 2006; Sagot, 2007).
Our method attempts to improve on and extend
the previous work by increasing the precision and
recall of the system to such a point that manual
validation might even be rendered unnecessary.
Yet another difference, to our knowledge, is that
in our method we cast the problem of lexicon ex-
traction as two subproblems: that of stemming and
following it, that of root word-form selection.
The input resources for our system are as fol-
lows: a) raw text corpus, b) morphological rules,
c) POS tagger and d) word-segmentation labelled
data. We output a stem lexicon and a root word-
form lexicon.
We take as input a raw text corpus and a set
of morphological rules. We first run a stemming
algorithm that uses the morphological rules and
some heuristics to obtain a stem dictionary. We
then create a root dictionary from the stem dictio-
nary.
The last two input resources are optional but
when a POS tagger is utilized, the F-score (har-
monic mean of precision and recall) of the root
lexicon can be as high as 94.6%.
In the rest of the paper, we provide a brief
overview of the morphological features of the
Hindi language, followed by a description of our
method including the specification of rules, the
corpora and the heuristics for stemming and root
word-form selection. We then evaluate the system
with and without the POS tagger.
121
2 Hindi Orthography and Morphology
There are some features peculiar to Hindi orthog-
raphy and to the character encoding system that
we use. These need to be compensated for in the
system. It was also found that Hindi?s inflectional
morphology has certain characteristics that sim-
plify the word segmentation rules.
2.1 Orthography
Hindi is written in the partially-phonemic Devana-
gari script. Most consonant clusters that occur in
the language are represented by characters and lig-
atures, while a very few are represented as diacrit-
ics. Vowels that follow consonants or consonant
clusters are marked with diacritics. However, each
consonant in the Devanagari script also carries an
implicit vowel a1 unless its absence is marked by a
special diacritic ?halant?. Vowels are represented
by vowel characters when they occur at the head
of a word or after another vowel.
The y sound sometimes does not surface in the
pronunciation when it occurs between two vow-
els. So suffixes where the y is followed by e or I
can be written in two ways, with or without the y
sound in them. For instance the suffix ie can also
be written as iye.
Certain stemming rules will therefore need to
be duplicated in order to accommodate the differ-
ent spelling possibilities and the different vowel
representations in Hindi. The character encoding
also plays a small but significant role in the ease
of stemming of Hindi word-forms.
2.2 Unicode Representation
We used Unicode to encode Hindi characters. The
Unicode representation of Devanagari treats sim-
ple consonants and vowels as separate units and so
makes it easier to match substrings at consonant-
vowel boundaries. Ligatures and diacritical forms
of consonants are therefore represented by the
same character code and they can be equated very
simply.
However, when using Unicode as the charac-
ter encoding, it must be borne in mind that there
are different character codes for the vowel diacrit-
ics and for the vowel characters for one and the
same vowel sound, and that the long and short
1In the discussion in Section 2 and in Table 1 and
Table 2, we have used a loose phonetic transcription
that resembles ITRANS (developed by Avinash Chopde
http://www.aczoom.com/itrans/).
Word Form Derivational Segmentation Root
karnA kar + nA kar
karAnA kar + A + nA kar
karvAnA kar + vA + nA kar
Word Form Inflectional Segmentation Root
karnA kar + nA kar
karAnA karA + nA karA
karvAnA karvA + nA karvA
Table 1: Morpheme Segmentation
laDkA Nominative Oblique
Singular laDkA laDke
Plural laDke laDkon
laDkI Nominative Oblique
Singular laDkI laDkI
Plural laDkI laDkiyAn
Table 2: Sample Paradigms
forms of the vowels are represented by different
codes. These artifacts of the character encoding
need to be compensated for when using substring
matches to identify the short vowel sound as being
part of the corresponding prolonged vowel sound
and when stemming.
2.3 Morphology
The inflectional morphology of Hindi does not
permit agglutination. This helps keep the num-
ber of inflectional morphological rules manage-
able. However, the derivational suffixes are agglu-
tinative, leading to an explosion in the number of
root word-forms in the inflectional root lexicon.
The example in Table 1 shows that verbs can
take one of the two causative suffixes A and vA.
These being derivational suffixes are not stemmed
in our system and cause the verb lexicon to be
larger than it would have otherwise.
2.4 Paradigms
Nouns, verbs and adjectives are the main POS cat-
egories that undergo inflection in Hindi according
to regular paradigm rules.
For example, Hindi nouns inflect for case and
number. The inflections for the paradigms that the
words laDkA (meaning boy) and laDkI (mean-
ing girl) belong to are shown in Table 2. The root
word-forms are laDkA and laDkI respectively
(the singular and nominative forms).
122
Hindi verbs are inflected by gender, number,
person, mood and tense. Hindi adjectives take
inflections for gender and case. The number of
inflected forms in different POS categories varies
considerably, with verbs tending to have a lot more
inflections than other POS categories.
3 System Description
In order to construct a morphological lexicon, we
used a rule-based approach combined with heuris-
tics for stem and root selection. When used in
concert with a POS tagger, they could extract a
very accurate morphological lexicon from a raw
text corpus. Our system therefore consists of the
following components:
1. A raw text corpus in the Hindi language large
enough to contain a few hundred thousand
unique word-forms and a smaller labelled
corpus to train a POS tagger with.
2. A list of rules comprising suffix strings and
constraints on the word-forms and POS cate-
gories that they can be applied to.
3. A stemmer that uses the above rules, and
some heuristics to identify and reduce in-
flected word-forms to stems.
4. A POS tagger to identify the POS category or
categories that the word forms in the raw text
corpus can belong to.
5. A root selector that identifies a root word-
form and its paradigm from a stem and a set
of inflections of the stem.
The components of the system are described in
more detail below.
3.1 Text Corpora
Rules alone are not always sufficient to identify
the best stem or root for a word-form, when the
words being stemmed have very few inflectional
forms or when a word might be stemmed in one
of many ways. In that case, a raw text corpus can
provide important clues for identifying them.
The raw text corpus that we use is the Web-
Duniya corpus which consists of 1.4 million sen-
tences of newswire and 21.8 million words. The
corpus, being newswire, is clearly not balanced.
It has a preponderance of third-person forms
whereas first and second person inflectional forms
are under-represented.
Name POS Paradigm Suffixes Root
laDkA noun {?A?,?e?,?on?} ?A?
laDkI noun {?I?,?iyAn?} ?I?
dho verb {??,?yogI?,?nA?,. . .} ??
chal verb {??,?ogI?,?nA?,. . .} ??
Table 3: Sample Paradigm Suffix Sets
Since Hindi word boundaries are clearly marked
with punctuation and spaces, tokenization was
an easy task. The raw text corpus yielded ap-
proximately 331000 unique word-forms. When
words beginning with numbers were removed, we
were left with about 316000 unique word-forms of
which almost half occurred only once in the cor-
pus.
In addition, we needed a corpus of 45,000
words labelled with POS categories using the IL-
POST tagset (Sankaran et al, 2008) for the POS
tagger.
3.2 Rules
The morphological rules input into the system are
used to recognize word-forms that together be-
long to a paradigm. Paradigms can be treated as a
set of suffixes that can be used to generate inflec-
tional word-forms from a stem. The set of suffixes
that constitutes a paradigm defines an equivalence
class on the set of unique word-forms in the cor-
pus.
For example, the laDkA paradigm in Table 2
would be represented by the set of suffix strings
{?A?, ?e?, ?on?} derived from the word-forms
laDkA, laDke and laDkon. A few paradigms
are listed in Table 3.
The suffix set formalism of a paradigm closely
resembles the one used in a previous attempt at
unsupervised paradigm extraction (Zeman, 2007)
but differs from it in that Zeman (2007) considers
the set of word-forms that match the paradigm to
be a part of the paradigm definition.
In our system, we represent the morphological
rules by a list of suffix add-delete rules. Each rule
in our method is a five-tuple {?, ?, ?, ?, ?} where:
? ? is the suffix string to be matched for the
rule to apply.
? ? is the portion of the suffix string after which
the stem ends.
? ? is a POS category in which the string ? is a
valid suffix.
123
? ? ? ? ?
?A? ?? Noun N1 ?A?
?on? ?? Noun N1,N3 ?A?
?e? ?? Noun N1 ?A?
?oyogI? ?o? Verb V5 ?o?
Table 4: Sample Paradigm Rules
Word Form ? Match Stem Root
laDkA laDk + A laDk laDkA
laDkon laDk + on laDk laDkA
laDke laDk + e laDk laDkA
dhoyogI dh + oyogI dh + o dho
Table 5: Rule Application
? ? is a list of paradigms that contain the suffix
string ?.
? ? is the root suffix
The sample paradigm rules shown in Table 4
would match the words laDkA, laDkon, laDke
and dhoyogI respectively and cause them to be
stemmed and assigned roots as shown in Table 5.
The rules by themselves can identify word-and-
paradigm entries from the raw text corpus if a suf-
ficient number of inflectional forms were present.
For instance, if the words laDkA and laDkon
were present in the corpus, by taking the intersec-
tion of the paradigms associated with the match-
ing rules in Table 4, it would be possible to infer
that the root word-form was laDkA and that the
paradigm was N1.
We needed to create about 300 rules for Hindi.
The rules could be stored in a list indexed by the
suffix in the case of Hindi because the number of
possible suffixes was small. For highly aggluti-
native languages, such as Tamil and Malayalam,
which can have thousands of suffixes, it would be
necessary to use a Finite State Machine represen-
tation of the rules.
3.3 Suffix Evidence
We define the term ?suffix evidence? for a poten-
tial stem as the number of word-forms in the cor-
pus that are composed of a concatenation of the
stem and any valid suffix. For instance, the suf-
fix evidence for the stem laDk is 2 if the word-
forms laDkA and laDkon are the only word-
forms with the prefix laDk that exist in the corpus
and A and on are both valid suffixes.
BSE Word-forms Accuracy
1 20.5% 79%
2 20.0% 70%
3 13.2% 70%
4 10.8% 81%
5 & more 35.5% 80%
Table 6: % Frequency and Accuracy by BSE
BSE Nouns Verbs Others
1 292 6 94
2 245 2 136
3 172 15 66
4 120 16 71
5 & more 103 326 112
Table 7: Frequency by POS Category
Table 6 presents word-form counts for differ-
ent suffix evidence values for the WebDuniya cor-
pus. Since the real stems for the word-forms were
not known, the prefix substring with the highest
suffix evidence was used as the stem. We shall
call this heuristically selected stem the best-suffix-
evidence stem and its suffix evidence as the best-
suffix-evidence (BSE).
It will be seen from Table 6 that about 20% of
the words have a BSE of only 1. Altogether about
40% of the words have a BSE of 1 or 2. Note
that all words have a BSE of atleast 1 since the
empty string is also considered a valid suffix. The
fraction is even higher for nouns as shown in Table
7.
It must be noted that the number of nouns with
a BSE of 5 or more is in the hundreds only be-
cause of erroneous concatenations of suffixes with
stems. Nouns in Hindi do not usually have more
than four inflectional forms.
The scarcity of suffix evidence for most word-
forms poses a huge obstacle to the extraction of a
high-coverage lexicon because :
1. There are usually multiple ways to pick a
stem from word-forms with a BSE of 1 or 2.
2. Spurious stems cannot be detected easily
when there is no overwhelming suffix evi-
dence in favour of the correct stem.
3.4 Gold Standard
The gold standard consists of one thousand word-
forms picked at random from the intersection of
124
the unique word-forms in the unlabelled Web-
Duniya corpus and the POS labelled corpus. Each
word-form in the gold standard was manually ex-
amined and a stem and a root word-form found for
it.
For word-forms associated with multiple POS
categories, the stem and root of a word-form were
listed once for each POS category because the seg-
mentation of a word could depend on its POS cat-
egory. There were 1913 word and POS category
combinations in the gold standard.
The creation of the stem gold standard needed
some arbitrary choices which had to be reflected
in the rules as well. These concerned some words
which could be stemmed in multiple ways. For in-
stance, the noun laDkI meaning ?girl? could be
segmented into the morphemes laDk and I or al-
lowed to remain unsegmented as laDkI. This is
because by doing the former, the stems of both
laDkA and laDkI could be conflated whereas
by doing the latter, they could be kept separate
from each other. We arbitrarily made the choice
to keep nouns ending in I unsegmented and made
our rules reflect that choice.
A second gold standard consisting of 1000
word-forms was also created to be used in eval-
uation and as training data for supervised algo-
rithms. The second gold standard contained 1906
word and POS category combinations. Only word-
forms that did not appear in the first gold standard
were included in the second one.
3.5 Stemmer
Since the list of valid suffixes is given, the stem-
mer does not need to discover the stems in the lan-
guage but only learn to apply the right one in the
right place. We experimented with three heuristics
for finding the right stem for a word-form. The
heuristics were:
? Longest Suffix Match (LSM) - Picking the
longest suffix that can be applied to the word-
form.
? Highest Suffix Evidence (HSE) - Picking the
suffix which yields the stem with the highest
value for suffix evidence.
? Highest Suffix Evidence with Supervised
Rule Selection (HSE + Sup) - Using labelled
data to modulate suffix matching.
3.5.1 Longest Suffix Match (LSM)
In the LSM heuristic, when multiple suffixes can
be applied to a word-form to stem it, we choose
the longest one. Since Hindi has concatenative
morphology with only postfix inflection, we only
need to find one matching suffix to stem it. It is
claimed in the literature that the method of us-
ing the longest suffix match works better than ran-
dom suffix selection (Sarkar and Bandyopadhyay,
2008). This heuristic was used as the baseline for
our experiments.
3.5.2 Highest Suffix Evidence (HSE)
In the HSE heuristic, which has been applied be-
fore to unsupervised morphological segmentation
(Goldsmith, 2001), stemming (Pandey and Sid-
diqui, 2008), and automatic paradigm extraction
(Zeman, 2007), when multiple suffixes can be ap-
plied to stem a word-form, the suffix that is picked
is the one that results in the stem with the high-
est suffix evidence. In our case, when computing
the suffix evidence, the following additional con-
straint is applied: all the suffixes used to compute
the suffix evidence score for any stem must be as-
sociated with the same POS category.
For example, the suffix yon is only applicable
to nouns, whereas the suffix ta is only applicable
to verbs. These two suffixes will therefore never
be counted together in computing the suffix evi-
dence for a stem. The algorithm for determining
the suffix evidence computes the suffix evidence
once for each POS category and then returns the
maximum.
In the absence of this constraint, the accuracy
drops as the size of the raw word corpus increases.
3.5.3 HSE and Supervised Rule Selection
(HSE + Sup)
The problem with the aforementioned heuristics is
that there are no weights assigned to rules. Since
the rules for the system were written to be as gen-
eral and flexible as possible, false positives were
commonly encountered. We propose a very sim-
ple supervised learning method to circumvent this
problem.
The training data used was a set of 1000 word-
forms sampled, like the gold standard, from the
unique word-forms in the intersection of the raw
text corpus and the POS labelled corpus. The set
of word-forms in the training data was disjoint
from the set of word-forms in the gold standard.
125
Rules Accur Prec Recall F-Score
Rules1 73.65% 68.25% 69.4% 68.8%
Rules2 75.0% 69.0% 77.6% 73.0%
Table 8: Comparison of Rules
Gold 1 Accur Prec Recall F-Score
LSM 71.6% 65.8% 66.1% 65.9%
HSE 76.7% 70.6% 77.9% 74.1%
HSE+Sup 78.0% 72.3% 79.8% 75.9%
Gold 2 Accur Prec Recall F-Score
LSM 75.7% 70.7% 72.7% 71.7%
HSE 75.0% 69.0% 77.6% 73.0%
HSE+Sup 75.3% 69.3% 78.0% 73.4%
Table 9: Comparison of Heuristics
The feature set consisted of two features: the
last character (or diacritic) of the word-form, and
the suffix. The POS category was an optional fea-
ture and used when available. If the number of in-
correct splits exceeded the number of correct splits
given a feature set, the rule was assigned a weight
of 0, else it was given a weight of 1.
3.5.4 Comparison
We compare the performance of our rules with
the performance of the Lightweight Stemmer for
Hindi (Ramanathan and Rao, 2003) with a re-
ported accuracy of 81.5%. The scores we report
in Table 8 are the average of the LSM scores
on the two gold standards. The stemmer using
the standard rule-set (Rules1) does not perform as
well as the Lightweight Stemmer. We then hand-
crafted a different set of rules (Rules2) with ad-
justments to maximize its performance. The ac-
curacy was better than Rules1 but not quite equal
to the Lightweight Stemmer. However, since our
gold standard is different from that used to eval-
uate the Lightweight Stemmer, the comparison is
not necessarily very meaningful.
As shown in Table 9, in F-score comparisons,
HSE seems to outperform LSM and HSE+Sup
seems to outperform HSE, but the improvement
in performance is not very large in the case of the
second gold standard. In terms of accuracy scores,
LSM outperforms HSE and HSE+Sup when eval-
uated against the second gold standard.
POS Correct Incorrect POS Errors
Noun 749 231 154
Verb 324 108 0
Adjective 227 49 13
Others 136 82 35
Table 10: Errors by POS Category
3.5.5 Error Analysis
Table 10 lists the number of correct stems, in-
correct stems, and finally a count of those incor-
rect stems that the HSE+Sup heuristic would have
gotten right if the POS category had been avail-
able. From the numbers it appears that a size-
able fraction of the errors, especially with noun
word-forms, is caused when a suffix of the wrong
POS category is applied to a word-form. More-
over, prior work in Bangla (Sarkar and Bandy-
opadhyay, 2008) indicates that POS category in-
formation could improve the accuracy of stem-
ming.
Assigning POS categories to word-forms re-
quires a POS tagger and a substantial amount of
POS labelled data as described below.
3.5.6 POS Tagging
The POS tagset used was the hierarchical tagset
IL-POST (Sankaran et al, 2008). The hierarchical
tagset supports broad POS categories like nouns
and verbs, less broad POS types like common and
proper nouns and finally, at its finest granularity,
attributes like gender, number, case and mood.
We found that with a training corpus of about
45,000 tagged words (2366 sentences), it was pos-
sible to produce a reasonably accurate POS tag-
ger2, use it to label the raw text corpus with broad
POS tags, and consequently improve the accuracy
of stemming. For our experiments, we used both
the full training corpus of 45,000 words and a sub-
set of the same consisting of about 20,000 words.
The POS tagging accuracies obtained were ap-
proximately 87% and 65% respectively.
The reason for repeating the experiment using
the 20,000 word subset of the training data was to
demonstrate that a mere 20,000 words of labelled
data, which does not take a very great amount of
2The Part-of-Speech tagger used was an implementa-
tion of a Cyclic Dependency Network Part-of-Speech tagger
(Toutanova et al, 2003). The following feature set was used
in the tagger: tag of previous word, tag of next word, word
prefixes and suffixes of length exactly four, bigrams and the
presence of numbers or symbols.
126
time and effort to create, can produce significant
improvements in stemming performance.
In order to assign tags to the words of the gold
standard, sentences from the raw text corpus con-
taining word-forms present in the gold standard
were tagged using a POS tagger. The POS cate-
gories assigned to each word-form were then read
off and stored in a table.
Once POS tags were associated with all the
words, a more restrictive criterion for matching a
rule to a word-form could be used to calculate the
BSE in order to determine the stem of the word-
form. When searching for rules, and consequently
the suffixes, to be applied to a word-form, only
rules whose ? value matches the word-form?s POS
category were considered. We shall call the HSE
heuristic that uses POS information in this way
HSE+Pos.
3.6 Root Selection
The stem lexicon obtained by the process de-
scribed above had to be converted into a root word-
form lexicon. A root word-form lexicon is in some
cases more useful than a stem lexicon, for the fol-
lowing reasons:
1. Morphological lexicons are traditionally in-
dexed by root word-forms
2. Multiple root word-forms may map to one
stem and be conflated.
3. Tools that use the morphological lexicon may
expect the lexicon to consist of roots instead
of stems.
4. Multiple root word-forms may map to one
stem and be conflated.
5. Stems are entirely dependent on the way
stemming rules are crafted. Roots are inde-
pendent of the stemming rules.
The stem lexicon can be converted into a root
lexicon using the raw text corpus and the morpho-
logical rules that were used for stemming, as fol-
lows:
1. For any word-form and its stem, list all rules
that match.
2. Generate all the root word-forms possible
from the matching rules and stems.
3. From the choices, select the root word-form
with the highest frequency in the corpus.
Relative frequencies of word-forms have been
used in previous work to detect incorrect affix at-
tachments in Bengali and English (Dasgupta and
Ng, 2007). Our evaluation of the system showed
that relative frequencies could be very effective
predictors of root word-forms when applied within
the framework of a rule-based system.
4 Evaluation
The goal of our experiment was to build a high-
coverage morphological lexicon for Hindi and to
evaluate the same. Having developed a multi-stage
system for lexicon extraction with a POS tagging
step following by stemming and root word-form
discovery, we proceeded to evaluate it as follows.
The stemming and the root discovery module
were evaluated against the gold standard of 1000
word-forms. In the first experiment, the precision
and recall of stemming using the HSE+Pos algo-
rithm were measured at different POS tagging ac-
curacies.
In the second experiment the root word-form
discovery module was provided the entire raw
word corpus to use in determining the best pos-
sible candidate for a root and tested using the gold
standard. The scores obtained reflect the perfor-
mance of the overall system.
For stemming, the recall was calculated as the
fraction of stems and suffixes in the gold standard
that were returned by the stemmer for each word-
form examined. The precision was calculated as
the fraction of stems and suffixes returned by the
stemmer that matched the gold standard. The F-
score was calculated as the harmonic mean of the
precision and recall.
The recall of the root lexicon was measured as
the fraction of gold standard roots that were in the
lexicon. The precision was calculated as the frac-
tion of roots in the lexicon that were also in the
gold standard. Accuracy was the percentage of
gold word-forms? roots that were matched exactly.
In order to approximately estimate the accuracy
of a stemmer or morphological analyzer that used
such a lexicon, we also calculated the accuracy
weighted by the frequency of the word-forms in
a small corpus of running text. The gold standard
tokens were seen in this corpus about 4400 times.
We only considered content words (nouns, verbs,
adjectives and adverbs) in this calculation.
127
Gold1 Accur Prec Recall F-Sco
POS 86.7% 82.4% 86.2% 84.2%
Sup+POS 88.2% 85.2% 87.3% 86.3%
Gold2 Accur Prec Recall F-Sco
POS 81.8% 77.8% 82.0% 79.8%
Sup+POS 83.5% 80.2% 82.6% 81.3%
Table 11: Stemming Performance Comparisons
Gold 1 Accur Prec Recall F-Sco
No POS 76.7% 70.6% 77.9% 74.1%
65% POS 82.3% 77.5% 81.4% 79.4%
87% POS 85.4% 80.8% 85.1% 82.9%
Gold POS 86.7% 82.4% 86.2% 84.2%
Table 12: Stemming Performance at Different
POS Tagger Accuracies
5 Results
The performance of our system using POS tag in-
formation is comparable to that obtained by Sarkar
and Bandyopadhyay (2008). Sarkar and Bandy-
opadhyay (2008) obtained stemming accuracies of
90.2% for Bangla using gold POS tags. So in the
comparisons in Table 11, we use gold POS tags
(row two) and also supervised learning (row three)
using the other gold corpus as the labelled training
corpus. We present the scores for the two gold
standards separately. It must be noted that Sarkar
and Bandyopadhyay (2008) conducted their ex-
periments on Bangla, and so the results are not
exactly comparable.
We also evaluate the performance of stemming
using HSE with POS tagging by a real tagger at
two different tagging accuracies - approximately
65% and 87% - as shown in Table 12. We com-
pare the performance with gold POS tags and a
baseline system which does not use POS tags. We
do not use labelled training data for this section of
the experiments and only evaluate against the first
gold standard.
Table 13 compares the F-scores for root discov-
Gold 1 Accur Prec Recall F-Sco
No POS 71.7% 77.6% 78.8% 78.1%
65% POS 82.5% 87.2% 88.9% 88.0%
87% POS 87.0% 94.1% 95.3% 94.6%
Gold POS 89.1% 95.4% 97.9% 96.6%
Table 13: Root Finding Accuracy
Gold 1 Stemming Root Finding
65% POS 85.6% 87.0%
87% POS 87.5% 90.6%
Gold POS 88.5% 90.2%
Table 14: Weighted Stemming and Root Finding
Accuracies (only Content Words)
ery at different POS tagging accuracies against a
baseline which excludes the use of POS tags alto-
gether. There seems to be very little prior work
that we can use for comparison here. To our
knowledge, the closest comparable work is a sys-
tem built by Oliver and Tadic? (2004) in order to
enlarge a Croatian Morphological Lexicon. The
overall performance reported by Tadic? et alwas
as follows: (precision=86.13%, recall=35.36%,
F1=50.14%).
Lastly, Table 14 shows the accuracy of stem-
ming and root finding weighted by the frequencies
of the words in a running text corpus. This was
calculated only for content words.
6 Conclusion
We have described a system for automatically con-
structing a root word-form lexicon from a raw
text corpus. The system is rule-based and uti-
lizes a POS tagger. Though preliminary, our re-
sults demonstrate that it is possible, using this
method, to extract a high-precision and high-recall
root word-form lexicon. Specifically, we show
that with a POS tagger capable of labelling word-
forms with POS categories at an accuracy of about
88%, we can extract root word-forms with an ac-
curacy of about 87% and a precision and recall of
94.1% and 95.3% respectively.
Though the system has been evaluated on Hindi,
the techniques described herein can probably be
applied to other inflectional languages. The rules
selected by the system and applied to the word-
forms also contain information that can be used to
determine the paradigm membership of each root
word-form. Further work could evaluate the accu-
racy with which we can accomplish this task.
7 Acknowledgements
We would like to thank our colleagues Priyanka
Biswas, Kalika Bali and Shalini Hada, of Mi-
crosoft Research India, for their assistance in the
creation of the Hindi root and stem gold standards.
128
References
Lionel Cle?ment, Beno??t Sagot and Bernard Lang.
2004. Morphology based automatic acquisition of
large-coverage lexica. In Proceedings of LREC
2004, Lisbon, Portugal.
Sajib Dasgupta and Vincent Ng. 2007. High-
Performance, Language-Independent Morphologi-
cal Segmentation. In Main Proceedings of NAACL
HLT 2007, Rochester, NY, USA.
Markus Forsberg, Harald Hammarstro?m and Aarne
Ranta. 2006. Morphological Lexicon Extraction
from Raw Text Data. In Proceedings of the 5th In-
ternational Conference on Advances in Natural Lan-
guage Processing, FinTAL, Finland.
John A. Goldsmith. 2001. Linguistica: An Automatic
Morphological Analyzer. In Arika Okrent and John
Boyle, editors, CLS 36: The Main Session, volume
36-1, Chicago Linguistic Society, Chicago.
Erika de Lima. 1998. Induction of a Stem Lexicon for
Two-Level Morphological Analysis. In Proceedings
of the Joint Conferences on New Methods in Lan-
guage Processing and Computational Natural Lan-
guage Learning, NeMLaP3/CoNLL98, pp 267-268,
Sydney, Australia.
Antoni Oliver, Marko Tadic?. 2004. Enlarging the
Croatian Morphological Lexicon by Automatic Lex-
ical Acquisition from Raw Corpora. In Proceedings
of LREC 2004, Lisbon, Portugal.
Amaresh Kumar Pandey and Tanveer J. Siddiqui.
2008. An Unsupervised Hindi Stemmer with
Heuristic Improvements. In Proceedings of the Sec-
ond Workshop on Analytics for Noisy Unstructured
Text Data, AND 2008, pp 99-105, Singapore.
A Ramanathan and D. D. Rao. 2003. A Lightweight
Stemmer for Hindi. Presented at EACL 2003, Bu-
dapest, Hungary.
Beno??t Sagot. 2005. Automatic Acquisition of a
Slovak Lexicon from a Raw Corpus. In Lecture
Notes in Artificial Intelligence 3658, Proceedings of
TSD?05, Karlovy Vary, Czech Republic.
Beno??t Sagot. 2007. Building a Morphosyntactic Lexi-
con and a Pre-Syntactic Processing Chain for Polish.
In Proceedings of LTC 2007, Poznan?, Poland.
Beno??t Sagot, Lionel Cle?ment, E?ric Villemonte de la
Clergerie and Pierre Boullier. 2006. The Lefff 2
Syntactic Lexicon for French: Architecture, Acqui-
sition, Use. In Proceedings of LREC?06, Genoa,
Italy.
Baskaran Sankaran, Kalika Bali, Monojit Choudhury,
Tanmoy Bhattacharya, Pushpak Bhattacharyya,
Girish Nath Jha, S. Rajendran, K. Saravanan, L.
Sobha and K.V. Subbarao. 2008. A Common Parts-
of-Speech Tagset Framework for Indian Languages.
In Proceedings of the Sixth International Language
Resources and Evaluation (LREC?08), Marrakech,
Morocco.
Sandipan Sarkar and Sivaji Bandyopadhyay. 2008.
Design of a Rule-based Stemmer for Natural Lan-
guage Text in Bengali. In Proceedings of the
IJCNLP-08 Workshop on NLP for Less Privileged
Languages, Hyderabad, India.
Kristina Toutanova, Dan Klein, Christopher D. Man-
ning and Yoram Singer. 2003. Feature-Rich Part-
of-Speech Tagging with a Cyclic Dependependency
Network In Proceedings of HLT-NAACL 2003
pages 252-259.
Daniel Zeman. 2007. Unsupervised Acquisition of
Morphological Paradigms from Tokenized Text. In
Working Notes for the Cross Language Evaluation
Forum CLEF 2007 Workshop, Budapest, Hungary.
129
Proceedings of the IJCNLP-08 Workshop on NLP for Less Privileged Languages, pages 19?26,
Hyderabad, India, January 2008. c?2008 Asian Federation of Natural Language Processing
Prototype Machine Translation System From Text-To-Indian Sign 
Language 
Tirthankar Dasgupta 
IIT, Kharagpur 
tirtha@ 
cse.iitkgp.ernet.in 
Sandipan Dandpat 
IIT, Kharagpur 
sandipan@
cse.iitkgp.ernet.in 
Anupam Basu 
IIT, Kharagpur 
anupambas@ 
gmail.com 
Abstract 
This paper presents a prototype Text-To-
Indian Sign Language (ISL) translation 
system. The system will help dissemination 
of information to the deaf people in India. 
The current system takes English sentence 
as input, performs syntactic analysis, and 
generates the corresponding ISL structure. 
Since ISL does not have any written form, 
the output is represented in terms of pre-
recorded video streams. The system uses 
Lexical Functional Grammar (LFG) for-
malism for representing ISL syntax.   
1 Introduction 
The All India Federation of the deaf estimates 
around 4 million deaf people and more than 10 
million hard of hearing people in India (Zeshan et 
al, 2004). Studies revealed that, one out of every 
five deaf people in the world is from India. More 
than 1 million deaf adults and around 0.5 million 
deaf children in India uses Indian Sign Language 
(henceforth called ISL) as a mode of communica-
tion (Zeshan et al 2004). ISL is not only used by 
the deaf people but also by the hearing parents of 
the deaf children, the hearing children of deaf  
adults and hearing deaf educators (Zeshan et al 
2004).  
Due to their inability in accessing information 
through common broadcast modes like television, 
radio etc., and communication for the deaf com-
munity in common places like railway, bank, and 
hospitals is difficult.  
Efforts to extend the existing means of commu-
nication for the hearing impaired include close cir-
cuit captioning in television and communication 
through interpreter. The first approach assumes a 
good knowledge in written languages like English, 
Hindi, or Bengali. The second approach is not al-
ways practically feasible.  
A large section of the hearing impaired in India 
uses ISL as their mode of communication. How-
ever, due to the inherent difficulty in their written 
texts, an automatic Text-to-ISL translation system 
could help to make more information and services 
accessible to the hearing impaired. Moreover, the 
system will not only improve information access, 
but it can also be used as an educational tool to 
learn ISL.  
Though some work has been done on machine 
translation (MT) from English to American or Brit-
ish Sign Language (SL) (Huenerfauth, 2003), but 
for ISL, MT systems are still in its infancy. The 
underlying architecture for most of the systems are 
based on:  
 
I. Direct translation: This requires knowledge 
of both the source and the target language. 
Moreover, word order of the output may not 
be the desired one.  
II. Statistical MT: It requires large parallel cor-
pora    which is very difficult to collect. 
III. Transfer based architecture. As ISL does not   
relate to other SLs of either Asia or Europe 
(Zeshan, 2003), the existing systems transfer 
grammar rules cannot be applied to translate 
English to ISL.  
 
Further, some of the systems are domain specific 
in nature, and cannot be used to generic systems. 
Hence, most of the above systems remain unusable 
for the deaf community of India. This is the prime 
motivation behind building a generic English Text-
to-ISL translation system. 
The objective of this paper is to present a proto-
type English-to-ISL generic machine translation 
19
system. Currently the system takes simple English 
sentences as input and generates ISL-gloss which 
may then be converted into the Hamburg Notation 
System (HamNoSys)1 (Prillwitz et. al, 1989). The 
HamNoSys representation will provide signing 
instructions to the sign synthesis module, to gener-
ate an animated representation of ISL to the user. 
Lexical Functional grammar (LFG) f-structure is 
used to represent ISL syntax.  
The paper is organized as follows: Section 2 
presents linguistic issues related to ISL. Section 3 
presents a brief summery of the related works. Sec-
tion 4 presents the overall system architecture. Sec-
tion 5 presents system evaluation and results. Sec-
tion 6 presents the sign synthesis via HamNoSys, 
and Section 7 presents conclusion and future work.  
2 ISL Linguistic Issues 
Indian Sign Language (ISL) is a visual-spatial lan-
guage which provides linguistic information using 
hands, arms, face, and head/body postures. A sign 
is a sequential or parallel construction of its man-
ual and non-manual components. A manual com-
ponent can be defined by several parameters like 
hand shape, orientation, position, and movements 
where as non-manual components are defined by 
facial expressions, eye gaze, and head/body pos-
ture (Zeshan, 2003). However, there exist some 
signs which may contain only manual or non-
manual components. For example the sign ?Yes? is 
signed by vertical head nod and it has no manual 
component.  
ISL lexicon is categorized according to the spa-
tial behavior of the signs (Zeshan, 2003). There are 
three open lexical classes: i) Signs whose place of 
articulation are fixed, like, ?hand?, ?teeth?, ?eye?, 
?me?, and ?you? as shown in Fig. 1. ii) Signs 
whose place of articulation can change, like, 
?good,? ?friend,? and ?marry? as shown in Fig. 2. 
iii) Directional signs are those where there is a 
movement between two points in space. For exam-
ple, in the sentence ?I help him? the head word is 
?help? and direction of the sign is from subject ?I? 
to the object ?him? (Fig. 3). Directional signs gen-
erally show verbal property (Zeshan, 2003). Apart 
from the directional signs, ISL morphology is 
mostly derivational in nature and there are no af-
fixes in signs. The closed lexical class contains 
                                                 
1 www.sign-lang.uni-hamburg.de/Projekte/HamNoSys 
classifier hand shapes, discourse markers, and non-
manual signs (Zeshan, 2003). A classifier hand 
shape contains specification related to hand con-
figuration that represents the characteristics of a 
referent. For example, consider the sentence ?Put 
the cup on the table?. Here the hand configuration 
will contain shape of a ?cup? added with a move-
ment to express the event ?put?.  
ISL discourse structure is classified into manual 
and non-manual markers. Manual discourse mark-
ers can occur either in clause final position (as in, 
?it?s over, what else we can do??) or in clause ini-
tial position (like, ?well, I have nothing to say?). 
The non-manual marker like ?head nodding? oc-
curs only in clause final position after the last 
manual sign of the clause.  
 
             
Me Eye 
 
Fig.1: Signs whose place of articulation is fixed 
(Vasistha et. al 1998) 
 
 
 
 
Friend
 
 
 
 
Fig. 2: Signs whose place of ar-
ticulation can change (Vasistha 
et. al 1998)
 
 
Fig. 3:  Directional Sign, ?I help you?. Taken 
from AYJNIHH workbook video CD.  
3 The State-of-Art for Text-to-Sign Lan-
guage 
In spite of the advancements in modern computer 
science technology, there is a paucity of research 
in developing machine translation (MT) system on 
sign language particularly in India (Zeshan et al 
2004). Some of the MT systems for other sign lan-
20
guage are briefly described below. The underlying 
MT architecture can be classified into i) Direct 
translation system, ii) Transfer based architecture 
and iii) Statistical MT. 
The direct translation approach generates the SL 
by direct replacement of the words of input English 
sentence. Generally the word order of the SL re-
mains the same as that of the English text. How-
ever, as in the case of English to ISL, the target SL 
may not allow the same word order. Also, the sys-
tem assumes a strong knowledge of both the Eng-
lish as well as the target SL. 
Some of the direct translation systems include:  
 
? TESSA: A Speech-To-British Sign Language 
(BSL) translation system that aims to provide a 
communication aid between a deaf person and a 
Post Office clerk. The system uses formulaic 
grammar approach where a set of pre-defined 
phrases are stored and translation is done by us-
ing a phrase lookup table. However, the use of 
small set of sentences as templates makes 
TESSA a very domain specific system. It as-
sumes a very restricted discourse between the 
participants (Cox, 2002). 
? The SignSynth project (Grieve- smith 1998; 
Grieve-smith, 1999) uses ASCII-Stokoe model 
for the representation of Signs. The animated 
output is generated by converting ASCII-Stokoe 
into VRML (Virtual Reality Modeling Lan-
guage). In his another project Grieve-Smith pro-
posed a Text to American Sign Language (ASL) 
machine translation system. The system has been 
evaluated in the weather information domain. 
 
In a transfer architecture system, the source lan-
guage representation is transformed into a suitable 
syntactically/semantically correct target language 
form by applying proper transfer grammar rules. 
These rules are dependent upon both the source 
and the target language. However, as the 
source/target language changes new rules are need 
to be added. The transfer grammar approach is not 
only used in text to SL MT systems but also in 
text-to-text MT systems, like the Shakti MT sys-
tem which is used to translate English text to Hindi 
(Bharati et. al., 2001; Bharati et. al., 2003). The 
transfer architecture systems include: 
 
? The ViSiCAST translator, which is a English to 
British Sign Language (BSL) translation tool 
(Marshall & S?f?r, 2001; Bangham et al, 2000). 
The system uses HPSG (Pollard and Sag, 1994) 
formalism to represent source text into BSL and 
the grammar is implemented using a Prolog based 
system ALE. The system handles discourse phe-
nomena by using Discourse Representation Struc-
ture (DRS) (Bos et. al, 1994) and the phonology is 
represented in HamNoSys. This is one of the most 
successful system developed so far (Huenerfauth, 
2003). 
? The ASL workbench (Speers, 2001) is a Text-
To-ASL MT system which uses Lexical Functional 
Grammar (LFG) (Kaplan, 1989) formalism to rep-
resent English f-structure into ASL. The system 
uses a very sophisticated phonological model 
which is based on Movement-Hold principle of 
ASL phonology (Lidell & Johnson 1989). 
? The TEAM project is a Text-To-ASL translation 
system where, the STAG (Synchronous Tree Ad-
joining Grammar) formalism is used to represent 
source text into ASL syntactic structure (Zhao et 
al, 2000). The system maintains a bilingual lexicon 
to identify the valid word-sign pair. The output of 
the linguistic module was a written ASL gloss no-
tation. The manual and non-manual information, 
including the morphological variation, are embed-
ded with in the ASL gloss notation. The output of 
the synthesis module uses animated human models 
(Avatar). 
 
In addition, An Example based MT system for 
English-Dutch sign language was proposed by 
(Morrissey and Way, 2005). Stein et.al. (2006) has 
proposed a statistical MT system which uses Hid-
den Markov Model and IBM models for training 
the data. However, due to paucity of well anno-
tated corpora, the system has been evaluated using 
a very small set of data. 
3.1 Indian Scenario 
INGIT is a Hindi-To-Indian Sign Language (ISL) 
Machine Translation system has been built for the 
railway reservation domain (Kar et. al, 2006). The 
system takes input from the reservation clerk and 
translates into ISL. The output of the system is an 
animated representation of the ISL-gloss strings 
via HamNoSys. INGIT is based on Hybrid-
formulaic grammar approach unlike TESSA which 
uses purely formulaic approach. Here, Fluid Con-
struction Grammar (FCG) (Steels and Beule, 2006) 
21
is used to implement the Formulaic grammar. This 
is the only Hindi text-to-ISL machine translation 
tool encountered by us so far. However, the system 
is domain specific in nature and cannot be used for 
generic purpose. Further, the system does not have 
to handle any structural divergence between the 
source and the target language, as in most of the 
cases both Hindi and ISL show the same word or-
der. 
4 ISL MT Architecture 
In order to overcome the above mentioned prob-
lem, we initially developed a direct translation sys-
tem, however due to its inherent drawbacks, as 
mentioned in section 3, we need some other ap-
proach. One of the most popular techniques is to 
use statistical or case based MT system. However 
ISL does not have any written form, so it is very 
difficult to find any natural source of parallel cor-
pora. Niedle et al (2000) have proposed an ap-
proach to collect corpus for statistical MT research, 
in his approach first, annotation standard for the 
various hand shape movements was developed, 
then the Sign Language performances were re-
corded, and finally the recorded videos were 
manually transcribed. This is a very slow and ex-
pensive process. Due to the difficulty in obtaining 
parallel corpora of ISL, the statistical MT ap-
proaches may not be a feasible solution to our 
problem. Hence we decided to build a rule based 
transfer grammar MT system discussed in this sec-
tion. 
The system architecture of the proposed English 
Text-To-ISL MT system is composed of the fol-
lowing four essential modules (see Fig. 4): 
 
1. Input text preprocessor and parser 
2. LFG f-structure representation 
3. Transfer Grammar Rules 
4. ISL Sentence Generation 
5. ISL  synthesis 
4.1 Text Analysis & Syntactic Parsing 
The current Text-To-ISL translator takes simple 
English sentence as an input to the parser. We de-
fine simple sentence as, a sentence containing only 
one main verb. The input sentence is then parsed 
using the Minipar parser (Lin, 1998) and a depend-
ency structure is constructed from the parse tree. 
However, before parsing, the input text is passed to 
the preprocessing unit, where we try to identify the 
frozen phrases2 and temporal expressions3 which 
the syntactic parser is unable to identify. We pre-
pare a phrase lookup table consisting of 350 frozen 
phrases and temporal expressions which are identi-
fied before the input text is parsed. The parsing 
stage also includes classification of plural nouns. 
The plurality is identified using an English mor-
phological analyzer. 
 
 
 
Fig. 4: Architecture of the Text-to-ISL MT system 
4.2 LFG Representation 
The Minipar generated dependency structure is 
more akin towards the LFG functional structure (f-
structure). The f-structure encodes grammatical 
relation (like subject, object, and tense) of the input 
sentence. It represents the internal structure of a 
sentence. This includes the representation of the 
higher syntactic and functional information of a 
sentence. This higher syntactic and functional in-
formation of a sentence is represented as a set of 
attribute-value pairs. In an attribute-value pair, the 
attribute corresponds to the name of a grammatical 
symbol (e.g. NUM, TENSE) or a syntactic function 
(e.g. SUBJ, OBJ) and the value is the corresponding 
feature possessed by the concerning constituent. 
For example, Fig. 5 shows the attribute-value pair 
for the sentence ?John Played Cricket?. The main 
advantage of f-structure is in its abstract represen-
tation of syntactic and grammatical information of 
a sentence. 
 
                                                 
2 Phrases that are composed of Idioms, and Metaphor 
3 Temporal Expressions contains Time, Day and Date. 
22
 
 
 
 
 
 
 
 
 F
4.3 ISL Generation 
In the generation stage, English f-structure is con-
verted to ISL f-structure by applying proper trans-
fer grammar rules. Two main operations are per-
formed during the generation phase: a) Lexical 
selection and b) Word order correspondence. 
Lexical selection is done using an English-ISL bi-
lingual lexicon. For example word like ?Dinner? in 
English is replaced by ?NIGHT FOOD? in ISL and 
?Mumbai? is replaced by the sign of ?BOMBAY?. 
 
(1) English: ?I had dinner with Sita? 
      ISL: ?I SITA WITH NIGHT FOOD FINISH? 
 
ISL has essentially a Subject-Object-Verb word 
order (unlike English which is Subject-Verb-
Object). For Example, (2) shows the change in 
word order from English to ISL.  
 
(2)  English: ?I have a computer? 
 ISL: ?I COMPUTER HAVE?. 
 
However, in some cases the sign sentence de-
pends upon the directionality of the verb as in (3). 
 
(3) English: ?I help you? 
 ISL: ?HELP + < hand movement from I-
            to-YOU>?. 
 
For sentences having only a subject and a verb, 
the subject always precedes the verb. Like: 
 
(4) English: ?The woman is deaf? 
 ISL:  ?WOMAN DEAF?. 
 
However, if the sentence contains a dummy sub-
ject (5), then the subject is removed from the out-
put. 
 
 (5) English: ?It is raining outside? 
 ISL: ?OUTSIDE RAINING? 
For negative sentences, a negation mark is used 
after the verb (6). The second bracket indicates a 
parallel non-manual component is attached with 
the sign ?LATE?.  
 
 (6) English: ?I am not late?  
 ISL: ?I {LATE + NOT}?. 
 
ig. 5: Attribute-Value pair for the sentence ISL has separate rules to handle adjectives oc-
curring before a noun. In most of the cases an ad-
jective must occur after the noun. However, if the 
adjective specifies a color then it should precede 
the noun (see (7) & (8)).  
?John Played Cricket? 
 
(7) English: ?The beautiful girl is playing?  
 ISL: ?GIRL BEAUTIFUL PLAY?  
 
(8) English: ?I see a black cat?  
 ISL: ?I BLACK CAT SEE?. 
 
WH-Interrogative markers (like who, what, 
when, and why) always occur at the end of the sen-
tence.  
 
 (9) English: ?When is your birthday?? 
 ISL: ?YOUR BIRTHDAY TIME+  
            QUESTION?. 
 
In case of yes/no type of questions, the sentence 
is followed by a non-manual yes-no marker 
(Zeshan, 2004). 
 
(10) English: ?Is the man deaf?? 
 ISL: ?MAN {DEAF} yes-no? 
 
Since ISL does not have any articles or conjunc-
tions, they are removed from the generated output 
as shown in example (2)-(10). 
5 System Evaluation  
Evaluating a Text-to-ISL MT system is difficult 
due to the absence of ISL written orthography. 
Hence, standard techniques for evaluating Text-
Text MT systems are not applicable for Text-to-
ISL systems. However, we have evaluated the sys-
tem based on the feedbacks of the ISL experts. The 
generated outputs of the system are shown to the 
ISL experts and are classified as either valid or 
invalid according to their understandability and 
quality. The system was evaluated on a set of 208 
23
sentences4. Table 1.1 summarizes the performance 
of the system. The overall system performance is 
around 90%. Most of the errors are due to com-
pound sentences and directional verbs5. To under-
stand the relative performance of the system on the 
simple sentences, we conducted two experiments 
removing compound construction and directional 
verbs. From the current experimental set up, 7% 
errors are propagated due to directional verbs and 
around 4% errors are due to compound construc-
tions.  
 
 No. of Sentences 
Accuracy 
(%) 
Overall Corpus size 208 89.4 
Sentences without di-
rectional verbs 193 96.37 
Sentences without 
compound construc-
tions 
201 92.53 
 
6 ISL Synthesis 
The ISL sentences thus generated are displayed via 
a stream of pre recorded videos or icons. However, 
it has been observed that the current approach of 
ISL synthesis is highly criticized (Grieve-Smith, 
1999). As, representing ISL signs by pre-recorded 
video will result in loss of information related to 
discourse, classifier predicate, and directionality of 
sign. Also, storing sign video takes a lot of mem-
ory overhead. To overcome this crisis further de-
velopments are in progress. We represent ISL signs 
by HamNoSys and the generated HamNoSys string 
will be passed to the signing avatar.  
6.1 HamNoSys 
Sign language does not have any written form. In 
order to define a sign we need a notation system. 
The Hamburg sign language Notation system 
(HamNoSys) is a phonetic transcription system 
used to transcribe signing gestures. It is a syntactic 
representation of a sign to facilitate computer 
processing. HamNoSys is composed of several 
parameters by which a signing gesture can be de-
fined like: 
                                                 
4  Corpus collected from ??A? level Introductory course in 
Indian Sign Language? Work Book AYJNIHH. 
5 Verbs corresponding to directional signs.
 
? Dominant hand?s shape. 
? Hand location with respect to the body. 
? Extended finger orientation. 
? Palm orientation 
? Movements (straight, circular or curved) 
? Non-manual signs. 
 
Fig. 9 shows an example where HamNoSys 
representation of the word ?WOMAN? is ex-
plained.  
 
 
 
 
 
 
In this example, the parameters like movement 
and non-manual signs are not present, as the sign 
?WOMAN? in ISL does not have these expres-
sions. Fig. 10 shows the ISL representation of 
?WOMAN?. 
 
 
 
 
7 Conclusion and Future works 
The paper presents a prototype text to ISL transla-
tion system. Our approach uses LFG f-structure to 
represent ISL syntax. As ISL does not have any 
written form, there is no standard source of ISL 
corpus. Hence, statistical MT methods are not fea-
sible under such a condition. Our system is still 
under development stage. The sign synthesis mod-
ule using an animated avatar has not been devel-
oped yet. We generate ISL output using pre-
recorded ISL videos. Further morphological func-
tionalities like, discourse, directionality, and classi-
fier predicates are handled minimally 
Table1.1: Evaluation Results 
Fig. 9: HamNoSys representation of ?WOMAN? 
Fig. 10: Sign of  ?WOMAN? 
(Vashista et.al, 1998) 
Extended Finger orientation
Handshape
Location
Palm
?? \  ???  
24
In the next stage of our work, we will try to 
handle directional sign, discourse, and classifiers. 
The sign representation should be done using an 
animated avatar via HamNoSys notation. We will 
also develop the sign annotation tool and finally, a 
larger corpus will be built for a better evaluation 
and results. 
References 
N. Badler, R. Bindiganavale, J. Allbeck, W. Schuler, L. 
Zhao, S. Lee, H. Shin, and M. Palmer 2000. Param-
eterized Action Representation and Natural Language 
Instructions for Dynamic Behavior Modification of 
Embodied Agents. AAAI Spring Symposium.  
J. A. Bangham, S. J. Cox, R. Elliot, J. R. W. Glauert, I. 
Marshall, S. Rankov, and M. Wells. 2000. Virtual 
signing: Capture, animation, storage and transmission 
? An overview of the ViSiCAST project. IEEE Semi-
nar on Speech and language processing for disabled 
and elderly people. 
A. Bharati, D. M. Sharma, R. Sangal. 2001. AnnCorra : 
An Introduction, Technical Report no: TR-LTRC-
014, LTRC, IIIT Hyderabad, Mar 2001, 
http://www.iiit.net/ltrc/ Publications/Techreports/TR-
LTRC-14 
A. Bharati, R. Moona, P. Reddy, B. Sankar, D.M. 
Sharma, R. Sangal, Machine Translation: The Shakti 
Approach, Pre-Conference Tutorial at ICON-2003. 
J. Bos, E. Mastenbroek, S. McGlashan, S. Millies, M. 
Pinkal. 1994. A Compositional DRS-based Formal-
ism for NLP Applications. Report 59. Universitaet 
des Saarlandes.  
S. Cox, M. Lincoln, J. Tryggvason, M. Nakisa, M . 
Wells, M. Tutt, S. Abbott. 2002. Tessa, a system to 
aid communication with deaf people. Fifth interna-
tional ACM conference on Assistive technologies. 
M. Huenerfauth. 2003. A Survey and Critique of 
American Sign Language Natural Language Genera-
tion and Machine Translation Systems. Technical Re-
port MS-CIS-03-32, Computer and Information Sci-
ence, University of Pennsylvania. 
A. Joshi, L. Levy and M. Takahashi. 1975. Tree Ad-
junct Grammar. Journal of computer and system sci-
ences. 
P. Kar, M. Reddy, A. Mukherjee, A. M. Raina. 2007. 
INGIT: Limited Domain Formulaic Translation from 
Hindi Strings to Indian Sign Language. ICON. 
Ronald M. Kaplan. 1989. The formal architecture of 
lexical-functional grammar. Journal of Information-
Science and Engineering 5: 305-322.  
Scott Liddell and R. E. Johnson. 1989. American Sign 
Language: The phonological base. Sign Language 
Studies 64: 195-277. 
D. Lin. 1998. Dependency-based evaluation of MINI-
PAR. In Workshop on the Evaluation of Parsing Sys-
tems, Granada, Spain,  
I. Marshall and ?. S?f?r. 2001. Extraction of semantic 
representations from syntactic SMU link grammar 
linkages.. In G. Angelova, editor, Proceedings of Re-
cent Advances in Natural Lanugage Processing, pp: 
154-159, Tzigov Chark, Bulgaria, September. 
S. Morrissey and A. Way. 2005. An Example-Based 
Approach to Translating Sign Language. In Proceed-
ings of Workshop Example-Based Machine Transla-
tion (MT X -05), Phuket, Thailand. 
C. Neidle, J. Kegl, D. MacLaughlin, B. Bahan, and R. 
G. Lee. 2000. The Syntax of American Sign Lan-
guage: Functional Categories and Hierarchical 
Structure. Cambridge, MA: The MIT Press. 
C. J. Pollard, and I. A. Sag. 1994. Head-driven Phrase 
Structure Grammar. University of Chicago Press, 
Chicago, IL. 
S. Prillwitz, R. Leven, H. Zienert, T. Hamke, and J. 
Henning. 1989. HamNoSys Version 2.0: Hamburg 
Notation System for Sign Languages: An Introduc-
tory Guide, volume 5 of International Studies on Sign 
Language and Communication of the Deaf. Signum 
Press, Hamburg, Germany,  
?. S?f?r and I. Marshall. 2001. .The architecture of an 
English-text-to-Sign-Languages translation system.. 
In G. Angelova, editor, Recent Advances in Natural 
Language Processing (RANLP), pp: 223-228. Tzigov 
Chark, Bulgaria. 
G. Angus Smith. 1998. Sign synthesis and sign phonol-
ogy. Proceedings of the First High Desert  Student 
Conference in Linguistics. 
G. Angus Smith. 1999. English to American Sign Lan-
guage machine translation of weather reports. Pro-
ceedings of the Second High Desert Student Confer-
ence in Linguistics. 
A. Speers. 1995. SL-Corpus: A computer tool for sign 
language corpora. Georgetown University.  
A. Speers. 2001. Representation of American Sign Lan-
guage for Machine Translation. PhD Dissertation, 
Department of Linguistics, Georgetown University. 
L. Steels and J. Beule. 2006, Unify and Merge in Fluid 
Construction Grammar, In: Lyon C., Nehaniv, L. & 
A. Cangelosi, Emergence and Evolution of Linguistic 
Communication, Lecture Notes in Computer Science. 
Springe-Verlag: Berlin,. 
25
D. Stein, J. Bungeroth and H. Ney. 2006. Morpho-
Syntax Based Statistical Methods for Sign Language 
Translation. In Proceedings of the 11th Annual 
conference of the European Association for Machine 
Translation. Oslo, Norway. 
M. Vasishta, J. Woodward and S. DeSantis. 1998. An 
Introduction to Indian Sign Language. All India Fed-
eration of the Deaf  (Third Edition). 
Elizabeth Winston. 1993. Spatial mapping in compara-
tive discourse frames in an American Sign Language 
lecture. Doctor of Philosophy in Linguistics diss., 
Georgetown University. 
L. Zhao, K. Kipper, W. Schuler, C. Vogler, N. Badler, 
and M. Palmer. 2000. A Machine Translation System 
from English to American Sign Language. Associa-
tion for Machine Translation in the Americas. 
U. Zeshan. 2003. Indo-Pakistani Sign Language Gram-
mar: A Typological Outline. Sign Language Studies - 
Volume 3, Number 2 , pp. 157-212. 
U. Zeshan. 2004. Interrogative Constructions in Signed 
Languages. Crosslinguistic Perspectives Language - 
Volume 80, Number 1, pp. 7-39. 
U. Zeshan, M. Vasishta, M. Sethna. 2004. Implementa-
tion of Indian sign language in educational settings- 
Volume 15, Number 2, Asia Pacific Disability Reha-
bilitation Journal, pp. 15-35 
26
Proceedings of the IJCNLP-08 Workshop on NER for South and South East Asian Languages, pages 17?24,
Hyderabad, India, January 2008. c?2008 Asian Federation of Natural Language Processing
A Hybrid Approach for Named Entity Recognition in Indian Languages
Sujan Kumar Saha Sanjay Chatterji Sandipan Dandapat
Indian Institute of Technology Indian Institute of Technology Indian Institute of Technology
Kharagpur, West Bengal Kharagpur, West Bengal Kharagpur, West Bengal
India - 721302 India - 721302 India - 721302
sujan.kr.saha@gmail.com sanjay chatter@yahoo.com sandipan@cse.iitkgp.ernet.in
Sudeshna Sarkar Pabitra Mitra
Indian Institute of Technology Indian Institute of Technology
Kharagpur, West Bengal Kharagpur, West Bengal
India - 721302 India - 721302
shudeshna@gmail.com pabitra@gmail.com
Abstract
In this paper we describe a hybrid system
that applies Maximum Entropy model (Max-
Ent), language specific rules and gazetteers
to the task of Named Entity Recognition
(NER) in Indian languages designed for the
IJCNLP NERSSEAL shared task. Starting
with Named Entity (NE) annotated corpora
and a set of features we first build a base-
line NER system. Then some language spe-
cific rules are added to the system to recog-
nize some specific NE classes. Also we have
added some gazetteers and context patterns
to the system to increase the performance.
As identification of rules and context pat-
terns requires language knowledge, we were
able to prepare rules and identify context
patterns for Hindi and Bengali only. For the
other languages the system uses the MaxEnt
model only. After preparing the one-level
NER system, we have applied a set of rules
to identify the nested entities. The system
is able to recognize 12 classes of NEs with
65.13% f-value in Hindi, 65.96% f-value in
Bengali and 44.65%, 18.74%, and 35.47%
f-value in Oriya, Telugu and Urdu respec-
tively.
1 Introduction
Named entity recognition involves locating and clas-
sifying the names in text. NER is an important
task, having applications in Information Extraction
(IE), Question Answering (QA), Machine Transla-
tion (MT) and in most other NLP applications.
This paper presents a Hybrid NER system for In-
dian languages which is designed for the IJCNLP
NERSSEAL shared task competition, the goal of
which is to perform NE recognition on 12 types
of NEs - person, designation, title-person, organiza-
tion, abbreviation, brand, title-object, location, time,
number, measure and term.
In this work we have identified suitable features
for the Hindi NER task. Orthography features, suf-
fix and prefix information, morphology informa-
tion, part-of-speech information as well as informa-
tion about the surrounding words and their tags are
used to develop a MaxEnt based Hindi NER sys-
tem. Then we realized that the recognition of some
classes will be better if we apply class specific lan-
guage rules in addition to the MaxEnt model. We
have defined rules for time, measure and number
classes. We made gazetteers based identification for
designation, title-person and some terms. Also we
have used person and location gazetteers as features
of MaxEnt for better identification of these classes.
Finally we have built a module for semi-automatic
extraction of context patterns and extracted context
patterns for person, location, organization and title-
object classes and these are added to the baseline
NER system.
The shared task was defined to build the NER sys-
tems for 5 Indian languages - Hindi, Bengali, Oriya,
Telugu and Urdu for which training data was pro-
17
vided. Among these 5 languages only Bengali and
Hindi are known to us but we have no knowledge for
other 3 languages. So we are unable to build rules
and extract context patterns for these languages. The
NER systems for these 3 languages contain only
the baseline system i.e. the MaxEnt system. Also
our baseline MaxEnt NER system uses morphologi-
cal and parts-of-speech (POS) information as a fea-
ture. Due to unavailability of morphological ana-
lyzer and POS tagger for these 3 languages, these in-
formation are not added to the systems. Among the
3 languages, only for Oriya NER system we have
used small gazetteers for person, location and des-
ignation extracted from the training data. For Ben-
gali and Hindi the developed systems are complete
hybrid systems containing rules, gazetteers, context
patterns and the MaxEnt model.
The paper is organized as follows. A brief sur-
vey of different techniques used for the NER task
in different languages and domains are presented in
Section 2. Also a brief survey on nested NE recog-
nition system is presented here. A discussion on
the training data is given in Section 3. The MaxEnt
based NER system is described in Section 4. Vari-
ous features used in NER are then discussed. Next
we present the experimental results and related dis-
cussions in Section 8. Finally Section 9 concludes
the paper.
2 Previous Work
A variety of techniques has been used for NER. The
two major approaches to NER are:
1. Linguistic approaches.
2. Machine Learning (ML) based approaches.
The linguistic approaches typically use rules man-
ually written by linguists. There are several rule-
based NER systems, containing mainly lexicalized
grammar, gazetteer lists, and list of trigger words,
which are capable of providing 88%-92% f-measure
accuracy for English (Grishman, 1995; McDonald,
1996; Wakao et al, 1996).
The main disadvantages of these rule-based tech-
niques are that these require huge experience and
grammatical knowledge of the particular language
or domain and these systems are not transferable to
other languages or domains.
ML based techniques for NER make use of a
large amount of NE annotated training data to ac-
quire high level language knowledge. Several ML
techniques have been successfully used for the NER
task of which Hidden Markov Model (HMM) (Bikel
et al, 1997), Maximum Entropy (MaxEnt) (Borth-
wick, 1999), Conditional Random Field (CRF) (Li
and Mccallum, 2004) are most common. Combina-
tions of different ML approaches are also used. Sri-
hari et al (2000) combines MaxEnt, Hidden Markov
Model (HMM) and handcrafted rules to build an
NER system.
NER systems use gazetteer lists for identifying
names. Both the linguistic approach (Grishman,
1995; Wakao et al, 1996) and the ML based ap-
proach (Borthwick, 1999; Srihari et al, 2000) use
gazetteer lists.
Linguistic approach uses handcrafted rules which
needs skilled linguistics. Some recent approaches
try to learn context patterns through ML which re-
duce amount of manual labour. Talukder et al(2006)
combined grammatical and statistical techniques to
create high precision patterns specific for NE extrac-
tion. An approach to lexical pattern learning for In-
dian languages is described by Ekbal and Bandopad-
hyay (2007). They used seed data and annotated cor-
pus to find the patterns for NER.
The NER task for Hindi has been explored by
Cucerzan and Yarowsky in their language indepen-
dent NER work which used morphological and con-
textual evidences (Cucerzan and Yarowsky, 1999).
They ran their experiment with 5 languages - Roma-
nian, English, Greek, Turkish and Hindi. Among
these the accuracy for Hindi was the worst. For
Hindi the system achieved 41.70% f-value with a
very low recall of 27.84% and about 85% precision.
A more successful Hindi NER system was devel-
oped by Wei Li and Andrew Mccallum (2004) using
Conditional Random Fields (CRFs) with feature in-
duction. They were able to achieve 71.50% f-value
using a training set of size 340k words. In Hindi
the maximum accuracy is achieved by Kumar and
Bhattacharyya, (2006). Their Maximum Entropy
Markov Model (MEMM) based model gives 79.7%
f-value.
All the NER systems described above are able
to detect one-level NEs. In recent years, the inter-
est in detection of nested NEs has increased. Here
18
we mention few attempts for nested NE detection.
Zhou et al (2004) described an approach to iden-
tify cascaded NEs from biomedical texts. They de-
tected the innermost NEs first and then they derived
rules to find the other NEs containing these as sub-
strings. Another approach, described by McDonald
et al (2005), uses structural multilevel classifica-
tion to deal with overlapping and discontinuous enti-
ties. B. Gu (2006) has treated the task of identifying
the nested NEs a binary classification problem and
solved it using support vector machines. For each
token in nested NEs, they used two schemes to set
its class label: labeling as the outermost entity or the
inner entities.
3 Training Data
The data used for the training of the systems was
provided. The annotated data uses Shakti Standard
Format (SSF). For our development we have con-
verted the SSF format data into the IOB formatted
text in which a B ? XXX tag indicates the first
word of an entity type XXX and I?XXX is used
for subsequent words of an entity. The tag O indi-
cates the word is outside of a NE. The training data
for Hindi contains more than 5 lakh words, for Ben-
gali about 160K words and about 93K, 64K and 36K
words for Oriya, Telugu and Urdu respectively.
In time of development we have observed that
the training data, provided by the organizers of the
shared task, contains several types of errors in NE
tagging. These errors in the training corpora affects
badly to the machine learning (ML) based models.
But we have not made corrections of the errors in
the training corpora in time of our development. All
the results shown in the paper are obtained using the
provided corpora without any modification in NE
annotation.
4 Maximum Entropy Based Model
We have used MaxEnt model to build the baseline
NER system. MaxEnt is a flexible statistical model
which assigns an outcome for each token based on
its history and features. Given a set of features and a
training corpus, the MaxEnt estimation process pro-
duces a model. For our development we have used
a Java based open-nlp MaxEnt toolkit1 to get the
1www.maxent.sourceforge.net
probability values of a word belonging to each class.
That is, given a sequence of words, the probability
of each class is obtained for each word. To find the
most probable tag corresponding to each word of a
sequence, we can choose the tag having the highest
class conditional probability value. But this method
is not good as it might result in an inadmissible as-
signment.
Some tag sequences should never happen. To
eliminate these inadmissible sequences we have
made some restrictions. Then we used a beam
search algorithm with a beam of length 3 with these
restrictions.
4.1 Features
MaxEnt makes use of different features for identify-
ing the NEs. Orthographic features (like capitaliza-
tion, decimal, digits), affixes, left and right context
(like previous and next words), NE specific trigger
words, gazetteer features, POS and morphological
features etc. are generally used for NER. In En-
glish and some other languages, capitalization fea-
tures play an important role as NEs are generally
capitalized for these languages. Unfortunately this
feature is not applicable for the Indian languages.
Also Indian person names are more diverse, lots of
common words having other meanings are also used
as person names. Li and Mccallum (2004) used the
entire word text, character n-grams (n = 2, 3, 4),
word prefix and suffix of lengths 2, 3 and 4, and 24
Hindi gazetteer lists as atomic features in their Hindi
NER. Kumar and Bhattacharyya (2006) used word
features (suffixes, digits, special characters), context
features, dictionary features, NE list features etc. in
their MEMM based Hindi NER system. In the fol-
lowing we have discussed about the features we have
identified and used to develop the Indian language
NER systems.
Static Word Feature: The previous and next
words of a particular word are used as features. The
previous m words (wi?m...wi?1) to next n words
(wi+1...wi+n) can be considered. During our exper-
iment different combinations of previous 4 to next 4
words are used.
Context Lists: Context words are defined as the
frequent words present in a word window for a par-
ticular class. We compiled a list of the most frequent
words that occur within a window of wi?3...wi+3
19
of every NE class. For example, location con-
text list contains the words like ?jAkara2? (go-
ing to), ?desha? (country), ?rAjadhAnI? (capital)
etc. and person context list contains ?kahA? (say),
?pradhAnama.ntrI? (prime minister) etc. For a
given word, the value of this feature correspond-
ing to a given NE type is set to 1 if the window
wi?3...wi+3 around the wi contains at last one word
from this list.
Dynamic NE tag: Named Entity tags of the pre-
vious words (ti?m...ti?1) are used as features.
First Word: If the token is the first word of a
sentence, then this feature is set to 1. Otherwise, it
is set to 0.
Contains Digit: If a token ?w? contains digit(s)
then the feature ContainsDigit is set to 1.
Numerical Word: For a token ?w? if the word
is a numerical word i.e. a word denoting a number
(e.g. eka (one), do (two), tina (three) etc.) then the
feature NumWord is set to 1.
Word Suffix: Word suffix information is helpful
to identify the NEs. Two types of suffix features
have been used. Firstly a fixed length word suffix of
the current and surrounding words are used as fea-
tures. Secondly we compiled lists of common suf-
fixes of person and place names in Hindi. For ex-
ample, ?pura?, ?bAda?, ?nagara? etc. are location
suffixes. We used binary features corresponding to
the lists - whether a given word has a suffix from a
particular list.
Word Prefix: Prefix information of a word may
also be helpful in identifying whether it is a NE. A
fixed length word prefix of current and surrounding
words are treated as features.
Root Information of Word: Indian languages
are morphologically rich. Words are inflected in var-
ious forms depending on its number, tense, person,
case etc. Identification of NEs becomes difficult for
these inflections. The task becomes easier if instead
of the inflected words, corresponding root words are
checked whether these are NE or not. For that task
we have used morphological analyzers for Hindi and
Bengali which are developed at IIT kharagpur.
Parts-of-Speech (POS) Information: The POS
of the current word and the surrounding words may
2All Hindi words are written in italics using the ?Itrans?
transliteration
be useful feature for NER. We have accessed to
Hindi and Bengali POS taggers developed at IIT
Kharagpur which has accuracy about 90%. The
tagset of the tagger contains 28 tags. We have used
the POS values of the current and surrounding to-
kens as features.
We realized that the detailed POS tagging is not
very relevant. Since NEs are noun phrases, the noun
tag is very relevant. Further the postposition follow-
ing a name may give a clue to the NE type for Hindi.
So we decided to use a coarse-grained tagset with
only three tags - nominal (Nom), postposition (PSP)
and other (O).
The POS information is also used by defining sev-
eral binary features. An example is the NomPSP
binary feature. The value of this feature is defined
to be 1 if the current token is nominal and the next
token is a PSP.
5 Language Specific Rules
After building of the MaxEnt model we have ob-
served that only a small set of rules are able to iden-
tify the classes like number, measure, time, more ef-
ficiently than the MaxEnt based model. Then we
have tried to define the rules for these classes. The
rule identification is done manually and requires lan-
guage knowledge. We have defined the required
rules for Bengali and Hindi but we are unable to do
the same for other 3 languages as the languages are
unknown to us. In the following we have mentioned
some example rules which are defined and used in
our system.
? IF ((Wi is a number or numeric word) AND
(Wi+1 is an unit))
THEN (Wi Wi+1) bigram is a measure NE.
? IF ((Wi is a number or numeric word) AND
(Wi+1 is a month-name) AND (Wi+2 is a 4
digit number))
THEN (Wi Wi+1 Wi+2) trigram is a time NE.
? IF ((Wi denotes a day of a week) AND (Wi+1
is a number or numeric word) AND (Wi+2 is a
month name))
THEN (Wi Wi+1 Wi+2) trigram is a time NE.
We have defined 36 rules in total for time, mea-
sure and number classes. These rules use some lists
20
which are built. These lists contain correspond-
ing entries both in the target language and in En-
glish. For example the months names list contains
the names according to the English calender and the
names according to the Indian calender. In the fol-
lowing we have mentioned the lists we have pre-
pared for the rule-based module.
? Names of months.
? Names of seasons.
? Days of a week.
? Names of units.
? Numerical words.
5.1 Semi-automatic Extraction of Context
Patterns
Similar to the rules defined for time, measure and
date classes, if efficient context patterns (CP) can
be extracted for a particular class, these can help
in identification of NEs of the corresponding class.
But extraction of CP requires huge labour if done
manually. We have developed a module for semi-
automatically extraction of context patterns. This
module makes use of the most frequent entities of
a particular class as seed for that class and finds the
surrounding tokens of the seed to extract effective
patterns. We mark a pattern as ?effective? if the pre-
cision of the pattern is very high. Precision of a pat-
tern is defined as the ratio of correct identification
and the total identification when the pattern is used
to identify NEs of a particular type from a text.
For our task we have extracted patterns for per-
son, location, organization and title-object classes.
These patterns are able to identify the NEs of a spe-
cific classes but detection of NE boundary is not
done properly by the patterns. For boundary detec-
tion we have added some heuristics and used POS
information of the surrounding words. The patterns
for a particular class may identify the NEs of other
classes also. For example the patterns for identify-
ing person names may also identify the designation
or title-persons. These need to be handled carefully
at the time of using patterns. In the following some
example patterns are listed which are able to identify
person names for Hindi.
? <PER> ne kahA ki
? <PER> kA kathana he.n
? mukhyama.ntrI <PER> Aja
? <PER> ne apane gra.ntha
? <PER> ke putra <PER>
6 Use of Gazetteer Lists
Lists of names of various types are helpful in name
identification. Firstly we have prepared the lists us-
ing the training corpus. But these are not sufficient.
Then we have compiled some specialized name lists
from different web sources. But the names in these
lists are in English, not in Indian languages. So we
have transliterated these English name lists to make
them useful for our NER task.
Using transliteration we have constructed several
lists. Which are, month name and days of the week,
list of common locations, location names list, first
names list, middle names list, surnames list etc.
The lists can be used in name identification in var-
ious ways. One way is to check whether a token is in
any list. But this approach is not good as it has some
limitations. Some words may present in two or more
gazetteer lists. Confusions arise to make decisions
for these words. Some words are in gazetteer lists
but sometimes these are used in text as not-name en-
tity. We have used these gazetteer lists as features of
MaxEnt. We have prepared several binary features
which are defined as whether a given word is in a
particular list.
7 Detection of Nested Entities
The training corpora used for the models, are not
annotated as nested. The maximal entities are an-
notated in the training corpus. For detection of the
nested NEs, we have derived some rules. For exam-
ple, if a particular word is a number or numeric word
and is a part of a NE type other than ?number?, then
we have made the nesting. Again, if any common lo-
cation identifier word like, jilA (district), shahara
(town) etc. is a part of a ?location? entity then we
have nested there. During one-level NE identifica-
tion, we have generated lists for all the identified lo-
cation and person names. Then we have searched
other NEs containing these as substring to make the
21
nesting. After preparing the one-level NER system,
we have applied the derived rules on it to identify
the nested entities.
8 Evaluation
The accuracies of the system are measured in terms
of the f-measure, which is the weighted harmonic
mean of precision and recall. Nested, maximal and
lexical accuracies are calculated separately. The
test data for all the five languages are provided.
The size of the shared task test files are: Hindi
- 38,704 words, Bengali - 32,796 words, Oriya -
26,988 words, Telugu - 7,076 words and Urdu -
12,805 words.
We have already mentioned that after preparing
a one-level NER system, the rule-based module is
used to modify it to a nested one. A number of ex-
periments are conducted considering various combi-
nations of features to identify the best feature set for
Indian language NER task. It is very difficult and
time consuming to conduct experiments for all the
languages. During the development we have con-
ducted all the experiments on Hindi and Bengali. We
have prepared a development test data composed of
24,265 words for Hindi and 10,902 word for Ben-
gali and accuracies of the system are tested on the
development data. The details of the experiments on
Hindi data for the best feature selection is described
in the following section.
8.1 Best Feature Set Selection
The performance of the system on the Hindi data
using various features are presented in Table 1.
They are summarized below. While experimenting
with static word features, we have observed that a
window of previous two words to next two words
(Wi?2...Wi+2) gives best results. But when sev-
eral other features are combined then smaller win-
dow (Wi?1...Wi+1) performs better. Similarly we
have experimented with suffixes of different lengths
and observed that the suffixes of length ? 2 gives
the best result for the Hindi NER task. In using
POS information, we have observed that the coarse-
grained POS tagger information is more effective
than the finer-grained POS values. The most in-
teresting fact we have observed that more complex
features do not guarantee to achieve better results.
For example, a feature set combined with current
and surrounding words, previous NE tag and fixed
length suffix information, gives a f-value 64.17%.
But when prefix information are added the f-value
decreased to 63.73%. Again when the context lists
are added to the feature set containing words, previ-
ous tags, suffix information, digit information and
the NomPSP binary feature, the accuracy has de-
creased to 67.33% from 68.0%.
Feature Overall
F-value
Word, NE Tag 58.92
Word, NE Tag, Suffix (? 2) 64.17
Word, NE Tag, Suffix (? 2),
Prefix
63.73
Word, NE Tag, Digit, Suffix 66.61
Word, NE Tag, Context List 63.57
Word, NE Tag, POS (full) 61.28
Word, NE Tag, Suffix (? 2),
Digit, NomPSP
68.60
Word, NE Tag, Suffix (? 2),
Digit, Context List, NomPSP
67.33
Word, NE Tag, Suffix (?
2), Digit, NomPSP, Linguis-
tic Rules
73.40
Word, NE Tag, Suffix (? 2),
Digit, NomPSP, Gazetteers
72.08
Word, NE Tag, Suffix (?
2), Digit, NomPSP, Linguis-
tic Rules, Gazetteers
74.53
Table 1: Hindi development set f-values for different
features
The feature set containing words, previous
tags, suffix information, digit information and the
NomPSP binary feature is the identified best feature
set without linguistic rules and gazetteer informa-
tion. Then we have added the linguistic rules, pat-
terns and gazetteer information to the system and the
changes in accuracies are shown in the table.
8.2 Results on the Test Data
The best identified feature set is used for the de-
velopment of the NER systems for all the five lan-
guages. We have already mentioned that for only
for Bengali and Hindi we have added linguistic rules
22
and gazetteer lists in the MaxEnt based NER sys-
tems. The accuracy of the system on the shared task
test data for all the languages are shown in Table 2.
Lan-
guage
Type Preci-
sion
Recall F-
measure
Bengali
Maximal 52.92 68.07 59.54
Nested 55.02 68.43 60.99
Lexical 62.30 70.07 65.96
Hindi
Maximal 75.19 58.94 66.08
Nested 79.58 58.61 67.50
Lexical 82.76 53.69 65.13
Oriya
Maximal 21.17 26.92 23.70
Nested 27.73 28.13 27.93
Lexical 51.51 39.40 44.65
Telugu
Maximal 10.47 9.64 10.04
Nested 22.05 13.16 16.48
Lexical 25.23 14.91 18.74
Urdu
Maximal 26.12 29.69 27.79
Nested 27.99 29.21 28.59
Lexical 37.58 33.58 35.47
Table 2: Accuracy of the system for all languages
The accuracies of Oriya, Telugu and Urdu lan-
guages are poor compared to the other two lan-
guages. The reasons are POS information, mor-
phological information, language specific rules and
gazetteers are not used for these languages. Also the
size of training data for these languages are smaller.
To mention, for Urdu, size of the training data is only
about 36K words which is very small to train a Max-
Ent model.
It is mentioned that we have prepared a set of rules
which are capable of identifying the nested NEs.
Once the one-level NER system has built, we have
applied the rules on it. In Table 3 we have shown
the f-values of each class after addition of the nested
rules. The detailed results for all languages are not
shown. In the table we have shown only the results
of Bengali and Hindi.
For both the languages ?title-person? and ?desig-
nation? classes are suffering from poor accuracies.
The reason is, in the training data and also in the
annotated test data, these classes contains many an-
notation errors. Also the classes being closely re-
lated to each other, the system fails to distinguish
them properly. The detection of the ?term? class is
Hindi Bengali
Class Maximal Nested Maximal Nested
Person 70.87 71.00 77.45 79.09
Desig-
nation
48.98 59.81 26.32 26.32
Organi-
zation
47.22 47.22 41.43 71.43
Abbre-
viation
- 72.73 51.61 51.61
Brand - - - -
Title-
person
- 60.00 5.19 47.61
Title-
object
41.32 40.98 72.97 72.97
Location 86.02 87.02 76.27 76.27
Time 67.42 67.42 56.30 56.30
Number 84.59 85.13 40.65 40.65
Measure 59.26 55.17 62.50 62.50
Term 48.91 50.51 43.67 43.67
Table 3: Comparison of maximal and nested f-
values for different classes of Hindi and Bengali
very difficult. In the test files amount of ?term? en-
tity is large, for Bengali - 434 and for Hindi - 1080,
so the poor accuracy of the class affects badly to the
overall accuracy. We have made rule-based identi-
fication for ?number?, ?measure? and ?time? classes;
the accuracies of these classes proves that the rules
need to be modified to achieve better accuracy for
these classes. Also the accuracy of the ?organiza-
tion? class is not high, because amount of organiza-
tion entities is not sufficient in the training corpus.
We have achieved good results for other two main
classes - ?person? and ?location?.
8.3 Comparison with Other Shared Task
Systems
The comparison of the accuracies of our system
and other shared task systems is given in Table 4.
From the comparison we can see that our system
has achieved the best accuracies for most of the lan-
guages.
9 Conclusion
We have prepared a MaxEnt based system for the
NER task in Indian languages. We have also added
23
Lan-
guage
Our S2 S6 S7
Bengali 65.96 39.77 40.63 59.39
Hindi 65.13 46.84 50.06 33.12
Oriya 44.65 45.84 39.04 28.71
Telugu 18.74 46.58 40.94 4.75
Urdu 35.47 44.73 43.46 35.52
Table 4: Comparison of our lexical f-measure accu-
racies with the systems : S2 - Praveen P.(2008), S6 -
Gali et al(2008) and S7 - Ekbal et al(2008)
rules and gazetteers for Bengali and Hindi. Also our
derived rules need to be modified for improvement
of the system. We have not made use of rules and
gazetteers for Oriya, Telugu and Urdu. As the size
of training data is not much for these 3 languages,
rules and gazetteers would be effective. We have
experimented with MaxEnt model only, other ML
methods like HMM, CRF or MEMM may be able
to give better accuracy. We have not worked much
on the detection of nested NEs. Proper detection of
nested entities may lead to further improvement of
performance and is under investigation.
References
Bikel Daniel M., Miller Scott, Schwartz Richard and
Weischedel Ralph. 1997. Nymble: A High Perfor-
mance Learning Name-finder. In Proceedings of the
Fifth Conference on Applied Natural Language Pro-
cessing, 194?201.
Borthwick Andrew. 1999. A Maximum Entropy Ap-
proach to Named Entity Recognition. Ph.D. thesis,
Computer Science Department, New York University.
Cucerzan Silviu and Yarowsky David. 1999. Language
Independent Named Entity Recognition Combining
Morphological and Contextual Evidence. In Proceed-
ings of the Joint SIGDAT Conference on EMNLP and
VLC 1999, 90?99.
Ekbal A. and Bandyopadhyay S. 2007. Lexical Pattern
Learning from Corpus Data for Named Entity Recog-
nition. In Proceedings of International Conference on
Natural Language Processing (ICON), 2007.
Ekbal A., Haque R., Das A., Poka V. and Bandyopad-
hyay S. 2008. Language Independent Named Entity
Recognition in Indian Languages In Proceedings of
IJCNLP workshop on NERSSEAL. (Accepted)
Gali K., Surana H., Vaidya A., Shishtla P. and Misra
Sharma D. 2008. Named Entity Recognition through
CRF Based Machine Learning and Language Specific
Heuristics In Proceedings of IJCNLP workshop on
NERSSEAL. (Accepted)
Grishman Ralph. 1995. The New York University Sys-
tem MUC-6 or Where?s the syntax? In Proceedings of
the Sixth Message Understanding Conference.
Gu B. 2006. Recognizing Nested Named Entities in GE-
NIA corpus. In Proceedings of the BioNLP Workshop
on Linking Natural Language Processing and Biology
at HLT-NAACL 06, pages 112-113.
Kumar N. and Bhattacharyya Pushpak. 2006. Named
Entity Recognition in Hindi using MEMM. In Techni-
cal Report, IIT Bombay, India..
Li Wei and McCallum Andrew. 2004. Rapid Develop-
ment of Hindi Named Entity Recognition using Condi-
tional Random Fields and Feature Induction (Short Pa-
per). In ACM Transactions on Computational Logic.
McDonald D. 1996. Internal and external evidence in the
identification and semantic categorization of proper
names. In B. Boguraev and J. Pustejovsky, editors,
Corpus Processing for Lexical Acquisition, 21?39.
McDonald R., Crammer K. and Pereira F. 2005. Flexible
text segmentation with structured multilabel classifica-
tion. In Proceedings of EMNLP05.
Praveen P. 2008. Hybrid Named Entity Recogni-
tion System for South-South East Indian Languages.
InProceedings of IJCNLP workshop on NERSSEAL.
(Accepted)
Srihari R., Niu C. and Li W. 2000. A Hybrid Approach
for Named Entity and Sub-Type Tagging. In Proceed-
ings of the sixth conference on Applied natural lan-
guage processing.
Talukdar Pratim P., Brants T., Liberman M., and Pereira
F. 2006. A context pattern induction method
for named entity extraction. In Proceedings of the
Tenth Conference on Computational Natural Lan-
guage Learning (CoNLL-X).
Wakao T., Gaizauskas R. and Wilks Y. 1996. Evaluation
of an algorithm for the recognition and classification
of proper names. In Proceedings of COLING-96.
Zhou G., Zhang J., Su J., Shen D. and Tan C. 2004.
Recognizing Names in Biomedical Texts: a Machine
Learning Approach. Bioinformatics, 20(7):1178-
1190.
24
Proceedings of the ACL 2007 Demo and Poster Sessions, pages 221?224,
Prague, June 2007. c?2007 Association for Computational Linguistics 
Automatic Part-of-Speech Tagging for Bengali: An Approach for 
Morphologically Rich Languages in a Poor Resource Scenario 
Sandipan Dandapat, Sudeshna Sarkar, Anupam Basu 
Department of Computer Science and Engineering 
Indian Institute of Technology Kharagpur 
India 721302 
{sandipan,sudeshna,anupam.basu}@cse.iitkgp.ernet.in 
 
Abstract 
This paper describes our work on build-
ing Part-of-Speech (POS) tagger for 
Bengali. We have use Hidden Markov 
Model (HMM) and Maximum Entropy 
(ME) based stochastic taggers. Bengali is 
a morphologically rich language and our 
taggers make use of morphological and 
contextual information of the words.  
Since only a small labeled training set is 
available (45,000 words), simple stochas-
tic approach does not yield very good re-
sults. In this work, we have studied the 
effect of using a morphological analyzer 
to improve the performance of the tagger. 
We find that the use of morphology helps 
improve the accuracy of the tagger espe-
cially when less amount of tagged cor-
pora are available. 
1 Introduction 
Part-of-Speech (POS) taggers for natural lan-
guage texts have been developed using linguistic 
rules, stochastic models as well as a combination 
of both (hybrid taggers). Stochastic models (Cut-
ting et al, 1992; Dermatas et al, 1995; Brants, 
2000) have been widely used in POS tagging for 
simplicity and language independence of the 
models. Among stochastic models, bi-gram and 
tri-gram Hidden Markov Model (HMM) are 
quite popular. Development of a high accuracy 
stochastic tagger requires a large amount of an-
notated text. Stochastic taggers with more than 
95% word-level accuracy have been developed 
for English, German and other European Lan-
guages, for which large labeled data is available. 
Our aim here is to develop a stochastic POS tag-
ger for Bengali but we are limited by lack of a 
large annotated corpus for Bengali. Simple 
HMM models do not achieve high accuracy 
when the training set is small. In such cases, ad-
ditional information may be coded into the 
HMM model to achieve higher accuracy (Cutting 
et al, 1992). The semi-supervised model de-
scribed in Cutting et al (1992), makes use of 
both labeled training text and some amount of 
unlabeled text. Incorporating a diverse set of 
overlapping features in a HMM-based tagger is 
difficult and complicates the smoothing typically 
used for such taggers. In contrast, methods based 
on Maximum Entropy (Ratnaparkhi, 1996), 
Conditional Random Field (Shrivastav, 2006) 
etc. can deal with diverse, overlapping features. 
1.1 Previous Work on Indian Language 
POS Tagging 
Although some work has been done on POS tag-
ging of different Indian languages, the systems 
are still in their infancy due to resource poverty. 
Very little work has been done previously on 
POS tagging of Bengali. Bengali is the main 
language spoken in Bangladesh, the second most 
commonly spoken language in India, and the 
fourth most commonly spoken language in the 
world. Ray et al (2003) describes a morphology-
based disambiguation for Hindi POS tagging. 
System using a decision tree based learning algo-
rithm (CN2) has been developed for statistical 
Hindi POS tagging (Singh et al, 2006). A rea-
sonably good accuracy POS tagger for Hindi has 
been developed using Maximum Entropy 
Markov Model (Dalal et al, 2007). The system 
uses linguistic suffix and POS categories of a 
word along with other contextual features. 
2 Our Approach 
The problem of POS tagging can be formally 
stated as follows. Given a sequence of words w1 
? wn, we want to find the corresponding se-
quence of tags t1 ? tn, drawn from a set of tags T. 
We use a tagset of 40 tags1. In this work, we ex-
plore supervised and semi-supervised bi-gram 
                                                 
1 http://www.mla.iitkgp.ernet.in/Tag.html 
221
 HMM and a ME based model. The bi-gram as-
sumption states that the POS-tag of a word de-
pends on the current word and the POS tag of the 
previous word. An ME model estimates the prob-
abilities based on the imposed constraints. Such 
constraints are derived from the training data, 
maintaining some relationship between features 
and outcomes. The most probable tag sequence 
for a given word sequence satisfies equation (1) 
and (2) respectively for HMM and ME model: 
1
1
... 1,
( | ) ( | )arg max i i i i
t tn i n
S P w t P t t ?
=
= ?      (1) 
1 1
1,
( ... | ... ) ( | )n n i i
i n
p t t w w p t h
=
= ?       (2) 
Here, hi is the context for word wi. Since the ba-
sic bigram model of HMM as well as the equiva-
lent ME models do not yield satisfactory accu-
racy, we wish to explore whether other available 
resources like a morphological analyzer can be 
used appropriately for better accuracy.  
2.1 HMM and ME based Taggers 
Three taggers have been implemented based on 
bigram HMM and ME model. The first tagger 
(we shall call it HMM-S) makes use of the su-
pervised HMM model parameters, whereas the 
second tagger (we shall call it HMM-SS) uses 
the semi supervised model parameters. The third 
tagger uses ME based model to find the most 
probable tag sequence for a given sequence of 
words.  
 
In order to further improve the tagging accuracy, 
we use a Morphological Analyzer (MA) and in-
tegrate morphological information with the mod-
els. We assume that the POS-tag of a word w can 
take values from the set TMA(w), where TMA(w) is 
computed by the Morphological Analyzer. Note 
that the size of TMA(w) is much smaller than T. 
Thus, we have a restricted choice of tags as well 
as tag sequences for a given sentence. Since the 
correct tag t for w is always in TMA(w) (assuming 
that the morphological analyzer is complete), it is 
always possible to find out the correct tag se-
quence for a sentence even after applying the 
morphological restriction. Due to a much re-
duced set of possibilities, this model is expected 
to perform better for both the HMM (HMM-S 
and HMM-SS) and ME models even when only a 
small amount of labeled training text is available. 
We shall call these new models HMM-S+MA, 
HMM-SS+ MA and ME+MA.  
 
Our MA has high accuracy and coverage but it 
still has some missing words and a few errors. 
For the purpose of these experiments we have 
made sure that all words of the test set are pre-
sent in the root dictionary that an MA uses. 
 
While MA helps us to restrict the possible choice 
of tags for a given word, one can also use suffix 
information (i.e., the sequence of last few charac-
ters of a word) to further improve the models. 
For HMM models, suffix information has been 
used during smoothing of emission probabilities, 
whereas for ME models, suffix information is 
used as another type of feature. We shall denote 
the models with suffix information with a ?+suf? 
marker. Thus, we have ? HMM-S+suf, HMM-
S+suf+MA, HMM-SS+suf etc. 
2.1.1 Unknown Word Hypothesis in HMM 
The transition probabilities are estimated by lin-
ear interpolation of unigrams and bigrams. For 
the estimation of emission probabilities add-one 
smoothing or suffix information is used for the 
unknown words. If the word is unknown to the 
morphological analyzer, we assume that the 
POS-tag of that word belongs to any of the open 
class grammatical categories (all classes of 
Noun, Verb, Adjective, Adverb and Interjection). 
2.1.2 Features of the ME Model 
Experiments were carried out to find out the 
most suitable binary valued features for the POS 
tagging in the ME model. The main features for 
the POS tagging task have been identified based 
on the different possible combination of the 
available word and tag context. The features also 
include prefix and suffix up to length four. We 
considered different combinations from the fol-
lowing set for obtaining the best feature set for 
the POS tagging task with the data we have. 
 { }11 2 2 1 2, , , , , , , 4, 4ii i i i i iF w w w w w t t pre suf+? ? + ? ?= ? ?  
 
Forty different experiments were conducted tak-
ing several combinations from set ?F? to identify 
the best suited feature set for the POS tagging 
task. From our empirical analysis we found that 
the combination of contextual features (current 
word and previous tag), prefixes and suffixes of 
length ? 4 gives the best performance for the ME 
model. It is interesting to note that the inclusion 
of prefix and suffix for all words gives better 
result instead of using only for rare words as is 
described in Ratnaparkhi (1996). This can be 
explained by the fact that due to small amount of 
annotated data, a significant number of instances 
222
 are not found for most of the word of the 
language vocabulary.   
3 Experiments 
We have a total of 12 models as described in 
subsection 2.1 under different stochastic tagging 
schemes. The same training text has been used to 
estimate the parameters for all the models. The 
model parameters for supervised HMM and ME 
models are estimated from the annotated text 
corpus. For semi-supervised learning, the HMM 
learned through supervised training is considered 
as the initial model. Further, a larger unlabelled 
training data has been used to re-estimate the 
model parameters of the semi-supervised HMM. 
The experiments were conducted with three dif-
ferent sizes (10K, 20K and 40K words) of the 
training data to understand the relative perform-
ance of the models as we keep on increasing the 
size of the annotated data.   
3.1 Training Data 
The training data includes manually annotated 
3625 sentences (approximately 40,000 words) 
for both supervised HMM and ME model. A 
fixed set of 11,000 unlabeled sentences (ap-
proximately 100,000 words) taken from CIIL 
corpus 2  are used to re-estimate the model pa-
rameter during semi-supervised learning. It has 
been observed that the corpus ambiguity (mean 
number of possible tags for each word) in the 
training text is 1.77 which is much larger com-
pared to the European languages (Dermatas et 
al., 1995).  
3.2 Test Data 
All the models have been tested on a set of ran-
domly drawn 400 sentences (5000 words) dis-
joint from the training corpus. It has been noted 
that 14% words in the open testing text are un-
known with respect to the training set, which is 
also a little higher compared to the European 
languages (Dermatas et al, 1995) 
3.3 Results 
We define the tagging accuracy as the ratio of 
the correctly tagged words to the total number of 
words. Table 1 summarizes the final accuracies 
achieved by different learning methods with the 
varying size of the training data. Note that the 
baseline model (i.e., the tag probabilities depends 
                                                 
2 A part of the EMILE/CIIL corpus developed at Cen-
tral Institute of Indian Languages (CIIL), Mysore. 
only on the current word) has an accuracy of 
76.8%.  
 
Accuracy Method 
10K 20K 40K 
HMM-S 57.53 70.61 77.29 
HMM-S+suf 75.12 79.76 83.85 
HMM-S+MA 82.39 84.06 86.64 
HMM-S+suf+MA 84.73 87.35 88.75 
HMM-SS 63.40 70.67 77.16 
HMM-SS+suf 75.08 79.31 83.76 
HMM-SS+MA 83.04 84.47 86.41 
HMM-SS+suf+MA 84.41 87.16 87.95 
ME 74.37 79.50 84.56 
ME+suf 77.38 82.63 86.78 
ME+MA 82.34 84.97 87.38 
ME+suf+MA 84.13 87.07 88.41 
Table 1: Tagging accuracies (in %) of different 
models with 10K, 20K and 40K training data. 
3.4 Observations 
We find that in both the HMM based models 
(HMM-S and HMM-SS), the use of suffix in-
formation as well as the use of a morphological 
analyzer improves the accuracy of POS tagging 
with respect to the base models. The use of MA 
gives better results than the use of suffix infor-
mation. When we use both suffix information as 
well as MA, the results is even better. 
 
HMM-SS does better than HMM-S when very 
little tagged data is available, for example, when 
we use 10K training corpus. However, the accu-
racy of the semi-supervised HMM models are 
slightly poorer than that of the supervised HMM 
models for moderate size training data and use of 
suffix information. This discrepancy arises due 
to the over-fitting of the supervised models in the 
case of small training data; the problem is allevi-
ated with the increase in the annotated data. 
 
As we have noted already the use of MA and/or 
suffix information improves the accuracy of the 
POS tagger. But what is significant to note is that 
the percentage of improvement is higher when 
the amount of training data is less. The HMM-
S+suf model gives an improvement of around 
18%, 9% and 6% over the HMM-S model for 
10K, 20K and 40K training data respectively. 
Similar trends are observed in the case of the 
semi-supervised HMM and the ME models. The 
use of morphological restriction (HMM-S+MA) 
gives an improvement of 25%, 14% and 9% re-
spectively over the HMM-S in case of 10K, 20K 
223
 and 40K training data. As the improvement due 
to MA decreases with increasing data, it might 
be concluded that the use of morphological re-
striction may not improve the accuracy when a 
large amount of training data is available. From 
our empirical observations we found that both 
suffix and morphological restriction (HMM-
S+suf+MA) gives an improvement of 27%, 17% 
and 12% over the HMM-S model respectively 
for the three different sizes of training data. 
 
The Maximum Entropy model does better than 
the HMM models for smaller training data. But 
with higher amount of training data the perform-
ance of the HMM and ME model are compara-
ble. Here also we observe that suffix information 
and MA have positive effect, and the effect is 
higher with poor resources.  
 
Furthermore, in order to estimate the relative per-
formance of the models, experiments were car-
ried out with two existing taggers: TnT (Brants, 
2000) and ACOPOST3. The accuracy achieved 
using TnT are 87.44% and 87.36% respectively 
with bigram and trigram model for 40K training 
data. The accuracy with ACOPOST is 86.3%.  
This reflects that the higher order Markov mod-
els do not work well under the current experi-
mental setup.  
3.5 Assessment of Error Types 
Table 2 shows the top five confusion classes for 
HMM-S+MA model. The most common types of 
errors are the confusion between proper noun 
and common noun and the confusion between 
adjective and common noun. This results from 
the fact that most of the proper nouns can be 
used as common nouns and most of the adjec-
tives can be used as common nouns in Bengali.  
 
Actual 
Class 
(frequency) 
Predicted 
Class 
% of total 
errors 
% of 
class 
errors 
NP(251) NN 21.03 43.82 
JJ(311) NN 5.16 8.68 
NN(1483) JJ 4.78 1.68 
DTA(100) PP 2.87 15.0 
NN(1483) VN 2.29 0.81 
Table 2: Five most common types of errors  
Almost all the confusions are wrong assignment 
due to less number of instances in the training 
corpora, including errors due to long distance 
phenomena. 
                                                 
3 http://maxent.sourceforge.net 
4 Conclusion 
In this paper we have described an approach for 
automatic stochastic tagging of natural language 
text for Bengali. The models described here are 
very simple and efficient for automatic tagging 
even when the amount of available annotated 
text is small. The models have a much higher 
accuracy than the na?ve baseline model. How-
ever, the performance of the current system is 
not as good as that of the contemporary POS-
taggers available for English and other European 
languages. The best performance is achieved for 
the supervised learning model along with suffix 
information and morphological restriction on the 
possible grammatical categories of a word. In 
fact, the use of MA in any of the models dis-
cussed above enhances the performance of the 
POS tagger significantly. We conclude that the 
use of morphological features is especially help-
ful to develop a reasonable POS tagger when 
tagged resources are limited.  
References 
A. Dalal, K. Nagaraj, U. Swant, S. Shelke and P. 
Bhattacharyya. 2007. Building Feature Rich POS 
Tagger for Morphologically Rich Languages: Ex-
perience in Hindi. ICON, 2007. 
A. Ratnaparkhi, 1996. A maximum entropy part-of-
speech tagger. EMNLP 1996. pp. 133-142. 
D. Cutting, J. Kupiec, J. Pederson and P. Sibun. 1992. 
A practical part-of-speech tagger. In Proc. of the 
3rd Conference on Applied NLP, pp. 133-140.  
E. Dermatas and K. George. 1995. Automatic stochas-
tic tagging of natural language texts. Computa-
tional Linguistics, 21(2): 137-163. 
M. Shrivastav, R. Melz, S. Singh, K. Gupta and 
P. Bhattacharyya, 2006. Conditional Random 
Field Based POS Tagger for Hindi. In Pro-
ceedings of the MSPIL, pp. 63-68. 
P. R. Ray, V. Harish, A. Basu and S. Sarkar, 2003. 
Part of Speech Tagging and Local Word Grouping 
Techniques for Natural Language Processing.  
ICON 2003. 
S. Singh, K. Gupta, M. Shrivastav and P. Bhat-
tacharyya, 2006. Morphological Richness Offset 
Resource Demand ? Experience in constructing a 
POS Tagger for Hindi. COLING/ACL 2006, pp. 
779-786. 
T. Brants. 2000. TnT ? A statistical part-of-sppech 
tagger. In Proc. of the 6th Applied NLP Conference, 
pp. 224-231.  
224
Bengali and Hindi to English CLIR Evaluation  
 
Debasis Mandal, Sandipan Dandapat, Mayank Gupta, Pratyush Banerjee, 
Sudeshna Sarkar 
 
Abstract 
 
Our participation in CLEF 2007 consisted of two Cross-lingual and one 
monolingual text retrieval in the Ad-hoc bilingual track. The cross-language 
task includes the retrieval of English documents in response to queries in 
two Indian languages, Hindi and Bengali. The Hindi and Bengali queries 
were first processed using a morphological analyzer (Bengali), a stemmer 
(Hindi) and a set of 200 Hindi and 273 Bengali stop words. The refined 
hindi queries were then looked into the Hindi-English bilingual lexicon, 
?Shabdanjali? (approx. 26K Hindi words) and all of the corresponding 
translations were considered for the equivalent English query generation, if a 
match was found. Rest of the query words were transliterated using the 
ITRANS scheme. For the Bengali query, we had to depend mostly on the 
translietrations due to the lack of any effective Bengali-English bilingual 
lexicon. The final equivalent English query was then fed into the Lucene 
Search engine for the monolingual retrieval of the English documents. The 
CLEF evaluations suggested the need for a rich bilingual lexicon, a good 
Named Entity Recognizer and a better transliterator for CLIR involving 
Indian languages. The best MAP values for Bengali and Hindi CLIR for our 
experiment were 7.26 and 4.77 which are 0.20 and 0.13 of our monolingual 
retrieval, respectively. 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
Proceedings of the NAACL HLT 2013 Demonstration Session, pages 10?13,
Atlanta, Georgia, 10-12 June 2013. c?2013 Association for Computational Linguistics
TMTprime: A Recommender System for MT and TM Integration
Aswarth Dara?, Sandipan Dandapat??, Declan Groves? and Josef van Genabith?
? Centre for Next Generation Localisation, School of Computing
Dublin City University, Dublin, Ireland
? Department of Computer Science and Engineering
IIT-Guwahati, Assam, India
{adara, dgroves, josef}@computing.dcu.ie, sdandapat@iitg.ernet.in
Abstract
TMTprime is a recommender system that fa-
cilitates the effective use of both transla-
tion memory (TM) and machine translation
(MT) technology within industrial language
service providers (LSPs) localization work-
flows. LSPs have long used Translation Mem-
ory (TM) technology to assist the translation
process. Recent research shows how MT sys-
tems can be combined with TMs in Computer
Aided Translation (CAT) systems, selecting
either TM or MT output based on sophis-
ticated translation quality estimation without
access to a reference. However, to date there
are no commercially available frameworks for
this. TMTprime takes confidence estimation
out of the lab and provides a commercially vi-
able platform that allows for the seamless inte-
gration of MT with legacy TM systems to pro-
vide the most effective (least effort/cost) trans-
lation options to human translators, based on
the TMTprime confidence score.
1 Introduction
Within the LSP community there is growing interest
in the use of MT as a means to increase automation
and reduce overall localisation project cost. When
high-quality MT output is available, translators see
significant productivity gains over translation from
scratch, but poor MT quality leads to frustration
and wasted time as suggested translations are dis-
carded in favour of providing a translation from
scratch. We present a commercially-relevant soft-
ware platform providing a translation confidence es-
timation metric and, based on this, a mechanism for
effectively integrating MT with TMs in localisation
workflows. The confidence metric ensures that only
?Author did this work during his post doctoral research at
CNGL.
those MT outputs that are guaranteed to require less
post-editing effort than the best corresponding TM
match are presented to the post-editor (He et al,
2010a). The MT is integrated seamlessly, and es-
tablished localisation cost estimation models based
on TM technologies still apply as upper bounds.
2 Related Work
MT confidence estimation and its relation to existing
TM scoring methods, together with how to make the
most effective use of both technologies, is an active
area of research.
(Specia, 2011) and (Specia et al, 2009, 2010) pro-
pose a confidence estimator that relates specifically
to the post-editing effort of translators. This re-
search uses regression on both the automatic scores
assigned to the MT and scores assigned by post-
editors and aims to model post-editors? judgements
of the translation quality between good and bad, or
among three levels of post-editing effort.
Our work is an extension of (He et al, 2010a,b,c),
and uses outputs and features relevant to the TM
and MT systems. We focus on using system exter-
nal features. This is important for cases where the
internals of the MT system are not available, as in
the use of MT as a service in a localisation work-
flow.1 Furthermore, instead of having to solve a
regression problem, our approach is based on solv-
ing an easier binary prediction problem (using Sup-
port Vector Machines) and can be easily integrated
into TMs. (He et al, 2010b) present a MT/TM seg-
ment recommender, (He et al, 2010c) a MT/TM n-
best list segment re-ranker and (He et al, 2010a) a
MT/TM integration method that can use matching
sub-segments in MT/TM combination. Importantly,
1(Specia et al, 2009) note that using glass-box features
when available, in addition to black-box features, offer only
small gains and also incur significant computational effort.
10
translators can tune the models for precision without
retraining the models.
Related research by (Simard and Isabelle., 2009)
focuses on combining TM information into an SMT
system for improving the performance of the MT
when a close match already exists within the TM.
(Koehn and Haddow, 2009) presents a post-editing
environment using information from the phrase-
based SMT system Moses.2 (Guerberof, 2009) com-
pares the post-editing effort required for TM and
MT output, respectively. (Tatsumi, 2009) studies the
correlation between automatic evaluation scores and
post-editing effort.
3 Translation Recommender
Figure 1: TMTprime Workflow
The workflow of the translation recommender is
shown in Figure 1. We train MT systems using a
significant portion of the training data and use these
models as well as TM outputs to obtain a recommen-
dation development data set. MT systems can be
either in-house, e.g. a Moses-based system, or ex-
ternally available systems, such as Microsoft Bing3
or Google Translate.4 For each sentence in the de-
velopment data set, we have access to the reference
as well as to the outputs for each of the MT and TM
systems. We then select the best MT (or TM) output
as the translation with the lowest TER score with
respect to the reference and label the data accord-
ingly. System-independent features for each trans-
lation output are fed as input to the SVM classi-
fier (Cortes and Vapnik, 1995). The SVM classi-
fier outputs class labels and the class labels are con-
verted into confidence scores using the techniques
given in (Lin et al, 2007). Relying on system inde-
pendent black-box features has allowed us to build
2http://www.statmt.org/moses/
3http://www.bing.com/translator
4http://translate.google.com/
a fully extendable platform that will allow any num-
ber of MT systems (or indeed TM systems) to be
plugged into the recommender with little effort.
4 Demo Description
Using the Amazon EC25 deployment as a back-end,
we have developed a front-end GUI for the system
(Figure 2). The interface allows the user to select
which of the available translation systems (whether
they be MT or TM) they wish to use within the rec-
ommender system. The user can input their own
pre-established estimated cost of post-editing, based
on error ranges. Typically the costs for post-editing
those translations which have a lower-error rate (i.e.
fewer errors) is less than the cost for post-editing
translations which have a greater number of errors,
as they are of lower quality. The user is requested to
upload a file for translation to the system.
Figure 2: TMTprime GUI
Once the user has selected their desired options,
the TMTprime platform provides various analysis
measures based on its recommendation engine, such
as how many segments from the input file are recom-
mended for translation by the various selected trans-
lation engines or TMs available. Based on the input
costs, it provides a visualisation of overall estimated
cost of either using an individual translation system
on its own, or using the recommender selecting the
best performing system on a segment-by-segment
basis. The TMTprime system is an implementa-
tion of a segment-based system selector selecting
the most appropriate available translation/TM sys-
tem for a given input. A snapshot of the results pro-
duced by TMTprime is given in Figure 3: the pie-
chart shows what percentage of segments are rec-
ommended from each of the translation systems; the
5http://aws.amazon.com/ec2/
11
bar-graph gives an estimated cost of using a single
translation system alone and the estimated cost when
using TMTprime?s combined recommendation. The
estimated cost using TMTprime is lower when com-
pared to using a single MT or TM system alone
(in the worst case, it will be the same as the best-
performing single translation engine or TM system).
This estimated cost includes both the cost for trans-
lation (currently uniform cost for each translation
system) and the cost required for post-editing. For
example, if the MT is an in-house system the cost
of translation will be (close to) zero whereas there is
potentially an additional base cost for using an exter-
nal MT engine. Finally, the interface provides statis-
tics related to various confidence levels for different
translation outputs across the various translation and
TM systems.
Figure 3: Results shown by TMTprime system
5 Experiments and Results
Evaluation targets two objectives and is described
below.
5.1 Correlation with Automatic Metrics
TER and METEOR are widely-used automatic met-
rics (Snover et al, 2006; Denkowski and Lavie,
2011) that calculate the quality of translation out-
put by comparing it against a human translation,
known as the reference translation. Our data sets
for the experiment consist of English-French trans-
lation memories from the IT domain. In all instances
MT was carried out for English-French translations.
As we have access to the reference target language
translations for our test set, we are able to calculate
the TER and METEOR scores for the three trans-
lation outputs (here TM, MaTrEx (Dandapat et al,
2010) and Microsoft Bing). For each sentence in the
test set, TMTprime recommends a particular transla-
tion output with a certain estimated confidence level
without access to a reference. We measure Pearson?s
correlation coefficient (Hollander and Wolfe, 1999)
between the recommendation scores, TER scores
and METEOR scores (for all system outputs) in or-
der to determine how well the TMTprime prediction
score correlates with the widely used automatic eval-
uation metrics. Results of these experiments are pro-
vided in Table 1 which shows there is a negative cor-
relation between TMTprime scores and TER scores.
This shows that both TMTprime scores and TER
scores are moving in opposite directions, supporting
the claim that the higher the recommendation scores,
the lower the TER scores. As TER is an error score,
the lower the TER score, the higher the quality of
the machine translation output compared to its refer-
ence. On the other hand, TMTprime scores are pos-
itively correlated with METEOR scores which sup-
ports the claim that the higher the recommendation
scores, the higher the METEOR scores.
Pearson?s r TER METEOR
TMTprime -0.402 0.447
Table 1: Correlation with automatic metrics
The evaluation has been performed on a test data
set of 2,500 sentences. Both the correlations are sig-
nificant at the (p<0.01) level.
5.2 Correlation with Post-Editing time
This is the most important and crucial metric for the
evaluation. For this experiment we made use of post-
editing data captured during a real-world translation
task, for English-French in the IT domain.
Pearson?s r TER METEOR PE Time
TMTprime -0.122 0.129 -0.132
Table 2: Correlation with Post-Editing times
For testing, we collect the post-editing times for
MT outputs from two different translators using a
commercial computer-aided translation (CAT tool)
in a real-world production scenario. The data set
consists of 1113 samples and is different from the
one used in the correlation with automatic metrics.
12
Post-editing times provide a real measure of the
amount of post-editing effort required to perfect the
output of the MT system. For this experiment, we
took the output of the MT system used in the task to-
gether with the post-editing times and measured the
Pearsons correlation coefficient between the TMT-
prime recommendation scores and the post-editing
(PE) times (only for MT output from a single sys-
tem since this data set does contain PE times for
other translation outputs). In addition, we also re-
peated the previous experiment setup for finding the
correlation between the TMTprime scores and the
automatically-produced TER, METEOR scores for
this data set. The results are given in Table 2.
The results show that the confidence scores do
correlate with automatic evaluation metrics and
post-editing times. Although the correlations do not
seem as strong as before, the results are statistically
significant (p<0.01).
6 Conclusions and Future Work
We present a commercially viable translation recom-
mender system which selects the best output from
multiple TM/MT outputs. We have shown that our
confidence score correlates with automatic metrics
and post-editing times. For future work, we are
looking into extending and evaluating the system for
different language pairs and data sets.
Acknowledgments
This work is supported by Science Foundation Ire-
land (Grants SFI11-TIDA-B2040 and 07/CE/I1142) as
part of the Centre for Next Generation Localisation
(www.cngl.ie) at Dublin City University. We would also
like to thank Symantec, Autodesk and Welocalize for
their support and provision of data sets used in our ex-
periments.
References
Cortes, Corinna and Vladimir Vapnik. 1995. Support-vector
networks. In Machine Learning. pages 273?297.
Dandapat, Sandipan, Mikel L. Forcada, Declan Groves, Ser-
gio Penkale, John Tinsley, and Andy Way. 2010. OpenMa-
TrEx: A free/open-source marker-driven example-based ma-
chine translation system. In Proceedings of the 7th interna-
tional conference on Advances in natural language process-
ing. Springer-Verlag, Berlin, Heidelberg, IceTAL?10, pages
121?126.
Denkowski, Michael and Alon Lavie. 2011. Meteor 1.3: Auto-
matic metric for reliable optimization and evaluation of ma-
chine translation systems. In Proceedings of the EMNLP
2011 Workshop on Statistical Machine Translation. Edin-
burgh, UK.
Guerberof, Ana. 2009. Productivity and quality in mt post-
editing. In Proceedings of Machine Translation Summit XII
- Workshop: Beyond Translation Memories: New Tools for
Translators. Ottawa, Canada.
He, Yifan, Yanjun Ma, J Roturier, Andy Way, and Josef van
Genabith. 2010a. Improving the post-editing experience us-
ing translation recommendation: A user study. In Proceed-
ings of the Ninth Conference of the Association for Ma-
chine Translation in the Americas. Denver, Colorado, AMTA
2010, pages 247?256.
He, Yifan, Yanjun Ma, Josef van Genabith, and Andy Way.
2010b. Bridging smt and tm with translation recommenda-
tion. In Proceedings of the 48th Annual Meeting of the Asso-
ciation for Computational Linguistics. Association for Com-
putational Linguistics, Uppsala, Sweden, ACL 2010, pages
622?630.
He, Yifan, Yanjun Ma, Andy Way, and Josef van Genabith.
2010c. Integrating n-best smt outputs into a tm system. In
Proceedings of the 23rd International Conference on Com-
putational Linguistics: Posters. Association for Computa-
tional Linguistics, Beijing, China, COLING 2010, pages
374?382.
Hollander, Myles and Douglas A. Wolfe. 1999. Nonparametric
Statistical Methods. John Wiley and Sons.
Koehn, Philip and Barry Haddow. 2009. Interactive assis-
tance to human translators using statistical machine trans-
lation methods. In Proceedings of MT Summit XII. Ottawa,
Canada, pages 73?80.
Lin, Hsuan-Tien, Chih-Jen Lin, and Ruby C. Weng. 2007. A
note on platt?s probabilistic outputs for support vector ma-
chines. Machine Learning 68(3):267?276.
Simard, Michael and Pierre Isabelle. 2009. Phrase-based ma-
chine translation in a computer-assisted translation environ-
ment. In Proceedings of Machine Translation Summit XII.
Ottawa, Canada, pages 120?127.
Snover, Matthew, Bonnie Dorr, Richard Schwartz, Linnea Mic-
ciulla, and John Makhoul. 2006. A study of translation edit
rate with targeted human annotation. In Proceedings of Asso-
ciation for Machine Translation in the Americas. Cambridge,
MA, pages 223?231.
Specia, Lucia. 2011. Exploiting objective annotations for mea-
suring translation post-editing effort. In Proceedings of the
15th Annual Conference of the European Association for
Machine Translation. Leuven, Belgium, EAMT 2011, pages
73?80.
Specia, Lucia, Nicola Cancedda, and Marc Dymetman. 2010. A
dataset for assessing machine translation evaluation metrics.
In Proceedings of LREC 2010. Valletta, Malta.
Specia, Lucia, Marco Turqui, Zhuoran Wang, John Shawe-
Taylor, and Craig Saunders. 2009. Improving the confidence
of machine translation quality estimates. In Proceedings
of Machine Translation Summit XII. Ottawa, Canada, pages
136?143.
Tatsumi, Midori. 2009. Correlation between automatic evalua-
tion scores, post-editing speed and some other factors. In
Proceedings of Machine Translation Summit XII. Ottawa,
Canada, pages 332?339.
13
Proceedings of the Joint 5th Workshop on Statistical Machine Translation and MetricsMATR, pages 143?148,
Uppsala, Sweden, 15-16 July 2010. c?2010 Association for Computational Linguistics
MATREX: The DCU MT System for WMT 2010
Sergio Penkale, Rejwanul Haque, Sandipan Dandapat, Pratyush Banerjee, Ankit K. Srivastava,
Jinhua Du, Pavel Pecina, Sudip Kumar Naskar, Mikel L. Forcada, Andy Way
CNGL, School of Computing
Dublin City University, Dublin 9, Ireland
{ spenkale, rhaque, sdandapat, pbanerjee, asrivastava, jdu, ppecina, snaskar, mforcada, away }@computing.dcu.ie
Abstract
This paper describes the DCU machine
translation system in the evaluation cam-
paign of the Joint Fifth Workshop on Sta-
tistical Machine Translation and Metrics
in ACL-2010. We describe the modular
design of our multi-engine machine trans-
lation (MT) system with particular focus
on the components used in this partici-
pation. We participated in the English?
Spanish and English?Czech translation
tasks, in which we employed our multi-
engine architecture to translate. We also
participated in the system combination
task which was carried out by the MBR
decoder and confusion network decoder.
1 Introduction
In this paper, we present the DCU multi-engine
MT system MATREX (Machine Translation using
Examples). This system exploits example-based
MT, statistical MT (SMT), and system combina-
tion techniques.
We participated in the English?Spanish (en?
es) and English?Czech (en?cs) translation
tasks. For these two tasks, we employ several
individual MT systems: 1) Baseline: phrase-
based SMT (Koehn et al, 2007); 2) EBMT:
Monolingually chunking both source and target
sides of the dataset using a marker-based chunker
(Gough and Way, 2004); 3) Factored translation
model (Koehn and Hoang, 2007); 4) Source-side
context-informed (SSCI) systems (Stroppa et al,
2007); 5) the moses-chart (a Moses imple-
mentation of the hierarchical phrase-based (HPB)
approach of Chiang (2007)) and 6) Apertium (For-
cada et al, 2009) rule-based machine translation
(RBMT). Finally, we use a word-level combina-
tion framework (Rosti et al, 2007) to combine the
multiple translation hypotheses and employ a new
rescoring model to generate the final translation.
For the system combination task, we first use
the minimum Bayes-risk (MBR) (Kumar and
Byrne, 2004) decoder to select the best hypoth-
esis as the alignment reference for the confusion
network (CN) (Mangu et al, 2000). We then build
the CN using the TER metric (Snover et al, 2006),
and finally search for the best translation.
The remainder of this paper is organised as fol-
lows: Section 2 details the various components of
our system, in particular the multi-engine strate-
gies used for the shared task. In Section 3, we
outline the complete system setup for the shared
task and provide evaluation results on the test set.
Section 4 concludes the paper.
2 The MATREX System
2.1 System Architecture
The MATREX system is a combination-based
multi-engine architecture, which exploits as-
pects of both the EBMT and SMT paradigms.
The architecture includes various individual sys-
tems: phrase-based, example-based, hierarchical
phrase-based and tree-based MT.
The combination structure uses the MBR and
CN decoders, and is based on a word-level com-
bination strategy (Du et al, 2009). In the final
stage, we use a new rescoring module to process
the N -best list generated by the combination mod-
ule. Figure 1 illustrates the architecture.
2.2 Example-Based Machine Translation
The EBMT system uses a language-specific, re-
duced set of closed-class marker morphemes or
lexemes (Gough and Way, 2004) to define a way
to segment sentences into chunks, which are then
aligned using an edit-distance-style algorithm, in
which edit costs depend on word-to-word transla-143
Figure 1: System Framework.
tion probabilities and the amount of word-to-word
cognates (Stroppa and Way, 2006).
Once these phrase pairs were obtained they
were merged with the phrase pairs extracted by
the baseline system adding word alignment infor-
mation.
2.3 Apertium RBMT
Apertium1 is a free/open-source platform for
RBMT. The current version of the en?es system
in Apertium was used for the system combination
task (section 2.7), and its morphological analysers
and part-of-speech taggers were used to build a
factored Moses model.
2.4 Factored Translation Model
We also used a factored model for the en?es
translation task. Factored models (Koehn and
Hoang, 2007) facilitate the translation by break-
ing it down into several factors which are further
combined using a log-linear model (Och and Ney,
2002).
We used three factors in our factored translation
model, which are used in two different decoding
paths: a surface form (SF) to SF translation factor,
a lemma to lemma translation factor, and a part-of-
speech (PoS) to PoS translation factor.
Finally, we used two decoding paths based on
1http://www.apertium.org
the above three translation factors: an SF to SF
decoding path and a path which maps lemma to
lemma, PoS to PoS, and an SF generated using
the TL lemma and PoS. The lemmas and PoS for
en and es were obtained using Apertium (sec-
tion 2.3).
2.5 Source-Side Context-informed PB-SMT
One natural way to express a context-informed
feature (h?MBL) is to view it as the conditional
probability of the target phrases (e?k) given the
source phrase (f?k) and its source-side context in-
formation (CI):
h?MBL = logP (e?k|f?k,CI(f?k)) (1)
We use a memory-based machine learning
(MBL) classifier (TRIBL:2 Daelemans and
van den Bosch (2005)) that is able to estimate
P (e?k|f?k,CI(f?k)) by similarity-based reasoning
over memorized nearest-neighbour examples of
source?target phrase translations. In equation (1),
SSCI may include any feature (lexical, syntactic,
etc.), which can provide useful information to
disambiguate a given source phrase. In addition
to using local words and PoS-tags as features,
as in (Stroppa et al, 2007), we incorporate
grammatical dependency relations (Haque et al,
2009a) and supertags (Haque et al, 2009b) as
syntactic source context features in the log-linear
PB-SMT model.
In addition to the above feature, we derived a
simple binary feature h?best, defined in (2):
h?best =
{
1 if e?k maximizes P (e?k|f?k,CI(f?k))
0 otherwise
(2)
We performed experiments by integrating these
two features, h?MBL and h?best, directly into the
log-linear framework of Moses.
2.6 Hierarchical PB-SMT model
For the en?cs translation task, we built
a weighted synchronous context-free grammar
model (Chiang, 2007) of translation that uses
the bilingual phrase pairs of PB-SMT as a start-
ing point to learn hierarchical rules. We used
the open-source Tree-Based translation system
moses-chart3 to perform this experiment.
2An implementation of TRIBL is freely available as part
of the TiMBL software package, which can be downloaded
from http://ilk.uvt.nl/timbl
3http://www.statmt.org/moses/?n=Moses.SyntaxTutorial144
2.7 System Combination
For multiple system combination, we used an
MBR-CN framework (Du et al, 2009, 2010) as
shown in Figure 1. Due to the varying word or-
der in the MT hypotheses, it is essential to define
the backbone which determines the general word
order of the CN. Instead of using a single system
output as the skeleton, we employ an MBR de-
coder to select the best single system output Er
from the merged N -best list by minimizing the
BLEU (Papineni et al, 2002) loss, as in (3):
r = argmin
i
Ns?
j=1
(1? BLEU(Ej , Ei)) (3)
where Ns indicates the number of translations in
the merged N -best list, and {Ei}Nsi=1 are the trans-
lations themselves. In our task, we only merge the
1-best output of each individual system.
The CN is built by aligning other hypotheses
against the backbone, based on the TER metric.
Null words are allowed in the alignment. Ei-
ther votes or different confidence measures are as-
signed to each word in the network. Each arc in
the CN represents an alternative word at that po-
sition in the sentence and the number of votes for
each word is counted when constructing the net-
work. The features we used are as follows:
? word posterior probability (Fiscus, 1997);
? 3, 4-gram target language model;
? word length penalty;
? Null word length penalty;
We use MERT (Och, 2003) to tune the weights
of the CN.
2.8 Rescoring
Rescoring is a very important part in post-
processing which can select a better hypothesis
from the N -best list. We augmented our previ-
ous rescoring model (Du et al, 2009) with more
large-scale data. The features we used include:
? Direct and inverse IBM model;
? 3, 4-gram target language model;
? 3, 4, 5-gram PoS language model (Schmid,
1994; Ratnaparkhi, 1996);
? Sentence length posterior probability (Zens
and Ney, 2006);
? N -gram posterior probabilities within the N -
Best list (Zens and Ney, 2006);
? Minimum Bayes Risk probability;
? Length ratio between source and target sen-
tence;
The weights are optimized via MERT.
3 Experimental Setup
This section describes our experimental setup for
the en?cs and en?es translation tasks.
3.1 Data
Bilingual data: In the experiments we used data
sets provided by the workshop organizers. For the
en?cs translation table extraction we employed
both parallel corpora (News-Commentary10 and
CzEng 0.9), and for the en?es experiments, we
used the Europarl(Koehn, 2005), News Commen-
tary and United Nations parallel data. We used a
maximum sentence length of 80 for en?es and
40 for en?cs. Detailed statistics are shown in Ta-
ble 1.
Corpus Langs. Sent. Source
tokens
Target
tokens
Europarl en?es 1.6M 43M 45M
News-comm en?es 97k 2.4M 2.7M
UN en?es 5.9M 160M 190M
News-Comm en?cs 85k 1.8M 1.6M
CzEng en?cs 7.8M 80M 69M
Table 1: Statistics of en?cs and en?es parallel data.
Monolingual data: For language modeling pur-
poses, in addition to the target parts of the bilin-
gual data, we used the monolingual News corpus
for cs; and the Gigaword corpus for es. For both
languages, we used the SRILM toolkit (Stolcke,
2002) to train a 5-gram language model using all
monolingual data provided. However, for en?es
we used the IRSTLM toolkit (Federico and Cet-
tolo, 2007) to train a 5-gram language model using
the es Gigaword corpus. Both language models
use modified Kneser-Ney smoothing (Chen and
Goodman, 1996). Statistics for the monolingual
corpora are given in Table 2.
Corpus Language Sentences Tokens
E/N/NC/UN es 9,6M 290M
Gigaword es 40M 1,2G
News cs 13M 210M
Table 2: Statistics of Monolingual Data. E/N/NC/UN
refers to Europarl/News/News Commentary/United Nations
corpora.
For all the systems except Apertium, we first
lowercase and tokenize all the monolingual and
bilingual data using the tools provided by the
WMT10 organizers. After translation, system
combination output is detokenised and true-cased.145
3.2 English?Czech (en?cs) Experiments
The CzEng corpus (Bojar and Z?abokrtsky?, 2009)
is a collection of parallel texts from sources of dif-
ferent quality and as such it contains some noise.
As the first step, we discarded those sentence pairs
having more than 10% of non-Latin characters.
The CzEng corpus is quite large (8M sen-
tence pairs). Although we were able to build
a vanilla SMT system on all parallel data avail-
able (News-Commentary + CzEng), we also at-
tempted to build additional systems using News-
Commentary data (which we considered in-
domain) and various in-domain subsets of CzEng
hoping to achieve better results on domain-
specific data.
For our first system, we selected 128,218 sen-
tence pairs from CzEng labeled as news. For the
other two systems, we selected subsets of 2M and
4M sentence pairs identified as most similar to
the development sets (as a sample of in-domain
data) based on cosine similarity of their represen-
tation in a TF-IDF weighted vector space model
(cf. Byrne et al (2003)). We also applied the
pseudo-relevavance-feedback technique for query
expansion (Manning et al, 2008) to select another
subset with 2M sentence pairs.
We used the output of 15 systems for sys-
tem combination for the en?cs translation task.
Among these, 5 systems were built using Moses
and varying the size of the training data (DCU-
All, DCU-Ex2M, DCU-4M, DCU-2M and DCU-
News); 9 context-informed PB-SMT systems
(DCU-SSCI-*) using (combinations of) various
context features (word, PoS, supertags and depen-
dency relations) trained only on the News Com-
mentary data (marked with ? in Table 4); and one
system using the moses-chart decoder, also
trained on the news commentary data.
3.3 English?Spanish (en?es) Experiments
Three baseline systems using Moses were built,
where we varied the amount of training data used:
? epn: This system uses all of the Europarl and
News-Commentary parallel data.
? UN-half: This system uses the data suplied
to ?epn?, plus an additional 2.1M sentences
pairs randomly selected from the United Na-
tions corpus.
? all: This system uses all of the available par-
allel data.
For en?es we also obtained output from the
factored model (trained only on the news com-
mentary corpus) and the Apertium RBMT sys-
tem. We also derived phrase alignments using the
MaTrEx EBMT system (Stroppa and Way, 2006),
and added those phrase translations in the Moses
phrase table. The systems marked with ? use a
language model built using the Spanish Gigaword
corpus, in addition to the one built using the pro-
vided monolingual data. These 6 sets of system
outputs are then used for system combination.
3.4 Experimental Results
The evaluation results for en?es and en?cs ex-
periments are shown in Table 3 and Table 4 re-
spectively. The output of the systems marked ?
were submitted in the shared tasks.
System BLEU NIST METEOR TER
DCU-half ?? 29.77% 7.68 59.86% 59.55%
DCU-all ?? 29.63% 7.66 59.82% 59.74%
DCU-epn ?? 29.45% 7.66 59.71% 59.64%
DCU-ebmt ?? 29.38% 7.62 59.59% 60.11%
DCU-factor 22.58% 6.56 54.94% 67.65%
DCU-apertium 19.22% 6.37 49.68% 67.68%
DCU-system-
combination ? 30.42% 7.78 60.56% 58.71%
Table 3: en?es experimental results.
System BLEU NIST METEOR TER
DCU-All 10.91% 4.60 39.18% 81.76%
DCU-Ex2M 10.63% 4.56 39.12% 81.96%
DCU-4M 10.61% 4.56 39.26% 82.04%
DCU-2M 10.48% 4.58 39.35% 81.56%
DCU-Chart 9.34% 4.25 37.04% 83.87%
DCU-News 8.64% 4.16 36.27% 84.96%
DCU-SSCI-ccg? 8.26% 4.02 34.76% 85.58%
DCU-SSCI-
supertag-pair? 8.11% 3.95 34.93% 86.63%
DCU-SSCI-
ccg-ltag? 8.09% 3.96 34.90% 86.62%
DCU-SSCI-PR? 8.06% 4.00 34.89% 85.99%
DCU-SSCI-base? 8.05% 3.97 34.61% 86.02%
DCU-SSCI-PRIR? 8.03% 3.99 34.81% 85.98%
DCU-SSCI-ltag? 8.00% 3.95 34.57% 86.41%
DCU-SSCI-PoS? 7.91% 3.94 34.57% 86.51%
DCU-SSCI-word? 7.57% 3.88 34.16% 87.14%
DCU-system-
combination ? 13.22% 4.98 40.39% 78.59%
Table 4: en?cs experimental results.
4 Conclusion
This paper presents the Dublin City University
MT system in WMT2010 shared task campaign.
This was DCU?s first attempt to translate from en
to es and cs in any shared task. We developed a
multi-engine framework which combined the out-
puts of several individual MT systems and gener-
ated a new N -best list after CN decoding. Then by146
using some global features, the rescoring model
generated the final translation output. The experi-
mental results demonstrated that the combination
module and rescoring module are effective in our
framework for both language pairs, and produce
statistically significant improvements as measured
by bootstrap resampling methods (Koehn, 2004)
on BLEU over the single best system.
Acknowledgements: This work is supported
by Science Foundation Ireland (Grant No.
07/CE/I1142) and by PANACEA, a 7th Frame-
work Research Programme of the European
Union, contract number 7FP-ITC-248064. M.L.
Forcada?s sabbatical stay at Dublin City Univer-
sity is supported by Science Foundation Ireland
through ETS Walton Award 07/W.1/I1802 and by
the Universitat d?Alacant (Spain).
References
Bojar, O. and Z?abokrtsky?, Z. (2009). CzEng0.9:
Large Parallel Treebank with Rich Annotation.
Prague Bulletin of Mathematical Linguistics,
92:63?83.
Byrne, W., Khudanpur, S., Kim, W., Kumar, S.,
Pecina, P., Virga, P., Xu, P., and Yarowsky, D.
(2003). The Johns Hopkins University 2003
Chinese?English machine translation system.
In Proceedings of MT Summit IX, pages 447?
450, New Orleans, LA.
Chen, S. F. and Goodman, J. (1996). An Empir-
ical Study of Smoothing Techniques for Lan-
guage Modeling. In Proc. 34th Ann. Meeting of
the Association for Computational Linguistics,
pages 310?318, San Francisco, CA.
Chiang, D. (2007). Hierarchical phrase-
based translation. Computational Linguistics,
33(2):201?228.
Daelemans, W. and van den Bosch, A. (2005).
Memory-Based Language Processing (Studies
in Natural Language Processing). Cambridge
University Press, New York, NY.
Du, J., He, Y., Penkale, S., and Way, A. (2009).
MaTrEx: The DCU MT System for WMT2009.
In Proc. 3rd Workshop on Statistical Machine
Translation, EACL 2009, pages 95?99, Athens,
Greece.
Du, J., Pecina, P., and Way, A. (2010). An
Augmented Three-Pass System Combination
Framework: DCU Combination System for
WMT 2010. In Proc. ACL 2010 Joint Workshop
in Statistical Machine Translation and Metrics
Matr, Uppsala, Greece.
Federico, M. and Cettolo, M. (2007). Efficient
Handling of N-gram Language Models for Sta-
tistical Machine Translation. In Proceedings
of the Second Workshop on Statistical Machine
Translation, pages 88?95, Prague, Czech Re-
public.
Fiscus, J. G. (1997). A post-processing sys-
tem to yield reduced word error rates: Recog-
nizer output voting error reduction (ROVER).
In Proceedings 1997 IEEE Workshop on Auto-
matic Speech Recognition and Understanding
(ASRU), pages 347?352, Santa Barbara, CA.
Forcada, M. L., Tyers, F. M., and Ram??rez-
Sa?nchez, G. (2009). The free/open-source ma-
chine translation platform Apertium: Five years
on. In Proceedings of the First International
Workshop on Free/Open-Source Rule-Based
Machine Translation FreeRBMT?09, pages 3?
10.
Gough, N. and Way, A. (2004). Robust Large-
Scale EBMT with Marker-Based Segmenta-
tion. In Proceedings of the 10th International
Conference on Theoretical and Methodological
Issues in Machine Translation (TMI-04), pages
95?104, Baltimore, MD.
Haque, R., Naskar, S. K., Bosch, A. v. d., and
Way, A. (2009a). Dependency relations as
source context in phrase-based smt. In Proc.
23rd Pacific Asia Conference on Language, In-
formation and Computation, pages 170?179,
Hong Kong, China.
Haque, R., Naskar, S. K., Ma, Y., and Way, A.
(2009b). Using supertags as source language
context in SMT. In EAMT-2009: Proceed-
ings of the 13th Annual Conference of the Eu-
ropean Association for Machine Translation,
pages 234?241, Barcelona, Spain.
Koehn, P. (2004). Statistical significance tests for
machine translation evaluation. In Proceedings
of EMNLP, volume 4, pages 388?395.
Koehn, P. (2005). Europarl: A Parallel Corpus
for Statistical Machine Translation. In Machine
Translation Summit X, pages 79?86, Phuket,
Thailand.
Koehn, P. and Hoang, H. (2007). Factored Trans-
lation Models. In Proceedings of the Joint Con-
ference on Empirical Methods in Natural Lan-
guage Processing and Computational Natural147
Language Learning (EMNLP-CoNLL), pages
868?876, Prague, Czech Republic.
Koehn, P., Hoang, H., Birch, A., Callison-Burch,
C., Federico, M., Bertoldi, N., Cowan, B.,
Shen, W., Moran, C., Zens, R., Dyer, C., Bo-
jar, O., Constantin, A., and Herbst, E. (2007).
Moses: Open Source Toolkit for Statistical Ma-
chine Translation. In Annual Meeting of the As-
sociation for Computational Linguistics (ACL),
demonstration session, pages 177?180, Prague,
Czech Republic.
Kumar, S. and Byrne, W. (2004). Minimum
Bayes-Risk Decoding for Statistical Machine
Translation. In Proceedings of the Joint Meet-
ing of the Human Language Technology Con-
ference and the North American Chapter of
the Association for Computational Linguistics
(HLT-NAACL 2004), pages 169?176, Boston,
MA.
Mangu, L., Brill, E., and Stolcke, A. (2000). Find-
ing consensus in speech recognition: Word er-
ror minimization and other applications of con-
fusion networks. Computer Speech and Lan-
guage, 14(4):373?400.
Manning, C. D., Raghavan, P., and Schu?tze, H.
(2008). Introduction to Information Retrieval.
Cambridge University Press.
Och, F. (2003). Minimum error rate training
in statistical machine translation. In Proceed-
ings of the 41st Annual Meeting of the Asso-
ciation for Computational Linguistics (ACL),
pages 160?167, Sapporo, Japan.
Och, F. and Ney, H. (2002). Discriminative train-
ing and maximum entropy models for statistical
machine translation. In Proceedings of ACL,
volume 2, pages 295?302.
Papineni, K., Roukos, S., Ward, T., and Zhu, W.-J.
(2002). BLEU: a Method for Automatic Eval-
uation of Machine Translation. In Proceedings
of the 40th Annual Meeting of the Association
for Computational Linguistics (ACL-02), pages
311?318, Philadelphia, PA.
Ratnaparkhi, A. (1996). A Maximum Entropy
Model for Part-Of-Speech Tagging. In Pro-
ceedings of the Empirical Methods in Natural
Language Processing Conference (EMNLP),
pages 133?142, Philadelphia, PA.
Rosti, A.-V. I., Xiang, B., Matsoukas, S.,
Schwartz, R., Ayan, N. F., and Dorr, B. J.
(2007). Combining outputs from multiple ma-
chine translation systems. In Proceedings of the
Joint Meeting of the Human Language Technol-
ogy Conference and the North American Chap-
ter of the Association for Computational Lin-
guistics (HLT-NAACL 2007), pages 228?235,
Rochester, NY.
Schmid, H. (1994). Probabilistic Part-of-Speech
Tagging Using Decision Trees. In Proceedings
of International Conference on New Methods
in Language Processing, pages 44?49, Manch-
ester, UK.
Snover, M., Dorr, B., Schwartz, R., Micciula, L.,
and Makhoul, J. (2006). A study of transla-
tion edit rate with targeted human annotation.
In Proceedings of the 7th Conference of the As-
sociation for Machine Translation in the Amer-
icas (AMTA 2006), pages 223?231, Cambridge,
MA.
Stolcke, A. (2002). SRILM - An Extensible Lan-
guage Modeling Toolkit. In Proceedings of
the International Conference Spoken Language
Processing, pages 901?904, Denver, CO.
Stroppa, N., van den Bosch, A., and Way, A.
(2007). Exploiting Source Similarity for SMT
using Context-Informed Features. In Proceed-
ings of the 11th International Conference on
Theoretical and Methodological Issues in Ma-
chine Translation (TMI-07), pages 231?240,
Sko?vde, Sweden.
Stroppa, N. and Way, A. (2006). MaTrEx: the
DCU machine translation system for IWSLT
2006. In Proceedings of the International Work-
shop on Spoken Language Translation, pages
31?36, Kyoto, Japan.
Zens, R. and Ney, H. (2006). N-gram Poste-
rior Probabilities for Statistical Machine Trans-
lation. In Proceedings of the Joint Meeting of
the Human Language Technology Conference
and the North American Chapter of the As-
sociation for Computational Linguistics (HLT-
NAACL 2006), pages 72?77, New York, NY.
148
Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics, pages 48?58,
Avignon, France, April 23 - 27 2012. c?2012 Association for Computational Linguistics
Combining EBMT, SMT, TM and IR Technologies for Quality and Scale
Sandipan Dandapat1, Sara Morrissey1, Andy Way2, Joseph van Genabith1
1 CNGL, School of Computing
Dublin City University, Glasnevin, Dublin 9, Ireland
{sdandapat,smorri,josef}@computing.dcu.ie
2 Applied Language Solutions, Delph, UK
andy.way@appliedlanguage.com
Abstract
In this paper we present a hybrid statisti-
cal machine translation (SMT)-example-based
MT (EBMT) system that shows significant
improvement over both SMT and EBMT base-
line systems. First we present a runtime
EBMT system using a subsentential transla-
tion memory (TM). The EBMT system is fur-
ther combined with an SMT system for effec-
tive hybridization of the pair of systems. The
hybrid system shows significant improvement
in translation quality (0.82 and 2.75 abso-
lute BLEU points) for two different language
pairs (English?Turkish (En?Tr) and English?
French (En?Fr)) over the baseline SMT sys-
tem. However, the EBMT approach suffers
from significant time complexity issues for a
runtime approach. We explore two methods to
make the system scalable at runtime. First, we
use an heuristic-based approach. Secondly, we
use an IR-based indexing technique to speed
up the time-consuming matching procedure of
the EBMT system. The index-based match-
ing procedure substantially improves run-time
speed without affecting translation quality.
1 Introduction
State-of-the-art phrase-based SMT (Koehn, 2010a)
is the most successful MT approach in many large
scale evaluations, such as WMT,1 IWSLT2 etc. At
the same time, work continues in the area of EBMT.
Some recent EBMT systems include Cunei (Phillips,
1http://www.statmt.org/wmt11/
2http://www.iwslt2011.org/
2011), CMU-EBMT (Brown, 2011) and OpenMa-
TrEx (Dandapat et al, 2010). The success of an
SMT system often depends on the amount of parallel
training corpora available for the particular language
pair. However, low translation accuracy has been
observed for language pairs with limited training re-
sources (Islam et al, 2010; Khalilov et al, 2010).
SMT systems effectively discard the actual training
data once the models (translation model and lan-
guage model) have been estimated. This can lead to
their inability to guarantee good quality translation
for sentences closely matching those in the train-
ing corpora. By contrast, EBMT systems usually
maintain a linked relationship between the full sen-
tence pairs in source and target texts. Because of this
EBMT systems can often capture long range depen-
dencies and rich morphology at runtime. In contrast
to SMT, however, most EBMT models lack a well-
formed probability model, which restricts the use of
statistical information in the translation process.
Keeping these in mind, our objective is to de-
velop a good quality MT system choosing the best
approach for each input in the form of a hybrid SMT-
EBMT approach. It is often the case that an EBMT
system produces a good translation where SMT sys-
tems fail and vice versa (Dandapat et al, 2011).
An EBMT system relies on past translations to
derive the target output for a given input. Run-
time EBMT approaches generally do not include
any training stage, which has the advantage of not
having to depend on time-consuming preprocessing.
On the other hand, their runtime complexity can be
considerable. This is due to the time-consuming
matching stage at runtime that finds the example
48
(or set of examples) which most closely matches
the source-language sentence to be translated. This
matching step often uses some variation of string
edit-distance measures (Levenshtein, 1965) which
has quadratic time complexity.3 This is quite time-
consuming even when a moderate amount of train-
ing examples are used for the matching procedure.
We adopt two alternative approaches to tackle the
above problem. First we use heuristics which are of-
ten useful to avoid some of the computations. For a
input sentence, in the matching process, we may not
need to compute the string edit distance with all sen-
tences in the example base. In order to prune some
of the computation, we rely on the fact that the in-
put sentence and its closest match sentence from the
example-base are likely to have a similar sentence
length. Search engine indexing is an effective way
of storing data for fast and accurate retrieval of in-
formation. During retrieval, a set of documents are
extracted based on their similarity to the input query.
In our second approach, we use this concept to effi-
ciently retrieve a potential set of suitable candidate
sentences from the example-base to find the closest
match. We index the entire example-base consider-
ing each source-side sentence as a document for the
indexer. We show that improvements can be made
with our approach in terms of time complexity with-
out affecting the translation quality.
The remainder of this paper is organized as fol-
lows. The next section presents work related to our
EBMT approach. Section 3 describes the MT sys-
tems used in our experiments. Section 4 focuses on
the two techniques used to make the system scalable.
Section 5 presents the experiments in detail. Section
6 presents and discusses the results and provides an
error analysis. We conclude in Section 7.
2 Related Work
The EBMT framework was first introduced by Na-
gao (1984) as the ?MT by analogy principle?. The
two main approaches to EBMT are distinguished
by the inclusion or exclusion of a preprocess-
ing/training stage. Approaches that incorporate a
3Ukkonen (1983) gave an algorithm for computing edit-
distance with the worst case complexity O(md), where m is
the length of the string and d is their edit distance. This is ef-
fective when m  d. We use word-based edit distance, so m
is shorter in length.
training stage are commonly called ?compiled ap-
proaches? (Cicekli and Gu?venir, 2001). Approaches
that do not include a training stage are often referred
to as ?pure? or ?runtime? EBMT approaches, e.g.
(Lepage and Denoual, 2005). These approaches
have the advantage that they do not depend on any
time-consuming preprocessing stages. On the other
hand, their runtime complexity can be considerable.
EBMT is often linked with the related concept of
translation memory (TM). A TM essentially stores
source- and target-language translation pairs for ef-
fective reuse of previous translations originally cre-
ated by human translators. TMs are often used to
store examples for EBMT systems. After retriev-
ing a set of examples with associated translations,
EBMT systems automatically extract translations of
suitable fragments and combine them to produce a
grammatical target output.
Phrase-based SMT systems (Koehn, 2010a), pro-
duce a source?target algned subsentential phrase
table which can be adapted as an additional TM
to be used in a CAT environment (Simard, 2003;
Bic?ici and Dymetman, 2008; Bourdaillet et al,
2009; Simard and Isabelle, 2009). Koehn and Senel-
lart (2010b) use SMT to produce the translation of
the non-matched fragments after obtaining the TM-
based match. EBMT phrases have also been used
to populate the knowledge database of an SMT sys-
tem (Groves et al, 2006). However, to the best of
our knowledge, the use of SMT phrase tables within
an EBMT system as an additional sub-sentential TM
has not been attempted so far. Some work has been
carried out to integrate MT in a CAT environment
to translate the whole segment using the MT sys-
tem when no sufficiently well matching translation
unit (TU) is found in the TM. The TransType sys-
tem (Langlais et al, 2002) integrates an SMT sys-
tem within a text editor to suggest possible continua-
tions of the translations being typed by the translator.
By contrast, our approach attempts to integrate the
subsentential TM obtained using SMT techniques
within an EBMT system.
3 MT Systems
The SMT system used in our hybrid SMT-
EBMT approach is the vanilla Moses4 decoder.
4http://www.statmt.org/moses/
49
Moses (Koehn et al, 2007) is a set of SMT tools
that include routines to automatically train a transla-
tion model for any language pair and an efficient de-
coder to find the most probable translation. Due to
lack of space and the wide usage of Moses, here we
focus more on the novel EBMT system we have de-
veloped for our hybrid SMT-EBMT approach. The
EBMT system described in this section is based on
previous work (Dandapat et al, 2010) and some of
the material has been reproduced here to make the
paper complete.
Like all other EBMT systems, our particular ap-
proach comprises three stages: matching, alignment
and recombination. Our EBMT system also uses a
subsentential TM in addition to the sentence aligned
example-base. Using the original TM as a train-
ing set, additional subsentential TUs (words and
phrases) are extracted from it based on word align-
ments and phrase pairs produced by Moses. These
subsentential TUs are used for alignment and recom-
bination stages of our EBMT system.
3.1 Building a Subsentential TM for EBMT
A TM for EBMT usually contains TUs linked at
the sentence, phrasal and word level. TUs can be
derived manually or automatically (e.g. using the
marker-hypothesis (Groves et al, 2006)). Usually,
TUs are linguistically motivated translation units.
In this paper however, we explore a different route,
as manual construction of high-quality TMs is time
consuming and expensive. Furthermore, only con-
sidering linguistically motivated TUs may limit the
matching potential of a TM. Because of this, we
used SMT technology to automatically create the
subsentential part of our TM at the phrase (i.e.
no longer necessarily linguistically motivated) and
word level. Based on Moses word alignment (using
GIZA++ (Och and Ney, 2003)) and phrase table con-
struction, we construct the additional TM for further
use within an EBMT approach.
Firstly, we add entries to the TM based on the
aligned phrase pairs from the Moses phrase table us-
ing the following two scores:
1. Direct phrase translation probabilities: ?(t|s)
2. Direct lexical weight: lex(t|s)
Table 1 shows an example of phrase pairs with the
associated probabilities learned by Moses. We keep
all target equivalents in a sorted order based on the
Table 1: Moses phrase equivalence probabilities.
English (s) Turkish (t) p(t|s) lex(t|s)
a hotel bir otel 0.826087 0.12843
a hotel bir otelde 0.086957 0.07313
a hotel otel mi 0.043478 0.00662
a hotel otel 0.043478 0.22360
above probabilities. This helps us in the matching
procedure, but during recombination we only con-
sider the most probable target equivalent. The fol-
lowing shows the resulting TUs in the TM for the
English source phrase a hotel.
a hotel? {bir otel, bir otelde, otel, otem mi}
Secondly, we add entries to the TM based on the
source-to-target word-aligned file. We also keep
the multiple target equivalents for a source word in
a sorted order. This essentially adds source- and
target-language equivalent word pairs into the TM.
Note that the entries in the TM may contain in-
correct source-target equivalents due to unreliable
word/phrase alignments produced by Moses.
3.2 EBMT Engine
The overview of the three stages of the EBMT en-
gine is given below:
Matching: In this stage, we find a sentence pair
?sc, tc? from the example-base that closely matches
with the input sentence s. We used a fuzzy-match
score (FMS) based on a word-level edit distance
metric (Wagner and Fischer, 1974) to find the closest
matching source-side sentence from the example-
base ({si}N1 ) based on Equation (i).
score(s, si) = 1? ED(s, si)/max(|s|, |si|) (i)
where |x| denotes the length (in words) of a sen-
tence, and ED(x, y) refers to the word-level edit dis-
tance between x and y. The EBMT system considers
the associated translation tc of the closest matching
source sentence sc, to build a skeleton for the trans-
lation of the input sentence s.
Alignment: After retrieving the closest fuzzy-
matched sentence pair ?sc, tc?, we identify the non-
matching fragments from the skeleton translation tc
in two steps.
50
Firstly, we find the matched and non-matched
segments between s and sc using edit distance
trace. Given the two sentences (s and sc), the al-
gorithm finds the minimum possible number of op-
erations (substitutions, additions and deletions) re-
quired to change the closest match sc into the in-
put sentence s. For example, consider the input
sentence s = w1w2w3w4w5w6w7w8 and sc =
w?1w
?
3w4w5w7w8w
?
9. Figure 1 shows the matched
and non-matched sequence between s and sc using
edit-distance trace.
s = w1 w2 w3 w4 w5 w6 w7 w8 ?
| | | | |
sc = w1 ? w?3 w4 w5 ? w7 w8 w
?
9
?
s = w1 w2 w3 w4 w5 w6 w7 w8 null
| ? | ? | ?
sc = w1 w
?
3 w4 w5 null w7 w8 w
?
9
Figure 1: Extraction of matched (underlined) and non-
matched (boxed) segments between s and sc.
Secondly, we align each non-matched segment in
sc with its associated translation using the TM and
the GIZA++ alignment. Based on the source-target
aligned pair in the TM, we mark the mismatched
segment in tc. We find the longest possible seg-
ment from the non-matched segment in sc that has a
matching target equivalent in tc based on the source-
target equivalents in the TM. We continue the pro-
cess recursively until no further segments of the non-
matched segment in sc can be matched with tc us-
ing the TM. Remaining non-matching segments in
sc are then aligned with segments in tc using the
GIZA++ word alignment information.
Recombination: In the recombination stage, we
add or substitute segments from the input sentence s
with the skeleton translation equivalent tc. We also
delete some segments from tc that have no corre-
spondence in s. After obtaining the source segments
(needs to be added or substituted in tc) from the in-
put s, we use our subsentential TM to translate these
segments. Details of the recombination process are
given in Algorithm 1.
3.3 An Illustrative Example
As a running example, for the input sentence in (1a)
the corresponding closest fuzzy-matched sentence
Algorithm 1 recombination(X,TM)
In: source segment X ,
subsentential translation memory TM
Out: translation of source segment X
1: mark all words of X as untranslated
(untranslatedPortions(X)? {X})
2: repeat
3: U = untranslatedPortions(X)
4: x = longest subsegment in untranslatedPortions(X)
such that (x, tx) ? TM;
5: substitute(X,x ? tx) {substitute x with its target
equivalent tx in X}
6: remove x from untranslatedPortions(X)
7: until (untranslatedPortions(X) = U )
8: return X
pair ?sc, tc? is shown in (1b) and (1c). The portion
marked with angled brackets in (1c) are aligned with
the mismatched portion in (1b). The character and
the following number in angled brackets indicate the
edit operation (?s? indicates substitution) and the in-
dex of the mismatched segment from the alignment
process respectively.
1. (a) s: i ?d like a <s#0:present> for <s#1:my
mother> .
(b) sc: i ?d like a <s#0:shampoo> for
<s#1:greasy hair> .
(c) tc: <s#1:yag?l? sac?lar> ic?in bir
<s#0:s?ampuan> istiyorum .
During recombination, we need to replace two
segments in (1c) {yag?l? sac?lar (greasy hair) and
s?ampuan (shampoo)} with the two corresponding
source segments in (1a) {my mother and present}
as an intermediate stage (2) along the way towards
producing a target equivalent.
(2)<1:my mother> ic?in bir <0:present> istiyorum .
Furthermore, replacing the untranslated segments
in (2) with the translations obtained using TM, we
derive the output translation in (3) of the original in-
put sentence in (1).
(3) <annem> ic?in bir <hediye> istiyorum .
4 Scalability
The main motivation of scalability is to improve
the speed of the EBMT system when using a large
example-base. The matching procedure in an EBMT
system finds the example (or a set of examples)
which closely matches the source-language string to
51
be translated. All matching processes necessarily in-
volve a distance or similarity measure. The most
widely used distance measure in EBMT matching
is Levenshtein distance (Levenshtein, 1965; Wagner
and Fischer, 1974) which has quadratic time com-
plexity. In our EBMT system, we find the clos-
est sentence at runtime from the whole example-
base for a given input sentence using the edit dis-
tance matching score. Thus, the matching step of
the EBMT system is a time-consuming process with
a runtime complexity of O(nm2), where n denotes
the size of the example-base and m denotes the av-
erage length (in words) of a sentence. Due to a
significant runtime complexity, the EBMT system
can only handle a moderate size example-base in the
matching stage. However, it is important to handle a
large example-base to improve the quality of an MT
system. In order to make the system scalable with
a larger example-base, we adopt two approaches for
finding the closest matching sentences efficiently.
4.1 Grouping
Our first attempt is heuristic-based. We divide the
example-base into bins based on sentence length. It
is anticipated that the sentence from the example-
base that most closely matches an input sentence
will fall into the group which has comparable length
to the length of the input sentence. First, we divide
the example-baseE into different bins based on their
word-level length E =
?l
i=1Ei and Ei
?
Ej = ?
for all i 6= j where 0 ? i, j ? l. Ei denotes the
set of sentences with length i and l is the maximum
length of a sentence in E. In order to find the clos-
est match for a test sentence (s of length k), we only
consider examples EG =
?x
m=0Ek?m, where x in-
dicates the window size. In our experiment, we con-
sider the value of x from 0 to 2. We find the closest-
match sc from EG for a given test sentence s. EG
has fewer sentences compared to E which will ef-
fectively reduce the time of the matching procedure.
4.2 Indexing
Our second approach to addressing time complexity
is to use indexing. We index the complete example-
base using an open-source IR engine SMART5 and
retrieve a potential set of candidate sentences (likely
5An open source IR system from Cornell University. ftp:
//ftp.cs.cornell.edu/pub/smart/
to contain the closest match sentence) from the
example-base. Unigrams extracted from the sen-
tences of the example-base are indexed using the
language model (LM) and complete sentences are
considered as retrievable units. In LM-based re-
trieval we assume that a given query is generated
from a unigram document language model. The ap-
plication of the LM retrieval model in our case re-
turns a sorted list of sentences from the example-
base ordered by the estimated probabilities of gen-
erating the given input sentence.
In order to improve the run-time performance,
we integrate the SMART retrieval engine within the
matching procedure of our EBMT system. The re-
trieval engine estimates a potential set of candidate
close-matching sentences from the example-base E
for a test sentence s. We assume that the closest
source-side match sc of the input sentence s can
take the value from the set EIR(s), where EIR(s) is
the potential set of close-matching sentences com-
puted by the LM-based retrieval engine. We have
used the top 50 candidate sentences from EIR(s).
Since the IR engine tries to retrieve the document
(sentences from E) for a given query (input) sen-
tence, it is likely to retrieve the closest match sen-
tence sc in the set EIR(s). Due to a much re-
duced set of possibilities, this approach improves the
run-time performance of the EBMT system without
hampering system accuracy. Finding this potential
set of candidate sentences will be much faster than
traditional edit-distance-based retrieval on the full
example-base as the worst case run time of the re-
triever is O(
?
?wi
si), where wi is a word in the in-
put sentence and si is the number of sentences in the
example-base that contain wi. Finding a set of can-
didate sentences took only 0.3 seconds and 116 sec-
onds, respectively, for 414 and 10,000 example in-
put sentences given 20k and 250k sentence example-
base in our En?Tr and En?Fr experiment on a 3GHz
Core 2 Duo machine with 4GB RAM.
5 Experiments
We conduct different experiments to report the ac-
curacy of our EBMT systems for En?Tr and En?Fr
translation tasks. In order to compare the perfor-
mance of our approaches we use two baseline sys-
tems. We use the Moses SMT system as one base-
52
line. Furthermore, based on the matching step (Sec-
tion 3.2) of the EBMT approach, we obtain the clos-
est target-side equivalent (the skeleton sentence) and
consider this as the baseline output for the input to
be translated. This is referred to as TM in the exper-
iment below. We will consider this as the baseline
accuracy for our EBMT using TM approach.
In addition, we conduct two experiments with our
EBMT system. After obtaining the skeleton trans-
lation through the matching and alignment steps, in
the recombination step, we use TM to translate any
unmatched segments based on Algorithm 1. We call
this EBMTTM.
We found that there are cases where the
EBMTTM system produces the correct translation
but SMT fails and vice-versa (Dandapat et al, 2011).
In order to further improve translation quality, we
use a combination of EBMT and SMT. Here we use
some features to decide whether to rely on the out-
put produced by the EBMTTM system. These fea-
tures include fuzzy match scoreFMS (as in (i)) and
the number of mismatched segments in each of s,
sc, tc (EqUS6 as in (1)). We assume that the transla-
tions of an input sentence s produced by EBMTTM
and SMT systems are respectively TEBMT(s) and
TSMT(s). If the value of FMS is greater than some
threshold and EqUS exists between s and sc, we
rely on the output TEBMT(s); otherwise we take the
output from TSMT(s). We refer to this system as
EBMTTM + SMT.
To test the scalability of the system, we con-
ducted two more experiments based on the ap-
proach described in Section 4. First, we con-
ducted an experiment based on the sentence length-
based grouping heuristics (Section 4.1). We re-
fer to this system asEBMTTM + SMT+ groupi,
where i indicates the window size while compar-
ing the length of the input sentence with the bins.
We conduct a second experiment based on the LM-
based indexing technique (Section 4.2) we have used
to retrieve a potential set of candidate sentences
from the indexed example-base. We call this sys-
tem EBMTTM + SMT+ index. Note that the
EBMTTM + SMT system is used as the baseline
accuracy while conducting the experiments for scal-
6If s, sc and tc agree in the number of mismatched segments,
EqUS evaluates to 1, otherwise 0.
ability of the EBMT system.
5.1 Data Used for Experiments
We used two data sets for all our experiments rep-
resenting two language pairs of different size and
type. In the first data-set, we have used the En?Tr
corpus from IWSLT09.7 The training data consists
of 19,972 parallel sentences. We used the IWSLT09
development set as our testset which consists of 414
sentences. The IWSLT09 data set is comprised of
short sentences (with an average of 9.5 words per
sentence) from a particular domain (the C-STAR
project?s Basic Travel Expression Corpus).
Our second data set consists of an En?Fr
corpus from the European Medicines Agency
(EMEA)8 (Tiedemann and Nygaard, 2009). The
training data consists of 250,806 unique parallel sen-
tences.9 As a testset we use a set of 10,000 ran-
domly drawn sentences disjoint from the training
corpus. This data also represents a particular domain
(medicine) but with longer sentence lengths (with an
average of 18.8 words per sentence) compared to the
IWSLT09 data.
6 Results and Observations
We used BLEU (Papineni et al, 2002) for automatic
evaluation of our EBMT systems. Table 2 shows
the accuracy obtained for both En?Tr and En?Fr by
the EBMTTM system described in Section 3. Here
we have two baseline systems (SMT and TM) as de-
scribed in the first two experiments in Section 5.
Table 2: Baseline BLEU scores of the two systems
and the scores for EBMTTM system.
System Language pairs
En?Tr En?Fr
SMT 23.59 55.04
TM 15.60 40.23
EBMTTM 20.08 48.31
Table 2 shows that EBMTTM has a lower system
accuracy than SMT for both the language pairs, but
7http://mastarpj.nict.go.jp/IWSLT2009/2009/12/downloads.html
8http://opus.lingfil.uu.se/EMEA.php
9A large number of duplicate sentences exists in the original
corpus (approximately 1M sentences). We remove duplicates
and consider sentences with unique translation equivalents.
53
better scores than TM alone. Tables 3 and 4 show
that combining EBMT with SMT systems shows im-
provements of 0.82 and 2.75 BLEU absolute over
the SMT baseline (Table 2) for both the En?Tr and
the En?Fr data sets. In each case, the improvement
of EBMTTM + SMT over the baseline SMT is sta-
tistically significant (reliability of 98%) using boot-
strap resampling (Koehn, 2004).
Table 3: En?Tr MT system accuracies of the com-
bined systems (EBMTTM + SMT) with different
combining factors. The second column indicates the
number (and percentage) of sentences translated by
the EBMTTM system during combination.
System: EBMTTM + SMT
Condition times
EBMTTM
used
BLEU
(in %)
FMS>0.85 35 (8.5%) 24.22
FMS>0.80 114 (27.5%) 23.99
FMS>0.70 197 (47.6%) 22.74
FMS>0.80 OR
(FMS>0.70 & EqUS)
165 (40.0%) 23.87
FMS>0.85 & EqUS 24 (5.8%) 24.41
FMS>0.80 & EqUS 76 (18.4%) 24.19
FMS>0.70 & EqUS 127 (30.7%) 24.08
Table 4: En?Fr MT system accuracies for the com-
bined systems (EBMTTM + SMT) with different
combining factors.
System: EBMTTM + SMT
Condition times
EBMTTM
used
BLEU
(in %)
FMS>0.85 3323 (33.2%) 57.79
FMS>0.80 4300 (43.0%) 57.55
FMS>0.70 5283 (52.8%) 57.05
FMS>0.60 6148 (61.5%) 56.25
FMS>0.80 OR
(FMS>0.70 & EqUS)
4707 (47.1%) 57.46
FMS>0.85 & EqUS 2358 (23.6%) 57.24
FMS>0.80 & EqUS 2953 (29.5%) 57.16
FMS>0.70 & EqUS 3360 (33.6%) 57.08
A particular objective of our work is to scale the
runtime EBMT system to a larger amount of train-
ing examples. We experiment with the two ap-
proaches described in Section 4 to improve the run
time of the system. Table 5 compares the run time of
the three systems (EBMTTM, EBMTTM + groupi
and EBMTTM + index) for both En?Tr and En?Fr
translation. Note that the SMT decoder takes 140
seconds and 310 minutes respectively for En?Tr and
En?Fr translation test sets.
Table 5: Running time of the three different systems.
System Language pairs
En?Tr En?Fr
(seconds) (minutes)
SMT 140.0 310.0
EBMTTM 295.9 2267.0
EBMTTM + group0 34.0 63.4
EBMTTM + group1 96.2 183.5
EBMTTM + group2 148.5 301.4
EBMTTM + index 2.7 2.6
Both the grouping and indexing methodologies
proved successful for system scalability with a max-
imum speedup of almost 2 orders of magnitude. We
also need to estimate the accuracy while combining
grouping and indexing techniques with the baseline
system (EBMTTM + SMT) to understand their rel-
ative performance. Table 6 provides the system ac-
curacy using the grouping and indexing techniques
for both the language pairs. We report the transla-
tion quality under three conditions. Similar trends
have been observed for other conditions.
6.1 Observations and Discussions
We find that the EBMTTM system has a lower ac-
curacy on its own compared to baseline SMT for
both the language pairs (Table 2). Nevertheless,
there are sentences which are better translated by the
EBMTTM approach compared to SMT, although
the overall document translation score is higher with
SMT. Thus, we combined the two systems based on
different features and found that the combined sys-
tem performs better. The highest relative improve-
ments in BLEU score are 3.47% and 1.05% respec-
tively for En?Tr and En?Fr translation. We found
that if an input has a high fuzzy match score (FMS)
with the example-base, then the EBMTTM system
does better compared to SMT. With our current ex-
perimental setup, we found that an FMS over 0.8
showed an improvement for En?Tr and a FMS over
0.6 showed improvement for En?Fr over the SMT
system. Figure 2 shows the effect in the translation
54
Table 6: BLEU scores of the three different systems for En?Tr and En?Fr under different conditions. i
denotes the number of bins considered during grouping.
Condition System
EBMTTM + SMT EBMTTM + SMT EBMTTM + SMT
+groupi +index
i=0 i=?1 i=?2
En?Tr
FMS>0.85 24.22 24.18 24.18 24.23 24.24
FMS>0.80 OR (FMS>0.70 & EqUS) 23.87 23.34 23.90 24.40 24.37
FMS>0.85 & EqUS 24.41 24.17 24.38 24.34 24.39
En?Fr
FMS>0.85 57.79 56.47 57.48 57.76 57.92
FMS>0.80 OR (FMS>0.70 & EqUS) 57.46 55.69 57.07 57.33 57.56
FMS>0.85 & EqUS 57.24 56.48 57.23 57.29 57.32
quality when different FMS thresholds were used to
combine the two systems.
However, FMS might not be the only factor for
triggering the EBMTTM system. We considered
EqUs as another factor which showed improvement
for En?Tr but showed negative effect for En?Fr.
Though an FMS over 0.7 for En?Tr shows no im-
provement in overall system accuracy, inclusion of
the EqUs feature along with FMS shows improve-
ment. Thus, the EBMTTM system is sometimes
more effective when the number of unmatched seg-
ment matches in s, sc and tc.
These observations show the effective use of our
EBMT approach in terms of translation quality.
However, we found that the EBMTTM system has
a very considerable runtime complexity. In order to
translate 414 test sentences from English into Turk-
ish, the basic EBMT system takes 295.9 seconds.
The situation becomes worse when using the large
example-base for En?Fr translation. Here, we found
that the system takes around 38 hours to translate
10k source English sentences into French. This is
a significant time complexity by any standard for a
runtime approach. However, both grouping and in-
dexing reduce the time complexity of the approach
considerably. The time reduction with grouping de-
pends on the number of bins considered to find the
closest sentence during the matching stage. Systems
with a lower number of bins take less time but cause
more of a drop in translation quality. The effect is
more prominent with the En?Fr system which uses
a larger example-base. We found a drop of abso-
lute 1.32 BLEU points while considering a single
bucket whose length is equal to the length of the
test sentence. This configuration takes 63 minutes to
translate 10k English sentences into French. There
is only a drop of 0.03 BLEU points when consider-
ing the 5 nearest bins (?2) for a given test sentence.
Nevertheless, there is not much of a reduction but it
increases the run time to 5 hours for the translation
of 10k sentences. Thus, the group-based method is
not effective enough to balance system accuracy and
run time.
Incorporation of the indexing technique into the
matching stage of EBMT shows the highest effi-
ciency gains in run time. Translating 10k sen-
tences from English into French takes only 158 sec-
onds. It is also interesting to note that with index-
ing, the BLEU score remained the same or even in-
creased. This is due to the fact that, compared to
FMS-based matching, a different closest-matching
sentence sc is selected for some of the input sen-
tences while using indexing, thus resulting in a dif-
ferent outcome to the system. Figure 3 compares
the number of times the EBMTTM + SMT + index
system is used in the hybrid system and the num-
ber of same closest-matching sentences selected by
EBMTTM + SMT + index systems under different
conditions for En?Tr. The use of index-based candi-
date selection for EBMT matching shows effective
55
 48.31
 55.04
 57.79
 0.2  0.4  0.6  0.8 0.9 1
BLE
U (%
)
Fuzzy Match Score
EBMTTM+SMTSMTEBMTTM
(a) En?Fr
 20.08
 23.59 24.22
 0.3  0.4  0.6  0.8 0.9  1
BLE
U (%
)
Fuzzy Match Score
EBMTTM+SMTSMTEBMTTM
(b) En?Tr
Figure 2: Effect of FMS in the combined EBMTTM + SMT system.
Table 7: The effect of indexing in selection sc and
in final translation.
Input: zeffix belongs to a group of medicines called
antivirals.
Ref : zeffix appartient a` une classe de
me?dicaments appele?s antiviraux.
baseline EBMTTM system
sc: simulect belongs to a group of medicines
called immunosuppressants.
st: simulect fait parti d ? une classe de
me?dicaments appele?s immunosuppresseurs.
Output: zeffix fait parti d ? une classe de
me?dicaments appele?s antiviraux.
EBMTTM + SMT + index system
sc: diacomit belongs to a group of medicines
called antiepileptics.
st: diacomit appartient a` un groupe de
me?dicaments appele?s antie?pileptiques.
Output: zeffix appartient a` un groupe de
me?dicaments appele?s antiviraux.
improvement in translation time, and BLEU scores
remained the same or increased. Due to the selec-
tion of different closest-matching sentence sc, some-
times the system produces better quality translation
which increases the system level BLEU score. Ta-
ble 7 shows one such En?Fr example where an
index-based technique produced a better translation
than the baseline (EBMTTM + SMT) system.
7 Conclusion
Our experiments show that EBMT approaches work
better compared to the SMT-based system for cer-
tain sentences when a high fuzzy match score is
Figure 3: Number of times EBMTTM + SMT + index
used in the hybrid system and the number of times
the same closest-matching sentences are selected by the
systems. a=FMS>0.85, b=FMS>0.85 & EqUS and
c=FMS>0.80 OR (FMS>0.70 & EqUS)
obtained for the input sentence with the example-
base. Thus a feature-based combination of EBMT-
and SMT-based systems produces better translation
quality than either of the individual systems. Inte-
gration of a SMT technology-based sub-sentential
TM with the EBMT framework (EBMTTM) has im-
proved translation quality in our experiments.
Our baseline EBMTTM system is a runtime ap-
proach which has high time complexity when us-
ing a large example-base. We found that the inte-
gration of IR-based indexing substantially improves
run time without affecting BLEU score. So far our
systems have been tested using moderately sized
example-bases from a closed domain corpus. In our
future work, we plan to use a much larger example-
base and wider-domain corpora.
56
Acknowledgments
This research is supported by Science Foundation
Ireland (Grants 07/CE/I1142, Centre for Next Gen-
eration Localisation).
References
S. Armstrong, C. Caffrey, M. Flanagan, D. Kenny, M.
O?Hagan and A. Way. 2006. Improving the Quality
of Automated DVD Subtitles via Example-Based Ma-
chine Translation. Translating and the Computer 28,
[no page number], London: Aslib, UK.
E. Bic?ici and M. Dymetman. 2008. Dynamic Translation
Memory: Using Statistical Machine Translation to Im-
prove Translation Memory. In Gelbukh, Alexander F.,
editor, In Proceedings of the 9th International Confer-
ence on Intelligent Text Processing and Computational
Linguistics (CICLing), volume 4919 of Lecture Notes
in Computer Science, pp 3-57 Springer Verlag.
J. Bourdaillet, S. Huet, F. Gotti, G. Lapalme and P.
Langlais. 2009. Enhancing the bilingual concor-
dancer TransSearch with word-level alignment. In
Proceedings, volume 5549 of Lecture Notes in Artifi-
cial Intelligence: 22nd Canadian Conference on Ar-
tificial of Intelligence (Canadian AI 2009), Springer-
Verlag, pp. 27-38.
R. D. Brown. 2011. The CMU-EBMT machine transla-
tion system. Machine Translation, 25(2):179?195.
I. Cicekli and H. A. Gu?venir. 2001. Learning trans-
lation templates from bilingual translation examples.
Applied Intelligence, 15(1):57?76.
S. Dandapat, S. Morrissey, A. Way and M.L. Forcada.
2011. Using Example-Based MT to Support Sta-
tistical MT when Translating Homogeneous Data in
Resource-Poor Settings. In Proceedings of the 15th
Annual Meeting of the European Association for Ma-
chine Translation (EAMT 2011), pp. 201-208. Leu-
ven, Belgium.
S. Dandapat, M.L. Forcada, D. Groves, S. Penkale,
J. Tinsley and A. Way. 2010. OpenMaTrEx:
a free/open-source marker-driven example-based ma-
chine translation system. In Proceedings of the 7th In-
ternational Conference on Natural Language Process-
ing (IceTAL 2010), pp. 121-126. Reykjav??k, Iceland.
D. Groves and A. Way. 2006. Hybridity in MT: Exper-
iments on the Europarl Corpus. In Proceedings of the
11th Conference of the European Association for Ma-
chine Translation (EAMT 2006), pp. 115-124. Oslo,
Norway.
M. Islam, J. Tiedemann and A. Eisele. 2010. English?
Bengali Phrase-based Machine Translation. In Pro-
ceedings of the 14th Annual Conference of the Eu-
ropean Association of Machine Translation, (EAMT
2010), [no page number], Saint-Raphae?l, France.
M. Khalilov, J.A.R. Fonollosa, I. Skadina, E. Bralitis
and L. Pretkalnina. 2010. English?Latvian SMT: the
Challenge of Translating into a Free Word Order Lan-
guage. In Proceedings of the 2nd International Work-
shop on Spoken Language Technologies for Under-
resourced Languages (SLTU 2010), [no page num-
ber], Saint-Raphae?l, France.
P. Koehn. 2010. Statistical Machine Translation, Cam-
bridge University Press, Cambridge, UK.
P. Koehn, H. Hoang, A. Birch, C. Callison-Burch, M.
Federico, N. Bertoldi, B. Cowan, W. Shen, C. Moran,
R. Zens, C. Dyer, O. Bojar, A. Constantin and E.
Herbst. 2007. Moses: open source toolkit for statisti-
cal machine translation. In Proceedings of the Demon-
stration and Poster Sessions at the 45th Annual Meet-
ing of the Association of Computational Linguistics
(ACL 2007), pp. 177-180. Prague, Czech Republic.
P. Koehn. 2004. Statistical significance tests for machine
translation evaluation. In Proceedings of the Confer-
ence on Empirical Methods in Natural Language Pro-
cessing (EMNLP 2004), pp. 388-395. Barcelona,
Spain.
P. Koehn and J. Senellart. 2004. Convergence of Trans-
lation Memory and Statistical Machine Translation. In
Proceedings of the AMTA workshop on MT Research
and the Translation Industry, pp. 21-23. Denever, CO.
P. Langlais, G. Lapalme and M. Loranger. 2002.
Development-evaluation cycles to boost translator?s
productivity. Machine Translation, 15(4):77?98.
Y. Lepage and E. Denoual. 2005. Purest ever example-
based machine translation: Detailed presentation and
assessment. Machine Translation, 19(3-4):251?282.
V. I. Levenshtein. 1965. Binary Codes Capable of Cor-
recting Deletions, Insertions, and Reversals. Dok-
lady Akademii Nauk SSSR, 163(4):845-848., English
translation in Soviet Physics Doklady,10(8), 707-710.
C. D. Manning, P. Raghavan and H. Schu?tze. 2008. In-
troduction to Information Retrieval. Cambridge Uni-
versity Press, Cambridge, UK.
M. Nagao. 1984. A Framework of a Machine Translation
between Japanese and English by Analogy Principle.
In Elithorn, A. and Banerji, R., editors, Artificial Hu-
man Intelligence, pp. 173?180, North-Holland, Ams-
terdam.
F. Och and H. Ney. 2003. A Systematic Comparison of
Various Statistical Alignment Models. Computational
Linguistics, 29(1):19?51.
K. Papineni, S. Roukos, T. Ward and W. J. Zhu. 2002.
BLEU: a Method for Automatic Evaluation of Ma-
chine Translation. In Proceedings of the 40th Annual
Meeting of the Association for Computational Linguis-
tics, (ACL 2002), pp. 311?318, Philadelphia, PA.
57
A. B. Phillips. 2011. Cunei: open-source machine trans-
lation with relevance-based models of each translation
instance. Machine Translation, 25(2):161?177.
M. Simard and P. Isabelle. 2009. Phrase-based Machine
Translation in a Computer-assisted Translation Mem-
ory. In Proceedings of the 12th Machine Translation
Summit, (MT Summit XII), pp. 120?127, Ottawa,
Canada.
M. Simard. 2003. Translation spotting for translation
memories. In Proceedings of the HLT-NAACL 2003,
Workshop on Building and Using Parallel Texts: Data
Driven Machine Translation and Beyond, pp. 65?72,
Edmonton, Canada.
H. Somers. 2003. An Overview of EBMT. In M. Carl
and A. Way , editors, Recent Advances in Example-
based Machine Translation, pp. 3-57, Kluwer Aca-
demic Publishers, Dordrecht, The Netherlands.
J. Tiedemann and L. Nygaard. 2009. News from OPUS
- A Collection of Multilingual Parallel Corpora with
Tools and Interfaces, in N. Nicolov, K. Bontcheva,
G. Angelova, R. Mitkov. (eds.), Recent Advances in
Natural Language Processing, V:237?248, John Ben-
jamins, Amsterdam, The Netherlands.
E. Ukkonen. 1983. On Approximate String Matching. In
Proceedings of International Conference on Founda-
tions of Computing Theory, (FCT 1983), pp. 487?496,
Borgholm, Sweden.
R. Wagner and M. Fischer. 1974. The String-to-String
Correction Problem. Journal of the Association for
Computing Machinery, 21:168?173.
58
