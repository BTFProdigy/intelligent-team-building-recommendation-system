Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational
Natural Language Learning, pp. 630?639, Prague, June 2007. c?2007 Association for Computational Linguistics
Treebank Annotation Schemes and Parser Evaluation for German
Ines Rehbein
NCLT
School of Computing, DCU,
Dublin, Ireland
irehbein@computing.dcu.ie
Josef van Genabith
NCLT,
School of Computing, DCU,
Dublin, Ireland
IBM Dublin Center for Advanced Studies
josef@computing.dcu.ie
Abstract
Recent studies focussed on the question
whether less-configurational languages like
German are harder to parse than English,
or whether the lower parsing scores are an
artefact of treebank encoding schemes and
data structures, as claimed by Ku?bler et al
(2006). This claim is based on the as-
sumption that PARSEVAL metrics fully re-
flect parse quality across treebank encoding
schemes. In this paper we present new ex-
periments to test this claim. We use the
PARSEVAL metric, the Leaf-Ancestor met-
ric as well as a dependency-based evalua-
tion, and present novel approaches measur-
ing the effect of controlled error insertion
on treebank trees and parser output. We
also provide extensive past-parsing cross-
treebank conversion. The results of the ex-
periments show that, contrary to Ku?bler et
al. (2006), the question whether or not Ger-
man is harder to parse than English remains
undecided.
1 Introduction
A long-standing and unresolved issue in the pars-
ing literature is whether parsing less-configurational
languages is harder than e.g. parsing English. Ger-
man is a case in point. Results from Dubey and
Keller (2003) suggest that state-of-the-art parsing
scores for German are generally lower than those ob-
tained for English, while recent results from Ku?bler
et al (2006) raise the possibility that this might
be an artefact of particular encoding schemes and
data structures of treebanks, which serve as training
resources for probabilistic parsers. Ku?bler (2005)
and Maier (2006) show that treebank annotation
schemes have considerable influence on parsing re-
sults. A comparison of unlexicalised PCFG pars-
ing (Ku?bler, 2005) trained and evaluated on the Ger-
man NEGRA (Skut et al, 1997) and the Tu?Ba-
D/Z (Telljohann et al, 2004) treebanks using LoPar
(Schmid, 2000) shows a difference in parsing results
of about 16%, using the PARSEVAL metric (Black
et al, 1991). Ku?bler et al (2006) conclude that,
contrary to what had been assumed, German is not
actually harder to parse than English, but that the
NEGRA annotation scheme does not support opti-
mal PCFG parsing performance.
Despite being the standard metric for measuring
PCFG parser performance, PARSEVAL has been
criticised for not representing ?real? parser quality
(Carroll et al, 1998; Brisco et al, 2002; Sampson
and Babarbczy, 2003). PARSEVAL checks label and
wordspan identity in parser output compared to the
original treebank trees. It neither weights results,
differentiating between linguistically more or less
severe errors, nor does it give credit to constituents
where the syntactic categories have been recognised
correctly but the phrase boundary is slightly wrong.
With this in mind, we question the assumption
that the PARSEVAL results for NEGRA and Tu?Ba-
D/Z reflect a real difference in quality between the
parser output for parsers trained on the two different
treebanks. As a consequence we also question the
conclusion that PARSEVAL results for German in
the same range as the parsing results for the English
630
Penn-II Treebank prove that German is not harder
to parse than the more configurational English. To
investigate this issue we present experiments on the
German TIGER treebank (Dipper et al, 2001) and
the Tu?Ba-D/Z treebank. TIGER is based on and ex-
tends the NEGRA data and annotation scheme. Our
error insertion and past-parsing treebank-encoding
experiments experiments show that the differences
in parsing results for the two treebanks are not
caused by a higher number of errors in the output
of the parser trained on the TIGER treebank, but are
due to the bias of the PARSEVAL metric towards an-
notation schemes (such as that of Tu?Ba-D/Z) with a
higher ratio of non-terminal/terminal nodes. The ex-
periments also show that compared to PARSEVAL
the Leaf-Ancestor metric is somewhat less suscep-
tible to non-terminal/terminal ratios and that con-
trary to the PARSEVAL results, dependency-based
evaluations score TIGER trained parsers higher than
Tu?Ba-D/Z trained parsers.
This paper is structured as follows: Section 2
gives an overview of the main features of the two
treebanks. Section 3 describes our first experiment,
where we systematically insert controlled errors into
the original treebank trees and compare the influence
of these modifications on the evaluation results in
the PARSEVAL metric and the Leaf-Ancestor met-
ric against the original, unmodified trees for both
treebanks. In Section 4 we present the second ex-
periment, where we extract an unlexicalised PCFG
from each of the treebanks. Then we convert the out-
put of the PCFG parser trained on the Tu?Ba-D/Z into
a TIGER-style format and evaluate the converted
trees. In Section 5 we present a dependency-based
evaluation and compare the results to the results of
the two other measures. The last section concludes.
2 The TIGER Treebank and the Tu?Ba-D/Z
The two German treebanks used in our experiments
are the TIGER Treebank (Release 2) and the Tu?ba-
D/Z (Release 2). The Tu?Ba-D/Z consists of approx-
imately 22 000 sentences, while the TIGER Tree-
bank is much larger with more than 50 000 sen-
tences. Both treebanks contain German newspaper
text and are annotated with phrase structure and de-
pendency (functional) information. Both treebanks
use the Stuttgart Tu?bingen POS Tag Set (Schiller
et al, 95). TIGER uses 49 different grammatical
function labels, while the Tu?Ba-D/Z utilises only
36 function labels. For the encoding of phrasal
node categories the Tu?Ba-D/Z uses 30 different cat-
egories, the TIGER Treebank uses a set of 27 cate-
gory labels.
Other major differences between the two tree-
banks are: in the Tiger Treebank long distance de-
pendencies are expressed through crossing branches
(Figure 1), while in the Tu?Ba-D/Z the same phe-
nomenon is expressed with the help of grammati-
cal function labels (Figure 2), where the node label
V-MOD encodes the information that the PP mod-
ifies the verb. The annotation in the Tiger Tree-
bank is rather flat and allows no unary branching,
whereas the nodes in the Tu?Ba-D/Z do contain unary
branches and a more hierarchical structure, resulting
in a much deeper tree structure than the trees in the
Tiger Treebank. This results in an average higher
number of nodes per sentence for the Tu?Ba-D/Z. Ta-
ble 1 shows the differences in the ratio of nodes for
the Tiger treebank and the Tu?Ba-D/Z.
phrasal phrasal words
nodes/sent nodes/word /sent
TIGER 8.29 0.47 17.60
Tu?Ba-D/Z 20.69 1.20 17.27
Table 1: Average number of phrasal nodes/words in
TIGER and Tu?Ba-D/Z
Figures 1 and 2 also illustrate the different annota-
tion of PPs in both annotation schemes. In the Tiger
treebank the internal structure of the PP is flat and
the adjective and noun inside the PP are directly at-
tached to the PP, while the Tu?Ba-D/Z is more hier-
archical and inserts an additional NP node.
Another major difference is the annotation of
topological fields in the style of Drach (1937) and
Ho?hle (1986) in the Tu?Ba-D/Z. The model captures
German word order, which accepts three possible
sentence configurations (verb first, verb second and
verb last), by providing fields like the initial field
(VF), the middle field (MF) and the final field (NF).
The fields are positioned relative to the verb, which
can fill in the left (LK) or the right sentence bracket
(VC). The ordering of topological fields is deter-
mined by syntactic constraints.
631
Auch mit staatlichen Auftr?agen sieht es schlecht aus.
?It also looks bad for public contracts.?
Figure 1: TIGER treebank tree
In Wales sieht es besser aus.
?Things seem better in Wales.?
Figure 2: Tu?Ba-D/Z treebank tree
2.1 Differences between TIGER and NEGRA
To date, most PCFG parsing for German has
been done using the NEGRA corpus as a train-
ing resource. The flat annotation scheme of the
TIGER treebank is based on the NEGRA anno-
tation scheme, but it also employs some impor-
tant extensions, which include the annotation of
verb-subcategorisation, appositions and parenthe-
ses, coordinations and the encoding of proper nouns
(Brants et al, 2002).
3 Treebank Preprocessing: Converting
TIGER Graphs into CFG Trees
The sentences in the TIGER treebank are repre-
sented as graphs with LDDs expressed through
crossing branches. Before being able to insert er-
rors or extract a PCFG we had to resolve these cross-
ing branches in the TIGER treebank. This was done
by attaching the non-head child nodes higher up in
the tree, following Ku?bler (2006). For the graph
in Figure 1 this would mean that the modifying PP
?Auch mit staatlichen Auftra?gen? (also for public
contracts) was attached directly to the S node, while
the head of the adjectival phrase (AP) remained in
it?s original position. As a side effect this leads to the
creation of some unary nodes in the TIGER trees.
We also inserted a virtual root node and removed
all functional labels from the TIGER and Tu?Ba-D/Z
trees.
4 Experiment I
Experiment I is designed to assess the impact
of identical errors on the two treebank encoding
schemes and the PARSEVAL1 and Leaf-Ancestor
evaluation metrics.
4.1 Experimental Setup
The TIGER treebank and the Tu?Ba-D/Z both con-
tain newspaper text, but from different German
newspapers. To support a meaningful comparison
we have to compare similar sentences from both
treebanks. In order to control for similarity we se-
lected all sentences of length 10 ? n ? 40 from
both treebanks. For all sentences with equal length
we computed the average number of prepositions,
determiners, nouns (and related POS such as proper
names and personal pronouns), interrogative pro-
nouns, finite verbs, infinite verbs, past participles
and imperative verb forms. For each sentence length
we selected all sentences from both treebanks which
showed an average for each of the POS listed above
which did not deviate more than 0.8 from the av-
erage for all sentences for this particular sentence
length. From this set we randomly selected 1024
sentences for each of the treebanks. This results in
two test sets, comparable in word length, syntactic
structure and complexity. Table 2 shows the ratio of
phrasal versus terminal nodes in the test sets.
We then inserted different types of controlled er-
rors automatically into the original treebank trees in
our test sets and evaluated the modified trees against
1In all our experiments we use the evalb metric (Sekine
and Collins, 1997), the most commonly used implementation
of the PARSEVAL metric.
632
phrasal phrasal nodes words
nodes/sent nodes/word /sent
TIGER 6.97 0.48 14.49
Tu?Ba-D/Z 19.18 1.30 14.75
Table 2: Average number of phrasal nodes/words in
the TIGER and Tu?Ba-D/Z test set
the original treebank trees, in order to assess the im-
pact of similar (controlled for type and number) er-
rors on the two encoding schemes.
4.2 Error Insertion
The errors fall into three types: attachment, span and
labeling (Table 3). We carried out the same number
of error insertions in both test sets.
Error description
ATTACH I Attach PPs inside an NP one level
higher up in the tree
ATTACH II Change verb attachment to noun
attachment for PPs on sentence level,
inside a VP or in the MF (middle field)
LABEL I Change labels of PPs to NP
LABEL II Change labels of VPs to PP
SPAN I Include adverb to the left of a PP
into the PP
SPAN II Include NN to the left of a PP
into the PP
SPAN III Combination of SPANI and SPANII
Table 3: Description of inserted error types
4.3 Results for Error Insertion for the Original
Treebank Trees
Table 4 shows the impact of the error insertion into
the original treebank trees on PARSEVAL results,
evaluated against the gold trees. PARSEVAL results
in all experiments report labelled precision and re-
call. The first error (PP attachment I, 85 insertions
in each test set) leads to a decrease in f-score of 1.16
for the TIGER test set, while for the Tu?Ba-D/Z test
set the same error only caused a decrease of 0.43.
The effect remains the same for all error types and
is most pronounced for the category label errors, be-
cause the frequency of the labels resulted in a large
number of substitutions. The last row lists the total
weighted average for all error types, weighted with
respect to their frequency of occurrence in the test
sets.
Table 4 clearly shows that the PARSEVAL
measure punishes the TIGER treebank annotation
TIGER Tu?Ba # errors
PP attachment I 98.84 99.57 85
PP attachment II 98.75 99.55 89
Label I 80.02 92.73 1427
Label II 93.00 97.45 500
SPAN I 99.01 99.64 71
SPAN II 97.47 99.08 181
SPAN III 96.51 98.73 252
total weighted ave. 87.09 95.30
Table 4: f-score for PARSEVAL results for error in-
sertion in the original treebank trees
scheme to a greater extent, while the same num-
ber and type of errors in the Tu?Ba-D/Z annotation
scheme does not have an equally strong effect on
PARSEVAL results for similar sentences.
4.4 Discussion: PARSEVAL and LA
Experiment I shows that the gap between the PAR-
SEVAL results for the two annotation schemes does
not reflect a difference in quality between the trees.
Both test sets contain the same number of sentences
with the same sentence length and are equivalent in
complexity and structure. They contain the same
number and type of errors. This suggests that the
difference between the results for the TIGER and
the Tu?Ba-D/Z test set are due to the higher ratio of
non-terminal/terminal nodes in the Tu?Ba-D/Z trees
(Table 1).
In order to obtain an alternative view on the
quality of our annotation schemes we used the
leaf-ancestor (LA) metric (Sampson and Babarbczy,
2003), a parser evaluation metric which measures
the similarity of the path from each terminal node
in the parse tree to the root node. The path con-
sists of the sequence of node labels between the ter-
minal node and the root node, and the similarity of
two paths is calculated by using the Levenshtein dis-
tance (Levenshtein, 1966). Table 5 shows the results
for the leaf-ancestor evaluation metric for our error
insertion test sets. Here the weighted average re-
sults for the two test sets are much closer to each
other (94.98 vs. 97.18 as against 87.09 vs. 95.30).
Only the label errors, due to the large numbers, show
a significant difference between the two annotation
schemes. Tables 4 and 5 show that compared to
PARSEVAL the LA metric is somewhat less sensi-
tive to the nonterminal/terminal ratio.
Figure 3 illustrates the different behaviour of the
633
TIGER Tu?Ba # errors
PP attachment I 99.62 99.70 85
PP attachment II 99.66 99.78 89
Label I 92.45 95.24 1427
Label II 96.05 99.28 500
SPAN I 99.82 99.84 71
SPAN II 99.51 99.77 181
SPAN III 99.34 99.62 252
total weighted ave. 94.98 97.18
Table 5: LA results for error insertion in the original
treebank trees
two evaluation metrics with respect to an example
sentence.
Sentence 9:
Die Stadtverwaltung von Venedig hat erstmals streunende
Katzen gez?ahlt.
?For the first time the city council of Venice has counted stray-
ing cats.?
(TOP
(S
(NP
(ART Die [the] )
(NN Stadtverwaltung [city counsil] )
(PP
(APPR von [of] )
(NE Venedig [Venice] )
)
)
(VAFIN hat [has] )
(VP
(ADV erstmals [for the first time] )
(NP
(ADJA streunende [straying] )
(NN Katzen [cats] )
)
(VVPP geza?hlt [counted] )
)
)
($. .)
)
Figure 3: Sentence 9 from the TIGER Test Set
Table 6 shows that all error types inserted into
Sentence 9 in our test set result in the same eval-
uation score for the PARSEVAL metric, while the
LA metric provides a more discriminative treatment
of PP attachment errors, label errors and span errors
for the same sentence (Table 6). However, the dif-
ferences in the LA results are only indirectly caused
by the different error types. They actually reflect
the number of terminal nodes affected by the error
insertion. For Label I and II the LA results vary
considerably, because the substitution of the PP for
an NP (Label I) in Figure 3 affects two terminal
nodes only (PP von [of] Venedig [Venice]), while
the change of the VP into a PP (Label II) alters
the paths of four terminal nodes (VP erstmals [for
the first time] streunende [straying] Katzen [cats]
geza?hlt [counted]) and therefore has a much greater
impact on the overall result for the sentence.
ERROR PARSEVAL LA
PP attachment I 83.33 96.30
Label I 83.33 96.00
Label II 83.33 91.00
SPAN II 83.33 96.40
Table 6: Evaluation results for Sentence 9
The Tu?Ba-D/Z benefits from its overall higher ra-
tio of nodes per sentence, resulting in a higher ratio
of non-terminal/terminal nodes per phrase and the
effect, that the inserted label error affects a smaller
number of terminal nodes than in the TIGER test set
for LA testing.
5 Experiment II
Ku?bler (2005) and Maier (2006) assess the impact of
the different treebank annotation schemes on PCFG
parsing by conducting a number of modifications
converting the Tu?Ba-D/Z into a format more sim-
ilar to the NEGRA (and hence TIGER) treebank.
After each modification they extract a PCFG from
the modified treebank and measure the effect of the
changes on parsing results. They show that with
each modification transforming the Tu?Ba-D/Z into
a more NEGRA-like format the parsing results also
become more similar to the results of the NEGRA
treebank, i.e. the results get worse. Maier takes this
as evidence that the Tu?Ba-D/Z is more adequate for
PCFG parsing. This assumption is based on the be-
lief that PARSEVAL results fully reflect parse qual-
ity across different treebank encoding schemes. This
is not always true, as shown in Experiment I.
In our second experiment we crucially change the
order of events in the Ku?bler (2005), Maier (2006)
and Ku?bler et al (2006) experiments: We first ex-
tract an unlexicalised PCFG from each of the orig-
inal treebanks. We then transform the output of
the parser trained on the Tu?Ba-D/Z into a format
more similar to the TIGER Treebank. In contrast to
Ku?bler (2005) and Maier (2006), who converted the
634
treebank before extracting the grammars in order to
measure the impact of single features like topologi-
cal fields or unary nodes on PCFG parsing, we con-
vert the trees in the parser output of a parser trained
on the original unconverted treebank resources. This
allows us to preserve the basic syntactic structure
and also the errors present in the output trees re-
sulting from a potential bias in the original tree-
bank training resources. The results for the original
parser output evaluated against the unmodified gold
trees should not be crucially different from the re-
sults for the modified parser output evaluated against
the modified gold trees.
5.1 Experimental Setup
For Experiment II we trained BitPar (Schmid, 2004),
a parser for highly ambiguous PCFG grammars, on
the two treebanks. The Tu?Ba-D/Z training data con-
sists of the 21067 treebank trees not included in the
Tu?Ba-D/Z test set. Because of the different size of
the two treebanks we selected 21067 sentences from
the TIGER treebank, starting from sentence 10000
(and excluding the sentences in the TIGER test set).
Before extracting the grammars we resolved the
crossing branches in the TIGER treebank as de-
scribed in Section 3. After this preprocessing step
we extracted an unlexicalised PCFG from each of
our training sets. Our TIGER grammar has a total of
21163 rule types, while the grammar extracted from
the Tu?Ba-D/Z treebank consists of 5021 rules only.
We parsed the TIGER and Tu?Ba-D/Z test set with
the extracted grammars, using the gold POS tags for
parser input. We then automatically converted the
Tu?Ba-D/Z output to a TIGER-like format and com-
pare the evaluation results for the unmodified trees
against the gold trees with the results for the con-
verted parser output against the converted gold trees.
5.2 Converting the Tu?Ba-D/Z Trees
The automatic conversion of the Tu?Ba-D/Z-style
trees includes the removal of topological fields and
unary nodes as well as the deletion of NPs inside
of PPs, because the NP child nodes are directly at-
tached to the PP in the TIGER annotation scheme.
As a last step in the conversion process we adapted
the Tu?Ba-D/Z node labels to the TIGER categories.
5.2.1 The Conversion Process: An Example
We demonstrate the conversion process using an
example sentence from the Tu?Ba-D/Z test set (Fig-
ure 4). The converted tree is given in Figure 5:
topological fields, here VF (initial field), MF (mid-
dle field) and LK (left sentence bracket), as well as
unary nodes have been removed. The category la-
bels have been changed to TIGER-style annotation.
Erziehungsurlaub nehmen bisher nur zwei Prozent der M?anner.
?Until now only two percent of the men take parental leave.?
Figure 4: Original Tu?Ba-D/Z-style gold tree
Figure 5: Converted TIGER-style gold tree
Figure 6 shows the unmodified parser output from
the Tu?Ba-D/Z trained grammar for the same string.
The parser incorrectly included all adverbs inside an
NP governed by the PP, while in the gold tree (Figure
4) both adverbs are attached to the PP. The modified
parser output is shown in Figure 7.
5.3 Results for Converted Parser Output
We applied the conversion method described above
to the original trees and the parser output for the sen-
tences in the TIGER and the Tu?Ba-D/Z test sets. Ta-
ble 7 shows PARSEVAL and LA results for the mod-
ified trees, evaluating the converted parser output
635
Figure 6: Parser output (Tu?Ba-D/Z grammar)
Figure 7: Converted parser output (Tu?Ba-D/Z)
for each treebank against the converted gold trees
of the same treebank. Due to the resolved crossing
branches in the TIGER treebank we also have some
unary nodes in the TIGER test set. Their removal
surprisingly improves both PARSEVAL and LA re-
sults. For the Tu?Ba-D/Z all conversions lead to a
decrease in precision and recall for the PARSEVAL
metric. Converting the trees parsed by the Tu?Ba-
D/Z grammar to a TIGER-like format produces an f-
score which is slightly lower than that for the TIGER
trees. The same is true for the LA metric, but not to
the same extent as for PARSEVAL. The LA met-
ric also gives slightly better results for the original
TIGER trees compared to the result for the unmodi-
fied Tu?Ba-D/Z trees.
The constant decrease in PARSEVAL results for
the modified trees is consistent with the results in
Ku?bler et al (2005), but our conclusions are slightly
different. Our experiment shows that the Tu?Ba-
D/Z annotation scheme does not generally produce
higher quality parser output, but that the PARSE-
VAL results are highly sensitive to the ratio of non-
terminal/terminal nodes. However, the parser output
for the grammar trained on the Tu?Ba-D/Z yields a
EVALB LA
prec. recall f-sco. avg.
TIGER 83.54 83.65 83.59 94.69
no Unary 84.33 84.48 84.41 94.83
Tu?Ba-D/Z 92.59 89.79 91.17 94.23
Tu?Ba-D/Z? TIGER
no Top 92.38 88.76 90.53 93.93
no Unary 89.96 85.67 87.76 93.59
no Top + no U. 88.44 82.24 85.23 92.91
no Top + no U. 87.15 79.52 83.16 92.47
+ no NP in PP
Table 7: The impact of the conversion process on
PARSEVAL and LA
higher precision in the PARSEVAL metric against
the Tu?Ba-D/Z gold trees than the parser output of
the TIGER grammar against the TIGER gold trees.
For PARSEVAL recall, the TIGER grammar gives
better results.
6 Experiment III
In Experiment I and II we showed that the tree-
based PARSEVAL metric is not a reliable measure
for comparing the impact of different treebank an-
notation schemes on the quality of parser output and
that the issue, whether German is harder to parse
than English, remains undecided. In Experiment III
we report a dependency-based evaluation and com-
pare the results to the results of the other metrics.
6.1 Dependency-Based (DB) Evaluation
The dependency-based evaluation used in the exper-
iments follows the method of Lin (1998) and Ku?bler
and Telljohann (2002), converting the original tree-
bank trees and the parser output into dependency re-
lations of the form WORD POS HEAD. Functional
labels have been omitted for parsing, therefore the
dependencies do not comprise functional informa-
tion. Figure 8 shows the original TIGER Treebank
representation for the CFG tree in Figure 3. Square
boxes denote grammatical functions. Figure 9 shows
the dependency relations for the same tree, indicated
by labelled arrows. Converted into a WORD POS
HEAD triple format the dependency tree looks as
follows (Table 8).
Following Lin (1998), our DB evaluation algo-
rithm computes precision and recall:
? Precision: the percentage of dependency re-
lationships in the parser output that are also
636
Figure 8: TIGER treebank representation for Figure 3
SB
NKPGNK NK OA
MO
OC
the  city counsil   of   Venice  has  for the    straying   cats  counted
                                      first time
Die    Stadtverwaltung    von    Venedig    hat     erstmals       streunende    Katzen    gez?hlt    
?For the first time the city counsil of Venice has counted straying cats.?
Figure 9: Dependency relations for Figure 8
found in the gold triples
? Recall: the percentage of dependency relation-
ships in the gold triples that are also found in
the parser output triples.
WORD POS HEAD
Die [the] ART Stadtverwaltung
Stadtverwaltung NN hat
[city counsil]
von [of] APPR Stadtverwaltung
Venedig [Venice] NE von
hat [has] VAFIN -
erstmals ADV geza?hlt
[for the first time]
streunende [straying] ADJA Katzen
Katzen [cats] NN geza?hlt
geza?hlt [counted] VVPP hat
Table 8: Dependency triples for Figure 9
We assessed the quality of the automatic conver-
sion methodology by converting the 1024 original
trees from each of our test sets into dependency rela-
tions, using the functional labels in the original trees
to determine the dependencies. Topological fields
in the Tu?Ba-D/Z test set have been removed before
extracting the dependency relationships.
We then removed all functional information from
the trees and converted the stripped trees into depen-
dencies, using heuristics to find the head. We eval-
uated the dependencies for the stripped gold trees
against the dependencies for the original gold trees
including functional labels and obtained an f-score
of 99.64% for TIGER and 99.13% for the Tu?Ba-D/Z
dependencies. This shows that the conversion is re-
liable and not unduly biased to either the TIGER or
Tu?Ba-D/Z annotation schemes.
6.2 Experimental Setup
For Experiment III we used the same PCFG gram-
mars and test sets as in Experiment II. Before ex-
tracting the dependency relationships we removed
the topological fields in the Tu?Ba-D/Z parser output.
As shown in Section 6.1, this does not penalise the
dependency-based evaluation results for the Tu?Ba-
D/Z. In contrast to Experiment II we used raw text
as parser input instead of the gold POS tags, allow-
637
ing a comparison with the gold tag results in Table 7.
6.3 Results
Table 9 shows the evaluation results for the three
different evaluation metrics. For the DB evalua-
tion the parser trained on the TIGER training set
achieves about 7% higher results for precision and
recall than the parser trained on the Tu?Ba-D/Z. This
result is clearly in contrast to the PARSEVAL scores,
which show higher results for precision and recall
for the Tu?Ba-D/Z. But contrary to the PARSEVAL
results on gold POS tags as parser input (Table 7),
the gap between the results for TIGER and Tu?Ba-
D/Z is not as wide as before. PARSEVAL gives
a labelled bracketing f-score of 81.12% (TIGER)
and 85.47% (Tu?Ba-D/Z) on raw text as parser in-
put, while the results on gold POS tags are more dis-
tinctive with an f-score of 83.59% for TIGER and
91.17% for Tu?Ba-D/Z. The LA results again give
better scores to the TIGER parser output, this time
the difference is more pronounced than for Experi-
ment II (Table 7).
Dependencies PARSEVAL LA
Prec Rec Prec Rec Avg
TIGER 85.71 85.72 81.21 81.04 93.88
Tu?Ba 76.64 76.63 87.24 83.77 92.58
Table 9: Parsing results for three evaluation metrics
The considerable difference between the results
for the metrics raises the question which of the met-
rics is the most adequate for judging parser output
quality across treebank encoding schemes.
7 Conclusions
In this paper we presented novel experiments assess-
ing the validity of parsing results measured along
different dimensions: the tree-based PARSEVAL
metric, the string-based Leaf-Ancestor metric and
a dependency-based evaluation. By inserting con-
trolled errors into gold treebank trees and measuring
the effects on parser evaluation results we gave new
evidence for the downsides of PARSEVAL which,
despite severe criticism, is still the standard mea-
sure for parser evaluation. We showed that PAR-
SEVAL cannot be used to compare the output of
PCFG parsers trained on different treebank anno-
tation schemes, because the results correlate with
the ratio of non-terminal/terminal nodes. Compar-
ing two different annotation schemes, PARSEVAL
consistently favours the one with the higher node ra-
tio.
We examined the influence of treebank annotation
schemes on unlexicalised PCFG parsing, and re-
jected the claim that the German Tu?Ba-D/Z treebank
is more appropriate for PCFG parsing than the Ger-
man TIGER treebank and showed that converting
the Tu?Ba-D/Z trained parser output to a TIGER-like
format leads to PARSEVAL results slightly worse
than the ones for the TIGER treebank trained parser.
Additional evidence comes from a dependency-
based evaluation, showing that, for the output of the
parser trained on the TIGER treebank, the mapping
from the CFG trees to dependency relations yields
better results than for the grammar trained on the
Tu?Ba-D/Z annotation scheme, even though PARSE-
VAL scores suggest that the TIGER-based parser
output trees are substantial worse than Tu?Ba-D/Z-
based parser output trees.
We have shown that different treebank annotation
schemes have a strong impact on parsing results for
similar input data with similar (simulated) parser er-
rors. Therefore the question whether a particular
language is harder to parse than another language
or not, can not be answered by comparing parsing
results for parsers trained on treebanks with differ-
ent annotation schemes. Comparing PARSEVAL-
based parsing results for a parser trained on the
Tu?Ba-D/Z or TIGER to results achieved by a parser
trained on the English Penn-II treebank (Marcus
et al, 1994) does not provide conclusive evidence
about the parsability of a particular language, be-
cause the results show a bias introduced by the
combined effect of annotation scheme and evalua-
tion metric. This means that the question whether
German is harder to parse than English, is still
undecided. A possible way forward is perhaps a
dependency-based evaluation of TIGER/Tu?Ba-D/Z
with Penn-II trained grammars for ?similar? test and
training sets and cross-treebank and -language con-
trolled error insertion experiments. Even this is not
entirely straightforward as it is not completely clear
what constitutes ?similar? test/training sets across
languages. We will attempt to pursue this in further
research.
638
Acknowledgements
We would like to thank the anomymous reviewers
for many helpful comments. This research has been
supported by a Science Foundation Ireland grant
04|IN|I527.
References
Black, E., S. P. Abney, D. Flickinger, C. Gdaniec, R. Gr-
ishman, P. Harrison, D. Hindle, R. Ingria, F. Jelinek,
J. Klavans, M. Liberman, M. P. Marcus, S. Roukos, B.
Santorini, and T. Strzalkowski. 1991. A procedure for
quantitatively comparing the syntactic coverage of En-
glish grammars. In Proceedings DARPA Speech and
Natural Language Workshop, Pacific Grove, CA, pp.
306-311.
Brants, Sabine, and Silvia Hansen. 2002. Developments
in the TIGER Annotation Scheme and their Realiza-
tion in the Corpus. In Proceedings of the Third Confer-
ence on Language Resources and Evaluation (LREC
2002), pp. 1643-1649 Las Palmas.
Briscoe, E. J., J. A. Carroll, and A. Copestake. 2002.
Relational evaluation schemes. In Proceedings Work-
shop ?Beyond Parseval - towards improved evaluation
measures for parsing systems?, 3rd International Con-
ference on Language Resources and Evaluation, pp.
4-38. Las Palmas, Canary Islands.
Carroll, J., E. Briscoe and A. Sanfilippo. 1998. Parser
evaluation: a survey and a new proposal. In Proceed-
ings of the 1st International Conference on Language
Resources and Evaluation, Granada, Spain. 447-454.
Dipper, S., T. Brants, W. Lezius, O. Plaehn, and G. Smith.
2001. The TIGER Treebank. In Third Workshop on
Linguistically Interpreted Corpora LINC-2001, Leu-
ven, Belgium.
Drach, Erich. 1937. Grundgedanken der Deutschen Sat-
zlehre. Frankfurt/M.
Dubey, A., and F. Keller. 2003. Probabilistic parsing for
German using sisterhead dependencies. In Proceed-
ings of the 41st Annual Meeting of the Association for
Computational Linguistics, Sapporo, Japan.
Ho?hle, Tilman. 1998. Der Begriff ?Mittelfeld?, An-
merkungen u?ber die Theorie der topologischen Felder.
In Akten des Siebten Internationalen Germansitenkon-
gresses 1985, pages 329-340, Go?ttingen, Germany.
Ku?bler, Sandra, and Heike Telljohann. 2002. Towards
a Dependency-Oriented Evaluation for Partial Pars-
ing. In Proceedings of Beyond PARSEVAL ? Towards
Improved Evaluation Measures for Parsing Systems
(LREC 2002 Workshop), Las Palmas, Gran Canaria,
June 2002.
Lin, Dekang. 1998. A dependency-based method for
evaluating broad-coverage parsers. Natural Language
Engineering, 1998.
Ku?bler, Sandra. 2005. How Do Treebank Annotation
Schemes Influence Parsing Results? Or How Not to
Compare Apples And Oranges. In Proceedings of
FANLP 2005), Borovets, Bulgaria, September 2005.
Ku?bler, Sandra, Erhard Hinrichs, and Wolfgang Maier.
2006. Is it Really that Difficult to Parse German?
In Proceedings of the 2006 Conference on Empiri-
cal Methods in Natural Language Processing, EMNLP
2006), Sydney, Australia, July 2006.
Levenshtein, V. I. 1966. Binary codes capable of correct-
ing deletions, insertions, and reversals. Soviet Physics
- Doklady, 10.707-10 (translation of Russian original
published in 1965).
Maier, Wolfgang. 2006. Annotation Schemes and
their Influence on Parsing Results. In Proceedings of
the COLING/ACL 2006 Student Research Workshop),
Sydney, Australia, July 2006.
Marcus, M., G. Kim, M. A. Marcinkiewicz, R. MacIn-
tyre, M. Ferguson, K. Katz and B. Schasberger. 1994.
The Penn Treebank: Annotating Predicate Argument
Structure. In Proceedings of the ARPA Human Lan-
guage Technology Workshop, Princeton, NJ.
Sampson, Geoffrey, and Anna Babarczy. 2003. A test
of the leaf-ancestor metric for parse accuracy. Natural
Language Engineering, 9 (4):365-380.
Schmid, Helmut. 2000. LoPar: Design and Implemen-
tation. Arbeitspapiere des Sonderforschungsbereiches
340, No. 149, IMS Stuttgart, July 2000.
Schmid, Helmut. 2004. Efficient Parsing of Highly Am-
biguous Context-Free Grammars with Bit Vectors. In
Proceedings of the 20th International Conference on
Computational Linguistics (COLING 2004), Geneva,
Switzerland.
Sekine, S. and M. J. Collins. 1997. The evalb software.
http://nlp.cs.nyu.edu/evalb/
Skut, Wojciech, Brigitte Krann, Thorsten Brants, and
Hans Uszkoreit. 1997. An annotation scheme for free
word order languages. In Proceedings of ANLP 1997,
Washington, D.C.
Telljohann, Heike, Erhard W. Hinrichs, Sandra Ku?bler,
and Heike Zinsmeister. 2005. Stylebook for
the Tu?bingen Treebank of Written German (Tu?Ba-
D/Z). Seminar fu?r Sprachwissenschaft, Universita?t
Tu?bingen, Germany.
Schiller, Anne, Simone Teufel, and Christine Thielen.
1995. Guidelines fr das Tagging deutscher Textcor-
pora mit STTS. Technical Report, IMS-CL, University
Stuttgart, 1995.
639
Proceedings of the Third ACL-SIGSEM Workshop on Prepositions, pages 57?64,
Trento, Italy, April 2006. c?2006 Association for Computational Linguistics
German Particle Verbs and Pleonastic Prepositions
Ines Rehbein
NCLT
School of Computing, DCU,
Dublin, Ireland
irehbein@computing.dcu.ie
Josef van Genabith
NCLT,
School of Computing, DCU,
Dublin, Ireland
josef@computing.dcu.ie
Abstract
This paper discusses the behaviour of Ger-
man particle verbs formed by two-way
prepositions in combination with pleonas-
tic PPs including the verb particle as a
preposition. These particle verbs have a
characteristic feature: some of them li-
cense directional prepositional phrases in
the accusative, some only allow for loca-
tive PPs in the dative, and some parti-
cle verbs can occur with PPs in the ac-
cusative and in the dative. Directional par-
ticle verbs together with directional PPs
present an additional problem: the par-
ticle and the preposition in the PP seem
to provide redundant information. The
paper gives an overview of the semantic
verb classes influencing this phenomenon,
based on corpus data, and explains the un-
derlying reasons for the behaviour of the
particle verbs. We also show how the re-
strictions on particle verbs and pleonastic
PPs can be expressed in a grammar theory
like Lexical Functional Grammar (LFG).
1 Introduction
The subject of this paper are German particle verbs
with pleonastic prepositions (5). In German there
are nine two-way prepositions which can either
govern the accusative or the dative: an, auf, hinter,
in, neben, ?uber, unter, vor and zwischen. The dif-
ference in case assignment also causes a different
interpretation of the semantics of the prepositional
phrase: if the preposition governs the dative it ex-
presses a locative relation (1), while the accusative
goes together with a directional interpretation (2).
(1) Das Bild h?angt [PP an der Wand].
Det Picture hang-3Sg [PP on?dir Detdat wall].
?The picture hangs on the wall.?
(2) Sie h?angt das Bild [PP an die Wand].
She hang-3Sg Det picture [PP onto+dir Detacc wall].
?She hangs the picture on the wall.?
The two-way prepositions combined as prefixes
with a verb form the so-called particle verbs (also
called separable prefix verbs). The particles im-
plicitly include directional information and can
change the aspectual mode and argument struc-
ture of their base verbs. Particle verbs can be dif-
ferentiated according to whether they allow for a
pleonastic combination with the particle in ques-
tion and the resulting syntactic and semantic ef-
fects.
Olsen (1998) refers to this phenomenon as the
Pleonastic Directional, where the verb particle al-
ready saturates the directional requirement of the
verb and therefore there should be no need for a
further preposition offering the same directional
information. However, example (5) shows that
pleonastic directionals can in fact occur with di-
rectional PPs, while in (3) the main verb (without
particle) combines with a directional PP and in (4)
only the particle verb is used.
(3) Sie steigt [PP in das Auto].
She climb-3SG [PP into+DIR Det car].
?She gets into the car.?
(4) Sie steigt ein.
She climb-3SG Part+DIR.
?She gets in.?
(5) Sie steigt [PP in das Auto] ein.
She gets [PP into+DIR Det car] Part+DIR.
?She gets into the car.?
The problem is that it is not clear what licenses
57
the directional preposition in cases such as (5) and
why it is not supressed by the verb particle.
The base verb in (3) licenses a directional PP,
which is part of the argument structure of the verb.
If there is a verb particle which saturates this di-
rectional requirement (4), then the realisation of
the PP is optional. Wunderlich (1983) argues that
particle verbs require a stereotype or contextually
given object equal to the internal argument of the
prepositional relation, which can be reconstructed
from the context and therefore can be omitted.
If the directional information is already repre-
sented by the particle, then the question arises
what licenses the directional PP. It could be argued
that the particle should suppress a directional PP
or, conversely that the directional PP should sup-
press the verb particle. The question which of the
two is selected first, the particle verb or the prepo-
sition, is discussed controversially. In a speaker-
oriented view the particle verb will be selected
first, while the theory of linear sentence processing
claims that the particle, which is only encountered
at the end of the sentence, should be omitted.
Particle verbs with pleonastic PPs exhibit an-
other interesting property: some of them only
allow for pleonastic prepositions governing da-
tive PPs while others trigger the accusative, and
some particle verbs can even go together with both
cases. The underlying reasons for those case pref-
erences are not completely clear.
It is obvious that there are certain verb classes
whose semantics seem to influence the case as-
signed by the preposition. This is strongly con-
nected with the influence of directional informa-
tion concerning the case preference of the particle
verb. Particle verbs which express directional in-
formation trigger PPs in the accusative, while par-
ticle verbs whose semantics contain no directional
component never combine with an accusative PP.
But why are there also particle verbs which are
able to combine with both cases?
The aim of this paper is to give an explana-
tion for this phenomenon, based on data gained
through corpus research. Section 2 describes char-
acteristic features of spatial prepositions and par-
ticle verbs. Section 3 presents a novel corpus-
based typology of verb classes triggering different
case for pleonastic prepositions, accounting for
regularities in their observed behaviour. Section
4 provides a novel account of particle verbs with
their pleonastic prepositions using the framework
of Lexical Functional Grammar (Bresnan, 2000).
The last section summarizes the main results es-
tablished in this paper.
2 Characteristic Features of Particle
Verbs and Spatial Prepositions
Spatial prepositions are binary relations between
two entities, where one of the entities is located
with respect to a region defined by the second en-
tity, specified through the preposition. The mean-
ing of a two-way preposition depends on the case
of the PP: if it is in the dative, its reading will be
interpreted as a static, non-directional localisation,
while the accusative triggers a directional interpre-
tation. In the latter case the preposition implies a
change of location of the theme referent from an
unspecified region into the neighbouring region of
the relatum (Witt, 1998).
In this paper we only deal with spatial prepo-
sitions, ignoring lexicalised prepositions without
semantic content, as in (6):
(6) Sie wartet auf den Bus.
She wait-3Sg for Det bus.
?She is waiting for the bus.?
Dalrymple (2001) refers to (6) as idiosyncratic
case, because the lexical form of the preposition
is not related to the semantic role of the argu-
ment, while oblique arguments which are marked
according to the semantic role of the argument are
assigned semantic case. Particle verbs formed by
two-way prepositions always have a semantic con-
tent.
The semantics of verb particles basing on spa-
tial prepositions is equivalent to the semantics of
the prepositions. They are also binary, but the in-
ternal argument of the relation is not explicitly ex-
pressed in the argument structure of the complex
verb, but can be omitted (see examples (3) and
(4)). The semantics of the particle is integrated
into the semantics of the base verb which requires
a directional complement.
In example (5) both particle verb and pleonastic
PP occur together. Here the PP specifies the im-
plicit reference object of the particle verb, and its
relation of localisation is congruent with the direc-
tional semantics of the particle.
These characteristic features of particle verbs
and spatial prepositions are constitutive for the
classification into semantic verb classes given in
Section 3.
58
3 Corpus-Based Classification of Particle
Verbs with Pleonastic Prepositions
The classification of particle verbs with pleonas-
tic prepositions into semantic verb classes is based
on the proposals by Witt (1998) extented by the
results of our own corpus research.1 Witt?s clas-
sification only considers particle verbs with the
particle ein-. He divides them into three ma-
jor groups: compositional formations, regular for-
mations and non-compositional formations, which
can be further subclassified into more fine-grained
subclasses (Figure 1).
1. Compositional Formations
(a) Verb bases are causative Verbs of Localisation
(b) Verb bases are (static) Verbs of Localisation
(c) Verb bases are intransitive Verbs of Motion
(d) Verb bases are transitive Verbs of Motion
(Transport Verbs)
2. Regular Formations
(a) Verb Bases are Activity-Verbs
(b) Verb Bases are ?eingravieren (to engrave)?-Verbs
3. Non-Compositional Formations: Extensions
of Meaning
(a) Verb Bases are ein-Verbs with the meaning:
?downward, inward, into itself?
(b) Verb Bases are ein-Verbs with the meaning:
?to enclose something?
Figure 1: Witt?s (1998) classification of particle
verbs with ein-
In contrast to Witt, our classification includes all
two-way prepositions as verb particles. As we are
trying to explain the behaviour of particle verbs in
regard to their ability to combine with pleonastic
PPs, we divide the corpus data into the following
groups: particle verbs licensing pleonastic PPs in
the accusative only (Group A), particle verbs li-
censing pleonastic PPs in the dative only (Group
B) and particle verbs which are able to govern ei-
ther accusative or dative PPs (Group C).
Each of these groups can be divided into a num-
ber of subgroups, formed by different semantic
verb types. Figure 2 gives an overview of our clas-
sification scheme.
1The corpora used for the research are the text basis
of the Digital Dictionary of German Language (DWDS)
(http://www.dwds.de/textbasis) and the corpora
of the Institute of German Language (IDS) in Mannheim
(http://www.ids-mannheim.de/cosmas2).
1. Group A (combine only with accusative PPs)
(a) Verb bases are (static) Verbs of Localisation
(b) Verb bases are intransitive Verbs of Motion
(c) Verb bases are transitive Verbs of Motion
(Transport Verbs)
(d) Verb bases are Verbs of Perception
(e) Verb bases express a Change of State
2. Group B (combine only with dative PPs)
(a) Verb bases are (static) Verbs of Localisation
(b) Verb bases are intransitive Verbs of Motion
(c) Verb bases are (causative) Verbs of Position
3. Group C (combine with accusative and dative PPs)
(a) Verb bases are intransitive Verbs of Motion
(b) Verb bases are transitive Verbs of Motion
(Transport Verbs)
(c) Verb bases express an Inclusion into an
Environment, Institution or Abstract Area
(d) Verb bases express Effects of Action
(eingravieren-Verbs)
Figure 2: Classification of particle verbs with two-
way prepositions
3.1 Group A
The verbs in Group A licence PPs in the accusative
and have a directional reading. Group A includes
Verbs of Motion, Verbs of Localisation, Transport
Verbs, and two further subgroups: verbs whose
meaning can be interpreted as a Direction of Per-
ception and verbs which express the Localisation
of a Change of State.
Verbs of Motion include einfahren ?to drive
into? or aufspringen ?to jump on? and can be de-
fined as follows: there is an X which undergoes
a change of location, whereby X is in a particu-
lar manner of motion and moves in the specified
direction into a not further specified neighbour re-
gion which is defined through the relatum.
Verbs of Localisation licencing PPs in the ac-
cusative are rather rare. Only one example is
attested in the corpus: einm?unden ?to discharge
into?. Here an X is described, which can be lo-
calised relativ to a Y in a particular direction. The
rarity of those verbs is probably due to the more
static character of localisation, which contradicts
the implicit directional reading of the accusative
case marking.
Transport Verbs such as eingie?en ?to pour
in?, einf?uhren ?to insert? and also verbs with more
metaphorical readings like einbinden (in die Kon-
59
ventionen einbinden, ?to weave sth into social con-
ventions?), can be defined in the following way:
there is an X which causes a change of location
for a Y, whereby Y is set into a particular manner
of motion and is moved in a specified direction.
Direction of Perception verbs include
einf?uhlen ?to empathise?, einsehen ?to see? or
einh?oren ?to listen?.
Localisation of a Change of State verbs in-
clude aufbl?ahen ?to bloat?, aufheizen ?to heat up?,
angleichen ?to conform to something? or aufrun-
den ?to round up?. Here the particle expresses the
direction to the changed, new state.
All particle verbs in Group A can be interpreted
as having a directional reading.
3.2 Group B
Particle Verbs in Group B licence pleonastic PPs
in the dative. They can be divided into the fol-
lowing subgroups: Verbs of Localisation, Verbs of
Movement and Position Verbs.
Verbs of Localisation also occur in Group A,
but here they have a static, non-directional inter-
pretation of localisation. Examples for this are
verbs like einquartieren ?to quarter?, anstehen ?to
queue?, auiegen ?to bear on? or zwischenlagern
?to store temporarily?.
(7) anPART stehen
(PART + to stand? to queue).
More formally they can be described as follows:
There is an X which is in a particular state (e.g.
in the state of standing) and can be localised in a
specific relation to a reference object.
Verbs of Motion include vorfahren ?to drive
up? or hinterherhecheln ?to pand after someone?.
They can be defined as follows: there is an X
which undergoes a change of location, whereby X
is in a particular manner of motion, moving into
the specified direction relative to the position of
the relatum. These verbs clearly include an im-
plicit direction, but in comparison to the Verbs of
Motion in Group A their reading allows for the
possibility that X is already in the same region as
the relatum, while the verbs in Group A describe
the intrusion of an X from the outside into a not
further specified neighbour region.
Verbs of Position include aufstellen ?to array?,
aufbahren ?to lay out? or hinterlegen ?to deposit?.
The definition states that there is an X which
causes a Y to change its position, whereby Y is
in a particular manner of motion, moving into a
specified direction. The focus hereby is not on the
movement but on the result of the event.
The verbs in Group B normaly have a nondi-
rectional, static interpretation, but they may also
allow for a directional interpretation, if theme ref-
erent and relatum are both positioned in the same
specified region (8).
(8) Sie stellt die Leiter [PP auf dem Podest] auf.
She put-3-Sg Det ladder [PP on Det platform] Part.
?She puts the ladder up on the platform.?
Here it is not the direction of a motion which is
described by the particle (the ladder may already
have been lying on the platform), but a change of
the orientation of the referent in relation to the re-
latum (the ladder has changed its orientation and
is in a more or less vertical position now).
3.3 Group C
Group C consists of particle verbs which can be
followed by a pleonastic PP in the accusative or
dative. The subgroups of Group C include Verbs
of Motion like einsickern ?to soak into?, ein-
marschieren ?to march in?, ansp?ulen ?to be washed
up? or vorladen ?to subpoena?, and Transport
Verbs such as aufh?angen ?to hang?, einschieben
?to insert?, einr?aumen ?to place in? or andocken ?to
dock?. Group C also consists of verbs which ex-
press an Inclusion into an Environment, Institu-
tion or Abstract Area like eingliedern ?to incor-
porate?, zwischenschalten ?to interpose?, aufrei-
hen ?to string? or auff?adeln ?to bead?. Another
verb group which belongs to Group C are verbs
which express the Localisation of Effects of Ac-
tion like einpr?agen ?to impress?, einbrennen ?to
burn-in?, eint?atowieren ?to tattoo? or aufdrucken
?to imprint?.
The following example illustrates the semantic
effect of the choice of case for the PP for the verbs
in Group C:
(9) sickert in die Erde ein
soak.3.Sg in Det.Acc soil PART
?soaks into the soil?
(10) sickert in der Erde ein
soak.3.Sg in Det.Dat soil PART
?soaks the soil?
Example (9) describes an event where an X
60
(rainwater) undergoes a directed motion during
which it enters into the region of the reference ob-
ject Y (the soil). In (10) the situation is different:
X is already located in the region of Y and now
is in the process of soaking through that region.
Figure 3 gives an illustration of the two examples.
Figure 3: Illustration of examples (9) and (10)
Characteristic for the verbs in Group C is their
directionality reading when going together with a
pleonastic PP in the accusative. When they are
combined with the dative, the particle still has its
directional character, but in contrast to the parti-
cle verbs in Group A the directionality does not
include an intrusion into another region but can
be interpreted as a movement inside of the region
given by the reference object.
Summarizing the results we can say that for
Group C the particle can have different functions
which influence the choice of case marking for the
PPs governed by the verb. If the particle has a
nondirectional reading, then only PPs in the dative
are allowed. If the particle expresses directional
information, then a further analysis is needed: it
has to be examined whether the semantics of the
particle verb includes the intrusion into a new re-
gion specified by the preposition. In this case the
PP has to be in the accusative. If the semantics of
the verb does not express an intrusion into a new
region, then the dative is chosen. Only particle
verbs whose semantics allow for a directional and
a locative interpretation belong to group C.
In Section 2 we noted that the semantics of the
verb particle is equivalent to the semantics of the
preposition, and that the PP specifies the implicit
reference object of the particle verb. However, this
is only true for PPs with accusative case marking.
The prepositions in PPs which are in the dative ex-
press a locative relation rather than a direction, so
their reference object can not be the same as the
one implicitly included in the verb particle. On the
syntactic level this results in them having a differ-
ent grammatical function: the accusative PP can
be considered as a verb complement, while the da-
tive PP is a free adjunct, modifying the informa-
tion of the verb particle. Therefore only accusative
PPs are ?pleonastic?.
4 Description of Particle Verbs with
Pleonastic Prepositions in LFG
This section will show how the framework of Lex-
ical Functional Grammar (LFG) can be used to
describe the particular behaviour of particle verbs
and pleonastic prepositions.
4.1 Short Introduction to LFG
LFG has a layer of representation for constituent
structure (c-structure), where surface information
is expressed through CFG trees, and a func-
tional layer (f-structure) for expressing grammat-
ical functions such as subject, object and adjunct.
In the f-structure each argument of a predicate is
assigned a particular grammatical function. This
two-level representation is based on the idea that
while surface representations may differ consider-
ably between various languages, f-structures tend
to be more abstract and invariant representations.
The correspondence between the two layers is
many-to-one: different nodes in the c-structure
may be associated with the same f-structure com-
ponent. The c-structure is determined by phrase
structure rules as in (11), while the annotation in
(12) links the c-structure categories to the corre-
sponding grammatical functions in the f-structure.
(11) S ? NP VP
(12) (? SUBJ)= ? ?=?
LFG is a non-transformational theory, syntactic
phenomena are treated locally through the specifi-
cation of rules and constraints in the lexicon.
4.2 Using LFG to Describe Particle Verbs
with Pleonastic Prepositions
The LFG formalisation developed here follows
and substantially extends the treatment of particle
verbs and prepositional phrases in the LFG gram-
mar for German in (Berman and Frank, 1996) and
(Butt, King, Nin?o and Segond, 1999).
4.2.1 Berman & Frank (1996)
Figure 4 shows the lexical entry for the German
particle verb einfahren ?to drive into? as described
in (Berman and Frank, 1996).
61
fahren V
(? PRED)=?EINFAHREN<?SUBJ), (? OBL DIR)>?
(? FORM)=c EIN
(? VERBTYPE)=PARTICLE VERB
...
ein PART
(? FORM)=EIN
Figure 4: Lexical entry for einfahren ?to drive in?
(Berman and Frank, 1996)
The predicate (PRED) shows the argument
structure of the verb, while the attribute VERB-
TYPE explicitly describes the verb as a particle
verb. The FORM attribute contains the lexical
form of the particle and is formulated as a con-
straint (=c) to check that the particle is lexically
filled. The particle itself has no PRED value of its
own but is analysed as part of the complex verb.
German prepositional phrases can either occur
as prepositional objects or as adjuncts. Accord-
ing to Berman and Frank (1996) the second group
is further subdivided into adjuncts which are sub-
categorized by the verb and free adjuncts. Accord-
ingly, in the analysis of (Berman and Frank, 1996),
each two-way preposition has three lexical entries.
In their analysis, prepositional objects are gov-
erned by the verb and have no PRED attribute of
their own. The lexical form of the preposition and
also its case are determined by the verb. The value
of the PCASE attribute is assigned the lexical form
of the preposition, while the preposition is not able
to subcategorize an object.2
As for adjuncts subcategorized by the verb no
particular preposition is selected in (Berman and
Frank, 1996), but the verb determines the seman-
tic content of the preposition (eg: LOC, DIR). The
preposition has its own PRED attribute and sub-
categorizes an object (Figure 5).
auf P
(? PRED)=?LOC<?OBJ)>?
(? PCASE)=LOC
(? PDET)=-.
Figure 5: Lexical entry (Berman & Frank, 1996)
for preposition auf ?on? (adjunct subcategorized
by the verb)
Free adjuncts on the other hand must have the
2Prepositional objects are of no concern here, because the
paper deals with spatial prepositions which always have a se-
mantic content.
semantic content LOC. Like the first type of ad-
juncts they have their own PRED attribute and
subcategorize an object, but their semantic content
is defined by the ROLE attribute (Figure 6).
auf P
(? PRED)=?OBL LOCAL<?OBJ)>?
(? ROLE)=LOCAL
(? OBJ AGR CAS GOV)=+
(? OBJ AGR CAS OBL)=+
(? PDET)=-.
Figure 6: Lexical entry (Berman & Frank, 1996)
for auf ?on? (free adjunct)
4.2.2 Formalisation of Group C Verbs
We concentrate on the formalisation of the par-
ticle verbs in Group C which can either licence a
pleonastic PP in the accusative or a PP in the da-
tive. Extending the analysis in (Berman and Frank,
1996) we provide two f-structure configurations,
depending on the case of the governed PP.
Figure 7 shows the f-structure for example (9).
Here the pleonastic PP in the accusative saturates
the argument OBL DIR subcategorized by the par-
ticle verb. Figure 8 gives the f-structure for exam-
ple (10), where the particle verb combines with a
dative PP.
?
??????????
PRED einsickern < SUBJ,OBL DIR >
OBL DIR
?
??????
PRED in < OBJ >
PART? FORM ein
PCASE DIR
PSEM +
OBJ
[ PRED Erde
SPEC die
CASE acc
]
?
??????
?
??????????
Figure 7: sickert [ PP in die Erde ]ACC ein
?soaks into the soil?
In contrast to Figure 7 the dative PP in Figure
8 does not contribute any information to the argu-
ment OBL DIR subcategorized by the verb but is
represented in the adjunct set. The verb particle
saturizes the OBL DIR argument, and the PRED
attribute of the object of OBL DIR is assigned the
value PRO. This enables the PRED value to be-
have like a variable which can be unified with any
other value as in Figure 8, where both the particle
and the pleonastic prepositional phrases add infor-
mation to OBL DIR:OBJ:PRED.
62
?
????????????????????
PRED einsickern < SUBJ,OBL DIR >
OBL DIR
?
?????
PRED in < OBJ >
PART? FORM ein
PCASE DIR
PSEM +
OBJ
[
PRED PRO
CASE acc
]
?
?????
ADJ
?
?????
?????
?
?????
PRED in < OBJ >
PCASE LOC
PSEM +
OBJ
[ PRED Erde
SPEC der
CASE dat
]
?
?????
?
?????
?????
?
????????????????????
Figure 8: sickert [ PP in der Erde ]DAT ein
?soaks (through) the soil?
4.2.3 Lexical Entries and Grammar Rules
In the f-structure in Figure 7 the pleonastic PP
is subcategorized by the particle verb. Figure 9
shows the corresponding lexical entry for the verb.
To prevent a locative PP in the dative from fill-
ing in the object position of the verb argument the
lexical entry specifies that the object has to be as-
signed accusative case.
einsickern V
(? PRED) = ?einsickern<(? SUBJ, ? OBL DIR)>?
(? OBL DIR:PART-FORM) = ein
(? OBL DIR:OBJ:CASE) = acc
Figure 9: Lexical entry for einsickern ?to soak?
However, as shown in example (4) the pleonas-
tic PP can be omitted. In this case the argument
OBL DIR subcategorized by the particle verb is
provided by the particle ein- whose lexical entry is
given in Figure 10.
ein PART
(? PRED) = ?in<(? OBJ)>?
(? PART-FORM) = ein
(? PCASE) = DIR
(? PSEM) = +
(? OBJ PRED ) = PRO
Figure 10: Lexical entry for the particle ein
In contrast to (Berman and Frank, 1996), in our
representation the particle is assigned the PRED
value ?in? in the lexicon. The cause for the diver-
gence between the lexical form of the particle and
its PRED value is due to the fact that the particle
ein- historically is derived from the preposition in
and regarding its semantic features is comparable
to the other two-way prepositions where particle
and preposition have the same lexical form.
The attributes PSEM and PCASE are added to
the representation of the verb particles in Berman
and Frank (1996). They are derived from the at-
tribute set for prepositions, indicating the anal-
ogy in the semantics of particle and preposition.
PSEM always has the value ?+? for particle verbs
formed by spatial prepositions, because they al-
ways have a semantic content. The attribute
PCASE expresses the directionality in the seman-
tics of the verb particle ( (? PCASE) = DIR).
The predicate of the particle licences an object
and behaves like a directional preposition. How-
ever, the object position is not lexically filled and
therefore is assigned the predicate value ?PRO?.
We also want to model the behaviour of the par-
ticle verb governing a locative PP in the dative
(Figure 8). The lexical entry of the particle verb
(Figure 9) explicitly requires accusative case as-
signment and prevents the locative dative PP from
filling in the object position of the verb argument.
The locative dative PP is attached to the adjunct
set in the grammar rule shown in Figure 11.3
VP? V ?=?
PP * ? ? ( ? ADJ)
(? OBJ CASE) 6= acc
(PP (? OBL DIR) = ?)
PART (? OBL DIR) = ?.
Figure 11: Grammar Rule specifying restrictions
on particle verbs with pleonastic PPs
The first PP in the grammar rule models the be-
haviour of a particle verb combining with one or
more locative PPs in the dative. The constraint
(? OBJ CASE) 6= acc ensures that this part of the
rule will not be applied to a pleonastic PP with ac-
cusative case assignment.4
The second PP in the grammar rule captures a
pleonastic PP in the accusative. The restriction
that this PP has to be in the accusative is specified
in the lexical entry for the particle verb (Figure
10). The last part of the rule expresses that the verb
particle PART is also mapped to the OBL DIR ar-
3For expository purposes we use a simple VP rather than
a topological analysis.
4The Kleene * notation indicates zero or more occurences
of PP.
63
gument of the complex verb and so is able to satu-
rate the argument structure of the verb.
The formalisation in Figure 8 and 9 is consistent
with the analysis that the particle has an implicit
reference object which is identical to the object of
a pleonastic PP in the accusative, but not to the
object of a dative PP. The formalisation gives an
adequate description of the behaviour of particle
verbs in Group C, but it does not suppress the li-
cencing of a pleonastic accusative PP for verbs in
Group B which combine with locative PPs in the
dative only. This problem is solved through the
specification of a constraint (=c) in the lexical en-
tries for all particle verbs in Group B (Figure 12).
vorfahren V
(? PRED) = ?vorfahren<(? SUBJ, ? OBL DIR)>?
(? OBL DIR:PART-FORM) = vor
(? OBL DIR:OBJ:CASE) = acc
(? OBL DIR:OBJ:PRED) =c PRO
Figure 12: Lexical entry for vorfahren ?to drive
up? (Group B)
The constraint checks that the predicate of the
object in the OBL DIR f-structure is instantiated
with the value ?PRO?. For all cases where the pred-
icate is lexically realised, the constraint fails and
thus the interpretation of pleonastic accusative PPs
in the OBL DIR position for Group B verbs is sup-
pressed.
5 Conclusions
The aim of this paper is to explain the behaviour of
German particle verbs formed by two-way prepo-
sitions and their ability to combine with pleonastic
PPs. A classification of particle verbs based on se-
mantic criteria was given, illustrating the restric-
tions imposed on their behaviour. It was shown
that particle verbs occurring only with accusative
PPs (Group A) always have a directional read-
ing including the intrusion of the theme referent
into a region specified by the relatum. Particle
verbs which can not combine with an accusative
PP (Group B) either have a static, nondirectional
reading or describe a directed movement where
the referent already may be present in the region
specified by the relatum.
Syntactically this results in the fact that the
accusative PP is able to saturate the argument
OBL DIR subcategorized by the particle verbs in
Group A. The dative PP functions as an adjunct
(Group B). Here the verb particle saturates the di-
rectional OBL DIR argument required by the verb.
Group C verbs allow both accusative and dative
PPs. Only particle verbs governing PPs in the ac-
cusative are pleonastic, but the PP either modifies
or adds new information to the inherent argument
structure of the particle verb and therefore is not
suppressed by the verb particle.
Our formalisation describes the behaviour of
particle verbs concerning their ability to licence
pleonastic PPs. The semantic criteria restricting
the behaviour of the particle verbs are embed-
ded into the LFG representation and enable us
to model the semantic differences on a syntactic
level.
References
Judith Berman and Anette Frank. 1996. Deutsche und
franzo?sische Syntax im Formalismus der LFG. Max
Niemeyer Verlag, Tu?bingen.
Joan Bresnan. 2000. Lexical-Functional Syntax.
Blackwell.
Miriam Butt, Tracy Holloway King, Mar??a-Eugenia
Nino and Fre?de?rique Segond. 1999. A Grammar
Writer?s Cookbook. CSLI Publications, Stanford,
California.
Mary Dalrymple. 2001. Syntax and Semantics. Lexical
Functional Grammar, volume 34. Academic Press,
San Diego, California.
Junji Okamoto. 2002. Particle-Bound Directions in
German Particle Verb Constructions. Projektbericht
V: Typological Investigation of Languages and Cul-
tures of the East and West. (Part II).
Susan Olsen. 1998. Semantische und konzeptuelle
Aspekte der Partikelverbbildung mit ein-. Stauffen-
burg, Tu?bingen.
James Witt. 1998. Kompositionalita?t und Regularita?t,
In: Olsen, Susan (ed). Semantische und konzeptuelle
Aspekte der Partikelverbbildung mit ein-. Stauffen-
burg, Tu?bingen.
Dieter Wunderlich. 1983. On the Compositionality of
German Prefix Verbs. In: R. Ba?uerle, Ch. Schwarze
and A. von Stechow (eds.) Meaning, Use and Inter-
pretation of Language. de Gruyter, Berlin.
64
Proceedings of the Third Linguistic Annotation Workshop, ACL-IJCNLP 2009, pages 19?26,
Suntec, Singapore, 6-7 August 2009. c?2009 ACL and AFNLP
Assessing the benefits of partial automatic pre-labeling for frame-semantic
annotation
Ines Rehbein and Josef Ruppenhofer and Caroline Sporleder
Computational Linguistics
Saarland University
{rehbein,josefr,csporled}@coli.uni-sb.de
Abstract
In this paper, we present the results of an
experiment in which we assess the useful-
ness of partial semi-automatic annotation
for frame labeling. While we found no con-
clusive evidence that it can speed up human
annotation, automatic pre-annotation does
increase its overall quality.
1 Introduction
Linguistically annotated resources play a crucial
role in natural language processing. Many recent
advances in areas such as part-of-speech tagging,
parsing, co-reference resolution, and semantic role
labeling have only been possible because of the cre-
ation of manually annotated corpora, which then
serve as training data for machine-learning based
NLP tools. However, human annotation of linguis-
tic categories is time-consuming and expensive.
While this is already a problem for major languages
like English, it is an even bigger problem for less-
used languages.
This data acquisition bottleneck is a well-known
problem and there have been numerous efforts to
address it on the algorithmic side. Examples in-
clude the development of weakly supervised learn-
ing methods such as co-training and active learning.
However, addressing only the algorithmic side is
not always possible and not always desirable in all
scenarios. First, some machine learning solutions
are not as generally applicable or widely re-usable
as one might think. It has been shown, for example,
that co-training does not work well for problems
which cannot easily be factorized into two indepen-
dent views (Mueller et al, 2002; Ng and Cardie,
2003). Some active learning studies suggest both
that the utility of the selected examples strongly
depends on the model used for classification and
that the example pool selected for one model can
turn out to be sub-optimal when another model is
trained on it at a later stage (Baldridge and Os-
borne, 2004). Furthermore, there are a number of
scenarios for which there is simply no alternative
to high-quality, manually annotated data; for exam-
ple, if the annotated corpus is used for empirical
research in linguistics (Meurers and Mu?ller, 2007;
Meurers, 2005).
In this paper, we look at this problem from the
data creation side. Specifically we explore whether
a semi-automatic annotation set-up in which a hu-
man expert corrects the output of an automatic sys-
tem can help to speed up the annotation process
without sacrificing annotation quality.
For our study, we explore the task of frame-
semantic argument structure annotation (Baker et
al., 1998). We chose this particular task because it
is a rather complex ? and therefore time-consuming
? undertaking, and it involves making a number of
different but interdependent annotation decisions
for each instance to be labeled (e.g. frame as-
signment and labeling of frame elements, see Sec-
tion 3.1). Semi-automatic support would thus be of
real benefit.
More specifically, we explore the usefulness of
automatic pre-annotation for the first step in the an-
notation process, namely frame assignment (word
sense disambiguation). Since the available inven-
tory of frame elements is dependent on the cho-
sen frame, this step is crucial for the whole anno-
tation process. Furthermore, semi-automatic an-
notation is more feasible for the frame labeling
sub-task. Most automatic semantic role labeling
systems (ASRL), including ours, tend to perform
much better on frame assignment than on frame
role labeling and correcting an erroneously chosen
19
frame typically also requires fewer physical opera-
tions from the annotator than correcting a number
of wrongly assigned frame elements.
We aim to answer three research questions in our
study: First, we explore whether pre-annotation of
frame labels can indeed speed up the annotation
process. This question is important because frame
assignment, in terms of physical operations of the
annotator, is a relatively minor effort compared to
frame role assignment and because checking a pre-
annotated frame still involves all the usual men-
tal operations that annotation from scratch does.
Our second major question is whether annotation
quality would remain acceptably high. Here the
concern is that annotators might tend to simply go
along with the pre-annotation, which would lead to
an overall lower annotation quality than they could
produce by annotating from scratch.1 Depending
on the purpose for which the annotations are to be
used, trading off accuracy for speed may or may
not be acceptable. Our third research question con-
cerns the required quality of pre-annotation for it
to have any positive effect. If the quality is too low,
the annotation process might actually be slowed
down because annotations by the automatic system
would have to be deleted before the new correct
one could be made. In fact, annotators might ig-
nore the pre-annotations completely. To determine
the effect of the pre-annotation quality, we not only
compared a null condition of providing no prior
annotation to one where we did, but we in fact com-
pared the null condition to two different quality
levels of pre-annotation, one that reflects the per-
formance of a state-of-the-art ASRL system and
an enhanced one that we artificially produced from
the gold standard.
2 Related Work
While semi-automatic annotation is frequently em-
ployed to create labeled data more quickly (see,
e.g., Brants and Plaehn (2000)), there are compar-
atively few studies which systematically look at
the benefits or limitations of this approach. One
of the earliest studies that investigated the advan-
tages of manually correcting automatic annotations
for linguistic data was carried out by Marcus et
al. (1993) in the context of the construction of the
Penn Treebank. Marcus et al (1993) employed
1This problem is also known in the context of resources
that are collaboratively constructed via the web (Kruschwitz
et al, 2009)
a post-correction set-up for both part-of-speech
and syntactic structure annotation. For pos-tagging
they compared the semi-automatic approach to a
fully manual annotation. They found that the semi-
automatic method resulted both in a significant
reduction of annotation time, effectively doubling
the word annotation rate, and in increased inter-
annotator agreement and accuracy.
Chiou et al (2001) explored the effect of au-
tomatic pre-annotation for treebank construction.
For the automatic step, they experimented with two
different parsers and found that both reduce over-
all annotation time significantly while preserving
accuracy. Later experiments by Xue et al (2002)
confirmed these findings.
Ganchev et al (2007) looked at semi-automatic
gene identification in the biomedical domain. They,
too, experimented with correcting the output of an
automatic annotation system. However, rather than
employing an off-the-shelf named entity tagger,
they trained a tagger maximized for recall. The
human annotators were then instructed to filter the
annotation, rejecting falsely labeled expressions.
Ganchev et al (2007) report a noticeable increase
in speed compared to a fully manual set-up.
The approach that is closest to ours is that of
Chou et al (2006) who investigate the effect of au-
tomatic pre-annotation for Propbank-style semantic
argument structure labeling. However that study
only looks into the properties of the semi-automatic
set-up; the authors did not carry out a control study
with a fully manual approach. Nevertheless Chou
et al (2006) provide an upper bound of the savings
obtained by the semi-automatic process in terms
of annotator operations. They report a reduction in
annotation effort of up to 46%.
3 Experimental setup
3.1 Frame-Semantic Annotation
The annotation scheme we use is that of FrameNet
(FN), a lexicographic project that produces a
database of frame-semantic descriptions of English
vocabulary. Frames are representations of proto-
typical events or states and their participants in the
sense of Fillmore (1982). In the FN database, both
frames and their participant roles are arranged in
various hierarchical relations (most prominently,
the is-a relation).
FrameNet links these descriptions of frames with
the words and multi-words (lexical units, LUs) that
evoke these conceptual structures. It also docu-
20
ments all the ways in which the semantic roles
(frame elements, FEs) can be realized as syntactic
arguments of each frame-evoking word by labeling
corpus attestations. As a small example, consider
the Collaboration frame, evoked in English by lexi-
cal units such as collaborate.v, conspire.v, collabo-
rator.n and others. The core set of frame-specific
roles that apply include Partner1, Partner2, Partners
and Undertaking. A labeled example sentence is
(1) [The two researchers Partners] COLLAB-
ORATED [on many papers Undertaking].
FrameNet uses two modes of annotation: full-
text, where the goal is to exhaustively annotate
the running text of a document with all the differ-
ent frames and roles that occur, and lexicographic,
where only instances of particular target words used
in particular frames are labeled.
3.2 Pilot Study
Prior to the present study we carried out a pilot
experiment comparing manual and semi-automatic
annotation of different segments of running text.
In this experiment we saw no significant effect
from pre-annotation. Instead we found that the
annotation speed and accuracy depended largely
on the order in which the texts were annotated and
on the difficulty of the segments. The influence
of order is due to the fact that FrameNet has more
than 825 frames and each frame has around two to
five core frame elements plus a number of non-core
elements. Therefore even experienced annotators
can benefit from the re-occuring of frames during
the ongoing annotation process.
Drawing on our experiences with the first exper-
iment, we chose a different experimental set-up for
the present study. To reduce the training effect, we
opted for annotation in lexicographic mode, restrict-
ing the number of lemmas (and thereby frames)
to annotate, and we started the experiment with
a training phase (see Section 3.5). Annotating in
lexicographic mode also gave us better control over
the difficulty of the different batches of data. Since
these now consist of unrelated sentences, we can
control the distribution of lemmas across the seg-
ments (see Section 3.4).
Furthermore, since the annotators in our pi-
lot study had often ignored the error-prone pre-
annotation, in particular for frame elements, we de-
cided not to pre-annotate frame elements and to ex-
periment with an enhanced level of pre-annotation
to explore the effect of pre-annotation quality.
3.3 Annotation Set-Up
The annotators included the authors and three com-
putational linguistics undergraduates who have
been performing frame-semantic annotation for at
least one year. While we use FrameNet data, our
annotation set-up is different. The annotation con-
sists of decorating automatically derived syntactic
constituency trees with semantic role labels using
the Salto tool (Burchardt et al, 2006) (see Figure 1).
By contrast, in FrameNet annotation a chunk parser
is used to provide phrase type and grammatical rela-
tions for the arguments of the target words. Further,
FrameNet annotators need to correct mistakes of
the automatic grammatical analysis, unlike in our
experiment. The first annotation step, frame as-
signment, involves choosing the correct frame for
the target lemma from a pull down menu; the sec-
ond step, role assignment, requires the annotators
to draw the available frame element links to the
appropriate syntactic constituent(s).
The annotators performed their annotation on
computers where access to the FrameNet website,
where gold annotations could have been found, was
blocked. They did, however, have access to local
copies of the frame descriptions needed for the
lexical units in our experiment. As the overall time
needed for the annotation was too long to do in
one sitting, the annotators did it over several days.
They were instructed to record the time (in minutes)
that they took for the annotation of each annotation
session.
Our ASRL system for state-of-the-art pre-
annotation was Shalmaneser (Erk and Pado, 2006).
The enhanced pre-annotation was created by man-
ually inserting errors into the gold standard.
3.4 Data
We annotated 360 sentences exemplifying all the
senses that were defined for six different lemmas in
FrameNet release 1.3. The lemmas were the verbs
rush, look, follow, throw, feel and scream. These
verbs were chosen for three reasons. First, they
have enough annotated instances in the FN release
that we could use some instances for testing and
still be left with a set of instances sufficiently large
to train our ASRL system. Second,we knew from
prior work with our automatic role labeler that it
had a reasonably good performance on these lem-
mas. Third, these LUs exhibit a range of difficulty
in terms of the number of senses they have in FN
(see Table 1) and the subtlety of the sense distinc-
21
Figure 1: The Salto Annotation Tool
Instances Senses
feel 134 6
follow 113 3
look 185 4
rush 168 2
scream 148 2
throw 155 2
Table 1: Lemmas used
tions ? e.g. the FrameNet senses of look are harder
to distinguish than those of rush. We randomly
grouped our sentences into three batches of equal
size and for each batch we produced three versions
corresponding to our three levels of annotation.
3.5 Study design
In line with the research questions that we want
to address and the annotators that we have avail-
able, we choose an experimental design that is
amenable to an analysis of variance. Specifically,
we randomly assign our 6 annotators (1-6) to three
groups of two (Groups I-III). Each annotator expe-
riences all three annotation conditions, namely no
pre-annotation (N), state-of-the-art pre-annotation
(S), and enhanced pre-annotation (E). This is the
within-subjects factor in our design, all other fac-
tors are between subjects. Namely, each group was
randomly matched to one of three different orders
in which the conditions can be experienced (see
Table 2). The orderings are designed to control
for the effects that increasing experience may have
on speed and quality. While all annotators end up
labeling all the same data, the groups also differ
as to which batch of data is presented in which
condition. This is intended as a check on any inher-
1st 2nd 3rd Annotators
Group I E S N 5, 6
Group II S N E 2, 4
Group III N E S 1, 3
Table 2: Annotation condition by order and group
ent differences in annotation difficulty that might
exist between the data sets. Finally, to rule out
difficulties with unfamiliar frames and frame el-
ements needed for the lexical units used in this
study, we provided some training to the annota-
tors. In the week prior to the experiment, they were
given 240 sentences exemplifying all 6 verbs in all
their senses to annotate and then met to discuss any
questions they might have about frame or FE dis-
tinctions etc. These 240 sentences were also used
to train the ASRL system.
4 Results
In addition to time, we measured precision, recall
and f-score for frame assignment and semantic role
assignment for each annotator. We then performed
an analysis of variance (ANOVA) on the outcomes
of our experiment. Our basic results are presented
in Table 3. As can be seen and as we expected,
our annotators differed in their performance both
with regard to annotation quality and speed. Below
we discuss our results with respect to the research
questions named above.
4.1 Can pre-annotation of frame assignment
speed up the annotation process?
Not surprisingly, there are considerable differences
in speed between the six annotators (Table 3),
22
Precision Recall F t p
Annotator 1
94/103 91.3 94/109 86.2 88.68 75 N
99/107 92.5 99/112 88.4 90.40 61 E
105/111 94.6 105/109 96.3 95.44 65 S
Annotator 2
93/105 88.6 93/112 83.0 85.71 135 S
86/98 87.8 86/112 76.8 81.93 103 N
98/106 92.5 98/113 86.7 89.51 69 E
Annotator 3
95/107 88.8 95/112 84.8 86.75 168 N
103/110 93.6 103/112 92.0 92.79 94 E
99/113 87.6 99/113 87.6 87.60 117 S
Annotator 4
106/111 95.5 106/112 94.6 95.05 80 S
99/108 91.7 99/113 87.6 89.60 59 N
105/112 93.8 105/113 92.9 93.35 52 E
Annotator 5
104/110 94.5 (104/112) 92.9 93.69 170 E
91/103 88.3 (91/113) 80.5 84.22 105 S
96/100 96.0 (96/113) 85.0 90.17 105 N
Annotator 6
102/106 96.2 102/112 91.1 93.58 124 E
94/105 89.5 94/112 83.9 86.61 125 S
93/100 93.0 93/113 82.3 87.32 135 N
Table 3: Results for frame assignment: precision,
recall, f-score (F), time (t) (frame and role as-
signment), pre-annotation (p): Non, Enhanced,
Shalmaneser
which are statistically significant with p ? 0.05.
Focussing on the order in which the text segments
were given to the annotators, we observe a sig-
nificant difference (p ? 0.05) in annotation time
needed for each of the segments. With one ex-
ception, all annotators took the most time on the
text segment given to them first, which hints at an
ongoing training effect.
The different conditions of pre-annotation (none,
state-of-the-art, enhanced) did not have a signifi-
cant effect on annotation time. However, all anno-
tators except one were in fact faster under the en-
hanced condition than under the unannotated con-
dition. The one annotator who was not faster anno-
tated the segment with the enhanced pre-annotation
before the other two segments; hence there might
have been an interaction between time savings from
pre-annotation and time savings due to a training
effect. This interaction between training effect and
degree of pre-annotation might be one reason why
we do not find a significant effect between anno-
tation time and pre-annotation condition. Another
reason might be that the pre-annotation only re-
duces the physical effort needed to annotate the
correct frame which is relatively minor compared
to the cognitive effort of determining (or verifying)
the right frame, which is required for all degrees of
pre-annotation.
4.2 Is annotation quality influenced by
automatic pre-annotation?
To answer the second question, we looked at the
relation between pre-annotation condition and f-
score. Even though the results in f-score for the
different annotators vary in extent (Table 4), there is
no significant difference between annotation qual-
ity for the six annotators.
Anot1 Anot2 Anot3 Anot4 Anot5 Anot6
91.5 85.7 89.0 92.7 89.4 89.2
Table 4: Average f-score for the 6 annotators
Next we performed a two-way ANOVA (Within-
Subjects design), and crossed the dependent vari-
able (f-score) with the two independent vari-
ables (order of text segments, condition of pre-
annotation). Here we found a significant effect
(p ? 0.05) for the impact of pre-annotation on an-
notation quality. All annotators achieved higher
f-scores for frame assignment on the enhanced pre-
annotated text segments than on the ones with no
pre-annotation. With one exception, all annotators
also improved on the already high baseline for the
enhanced pre-annotation (Table 5).
Seg. Precision Recall f-score
Shalmaneser
A (70/112) 62.5 (70/96) 72.9 67.30
B (75/113) 66.4 (75/101) 74.3 70.13
C (66/113) 58.4 (66/98) 67.3 62.53
Enhanced Pre-Annotation
A (104/112) 92.9 (104/111) 93.7 93.30
B (103/112) 92.0 (103/112) 92.0 92.00
C (99/113) 87.6 (99/113) 87.6 87.60
Table 5: Baselines for automatic pre-annotation
(Shalmaneser) and enhanced pre-annotation
The next issue concerns the question of whether
annotators make different types of errors when pro-
vided with the different styles of pre-annotation.
We would like to know if erroneous frame assign-
ment, as done by a state-of-the-art ASRL will tempt
annotators to accept errors they would not make in
the first place. To investigate this issue, we com-
pared f-scores for each of the frames for all three
pre-annotation conditions with f-scores for frame
assignment achieved by Shalmaneser. The boxplot
in Figure 2 shows the distribution of f-scores for
each frame for the different pre-annotation styles
and for Shalmaneser. We can see that the same
23
Figure 2: F-Scores per frame for human annotators on different levels of pre-annotation and for Shal-
maneser
error types are made by human annotators through-
out all three annotation trials, and that these errors
are different from the ones made by the ASRL.
Indicated by f-score, the most difficult frames
in our data set are Scrutiny, Fluidic motion, Seek-
ing, Make noise and Communication noise. This
shows that automatic pre-annotation, even if noisy
and of low quality, does not corrupt human anno-
tators on a grand scale. Furthermore, if the pre-
annotation is good it can even improve the overall
annotation quality. This is in line with previous
studies for other annotation tasks (Marcus et al,
1993).
4.3 How good does pre-annotation need to be
to have a positive effect?
Comparing annotation quality on the automatically
pre-annotated texts using Shalmaneser, four out of
six annotators achieved a higher f-score than on the
non-annotated sentences. The effect, however, is
not statistically significant. This means that pre-
annotation produced by a state-of-the-art ASRL
system is not yet good enough a) to significantly
speed up the annotation process, and b) to improve
the quality of the annotation itself. On the positive
side, we also found no evidence that the error-prone
pre-annotation decreases annotation quality.
Most interestingly, the two annotators who
showed a decrease in f-score on the text segments
pre-annotated by Shalmaneser (compared to the
text segments with no pre-annotation provided)
had been assigned to the same group (Group I).
Both had first annotated the enhanced, high-quality
pre-annotation, in the second trial the sentences
pre-annotated by Shalmaneser, and finally the texts
with no pre-annotation. It might be possible that
they benefitted from the ongoing training, resulting
in a higher f-score for the third text segment (no
pre-annotation). For this reason, we excluded their
annotation results from the data set and performed
another ANOVA, considering the remaining four
annotators only.
Figure 3 illustrates a noticeable trend for the in-
teraction between pre-annotation and annotation
quality: all four annotators show a decrease in
annotation quality on the text segments without
pre-annotation, while both types of pre-annotation
(Shalmaneser, Enhanced) increase f-scores for hu-
man annotation. There are, however, differences
between the impact of the two pre-annotation types
on human annotation quality: two annotators show
better results on the enhanced, high-quality pre-
24
Figure 3: Interaction between pre-annotation and
f-score
annotation, the other two perform better on the
texts pre-annotated by the state-of-the-art ASRL.
The interaction between pre-annotation and f-score
computed for the four annotators is weakly signifi-
cant with p ? 0.1.
Next we investigated the influence of pre-
annotation style on annotation time for the four
annotators. Again we can see an interesting pat-
tern: The two annotators (A1, A3) who annotated
in the order N-E-S, both take most time for the
texts without pre-annotation, getting faster on the
text pre-processed by Shalmaneser, while the least
amount of time was needed for the enhanced pre-
annotated texts (Figure 4). The two annotators (A2,
A4) who processed the texts in the order S-N-E,
showed a continuous reduction in annotation time,
probably caused by the interaction of training and
data quality. These observations, however, should
be taken with a grain of salt, as they outline trends,
but due to the low number of annotators, could not
be substantiated by statistical tests.
4.4 Semantic Role Assignment
As described in Section 3.5, we provided pre-
annotation for frame assignment only, therefore
we did not expect any significant effects of the dif-
ferent conditions of pre-annotation on the task of
semantic role labeling. To allow for a meaningful
Figure 4: Interaction between pre-annotation and
time
comparison, the evaluation of semantic role assign-
ment was done on the subset of frames annotated
correctly by all annotators.
As with frame assignment, there are consid-
erable differences in annotation quality between
the annotators. In contrast to frame assignment,
here the differences are statistically significant
(p ? 0.05). Table 6 shows the average f-score
for each annotator on the semantic role assignment
task.
Anot1 Anot2 Anot3 Anot4 Anot5 Anot6
85.2 80.1 87.7 89.2 82.5 84.3
Table 6: Average f-scores for the 6 annotators
As expected, neither the condition of pre-
annotation nor the order of text segments had any
significant effect on the quality of semantic role
assignment.2
5 Conclusion and future work
In the paper we presented experiments to assess
the benefits of partial automatic pre-annotation on
a frame assignment (word sense disambiguation)
task. We compared the impact of a) pre-annotations
2The annotation of frame and role assignment was done as
a combined task, therefore we do not report separate results
for annotation time for semantic role assignment.
25
provided by a state-of-the-art ASRL, and b) en-
hanced, high-quality pre-annotation on the annota-
tion process. We showed that pre-annotation has
a positive effect on the quality of human annota-
tion: the enhanced pre-annotation clearly increased
f-scores for all annotators, and even the noisy, error-
prone pre-annotations provided by the ASRL sys-
tem did not lower the quality of human annotation.
We suspect that there is a strong interaction
between the order in which the text segments
are given to the annotators and the three annota-
tion conditions, resulting in lower f-scores for the
group of annotators who processed the ASRL pre-
annotations in the first trial, where they could not
yet profit from the same amount of training as the
other two groups.
The same problem occurs with annotation time.
We have not been able to show that automatic
pre-annotation speeds up the annotation process.
However, we suspect that here, too, the interaction
between training effect and annotation condition
made it difficult to reach a significant improve-
ment. One way to avoid the problem would be a
further split of the test data, so that the different
types of pre-annotation could be presented to the
annotators at different stages of the annotation pro-
cess. This would allow us to control for the strong
bias through incremental training, which we can-
not avoid if one group of annotators is assigned
data of a given pre-annotation type in the first trial,
while another group encounters the same type of
data in the last trial. Due to the limited number
of annotators we had at our disposal as well as
the amount of time needed for the experiments we
could not sort out the interaction between order
and annotation conditions. We will take this issue
up in future work, which also needs to address the
question of how good the automatic pre-annotation
should be to support human annotation. F-scores
for the enhanced pre-annotation provided in our
experiments were quite high, but it is possible that
a similar effect could be reached with automatic
pre-annotations of somewhat lower quality.
The outcome of our experiments provides strong
motivation to improve ASRL systems, as automatic
pre-annotation of acceptable quality does increase
the quality of human annotation.
References
C. F. Baker, C. J. Fillmore, J. B. Lowe. 1998. The
berkeley framenet project. In Proceedings of the
17th international conference on Computational lin-
guistics, 86?90, Morristown, NJ, USA. Association
for Computational Linguistics.
J. Baldridge, M. Osborne. 2004. Active learning
and the total cost of annotation. In Proceedings of
EMNLP.
T. Brants, O. Plaehn. 2000. Interactive corpus annota-
tion. In Proceedings of LREC-2000.
A. Burchardt, K. Erk, A. Frank, A. Kowalski, S. Pado?.
2006. SALTO ? a versatile multi-level annotation
tool. In Proceedings of LREC.
F.-D. Chiou, D. Chiang, M. Palmer. 2001. Facilitat-
ing treebank annotation using a statistical parser. In
Proceedings of HLT-2001.
W.-C. Chou, R. T.-H. Tsai, Y.-S. Su, W. Ku, T.-Y. Sung,
W.-L. Hsu. 2006. A semi-automatic method for an-
notation a biomedical proposition bank. In Proceed-
ings of FLAC-2006.
K. Erk, S. Pado. 2006. Shalmaneser - a flexible tool-
box for semantic role assignment. In Proceedings of
LREC, Genoa, Italy.
C. J. F. C. J. Fillmore, 1982. Linguistics in the Morning
Calm, chapter Frame Semantics, 111?137. Hanshin
Publishing, Seoul, 1982.
K. Ganchev, F. Pereira, M. Mandel, S. Carroll, P. White.
2007. Semi-automated named entity annotation.
In Proceedings of the Linguistic Annotation Work-
shop, 53?56, Prague, Czech Republic. Association
for Computational Linguistics.
U. Kruschwitz, J. Chamberlain, M. Poesio. 2009. (lin-
guistic) science through web collaboration in the
ANAWIKI project. In Proceedings of WebSci?09.
M. P. Marcus, B. Santorini, M. A. Marcinkiewicz.
1993. Building a large annotated corpus of En-
glish: The Penn Treebank. Computational Linguis-
tics, 19(2):313?330.
W. D. Meurers, S. Mu?ller. 2007. Corpora and syntax
(article 44). In A. Lu?deling, M. Kyto?, eds., Corpus
linguistics. Mouton de Gruyter, Berlin.
W. D. Meurers. 2005. On the use of electronic cor-
pora for theoretical linguistics. case studies from
the syntax of german. Lingua, 115(11):1619?
1639. http://purl.org/net/dm/papers/
meurers-03.html.
C. Mueller, S. Rapp, M. Strube. 2002. Applying co-
training to reference resolution. In Proceedings of
40th Annual Meeting of the Association for Com-
putational Linguistics, 352?359, Philadelphia, Penn-
sylvania, USA. Association for Computational Lin-
guistics.
V. Ng, C. Cardie. 2003. Bootstrapping corefer-
ence classifiers with multiple machine learning algo-
rithms. In Proceedings of the 2003 Conference on
Empirical Methods in Natural Language Processing
(EMNLP-2003).
N. Xue, F.-D. Chiou, M. Palmer. 2002. Building a
large-scale annotated chinese corpus. In Proceed-
ings of Coling-2002.
26
Proceedings of the 11th International Conference on Parsing Technologies (IWPT), pages 134?137,
Paris, October 2009. c?2009 Association for Computational Linguistics
Scalable Discriminative Parsing for German
Yannick Versley
SFB 833
Universita?t Tu?bingen
versley@sfs.uni-tuebingen.de
Ines Rehbein
Dep. of Computational Linguistics
Universita?t des Saarlandes
rehbein@coli.uni-sb.de
Abstract
Generative lexicalized parsing models,
which are the mainstay for probabilistic
parsing of English, do not perform as well
when applied to languages with differ-
ent language-specific properties such as
free(r) word order or rich morphology. For
German and other non-English languages,
linguistically motivated complex treebank
transformations have been shown to im-
prove performance within the framework
of PCFG parsing, while generative lexical-
ized models do not seem to be as easily
adaptable to these languages.
In this paper, we show a practical way
to use grammatical functions as first-class
citizens in a discriminative model that al-
lows to extend annotated treebank gram-
mars with rich feature sets without hav-
ing to suffer from sparse data problems.
We demonstrate the flexibility of the ap-
proach by integrating unsupervised PP at-
tachment and POS-based word clusters
into the parser.
1 Introduction
To capture the semantic relations inherent in a
text, parsing has to recover both structural infor-
mation and grammatical functions, which com-
monly coincide in English, but not in freer
word order languages such as German. In-
stead one has to make use of morphological fea-
tures in addition to exploiting ordering preferences
such as the (violatable) default ordering of (sub-
ject<)dative<accusative.
Because of this fact, many successful ap-
proaches for German PCFG parsing (Schiehlen,
2004; Dubey, 2005; Versley, 2005) use annotated
treebank grammars where the constituent trees
from the treebank are enriched with further lin-
guistic information that allows an adequate recon-
struction of syntactic relationships, suggesting that
probabilistic context-free grammars are an ade-
quate tool for parsing these languages.
In the ACL 2008 workshop on Parsing Ger-
man (Ku?bler, 2008), Rafferty and Manning (2008)
used a lexicalized PCFG parser using markoviza-
tion and parent annotation, but no linguistically in-
spired transformations; Rafferty and Manning did
quite well on constituents, but were not success-
ful in reconstructing grammatical functions, with
results considerably worse than for other submis-
sions in the shared task.
The framework we present in this paper ? an-
notated treebank grammars with a discriminative
model that allows lexicalization based on gram-
matical function assignment, as well as the ad-
dition of features based on unsupervised learn-
ing, including PP attachment and word clusters ?
shows that it is possible to achieve good improve-
ments over generative lexicalized models by using
the additional flexibility gained over standard lex-
icalized PCFG models. Our approach offers more
flexibility than generative PCFG models, while
computational costs for development and practi-
cal use are still acceptable. While we only present
results for German, we are confident that the re-
sults carry over to other languages where anno-
tated treebank grammars have been used success-
fully.
2 Parsing German with Morphology and
Valence Information
As a base parser, we use BitPar (Schmid, 2004),
a fast unlexicalized PCFG parser based on a first
pass where non-probabilistic bottom-up parsing
and top-down filtering is carried out efficiently by
storing the chart in bit vectors, and construct the
probabilistic chart only after top-down filtering.
We use an annotated treebank PCFG that is de-
134
rived from the Tiger treebank and largely inspired
by earlier work on annotated treebank grammars
for German (Schiehlen, 2004; Dubey, 2005; Vers-
ley, 2005).
Subcategorization With respect to the treebank
grammar, we refine the node labels with linguisti-
cally important information that is only implicit in
the treebank but would be tedious (and pointless)
to annotate by hand:
Firstly, we annotate NPs by case; clause nodes
(S and VP) are subcategorized by the clause type
(fin,inf,izu,rel), and NPs and PPs with a relative
pronoun are marked. Comparative phrases (e.g.,
bigger [than a house], marked as NP in Tiger and
Tu?Ba-D/Z) are marked by adding a ?CC? ending
to the node label. Finally, auxiliaries are split ac-
cording to their verb lemma into sein (be), haben
(have), werden (become).
To aid the identification of noun phrase case,
we add information related to case/number/gender
syncretism to the preterminal labels of determin-
ers, nouns, and adjectives (for details, see Versley,
2005) that allows to accurately determine the set of
possible cases while keeping the size of the tagset
relatively small .
Verb Valence We use information from the lex-
icon of the WCDG parser for German (Foth and
Menzel, 2006) to mark verbs according to the ar-
guments that they can take. While the WCDG
lexicon contains more information, we only en-
code the possibility of accusative and dative com-
plements, ignoring entries for genitive or clausal
complements.
Markovization with Argument Marking It
has been noted consistently (Klein and Manning,
2003; Schiehlen, 2004) that using markovization
- replacing the original treebank rules by an ap-
proximation that only considers a limited context
window of one or two siblings - improves re-
sults at least for a constituency-based evaluation.
However, in some cases this simple markoviza-
tion scheme leads to undesirable results includ-
ing sentences with multiple subjects, as predica-
tive arguments also have nominative case. To
avoid this, we additionally mark which arguments
have already been seen, yielding node labels such
as S fin<VVFIN a<RNP a<sa in the case of a
partial constituent for a finite sentence (S fin)
expanding to the right (<R) where both subject (s)
and accusative object (a) have already been seen.
Unknown Words For the base PCFG parse, we
use a decision tree with 43 regular expressions as
features, five of which are tailored towards rec-
ognizing the past and zu-infinitive form of sep-
arable prefix verbs (abarbeiten ? abgearbeitet,
abzuarbeiten), which cannot be recognized by
considering suffixes only. The extended part of
speech tags for verbs (which contain valency in-
formation) are interpolated between the distribu-
tion at the concrete leaf of the decision tree and the
global valency distribution for the (coarse) part-of-
speech tag.
Additionally, we use SMOR (Schmid et al,
2004) in conjunction with the verb lexicon and
a gazetteer list containing person and location
names to determine possible fine-grained part-of-
speech tags for unknown words.
Restoring Grammatical Functions Adding
edge labels to the nodes in PCFG parsing easily
creates sparse data problems, as reported by
Rafferty and Manning (2008), who witness a drop
in constituent F-measure (excluding grammatical
function labels) when they include function labels
in the symbols of their PCFG. On the other hand,
the informativity of grammatical function labels
for the contents of the node does not always
justify their cost in terms of data sparseness.
Thus, we chose an approach where we include
linguistically relevant information in the node
labels (see above), and use the finer categoriza-
tion to restore the grammatical function labels
automatically: Using the most frequent function
label sequence associated with a rule yields good
results even in the presence of markovization,
where some of the surrounding context is lost.
Furthermore, this approach allows us to use the
grammatical function label assignments in the
subsequent discriminative model, thus yielding
typed dependencies rather than the unlabeled
dependencies that are used in the lexicalization
model of the Stanford parser.
3 Discriminative Parsing
Generative parsing models are based on few dis-
tributions that use different feature combinations
based on smoothing; incorporating additional fea-
tures into these is very difficult at best.
As a result, the use of external preferences in
such parsers is usually limited to approaches that
reattach dependents in the output of the parser
rather than integrating them in the parsing process.
135
Settings no GFs with GFs
Rafferty and Manning (2008) 77.40 NA
?, training with GFs 72.09 60.48
markov[unlex] 74.66 62.47
markov+parent[unlex] 73.94 61.63
markovGF[unlex] 75.00 63.58
markov[lex] 77.68 66.05
markovGF[lex] 77.55 66.69
markovGF[+pp] 78.43 67.90
Table 1: Evaluation results: PARSEVAL F1 on
PaGe development set
Discriminative parsing for unification-based
grammar commonly uses the conditional random
field formulation introduced by Miyao and Tsu-
jii (2002) and Geman and Johnson (2002), which
uses local features to select a parse from a packed
forest. The much larger cost in terms of mem-
ory and time compared to generative models has
until recently made this approach largely unattrac-
tive (but see Finkel et al, 2008, who distributes the
learning process over several powerful machines).
An alternative use of discriminative models
has been to incorporate global features, either by
reranking (e.g. Charniak and Johnson, 2005, or
Ku?bler et al, 2009 for German) or by beam search
over a pruned parse forest (Huang, 2008). How-
ever, Huang shows that a discriminative model us-
ing only local features reaps most of the benefits
of the global model and performs at a similar level
than earlier reranking-based approaches, pointing
to the fact that local ambiguities often result in the
n-best list not containing the correct parse.
The model we propose here extracts a pruned
parse forest from a simple unlexicalized parser and
then uses a factored discriminative model to apply
a rich set of features using the lexicalized parse
tree and its typed dependencies.
CRF parsing on pruned forests We extract a
pruned forest that contains exactly those nodes and
edges that can occur in trees that have a probabil-
ity ? pbest ? t, where in practice a threshold of
t = 10?3 ensures that no good parse is pruned
away while at the same time, the resulting forest
has only few nodes and edges.
For training, we extract an oracle tree, which is
selected according to a combination of correct (an-
notated grammar) constituents, the absence of in-
correct constituents, and the likelihood of the tree,
to account for the fact that the forest does not al-
fW-w-pos, CW-w-pos word form, cluster
f-sp, fS-sp-size node label, node size(1)
f-sp-RHS rule expansion
LDdir-sp-sd-hsd daughter attachment
LH-sp-sd-hsd-hld head projection
Lddir-hsp-hsd dependency (pos-pos)
Lddir-hsp-hsd-dist attachment length(1)
Ledir-hsp-hsd-hld dependency (pos-lemma)
Lfdir-hsp-hlp-hsd dependency (pos-lemma)
Lfdir-hsp-hlp-hsd-GF typed dep. (lemma-pos)
LhGF-hcp-hlp-hcd-hld typed dep. (lemma-lemma)
MIpp-prep, MIpp0-prep PP attach (noun)
MIppV-prep, MIppV0-prep PP attach (verb)
1) node sizes and attachment distances are discretized.
dir: one of H(head), L/R(head dep), B/I/E(nonheaded dep)
sp/d constituent symbol (parent/dep), hsp/d head cat, hc
head cat (coarse), hl head lemma
Table 2: List of Features
ways contain the exact gold tree. We then use
the AMIS maximum entropy learner of Miyao and
Tsujii (2002) to learn the discriminative model by
creating a forest from a grammar learned on the
remaining 4/5 of the training data.
Efficiency Parsing using the discriminative
model is quite efficient, with a memory con-
sumption for the whole system at about 270MB,
including the data used to determine the corpus
derived features (word clusters, mutual informa-
tion statistics, semantic role clusters). Parsing
speed is at 1.65sec./sentence on a 1.5GHz Pen-
tium M, against 1.84sec./sent for BitPar alone
when not using the tag filter for unknown words.
The time needed for learning can be reduced
by keeping the pruned parse charts and only re-
running the part of lexicalization and discrimina-
tive feature extraction; when reusing the old pa-
rameters as a starting point for AMIS? model esti-
mation, the turn-around time including feature ex-
traction is below two hours.
3.1 Clustering for unknown words
To improve the behaviour on unknown words
where morphological analyzer and regular expres-
sions do not yield informative preferences, we ex-
ploit a large, part-of-speech-tagged corpus to in-
duce clusters which provide robust information
that is useful even in our case where preterminals
in the PCFG are finer than standard POS tags.
The following features were gathered and used
by weighting by the pointwise mutual information
between the word and feature occurrences:
The context feature retrieves windows of high-
frequent words surrounding the word in question
136
(e.g. der mit for ?der Mann mit den Blumen?).
The context2 feature retrieves windows of one
high-frequent word and one part-of-speech tag
surrounding the word in question (e.g. der NN for
?der scho?ne Mann?).
The postag feature simply retrieves the part-of-
speech tag that is assigned to the word.
The result of using the repeated bisecting
k-means implementation of CLUTO (Steinbach
et al, 2000) on the resulting features yields syntac-
tically sensible clusters containing years, money
sums, last names, or place names.
3.2 Unsupervised PP Attachment and
Subject-Object preferences
We used simple part-of-speech tag patterns to
gather statistics on the association between nouns
and immediately following prepositions, as well
as between prepositions and closely following
verbs on the DE-WaC corpus (Baroni and Kilgar-
iff, 2006), an 1.7G words sample of the German-
language WWW. The mutual information values
for PP attachment are made available to the parser
as features that are weighted by the mutual infor-
mation value.
4 Evaluation and Discussion
To evaluate our approach, we use the dataset
used for the ACL-2008 Parsing German Workshop
(Ku?bler, 2008) that contains 26,116 sentences of
the TIGER treebank (Brants et al, 2002), in a 8:1:1
split of training, testing, and evaluation data, and
validate our approach on the development data,
where the results published by Rafferty and Man-
ning (2008) provide a useful comparison. All our
experiments are done using tags automatically as-
signed by the parser, which reaches a tagging ac-
curacy of about 97.5% according to the EVALB
output.
We find that our final model, combining aug-
menting the treebank labels with lingustic infor-
mation in addition to lexicalization and unsuper-
vised PP attachment works better than the best-
performing models of Rafferty and Manning, with
a very large improvement in grammatical func-
tions that is only surpassed by the Berkeley Parser
(Petrov and Klein, 2008), showing that our combi-
nation of annotated treebank grammars with a fac-
tored discriminative model not only allows great
control and flexibility for experimenting with the
inclusion of novel features, but also yields very
good results compared with the state of the art
for German (see table 1 for results on the Tiger
treebank). Preliminary results on Tu?Ba-D/Z with
a subset of the transformations of Versley (2005)
show the same tendency as the results for Tiger,
with 91.3% for constituents only, and 80.1% in-
cluding function labels (compared to 88.9% and
77.2% for the Stanford parser).
Future work will investigate the impact of in-
cluding additional features into the discriminative
parsing model.
References
Baroni, M. and Kilgariff, A. (2006). Large linguistically-
processed web corpora for multiple languages. In EACL
2006.
Brants, S., Dipper, S., Hansen, S., Lezius, W., and Smith, G.
(2002). The TIGER treebank. In Proc. TLT 2002.
Charniak, E. and Johnson, M. (2005). Coarse-to-fine n-best
parsing and maxent discriminative reranking. In Proc.
ACL 2005.
Dubey, A. (2005). What to do when lexicalization fails: pars-
ing German with suffix analysis and smoothing. In ACL-
2005.
Finkel, J. R., Kleeman, A., and Manning, C. D. (2008). Effi-
cient, feature-based, conditional random field parsing. In
ACL/HLT-2008.
Foth, K. and Menzel, W. (2006). Hybrid parsing: Using prob-
abilistic models as predictors for a symbolic parser. In
ACL 2006.
Geman, S. and Johnson, M. (2002). Dynamic programming
for parsing and estimation of stochastic unification-based
grammars. In ACL 2002.
Huang, L. (2008). Forest reranking: Discriminative parsing
with non-local features. In HLT/ACL 2008.
Klein, D. and Manning, C. D. (2003). Accurate unlexicalized
parsing. In ACL 2003.
Ku?bler, S. (2008). The PaGe 2008 shared task on parsing
German. In Proceedings of the ACL-2008 Workshop on
Parsing German.
Ku?bler, S., Hinrichs, E., Maier, W., and Klett, E. (2009). Pars-
ing coordinations. In EACL 2009.
Miyao, Y. and Tsujii, J. (2002). Maximum entropy estimation
for feature forests. In HLT 2002.
Petrov, S. and Klein, D. (2008). Parsing German with latent
variable grammars. In Parsing German Workshop at ACL-
HLT 2008.
Rafferty, A. and Manning, C. D. (2008). Parsing three Ger-
man treebanks: Lexicalized and unlexicalized baselines.
In ACL?08 workshop on Parsing German.
Schiehlen, M. (2004). Annotation strategies for probabilistic
parsing in German. In Proc. Coling 2004.
Schmid, H. (2004). Efficient parsing of highly ambiguous
context-free grammars with bit vectors. In Proc. Coling
2004.
Schmid, H., Fitschen, A., and Heid, U. (2004). SMOR: A
German computational morphology covering derivation,
composition and inflection. In Proceedings of LREC 2004.
Steinbach, M., Karypis, G., and Kumar, V. (2000). A com-
parison of document clustering techniques. In KDD Work-
shop on Text Mining.
Versley, Y. (2005). Parser evaluation across text types. In
Proceedings of the Fourth Workshop on Treebanks and
Linguistic Theories (TLT 2005).
137
Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 949?957,
Beijing, August 2010
Bringing Active Learning to Life
Ines Rehbein
Computational Linguistics
Saarland University
Josef Ruppenhofer
Computational Linguistics
Saarland University
{rehbein|josefr|apalmer}@coli.uni-sb.de
Alexis Palmer
Computational Linguistics
Saarland University
Abstract
Active learning has been applied to dif-
ferent NLP tasks, with the aim of limit-
ing the amount of time and cost for human
annotation. Most studies on active learn-
ing have only simulated the annotation
scenario, using prelabelled gold standard
data. We present the first active learning
experiment for Word Sense Disambigua-
tion with human annotators in a realistic
environment, using fine-grained sense dis-
tinctions, and investigate whether AL can
reduce annotation cost and boost classifier
performance when applied to a real-world
task.
1 Introduction
Active learning has recently attracted attention as
having the potential to overcome the knowledge
acquisition bottleneck by limiting the amount of
human annotation needed to create training data
for statistical classifiers. Active learning has been
shown, for a number of different NLP tasks, to re-
duce the number of manually annotated instances
needed for obtaining a consistent classifier perfor-
mance (Hwa, 2004; Chen et al, 2006; Tomanek et
al., 2007; Reichart et al, 2008).
The majority of such results have been achieved
by simulating the annotation scenario using prela-
belled gold standard annotations as a stand-in for
real-time human annotation. Simulating annota-
tion allows one to test different parameter set-
tings without incurring the cost of human anno-
tation. There is, however, a major drawback: we
do not know whether the results of experiments
performed using hand-corrected data carry over to
real-world scenarios in which individual human
annotators produce noisy annotations. In addi-
tion, we do not know to what extent error-prone
annotations mislead the learning process. A sys-
tematic study of the impact of erroneous annota-
tion on classifier performance in an active learn-
ing (AL) setting is overdue. We need to know a)
whether the AL approach can really improve clas-
sifier performance and save annotation time when
applied in a real-world scenario with noisy data,
and b) whether AL works for classification tasks
with fine-grained or complex annotation schemes
and a low inter-annotator agreement.
In this paper we bring active learning to life in
the context of frame semantic annotation of Ger-
man texts within the SALSA project (Burchardt
et al, 2006). Specifically, we apply AL methods
for learning to assign semantic frames to predi-
cates, following Erk (2005) in treating frame as-
signment as a Word Sense Disambiguation task.
Under our fine-grained annotation scheme, anno-
tators have to deal with a high level of ambigu-
ity, resulting in low inter-annotator agreement for
some word senses. This fact, along with the po-
tential for wrong annotation decisions or possi-
ble biases from individual annotators, results in
an annotation environment in which we get noisy
data which might mislead the classifier. A sec-
ond characteristic of our scenario is that there is no
gold standard for the newly annotated data, which
means that evaluation is not straightforward. Fi-
nally, we have multiple annotators whose deci-
949
sions on particular instances may diverge, raising
the question of which annotations should be used
to guide the AL process. This paper thus investi-
gates whether active learning can be successfully
applied in a real-world scenario with the particular
challenges described above.
Section 2 of the paper gives a short overview
of the AL paradigm and some related work, and
Section 3 discusses the multi-annotator scenario.
In Section 4 we present our experimental design
and describe the data we use. Section 5 presents
results, and Section 6 concludes.
2 Active Learning
The active learning approach aims to reduce the
amount of manual annotation needed to create
training data sufficient for developing a classifier
with a given performance. At each iteration of
the AL cycle, the actual knowledge state of the
learner guides the learning process by determin-
ing which instances are chosen next for annota-
tion. The main goal is to advance the learning
process by selecting instances which provide im-
portant information for the machine learner.
In a typical active learning scenario, a small set
of manually labelled seed data serves as the ini-
tial training set for the classifier (learner). Based
on the predictions of the classifier, a large pool
of unannotated instances is queried for the next
instance (or batch of instances) to be presented
to the human annotator (sometimes called the or-
acle). The underlying active learning algorithm
controlling the learning process tries to select the
most informative instances in order to get a strong
boost in classifier performance. Different meth-
ods can be used for determining informativity of
instances. We use uncertainty sampling (Cohn et
al., 1995) in which ?most informative? instances
are those for which the classifier has the lowest
confidence in its label predictions. The rough in-
tuition behind this selection method is that it iden-
tifies instance types which have yet to be encoun-
tered by the classifier. The learning process pro-
ceeds by presenting the selected instances to the
human annotator, who assigns the correct label.
The newly-annotated instances are added to the
seed data and the classifier is re-trained on the new
data set. The newly trained classifier now picks
the next instances, based on its updated knowl-
edge, and the process repeats. If the learning pro-
cess can provide precisely that information which
the classifier still needs to learn, a smaller number
of instances should suffice to achieve the same ac-
curacy as on a larger training set of randomly se-
lected training examples.
Active learning has been applied to a num-
ber of natural language processing tasks like
POS tagging (Ringger et al, 2007), NER (Laws
and Schu?tze, 2008; Tomanek and Hahn, 2009),
syntactic parsing (Osborne and Baldridge, 2004;
Hwa, 2004), Word Sense Disambiguation (Chen
et al, 2006; Chan and Ng, 2007; Zhu and Hovy,
2007; Zhu et al, 2008) and morpheme gloss-
ing for language documentation (Baldridge and
Palmer, 2009). While most of these studies suc-
cessfully show that the same classification accu-
racy can be achieved with a substantially smaller
data set, these findings are mostly based on simu-
lations using gold standard data.
For our task of Word Sense Disambiguation
(WSD), mixed results have been achieved. AL
seems to improve results in a WSD task with
coarse-grained sense distinctions (Chan and Ng,
2007), but the results of (Dang, 2004) raise doubts
as to whether AL can successfully be applied to
a fine-grained annotation scheme, where Inter-
Annotator Agreement (IAA) is low and thus the
consistency of the human annotations decreases.
In general, AL has been shown to reduce the cost
of annotation when applied to classification tasks
where a single human annotator predicts labels for
new data points with a reasonable consistency and
accuracy. It is not clear whether the same settings
can be applied to a multi-annotator environment
where IAA is low.
3 Active Learning in a realistic task
including multiple annotators
Another possible difference between active learn-
ing simulations and real-world scenarios is the
multi-annotator environment. In such a setting,
two or more annotators assign labels to the same
instances, which are then merged to check for con-
flicting decisions from different annotators. This
is standard practise in many annotation projects
doing fine-grained semantic annotation with a
950
high level of ambiguity, and it necessitates that all
annotators work on the same data set.
Replicating an active learning simulation on
hand-corrected data, starting with a fixed set of
seed data and fixed parameter settings, using the
same algorithm, will always result in the same
training set selected from the pool. Human anno-
tators, however, will assign different labels to the
same instances, thus influencing the selection of
the next instance from the pool. This means that
individual annotators might end up with very dif-
ferent sets of annotated data, depending on factors
like their interpretation of the annotation guide-
lines, an implicit bias towards a particular label,
or simply errors made during annotation.
There is not much work addressing this prob-
lem. (Donmez and Carbonell, 2008) consider
modifications of active learning to accommodate
variability of annotators. (Baldridge and Palmer,
2009) present a real-world study with human an-
notators in the context of language documenta-
tion. The task consists of producing interlin-
ear glossed text, including morphological and
grammatical analysis, and can be described as
a sequence labelling task. Annotation cost is
measured as the actual time needed for annota-
tion. Among other settings, the authors compare
the performance of two annotators with different
grades of expertise. The classifier trained on the
data set created by the expert annotator in an ac-
tive learning setting does obtain a higher accuracy
on the gold standard. For the non-expert annota-
tor, however, the active learning setting resulted
in a lower accuracy than for a classifier trained on
a randomly selected data set. This finding sug-
gests that the quality of annotation needs to be
high enough for active learning to actually work,
and that annotation noise is a problem for AL.
There are two problems arising from this:
1. It is not clear whether active learning will
work when applied to noisy data
2. It is not straightforward to apply active learn-
ing to a real-world scenario, where low IAA
asks for multiple annotators
In our experiment we address these questions
by systematically investigating the impact of an-
notation noise on classifier performance and on
the composition of the training set. The next sec-
tion presents the experimental design and the data
used in our experiment.
4 Experimental Design
In the experiment we annotated 8 German cau-
sation nouns, namely Ausgang, Anlass, Ergeb-
nis, Resultat, Grund, Konsequenz, Motiv, Quelle
(outcome, occasion, effect, result, reason, con-
sequence, motive, source of experience). These
nouns were chosen because they exhibit a range
of difficulty in terms of the number of senses they
have in our annotation scheme. They all encode
subtle distinctions between different word senses,
but some of them are clearly easier to disam-
biguate than others. For instance, although Aus-
gang has 9 senses, they are easier to distinguish
for humans than the 4 senses of Konsequenz.
Six annotators participated in the experiment.
While all annotators were trained, having at least
one year experience in frame-semantic annota-
tion, one of the annotators is an expert with several
years of training and working experience in the
Berkeley FrameNet Project. This annotator also
defined the frames (word senses) used in our ex-
periment.
Prior to the experiment, all annotators were
given 100 randomly chosen sentences. After
annotating the training data, problematic cases
were discussed to make sure that the annotators
were familiar with the fine-grained distinctions
between word senses in the annotation scheme.
The data sets used for training were adjudicated
by two of the annotators (one of them being the
expert) and then used as a gold standard to test
classifier performance in the active learning pro-
cess.
4.1 Data and Setup
For each lemma we extracted sentences from the
Wahrig corpus1 containing this particular lemma.
The annotators had to assign word senses to 300
instances for each target word, split into 6 pack-
ages of 50 sentences each. This resulted in 2,400
annotated instances per annotator (14,400 anno-
tated instances in total). The annotation was done
1The Wahrig corpus includes more than 113 mio. sen-
tences from German newspapers and magazines covering
topics such as politics, science, fashion, and others.
951
Anlass Motiv Konsequenz Quelle Ergebnis / Resultat Ausgang Grund
Occasion (37) Motif (47) Causation (32) Relational nat feat.(3) Causation (4/10) Outcome (67) Causation (24)
Reason (63) Reason(53) Level of det.(6) Source of getting (14) Competitive score(12/36) Have leave (4) Reason (58)
Response (61) Source of exp. (14) Decision (11/6) Portal (21) Death (1)
MWE1 (1) Source of info. (56) Efficacy (2/3) Outgoing goods (4) Part orientation. (0)
Well (6) Finding out (24/23) Ostomy (0) Locale by owner(3)
Emissions source (7) Mathematics (1/0) Origin (5) Surface earth (0)
Operating result (36/5) Tech output (7) Bottom layer (0)
Outcome (10/17) Process end (2) Soil (1)
Departing (1) CXN1 (0)
CXN2 (0)
MWE1 (0)
MWE2 (10)
MWE3 (0)
MWE4 (3)
MWE5 (0)
MWE6 (0)
Fleiss? kappa for the 6 annotators for the 150 instances annotated in the random setting
0.67 0.79 0.55 0.77 0.63 / 0.59 0.82 0.43
Table 1: 8 causation nouns and their word senses (numbers in brackets give the distribution of word
senses in the gold standard (100 sentences); CXN: constructions, MWE: multi-word expressions; note
that Ergebnis and Resultat are synonyms and therefore share the same set of frames.)
using a Graphical User Interface where the sen-
tence was presented to the annotator, who could
choose between all possible word senses listed in
the GUI. The annotators could either select the
frame by mouse click or use keyboard shortcuts.
For each instance we recorded the time it took
the annotator to assign an appropriate label. To
ease the reading process the target word was high-
lighted.
As we want to compare time requirements
needed for annotating random samples and sen-
tences selected by active learning, we had to con-
trol for training effects which might speed up the
annotation. Therefore we changed the annotation
setting after each package, meaning that the first
annotator started with 50 sentences randomly se-
lected from the pool, then annotated 50 sentences
selected by AL, followed by another 50 randomly
chosen sentences, and so on. We divided the an-
notators into two groups of three annotators each.
The first group started annotating in the random
setting, the second group in the AL setting. The
composition of the groups was changed for each
lemma, so that each annotator experienced all dif-
ferent settings during the annotation process. The
annotators were not aware of which setting they
were in.
Pool data For the random setting we randomly
selected three sets of sentences from the Wahrig
corpus which were presented for annotation to all
six annotators. This allows us to compare annota-
tion time and inter-annotator agreement between
the annotators. For the active learning setting we
randomly selected three sets of 2000 sentences
each, from which the classifier could pick new in-
stances during the annotation process. This means
that for each trial the algorithm could select 50 in-
stances out of a pool of 2000 sentences. On any
given AL trial each annotator uses the same pool
as all the other annotators. In an AL simulation
with fixed settings and gold standard labels this
would result in the same subset of sentences se-
lected by the classifier. For our human annotators,
however, due to different annotation decisions the
resulting set of sentences is expected to differ.
Sampling method Uncertainty sampling is a
standard sampling method for AL where new in-
stances are selected based on the confidence of the
classifier for predicting the appropriate label. Dur-
ing early stages of the learning process when the
classifier is trained on a very small seed data set,
it is not beneficial to add the instances with the
lowest classifier confidence. Instead, we use a dy-
namic version of uncertainty sampling (Rehbein
and Ruppenhofer, 2010), based on the confidence
of a maximum entropy classifier2, taking into ac-
count how much the classifier has learned so far.
In each iteration one new instance is selected from
the pool and presented to the oracle. After anno-
tation the classifier is retrained on the new data
set. The modified uncertainty sampling results in
a more robust classifier performance during early
stages of the learning process.
2http://maxent.sourceforge.net
952
Anlass Motiv Konsequenz Quelle Ergebnis Resultat Ausgang Grund
R U R U R U R U R U R U R U R U
A1 8.6 9.6 5.9 6.6 10.7 10.5 6.0 4.8 10.5 7.4 10.1 9.6 6.4 10.0 10.2 11.1
A2 4.4 5.7 4.8 5.9 8.2 9.2 4.9 4.9 6.4 4.4 11.7 8.5 5.1 7.7 9.0 9.3
A3 9.9 9.2 6.8 6.7 6.8 8.3 7.4 6.1 9.4 7.6 9.0 12.3 7.5 8.5 11.7 10.2
A4 5.8 4.9 3.6 3.6 9.9 11.3 4.8 3.5 7.9 7.1 9.7 11.1 3.6 4.1 9.9 9.4
A5 3.0 3.5 3.0 2.6 4.8 4.9 3.8 3.0 6.8 4.8 6.7 6.1 3.1 3.5 6.3 6.0
A6 5.4 6.3 5.3 4.7 6.7 8.6 5.4 4.6 7.8 6.1 8.7 9.0 6.9 6.6 9.3 8.5
? 6.2 6.5 4.9 5.0 7.8 8.8 5.4 4.5 8.1 6.2 9.3 9.4 5.4 6.7 9.4 9.1
sl 25.8 27.8 27.8 26.0 24.2 25.8 24.9 26.5 25.7 25.2 29.0 35.9 25.5 27.9 26.8 29.7
Table 2: Annotation time (sec/instance) per target/annotator/setting and average sentence length (sl)
5 Results
The basic idea behind active learning is to se-
lect the most informative instances for annotation.
The intuition behind ?more informative? is that
these instances support the learning process, so we
might need fewer annotated instances to achieve
a comparable classifier performance, which could
decrease the cost of annotation. On the other
hand, ?more informative? also means that these
instances might be more difficult to annotate, so it
is only fair to assume that they might need more
time for annotation, which increases annotation
cost. To answer the question of whether AL re-
duces annotation cost or not we have to check a)
how long it took the annotators to assign labels
to the AL samples compared to the randomly se-
lected instances, and b) how many instances we
need to achieve the best (or a sufficient) perfor-
mance in each setting. Furthermore, we want to
investigate the impact of active learning on the
distribution of the resulting training sets and study
the correlation between the performance of the
classifier trained on the annotated data and these
factors: the difficulty of the annotation task (as-
sessed by IAA), expertise and individual proper-
ties of the annotators.
5.1 Does AL speed up the annotation process
when working with noisy data?
Table 2 reports annotation times for each annota-
tor and target for random sampling (R) and uncer-
tainty sampling (U). For 5 out of 8 targets the time
needed for annotating in the AL setting (averaged
over all annotators) was higher than for annotat-
ing the random samples. To investigate whether
this might be due to the length of the sentences
in the samples, Table 2 shows the average sen-
tence length for random samples and AL samples
for each target lemma. Overall, the sentences se-
lected by the classifier during AL are longer (26.2
vs. 28.1 token per sentence), and thus may take
the annotators more time to read.3 However, we
could not find a significant correlation (Spearman
rank correlation test) between sentence length and
annotation time, nor between sentence length and
classifier confidence.
The three target lemmas which took longer to
annotate in the random setting are Ergebnis (re-
sult), Grund (reason) and Quelle (source of expe-
rience). This observation cannot be explained by
sentence length. While sentence length for Ergeb-
nis is nearly the same in both settings, for Grund
and Quelle the sentences picked by the classi-
fier in the AL setting are significantly longer and
therefore should have taken more time to anno-
tate. To understand the underlying reason for this
we have to take a closer look at the distribution of
word senses in the data.
5.2 Distribution of word senses in the data
In the literature it has been stated that AL implic-
itly alleviates the class imbalance problem by ex-
tracting more balanced data sets, while random
sampling tends to preserve the sense distribution
present in the data (Ertekin et al, 2007). We could
not replicate this finding when using noisy data
to guide the learning process. Table 3 shows the
distribution of word senses for the target lemma
Ergebnis a) in the gold standard, b) in the random
samples, and c) in the AL samples.
The variance in the distribution of word senses
in the random samples and the gold standard can
3The correlation between sentence length and annotation
time is not obvious, as the annotators only have to label one
target in each sentence. For ambiguous sentences, however,
reading time may be longer, while for the clear cases we do
not expect a strong effect.
953
Ergebnis
Frame gold (%) R (%) U (%)
Causation 4.0 4.8 3.7
Outcome 10.0 17.8 10.5
Finding out 24.0 26.2 8.2
Efficacy 2.0 0.8 0.1
Decision 11.0 5.1 3.2
Mathematics 1.0 1.6 0.4
Operating result 36.0 24.5 66.7
Competitive score 12.0 19.2 7.2
Table 3: Distribution of frames (word senses) for
the lemma Ergebnis in the gold standard (100 sen-
tences), in the random samples (R) and AL sam-
ples (U) (150 sentences each)
be explained by low inter-annotator agreement
caused by the high level of ambiguity for the tar-
get lemmas. The frame distribution in the data
selected by uncertainty sampling, however, cru-
cially deviates from those of the gold standard
and the random samples. A disproportionately
high 66% of the instances selected by the classi-
fier have been assigned the label Operating result
by the human annotators. This is the more sur-
prising as this frame is fairly easy for humans to
distinguish.
The classifier, however, proved to have seri-
ous problems learning this particular word sense
and thus repeatedly selected more instances of this
frame for annotation. As a result, the distribution
of word senses in the training set for the uncer-
tainty samples is highly skewed, having a nega-
tive effect on the overall classifier performance.
The high percentage of instances of the ?easy-to-
decide? frame Operating result explains why the
instances for Ergebnis took less time to annotate
in the AL setting. Thus we can conclude that an-
notating the same number of instances on average
takes more time in the AL setting, and that this
effect is not due to sentence length.
5.3 What works, what doesn?t, and why
For half of the target lemmas (Motiv, Konsequenz,
Quelle, Ausgang), we did obtain best results in
the AL setting (Table 4). For Ausgang and Mo-
tiv AL gives a substantial boost in classifier per-
formance of 5% and 7% accuracy, while the gains
for Konsequenz and Quelle are somewhat smaller
with 2% and 1%, and for Grund the highest accu-
racy was reached on both the AL and the random
Random Uncertainty
50 100 150 50 100 150
Anlass 0.85 0.86 0.85 0.84 0.85 0.84
Motiv 0.57 0.62 0.63 0.64 0.67 0.70
Konseq. 0.55 0.59 0.60 0.61 0.62 0.62
Quelle 0.56 0.53 0.54 0.52 0.52 0.57
Ergebnis 0.39 0.42 0.41 0.39 0.37 0.38
Resultat 0.31 0.35 0.37 0.32 0.34 0.34
Ausgang 0.67 0.69 0.69 0.68 0.72 0.74
Grund 0.48 0.47 0.47 0.47 0.44 0.48
Table 4: Avg. classifier performance (acc.) over
all annotators for the 8 target lemmas when train-
ing on 50, 100 and 150 annotated instances for
random samples and uncertainty samples
sample.
Figure 1 (top row) shows the learning curves
for Resultat, our worst-performing lemma, for the
classifier trained on the manually annotated sam-
ples for each individual annotator. The solid black
line represents the majority baseline, obtained by
assigning the most frequent word sense in the gold
standard to all instances. For both random and AL
settings, results are only slightly above the base-
line. The curves for the AL setting show how erro-
neous decisions can mislead the classifier, result-
ing in classifier accuracy below the baseline for
two of the annotators, while the learning curves
for these two annotators on the random samples
show the same trend as for the other 4 annotators.
For Konsequenz (Figure 1, middle), the classi-
fier trained on the AL samples yields results over
the baseline after around 25 iterations, while in
the random sampling setting it takes at least 100
iterations to beat the baseline. For Motiv (Figure
1, bottom row), again we observe far higher re-
sults in the AL setting. A possible explanation for
why AL seems to work for Ausgang, Motiv and
Quelle might be the higher IAA4 (? 0.825, 0.789,
0.768) as compared to the other target lemmas.
This, however, does not explain the good results
achieved on the AL samples for Konsequenz, for
which IAA was quite low with ? 0.554.
Also startling is the fact that AL seems to work
particularly well for one of the annotators (A6,
Figure 1) but not for others. Different possible ex-
planations come to mind: (a) the accuracy of the
annotations for this particular annotator, (b) the
4IAA was computed on the random samples, as the AL
samples do not include the same instances.
954
0 50 100 150
0.
10
0.
20
0.
30
0.
40
Resultat (Random Sampling)
no. of iterations
a
cc
u
ra
cy
Annotator
A1
A2
A3
A4
A5
A6
0 50 100 150
0.
10
0.
20
0.
30
0.
40
Resultat (Uncertainty Sampling)
no. of iterations
a
cc
u
ra
cy
0 50 100 150
0.
3
0.
4
0.
5
0.
6
0.
7
Konsequenz (Random Sampling)
no. of iterations
a
cc
u
ra
cy
0 50 100 150
0.
3
0.
4
0.
5
0.
6
0.
7
Konsequenz (Uncertainty Sampling)
no. of iterations
a
cc
u
ra
cy
0 50 100 150
0.
50
0.
60
0.
70
Motiv (Random Sampling)
no. of iterations
a
cc
u
ra
cy
0 50 100 150
0.
50
0.
60
0.
70
Motiv (Uncertainty Sampling)
no. of iterations
a
cc
u
ra
cy
Figure 1: Active learning curves for Resultat, Konsequenz and Motiv (random sampling versus uncer-
tainty sampling; the straight black line shows the majority baseline)
955
Konsequenz A1 A2 A3 A4 A5 A6
human 0.80 0.72 0.89 0.73 0.89 0.76
maxent 0.60 0.63 0.67 0.60 0.63 0.64
Table 5: Acc. for human annotators against the
adjudicated random samples and for the classifier
instances selected by the classifier based on the
annotation decisions of the individual annotators,
and (c) the distribution of frames in the annotated
training sets for the different annotators.
To test (a) we evaluated the annotated ran-
dom samples for Konsequenz for each annotator
against the adjudicated gold standard. Results
showed that there is no strong correlation between
the accuracy of the human annotations and the
performance of the classifier trained on these an-
notations. The annotator for whom AL worked
best had a medium score of 0.76 only, while the
annotator whose annotations were least helpful
for the classifier showed a good accuracy of 0.80
against the gold standard.
Next we tested (b) the impact of the particu-
lar instances in the AL samples for the individ-
ual annotators on classifier performance. We took
all instances in the AL data set from A6, whose
annotations gave the greatest boost to the clas-
sifier, removed the frame labels and gave them
to the remaining annotators for re-annotation.
Then we trained the classifier on each of the re-
annotated samples and compared classifier perfor-
mance. Results for 3 of the remaining annotators
were in the same range or even higher than the
ones for A6 (Figure 2). For 2 annotators, however,
results remained far below the baseline.
This again shows that the AL effect is not di-
rectly dependent on the accuracy of the individual
annotators, but that particular instances are more
informative for the classifier than others. Another
crucial point is (c) the distribution of frames in
the samples. In the annotated samples for A1 and
A2 the majority frame for Konsequenz is Causa-
tion, while in the samples for the other annotators
Response was more frequent. In our test set Re-
sponse also is the most frequent frame, therefore it
is not surprising that the classifiers trained on the
samples of A3 to A6 show a higher performance.
This means that high-quality annotations (identi-
fied by IAA) do not necessarily provide the in-
0 50 100 150
0.
3
0.
4
0.
5
0.
6
0.
7
Konsequenz: Re?annotated samples
no. of iterations
ac
cu
ra
cy
Annotator
A1
A2
A3
A4
A5
A6
Figure 2: Re-annotated instances for Konsequenz
(AL samples from annotator A6)
formation from which the classifier benefits most,
and that in a realistic annotation task address-
ing the class imbalance problem (Zhu and Hovy,
2007) is crucial.
6 Conclusions
We presented the first experiment applying AL in
a real-world scenario by integrating the approach
in an ongoing annotation project. The task and
annotation environment pose specific challenges
to the AL paradigm. We showed that annotation
noise caused by biased annotators as well as erro-
neous annotations mislead the classifier and result
in skewed data sets, and that for this particular task
no time savings are to be expected when applied
to a realistic scenario. Under certain conditions,
however, classifier performance can improve over
the random sampling baseline even on noisy data
and thus yield higher accuracy in the active learn-
ing setting. Critical features which seem to influ-
cence the outcome of AL are the amount of noise
in the data as well as the distribution of frames
in training- and test sets. Therefore, addressing
the class imbalance problem is crucial for apply-
ing AL to a real annotation task.
956
Acknowledgments
This work was funded by the German Research
Foundation DFG (grant PI 154/9-3 and the MMCI
Cluster of Excellence).
References
Baldridge, Jason and Alexis Palmer. 2009. How well
does active learning actually work?: Time-based
evaluation of cost-reduction strategies for language
documentation. In Proceedings of EMNLP 2009.
Burchardt, Aljoscha, Katrin Erk, Anette Frank, An-
drea Kowalski, Sebastian Pado?, and Manfred Pinkal.
2006. The salsa corpus: a german corpus resource
for lexical semantics. In Proceedings of LREC-
2006.
Chan, Yee Seng and Hwee Tou Ng. 2007. Domain
adaptation with active learning for word sense dis-
ambiguation. In Proceedings of ACL-2007.
Chen, Jinying, Andrew Schein, Lyle Ungar, and
Martha Palmer. 2006. An empirical study of the
behavior of active learning for word sense disam-
biguation. In Proceedings of NAACL-2006, New
York, NY.
Cohn, David A., Zoubin Ghahramani, and Michael I.
Jordan. 1995. Active learning with statistical mod-
els. In Tesauro, G., D. Touretzky, and T. Leen, ed-
itors, Advances in Neural Information Processing
Systems, volume 7, pages 705?712. The MIT Press.
Dang, Hoa Trang. 2004. Investigations into the role
of lexical semantics in word sense disambiguation.
PhD dissertation, University of Pennsylvania, Penn-
sylvania, PA.
Donmez, Pinar and Jaime G. Carbonell. 2008. Proac-
tive learning: Cost-sensitive active learning with
multiple imperfect oracles. In Proceedings of
CIKM08.
Erk, Katrin. 2005. Frame assignment as word sense
disambiguation. In Proceedings of the IWCS-6.
Ertekin, S?eyda, Jian Huang, L?eon Bottou, and Lee
Giles. 2007. Learning on the border: active learn-
ing in imbalanced data classification. In Proceed-
ings of CIKM ?07.
Hwa, Rebecca. 2004. Sample selection for statisti-
cal parsing. Computational Linguistics, 30(3):253?
276.
Laws, Florian and Heinrich Schu?tze. 2008. Stopping
criteria for active learning of named entity recogni-
tion. In Proceedings of Coling 2008.
Osborne, Miles and Jason Baldridge. 2004.
Ensemble-based active learning for parse selection.
In Proceedings of HLT-NAACL 2004.
Rehbein, Ines and Josef Ruppenhofer. 2010. Theres
no data like more data? revisiting the impact of
data size on a classification task. In Proceedings
of LREC-07, 2010.
Reichart, Roi, Katrin Tomanek, Udo Hahn, and Ari
Rappoport. 2008. Multi-task active learning for
linguistic annotations. In Proceedings of ACL-08:
HLT.
Ringger, Eric, Peter Mcclanahan, Robbie Haertel,
George Busby, Marc Carmen, James Carroll, and
Deryle Lonsdale. 2007. Active learning for part-
of-speech tagging: Accelerating corpus annotation.
In Proceedings of ACL Linguistic Annotation Work-
shop.
Tomanek, Katrin and Udo Hahn. 2009. Reducing
class imbalance during active learning for named
entity annotation. In Proceedings of the 5th Interna-
tional Conference on Knowledge Capture, Redondo
Beach, CA.
Tomanek, Katrin, Joachim Wermter, and Udo Hahn.
2007. An approach to text corpus construction
which cuts annotation costs and maintains corpus
reusability of annotated data. In Proceedings of
EMNLP-CoNLL 2007.
Zhu, Jingbo and Ed Hovy. 2007. Active learning
for word sense disambiguation with methods for ad-
dressing the class imbalance problem. In Proceed-
ings of EMNLP-CoNLL 2007.
Zhu, Jingbo, Huizhen Wang, Tianshun Yao, and Ben-
jamin K. Tsou. 2008. Active learning with sam-
pling by uncertainty and density for word sense dis-
ambiguation and text classification. In Proceedings
of Coling 2008.
957
Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 1087?1097,
Uppsala, Sweden, 11-16 July 2010. c?2010 Association for Computational Linguistics
Hard Constraints for Grammatical Function Labelling
Wolfgang Seeker
University of Stuttgart
Institut fu?r Maschinelle Sprachverarbeitung
seeker@ims.uni-stuttgart.de
Ines Rehbein
University of Saarland
Dep. for Comp. Linguistics & Phonetics
rehbein@coli.uni-sb.de
Jonas Kuhn
University of Stuttgart
Institut fu?r Maschinelle Sprachverarbeitung
jonas@ims.uni-stuttgart.de
Josef van Genabith
Dublin City University
CNGL and School of Computing
josef@computing.dcu.ie
Abstract
For languages with (semi-) free word or-
der (such as German), labelling gramma-
tical functions on top of phrase-structural
constituent analyses is crucial for making
them interpretable. Unfortunately, most
statistical classifiers consider only local
information for function labelling and fail
to capture important restrictions on the
distribution of core argument functions
such as subject, object etc., namely that
there is at most one subject (etc.) per
clause. We augment a statistical classifier
with an integer linear program imposing
hard linguistic constraints on the solution
space output by the classifier, capturing
global distributional restrictions. We show
that this improves labelling quality, in par-
ticular for argument grammatical func-
tions, in an intrinsic evaluation, and, im-
portantly, grammar coverage for treebank-
based (Lexical-Functional) grammar ac-
quisition and parsing, in an extrinsic eval-
uation.
1 Introduction
Phrase or constituent structure is often regarded as
an analysis step guiding semantic interpretation,
while grammatical functions (i. e. subject, object,
modifier etc.) provide important information rele-
vant to determining predicate-argument structure.
In languages with restricted word order (e. g.
English), core grammatical functions can often
be recovered from configurational information in
constituent structure analyses. By contrast, sim-
ple constituent structures are not sufficient for less
configurational languages, which tend to encode
grammatical functions by morphological means
(Bresnan, 2001). Case features, for instance, can
be important indicators of grammatical functions.
Unfortunately, many of these languages (including
German) exhibit strong syncretism where morpho-
logical cues can be highly ambiguous with respect
to functional information.
Statistical classifiers have been successfully
used to label constituent structure parser output
with grammatical function information (Blaheta
and Charniak, 2000; Chrupa?a and Van Genabith,
2006). However, as these approaches tend to
use only limited and local context information
for learning and prediction, they often fail to en-
force simple yet important global linguistic con-
straints that exist for most languages, e. g. that
there will be at most one subject (object) per sen-
tence/clause.1
?Hard? linguistic constraints, such as these,
tend to affect mostly the ?core grammatical func-
tions?, i. e. the argument functions (rather than
e. g. adjuncts) of a particular predicate. As these
functions constitute the core meaning of a sen-
tence (as in: who did what to whom), it is impor-
tant to get them right. We present a system that
adds grammatical function labels to constituent
parser output for German in a postprocessing step.
We combine a statistical classifier with an inte-
ger linear program (ILP) to model non-violable
global linguistic constraints, restricting the solu-
tion space of the classifier to those labellings that
comply with our set of global constraints. There
are, of course, many other ways of including func-
tional information into the output of a syntactic
parser. Klein and Manning (2003) show that merg-
ing some linguistically motivated function labels
with specific syntactic categories can improve the
performance of a PCFG model on Penn-II En-
1Coordinate subjects/objects form a constituent that func-
tions as a joint subject/object.
1087
glish data.2 Tsarfaty and Sim?aan (2008) present
a statistical model (Relational-Realizational Pars-
ing) that alternates between functional and config-
urational information for constituency tree pars-
ing and Hebrew data. Dependency parsers like
the MST parser (McDonald and Pereira, 2006) and
Malt parser (Nivre et al, 2007) use function labels
as core part of their underlying formalism. In this
paper, we focus on phrase structure parsing with
function labelling as a post-processing step.
Integer linear programs have already been suc-
cessfully used in related fields including semantic
role labelling (Punyakanok et al, 2004), relation
and entity classification (Roth and Yih, 2004), sen-
tence compression (Clarke and Lapata, 2008) and
dependency parsing (Martins et al, 2009). Early
work on function labelling for German (Brants et
al., 1997) reports 94.2% accuracy on gold data (a
very early version of the TiGer Treebank (Brants
et al, 2002)) using Markov models. Klenner
(2007) uses a system similar to ? but more re-
stricted than ? ours to label syntactic chunks de-
rived from the TiGer Treebank. His research fo-
cusses on the correct selection of predefined sub-
categorisation frames for a verb (see also Klenner
(2005)). By contrast, our research does not involve
subcategorisation frames as an external resource,
instead opting for a less knowledge-intensive ap-
proach. Klenner?s system was evaluated on gold
treebank data and used a small set of 7 dependency
labels. We show that an ILP-based approach can
be scaled to a large and comprehensive set of 42
labels, achieving 97.99% label accuracy on gold
standard trees. Furthermore, we apply the sys-
tem to automatically parsed data using a state-of-
the-art statistical phrase-structure parser with a la-
bel accuracy of 94.10%. In both cases, the ILP-
based approach improves the quality of argument
function labelling when compared with a non-ILP-
approach. Finally, we show that the approach
substantially improves the quality and coverage
(from 93.6% to 98.4%) of treebank-based Lexical-
Functional Grammars for German over previous
work in Rehbein and van Genabith (2009).
The paper is structured as follows: Section 2
presents basic data demonstrating the challenges
presented by German word order and case syn-
cretism for the function labeller. Section 3 de-
2Table 6 shows that for our data a model with merged
category and function labels (but without hard constraints!)
performs slightly worse than the ILP approach developed in
this paper.
scribes the labeller including the feature model of
the classifier and the integer linear program used
to pick the correct labelling. The evaluation part
(Section 4) is split into an intrinsic evaluation mea-
suring the quality of the labelling directly using
the German TiGer Treebank (Brants et al, 2002),
and an extrinsic evaluation where we test the im-
pact of the constraint-based labelling on treebank-
based automatic LFG grammar acquisition.
2 Data
Unlike English, German exhibits a relatively free
word order, i. e. in main clauses, the verb occu-
pies second position (the last position in subor-
dinated clauses) and arguments and adjuncts can
be placed (fairly) freely. The grammatical func-
tion of a noun phrase is marked morphologically
on its constituting parts. Determiners, pronouns,
adjectives and nouns carry case markings and in
order to be well-formed, all parts of a noun phrase
have to agree on their case features. German uses
a nominative?accusative system to mark predicate
arguments. Subjects are marked with nominative
case, direct objects carry accusative case. Further-
more, indirect objects are mostly marked with da-
tive case and sometimes genitive case.
(1) Der Lo?we
NOM
the lion
gibt
gives
dem Wolf
DAT
the wolf
einen Besen.
ACC
a broom
The lion gives a broom to the wolf.
(1) shows a sentence containing the ditransi-
tive verb geben (to give) with its three arguments.
Here, the subject is unambiguously marked with
nominative case (NOM), the indirect object with
dative case (DAT) and the direct object with ac-
cusative case (ACC). (2) shows possible word or-
ders for the arguments in this sentence.3
(2) Der Lo?we gibt einen Besen dem Wolf.
Dem Wolf gibt der Lo?we einen Besen.
Dem Wolf gibt einen Besen der Lo?we.
Einen Besen gibt der Lo?we dem Wolf.
Einen Besen gibt dem Wolf der Lo?we.
Since all permutations of arguments are possi-
ble, there is no chance for a statistical classifier to
decide on the correct function of a noun phrase by
its position alone. Introducing adjuncts to this ex-
ample makes matters even worse.
3Note that although (apart from the position of the finite
verb) there are no syntactic restrictions on the word order,
there are restrictions pertaining to phonological or informa-
tion structure.
1088
Case information for a given noun phrase can
give a classifier some clue about the correct ar-
gument function, since functions are strongly re-
lated to case values. Unfortunately, the German
case system is complex (see Eisenberg (2006) for
a thorough description) and exhibits a high degree
of case syncretism. (3) shows a sentence where
both argument NPs are ambiguous between nom-
inative or accusative case. In such cases, addi-
tional semantic or contextual information is re-
quired for disambiguation. A statistical classifier
(with access to local information only) runs a high
risk of incorrectly classifying both NPs as sub-
jects, or both as direct objects or even as nominal
predicates (which are also required to carry nom-
inative case). This would leave us with uninter-
pretable results. Uninterpretability of this kind can
be avoided if we are able to constrain the number
of subjects and objects globally to one per clause.4
(3) Das Schaf
NOM/ACC
the sheep
sieht
sees
das Ma?dchen.
NOM/ACC
the girl
EITHER The sheep sees the girl
OR The girl sees the sheep.
3 Grammatical Function Labelling
Our function labeller was developed and tested on
the TiGer Treebank (Brants et al, 2002). The
TiGer Treebank is a phrase-structure and gram-
matical function annotated treebank with 50,000
newspaper sentences from the Frankfurter Rund-
schau (Release 2, July 2006). Its overall anno-
tation scheme is quite flat to account for the rel-
atively free word order of German and does not
allow for unary branching. The annotations use
non-projective trees modelling long distance de-
pendencies directly by crossing branches. Words
are lemmatised and part-of-speech tagged with the
Stuttgart-Tu?bingen Tag Set (STTS) (Schiller et al,
1999) and contain morphological annotations (Re-
lease 2). TiGer uses 25 syntactic categories and a
set of 42 function labels to annotate the grammat-
ical function of a phrase.
The function labeller consists of two main com-
ponents, a maximum entropy classifier and an in-
teger linear program. This basic architecture was
introduced by Punyakanok et al (2004) for the
task of semantic role labelling and since then has
been applied to different NLP tasks without signif-
icant changes. In our case, its input is a bare tree
4Although the classifier may, of course, still identify the
wrong phrase as subject or object.
structure (as obtained by a standard phrase struc-
ture parser) and it outputs a tree structure where
every node is labelled with the grammatical rela-
tion it bears to its mother node. For each possi-
ble label and for each node, the classifier assigns
a probability that this node is labelled by this la-
bel. This results in a complete probability distri-
bution over all labels for each node. An integer
linear program then tries to find the optimal over-
all tree labelling by picking for each node the label
with the highest probability without violating any
of its constraints. These constraints implement lin-
guistic rules like the one-subject-per-sentence rule
mentioned above. They can also be used to cap-
ture treebank particulars, such as for example that
punctuation marks never receive a label.
3.1 The Feature Model
Maximum entropy classifiers have been used in a
wide range of applications in NLP for a long time
(Berger et al, 1996; Ratnaparkhi, 1998). They
usually give good results while at the same time
allowing for the inclusion of arbitrarily complex
features. They also have the advantage that they
directly output probability distributions over their
set of labels (unlike e. g. SVMs).
The classifier uses the following features:
? the lemma (if terminal node)
? the category (the POS for terminal nodes)
? the number of left/right sisters
? the category of the two left/right sisters
? the number of daughters
? the number of terminals covered
? the lemma of the left/right corner terminal
? the category of the left/right corner terminal
? the category of the mother node
? the category of the mother?s head node
? the lemma of the mother?s head node
? the category of the grandmother node
? the category of the grandmother?s head node
? the lemma of the grandmother?s head node
? the case features for noun phrases
? the category for PP objects
? the lemma for PP objects (if terminal node)
These features are also computed for the head
of the phrase, determined using a set of head-
finding rules in the style of Magerman (1995)
adapted to TiGer. For lemmatisation, we use Tree-
Tagger (Schmid, 1994) and case features of noun
1089
phrases are obtained from a full German morpho-
logical analyser based on (Schiller, 1994). If a
noun phrase consists of a single word (e. g. pro-
nouns, but also bare common nouns and proper
nouns), all case values output by the analyser are
used to reflect the case syncretism. For multi-word
noun phrases, the case feature is computed by tak-
ing the intersection of all case-bearing words in-
side the noun phrase, i. e. determiners, pronouns,
adjectives, common nouns and proper nouns. If,
for some reason (e.g., due to a bracketing error in
phrase structure parsing), the intersection turns out
to be empty, all four case values are assigned to the
phrase.5
3.2 Constrained Optimisation
In the second step, a binary integer linear pro-
gram is used to select those labels that optimise the
whole tree labelling. A linear program consists of
a linear objective function that is to be maximised
(or minimised) and a set of constraints which im-
pose conditions on the variables of the objective
function (see (Clarke and Lapata, 2008) for a short
but readable introduction). Although solving a lin-
ear program has polynomial complexity, requiring
the variables to be integral or binary makes find-
ing a solution exponentially hard in the worst case.
Fortunately, there are efficient algorithms which
are capable of handling a large number of vari-
ables and constraints in practical applications.6
For the function labeller, we define the set of
binary variables V = N ? L to be the crossprod-
uct of the set of nodes N and the set of labels L.
Setting a variable xn,l to 1 means that node n is
labelled by label l. Every variable is weighted by
the probability wn,l = P (l|f(n)) which the clas-
sifier has assigned to this node-label combination.
The objective function that we seek to optimise is
defined as the sum over all weighted variables:
max
?
n?N
?
l?L
wn,lxn,l (4)
Since we want every node to receive exactly one
5We decided to train the classifier on automatically
assigned and possibly ambiguous morphological informa-
tion instead of on the hand-annotated and manually disam-
biguated morphological information provided by TiGer be-
cause we want the classifier to learn the German case syn-
cretism. This way, the classifier will perform better when pre-
sented with unseen data (e.g. from parser output) for which
no hand-annotated morphological information is available.
6See lpsolve (http://lpsolve.sourceforge.net/) or GLPK
(http://www.gnu.org/software/glpk/glpk.html) for open-
source implementations
label, we add a constraint that for every node n,
exactly one of its variables is set to 1.
?
l?L
xn,l = 1 (5)
Up to now, the whole system is doing exactly
the same as an ordinary classifier that always takes
the most probable label for each node. We will
now add additional global and local linguistic con-
straints.7
The first and most important constraint restricts
the number of each argument function (as opposed
to modifier functions) to at most one per clause.
Let D ? N ? N be the direct dominance rela-
tion between the nodes of the current tree. For ev-
ery node n with category S (sentence) or VP (verb
phrase), at most one of its daughters is allowed
to be labelled SB (subject). The single-subject-
function condition is defined as:
cat(n) ? {S, V P} ??
?
?n,m??D
xm,SB ? 1 (6)
Identical constraints are added for labels OA,
OA2, DA, OG, OP, PD, OC, EP.8
We add further constraints to capture the follow-
ing linguistic restrictions:
? Of all daughters of a phrase, only one is allowed
to be labelled HD (head).
?
?n,m??D
xm,HD ? 1 (7)
? If a noun phrase carries no case feature for nom-
inative case, it cannot be labelled SB, PD or EP.
case(n) 6= nom ??
?
l?{SB,PD,EP}
xn,l = 0
(8)
? If a noun phrase carries no case feature for ac-
cusative case, it cannot be labelled OA or OA2.
? If a noun phrase carries no case feature for da-
tive case, it cannot be labelled DA.
? If a noun phrase carries no case feature for gen-
itive case, it cannot be labelled OG or AG9.
7Note that some of these constraints are language specific
in that they represent linguistic facts about German and do
not necessarily hold for other languages. Furthermore, the
constraints are treebank specific to a certain degree in that
they use a TiGer-specific set of labels and are conditioned on
TiGer-specific configurations and categories.
8SB = subject, OA = accusative object, OA2 = sec-
ond accusative object, DA = dative, OG = genitive object,
OP = prepositional object, PD = predicate, OC = clausal ob-
ject, EP = expletive es
9AG = genitive adjunct
1090
Unlike Klenner (2007), we do not use prede-
fined subcategorization frames, instead letting the
statistical model choose arguments.
In TiGer, sentences whose main verbs are
formed from auxiliary-participle combinations,
are annotated by embedding the participle under
an extra VP node and non-subject arguments are
sisters to the participle. Therefore we add an ex-
tension of the constraint in (6) to the constraint set
in order to also include the daughters of an embed-
ded VP node in such a case.
Because of the particulars of the annotation
scheme of TiGer, we can decide some labels in
advance. As mentioned before, punctuation does
not get a label in TiGer. We set the label for those
nodes to ?? (no label). Other examples are:
? If a node?s category is PTKVZ (separated verb
particle), it is labeled SVP (separable verb par-
ticle).
cat(n) = PTKV Z ?? xn,SV P = 1 (9)
? If a node?s category is APPR, APPRART,
APPO or APZR (prepositions), it is labeled AC
(adpositional case marker).
? All daughters of an MTA node (multi-token
adjective) are labeled ADC (adjective compo-
nent).
These constraints are conditioned on part-of-
speech tags and require high POS-tagging accu-
racy (when dealing with raw text).
Due to the constraints imposed on the classifi-
cation, the function labeller can no longer assign
two subjects to the same S node. Faced with two
nodes whose most probable label is SB, it has to
decide on one of them taking the next best label for
the other. This way, it outputs the optimal solution
with respect to the set of constraints. Note that this
requires the feature model not only to rank the cor-
rect label highest but also to provide a reasonable
ranking of the other labels as well.
4 Evaluation
We conducted a number of experiments using
1,866 sentences of the TiGer Dependency Bank
(Forst et al, 2004) as our test set. The TiGerDB is
a part of the TiGer Treebank semi-automatically
converted into a dependency representation. We
use the manually labelled TiGer trees correspond-
ing to the sentences in the TiGerDB for assessing
the labelling quality in the intrinsic evaluation, and
the dependencies from TiGerDB for assessing the
quality and coverage of the automatically acquired
LFG resources in the extrinsic evaluation.
In order to test on real parser output, the test
set was parsed with the Berkeley Parser (Petrov et
al., 2006) trained on 48k sentences of the TiGer
corpus (Table 1), excluding the test set. Since the
Berkeley Parser assumes projective structures, the
training data and test data were made projective by
raising non-projective nodes in the tree (Ku?bler,
2005).
precision 83.60 recall 82.81
f-score 83.20 tagging acc. 97.97
Table 1: evalb unlabelled parsing scores on test set for Berke-
ley Parser trained on 48,000 sentences (sentence length? 40)
The maximum entropy classifier of the func-
tion labeller was trained on 46,473 sentences of
the TiGer Treebank (excluding the test set) which
yields about 1.2 million nodes as training samples.
For training the Maximum Entropy Model, we
used the BLMVM algorithm (Benson and More,
2001) with a width factor of 1.0 (Kazama and Tsu-
jii, 2005) implemented in an open-source C++ li-
brary from Tsujii Laboratory.10 The integer linear
program was solved with the simplex algorithm in
combination with a branch-and-bound method us-
ing the freely available GLPK.11
4.1 Intrinsic Evaluation
In the intrinsic evaluation, we measured the qual-
ity of the labelling itself. We used the node
span evaluation method of (Blaheta and Char-
niak, 2000) which takes only those nodes into ac-
count which have been recognised correctly by the
parser, i.e. if there are two nodes in the parse and
the reference treebank tree which cover the same
word span. Unlike Blaheta and Charniak (2000)
however, we do not require the two nodes to carry
the same syntactic category label.12
Table 2 shows the results of the node span eval-
uation. The labeller achieves close to 98% label
accuracy on gold treebank trees which shows that
the feature model captures the differences between
the individual labels well. Results on parser output
are about 4 percentage points (absolute) lower as
parsing errors can distort local context features for
the classifier even if the node itself has been parsed
10http://www-tsujii.is.s.u-tokyo.ac.jp/?tsuruoka/maxent/
11http://www.gnu.org/software/glpk/glpk.html
12We also excluded the root node, all punctuation marks
and both nodes in unary branching sub-trees from evaluation.
1091
correctly. The addition of the ILP constraints im-
proves results only slightly since the constraints
affect only (a small number of) argument labels
while the evaluation considers all 40 labels occur-
ring in the test set. Since the constraints restrict the
selection of certain labels, a less probable label has
to be picked by the labeller if the most probable
is not available. If the classifier is ranking labels
sensibly, the correct label should emerge. How-
ever, with an incorrect ranking, the ILP constraints
might also introduce new errors.
label accuracy error red.
without constraints
gold 44689/45691 = 97.81% ?
parser 40578/43140 = 94.06% ?
with constraints
gold 44773/45691 = 97.99%* 8.21%
parser 40593/43140 = 94.10% 0.68%
Table 2: label accuracy and error reduction (all labels) for
node span evaluation, * statistically significant, sign test, ? =
0.01 (Koo and Collins, 2005)
As the main target of the constraint set are argu-
ment functions, we also tested the quality of argu-
ment labels. Table 3 shows the node span evalua-
tion in terms of precision, recall and f-score for ar-
gument functions only, with clear statistically sig-
nificant improvements.
prec. rec. f-score
without constraints
gold standard 92.41 91.86 92.13
parser output 88.14 86.43 87.28
with constraints
gold standard 94.31 92.76 93.53*
parser output 89.51 86.73 88.09*
Table 3: node span results for the test set, argument functions
only (SB, EP, PD, OA, OA2, DA, OG, OP, OC), * statistically
significant, sign test, ? = 0.01 (Koo and Collins, 2005)
For comparison and to establish a highly com-
petitive baseline, we use the best-scoring system
in (Chrupa?a and Van Genabith, 2006), trained and
tested on exactly the same data sets. This purely
statistical labeller achieves accuracy of 96.44%
(gold) and 92.81% (parser) for all labels, and f-
scores of 89.88% (gold) and 84.98% (parser) for
argument labels. Tables 2 and 3 show that our sys-
tem (with and even without ILP constraints) com-
prehensively outperforms all corresponding base-
line scores.
The node span evaluation defines a correct la-
belling by taking only those nodes (in parser out-
put) into account that have a corresponding node
in the reference tree. However, as this restricts at-
tention to correctly parsed nodes, the results are
somewhat over-optimistic. Table 4 provides the
results obtained from an evalb evaluation of the
same data sets.13 The gold standard scores are
high confirming our previous findings about the
performance of the function labeller. However,
the results on parser output are much worse. The
evaluation scores are now taking the parsing qual-
ity into account (Table 1). The considerable drop
in quality between gold trees and parser output
clearly shows that a good parse tree is an impor-
tant prerequisite for reasonable function labelling.
This is in accordance with previous findings by
Punyakanok et al (2008) who emphasise the im-
portance of syntactic parsing for the closely re-
lated task of semantic role labelling.
prec. rec. f-score
without constraints
gold standard 95.94 95.94 95.94
parser output 76.27 75.55 75.91
with constraints
gold standard 96.21 96.21 96.21
parser output 76.36 75.64 76.00
Table 4: evalb results for the test set
4.1.1 Subcategorisation Frames
Early on in the paper we mention that, unlike e. g.
Klenner (2007), we did not include predefined
subcategorisation frames into the constraint set,
but rather let the joint statistical and ILP models
decide on the correct type of arguments assigned
to a verb. The assumption is that if one uses prede-
fined subcategorisation frames which fix the num-
ber and type of arguments for a verb, one runs the
risk of excluding correct labellings due to missing
subcat frames, unless a very comprehensive and
high quality subcat lexicon resource is available.
In order to test this assumption, we run an addi-
tional experiment with about 10,000 verb frames
for 4,508 verbs, which were automatically ex-
tracted from our training section. Following Klen-
ner (2007), for each verb and for each subcat frame
for this verb attested at least once in the training
data, we introduce a new binary variable fn to
the ILP model representing the n-th frame (for the
verb) weighted by its frequency.
We add an ILP constraint requiring exactly one
of the frames to be set to one (each verb has to have
a subcat frame) and replace the ILP constraint in
(6) by:
13Function labels were merged with the category symbols.
1092
??n,m??D
xm,SB ?
?
SB?fi
fi = 0 (10)
This constraint requires the number of subjects
in a phrase to be equal to the number of selected14
verb frames that require a subject. As each verb
is constrained to ?select? exactly one subcat frame
(see additional ILP constraint above), there is at
most one subject per phrase, if the frame in ques-
tion requires a subject. If the selected frame does
not require a subject, then the constraint blocks the
assignment of subjects for the entire phrase. The
same was done for the other argument functions
and as before we included an extension of this con-
straint to cover embedded VPs. For unseen verbs
(i.e. verbs not attested in the training set) we keep
the original constraints as a back-off.
prec. rec. f-score
all labels (cmp. Table 2)
gold standard 97.24 97.24 97.24
parser output 93.43 93.43 93.43
argument functions only (cmp. Table 3)
gold standard 91.36 90.12 90.74
parser output 86.64 84.38 85.49
Table 5: node span results for the test set using constraints
with automatically extracted subcat frames
Table 5 shows the results of the test set node
span evaluation when using the ILP system en-
hanced with subcat frames. Compared to Tables 2
and 3, the results are clearly inferior, and particu-
larly so for argument grammatical functions. This
seems to confirm our assumption that, given our
data, letting the joint statistical and ILP model de-
cide argument functions is superior to an approach
that involves subcat frames. However, and impor-
tantly, our results do not rule out that a more com-
prehensive subcat frame resource may in fact re-
sult in improvements.
4.2 Extrinsic Evaluation
Over the last number of years, treebank-based
deep grammar acquisition has emerged as an
attractive alternative to hand-crafting resources
within the HPSG, CCG and LFG paradigms
(Miyao et al, 2003; Clark and Hockenmaier,
2002; Cahill et al, 2004). While most of the ini-
tial development work focussed on English, more
recently efforts have branched to other languages.
Below we concentrate on LFG.
14The variable representing this frame has been set to 1.
Lexical-Functional Grammar (Bresnan, 2001)
is a constraint-based theory of grammar with min-
imally two levels of representation: c(onstituent)-
structure and f(unctional)-structure. C-structure
(CFG trees) captures language specific surface
configurations such as word order and the hier-
archical grouping of words into phrases, while
f-structure represents more abstract (and some-
what more language independent) grammatical re-
lations (essentially bilexical labelled dependencies
with some morphological and semantic informa-
tion, approximating to basic predicate-argument
structures) in the form of attribute-value struc-
tures. F-structures are defined in terms of equa-
tions annotated to nodes in c-structure trees (gram-
mar rules). Treebank-based LFG acquisition was
originally developed for English (Cahill, 2004;
Cahill et al, 2008) and is based on an f-structure
annotation algorithm that annotates c-structure
trees (from a treebank or parser output) with
f-structure equations, which are read off of the tree
and passed on to a constraint solver producing an
f-structure for the given sentence. The English
annotation algorithm (for Penn-II treebank-style
trees) relies heavily on configurational and catego-
rial information, translating this into grammatical
functional information (subject, object etc.) rep-
resented at f-structure. LFG is ?functional? in the
mathematical sense, in that argument grammatical
functions have to be single valued (there cannot be
two or more subjects etc. in the same clause). In
fact, if two or more values are assigned to a single
argument grammatical function in a local tree, the
LFG constraint solver will produce a clash (i. e.
it will fail to produce an f-structure) and the sen-
tence will be considered ungrammatical (in other
words, the corresponding c-structure tree will be
uninterpretable).
Rehbein (2009) and Rehbein and van Genabith
(2009) develop an f-structure annotation algorithm
for German based on the TiGer treebank resource.
Unlike the English annotation algorithm and be-
cause of the language-particular properties of Ger-
man (see Section 2), the German annotation al-
gorithm cannot rely on c-structure configurational
information, but instead heavily uses TiGer func-
tion labels in the treebank. Learning function la-
bels is therefore crucial to the German LFG an-
notation algorithm, in particular when parsing raw
text. Because of the strong case syncretism in Ger-
man, traditional classification models using local
1093
information only run the risk of predicting mul-
tiple occurences of the same function (subject,
object etc.) at the same level, causing feature
clashes in the constraint solver with no f-structure
being produced. Rehbein (2009) and Rehbein
and van Genabith (2009) identify this as a major
problem resulting in a considerable loss in cov-
erage of the German annotation algorithm com-
pared to English, in particular for parsing raw text,
where TiGer function labels have to be supplied by
a machine-learning-based method and where the
coverage of the LFG annotation algorithm drops
to 93.62% with corresponding drops in recall and
f-scores for the f-structure evaluations (Table 6).
Below we test whether the coverage problems
caused by incorrect multiple assignments of gram-
matical functions can be addressed using the com-
bination of classifier with ILP constraints devel-
oped in this paper. We report experiments where
automatically parsed and labelled data are handed
over to an LFG f-structure computation algorithm.
The f-structures produced are converted into a
dependency triple representation (Crouch et al,
2002) and evaluated against TiGerDB.
cov. prec. rec. f-score
upper bound 99.14 85.63 82.58 84.07
without constraints
gold 95.82 84.71 76.68 80.49
parser 93.41 79.70 70.38 74.75
with constraints
gold 99.30 84.62 82.15 83.37
parser 98.39 79.43 75.60 77.47
Rehbein 2009
parser 93.62 79.20 68.86 73.67
Table 6: f-structure evaluation results for the test set against
TigerDB
Table 6 shows the results of the f-structure
evaluation against TiGerDB, with 84.07% f-score
upper-bound results for the f-structure annotation
algorithm on the original TiGer treebank trees
with hand-annotated function labels. Using the
function labeller without ILP constraints results in
drastic drops in coverage (between 4.5% and 6.5%
points absolute) and hence recall (6% and 12%)
and f-score (3.5% and 9.5%) for both gold trees
and parser output (compared to upper bounds).
By contrast, with ILP constraints, the loss in cov-
erage observed above almost completely disap-
pears and recall and f-scores improve by between
4.4% and 5.5% (recall) and 3% (f-score) abso-
lute (over without ILP constraints). For compar-
ison, we repeated the experiment using the best-
scoring method of Rehbein (2009). Rehbein trains
the Berkeley Parser to learn an extended category
set, merging TiGer function labels with syntactic
categories, where the parser outputs fully-labelled
trees. The results show that this approach suf-
fers from the same drop in coverage as the classi-
fier without ILP constraints, with recall about 7%
and f-score about 4% (absolute) lower than for the
classifier with ILP constraints.
Table 7 shows the dramatic effect of the ILP
constraints on the number of sentences in the test
set that have multiple argument functions of the
same type within the same clause. With ILP con-
straints, the problem disappears and therefore, less
feature-clashes occur during f-structure computa-
tion.
no constraints constraints
gold 185 0
parser 212 0
Table 7: Number of sentences in the test set with doubly an-
notated argument functions
In order to assess whether ILP constraints help
with coverage only or whether they affect the qual-
ity of the f-structures as well, we repeat the experi-
ment in Table 6, however this time evaluating only
on those sentences that receive an f-structure, ig-
noring the rest. Table 8 shows that the impact of
ILP constraints on quality is much less dramatic
than on coverage, with only very small variations
in precison, recall and f-scores across the board,
and small increases over Rehbein (2009).
cov. prec. rec. f-score
no constr. 93.41 79.70 77.89 78.79
constraints 98.39 79.43 77.85 78.64
Rehbein 93.62 79.20 76.43 77.79
Table 8: f-structure evaluation results for parser output ex-
cluding sentences without f-structures
Early work on automatic LFG acquisition and
parsing for German is presented in Cahill et al
(2003) and Cahill (2004), adapting the English
Annotation Algorithm to an earlier and smaller
version of the TiGer treebank (without morpho-
logical information) and training a parser to learn
merged Tiger function-category labels, and report-
ing 95.75% coverage and an f-score of 74.56%
f-structure quality against 2,000 gold treebank
trees automatically converted into f-structures.
Rehbein (2009) uses the larger Release 2 of the
treebank (with morphological information) report-
ing 77.79% f-score and coverage of 93.62% (Ta-
1094
ble 8) against the dependencies in the TiGerDB
test set. The only rule-based approach to German
LFG-parsing we are aware of is the hand-crafted
German grammar in the ParGram Project (Butt
et al, 2002). Forst (2007) reports 83.01% de-
pendency f-score evaluated against a set of 1,497
sentences of the TiGerDB. It is very difficult to
compare results across the board, as individual pa-
pers use (i) different versions of the treebank, (ii)
different (sections of) gold-standards to evaluate
against (gold TiGer trees in TigerDB, the depen-
dency representations provided by TigerDB, auto-
matically generated gold-standards etc.) and (iii)
different label/grammatical function sets. Further-
more, (iv) coverage differs drastically (with the
hand-crafted LFG resources achieving about 80%
full f-structures) and finally, (v) some of the gram-
mars evaluated having been used in the generation
of the gold standards, possibly introducing a bias
towards these resources: the German hand-crafted
LFG was used to produce TiGerDB (Forst et al,
2004). In order to put the results into some per-
spective, Table 9 shows an evaluation of our re-
sources against a set of automatically generated
gold standard f-structures produced by using the
f-structure annotation algorithm on the original
hand-labelled TiGer gold trees in the section cor-
responding to TiGerDB: without ILP constraints
we achieve a dependency f-score of 84.35%, with
ILP constraints 87.23% and 98.89% coverage.
cov. prec. rec. f-score
without constraints
gold 95.24 97.76 90.93 94.22
parser 93.35 88.71 80.40 84.35
with constraints
gold 99.30 97.66 97.33 97.50
parser 98.89 88.37 86.12 87.23
Table 9: f-structure evaluation results for the test set against
automatically generated goldstandard (1,850 sentences)
5 Conclusion
In this paper, we addressed the problem of assign-
ing grammatical functions to constituent struc-
tures. We have proposed an approach to grammat-
ical function labelling that combines the flexibil-
ity of a statistical classifier with linguistic expert
knowledge in the form of hard constraints imple-
mented by an integer linear program. These con-
straints restrict the solution space of the classifier
by blocking those solutions that cannot be correct.
One of the strengths of an integer linear program
is the unlimited context it can take into account
by optimising over the entire structure, providing
an elegant way of supporting classifiers with ex-
plicit linguistic knowledge while at the same time
keeping feature models small and comprehensi-
ble. Most of the constraints are direct formaliza-
tions of linguistic generalizations for German. Our
approach should generalise to other languages for
which linguistic expertise is available.
We evaluated our system on the TiGer corpus
and the TiGerDB and gave results on gold stan-
dard trees and parser output. We also applied
the German f-structure annotation algorithm to
the automatically labelled data and evaluated the
system by measuring the quality of the resulting
f-structures. We found that by using the con-
straint set, the function labeller ensures the inter-
pretability and thus the usefulness of the syntac-
tic structure for a subsequently applied processing
step. In our f-structure evaluation, that means, the
f-structure computation algorithm is able to pro-
duce an f-structure for almost all sentences.
Acknowledgements
The first author would like to thank Gerlof Bouma
for a lot of very helpful discussions. We would
like to thank our anonymous reviewers for de-
tailed and helpful comments. The research was
supported by the Science Foundation Ireland SFI
(Grant 07/CE/I1142) as part of the Centre for
Next Generation Localisation (www.cngl.ie) and
by DFG (German Research Foundation) through
SFB 632 Potsdam-Berlin and SFB 732 Stuttgart.
References
Steven J. Benson and Jorge J. More. 2001. A limited
memory variable metric method in subspaces and
bound constrained optimization problems. Techni-
cal report, Argonne National Laboratory.
Adam L. Berger, Vincent J.D. Pietra, and Stephen A.D.
Pietra. 1996. A maximum entropy approach to nat-
ural language processing. Computational linguis-
tics, 22(1):71.
Don Blaheta and Eugene Charniak. 2000. Assigning
function tags to parsed text. In Proceedings of the
1st North American chapter of the Association for
Computational Linguistics conference, pages 234 ?
240, Seattle, Washington. Morgan Kaufmann Pub-
lishers Inc.
Thorsten Brants, Wojciech Skut, and Brigitte Krenn.
1997. Tagging grammatical functions. In Proceed-
ings of EMNLP, volume 97, pages 64?74.
1095
Sabine Brants, Stefanie Dipper, Silvia Hansen, Wolf-
gang Lezius, and George Smith. 2002. The TIGER
treebank. In Proceedings of the Workshop on Tree-
banks and Linguistic Theories, page 2441.
Joan Bresnan. 2001. Lexical-Functional Syntax.
Blackwell Publishers.
Miriam Butt, Helge Dyvik, Tracy Halloway King, Hi-
roshi Masuichi, and Christian Rohrer. 2002. The
parallel grammar project. In COLING-02 on Gram-
mar engineering and evaluation-Volume 15, volume
pages, page 7. Association for Computational Lin-
guistics.
Aoife Cahill, Martin Forst, Mairead McCarthy, Ruth
ODonovan, Christian Rohrer, Josef van Genabith,
and Andy Way. 2003. Treebank-based multilingual
unification-grammar development. In Proceedings
of the Workshop on Ideas and Strategies for Multi-
lingual Grammar Development at the 15th ESSLLI,
page 1724.
Aoife Cahill, Michael Burke, Ruth O?Donovan, Josef
van Genabith, and Andy Way. 2004. Long-
distance dependency resolution in automatically ac-
quired wide-coverage PCFG-based LFG approxima-
tions. Proceedings of the 42nd Annual Meeting
on Association for Computational Linguistics - ACL
?04, pages 319?es.
Aoife Cahill, Michael Burke, Ruth O?Donovan, Stefan
Riezler, Josef van Genabith, and Andy Way. 2008.
Wide-Coverage Deep Statistical Parsing Using Au-
tomatic Dependency Structure Annotation. Compu-
tational Linguistics, 34(1):81?124, Ma?rz.
Aoife Cahill. 2004. Parsing with Automatically Ac-
quired, Wide-Coverage, Robust, Probabilistic LFG
Approximations. Ph.D. thesis, Dublin City Univer-
sity.
Grzegorz Chrupa?a and Josef Van Genabith. 2006.
Using machine-learning to assign function labels
to parser output for Spanish. In Proceedings of
the COLING/ACL main conference poster session,
page 136143, Sydney. Association for Computa-
tional Linguistics.
Stephen Clark and Judith Hockenmaier. 2002. Evalu-
ating a wide-coverage CCG parser. In Proceedings
of the LREC 2002, pages 60?66.
James Clarke and Mirella Lapata. 2008. Global in-
ference for sentence compression an integer linear
programming approach. Journal of Artificial Intelli-
gence Research, 31:399?429.
Richard Crouch, Ronald M. Kaplan, Tracy Halloway
King, and Stefan Riezler. 2002. A comparison of
evaluation metrics for a broad-coverage stochastic
parser. In Proceedings of LREC 2002 Workshop,
pages 67?74, Las Palmas, Canary Islands, Spain.
Peter Eisenberg. 2006. Grundriss der deutschen
Grammatik: Das Wort. J.B. Metzler, Stuttgart, 3
edition.
Martin Forst, Nu?ria Bertomeu, Berthold Crysmann,
Frederik Fouvry, Silvia Hansen-Shirra, and Valia
Kordoni. 2004. Towards a dependency-based gold
standard for German parsers The TiGer Dependency
Bank. In Proceedings of the COLING Workshop
on Linguistically Interpreted Corpora (LINC ?04),
Geneva, Switzerland.
Martin Forst. 2007. Filling Statistics with Linguistics
Property Design for the Disambiguation of German
LFG Parses. In Proceedings of ACL 2007. Associa-
tion for Computational Linguistics.
Jun?Ichi Kazama and Jun?Ichi Tsujii. 2005. Maxi-
mum entropy models with inequality constraints: A
case study on text categorization. Machine Learn-
ing, 60(1):159194.
Dan Klein and Christopher D. Manning. 2003. Accu-
rate unlexicalized parsing. In Proceedings of ACL
2003, pages 423?430, Morristown, NJ, USA. Asso-
ciation for Computational Linguistics.
Manfred Klenner. 2005. Extracting Predicate Struc-
tures from Parse Trees. In Proceedings of the
RANLP 2005.
Manfred Klenner. 2007. Shallow dependency label-
ing. In Proceedings of the ACL 2007 Demo and
Poster Sessions, page 201204, Prague. Association
for Computational Linguistics.
Terry Koo and Michael Collins. 2005. Hidden-
variable models for discriminative reranking. In
Proceedings of the conference on Human Language
Technology and Empirical Methods in Natural Lan-
guage Processing - HLT ?05, pages 507?514, Mor-
ristown, NJ, USA. Association for Computational
Linguistics.
Sandra Ku?bler. 2005. How Do Treebank Annotation
Schemes Influence Parsing Results? Or How Not to
Compare Apples And Oranges. In Proceedings of
RANLP 2005, Borovets, Bulgaria.
David M. Magerman. 1995. Statistical decision-tree
models for parsing. In Proceedings of the 33rd an-
nual meeting on Association for Computational Lin-
guistics, page 276283, Morristown, NJ, USA. Asso-
ciation for Computational Linguistics Morristown,
NJ, USA.
Andre? F. T. Martins, Noah A. Smith, and Eric P. Xing.
2009. Concise integer linear programming formu-
lations for dependency parsing. In Proceedings of
ACL 2009.
Ryan McDonald and Fernando Pereira. 2006. Online
learning of approximate dependency parsing algo-
rithms. In Proceedings of EACL, volume 6.
Yusuke Miyao, Takashi Ninomiya, and Jun?ichi Tsujii.
2003. Probabilistic modeling of argument structures
including non-local dependencies. In Proceedings
of the Conference on Recent Advances in Natural
Language Processing RANLP 2003, volume 2.
1096
Joakim Nivre, Johan Hall, Jens Nilsson, Atanas
Chanev, Gu?lsen Eryigit, Sandra Ku?bler, Svetoslav
Marinov, and Erwin Marsi. 2007. MaltParser:
A language-independent system for data-driven de-
pendency parsing. Natural Language Engineering,
13(2):95?135, Januar.
Slav Petrov, Leon Barrett, Romain Thibaux, and Dan
Klein. 2006. Learning accurate, compact, and
interpretable tree annotation. In Proceedings of
the 21st International Conference on Computational
Linguistics and the 44th annual meeting of the ACL
- ACL ?06, pages 433?440, Morristown, NJ, USA.
Association for Computational Linguistics.
Vasin Punyakanok, Wen-Tau Yih, Dan Roth, and Dav
Zimak. 2004. Semantic role labeling via integer
linear programming inference. In Proceedings of
the 20th international conference on Computational
Linguistics - COLING ?04, Morristown, NJ, USA.
Association for Computational Linguistics.
Vasin Punyakanok, Dan Roth, and Wen-tau Yih. 2008.
The Importance of Syntactic Parsing and Inference
in Semantic Role Labeling. Computational Linguis-
tics, 34(2):257?287, Juni.
Adwait Ratnaparkhi. 1998. Maximum Entropy Models
for Natural Language Ambiguity Resolution. Ph.D.
thesis, University of Pennsylvania.
Ines Rehbein and Josef van Genabith. 2009. Auto-
matic Acquisition of LFG Resources for German-
As Good as it gets. In Miriam Butt and Tracy Hol-
loway King, editors, Proceedings of LFG Confer-
ence 2009. CSLI Publications.
Ines Rehbein. 2009. Treebank-based grammar acqui-
sition for German. Ph.D. thesis, Dublin City Uni-
versity.
Dan Roth and Wen-Tau Yih. 2004. A linear program-
ming formulation for global inference in natural lan-
guage tasks. In Proceedings of CoNNL 2004.
Anne Schiller, Simone Teufel, and Christine Sto?ckert.
1999. Guidelines fu?r das Tagging deutscher
Textcorpora mit STTS (Kleines und gro?es Tagset).
Technical Report August, Universita?t Stuttgart.
Anne Schiller. 1994. Dmor - user?s guide. Technical
report, University of Stuttgart.
Helmut Schmid. 1994. Probabilistic Part-of-Speech
Tagging Using Decision Trees. In Proceedings of
International Conference on New Methods in Lan-
guage Processing, volume 12. Manchester, UK.
Reut Tsarfaty and Khalil Sima?an. 2008. Relational-
realizational parsing. In Proceedings of the 22nd In-
ternational Conference on Computational Linguis-
tics - COLING ?08, pages 889?896, Morristown, NJ,
USA. Association for Computational Linguistics.
1097
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 43?51,
Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational Linguistics
Evaluating the Impact of Coder Errors on Active Learning
Ines Rehbein
Computational Linguistics
Saarland University
rehbein@coli.uni-sb.de
Josef Ruppenhofer
Computational Linguistics
Saarland University
josefr@coli.uni-sb.de
Abstract
Active Learning (AL) has been proposed as a
technique to reduce the amount of annotated
data needed in the context of supervised clas-
sification. While various simulation studies
for a number of NLP tasks have shown that
AL works well on goldstandard data, there is
some doubt whether the approach can be suc-
cessful when applied to noisy, real-world data
sets. This paper presents a thorough evalua-
tion of the impact of annotation noise on AL
and shows that systematic noise resulting from
biased coder decisions can seriously harm the
AL process. We present a method to filter out
inconsistent annotations during AL and show
that this makes AL far more robust when ap-
plied to noisy data.
1 Introduction
Supervised machine learning techniques are still the
mainstay for many NLP tasks. There is, how-
ever, a well-known bottleneck for these approaches:
the amount of high-quality data needed for train-
ing, mostly obtained by human annotation. Active
Learning (AL) has been proposed as a promising ap-
proach to reduce the amount of time and cost for hu-
man annotation. The idea behind active learning is
quite intuitive: instead of annotating a large number
of randomly picked instances we carefully select a
small number of instances that are maximally infor-
mative for the machine learning classifier. Thus a
smaller set of data points is able to boost classifier
performance and to yield an accuracy comparable to
the one obtained when training the same system on
a larger set of randomly chosen data.
Active learning has been applied to several NLP
tasks like part-of-speech tagging (Ringger et al,
2007), chunking (Ngai and Yarowsky, 2000), syn-
tactic parsing (Osborne and Baldridge, 2004; Hwa,
2004), Named Entity Recognition (Shen et al,
2004; Laws and Schu?tze, 2008; Tomanek and Hahn,
2009), Word Sense Disambiguation (Chen et al,
2006; Zhu and Hovy, 2007; Chan and Ng, 2007),
text classification (Tong and Koller, 1998) or statis-
tical machine translation (Haffari and Sarkar, 2009),
and has been shown to reduce the amount of anno-
tated data needed to achieve a certain classifier per-
formance, sometimes by as much as half. Most of
these studies, however, have only simulated the ac-
tive learning process using goldstandard data. This
setting is crucially different from a real world sce-
nario where we have to deal with erroneous data
and inconsistent annotation decisions made by the
human annotators. While simulations are an indis-
pensable instrument to test different parameters and
settings, it has been shown that when applying AL
to highly ambiguous tasks like e.g. Word Sense
Disambiguation (WSD) with fine-grained sense dis-
tinctions, AL can actually harm the learning process
(Dang, 2004; Rehbein et al, 2010). Dang suggests
that the lack of a positive effect of AL might be due
to inconsistencies in the human annotations and that
AL cannot efficiently be applied to tasks which need
double blind annotation with adjudication to insure
a sufficient data quality. Even if we take a more opti-
mistic view and assume that AL might still be useful
even for tasks featuring a high degree of ambiguity,
it remains crucial to address the problem of annota-
tion noise and its impact on AL.
43
In this paper we present a thorough evaluation of
the impact of annotation noise on AL. We simulate
different types of coder errors and assess the effect
on the learning process. We propose a method to de-
tect inconsistencies and remove them from the train-
ing data, and show that our method does alleviate the
problem of annotation noise in our experiments.
The paper is structured as follows. Section 2 re-
ports on recent research on the impact of annota-
tion noise in the context of supervised classification.
Section 3 describes the experimental setup of our
simulation study and presents results. In Section 4
we present our filtering approach and show its im-
pact on AL performance. Section 5 concludes and
outlines future work.
2 Related Work
We are interested in the question whether or not AL
can be successfully applied to a supervised classifi-
cation task where we have to deal with a consider-
able amount of inconsistencies and noise in the data,
which is the case for many NLP tasks (e.g. sen-
timent analysis, the detection of metaphors, WSD
with fine-grained word senses, to name but a few).
Therefore we do not consider part-of-speech tag-
ging or syntactic parsing, where coders are expected
to agree on most annotation decisions. Instead,
we focus on work on AL for WSD, where inter-
coder agreement (at least for fine-grained annotation
schemes) usually is much lower than for the former
tasks.
2.1 Annotation Noise
Studies on active learning for WSD have been lim-
ited to running simulations of AL using gold stan-
dard data and a coarse-grained annotation scheme
(Chen et al, 2006; Chan and Ng, 2007; Zhu and
Hovy, 2007). Two exceptions are Dang (2004) and
Rehbein et al (2010) who both were not able to
replicate the positive findings obtained for AL for
WSD on coarse-grained sense distinctions. A pos-
sible reason for this failure is the amount of annota-
tion noise in the training data which might mislead
the classifier during the AL process. Recent work on
the impact of annotation noise on a machine learning
task (Reidsma and Carletta, 2008) has shown that
random noise can be tolerated in supervised learn-
ing, while systematic errors (as caused by biased an-
notators) can seriously impair the performance of a
supervised classifier even if the observed accuracy
of the classifier on a test set coming from the same
population as the training data is as high as 0.8.
Related work (Beigman Klebanov et al, 2008;
Beigman Klebanov and Beigman, 2009) has been
studying annotation noise in a multi-annotator set-
ting, distinguishing between hard cases (unreliably
annotated due to genuine ambiguity) and easy cases
(reliably annotated data). The authors argue that
even for those data points where the annotators
agreed on one particular class, a proportion of the
agreement might be merely due to chance. Fol-
lowing this assumption, the authors propose a mea-
sure to estimate the amount of annotation noise in
the data after removing all hard cases. Klebanov
et al (2008; 2009) show that, according to their
model, high inter-annotator agreement (?) achieved
in an annotation scenario with two annotators is no
guarantee for a high-quality data set. Their model,
however, assumes that a) all instances where anno-
tators disagreed are in fact hard cases, and b) that for
the hard cases the annotators decisions are obtained
by coin-flips. In our experience, some amount of
disagreement can also be observed for easy cases,
caused by attention slips or by a deviant interpre-
tation of some class(es) by one of the annotators,
and the annotation decision of an individual annota-
tor cannot so much be described as random choice
(coin-flip) but as systematically biased selection,
causing the types of errors which have been shown
to be problematic for supervised classification (Rei-
dsma and Carletta, 2008).
Further problems arise in the AL scenario where
the instances to be annotated are selected as a func-
tion of the sampling method and the annotation
judgements made before. Therefore, Beigman and
Klebanov Beigman (2009)?s approach of identify-
ing unreliably annotated instances by disagreement
is not applicable to AL, as most instances are anno-
tated only once.
2.2 Annotation Noise and Active Learning
For AL to be succesful, we need to remove system-
atic noise in the training data. The challenge we face
is that we only have a small set of seed data and no
information about the reliability of the annotations
44
assigned by the human coders.
Zhu et al (2008) present a method for detecting
outliers in the pool of unannotated data to prevent
these instances from becoming part of the training
data. This approach is different from ours, where
we focus on detecting annotation noise in the man-
ually labelled training data produced by the human
coders.
Schein and Ungar (2007) provide a systematic in-
vestigation of 8 different sampling methods for AL
and their ability to handle different types of noise
in the data. The types of noise investigated are a)
prediction residual error (the portion of squared er-
ror that is independent of training set size), and b)
different levels of confusion among the categories.
Type a) models the presence of unknown features
that influence the true probabilities of an outcome: a
form of noise that will increase residual error. Type
b) models categories in the data set which are intrin-
sically hard to disambiguate, while others are not.
Therefore, type b) errors are of greater interest to us,
as it is safe to assume that intrinsically ambiguous
categories will lead to biased coder decisions and
result in the systematic annotation noise we are in-
terested in.
Schein and Ungar observe that none of the 8
sampling methods investigated in their experiment
achieved a significant improvement over the random
sampling baseline on type b) errors. In fact, en-
tropy sampling and margin sampling even showed a
decrease in performance compared to random sam-
pling. For AL to work well on noisy data, we need
to identify and remove this type of annotation noise
during the AL process. To the best of our knowl-
edge, there is no work on detecting and removing
annotation noise by human coders during AL.
3 Experimental Setup
To make sure that the data we use in our simula-
tion is as close to real-world data as possible, we do
not create an artificial data set as done in (Schein
and Ungar, 2007; Reidsma and Carletta, 2008) but
use real data from a WSD task for the German verb
drohen (threaten).1 Drohen has three different word
senses which can be disambiguated by humans with
1The data has been provided by the SALSA project:
http://www.coli.uni-saarland.de/projects/salsa
a high accuracy.2 This point is crucial to our setup.
To control the amount of noise in the data, we need
to be sure that the initial data set is noise-free.
For classification we use a maximum entropy
classifier.3 Our sampling method is uncertainty sam-
pling (Lewis and Gale, 1994), a standard sampling
heuristic for AL where new instances are selected
based on the confidence of the classifier for predict-
ing the appropriate label. As a measure of uncer-
tainty we use Shannon entropy (1) (Zhang and Chen,
2002) and the margin metric (2) (Schein and Ungar,
2007). The first measure considers the model?s pre-
dictions q for each class c and selects those instances
from the pool where the Shannon entropy is highest.
?
?
c
qc log qc (1)
The second measure looks at the difference be-
tween the largest two values in the prediciton vector
q, namely the two predicted classes c, c? which are,
according to our model, the most likely ones for in-
stance xn, and selects those instances where the dif-
ference (margin) between the two predicted proba-
bilities is the smallest. We discuss some details of
this metric in Section 4.
Mn = |P (c|xn) ? P (c?|xn)| (2)
The features we use for WSD are a combination
of context features (word token with window size 11
and POS context with window size 7), syntactic fea-
tures based on the output of a dependency parser4
and semantic features based on GermaNet hyper-
onyms. These settings were tuned to the target verb
by (Rehbein et al, 2009). All results reported below
are averages over a 5-fold cross validation.
3.1 Simulating Coder Errors in AL
Before starting the AL trials we automatically sepa-
rate the 2,500 sentences into test set (498 sentences)
and pool (2,002 sentences),5 retaining the overall
distribution of word senses in the data set. We in-
sert a varying amount of noise into the pool data,
2In a pilot study where two human coders assigned labels to
a set of 100 sentences, the coders agreed on 99% of the data.
3http://maxent.sourceforge.net
4The MaltParser: http://maltparser.org
5The split has been made automatically, the unusual num-
bers are caused by rounding errors.
45
test pool
ALrand ALbias
% errors 0% 0% 30% 30%
drohen1-salsa 126 506 524 514
Comittment 129 520 522 327
Run risk 243 976 956 1161
Total 498 2002 2002 2002
Table 1: Distribution of word senses in pool and test sets
starting from 0% up to 30% of noise, increasing by
2% in each trial.
We assess the impact of annotation noise on ac-
tive learning in three different settings. In the first
setting, we randomly select new instances from the
pool (random sampling; rand). In the second setting,
we randomly replace n percent of all labels (from 0
to 30) in the pool by another label before starting
the active learning trial, but retain the distribution of
the different labels in the pool data (active learning
with random errors); (Table 1, ALrand, 30%). In
the third setting we simulate biased decisions by a
human annotator. For a certain fraction (0 to 30%)
of instances of a particular non-majority class, we
substitute the majority class label for the gold label,
thereby producing a more skewed distribution than
in the original pool (active learning with biased er-
rors); (Table 1, ALbias, 30%).
For all three settings (rand, ALrand, ALbias) and
each degree of noise (0-30%), we run active learning
simulations on the already annotated data, simulat-
ing the annotation process by selecting one new, pre-
labelled instance per trial from the pool and, instead
of handing them over to a human coder, assigning
the known (possibly erroneous) label to the instance
and adding it to the training set. We use the same
split (test, pool) for all three settings and all degrees
of noise, with identical test sets for all trials.
3.2 Results
Figure 1 shows active learning curves for the differ-
ent settings and varying degrees of noise. The hori-
zontal black line slightly below 0.5 accuracy shows
the majority baseline (the performance obtained
when always assigning the majority class). For all
degrees of randomly inserted noise, active learning
(ALrand) outperforms random sampling (rand) at an
early stage in the learning process. Looking at the
biased errors (ALbias), we see a different picture.
With a low degree of noise, the curves for ALrand
and ALbias are very similar. When inserting more
noise, performance for ALbias decreases, and with
around 20% of biased errors in the pool AL performs
worse than our random sampling baseline. In the
random noise setting (ALrand), even after inserting
30% of errors AL clearly outperforms random sam-
pling. Increasing the size of the seed data reduces
the effect slightly, but does not prevent it (not shown
here due to space limitations). This confirms the
findings that under certain circumstances AL per-
forms worse than random sampling (Dang, 2004;
Schein and Ungar, 2007; Rehbein et al, 2010). We
could also confirm Schein and Ungar (2007)?s obser-
vation that margin sampling is less sensitive to cer-
tain types of noise than entropy sampling (Table 2).
Because of space limitations we only show curves
for margin sampling. For entropy sampling, the gen-
eral trend is the same, with results being slightly
lower than for margin sampling.
4 Detecting Annotation Noise
Uncertainty sampling using the margin metric se-
lects instances for which the difference between
classifier predictions for the two most probable
classes c, c? is very small (Section 3, Equation 2).
When selecting unlabelled instances from the pool,
this metric picks examples which represent regions
of uncertainty between classes which have yet to be
learned by the classifier and thus will advance the
learning process. Our human coder, however, is not
the perfect oracle assumed in most AL simulations,
and might also assign incorrect labels. The filter ap-
proach has two objectives: a) to detect incorrect la-
bels assigned by human coders, and b) to prevent
the hard cases (following the terminology of Kle-
banov et al (2008)) from becoming part of the train-
ing data.
We proceed as follows. Our approach makes use
of the limited set of seed data S and uses heuris-
tics to detect unreliably annotated instances. We
assume that the instances in S have been validated
thoroughly. We train an ensemble of classifiers E
on subsets of S, and use E to decide whether or not
a newly annotated instance should be added to the
seed.
46
error=2%
Training size
Ac
cu
ra
cy
0 250 600 950
0.
4
0.
5
0.
6
0.
7
0.
8
rand
al_rand
al_bias
error=6%
Training size
Ac
cu
ra
cy
0 250 600 950
0.
4
0.
5
0.
6
0.
7
0.
8
error=10%
Training size
Ac
cu
ra
cy
0 250 600 950
0.
4
0.
5
0.
6
0.
7
0.
8
error=14%
Training size
Ac
cu
ra
cy
0 250 600 950
0.
4
0.
5
0.
6
0.
7
0.
8
error=18%
Training size
Ac
cu
ra
cy
0 250 600 950
0.
4
0.
5
0.
6
0.
7
0.
8
rand
al_rand
al_bias
error=22%
Training size
Ac
cu
ra
cy
0 250 600 950
0.
4
0.
5
0.
6
0.
7
0.
8
error=26%
Training size
Ac
cu
ra
cy
0 250 600 950
0.
4
0.
5
0.
6
0.
7
0.
8
error=30%
Training size
Ac
cu
ra
cy
0 250 600 950
0.
4
0.
5
0.
6
0.
7
0.
8
Figure 1: Active learning curves for varying degrees of noise, starting from 0% up to 30% for a training size up to
1200 instances (solid circle (black): random sampling; filled triangle point-up (red): AL with random errors; cross
(green): AL with biased errors)
47
filter % error 0 4 8 12 16 20 24 28 30
- rand 0.763 0.752 0.736 0.741 0.726 0.708 0.707 0.677 0.678
entropy - ALrand 0.806 0.786 0.779 0.743 0.752 0.762 0.731 0.724 0.729
entropy y ALrand 0.792 0.786 0.777 0.760 0.771 0.748 0.730 0.729 0.727
margin - ALrand 0.795 0.795 0.782 0.771 0.758 0.755 0.737 0.719 0.708
margin y ALrand 0.800 0.785 0.773 0.777 0.765 0.766 0.734 0.735 0.718
entropy - ALbias 0.806 0.793 0.759 0.748 0.702 0.651 0.625 0.630 0.622
entropy y ALbias 0.802 0.781 0.777 0.735 0.702 0.678 0.687 0.624 0.616
margin - ALbias 0.795 0.789 0.770 0.753 0.706 0.684 0.656 0.634 0.624
margin y ALbias 0.787 0.781 0.787 0.768 0.739 0.700 0.671 0.653 0.651
Table 2: Accuracy for the different sampling methods without and with filtering after adding 500 instances to the seed
data
There are a number of problems with this ap-
proach. First, there is the risk of overfitting S. Sec-
ond, we know that classifier accuracy in the early
phase of AL is low. Therefore, using classifier pre-
dictions at this stage to accept or reject new in-
stances could result in poor choices that might harm
the learning proceess. To avoid this and to gener-
alise over S to prevent overfitting, we do not directly
train our ensemble on instances from S. Instead, we
create new feature vectors Fgen on the basis of the
feature vectors Fseed in S. For each class in S, we
extract all attribute-value pairs from the feature vec-
tors for this particular class. For each class, we ran-
domly select features (with replacement) from Fseed
and combine them into a new feature vector Fgen,
retaining the distribution of the different classes in
the data. As a result, we obtain a more general set of
feature vectors Fgen with characteristic features be-
ing distributed more evenly over the different feature
vectors.
In the next step we train n = 5 maximum en-
tropy classifiers on subsets of Fgen, excluding the
instances last annotated by the oracle. Each subset
is half the size of the current S. We use the ensemble
to predict the labels for the new instances and, based
on the predictions, accept or reject these, following
the two heuristics below (also see Figure 2).
1. If all n ensemble classifiers agree on one label
but disagree with the oracle ? reject.
2. If the sum of the margins predicted by the en-
semble classifiers is below a particular theshold
tmargin ? reject.
The threshold tmargin was set to 0.01, based on a
qualitative data analysis.
AL with Filtering:
Input: annotated seed data S,
unannotated pool P
AL loop:
? train classifier C on S
? let C predict labels for data in P
? select new instances from P according to
sampling method, hand over to oracle for
annotation
Repeat: after every c new instances
annotated by the oracle
? for each class in S, extract sets of
features Fseed
? create new, more general feature vectors
Fgen from this set (with replacement)
? train an ensemble E of n classifiers on
different subsets of Fgen
Filtering Heuristics:
? if all n classifier in E agree on label
but disagree with oracle:
? remove instance from seed
? if margin is less than threshold tmargin:
? remove instance from seed
Until done
Figure 2: Heuristics for filtering unreliable data points
(parameters used: initial seed size: 9 sentences, c = 10,
n = 5, tmargin = 0.01)
48
In each iteration of the AL process, one new in-
stance is selected using margin sampling. The in-
stance is presented to the oracle who assigns a label.
Then the instance is added to the seed data, thus in-
fluencing the selection of the next data point to be
annotated. After 10 new instances have been added,
we apply the filter technique which finally decides
whether the newly added instances will remain in
the seed data or will be removed.
Figure 3 shows learning curves for the filter ap-
proach. With increasing amount of errors in the
pool, a clear pattern emerges. For both sampling
methods (ALrand, ALbias), the filtering step clearly
improves results. Even for the noisier data sets with
up to 26% of errors, ALbias with filtering performs
at least as well as random sampling.
4.1 Error Analysis
Next we want to find out what kind of errors the
system could detect. We want to know whether the
approach is able to detect the errors previously in-
serted into the data, and whether it manages to iden-
tify hard cases representing true ambiguities.
To answer these questions we look at one fold of
the ALbias data with 10% of noise. In 1,200 AL it-
erations the system rejected 116 instances (Table 3).
The major part of the rejections was due to the ma-
jority vote of the ensemble classifiers (first heuris-
tic, H1) which rejects all instances where the en-
semble classifiers agree with each other but disagree
with the human judgement. Out of the 105 instances
rejected by H1, 41 were labelled incorrectly. This
means that we were able to detect around half of the
incorrect labels inserted in the pool.
11 instances were filtered out by the margin
threshold (H2). None of these contained an incor-
errors inserted in pool 173
err. instances selected by AL 93
instances rejected by H1+H2 116
instances rejected by H1 105
true errors rejected by H1 41
instances rejected by H2 11
true errors rejected by H2 0
Table 3: Error analysis of the instances rejected by the
filtering approach
rect label. On first glance H2 seems to be more le-
nient than H1, considering the number of rejected
sentences. This, however, could also be an effect of
the order in which we apply the filters.
The different word senses are evenly distributed
over the rejected instances (H1: Commitment 30,
drohen1-salsa 38, Run risk 36; H2: Commitment 3,
drohen1-salsa 4, Run risk 4). This shows that there
is less uncertainty about the majority word sense,
Run risk.
It is hard to decide whether the correctly labelled
instances rejected by the filtering method would
have helped or hurt the learning process. Simply
adding them to the seed data after the conclusion
of AL would not answer this question, as it would
merely tell us whether they improve classification
accuracy further, but we still would not know what
impact these instances would have had on the selec-
tion of instances during the AL process.
5 Conclusions
This paper shows that certain types of annotation
noise cause serious problems for active learning ap-
proaches. We showed how biased coder decisions
can result in an accuracy for AL approaches which
is below the one for random sampling. In this case,
it is necessary to apply an additional filtering step
to remove the noisy data from the training set. We
presented an approach based on a resampling of the
features in the seed data and guided by an ensemble
of classifiers trained on the resampled feature vec-
tors. We showed that our approach is able to detect
a certain amount of noise in the data.
Future work should focus on finding optimal pa-
rameter settings to make the filtering method more
robust even for noisier data sets. We also plan to im-
prove the filtering heuristics and to explore further
ways of detecting human coder errors. Finally, we
plan to test our method in a real-world annotation
scenario.
6 Acknowledgments
This work was funded by the German Research
Foundation DFG (grant PI 154/9-3). We would like
to thank the anonymous reviewers for their helpful
comments and suggestions.
49
error=2%
Training size
Ac
cu
ra
cy
0 250 600 950
0.
4
0.
5
0.
6
0.
7
0.
8
rand
ALrand
ALrand_f
ALbias
ALbias_f
error=6%
Training size
Ac
cu
ra
cy
0 250 600 950
0.
4
0.
5
0.
6
0.
7
0.
8
error=10%
Training size
Ac
cu
ra
cy
0 250 600 950
0.
4
0.
5
0.
6
0.
7
0.
8
error=14%
Training size
Ac
cu
ra
cy
0 250 600 950
0.
4
0.
5
0.
6
0.
7
0.
8
error=18%
Training size
Ac
cu
ra
cy
0 250 600 950
0.
4
0.
5
0.
6
0.
7
0.
8
rand
ALrand
ALrand_f
ALbias
ALbias_f
error=22%
Training size
Ac
cu
ra
cy
0 250 600 950
0.
4
0.
5
0.
6
0.
7
0.
8
error=26%
Training size
Ac
cu
ra
cy
0 250 600 950
0.
4
0.
5
0.
6
0.
7
0.
8
error=30%
Training size
Ac
cu
ra
cy
0 250 600 950
0.
4
0.
5
0.
6
0.
7
0.
8
Figure 3: Active learning curves for varying degrees of noise, starting from 0% up to 30% for a training size up to
1200 instances (solid circle (black): random sampling; open circle (red): ALrand; cross (green): ALrand with filtering;
filled triangle point-up (black): ALbias; plus (blue): ALbias with filtering)
50
References
Beata Beigman Klebanov and Eyal Beigman. 2009.
From annotator agreement to noise models. Compu-
tational Linguistics, 35:495?503, December.
Beata Beigman Klebanov, Eyal Beigman, and Daniel
Diermeier. 2008. Analyzing disagreements. In Pro-
ceedings of the Workshop on Human Judgements in
Computational Linguistics, HumanJudge ?08, pages
2?7, Morristown, NJ, USA. Association for Compu-
tational Linguistics.
Yee Seng Chan and Hwee Tou Ng. 2007. Domain adap-
tation with active learning for word sense disambigua-
tion. In Proceedings of ACL-2007.
Jinying Chen, Andrew Schein, Lyle Ungar, and Martha
Palmer. 2006. An empirical study of the behavior of
active learning for word sense disambiguation. In Pro-
ceedings of NAACL-2006, New York, NY.
Hoa Trang Dang. 2004. Investigations into the role of
lexical semantics in word sense disambiguation. PhD
dissertation, University of Pennsylvania, Pennsylva-
nia, PA.
Gholamreza Haffari and Anoop Sarkar. 2009. Active
learning for multilingual statistical machine transla-
tion. In Proceedings of the Joint Conference of the
47th Annual Meeting of the ACL and the 4th Interna-
tional Joint Conference on Natural Language Process-
ing of the AFNLP: Volume 1 - Volume 1, pages 181?
189. Association for Computational Linguistics.
Rebecca Hwa. 2004. Sample selection for statistical
parsing. Computational Linguistics, 30(3):253?276.
Florian Laws and H. Schu?tze. 2008. Stopping crite-
ria for active learning of named entity recognition.
In Proceedings of the 22nd International Conference
on Computational Linguistics (Coling 2008), Manch-
ester, UK, August.
David D. Lewis and William A. Gale. 1994. A sequential
algorithm for training text classifiers. In Proceedings
of ACM-SIGIR, Dublin, Ireland.
Grace Ngai and David Yarowsky. 2000. Rule writing
or annotation: cost-efficient resource usage for base
noun phrase chunking. In Proceedings of the 38th An-
nual Meeting on Association for Computational Lin-
guistics, pages 117?125, Stroudsburg, PA, USA. As-
sociation for Computational Linguistics.
Miles Osborne and Jason Baldridge. 2004. Ensemble-
based active learning for parse selection. In Proceed-
ings of HLT-NAACL 2004.
Ines Rehbein, Josef Ruppenhofer, and Jonas Sunde.
2009. Majo - a toolkit for supervised word sense dis-
ambiguation and active learning. In Proceedings of
the 8th Workshop on Treebanks and Linguistic Theo-
ries (TLT-8), Milano, Italy.
Ines Rehbein, Josef Ruppenhofer, and Alexis Palmer.
2010. Bringing active learning to life. In Proceed-
ings of the 23rd International Conference on Compu-
tational Linguistics (COLING 2010), Beijing, China.
Dennis Reidsma and Jean Carletta. 2008. Reliability
measurement without limits. Computational Linguis-
tics, 34:319?326.
Eric Ringger, Peter McClanahan, Robbie Haertel, George
Busby, Marc Carmen, James Carroll, Kevin Seppi, and
Deryle Lonsdale. 2007. Active learning for part-of-
speech tagging: Accelerating corpus annotation. In
Proceedings of the Linguistic Annotation Workshop,
Prague.
Andrew I. Schein and Lyle H. Ungar. 2007. Active learn-
ing for logistic regression: an evaluation. Machine
Learning, 68:235?265.
Dan Shen, Jie Zhang, Jian Su, Guodong Zhou, and Chew-
Lim Tan. 2004. Multi-criteria-based active learning
for named entity recognition. In Proceedings of the
42nd Annual Meeting on Association for Computa-
tional Linguistics, Stroudsburg, PA, USA. Association
for Computational Linguistics.
Katrin Tomanek and Udo Hahn. 2009. Reducing class
imbalance during active learning for named entity an-
notation. In Proceedings of the 5th International Con-
ference on Knowledge Capture, Redondo Beach, CA.
Simon Tong and Daphne Koller. 1998. Support vector
machine active learning with applications to text clas-
sification. In Proceedings of the Seventeenth Interna-
tional Conference on Machine Learning (ICML-00),
pages 287?295.
Cha Zhang and Tsuhan Chen. 2002. An active learn-
ing framework for content-based information retrieval.
IEEE Transactions on Multimedia, 4(2):260?268.
Jingbo Zhu and Edward Hovy. 2007. Active learning for
word sense disambiguation with methods for address-
ing the class imbalance problem. In Proceedings of the
2007 Joint Conference on Empirical Methods in Natu-
ral Language Processing and Computational Natural
Language Learning, Prague, Czech Republic.
Jingbo Zhu, Huizhen Wang, Tianshun Yao, and Ben-
jamin K. Tsou. 2008. Active learning with sampling
by uncertainty and density for word sense disambigua-
tion and text classification. In Proceedings of the 22nd
International Conference on Computational Linguis-
tics (Coling 2008), Manchester, UK.
51
Proceedings of the NAACL HLT 2010 First Workshop on Statistical Parsing of Morphologically-Rich Languages, pages 1?12,
Los Angeles, California, June 2010. c?2010 Association for Computational Linguistics
Statistical Parsing of Morphologically Rich Languages (SPMRL)
What, How and Whither
Reut Tsarfaty
Uppsala Universitet
Djame? Seddah
Alpage (Inria/Univ. Paris-Sorbonne)
Yoav Goldberg
Ben Gurion University
Sandra Ku?bler
Indiana University
Marie Candito
Alpage (Inria/Univ. Paris 7)
Jennifer Foster
NCLT, Dublin City University
Yannick Versley
Universita?t Tu?bingen
Ines Rehbein
Universita?t Saarbru?cken
Lamia Tounsi
NCLT, Dublin City University
Abstract
The term Morphologically Rich Languages
(MRLs) refers to languages in which signif-
icant information concerning syntactic units
and relations is expressed at word-level. There
is ample evidence that the application of read-
ily available statistical parsing models to such
languages is susceptible to serious perfor-
mance degradation. The first workshop on sta-
tistical parsing of MRLs hosts a variety of con-
tributions which show that despite language-
specific idiosyncrasies, the problems associ-
ated with parsing MRLs cut across languages
and parsing frameworks. In this paper we re-
view the current state-of-affairs with respect
to parsing MRLs and point out central chal-
lenges. We synthesize the contributions of re-
searchers working on parsing Arabic, Basque,
French, German, Hebrew, Hindi and Korean
to point out shared solutions across languages.
The overarching analysis suggests itself as a
source of directions for future investigations.
1 Introduction
The availability of large syntactically annotated cor-
pora led to an explosion of interest in automati-
cally inducing models for syntactic analysis and dis-
ambiguation called statistical parsers. The devel-
opment of successful statistical parsing models for
English focused on the Wall Street Journal Penn
Treebank (PTB, (Marcus et al, 1993)) as the pri-
mary, and sometimes only, resource. Since the ini-
tial release of the Penn Treebank (PTB Marcus et
al. (1993)), many different constituent-based parsing
models have been developed in the context of pars-
ing English (e.g. (Magerman, 1995; Collins, 1997;
Charniak, 2000; Chiang, 2000; Bod, 2003; Char-
niak and Johnson, 2005; Petrov et al, 2006; Huang,
2008; Finkel et al, 2008; Carreras et al, 2008)).
At their time, each of these models improved the
state-of-the-art, bringing parsing performance on the
standard test set of the Wall-Street-Journal to a per-
formance ceiling of 92% F1-score using the PARS-
EVAL evaluation metrics (Black et al, 1991). Some
of these parsers have been adapted to other lan-
guage/treebank pairs, but many of these adaptations
have been shown to be considerably less successful.
Among the arguments that have been proposed
to explain this performance gap are the impact of
small data sets, differences in treebanks? annotation
schemes, and inadequacy of the widely used PARS-
EVAL evaluation metrics. None of these aspects in
isolation can account for the systematic performance
deterioration, but observed from a wider, cross-
linguistic perspective, a picture begins to emerge ?
that the morphologically rich nature of some of the
languages makes them inherently more susceptible
to such performance degradation. Linguistic factors
associated with MRLs, such as a large inventory of
word-forms, higher degrees of word order freedom,
and the use of morphological information in indi-
cating syntactic relations, makes them substantially
harder to parse with models and techniques that have
been developed with English data in mind.
1
In addition to these technical and linguistic fac-
tors, the prominence of English parsing in the litera-
ture reduces the visibility of research aiming to solve
problems particular to MRLs. The lack of stream-
lined communication among researchers working
on different MRLs often leads to a reinventing the
wheel syndrome. To circumvent this, the first work-
shop on Statistical Parsing of Morphologically Rich
Languages (SPMRL 2010) offers a platform for
this growing community to share their views of the
different problems and oftentimes similar solutions.
We identify three main types of challenges, each
of which raises many questions. Many of the ques-
tions are yet to be conclusively answered. The first
type of challenges has to do with the architectural
setup of parsing MRLs: What is the nature of the in-
put? Can words be represented abstractly to reflect
shared morphological aspects? How can we cope
with morphological segmentation errors propagated
through the pipeline? The second type concerns the
representation of morphological information inside
the articulated syntactic model: Should morpholog-
ical information be encoded at the level of PoS tags?
On dependency relations? On top of non-terminals
symbols? How should the integrated representations
be learned and used? A final genuine challenge
has to do with sound estimation for lexical probabil-
ities: Given the finite, and often rather small, set of
data, and the large number of morphological analy-
ses licensed by rich inflectional systems, how can we
analyze words unseen in the training data?
Many of the challenges reported here are mostly
irrelevant when parsing Section 23 of the PTB but
they are of primordial importance in other tasks, in-
cluding out-of-domain parsing, statistical machine
translation, and parsing resource-poor languages.
By synthesizing the contributions to the workshop
and bringing it to the forefront, we hope to advance
the state of the art of statistical parsing in general.
In this paper we therefore take the opportunity
to analyze the knowledge that has been acquired in
the different investigations for the purpose of iden-
tifying main bottlenecks and pointing out promising
research directions. In section 2, we define MRLs
and identify syntactic characteristics associated with
them. We then discuss work on parsing MRLs in
both the dependency-based and constituency-based
setup. In section 3, we review the types of chal-
lenges associated with parsing MRLs across frame-
works. In section 4, we focus on the contributions to
the SPMRL workshop and identify recurring trends
in the empirical results and conceptual solutions. In
section 5, we analyze the emerging picture from a
bird?s eye view, and conclude that many challenges
could be more faithfully addressed in the context of
parsing morphologically ambiguous input.
2 Background
2.1 What are MRLs?
The term Morphologically Rich Languages (MRLs)
is used in the CL/NLP literature to refer to languages
in which substantial grammatical information, i.e.,
information concerning the arrangement of words
into syntactic units or cues to syntactic relations, is
expressed at word level.
The common linguistic and typological wisdom is
that ?morphology competes with syntax? (Bresnan,
2001). In effect, this means that rich morphology
goes hand in hand with a host of nonconfigurational
syntactic phenomena of the kind discussed by Hale
(1983). Because information about the relations be-
tween syntactic elements is indicated in the form of
words, these words can freely change their positions
in the sentence. This is referred to as free word or-
der (Mithun, 1992). Information about the group-
ing of elements together can further be expressed by
reference to their morphological form. Such logical
groupings of disparate elements are often called dis-
continuous constituents. In dependency structures,
such discontinuities impose nonprojectivity. Finally,
rich morphological information is found in abun-
dance in conjunction with so-called pro-drop or zero
anaphora. In such cases, rich morphological infor-
mation in the head (or co-head) of the clause of-
ten makes it possible to omit an overt subject which
would be semantically impoverished.
English, the most heavily studied language within
the CL/NLP community, is not an MRL. Even
though a handful of syntactic features (such as per-
son and number) are reflected in the form of words,
morphological information is often secondary to
other syntactic factors, such as the position of words
and their arrangement into phrases. German, an
Indo-European language closely related to English,
already exhibits some of the properties that make
2
parsing MRLs problematic. The Semitic languages
Arabic and Hebrew show an even more extreme case
in terms of the richness of their morphological forms
and the flexibility in their syntactic ordering.
2.2 Parsing MRLs
Pushing the envelope of constituency parsing:
The Head-Driven models of the type proposed
by Collins (1997) have been ported to parsing
many MRLs, often via the implementation of Bikel
(2002). For Czech, the adaptation by Collins et al
(1999) culminated in an 80 F1-score.
German has become almost an archetype of the
problems caused by MRLs; even though German
has a moderately rich morphology and a moder-
ately free word order, parsing results are far from
those for English (see (Ku?bler, 2008) and references
therein). Dubey (2005) showed that, for German
parsing, adding case and morphology information
together with smoothed markovization and an ade-
quate unknown-word model is more important than
lexicalization (Dubey and Keller, 2003).
For Modern Hebrew, Tsarfaty and Sima?an (2007)
show that a simple treebank PCFG augmented with
parent annotation and morphological information as
state-splits significantly outperforms Head-Driven
markovized models of the kind made popular by
Klein and Manning (2003). Results for parsing
Modern Standard Arabic using Bikel?s implemen-
tation on gold-standard tagging and segmentation
have not improved substantially since the initial re-
lease of the treebank (Maamouri et al, 2004; Kulick
et al, 2006; Maamouri et al, 2008).
For Italian, Corazza et al (2004) used the Stan-
ford parser and Bikel?s parser emulation of Collins?
model 2 (Collins, 1997) on the ISST treebank, and
obtained significantly lower results compared to En-
glish. It is notable that these models were ap-
plied without adding morphological signatures, us-
ing gold lemmas instead. Corazza et al (2004) fur-
ther tried different refinements including parent an-
notation and horizontal markovization, but none of
them obtained the desired improvement.
For French, Crabbe? and Candito (2008) and Sed-
dah et al (2010) show that, given a corpus compara-
ble in size and properties (i.e. the number of tokens
and grammar size), the performance level, both for
Charniak?s parser (Charniak, 2000) and the Berke-
ley parser (Petrov et al, 2006) was higher for pars-
ing the PTB than it was for French. The split-merge-
smooth implementation of (Petrov et al, 2006) con-
sistently outperform various lexicalized and unlexi-
calized models for French (Seddah et al, 2009) and
for many other languages (Petrov and Klein, 2007).
In this respect, (Petrov et al, 2006) is considered
MRL-friendly, due to its language agnostic design.
The rise of dependency parsing: It is commonly
assumed that dependency structures are better suited
for representing the syntactic structures of free word
order, morphologically rich, languages, because this
representation format does not rely crucially on the
position of words and the internal grouping of sur-
face chunks (Mel?c?uk, 1988). It is an entirely differ-
ent question, however, whether dependency parsers
are in fact better suited for parsing such languages.
The CoNLL shared tasks on multilingual depen-
dency parsing in 2006 and 2007 (Buchholz and
Marsi, 2006; Nivre et al, 2007a) demonstrated that
dependency parsing for MRLs is quite challenging.
While dependency parsers are adaptable to many
languages, as reflected in the multiplicity of the lan-
guages covered,1 the analysis by Nivre et al (2007b)
shows that the best result was obtained for English,
followed by Catalan, and that the most difficult lan-
guages to parse were Arabic, Basque, and Greek.
Nivre et al (2007a) drew a somewhat typological
conclusion, that languages with rich morphology
and free word order are the hardest to parse. This
was shown to be the case for both MaltParser (Nivre
et al, 2007c) and MST (McDonald et al, 2005), two
of the best performing parsers on the whole.
Annotation and evaluation matter: An emerg-
ing question is therefore whether models that have
been so successful in parsing English are necessar-
ily appropriate for parsing MRLs ? but associated
with this question are important questions concern-
ing the annotation scheme of the related treebanks.
Obviously, when annotating structures for languages
with characteristics different than English one has to
face different annotation decisions, and it comes as
no surprise that the annotated structures for MRLs
often differ from those employed in the PTB.
1The shared tasks involved 18 languages, including many
MRLs such as Arabic, Basque, Czech, Hungarian, and Turkish.
3
For Spanish and French, it was shown by Cowan
and Collins (2005) and in (Arun and Keller, 2005;
Schluter and van Genabith, 2007), that restructuring
the treebanks? native annotation scheme to match
the PTB annotation style led to a significant gain in
parsing performance of Head-Driven models of the
kind proposed in (Collins, 1997). For German, a
language with four different treebanks and two sub-
stantially different annotation schemes, it has been
shown that a PCFG parser is sensitive to the kind of
representation employed in the treebank.
Dubey and Keller (2003), for example, showed
that a simple PCFG parser outperformed an emula-
tion of Collins? model 1 on NEGRA. They showed
that using sister-head dependencies instead of head-
head dependencies improved parsing performance,
and hypothesized that it is due to the flatness of
phrasal annotation. Ku?bler et al (2006) showed con-
siderably lower PARSEVAL scores on NEGRA (Skut
et al, 1998) relative to the more hierarchically struc-
tured Tu?Ba-D/Z (Hinrichs et al, 2005), again, hy-
pothesizing that this is due to annotation differences.
Related to such comparisons is the question of the
relevance of the PARSEVAL metrics for evaluating
parsing results across languages and treebanks. Re-
hbein and van Genabith (2007) showed that PARS-
EVAL measures are sensitive to annotation scheme
particularities (e.g. the internal node ratio). It was
further shown that different metrics (i.e. the Leaf-
ancestor path (Sampson and Babarczy, 2003) and
dependency based ones in (Lin, 1995)) can lead to
different performance ranking. This was confirmed
also for French by Seddah et al (2009).
The questions of how to annotate treebanks for
MRLs and how to evaluate the performance of the
different parsers on these different treebanks is cru-
cial. For the MRL parsing community to be able to
assess the difficulty of improving parsing results for
French, German, Arabic, Korean, Basque, Hindi or
Hebrew, we ought to first address fundamental ques-
tions including: Is the treebank sufficiently large
to allow for proper grammar induction? Does the
annotation scheme fit the language characteristics?
Does the use of PTB annotation variants for other
languages influence parsing results? Does the space-
delimited tokenization allow for phrase boundary
detection? Do the results for a specific approach
generalize to more than one language?
3 Primary Research Questions
It is firmly established in theoretical linguistics that
morphology and syntax closely interact through pat-
terns of case marking, agreement, clitics and various
types of compounds. Because of such close interac-
tions, we expect morphological cues to help parsing
performance. But in practice, when trying to incor-
porate morphological information into parsing mod-
els, three types of challenges present themselves:
Architecture and Setup: When attempting to
parse complex word-forms that encapsulate both
lexical and functional information, important archi-
tectural questions emerge, namely, what is the na-
ture of the input that is given to the parsing system?
Does the system attempt to parse sequences of words
or does it aim to assign structures to sequences of
morphological segments? If the former is the case,
how can we represent words abstractly so as to re-
flect shared morphological aspects between them?
If the latter is the case, how can we arrive at a good
enough morphological segmentation for the purpose
of statistical parsing, given raw input texts?
When working with morphologically rich lan-
guages such as Hebrew or Arabic, affixes may have
syntactically independent functions. Many parsing
models assume segmentation of the syntactically in-
dependent parts, such as prepositions or pronominal
clitics, prior to parsing. But morphological segmen-
tation requires disambiguation which is non-trivial,
due to case syncretism and high morphological am-
biguity exhibited by rich inflectional systems. The
question is then when should we disambiguate the
morphological analyses of input forms? Should we
do that prior to parsing or perhaps jointly with it?2
Representation and Modeling: Assuming that
the input to our system reflects morphological infor-
mation, one way or another, which types of morpho-
2Most studies on parsing MRLs nowadays assume the gold
standard segmentation and disambiguated morphological infor-
mation as input. This is the case, for instance, for the Arabic
parsing at CoNLL 2007 (Nivre et al, 2007a). This practice de-
ludes the community as to the validity of the parsing results
reported for MRLs in shared tasks. Goldberg et al (2009), for
instance, show a gap of up to 6pt F1-score between performance
on gold standard segmentation vs. raw text. One way to over-
come this is to devise joint morphological and syntactic disam-
biguation frameworks (cf. (Goldberg and Tsarfaty, 2008)).
4
logical information should we include in the parsing
model? Inflectional and/or derivational? Case infor-
mation and/or agreement features? How can valency
requirements reflected in derivational morphology
affect the overall syntactic structure? In tandem with
the decision concerning the morphological informa-
tion to include, we face genuine challenges concern-
ing how to represent such information in the syntac-
tic model, be it constituency-based or dependency-
based. Should we encode morphological informa-
tion at the level of PoS tags and/or on top of syn-
tactic elements? Should we decorate non-terminals
nodes and/or dependency arcs or both?
Incorporating morphology in the statistical model
is often even more challenging than the sum of
these bare decisions, because of the nonconfigu-
rational structures (free word order, discontinuous
constituents) for rich markings are crucial (Hale,
1983). The parsing models designed for English of-
ten focus on learning rigid word order, and they do
not take morphological information into account (cf.
developing parsers for German (Dubey and Keller,
2003; Ku?bler et al, 2006)). The more complex ques-
tion is therefore: what type of parsing model should
we use for parsing MRLs? shall we use a general
purpose implementation and attempt to amend it?
how? or perhaps we should devise a new model from
first principles, to address nonconfigurational phe-
nomena effectively? using what form of representa-
tion? is it possible to find a single model that can
effectively cope with different kinds of languages?
Estimation and Smoothing: Compared to En-
glish, MRLs tend to have a greater number of word
forms and higher out-of-vocabulary (OOV) rates,
due to the many feature combinations licensed by
the inflectional system. A typical problem associ-
ated with parsing MRLs is substantial lexical data
sparseness due to high morphological variation in
surface forms. The question is therefore, given our
finite, and often fairly small, annotated sets of data,
how can we guess the morphological analyses, in-
cluding the PoS tag assignment and various features,
of an OOV word? How can we learn the probabil-
ities of such assignments? In a more general setup,
this problem is akin to handling out-of-vocabulary
or rare words for robust statistical parsing, and tech-
niques for domain adaptation via lexicon enhance-
Constituency-Based Dependency-Based
Arabic (Attia et al, 2010) (Marton et al, 2010)?
Basque - (Bengoetxea and Gojenola, 2010)
English (Attia et al, 2010) -
French (Attia et al, 2010)
(Seddah et al, 2010)
(Candito and Seddah, 2010)? -
German (Maier, 2010) -
Hebrew (Tsarfaty and Sima?an, 2010) (Goldberg and Elhadad, 2010)?
Hindi - (Ambati et al, 2010a)?
(Ambati et al, 2010b)
Korean (Chung et al, 2010) -
Table 1: An overview of SPMRL contributions. (? report
results also for non-gold standard input)
ment (also explored for English and other morpho-
logically impoverished languages).
So, in fact, incorporating morphological informa-
tion inside the syntactic model for the purpose of
statistical parsing is anything but trivial. In the next
section we review the various approaches taken in
the individual contributions of the SPMRL work-
shop for addressing such challenges.
4 Parsing MRLs: Recurring Trends
The first workshop on parsing MRLs features 11
contributions for a variety of languages with a
range of different parsing frameworks. Table 1 lists
the individual contributions within a cross-language
cross-framework grid. In this section, we focus on
trends that occur among the different contributions.
This may be a biased view since some of the prob-
lems that exist for parsing MRLs may have not been
at all present, but it is a synopsis of where we stand
with respect to problems that are being addressed.
4.1 Architecture and Setup: Gold vs. Predicted
Morphological Information
While morphological information can be very infor-
mative for syntactic analysis, morphological anal-
ysis of surface forms is ambiguous in many ways.
In German, for instance, case syncretism (i.e. a sin-
gle surface form corresponding to different cases) is
pervasive, and in Hebrew and Arabic, the lack of vo-
calization patterns in written texts leads to multiple
morphological analyses for each space-delimited to-
ken. In real world situations, gold morphological in-
formation is not available prior to parsing. Can pars-
ing systems make effective use of morphology even
when gold morphological information is absent?
5
Several papers address this challenge by present-
ing results for both the gold and the automatically
predicted PoS and morphological information (Am-
bati et al, 2010a; Marton et al, 2010; Goldberg and
Elhadad, 2010; Seddah et al, 2010). Not very sur-
prisingly, all evaluated systems show a drop in pars-
ing accuracy in the non-gold settings.
An interesting trend is that in many cases, us-
ing noisy morphological information is worse than
not using any at all. For Arabic Dependency pars-
ing, using predicted CASE causes a substantial drop
in accuracy while it greatly improves performance
in the gold setting (Marton et al, 2010). For
Hindi Dependency Parsing, using chunk-internal
cues (i.e. marking non-recursive phrases) is benefi-
cial when gold chunk-boundaries are available, but
suboptimal when they are automatically predicted
(Ambati et al, 2010a). For Hebrew Dependency
Parsing with the MST parser, using gold morpholog-
ical features shows no benefit over not using them,
while using automatically predicted morphological
features causes a big drop in accuracy compared to
not using them (Goldberg and Elhadad, 2010). For
French Constituency Parsing, Seddah et al (2010)
and Candito and Seddah (2010) show that while
gold information for the part-of-speech and lemma
of each word form results in a significant improve-
ment, the gain is low when switching to predicted
information. Reassuringly, Ambati et al (2010a),
Marton et al (2010), and Goldberg and Elhadad
(2010) demonstrate that some morphological infor-
mation can indeed be beneficial for parsing even in
the automatic setting. Ensuring that this is indeed
so, appears to be in turn linked to the question of
how morphology is represented and incorporated in
the parsing model.
The same effect in a different guise appears in
the contribution of Chung et al (2010) concerning
parsing Korean. Chung et al (2010) show a sig-
nificant improvement in parsing accuracy when in-
cluding traces of null anaphors (a.k.a. pro-drop) in
the input to the parser. Just like overt morphology,
traces and null elements encapsulate functional in-
formation about relational entities in the sentence
(the subject, the object, etc.), and including them at
the input level provides helpful disambiguating cues
for the overall structure that represents such rela-
tions. However, assuming that such traces are given
prior to parsing is, for all practical purposes, infeasi-
ble. This leads to an interesting question: will iden-
tifying such functional elements (marked as traces,
overt morphology, etc) during parsing, while com-
plicating that task itself, be on the whole justified?
Closely linked to the inclusion of morphological
information in the input is the choice of PoS tag set
to use. The generally accepted view is that fine-
grained PoS tags are morphologically more informa-
tive but may be harder to statistically learn and parse
with, in particular in the non-gold scenario. Mar-
ton et al (2010) demonstrate that a fine-grained tag
set provides the best results for Arabic dependency
parsing when gold tags are known, while a much
smaller tag set is preferred in the automatic setting.
4.2 Representation and Modeling:
Incorporating Morphological Information
Many of the studies presented here explore the use
of feature representation of morphological informa-
tion for the purpose of syntactic parsing (Ambati et
al., 2010a; Ambati et al, 2010b; Bengoetxea and
Gojenola, 2010; Goldberg and Elhadad, 2010; Mar-
ton et al, 2010; Tsarfaty and Sima?an, 2010). Clear
trends among the contributions emerge concerning
the kind of morphological information that helps sta-
tistical parsing. Morphological CASE is shown to be
beneficial across the board. It is shown to help for
parsing Basque, Hebrew, Hindi and to some extent
Arabic.3 Morphological DEFINITENESS and STATE
are beneficial for Hebrew and Arabic when explic-
itly represented in the model. STATE, ASPECT and
MOOD are beneficial for Hindi, but only marginally
beneficial for Arabic. CASE and SUBORDINATION-
TYPE are the most beneficial features for Basque
transition-based dependency parsing.
A closer view into the results mentioned in the
previous paragraph suggests that, beyond the kind
of information that is being used, the way in which
morphological information is represented and used
by the model has substantial ramification as to
whether or not it leads to performance improve-
ments. The so-called ?agreement features? GEN-
DER, NUMBER, PERSON, provide for an interesting
case study in this respect. When included directly as
3For Arabic, CASE is useful when gold morphology infor-
mation is available, but substantially hurt results when it is not.
6
machine learning features, agreement features ben-
efit dependency parsing for Arabic (Marton et al,
2010), but not Hindi (dependency) (Ambati et al,
2010a; Ambati et al, 2010b) or Hebrew (Goldberg
and Elhadad, 2010). When represented as simple
splits of non-terminal symbols, agreement informa-
tion does not help constituency-based parsing per-
formance for Hebrew (Tsarfaty and Sima?an, 2010).
However, when agreement patterns are directly rep-
resented on dependency arcs, they contribute an im-
provement for Hebrew dependency parsing (Gold-
berg and Elhadad, 2010). When agreement is en-
coded at the realization level inside a Relational-
Realizational model (Tsarfaty and Sima?an, 2008),
agreement features improve the state-of-the-art for
Hebrew parsing (Tsarfaty and Sima?an, 2010).
One of the advantages of the latter study is that
morphological information which is expressed at the
level of words gets interpreted elsewhere, on func-
tional elements higher up the constituency tree. In
dependency parsing, similar cases may arise, that
is, morphological information might not be as use-
ful on the form on which it is expressed, but would
be more useful at a different position where it could
influence the correct attachment of the main verb
to other elements. Interesting patterns of that sort
occur in Basque, where the SUBORDINATIONTYPE
morpheme attaches to the auxiliary verb, though it
mainly influences attachments to the main verb.
Bengoetxea and Gojenola (2010) attempted two
different ways to address this, one using a trans-
formation segmenting the relevant morpheme and
attaching it to the main verb instead, and another
by propagating the morpheme along arcs, through
a ?stacking? process, to where it is relevant. Both
ways led to performance improvements. The idea of
a segmentation transformation imposes non-trivial
pre-processing, but it may be that automatically
learning the propagation of morphological features
is a promising direction for future investigation.
Another, albeit indirect, way to include morpho-
logical information in the parsing model is using
so-called latent information or some mechanism
of clustering. The general idea is the following:
when morphological information is added to stan-
dard terminal or non-terminal symbols, it imposes
restrictions on the distribution of these no-longer-
equivalent elements. Learning latent informa-
tion does not represent morphological information
directly, but presumably, the distributional restric-
tions can be automatically learned along with the
splits of labels symbols in models such as (Petrov
et al, 2006). For Korean (Chung et al, 2010),
latent information contributes significant improve-
ments. One can further do the opposite, namely,
merging terminals symbols for the purpose of ob-
taining an abstraction over morphological features.
When such clustering uses a morphological signa-
ture of some sort, it is shown to significantly im-
prove constituency-based parsing for French (Can-
dito and Seddah, 2010).
4.3 Representation and Modeling: Free Word
Order and Flexible Constituency Structure
Off-the-shelf parsing tools are found in abundance
for English. One problematic aspect of using them
to parse MRLs lies in the fact that these tools fo-
cus on the statistical modeling of configurational
information. These models often condition on the
position of words relative to one another (e.g. in
transition-based dependency parsing) or on the dis-
tance between words inside constituents (e.g. in
Head-Driven parsing). Many of the contributions to
the workshop show that working around existing im-
plementations may be insufficient, and we may have
to come up with more radical solutions.
Several studies present results that support the
conjecture that when free word-order is explicitly
taken into account, morphological information is
more likely to contribute to parsing accuracy. The
Relational-Realizational model used in (Tsarfaty
and Sima?an, 2010) allows for reordering of con-
stituents at a configuration layer, which is indepen-
dent of the realization patterns learned from the data
(vis-a`-vis case marking and agreement). The easy-
first algorithm of (Goldberg and Elhadad, 2010)
which allows for significant flexibility in the order of
attachment, allows the model to benefit from agree-
ment patterns over dependency arcs that are easier
to detect and attach first. The use of larger subtrees
in (Chung et al, 2010) for parsing Korean, within a
Bayesian framework, allows the model to learn dis-
tributions that take more elements into account, and
thus learn the different distributions associated with
morphologically marked elements in constituency
structures, to improve performance.
7
In addition to free word order, MRLs show higher
degree of freedom in extraposition. Both of these
phenomena can result in discontinuous structures.
In constituency-based treebanks, this is either an-
notated as additional information which has to be
recovered somehow (traces in the case of the PTB,
complex edge labels in the German Tu?Ba-D/Z), or
as discontinuous phrase structures, which cannot be
handled with current PCFG models. Maier (2010)
suggests the use of Linear Context-Free Rewriting
Systems (LCFRSs) in order to make discontinuous
structure transparent to the parsing process and yet
preserve familiar notions from constituency.
Dependency representation uses non-projective
dependencies to reflect discontinuities, which is
problematic to parse with models that assume pro-
jectivity. Different ways have been proposed to deal
with non-projectivity (Nivre and Nilsson, 2005; Mc-
Donald et al, 2005; McDonald and Pereira, 2006;
Nivre, 2009). Bengoetxea and Gojenola (2010)
discuss non-projective dependencies in Basque and
show that the pseudo-projective transformation of
(Nivre and Nilsson, 2005) improves accuracy for de-
pendency parsing of Basque. Moreover, they show
that in combination with other transformations, it
improves the utility of these other ones, too.
4.4 Estimation and Smoothing: Coping with
Lexical Sparsity
Morphological word form variation augments the
vocabulary size and thus worsens the problem of lex-
ical data sparseness. Words occurring with medium-
frequency receive less reliable estimates, and the
number of rare/unknown words is increased. One
way to cope with the one of both aspects of this
problem is through clustering, that is, providing an
abstract representation over word forms that reflects
their shared morphological and morphosyntactic as-
pects. This was done, for instance, in previous work
on parsing German. Versley and Rehbein (2009)
cluster words according to linear context features.
These clusters include valency information added to
verbs and morphological features such as case and
number added to pre-terminal nodes. The clusters
are then integrated as features in a discriminative
parsing model to cope with unknown words. Their
discriminative model thus obtains state-of-the-art re-
sults on parsing German.
Several contribution address similar challenges.
For constituency-based generative parsers, the sim-
ple technique of replacing word forms with more
abstract symbols is investigated by (Seddah et al,
2010; Candito and Seddah, 2010). For French, re-
placing each word form by its predicted part-of-
speech and lemma pair results in a slight perfor-
mance improvement (Seddah et al, 2010). When
words are clustered, even according to a very local
linear-context similarity measure, measured over a
large raw corpus, and when word clusters are used in
place of word forms, the gain in performance is even
higher (Candito and Seddah, 2010). In both cases,
the technique provides more reliable estimates for
in-vocabulary words, since a given lemma or cluster
appear more frequently. It also increases the known
vocabulary. For instance, if a plural form is un-
seen in the training set but the corresponding singu-
lar form is known, then in a setting of using lemmas
in terminal symbols, both forms are known.
For dependency parsing, Marton et al (2010) in-
vestigates the use of morphological features that in-
volve some semantic abstraction over Arabic forms.
The use of undiacritized lemmas is shown to im-
prove performance. Attia et al (2010) specifically
address the handling of unknown words in the latent-
variable parsing model. Here again, the technique
that is investigated is to project unknown words to
more general symbols using morphological clues. A
study on three languages, English, French and Ara-
bic, shows that this method helps in all cases, but
that the greatest improvement is obtained for Arabic,
which has the richest morphology among three.
5 Where we?re at
It is clear from the present overview that we are
yet to obtain a complete understanding concerning
which models effectively parse MRLs, how to an-
notate treebanks for MRLs and, importantly, how
to evaluate parsing performance across types of lan-
guages and treebanks. These foundational issues are
crucial for deriving more conclusive recommenda-
tions as to the kind of models and morphological
features that can lead to advancing the state-of-the-
art for parsing MRLs. One way to target such an
understanding would be to encourage the investiga-
tion of particular tasks, individually or in the context
8
of shared tasks, that are tailored to treat those prob-
lematic aspects of MRLs that we surveyed here.
So far, constituency-based parsers have been as-
sessed based on their performance on the PTB (and
to some extent, across German treebanks (Ku?bler,
2008)) whereas comparison across languages was
rendered opaque due to data set differences and
representation idiosyncrasies. It would be interest-
ing to investigate such a cross-linguistic compari-
son of parsers in the context of a shared task on
constituency-based statistical parsing, in additional
to dependency-based ones as reported in (Nivre et
al., 2007a). Standardizing data sets for a large
number of languages with different characteristics,
would require us, as a community, to aim for
constituency-representation guidelines that can rep-
resent the shared aspects of structures in different
languages, while at the same time allowing differ-
ences between them to be reflected in the model.
Furthermore, it would be a good idea to intro-
duce parsing tasks, for either constituent-based or
dependency-based setups, which consider raw text
as input, rather than morphologically segmented
and analyzed text. Addressing the parsing prob-
lem while facing the morphological disambiguation
challenge in its full-blown complexity would be il-
luminating and educating for at least two reasons:
firstly, it would give us a better idea of what is the
state-of-the-art for parsing MRLs in realistic scenar-
ios. Secondly, it might lead to profound insights
about the potentially successful ways to use mor-
phology inside a parser, which may differ from the
insights concerning the use of morphology in the
less realistic parsing scenarios, where gold morpho-
logical information is given.
Finally, to be able to perceive where we stand
with respect to parsing MRLs and how models fare
against one another across languages, it would be
crucial to arrive at evaluation metrics that capture
information that is shared among the different repre-
sentations, for instance, functional information con-
cerning predicate-argument relations. Using the dif-
ferent kinds of measures in the context of cross-
framework tasks will help us understand the util-
ity of the different evaluation metrics that have been
proposed and to arrive at a clearer picture of what it
is that we wish to compare, and how we can faith-
fully do so across models, languages and treebanks.
6 Conclusion
This paper presents the synthesis of 11 contributions
to the first workshop on statistical parsing for mor-
phologically rich languages. We have shown that
architectural, representational, and estimation issues
associated with parsing MRLs are found to be chal-
lenging across languages and parsing frameworks.
The use of morphological information in the non
gold-tagged input scenario is found to cause sub-
stantial differences in parsing performance, and in
the kind of morphological features that lead to per-
formance improvements.
Whether or not morphological features help pars-
ing also depends on the kind of model in which
they are embedded, and the different ways they are
treated within. Furthermore, sound statistical esti-
mation methods for morphologically rich, complex
lexica, turn out to be crucial for obtaining good pars-
ing accuracy when using general-purpose models
and algorithms. In the future we hope to gain better
understanding of the common pitfalls in, and novel
solutions for, parsing morphologically ambiguous
input, and to arrive at principled guidelines for se-
lecting the model and features to include when pars-
ing different kinds of languages. Such insights may
be gained, among other things, in the context of
more morphologically-aware shared parsing tasks.
Acknowledgements
The program committee would like to thank
NAACL for hosting the workshop and SIGPARSE
for their sponsorship. We further thank INRIA Al-
page team for their generous sponsorship. We are
finally grateful to our reviewers and authors for their
dedicated work and individual contributions.
References
Bharat Ram Ambati, Samar Husain, Sambhav Jain,
Dipti Misra Sharma, and Rajeev Sangal. 2010a. Two
methods to incorporate local morphosyntactic features
in Hindi dependency parsing. In Proceedings of the
NAACL/HLT Workshop on Statistical Parsing of Mor-
phologically Rich Languages (SPMRL 2010), Los An-
geles, CA.
Bharat Ram Ambati, Samar Husain, Joakim Nivre, and
Rajeev Sangal. 2010b. On the role of morphosyntactic
features in Hindi dependency parsing. In Proceedings
9
of the NAACL/HLT Workshop on Statistical Parsing of
Morphologically Rich Languages (SPMRL 2010), Los
Angeles, CA.
Abhishek Arun and Frank Keller. 2005. Lexicalization
in crosslinguistic probabilistic parsing: The case of
French. In Proceedings of the 43rd Annual Meeting of
the Association for Computational Linguistics, pages
306?313, Ann Arbor, MI.
Mohammed Attia, Jennifer Foster, Deirdre Hogan,
Joseph Le Roux, Lamia Tounsi, and Josef van Gen-
abith. 2010. Handling unknown words in statistical
latent-variable parsing models for Arabic, English and
French. In Proceedings of the NAACL/HLT Workshop
on Statistical Parsing of Morphologically Rich Lan-
guages (SPMRL 2010), Los Angeles, CA.
Kepa Bengoetxea and Koldo Gojenola. 2010. Applica-
tion of different techniques to dependency parsing of
Basque. In Proceedings of the NAACL/HLT Workshop
on Statistical Parsing of Morphologically Rich Lan-
guages (SPMRL 2010), Los Angeles, CA.
Daniel M. Bikel. 2002. Design of a multi-lingual,
parallel-processing statistical parsing engine. In Pro-
ceedings of the Second International Conference on
Human Language Technology Research, pages 178?
182. Morgan Kaufmann Publishers Inc. San Francisco,
CA, USA.
E. Black, S. Abney, D. Flickinger, C. Gdaniec, R. Gr-
ishman, P. Harrison, D. Hindle, R. Ingria, F. Jelinek,
J. Klavans, M. Liberman, M. Marcus, S. Roukos,
B. Santorini, and T. Strzalkowski. 1991. A procedure
for quantitatively comparing the syntactic coverage
of English grammars. In Proceedings of the DARPA
Speech and Natural Language Workshop, pages 306?
311, San Mateo (CA). Morgan Kaufman.
Rens Bod. 2003. An efficient implementation of a new
DOP model. In Proceedings of the tenth conference
on European chapter of the Association for Computa-
tional Linguistics, pages 19?26, Budapest, Hungary.
Joan Bresnan. 2001. Lexical-Functional Syntax. Black-
well, Oxford.
Sabine Buchholz and Erwin Marsi. 2006. CoNLL-X
shared task on multilingual dependency parsing. In
Proceedings of the Tenth Conference on Computa-
tional Language Learning (CoNLL), pages 149?164,
New York, NY.
Marie Candito and Djame? Seddah. 2010. Parsing word
clusters. In Proceedings of the NAACL/HLT Workshop
on Statistical Parsing of Morphologically Rich Lan-
guages (SPMRL 2010), Los Angeles, CA.
Xavier Carreras, Michael Collins, and Terry Koo. 2008.
TAG, dynamic programming, and the perceptron for
efficient, feature-rich parsing. In Proceedings of the
Twelfth Conference on Computational Natural Lan-
guage Learning (CoNLL), pages 9?16, Manchester,
UK.
Eugene Charniak and Mark Johnson. 2005. Course-to-
fine n-best-parsing and maxent discriminative rerank-
ing. In Proceedings of the 43rd Annual Meeting of the
ACL, pages 173?180, Barcelona, Spain, June.
Eugene Charniak. 2000. A maximum-entropy-inspired
parser. In Proceedings of the 1st Annual Meeting of the
North American Chapter of the ACL (NAACL), Seattle.
David Chiang. 2000. Statistical parsing with an
automatically-extracted Tree Adjoining Grammar. In
Proceedings of the 38th Annual Meeting on Associ-
ation for Computational Linguistics, pages 456?463,
Hong Kong. Association for Computational Linguis-
tics Morristown, NJ, USA.
Tagyoung Chung, Matt Post, and Daniel Gildea. 2010.
Factors affecting the accuracy of Korean parsing. In
Proceedings of the NAACL/HLT Workshop on Sta-
tistical Parsing of Morphologically Rich Languages
(SPMRL 2010), Los Angeles, CA.
Michael Collins, Jan Hajic?, Lance Ramshaw, and
Christoph Tillmann. 1999. A statistical parser for
Czech. In Proceedings of the 37th Annual Meeting
of the ACL, volume 37, pages 505?512, College Park,
MD.
Michael Collins. 1997. Three Generative, Lexicalized
Models for Statistical Parsing. In Proceedings of the
35th Annual Meeting of the Association for Computa-
tional Linguistics, pages 16?23, Madrid, Spain.
Anna Corazza, Alberto Lavelli, Giogio Satta, and
Roberto Zanoli. 2004. Analyzing an Italian treebank
with state-of-the-art statistical parsers. In Proceedings
of the Third Third Workshop on Treebanks and Lin-
guistic Theories (TLT 2004), Tu?bingen, Germany.
Brooke Cowan and Michael Collins. 2005. Morphology
and reranking for the statistical parsing of Spanish. In
in Proceedins of EMNLP.
Benoit Crabbe? and Marie Candito. 2008. Expe?riences
d?analyse syntaxique statistique du franc?ais. In Actes
de la 15e`me Confe?rence sur le Traitement Automatique
des Langues Naturelles (TALN?08), pages 45?54, Avi-
gnon, France.
Amit Dubey and Frank Keller. 2003. Probabilistic pars-
ing for German using sister-head dependencies. In In
Proceedings of the 41st Annual Meeting of the Asso-
ciation for Computational Linguistics, pages 96?103,
Ann Arbor, MI.
Amit Dubey. 2005. What to do when lexicalization fails:
parsing German with suffix analysis and smoothing.
In 43rd Annual Meeting of the Association for Compu-
tational Linguistics.
10
Jenny Rose Finkel, Alex Kleeman, and Christopher D.
Manning. 2008. Efficient, feature-based, conditional
random field parsing. In Proceedings of ACL.
Yoav Goldberg and Michael Elhadad. 2010. Easy-
first dependency parsing of Modern Hebrew. In Pro-
ceedings of the NAACL/HLT Workshop on Statistical
Parsing of Morphologically Rich Languages (SPMRL
2010), Los Angeles, CA.
Yoav Goldberg and Reut Tsarfaty. 2008. A single frame-
work for joint morphological segmentation and syntac-
tic parsing. In Proceedings of the 46nd Annual Meet-
ing of the Association for Computational Linguistics.
Yoav Goldberg, Reut Tsarfaty, Meni Adler, and Michael
Elhadad. 2009. Enhancing unlexicalized parsing per-
formance using a wide coverage lexicon, fuzzy tag-set
mapping, and em-hmm-based lexical probabilities. In
Proceedings of the 12th Conference of the European
Chapter of the Association for Computational Linguis-
tics, pages 327?335.
Kenneth L. Hale. 1983. Warlpiri and the grammar of
non-configurational languages. Natural Language and
Linguistic Theory, 1(1).
Erhard W. Hinrichs, Sandra Ku?bler, and Karin Naumann.
2005. A unified representation for morphological,
syntactic, semantic, and referential annotations. In
Proceedings of the ACL Workshop on Frontiers in Cor-
pus Annotation II: Pie in the Sky, pages 13?20, Ann
Arbor, MI.
Liang Huang. 2008. Forest reranking: Discriminative
parsing with non-local features. In Proceedings of
ACL.
Dan Klein and Christopher D. Manning. 2003. Accurate
unlexicalized parsing. In Proceedings of ACL, pages
423?430.
Sandra Ku?bler, Erhard W. Hinrichs, and Wolfgang Maier.
2006. Is it really that difficult to parse German?
In Proceedings of the 2006 Conference on Empirical
Methods in Natural Language Processing, pages 111?
119, Sydney, Australia, July. Association for Compu-
tational Linguistics.
Sandra Ku?bler. 2008. The PaGe 2008 shared task on
parsing German. In Proceedings of the Workshop on
Parsing German, pages 55?63. Association for Com-
putational Linguistics.
Seth Kulick, Ryan Gabbard, and Mitchell Marcus. 2006.
Parsing the Arabic treebank: Analysis and improve-
ments. In Proceedings of TLT.
Dekang Lin. 1995. A dependency-based method for
evaluating broad-coverage parsers. In International
Joint Conference on Artificial Intelligence, pages
1420?1425, Montreal.
Mohamed Maamouri, Ann Bies, Tim Buckwalter, and
Wigdan Mekki. 2004. The Penn Arabic treebank:
Building a large-scale annotated Arabic corpus. In
Proceedings of NEMLAR International Conference on
Arabic Language Resources and Tools.
Mohamed Maamouri, Ann Bies, and Seth Kulick. 2008.
Enhanced annotation and parsing of the Arabic tree-
bank. In Proceedings of INFOS.
David M. Magerman. 1995. Statistical decision-tree
models for parsing. In Proceedings of the 33rd An-
nual Meeting on Association for Computational Lin-
guistics, pages 276?283, Cambridge, MA.
Wolfgang Maier. 2010. Direct parsing of discontin-
uous constituents in german. In Proceedings of the
NAACL/HLT Workshop on Statistical Parsing of Mor-
phologically Rich Languages (SPMRL 2010), Los An-
geles, CA.
Mitchell Marcus, Beatrice Santorini, and Mary Ann
Marcinkiewicz. 1993. Building a large annotated cor-
pus of English: The Penn Treebank. Computational
Linguistics, 19(2):313?330.
Yuval Marton, Nizar Habash, and Owen Rambow. 2010.
Improving Arabic dependency parsing with lexical and
inflectional morphological features. In Proceedings of
the NAACL/HLT Workshop on Statistical Parsing of
Morphologically Rich Languages (SPMRL 2010), Los
Angeles, CA.
Ryan T. McDonald and Fernando C. N. Pereira. 2006.
Online learning of approximate dependency parsing
algorithms. In Proc. of EACL?06.
Ryan T. McDonald, Koby Crammer, and Fernando C. N.
Pereira. 2005. Online large-margin training of depen-
dency parsers. In Proc. of ACL?05, Ann Arbor, USA.
Igor Mel?c?uk. 1988. Dependency Syntax: Theory and
Practice. State University of New York Press.
Marianne Mithun. 1992. Is basic word order universal?
In Doris L. Payne, editor, Pragmatics of Word Order
Flexibility. John Benjamins, Amsterdam.
Joakim Nivre and Jens Nilsson. 2005. Pseudo-projective
dependency parsing. In Proceedings of the 43rd An-
nual Meeting of the Association for Computational
Linguistics (ACL), Ann Arbor, MI.
Joakim Nivre, Johan Hall, Sandra Ku?bler, Ryan Mc-
Donald, Jens Nilsson, Sebastian Riedel, and Deniz
Yuret. 2007a. The CoNLL 2007 shared task on depen-
dency parsing. In Proceedings of the CoNLL Shared
Task Session of EMNLP-CoNLL 2007, pages 915?932,
Prague, Czech Republic, June. Association for Com-
putational Linguistics.
Joakim Nivre, Johan Hall, Sandra Ku?bler, Ryan McDon-
ald, Jens Nilsson, Sebastian Riedel, and Deniz Yuret.
2007b. The CoNLL 2007 shared task on dependency
parsing. In Proceedings of the CoNLL Shared Task
Session of EMNLP-CoNLL 2007, pages 915?932.
11
Joakim Nivre, Johan Hall, Jens Nilsson, Atanas Chanev,
Gu?ls?en Eryig?it, Sandra Ku?bler, Svetoslav Marinov,
and Erwin Marsi. 2007c. MaltParser: A language-
independent system for data-driven dependency pars-
ing. Natural Language Engineering, 13(2):95?135.
Joakim Nivre. 2009. Non-projective dependency parsing
in expected linear time. In Proceedings of the Joint
Conference of the 47th Annual Meeting of the ACL and
the 4th International Joint Conference on Natural Lan-
guage Processing of the AFNLP.
Slav Petrov and Dan Klein. 2007. Improved inference
for unlexicalized parsing. In Human Language Tech-
nologies 2007: The Conference of the North American
Chapter of the Association for Computational Linguis-
tics; Proceedings of the Main Conference.
Slav Petrov, Leon Barrett, Romain Thibaux, and Dan
Klein. 2006. Learning accurate, compact, and inter-
pretable tree annotation. In Proceedings of the 21st In-
ternational Conference on Computational Linguistics
and 44th Annual Meeting of the Association for Com-
putational Linguistics, Sydney, Australia, July. Asso-
ciation for Computational Linguistics.
Ines Rehbein and Josef van Genabith. 2007. Treebank
annotation schemes and parser evaluation for German.
In Proceedings of the 2007 Joint Conference on Em-
pirical Methods in Natural Language Processing and
Computational Natural Language Learning (EMNLP-
CoNLL), Prague, Czech Republic.
Geoffrey Sampson and Anna Babarczy. 2003. A test of
the leaf-ancestor metric for parse accuracy. Natural
Language Engineering, 9(04):365?380.
Natalie Schluter and Josef van Genabith. 2007. Prepar-
ing, restructuring, and augmenting a French Treebank:
Lexicalised parsers or coherent treebanks? In Proc. of
PACLING 07, Melbourne, Australia.
Djame? Seddah, Marie Candito, and Benoit Crabbe?. 2009.
Cross parser evaluation and tagset variation: A French
Treebank study. In Proceedings of the 11th Interna-
tion Conference on Parsing Technologies (IWPT?09),
pages 150?161, Paris, France, October. Association
for Computational Linguistics.
Djame? Seddah, Grzegorz Chrupa?a, Ozlem Cetinoglu,
Josef van Genabith, and Marie Candito. 2010.
Lemmatization and statistical lexicalized parsing of
morphologically-rich languages. In Proceedings of the
NAACL/HLT Workshop on Statistical Parsing of Mor-
phologically Rich Languages (SPMRL 2010), Los An-
geles, CA.
Wojciech Skut, Thorsten Brants, Brigitte Krenn, and
Hans Uszkoreit. 1998. A linguistically interpreted
corpus of German newspaper texts. In ESSLLI
Workshop on Recent Advances in Corpus Annotation,
Saarbru?cken, Germany.
Reut Tsarfaty and Khalil Sima?an. 2007. Three-
dimensional parametrization for parsing morphologi-
cally rich languages. In Proceedings of the 10th Inter-
national Conference on Parsing Technologies (IWPT),
pages 156?167.
Reut Tsarfaty and Khalil Sima?an. 2008. Relational-
Realizational parsing. In Proceedings of the 22nd In-
ternational Conference on Computational Linguistics,
pages 889?896.
Reut Tsarfaty and Khalil Sima?an. 2010. Model-
ing morphosyntactic agreement in constituency-based
parsing of Modern Hebrew. In Proceedings of the
NAACL/HLT Workshop on Statistical Parsing of Mor-
phologically Rich Languages (SPMRL 2010), Los An-
geles, CA.
Yannick Versley and Ines Rehbein. 2009. Scalable dis-
criminative parsing for german. In Proceedings of the
11th International Conference on Parsing Technolo-
gies (IWPT?09), pages 134?137, Paris, France, Octo-
ber. Association for Computational Linguistics.
12
Proceedings of the 3rd Workshop on Computational Approaches to Subjectivity and Sentiment Analysis, pages 104?109,
Jeju, Republic of Korea, 12 July 2012. c?2012 Association for Computational Linguistics
Semantic frames as an anchor representation for sentiment analysis
Josef Ruppenhofer
Department of Information Science
and Natural Language Processing
University of Hildesheim, Germany
ruppenho@uni-hildesheim.de
Ines Rehbein
SFB 632: Information Structure
German Department
Potsdam University, Germany
irehbein@uni-potsdam.de
Abstract
Current work on sentiment analysis is char-
acterized by approaches with a pragmatic fo-
cus, which use shallow techniques in the inter-
est of robustness but often rely on ad-hoc cre-
ation of data sets and methods. We argue that
progress towards deep analysis depends on
a) enriching shallow representations with lin-
guistically motivated, rich information, and b)
focussing different branches of research and
combining ressources to create synergies with
related work in NLP. In the paper, we propose
SentiFrameNet, an extension to FrameNet, as
a novel representation for sentiment analysis
that is tailored to these aims.
1 Introduction
Sentiment analysis has made a lot of progress on
more coarse-grained analysis levels using shallow
techniques. However, recent years have seen a trend
towards more fine-grained and ambitious analyses
requiring more linguistic knowledge and more com-
plex statistical models. Recent work has tried to pro-
duce relatively detailed summaries of opinions ex-
pressed in news texts (Stoyanov and Cardie, 2011);
to assess the impact of quotations from business
leaders on stock prices (Drury et al, 2011); to detect
implicit sentiment (Balahur et al, 2011); etc. Ac-
cordingly, we can expect that greater demands will
be made on the amount of linguistic knowledge, its
representation, and the evaluation of systems.
Against this background, we argue that it is
worthwhile to complement the existing shallow
and pragmatic approaches with a deep, lexical-
semantics based one in order to enable deeper analy-
sis. We report on ongoing work in constructing Sen-
tiFrameNet, an extension of FrameNet (Baker et al,
1998) offering a novel representation for sentiment
analysis based on frame semantics.
2 Shallow and pragmatic approaches
Current approaches to sentiment analysis are mainly
pragmatically oriented, without giving equal weight
to semantics. One aspect concerns the identifica-
tion of sentiment-bearing expressions. The anno-
tations in the MPQA corpus (Wiebe et al, 2005),
for instance, were created without limiting what an-
notators can annotate in terms of syntax or lexicon.
While this serves the spirit of discovering the variety
of opinion expressions in actual contexts, it makes
it difficult to match opinion expressions when us-
ing the corpus as an evaluation dataset as the same
or similar structures may be treated differently. A
similar challenge lies in distinguishing so-called po-
lar facts from inherently sentiment-bearing expres-
sions. For example, out of context, one would not
associate any of the words in the sentence Wages
are high in Switzerland with a particular evaluative
meaning. In specific contexts, however, we may
take the sentence as reason to either think positively
or negatively of Switzerland: employees receiving
wages may be drawn to Switzerland, while employ-
ers paying wages may view this state of affairs neg-
atively. As shown by the inter-annotator agreement
results reported by (Toprak et al, 2010), agreement
on distinguishing polar facts from inherently eval-
uative language is low. Unsurprisingly, many ef-
forts at automatically building up sentiment lexica
simply harvest expressions that frequently occur as
part of polar facts without resolving whether the sub-
jectivity clues extracted are inherently evaluative or
104
merely associated with statements of polar fact.
Pragmatic considerations also lead to certain ex-
pressions of sentiment or opinion being excluded
from analysis. (Seki, 2007), for instance, annotated
sentences as ?not opinionated? if they contain indi-
rect hearsay evidence or widely held opinions.
In the case of targets, the work by (Stoyanov and
Cardie, 2008) exhibits a pragmatic focus as well.
These authors distinguish between (a) the topic of
a fine-grained opinion, defined as the real-world ob-
ject, event or abstract entity that is the subject of the
opinion as intended by the opinion holder; (b) the
topic span associated with an opinion expression is
the closest, minimal span of text that mentions the
topic; and (c) the target span defined as the span
of text that covers the syntactic surface form com-
prising the contents of the opinion. As the defini-
tions show, (Stoyanov and Cardie, 2008) focus on
text-level, pragmatic relevance by paying attention
to what the author intends, rather than concentrat-
ing on the explicit syntactic dependent (their target
span) as the topic. This pragmatic focus is also in
evidence in (Wilson, 2008)?s work on contextual po-
larity classification, which uses features in the clas-
sification that are syntactically independent of the
opinion expression such as the number of subjectiv-
ity clues in adjoining sentences.
Among lexicon-driven approaches, we find that
despite arguments that word sense distinctions are
important to sentiment analysis (Wiebe and Mihal-
cea, 2006), often-used resources do not take them
into account and new resources are still being cre-
ated which operate on the more shallow lemma-level
(e.g. (Neviarouskaya et al, 2009)). Further, most
lexical resources do not adequately represent cases
where multiple opinions are tied to one expression
and where presuppositions and temporal structure
come into play. An example is the verb despoil:
there is a positive opinion by the reporter about the
despoiled entity in its former state, a negative opin-
ion about its present state, and (inferrable) negative
sentiment towards the despoiler. In most resources,
the positive opinion will not be represented.
The most common approach to the task is an in-
formation extraction-like pipeline. Expressions of
opinion, sources and targets are often dealt with sep-
arately, possibly using separate resources. Some
work such as (Kim and Hovy, 2006) has explored
the connection to role labeling. One reason not to
pursue this is that ?in many practical situations, the
annotation beyond opinion holder labeling is too ex-
pensive? (Wiegand, 2010, p.121). (Shaikh et al,
2007) use semantic dependencies and composition
rules for sentence-level sentiment scoring but do not
deal with source and target extraction. The focus on
robust partial solutions, however, prevents the cre-
ation of an integrated high-quality resource.
3 The extended frame-semantic approach
We now sketch a view of sentiment analysis on the
basis of an appropriately extended model of frame
semantic representation.1
Link to semantic frames and roles Since the pos-
sible sources and targets of opinion are usually iden-
tical to a predicate?s semantic roles, we add opinion
frames with slots for Source, Target, Polarity and
Intensity to the FrameNet database. We map the
Source and Target opinion roles to semantic roles
as appropriate, which enables us to use semantic
role labeling systems in the identification of opinion
roles (Ruppenhofer et al, 2008).
In SentiFrameNet al lexical units (LUs) that are
inherently evaluative are associated with opinion
frames. The language of polar facts is not associ-
ated with opinion frames. However, we show in the
longer version of this paper (cf. footnote 1) how we
support certain types of inferred sentiment. With re-
gard to targets, our representation selects as targets
of opinion the target spans of (Stoyanov and Cardie,
2008) rather than their opinion topics (see Section
2). For us, opinion topics that do not coincide with
target spans are inferential opinion targets.
Formal diversity of opinion expressions For fine-
grained sentiment-analysis, handling the full vari-
ety of opinion expressions is indispensable. While
adjectives in particular have often been found to
be very useful cues for automatic sentiment anal-
ysis (Wiebe, 2000; Benamara et al, 2007), eval-
uative meaning pervades all major lexical classes.
There are many subjective multi-words and idioms
such as give away the store and evaluative mean-
ing also attaches to grammatical constructions, even
ones without obligatory lexical material. An exam-
1We present a fuller account of our ideas in an unpublished
longer version of this paper, available from the authors? web-
sites.
105
ple is the construction exemplified by Him be a doc-
tor? The so-called What, me worry?-construction
(Fillmore, 1989) consists only of an NP and an in-
finitive phrase. Its rhetorical effect is to express the
speaker?s surprise or incredulity about the proposi-
tion under consideration. The FrameNet database
schema accommodates not only single and multi-
words but also handles data for a constructicon (Fill-
more et al, to appear) that pairs grammatical con-
structions with meanings.
Multiple opinions We need to accommodate multi-
ple opinions relating to the same predicate as in the
case of despoil mentioned above. Predicates with
multiple opinions are not uncommon: in a 100-item
random sample taken from the Pittsburgh subjectiv-
ity clues, 17 involved multiple opinions.
The use of opinion frames as described above en-
ables us to readily represent multiple opinions. For
instance, the verb brag in the modified Bragging
frame has two opinion frames. The first one has pos-
itive polarity and represents the frame-internal point
of view. The SPEAKER is the Source relative to the
TOPIC as the Target. The second opinion frame has
negative polarity, representing the reporter?s point of
view. The SPEAKER is the Target but the Source is
unspecified, indicating that it needs to be resolved
to an embedded source. For a similar representation
of multiple opinions in a Dutch lexical resource, see
(Maks and Vossen, 2011).
Event structure and presuppositions A complete
representation of subjectivity needs to include event
and presuppositional structure. This is necessary,
for instance, for predicates like come around (on) in
(1), which involve changes of opinion relative to the
same target by the same source. Without the pos-
sibility of distinguishing between attitudes held at
different times, the sentiment associated with these
predicates cannot be modeled adequately.
(1) Newsom is still against extending weekday me-
tering to evenings, but has COME AROUND on
Sunday enforcement.
For come around (on), we want to to distinguish
its semantics from that of predicates such as ambiva-
lent and conflicted, where a COGNIZER simultane-
ously holds opposing valuations of (aspects of) a tar-
get. Following FrameNet?s practice, we model pre-
supposed knowledge explicitly in SentiFrameNet by
Figure 1: Frame analysis for "Come around"
using additional frames and frame relations. A par-
tial analysis of come around is sketched in Figure 1.
We use the newly added Come around scenario
frame as a background frame that ties together all
the information we have about instances of coming
around. Indicated by the dashed lines are the SUB-
FRAMES of the scenario. Among them are three
instances of the Deciding frame (solid lines), all
related temporally (dashed-dotted) and in terms of
content to an ongoing Discussion. The initial dif-
ference of opinion is encoded by the fact that De-
ciding1 and Deciding2 share the same POSSIBILI-
TIES but differ in the DECISION. The occurrence
of Come_around leads to Deciding3, which has the
same COGNIZER as Deciding1 but its DECISION is
now identical to that in Deciding2, which has been
unchanged. The sentiment information we need is
encoded by simply stating that there is a sentiment
of positive polarity of the COGNIZER (as source)
towards the DECISION (as target) in the Deciding
frame. (This opinion frame is not displayed in the
graphic.) The Come around frame itself is not as-
106
sociated with sentiment information, which seems
right given that it does not include a DECISION as a
frame element but only includes the ISSUE.
For a discussion of how SentiFrameNet captures
factuality presuppositions by building on (Saur?,
2008)?s work on event factuality, we refer the inter-
ested reader to the longer version of the paper.
Modulation, coercion and composition Speakers
can shift the valence or polarity of sentiment-bearing
expressions through some kind of negation operator,
or intensify or attenuate the impact of an expression.
Despite these interacting influences, it is desirable to
have at least a partial ordering among predicates re-
lated to the same semantic scale; we want to be able
to find out from our resource that good is less pos-
itive than excellent, while there may be no ordering
between terrific and excellent. In SentiFrameNet, an
ordering between the polarity strength values of dif-
ferent lexical units is added on the level of frames.
The frame semantic approach also offers new per-
spectives on sentiment composition. We can, for in-
stance, recognize cases of presupposed sentiment,
as in the case of the noun revenge, which are not
amenable to shifting by negation: She did not take
revenge does not imply that there is no negative eval-
uation of some injury inflicted by an offender.
Further, many cases of what has been called va-
lence shifting for us are cases where the evaluation
is wholly contained in a predicate.
(2) Just barely AVOIDED an accident today.
(3) I had served the bank for 22 years and had
AVOIDED a promotion since I feared that I
would be transferred out of Chennai city.
If we viewed avoid as a polarity shifter and fur-
ther treated nouns like promotion and accident as
sentiment-bearing (rather than treating them as de-
noting events that affect somebody positively or neg-
atively) we should expect that while (2) has positive
sentiment, (3) has negative sentiment. But that is not
so: accomplished intentional avoiding is always pos-
itive for the avoider. Also, the reversal analysis for
avoid cannot deal with complements that have no in-
herent polarity. It readily follows from the coercion
analysis that I avoid running into her is negative but
that cannot be derived in e.g. (Moilanen and Pul-
man, 2007)?s compositional model which takes into
account inherent lexical polarity, which run (into)
lacks. The fact that avoid imposes a negative evalu-
ation by its subject on its object can easily be mod-
eled using opinion frames.
4 Impact and Conclusions
Deep analysis Tying sentiment analysis to frame se-
mantics enables immediate access to a deeper lexical
semantics. Given particular application-interests,
for instance, identifying statements of uncertainty,
frames and lexical units relevant to the task can
be pulled out easily from the general resource. A
frame-based treatment also improves over resources
such as SentiWordNet (Baccianella et al, 2008),
which, while representing word meanings, lacks any
representation of semantic roles.
Theoretical insights New research questions await,
among them: whether predicates with multiple opin-
ions can be distinguished automatically from ones
with only one, and whether predicates carrying fac-
tivity or other sentiment-related presuppositions can
be discovered automatically. Further, our approach
lets us ask how contextual sentiment is, and how
much of the analysis of pragmatic annotations can
be derived from lexical and syntactic knowledge.
Evaluation With a frame-based representation,
the units of annotation are pre-defined by a gen-
eral frame semantic inventory and systems can read-
ily know what kind of units to target as potential
opinion-bearing expressions. Once inherent seman-
tics and pragmatics are distinguished, the correct-
ness of inferred (pragmatic) targets and the polarity
towards them can be weighted differently from that
of immediate (semantic) targets and their polarity.
Synergy On our approach, lexically inherent sen-
timent information need not be annotated, it can be
imported automatically once the semantic frame?s
roles are annotated. Only pragmatic information
needs to be labeled manually. By expanding the
FrameNet inventory and creating annotations, we
improve a lexical resource and create role-semantic
annotationsas well as doing sentiment analysis.
We have proposed SentiFrameNet as a linguisti-
cally sound, deep representation for sentiment anal-
ysis, extending an existing resource. Our approach
complements pragmatic approaches, allows us to
join forces with related work in NLP (e.g. role label-
ing, event factuality) and enables new insights into
the theoretical foundations of sentiment analysis.
107
References
S. Baccianella, A. Esuli, and F. Sebastiani. 2008. SEN-
TIWORDNET 3.0: An enhanced lexical resource
for sentiment analysis and opinion mining. In Pro-
ceedings of the Seventh conference on International
Language Resources and Evaluation LREC10, pages
2200?2204. European Language Resources Associa-
tion (ELRA).
Collin F. Baker, Charles J. Fillmore, and John B. Lowe.
1998. The Berkeley Framenet Project. In Proceed-
ings of the 36th Annual Meeting of the Association
for Computational Linguistics and 17th International
Conference on Computational Linguistics-Volume 1,
pages 86?90. Association for Computational Linguis-
tics.
Alexandra Balahur, Jes?s M. Hermida, and Andr?s Mon-
toyo. 2011. Detecting implicit expressions of senti-
ment in text based on commonsense knowledge. In
Proceedings of the 2nd Workshop on Computational
Approaches to Subjectivity and Sentiment Analysis
(WASSA 2.011), pages 53?60, Portland, Oregon, June.
Association for Computational Linguistics.
Farah Benamara, Sabatier Irit, Carmine Cesarano, Napoli
Federico, and Diego Reforgiato. 2007. Sentiment
analysis : Adjectives and adverbs are better than ad-
jectives alone. In Proc of Int Conf on Weblogs and
Social Media, pages 1?4.
Brett Drury, Ga?l Dias, and Lu?s Torgo. 2011. A con-
textual classification strategy for polarity analysis of
direct quotations from financial news. In Proceedings
of the International Conference Recent Advances in
Natural Language Processing 2011, pages 434?440,
Hissar, Bulgaria, September. RANLP 2011 Organising
Committee.
Charles J. Fillmore, Russell Lee-Goldman, and Russell
Rhodes, to appear. Sign-based Construction Gram-
mar, chapter The FrameNet Constructicon. CSLI,
Stanford, CA.
Charles J. Fillmore. 1989. Grammatical construction
theory and the familiar dichotomies. In R. Dietrich
and C.F. Graumann, editors, Language processing in
social context, pages 17?38. North-Holland/Elsevier,
Amsterdam.
S.M. Kim and E. Hovy. 2006. Extracting opinions, opin-
ion holders, and topics expressed in online news media
text. In Proceedings of the Workshop on Sentiment and
Subjectivity in Text, pages 1?8. Association for Com-
putational Linguistics.
Isa Maks and Piek Vossen. 2011. A verb lexicon model
for deep sentiment analysis and opinion mining appli-
cations. In Proceedings of the 2nd Workshop on Com-
putational Approaches to Subjectivity and Sentiment
Analysis (WASSA 2.011), pages 10?18, Portland, Ore-
gon, June. Association for Computational Linguistics.
Karo Moilanen and Stephen Pulman. 2007. Senti-
ment composition. In Proceedings of RANLP 2007,
Borovets, Bulgaria.
A. Neviarouskaya, H. Prendinger, and M. Ishizuka.
2009. Sentiful: Generating a reliable lexicon for senti-
ment analysis. In Affective Computing and Intelligent
Interaction and Workshops, 2009. ACII 2009. 3rd In-
ternational Conference on, pages 1?6. Ieee.
J. Ruppenhofer, S. Somasundaran, and J. Wiebe. 2008.
Finding the sources and targets of subjective expres-
sions. In LREC, Marrakech, Morocco.
Roser Saur?. 2008. A Factuality Profiler for Eventualities
in Text. Ph.d., Brandeis University.
Yohei Seki. 2007. Crosslingual opinion extraction from
author and authority viewpoints at ntcir-6. In Proceed-
ings of NTCIR-6 Workshop Meeting, Tokyo, Japan.
Mostafa Shaikh, Helmut Prendinger, and Ishizuka Mit-
suru. 2007. Assessing sentiment of text by semantic
dependency and contextual valence analysis. Affec-
tive Computing and Intelligent Interaction, pages 191?
202.
Veselin Stoyanov and Claire Cardie. 2008. Topic
identification for fine-grained opinion analysis. In
Proceedings of the 22nd International Conference on
Computational Linguistics - Volume 1, COLING ?08,
pages 817?824, Stroudsburg, PA, USA. Association
for Computational Linguistics.
Veselin Stoyanov and Claire Cardie. 2011. Auto-
matically creating general-purpose opinion summaries
from text. In Proceedings of RANLP 2011, pages 202?
209, Hissar, Bulgaria, September.
Cigdem Toprak, Niklas Jakob, and Iryna Gurevych.
2010. Sentence and expression level annotation of
opinions in user-generated discourse. In Proceedings
of ACL-10, the 48th Annual Meeting of the Association
for Computational Linguistics, Portland. Association
for Computational Linguistics.
Janyce Wiebe and Rada Mihalcea. 2006. Word sense and
subjectivity. In Proceedings of the 21st International
Conference on Computational Linguistics and the 44th
annual meeting of the Association for Computational
Linguistics, ACL-44, pages 1065?1072, Stroudsburg,
PA, USA. Association for Computational Linguistics.
Janyce Wiebe, Theresa Wilson, and Claire Cardie. 2005.
Annotating expressions of opinions and emotions
in language. Language Resources and Evaluation,
39(2/3):164?210.
Janyce Wiebe. 2000. Learning subjective adjectives
from corpora. In Proceedings of the Seventeenth
National Conference on Artificial Intelligence (AAAI-
2000), pages 735?740, Austin, Texas.
108
Michael Wiegand. 2010. Hybrid approaches to senti-
ment analysis. Ph.D. thesis, Saarland University, Saar-
br?cken.
Theresa Ann Wilson. 2008. Fine-grained Subjectivity
and Sentiment Analysis: Recognizing the Intensity, Po-
larity, and Attitudes of Private States. Ph.D. thesis,
University of Pittsburgh.
109
LAW VIII - The 8th Linguistic Annotation Workshop, pages 20?28,
Dublin, Ireland, August 23-24 2014.
POS error detection in automatically annotated corpora
Ines Rehbein
SFB 632 Information Structure
German Department
Potsdam University
irehbein@uni-potsdam.de
Abstract
Recent work on error detection has shown that the quality of manually annotated corpora can
be substantially improved by applying consistency checks to the data and automatically identi-
fying incorrectly labelled instances. These methods, however, can not be used for automatically
annotated corpora where errors are systematic and cannot easily be identified by looking at the
variance in the data. This paper targets the detection of POS errors in automatically annotated
corpora, so-called silver standards, showing that by combining different measures sensitive to
annotation quality we can identify a large part of the errors and obtain a substantial increase in
accuracy.
1 Introduction
Today, linguistically annotated corpora are an indispensable resource for many areas of linguistic re-
search. However, since the emergence of the first digitised corpora in the 60s, the field has changed
considerably. What was considered ?very large? in the last decades is now considered to be rather small.
Through the emergence of Web 2.0 and the spread of user-generated content, more and more data is
accessible for building corpora for specific purposes.
This presents us with new challenges for automatic preprocessing and annotation. While conventional
corpora mostly include written text which complies to grammatical standards, the new generation of
corpora contain texts from very different varieties, displaying features of spoken language, regional
variety, ungrammatical content, typos and non-canonical spelling. A large portion of the vocabulary are
unknown words (that is, not included in the training data). As a result, the accuracy of state-of-the-art
NLP tools on this type of data is often rather low. In combination with the increasing corpus sizes,
it seems that we have to lower our expectations with respect to the quality of the annotations. Time-
consuming double annotation or a manual correction of the whole corpus is often not feasible. Thus,
the use of so-called silver standards has been discussed (Hahn et al., 2010; Kang et al., 2012; Paulheim,
2013), along with their adequacy to replace carefully hand-crafted gold standard corpora.
Other approaches to address this problem come from the areas of domain adaptation and error detec-
tion. In the first field, the focus is on adapting NLP tools or algorithms to data from new domains, thus
increasing the accuracy of the tools. In error detection, the goal is to automatically identify erroneous
labels in the data and either hand those instances to a human annotator for manual correction, or to auto-
matically correct those cases. Here, the focus is not on improving the tools but on increasing the quality
of the corpus and, at the same time, reducing human effort. These approaches are not mutually exclusive
but can be seen as complementary methods for building high-quality language resources at a reasonable
expense.
We position our work at the interface of these fields. Our general objective is to build a high-quality
linguistic resource for informal spoken youth language, annotated with parts of speech (POS) informa-
tion. As we do not have the resources for proofing the whole corpus, we aim at building a silver standard
where the quality of the annotations is high enough to be useful for linguistic research. For automatic
This work is licensed under a Creative Commons Attribution 4.0 International Licence. Page numbers and proceedings
footer are added by the organisers. Licence details: http://creativecommons.org/licenses/by/4.0/
20
preprocessing, we use tagging models adapted to our data. The main contribution of this paper is in
developing and evaluating methods for POS error detection in automatically annotated corpora. We
show that our approach not only works for our data but can also be applied to canonical text from the
newspaper domain, where the POS accuracy of standard NLP tools is quite high.
The paper is structured as follows. Section 2 reviews related work on detecting annotation errors in
corpora. Section 3 describes the underlying assumptions of our approach. In Section 4, we describe
the experimental setup and data used in our experiments, and we present our results in Section 5. We
conclude in Section 6.
2 Related Work
Most work on (semi-)automatic POS error detection has focussed on identifying errors in POS assigned
by human annotators where variation in word-POS assignments in the corpus can be caused either by
ambiguous word forms which, depending on the context, can belong to different word classes, or by
incorrect judgments made by the annotators (Eskin, 2000; van Halteren, 2000; Kv?eto?n and Oliva, 2002;
Dickinson and Meurers, 2003; Loftsson, 2009).
The variation n-gram algorithm (Dickinson and Meurers, 2003) allows users to identify potentially
incorrect tagger predictions by looking at the variation in the assignment of POS tags to a particular word
ngram. The algorithm produces a ranked list of varying tagger decisions which have to be processed by
a human annotator. Potential tagger errors are positioned at the top of the list. Later work (Dickinson,
2006) extends this approach and explores the feasibility of automatically correcting these errors.
Eskin (2000) describes a method for error identification using anomaly detection, where anomalies
are defined as elements coming from a distribution different from the one in the data at hand. Kv?eto?n
and Oliva (2002) present an approach to error detection based on a semi-automatically compiled list
of impossible ngrams. Instances of these ngrams in the data are assumed to be tagging errors and are
selected for manual correction.
All these approaches are tailored towards identifying human annotation errors and cannot be applied
to our setting where we have to detect systematic errors made by automatic POS taggers. Thus, we can
not rely on anomalies or impossible ngrams in the data, as the errors made by the taggers are consistent
and, furthermore, our corpus of non-canonical spoken language includes many structures which are
considered impossible in Standard German.
Rocio et al. (2007) address the problem of finding systematic errors in POS tagger predictions. Their
method is based on a modified multiword unit extraction algorithm which extracts cohesive sequences
of tags from the corpus. These sequences are then sorted manually into linguistically sound ngrams
and potential errors. This approach addresses the correction of large, automatically annotated corpora. It
successfully identifies (a small number of) incorrectly tagged high-frequency sequences in the text which
are often based on tokenisation errors. The more diverse errors due to lexical ambiguity, which we have
to deal with in our data, are not captured by this approach.
Most promising is the approach of Loftsson (2009) who evaluates different methods for error detection,
including an ensemble of five POS taggers, where error candidates are defined as those instances for
which the predictions of the five taggers disagree. His method successfully identifies POS errors and
thus increases the POS accuracy in the corpus. Using the tagger ensemble, Loftsson (2009) is able to
identify error candidates with a precision of around 16%. He does not report recall, that is how many of
the erroneously tagged instances in the corpus have been found. We apply the ensemble method to our
data and use it as our baseline.
Relevant to us is also the work by Dligach and Palmer (2011), who show how the need for double
annotation can be efficiently reduced by only presenting carefully selected instances to the annotators
for a second vote. They compare two different selection methods. In the first approach, they select
all instances where a machine learning classifier disagrees with the human judgement. In the second
approach, they use the probability score of a maximum entropy classifier, selecting instances with the
smallest prediction margin (the difference between the probabilities for the two most probable predic-
tions). Dligach and Palmer (2011) test their approach in a Word Sense Disambiguation task. The main
21
ideas of this work, however, can be easily applied to POS tagging.
3 Identifying Systematic POS Errors
Taggers make POS errors for a number of reasons. First of all, anomalies in the input can cause the tagger
to assign an incorrect tag, e.g. for noisy input with spelling or tokenisation errors. Another source of
errors are out-of-vocabulary words, that is word forms unknown to the tagger because they do not exist
in the training data. A third reason for incorrect tagger judgments are word forms which are ambiguous
between different parts of speech. Those cases can be further divided into cases where the information
for identifying the correct label is there but the tagger does not make use of it, and into cases that are truly
ambiguous, meaning that even a human annotator would not be able to disambiguate the correct POS
tag. Tagger errors can also be caused by ill-defined annotation schemes or errors in the gold standard
(see Manning (2011) for a detailed discussion on different types of POS errors).
To assess the difficulty of the task, it might be interesting to look at the agreement achieved by human
annotators for POS tagging German. The inter-annotator agreement for POS annotation with the STTS
on written text is quite high with around 0.97-0.98 Fleiss ?, and for annotating spoken text using an
extended version of the STTS similar numbers can be obtainded (Rehbein and Schalowski, 2013).
In this work, we are not so much interested in finding tokenisation and spelling errors but in identifying
automatic tagger errors due to lexical ambiguity. Our work is based on the following assumptions:
Assumption 1: Instances of word forms which are labelled differently by different taggers
are potential POS errors.
Assumption 2: POS tags which have been assigned with a low probability by the tagger
are potential POS errors.
In the remainder of the paper, we present the development of a system for error detection and its evalu-
ation on a corpus of informal, spontaneous dialogues and on German newspaper text. We report precision
and recall for our system. Precision is computed as the number of correctly identified error candidates,
divided by the number of all (correctly and incorrectly identified) error candidates (number of true posi-
tives / (number of true positives + false positives)), and recall by dividing the number of identified errors
by the total number of errors in the data (true positives / (true positives + false negatives)).
4 Experimental Setup
The data we use in our experiments comes from two sources, i) from a corpus of informal, spoken
German youth language (The KiezDeutsch Korpus (KiDKo) Release 1.0) (Rehbein et al., 2014), and ii)
from the TIGER corpus (Brants et al., 2002), a German newspaper corpus.
4.1 Kiezdeutsch ? Informal youth language
KiDKo is a new language resource including informal, spontaneous dialogues from peer-to-peer commu-
nication of adolescents. The current version of the corpus includes the audio signals aligned with tran-
scriptions, as well as a normalisation layer and POS annotations. Additional annotation layers (Chunk-
ing, Topological Fields) are in progress.
The transcription scheme has an orthographic basis but, in order to enable investigations of prosodic
characteristics of the data, it also tries to closely capture the pronunciation, including pauses, and en-
codes disfluencies and primary accents. On the normalisation layer, non-canonical pronunciations and
capitalisation are reduced to standard German spelling. The normalisation is done on the token level,
and non-canonical word order as well as disfluencies are included in the normalised version of the data
(Example 1).
(1) [transcription]:
[normalisation]:
isch
Ich
hab
habe
au
au #
(?)
PAUSE
isch
Ich
hab
habe ,
isch
ich
hab
habe
auch
auch
?ah
?ah
FLATrate
Flatrate .
I have too # I have , I have too uh flatrate .
?I have ... I have, I have a flatrate, too.? (MuH23MT)
22
KiDKo TIGER
Baseline taggers avg. (5-fold) dev test dev test
Brill 94.4 94.7 93.8 96.8 96.8
Treetagger 95.1 95.5 94.8 97.2 97.4
Stanford 95.3 95.6 94.7 97.4 97.5
Hunpos 95.6 95.8 94.8 97.4 97.5
CRF 96.9 97.4 96.1 97.9 98.0
Table 1: Baseline results for different taggers on KiDKo and TIGER (results on KiDKo are given for a
5-fold cross validation (5-fold) and for the development and test set)
We plan to release the POS tagged version of the corpus in summer 2014. Due to legal constraints,
the audio files will have restricted access and can only be accessed locally while the transcribed and
annotated version of the corpus will be available over the internet via ANNIS (Zeldes et al., 2009).
1
4.2 The TIGER corpus
The second corpus we use in our experiments is the TIGER corpus (release 2.2), a German newspaper
corpus with approximately 50,000 sentences (900,000 tokens). We chose TIGER to show that our ap-
proach is not tailored towards one particular text type but can be applied to corpora of different sizes and
from different domains.
4.3 Baseline
In our experiments, we use a subpart of KiDKo with 103,026 tokens, split into a training set with 66,024
tokens, a development set with 16,530 tokens, and a test set with 20,472 tokens. The TIGER data was
also split into a training set (709,740 tokens), a development set (88,437 tokens) and a test set (90,061
tokens).
To test our first assumption, we trained an ensemble of five taggers on the two corpora (see list below),
and checked all instances where the taggers disagreed. We consider all cases as disagreements where at
least one of the five taggers made a prediction different from the other taggers.
The five taggers we use reflect different approaches to POS tagging (including Transformation-based
Learning, Markov Models, Maximum Entropy, Decision Trees, and Conditional Random Fields):
? the Brill tagger (Brill, 1992)
? the Hunpos tagger
2
? the Stanford POS tagger (Toutanova and Manning, 2000)
? the Treetagger (Schmid, 1995)
? a CRF-based tagger, using the CRFSuite
3
Table 1 shows the accuracies of the different taggers on KiDKo and on TIGER (because of the smaller
size of KiDKo, we also report numbers from a 5-fold cross validation on the training data). The CRF-
based tagger gives the best results on the spoken language data as well as on TIGER. For more details
on the implementation and features of the CRF tagger, please refer to (Rehbein et al., 2014).
For the KiDKo development set, we have 1,228 cases where the taggers disagree, that is 1,228 error
candidates, and 1,797 instances in the test set. Out of those, 267 (dev) and 558 (test) are true errors
(Table 2). This means that the precision of this simple heuristic is between 21.7% and 33%, with a recall
between 61.1 and 70.8%. For TIGER, precision and recall are higher. Applying this simple heuristic, we
are able to identify around 70% of the errors in the data, with a precision of around 27%. We consider
this as our baseline.
1
ANNIS (ANNotation of Information Structure) is a corpus search and visualisation interface which allows the user to
formulate complex search queries which can combine multiple layers of annotation.
2
The Hunpos tagger is an open source reimplementation of the TnT tagger (https://code.google.com/p/
hunpos)
3
http://www.chokkan.org/software/crfsuite/
23
tokens candidates true err. out of % prec % rec.
KiDKo
dev 16,530 1,228 267 437 21.7 61.1
test 20,472 1,797 558 788 33.0 70.8
TIGER
dev 88,437 4,580 1,280 1,818 27.9 70.4
test 90,061 4,618 1,246 1,754 27.0 71.0
Table 2: Number of error candidates identified by the disagreements in the ensemble tagger predictions
(baseline)
5 Finding measures for error detection
When defining measures for error detection, we have to balance precision against recall. Depending
on our research goal and resources available for corpus creation, we might either want to obtain a high
precision, meaning that we only have to look at a small number of instances which are most probably
true POS errors, or we might want to build a high-quality corpus where nearly all errors have been found
and corrected, at the cost of having to look at many instances which are mostly correct.
5.1 Increasing precision
First, we try to improve precision and thus to reduce the number of false positives we have to look at
during the manual correction phase. We do this by training a CRF classifier to detect errors in the output
of the ensemble taggers. The features we use are shown in Table 3 and include the word form, the tags
predicted by the tagger ensemble, ngram combinations of the ensemble POS tags, word and POS context
for different context windows for the POS predicted by the CRF tagger and the Treetagger, a combination
of word form and POS context (for CRF, Treetagger, and combinations of both; for window sizes of 3
and 4 with varying start and end positions), and the class label (1: error, 0: correct).
We experimented with different feature combinations and settings. Our basic feature set gives us high
precision on both data sets, with very low recall. Only around 4-6% of all errors are found. However,
precision is between 55-65%, meaning that the majority of the selected candidates are true errors.
Our extended feature sets (I and II) aim at improving recall by alleviating the sparse data problem.
The extended feature set I extracts new features where the tags from the fine-grained German tagset, the
STTS (Schiller et al., 1999), are converted into the coarse-grained universal tagset of Petrov et al. (2012),
basic features example
word form der (the)
lowercased word form der
ensemble tags PDS ART PDS PDS ART
POS context (CRF) ADV:PROAV:VAFIN:APPR, PROAV:VAFIN:APPR:PDS, ...
POS context (tree) PROAV:VAFIN:APPR, VAFIN:APPR:ART, ...
word form with POS context (CRF) PROAV:VAFIN:APPR:der, VAFIN:APPR:der:APPR, ...
word form with POS context (CRF:tree) PROAV:VAFIN:APPR:der, ..., der:APPR:ART:NN, ...
extended features I: universal POS
universal ensemble tags P D P P D
universal POS ngrams P:D, P:P, P:P, ..., P:P:P:D, P:D:P:P:D
universal POS context (CRF) ADV:P:VF:ADP, P:VF:ADP:P, ...
word form with universal POS context (CRF) P:VF:ADP:der, VF:ADP:der:ADP, ADP:der:ADP:D, ...
word form with universal POS context (CRF:tree) VF:VF:ADP:ADP:der, ADP:ADP:der:ADP:ADP, ...
extended features II: brown clusters
brown cluster for word form 110111011111
brown cluster with universal POS context (CRF) ADV:P:110111111110:ADP, P:110111111110:ADP:P, ...
class label (1 or 0) 1
Table 3: Features used for error detection
24
tokens candidates true err. out of % prec % rec
KiDKo
basic features
dev 16,530 32 21 437 65.6 4.8
test 20,472 59 32 788 54.2 4.1
extended features I (universal POS)
dev 16,530 77 38 437 49.3 8.7
test 20,472 172 88 788 51.2 11.2
extended features II (universal POS, Brown clusters)
dev 16,530 88 50 437 56.8 11.4
test 20,472 205 104 788 50.7 13.2
TIGER
basic features
dev 88,437 163 101 1,818 62.0 5.6
test 90,061 202 111 1,754 54.9 6.3
extended features I (universal POS)
dev 88,437 564 348 1,818 61.7 19.1
test 90,061 588 347 1,754 59.0 19.8
extended features II (universal POS, Brown clusters)
dev 88,437 501 318 1,818 63.5 17.5
test 90,061 518 298 1,754 57.5 17.0
Table 4: Number of error candidates identified by the classifier, precision (prec) and recall (rec)
with minor modifications.
4
On KiDKo, the universal POS features increase recall from around 5% up to
8-14%. On TIGER, the results are more substantial. Here, our recall increases from 5-6% up to nearly
20%, while precision is still in the same range (Table 4).
Our basic features were designed to add more (local) context useful for disambiguating between the
different tags. Especially the right context (assigned POS) includes information which often helps, e.g.
when distinguishing between a substitutive demonstrative pronoun (PDS) and a determiner (ART), which
is a frequent error especially in the spoken language data.
We try to achieve further improvements by adding new features where we replace the word forms
with Brown word cluster paths (Brown et al., 1992).
5
The extended features are designed to address the
unknown word problem by generalising overvi t word forms. On the smaller KiDKo data set, this again
has a positive effect, increasing both precision and recall. On TIGER, however, the results are mixed,
with a higher precision on the development set but a somewhat lower recall for both, development and
test sets. This is not surprising, as semi-supervised techniques are expected to help most for settings
where data sparseness is an issue.
Overall, our error detection classifier is able to identify errors in the corpus with a good precision,
meaning that only a small number of instances have to be checked manually in order to achieve an error
rate reduction in the range of 11-17%. This approach seems suitable when limited resources are available
for manual correction, thus asking for a method with high precision and low time requirements.
5.2 Increasing recall
While our attempts to increase precision were quite successful, we had to put up with a severe loss in
recall. However, we would like to keep precision reasonably high but also to increase recall. Our next
approach takes into account the marginal probabilities of the predictions (0: correct/1: error) of the CRF-
based error detection classifier. We not only check those instances which the classifier has labelled as
4
For instance, instead of converting all verb tags to V, we keep a tag for finite verbs (VF).
5
The word clusters have been trained on the Huge German Corpus (HGC) (Fitschen, 2004), using a cluster size of 1000, a
frequency threshold of 40 and a maximum path length of 12.
25
tokens threshold candidates true err. out of % prec % rec
KiDKo
extended features II (universal POS, Brown clusters)
dev 16,530 0.8 286 120 437 42.0 27.5
dev 16,530 0.85 350 138 437 39.4 31.6
test 20,472 0.8 472 190 788 40.2 24.1
test 20,472 0.85 561 227 788 40.5 28.8
TIGER
extended features I (universal POS)
dev 88,437 0.8 1,208 602 1,818 49.8 33.1
dev 88,437 0.85 1,431 658 1,818 46.0 36.2
test 90,061 0.8 1,276 605 1,754 47.4 34.5
test 90,061 0.85 1,554 670 1,754 43.1 38.2
Table 5: Number of error candidates identified by the classifier using a marginal probability threshold
incorrect, but also those which have been labelled as correct, but with a marginal probability below a
particular threshold. Table 5 gives results for a threshold of 0.8 and 0.85, using the best-scoring feature
sets from the last experiment.
Our new measure results in a substantial increase in recall. Setting the threshold to 0.85, we are now
able to detect around 30% of the errors in KiDKo and 36 to 38% in TIGER, while precision is still
reasonably high. Figure 1 shows the relation between precision and recall for different thresholds from
0.95 to 0.1. Setting the threshold to 0.8, for example, would result in an error prediction presicion of
around 40-42% for KiDKo and of around 47-50% for TIGER. Recall for error identification using a
threshold of 0.8 would be in the range of 24-27.5% for KiDKo and 33-34.5% for TIGER. If we wanted
to increase recall up to 50% for KiDKo, we would have to use a marginal probability threshold of
approximately 0.65, and precision would drop to around 14%. This knowledge allows us to make an
informed decision during corpus compilation, either starting from the POS accuracy we want to achieve,
or from the resources we have for manual correction, and to predict the POS accuracy of the final corpus.
Figure 1: Trade-off between precision and recall for different marginal probability thresholds
26
6 Conclusions
In the paper, we presented and evaluated a system for automatic error detection in POS tagged corpora,
with the goal of increasing the quality of so-called silver standards with minimal human effort. Our
baseline, a simple heuristic based on disagreements in tagger predictions, allows us to identify between
60 and 70% of all errors in our two data sets, but with a low precision. We show how to refine this
method, training a CRF-based classifier which is able to identify POS errors in tagger output with a
much higher precision, thus reducing the need for manual correction.
Our method is able to find different types of POS errors, including the ones most frequently made by
the tagger (adjectives, adverbs, proper names, foreign language material, finite verbs, verb particles, and
more). Furthermore, it allows us to define the parameters which are most adequate for the task at hand,
either aiming at high precision at the cost of recall, or increasing recall (and thus the annotation quality
of the corpus) at the cost of greater manual work load. In addition, our method can easily be applied to
different corpora and new languages.
References
Sabine Brants, Stefanie Dipper, Silvia Hansen, Wolfgang Lezius, and George Smith. 2002. The TIGER treebank.
In Proceedings of the First Workshop on Treebanks and Linguistic Theories, pages 24?42.
Eric Brill. 1992. A simple rule-based part of speech tagger. In 3rd conference on Applied natural language
processing (ANLC?92), Trento, Italy.
Peter F. Brown, Peter V. deSouza, Robert L. Mercer, Vincent J. Della Pietra, and Jenifer C. Lai. 1992. Class-based
n-gram models of natural language. Computational Linguistics, 18(4):467?479.
Markus Dickinson and Detmar W. Meurers. 2003. Detecting errors in part-of-speech annotation. In 10th Confer-
ence of the European Chapter of the Association for Computational Linguistics (EACL-03).
Markus Dickinson. 2006. From detecting errors to automatically correcting them. In Annual Meeting of The
European Chapter of The Association of Computational Linguistics (EACL-06), Trento, Italy.
Dmitriy Dligach and Martha Palmer. 2011. Reducing the need for double annotation. In Proceedings of the 5th
Linguistic Annotation Workshop, LAW V ?11.
Eleazar Eskin. 2000. Automatic corpus correction with anomaly detection. In 1st Conference of the North
American Chapter of the Association for Computational Linguistics (NAACL), Seattle, Washington.
Arne Fitschen. 2004. Ein computerlinguistisches Lexikon als komplexes System. Ph.D. thesis, Institut f?ur
Maschinelle Sprachverarbeitung der Universit?at Stuttgart.
U. Hahn, K. Tomanek, E. Beisswanger, and E. Faessler. 2010. A proposal for a configurable silver standard. In
The Fourth Linguistic Annotation Workshop, LAW 2010, pages 235?242.
Ning Kang, Erik van Mulligen, and Jan Kors. 2012. Training text chunkers on a silver standard corpus: can silver
replace gold? BMC Bioinformatics, 13(1):17.
Pavel Kv?eto?n and Karel Oliva. 2002. (Semi-)Automatic detection of errors in PoS-tagged corpora. In 19th
International Conference on Computational Linguistics (COLING-02).
Hrafn Loftsson. 2009. Correcting a POS-tagged corpus using three complementary methods. In Proceedings of
the 12th Conference of the European Chapter of the ACL (EACL 2009), Athens, Greece, March.
Christopher D. Manning. 2011. Part-of-speech tagging from 97linguistics? In Proceedings of the 12th Interna-
tional Conference on Computational Linguistics and Intelligent Text Processing - Volume Part I, CICLing?11,
pages 171?189.
Heiko Paulheim. 2013. Dbpedianyd - a silver standard benchmark dataset for semantic relatedness in dbpedia. In
CEUR Workshop, CEUR Workshop Proceedings. CEUR-WS.org.
Slav Petrov, Dipanjan Das, and Ryan T. McDonald. 2012. A universal part-of-speech tagset. In The Eighth
International Conference on Language Resources and Evaluation (LREC-2012), pages 2089?2096.
27
Ines Rehbein and S?oren Schalowski. 2013. STTS goes Kiez ? Experiments on annotating and tagging urban youth
language. Journal for Language Technology and Computational Linguistics.
Ines Rehbein, S?oren Schalowski, and Heike Wiese. 2014. The KiezDeutsch Korpus (KiDKo) release 1.0. In The
9th International Conference on Language Resources and Evaluation (LREC-14), Reykjavik, Iceland.
Vitor Rocio, Joaquim Silva, and Gabriel Lopes. 2007. Detection of strange and wrong automatic part-of-speech
tagging. In Proceedings of the Aritficial Intelligence 13th Portuguese Conference on Progress in Artificial
Intelligence, EPIA?07.
Anne Schiller, Simone Teufel, and Christine Thielen. 1999. Guidelines f?ur das Tagging deutscher Textkorpora
mit STTS. Technical report, Universit?at Stuttgart, Universit?at T?ubingen.
Helmut Schmid. 1995. Improvements in part-of-speech tagging with an application to German. In ACL SIGDAT-
Workshop.
Kristina Toutanova and Christopher D. Manning. 2000. Enriching the knowledge sources used in a maximum
entropy part-of-speech tagger. In Proceedings of the conference on Empirical methods in natural language
processing and very large corpora, EMNLP ?00, Hong Kong.
Hans van Halteren. 2000. The detection of inconsistency in manually tagged text. In Proceedings of the COLING-
2000 Workshop on Linguistically Interpreted Corpora, Centre Universitaire, Luxembourg, August.
Amir Zeldes, Julia Ritz, Anke L?udeling, and Christian Chiarcos. 2009. Annis: A search tool for multi-layer
annotated corpora. In Corpus Linguistics 2009.
28
