Proceedings of the 4th International Workshop on Semantic Evaluations (SemEval-2007), pages 414?417,
Prague, June 2007. c?2007 Association for Computational Linguistics
UOY: A Hypergraph Model For Word Sense Induction & Disambiguation
Ioannis P. Klapaftis
University of York
Department of Computer Science
giannis@cs.york.ac.uk
Suresh Manandhar
University of York
Department of Computer Science
suresh@cs.york.ac.uk
Abstract
This paper is an outcome of ongoing re-
search and presents an unsupervised method
for automatic word sense induction (WSI)
and disambiguation (WSD). The induction
algorithm is based on modeling the co-
occurrences of two or more words using
hypergraphs. WSI takes place by detect-
ing high-density components in the co-
occurrence hypergraphs. WSD assigns to
each induced cluster a score equal to the sum
of weights of its hyperedges found in the lo-
cal context of the target word. Our system
participates in SemEval-2007 word sense in-
duction and discrimination task.
1 Introduction
The majority of both supervised and unsupervised
approaches to WSD is based on the ?fixed-list? of
senses paradigm where the senses of a target word
is a closed list of definitions coming from a stan-
dard dictionary (Agirre et al, 2006). Lexicographers
have long warned about the problems of such an ap-
proach, since dictionaries are not suited to this task;
they often contain general definitions, they suffer
from the lack of explicit semantic and topical rela-
tions or interconnections, and they often do not re-
flect the exact content of the context, in which the
target word appears (Veronis, 2004).
To overcome this limitation, unsupervised WSD
has moved towards inducing the senses of a target
word directly from a corpus, and then disambiguat-
ing each instance of it. Most of the work in WSI
is based on the vector space model, where the con-
text of each instance of a target word is represented
as a vector of features (e.g second-order word co-
occurrences) (Schutze, 1998; Purandare and Peder-
sen, 2004). These vectors are clustered and the re-
sulting clusters represent the induced senses. How-
ever, as shown experimentally in (Veronis, 2004),
vector-based techniques are unable to detect low-
frequency senses of a target word.
Recently, graph-based methods were employed in
WSI to isolate highly infrequent senses of a target
word. HyperLex (Veronis, 2004) and the adaptation
of PageRank (Brin and Page, 1998) in (Agirre et al,
2006) have been shown to outperform the most fre-
quent sense (MFS) baseline in terms of supervised
recall, but they still fall short of supervised WSD
systems.
Graph-based approaches operate on a 2-
dimensional space, assuming a one-to-one relation-
ship between co-occurring words. However, this
assumption is insufficient, taking into account the
fact that two or more words are usually combined
to form a relationship of concepts in the context.
Additionally, graph-based approaches fail to model
and exploit the existence of collocations or terms
consisting of more than two words.
This paper proposes a method for WSI, which
is based on a hypergraph model operating on
a n-dimensional space. In such a model, co-
occurrences of two or more words are represented
using weighted hyperedges. A hyperedge is a more
expressive representation than a simple edge, be-
cause it is able to capture the information shared
by two or more words. Our system participates in
414
SemEval-2007 word sense induction and discrimi-
nation task (SWSID) (Agirre and Soroa, 2007).
2 Sense Induction & Disambiguation
This section presents the induction and disambigua-
tion algorithms.
2.1 Sense Induction
2.1.1 The Hypergraph Model
A hypergraph H = (V, F ) is a generalization of
a graph, which consists of a set of vertices V and a
set of hyperedges F ; each hyperedge is a subset of
vertices. While an edge relates 2 vertices, a hyper-
edge relates n vertices (where n ? 1). In our prob-
lem, we represent each word by a vertex and any
set of co-occurring related words by a hyperedge.
In our approach, we restrict hyperedges to 2, 3 or
4 words. Figure 1 shows an example of an abstract
hypergraph model 1.
Figure 1: An example of a Hypergraph
The degree of a vertex is the number of hyper-
edges it belongs to, and the degree of a hyperedge is
the number of vertices it contains. A path in the hy-
pergraph model is a sequence of vertices and hyper-
edges such as v1, f1, ..., vi?1, fi?1, vi, where vk are
vertices, fk are hyperedges, each hyperedge fk con-
tains vertices to its left and right in the path and no
hyperedge or vertex is repeated. The length of a path
is the number of hyperedges it contains, the distance
between two vertices is the shortest path between
them and the distance between two hyperedges is the
minimum distance of all the pairs of their vertices.
2.1.2 Building The Hypergraph
Let bp be the base corpus from which we induce
the senses of a target word tw. Our bp consists of
BNC and all the SWSID paragraphs containing the
1Image was taken from Wikipedia (Rocchini, 2006)
target word. The total size of bp is 2000 paragraphs.
Note that if SWSID paragraphs of tw are more than
2000, BNC is not used.
In order to build the hypergraph, tw is removed
from bp and each paragraph pi is POS-tagged. Fol-
lowing the example in (Agirre et al, 2006), only
nouns are kept and lemmatised. We apply two fil-
tering heuristics. The first one is the minimum fre-
quency of nouns (parameter p1), and the second one
is the minimum size of a paragraph (parameter p2).
A key problem at this stage is the determination of
related vertices (nouns), which can be grouped into
hyperedges and the weighting of each such hyper-
edge. We deal with this problem by using associa-
tion rules (Agrawal and Srikant, 1994). Frequent hy-
peredges are detected by calculating support, which
should exceed a user-defined threshold (parameter
p3).
Let f be a candidate hyperedge and a, b, c its ver-
tices. Then freq(a, b, c) is the number of para-
graphs in bp, which contain all the vertices of f , and
n is the total size of bp. Support of f is shown in
Equation 1.
support(f) =
freq(a, b, c)
n
(1)
The weight assigned to each collected hyperedge,
f , is the average of m calculated confidences, where
m is the size of f . Let f be a hyperedge containing
the vertices a, b, c. The confidence for the rule r0 =
{a, b} => {c} is defined in Equation 2.
confidence(r0) =
freq(a, b, c)
freq(a, b)
(2)
Since there is a three-way relationship among a, b
and c, we have two more rules r1 = {a, c} => {b}
and r2 = {b, c} => {a}. Hence, the weighting of
f is the average of the 3 calculated confidences. We
apply a filtering heuristic (parameter p4) to remove
hyperedges with low weights from the hypergraph.
At the end of this stage, the constructed hypergraph
is reduced, so that our hypergraph model agrees with
the one described in subsection 2.1.1.
2.1.3 Extracting Senses
Preliminary experiments on 10 nouns of
SensEval-3 English lexical-sample task (Mihalcea
et al, 2004) (S3LS), suggested that our hypergraphs
415
are small-world networks, since they exhibited
a high clustering coefficient and a small average
path length. Furthermore, the frequency of vertices
with a given degree plotted against the degree
showed that our hypergraphs satisfy a power-law
distribution P (d) = c ? d??, where d is the vertex
degree, P (d) is the frequency of vertices with
degree d. Figure 2 shows the log-log plot for the
noun difference of S3LS.
Figure 2: Log-log plot for the noun difference.
In order to extract the senses of the target word,
we modify the HyperLex algorithm (Veronis, 2004)
for selecting the root hubs of the hypergraph as fol-
lows. At each step, the algorithm finds the vertex vi
with the highest degree, which is selected as a root
hub, according to two criteria.
The first one is the minimum number of hyper-
edges it belongs to (parameter p5), and the second is
the average weight of the first p5 hyperedges (para-
meter p6) 2. If these criteria are satisfied, then hyper-
edges containing vi are grouped to a single cluster cj
(new sense) with a 0 distance from vi, and removed
from the hypergraph. The process stops, when there
is no vertex eligible to be a root hub.
Each remaining hyperedge, fk, is assigned to the
cluster, cj , closest to it, by calculating the minimum
distance between fk and each hyperedge of cj as de-
fined in subsection 2.1.1. The weight assigned to fk
is inversely proportional to its distance from cj .
2.2 Word Sense Disambiguation
Given an instance of the target word, tw, paragraph
pi containing tw is POS-tagged, nouns are kept and
2Hyperedges are sorted in decreasing order of weight
lemmatised. Next, each induced cluster cj is as-
signed a score equal to the sum of weights of its
hyperedges found in pi.
3 Evaluation
3.1 Preliminary Experiments
This method is an outcome of ongoing research.
Due to time restrictions we were able to test and
tune (Table 1), but not optimize, our system only on
a very small set of nouns of S3LS targeting at a high
supervised recall. Our supervised recall on the 10
first nouns of S3LS was 66.8%, 9.8% points above
the MFS baseline.
Parameter Value
p1:Minimum frequency of a noun 8
p2:Minimum size of a paragraph 4
p3:Support threshold 0.002
p4:Average confidence threshold 0.2
p5:Minimum number of hyperedges 6
p6:Minimum average weight of hyperedges 0.25
Table 1: Chosen parameters for our system
3.2 SemEval-2007 Results
Tables 2 and 3 show the average supervised recall,
FScore, entropy and purity of our system on nouns
and verbs of the test data respectively. The submit-
ted answer consisted only of the winning cluster per
instance of a target word, in effect assigning it with
weight 1 (default).
Entropy measures how well the various gold stan-
dard senses are distributed within each cluster, while
purity measures how pure a cluster is, containing ob-
jects from primarily one class. In general, the lower
the entropy and the larger the purity values, the bet-
ter the clustering algorithm performs.
Measure Proposed methodology MFS
Entropy 25.5 46.3
Purity 89.8 82.4
FScore 65.8 80.7
Sup. Recall 81.6 80.9
Table 2: System performance for nouns.
For nouns our system achieves a low entropy and
a high purity outperforming the MFS baseline, but a
lower FScore. This can be explained by the fact that
the average number of clusters we produce for nouns
is 11, while the gold standard average of senses is
around 2.8. For verbs the performance of our system
416
is worse than for nouns, although entropy and purity
still outperform the MFS baseline. FScore is very
low, despite that the average number of clusters we
produce for verbs (around 8) is less than the number
of clusters we produce for nouns. This means that
for verbs the senses of gold standard are much more
spread among induced clusters than for nouns, caus-
ing a low unsupervised recall. Overall, FScore re-
sults are in accordance with the idea of microsenses
mentioned in (Agirre et al, 2006). FScore is biased
towards clusters similar to the gold standard senses
and cannot capture that theory.
Measure Proposed methodology MFS
Entropy 28.9 44.4
Purity 82.0 77
F-score 45.1 76.8
Sup. Recall 73.3 76.2
Table 3: System performance for verbs.
Our supervised recall for verbs is 73.3%, and be-
low the MFS baseline (76.2%), which no system
managed to outperform. For nouns our supervised
recall is 81.6%, which is around 0.7% above the
MFS baseline. In order to fully examine the perfor-
mance of our system we applied a second evaluation
of our methodology using the SWSID official soft-
ware.
The solution per target word instance included the
entire set of clusters with their associated weights
(Table 4). Results show that the submitted answer
(instance - winning cluster), was degrading seri-
ously our performance both for verbs and nouns due
to the loss of information in the mapping step.
POS Proposed Methodology MFS
Nouns 84.3 80.9
Verbs 75.6 76.2
Total 80.2 78.7
Table 4: Supervised recall in second evaluation.
Our supervised recall for nouns has outperformed
the MFS baseline by 3.4% with the best system
achieving 86.8%. Performance for verbs is 75.6%,
0.6% below the best system and MFS.
4 Conclusion
We have presented a hypergraph model for word
sense induction and disambiguation. Preliminary
experiments suggested that our reduced hypergraphs
are small-world networks. WSI identifies the highly
connected components (hubs) in the hypergraph,
while WSD assigns to each cluster a score equal to
the sum of weights of its hyperedges found in the
local context of a target word.
Results show that our system achieves high en-
tropy and purity performance outperforming the
MFS baseline. Our methodology achieves a low
FScore producing clusters that are dissimilar to the
gold standard senses. Our supervised recall for
nouns is 3.4% above the MFS baseline. For verbs,
our supervised recall is below the MFS baseline,
which no system managed to outperform.
References
Eneko Agirre and Aitor Soroa. 2007. Semeval-2007 task
2: Evaluating word sense induction and discrimination
systems. In Proceedings of SemEval-2007. ACL.
Eneko Agirre, David Mart??nez, Oier Lo?pez de Lacalle,
and Aitor Soroa. 2006. Two graph-based algorithms
for state-of-the-art wsd. In Proceedings of the EMNLP
Conference, pages 585?593. ACL.
Rakesh Agrawal and Ramakrishnan Srikant. 1994. Fast
algorithms for mining association rules in large data-
bases. In VLDB ?94: Proceedings of the 20th Inter-
national Conference on Very Large DataBases, pages
487?499, USA. Morgan Kaufmann Publishers Inc.
Sergey Brin and Lawrence Page. 1998. The anatomy of
a large-scale hypertextual Web search engine. Com-
puter Networks and ISDN Systems, 30(1?7):107?117.
Rada Mihalcea, Timothy Chklovski, and Adam Kilgar-
riff. 2004. The senseval-3 english lexical sample task.
In R. Mihaleca and P. Edmonds, editors, SensEval-3
Proceedings, pages 25?28, Spain, July. ACL.
Amruta Purandare and Ted Pedersen. 2004. Word sense
discrimination by clustering contexts in vector and
similarity spaces. In Proceedings of CoNLL-2004,
pages 41?48. ACL.
Claudio Rocchini. 2006. Hypergraph sample image.
Wikipedia.
Hinrich Schutze. 1998. Automatic word sense discrimi-
nation. Computational Linguistics, 24(1):97?123.
Jean Veronis. 2004. Hyperlex:lexical cartography for
information retrieval. Computer Speech & Language,
18(3).
417
Proceedings of the NAACL HLT Workshop on Unsupervised and Minimally Supervised Learning of Lexical Semantics, pages 36?44,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
Graph Connectivity Measures for Unsupervised Parameter Tuning
of Graph-Based Sense Induction Systems
Ioannis Korkontzelos, Ioannis Klapaftis and Suresh Manandhar
Department of Computer Science
The University of York
Heslington, York, YO10 5NG, UK
{johnkork, giannis, suresh}@cs.york.ac.uk
Abstract
Word Sense Induction (WSI) is the task of
identifying the different senses (uses) of a tar-
get word in a given text. This paper focuses
on the unsupervised estimation of the free pa-
rameters of a graph-based WSI method, and
explores the use of eight Graph Connectiv-
ity Measures (GCM) that assess the degree of
connectivity in a graph. Given a target word
and a set of parameters, GCM evaluate the
connectivity of the produced clusters, which
correspond to subgraphs of the initial (unclus-
tered) graph. Each parameter setting is as-
signed a score according to one of the GCM
and the highest scoring setting is then selected.
Our evaluation on the nouns of SemEval-2007
WSI task (SWSI) shows that: (1) all GCM es-
timate a set of parameters which significantly
outperform the worst performing parameter
setting in both SWSI evaluation schemes, (2)
all GCM estimate a set of parameters which
outperform the Most Frequent Sense (MFS)
baseline by a statistically significant amount
in the supervised evaluation scheme, and (3)
two of the measures estimate a set of parame-
ters that performs closely to a set of parame-
ters estimated in supervised manner.
1 Introduction
Using word senses instead of word forms is essential
in many applications such as information retrieval
(IR) and machine translation (MT) (Pantel and Lin,
2002). Word senses are a prerequisite for word sense
disambiguation (WSD) algorithms. However, they
are usually represented as a fixed-list of definitions
of a manually constructed lexical database. The
fixed-list of senses paradigm has several disadvan-
tages. Firstly, lexical databases often contain general
definitions and miss many domain specific senses
(Agirre et al, 2001). Secondly, they suffer from the
lack of explicit semantic and topical relations be-
tween concepts (Agirre et al, 2001). Thirdly, they
often do not reflect the exact content of the context
in which the target word appears (Veronis, 2004).
WSI aims to overcome these limitations of hand-
constructed lexicons.
Most WSI systems are based on the vector-space
model that represents each context of a target word
as a vector of features (e.g. frequency of cooccur-
ring words). Vectors are clustered and the resulting
clusters are taken to represent the induced senses.
Recently, graph-based methods have been employed
to WSI (Dorow and Widdows, 2003; Veronis, 2004;
Agirre and Soroa, 2007b).
Typically, graph-based approaches represent each
word co-occurring with the target word, within a
pre-specified window, as a vertex. Two vertices
are connected via an edge if they co-occur in one
or more contexts of the target word. This co-
occurrence graph is then clustered employing differ-
ent graph clustering algorithms to induce the senses.
Each cluster (induced sense) consists of words ex-
pected to be topically related to the particular sense.
As a result, graph-based approaches assume that
each context word is related to one and only one
sense of the target one.
Recently, Klapaftis and Manandhar (2008) argued
that this assumption might not be always valid, since
a context word may be related to more than one
senses of the target one. As a result, they pro-
36
posed the use of a graph-based model for WSI, in
which each vertex of the graph corresponds to a
collocation (word-pair) that co-occurs with the tar-
get word, while edges are drawn based on the co-
occurrence frequency of their associated colloca-
tions. Clustering of this collocational graph would
produce clusters, which consist of a set of collo-
cations. The intuition is that the produced clusters
will be less sense-conflating than those produced
by other graph-based approaches, since collocations
provide strong and consistent clues to the senses of
a target word (Yarowsky, 1995).
The collocational graph-based approach as well
as the majority of state-of-the-art WSI systems es-
timate their parameters either empirically or by em-
ploying supervised techniques. The SemEval-2007
WSI task (SWSI) participating systems UOY and
UBC-AS used labeled data for parameter estimation
(Agirre and Soroa, 2007a), while the authors of I2R,
UPV SI and UMND2 have empirically chosen val-
ues for their parameters. This issue imposes limits
on the unsupervised nature of these algorithms, as
well as on their performance on different datasets.
More specifically, when applying an unsupervised
WSI system on different datasets, one cannot be sure
that the same set of parameters is appropriate for all
datasets (Karakos et al, 2007). In most cases, a new
parameter tuning might be necessary. Unsupervised
estimation of free parameters may enhance the unsu-
pervised nature of systems, making them applicable
to any dataset, even if there are no tagged data avail-
able.
In this paper, we focus on estimating the free
parameters of the collocational graph-based WSI
method (Klapaftis and Manandhar, 2008) using
eight graph connectivity measures (GCM). Given a
parameter setting and the associated induced cluster-
ing solution, each induced cluster corresponds to a
subgraph of the original unclustered graph. A graph
connectivity measure GCMi scores each cluster by
evaluating the degree of connectivity of its corre-
sponding subgraph. Each clustering solution is then
assigned the average of the scores of its clusters. Fi-
nally, the highest scoring solution is selected.
Our evaluation on the nouns of SWSI shows
that GCM improve the worst performing parame-
ter setting by large margins in both SWSI evaluation
schemes, although they are below the best perform-
ing parameter setting. Moreover, the evaluation in
a WSD setting shows that all GCM estimate a set
of parameters which are above the Most Frequent
Sense (MFS) baseline by a statistically significant
amount. Finally our results show that two of the
measures, i.e. average degree and weighted average
degree, estimate a set of parameters that performs
closely to a set of parameters estimated in a super-
vised manner. All of these findings, suggest that
GCM are able to identify useful differences regard-
ing the quality of the induced clusters for different
parameter combinations, in effect being useful for
unsupervised parameter estimation.
2 Collocational graphs for WSI
Let bc, be the base corpus, which consists of para-
graphs containing the target word tw. The aim is
to induce the senses of tw given bc as the only in-
put. Let rc be a large reference corpus. In Klapaftis
and Manandhar (2008) the British National Corpus1
is used as a reference corpus. The WSI algorithm
consists of the following stages.
Corpus pre-processing The target of this stage is
to filter the paragraphs of the base corpus, in order to
keep the words which are topically (and possibly se-
mantically) related to the target one. Initially, tw is
removed from bc and both bc and rc are PoS-tagged.
In the next step, only nouns are kept in the para-
graphs of bc, since they are characterised by higher
discriminative ability than verbs, adverbs or adjec-
tives which may appear in a variety of different con-
texts. At the end of this pre-processing step, each
paragraph of bc and rc is a list of lemmatized nouns
(Klapaftis and Manandhar, 2008).
In the next step, the paragraphs of bc are fil-
tered by removing common nouns which are noisy;
contextually not related to tw. Given a contex-
tual word cw that occurs in the paragraphs of bc, a
log-likelihood ratio (G2) test is employed (Dunning,
1993), which checks if the distribution of cw in bc
is similar to the distribution of cw in rc; p(cw|bc) =
p(cw|rc) (null hypothesis). If this is true, G2 has a
small value. If this value is less than a pre-specified
threshold (parameter p1) the noun is removed from
bc.
1The British National Corpus (BNC) (2001, version 2). Dis-
tributed by Oxford University Computing Services.
37
Target: cnn nbc Target: nbc news
nbc tv nbc tv
cnn tv soap opera
cnn radio nbc show
news newscast news newscast
radio television nbc newshour
cnn headline cnn headline
nbc politics radio tv
breaking news breaking news
Table 1: Collocations connected to cnn nbc and nbc news
This process identifies nouns that are more indica-
tive in bc than in rc and vice versa. However, in this
setting we are not interested in nouns which have
a distinctive frequency in rc. As a result, each cw
which has a relative frequency in bc less than in rc
is filtered out. At the end of this stage, each para-
graph of bc is a list of nouns which are assumed to
be contextually related to the target word tw.
Creating the initial collocational graph The tar-
get of this stage is to determine the related nouns,
which will form the collocations, and the weight of
each collocation. Klapaftis and Manandhar (2008)
consider collocations of size 2, i.e. pairs of nouns.
For each paragraph of bc of size n, collocations
are identified by generating all the possible (cn2
)
combinations. The frequency of a collocation c is
the number of paragraphs in the whole SWSI corpus
(27132 paragraphs), in which c occurs.
Each collocation is assigned a weight, measuring
the relative frequency of two nouns co-occurring.
Let freqij denote the number of paragraphs in
which nouns i and j cooccur, and freqj denote the
number of paragraphs, where noun j occurs. The
conditional probability p(i|j) is defined in equation
1, and p(j|i) is computed in a similar way. The
weight of collocation cij is the average of these con-
ditional probabilities wcij = p(i|j) + p(j|i).
p(i|j) = freqijfreqj (1)
Finally, Klapaftis and Manandhar (2008) only ex-
tract collocations which have frequency (parame-
ter p2) and weight (parameter p3) higher than pre-
specified thresholds. This filtering appears to com-
pensate for inaccuracies in G2, as well as for low-
frequency distant collocations that are ambiguous.
Each weighted collocation is represented as a ver-
tex. Two vertices share an edge, if they co-occur in
one or more paragraphs of bc.
Populating and weighing the collocational graph
The constructed graph, G, is sparse, since the pre-
vious stage attempted to identify rare events, i.e.
co-occurring collocations. To address this problem,
Klapaftis and Manandhar (2008) apply a smooth-
ing technique, similar to the one in Cimiano et
al. (2005), extending the principle that a word is
characterised by the company it keeps (Firth, 1957)
to collocations. The target is to discover new edges
between vertices and to assign weights to all edges.
Each vertex i (collocation ci) is associated to
a vector V Ci containing its neighbouring vertices
(collocations). Table 1 shows an example of two
vertices, cnn nbc and nbc news, which are discon-
nected in G of the target word network. The example
was taken from Klapaftis and Manandhar (2008).
In the next step, the similarity between all vertex
vectors V Ci and V Cj is calculated using the Jaccard
coefficient, i.e. JC(V Ci, V Cj) = |V Ci?V Cj ||V Ci?V Cj | . Twocollocations ci and cj are mutually similar if ci is the
most similar collocation to cj and vice versa.
Given that collocations ci and cj are mutually
similar, an occurrence of a collocation ck with one
of ci, cj is also counted as an occurrence with the
other collocation. For example in Table 1, if cnn nbc
and nbc news are mutually similar, then the zero-
frequency event between nbc news and cnn tv is
set equal to the joint frequency between cnn nbc
and cnn tv. Marginal frequencies of collocations
are updated and the overall result is consequently a
smoothing of relative frequencies.
The weight applied to each edge connecting ver-
tices i and j (collocations ci and cj ) is the maximum
of their conditional probabilities: p(i|j) = freqijfreqj ,where freqi is the number of paragraphs collocation
ci occurs. p(j|i) is defined similarly.
Inducing senses and tagging In this final stage,
the collocational graph is clustered to produced the
senses (clusters) of the target word. The clustering
method employed is Chinese Whispers (CW) (Bie-
mann, 2006). CW is linear to the number of graph
edges, while it offers the advantage that it does not
require any input parameters, producing the clusters
of a graph automatically.
38
Figure 1: An example undirected weighted graph.
Initially, CW assigns all vertices to different
classes. Each vertex i is processed for a number of
iterations and inherits the strongest class in its lo-
cal neighbourhood (LN) in an update step. LN is
defined as the set of vertices which share an edge
with i. In each iteration for vertex i: each class, cl,
receives a score equal to the sum of the weights of
edges (i, j), where j has been assigned to class cl.
The maximum score determines the strongest class.
In case of multiple strongest classes, one is chosen
randomly. Classes are updated immediately, mean-
ing that a vertex can inherit from its LN classes that
were introduced in the same iteration.
Once CW has produced the clusters of a target
word, each of the instances of tw is tagged with
one of the induced clusters. This process is simi-
lar to Word Sense Disambiguation (WSD) with the
difference that the sense repository has been auto-
matically produced. Particularly, given an instance
of tw in paragraph pi: each induced cluster cl is as-
signed a score equal to the number of its collocations
(i.e. pairs of words) occurring in pi. We observe that
the tagging method exploits the one sense per collo-
cation property (Yarowsky, 1995), which means that
WSD based on collocations is probably finer than
WSD based on simple words, since ambiguity is re-
duced (Klapaftis and Manandhar, 2008).
3 Unsupervised parameter tuning
In this section we investigate unsupervised ways to
address the issue of choosing parameter values. To
this end, we employ a variety of GCM, which mea-
sure the relative importance of each vertex and as-
sess the overall connectivity of the corresponding
graph. These measures are average degree, cluster
coefficient, graph entropy and edge density (Navigli
and Lapata, 2007; Zesch and Gurevych, 2007).
GCM quantify the degree of connectivity of the
produced clusters (subgraphs), which represent the
senses (uses) of the target word for a given cluster-
ing solution (parameter setting). Higher values of
GCM indicate subgraphs (clusters) of higher con-
nectivity. Given a parameter setting, the induced
clustering solution and a graph connectivity measure
GCMi, each induced cluster is assigned the result-
ing score of applying GCMi on the corresponding
subgraph of the initial unclustered graph. Each clus-
tering solution is assigned the average of the scores
of its clusters (table 6), and the highest scoring one
is selected.
For each measure, we have developed two ver-
sions, i.e. one which considers the edge weights in
the subgraph, and a second which does not. In the
following description the terms graph and subgraph
are interchangeable.
Let G = (V,E) be an undirected graph (in-
duced sense), where V is a set of vertices and E =
{(u, v) : u, v ? V } a set of edges connecting vertex
pairs. Each edge is weighted by a positive weight,
W : wuv ? [0,?). Figure 1 shows a small example
to explain the computation of GCM. The graph con-
sists of 8 vertices, |V | = 8, and 10 edges, |E| = 10.
Edge weights appear on edges, e.g. wab = 14 .
Average Degree The degree (deg) of a vertex u is
the number of edges connected to u:
deg(u) = |{(u, v) ? E : v ? V }| (2)
The average degree (AvgDeg) of a graph can be
computed as:
AvgDeg(G(V,E)) = 1|V |
?
u?V
deg(u) (3)
The first row of table 2 shows the vertex degrees
of the example graph (figure 1) and AvgDeg(G) =
20
8 = 2.5.Edge weights can be integrated into the degree
computation. Let mew be the maximum edge
weight in the graph:
mew = max
(u,v)?E
wuv (4)
Average Weighted Degree The weighted de-
gree(w deg) of a vertex is defined as:
w deg(u) = 1|V |
?
(u,v)?E
wuv
mew (5)
39
a b c d e f g h
deg(u) 2 2 3 4 3 3 2 1
wdeg(u) 54 1 52 94 74 32 32 14
Tu 1 1 1 1 1 2 1 0
cc(u) 1 1 13 16 13 23 1 0
WTu 34 1 14 14 12 32 14 0
wcc(u) 34 1 112 124 16 12 14 0
p(u) 110 110 320 15 320 320 110 120
en(u) ? 100 33 33 41 46 41 41 33 22
wp(u) 116 120 18 980 780 340 340 180
we(u) ? 100 25 22 38 35 31 28 28 8
Table 2: Computations of graph connectivity measures
and relevant quantities on the example graph (figure 1).
Average weighted degree (AvgWDeg), similarly to
AvgDeg, is averaged over all vertices of the graph.
In the graph of figure 1, mew = 1. The second row
of table 2 shows the weighted degrees of all vertices.
AvgWDeg(G) = 4836 ' 1.33.
Average Cluster Coefficient The cluster coeffi-
cient (cc) of a vertex, u, is defined as:
cc(u) = Tu2?1ku(ku ? 1) (6)
Tu =
?
(u,v)?E
?
(v,x)?E
x 6=u
1 (7)
Tu is the number of edges between the ku neigh-
bours of u. Obviously ku = deg(u). 2?1ku(ku? 1)
would be the number of edges between the neigh-
bours of u if the graph they define was fully con-
nected. Average cluster coefficient (AvgCC) is aver-
aged over all vertices of the graph.
The computations of Tu and cc(u) on the example
graph are shown in the third and fourth rows of table
2. Consequently, AvgCC(G) = 916 = 0.5625.
Average Weighted Cluster Coefficient Let WTu
be the sum of edge weights between the neighbours
of u over mew. Weighted cluster coefficient (wcc)
can be computed as:
wcc(u) = WTu2?1ku(ku ? 1) (8)
WTu = 1mew
?
(u,v)?E
?
(v,x)?E
x 6=u
wvx (9)
Average weighted cluster coefficient (AvgWCC) is
averaged over all vertices of the graph. The com-
putations of WTu and wcc(u) on the example graph
(figure 1) are shown in the fifth and sixth rows of
table 2 and AvgWCC(G) = 678?24 ' 0.349.
Graph Entropy Entropy measures the amount of
information (alternatively the uncertainty) in a ran-
dom variable. For a graph, high entropy indicates
that many vertices are equally important and low en-
tropy that only few vertices are relevant (Navigli and
Lapata, 2007). The entropy (en) of a vertex u can be
defined as:
en(u) = ?p(u) log2 p(u) (10)
The probability of a vertex, p(u), is determined by
the degree distribution:
p(u) =
{deg(u)
2|E|
}
u?V
(11)
Graph entropy (GE) is computed by summing all
vertex entropies and normalising by log2 |V |. The
seventh and eighth row of table 2 show the compu-
tations of p(u) and en(u) on the example graph, re-
spectively. Thus, GE ' 0.97.
Weighted Graph Entropy Similarly to previous
graph connectivity measures, the weighted entropy
(wen) of a vertex u is defined as:
we(u) = ?wp(u) log2 wp(u) (12)
where: wp(u) =
{ w deg(u)
2 ?mew ? |E|
}
u?V
Weighted graph entropy (GE) is computed by sum-
ming all vertex weighted entropies and normalising
by log2 |V |. The last two rows of table 2 show the
computations of wp(u) and we(u) on the example
graph. Consequently, WGE ' 0.73.
Edge Density and Weighted Edge Density Edge
density (ed) quantifies how many edges the graph
has, as a ratio over the number of edges of a fully
connected graph of the same size:
A(V ) = 2
(|V |
2
)
(13)
40
Edge density (ed) is a global graph connectivity
measure; it refers to the whole graph and not a spe-
cific vertex. Edge density (ed) and weighted edge
density (wed) can be defined as follows:
ed(G(V,E)) = |E|A(V ) (14)
wed(G(V,E)) = 1A(V )
?
(u,v)?E
wu,v
mew (15)
In the graph of figure 1: A(V ) = 2(82
) = 28,
ed(G) = 1028 ' 0.357,
? wu,v
mew = 6 and wed(G) =6
28 ' 0.214.The use of the aforementioned GCM allows the
estimation of a different parameter setting for each
target word. Table 3 shows the parameters of the col-
locational graph-based WSI system (Klapaftis and
Manandhar, 2008). These parameters affect how the
collocational graph is constructed, and in effect the
quality of the induced clusters.
4 Evaluation
4.1 Experimental setting
The collocational WSI approach was evaluated un-
der the framework and corpus of SemEval-2007
WSI task (Agirre and Soroa, 2007a). The corpus
consists of text of the Wall Street Journal corpus,
and is hand-tagged with OntoNotes senses (Hovy et
al., 2006). The evaluation focuses on all 35 nouns of
SWSI. SWSI task employs two evaluation schemes.
In unsupervised evaluation, the results are treated as
clusters of contexts and gold standard (GS) senses
as classes. In a perfect clustering solution, each in-
duced cluster contains the same contexts as one of
the classes (Homogeneity), and each class contains
the same contexts as one of the clusters (Complete-
ness). F-Score is used to assess the overall quality of
clustering. Entropy and purity are also used, com-
plementarily. F-Score is a better measure than en-
tropy or purity, since F-Score measures both homo-
geneity and completeness, while entropy and purity
measure only the former. In the second scheme, su-
pervised evaluation, the training corpus is used to
map the induced clusters to GS senses. The testing
corpus is then used to measure WSD performance
(Table 4, Sup. Recall).
The graph-based collocational WSI method is re-
ferred as Col-Sm (where ?Col? stands for the ?col-
Parameter Range Value
G2 threshold 5, 10, 15 p1 = 5
Collocation frequency 4, 6, 8, 10 p2 = 8
Collocation weight 0.2, 0.3, 0.4 p3 = 0.2
Table 3: Parameters ranges and values in Klapaftis and
Manandhar (2008)
locational WSI? approach and ?Sm? for its ver-
sion using ?smoothing?). Col-Bl (where ?Bl? stands
for ?baseline?) refers to the same system without
smoothing. The parameters of Col-Sm were origi-
nally estimated by cross-validation on the training
set of SWSI. Out of 72 parameter combinations, the
setting with the highest F-Score was chosen and ap-
plied to all 35 nouns of the test set. This is referred
as Col-Sm-org (where ?org? stands for ?original?) in
Table 4. Table 3 shows all values for each parameter,
and the chosen values, under supervised parameter
estimation2. Col-Bl-org (Table 4) induces senses as
Col-Sm-org does, but without smoothing.
In table 4, Col-Sm-w (respectively Col-Bl-w)
refers to the evaluation of Col-Sm (Col-Bl), follow-
ing the same technique for parameter estimation as
in Klapaftis and Manandhar (2008) for each target
word separately (?w? stands for ?word?). Given that
GCM are applied for each target word separately,
these baselines will allow to see the performance of
GCM compared to a supervised setting.
The 1c1inst baseline assigns each instance to a
distinct cluster, while the 1c1w baseline groups all
instances of a target word into a single cluster. 1c1w
is equivalent to MFS in this setting. The fifth column
of table 4 shows the average number of clusters.
The SWSI participant systems UOY and UBC-AS
used labeled data for parameter estimation. The au-
thors of I2R, UPV SI and UMND2 have empirically
chosen values for their parameters.
The next subsection presents the evaluation of
GCM as well as the results of SWSI systems. Ini-
tially, we provide a brief discussion on the differ-
ences between the two evaluation schemes of SWSI
that will allow for a better understanding of GCM
performance.
4.2 Analysis of results and discussion
Evaluation of WSI methods is a difficult task. For
instance, 1c1inst (Table 4) achieves perfect purity
2CW performed 200 iterations for all experiments, because
it is not guaranteed to converge.
41
System Unsupervised Evaluation Sup.
FSc. Pur. Ent. # Cl. Recall
Col-Sm-org 78.0 88.6 31.0 5.9 86.4
Col-Bl-org 73.1 89.6 29.0 8.0 85.6
Col-Sm-w 80.9 88.0 32.5 4.3 85.5
Col-Bl-w 78.1 88.3 31.7 5.4 84.3
UBC-AS 80.8 83.6 43.5 1.6 80.7
UPV SI 69.9 87.4 30.9 7.2 82.5
I2R 68.0 88.4 29.7 3.1 86.8
UMND2 67.1 85.8 37.6 1.7 84.5
UOY 65.8 89.8 25.5 11.3 81.6
1c1w-MFS 80.7 82.4 46.3 1 80.9
1c1inst 6.6 100 0 73.1 N/A
Table 4: Evaluation of WSI systems and baselines.
and entropy. However, F-Score of 1c1inst is low,
because the GS senses are spread among clusters,
decreasing unsupervised recall. Supervised recall of
1c1inst is undefined, because each cluster tags only
one instance. Hence, clusters tagging instances in
the test corpus do not tag any instances in the train
corpus and the mapping cannot be performed. 1c1w
achieves high F-Score due to the dominance of MFS
in the testing corpus. However, its purity, entropy
and supervised recall are much lower than other sys-
tems, because it only induces the dominant sense.
Clustering solutions that achieve high supervised
recall do not necessarily achieve high F-Score,
mainly because F-Score penalises systems for in-
ducing more clusters than the corresponding GS
classes, as 1cl1inst does. Supervised evaluation
seems to be more neutral regarding the number of
clusters, since clusters are mapped into a weighted
vector of senses. Thus, inducing a number of clus-
ters similar to the number of senses is not a require-
ment for good results (Agirre and Soroa, 2007a).
High supervised recall means high purity and en-
tropy, as in I2R, but not vice versa, as in UOY. UOY
produces many clean clusters, however these are un-
reliably mapped to senses due to insufficient train-
ing data. On the contrary, I2R produces a few clean
clusters, which are mapped more reliably.
Comparing the performance of SWSI systems
shows that none performs well in both evaluation
settings, in effect being biased against one of the
schemes. However, this is not the case for the collo-
cational WSI method, which achieves a high perfor-
mance in both evaluation settings.
Table 6 presents the results of applying the graph
System Bound Unsupervised Evaluation Sup.
type FSc. Pur. Ent. # Cl. Recall
Col-Sm MaxR 79.3 90.5 26.6 7.0 88.6
Col-Sm MinR 62.9 89.0 26.7 12.7 78.8
Col-Bl MaxR 72.9 91.8 23.2 9.6 88.7
Col-Bl MinR 57.5 89.0 26.4 14.4 76.2
Col-Sm MaxF 83.2 90.0 28.7 4.9 86.6
Col-Sm MinF 43.6 90.2 22.1 17.6 83.7
Col-Bl MaxF 81.1 90.0 28.7 5.3 81.8
Col-Bl MinF 34.1 90.5 20.5 20.4 81.5
Table 5: Upper and lower performance bounds for sys-
tems Col-Sm and Col-Bl.
connectivity measures of section 3 in order to choose
the parameter values for the collocational WSI sys-
tem, for each word separately. The evaluation is
done both for Col-Sm and Col-Bl that use and ignore
smoothing, respectively.
To evaluate the supervised recall performance
using the graph connectivity measures, we com-
puted both the upper and lower bounds of Col-Sm,
i.e. the best and worst supervised recall, respectively
(MaxR and MinR in table 5). In the former case,
we selected the parameter combination per target
word that performs best (Col-Sm, MaxR in table 5),
which resulted in 88.6% supervised recall (F-Score:
79.3%), while in the latter we selected the worst per-
forming one, which resulted in 78.8% supervised re-
call (F-Score: 62.9%). In table 6 we observe that
the supervised recall of all measures is significantly
lower than the upper bound. However, all measures
perform significantly better than the lower bound
(McNemar?s test, confidence level: 95%); the small-
est difference is 4.9%, in the case of weighted edge
density. The picture is the same for Col-Bl.
In the same vein, we computed both the upper and
lower bounds of Col-Sm in terms of F-Score, 83.2%
and 43.6%, respectively (Col-Sm, MinF and MaxF
in table 5). The performance of the system is lower
than the upper bound, for all GCM. Despite that, we
observe that all measures except edge density and
weighted edge density outperform the lower bound
by large margins.
The comparison of GCM performance against
the lower and upper bounds of Col-Sm and Col-Bl
shows that GCM are able to identify useful differ-
ences regarding the degree of connectivity of in-
duced clusters, and in effect suggest parameter val-
ues that perform significantly better than the worst
42
Col-Sm Col-Bl
Unsupervised Evaluation Sup. Unsupervised Evaluation Sup.
Graph Connectivity Measure FSc Pur. Ent. # Cl. Recall FSc Pur. Ent. # Cl. Recall
Average Degree 79.2 87.2 34.2 3.9 84.8 77.5 31.3 88.4 5.7 83.8
Average Weighted Degree 77.1 87.8 32.0 5.5 84.2 75.1 28.3 89.6 8.5 83.3
Average Cluster Coefficient 72.5 88.8 28.5 9.1 83.9 68.7 24.0 90.9 12.9 83.9
Average Weighted Cluster Coefficient 65.8 88.4 28.0 9.6 84.1 68.9 22.4 91.3 13.9 83.7
Graph Entropy 67.0 89.6 25.9 12.3 83.8 68.5 22.1 91.8 14.4 84.4
Weighted Graph Entropy 72.7 89.4 28.1 9.6 84.1 72.2 23.5 91.2 12.5 84.0
Edge Density 47.8 91.8 19.4 18.4 84.8 42.0 16.9 92.8 21.9 84.1
Weighted Edge Density 53.4 90.2 23.1 15.5 83.7 42.2 17.1 92.7 21.9 83.9
Table 6: Unsupervised & supervised evaluation of the collocational WSI approach using graph connectivity measures.
case. However, they are all unable to approximate
the upper bound for both evaluation schemes, which
is also the case for the supervised estimation of pa-
rameters per target word (Col-Sm-w and Col-Bl-w).
In Table 6, we also observe that all measures
achieve higher supervised recall scores than the
MFS baseline. The increase is statistically signif-
icant (McNemar?s test, confidence level: 95%) in
all cases. This result shows that irrespective of the
number of clusters produced (low F-Score), GCM
are able to estimate a set of parameters that provides
clean clusters (low entropy), which when mapped to
GS senses improve upon the most frequent heuristic,
unlike the majority of unsupervised WSD systems.
Regarding the comparison between different
GCM, we observe that average degree and weighted
average degree for Col-Sm (Col-Bl) perform
closely to Col-Sm-w (Col-Bl-w) for both evaluation
schemes. This is due to the fact that they produce a
number of clusters similar to Col-Sm-w (Col-Bl-w),
while at the same time their distributions of clusters
over the target words? instances are also similar.
On the contrary, the remaining GCM tend to pro-
duce larger numbers of clusters compared to both
Col-Sm-w (Col-Bl-w) and the GS, in effect being
penalised by F-Score. As it has already been men-
tioned, supervised recall is less affected by a large
number of clusters, which causes small differences
among GCM.
Determining whether the weighted or unweighted
version of GCM performs better depends on the
GCM itself. Weighted graph entropy performs in all
cases better than the unweighted version. For aver-
age cluster coefficient and edge density, we cannot
extract a safe conclusion. Unweighted average de-
gree performs better than the weighted version.
5 Conclusion and future work
In this paper, we explored the use of eight graph con-
nectivity measures for unsupervised estimation of
free parameters of a collocational graph-based WSI
system. Given a parameter setting and the associ-
ated induced clustering solution, each cluster was
scored according to the connectivity degree of its
corresponding subgraph, as assessed by a particular
graph connectivity measure. Each clustering solu-
tion was then assigned the average of its clusters?
scores, and the highest scoring one was selected.
Evaluation on the nouns of SemEval-2007 WSI
task (SWSI) showed that all eight graph connectiv-
ity measures choose parameters for which the corre-
sponding performance of the system is significantly
higher than the lower performance bound, for both
the supervised and unsupervised evaluation scheme.
Moreover, the selected parameters produce results
which outperform the MFS baseline by a statisti-
cally significant amount in the supervised evalua-
tion scheme. The best performing measures, average
degree and weighted average degree, perform com-
parably well to the set of parameters chosen by a
supervised parameter estimation. In general, graph
connectivity measures can quantify significant dif-
ferences regarding the degree of connectivity of in-
duced clusters.
Future work focuses on further exploiting graph
connectivity measures. Graph theoretic literature
proposes a variety of measures capturing graph
properties. Some of these measures might help in
improving WSI performance, while at the same time
keeping graph-based WSI systems totally unsuper-
vised.
43
References
Eneko Agirre and Aitor Soroa. 2007a. Semeval-2007
task 02: Evaluating word sense induction and discrim-
ination systems. In Proceedings of the Fourth Interna-
tional Workshop on Semantic Evaluations (SemEval-
2007), pages 7?12, Prague, Czech Republic. Associa-
tion for Computational Linguistics.
Eneko Agirre and Aitor Soroa. 2007b. Ubc-as: A graph
based unsupervised system for induction and classi-
fication. In Proceedings of the Fourth International
Workshop on Semantic Evaluations (SemEval-2007),
pages 346?349, Prague, Czech Republic. Association
for Computational Linguistics.
Eneko Agirre, Olatz Ansa, Eduard Hovy, and David Mar-
tinez. 2001. Enriching wordnet concepts with topic
signatures, Sep.
Chris Biemann. 2006. Chinese whispers - an efficient
graph clustering algorithm and its application to nat-
ural language processing problems. In Proceedings
of TextGraphs: the Second Workshop on Graph Based
Methods for Natural Language Processing, pages 73?
80, New York City, June. Association for Computa-
tional Linguistics.
Philipp Cimiano, Andreas Hotho, and Steffen Staab.
2005. Learning concept hierarchies from text corpora
using formal concept analysis. Journal of Artificial In-
telligence research, 24:305?339.
Beate Dorow and Dominic Widdows. 2003. Discover-
ing corpusspecific word senses. In Proceedings 10th
conference of the European chapter of the ACL, pages
79?82, Budapest, Hungary.
Ted E. Dunning. 1993. Accurate methods for the statis-
tics of surprise and coincidence. Computational Lin-
guistics, 19(1):61?74.
John R. Firth. 1957. A synopsis of linguistic theory,
1930-1955. Studies in Linguistic Analysis, pages 1?
32.
Eduard Hovy, Mitchell Marcus, Martha Palmer, Lance
Ramshaw, and Ralph Weischedel. 2006. Ontonotes:
The 90% solution. In Proceedings of the Human Lan-
guage Technology Conference of the NAACL, Com-
panion Volume: Short Papers, pages 57?60, New York
City, USA. Association for Computational Linguistics.
Damianos Karakos, Jason Eisner, Sanjeev Khudanpur,
and Carey Priebe. 2007. Cross-instance tuning of un-
supervised document clustering algorithms. In Human
Language Technologies 2007: The Conference of the
North American Chapter of the Association for Com-
putational Linguistics; Proceedings of the Main Con-
ference, pages 252?259, Rochester, New York, April.
Association for Computational Linguistics.
Ioannis P. Klapaftis and Suresh Manandhar. 2008. Word
sense induction using graphs of collocations. In In
Proceedings of the 18th European Conference on Ar-
tificial Intelligence, (ECAI-2008), Patras, Greece.
R. Navigli and M. Lapata. 2007. Graph connectiv-
ity measures for unsupervised word sense disambigua-
tion. In 20th International Joint Conference on Artifi-
cial Intelligence (IJCAI 2007), pages 1683?1688, Hy-
derabad, India, January.
Patrick Pantel and Dekang Lin. 2002. Discovering
word senses from text. In KDD ?02: Proceedings
of the eighth ACM SIGKDD international conference
on Knowledge discovery and data mining, pages 613?
619, New York, NY, USA. ACM Press.
Jean Veronis. 2004. Hyperlex: lexical cartography for
information retrieval. Computer Speech & Language,
18(3):223?252, July.
David Yarowsky. 1995. Unsupervised word sense disam-
biguation rivaling supervised methods. In Meeting of
the Association for Computational Linguistics, pages
189?196.
Torsten Zesch and Iryna Gurevych. 2007. Analysis of
the wikipedia category graph for NLP applications. In
Proceedings of the Second Workshop on TextGraphs:
Graph-Based Algorithms for Natural Language Pro-
cessing, pages 1?8, Rochester, NY, USA. Association
for Computational Linguistics.
44
Proceedings of the NAACL HLT Workshop on Semantic Evaluations: Recent Achievements and Future Directions, pages 117?122,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
SemEval-2010 Task 14: Evaluation Setting for Word Sense Induction &
Disambiguation Systems
Suresh Manandhar
Department of Computer Science
University of York
York, UK, YO10 5DD
suresh@cs.york.ac.uk
Ioannis P. Klapaftis
Department of Computer Science
University of York
York, UK, YO10 5DD
giannis@cs.york.ac.uk
Abstract
This paper presents the evaluation setting
for the SemEval-2010 Word Sense Induction
(WSI) task. The setting of the SemEval-2007
WSI task consists of two evaluation schemes,
i.e. unsupervised evaluation and supervised
evaluation. The first one evaluates WSI meth-
ods in a similar fashion to Information Re-
trieval exercises using F-Score. However,
F-Score suffers from the matching problem
which does not allow: (1) the assessment of
the entire membership of clusters, and (2) the
evaluation of all clusters in a given solution. In
this paper, we present the use of V-measure as
a measure of objectively assessing WSI meth-
ods in an unsupervised setting, and we also
suggest a small modification on the supervised
evaluation.
1 Introduction
WSI is the task of identifying the different senses
(uses) of a target word in a given text. WSI is a field
of significant value, because it aims to overcome the
limitations originated by representing word senses
as a fixed-list of dictionary definitions. These lim-
itations of hand-crafted lexicons include the use of
general sense definitions, the lack of explicit seman-
tic and topical relations between concepts (Agirre et
al., 2001), and the inability to reflect the exact con-
tent of the context in which a target word appears
(Ve?ronis, 2004).
Given the significance of WSI, the objective as-
sessment and comparison of WSI methods is cru-
cial. The first effort to evaluate WSI methods un-
der a common framework (evaluation schemes &
dataset) was undertaken in the SemEval-2007 WSI
task (SWSI) (Agirre and Soroa, 2007), where two
separate evaluation schemes were employed. The
first one, unsupervised evaluation, treats the WSI re-
sults as clusters of target word contexts and Gold
Standard (GS) senses as classes. The traditional
clustering measure of F-Score (Zhao et al, 2005) is
used to assess the performance of WSI systems. The
second evaluation scheme, supervised evaluation,
uses the training part of the dataset in order to map
the automatically induced clusters to GS senses. In
the next step, the testing corpus is used to measure
the performance of systems in a Word Sense Disam-
biguation (WSD) setting.
A significant limitation of F-Score is that it does
not evaluate the make up of clusters beyond the
majority class (Rosenberg and Hirschberg, 2007).
Moreover, F-Score might also fail to evaluate clus-
ters which are not matched to any GS class due
to their small size. These two limitations define
the matching problem of F-Score (Rosenberg and
Hirschberg, 2007) which can lead to: (1) identical
scores between different clustering solutions, and
(2) inaccurate assessment of the clustering quality.
The supervised evaluation scheme employs a
method in order to map the automatically induced
clusters to GS senses. As a result, this process might
change the distribution of clusters by mapping more
than one clusters to the same GS sense. The out-
come of this process might be more helpful for sys-
tems that produce a large number of clusters.
In this paper, we focus on analysing the SemEval-
2007 WSI evaluation schemes showing their defi-
ciencies. Subsequently, we present the use of V-
117
measure (Rosenberg and Hirschberg, 2007) as an
evaluation measure that can overcome the current
limitations of F-Score. Finally, we also suggest
a small modification on the supervised evaluation
scheme, which will possibly allow for a more reli-
able estimation of WSD performance. The proposed
evaluation setting will be applied in the SemEval-
2010 WSI task.
2 SemEval-2007 WSI evaluation setting
The SemEval-2007 WSI task (Agirre and Soroa,
2007) evaluates WSI systems on 35 nouns and 65
verbs. The corpus consists of texts of the Wall Street
Journal corpus, and is hand-tagged with OntoNotes
senses (Hovy et al, 2006). For each target word tw,
the task consists of firstly identifying the senses of
tw (e.g. as clusters of target word instances, co-
occurring words, etc.), and secondly tagging the in-
stances of the target word using the automatically
induced clusters. In the next sections, we describe
and review the two evaluation schemes.
2.1 SWSI unsupervised evaluation
Let us assume that given a target word tw, a WSI
method has produced 3 clusters which have tagged
2100 instances of tw. Table 1 shows the number of
tagged instances for each cluster, as well as the com-
mon instances between each cluster and each gold
standard sense.
F-Score is used in a similar fashion to Information
Retrieval exercises. Given a particular gold standard
sense gsi of size ai and a cluster cj of size aj , sup-
pose aij instances in the class gsi belong to cj . Pre-
cision of class gsi with respect to cluster cj is de-
fined as the number of their common instances di-
vided by the total cluster size, i.e. P(gsi, cj) = aijaj .The recall of class gsi with respect to cluster cj is
defined as the number of their common instances di-
vided by the total sense size, i.e. R(gsi, cj) = aijai .The F-Score of gsi with respect to cj , F (gsi, cj), is
then defined as the harmonic mean of P (gsi, cj) and
R(gsi, cj).
The F-Score of class gsi, F (gsi), is the maximum
F (gsi, cj) value attained at any cluster. Finally, the
F-Score of the entire clustering solution is defined
as the weighted average of the F-Scores of each GS
sense (Formula 1), where q is the number of GS
senses and N is the total number of target word in-
gs1 gs2 gs3
cl1 500 100 100
cl2 100 500 100
cl3 100 100 500
Table 1: Clusters & GS senses matrix.
stances. If the clustering is identical to the original
classes in the datasets, F-Score will be equal to one.
In the example of Table 1, F-Score is equal to 0.714.
F ? Score =
q?
i=1
|gsi|
N F (gsi) (1)
As it can be observed, F-Score assesses the qual-
ity of a clustering solution by considering two dif-
ferent angles, i.e. homogeneity and completeness
(Rosenberg and Hirschberg, 2007). Homogeneity
refers to the degree that each cluster consists of
data points, which primarily belong to a single GS
class. On the other hand, completeness refers to the
degree that each GS class consists of data points,
which have primarily been assigned to a single clus-
ter. A perfect homogeneity would result in a preci-
sion equal to 1, while a perfect completeness would
result in a recall equal to 1.
Purity and entropy (Zhao et al, 2005) are also
used in SWSI as complementary measures. How-
ever, both of them evaluate only the homogeneity of
a clustering solution disregarding completeness.
2.2 SWSI supervised evaluation
In supervised evaluation, the target word corpus is
split into a testing and a training part. The training
part is used to map the automatically induced clus-
ters to GS senses. In the next step, the testing corpus
is used to evaluate WSI methods in a WSD setting.
Let us consider the example shown in Table 1 and
assume that this matrix has been created by using the
training part of our corpus. Table 1 shows that cl1 is
more likely to be associated with gs1, cl2 is more
likely to be associated with gs2, and cl3 is more
likely to be associated with gs3. This information
from the training part is utilised to map the clusters
to GS senses.
Particularly, the matrix shown in Table 1 is nor-
malised to produce a matrix M , in which each en-
try depicts the conditional probability P (gsi|clj).
Given an instance I of tw from the testing cor-
pus, a row cluster vector IC is created, in which
118
System F-Sc. Pur. Ent. # Cl. WSD
1c1w-MFS 78.9 79.8 45.4 1 78.7
UBC-AS 78.7 80.5 43.8 1.32 78.5
upv si 66.3 83.8 33.2 5.57 79.1
UMND2 66.1 81.7 40.5 1.36 80.6
I2R 63.9 84.0 32.8 3.08 81.6
UOY 56.1 86.1 27.1 9.28 77.7
1c1inst 9.5 100 0 139 N/A
Table 2: SWSI Unsupervised & supervised evaluation.
each entry k corresponds to the score assigned to
clk to be the winning cluster for instance I . The
product of IC and M provides a row sense vec-
tor, IG, in which the highest scoring entry a de-
notes that gsa is the winning sense for instance I .
For example, if we produce the row cluster vector
[cl1 = 0.8, cl2 = 0.1, cl3 = 0.1], and multiply
it with the normalised matrix of Table 1, then we
would get a row sense vector in which gs1 would be
the winning sense with a score equal to 0.6.
2.3 SWSI results & discussion
Table 2 shows the unsupervised and supervised per-
formance of systems participating in SWSI. As far
as the baselines is concerned, the 1c1w baseline
groups all instances of a target word into a single
cluster, while the 1c1inst creates a new cluster for
each instance of a target word. Note that the 1c1w
baseline is equivalent to the MFS in the supervised
evaluation. As it can be observed, a system with low
entropy (high purity) does not necessarily achieve
high F-Score. This is due to the fact that entropy
and purity only measure the homogeneity of a clus-
tering solution. For that reason, the 1c1inst baseline
achieves a perfect entropy and purity, although its
clustering solution is far from ideal.
On the contrary, F-Score has a significant advan-
tage over purity and entropy, since it measures both
homogeneity (precision) and completeness (recall)
of a clustering solution. However, F-Score suffers
from the matching problem, which manifests itself
either by not evaluating the entire membership of a
cluster, or by not evaluating every cluster (Rosen-
berg and Hirschberg, 2007). The former situation is
present, due to the fact that F-Score does not con-
sider the make-up of the clusters beyond the major-
ity class (Rosenberg and Hirschberg, 2007). For ex-
ample, in Table 3 the F-Score of the clustering so-
gs1 gs2 gs3
cl1 500 0 200
cl2 200 500 0
cl3 0 200 500
Table 3: Clusters & GS senses matrix.
lution is 0.714 and equal to the F-Score of the clus-
tering solution shown in Table 1, although these are
two significantly different clustering solutions. In
fact, the clustering shown in Table 3 should have
a better homogeneity than the clustering shown in
Table 1, since intuitively speaking each cluster con-
tains fewer classes. Moreover, the second clustering
should also have a better completeness, since each
GS class contains fewer clusters.
An additional instance of the matching problem
manifests itself, when F-Score fails to evaluate the
quality of smaller clusters. For example, if we add
in Table 3 one more cluster (cl4), which only tags
50 additional instances of gs1, then we will be able
to observe that this cluster will not be matched to
any of the GS senses, since cl1 is matched to gs1.
Although F-Score will decrease since the recall of
gs1 will decrease, the evaluation setting ignores the
perfect homogeneity of this small cluster.
In Table 2, we observe that no system managed to
outperform the 1c1w baseline in terms of F-Score.
At the same time, some systems participating in
SWSI were able to outperform the equivalent of the
1c1w baseline (MFS) in the supervised evaluation.
For example, UBC-AS achieved the best F-Score
close to the 1c1w baseline. However, by looking at
its supervised recall, we observe that it is below the
MFS baseline.
A clustering solution, which achieves high super-
vised recall, does not necessarily achieve high F-
Score. One reason for that stems from the fact that
F-Score penalises systems for getting the number of
GS classes wrongly, as in 1c1inst baseline. Accord-
ing to Agirre & Soroa (2007), supervised evaluation
seems to be more neutral regarding the number of
induced clusters, because clusters are mapped into a
weighted vector of senses, and therefore inducing a
number of clusters similar to the number of senses
is not a requirement for good results.
However, a large number of clusters might also
lead to an unreliable mapping of clusters to GS
senses. For example, high supervised recall also
119
means high purity and low entropy as in I2R, but not
vice versa as in UOY. UOY produces a large number
of clean clusters, in effect suffering from an unreli-
able mapping of clusters to senses due to the lack of
adequate training data.
Moreover, an additional supervised evaluation of
WSI methods using a different dataset split resulted
in a different ranking, in which all of the systems
outperformed the MFS baseline (Agirre and Soroa,
2007). This result indicates that the supervised eval-
uation might not provide a reliable estimation of
WSD performance, particularly in the case where
the mapping relies on a single dataset split.
3 SemEval-2010 WSI evaluation setting
3.1 Unsupervised evaluation using V-measure
Let us assume that the dataset of a target word tw
comprises of N instances (data points). These data
points are divided into two partitions, i.e. a set of au-
tomatically generated clusters C = {cj |j = 1 . . . n}
and a set of gold standard classes GS = {gsi|gs =
1 . . .m}. Moreover, let aij be the number of data
points, which are members of class gsi and elements
of cluster cj .
V-measure assesses the quality of a clustering so-
lution by explicitly measuring its homogeneity and
its completeness (Rosenberg and Hirschberg, 2007).
Recall that homogeneity refers to the degree that
each cluster consists of data points which primar-
ily belong to a single GS class. V-measure assesses
homogeneity by examining the conditional entropy
of the class distribution given the proposed cluster-
ing, i.e. H(GS|C). H(GS|C) quantifies the re-
maining entropy (uncertainty) of the class distribu-
tion given that the proposed clustering is known. As
a result, when H(GS|C) is 0, we have the perfectly
homogeneous solution, since each cluster contains
only those data points that are members of a single
class. However in an imperfect situation, H(GS|C)
depends on the size of the dataset and the distribu-
tion of class sizes. As a result, instead of taking the
raw conditional entropy, V-measure normalises it by
the maximum reduction in entropy the clustering in-
formation could provide, i.e. H(GS).
Formulas 2 and 3 define H(GS) and H(GS|C).
When there is only a single class (H(GS) = 0), any
clustering would produce a perfectly homogeneous
solution. In the worst case, the class distribution
within each cluster is equal to the overall class dis-
tribution (H(GS|C) = H(GS)), i.e. clustering pro-
vides no new information. Overall, in accordance
with the convention of 1 being desirable and 0 unde-
sirable, the homogeneity (h) of a clustering solution
is 1 if there is only a single class, and 1? H(GS|C)H(GS) in
any other case (Rosenberg and Hirschberg, 2007).
H(GS) = ?
|GS|?
i=1
?|C|
j=1 aij
N log
?|C|
j=1 aij
N (2)
H(GS|C) = ?
|C|?
j=1
|GS|?
i=1
aij
N log
aij?|GS|
k=1 akj
(3)
Symmetrically to homogeneity, completeness refers
to the degree that each GS class consists of data
points, which have primarily been assigned to a sin-
gle cluster. To evaluate completeness, V-measure
examines the distribution of cluster assignments
within each class. The conditional entropy of the
cluster given the class distribution, H(C|GS), quan-
tifies the remaining entropy (uncertainty) of the clus-
ter given that the class distribution is known.
Consequently, when H(C|GS) is 0, we have the
perfectly complete solution, since all the data points
of a class belong to the same cluster. Therefore,
symmetrically to homogeneity, the completeness c
of a clustering solution is 1 if there is only a sin-
gle cluster (H(C) = 0), and 1 ? H(C|GS)H(C) in any
other case. In the worst case, completeness will be
equal to 0, particularly when H(C|GS) is maxi-
mal and equal to H(C). This happens when each
GS class is included in all clusters with a distribu-
tion equal to the distribution of sizes (Rosenberg and
Hirschberg, 2007). Formulas 4 and 5 define H(C)
and H(C|GS). Finally h and c can be combined and
produce V-measure, which is the harmonic mean of
homogeneity and completeness.
H(C) = ?
|C|?
j=1
?|GS|
i=1 aij
N log
?|GS|
i=1 aij
N (4)
H(C|GS) = ?
|GS|?
i=1
|C|?
j=1
aij
N log
aij?|C|
k=1 aik
(5)
Returning to our clustering example in Table 1, its
V-measure is equal to 0.275. In section 2.3, we
also presented an additional clustering (Table 3),
which had the same F-Score as the clustering in Ta-
ble 1, despite the fact that it intuitively had a bet-
ter completeness and homogeneity. The V-measure
120
of the second clustering solution is equal to 0.45,
and higher than the V-measure of the first cluster-
ing. This result shows that V-measure is able to
discriminate between these two clusterings by con-
sidering the make-up of the clusters beyond the ma-
jority class. Furthermore, it is straightforward from
the description in this section, that V-measure evalu-
ates each cluster in terms of homogeneity and com-
pleteness, unlike F-Score which relies on a post-hoc
matching.
3.2 V-measure results & discussion
Table 4 shows the performance of SWSI partici-
pating systems according to V-measure. The last
four columns of Table 4 show the weighted aver-
age homogeneity and completeness for nouns and
verbs. Note that the homogeneity and complete-
ness columns are weighted averages over all nouns
or verbs, and are not used for the calculation of
the weighted average V-measure (second column).
The latter is calculated by measuring for each tar-
get word?s clustering solution the harmonic mean of
homogeneity and completeness separately, and then
producing the weighted average.
As it can be observed in Table 4, all WSI sys-
tems have outperformed the random baseline which
means that they have learned useful information.
Moreover, Table 4 shows that on average all sys-
tems have outperformed the 1c1w baseline, which
groups the instances of a target word to a single clus-
ter. The completeness of the 1c1w baseline is equal
to 1 by definition, since all instances of GS classes
are grouped to a single cluster. However, this solu-
tion is as inhomogeneous as possible and causes a
homogeneity equal to 0 in the case of nouns. In the
verb dataset however, some verbs appear with only
one sense, in effect causing the 1c1w homogeneity
to be equal to 1 in some cases, and the average V-
measure greater than 0.
In Table 4, we also observe that the 1c1inst base-
line achieves a high performance. In nouns only I2R
is able to outperform this baseline, while in verbs the
1c1inst baseline achieves the highest result. By the
definition of homogeneity (section 3.1), this baseline
is perfectly homogeneous, since each cluster con-
tains one instance of a single sense. However, its
completeness is not 0, as one might intuitively ex-
pect. This is due to the fact that V-measure consid-
ers as the worst solution in terms of completeness
the one, in which each class is represented by ev-
ery cluster, and specifically with a distribution equal
to the distribution of cluster sizes (Rosenberg and
Hirschberg, 2007). This worst solution is not equiv-
alent to the 1c1inst, hence completeness of 1c1inst
is greater than 0. Additionally, completeness of this
baseline benefits from the fact that around 18% of
GS senses have only one instance in the test set.
Note however, that on average this baseline achieves
a lower completeness than most of the systems.
Another observation from Table 4 is that upv si
and UOY have a better ranking than in Table 2. Note
that these systems have generated a higher number
of clusters than the GS number of senses. In verbs
UOY has been extensively penalised by the F-Score.
The inspection of their answers shows that both sys-
tems generate highly skewed distributions, in which
a small number of clusters tag the majority of in-
stances, while a larger number tag only a few. As
mentioned in sections 2.1 and 2.3, these small clus-
ters might not be matched to any GS sense, hence
they will decrease the unsupervised recall of a GS
class, and consequently the F-Score. However, their
high homogeneity is not considered in the calcula-
tion of F-Score. On the contrary, V-measure is able
to evaluate the quality of these small clusters, and
provide a more objective assessment.
Finally, in our evaluation we observe that I2R
has on average the highest performance among the
SWSI methods. This is due to its high V-measure in
nouns, but not in verbs. Particularly in nouns, I2R
achieves a consistent performance in terms of ho-
mogeneity and completeness without being biased
towards one of them, as is the case for the rest of
the systems. For example, UOY and upv si achieve
on average the highest homogeneity (42.5 & 32.8
resp.) and the worst completeness (11.5 & 13.2
resp.). The opposite picture is present for UBC-AS
and UMND2. Despite that, UBC-AS and UMND2
perform better than I2R in verbs, due to the small
number of generated clusters (high completeness),
and a reasonable homogeneity mainly due to the ex-
istence of verbs with one GS sense.
3.3 Modified supervised WSI evaluation
In section 2.3, we mentioned that supervised eval-
uation might favor methods which produce many
121
System V-measure Homogeneity Completeness
Total Nouns Verbs Nouns Verbs Nouns Verbs
1c1inst 21.6 19.2 24.3 100.0 100.0 11.3 15.8
I2R 16.5 22.3 10.1 31.6 27.3 20.0 10.0
UOY 15.6 17.2 13.9 38.9 46.6 12.0 11.1
upv si 15.3 18.2 11.9 37.1 28.0 14.5 11.8
UMND2 12.1 12.0 12.2 18.1 15.3 55.8 63.6
UBC-AS 7.8 3.7 12.4 4.0 13.7 90.6 93.0
Rand 7.2 4.9 9.7 12.0 30.0 14.1 14.3
1c1w 6.3 0.0 13.4 0.0 13.4 100.0 100.0
Table 4: V-Measure, homogeneity and completeness of SemEval-2007 WSI systems. The range of V-measure, homo-
geneity & completeness is 0-100.
clusters, since the mapping step can artificially in-
crease completeness. Furthermore, we have shown
that generating a large number of clusters might lead
to an unreliable mapping of clusters to GS senses
due to the lack of adequate training data.
Despite that, the supervised evaluation can be
considered as an application-oriented evaluation,
since it allows the transformation of unsupervised
WSI systems to semi-supervised WSD ones. Given
the great difficulty of unsupervised WSD systems to
outperform the MFS baseline as well as the SWSI
results, which show that some systems outperform
the MFS by a significant amount in nouns, we be-
lieve that this evaluation scheme should be used to
compare against supervised WSD methods.
In section 2.3, we also mentioned that the super-
vised evaluation on two different test/train splits pro-
vided a different ranking of methods, and more im-
portantly a different ranking with regard to the MFS.
To deal with that problem, we believe that it would
be reasonable to perform k-fold cross validation in
order to collect statistically significant information.
4 Conclusion
We presented and discussed the limitations of the
SemEval-2007 evaluation setting for WSI methods.
Based on our discussion, we described the use of
V-measure as the measure of assessing WSI perfor-
mance on an unsupervised setting, and presented the
results of SWSI WSI methods. We have also sug-
gested a small modification on the supervised eval-
uation scheme, which will allow for a more reliable
estimation of WSD performance. The new evalu-
ation setting will be applied in the SemEval-2010
WSI task.
Acknowledgements
This work is supported by the European Commis-
sion via the EU FP7 INDECT project, Grant No.
218086, Research area: SEC-2007-1.2-01 Intelli-
gent Urban Environment Observation System. The
authors would like to thank the anonymous review-
ers for their useful comments.
References
Eneko Agirre and Aitor Soroa. 2007. Semeval-2007
task 02: Evaluating word sense induction and discrim-
ination systems. In Proceedings of the 4rth Interna-
tional Workshop on Semantic Evaluations, pages 7?12,
Prague, Czech Republic, June. ACL.
Eneko Agirre, Olatz Ansa, David Martinez, and Eduard
Hovy. 2001. Enriching wordnet concepts with topic
signatures. In Proceedings of the NAACL workshop on
WordNet and Other Lexical Resources: Applications,
Extensions and Customizations. ACL.
Eduard Hovy, Mitchell Marcus, Martha Palmer, Lance
Ramshaw, and Ralph Weischedel. 2006. Ontonotes:
The 90% solution. In Proceedings of the Human
Language Technology / North American Association
for Computational Linguistics conference, New York,
USA.
Andrew Rosenberg and Julia Hirschberg. 2007. V-
measure: A conditional entropy-based external cluster
evaluation measure. In Proceedings of the 2007 Joint
Conference on Empirical Methods in Natural Lan-
guage Processing and Computational Natural Lan-
guage Learning (EMNLP-CoNLL), pages 410?420.
Jean Ve?ronis. 2004. Hyperlex: lexical cartography for
information retrieval. Computer Speech & Language,
18(3):223?252.
Ying Zhao, George Karypis, and Usam Fayyad.
2005. Hierarchical clustering algorithms for docu-
ment datasets. Data Mining and Knowledge Discov-
ery, 10(2):141?168.
122
Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 745?755,
MIT, Massachusetts, USA, 9-11 October 2010. c?2010 Association for Computational Linguistics
Word Sense Induction & Disambiguation Using
Hierarchical Random Graphs
Ioannis P. Klapaftis
Department of Computer Science
University of York
United Kingdom
giannis@cs.york.ac.uk
Suresh Manandhar
Department of Computer Science
University of York
United Kingdom
suresh@cs.york.ac.uk
Abstract
Graph-based methods have gained attention in
many areas of Natural Language Processing
(NLP) including Word Sense Disambiguation
(WSD), text summarization, keyword extrac-
tion and others. Most of the work in these ar-
eas formulate their problem in a graph-based
setting and apply unsupervised graph cluster-
ing to obtain a set of clusters. Recent studies
suggest that graphs often exhibit a hierarchi-
cal structure that goes beyond simple flat clus-
tering. This paper presents an unsupervised
method for inferring the hierarchical group-
ing of the senses of a polysemous word. The
inferred hierarchical structures are applied to
the problem of word sense disambiguation,
where we show that our method performs sig-
nificantly better than traditional graph-based
methods and agglomerative clustering yield-
ing improvements over state-of-the-art WSD
systems based on sense induction.
1 Introduction
A number of NLP problems can be cast into a graph-
based framework, in which entities are represented
as vertices in a graph and relations between them are
depicted by weighted or unweighted edges. For in-
stance, in unsupervised WSD a number of methods
(Widdows and Dorow, 2002; Ve?ronis, 2004; Agirre
et al, 2006) have constructed word co-occurrence
graphs for a target polysemous word and applied
graph-clustering to obtain the clusters (senses) of
that word.
Similarly in text summarization, Mihalcea (2004)
developed a method, in which sentences are rep-
resented as vertices in a graph and edges between
them are drawn according to their common tokens
or words of a given POS category, e.g. nouns.
Graph-based ranking algorithms, such as PageRank
(Brin and Page, 1998), were then applied in order
to determine the significance of sentences. In the
same vein, graph-based methods have been applied
to other problems such as determining semantic sim-
ilarity of text (Ramage et al, 2009).
Recent studies (Clauset et al, 2006; Clauset et
al., 2008) suggest that graphs exhibit a hierarchi-
cal structure (e.g. a binary tree), in which vertices
are divided into groups that are further subdivided
into groups of groups, and so on, until we reach the
leaves. This hierarchical structure provides addi-
tional information as opposed to flat clustering by
explicitly including organisation at all scales of a
graph (Clauset et al, 2008). In this paper, we present
an unsupervised method for inferring the hierarchi-
cal structure (binary tree) of a graph, in which ver-
tices are the contexts of a polysemous word and
edges represent the similarity between contexts. The
method that we use to infer that hierarchical struc-
ture is the Hierarchical Random Graphs (HRGs) al-
gorithm due to Clauset et al (2008).
The binary tree produced by our method groups
the contexts of a polysemous word at different
heights of the tree. Thus, it induces the senses of
that word at different levels of sense granularity. To
evaluate our method, we apply it to the problem of
noun sense disambiguation showing that inferring
the hierarchical structure using HRGs provides ad-
ditional information from the observed graph lead-
ing to improved WSD performance compared to: (1)
745
Figure 1: Stages of the proposed method.
simple flat clustering, and (2) traditional agglomera-
tive clustering. Finally, we compare our results with
state-of-the-art sense induction systems and show
that our method yields improvements. Figure 1
shows the different stages of the proposed method
that we describe in the following sections.
2 Related work
Typically, graph-based methods, when applied to
unsupervised sense disambiguation represent each
word wi co-occurring with the target word tw as a
vertex. Two vertices are connected via an edge if
they co-occur in one or more contexts of tw. Once
the co-occurrence graph of tw has been constructed,
different graph clustering algorithms are applied to
induce the senses. Each cluster (induced sense) con-
sists of a set of words that are semantically related to
the particular sense. Figure 2 shows an example of
a graph for the target word paper that appears with
two different senses scholarly article and newspa-
per.
Ve?ronis (2004) has shown that co-occurrence
graphs are small-world networks that contain highly
dense subgraphs representing the different clusters
(senses) of the target word (Ve?ronis, 2004). To iden-
tify these dense regions Ve?ronis?s algorithm itera-
tively finds their hubs, where a hub is a vertex with a
very high degree. The degree of a vertex is defined to
be the number of edges incident to that vertex. The
identified hub is then deleted along with its direct
neighbours from the graph producing a new cluster.
For example, in Figure 2 the highest degree ver-
tex, news, is the first hub, which would be deleted
along with its direct neighbours. The deleted re-
gion corresponds to the newspaper sense of the tar-
get word paper. Ve?ronis (2004) further processed
the identified clusters (senses), in order to assign the
rest of graph vertices to the identified clusters by
utilising the minimum spanning tree of the original
graph.
In Agirre et al (2006), the algorithm of Ve?ronis
(2004) is analysed and assessed on the SensEval-3
dataset (Snyder and Palmer, 2004), after optimis-
ing its parameters on the SensEval-2 dataset (Ed-
monds and Dorow, 2001). The results show that the
WSD F-Score outperforms the Most Frequent Sense
(MFS) baseline by approximately 10%, while induc-
ing a large number of clusters (with averages of 60
to 70).
Another graph-based method is presented in
(Dorow and Widdows, 2003). They extract only
noun neighbours that appear in conjunctions or dis-
junctions with the target word. Additionally, they
extract second-order co-occurrences. Nouns are rep-
resented as vertices, while edges between vertices
are drawn, if their associated nouns co-occur in con-
junctions or disjunctions more than a given num-
ber of times. This co-occurrence frequency is also
used to weight the edges. The resulting graph is
then pruned by removing the target word and ver-
tices with a low degree. Finally, the MCL algorithm
(Dongen, 2000) is used to cluster the graph and pro-
duce a set of clusters (senses) each one consisting of
a set of contextually related words.
Chinese Whispers (CW) (Biemann, 2006) is a
parameter-free1 graph clustering method that has
been applied in sense induction to cluster the co-
occurrence graph of a target word (Biemann, 2006),
as well as a graph of collocations related to the tar-
get word (Klapaftis and Manandhar, 2008). The
evaluation of the collocational-graph method in the
SemEval-2007 sense induction task (Agirre and
Soroa, 2007) showed promising results.
All the described methods for sense induction ap-
1One needs to specify only the number of iterations. The
number of clusters is generated automatically.
746
Figure 2: Graph of words for the target word paper.
Numbers inside vertices correspond to their degree.
Figure 3: Running example of graph creation
ply flat graph clustering methods to derive the clus-
ters (senses) of a target word. As a result, they ne-
glect the fact that their constructed graphs often ex-
hibit a hierarchical structure that is useful in several
tasks including word sense disambiguation.
3 Building a graph of contexts
This section describes the process of creating a
graph of contexts for a polysemous target word. Fig-
ure 3 provides a running example of the different
stages of our method. In the example, the target
word paper appears with the scholarly article sense
in the contexts A, B, and with the newspaper sense
in the contexts C and D.
3.1 Corpus preprocessing
Let bc denote the base corpus consisting of the con-
texts containing the target word tw. In our work,
a context is defined as a paragraph2 containing the
target word.
The aim of this stage is to capture nouns contex-
tually related to tw. Initially, the target word is re-
moved from bc and part-of-speech tagging is applied
to each context. Following the work in (Ve?ronis,
2004; Agirre et al, 2006) only nouns are kept and
lemmatised. In the next step, the distribution of each
noun in the base corpus is compared to the distri-
bution of the same noun in a reference corpus3 us-
ing the log-likelihood ratio (G2) (Dunning, 1993).
Nouns with a G2 below a pre-specified threshold
(parameter p1) are removed from each paragraph of
the base corpus. The upper left part of Figure 3
shows the words kept as a result of this stage.
3.2 Graph creation
Graph vertices: To create the graph of vertices, we
represent each context ci as a vertex in a graph G.
Graph edges: Edges between the vertices of the
graph are drawn based on their similarity, defined
in Equation 1, where simcl(ci, cj) is the colloca-
tional weight of contexts ci, cj and simwd(ci, cj)
is their bag-of-words weight. If the edge weight
W (ci, cj) is above a prespecified threshold (param-
eter p3), then an edge is drawn between the corre-
sponding vertices in the graph.
W (ci, cj) =
1
2
(simcl(ci, cj) + simwd(ci, cj)) (1)
Collocational weight: The limited polysemy of col-
locations can be exploited to compute the similarity
between contexts ci and cj . In our setting, a colloca-
tion is a juxtaposition of two nouns within the same
context. Thus, given a context ci, each of its nouns
is combined with any other noun yielding a total of
(N
2
)
collocations for a context with N nouns. Each
collocation, clij is weighted using the log-likelihood
ratio (G2) (Dunning, 1993) and is filtered out if the
G2 is below a prespecified threshold (parameter p2).
At the end of this process, each context ci of tw is
associated with a vector of collocations (vi). The
upper right part of Figure 3 shows the collocations
associated with each context of our example.
2Our definition of context is equivalent to an instance of the
target word in the SemEval-2007 sense induction task dataset
(Agirre and Soroa, 2007).
3The British National Corpus, 2001, Distributed by Oxford
University Computing Services.
747
Given two contexts ci and cj , we calculate their
collocational weight using the Jaccard coefficient
on the collocational vectors, i.e. simcl(ci, cj) =
|vi?vj |
|vi?vj |
. The selection of Jaccard is based on the work
of Weeds et al (2004), who analyzed the variation
in a word?s distributionally nearest neighbours with
respect to a variety of similarity measures. Their
analysis showed that there are three classes of mea-
sures, i.e. those selecting distributionally more gen-
eral neighbours (e.g. cosine), those selecting distri-
butionally less general neighbours (e.g. AMCRM-
Precision (Weeds et al, 2004)) and those without a
bias towards the distributional generality of a neigh-
bour (e.g. Jaccard). In our setting, we are interested
in calculating the similarity between two contexts
without any bias. We selected Jaccard, since the rest
of that class?s measures are based on pointwise mu-
tual information that assigns high weights to infre-
quent events.
Bag-of-words weight: Estimating context similar-
ity using collocations may provide reliable estimates
regarding the existence of an edge in the graph, how-
ever, it also suffers from data sparsity. For this rea-
son, we also employ a bag-of-words model. Specif-
ically, each context ci is associated with a vector gi
that contains the nouns kept as result of the corpus
preprocessing stage. The upper left part of Figure
3 shows the words associated with each context of
our example. Given two contexts ci and cj , we cal-
culate their bag-of-words weight using the Jaccard
coefficient on the word vectors, i.e. simwd(ci, cj) =
|gi?gj |
|gi?gj |
.
The collocational weight and bag-of-words
weight are averaged to derive the edge weight be-
tween two contexts as defined in Equation 1. The
resulting graph of our running example is shown on
the bottom of Figure 3. This graph is the input to the
hierarchical random graphs method (Clauset et al,
2008) described in the next section.
4 Hierarchical Random Graphs for sense
induction
In this section, we describe the process of inferring
the hierarchical structure of the graph of contexts
using hierarchical random graphs (Clauset et al,
2008).
Figure 4: Two dendrograms for the graph in Figure 3.
4.1 The Hierarchical Random Graph model
A dendrogram is a binary tree with n leaves and
n ? 1 parents. Figure 4 shows an example of two
dendrograms with 4 leaves and 3 parents. Given a
set of n contexts that we need to arrange hierarchi-
cally, let us denote by G = (V,E) the graph of con-
texts, where V = {v0, v1 . . . vn} is the set of ver-
tices, E = {e0, e1 . . . em} is the set of edges and
ek = {vi, vj}.
Given an undirected graph G, each of its n ver-
tices is a leaf in a dendrogram, while the internal
nodes of that dendrogram indicate the hierarchical
relationships among the leaves. We denote this or-
ganisation byD = {D1, D2, . . . Dn?1}, where each
Dk is an internal node. Every pair of nodes (vi, vj)
is associated with a unique Dk, which is their low-
est common ancestor in the tree. In this manner D
partitions the edges that exist in G.
The primary assumption in the hierarchical ran-
dom graph model is that edges in G exist indepen-
dently, but with a probability that is not identically
distributed. In particular, the probability that an edge
{vi, vj} exists in G is given by a parameter ?k asso-
ciated with Dk, the lowest common ancestor of vi
and vj in D. In this manner, the topological struc-
ture D and the vector of probabilities ~? define the
HRG given by H(D, ~?) (Clauset et al, 2008).
748
4.2 HRG parameterisation
Assuming a uniform prior over all HRGs, the target
is to identify the parameters of D and ~?, so that the
chosen HRG is statistically similar to G. Let Dk be
an internal node of dendrogram D and f(Dk) be the
number of edges between the vertices of the subtrees
of the subtree rooted at Dk that actually exist in G.
For example, in Figure 4(A), f(D2) = 1, because
there is one edge in G connecting vertices B and C.
Let l(Dk) be the number of leaves in the left subtree
of Dk, and r(Dk) be the number of leaves in the
right subtree. For example in Figure 4(A), l(D2) =
2 and r(D2) = 2. The likelihood of the hierarchical
random graph (D, ~?) is defined in Equation 2, where
A(Dk) = l(Dk)r(Dk)? f(Dk).
L(D, ~?) =
?
Dk?D
?f(Dk)k (1? ?k)
A(Dk) (2)
The probabilities ?k that maximise the likelihood
of a dendrogram D can be easily estimated using
the method of MLE i.e ?k =
f(Dk)
l(Dk)r(Dk)
. Substi-
tuting this into Equation 2 yields Equation 3. For
numerical reasons, it is more convenient to work
with the logarithm of the likelihood which is defined
in Equation 4, where h(?k) = ??k log ?k ? (1 ?
?k) log (1? ?k).
L(D) =
?
Dk?D
[?
?k
k (1? ?k)
1??k ]l(Dk)r(Dk) (3)
logL(D) = ?
?
Dk?D
h(?k)l(Dk)r(Dk) (4)
As can be observed, each term ?l(Dk)r(Dk)h(?k)
is maximised when ?k approaches 0 or 1. This
means that high-likelihood dendrograms partition
vertices into subtrees, such that the connections
among their vertices in the observed graph are either
very rare or very common (Clauset et al, 2008). For
example, consider the two dendrograms in Figures
4(A) and 4(B). We observe that 4(A) is more likely
than 4(B), since it provides a better division of the
network leaves. Particularly, the likelihood of 4(A)
is L(D1) = (11 ? (1? 1)1) ? (11 ? (1? 1)1) ? (0.251 ?
(1 ? 0.25)3) = 0.105, while the likelihood of 4(B)
is L(D2) = (00 ? (1? 0)1) ? (11 ? (1? 1)1) ? (0.52 ?
(1? 0.5)2) = 0.062.
4.2.1 MCMC sampling
Finding the values of ?k using the MLE method
is straightforward. However, this is not the case
for maximising the likelihood function over the
space of all possible dendrograms. Given a graph
with n vertices, i.e. n leaves in each dendrogram,
the total number of different dendrograms is super-
exponential ((2n? 3)!! ?
?
2(2n)n?1e?n) (Clauset
et al, 2006).
To deal with this problem, we use a Markov
Chain Monte Carlo (MCMC) method that samples
dendrograms from the space of dendrogram mod-
els with probability proportional to their likelihood.
Each time MCMC samples a dendrogram with a
new highest likelihood, that dendrogram is stored.
Hence, our goal is to choose the highest likelihood
dendrogram once MCMC has converged.
Following the work in (Clauset et al, 2008),
we pick a set of transitions between dendrograms,
where a transition is a re-arrangement of the sub-
trees of a dendrogram. In particular, given a current
dendrogram Dcurr, each internal node Dk of Dcurr
is associated with three subtrees of Dcurr. For in-
stance, in Figure 5A, the subtrees st1 and st2 are
derived from the two children of Dk and the third
st3 from its sibling. Given a current dendrogram,
Dcurr, the algorithm proceeds as follows:
1. Choose an internal node, Dk ? Dcurr uni-
formly.
2. Generate two possible new configurations of
the subtrees of Dk (See Figure 5).
3. Choose one of the configurations uniformly to
generate a new dendrogram, Dnext.
4. Accept or reject Dnext according to
Metropolis-Hastings (MH) rule.
5. If transition is accepted, then Dcurr = Dnext.
6. GOTO 1.
According to MH rule (Newman and Barkema,
1999), a transition is accepted if logL(Dnext) ?
logL(Dcurr); otherwise the transition is accepted
with probability L(Dnext)L(Dcurr) . These transitions define
an ergodic Markov chain, hence its stationary distri-
bution can be reached (Clauset et al, 2008).
749
Figure 5: (A) current configuration for internal node Dk and its associated subtrees (B) first alternative configuration,
(C) second alternative configuration. Note that swapping st1, st2 in (A) results in an equivalent tree. Hence, this
configuration is excluded.
In our experiments, we noticed that the algorithm
converged relatively quickly. The same behaviour
(roughly O(n2) steps) was also noticed in Clauset et
al. (2008), when considering graphs with thousands
of vertices.
5 HRGs for sense disambiguation
5.1 Sense mapping
The output of HRG learning is a dendrogramD with
n leaves (contexts) and n?1 internal nodes. To per-
form sense disambiguation, we mapped the internal
nodes to gold standard senses using a sense-tagged
corpus. Such a sense-tagged corpus is needed when
induced word senses need to be mapped to a gold
standard sense inventory.
Instead of using a hard mapping from the den-
drogram internal nodes to the Gold Standard (GS)
senses, we use a soft probabilistic mapping and cal-
culateP (sk|Di), i.e the probability of sense sk given
node Di. Let F (Di) be the set of training contexts
grouped by internal node Di. Let F ?(sk) be the set
of training contexts that are tagged with sense sk.
Then the conditional probability, P (sk|Di), is de-
fined in Equation 5.
P (sk|Di) =
|F (Di) ? F ?(sk)|
|F (Di)|
(5)
Table 1 provides a sense-tagged corpus for the
running example of Figure 3. Using this corpus
and the tree in Figure 4(A), P (s1|D2) = 23 and
P (s2|D2) = 13 . In Figure 4(A) the rest of the calcu-
lated conditional probabilities are given.
5.2 Sense tagging
For evaluation we compared the proposed method
against the current state-of-the-art sense induction
GS sense Context ID Context words
s1 A journal, scholar, observation
science, paper
s1 B scholar, scholar, author,
publication, paper
s2 D times, guardian,
journalist, paper
Table 1: Sense-tagged corpus for the example in Figure 3
systems in the WSD task. We followed the setting
of SemEval-2007 sense induction task (Agirre and
Soroa, 2007). In this setting, the base corpus (bc)
(Section 3.1) for a target word consists both of the
training and testing corpus. As a result, a testing
context cj of tw is a leaf in the generated dendro-
gram. The process of disambiguating cj is straight-
forward exploiting the structural information pro-
vided by HRGs.
w(sk, cj) =
?
Di?H(cj)
P (sk|Di) ? ?i (6)
w(s?, cj) = argmax sk(w(sk, cj)) (7)
Let H(cj) denote the set of parents for context cj .
Then, the weight assigned to sense sk is the sum of
weighted scores provided by each identified parent.
This is shown in Equation 6, where ?i is the proba-
bility associated with each internal nodeDi from the
hierarchical random graph (see Figure 4(A)). This
probability reflects the discriminating ability of in-
ternal nodes.
Finally, the highest weight determines the win-
ning sense for context cj (Equation 7). In our ex-
ample (Figure 4(A)), w(s1, C) = (0 ?1+ 23 ?0.25) =
0.16 andw(s2, C) = (1?1+ 13 ?0.25) = 1.08. Hence,
s2 is the winning sense.
750
Parameter Range
G2 word threshold (p1) 15,25,35,45
G2 collocation threshold (p2) 10,15,20
Edge similarity threshold (p3) 0.05,0.09,0.13
Table 2: Parameter values used in the evaluation.
6 Evaluation
6.1 Evaluation setting & baselines
We evaluate our method on the nouns of the
SemEval-2007 word sense induction task (Agirre
and Soroa, 2007) under the second evaluation setting
of that task, i.e. supervised evaluation. Specifically,
we use the standard WSD measures of precision and
recall in order to produce their harmonic mean (F-
Score). The official scoring software of that task has
been used in our evaluation. Note that the unsuper-
vised measures of that task are not directly applica-
ble to our induced hierarchies, since they focus on
assessing flat clustering methods.
The first aim of our evaluation is to test whether
inferring the hierarchical structure of the constructed
graphs improves WSD performance. For that reason
our first baseline, Chinese Whispers Unweighted
version (CWU), takes as input the same unweighted
graph of contexts as HRGs in order to produce a
flat clustering. The set of produced clusters is then
mapped to GS senses using the training dataset and
performance is then measured on the testing dataset.
We followed the same sense mapping method as in
the SemEval-2007 sense induction task (Agirre and
Soroa, 2007).
Our second baseline, Chinese Whispers Weighted
version (CWW), is similar to the previous one, with
the difference that the edges of the input graph
are weighted using Equation 1. For clustering the
graphs of CWU and CWW we employ, Chinese
Whispers4 (Biemann, 2006).
The second aim of our evaluation is to assess
whether the hierarchical structure inferred by HRGs
is more informative than the hierarchical struc-
ture inferred by traditional Hierarchical Clustering
(HAC). Hence, our third baseline, takes as input a
similarity matrix of the graph vertices and performs
bottom-up clustering with average-linkage, which
has already been used in WSI in (Pantel and Lin,
4The number of iterations for CW was set to 200.
2003) and was shown to have superior or similar per-
formance to single-linkage and complete-linkage in
the related problem of learning a taxonomy of senses
(Klapaftis and Manandhar, 2010).
To calculate the similarity matrix of vertices we
follow a process similar to the one used in Sec-
tion 4.2 for calculating the probability of an inter-
nal node. The similarity between two vertices is
calculated according to the degree of connected-
ness among their direct neighbours. Specifically,
we would like to assign high similarity to pairs of
vertices, whose neighbours are close to forming a
clique.
Given two vertices (contexts) ci and cj , let
N(ci, cj) be the set of their neighbours andK(ci, cj)
be the set of edges between the vertices inN(ci, cj).
The maximum number of edges that could exist be-
tween vertices in N(ci, cj) is
(|N(ci,cj)|
2
)
. Thus, the
similarity of ci, cj is set equal to the number of
edges that actually exist in that neighbourhood di-
vided by the total number of edges that could exist
( |K(ci,cj)|
(|N(ci,cj)|2 )
).
The disambiguation process using the HAC tree
is identical to the one presented in Section 5.2 with
the only difference that the internal probability, ?i,
in Equation 6 does not exist for HAC. Hence, we re-
placed it with the factor 1|H(Di)| , whereH(Di) is the
set of children of internal node Di. This factor pro-
vides lower weights for nodes high in the tree, since
their discriminating ability will possibly be lower.
6.2 Results & discussion
Table 2 shows the parameter values used in the eval-
uation. Figure 6(A) shows the performance of the
proposed method against the baselines for p3 = 0.05
and different p1 and p2 values. Figure 6(B) il-
lustrates the results of the same experiment using
p3 = 0.09. In both figures, we observe that HRGs
outperform the CWU baseline under all parameter
combinations. In particular, all of the 12 perfor-
mance differences for p3 = 0.09 are statistically
significant using McNemar?s test at 95% confidence
level, while for p3 = 0.05 only 2 out of the 12 per-
formance differences were not judged as significant
from the test.
The picture is the same for p3 = 0.13, where
CWU performs significantly worse than for p3 =
751
Figure 6: Performance analysis of HRGs, CWU, CWW & HAC for different parameter combinations (Table 2). (A)
All combinations of p1, p2 and p3 = 0.05. (B) All combinations of p1, p2 and p3 = 0.09.
0.05 and p3 = 0.09. Specifically, the largest perfor-
mance difference between HRGs and CWU is 9.4%
at p1 = 25, p2 = 10 and p3 = 0.13. Setting the ver-
tex similarity threshold (p3) equal to 0.13 leads to
more sparse and disconnected graphs, which causes
Chinese Whispers to produce a large number of clus-
ters. This leads to sparsity problems and unreliable
mapping of clusters to GS senses due to the lack of
adequate training data. In contrast, HRGs suffer less
at this high threshold, although their performance
when p3<0.13 is better.
This picture does not change for the weighted ver-
sion of Chinese Whispers (CWW) which performs
worse than CWU. This is because CWW produces
a smaller number of clusters than CWU that con-
flate the target word senses. It seems that using
weighted edges creates a bias towards the MFS, in
effect missing rare senses of a target word. This
means that a number of words in the bag-of-words
context vectors and collocations in the collocational
context vectors (Section 3.2) are associated to more
than one sense of the target word and most strongly
associated to the MFS. As a result, increasing the p1
threshold to 25 and 35 leads to a higher performance
for CWW, since many of these words and colloca-
tions are filtered out.
Overall, the comparison of HRGs against the
CWU and CWW baselines has shown that inferring
the hierarchical structure of observed graphs leads
to improved WSD performance as opposed to using
flat clustering. This is because HRGs are able to in-
fer both the hierarchical structure of the graph and
include the probabilities, ?k, associated with each
internal node. These probabilities reflect the dis-
criminating ability of each node, offering informa-
tion missed by flat clustering.
In Figures 6(A) and 6(B) we observe that HRGs
perform significantly better than HAC. In particular,
all of their performance differences are statistically
significant for these parameter values. The largest
performance difference is 6.0% at p1 = 45, p2 = 10
and p3 = 0.05. However, this picture is not the same
when considering a higher context similarity thresh-
old (p3 = 0.13) as Figure 7 shows. In particular,
HRGs and HAC perform similarly for p3 = 0.13,
while the majority of performance differences are
not statistically significant.
The similar behaviour of HRGs and HAC at this
threshold is caused both by the worse performance
of HRGs and the improved performance of HAC as
opposed to lower p3 values. As it has been men-
tioned, setting p3 = 0.13 leads to sparse and dis-
connected graphs. Additionally, the likelihood func-
tion (Equation 3) is maximised when the probabil-
ity, ?k, of an internal node, Dk, approaches 0 or 1.
This creates a bias towards dendrograms, in which a
large number of internal nodes have zero probabil-
ity. These dendrograms might be a good-fit to the
observed graph, but not to the GS.
In contrast, HAC is less affected, because it never
considers creating an internal node, when the max-
imum similarity among any pair of two candidate
752
Figure 7: Performance of HRGs and HAC for different
parameter combinations (Table 2). All combinations of
p1, p2 and p3 ? 0.13.
subtrees is zero. Additionally, our experiments show
that HAC is unable to deal with noise when con-
sidering sparse graphs (p3<0.13). For that reason,
the F-Score of HAC increases as the edge similarity
threshold decreases.
To further investigate this issue and test whether
HAC is able to achieve a higher F-Score than HRGs
in higher p3 values, we executed two more experi-
ments for HAC and HRGs increasing p3 to 0.17 and
0.21 respectively. In the first case we observed that
the performance of HAC remained relatively stable
compared to p3 = 0.13, while in the second case the
performance of HAC decreased as Figure 7 shows.
In both cases, HAC performed significantly better
than HRGs.
Overall, the comparison of HRGs against HAC
has shown that HRGs perform significantly better
than HAC when considering connected or less sparse
graphs (p3<0.13). This is due to the fact that HAC
creates dendrograms, in which connections within
the clusters are dense, while connections between
the clusters are sparse, i.e. it only considers assorta-
tive structures. In contrast, HRGs also consider dis-
assortative dendrograms, i.e. dendrograms in which
vertices are less likely to be connected on small
scales than on large ones, as well as mixtures of
assortative and disassortative (Clauset et al, 2008).
This is achieved by allowing the probability ?k of
a node k to vary arbitrarily throughout the dendro-
gram.
HAC performs similarly or better than HRGs for
largely disconnected and sparse graphs, because
HRGs become biased towards disassortative trees
which are not a good fit to the GS (Figure 7). De-
spite that, our evaluation has also shown that the best
performance of HAC (F-Score = 86.0% at p1 = 15,
p2 = 10, p3 = 0.13) is significantly lower than
the best performance of HRGs (F-Score = 87.6% at
p1 = 35, p2 = 10, p3 = 0.09).
6.3 Comparison to state-of-the-art methods
Table 3 compares the best performing parameter
combination of our method against state-of-the-art
methods. Table 3 also includes the best performance
of our baselines, i.e HAC, CWU and CWW.
Brody & Lapata (2009) presented a sense induc-
tion method that is related to Latent Dirichlet Al-
location (Blei et al, 2003). In their work, they
model the target word instances as samples from a
multinomial distribution over senses which are suc-
cessively characterized as distributions over words
(Brody and Lapata, 2009). A significant advantage
of their method is the inclusion of more than one
layer in the LDA setting, where each layer corre-
sponds to a different feature type e.g. dependency
relations, bigrams, etc. The inclusion of different
feature types as separate models in the sense in-
duction process can easily be modeled in our set-
ting, by inferring a different hierarchy of target word
instances according to each feature type, and then
combining all of them to a consensus tree. In this
work, we have focused on extracting a single hierar-
chy combining word co-occurrence and bigram fea-
tures.
Niu et al (2007) developed a vector-based
method that performs sense induction by group-
ing the contexts of a target word using three types
of features, i.e. POS tags of neighbouring words,
word co-occurrences and local collocations. The se-
quential information bottleneck algorithm (Slonim
et al, 2002) is applied for clustering. HRGs perform
slightly better than the methods of Brody & Lap-
ata (2009) and Niu et al (2007), although the dif-
ferences are not significant (McNemar?s test at 95%
confidence level).
Klapaftis & Manandhar (2008) developed a
graph-based sense induction method, in which ver-
tices correspond to collocations related to the tar-
get word and edges between vertices are drawn ac-
753
System Performance (%)
HRGs 87.6
(Brody and Lapata, 2009) 87.3
(Niu et al, 2007) 86.8
(Klapaftis and Manandhar, 2008) 86.4
HAC 86.0
CWU 85.1
CWW 84.7
(Pedersen, 2007) 84.5
MFS 80.9
Table 3: HRGs against recent methods & baselines.
cording to the co-occurrence frequency of the cor-
responding collocations. The constructed graph is
smoothed to identify more edges between vertices
and then clustered using Chinese Whispers (Bie-
mann, 2006). This method is related to the basic
inputs of our presented method. Despite that, it is
a flat clustering method that ignores the hierarchical
structure exhibited by observed graphs. The previ-
ous section has shown that inferring the hierarchical
structure of graphs leads to superior WSD perfor-
mance.
Pedersen (2007) presented SenseClusters, a
vector-based method that clusters second order co-
occurrence vectors using k-means, where k is auto-
matically determined using the Adapted Gap Statis-
tic (Pedersen and Kulkarni, 2006). As can be ob-
served, HRGs perform significantly better than the
methods of Pedersen (2007) and Klapaftis & Man-
andhar (2008) (McNemar?s test at 95% confidence
level).
Finally, Table 3 shows that the best performing
parameter combination of HRGs achieves a signifi-
cantly higher F-Score than the best performing pa-
rameter combination of HAC, CWU and CWW. Fur-
thermore, HRGs outperform the most frequent sense
baseline by 6.7%.
7 Conclusion & future work
We presented an unsupervised method for inferring
the hierarchical grouping of the senses of a polyse-
mous word. Our method creates a graph, in which
vertices correspond to contexts of a polysemous tar-
get word and edges between them are drawn ac-
cording to their similarity. The hierarchical random
graphs algorithm (Clauset et al, 2008) was applied
to the constructed graph in order to infer its hierar-
chical structure, i.e. binary tree.
The learned tree provides an induction of the
senses of a given word at different levels of sense
granularity and was applied to the problem of WSD.
The WSD process mapped the tree?s internal nodes
to GS senses using a sense tagged corpus, and then
tagged new instances by exploiting the structural in-
formation provided by the tree.
Our experimental results have shown that our
graphs exhibit hierarchical organisation that can
be captured by HRGs, in effect providing im-
proved WSD performance compared to flat cluster-
ing. Additionally, our comparison against hierarchi-
cal agglomerative clustering with average-linkage
has shown that HRGs perform significantly better
than HAC when the graphs do not suffer from spar-
sity (disconnected graphs). The comparison with
state-of-the-art sense induction systems has shown
that our method yields improvements.
Our future work focuses on using different feature
types, e.g. dependency relations, second-order co-
occurrences, named entities and others to construct
our undirected graphs and then applying HRGs, in
order to measure the impact of each feature type
on the induced hierarchical structures within a WSD
setting. Moreover, following the work in (Clauset et
al., 2008), we are also working on using MCMC in
order to sample more than one dendrogram at equi-
librium, and then combine them to a consensus tree.
This consensus tree might be able to express a larger
amount of topological features of the initial undi-
rected graph.
Finally in terms of evaluation, our future work
also focuses on evaluating HRGs using a fine-
grained sense inventory, extending the evaluation on
the SemEval-2010 WSI task dataset (Manandhar et
al., 2010) as well as applying HRGs to other related
tasks such as taxonomy learning.
Acknowledgements
This work is supported by the European Com-
mission via the EU FP7 INDECT project, Grant
No.218086, Research area: SEC-2007-1.2-01 Intel-
ligent Urban Environment Observation System. The
authors would like to thank the anonymous review-
ers for their useful comments.
754
References
Eneko Agirre and Aitor. Soroa. 2007. Semeval-2007
Task 02: Evaluating Word Sense Induction and Dis-
crimination Systems. In Proceedings of SemEval-
2007, pages 7?12, Prague, Czech Republic.
Eneko Agirre, David Mart??nez, Oier Lo?pez de Lacalle,
and Aitor Soroa. 2006. Two Graph-based Algorithms
for State-of-the-art WSD. In Proceedings of EMNLP-
2006, pages 585?593, Sydney, Australia.
Chris Biemann. 2006. Chinese Whispers - An Efficient
Graph Clustering Algorithm and its Application to
Natural Language Processing Problems. In Proceed-
ings of TextGraphs, pages 73?80, New York, USA.
David M. Blei, Andrew Y. Ng, and Michael I. Jordan.
2003. Latent Dirichlet Allocation. J. Mach. Learn.
Res., 3:993?1022.
Sergey Brin and Lawrence Page. 1998. The Anatomy
of a Large-Scale Hypertextual Web Search Engine.
Comput. Netw. ISDN Syst., 30(1-7):107?117.
Samuel Brody and Mirella Lapata. 2009. Bayesian Word
Sense Induction. In Proceedings of EACL-2009, pages
103?111, Athens, Greece. ACL.
Aaron Clauset, Cristopher Moore, and Mark E. J. New-
man. 2006. Structural Inference of Hierarchies in Net-
works. In Proceedings of the ICML-2006 Workshop
on Social Network Analysis, pages 1?13, Pittsburgh,
USA.
Aaron Clauset, Cristopher Moore, and Mark E. J. New-
man. 2008. Hierarchical Structure and the Prediction
of Missing Links in Networks. Nature, 453(7191):98?
101.
Stijn Dongen. 2000. Performance Criteria for Graph
Clustering and Markov Cluster Experiments. Tech-
nical report, CWI (Centre for Mathematics and Com-
puter Science), Amsterdam, The Netherlands.
Beate Dorow and Dominic Widdows. 2003. Discovering
Corpus-specific Word Senses. In Proceedings of the
EACL-2003, pages 79?82, Budapest, Hungary.
Ted Dunning. 1993. Accurate Methods for the Statistics
of Surprise and Coincidence. Computational Linguis-
tics, 19(1):61?74.
Phil Edmonds and Beate Dorow. 2001. Senseval-2:
Overview. In Proceedings of SensEval-2, pages 1?5,
Toulouse, France.
Ioannis P. Klapaftis and Suresh Manandhar. 2008. Word
Sense Induction Using Graphs of Collocations. In
Proceedings of ECAI-2008, pages 298?302, Patras,
Greece.
Ioannis P. Klapaftis and Suresh Manandhar. 2010. Tax-
onomy Learning Using Word Sense Induction. In Pro-
ceedings of NAACL-HLT-2010, pages 82?90, Los An-
geles, California, June. ACL.
Suresh Manandhar, Ioannis P. Klapaftis, Dmitriy Dli-
gach, and Sameer S. Pradhan. 2010. Semeval-2010
Task 14: Word Sense Induction & Disambiguation. In
Proceedings of SemEval-2, Uppsala, Sweden. ACL.
Rada Mihalcea. 2004. Graph-based Ranking Algorithms
for Sentence Extraction, Applied to Text Summariza-
tion. In Proceedings of the ACL 2004 on Interactive
poster and demonstration sessions, page 20, Morris-
town, NJ, USA.
Mark Newman and Gerard Barkema. 1999. Monte Carlo
Methods in Statistical Physics. Oxford: Clarendon
Press, New York, USA.
Zheng-Yu Niu, Dong-Hong Ji, and Chew-Lim Tan. 2007.
I2R: Three Systems for Word Sense Discrimination,
Chinese Word Sense Disambiguation, and English
Word Sense Disambiguation. In Proceedings of
SemEval-2007, pages 177?182, Prague, Czech Repub-
lic.
Patrick Pantel and Dekang Lin. 2003. Automatically
Discovering Word Senses. In Proceedings of NAACL-
HLT-2003, pages 21?22, Morristown, NJ, USA.
Ted Pedersen and Anagha Kulkarni. 2006. Automatic
Cluster Stopping With Criterion Functions and the gap
Statistic. In Proceedings of the 2006 Conference of the
North American Chapter of the ACL on Human Lan-
guage Technology, pages 276?279, Morristown, NJ,
USA.
Ted Pedersen. 2007. UMND2 : Senseclusters Applied to
the Sense Induction Task of Senseval-4. In Proceed-
ings of SemEval-2007, pages 394?397, Prague, Czech
Republic.
Daniel Ramage, Anna N. Rafferty, and Christopher D.
Manning. 2009. Random Walks for Text Semantic
Similarity. In Proceedings of TextGraphs-4, Suntec,
Singapore, August.
Noam Slonim, Nir Friedman, and Naftali Tishby. 2002.
Unsupervised Document Classification Using Sequen-
tial Information Maximization. In SIGIR 2002, pages
129?136, New York, NY, USA. ACM.
Benjamin Snyder and Martha Palmer. 2004. The En-
glish All-words Task. In Rada Mihalcea and Phil Ed-
monds, editors, In Proceedings of Senseval-3, pages
41?43, Barcelona, Spain.
Jean Ve?ronis. 2004. Hyperlex: Lexical Cartography for
Information Retrieval. Computer Speech & Language,
18(3):223?252.
Julie Weeds, David Weir, and Diana McCarthy. 2004.
Characterising Measures of Lexical Distributional
Similarity. In Proceedings of COLING-2004, pages
10?15, Morristown, NJ, USA.
Dominic Widdows and Beate Dorow. 2002. A Graph
Model for Unsupervised Lexical Acquisition. In Pro-
ceedings of Coling-2002, pages 1?7, Morristown, NJ,
USA.
755
Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 82?90,
Los Angeles, California, June 2010. c?2010 Association for Computational Linguistics
Taxonomy Learning Using Word Sense Induction
Ioannis P. Klapaftis
Department of Computer Science
The University of York
York, UK, YO10 5DD
giannis@cs.york.ac.uk
Suresh Manandhar
Department of Computer Science
The University of York
York, UK, YO10 5DD
suresh@cs.york.ac.uk
Abstract
Taxonomies are an important resource for a
variety of Natural Language Processing (NLP)
applications. Despite this, the current state-
of-the-art methods in taxonomy learning have
disregarded word polysemy, in effect, devel-
oping taxonomies that conflate word senses.
In this paper, we present an unsupervised
method that builds a taxonomy of senses
learned automatically from an unlabelled cor-
pus. Our evaluation on two WordNet-derived
taxonomies shows that the learned taxonomies
capture a higher number of correct taxonomic
relations compared to those produced by tradi-
tional distributional similarity approaches that
merge senses by grouping the features of each
word into a single vector.
1 Introduction
A concept or a sense, s, can be defined as the mean-
ing of a word or a multiword expression. A con-
cept s can be linguistically realised by more than one
word while at the same time a wordw can be the lin-
guistic realisation of more than one concept. Given
a set of concepts S, taxonomy learning is the task of
hierarchically classifying the elements in S in an au-
tomatic manner. For example, consider a set of con-
cepts linguistically realised by the words/multiword
expressions LAN, computer network, internet, mesh-
work, gauze, snood. Taxonomy learning methods
produce taxonomies, such as the ones shown in Fig-
ures 1 (a) and 1 (b).
By observing Figure 1 (a), we can express IS-
A statements, such as Internet IS-A Computer Net-
work etc. However, the same does not apply to the
Figure 1: A labelled and an unlabelled concept taxonomy
taxonomy in Figure 1 (b), since this taxonomy is not
fully labelled. Despite this, its hierarchical organ-
isation clearly shows that the concepts are divided
into groups, which are further subdivided into sub-
groups and so forth, until we reach a level where
each concept belongs to its own group. Unlabelled
taxonomies are typically produced by agglomera-
tive hierarchical clustering algorithms (King, 1967;
Sneath and Sokal, 1973).
The knowledge encoded in taxonomies can be
utilised in a range of NLP applications. For in-
stance, taxonomies can be used in information re-
trieval to expand a user query with semantically re-
lated words or to enhance document representation
by abstracting from plain words and adding concep-
tual information (Cimiano, 2006). WordNet?s (Fell-
baum, 1998) taxonomic relations have also been
used in Word Sense Disambiguation (WSD) (Nav-
igli and Velardi, 2004b). In named entity recog-
nition, methods relying on gazetteers could make
82
use of automatically acquired taxonomies (Cimiano,
2006), while question answering systems have also
benefited (Moldovan and Novischi, 2002).
Despite the wide uses of taxonomies, the majority
of methods disregard or do not deal effectively with
word polysemy, in effect, developing taxonomies
that conflate the senses of words (see Section 2).
In this work, we show that Word Sense Induction
(WSI) can be effectively employed to address this
limitation of existing methods.
We present a novel method that employs WSI to
generate the different senses of a set of target words
from an unlabelled corpus and then produces a tax-
onomy of senses using Hierarchical Agglomerative
Clustering (HAC) (King, 1967; Sneath and Sokal,
1973). We evaluate our method on two WordNet-
derived sub-taxonomies and show that our method
leads to the development of concept hierarchies that
capture a higher number of correct taxonomic rela-
tions in comparison to those generated by current
distributional similarity approaches.
2 Related work
Initial research on taxonomy learning focused on
identifying in a given text lexico-syntactic patterns
that suggest hyponymy relations (Hearst, 1992). For
instance, the pattern NP0 such as NP1,. . . ,NPn
suggests that NP0 is a hypernym of NPi. For ex-
ample, given the phrase Fruits, such as oranges, ap-
ples,..., the above pattern would suggest that fruit
is a hypernym of orange and apple. These pattern-
based approaches operate at the word level by learn-
ing lexical relations between words rather than be-
tween senses of words.
In the same spirit, other work attempted to exploit
the regularities of dictionary entries to identify hy-
ponymy relations (Amsler, 1981). For example in
WordNet, WAN is defined as a computer network
that spans . . . . Hence, one can easily induce that
WAN is a hyponym of computer network by assum-
ing that the first noun phrase in the definition is a hy-
pernym of the target word. These approaches learn
lexical relations at the sense level since dictionaries
separate the senses of a word. However this would
be true if and only if the glosses of the dictionaries
were sense-annotated, which is not the case for the
majority of electronic dictionaries (Cimiano, 2006).
Another limitation is that taxonomies are built ac-
cording to the sense distinctions present in dictio-
naries and not according to the actual use of words
in the corpus.
The majority of taxonomy learning approaches
are based on the distributional hypothesis (Harris,
1968). Typically, distributional similarity methods
(Cimiano et al, 2004; Cimiano et al, 2005; Faure
and Ne?dellec, 1998; Reinberger and Spyns, 2004;
Caraballo, 1999) utilise syntactic dependencies such
as subject/verb, object/verb relations, conjunctive
and appositive constructions and others. These de-
pendencies are used to extract the features that serve
as the dimensions of the vector space. Each target
noun is then represented as a vector of extracted fea-
tures where the frequency of co-occurrence of the
target noun with each feature is used to calculate the
weight of that feature. The constructed vectors are
the input to hierarchical clustering or formal concept
analysis (Ganter and Wille, 1999) to produce a tax-
onomy. These approaches assume that a target noun
is monosemous creating one vector of features for
each target noun. This limitation can lead to a num-
ber of problems.
Firstly, the constructed taxonomies might be bi-
ased towards the inclusion of taxonomic relation-
ships between the most frequent senses of tar-
get nouns, ignoring interesting taxonomic relations
where less frequent senses are present. For exam-
ple, consider the word house. Current distributional
similarity methods would possibly capture the hy-
ponyms of its Most Frequent Sense (MFS1), how-
ever ignoring the hyponyms of less frequent senses
of house, e.g. casino, theater, etc. Given that word
senses typically follow a Zipf distribution, these
methods construct vectors dominated by the MFS of
words. This bias significantly degrades the useful-
ness of learned taxonomies.
Secondly, given that distributional similarity ap-
proaches rely on the computation of pairwise simi-
larities between target words, merging their senses
to a single vector might lead to unreliable similarity
estimates. For example, merging the features of the
different senses of house could provide a lower sim-
ilarity with its monosemous hyponym beach house,
since only the first sense of house is related to beach
1WordNet: A dwelling that serves as living quarters . . .
83
house. This problem might lead both to inclusion
of incorrect or loss of correct taxonomic relations.
In our work, we aim to overcome these drawbacks
by identifying the different senses with which target
words appear in text and then building a hierarchy
of the identified senses.
Soft clustering approaches (Reinberger and
Spyns, 2004; Reinberger et al, 2003) have also been
applied to taxonomy learning to deal with polysemy.
These methods associate each verb with a vector of
features, where each feature is a noun appearing as
a subject or object of that verb. That way a noun can
appear in different vectors, hence in different clus-
ters during hierarchical clustering as a result of its
polysemy. However, the underlying assumption is
that a verb is monosemous with respect to its associ-
ated vector of nouns. This assumption is not always
valid and can cause the problems mentioned above.
Other work in taxonomy learning exploits the
head/modifier relationships to create taxonomic re-
lations (Buitelaar et al, 2004; Hwang, 1999;
Sa?nchez and Moreno, 2005). These relations are
used to create: (1) a class (concept) for each head,
and (2) subclasses by adding nominal or adjectival
modifiers. For example, credit card IS-A card. The
corresponding hyponymy relations are learned at the
lexical level disregarding word polysemy. Some of
these approaches identified the problem of polysemy
and applied sense disambiguation with respect to
WordNet in order to capture the different senses of a
target term (Navigli and Velardi, 2004b; Navigli and
Velardi, 2004a). Specifically, the taxonomy built by
exploiting head/modifiers relations was modified ac-
cording to WordNet?s hyponymy relations between
senses of disambiguated terms. One important de-
ficiency of using sense disambiguation is that dic-
tionaries miss many domain-specific senses. Addi-
tionally, the fixed-list of senses paradigm prohibits
learning word senses according to their use in con-
text. The use of sense induction we propose in this
paper aims to overcome these limitations.
3 Method
Given a set of words W , a WSI method is applied
to each wi ? W (Section 3.1). The outcome of the
first stage is a set of senses, S, where each swi ? S
denotes the i-th sense of word w ? W . This set
Figure 2: WSI for network & LAN
of senses is the input to hierarchical clustering that
produces a hierarchy of senses (Section 3.2).
3.1 Word sense induction
WSI is the task of identifying the senses of a tar-
get word in a given text. Recent WSI methods
were evaluated under the framework of SemEval-
2007 WSI task (SWSI) (Agirre and Soroa, 2007).
The evaluation framework defines two types of as-
sessment, i.e. evaluation in: (1) a clustering and
(2) a WSD setting. Based on this evaluation, we se-
lected the method of Klapaftis & Manandhar (2008)
(henceforth referred to as KM) that achieves high F-
score in both evaluation schemes as compared to the
systems participating in SWSI. We briefly describe
KM mentioning its parameters used in our evalua-
tion (Section 4). Figures 2 (a) and 2 (b) describe the
different steps for inducing the senses of the target
words network and LAN.
Corpus preprocessing: The input to KM is a
base corpus bc, in which the target word w appears
in each paragraph. In Figure 2 (a), the base cor-
pus consists of the paragraphs A, B, C and D. The
aim of this stage is to capture nouns contextually
84
related to w. Initially, the target word is removed
from bc, part-of-speech tagging is applied to each
paragraph, only nouns are kept and lemmatised. In
the next step, the distribution of each noun is com-
pared to the distribution of the same noun in a ref-
erence corpus2 using the log-likelihood ratio (G2)
(Dunning, 1993). Nouns with a G2 below a pre-
specified threshold (parameter p1) are removed from
each paragraph. Figure 2 (a) shows the remaining
nouns for each paragraph of bc.
Graph creation & clustering: In the setting of
KM, a collocation is a juxtaposition of two nouns
within the same paragraph. Thus, each noun is com-
bined with any other noun yielding a total of
(N
2
)
collocations for a paragraph with N nouns. Each
collocation, cij , is assigned a weight that measures
the relative frequency of two nouns co-occurring.
This weight is the average of the conditional prob-
abilities p(ni|nj) and p(nj |ni), where p(ni|nj) =
f(cij)
f(nj)
, f(cij) is the number of paragraphs nouns ni,
nj co-occur and f(nj) is the number of paragraphs
in which nj appears. Collocations are filtered with
respect to their frequency (parameter p2) and weight
(parameter p3). Each retained collocation is rep-
resented as a vertex. Edges between vertices are
present, if two collocations co-occur in one or more
paragraphs. Figure 2 (a) shows that this process has
generated 24 collocations for the target word net-
work. On the top right of the figure we also observe
the collocations associated with each paragraph.
In the next step, a smoothing technique is applied
to discover new edges between vertices. The weight
applied to each edge connecting vertices vi and vj
(collocations cab, cde) is the maximum of their con-
ditional probabilities (max(p(cab|cde), p(cde|cab))).
Finally, the graph is clustered using Chinese whis-
pers (Biemann, 2006). The final output is a set of
senses, each one represented by a set of contextually
related collocations. In Figure 2, we generated two
senses for network and one sense for LAN.
3.2 Hierarchical clustering of senses
Given the set of senses S, our task at this point is to
hierarchically classify the senses using HAC. Con-
sider for example the words network and LAN, and
2The British National Corpus, 2001, Distributed by Oxford
University Computing Services.
Senses computer meshwork LAN
network
computer network 1 0.0 0.66
meshwork 0.0 1 0.14
LAN 0.66 0.14 1
Table 1: Similarity matrix for HAC.
Figure 3: WSI & HAC example
let us assume that the WSI process has generated
the senses in Figures 2 (a) and 2 (b). HAC oper-
ates by treating each sense as a singleton cluster and
then successively merging the most similar clusters
according to a pre-defined similarity function. This
process iterates until all clusters have been merged
into a single cluster taken to be the root.
To calculate the pairwise similarities between
senses we exploit the attributes that represent each
sense, i.e. their collocations. Let BC be the cor-
pus resulting from the union of the base corpora of
all words in W . In our example, BC would consist
of the paragraphs, in which the words network and
LAN appear, i.e. A, B, ..., G. An induced sense tags
a paragraph, if one or more of its collocations ap-
pear in that paragraph. Thus, each induced sense is
associated with a set of paragraph labels that denote
the paragraphs tagged by that sense. Figure 3 shows
the paragraph labels tagged by each sense of our ex-
ample. Finally, given two senses sai , s
b
i and their
corresponding sets of tagged paragraphs fai and f
b
i ,
we use the Jaccard coefficient to calculate their sim-
ilarity, i.e. JC(sai , s
b
i) =
|fai ?f
b
i |
|fai ?f
b
i |
, where skj denotes
the j-th sense of word k. The resulting similarity
matrix of our example is shown in Table 1. Given
that matrix, HAC would first group computer net-
work and LAN as they have the highest similarity
(Figure 3). In the final iteration, the remaining two
clusters (Cluster 1 & meshwork) would be grouped
to the root.
An important parameter of HAC is the choice
of the technique for calculating cluster similarities.
Note that as we move towards the higher levels of
85
the taxonomy clusters contain more than one sets of
tagged paragraphs (Figure 3 - Cluster 1), hence the
choice of the similarity function is crucial. We ex-
periment with three techniques, i.e. single-linkage,
complete-linkage and average-linkage. The first one
defines the similarity between two clusters as the
maximum similarity among all the pairs of their cor-
responding feature sets. The second considers the
minimum similarity among all the pairs, while the
third calculates the average similarity of all the pairs.
4 Evaluation
We evaluate our method with respect to two
WordNet-derived sub-taxonomies (Section 4.3). For
that reason, it is necessary to map the induced senses
to WordNet before applying HAC. Note that the
mapping process might map more than one induced
senses to the same WordNet sense. In that case,
these induced senses are merged to a single one
along with their corresponding collocations.
4.1 Mapping WSI clusters to WordNet senses
The process of mapping the induced senses to Word-
Net is straightforward. Let w ? W be a word with
n senses in WordNet. A WordNet sense i of w is de-
noted bywswi , i = [1, n]. Let us also assume that the
WSI method has produced m senses for w, where
each sense j is denoted as swj , j = [1,m]. Each in-
duced sense swj is associated with a set of features
fwj as in the previous section. These features are the
paragraphs (paragraph labels) of BC tagged by swj .
In the next step, each WordNet sense wswi is associ-
ated with its WordNet signature gwi that contains the
following semantic features: hypernyms/hyponyms,
meronyms/holonyms and synonyms of wswi . For
example, the signature of the fifth WordNet sense
of network would contain internet, cyberspace and
other semantically related words. Table 2 shows par-
tial signatures for each sense of network.
The signature gwi is used to formalise the Word-
Net sense wswi as a set of features q
w
i . These fea-
tures are the paragraphs (paragraph labels) of BC
that contain one or more of the aforementioned se-
mantically related to wswi words that exist in g
w
i .
Given an induced sense swj , a similarity score is cal-
culated between swj and each WordNet sense of w.
The maximum score determines the WordNet sense
WordNet sense Semantically related words/phrases
1 reticulum, RF, RAS
2 communication system/equipment
3 gauze, snood, tulle
4 reseau, reticle, reticulation
5 net, internet, cyberspace
Table 2: Semantically related words/phrases to network
label that will be assigned to swj , i.e. label(s
w
j ) =
argmaxi JC(f
w
j , q
w
i ), where JC is the Jaccard sim-
ilarity coefficient. In the example of Figure 2 (a),
the computer network sense would be mapped to the
fifth WordNet sense of network, since there is a sig-
nificant overlap between the paragraphs tagged by
the induced and that WordNet sense.
4.2 Evaluation measures
For the purposes of this section we present one gold
standard taxonomy (Figure 1 (a)) and a second de-
rived from our method (Figure 1 (b)). The compari-
son of these taxonomies is based on the semantic co-
topy of a node, which has also been used in (Maed-
che and Staab, 2002; Cimiano et al, 2005). In par-
ticular, the semantic cotopy of a node is defined as
the set of all its super- and subnodes excluding the
root and including that node. For example, the se-
mantic cotopy of computer network in Figure 1 (a)
is {computer network, internet, LAN}. There are
two issues, which make the evaluation difficult.
The first one is that HAC produces a taxonomy in
which all internal nodes are unlabelled, as opposed
to the gold standard taxonomy. In Figure 1 (b), we
have manually labelled internal nodes with their IDs
for clarity. For example, the semantic cotopy of the
node New Cluster 1 in Figure 1 (b) is {computer net-
work, internet, LAN, New Cluster 1, New Cluster
0}. By comparing the cotopies of nodes computer
network in Figure 1 (a) and New Cluster 1 in Fig-
ure 1 (b), we observe that the automatic method has
successfully grouped all of the hypernyms and hy-
ponyms of computer network under New Cluster 1.
However, the corresponding cotopies are not iden-
tical, because the cotopy of New Cluster 1 also in-
cludes the labels produced by HAC.
To deal with this problem, we use a version of se-
mantic cotopy for nodes in the automatically learned
taxonomy which excludes nodes that do not exist in
WordNet. That way the semantic cotopies of New
Cluster 1 in Figure 1 (b) and computer network in
86
Figure 1 (a) will yield maximum similarity.
The second issue is that the nodes that exist in the
gold standard taxonomy are leaf nodes in the auto-
matically learned taxonomy. As a result, the seman-
tic cotopy of LAN in Figure 1 (b) is {LAN} since
all of its supernodes do not exist in WordNet. In
contrast, the semantic cotopy of LAN in Figure 1
(a) is {LAN, computer network}. We observe that
there is an overlap between the two cotopies derived
by the existence of the same concept in both tax-
onomies, i.e. LAN. In fact, all of the leaf nodes of
a learned taxonomy will have a small overlap with
the corresponding concept in the gold standard. For
this problem, we observe that in our automatically
learned taxonomies it does not make sense to cal-
culate the semantic cotopy of leaf nodes. On the
contrary, we need to evaluate the internal nodes that
group the leaf nodes. Let us assume the following
notation:
TA = automatically learned taxonomy
?i = node in a taxonomy
C(TA) = internal nodes + leaf nodes of TA
I(TA) = internal nodes of TA
TG = gold standard taxonomy
C(TG) = internal nodes + leaf nodes of TG
I(TG) = internal nodes of TG
hyper(?i) = supernodes of ?i excluding the root
hypo(?i) = subnodes of ?i including ?i
For ?i ? I(TA), the semantic cotopy is defined as:
SC ?(?i) = (hyper(?i) ? hypo(?i)) ? C(TG)
For ?i ? C(TG), the semantic cotopy is defined as:
SC ??(?i) = (hyper(?i) ? hypo(?i))
P (?i, ?j) =
|SC ?(?i) ? SC ??(?j)|
|SC ?(?i)|
(1)
R(?i, ?j) =
|SC ?(?i) ? SC ??(?j)|
|SC ??(?j)|
(2)
F (?i, ?j) =
2P (?i, ?j)R(?i, ?j)
P (?i, ?j) +R(?i, ?j)
(3)
Precision, recall and harmonic mean of node ?i ?
I(TA) with respect to node ?j ? C(TG) are de-
fined in Equations 1, 2 and 3. The F-score, FS, of
node ?i ? I(TA) is the maximum F attained at any
?j ? C(TG) (FS(?i) = argmaxj F (?i, ?j)). Fi-
nally, the similarity TS of the entire taxonomy to
the gold standard taxonomy is the average of the
F-scores of each ?i ? I(TA) (Equation 4). The
TS(TA, TG) in Figure 1 is 0.9. All nodes of TA
have a perfect match, apart from New Cluster 0 and
New Cluster 2, which are matched against computer
network and meshwork respectively, having a per-
fect precision but a lower recall since the cotopies
of computer network and meshwork consist of three
concepts. The automatically learned taxonomy has
two redundant clusters that decrease its similarity.
TS(TA, TG) =
1
|I(TA)|
?
?i?I(TA)
FS(?i) (4)
The similarity measure TS(TA, TG) provides the
similarity of the automatically learned taxonomy to
the gold standard one, but it is not symmetric. Cal-
culating the taxonomic similarity one way might not
provide accurate results, in cases where TA misses
senses of the gold standard. This is due to the
fact that we would only evaluate the internal nodes
of TA, partially ignoring the fact that TA might
have missed some parts of the gold standard taxon-
omy. For that reason, we also calculate TS(TG, TA)
which provides the similarity of the gold standard
taxonomy to the automatically learned one. Fi-
nally, taxonomic similarities are combined to pro-
duce their harmonic mean (Equation 5).
TxSm(TA, TG) =
2TS(TG, TA)TS(TA, TG)
TS(TG, TA) + TS(TA, TG)
(5)
4.3 Evaluation datasets & setting
The first gold standard taxonomy is derived by ex-
tracting from WordNet al the hyponyms of the
senses of the word network. The extracted taxonomy
contains 29 senses linguistically realized by 24 word
sets (one sense might be expressed with more than
one words), since network has 5 senses and reseau
has 2 senses in the gold standard taxonomy. Note
that we have disregarded senses only expressed by
multiword expressions. The average polysemy of
words is around 1.7. The second taxonomy is de-
rived by extracting the concepts under the senses of
the word speaker. The speaker taxonomy contains
52 senses linguistically realized by 50 word sets,
since speaker has 3 senses included in the taxonomy.
The average polysemy of words is around 1.58.
To create our datasets3 we use the Yahoo! search
api4. For each word w in each of the datasets, we is-
3Available in http://www.cs.york.ac.uk/aig/projects/indect/taxlearn
4http://developer.yahoo.com/search/ [Accessed:10/06/2009]
87
Parameter Range
G2 threshold (p1) 5,10
Collocation frequency (p2) 4,6,8
Collocation weight (p3) 0.1,0.2,0.3,0.4
Table 3: Chosen parameters for the KM WSI method.
sue a query to Yahoo! that contains w and we down-
load a maximum of 1000 pages. In cases where
a particular sense is expressed by more than one
word, the query was formulated by including all the
words and putting the keyword OR between them.
For each page we extracted fragments of text (para-
graphs) that occur in <p> </p> html tags. We ex-
tracted 58956 and 78691 paragraphs for the network
and speaker dataset respectively. The reason we ex-
tracted on average less content for the second dataset
was that Yahoo! provided a small number of results
for rare words such as alliterator, anecdotist, etc.
Table 3 shows the parameter ranges for the WSI
method. Our method is evaluated according to these
parameters. Our first baseline is RAND, which per-
forms a random hierarchical clustering of senses to
produce a binary tree. In each iteration two clusters
are randomly chosen and form a new cluster, until
we end up with one cluster taken to be the root. The
performance of RAND is calculated by executing the
random algorithm 10 times and then averaging the
results. The second baseline is the taxonomy most
frequent sense baseline (TL MFS), in which we do
not perform WSI. Instead, given a parameter setting
and a word w, all the collocations of w are grouped
into one vector, which will possibly be dominated
by collocations related to the MFS of w. WordNet
mapping takes place and finally HAC with average-
linkage is applied to create the taxonomy.
4.4 Results & discussion
Figures 4 (a) and 4 (b) show the performance
of HAC with single-linkage (HAC SNG), average-
linkage (HAC AVG) and complete-linkage (HAC
CMP) against RAND for p1 = 5 and different com-
binations of p2 and p3. It is clear that HAC SNG and
HAC AVG outperform RAND by very large margins
under all parameter combinations. In the network
dataset, both of them achieve their highest distance
from RAND (27.84%) at p2 = 8 and p3 = 0.2. In the
speaker dataset, their highest distance from RAND
(20.97% and 19.63% respectively) is achieved at
p2 = 4 and p3 = 0.1. HAC CMP performs worse
than the other HAC versions, yet it clearly outper-
forms RAND in all but one parameter combinations
(p1 = 5, p2 = 6, p3 = 0.4) in the speaker dataset.
Generally, for collocation weight equal to 0.4 the
performance of all HAC versions drops. At this
high collocation weight the WSI method produces a
larger number of small clusters than in lower thresh-
olds. This issue negatively affects both the map-
ping process and HAC. For example in the speaker
dataset, for p1 = 5, p2 = 8 and p3 = 0.1 our tax-
onomies contained 86.54% of the gold standard tax-
onomy senses. Increasing the collocation weight to
0.2 did not have any effect, but increasing the weight
to 0.3 and then 0.4 led to 71.15% and 65.38% sense
coverage. Overall, our conclusion is that all HAC
versions exploit the WSI method and learn useful
information better than chance. The picture is the
same for p1 = 10.
Figures 4 (c) and 4 (d) show the performance of
HAC versions against the TL MFS baseline in the
same parameter setting as above. We observe that
both HAC SNG and HAC AVG perform significantly
better than TL MFS apart from p3 = 0.4, in which
case all HAC versions perform worse. In the network
dataset, the largest performance difference for HAC
SNG is 10.12% and for HAC AVG 9.9% at p2 = 6
and p3 = 0.2. In the speaker dataset, the largest per-
formance difference for HAC SNG is 10.83% and
for HAC AVG 7.83% at p2 = 8 and p3 = 0.2. HAC
CMP performs worse than TL MFS under most pa-
rameter settings in both datasets. The picture is the
same for p1 = 10.
Overall, the analysis of the WSI-based taxonomy
learning approach against TL MFS shows that HAC
SNG and HAC AVG perform better than TL MFS
under all parameter combinations for both datasets.
The main reason for their superior performance is
that their learned taxonomies contain a higher num-
ber of senses than TL MFS as a result of the sense
induction process. This greater sense coverage leads
to the discovery of a higher number of correct taxo-
nomic relations between senses than TL MFS, hence
in a better performance. To conclude, our results
verify our hypothesis and suggest that the unsuper-
vised learning of word senses contributes to produc-
ing taxonomies with a higher similarity to the gold
standard ones than traditional distributional similar-
ity methods.
88
Figure 4: Performance analysis of the proposed method for p1 = 5 and different combinations of p2 and p3.
Despite that, our evaluation also shows that in
most cases HAC CMP is unable to exploit the in-
duced senses and performs worse than TL MFS,
HAC SNG and HAC AVG. This result was not ex-
pected, since HAC SNG employs a local criterion to
merge two clusters and does not consider the global
structure of the clusters, in effect, being biased to-
wards elongated clusters. The observation of the
gold standard taxonomies shows that they consist
both of cohyponym concepts which are expected
to be contextually related, but also of cohyponyms
which are not expected to appear in similar contexts.
For example, someone would expect a high similar-
ity between WAN, LAN, or between snood and tulle.
However, the same does not apply for snood and
cheesecloth or tulle and grillwork, because cheese-
cloth and grillwork appear in significantly different
contexts than snood and tulle. Despite that, all of
them are cohyponyms. This issue is more prevalent
in the speaker dataset, where concepts such as loud-
speaker, tannoy, woofer are expected to be contex-
tually related, while cohyponyms such as whisperer,
lecturer and interviewer are not. This means that the
gold standard taxonomies include elongated clusters
and explains the superior performance of HAC SNG.
This issue is not affecting HAC AVG, but it has a sig-
nificant effect on HAC CMP. Generally, HAC CMP
employs a non-local criterion by considering the di-
ameter of a candidate cluster. This results in com-
pact clusters with small diameters, as opposed to
elongated ones.
5 Conclusion
We presented an unsupervised method for taxonomy
learning that employs WSI to identify the senses of
target words and then builds a taxonomy of these
senses using HAC. We have shown that dealing with
polysemy by means of sense induction helps to de-
velop taxonomies that capture a higher number of
correct taxonomic relations than traditional distribu-
tional similarity methods, which associate each tar-
get word with one vector of features, in effect, merg-
ing its senses.
Acknowledgements
This work is supported by the European Commis-
sion via the EU FP7 INDECT project, Grant No.
218086, Research area: SEC-2007-1.2-01 Intelli-
gent Urban Environment Observation System.
89
References
E. Agirre and A. Soroa. 2007. SemEval-2007 Task
02: Evaluating Word Sense Induction and Discrimi-
nation Systems. In Proceedings of the Fourth Interna-
tional Workshop on Semantic Evaluations, pages 7?12,
Prague, Czech Republic.
R. A. Amsler. 1981. A Taxonomy for English Nouns and
Verbs. In Proceedings of the 19th ACL Conference,
pages 133?138, Stanford, California.
C. Biemann. 2006. Chinese Whispers - An Efficient
Graph Clustering Algorithm and its Application to
Natural Language Processing Problems. In Proceed-
ings of TextGraphs, pages 73?80, New York,USA.
P. Buitelaar, D. Olejnik, and M. Sintek. 2004. A Ptote?ge?
Plug-in for Ontology Extraction from Text Based on
Linguistic Analysis. In Proceedings of the 1st Euro-
pean Semantic Web Symposium, pages 31?44, Crete,
Greece. CEUR-WS.org.
S. A. Caraballo. 1999. Automatic Construction of a
Hypernym-labeled Noun Hierarchy from Text. In Pro-
ceedings of the 37th ACL Conference, pages 120?126,
College Park, Maryland.
P. Cimiano, A. Hotho, and S. Staab. 2004. Compar-
ing Conceptual, Divisive and Agglomerative Cluster-
ing for Learning Taxonomies from Text. In Proceed-
ings of the 16th ECAI Conference, pages 435?439, Va-
lencia, Spain.
P. Cimiano, A. Hotho, and S. Staab. 2005. Learning
Concept Hieararchies from Text Corpora Using For-
mal Concept Analysis. Journal of Artificial Intelli-
gence Research, 24:305?339.
P. Cimiano. 2006. Ontology Learning and Population
from Text: Algorithms, Evaluation and Applications.
Springer-Verlag New York, Inc., Secaucus, NJ, USA.
T. Dunning. 1993. Accurate Methods for the Statistics of
Surprise and Coincidence. Computational Linguistics,
19(1):61?74.
D. Faure and C. Ne?dellec. 1998. A Corpus-based Con-
ceptual Clustering Method for Verb Frames and On-
tology Acquisition. In LREC workshop on Adapting
lexical and corpus resources to sublanguages and ap-
plications, pages 5?12, Granada, Spain.
C. Fellbaum. 1998. Wordnet: An Electronic Lexical
Database. MIT Press, Cambridge, Massachusetts,
USA.
B. Ganter and R. Wille. 1999. Formal Concept Anal-
ysis: Mathematical Foundations. Springer-Verlag
New York, Inc., Secaucus, NJ, USA. Translator-C.
Franzke.
Z. Harris. 1968. Mathematical Structures of Language.
Wiley, New York, USA.
M. A. Hearst. 1992. Automatic Acquisition of Hy-
ponyms from Large Text Corpora. In Proceedings of
the 14th Coling Conference, pages 539?545, Nantes,
France.
C. H. Hwang. 1999. Incompletely and Imprecisely
Speaking: Using Dynamic Ontologies for Represent-
ing and Retrieving Information. In Proceedings of
the 6th International Workshop on Knowledge Repre-
sentation Meets Databases, pages 14?20, Linkoping,
Sweden. CEUR-WS.org.
B. King. 1967. Step-wise Clustering Procedures. Jour-
nal of the American Statistical Association, 69:86?
101.
I. P. Klapaftis and S. Manandhar. 2008. Word Sense In-
duction Using Graphs of Collocations. In Proceedings
of the 18th ECAI Conference, pages 298?302, Patras,
Greece. IOS Press.
A. Maedche and S. Staab. 2002. Measuring Similarity
between Ontologies. In Proceedings of the European
Conference on Knowledge Acquisition and Manage-
ment (EKAW), pages 251?263, London,UK. Springer-
Verlag.
D. Moldovan and A. Novischi. 2002. Lexical Chains
for Question Answering. In Proceedings of the 19th
Coling Conference, pages 1?7, Taipei, Taiwan.
R. Navigli and P. Velardi. 2004a. Learning Domain On-
tologies from Document Warehouses and Dedicated
web Sites. Computational Linguistics, 30(2):151?179.
R. Navigli and P. Velardi. 2004b. Structural Semantic In-
terconnection: a Knowledge-based Approach to Word
Sense Disambiguation. In Proceedings of Senseval-
3: Third International Workshop on the Evaluation of
Systems for the Semantic Analysis of Text, pages 179?
182, Barcelona, Spain.
M.L. Reinberger and P. Spyns. 2004. Discovering
Knowledge in Texts for the Learning of Dogma-
inspired Ontologies. In Proceedings of the ECAI
Workshop on Ontology Learning and Population,
pages 19?24, Valencia, Spain.
M. L. Reinberger, P. Spyns, W. Daelemans, and R. Meers-
man. 2003. Mining for Lexons: Applying Unsuper-
vised Learning Methods to create ontology bases. In
CoopIS/DOA/ODBASE, pages 803?819.
D. Sa?nchez and A. Moreno. 2005. Web-scale Taxon-
omy Learning. In Proceedings of the Workshop on
Learning and Extending Ontologies by using Machine
Learning methods, pages 53?60, Bonn, Germany.
P. H. A. Sneath and R. R. Sokal. 1973. Numerical Taxon-
omy, The Principles and Practice of Numerical Clas-
sification. W. H. Freeman, San Francisco, USA.
90
Proceedings of the 5th International Workshop on Semantic Evaluation, ACL 2010, pages 63?68,
Uppsala, Sweden, 15-16 July 2010.
c?2010 Association for Computational Linguistics
SemEval-2010 Task 14: Word Sense Induction & Disambiguation
Suresh Manandhar
Department of Computer Science
University of York, UK
Ioannis P. Klapaftis
Department of Computer Science
University of York, UK
Dmitriy Dligach
Department of Computer Science
University of Colorado, USA
Sameer S. Pradhan
BBN Technologies
Cambridge, USA
Abstract
This paper presents the description and
evaluation framework of SemEval-2010
Word Sense Induction & Disambiguation
task, as well as the evaluation results of 26
participating systems. In this task, partici-
pants were required to induce the senses of
100 target words using a training set, and
then disambiguate unseen instances of the
same words using the induced senses. Sys-
tems? answers were evaluated in: (1) an
unsupervised manner by using two clus-
tering evaluation measures, and (2) a su-
pervised manner in a WSD task.
1 Introduction
Word senses are more beneficial than simple word
forms for a variety of tasks including Information
Retrieval, Machine Translation and others (Pantel
and Lin, 2002). However, word senses are usually
represented as a fixed-list of definitions of a manu-
ally constructed lexical database. Several deficien-
cies are caused by this representation, e.g. lexical
databases miss main domain-specific senses (Pan-
tel and Lin, 2002), they often contain general defi-
nitions and suffer from the lack of explicit seman-
tic or contextual links between concepts (Agirre
et al, 2001). More importantly, the definitions of
hand-crafted lexical databases often do not reflect
the exact meaning of a target word in a given con-
text (V?eronis, 2004).
Unsupervised Word Sense Induction (WSI)
aims to overcome these limitations of hand-
constructed lexicons by learning the senses of a
target word directly from text without relying on
any hand-crafted resources. The primary aim of
SemEval-2010 WSI task is to allow comparison
of unsupervised word sense induction and disam-
biguation systems.
The target word dataset consists of 100 words,
50 nouns and 50 verbs. For each target word, par-
ticipants were provided with a training set in or-
der to learn the senses of that word. In the next
step, participating systems were asked to disam-
biguate unseen instances of the same words using
their learned senses. The answers of the systems
were then sent to organisers for evaluation.
2 Task description
Figure 1 provides an overview of the task. As
can be observed, the task consisted of three
separate phases. In the first phase, train-
ing phase, participating systems were provided
with a training dataset that consisted of a
set of target word (noun/verb) instances (sen-
tences/paragraphs). Participants were then asked
to use this training dataset to induce the senses
of the target word. No other resources were al-
lowed with the exception of NLP components for
morphology and syntax. In the second phase,
testing phase, participating systems were pro-
vided with a testing dataset that consisted of a
set of target word (noun/verb) instances (sen-
tences/paragraphs). Participants were then asked
to tag (disambiguate) each testing instance with
the senses induced during the training phase. In
the third and final phase, the tagged test instances
were received by the organisers in order to evalu-
ate the answers of the systems in a supervised and
an unsupervised framework. Table 1 shows the to-
tal number of target word instances in the training
and testing set, as well as the average number of
senses in the gold standard.
The main difference of the SemEval-2010 as
compared to the SemEval-2007 sense induction
task is that the training and testing data are treated
separately, i.e the testing data are only used for
sense tagging, while the training data are only used
63
Figure 1: Training, testing and evaluation phases of SemEval-2010 Task 14
Training set Testing set Senses (#)
All 879807 8915 3.79
Nouns 716945 5285 4.46
Verbs 162862 3630 3.12
Table 1: Training & testing set details
for sense induction. Treating the testing data as
new unseen instances ensures a realistic evalua-
tion that allows to evaluate the clustering models
of each participating system.
The evaluation framework of SemEval-2010
WSI task considered two types of evaluation.
In the first one, unsupervised evaluation, sys-
tems? answers were evaluated according to: (1) V-
Measure (Rosenberg and Hirschberg, 2007), and
(2) paired F-Score (Artiles et al, 2009). Nei-
ther of these measures were used in the SemEval-
2007 WSI task. Manandhar & Klapaftis (2009)
provide more details on the choice of this evalu-
ation setting and its differences with the previous
evaluation. The second type of evaluation, super-
vised evaluation, follows the supervised evalua-
tion of the SemEval-2007 WSI task (Agirre and
Soroa, 2007). In this evaluation, induced senses
are mapped to gold standard senses using a map-
ping corpus, and systems are then evaluated in a
standard WSD task.
2.1 Training dataset
The target word dataset consisted of 100 words,
i.e. 50 nouns and 50 verbs. The training dataset
for each target noun or verb was created by follow-
ing a web-based semi-automatic method, similar
to the method for the construction of Topic Signa-
tures (Agirre et al, 2001). Specifically, for each
WordNet (Fellbaum, 1998) sense of a target word,
we created a query of the following form:
<Target Word> AND <Relative Set>
The <Target Word> consisted of the target
word stem. The <Relative Set> consisted of a
disjunctive set of word lemmas that were related
Word Query
Sense
Sense 1 failure AND (loss OR nonconformity OR test
OR surrender OR ?force play? OR ...)
Sense 2 failure AND (ruination OR flop OR bust
OR stall OR ruin OR walloping OR ...)
Table 2: Training set creation: example queries for
target word failure
to the target word sense for which the query was
created. The relations considered were WordNet?s
hypernyms, hyponyms, synonyms, meronyms and
holonyms. Each query was manually checked by
one of the organisers to remove ambiguous words.
The following example shows the query created
for the first
1
and second
2
WordNet sense of the
target noun failure.
The created queries were issued to Yahoo!
search API
3
and for each query a maximum of
1000 pages were downloaded. For each page we
extracted fragments of text that occurred in <p>
</p> html tags and contained the target word
stem. In the final stage, each extracted fragment of
text was POS-tagged using the Genia tagger (Tsu-
ruoka and Tsujii, 2005) and was only retained, if
the POS of the target word in the extracted text
matched the POS of the target word in our dataset.
2.2 Testing dataset
The testing dataset consisted of instances of the
same target words from the training dataset. This
dataset is part of OntoNotes (Hovy et al, 2006).
We used the sense-tagged dataset in which sen-
tences containing target word instances are tagged
with OntoNotes (Hovy et al, 2006) senses. The
texts come from various news sources including
CNN, ABC and others.
1
An act that fails
2
An event that does not accomplish its intended purpose
3
http://developer.yahoo.com/search/ [Access:10/04/2010]
64
G1
G
2
G
3
C
1
10 10 15
C
2
20 50 0
C
3
1 10 60
C
4
5 0 0
Table 3: Clusters & GS senses matrix.
3 Evaluation framework
For the purposes of this section we provide an ex-
ample (Table 3) in which a target word has 181
instances and 3 GS senses. A system has gener-
ated a clustering solution with 4 clusters covering
all instances. Table 3 shows the number of com-
mon instances between clusters and GS senses.
3.1 Unsupervised evaluation
This section presents the measures of unsuper-
vised evaluation, i.e V-Measure (Rosenberg and
Hirschberg, 2007) and (2) paired F-Score (Artiles
et al, 2009).
3.1.1 V-Measure evaluation
Let w be a target word with N instances (data
points) in the testing dataset. Let K = {C
j
|j =
1 . . . n} be a set of automatically generated clus-
ters grouping these instances, and S = {G
i
|i =
1 . . .m} the set of gold standard classes contain-
ing the desirable groupings of w instances.
V-Measure (Rosenberg and Hirschberg, 2007)
assesses the quality of a clustering solution by ex-
plicitly measuring its homogeneity and its com-
pleteness. Homogeneity refers to the degree that
each cluster consists of data points primarily be-
longing to a single GS class, while completeness
refers to the degree that each GS class consists of
data points primarily assigned to a single cluster
(Rosenberg and Hirschberg, 2007). Let h be ho-
mogeneity and c completeness. V-Measure is the
harmonic mean of h and c, i.e. VM =
2?h?c
h+c
.
Homogeneity. The homogeneity, h, of a clus-
tering solution is defined in Formula 1, where
H(S|K) is the conditional entropy of the class
distribution given the proposed clustering and
H(S) is the class entropy.
h =
{
1, if H(S) = 0
1?
H(S|K)
H(S)
, otherwise
(1)
H(S) = ?
|S|
?
i=1
?
|K|
j=1
a
ij
N
log
?
|K|
j=1
a
ij
N
(2)
H(S|K) = ?
|K|
?
j=1
|S|
?
i=1
a
ij
N
log
a
ij
?
|S|
k=1
a
kj
(3)
When H(S|K) is 0, the solution is perfectly
homogeneous, because each cluster only contains
data points that belong to a single class. How-
ever in an imperfect situation, H(S|K) depends
on the size of the dataset and the distribution of
class sizes. Hence, instead of taking the raw con-
ditional entropy, V-Measure normalises it by the
maximum reduction in entropy the clustering in-
formation could provide, i.e. H(S). When there
is only a single class (H(S) = 0), any clustering
would produce a perfectly homogeneous solution.
Completeness. Symmetrically to homogeneity,
the completeness, c, of a clustering solution is de-
fined in Formula 4, where H(K|S) is the condi-
tional entropy of the cluster distribution given the
class distribution and H(K) is the clustering en-
tropy. When H(K|S) is 0, the solution is perfectly
complete, because all data points of a class belong
to the same cluster.
For the clustering example in Table 3, homo-
geneity is equal to 0.404, completeness is equal to
0.37 and V-Measure is equal to 0.386.
c =
{
1, if H(K) = 0
1?
H(K|S)
H(K)
, otherwise
(4)
H(K) = ?
|K|
?
j=1
?
|S|
i=1
a
ij
N
log
?
|S|
i=1
a
ij
N
(5)
H(K|S) = ?
|S|
?
i=1
|K|
?
j=1
a
ij
N
log
a
ij
?
|K|
k=1
a
ik
(6)
3.1.2 Paired F-Score evaluation
In this evaluation, the clustering problem is trans-
formed into a classification problem. For each
cluster C
i
we generate
(
|C
i
|
2
)
instance pairs, where
|C
i
| is the total number of instances that belong to
cluster C
i
. Similarly, for each GS class G
i
we gen-
erate
(
|G
i
|
2
)
instance pairs, where |G
i
| is the total
number of instances that belong to GS class G
i
.
Let F (K) be the set of instance pairs that ex-
ist in the automatically induced clusters and F (S)
be the set of instance pairs that exist in the gold
standard. Precision can be defined as the number
of common instance pairs between the two sets to
the total number of pairs in the clustering solu-
tion (Equation 7), while recall can be defined as
the number of common instance pairs between the
two sets to the total number of pairs in the gold
65
standard (Equation 8). Finally, precision and re-
call are combined to produce the harmonic mean
(FS =
2?P ?R
P+R
).
P =
|F (K) ? F (S)|
|F (K)|
(7)
R =
|F (K) ? F (S)|
|F (S)|
(8)
For example in Table 3, we can generate
(
35
2
)
in-
stance pairs for C
1
,
(
70
2
)
for C
2
,
(
71
2
)
for C
3
and
(
5
2
)
for C
4
, resulting in a total of 5505 instance
pairs. In the same vein, we can generate
(
36
2
)
in-
stance pairs for G
1
,
(
70
2
)
for G
2
and
(
75
2
)
for G
3
. In
total, the GS classes contain 5820 instance pairs.
There are 3435 common instance pairs, hence pre-
cision is equal to 62.39%, recall is equal to 59.09%
and paired F-Score is equal to 60.69%.
3.2 Supervised evaluation
In this evaluation, the testing dataset is split into a
mapping and an evaluation corpus. The first one
is used to map the automatically induced clusters
to GS senses, while the second is used to evaluate
methods in a WSD setting. This evaluation fol-
lows the supervised evaluation of SemEval-2007
WSI task (Agirre and Soroa, 2007), with the dif-
ference that the reported results are an average
of 5 random splits. This repeated random sam-
pling was performed to avoid the problems of the
SemEval-2007 WSI challenge, in which different
splits were providing different system rankings.
Let us consider the example in Table 3 and as-
sume that this matrix has been created by using the
mapping corpus. Table 3 shows that C
1
is more
likely to be associated with G
3
, C
2
is more likely
to be associated with G
2
, C
3
is more likely to be
associated with G
3
and C
4
is more likely to be as-
sociated with G
1
. This information can be utilised
to map the clusters to GS senses.
Particularly, the matrix shown in Table 3 is nor-
malised to produce a matrix M , in which each
entry depicts the estimated conditional probabil-
ity P (G
i
|C
j
). Given an instance I of tw from
the evaluation corpus, a row cluster vector IC is
created, in which each entry k corresponds to the
score assigned to C
k
to be the winning cluster for
instance I . The product of IC and M provides a
row sense vector, IG, in which the highest scor-
ing entry a denotes that G
a
is the winning sense.
For example, if we produce the row cluster vector
[C
1
= 0.8, C
2
= 0.1, C
3
= 0.1, C
4
= 0.0], and
System VM (%) VM (%) VM (%) #Cl
(All) (Nouns) (Verbs)
Hermit 16.2 16.7 15.6 10.78
UoY 15.7 20.6 8.5 11.54
KSU KDD 15.7 18 12.4 17.5
Duluth-WSI 9 11.4 5.7 4.15
Duluth-WSI-SVD 9 11.4 5.7 4.15
Duluth-R-110 8.6 8.6 8.5 9.71
Duluth-WSI-Co 7.9 9.2 6 2.49
KCDC-PCGD 7.8 7.3 8.4 2.9
KCDC-PC 7.5 7.7 7.3 2.92
KCDC-PC-2 7.1 7.7 6.1 2.93
Duluth-Mix-Narrow-Gap 6.9 8 5.1 2.42
KCDC-GD-2 6.9 6.1 8 2.82
KCDC-GD 6.9 5.9 8.5 2.78
Duluth-Mix-Narrow-PK2 6.8 7.8 5.5 2.68
Duluth-MIX-PK2 5.6 5.8 5.2 2.66
Duluth-R-15 5.3 5.4 5.1 4.97
Duluth-WSI-Co-Gap 4.8 5.6 3.6 1.6
Random 4.4 4.2 4.6 4
Duluth-R-13 3.6 3.5 3.7 3
Duluth-WSI-Gap 3.1 4.2 1.5 1.4
Duluth-Mix-Gap 3 2.9 3 1.61
Duluth-Mix-Uni-PK2 2.4 0.8 4.7 2.04
Duluth-R-12 2.3 2.2 2.5 2
KCDC-PT 1.9 1 3.1 1.5
Duluth-Mix-Uni-Gap 1.4 0.2 3 1.39
KCDC-GDC 7 6.2 7.8 2.83
MFS 0 0 0 1
Duluth-WSI-SVD-Gap 0 0 0.1 1.02
Table 4: V-Measure unsupervised evaluation
multiply it with the normalised matrix of Table 3,
then we would get a row sense vector in which G
3
would be the winning sense with a score equal to
0.43.
4 Evaluation results
In this section, we present the results of the 26
systems along with two baselines. The first base-
line, Most Frequent Sense (MFS), groups all test-
ing instances of a target word into one cluster. The
second baseline, Random, randomly assigns an in-
stance to one out of four clusters. The number
of clusters of Random was chosen to be roughly
equal to the average number of senses in the GS.
This baseline is executed five times and the results
are averaged.
4.1 Unsupervised evaluation
Table 4 shows the V-Measure (VM) performance
of the 26 systems participating in the task. The last
column shows the number of induced clusters of
each system in the test set.The MFS baseline has a
V-Measure equal to 0, since by definition its com-
pleteness is 1 and homogeneity is 0. All systems
outperform this baseline, apart from one, whose
V-Measure is equal to 0. Regarding the Random
baseline, we observe that 17 perform better, which
indicates that they have learned useful information
better than chance.
Table 4 also shows that V-Measure tends to
favour systems producing a higher number of clus-
66
System FS (%) FS (%) FS (%) #Cl
(All) (Nouns) (Verbs)
MFS 63.5 57.0 72.7 1
Duluth-WSI-SVD-Gap 63.3 57.0 72.4 1.02
KCDC-PT 61.8 56.4 69.7 1.5
KCDC-GD 59.2 51.6 70.0 2.78
Duluth-Mix-Gap 59.1 54.5 65.8 1.61
Duluth-Mix-Uni-Gap 58.7 57.0 61.2 1.39
KCDC-GD-2 58.2 50.4 69.3 2.82
KCDC-GDC 57.3 48.5 70.0 2.83
Duluth-Mix-Uni-PK2 56.6 57.1 55.9 2.04
KCDC-PC 55.5 50.4 62.9 2.92
KCDC-PC-2 54.7 49.7 61.7 2.93
Duluth-WSI-Gap 53.7 53.4 53.9 1.4
KCDC-PCGD 53.3 44.8 65.6 2.9
Duluth-WSI-Co-Gap 52.6 53.3 51.5 1.6
Duluth-MIX-PK2 50.4 51.7 48.3 2.66
UoY 49.8 38.2 66.6 11.54
Duluth-Mix-Narrow-Gap 49.7 47.4 51.3 2.42
Duluth-WSI-Co 49.5 50.2 48.2 2.49
Duluth-Mix-Narrow-PK2 47.8 37.1 48.2 2.68
Duluth-R-12 47.8 44.3 52.6 2
Duluth-WSI-SVD 41.1 37.1 46.7 4.15
Duluth-WSI 41.1 37.1 46.7 4.15
Duluth-R-13 38.4 36.2 41.5 3
KSU KDD 36.9 24.6 54.7 17.5
Random 31.9 30.4 34.1 4
Duluth-R-15 27.6 26.7 28.9 4.97
Hermit 26.7 24.4 30.1 10.78
Duluth-R-110 16.1 15.8 16.4 9.71
Table 5: Paired F-Score unsupervised evaluation
ters than the number of GS senses, although V-
Measure does not increase monotonically with the
number of clusters increasing. For that reason,
we introduced the second unsupervised evaluation
measure (paired F-Score) that penalises systems
when they produce: (1) a higher number of clus-
ters (low recall) or (2) a lower number of clusters
(low precision), than the GS number of senses.
Table 5 shows the performance of systems us-
ing the second unsupervised evaluation measure.
In this evaluation, we observe that most of the sys-
tems perform better than Random. Despite that,
none of the systems outperform the MFS baseline.
It seems that systems generating a smaller number
of clusters than the GS number of senses are bi-
ased towards the MFS, hence they are not able to
perform better. On the other hand, systems gen-
erating a higher number of clusters are penalised
by this measure. Systems generating a number of
clusters roughly the same as the GS tend to con-
flate the GS senses lot more than the MFS.
4.2 Supervised evaluation results
Table 6 shows the results of this evaluation for a
80-20 test set split, i.e. 80% for mapping and 20%
for evaluation. The last columns shows the aver-
age number of GS senses identified by each sys-
tem in the five splits of the evaluation datasets.
Overall, 14 systems outperform the MFS, while 17
of them perform better than Random. The ranking
of systems in nouns and verbs is different. For in-
System SR (%) SR (%) SR (%) #S
(All) (Nouns) (Verbs)
UoY 62.4 59.4 66.8 1.51
Duluth-WSI 60.5 54.7 68.9 1.66
Duluth-WSI-SVD 60.5 54.7 68.9 1.66
Duluth-WSI-Co-Gap 60.3 54.1 68.6 1.19
Duluth-WSI-Co 60.8 54.7 67.6 1.51
Duluth-WSI-Gap 59.8 54.4 67.8 1.11
KCDC-PC-2 59.8 54.1 68.0 1.21
KCDC-PC 59.7 54.6 67.3 1.39
KCDC-PCGD 59.5 53.3 68.6 1.47
KCDC-GDC 59.1 53.4 67.4 1.34
KCDC-GD 59.0 53.0 67.9 1.33
KCDC-PT 58.9 53.1 67.4 1.08
KCDC-GD-2 58.7 52.8 67.4 1.33
Duluth-WSI-SVD-Gap 58.7 53.2 66.7 1.01
MFS 58.7 53.2 66.6 1
Duluth-R-12 58.5 53.1 66.4 1.25
Hermit 58.3 53.6 65.3 2.06
Duluth-R-13 58.0 52.3 66.4 1.46
Random 57.3 51.5 65.7 1.53
Duluth-R-15 56.8 50.9 65.3 1.61
Duluth-Mix-Narrow-Gap 56.6 48.1 69.1 1.43
Duluth-Mix-Narrow-PK2 56.1 47.5 68.7 1.41
Duluth-R-110 54.8 48.3 64.2 1.94
KSU KDD 52.2 46.6 60.3 1.69
Duluth-MIX-PK2 51.6 41.1 67.0 1.23
Duluth-Mix-Gap 50.6 40.0 66.0 1.01
Duluth-Mix-Uni-PK2 19.3 1.8 44.8 0.62
Duluth-Mix-Uni-Gap 18.7 1.6 43.8 0.56
Table 6: Supervised recall (SR) (test set split:80%
mapping, 20% evaluation)
stance, the highest ranked system in nouns is UoY,
while in verbs Duluth-Mix-Narrow-Gap. It seems
that depending on the part-of-speech of the target
word, different algorithms, features and parame-
ters? tuning have different impact.
The supervised evaluation changes the distri-
bution of clusters by mapping each cluster to a
weighted vector of senses. Hence, it can poten-
tially favour systems generating a high number of
homogeneous clusters. For that reason, we applied
a second testing set split, where 60% of the testing
corpus was used for mapping and 40% for eval-
uation. Reducing the size of the mapping corpus
allows us to observe, whether the above statement
is correct, since systems with a high number of
clusters would suffer from unreliable mapping.
Table 7 shows the results of the second super-
vised evaluation. The ranking of participants did
not change significantly, i.e. we observe only dif-
ferent rankings among systems belonging to the
same participant. Despite that, Table 7 also shows
that the reduction of the mapping corpus has a dif-
ferent impact on systems generating a larger num-
ber of clusters than the GS number of senses.
For instance, UoY that generates 11.54 clusters
outperformed the MFS by 3.77% in the 80-20 split
and by 3.71% in the 60-40 split. The reduction of
the mapping corpus had a minimal impact on its
performance. In contrast, KSU KDD that gener-
ates 17.5 clusters was below the MFS by 6.49%
67
System SR (%) SR (%) SR (%) #S
(All) (Nouns) (Verbs)
UoY 62.0 58.6 66.8 1.66
Duluth-WSI-Co 60.1 54.6 68.1 1.56
Duluth-WSI-Co-Gap 59.5 53.5 68.3 1.2
Duluth-WSI-SVD 59.5 53.5 68.3 1.73
Duluth-WSI 59.5 53.5 68.3 1.73
Duluth-WSI-Gap 59.3 53.2 68.2 1.11
KCDC-PCGD 59.1 52.6 68.6 1.54
KCDC-PC-2 58.9 53.4 67.0 1.25
KCDC-PC 58.9 53.6 66.6 1.44
KCDC-GDC 58.3 52.1 67.3 1.41
KCDC-GD 58.3 51.9 67.6 1.42
MFS 58.3 52.5 66.7 1
KCDC-PT 58.3 52.2 67.1 1.11
Duluth-WSI-SVD-Gap 58.2 52.5 66.7 1.01
KCDC-GD-2 57.9 51.7 67.0 1.44
Duluth-R-12 57.7 51.7 66.4 1.27
Duluth-R-13 57.6 51.1 67.0 1.48
Hermit 57.3 52.5 64.2 2.27
Duluth-R-15 56.5 50.0 66.1 1.76
Random 56.5 50.2 65.7 1.65
Duluth-Mix-Narrow-Gap 56.2 47.7 68.6 1.51
Duluth-Mix-Narrow-PK2 55.7 46.9 68.5 1.51
Duluth-R-110 53.6 46.7 63.6 2.18
Duluth-MIX-PK2 50.5 39.7 66.1 1.31
KSU KDD 50.4 44.3 59.4 1.92
Duluth-Mix-Gap 49.8 38.9 65.6 1.04
Duluth-Mix-Uni-PK2 19.1 1.8 44.4 0.63
Duluth-Mix-Uni-Gap 18.9 1.5 44.2 0.56
Table 7: Supervised recall (SR) (test set split:60%
mapping, 40% evaluation)
in the 80-20 split and by 7.83% in the 60-40 split.
The reduction of the mapping corpus had a larger
impact in this case. This result indicates that the
performance in this evaluation also depends on the
distribution of instances within the clusters. Sys-
tems generating a skewed distribution, in which a
small number of homogeneous clusters tag the ma-
jority of instances and a larger number of clusters
tag only a few instances, are likely to have a bet-
ter performance than systems that produce a more
uniform distribution.
5 Conclusion
We presented the description, evaluation frame-
work and assessment of systems participating in
the SemEval-2010 sense induction task. The eval-
uation has shown that the current state-of-the-art
lacks unbiased measures that objectively evaluate
clustering.
The results of systems have shown that their
performance in the unsupervised and supervised
evaluation settings depends on cluster granularity
along with the distribution of instances within the
clusters. Our future work will focus on the assess-
ment of sense induction on a task-oriented basis as
well as on clustering evaluation.
Acknowledgements
We gratefully acknowledge the support of the EU
FP7 INDECT project, Grant No. 218086, the Na-
tional Science Foundation Grant NSF-0715078,
Consistent Criteria for Word Sense Disambigua-
tion, and the GALE program of the Defense Ad-
vanced Research Projects Agency, Contract No.
HR0011-06-C-0022, a subcontract from the BBN-
AGILE Team.
References
Eneko Agirre and Aitor Soroa. 2007. SemEval-2007
Task 02: Evaluating Word Sense Induction and Dis-
crimination Systems. In Proceedings of SemEval-
2007, pages 7?12, Prague, Czech Republic. ACL.
Eneko Agirre, Olatz Ansa, David Martinez, and Eduard
Hovy. 2001. Enriching Wordnet Concepts With
Topic Signatures. ArXiv Computer Science e-prints.
Javier Artiles, Enrique Amig?o, and Julio Gonzalo.
2009. The role of named entities in web people
search. In Proceedings of EMNLP, pages 534?542.
ACL.
Christiane Fellbaum. 1998. Wordnet: An Electronic
Lexical Database. MIT Press, Cambridge, Mas-
sachusetts, USA.
Eduard Hovy, Mitchell Marcus, Martha Palmer, Lance
Ramshaw, and Ralph Weischedel. 2006. Ontonotes:
the 90% solution. In Proceedings of NAACL, Com-
panion Volume: Short Papers on XX, pages 57?60.
ACL.
Suresh Manandhar and Ioannis P. Klapaftis. 2009.
Semeval-2010 Task 14: Evaluation Setting for Word
Sense Induction & Disambiguation Systems. In
DEW ?09: Proceedings of the Workshop on Se-
mantic Evaluations: Recent Achievements and Fu-
ture Directions, pages 117?122, Boulder, Colorado,
USA. ACL.
Patrick Pantel and Dekang Lin. 2002. Discovering
Word Senses from Text. In KDD ?02: Proceedings
of the 8th ACM SIGKDD Conference, pages 613?
619, New York, NY, USA. ACM.
Andrew Rosenberg and Julia Hirschberg. 2007. V-
measure: A Conditional Entropy-based External
Cluster Evaluation Measure. In Proceedings of the
2007 EMNLP-CoNLL Joint Conference, pages 410?
420, Prague, Czech Republic.
Yoshimasa Tsuruoka and Jun??chi Tsujii. 2005. Bidi-
rectional Inference With the Easiest-first Strategy
for Tagging Sequence Data. In Proceedings of
the HLT-EMNLP Joint Conference, pages 467?474,
Morristown, NJ, USA.
Jean V?eronis. 2004. Hyperlex: Lexical Cartography
for Information Retrieval. Computer Speech & Lan-
guage, 18(3):223?252.
68
Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 2: Seventh International Workshop on Semantic
Evaluation (SemEval 2013), pages 290?299, Atlanta, Georgia, June 14-15, 2013. c?2013 Association for Computational Linguistics
SemEval-2013 Task 13:
Word Sense Induction for Graded and Non-Graded Senses
David Jurgens
Dipartimento di Informatica
Sapienza Universita` di Roma
jurgens@di.uniroma1.it
Ioannis Klapaftis
Search Technology Center Europe
Microsoft
ioannisk@microsoft.com
Abstract
Most work on word sense disambiguation has
assumed that word usages are best labeled
with a single sense. However, contextual am-
biguity or fine-grained senses can potentially
enable multiple sense interpretations of a us-
age. We present a new SemEval task for evalu-
ating Word Sense Induction and Disambigua-
tion systems in a setting where instances may
be labeled with multiple senses, weighted by
their applicability. Four teams submitted nine
systems, which were evaluated in two settings.
1 Introduction
Word Sense Disambiguation (WSD) attempts to
identify which of a word?s meanings applies in a
given context. A long-standing task, WSD is fun-
damental to many NLP applications (Navigli, 2009).
Typically, each usage of a word is treated as express-
ing only a single sense. However, contextual ambi-
guity as well as the relatedness of certain meanings
can potentially elicit multiple sense interpretations.
Recent work has shown that annotators find multi-
ple applicable senses in a given target word context
when using fine-grained sense inventories such as
WordNet (Ve?ronis, 1998; Murray and Green, 2004;
Erk et al, 2009; Passonneau et al, 2012b; Jurgens,
2013; Navigli et al, 2013). Such contexts would be
better annotated with multiple sense labels, weight-
ing each sense according to its applicability (Erk et
al., 2009; Jurgens, 2013), in effect allowing ambigu-
ity or multiple interpretations to be explicitly mod-
eled. Accordingly, the first goal of this task is to
evaluate WSD systems in a setting where instances
may be labeled with one or more senses, weighted
by their applicability.
WSD methods are ultimately defined and poten-
tially restricted by their choice in sense inventory;
for example, a sense inventory may have insufficient
sense-annotated data to build WSD systems for spe-
cific types of text (e.g., social media), or the inven-
tory may lack domain-specific senses. Word Sense
Induction (WSI) has been proposed as a method for
overcoming such limitations by learning the senses
automatically from text. In essence, a WSI algo-
rithm acts as a lexicographer by grouping word us-
ages according to their shared meaning. The sec-
ond goal of this task is to assess the performance of
WSI algorithms when they are able to model multi-
ple meanings of a usage with graded senses.
Task 12 focuses on disambiguating senses for 50
target lemmas: 20 nouns, 20 verbs, and 10 adjectives
(Sec. 2). Since the Task evaluates only unsupervised
systems, no training data was provided; however, to
enable more comparison, Unsupervised WSD sys-
tems were also allowed to participate. Participat-
ing systems were evaluated in two settings (Sec. 3),
depending on whether they used induced senses or
WordNet 3.1 senses for their annotations. The re-
sults (Sec. 5) demonstrate a substantial improvement
over the competitive most frequent sense baseline.
2 Task Description
This task required participating systems to annotate
instances of nouns, verb, and adjectives using Word-
Net 3.1 (Fellbaum, 1998), which was selected due
to its fine-grained senses. Participants could label
each instance with one or more senses, weighting
290
We all are relieved to lay aside our fight-or-flight reflexes and to commemorate our births from out of the dark
centers of the women, to feel the complexity of our love and frustration with each other, to stretch our cognition to
encompass the thoughts of every entity we know.
dark%3:00:01:: ? devoid of or deficient in light or brightness; shadowed or black
dark%3:00:00:: ? secret
I ask because my practice has always been to allow about five minutes grace, then remove it.
ask%2:32:02:: ? direct or put; seek an answer to
ask%2:32:04:: ? address a question to and expect an answer from
Table 1: Example instances with multiple senses due to intended double meanings (top) or contextual am-
biguity (bottom). Senses are specified using their WordNet 3.1 sense keys.
each by their applicability. Table 1 highlights two
example contexts where multiple senses apply. The
first example shows a case of an intentional dou-
ble meaning that evokes both the physical aspect of
dark.a as being devoid of light and the causal re-
sult of being secret. In contrast, the second example
shows a case of multiple interpretations from ambi-
guity; a different preceding context could generate
the alternate interpretations ?I ask [you] because?
(sense ask%2:32:04::) or ?I ask [the question]
because? (sense ask%2:32:02::).
2.1 Data
Three datasets were provided with the task. The trial
dataset provided weighted word sense annotations
using the data gathered by Erk et al (2009). The
trial dataset consisted of 50 contexts for eight words,
where each context was labeled with WordNet 3.0
sense ratings from three untrained lexicographers.
Due to the unsupervised nature of the task, partic-
ipants were not provided with sense-labeled training
data. However, WSI systems were provided with the
ukWaC corpus (Baroni et al, 2009) to use in induc-
ing senses. Previous SemEval WSI tasks had pro-
vided participants with corpora specific to the task?s
target terms; in contrast, this task opted to use a large
corpus to enable WSI methods that require corpus-
wide statistics, e.g., statistical associations.
Test data was drawn from the Open American
National Corpus (Ide and Suderman, 2004, OANC)
across a variety of genres and from both the spoken
and written portions of the corpus, summarized in
Table 2. All contexts were manually inspected to en-
sure that the lemma being disambiguated was of the
correct part of speech and had an interpretation that
matched at least one WordNet 3.1 sense. This filter-
ing also removed instances that were in a colloca-
tion, or had an idiomatic meaning. Ultimately, 4664
contexts were used as test data, with a minimum of
22 and a maximum of 100 contexts per word.
2.2 Sense Annotation
Recent work proposes to gather sense annotations
using crowdsourcing in order to reduce the time
and cost of acquiring sense-annotated corpora (Bie-
mann and Nygaard, 2010; Passonneau et al, 2012b;
Rumshisky et al, 2012; Jurgens, 2013). There-
fore, we initially annotated the Task?s data using the
method of Jurgens (2013), where workers on Ama-
zon Mechanical Turk (AMT) rated all senses of a
word on a Likert scale from one to five, indicat-
ing the sense does not apply at all or completely
applies, respectively. Twenty annotators were as-
signed per instance, with their ratings combined by
selecting the most frequent rating. However, we
found that while the annotators achieved moderate
inter-annotator agreement (IAA), the resulting an-
notations were not of high enough quality to use in
the Task?s evaluations. Specifically, for some senses
and contexts, AMT annotators required more infor-
mation about sense distinctions than was feasible to
integrate into the AMT setting, which led to consis-
tent but incorrect sense assignments.
Therefore, the test data was annotated by the two
authors, with the first author annotating all instances
and the second author annotating a 10% sample of
each lemma?s instances in order to calculate IAA.
IAA was calculated using Krippendorff?s ? (Krip-
pendorff, 1980; Artstein and Poesio, 2008), which is
an agreement measurement that adjusts for chance,
291
Spoken Written
Genre Face-to-face Telephone Fiction Journal Letters Non-fiction Technical Travel Guides All
Instances 52 699 127 2403 103 477 611 192 4664
Tokens 1742 30,700 3438 69,479 2238 11,780 17,337 4490 141,204
Mean senses/inst. 1.17 1.08 1.15 1.13 1.31 1.10 1.11 1.11 1.12
Table 2: Test data used in Task 12, divided according to source type
ranging in (?1, 1] for interval data, where 1 indi-
cates perfect agreement and -1 indicates systematic
disagreement; two random annotations have an ex-
pected ? of zero. We treat each sense and instance
combination as a separate item to rate. The total IAA
for the dataset was 0.504, and on individual words,
ranged from 0.903 for number.n to 0.00 for win.v.
While this IAA is less than the 0.8 recommended by
Krippendorff (2004), it is consistent with the IAA
distribution for the sense annotations of MASC on
other parts of the OANC corpus: Passonneau et al
(2012a) reports an ? of 0.88 to -0.02 with the MASI
statistic (Passonneau et al, 2006).
Table 2 summarizes the annotation statistics for
the Task?s data. The annotation process resulted in
far fewer senses per instance in the trial data, which
we attribute to using trained annotators. An analysis
across the corpora genres showed that the multiple-
sense annotation rates were similar. Due to the vari-
ety of contextual sources, all lemmas were observed
with at least two distinct senses.
3 Evaluation
We adopt a two-part evaluation setting used in pre-
vious SemEval WSI and WSD tasks (Agirre and
Soroa, 2007; Manandhar et al, 2010). The first eval-
uation uses a traditional WSD task that directly com-
pares WordNet sense labels. For WSI systems, their
induced sense labels are converted to WordNet 3.1
labels via a mapping procedure. The second evalu-
ation performs a direct comparison of the two sense
inventories using clustering comparisons.
3.1 WSD Task
In the first evaluation, we adopt a WSD task with
three objectives: (1) detecting which senses are ap-
plicable, (2) ranking senses by their applicability,
and (3) measuring agreement in applicability rat-
ings with human annotators. Each objectives uses
a specific measurement: (1) the Jaccard Index, (2)
positionally-weighted Kendall?s ? similarity, and
(3) a weighted variant of Normalized Discounted
Cumulative Gain, respectively. Each measure is
bounded in [0, 1], where 1 indicates complete agree-
ment with the gold standard. We generalize the tra-
ditional definition of WSD Recall such that it mea-
sures the average score for each measure across all
instances, including those not labeled by the system.
Systems are ultimately scored using the F1 measure
between each objective?s measure and Recall.
3.1.1 Transforming Induced Sense Labels
In the WSD setting, induced sense labels may be
transformed into a reference inventory (e.g., Word-
Net 3.1) using a sense mapping procedure. We fol-
low the 80/20 setup of Manandhar et al (2010),
where the corpus is randomly divided into five par-
titions, four of which are used to learn the sense
mapping; the sense labels for the held-out partition
are then converted and compared with the gold stan-
dard. This process is repeated so that each partition
is tested once. For learning the sense mapping func-
tion, we use the distribution mapping technique of
Jurgens (2012), which takes into account the sense
applicability weights in both labelings.
3.1.2 Jaccard Index
Given two sets of sense labels for an instance,
X and Y , the Jaccard Index is used to measure the
agreement: |X?Y ||X?Y | . The Jaccard Index is maximized
when X and Y use identical labels, and is mini-
mized when the sets of sense labels are disjoint.
3.1.3 Positionally-Weighted Kendall?s ?
Rank correlations have been proposed for evalu-
ating a system?s ability to order senses by applicabil-
ity; in previous work, both Erk and McCarthy (2009)
and Jurgens (2012) propose rank correlation coeffi-
cients that assume all positions in the ranking are
equally important. However, in the case of graded
292
sense evaluation, often only a few senses are appli-
cable, with the applicability ratings of the remain-
ing senses being relatively inconsequential. There-
fore, we consider an alternate rank scoring based on
Kumar and Vassilvitskii (2010), which weights the
penalty of reordering the lower positions less than
the penalty of reordering the first ranks.
Kendall?s ? distance, K, is a measure of the
number of item position swaps required to make
two sequences identical. Kumar and Vassilvitskii
(2010) extend this distance definition using a vari-
able penalty function ? for the cost of swapping two
positions, which we denote K?. By using an appro-
priate ?, K? can be biased towards the correctness
of higher ranks by assigning a smaller ? to lower
ranks. Because K? is a distance measure, its value
range will be different depending on the number of
ranks used. Therefore, to convert the measure to a
similarity we normalize the distance to [0, 1] by di-
viding by the maximum K? distance and then sub-
tracting the distance from one. Given two rankings
x and y where x is the reference by which y is to be
measured, we may compute the normalized similar-
ity using
Ksim? = 1?
K?(x, y)
Kmax? (x)
. (1)
Equation 1 has its maximal value of one when rank-
ing y is identical to ranking x, and its minimal value
of zero when y is in the reverse order as x. We refer
to this value as the positionally-weighted Kendall?s
? similarity, Ksim? . As defined, K
sim
? does not ac-
count for ties. Therefore, we arbitrarily break ties in
a deterministic fashion for both rankings. Second,
we define ? to assign higher cost to the first ranks:
the cost to move an item into position i, ?i, is de-
fined as n?(i+1)n , where n is the number of senses.
3.1.4 Weighted NDCG
To compare the applicability ratings for sense an-
notations, we recast the annotation process in an In-
formation Retrieval setting: Given an example con-
text acting as a query over a word?s senses, the task
is to retrieve all applicable senses, ranking and scor-
ing them by their applicability. Moffat and Zobel
(2008) propose using Discounted Cumulative Gain
(DCG) as a method to compare a ranking against a
baseline. Given (1) a gold standard weighting of the
k senses applicable to a context, where wi denotes
the applicability for sense i in the gold standard, and
(2) a ranking of the k senses by some method, the
DCG may be calculated as
?k
i=1
2wi+1?1
log2(i+1)
. DCG is
commonly normalized to [0, 1] so that the value is
comparable when computed on rankings with dif-
ferent k and weight values. To normalize, the maxi-
mum value is calculated by first computing the DCG
on the ranking when the k items are sorted by their
weights, referred as the Ideal DCG (IDCG), and then
normalizing as NDCG = DCGIDCG .
The DCG only considers the weights assigned
in the gold standard, which potentially masks im-
portance differences in the weights assigned to the
senses. Therefore, we propose weighting the DCG
by the relative difference in the two weights. Given
an alternate weighting of the k items, denoted as w?i,
WDCG =
k?
i=1
min(wi,w?i)
max(wi,w?i)
(
2wi+1 ? 1
)
log2(i)
. (2)
The key impact in Equation 2 comes from weight-
ing an item?s contribution to the score by its rela-
tive deviation in absolute weight. A set of weights
that achieves an equivalent ranking may have a low
WDCG if the weights are significantly higher or
lower than the reference. Equation 2 may be nor-
malized in the same way as the DCG. We refer to
this final normalized measure as the Weighted Nor-
malized Discounted Cumulative Gain (WNDCG).
3.2 Sense Cluster Comparisons
Sense induction can be viewed as an unsupervised
clustering task where usages of a word are grouped
into clusters, each representing uses of the same
meaning. In previous SemEval tasks on sense in-
duction, instances were labeled with a single sense,
which yields a partition over the instances into dis-
joint sets. The proposed partition can then be com-
pared with a gold-standard partition using many ex-
isting clustering comparison methods, such as the
V-Measure (Rosenberg and Hirschberg, 2007) or
paired FScore (Artiles et al, 2009). Such cluster
comparison methods measure the degree of similar-
ity between the sense boundaries created by lexicog-
raphers and those created by WSI methods.
In the present task, instances are potentially la-
beled both with multiple senses and with weights
293
reflecting the applicability. This type of sense label-
ing produces a fuzzy clustering: An instance may
belong to one or more sense clusters with its clus-
ter membership relative to its weight for that sense.
Formally, we refer to (1) a solution where the sets
of instances overlap as a cover and (2) a solution
where the sets overlap and instances may have par-
tial memberships in a set as fuzzy cover.
We propose two new fuzzy measures for com-
paring fuzzy sense assignments: Fuzzy B-Cubed
and Fuzzy Normalized Mutual Information. The
two measures provide complementary information.
B-Cubed summarizes the performance per instance
and therefore provides an estimate of how well a sys-
tem would perform on a new corpus with a similar
sense distribution. In contrast, Fuzzy NMI is mea-
sured based on the clusters rather than the instances,
thereby providing a performance analysis that is in-
dependent of the corpus sense distribution.
3.2.1 Fuzzy B-Cubed
Bagga and Baldwin (1998) proposed a clustering
evaluation known as B-Cubed, which compares two
partitions on a per-item basis. Amigo? et al (2009)
later extended the definition of B-Cubed to compare
overlapping clusters (i.e., covers). We generalize B-
Cubed further to handle the case of fuzzy covers.
B-Cubed is based on precision and recall, which es-
timate the fit between two clusterings, X and Y at
the item level. For an item i, precision reflects how
many items sharing a cluster with i inX appear in its
cluster in Y ; conversely, recall measures how many
items sharing a cluster in Y with i also appear in its
cluster in X . The final B-Cubed value is the har-
monic mean of the two scores.
To generalize B-Cubed to fuzzy covers, we adopt
the formalization of Amigo? et al (2009), who define
item-based precision and recall functions, P and R,
in terms of a correctness function, C ? {0, 1}. For
notational brevity, let avg be a function that returns
the mean value of a series, and ?x(i) denote the set
of clusters in clusteringX of which item i is a mem-
ber. B-Cubed precision and recall may therefore cal-
culated over all n items:
B-Cubed Precision = avg
i
[ avg
j 6=i???y(i)
P (i, j)] (3)
B-Cubed Recall = avg
i
[ avg
j 6=i???x(i)
R(i, j)]. (4)
When comparing partitions, P and R are defined as
1 if two items cluster labels are identical. To gen-
eralize B-Cubed for fuzzy covers, we redefine P
and R to account for differences in the partial clus-
ter membership of items. Let `X(i) denote the set
of clusters of which i is a member, and wk(i) de-
note the membership weight of item i in cluster k in
X . We therefore define C with respect to X of two
items as
C(i, j,X) =
?
k?`X(i)?`X(j)
1?|wk(i)?wk(j)|. (5)
Equation 5 is maximized when i and j have
identical membership weights in the clusters of
which they are members. Importantly, Equation
5 generalizes to the correctness operations both
when comparing partitions and covers, as defined
by Amigo? et al (2009). Item-based Precision
and Recall are then defined using Equation 5 as
P (i, j,X) = Min(C(i,j,X),C(i,j,Y ))C(i,j,X) and R(i, j,X) =
Min(C(i,j,X),C(i,j,Y ))
C(i,j,Y ) , respectively. These fuzzy gen-
eralizations are used in Equations 3 and 4.
3.2.2 Fuzzy Normalized Mutual Information
Mutual information measures the dependence be-
tween two random variables. In the context of
clustering evaluation, mutual information treats the
sense labels as random variables and measures the
level of agreement in which instances are labeled
with the same senses (Danon et al, 2005). For-
mally, mutual information is defined as I(X;Y ) =
H(X)?(H(X|Y ) whereH(X) denotes the entropy
of the random variable X that represents a parti-
tion, i.e., the sets of instances assigned to each sense.
Typically, mutual information is normalized to [0, 1]
in order to facilitate comparisons between multiple
clustering solutions on the same scale (Luo et al,
2009), with Max(H(X), H(Y )) being the recom-
mended normalizing factor (Vinh et al, 2010).
In its original formulation Mutual information
is defined only to compare non-overlapping cluster
partitions. Therefore, we propose a new definition of
mutual information between fuzzy covers using ex-
tension of Lancichinetti et al (2009) for calculating
the normalized mutual information between covers.
In the case of partitions, a clustering is represented
as a discrete random variable whose states denote
the probability of being assigned to each cluster. In
294
the fuzzy cover setting, each item may be assigned
to multiple clusters and no longer has a binary as-
signment to a cluster, but takes on a value in [0, 1].
Therefore, each cluster Xi can be represented sepa-
rately as a continuous random variable, with the en-
tire fuzzy cover denoted as the variableX1...k, where
the ith entry of X is the continuous random vari-
able for cluster i. However, by modeling clusters us-
ing continuous domain, differential entropy must be
used for the continuous variables; importantly, dif-
ferential entropy does not obey the same properties
as discrete entropy and may be negative.
To avoid calculating entropy in the continuous do-
main, we therefore propose an alternative method of
computing mutual information based on discretiz-
ing the continuous values of Xi in the fuzzy set-
ting. For the continuous random variable Xi, we
discretize the value by dividing up probability mass
into discrete bins. That is, the support of Xi is parti-
tioned into disjoint ranges, each of which represents
a discrete outcome of Xi. As a result, Xi becomes a
categorical distribution over a set of weights ranges
{w1, . . . , wn} that denote the strength of member-
ship in the fuzzy set. With respect to sense annota-
tion, this discretization process is analogous to hav-
ing an annotator rate the applicability of a sense for
an instance using a Likert scale instead of using a
rational number within a fixed bound.
Discretizing the continuous cluster membership
ratings into bins allows us to avoid the problematic
interpretation of entropy in the continuous domain
while still expanding the definition of mutual infor-
mation from a binary cluster membership to one of
degrees. Using the definition of Xi and Yj as a cate-
gorical variables over discrete ratings, we may then
estimate the entropy and joint entropy as follows.
H(Xi) =
n?
i=1
p(wi)log2p(wi) (6)
where p(wi) is the probability of an instance being
labeled with rating wi Similarly, we may define the
joint entropy of two fuzzy clusters as
H(Xk, Yl) =
n?
i=1
m?
j=1
p(wi, wj)log2p(wi, wj) (7)
where p(wi, wj) is the probability of an instance be-
ing labeled with rating wi in cluster Xk and wj in
cluster Yl, and m denotes the number of bins for Yl.
The conditional entropy between two clusters may
then be calculated as
H(Xk|Yl) = H(Xk, Yl)?H(Yl).
Together, Equations 6 and 7 may be used to define
I(X,Y ) as in the original definition. We then nor-
malize using the method of McDaid et al (2011).
Based on the limited range of fuzzy memberships
in [0, 1], we selected uniformly distributed bins in
[0, 1] at 0.1 intervals when discretizing the member-
ship weights for sense labelings.
3.3 Baselines
Task 12 included multiple baselines based on mod-
eling different types of WSI and WSD systems.
Due to space constraints, we include only the four
most descriptive here: (1) Semcor MFS which la-
bels each instance with the most frequent sense of
that lemma in SemCor, (2) Semcor Ranked Senses
baseline, which labels each instance with all of the
target lemma?s senses, ranked according to their fre-
quency in SemCor, using weights n?i+1n , where n is
the number of senses and i is the rank, (3) 1c1inst
which labels each instance with its own induced
sense and (4) All-instances, One sense which la-
bels all instances with the same induced sense. The
first two baselines directly use WordNet 3.1 senses,
while the last two use induced senses.
4 Participating Systems
Four teams submitted nine systems, seven of which
used induced sense inventories. AI-KU submitted
three WSI systems based on a lexical substitution
method; a language model is built from the target
word?s contexts in the test data and the ukWaC cor-
pus and then Fastsubs (Yuret, 2012) is used to iden-
tify lexical substitutes for the target. Together, the
contexts of the target and substitutes are used to
build a distributional model using the S-CODE al-
gorithm (Maron et al, 2010). The resulting contex-
tual distributions are then clustered using K-means
to identify word senses. The University of Mel-
bourne (Unimelb) team submitted two WSI systems
based on the approach of Lau et al (2012). Their
systems use a Hierarchical Dirichlet Process (Teh
et al, 2006) to automatically infer the number of
senses from contextual and positional features. Un-
295
WSD F1 Cluster Comparison
Team System Jac. Ind. Ksim? WNDCG Fuzzy NMI Fuzzy B-Cubed #Cl #S
AI-KU Base 0.197 0.620 0.387 0.065 0.390 7.76 6.61
AI-KU add1000 0.197 0.606 0.215 0.035 0.320 7.76 6.61
AI-KU remove5-add1000 0.244 0.642 0.332 0.039 0.451 3.12 5.33
Unimelb 5p 0.218 0.614 0.365 0.056 0.459 2.37 5.97
Unimelb 50k 0.213 0.620 0.371 0.060 0.483 2.48 6.08
UoS #WN Senses 0.192 0.596 0.315 0.047 0.201 8.08 6.77
UoS top-3 0.232 0.625 0.374 0.045 0.448 3.00 5.44
La Sapienza system-1 0.149 0.507 0.311 - - - 8.69
La Sapienza system-2 0.149 0.510 0.383 - - - 8.67
All-instances, One sense 0.192 0.609 0.288 0.0 0.623 1.00 6.62
1c1inst 0.0 0.0 0.0 0.071 0.0 1.00 0.0
Semcor MFS 0.455 0.465 0.339 - - - 1.00
Semcor Ranked Senses 0.149 0.559 0.489 - - - 8.66
Table 3: Performance on the five evaluation measures for all system and selected baselines. Top system
performances are marked in bold.
like other teams, the Unimelb systems were trained
on a Wikipedia corpus instead of the ukWaC cor-
pus. The University of Sussex (UoS) team submit-
ted two WSI systems that use dependency-parsed
features from the corpus, which are then clustered
into senses using the MaxMax algorithm (Hope and
Keller, 2013); the resulting fine-grained clusters are
then combined based on their degree of separabil-
ity. The La Sapienza team submitted two Unsu-
pervised WSD systems based applying Personal-
ized Page Rank (Agirre and Soroa, 2009) over a
WordNet-based network to compare the similarity of
each sense with the similarity of the context, ranking
each sense according to its similarity.
5 Results and Discussion
Table 3 shows the main results for all instances. Ad-
ditionally, we report the number of induced clusters
used to label each sense as #Cl and the number of
resulting WordNet 3.1 senses for each sense with
#S. As in previous WSD tasks, the MFS baseline
was quite competitive, outperforming all systems on
detecting which senses were applicable, measured
using the Jaccard Index. However, most systems
were able to outperform the MFS baseline on rank-
ing senses and quantifying their applicability.
Previous cluster comparison evaluations often
faced issues with the measures being biased either
towards the 1c1inst baseline or labeling all instances
with the same sense. However, Table 3 shows that
Team System F1 NMI B-Cubed
AI-KU Base 0.641 0.045 0.351
AI-KU add1000 0.601 0.023 0.288
AI-KU remove5-add1000 0.628 0.026 0.421
Unimelb 5p 0.596 0.035 0.421
Unimelb 50k 0.605 0.039 0.441
UoS #WN Senses 0.574 0.031 0.180
UoS top-3 0.600 0.028 0.414
La Sapienza System-1 0.204 - -
La Sapienza System-2 0.217 - -
All-instances, One sense 0.569 0.0 0.570
1c1inst 0.0 0.018 0.0
Semcor MFS 0.477 0.0 0.570
Table 4: System performance in the single-sense set-
ting. Top system performances are marked in bold.
systems are capable of performing well in both the
Fuzzy NMI and Fuzzy B-Cubed measures, thereby
avoiding the extreme performance of either baseline.
An analysis of the systems? results showed that
many systems labeled instances with a high num-
ber of senses, which could have been influenced by
the trial data having significantly more instances la-
beled with multiple senses than the test data. There-
fore, we performed a second analysis that parti-
tioned the test set into two sets: those labeled with
a single sense and those with multiple senses. For
single-sense set, we modified the test setting to have
systems also label instances with a single sense:
(1) the sense mapping function for WSI systems
(Sec. 3.1.1) was modified so that after the mapping,
296
WSD F1 Cluster Comparison
Team System Jac. Ind. Ksim? WNDCG Fuzzy NMI Fuzzy B-Cubed
AI-KU Base 0.394 0.617 0.317 0.029 0.078
AI-KU add1000 0.394 0.620 0.214 0.014 0.061
AI-KU remove5-add1000 0.434 0.585 0.290 0.004 0.116
Unimelb 5p 0.436 0.585 0.286 0.019 0.130
Unimelb 5000k 0.414 0.602 0.298 0.021 0.134
UoS #WN Senses 0.367 0.627 0.313 0.036 0.037
UoS top-3 0.421 0.574 0.302 0.006 0.113
La Sapienza system-1 0.263 0.660 0.447 - -
La Sapienza system-2 0.412 0.694 0.536 - -
All-instances, One sense 0.387 0.635 0.254 0.0 0.130
1c1inst 0.0 0.0 0.0 0.300 0.0
Semcor MFS 0.283 0.373 0.197
Semcor Ranked Senses 0.263 0.593 0.395
Table 5: System performance on all instances labeled with multiple senses. Top system performances are
marked in bold.
only the highest-weighted WordNet 3.1 sense was
used, and (2) the La Sapienza system output was
modified to retain only the highest weighted sense.
In this single-sense setting, systems were evaluated
using the standard WSD Precision and Recall mea-
sures; we report the F1 measure of Precision and Re-
call. The remaining subset of instances annotated
with multiple senses were evaluated separately.
Table 4 shows the systems? performance on
single-sense instances, revealing substantially in-
creased performance and improvement over the
MFS baseline for WSI systems. Notably, the per-
formance of the best sense-remapped WSI systems
surpasses the performance of many supervised WSD
systems in previous WSD evaluations (Kilgarriff,
2002; Mihalcea et al, 2004; Pradhan et al, 2007;
Agirre et al, 2010). This performance suggests that
WSI systems using graded labels provide a way to
leverage huge amounts of unannotated corpus data
for finding sense-related features in order to train
semi-supervised WSD systems.
Table 5 shows the performance on the subset of
instances that were annotated with multiple senses.
We note that in this setting, the mapping proce-
dure transforms the All-Instances One Sense base-
line into the average applicability rating for each
sense in the test corpus. Notably, the La Sapienza
systems sees a significant performance increase in
this setting; their systems label each instance with
all of the lemma?s senses, which significantly de-
grades performance in the most common case where
only a single sense applies. However, when multi-
ple senses are known to be present, their method for
quantifying sense applicability appears closest to the
gold standard judgments. Furthermore, the majority
of WSI systems are able to surpass all four baselines
on identifying which senses are present and quanti-
fying their applicability.
6 Conclusion
We have introduced a new evaluation setting for
WSI and WSD systems where systems are measured
by their ability to detect and weight multiple appli-
cable senses for a single context. Four teams submit-
ted nine systems, annotating a total of 4664 contexts
for 50 words from the OANC. Many systems were
able to surpass the competitive MFS baseline. Fur-
thermore, when WSI systems were trained to pro-
duce only a single sense label, the performance of
resulting semi-supervised WSD systems surpassed
that of many supervised systems in previous WSD
evaluations. Future work may assess the impact of
graded sense annotations in a task-based setting. All
materials have been released on the task website.1
Acknowledgments
We thank Rebecca Passonneau for her feedback and
suggestions for target lemmas used in this task.
1
http://www.cs.york.ac.uk/semeval-2013/task13/
297
References
Eneko Agirre and Aitor Soroa. 2007. Semeval-2007 task
2: Evaluating word sense induction and discrimination
systems. In Proceedings of the Fourth International
Workshop on Semantic Evaluations, pages 7?12. ACL.
Eneko Agirre and Aitor Soroa. 2009. Personalizing
PageRank for Word Sense Disambiguation. In Pro-
ceedings of EACL, pages 33?41. ACL.
Eneko Agirre, Oier Lo?pez De Lacalle, Christine Fell-
baum, Andrea Marchetti, Antonio Toral, and Piek
Vossen. 2010. SemEval-2010 task 17: All-words
word sense disambiguation on specific domains. In
Proceedings of SemEval-2010. ACL.
Enrique Amigo?, Julio Gonzalo, Javier Artiles, and Felisa
Verdejo. 2009. A comparison of extrinsic clustering
evaluation metrics based on formal constraints. Infor-
mation Retrieval, 12(4):461?486.
Javier Artiles, Enrique Amigo?, and Julio Gonzalo. 2009.
The role of named entities in web people search. In
Proceedings of EMNLP, pages 534?542. Association
for Computational Linguistics.
Ron Artstein and Massimo Poesio. 2008. Inter-coder
agreement for computational linguistics. Computa-
tional Linguistics, 34(4):555?596.
Amit Bagga and Breck Baldwin. 1998. Algorithms for
scoring coreference chains. In Proceedings of the Lin-
guistic Coreference Workshop at LREC, pages 563?
566.
Marco Baroni, Silvia Bernardini, Adriano Ferraresi, and
Eros Zanchetta. 2009. The WaCky wide web: A
collection of very large linguistically processed web-
crawled corpora. Language Resources and Evalua-
tion, 43(3):209?226.
Chris Biemann and Valerie Nygaard. 2010. Crowdsourc-
ing wordnet. In The 5th International Conference of
the Global WordNet Association (GWC-2010).
Leon Danon, Albert D??az-Guilera, Jordi Duch, and Alex
Arenas. 2005. Comparing community structure iden-
tification. Journal of Statistical Mechanics: Theory
and Experiment, 2005(09):P09008.
Katrin Erk and Diana McCarthy. 2009. Graded word
sense assignment. In Proceedings of Empirical Meth-
ods in Natural Language Processing (EMNLP), pages
440?449. ACL.
Katrin Erk, Diana McCarthy, and Nicholas Gaylord.
2009. Investigations on word senses and word us-
ages. In Proceedings of the Joint Conference of the
47th Annual Meeting of the ACL and the 4th Interna-
tional Joint Conference on Natural Language Process-
ing of the AFNLP, pages 10?18. ACL.
Christine Fellbaum, editor. 1998. WordNet: An Elec-
tronic Lexical Database. The MIT Press.
David Hope and Bill Keller. 2013. MaxMax: A Graph-
Based Soft Clustering Algorithm Applied to Word
Sense Induction. In Proceedings of CICLing, pages
368?381.
Nancy Ide and Keith Suderman. 2004. The american
national corpus first release. In Proceedings of the
Fourth Language Resources and Evaluation Confer-
ence, pages 1681?1684.
David Jurgens. 2012. An Evaluation of Graded Sense
Disambiguation using Word Sense Induction. In Pro-
ceedings of *SEM, the First Joint Conference on Lexi-
cal and Computational Semantics. ACL.
David Jurgens. 2013. Embracing Ambiguity: A Com-
parison of Annotation Methodologies for Crowdsourc-
ing Word Sense Labels. In Proceedings of the North
American Chapter of the Association for Computa-
tional Linguistics (NAACL). ACL.
Adam Kilgarriff. 2002. English lexical sample
task description. In Proceedings of ACL-SIGLEX
SENSEVAL-2 Workshop.
Klaus Krippendorff. 1980. Content Analysis: An Intro-
duction to Its Methodology. Sage, Beverly Hills, CA.
Klaus Krippendorff. 2004. Content Analysis: An In-
troduction to Its Methodology. Sage, Thousand Oaks,
CA, second edition.
Ravi Kumar and Sergei Vassilvitskii. 2010. General-
ized distances between rankings. In Proceedings of
the 19th International Conference on World Wide Web
(WWW), pages 571?580. ACM.
Andrea Lancichinetti, Santo Fortunato, and Ja?nos
Kerte?sz. 2009. Detecting the overlapping and hierar-
chical community structure in complex networks. New
Journal of Physics, 11(3):033015.
Jey Han Lau, Paul Cook, Diana McCarthy, David New-
man, and Timothy Baldwin. 2012. Word sense induc-
tion for novel sense detection. In Proceedings of the
13th Conference of the European Chapter of the Asso-
ciation for computational Linguistics (EACL 2012).
Ping Luo, Hui Xiong, Guoxing Zhan, Junjie Wu, and
Zhongzhi Shi. 2009. Information-theoretic distance
measures for clustering validation: Generalization and
normalization. IEEE Transactions on Knowledge and
Data Engineering, 21(9):1249?1262.
Suresh Manandhar, Ioannis P. Klapaftis, Dmitriy Dli-
gach, and Sameer S. Pradhan. 2010. SemEval-2010
task 14: Word sense induction & disambiguation. In
Proceedings of the 5th International Workshop on Se-
mantic Evaluation, pages 63?68. ACL.
Yariv Maron, Michael Lamar, and Elie Bienenstock.
2010. Sphere embedding: An application to part-of-
speech induction. In Proceedings of Advances in Neu-
ral Information Processing Systems (NIPS).
298
Aaron F. McDaid, Derek Greene, and Neil Hurley. 2011.
Normalized mutual information to evaluate overlap-
ping community finding algorithms. arXiv:1110.2515.
Rada Mihalcea, Timothy Chklovski, and Adam Kilgar-
riff. 2004. The Senseval-3 English lexical sample
task. In Senseval-3: Third International Workshop on
the Evaluation of Systems for the Semantic Analysis of
Text, pages 25?28. ACL.
Alistair Moffat and Justin Zobel. 2008. Rank-biased
precision for measurement of retrieval effectiveness.
ACM Transactions on Information Systems (TOIS),
27(1):2.
G. Craig Murray and Rebecca Green. 2004. Lexical
knowledge and human disagreement on a wsd task.
Computer Speech & Language, 18(3):209?222.
Roberto Navigli, David Jurgens, and Daniele Vanilla.
2013. Semeval-2013 task 12: Multilingual word sense
disambiguation. In Proceedings of the 7th Interna-
tional Workshop on Semantic Evaluation.
Roberto Navigli. 2009. Word Sense Disambiguation: A
Survey. ACM Computing Surveys, 41(2):1?69.
Rebecca Passonneau, Nizar Habash, and Owen Rambow.
2006. Inter-annotator agreement on a multilingual se-
mantic annotation task. In Proceedings of the Fifth
International Conference on Language Resources and
Evaluation (LREC), pages 1951?1956.
Rebecca J Passonneau, Collin Baker, Christiane Fell-
baum, and Nancy Ide. 2012a. The MASC word sense
sentence corpus. In Proceedings of LREC.
Rebecca J. Passonneau, Vikas Bhardwaj, Ansaf Salleb-
Aouissi, and Nancy Ide. 2012b. Multiplicity and
word sense: evaluating and learning from multiply la-
beled word sense annotations. Language Resources
and Evaluation, pages 1?34.
Sameer S. Pradhan, Edward Loper, Dmitriy Dligach, and
Martha Palmer. 2007. SemEval-2007 task 17: English
lexical sample, SRL, and all-words. In Proceedings of
the 4th International Workshop on Semantic Evalua-
tions. ACL.
Andrew Rosenberg and Julia Hirschberg. 2007. V-
measure: A conditional entropy-based external cluster
evaluation measure. In Proceedings of the 2007 Joint
Conference on Empirical Methods in Natural Lan-
guage Processing and Computational Natural Lan-
guage Learning (EMNLP-CoNLL). ACL.
Anna Rumshisky, Nick Botchan, Sophie Kushkuley, and
James Pustejovsky. 2012. Word sense inventories by
non-experts. In Proceedings of the 8th International
Conference on Language Resources and Evaluation
(LREC).
Yee Whye Teh, Michael I. Jordan, Matthew J. Beal, and
David M. Blei. 2006. Hierarchical dirichlet pro-
cesses. Journal of the American Statistical Associa-
tion, 101(476):1566?1581.
Jean Ve?ronis. 1998. A study of polysemy judgments and
inter-annotator agreement. In Program and advanced
papers of the Senseval workshop.
Nguyen Xuan Vinh, Julien Epps, and James Bailey.
2010. Information theoretic measures for clusterings
comparison: Variants, properties, normalization and
correction for chance. The Journal of Machine Learn-
ing Research, 11:2837?2854.
Deniz Yuret. 2012. FASTSUBS: An Efcient Admissible
Algorithm for Finding the Most Likely Lexical Substi-
tutes Using a Statistical Language Model. Computing
Research Repository (CoRR).
299
