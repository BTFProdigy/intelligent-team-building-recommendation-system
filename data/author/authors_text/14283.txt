Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008), pages 465?472
Manchester, August 2008
Stopping Criteria for Active Learning of Named Entity Recognition
Florian Laws
Institute for NLP
Universit?at Stuttgart
fl@ifnlp.org
Hinrich Sch?utze
Institute for NLP
Universit?at Stuttgart
hs999@ifnlp.org
Abstract
Active learning is a proven method for re-
ducing the cost of creating the training sets
that are necessary for statistical NLP. How-
ever, there has been little work on stopping
criteria for active learning. An operational
stopping criterion is necessary to be able
to use active learning in NLP applications.
We investigate three different stopping cri-
teria for active learning of named entity
recognition (NER) and show that one of
them, gradient-based stopping, (i) reliably
stops active learning, (ii) achieves near-
optimal NER performance, (iii) and needs
only about 20% as much training data as
exhaustive labeling.
1 Introduction
Supervised statistical learning methods are impor-
tant and widely successful tools for natural lan-
guage processing. These methods learn by esti-
mating a statistical model on labeled training data.
Often, these models require a large amount of
training data that needs to be hand-annotated by
human experts. This is time-consuming and ex-
pensive. Active learning (AL) reduces this annota-
tion effort by selecting unlabeled examples that are
maximally informative for the statistical learning
method and handing them to a human annotator
for labeling. The statistical model is then updated
with the newly gathered information. In this pa-
per, we adopt the uncertainty sampling approach
to AL (Lewis and Gale, 1994). Uncertainty sam-
pling selects those examples in the pool as most in-
c
? 2008. Licensed under the Creative Commons
Attribution-Noncommercial-Share Alike 3.0 Unported li-
cense (http://creativecommons.org/licenses/by-nc-sa/3.0/).
Some rights reserved.
formative for which the statistical classifier is least
certain in its classification decision.
While AL is an active area of research in NLP,
the issue of determining when to stop the AL
process has only recently come into focus (Zhu
and Hovy, 2007; Vlachos, 2008). This is some-
what surprising because the main purpose of ac-
tive learning is to save on annotation effort; decid-
ing on the point when enough data is annotated is
crucial to fulfilling this goal.
We investigate three different stopping criteria
in this paper. First, a user of a classification system
may want to set a minimum absolute performance
for the system to be deployed. The standard way of
assessing classifier performance uses a held-out la-
beled test set. However, labeling a test set of suffi-
cient size is contrary to the goal of minimizing an-
notation effort and impractical in most real-world
settings. We will show that the classifier can esti-
mate its own performance using only an unlabeled
reference set and propose to stop active learning
if estimated performance reaches the threshold set
by the user. The estimation is somewhat inaccu-
rate, however, and we investigate possible reasons
for estimation error.
An alternative criterion is based on maximum
possible performance. We will show that our per-
formance estimation method supports stopping AL
at a point where performance is almost optimal.
The third and last criterion is convergence. The
basic idea here is to stop active learning when more
examples from the pool do not contribute more
information, indicated either by the fact that the
classifier has reached maximum performance or by
the fact that the ?uncertainty? of the classifier can-
not be decreased further. We determine the point
where the pool has become uninformative by com-
puting the gradient of either performance or uncer-
465
tainty.
This paper is organized as follows. Section 2
shows that three uncertainty measures achieve
near-optimal performance for NER at a fraction
of the labeling cost of exhaustive labeling of the
training set. In Section 3, we introduce a new
method for estimating the performance of an ac-
tively learned classifier in support of stopping ac-
tive learning when a certain level of performance
has been reached. Section 4 shows that the stop-
ping criterion of reaching peak confidence is not
applicable to NER with multiclass logistic regres-
sion. Section 5 presents stopping criteria based
on convergence. Sections 6 and 7 discuss related
work and present our conclusions.
2 Selection Functions
For measuring the uncertainty of a classification
decision in uncertainty sampling there exist diverse
measures appropriate for different basic classifiers
(e.g. margin-based measures for SVMs, and mea-
sures based on class probability for classification).
Choosing such an uncertainty measure is relatively
straightforward for a binary classification problem,
but for multiclass problems we need different mea-
sures, and it is not obvious which will perform
best.
Following Schein (2005), but in the context of
NER, we compare several measures of uncertainty
for multiclass logistic regression. For a given mea-
sureM
i,X
, we select in each iteration the unlabeled
example(s) in the pool that have the smallest value
for M
i,X
(corresponding to the maximum uncer-
tainty).
1-Entropy.
M
i,1-Entropy
= 1 ?H(p?(.|x
i
))
= 1 +
?
j
p?(c
j
|x
i
) log p?(c
j
|x
i
)
where p?(c
j
|x
i
) is the current estimate of the proba-
bility of class c
j
given the example x
i
.
1
1-Entropy
favors examples where the classifier assigns simi-
lar probabilities to all classes.
Margin. If c and c
?
are the two most likely
classes, the margin is defined as follows:
M
i,Margin
= |p?(c|x
i
) ? p?(c
?
|x
i
)|
Margin picks examples where the distinction be-
tween two likely classes is hard.
1
We use 1-Entropy instead of entropy, so all three mea-
sures will have lower values for less certain instances.
MinMax.
M
i,MinMax
= max
j
(p?(c
j
|x
i
))
The rationale here is that a low probability of the
selected class indicates uncertainty. We propose
MinMax as a measure that is more directly based
on the classifier?s decision for a particular exam-
ple. The other two measures also take into account
the classifier?s assessment of classes that were not
chosen for the unlabeled example.
2.1 Experiments
We used the newswire section of the ACE 2005
Multilingual Training Corpus (128 documents,
66,015 tokens) for our experiments. A subset of
the documents was randomly sampled into an eval-
uation set that consists of 6301 tokens. We used
30.000 of the remaining tokens as the uncertainty
sampling pool. The rest was left aside for future
experiments. We use the BBR package (Genkin et
al., 2007) for binary logistic regression as our base
classifier, with default values for all of BBR?s pa-
rameters. As our main focus is on AL, we only
use basic features like capitalization, puctuation as
well as word identity, prefixes and suffixes, each
for the classified word itself and for left and right
contexts.
We train separate classifiers for each named en-
tity (NE) class and another one for the class ?not an
NE? (0). For each token we normalize the output
probability of the individual classifiers so they sum
to 1 and then select for each token the class with
the highest probability. Evaluation is performed by
comparing individual tokens to the gold standard.
2
Using all labeled training data as our fully super-
vised baseline results in a performance of 78.7%
F
1
(henceforth: F ) and 96.6% accuracy. This is
comparable to the accuracy of 96.29% reported
by (Daume III, 2007) on the newswire domain.
Daum?e?s work is the only study known to us that
uses the ACE dataset, but not the proprietary ACE
value score. In the rest of this paper, we report F
scores, because we believe that F is a more infor-
mative measure for NER than accuracy.
We use AL based on uncertainty sampling. We
start with a seed set of ten consecutive tokens
randomly selected from the training pool and la-
bel it. In each round of AL we select the ten
tokens with the smallest value of M
i,X
(where
2
Chunk-based NER results are not directly comparable
with this token-based evaluation.
466
Selection Baseline Peak perf.
1-Entropy 78.7 2139 (7.1%) 80.8 3460 (11.5%)
MinMax 78.7 2108 (7.0%) 80.8 3650 (12.1%)
Margin 78.7 2019 (6.7%) 81.2 3694 (12.3%)
Table 1: Percentage of data needed by AL to reach
baseline or peak performance.
X ? {1-Entropy,Margin,MinMax}) from the re-
maining pool, including tokens with the label 0.
We then label these tokens and add them to the la-
beled training set. The classifiers are retrained with
the new training set and the AL loop repeats. We
performed 20 runs of the experiments, each with
the same sampling pool, but a different seed set,
randomly selected as described above.
Table 1 shows that AL is quite successful for
NER. Only 7% of the training data is needed to
achieve the same performance as the supervised
baseline.
Furthermore we find that after the baseline per-
formance is reached the increase in performance
quickly levels off to a point where using more
training data does not yield performance improve-
ments anymore. In fact, our experiments show
that there is a peak in performance reached at
about 12% of the training data and performance
decreases again after this point (see Figure 1).
The peak is more prominent if the pool is large.
On a pool of 30,000 tokens, peak performance is
about 2.5% F -Score better than the baseline; on a
6000 token pool, the difference is only about 1.7%.
Therefore, once the peak is reached, the AL pro-
cess should stop, even if the annotation budget is
not yet used up.
0 2000 4000 6000 8000 10000
0.
60
0.
65
0.
70
0.
75
0.
80
Training examples
F?
Sc
or
e
Margin
1?Entropy
MinMax
Figure 1: Performance as a function of number of
labeled training examples used
Comparing the different selection functions, we
found little difference between their performance.
Margin performs significantly better (Student?s t-
test, ? = 0.05), but the difference is small (< 1%
F -Score). If we compare two AL processes (say
Margin and 1-Entropy) that were started with the
same pool and seed set and stop both processes
when they each reach their respective peak per-
formances, Margin has a better peak performance
of 0.3% F -Score on average (significant at ? =
0.05).
The differences between 1-Entropy andMinMax
are not statistically significant, except for a short
start-up phase (see Figure 1).
3 Performance Estimation
In practical applications, classifiers can only be re-
liably deployed when they attain a predefined min-
imum absolute performance level. Thus, we would
like to determine if this level has been reached and
then stop the annotation process. However, this is
not a simple task, because in these settings there
is no labeled test set available to evaluate perfor-
mance. Creating this test set would mean a sub-
stantial annotation effort, which is what we want
to avoid by using AL in the first place. Therefore,
we will try to estimate the classifier?s performance
on unlabeled data.
Following Lewis (1995), we estimate the F -
Score based on the current estimates of the class
probabilities. Based on the F measure?s definition
as the harmonic mean of precision (P) and recall
(R), we can write F as a function of true positives
(TP), false positives (TP) and false negatives (FN):
F =
2 ? P ?R
P +R
=
2TP
2TP + FP + FN
Similar to Lewis, we estimate
?
TP,
?
FP,
?
FN, but we
need to extend their work from binary classifica-
tion to 1-vs-all multiclass classification:
?
TP =
n
?
i
E
?
j
p?(c
j
|x
i
)d
i,j
(1)
?
FP =
n
?
i
E
?
j
(1 ? p?(c
j
|x
i
))d
i,j
(2)
?
FN =
n
?
i
E
?
j
p?(c
j
|x
i
)(1 ? d
i,j
) (3)
where n is the number of examples, E is the num-
ber of named entity classes, excluding the ?not
467
an NE? class. p?(c
j
|x
i
) is the estimated probabil-
ity that example x
i
has class c
j
. The flag d
i,j
indicates ?is winning class?: d
i,j
= 1 if j =
argmax
j
p?(c
j
|x
i
) and d
i,j
= 0 else.
Like standard NER evaluation schemes, e.g.
(Tjong Kim Sang and De Meulder, 2003), we con-
sider only those decisions to be TPs where (i) the
reference class matches the selected class and (ii)
this class is not ?not an NE?. When estimating
TP, we assume that the probability of a match
equals the probability of the selected class (which
is p?(c
j
|x
i
) ? d
i,j
). The probability of making an
FP error is just the remaining probability mass.
For FN, we can calculate the estimated probabil-
ity by summing up the class probabilities of the
non-selected named entity classes.
3.1 Evaluation of Performance Estimation
To evaluate the performance estimation method,
we ran it on an unlabeled reference set. The ref-
erence set is a set of unlabeled data distinct from
the sampling pool. In our experiments, we use the
tokens in the test set from 2.1, but with the labels
stripped off.
We compare the true performance on the test set
(reported as ?True? in Table 2) with the estimate
(reported as ?Lewis?). The ? columns report the
difference of the named method to ?True?. We also
tested leave-one-out (LOO) estimation of F , P and
R using the data of the selected training set.
True Lewis ? Lewis LOO ? LOO
F 79 92 +13 85 +6
P 81 92 +11 86 +5
R 77 92 +15 84 +7
Table 2: Performance estimation. LOO and Lewis
overestimate true F by 6% and 13%, respectively.
We find that both methods overestimate preci-
sion and recall by a large margin. We also note
that the peak in performance at about 4200 train-
ing examples that we found when evaluating on
held-out data (see Figure 1) does not occur when
evaluating performance using the Lewis method.
Instead, the estimate of F grows monotonically.
This means that we cannot use a peak of estimated
F as a criterion for stopping. When setting an ab-
solute threshold of F = 80% for stopping, active
learning stops at about 1000 iterations, yielding a
true performance of only F = 73% (selection by
Margin, 20 trials). This indicates that we cannot
directly use Lewis estimates for stopping.
3.2 Error Analysis
The reason for the overestimation is that the logis-
tic regression classifier is too confident in its own
decision. For positive decisions, the class proba-
bility very often is close to 1, for negative deci-
sions, it is close to 0. As a result, the estimator
gives very little score for FN (Equation 3) or FP
(Equation 2) in most instances, which leads to the
high overestimation of performance.
To verify this, we grouped the empirical prob-
ability of a selected class being the correct class
in bins according to the estimated probability of
the logistic classifier. Table 3 shows this empiri-
cal probability given a class and its estimate. The
table is split into two halves, such that the empir-
ical probabilities for positive decisions (the class
got chosen as the best class) and negative deci-
sions are shown separately. The top value in each
cell (?emp?) shows the empirical probability as op-
posed to the estimated probability, which is the
value below (?est?). The product of the differ-
ence of these two probabilities and the number of
instances that were counted into this bin (?cnt?),
gives an estimate of how much the probability es-
timates in the bin contribute to the error (absolute
value) of the performance estimation.
The table shows that class probabilities are in
fact estimated too optimistically. For many of the
entries in the positives table, the estimated prob-
abilities are greater than the empirical probabili-
ties. In the negatives table, the estimated proba-
bilities are smaller. In both cases, the estimates
are closer to the respective extreme values 1 or 0,
which means they are overconfident. Note that for
positive decisions, the estimation error of the val-
ues in a single bin contributes to the overall estima-
tion error in two ways: overestimating TPs and un-
derestimating FPs. For example, the estimation er-
ror for the cell in bold is 29.2, contributing ?29.2
for FP (underestimation) and +29.2 for TP (over-
estimation). Also note that due to the high num-
ber of non-NE tokens in the text, there is a large
number of negative decisions for each entity-class
classifier; thus, small differences in the probabili-
ties make large contributions to error.
We ran a separate experiment in which we
trained a classifier on the entire labeled pool. The
Lewis estimator overestimated F by 12% in this
case. This indicates that the estimation error does
not primarily come from the biased selection of
training examples inherent in the selective sam-
468
negative decisions positive decisions
0-.2 .2-.4 .4-.6 .2-.4 .4-.6 .6-.8 .8-1
O emp 0.0643 0.269 0.25 0.0 0.25 0.233 0.991
est 0.00825 0.295 0.438 0.394 0.537 0.714 0.999
cnt 607 26 12 1 16 30 5609
err 34 -0.67 -2.25 0.394(tn) 4.6 (tn) 14.4 (tn) 45.4 (tn)
GPE emp 0.00384 0.391 0.5 0.0 0.333 0.571 0.875
est 0.000812 0.296 0.435 0.357 0.535 0.687 0.989
cnt 5985 23 6 1 9 21 256
err 18.1 (fn) 2.19 (fn) 0.388 (fn) 0.357 (fp) 1.82 (fp) 2.42 (fp) 29.2 (fp)
ORG emp 0.00853 0.393 0.667 0.5 0.615 0.828
est 0.000847 0.283 0.441 0.545 0.71 0.968
cnt 6093 28 12 14 26 128
err 46.8 (fn) 3.06 (fn) 2.7 (fn) 0.631 (fp) 2.46 (fp) 17.9 (fp)
PER emp 0.0041 0.455 0.5 0.273 0.5 0.93
est 0.000748 0.283 0.48 0.563 0.718 0.98
cnt 6102 22 6 11 18 142
err 20.4 (fn) 3.78 (fn) 0.121 (fn) 3.2 (fp) 3.93 (fp) 7.19 (fp)
Table 3: Empirical probabilities and contribution to estimation errors. (We omit small classes and empty
columns.) Example (cell in bold): 256 tokens were estimated to be a GPE by the classifier with estimated
probabilities between 0.8 and 1.0. The average estimate was 0.989. In reality, only 224 of these tokens
(87.5%) were GPEs. The contribution of this cell to the overall FP count is (0.989?0.875) ?256 ? 29.2.
pling method, but from bias inherent in either the
whole pool of training data or the base classifier.
3.3 Towards a Better Estimate
Over-optimistic estimates for precision and recall
stem from the classifier?s over-optimistic probabil-
ity estimates. We try to correct the estimates by
replacing the predicted class probabilities with the
appropriate value in an empirical probability table
like the one shown in Table 3. However, since
in practice we do not have labels for the test set,
we cannot compute the empirical probabilities di-
rectly. Instead, we use leave-one-out estimation to
bootstrap the adjustment table from the selected
training data. The adjusted estimation shows a
marked increase in the estimates for FP and FN,
leading to a quite accurate estimate for precision
(+5 absolute error), but the now pessimistic esti-
mate for recall (?16) leads to underestimation of
F -Score overall (?8) (see Table 4).
True Lewis adj. Lewis ? adj. Lewis
F 78 91 70 -8
P 81 93 86 +5
R 76 89 60 -16
TP 520 596 555 +35
FP 125 48 90 -35
FN 163 70 379 +216
Table 4: Lewis estimation with adjusted probabili-
ties
As we see, the adjustment overshoots for recall,
indicating that the new estimated probabilities are
still off. There could be several reasons for this.
The first reason is that the bin width is quite coarse,
as there are only five bins for the entire probability
interval, each bin covering a range of 0.2. How-
ever, using finer bin widths can lead to data spar-
sity problems.
Another reason might be the estimation errors
within individual bins that compound to a quite
large overall error especially in the negative case.
Finally, differences in the distributions of training
set and reference set could cause unreliable esti-
mates. The empirical probabilities for the adjust-
ment table are estimated with leave-one-out on the
training set. However, since the training set is cre-
ated by selective sampling, it will be biased.
4 Confidence-based Stopping
We have found that performance estimation is not
yet reliable enough to stop when a desired perfor-
mance level is reached. However, since there is
a maximum performance that can be reached on
any given sampling pool, the annotation process
still should stop at this point regardless of whether
a target performance level has been reached or
not. We therefore seek a stopping criterion that
finds the maximum possible performance when the
classifier is iteratively trained on a given sampling
pool. Again, in practice we do not have a labeled
test set to evaluate against, so we have to try to find
the stopping point from either the remaining pool,
or the separate unlabeled reference set.
Vlachos (2008) proposes to calculate the confi-
dence of the classifier by using the average uncer-
469
tainty on the unlabeled reference set. For multi-
class problems, he uses SVM classifiers with the
SVM margin size as the uncertainty measure. Us-
ing this measure, Vlachos reports finding, albeit
distorted by fluctuations, a peak pattern in this con-
fidence measure that coincides with reaching max-
imal performance in his experiments. He then sug-
gests to use this peak confidence as the stopping
criterion.
However, in our experiments with multiclass
logistic regression, we could not find this peak
pattern when calculating the confidence using the
three uncertainty measures introduced above: 1-
Entropy, Margin and MinMax.
0 2000 4000 6000 8000 10000
0.
80
0.
85
0.
90
0.
95
1.
00
Iterations
Co
nf
id
en
ce
 o
n 
un
la
be
le
d 
re
fe
re
nc
e 
da
ta
1?Entropy
Minmax
Margin
Figure 2: Confidence on unlabeled reference set
(selection: 1-Entropy). The vertical lines indi-
cate when baseline and optimal performance are
reached. There is no peak pattern in the curves,
so reaching peak confidence cannot be used as a
stopping criterion.
In Figure 2, we show the three measures, av-
eraged over 20 trials as described in section 2.1.
Due to instability of AL during start-up, there are
some fluctuations in the first 100 iterations. Af-
ter 500 iterations the confidence curves stabilize
and at about 4000 iterations approach asymptotes,
without exhibiting peak patterns. Thus, the pro-
posed criterion of peak confidence based on aver-
age reference uncertainty does not seem applicable
for controlling AL with multiclass logistic regres-
sion.
5 Gradient-based Stopping
Since we cannot use peaks for stopping, we pro-
pose to stop when a base measurement character-
0 2000 4000 6000 8000 10000
0.
0
0.
2
0.
4
0.
6
0.
8
1.
0
Iterations
M
ar
gi
n
Figure 3: Margin uncertainty of selected instance
(single run). The graph demonstrates that without
smoothing this criterion is too noisy.
izing the progress of active learning has converged.
We identify the point of convergence by computing
gradients. We find that the rise of the performance
estimation slows to an almost horizontal slope at
about the time when the true performance reaches
its peak. We therefore propose the following new
stopping criterion: Estimate the gradient of the
curve and stop when it approaches 0. Since we
do not need an accurate estimation of absolute per-
formance here, we can use the unadjusted Lewis
estimate for this method. We call this stopping cri-
terion (estimated) performance convergence.
In a similar way, we can use the gradient of the
uncertainty of the last selected instance. The in-
stance that was selected last is always the one with
maximum uncertainty, and thus the most informa-
tive for training. When the uncertainty measure
comes close to the extreme value of 1, we decide
that there are no informative examples left in the
pool and we stop the AL process. (Unfortunately,
1 is minimum uncertainty and 0 is maximum un-
certainty according to our definitions of the three
measures.) The gradient of the uncertainty mea-
sure approaches 0 at this point (see Figure 3), so
we can again use a gradient criterion for imple-
menting this idea. We call this stopping criterion
uncertainty convergence.
In Figure 3, which shows a graph of the Mar-
gin uncertainty of the selected instance, we can
also see that it is quite noisy. The value drops
sharply when some examples are encountered but
quickly returns to the previous level after a few it-
erations. The performance estimation measure is
470
slightly noisy as well, so we need a robust way of
computing the gradient. We achieve this with a
moving median approach. At each step, we com-
pute the median of w
2
= {a
n?k
, . . . , a
n
} (the last
n values) and of w
1
= {a
n?k?1
, . . . , a
n?1
} (the
previous last n values). Each value a
i
is the per-
formance at iteration i (for the performance gradi-
ent) or the uncertainty of the instance selected in
iteration i (for the uncertainty gradient).
We then estimate the gradient using the medians
of the two windows:
g = (median(w
2
) ? median(w
1
))/1 (4)
For the performance estimate, which is less noisy,
we can also use the arithmetic mean instead of the
median. In this case, we simply replace ?median?
with ?mean? in Equation 4.
We found that a window of size k = 100 yields
good results in mitigating the noise while still re-
acting fast enough to the changes in the gradient.
We combine this criterion with a maximum crite-
rion and only stop if the last value a
n
is a newmax-
imum. We stop the AL process when (i) the current
certainty or estimated performance is a new max-
imum and (ii) the newly calculated gradient g is
positive and (iii) g falls below a predefined level .
5.1 Evaluation
We show the results of gradient stopping applied
to each of the three uncertainty measures and the
Lewis estimate. For comparison, we also include
results with a threshold-based criterion, where AL
stops when the uncertainty measure of the selected
instance reaches a threshold of 1?. This is similar
to (Zhu and Hovy, 2007), but extended by us to all
three uncertainty measures.
Table 5 shows results for each criterion. The
?Stop? value indicates number of tokens at which
the stopping criterion stopped AL. ??Bl? indicates
the difference between baseline performance and
performance at the stopping point, ??Pk? the dif-
ference to peak performance. The ?sd? columns
show the respective standard deviations.
We find that all stopping criteria stop before
20% of the pool is used, providing a large reduc-
tion in annotation effort. While the point of peak
performance can not be precisely found by the cri-
teria, all criteria reliably stop at a performance
level that surpasses the fully supervised baseline.
The threshold criteria seem to be a bit better in
finding a stopping point closer to optimal perfor-
mance. Not unsurprisingly, the stopping function
that matches the selection function performs best.
The gradient methods, however, seem to be provid-
ing better-than-baseline performance more consis-
tently (less variation) and might require less tuning
of the threshold parameter when other factors (e.g.,
the batch size) change. If lower noise allows it, as
for the Lewis estimate, moving averages should be
used in place of moving medians.
6 Related Work
Sch?utze et al (2006) studied a Lewis-based per-
formance estimation method in a binary text clas-
sification setting. They attribute difficulties in esti-
mating recall to a ?missed cluster effect?, meaning
that the active sampling procedure is failing to se-
lect some clusters of relevant training examples in
the pool that are too dissimilar to the relevant ex-
amples already known. Diversity measures as pro-
posed by (Shen et al, 2004) might help in mitigat-
ing this effect, but our experiments show that there
are fundamental differences between text classifi-
cation and NER. Since missed clusters of relevant
examples in the training data would eventually be
used as we exhaustively label the entire pool, we
should see improvements in recall when the missed
clusters get used. Instead, we observed in section
2.1, that there are no further performance gains af-
ter a certain portion of the pool is labeled. Thus, all
examples that the classifier can make use of must
have been taken into account, and there appear to
be no missed clusters.
Tomanek et al (2007) present a stopping cri-
terion for query-by-committee-based AL that is
based on the rate of disagreement of the classifiers
in the committee. While our uncertainty conver-
gence criterion can only be applied to uncertainty
sampling, the performance convergence criterion
can be used in a committee-based setting.
Li and Sethi (2006) estimate the conditional er-
ror as a measure of uncertainty in selection (instead
of using it for stopping as we do), using a variable-
bin histogram for improving the error estimates.
They do not evaluate the quality of the probabil-
ity estimates. As with our stopping criterion, we
expect this selection criterion to be the more ef-
fective the more accurate the probability estimates
are. We therefore believe that our method of im-
proving probability estimates based on LOO bins
could improve their selection criterion.
471
Stop crit.  Peak Stop ? Bl sd ? Pk sd
1-Entropy threshold 0.01 80.8 3645 12.0% 1.44 0.7 ?0.68 0.4
MinMax threshold 0.01 80.8 3133 10.3% 0.11 1.0 ?2.0 0.8
Margin threshold 0.01 80.8 3158 10.4% 1.1 0.8 ?1.0 0.8
1-Entropy gradient 0.00005 80.8 4572 15.0% 0.97 0.4 ?1.1 0.5
MinMax gradient 0.00005 80.8 4397 14.5% 1.02 0.4 ?1.1 0.5
Margin gradient 0.00005 80.8 5292 17.5% 0.81 0.3 ?1.32 0.4
Lewis grd. (Median) 0.00005 80.8 2791 9.2% 0.8 1.4 ?1.3 1.4
Lewis grd. (Mean) 0.00005 80.8 3999 13.1% 1.1 0.8 ?0.95 0.6
Table 5: Performance at stopping points (baseline perf. 78.7, Selection: 1-Entropy)
7 Conclusion and Future Work
In this paper, we presented several criteria to stop
the AL process. For stopping the training at a
user-defined performance level, we proposed a
method for estimating classifier performance in a
multiclass classification setting. While we could
achieve acceptable accuracy in estimation of pre-
cision, we find that recall estimation is hard. Esti-
mation is not accurate enough to assist in making
a reliable decision if the performance of the classi-
fier is acceptable for practical use. In the future, we
plan to improve on performance estimation quality,
e.g., by using the variable-bin approach suggested
by Li and Sethi (2006).
Nevertheless, we showed that the gradient of the
performance estimate can successfully be used as
a stopping criterion relative to the optimal perfor-
mance that is attainable on a given pool. We also
describe stopping criteria based on the gradient of
the uncertainty measure of the instances selected
for training. The criteria reliably determine stop-
ping points that result in a performance that is bet-
ter than the supervised baseline and close to the
optimal performance. We believe that these crite-
ria can be applied to any AL setting based on un-
certainty sampling, not just NER.
If it turns out that the maximum possible per-
formance does not meet a user?s expectations, the
user needs to acquire fresh data and refill the pool.
This might lead to an approach to reduce the com-
putational cost of AL we want to evaluate in fu-
ture work: Subdivide a large sampling pool into
smaller sub-pools, run AL sequentially on the sub-
pools. When the stopping criterion is reached,
switch to the next sub-pool.
We also found that uncertainty curves of the se-
lected examples are quite noisy. We would like to
investigate which properties of the training exam-
ples cause these drops in the uncertainty curve.
References
Daume III, Hal. 2007. Frustratingly easy domain adap-
tation. In ACL-07, pages 256?263.
Genkin, A., D.D. Lewis, and D. Madigan. 2007.
Large-scale bayesian logistic regression for text cat-
egorization. Technometrics, 49(3):291?304.
Lewis, D.D. and W.A. Gale. 1994. A sequential algo-
rithm for training text classifiers. ACM SIGIR.
Lewis, D.D. 1995. Evaluating and optimizing au-
tonomous text classification systems. ACM SIGIR.
Li, M. and I.K. Sethi. 2006. Confidence-Based Ac-
tive Learning. IEEE Transactions on Pattern Analy-
sis and Machine Intelligence, 28(8):1251?1261.
Schein, Andrew I. 2005. Active Learning for Logistic
Regression. Ph.D. thesis, University of Pennsylva-
nia.
Sch?utze, H., E. Velipasaoglu, and J.O. Pedersen. 2006.
Performance thresholding in practical text classifica-
tion. In CIKM, pages 662?671.
Shen, Dan, Jie Zhang, Jian Su, Guodong Zhou, and
Chew-Lim Tan. 2004. Multi-criteria-based active
learning for named entity recognition. In ACL ?04.
Tjong Kim Sang, Erik F. and Fien De Meulder.
2003. Introduction to the conll-2003 shared task:
Language-independent named entity recognition. In
Proceedings of CoNLL-2003, pages 142?147.
Tomanek, Katrin, Joachim Wermter, and Udo Hahn.
2007. An approach to text corpus construction
which cuts annotation costs and maintains reusabil-
ity of annotated data. In EMNLP-CoNLL.
Vlachos, Andreas. 2008. A stopping criterion for
active learning. Computer Speech and Language,
22(3):295?312.
Zhu, J. and E. Hovy. 2007. Active learning for word
sense disambiguation with methods for addressing
the class imbalance problem. In EMNLP-CoNLL.
472
Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 917?926,
Honolulu, October 2008. c?2008 Association for Computational Linguistics
A graph-theoretic model of lexical syntactic acquisition
Hinrich Schu?tze and Michael Walsh
Institute for Natural Language Processing
University of Stuttgart, Germany
{hs999,walsh}@ifnlp.org
Abstract
This paper presents a graph-theoretic model of
the acquisition of lexical syntactic representa-
tions. The representations the model learns
are non-categorical or graded. We propose a
new evaluation methodology of syntactic ac-
quisition in the framework of exemplar theory.
When applied to the CHILDES corpus, the
evaluation shows that the model?s graded syn-
tactic representations perform better than pre-
viously proposed categorical representations.
1 Introduction
In recent years, exemplar theory has had great ex-
planatory success in phonetics. Exemplar theory
posits that linguistic production and perception are
not mediated via abstract categories, but that instead
each production and perception of a linguistic unit
is stored and retained. Linguistic inference then di-
rectly operates on these stored exemplars. In this pa-
per, we propose a new approach to lexical syntactic
acquisition in the framework of exemplar theory.
Our approach uses an evaluation measure that
is different from previous work. Lexical syntac-
tic acquisition is most often evaluated with respect
to standard syntactic categories like verb and noun.
Our first contribution in this paper is that we instead
evaluate learned representations in the context of a
syntactic task. This task is the determination of an
aspect of grammaticality that we call local syntactic
coherence.
Our second contribution is a graph-theoretic
model of the acquisition of lexical syntactic rep-
resentations that is more rigorous than previous
heuristic proposals. The graph-theoretic model
can learn both categorical and non-categorical (or
graded) representations. The model is also a unified
framework for syntagmatic and paradigmatic rela-
tions (as will be discussed below), and for lower-
order syntactic relations (those that can be directly
observed from the input) and higher-order syntac-
tic relations (those that require some generalization
from what is directly observable).
Redington et al (1998) give an influential account
of the acquisition of lexical syntactic representations
in which a standard syntactic category like verb or
noun is assigned to each word. Our third contribu-
tion is to show that, in the context of acquisition,
graded representations are superior to standard cat-
egorical representations in supporting judgments of
local syntactic coherence. A graded representation
formalism is one that, for any two words, can rep-
resent a third word whose syntactic properties are
intermediate between the two words (Manning and
Schu?tze, 1999).
Clearly exemplar theory is not the only frame-
work in which lexical acquisition has been explored.
Gleitman (1990) for example argues for syntactic
bootstrapping to infer lexical semantics, work not at
odds with our own (see discussion on the role of se-
mantics below). Our argument for the importance
of distributional evidence does not call into question
the large body of work in child language acquisition
that demonstrates that ?part of the capacity to learn
languages must be ?innate? ? (Gleitman and New-
port, 1995). Tabula rasa learning is not possible. Our
goal is not to show that language acquisition pro-
ceeds with a minimum of inductive bias. Rather, we
attempt to formalize one aspect of language acquisi-
tion, the use of distributional information.
The paper is organized as follows. Section 2 moti-
vates the exemplar-theoretic approach by reviewing
its success in phonetics. Section 3 defines local syn-
tactic coherence, which is the basis for a new evalu-
ation methodology for the acquisition of lexical rep-
resentations. Section 4 develops the graph-theoretic
model. Section 5 compares graded and categorical
representations for the task of inferring local syn-
917
tactic coherence. Section 6 presents our evaluation.
Sections 7 and 8 discuss related and future work, and
present our conclusions.
2 Exemplar theory
The general idea of research into exemplars in
speech production and perception is that encoun-
tered items (segments, words, sentences etc.) are
stored in great detail in memory along with rich
linguistic and extra-linguistic context information.
These exemplars are organized into clouds of mem-
ory traces with similar traces lying close to each
other while dissimilar traces are more distant. A
number of such models have had great success in
accounting for production and perception phenom-
ena in phonetics. E.g., Johnson (1997) offers an
exemplar model which challenges the notion that
speech is perceived through a process of normal-
ization whereby a speaker-specific representation is
mapped or normalized into a speaker-neutral cate-
gorical abstraction. Johnson?s model successfully
treats aspects of vowel perception, sex identifica-
tion, and speaker variability. Crucially, no normal-
ization of percepts into categorical representations
takes place. The correct identification of phonemes
and words in his model is a function of direct com-
parison to richly detailed exemplars stored in mem-
ory. Other examples of exemplar-theoretic phonetic
accounts include (Goldinger, 1997), (Pierrehumbert,
2001), and our own work (Schu?tze et al, 2007). Ex-
emplar theory?s success in phonetics motivates us to
investigate its use as a model for local syntactic phe-
nomena.
3 Local syntactic coherence
In the context sequence model for exemplar-
theoretic phonetics (Wade et al, 2008), we represent
speech using amplitude envelopes derived from the
acoustic signal and then compute similarity as the
integral over the correlation of the two acoustic sig-
nals.
For the syntactic level, we need a representa-
tion that has two key properties of the represen-
tation we use in phonetics in order to support an
exemplar-theoretic account. First, the representa-
tion must be directly derivable from the perceived
input. In particular, it cannot rely on the results of
any disambiguation that would occur either as part
of exemplar-theoretic perception or in further down-
stream processing. Second, it must support similar-
ity computations. Accordingly, we first motivate the
representation we use and then introduce a similarity
measure on these representations.
Representation. There are two main sources1 of
directly observable information about the syntactic
properties of words: semantic cues (e.g., things are
often referred to with nouns) and the neighbors of
a word in sentences that it is used in. In this pa-
per, we only consider the second source of informa-
tion for acquisition, lexical neighbors.2 We further
limit ourselves to the immediate left and right lexical
neighbors (see discussion in Section 7).
When using lexical neighbors as the basis of rep-
resentation, we have to make a basic choice as to
whether we look at left and right neighbors sepa-
rately or whether we only look at the ?correlated?
neighborhood information of left and right neigh-
bors jointly. Our approach is based on the first alter-
native: we separate the processing of left and right
neighbors. We do this for two reasons. First, gener-
alization improves and model complexity decreases
if left-neighbor information and right-neighbor in-
formation are looked at separately. E.g., the right
neighbors of to, might and not are similar because
all three words can be followed by base verbs like
dance: to dance, might dance, (might) not dance.
But their left neighbors are very different.
Second, exemplar-theoretic similarity is best de-
fined at the smallest possible scale in order to allow
optimal matching between parts of the stimulus and
parts of memory. In phonetics, we use a time scale
of 10s of milliseconds or even less. Conceivably,
one could also use segments (e.g., consonants and
vowels) as the smallest unit; however, this would
presume a segmented signal. And segmentation is
part of the perception task we want to explain in the
first place.
Separating left and right neighbors ? which
amounts to looking at left and right local contexts
of each word separately ? is the smallest scale we
can operate at when doing syntactic matching. We
1A comprehensive account of acquisition must also include
morphology. See Christiansen et al (2004).
2Psycholinguistic evidence for the importance of neighbor
information for learning categories includes (Mintz, 2002).
918
choose this small scale for the same reasons as we
choose a small scale in phonetics: to ensure maxi-
mum flexibility when matching parts of the stimulus
with exemplars in memory. Using words, bigrams or
larger units would reduce the flexibility in matching
and require a larger amount of experience (or train-
ing data) to learn a particular generalization.
We refer to the representations of left and right
contexts of a given word as half-words. In other
words, we split a word into two entities, a left half-
word that characterizes its behavior to the left and
a right half-word that characterizes its behavior to
the right. Thus left-context and right-context com-
ponents of the representation of a given focus word
are defined, where a left (right) half-word consists
of a probability distribution over all words that oc-
cur to the left (right) of the focus word and the
dimensionality of the vector for each word is de-
pendent on the number of distinct neighbors (left
and right). For example, having experienced take
doll twice and drop doll once, then the left con-
text distribution, or left half-word of doll, dolll, is
P (take) = 2/3, P (drop) = 1/3. By extension, the
phrase take the doll is represented as the following
six half-words: takel, taker , thel, ther , dolll , and
dollr .
Distance measure. The basic intuition behind lo-
cal syntactic coherence is that an important compo-
nent of syntactic wellformedness ? and a compo-
nent that is of particular importance in acquisition
? is whether a similar sequence has already been
stored as grammatical in memory. The same way
that a phonetic signal that is well-formed in a partic-
ular language has many similar exemplars in mem-
ory, a syntactic sequence should also be licensed by
similar, previously perceived sequences in memory.
To operationalize this notion, we need to be able to
compute the similarity or distance between an in-
put stimulus and exemplars in memory. We do this
by first defining a distance measure for sequences of
fixed length.
The distance ? between two sequences of half-
words < g1, . . . , gn > and < h1, . . . , hn > is de-
fined to be the sum of the distances of their half-
words:
?(<g1, . . . , gn>,<h1, . . . , hn>) =
?n
i=1 ?(gi, hi)
This definition presupposes a definition of the dis-
tance of two half-words which will be given below.
We then call a sequence of n half-words
g1, . . . , gn locally coherent if there is a sequence
h1, . . . , hn in memory with ?(< g1, . . . , gn >,<
h1, . . . , hn >) < ? where ? is a parameter.
Finally, we define a sentence to be locally n-
coherent if all of its subsequences of length n are
locally coherent.
The graph-theoretic model that is introduced in
the next section will be evaluated with respect to
how well it captures local syntactic coherence. This
enables us to evaluate the model with respect to a
task as opposed to its ability to reproduce a particu-
lar linguistic representation of syntactic categories.3
Obviously, the notion of local syntactic coherence
only captures some aspects of syntax ? e.g., it does
not capture long-distance dependencies. However,
it is a plausible component of syntactic competence
and a plausible intermediate step in the acquisition
of syntax.
4 Graph-theoretic model
We briefly review the structuralist notions of syntag-
matic and paradigmatic relationships that have been
frequently used in prior work in NLP (e.g., (Church
et al, 1994)). De Saussure defined a syntagmatic
relationship between two words as their contigu-
ous occurrence in a sentence and a paradigmatic re-
lationship as mutual substitutability (de Saussure,
1962) (although he used the term rapport associ-
atif instead of paradigmatic). E.g., brown and dog
stand in a syntagmatic relationship with each other
in the phrase brown dog; brown and black stand in a
paradigmatic relationship with each other with re-
spect to the position between the and dog in the
phrase the X dog. De Saussure?s conceptualization
of syntactic relationships captures the fact that both
admissible neighbors and admissible substitutes in
language are an important part of the characteriza-
tion of the syntactic properties of a word.
We formalize the two relations as distribu-
tions over words, where we assume a vocabulary
{w1, . . . , wV } and V is the number of words in the
vocabulary.
We denote the left syntagmatic distribution of wi
3Freudenthal et al (2004) have much the same motivation
in introducing an evaluation measure of syntactic acquisition
based on chunking.
919
by pi,s,l,m where i is the vocabulary index of wi, s
stands for syntagmatic, l for left and m is the order
of the distribution as discussed below. Intuitively,
pi,s,l,m(wj) is the probability that word wj occurs to
the left of wi. Similarly, for the left paradigmatic
distribution of wi, pi,p,l,m(wj) is the probability that
wj can be substituted for wi without changing local
syntactic coherence as far as the context to the left
is concerned. Note that we distinguish between left
and right paradigmatic distributions. A word wj can
be a perfect substitute for wi as far as the context to
the left is concerned, but a very unlikely substitute as
far as the context to the right is concerned. E.g., in
the phrase She loves her job, the word him is a good
left-context substitute for her, but a terrible right-
context substitute for her.
We will now show how the syntag-
matic/paradigmatic (henceforth: syn/para) dis-
tributions are defined iteratively, based on the
bigram distribution pww, and grounded by defining
pi,p,l,1 and pi,p,r,1.
pww(wiwj) is the probability that the bigram
wiwj occurs, that is, that wi and wj occur next to
each other (and in that order). We define the V ? V
joint probability matrix J by Jij = pww(wiwj).
Denote by N the diagonal V ?V matrix that con-
tains in Nii the reciprocal of pw(wi) where pw is the
marginal distribution of pww:
V
?
j=1
pww(wiwj) =
V
?
j=1
pww(wjwi) = pw(wi) =
1
Nii
The conditional probability pleft of the fol-
lowing word and the conditional probability
pright of the preceding word can be computed
by multiplying (the transpose of) J and N :
pleft(wi|wj) = pww(wiwj)/pw(wj) = (JN)ij ; and
pright(wi|wj) = (JTN)ij .
The ?grounding? paradigmatic distributions of or-
der 1 are defined as follows.
pi,p,l,1(wj) = pi,p,r,1(wj) =
{
0 if wi 6= wj
1 if wi = wj
In other words, each word has only one perfect left
/ right substitute and that perfect substitute is itself.
We define the syn/para distributions of higher order
recursively:
pi,s,l,m = JNpi,p,l,m (1)
pi,p,r,m pi,s,r,m
woman
girl
boy
man
ran
sang
laughed
cried
Figure 1: The distribution of typical right neighbors (the
right syntagmatic distribution pi,s,r,m) is computed from
the distribution of typical ?right substitutes? (the right
paradigmatic distribution pi,p,r,m).
pi,p,l,m = JTNpi,s,l,m?1 (2)
pi,s,r,m = JTNpi,p,r,m (3)
pi,p,r,m = JNpi,s,r,m?1 (4)
Basic matrix arithmetic shows that pi,s,l,1 is sim-
ply pleft(.|wi) and pi,s,r,1 is pright(.|wi).
For higher orders, the principle underlying Eq.s
1?4 is that when moving from left to right, we use
pright (that is, JTN ), the conditional distribution that
characterizes right neighbors; when moving from
right to left, we use pleft (that is, JN ), the condi-
tional distribution that characterizes left neighbors.
This is graphically shown in Fig. 1.
As illustrated by Fig. 1, the underlying graph for
pi,s,r,m and pi,p,r,m is a weighted bipartite directed
graph that connects the vocabulary on the left with
the vocabulary on the right. A directed edge from
wi on the left to wj on the right is weighted with
pww(wiwj)/pw(wi). A directed edge from wj on
the right to wi on the left (not shown) is weighted
with pww(wiwj)/pw(wj).
Eq.s 1?4 define four Markov chains:
pi,s,l,m = (JNJTN)pi,s,l,m?1 (5)
pi,p,l,m = (JTNJN)pi,p,l,m?1 (6)
pi,s,r,m = (JTNJN)pi,s,r,m?1 (7)
pi,p,r,m = (JNJTN)pi,p,r,m?1 (8)
It is easy to see that pw is a stationary distribution
for Eq. 1?4. Writing ~x for pw, we have:
(JN~x)i =
V
?
j=1
pww(wiwj)
pw(wj)
pw(wj) = pw(wi) = xi
(JTN~x)i =
V
?
j=1
pww(wjwi)
pw(wj)
pw(wj) = pw(wi) = xi
920
Hence, pw is a solution for Eq.s (5)?(8).
The series converge if JNJTN and JTNJN
are ergodic, i.e., if the chain is aperiodic and irre-
ducible (Kemeny and Snell, 1976). Observe that
for many simple probabilistic context-free gram-
mars (PCFGs) the series in Eq. 1?4 will not con-
verge. For simple PCFGs, the alternation between
syntagmatic and paradigmatic distributions is peri-
odic. E.g., if inflected verb forms only occur after
nouns and nouns only before inflected verb forms,
then the right syntagmatic distributions of nouns will
have non-zero activation only for verbs and the right
paradigmatic distributions of nouns will have non-
zero activation only for nouns, thus preventing con-
vergence.4
The key difference between a simple PCFG and
natural language is ambiguity and noise. Because
of ambiguity and noise, JNJTN and JTNJN are
likely to be ergodic ? there is always a small non-
zero probability that two words can occur next to
each other. Ambiguity and noise have the same ef-
fect as teleportation for PageRank (Brin and Page,
1998) in the sense that we can jump from each word
to each other word with non-zero probability.
Assuming that the Markov chains are ergodic, all
four converge to pw: pi,p,r,? = pi,p,l,? = pi,s,r,? =
pi,s,l,? = pw, for 1 ? i ? V .
Thus, in this formalization, given enough itera-
tions, syntagmatic and paradigmatic distributions of
words eventually all become identical with the prior
distribution pw. This is surprising because linguisti-
cally and computationally syntagmatic and paradig-
matic relations are fundamentally different.
However, on closer inspection, we observe that
limiting the number of iterations is often beneficial
when computing solutions to a problem iteratively.
E.g., the expectation-maximization algorithm is of-
ten stopped early because results close to conver-
gence are worse than results obtained after a small
number of iterations. From the point of view of
modeling human language acquisition, early stop-
ping is perhaps also more realistic since humans are
unlikely to perform a large number of iterations.
4However, non-ergodicity of JN does not imply non-
ergodicity of JNJT N and JT NJN , so Eq. (5)?(8) can con-
verge even for non-ergodic JN .
g
g
g
g
g
g
g g g g g g g g g
2 4 6 8 10 12 14
0.
0
0.
1
0.
2
0.
3
0.
4
0.
5
0.
6
0.
7
iteration m
JS
 d
ive
rg
en
ce
 o
f r
ig
ht
 s
yn
t. 
di
st
rib
ut
io
ns
t t t t t t t t t t t t t t t
g
t
elephant?giraffe
elephant?the
Figure 2: The distance between elephant and giraffe
(measured by the Jensen-Shannon divergence) is accu-
rately represented after a number of iterations. The words
elephant and the retain their large distance.
Example 1. For the following matrix J
?
?
?
?
w1 w2 w3
w1 82/1002 77/1002 112/1002
w2 90/1002 18/1002 107/1002
w3 99/1002 120/1002 297/1002
?
?
?
?
we get p1,s,r,1 = (0.31, 0.28, 0.41) by comput-
ing the product JTNp1,p,r,1. E.g., p1,s,r,1(w2) =
pww(w1w2)/pw(w1) ? 1.0 = 77/(82 + 77 + 112) ?
0.28.
By iteration m = 4, the series pi,s,r,m (Eq. (7))
and pi,p,r,m (Eq. (8)) have converged to:
pi,s,r,m = pi,p,r,m = (0.2704, 0.2145, 0.5149)
for all three words wi. One can easily verify that
this is pw. E.g., pw(w1) = (82 + 90 + 99)/1002 =
(82 + 77 + 112)/1002 ? 0.27045.
Example 2. We computed 15 iterations of
syn/para distributions for the corpus: The giraffe
ran. An elephant fell. The man ran. An aunt fell. The
man slept. The aunt slept. Fig. 2 shows that the dis-
tance between the right syntagmatic distributions of
elephant and giraffe is large for m = 1. The reason
is that the two words have no right neighbors in com-
mon. The right neighbors of the two words are ran
and fell. Although ran and fell have no left neighbors
in common, their left neighbors have a right neigh-
bor in common: the word slept. This indirect simi-
larity information is exploited to deduce by iteration
921
15 that the two words are very similar with respect to
their right syntactic context. In contrast, no such in-
ference, even a very indirect one, is possible for the
right contexts of elephant and the. Consequently, the
distance between the two distributions remains high
and unchanged with higher iterations.
In this case, the Markov chain is not ergodic and
the syntagmatic and paradigmatic series (Eq.s (5)?
(8)) do not converge to pw.
5 Experimental evaluation
Recall from Section 3 that our evaluation task is to
discriminate sentences that exhibit local coherence
from those that do not; that sentences are repre-
sented as sequences of half-words; that syntactic co-
herence of a sentence is defined as all subsequences
of a given length n exhibiting local coherence; and
that a subsequence is locally coherent if its distance
from a sequence in memory is less than ?.
These definitions can be applied to the graph
model as follows. A left half-word is a left syntag-
matic (or paradigmatic) distribution and a right half-
word is a right syntagmatic (or paradigmatic) distri-
bution. We compute the distance of two half-words
either as the Jensen-Shannon (JS) divergence (Lin,
1991) or as (1? cos(?)). JS divergence is more ap-
propriate for the comparison of probability distribu-
tions. But the cosine is more efficient when a sparse
vector is compared to a dense vector.5 We therefore
employ the cosine for the compute-intensive experi-
ments in Section 6.
The baseline representation is the categorical rep-
resentation proposed by Redington et al (1998). A
difficulty in replicating their experiments is that they
use hierarchical agglomerative clustering (HAC),
which eventually agglomerates all words in a sin-
gle category. To circumvent the need for a stop-
ping criterion, we represent each word as the tem-
poral sequence of clusters it occurred in during ag-
glomeration and define the distance of two words as
the agglomeration step in which the two words are
joined in a cluster. E.g., given the agglomeration se-
quences {1}, {1, 2}, {1, 2, 4}, {1, 2, 3, 4} for w1 and
{4}, {4}, {1, 2, 4}, {1, 2, 3, 4} for w4, the distance
5This is so because, when computing the cosine, we can ig-
nore all dimensions where one of the two vectors has a zero
value.
between w1 and w4 is 3 since they are joined in step
3 when cluster {1, 2, 4} is created.
For both graded (graph-theoretic) and categorical
(cluster-based) representations, we need to set the
parameter ? that is the boundary between locally co-
herent and locally incoherent sentences. This pa-
rameter gives rise to a precision-recall tradeoff. A
small ? will impose strict requirements on which se-
quences in memory match, resulting in false nega-
tive decisions for local grammaticality. A large ?
will incorrectly judge many locally incoherent se-
quences to be grammatical.
We will pick the optimal ? in both cases. For
categorical representations, this amounts to select-
ing the HAC dendrogram with optimal performance.
The experiment below evaluates whether grammati-
cal and ungrammatical sentences are well separated
by the proposed measure.6
Experiment on CHILDES. We used the well-
known CHILDES database (MacWhinney, 2000), a
corpus of conversations between young children and
their playmates, siblings, and caretakers. In order to
avoid mixing varieties of English (e.g., British En-
glish vs. American English), we selected the largest
homogeneous subcorpus of CHILDES, the Manch-
ester corpus. It contains roughly 350,000 sentences
and 1.5 million words. This is a conservative esti-
mate of the amount of child-directed speech a child
would receive annually (Redington et al, 1998). All
names in the corpus (i.e., all capitalized words) were
replaced with a special word ? n ?. A boundary
symbol ? b ? was introduced to separate sentences.
The representation of the corpus is then a concate-
nation of all its sentences. The vocabulary consists
of V = 8601 words.
Construction of the evaluation set. We tested
the ability of the two models to distinguish locally
coherent vs. incoherent sentences by selecting 100
unattested sentences from the corpus, which were
not used to train the model. We only selected unat-
tested sentences that were not a substring of a sen-
tence in the training corpus since, presumably, any
substring of a sentence in the training corpus is lo-
cally coherent. A further constraint was that the
6This evaluation of ?separation? is not directly an evaluation
of classification performance, but more similar to an evaluation
of ranking using AUC or an evaluation of clustering using a
measure like purity.
922
unattested sentence was not allowed to contain a
word that did not occur in the training corpus, the
rationale being that we want to address the prob-
lem of local coherence for known words only since
unknown words present special challenges. Finally,
we ensured that each unattested sentence contained
a word that occurred in only one sentence type in
the training corpus. In early experiments, we found
that local grammatical inference for frequent words
is easy as there is redundant evidence available that
characterizes legal syntactic environments for fre-
quent words. Since rare words are a key challenge in
syntactic acquisition, we only selected sentences as
unattested sentences that contained at least one rare
word (where a rare word is defined as a word that
occurs once in the training set).
100 ungrammatical sentences were generated by
randomly selecting and concatenating words from
the vocabulary. Ungrammatical sentences were
matched in length to unattested sentences, so that
both sets contained the same number of sentences
of a given length. As with unattested sentences, un-
grammatical sentences that were substrings of sen-
tences in the training corpus were eliminated. As
there are many more infrequent words than frequent
words in the vocabulary, the construction ensured
that, as with unattested sentences, infrequent words
were overrepresented in ungrammatical sentences.
To summarize, our setup consists of 348,463
training sentences, 100 unattested grammatical sen-
tences and 100 ungrammatical sentences.
The task of discriminating the 100 unattested
from the 100 ungrammatical sentences cannot be
solved perfectly as CHILDES contains ungrammat-
ical sentences, a few of which were randomly se-
lected as unattested sentences (e.g., yes pleas, which
is missing the final letter). Similarly, one or two
of the automatically generated ungrammatical sen-
tences were actually grammatical.
Since the test set does not consist of a random
sample of sentences, performance on the test set is
not a direct indicator of the percentage of sentences
that the model can correctly discriminate in a child?s
typical input. A large proportion of sentences in
child input are simple 1-word, 2-word, and 3-word
sentences that even simplistic models can evaluate
with high accuracy. However, the test set is appro-
priate for a comparative evaluation of graded and
x
x
x
x
x
x
x x
x x
2 4 6 8 10
0.
5
0.
6
0.
7
0.
8
0.
9
1.
0
number of half words
a
cc
u
ra
cy
 o
f d
isc
rim
in
at
io
n
c
c c c
c
c
c
c
c c
x
c
graded
categorical
Figure 3: Accuracy of discrimination between grammati-
cal and ungrammatical sentences for graded and categor-
ical representations.
categorical syntactic representations in language ac-
quisition, which is one of the goals of the paper. Dif-
ficult sentences (those with rare words and greater
length) are overrepresented in the test set as the dis-
crimination of short sentences containing only fre-
quent words can easily be done by simplistic mod-
els. Thus, a test set of ?easy? sentences would not
distinguish good models from bad models.
Discrimination experiment. In order to train the
graph model, the entries of matrix J were estimated
using maximum likelihood based on the training
corpus. pi,s,l,1 and pi,s,r,1 were then computed for
all 8601 words. Replicating (Redington et al, 1998),
the most frequent 1000 words were clustered (using
single-link HAC, Manning and Schu?tze (1999)). For
each remaining word w, the closest neighbor w? in
the 1000 most frequent words was determined and
w was then assigned to the cluster of w?.
Fig. 3 shows the performance of graded and cat-
egorical representations for different subsequence
sizes n. To compute the accuracy for each n, the ?
with optimal discrimination performance was cho-
sen (for both graded and categorical).
For a subsequence of size n = 1, the performance
is 0.5 in both cases since the 200-sentence test set
does not contain unknown words. So for every half-
word, there is a sequence of one half-word in the
training corpus with distance 0. Thus, all sentences
923
get the same local coherence scores, both for graded
and categorical representations.
This argument does not apply to n = 2 since we
earlier defined a sentence to be locally coherent if
all of its subsequences are coherent. While subse-
quences of 2 half-words that are part of the same
word have local coherence score 0, this is not true of
subsequences of 2 half-words that are part of differ-
ent words, e.g., the subsequence <blackr,dogl> in
black dog. If black dog does not occur in the train-
ing set, then its local coherence score is > 0.
The main result of the experiment is that except
for n=1 (p = 1) and n=2 (p = 0.39) the differences
between categorical and graded representations are
significant (?2 test, p < 0.05 for 3 ? n ? 10). This
is evidence that graded representations are more ac-
curate when determining local syntactic coherence
and grammaticality than categorical representations.
The experimental results demonstrate that, for
syntagmatic distributions of order 1, graded repre-
sentations discriminate locally coherent vs. incoher-
ent sentences better than categorical representations.
We attribute this to the ability of exemplar theory to
incorporate rich context information into discrimi-
nation decisions. This is of particular importance
for ambiguous words. Categorical representations of
ambiguous words are problematic because they are
either too similar or not similar enough to the two
alternatives. E.g., if a word with a verb/noun ambi-
guity is represented as one of the alternatives, say,
as a verb, then subsequences containing its noun use
will no longer be similar to other subsequences with
nouns. If a special conflation category noun/verb is
introduced, then we are faced with the same prob-
lem: subsequences containing the noun/verb cate-
gory are not similar to subsequences containing ei-
ther non-ambiguous verbs or non-ambiguous nouns.
6 Higher-order distributions
The main motivation for higher-order distributions
is that syntagmatic vectors of order 1 do not per-
form well for some infrequent words. In the ele-
phant/giraffe example above, the distance between
the two words is close to maximum for order 1 repre-
sentations because each occurs only once, in entirely
different contexts. As we showed in Fig. 2, higher-
order representations address this problem because
s
s
s
s
s
s
s
s
s
2 4 6 8 10
0.
70
0.
75
0.
80
0.
85
0.
90
0.
95
number of half words
a
cc
u
ra
cy
 o
f d
isc
rim
in
at
io
n
p p
p
p
p
p
p p
p
t
t
t
t
t
t t
t
t
q
q q
q
q
q q
q
q
s
p
t
q
synt?1
para?2
synt?2
para?3
Figure 4: Accuracy of discrimination between grammat-
ical and ungrammatical sentences of the exemplar-based
method for different orders. Key: synt = syntagmatic,
para = paradigmatic; s is of order 1; p and t are of order
2; q is of order 3.
they exploit indirect evidence about the syntactic
properties of words.
To evaluate higher-order representations on
CHILDES, we used the same setup as before, but
computed several additional iterations. We also lim-
ited the experiments to a subset consisting of 60,000
words of the Manchester corpus. It contains only
V=1666 different words, which reduces the storage
requirements for the syn/para distributions (which is
2 ?V 2 for each order) and the cost of the matrix mul-
tiplications. We also used (1? cos(?)) instead of JS
divergence as distance measure.
The results of the experiment are shown in Fig. 4.
Higher-order representations are clearly superior for
short subsequences, especially for n = 2 and n = 3
(and up to 5 half-words when comparing synt-1 and
para-2). However, for long subsequences, there is no
consistent difference between the syntagmatic distri-
bution of order 1 (synt-1) and higher order distribu-
tions. Apparently, the generalized information avail-
able in higher orders is not helpful in local grammat-
ical inference if long contexts are considered.
We were surprised that the best-performing dis-
tribution for short sequences is para-2 (paradigmatic
distribution of order 2), not a higher order distri-
bution. E.g., para-3 performs worse than para-2.
924
We would expect the performance to decrease with
higher order eventually since the distributions con-
verge towards pw. The fact that this happens so early
in this experiment merits further investigation.
7 Related work
Data-oriented parsing (Bod et al, 2003) shares
basic assumptions about linguistic inference with
exemplar-based theory, but it does not model or use
the similarity between input and stored exemplars.
Previous work on exemplar theory in syntax (Abbot-
Smith and Tomasello, 2006; Bybee, 2006; Hay and
Bresnan, 2006) has not been computational or for-
mal. Previous work on non-categorical representa-
tions of words has viewed these representations as
an intermediate step for arriving at categorical parts
of speech (Redington et al, 1998; Schu?tze, 1995;
Clark, 2003). Consequently, all of these papers eval-
uate their results by comparing induced categories to
gold-standard parts of speech.
Redington et al (1998) did not find a difference in
categorization accuracy between simple syntagmatic
representation and those using non-adjacent words.
The BEAGLE model (Jones and Mewhort, 2007),
and related work (Sahlgren et al, 2008), merges co-
occurrence information and word order information
into a single composite vector through a process of
vector convolution. Our model differs in that it ex-
plicitly captures the recursive relationship between
the orders in a unified framework.
Previous graph-theoretic work (Biemann, 2006)
uses order 1 representations. Several papers have
looked at higher-order representations, but have not
examined the equivalence of syn/para distributions
when formalized as Markov chains (Schu?tze and
Pedersen, 1993; Lund and Burgess, 1996; Edmonds,
1997; Rapp, 2002; Biemann et al, 2004; Lemaire
and Denhie`re, 2006). Toutanova et al (2004) found
that their graph model of predicate argument struc-
ture deteriorated after a small number of iterations
of the random walk, similar to our findings.
8 Conclusions and Future Work
In this paper, we have presented a graph-theoretic
model of the acquisition of lexical syntactic rep-
resentations and a new exemplar-based evaluation
of lexical syntactic acquisition. When applied to
the CHILDES corpus, the evaluation shows that
the graded syntactic representations learned by the
model perform significantly better than previously
proposed categorical representations. An initial
evaluation of high-order representations showed lit-
tle improvement over low-order representations.
In future work, we intend to investigate the in-
fluence of noise and ambiguity on the quality of
the representations in order to characterize when
higher order representations improve generalization
and exemplar-theoretic inference. We also want
to address that the model as it currently stands is
trained under the false assumption that the train-
ing input is grammatical. Ungrammatical test input
which matches a learned ungrammatical sequence
will be deemed grammatical. Future work will ex-
amine how to best treat this challenge, e.g., by using
an estimation of density instead of the simplistic ?1
nearest neighbor? distance used here.
The most important future work concerns class-
based language models. The cognitive-linguistic
tradition we have mainly addressed in this paper
has focused on the task of learning traditional parts
of speech and has usually not discussed the rele-
vance of language models to acquisition. If, as we
have argued, instead of learning traditional parts of
speech the focus should be on performance in par-
ticular language processing tasks (like grammatical-
ity judgments), then language models are the nat-
ural competing account that we must compare our
work to. Of particular relevance are class-based lan-
guage models (e.g., (Saul and Pereira, 1997; Brown
et al, 1992)). In ongoing work, we are attempting
to show that the exemplar-theoretic model performs
better on grammaticality judgments than class-based
language models.
Acknowledgements. This research was funded by
the German Research Council (DFG, Grant SFB
732). We thank K. Rothenha?usler, H. Schmid and
the reviewers for their valuable comments.
References
Abbot-Smith, Kirsten and Michael Tomasello. 2006.
Exemplar-learning and schematization in a usage-
based account of syntactic acquisition. The Linguistic
Review, 23:275?290.
925
Biemann, Chris, Stefan Bordag, and Uwe Quasthoff.
2004. Automatic acquisition of paradigmatic relations
using iterated co-occurrences. In LREC.
Biemann, Chris. 2006. Unsupervised part-of-speech tag-
ging employing efficient graph clustering. In ACL.
Bod, Rens, Remko Scha, and Khalil Sima?an. 2003.
Data-Oriented Parsing. CSLI Publications.
Brin, Sergey and Lawrence Page. 1998. The anatomy
of a large-scale hypertextual web search engine. In
WWW, pages 107?117.
Brown, Peter F., Peter V. deSouza, Robert L. Mercer, Vin-
cent J. Della Pietra, and Jenifer C. Lai. 1992. Class-
based n-gram models of natural language. Comput.
Linguist., 18(4):467?479.
Bybee, Joan L. 2006. From usage to grammar: The
mind?s response to repetition. Language, 82:711?733.
Christiansen, Morten, Luca Onnis, Padraic Monaghan,
and Nick Chater. 2004. Happy endings in language
acquisition. In AMLaP.
Church, Kenneth, Patrick Hanks, Donald Hindle,
William Gale, and Rosamund Moon. 1994. Lexical
substitutability. In Atkins, B.T.S. and A. Zampolli, ed-
itors, Computational Approaches to the Lexicon. OUP.
Clark, Alexander. 2003. Combining distributional and
morphological information for part of speech induc-
tion. In EACL, pages 59?66.
de Saussure, Ferdinand. 1962. Cours de linguistique
ge?ne?rale. Payot, Paris. Originally published in 1916.
Edmonds, Philip. 1997. Choosing the word most typical
in context using a lexical co-occurrence network. In
ACL, pages 507?509.
Freudenthal, Daniel, Julian Pine, and Fernand Gobet.
2004. Resolving ambiguities in the extraction of syn-
tactic categories through chunking. In ICCM.
Gleitman, Lila and Elissa Newport. 1995. The invention
of language by children: Environmental and biologi-
cal influences on the acquisition of language. In Gleit-
man, Lila and Mark Liberman, editors, Language: An
invitation to cognitive science. MIT Press, 2nd edition.
Gleitman, Lila. 1990. The structural sources of verb
meanings. Language Acquisition, 1:3?55.
Goldinger, Stephen D. 1997. Words and voices?
perception and production in an episodic lexicon. In
(Johnson and Mullennix, 1997).
Hay, Jennifer and Joan Bresnan. 2006. Spoken syntax:
The phonetics of giving a hand in New Zealand En-
glish. The Linguistic Review, 23.
Johnson, Keith and John W. Mullennix, editors. 1997.
Talker Variability in Speech Processing. Academic
Press.
Johnson, Keith. 1997. Speech perception without
speaker normalization. In (Johnson and Mullennix,
1997).
Jones, Michael N. and Douglas J.K. Mewhort. 2007.
Representing word meaning and order information in
a composite holographic lexicon. Psychological Re-
view, 114:1?37.
Kemeny, John G. and J. Laurie Snell. 1976. Finite
Markov Chains. Springer, New York.
Lemaire, Benoit and Guy Denhie`re. 2006. Effects of
high-order co-occurrences on word semantic similar-
ity. Behaviour, Brain & Cognition, 18(1).
Lin, Jianhua. 1991. Divergence measures based on the
Shannon entropy. IEEE Trans. Inf. Theory, 37(1):145?
151.
Lund, Kevin and Curt Burgess. 1996. Producing
high-dimensional semantic spaces from lexical co-
occurrence. Behavior Research Methods, Instrumen-
tation, and Computers, 28:203?208.
MacWhinney, Brian. 2000. The CHILDES project:
Tools for analyzing talk. Lawrence Erlbaum.
Manning, Christopher D. and Hinrich Schu?tze. 1999.
Foundations of Statistical Natural Language Process-
ing. MIT Press, Boston, MA.
Mintz, Toben H. 2002. Category induction from dis-
tributional cues in an artificial language. Memory &
Cognition, 30:678?686.
Pierrehumbert, Janet. 2001. Exemplar dynamics: Word
frequency, lenition and contrast. In Bybee, Joan and
Paul Hopper, editors, Frequency and the Emergence of
Linguistic Structure, pages 137?157. Benjamins.
Rapp, Reinhard. 2002. The computation of word as-
sociations: comparing syntagmatic and paradigmatic
approaches. In Coling.
Redington, Martin, Nick Chater, and Steven Finch.
1998. Distributional information: A powerful cue
for acquiring syntactic categories. Cognitive Science,
22(4):425?469.
Sahlgren, Magnus, Anders Holst, and Jussi Karlgren.
2008. Permutations as a means to encode order in
word space. In CogSci.
Saul, Lawrence and Fernando Pereira. 1997. Aggre-
gate and mixed-order markov models for statistical
language processing. In EMNLP, pages 81?89.
Schu?tze, Hinrich and Jan Pedersen. 1993. A vector
model for syntagmatic and paradigmatic relatedness.
In UW Centre for the New OED and Text Research.
Schu?tze, Hinrich, Michael Walsh, Travis Wade, and
Bernd Mo?bius. 2007. Towards a unified exemplar-
theoretic model of phonetic and syntactic phenomena.
In CogSci, Poster Session.
Schu?tze, Hinrich. 1995. Distributional part-of-speech
tagging. In EACL, pages 141?148.
Toutanova, Kristina, Christopher D. Manning, and An-
drew Y. Ng. 2004. Learning random walk models for
inducing word dependency distributions. In ICML.
Wade, Travis, Grzegorz Dogil, Hinrich Schu?tze, Michael
Walsh, and Bernd Mo?bius. 2008. Syllable fre-
quency effects in a context-sensitive segment produc-
tion model. Submitted.
926
Proceedings of the 12th Conference of the European Chapter of the ACL, pages 282?290,
Athens, Greece, 30 March ? 3 April 2009. c?2009 Association for Computational Linguistics
Rich bitext projection features for parse reranking
Alexander Fraser Renjing Wang
Institute for Natural Language Processing
University of Stuttgart
{fraser,wangrg}@ims.uni-stuttgart.de
Hinrich Schu?tze
Abstract
Many different types of features have
been shown to improve accuracy in parse
reranking. A class of features that thus far
has not been considered is based on a pro-
jection of the syntactic structure of a trans-
lation of the text to be parsed. The intu-
ition for using this type of bitext projec-
tion feature is that ambiguous structures
in one language often correspond to un-
ambiguous structures in another. We show
that reranking based on bitext projection
features increases parsing accuracy signif-
icantly.
1 Introduction
Parallel text or bitext is an important knowledge
source for solving many problems such as ma-
chine translation, cross-language information re-
trieval, and the projection of linguistic resources
from one language to another. In this paper, we
show that bitext-based features are effective in ad-
dressing another NLP problem, increasing the ac-
curacy of statistical parsing. We pursue this ap-
proach for a number of reasons. First, one lim-
iting factor for syntactic approaches to statistical
machine translation is parse quality (Quirk and
Corston-Oliver, 2006). Improved parses of bi-
text should result in improved machine translation.
Second, as more and more texts are available in
several languages, it will be increasingly the case
that a text to be parsed is itself part of a bitext.
Third, we hope that the improved parses of bitext
will serve as higher quality training data for im-
proving monolingual parsing using a process sim-
ilar to self-training (McClosky et al, 2006).
It is well known that different languages encode
different types of grammatical information (agree-
ment, case, tense etc.) and that what can be left
unspecified in one language must be made explicit
NP
NP
NP
DT
a
NN
baby
CC
and
NP
DT
a
NN
woman
SBAR
who had gray hair
Figure 1: English parse with high attachment
in another. This information can be used for syn-
tactic disambiguation. However, it is surprisingly
hard to do this well. We use parses and alignments
that are automatically generated and hence imper-
fect. German parse quality is considered to be
worse than English parse quality, and the annota-
tion style is different, e.g., NP structure in German
is flatter.
We conduct our research in the framework of
N-best parse reranking, but apply it to bitext and
add only features based on syntactic projection
from German to English. We test the idea that,
generally, English parses with more isomorphism
with respect to the projected German parse are bet-
ter. The system takes as input (i) English sen-
tences with a list of automatically generated syn-
tactic parses, (ii) a translation of the English sen-
tences into German, (iii) an automatically gen-
erated parse of the German translation, and (iv)
an automatically generated word alignment. We
achieve a significant improvement of 0.66 F1 (ab-
solute) on test data.
The paper is organized as follows. Section 2
outlines our approach and section 3 introduces the
model. Section 4 describes training and section 5
presents the data and experimental results. In sec-
tion 6, we discuss previous work. Section 7 ana-
lyzes our results and section 8 concludes.
282
NP
NP
DT
a
NN
baby
CC
and
NP
NP
DT
a
NN
woman
SBAR
who had gray hair
Figure 2: English parse with low attachment
CNP
NP
ART
ein
NN
Baby
KON
und
NP
ART
eine
NN
Frau
,
,
S
die...
Figure 3: German parse with low attachment
2 Approach
Consider the English sentence ?He saw a baby and
a woman who had gray hair?. Suppose that the
baseline parser generates two parses, containing
the NPs shown in figures 1 and 2, respectively, and
that the semantically more plausible second parse
in figure 2 is correct. How can we determine that
the second parse should be favored? Since we are
parsing bitext, we can observe the German trans-
lation which is ?Er sah ein Baby und eine Frau,
die graue Haare hatte? (glossed: ?he saw a baby
and a woman, who gray hair had?). The singular
verb in the subordinate clause (?hatte?: ?had?) in-
dicates that the subordinate S must be attached low
to ?woman? (?Frau?) as shown in figure 3.
We follow Collins? (2000) approach to discrim-
inative reranking (see also (Riezler et al, 2002)).
Given a new sentence to parse, we first select the
best N parse trees according to a generative model.
Then we use new features to learn discriminatively
how to rerank the parses in this N-best list. We
use features derived using projections of the 1-best
German parse onto the hypothesized English parse
under consideration.
In more detail, we take the 100 best English
parses from the BitPar parser (Schmid, 2004) and
rerank them. We have a good chance of finding the
optimal parse among the 100-best1. An automati-
cally generated word alignment determines trans-
lational correspondence between German and En-
glish. We use features which measure syntactic di-
1Using an oracle to select the best parse results in an F1
of 95.90, an improvement of 8.01 absolute over the baseline.
vergence between the German and English trees to
try to rank the English trees which have less diver-
gence higher. Our test set is 3718 sentences from
the English Penn treebank (Marcus et al, 1993)
which were translated into German. We hold out
these sentences, and train BitPar on the remain-
ing Penn treebank training sentences. The average
F1 parsing accuracy of BitPar on this test set is
87.89%, which is our baseline2. We implement
features based on projecting the German parse to
each of the English 100-best parses in turn via the
word alignment. By performing cross-validation
and measuring test performance within each fold,
we compare our new system with the baseline on
the 3718 sentence set. The overall test accuracy
we reach is 88.55%, a statistically significant im-
provement over baseline of 0.66.
Given a word alignment of the bitext, the sys-
tem performs the following steps for each English
sentence to be parsed:
(i) run BitPar trained on English to generate 100-
best parses for the English sentence
(ii) run BitPar trained on German to generate the
1-best parse for the German sentence
(iii) calculate feature function values which mea-
sure different kinds of syntactic divergence
(iv) apply a model that combines the feature func-
tion values to score each of the 100-best parses
(v) pick the best parse according to the model
3 Model
We use a log-linear model to choose the best En-
glish parse. The feature functions are functions
on the hypothesized English parse e, the German
parse g, and the word alignment a, and they as-
sign a score (varying between 0 and infinity) that
measures syntactic divergence. The alignment of
a sentence pair is a function that, for each English
word, returns a set of German words that the En-
glish word is aligned with as shown here for the
sentence pair from section 2:
Er sah ein Baby und eine Frau , die graue Haare
hatte
He{1} saw{2} a{3} baby{4} and{5} a{6}
woman{7} who{9} had{12} gray{10} hair{11}
Feature function values are calculated either by
taking the negative log of a probability, or by using
a heuristic function which scales in a similar fash-
2The test set is very challenging, containing English sen-
tences of up to 99 tokens.
283
ion3. The form of the log-linear model is shown in
eq. 1. There are M feature functions h1, . . . , hM .
The vector ? is used to control the contribution of
each feature function.
p?(e|g, a) =
exp(??i ?ihi(e, g, a))
?
e? exp(?
?
i ?ihi(e?, g, a))
(1)
Given a vector of weights ?, the best English
parse e? can be found by solving eq. 2. The model
is trained by finding the weight vector ? which
maximizes accuracy (see section 4).
e? = argmax
e
p?(e|g, a)
= argmin
e
exp(
?
i
?ihi(e, g, a)) (2)
3.1 Feature Functions
The basic idea behind our feature functions is that
any constituent in a sentence should play approx-
imately the same syntactic role and have a similar
span as the corresponding constituent in a trans-
lation. If there is an obvious disagreement, it
is probably caused by wrong attachment or other
syntactic mistakes in parsing. Sometimes in trans-
lation the syntactic role of a given semantic consti-
tutent changes; we assume that our model penal-
izes all hypothesized parses equally in this case.
For the initial experiments, we used a set of 34
probabilistic and heuristic feature functions.
BitParLogProb (the only monolingual feature)
is the negative log probability assigned by BitPar
to the English parse. If we set ?1 = 1 and ?i = 0
for all i 6= 1 and evaluate eq. 2, we will select the
parse ranked best by BitPar.
In order to define our feature functions, we first
introduce auxiliary functions operating on indi-
vidual word positions or sets of word positions.
Alignment functions take an alignment a as an ar-
gument. In the descriptions of these functions we
omit a as it is held constant for a sentence pair (i.e.,
an English sentence and its German translation).
f(i) returns the set of word positions of German
words aligned with an English word at position i.
f ?(i) returns the leftmost word position of the
German words aligned with an English word at po-
sition i, or zero if the English word is unaligned.
f?1(i) returns the set of positions of English
3For example, a probability of 1 is a feature value of 0,
while a low probability is a feature value which is ? 0.
words aligned with a German word at position i.
f ??1(i) returns the leftmost word position of the
English words aligned with a German word at po-
sition i, or zero if the German word is unaligned.
We overload the above functions to allow the ar-
gument i to be a set, in which case union is used,
for example, f(i) = ?j?if(j). Positions in a
tree are denoted with integers. First, the POS tags
are numbered from 1 to the length of the sentence
(i.e., the same as the word positions). Constituents
higher in the tree are also indexed using consecu-
tive integers. We refer to the constituent that has
been assigned index i in the tree t as ?constituent i
in tree t? or simply as ?constituent i?. The follow-
ing functions have the English and German trees
as an implicit argument; it should be obvious from
the argument to the function whether the index
i refers to the German tree or the English tree.
When we say ?constituents?, we include nodes
on the POS level of the tree. Our syntactic trees
are annotated with a syntactic head for each con-
stituent. Finally, the tag at position 0 is NULL.
mid2sib(i) returns 0 if i is 0, returns 1 if i has
exactly two siblings, one on the left of i and one
on the right, and otherwise returns 0.
head(i) returns the index of the head of i. The
head of a POS tag is its own position.
tag(i) returns the tag of i.
left(i) returns the index of the leftmost sibling of
i.
right(i) returns the index of the rightmost sibling.
up(i) returns the index of i?s parent.
?(i) returns the set of word positions covered by
i. If i is a set, ? returns all word positions between
the leftmost position covered by any constituent in
the set and the rightmost position covered by any
constituent in the set (inclusive).
n(A) returns the size of the set A.
c(A) returns the number of characters (including
punctuation and excluding spaces) covered by the
constituents in set A.
JpiK is 1 if pi is true, and 0 otherwise.
l and m are the lengths in words of the English and
German sentences, respectively.
3.1.1 Count Feature Functions
Feature CrdBin counts binary events involving
the heads of coordinated phrases. If in the English
parse we have a coordination where the English
CC is aligned only with a German KON, and both
have two siblings, then the value contributed to
CrdBin is 1 (indicating a constraint violation) un-
284
less the head of the English left conjunct is aligned
with the head of the German left conjunct and like-
wise the right conjuncts are aligned. Eq. 3 calcu-
lates the value of CrdBin.
l
?
i=1
J(tag(i) = CCKJ(n(f(i)) = 1K mid2sib(i)
mid2sib(f ?(i)) Jtag(f ?(i)) = KON-CDK
J[head(left(f ?(i))) 6= f ?(head(left(i)))] OR
[head(right(f ?(i))) 6= f ?(head(right(i)))]K (3)
Feature Q simply captures a mismatch between
questions and statements. If an English sentence is
parsed as a question but the parallel German sen-
tence is not, or vice versa, the feature value is 1;
otherwise the value is 0.
3.1.2 Span Projection Feature Functions
Span projection features calculate the percentage
difference between a constituent?s span and the
span of its projection. Span size is measured in
characters or words. To project a constituent in
a parse, we use the word alignment to project all
word positions covered by the constituent and then
look for the smallest covering constituent in the
parse of the parallel sentence.
CrdPrj is a feature that measures the diver-
gence in the size of coordination constituents and
their projections. If we have a constituent (XP1
CC XP2) in English that is projected to a German
coordination, we expect the English and German
left conjuncts to span a similar percentage of their
respective sentences, as should the right conjuncts.
The feature computes a character-based percent-
age difference as shown in eq. 4.
l
?
i=1
Jtag(i) = CCKJn(f(i)) = 1K (4)
Jtag(f ?(i)) = KON-CDK
mid2sib(i)mid2sib(f ?(i))
(|c(?(left(i)))r ?
c(?(left(f ?(i))))
s |
+|c(?(right(i)))r ?
c(?(right(f ?(i))))
s |)
r and s are the lengths in characters of the En-
glish and German sentences, respectively. In the
English parse in figure 1, the left conjunct has 5
characters and the right conjunct has 6, while in
figure 2 the left conjunct has 5 characters and the
right conjunct has 20. In the German parse (fig-
ure 3) the left conjunct has 7 characters and the
right conjunct has 27. Finally, r = 33 and s = 42.
Thus, the value of CrdPrj is 0.48 for the first hy-
pothesized parse and 0.05 for the second, which
captures the higher divergence of the first English
parse from the German parse.
POSParentPrj is based on computing the span
difference between all the parent constituents of
POS tags in a German parse and their respective
coverage in the corresponding hypothesized parse.
The feature value is the sum of all the differences.
POSPar(i) is true if i immediately dominates a
POS tag. The projection direction is from German
to English, and the feature computes a percentage
difference which is character-based. The value of
the feature is calculated in eq. 5, where M is the
number of constituents (including POS tags) in the
German tree.
M
?
i=1
JPOSPar(i)K|c(?(i))s ?
c(?(f?1(?(i))))
r |
(5)
The right conjunct in figure 3 is a POSParent
that corresponds to the coordination NP in fig-
ure 1, contributing a score of 0.21, and to the right
conjunct in figure 2, contributing a score of 0.04.
For the two parses of the full sentences contain-
ing the NPs in figure 1 and figure 2, we sum over
7 POSParents and get a value of 0.27 for parse 1
and 0.11 for parse 2. The lower value for parse
2 correctly captures the fact that the first English
parse has higher divergence than the second due to
incorrect high attachment.
AbovePOSPrj is similar to POSParentPrj, but
it is word-based and the projection direction is
from English to German. Unlike POSParentPrj
the feature value is calculated over all constituents
above the POS level in the English tree.
Another span projection feature function is
DTNNPrj, which projects English constituents of
the form (NP(DT)(NN)). DTNN(i) is true if i
is an NP immediately dominating only DT and
NN. The feature computes a percentage difference
which is word-based, shown in eq. 6.
L
?
i=1
JDTNN(i)K|n(?(i))l ?
n(?(f(?(i))))
m | (6)
L is the number of constituents in the English
tree. This feature is designed to disprefer parses
285
where constituents starting with ?DT NN?, e.g.,
(NP (DT NN NN NN)), are incorrectly split into
two NPs, e.g., (NP (DT NN)) and (NP (NN NN)).
This feature fires in this case, and projects the (NP
(DT NN)) into German. If the German projection
is a surprisingly large number of words (as should
be the case if the German also consists of a deter-
miner followed by several nouns) then the penalty
paid by this feature is large. This feature is impor-
tant as (NP (DT NN)) is a very common construc-
tion.
3.1.3 Probabilistic Feature Functions
We use Europarl (Koehn, 2005), from which we
extract a parallel corpus of approximately 1.22
million sentence pairs, to estimate the probabilis-
tic feature functions described in this section.
For the PDepth feature, we estimate English
parse depth probability conditioned on German
parse depth from Europarl by calculating a sim-
ple probability distribution over the 1-best parse
pairs for each parallel sentence. A very deep Ger-
man parse is unlikely to correspond to a flat En-
glish parse and we can penalize such a parse using
PDepth. The index i refers to a sentence pair in
Europarl, as does j. Let li and mi be the depths
of the top BitPar ranked parses of the English and
German sentences, respectively. We calculate the
probability of observing an English tree of depth
l? given German tree of depth m? as the maxi-
mum likelihood estimate, shown in eq. 7, where
?(z, z?) = 1 if z = z? and 0 otherwise. To avoid
noisy feature values due to outliers and parse er-
rors, we bound the value of PDepth at 5 as shown
in eq. 84.
p(l?|m?) =
?
i ?(l?, li)?(m?,mi)
?
j ?(m?,mj)
(7)
min(5,? log10(p(l?|m?))) (8)
The full parse of the sentence containing the En-
glish high attachment has a parse depth of 8 while
the full parse of the sentence containing the En-
glish low attachment has a depth of 9. Their fea-
ture values given the German parse depth of 6 are
? log10(0.12) = 0.93 and ? log10(0.14) = 0.84.
The wrong parse is assigned a higher feature value
indicating its higher divergence.
The feature PTagEParentGPOSGParent mea-
sures tagging inconsistency based on estimating
4Throughout this paper, assume log(0) = ??.
the probability that for an English word at posi-
tion i, the parent of its POS tag has a particular
label. The feature value is calculated in eq. 10.
q(i, j) = p(tag(up(i))|tag(j), tag(up(j))) (9)
l
?
i=1
min(5,
?
j?f(i) ? log10(q(i, j))
n(f(i)) ) (10)
Consider (S(NP(NN fruit))(VP(V flies))) and
(NP(NN fruit)(NNS flies)) with the translation
(NP(NNS Fruchtfliegen)). Assume that ?fruit?
and ?flies? are aligned with the German com-
pound noun ?Fruchtfliegen?. In the incorrect En-
glish parse the parent of the POS of ?fruit? is
NP and the parent of the POS of ?flies? is VP,
while in the correct parse the parent of the POS of
?fruit? is NP and the parent of the POS of ?flies?
is NP. In the German parse the compound noun
is POS-tagged as an NNS and the parent is an
NP. The probabilities considered for the two En-
glish parses are p(NP|NNS,NP) for ?fruit? in both
parses, p(VP|NNS,NP) for ?flies? in the incorrect
parse, and p(NP|NNS,NP) for ?flies? in the cor-
rect parse. A German NNS in an NP has a higher
probability of being aligned with a word in an En-
glish NP than with a word in an English VP, so the
second parse will be preferred.
As with the PDepth feature, we use relative
frequency to estimate this feature. When an En-
glish word is aligned with two words, estimation is
more complex. We heuristically give each English
and German pair one count. The value calculated
by the feature function is the geometric mean5 of
the pairwise probabilities, see eq. 10.
3.1.4 Other Features
Our best system uses the nine features we have
described in detail so far. In addition, we imple-
mented the following 25 other features, which did
not improve performance (see section 7): (i) 7
?ptag? features similar to PTagEParentGPOSG-
Parent but predicting and conditioning on differ-
ent combinations of tags (POS tag, parent of POS,
grandparent of POS)
(ii) 10 ?prj? features similar to POSParentPrj
measuring different combinations of character and
word percentage differences at the POS parent and
5Each English word has the same weight regardless of
whether it was aligned with one or with more German words.
286
POS grandparent levels, projecting from both En-
glish and German
(iii) 3 variants of the DTNN feature function
(iv) A NPPP feature function, similar to the
DTNN feature function but trying to counteract a
bias towards (NP (NP) (PP)) units
(v) A feature function which penalizes aligning
clausal units to non-clausal units
(vi) The BitPar rank
4 Training
Log-linear models are often trained using the
Maximum Entropy criterion, but we train our
model directly to maximize F1. We score F1 by
comparing hypothesized parses for the discrimi-
native training set with the gold standard. To try
to find the optimal ? vector, we perform direct ac-
curacy maximization, meaning that we search for
the ? vector which directly optimizes F1 on the
training set.
Och (2003) has described an efficient exact one-
dimensional accuracy maximization technique for
a similar search problem in machine translation.
The technique involves calculating an explicit
representation of the piecewise constant function
gm(x) which evaluates the accuracy of the hy-
potheses which would be picked by eq. 2 from a
set of hypotheses if we hold all weights constant,
except for the weight ?m, which is set to x. This
is calculated in one pass over the data.
The algorithm for training is initialized with a
choice for ? and is described in figure 4. The func-
tion F1(?) returns F1 of the parses selected using
?. Due to space we do not describe step 8 in detail
(see (Och, 2003)). In step 9 the algorithm per-
forms approximate normalization, where feature
weights are forced towards zero. The implemen-
tation of step 9 is straight-forward given the M
explicit functions gm(x) created in step 8.
5 Data and Experiments
We used the subset of the Wall Street Journal
investigated in (Atterer and Schu?tze, 2007) for
our experiments, which consists of all sentences
that have at least one prepositional phrase attach-
ment ambiguity. This difficult subset of sentences
seems particularly interesting when investigating
the potential of information in bitext for improv-
ing parsing performance. The first 500 sentences
of this set were translated from English to German
by a graduate student and an additional 3218 sen-
1: Algorithm TRAIN(?)
2: repeat
3: add ? to the set s
4: let t be a set of 1000 randomly generated vectors
5: let ? = argmax??(s?t) F1(?)
6: let ?? = ?
7: repeat
8: repeatedly run one-dimensional error minimiza-
tion step (updating a single scalar of the vector ?)
until no further error reduction
9: adjust each scalar of ? in turn towards 0 such that
there is no increase in error (if possible)
10: until no scalar in ? changes in last two steps (8 and
9)
11: until ? = ??
12: return ?
Figure 4: Sketch of the training algorithm
tences by a translation bureau. We withheld these
3718 English sentences (and an additional 1000
reserved sentences) when we trained BitPar on the
Penn treebank.
Parses. We use the BitPar parser (Schmid,
2004) which is based on a bit-vector im-
plementation (cf. (Graham et al, 1980)) of
the Cocke-Younger-Kasami algorithm (Kasami,
1965; Younger, 1967). It computes a compact
parse forest for all possible analyses. As all pos-
sible analyses are computed, any number of best
parses can be extracted. In contrast, other treebank
parsers use sophisticated search strategies to find
the most probable analysis without examining the
set of all possible analyses (Charniak et al, 1998;
Klein and Manning, 2003). BitPar is particularly
useful for N-best parsing as the N-best parses can
be computed efficiently.
For the 3718 sentences in the translated set, we
created 100-best English parses and 1-best Ger-
man parses. The German parser was trained on
the TIGER treebank. For the Europarl corpus, we
created 1-best parses for both languages.
Word Alignment. We use a word alignment
of the translated sentences from the Penn tree-
bank, as well as a word alignment of the Europarl
corpus. We align these two data sets together
with data from the JRC Acquis (Steinberger et al,
2006) to try to obtain better quality alignments (it
is well known that alignment quality improves as
the amount of data increases (Fraser and Marcu,
2007)). We aligned approximately 3.08 million
sentence pairs. We tried to obtain better alignment
quality as alignment quality is a problem in many
cases where syntactic projection would otherwise
work well (Fossum and Knight, 2008).
287
System Train +base Test +base
1 Baseline 87.89 87.89
2 Contrastive 88.70 0.82 88.45 0.56
(5 trials/fold)
3 Contrastive 88.82 0.93 88.55 0.66
(greedy selection)
Table 1: Average F1 of 7-way cross-validation
To generate the alignments, we used Model 4
(Brown et al, 1993), as implemented in GIZA++
(Och and Ney, 2003). As is standard practice, we
trained Model 4 with English as the source lan-
guage, and then trained Model 4 with German as
the source language, resulting in two Viterbi align-
ments. These were combined using the Grow Diag
Final And symmetrization heuristic (Koehn et al,
2003).
Experiments. We perform 7-way cross-
validation on 3718 sentences. In each fold of the
cross-validation, the training set is 3186 sentences,
while the test set is 532 sentences. Our results are
shown in table 1. In row 1, we take the hypothesis
ranked best by BitPar. In row 2, we train using the
algorithm outlined in section 4. To cancel out any
effect caused by a particularly effective or ineffec-
tive starting ? value, we perform 5 trials each time.
Columns 3 and 5 report the improvement over the
baseline on train and test respectively. We reach
an improvement of 0.56 over the baseline using
the algorithm as described in section 4.
Our initial experiments used many highly cor-
related features. For our next experiment we use
greedy feature selection. We start with a ? vector
that is zero for all features, and then run the error
minimization without the random generation of
vectors (figure 4, line 4). This means that we add
one feature at a time. This greedy algorithm winds
up producing a vector with many zero weights. In
row 3 of table 1, we used the greedy feature selec-
tion algorithm and trained using F1, resulting in
a performance of 0.66 over the baseline which is
our best result. We performed a planned one-tailed
paired t-test on the F1 scores of the parses selected
by the baseline and this system for the 3718 sen-
tences (parses were taken from the test portion
of each fold). We found that there is a signifi-
cant difference with the baseline (t(3717) = 6.42,
p < .01). We believe that using the full set of 34
features (many of which are very similar to one
another) made the training problem harder with-
out improving the fit to the training data, and that
greedy feature selection helps with this (see also
section 7).
6 Previous Work
As we mentioned in section 2, work on parse
reranking is relevant, but a vital difference is that
we use features based only on syntactic projection
of the two languages in a bitext. For an overview
of different types of features that have been used in
parse reranking see Charniak and Johnson (2005).
Like Collins (2000) we use cross-validation to
train our model, but we have access to much less
data (3718 sentences total, which is less than 1/10
of the data Collins used). We use rich feature func-
tions which were designed by hand to specifically
address problems in English parses which can be
disambiguated using the German translation.
Syntactic projection has been used to bootstrap
treebanks in resource poor languages. Some ex-
amples of projection of syntactic parses from En-
glish to a resource poor language for which no
parser is available are the works of Yarowsky and
Ngai (2001), Hwa et al (2005) and Goyal and
Chatterjee (2006). Our work differs from theirs
in that we are performing a parse reranking task
in English using knowledge gained from German
parses, and parsing accuracy is generally thought
to be worse in German than in English.
Hopkins and Kuhn (2006) conducted research
with goals similar to ours. They showed how to
build a powerful generative model which flexibly
incorporates features from parallel text in four lan-
guages, but were not able to show an improvement
in parsing performance. After the submission of
our paper for review, two papers outlining relevant
work were published. Burkett and Klein (2008)
describe a system for simultaneously improving
Chinese and English parses of a Chinese/English
bitext. This work is complementary to ours. The
system is trained using gold standard trees in both
Chinese and English, in contrast with our system
which only has access to gold standard trees in En-
glish. Their system uses a tree alignment which
varies within training, but this does not appear to
make a large difference in performance. They use
coarsely defined features which are language in-
dependent. We use several features similar to their
two best performing sets of features, but in con-
trast with their work, we also define features which
are specifically aimed at English disambiguation
problems that we have observed can be resolved
288
using German parses. They use an in-domain
Chinese parser and out-of-domain English parser,
while for us the English parser is in-domain and
the German parser is out-of-domain, both of which
make improving the English parse more difficult.
Their Maximum Entropy training is more appro-
priate for their numerous coarse features, while
we use Minimum Error Rate Training, which is
much faster. Finally, we are projecting from a sin-
gle German parse which is a more difficult prob-
lem. Fossum and Knight (2008) outline a system
for using Chinese/English word alignments to de-
termine ambiguous English PP-attachments. They
first use an oracle to choose PP-attachment deci-
sions which are ambiguous in the English side of a
Chinese/English bitext, and then build a classifier
which uses information from a word alignment to
make PP-attachment decisions. No Chinese syn-
tactic information is required. We use automati-
cally generated German parses to improve English
syntactic parsing, and have not been able to find a
similar phenomenon for which only a word align-
ment would suffice.
7 Analysis
We looked at the weights assigned during the
cross-validation performed to obtain our best re-
sult. The weights of many of the 34 features we
defined were frequently set to zero. We sorted
the features by the number of times the relevant
? scalar was zero (i.e., the number of folds of
the cross-validation for which they were zero; the
greedy feature selection is deterministic and so we
do not run multiple trials). We then reran the same
greedy feature selection algorithm as was used in
table 1, row 3, but this time using only the top
9 feature values, which were the features which
were active on 4 or more folds6. The result was an
improvement on train of 0.84 and an improvement
on test of 0.73. This test result may be slightly
overfit, but the result supports the inference that
these 9 feature functions are the most important.
We chose these feature functions to be described
in detail in section 3. We observed that the variants
of the similar features POSParentPrj and Above-
POSPrj projected in opposite directions and mea-
sured character and word differences, respectively,
and this complementarity seems to help.
6We saw that many features canceled one another out on
different folds. For instance either the word-based or the
character-based version of DTNN was active in each fold,
but never at the same time as one another.
We also tried to see if our results depended
strongly on the log-linear model and training algo-
rithm, by using the SVM-Light ranker (Joachims,
2002). In order to make the experiment tractable,
we limited ourselves to the 8-best parses (rather
than 100-best). Our training algorithm and model
was 0.74 better than the baseline on train and 0.47
better on test, while SVM-Light was 0.54 better
than baseline on train and 0.49 better on test (us-
ing linear kernels). We believe that the results are
not unduly influenced by the training algorithm.
8 Conclusion
We have shown that rich bitext projection features
can improve parsing accuracy. This confirms the
hypothesis that the divergence in what information
different languages encode grammatically can be
exploited for syntactic disambiguation. Improved
parsing due to bitext projection features should be
helpful in syntactic analysis of bitexts (by way of
mutual syntactic disambiguation) and in comput-
ing syntactic analyses of texts that have transla-
tions in other languages available.
Acknowledgments
This work was supported in part by Deutsche
Forschungsgemeinschaft Grant SFB 732. We
would like to thank Helmut Schmid for support of
BitPar and for his many helpful comments on our
work. We would also like to thank the anonymous
reviewers.
References
Michaela Atterer and Hinrich Schu?tze. 2007. Preposi-
tional phrase attachment without oracles. Computa-
tional Linguistics, 33(4).
Peter F. Brown, Stephen A. Della Pietra, Vincent J.
Della Pietra, and R. L. Mercer. 1993. The mathe-
matics of statistical machine translation: parameter
estimation. Computational Linguistics, 19(2).
David Burkett and Dan Klein. 2008. Two lan-
guages are better than one (for syntactic parsing). In
EMNLP.
Eugene Charniak and Mark Johnson. 2005. Coarse-
to-fine n-best parsing and MaxEnt discriminative
reranking. In ACL.
Eugene Charniak, Sharon Goldwater, and Mark John-
son. 1998. Edge-based best-first chart parsing. In
Proceedings of the Sixth Workshop on Very Large
Corpora.
289
Michael Collins. 2000. Discriminative reranking for
natural language parsing. In ICML.
Victoria Fossum and Kevin Knight. 2008. Using bilin-
gual Chinese-English word alignments to resolve
PP-attachment ambiguity in English. In AMTA.
Alexander Fraser and Daniel Marcu. 2007. Measuring
word alignment quality for statistical machine trans-
lation. Computational Linguistics, 33(3).
Shailly Goyal and Niladri Chatterjee. 2006. Parsing
aligned parallel corpus by projecting syntactic re-
lations from annotated source corpus. In Proceed-
ings of the COLING/ACL main conference poster
sessions.
Susan L. Graham, Michael A. Harrison, and Walter L.
Ruzzo. 1980. An improved context-free recognizer.
ACM Transactions on Programming Languages and
Systems, 2(3).
Mark Hopkins and Jonas Kuhn. 2006. A framework
for incorporating alignment information in parsing.
In Proceedings of the EACL 2006 Workshop on
Cross-Language Knowledge Induction.
Rebecca Hwa, Philip Resnik, Amy Weinberg, Clara
Cabezas, and Okan Kolak. 2005. Bootstrapping
parsers via syntactic projection across parallel texts.
Nat. Lang. Eng., 11(3).
Thorsten Joachims. 2002. Optimizing search en-
gines using clickthrough data. In Proceedings of the
Eighth ACM SIGKDD.
Takao Kasami. 1965. An efficient recognition and syn-
tax analysis algorithm for context-free languages.
Technical Report AFCRL-65-7558, Air Force Cam-
bridge Research Laboratory.
Dan Klein and Christopher Manning. 2003. A* pars-
ing: fast exact viterbi parse selection. In HLT-
NAACL.
Philipp Koehn, Franz J. Och, and Daniel Marcu. 2003.
Statistical phrase-based translation. In HLT-NAACL.
Philipp Koehn. 2005. Europarl: a parallel corpus for
statistical machine translation. In MT Summit X.
Mitchell P. Marcus, Beatrice Santorini, and Mary Ann
Marcinkiewicz. 1993. Building a large annotated
corpus of English: the Penn treebank. Computa-
tional Linguistics, 19(2).
David McClosky, Eugene Charniak, and Mark John-
son. 2006. Effective self-training for parsing. In
HLT-NAACL.
Franz J. Och and Hermann Ney. 2003. A systematic
comparison of various statistical alignment models.
Computational Linguistics, 29(1).
Franz J. Och. 2003. Minimum error rate training in
statistical machine translation. In ACL.
Chris Quirk and Simon Corston-Oliver. 2006. The im-
pact of parse quality on syntactically-informed sta-
tistical machine translation. In EMNLP.
Stefan Riezler, Tracy H. King, Ronald M. Kaplan,
Richard S. Crouch, John T. Maxwell III, and Mark
Johnson. 2002. Parsing the Wall Street Journal us-
ing a lexical-functional grammar and discriminative
estimation techniques. In ACL.
Helmut Schmid. 2004. Efficient parsing of highly am-
biguous context-free grammars with bit vectors. In
COLING.
Ralf Steinberger, Bruno Pouliquen, Anna Widiger,
Camelia Ignat, Tomaz Erjavec, Dan Tufis, and
Daniel Varga. 2006. The JRC-Acquis: a multilin-
gual aligned parallel corpus with 20+ languages. In
LREC.
David Yarowsky and Grace Ngai. 2001. Inducing mul-
tilingual POS taggers and NP bracketers via robust
projection across aligned corpora. In NAACL.
Daniel H. Younger. 1967. Recognition of context-free
languages in time n3. Information and Control, 10.
290
Proceedings of the 12th Conference of the European Chapter of the ACL, pages 728?736,
Athens, Greece, 30 March ? 3 April 2009. c?2009 Association for Computational Linguistics
Frequency Matters: Pitch Accents and Information Status
Katrin Schweitzer, Michael Walsh, Bernd Mo?bius,
Arndt Riester, Antje Schweitzer, Hinrich Schu?tze
University of Stuttgart
Stuttgart, Germany
<firstname>.<surname>@ims.uni-stuttgart.de
Abstract
This paper presents the results of a series
of experiments which examine the impact
of two information status categories (given
and new) and frequency of occurrence on
pitch accent realisations. More specifi-
cally the experiments explore within-type
similarity of pitch accent productions and
the effect information status and frequency
of occurrence have on these productions.
The results indicate a significant influence
of both pitch accent type and information
status category on the degree of within-
type variability, in line with exemplar-
theoretic expectations.
1 Introduction
It seems both intuitive and likely that prosody
should have a significant role to play in marking
information status in speech. While there are well
established expectations concerning typical asso-
ciations between categories of information status
and categories of pitch accent, e.g. rising L?H
accents are often a marker for givenness, there
is nevertheless some variability here (Baumann,
2006). Furthermore, little research has focused on
how pitch accent tokens of the same type are re-
alised nor have the effects of information status
and frequency of occurrence been considered.
From the perspective of speech technology, the
tasks of automatically inferring and assigning in-
formation status clearly have significant impor-
tance for speech synthesis and speech understand-
ing systems.
The research presented in this paper examines a
number of questions concerning the relationship
between two information status categories (new
and given), and how tokens of associated pitch ac-
cent types are realised. Furthermore the effect of
frequency of occurrence is also examined from an
exemplar-theoretic perspective.
The questions directly addressed in this paper
are as follows:
1. How are different tokens of a pitch accent
type realised?
Does frequency of occurrence of the pitch ac-
cent type play a role?
2. What effect does information status have on
realisations of a pitch accent type?
Does frequency of occurrence of the informa-
tion status category play a role?
3. Does frequency of occurrence in pitch ac-
cents and in information status play a role,
i.e. is there a combined effect?
In examining the realisation of pitch accent to-
kens, their degree of similarity is the characteristic
under investigation. Similarity is calculated by de-
termining the cosine of the angle between pairs of
pitch accent vector representations (see section 6).
The results in this study are examined from
an exemplar-theoretic perspective (see section 3).
The expectations within that framework are based
upon two different aspects. Firstly, it is expected
that, since all exemplars are stored, exemplars of
a type that occur often, offer the speaker a wider
selection of exemplars to choose from during pro-
duction (Schweitzer and Mo?bius, 2004), i.e. the
realisations are expected to be more variable than
those of a rare type. However, another aspect of
Exemplar Theory has to be considered, namely en-
trenchment (Pierrehumbert, 2001; Bybee, 2006).
The central idea here is that frequently occurring
behaviours undergo processes of entrenchment,
they become in some sense routine. Therefore re-
alisations of a very frequent type are expected to
be realised similar to each other. Thus, similarity
and variability are expressions of the same charac-
teristic: the higher the degree of similarity of pitch
accent tokens, the lower their realisation variabil-
ity.
728
The structure of this paper is as follows: Sec-
tion 2 briefly examines previous work on the in-
teraction of information status categories and pitch
accents. Section 3 provides a short introduction to
Exemplar Theory. In this study similarity of pitch
accent realisations on syllables, annotated with the
information status categories of the words they be-
long to, is examined using the parametric intona-
tion model (Mo?hler, 1998) which is outlined in
Section 4. Section 5 discusses the corpus em-
ployed. Section 6 introduces a general methodol-
ogy which is used in the experiments in Sections 7,
8 and 9. Section 10 then presents some discussion,
conclusions and opportunities for future research.
2 Information Status and Intonation
It is commonly assumed that pitch accents are the
main correlate of information status1 in speech
(Halliday, 1967). Generally, accenting is said
to signal novelty while deaccenting signals given
information (Brown, 1983), although there is
counter evidence: various studies note given in-
formation being accented (Yule, 1980; Bard and
Aylett, 1999). Terken and Hirschberg (1994) point
out that new information can also be deaccented.
As for the question of which pitch accent type
(in terms of ToBI categories (Silverman et al,
1992)) is typically assigned to different degrees of
givenness, Pierrehumbert and Hirschberg (1990)
find H? to be the standard novelty accent for En-
glish, a finding which has also been confirmed by
Baumann (2006) and Schweitzer et al (2008) for
German. Given information on the other hand, if
accented at all, is found to carry L? accent in En-
glish (Pierrehumbert and Hirschberg, 1990). Bau-
mann (2006) finds deaccentuation to be the most
preferred realisation for givenness in his experi-
mental phonetics studies on German. However,
Baumann (2006) points out that H+L? has also
been found as a marker of givenness in a German
corpus study. Previous findings on the corpus used
in the present study found L?H being the typical
marker for givenness (Schweitzer et al, 2008).
Leaving the phonological level and examining
correlates of information status in acoustic detail,
Kohler (1991) reports that in a falling accent, an
early peak indicates established facts, while a me-
dial peak is used to mark novelty. In a recent
1The term information status is used in (Prince, 1992) for
the first time. Before that the terms givenness, novelty or in-
formation structure were used for these concepts.
study Ku?gler and Fe?ry (2008) found givenness to
lower the high tones of prenuclear pitch accents
and to cancel them out postnuclearly. These find-
ings among others (Ku?gler and Fe?ry, 2008) moti-
vate an examination of the acoustic detail of pitch
accent shape across different information status
categories.
The experiments presented here go one step fur-
ther, however, in that they also investigate poten-
tial exemplar-theoretic effects.
3 Exemplar Theory
Exemplar Theory is concerned with the idea that
the acquisition of language is significantly facil-
itated by repeated exposure to concrete language
input, and it has successfully accounted for a num-
ber of language phenomena, including diachronic
language change and frequency of occurrence ef-
fects (Bybee, 2006), the emergence of gram-
matical knowledge (Abbot-Smith and Tomasello,
2006), syllable duration variability (Schweitzer
and Mo?bius, 2004; Walsh et al, 2007), entrench-
ment and lenition (Pierrehumbert, 2001), among
others. Central to Exemplar Theory are the notions
of exemplar storage, frequency of occurrence, re-
cency of occurrence, and similarity. There is an
increasing body of evidence which indicates that
significant storage of language input exemplars,
rich in detail, takes place in memory (Johnson,
1997; Croot and Rastle, 2004; Whiteside and Var-
ley, 1998). These stored exemplars are then em-
ployed in the categorisation of new input percepts.
Similarly, production is facilitated by accessing
these stored exemplars. Computational models of
the exemplar memory also argue that it is in a con-
stant state of flux with new inputs updating it and
old unused exemplars gradually fading away (Pier-
rehumbert, 2001).
Up to now, virtually no exemplar-theoretic re-
search has examined pitch accent prosody (but
see Marsi et al (2003) for memory-based predic-
tion of pitch accents and prosodic boundaries, and
Walsh et al (2008)(discussed below)) and to the
authors? knowledge this paper represents the first
attempt to examine the relationship between pitch
accent prosody and information status from an
exemplar-theoretic perspective. Given the consid-
erable weight of evidence for the influence of fre-
quency of occurrence effects in a variety of other
linguistic domains it seems reasonable to explore
such effects on pitch accent and information sta-
729
tus realisations. For example, what effect might
givenness have on a frequently/infrequently occur-
ring pitch accent? Does novelty produce a similar
result?
The search for possible frequency of occur-
rence effects takes place with respect to pitch ac-
cent shapes captured by the parametric intonation
model discussed next.
4 The Parametric Representation of
Intonation Events - PaIntE
The model approximates stretches of F0 by em-
ploying a phonetically motivated model function
(Mo?hler, 1998). This function consists of the sum
of two sigmoids (rising and falling) with a fixed
time delay which is selected so that the peak does
not fall below 96% of the function?s range. The re-
sulting function has six parameters which describe
the contour and were employed in the analysis: pa-
rameters a1 and a2 express the gradient of the ac-
cent?s rise and fall, parameter b describes the ac-
cent?s temporal alignment (which has been shown
to be crucial in the description of an accent?s shape
(van Santen and Mo?bius, 2000)), c1 and c2 model
the ranges of the rising and falling amplitude of
the accent?s contour, respectively, and parameter d
expresses the peak height of the accent.2 These six
parameters are thus appropriate to describe differ-
ent pitch accent shapes.
For the annotation of intonation the GToBI(S)
annotation scheme (Mayer, 1995) was used. In
earlier versions of PaIntE, the approximation of
the F0-contour for H?L and H? was carried out on
the accented and post?accented syllables. How-
ever, for these accents the beginning of the rise is
likely to start at the preaccented syllable. In the
current version of PaIntE the window used for the
approximation of the F0-contour for H?L and H?
accents has been extended to the preaccented syl-
lable, so that the parameters are calculated over
the span of the accented syllables and its immedi-
ate neighbours (unless it is followed by a boundary
tone which causes the window to end at the end of
the accented syllable).
5 Corpus
The experiments that follow (sections 7, 9 and 8),
were carried out on German pitch accents from the
2Further information and illustrations concerning the me-
chanics of the PaIntE model can be found in Mo?hler and
Conkie (1998).
IMS Radio News Corpus (Rapp, 1998). This cor-
pus was automatically segmented and manually la-
belled according to GToBI(S) (Mayer, 1995). In
the corpus, 1233 syllables are associated with an
L?H accent, 704 with an H?L accent and 162 with
an H? accent.
The corpus contains data from three speakers,
two female and a male one, but the majority of the
data is produced by the male speaker (888 L?H
accents, 527 H?L accents and 152 H? accents). In
order to maximise the number of tokens, all three
speakers were combined. Of the analysed data,
77.92% come from the male speaker. However,
it is not necessarily the case that the same percent-
age of the variability also comes from this speaker:
Both, PaIntE and z-scoring (cf. section 6) nor-
malise across speakers, so the contribution from
each individual speaker is unclear.
The textual transcription of the corpus was an-
notated with respect to information status using
the annotation scheme proposed by Riester (2008).
In this taxonomy information status categories re-
flect the default contexts in which presuppositions
are resolved, which include e. g. discourse context,
environment context or encyclopaedic context.
The annotations are based solely on the written
text and follow strict semantic criteria. Given that
textual information alone (i.e. without prosodic
or speech related information) is not necessarily
sufficient to unambiguously determine the infor-
mation status associated with a particular word,
there are therefore cases where words have mul-
tiple annotations, reflecting underspecification of
information status. However, it is important to
note that in all the experiments reported here, only
unambiguous cases are considered.
The rich annotation scheme employed in the
corpus makes establishing inter-annotator agree-
ment a time-consuming task which is currently un-
derway. Nevertheless, the annotation process was
set up in a way to ensure a maximal smoothing of
uncertainties. Texts were independently labelled
by two annotators. Subsequently, a third, more ex-
perienced annotator compared the two results and,
in the case of discrepancies, took a final decision.
In the present study the categories given and
new are examined. These categories do not rep-
resent a binary distinction but are two extremes
from a set of clearly distinguished categories. For
the most part they correspond to the categories tex-
tually given and brand-new that are used in Bau-
730
mann (2006), but their scope is more tightly con-
strained. The information status annotations are
mapped to the phonetically transcribed speech sig-
nals, from which individual syllable tokens bear-
ing information status are derived.
Syllables for which one of the PaIntE-
parameters was identified as an outlier, were re-
moved. Outliers were defined such that the upper
2.5 percentile as well as the lower 2.5 percentile
of the data were excluded. This led to a reduced
number of pitch accent tokens: 1021 L?H accents,
571H?L accents and 134H? accents. Thus, there
is a continuum of frequency of occurrence, high to
low, from L?H to H?.
With respect to information status, 102 L?H ac-
cents, 87H?L accents and 21H? accents were un-
ambiguously labelled as new. For givenness the
number of tokens is: 114 L?H accents, 44H?L ac-
cents and 10H? accents.
6 General Methodology
In the experiments the general methodology for
calculation of similarity detailed in this section
was employed.
For tokens of the pitch accent types L?H, H?L
and H?, each token was modelled using the full
set of PaIntE parameters. Thus, each token was
represented in terms of a 6-dimensional vector.
For each of the pitch accent types the following
steps were carried out:
? For each 6-dimensional pitch accent category
token calculate the z-score value for each di-
mension. The z-score value represents the
number of standard deviations the value is
away from the mean value for that dimension
and allows comparison of values from differ-
ent normal distributions. The z-score is given
by:
z ? scoredim =
valuedim ?meandim
sdevdim
(1)
Hence, at this point each pitch accent is repre-
sented by a 6-dimensional vector where each
dimension value is a z-score.
? For each token z-scored vector calculate how
similar it is to every other z-scored vector
within the same pitch accent category, and,
in Experiment 2 and 3, with the same infor-
mation status value (e.g. new), using the co-
sine of the angle between the vectors. This is
given by:
cos(~i,~j) =
~i ?~j
?~i ?? ~j ?
(2)
where i and j are vectors of the same pitch ac-
cent category and ? represents the dot prod-
uct.
Each comparison between vectors yields a
similarity score in the range [-1,1], where -1
represents high dissimilarity and 1 represents
high similarity.
The experiments that follow examine distribu-
tions of token similarity. In order to establish
whether distributions differ significantly two dif-
ferent levels of significance were employed, de-
pending on the number of pairwise comparisons
performed.
When comparing two distributions (i.e. per-
forming one test), the significance level was set to
? = 0.05. In those cases where multiple tests were
carried out (Experiment 1 and Experiment 3), the
level of significance was adjusted (Bonferroni cor-
rection) according to the following formula:
? = 1? (1? ?1)
1
n (3)
where ?1 represents the target significance level
(set to 0.05) and n represents the number of tests
being performed. The Bonferroni correction is of-
ten discussed controversially. The main criticism
concerns the increased likelihood of type II errors
that lead to non-significance of actually significant
findings (Pernegger, 1998). Although this conser-
vative adjustment was applied, the statistical tests
in this study resulted in significant p-values indi-
cating the robustness of the findings.
7 Experiment 1: Examining frequency of
occurrence effects in pitch accents
In accordance with the general methodology set
out in section 6, the PaIntE vectors of pitch ac-
cent tokens of types L?H, H?L, and H? were all
z-scored and, within each type, every token was
compared for similarity against every other token
of the same type, using the cosine of the angle be-
tween their vectors. In essence, this experiment
illustrates how similarly pitch accents of the same
type are realised.
Figure 1 depicts the results of the analysis. It
shows the density plot for each distribution of
cosine-similarity comparison values, whereby the
731
?1.0 ?0.5 0.0 0.5 1.0
0.0
0.2
0.4
0.6
0.8
Frequency of Occurrence Effects in Pitch Accents
Cosine?Similarity Comparison Values
Den
sity
H*LL*HH*
Figure 1: Density plots for similarity within pitch ac-
cent types. All distributions differ significantly from each
other. There is a trend towards greater similarity from high-
frequency L?H to low-frequency H?.
distributions can be compared directly ? irrespec-
tive of the different number of data points.
An initial observation is that L?H tokens tend
to be realised fairly variably, the main portion
of the distribution is centred around zero. To-
kens of H?L tend to be produced more simi-
larly (i.e. the distribution is centred around a
higher similarity value), and tokens of H? more
similarly again. These three distributions were
tested against each other for significance using the
Kolmogorov-Smirnov test (? = 0.017), yielding
p-values of p  0.001. Thus there are significant
differences between these distributions.
What is particularly noteworthy is that a de-
crease in frequency of occurrence across pitch ac-
cent types co-occurs significantly with an increase
in within-type token similarity.
While the differences between the graphed dis-
tributions do not appear to be highly marked
the frequency of occurrence effect is nevertheless
in keeping with exemplar-theoretic expectations
as posited by Bybee (2006) and Schweitzer and
Mo?bius (2004), that is, the high frequency of oc-
currence entails a large number of stored exem-
plars, giving the speaker the choice from among
a large number of production targets. This wider
choice leads to a broader range of chosen targets
for different productions and thus to more variable
realisations of tokens of the same type.
?1.0 ?0.5 0.0 0.5 1.0
0.0
0.2
0.4
0.6
0.8
1.0
H*L: Frequency of Occurrence Effects 
  in Information Status Categories
Cosine?Similarity Comparison Values
Den
sity
givennew
Figure 2: Density plots for similarity of H?L tokens. To-
kens of the low-frequency information status category given
display greater similarity to each other than those of the high-
frequency information status category new.
Walsh et al (2008) also reported significant
differences between these distributions, however,
there did not appear to be a clear frequency of oc-
currence effect. The results in the present study
differ from their results because the distributions
centre around different ranges of the similarity
scale clearly indicating that each accent type be-
haves differently in terms of similarity/variability
between the tokens of the respective type. The dif-
ferences between the two findings can be ascribed
to the augmented PaIntE model (section 4).
Given the results from this experiment, the next
experiment seeks to establish what relationship, if
any, exists between information status and pitch
accent production variability.
8 Experiment 2: Examining frequency of
occurrence effects in information
status categories
This experiment was carried out in the same man-
ner as Experiment 1 above with the exception that
in this experiment a subset of the corpus was em-
ployed: only syllables that were unambiguously
labelled with either the information status cate-
gory new or the category given were included in
the analyses. The experiment aims to investigate
the effect of information status on the similar-
ity/variability of tokens of different pitch accent
types. For each pitch accent type, tokens that were
labelled with the information status category new
732
?1.0 ?0.5 0.0 0.5 1.0
0.0
0.2
0.4
0.6
0.8
L*H: Frequency of Occurrence Effects 
  in Information Status Categories
Cosine?Similarity Comparison Values
Den
sity
givennew
Figure 3: Density plots for similarity of L?H tokens. The
curves differ significantly, a trend towards greater similarity
is not observable. The number of tokens for both information
status categories is comparable.
were compared to tokens labelled as given. Again,
a pairwise Kolmogorov-Smirnov test was applied
for each comparison (? = 0.05). Figure 2 depicts
the results for H?L accents. The K-S test yielded a
highly significant difference between the two dis-
tributions (p  0.001), reflecting the clearly visi-
ble difference between the two curves. It is note-
worthy here that for H?L the information status
category new is more frequent than the category
given. Indeed, approximately twice as many are
labelled as new than those labelled given. Figure 2
illustrates that new H?L accents are realised more
variably than given ones. That is, again, an in-
crease in frequency of occurrence co-occurs with
an increase in similarity, this time at the level of
information status.
Figure 3 depicts the difference in similar-
ity/variability for L?H between new tokens and
given tokens. It is clearly visible that the two
curves do not differ as much as those under the
H?L condition. Both curves centre around zero re-
flecting the fact that for both types the tokens are
variable. Although the Kolmogorov-Smirnov test
indicates significance (? = 0.05, p = 0.044), the
nature of the impact that information status has in
this case is unclear.
Here again an effect of frequency of occurrence
might be the reason for this result. The high fre-
quency of L?H accents in general results in a rel-
ative high frequency of given L?H tokens. So the
?1.0 ?0.5 0.0 0.5 1.0
0.0
0.2
0.4
0.6
0.8
Effect of Information Status Category "new" 
 across Pitch Accent Types
Cosine?Similarity Comparison Values
Den
sity
H*LL*HH*
Figure 4: Density plots for similarity of new tokens across
three pitch accent types. In comparison to fig. 1 the trend
towards greater similarity from high-frequency L?H to low-
frequency H? is even more pronounced.
token number for both types is similar (102 new
L?H tokens vs. 114 given L?H tokens), there is
high frequency in both cases, hence variability.
These results, particularly in the case of H?L
(fig. 2) indicate that information status affects
pitch accent realisation. The next experiment
compares the effect across different pitch accent
types.
9 Experiment 3: Examining the effect of
information status across pitch accent
types
This experiment was carried out in the same man-
ner as Experiments 1 and 2 above. For each pitch
accent type, figure 4 depicts within-type pitch ac-
cent similarity for tokens unambiguously labelled
as new.
As with Experiments 1 and 2, frequency of
occurrence once more appears to play a signifi-
cant role. Again, all Kolmogorov-Smirnov tests
yielded significant results (p < 0.017 in all cases).
Indeed, the difference between the distributions
of L?H, H?L, and H? similarity plots appears to
be considerably more prominent than in Experi-
ment 1 (see fig. 1). This indicates that under the
condition of novelty the frequency of occurrence
effect is more pronounced. In other words, there is
a considerably more noticeable difference across
the distributions of L?H, H?L and H?, when nov-
733
?1.0 ?0.5 0.0 0.5 1.0
0.0
0.2
0.4
0.6
0.8
1.0
1.2
Effect of Information Status Category "given" 
 across Pitch Accent Types
Cosine?Similarity Comparison Values
Den
sity
H*LL*HH*
Figure 5: Density plots for similarity of given tokens across
three pitch accent types. Mid-frequency H?L displays greater
similarity than high-frequency L?H. For lowest frequency H?
(only 10 tokens) the trend cannot be observed.
elty is considered: novelty compounds the fre-
quency of occurrence effect.
Figure 5 illustrates results of the same analysis
methodology but applied to tokens of pitch accents
unambiguously labelled as given. Once again
there is a considerable difference between the dis-
tributions of L?H and H?L tokens (p < 0.017).
And again, this difference reflects a more pro-
nounced frequency of occurrence effect for given
tokens than for all accents pooled (as described
in Experiment 1): the information status category
given compounds the frequency of occurrence ef-
fect for L?H and H?L.
For H? the result is not as clear as for the two
more frequent accents. The comparison between
H? and L?H results in a significant difference
(p < 0.017) whereas the comparison between H?
and H?L is slightly above the conservative signif-
icance level (p = 0.0186). Moreover, the dis-
tribution is centred between the distributions for
L?H and H?L and it is thus not clear how to inter-
pret this result with respect to a possible frequency
of occurrence effect. However, having only ten
instances of given H?, the explanatory power of
these comparisons is questionable.
10 Discussion
The experiments discussed above yield a num-
ber of interesting results with implications for re-
search in prosody, information status, the interac-
tion between the two domains, and for exemplar
theory.
Returning to the first question posed at the out-
set in section 1, it is quite clear from Experiment 1
that a certain amount of variability exists when
different tokens of the same pitch accent type are
produced. It is also clear, from the same experi-
ment, that the frequency of occurrence of the pitch
accent type does indeed play a role: with an in-
crease in frequency comes an increase in vari-
ability. This result is in line with the exemplar-
theoretic view that since all exemplars are stored,
exemplars of a type that occur often are more vari-
able because they offer the speaker a wider se-
lection of exemplars to choose from during pro-
duction (Schweitzer and Mo?bius, 2004). How-
ever, with respect to entrenchment (Pierrehum-
bert, 2001; Bybee, 2006), i.e. the idea that fre-
quently occurring behaviours undergo processes
of entrenchment, in Experiment 1 one might ex-
pect to see greater similarity in the realisations of
L?H. However, it is important to note that while
tokens of L?H are not particularly similar to each
other (the bulk of the distribution is around zero
(see figure 1)), they are not too dissimilar either.
That is, they rest at the midpoint of the similar-
ity continuum produced by cosine calculation, in
quite a normal looking distribution. This is not
at odds with the idea of entrenchment. As pro-
ductions of a pitch accent type become more fre-
quent, the distribution of similarity spreads from
the right side of the graph (where infrequent and
highly similar H? tokens lie) leftwards (through
H?L) to the point where the L?H distribution is
found. Beyond this point tokens are excessively
different.
The second question posed in section 1, and ad-
dressed in Experiment 2, sought to ascertain the
impact, if any, information status has on pitch ac-
cent realisation. Distributions of given and new
H?L similarity scores differed significantly, as
did distributions of given and new L?H similar-
ity scores, indicating that information status af-
fects realisation. In other words, for both pitch
accent types, given and new tokens behave dif-
ferently. Concerning the frequency of occurrence
of the information status categories, certainly in
the case of H?L the higher frequency new tokens
exhibited more variability. In the case of L?H
similar numbers of new and given tokens, possi-
bly due to the high frequency of L?H in general,
734
?1.0 ?0.5 0.0 0.5 1.0
0.0
0.2
0.4
0.6
0.8
1.0
Combined Frequency of Occurrence Effect 
 on L*H and H*L
Cosine?Similarity Comparison Values
Den
sity
given L*H new L*Hnew H*Lgiven H*L
Figure 6: Density plots for similarity of combinations of
information status categories given and new with pitch ac-
cent types L?H and H?L. The distributions show a clear
trend towards greater similarity form high-frequency ?given
L?H? and ?new L?H? to mid-frequency ?new H?L? and
low-frequency ?given H?L?.
led to visually similar yet significantly different
distributions. Once again sensitivity to frequency
of occurrence seems to be present, in line with
exemplar-theoretic predictions.
The final question concerns the possibility of a
combined effect of pitch accent frequency of oc-
currence and information status frequency of oc-
currence. Figures 4 and 5 depict a clear com-
pounding effect of both information status cate-
gories across the different pitch accent types (and
their inherent frequencies) when compared to fig-
ure 1. Interestingly, the less frequently occurring
given appears to have a greater impact, particularly
on high frequency L?H.
Figure 6 displays all possible combinations of
L?H, H?L, given and new. H? is omitted in this
graph because of the small number of tokens (10
given, 21 new) and the resulting lack of explana-
tory power. It is evident that an overall frequency
of occurrence effect can be observed: ?given L?H?
and ?new L?H?, which have a similar number of
instances (114 vs. 102 tokens) both centre around
zero and are thus the most leftward skewed curves
in the graph. The distribution of ?new H?L? (87
tokens) shows a trend towards the right hand side
of the graph and thus represents greater similarity
of the tokens. The distribution of similarity values
for the least frequent combination of pitch accent
and information status, ?given H?L? (44 tokens),
centres between 0.5 and 1.0 and is thus the most
rightward curve in the graph, reflecting the high-
est similarity between the tokens.
These results highlight an intricate relationship
between pitch accent production and information
status. The information status of the word influ-
ences not only the type and shape of the pitch ac-
cent (Pierrehumbert and Hirschberg, 1990; Bau-
mann, 2006; Ku?gler and Fe?ry, 2008; Schweitzer et
al., 2008) but also the similarity of tokens within a
pitch accent type. Moreover, this effect is well ex-
plainable within the framework of Exemplar The-
ory as it is subject to frequency of occurrence:
tokens of rare types are produced more similar to
each other than tokens of frequent types.
In the context of speech technology, unfortu-
nately the high variability in highly frequent pitch
accents has a negative consequence, as the correla-
tion between a certain pitch accent or a certain in-
formation status category and the F0 contour is not
a one-to-one relationship. However, forewarned
is forearmed and perhaps a finer grained contex-
tual analysis might yield more context specific so-
lutions.
11 Future Work
The methodology outlined in section 6 gives a lu-
cid insight into the levels of similarity found in
pitch accent realisations. Further insights, how-
ever, could be gleaned from a fine-grained exam-
ination of the PaIntE parameters. For example,
which parameters differ and under what conditions
when examining highly variable tokens? Informa-
tion status evidently plays a role in pitch accent
production but the contexts in which this takes
place have yet to be examined. In addition, the
role of information structure (focus-background,
contrast) also needs to be investigated. A further
line of research worth pursuing concerns the im-
pact of information status on the temporal struc-
ture of spoken utterances and possible compound-
ing with frequency of occurrence effects.
References
Kirsten Abbot-Smith and Michael Tomasello. 2006.
Exemplar-learning and schematization in a usage-
based account of syntactic acquisition. The Linguis-
tic Review, 23(3):275?290.
Ellen G. Bard and M. P. Aylett. 1999. The dissocia-
tion of deaccenting, givenness, and syntactic role in
735
spontaneous speech. In Proceedings of ICPhS (San
Francisco), volume 3, pages 1753?1756.
Stefan Baumann. 2006. The Intonation of Givenness
? Evidence from German., volume 508 of Linguis-
tische Arbeiten. Niemeyer, Tu?bingen. Ph.D. thesis,
Saarland University.
Gillian Brown. 1983. Prosodic structure and the
given/new distinction. In Anne Cutler and D. Robert
Ladd, editors, Prosody: Models and Measurements,
pages 67?77. Springer, New York.
Joan Bybee. 2006. From usage to grammar: The
mind?s response to repetition. Language, 84:529?
551.
Karen Croot and Kathleen Rastle. 2004. Is there
a syllabary containing stored articulatory plans for
speech production in English? In Proceedings of the
10th Australian International Conference on Speech
Science and Technology (Sydney), pages 376?381.
Michael A. K. Halliday. 1967. Intonation and Gram-
mar in British English. Mouton, The Hague.
Keith Johnson. 1997. Speech perception without
speaker normalization: An exemplar model. In
K. Johnson and J. W. Mullennix, editors, Talker
Variability in Speech Processing, pages 145?165.
Academic Press, San Diego.
Klaus J. Kohler. 1991. Studies in german intonation.
AIPUK (Univ. Kiel), 25.
Frank Ku?gler and Caroline Fe?ry. 2008. Pitch accent
scaling on given, new and focused constituents in
german. Journal of Phonetics.
Erwin Marsi, Martin Reynaert, Antal van den Bosch,
Walter Daelemans, and Ve?ronique Hoste. 2003.
Learning to predict pitch accents and prosodic
boundaries in dutch. In Proceedings of the ACL-
2003 Conference (Sapporo, Japan), pages 489?496.
Jo?rg Mayer. 1995. Transcribing German In-
tonation ? The Stuttgart System. Technical
report, Universita?t Stuttgart. http://www.ims.uni-
stuttgart.de/phonetik/joerg/labman/STGTsystem.html.
Gregor Mo?hler and Alistair Conkie. 1998. Paramet-
ric modeling of intonation using vector quantization.
In Third Intern. Workshop on Speech Synth (Jenolan
Caves), pages 311?316.
Gregor Mo?hler. 1998. Describing intonation with a
parametric model. In Proceedings ICSLP, volume 7,
pages 2851?2854.
T. V. Pernegger. 1998. What?s wrong with Bonferroni
adjustment. British Medical Journal, 316:1236?
1238.
Janet Pierrehumbert and Julia Hirschberg. 1990. The
meaning of intonational contours in the interpreta-
tion of discourse. In P. R. Cohen, J. Morgan, and
M. E. Pollack, editors, Intentions in Communication,
pages 271?311. MIT Press, Cambridge.
Janet Pierrehumbert. 2001. Exemplar dynamics: Word
frequency, lenition and contrast. In Joan Bybee and
Paul Hopper, editors, Frequency and the Emergence
of Linguistic Structure, pages 137?157. Amsterdam.
Ellen F. Prince. 1992. The ZPG Letter: Subjects, Def-
initeness and Information Status. In W. C. Mann
and S. A. Thompson, editors, Discourse Descrip-
tion: Diverse Linguistic Analyses of a Fund-Raising
Text, pages 295?325. Amsterdam.
Stefan Rapp. 1998. Automatisierte Erstellung von Ko-
rpora fu?r die Prosodieforschung. Ph.D. thesis, IMS,
Universita?t Stuttgart. AIMS 4 (1).
Arndt Riester. 2008. A Semantic Explication of In-
formation Status and the Underspecification of the
Recipients? Knowledge. In Atle Gr?nn, editor, Pro-
ceedings of Sinn und Bedeutung 12, Oslo.
Antje Schweitzer and Bernd Mo?bius. 2004. Exemplar-
based production of prosody: Evidence from seg-
ment and syllable durations. In Speech Prosody
2004 (Nara, Japan), pages 459?462.
Katrin Schweitzer, Arndt Riester, Hans Kamp, and
Grzegorz Dogil. 2008. Phonological and acoustic
specification of information status - a semantic and
phonetic analysis. Poster at ?Experimental and The-
oretical Advances in Prosody?, Cornell University.
Kim Silverman, Mary Backman, John Pitrelli, Mari
Ostendorf, Colin Wightman, Patti Price, Janet Pier-
rehumbert, and Julia Hirschberg. 1992. Tobi: A
standard for Labeling English Prosody. In Proceed-
ings of ICSLP (Banff, Kanada), volume 2, pages
867?870, Banff, Canada.
Jacques Terken and Julia Hirschberg. 1994. Deaccen-
tuation of words representing ?given? information:
effects of persistence of grammatical function and
surface position. Language and Speech, 37:125?
145.
Jan P. H. van Santen and BerndMo?bius. 2000. A quan-
titative model of F0 generation and alignment. In
A. Botinis, editor, Intonation?Analysis, Modelling
and Technology, pages 269?288. Kluwer.
Michael Walsh, Hinrich Schu?tze, Bernd Mo?bius, and
Antje Schweitzer. 2007. An exemplar-theoretic ac-
count of syllable frequency effects. In Proceedings
of ICPhS (Saarbru?cken), pages 481?484.
Michael Walsh, Katrin Schweitzer, Bernd Mo?bius, and
Hinrich Schu?tze. 2008. Examining pitch-accent
variability from an exemplar-theoretic perspective.
In Proceedings of Interspeech 2008 (Brisbane).
Sandra P. Whiteside and Rosemary A. Varley. 1998.
Dual-route phonetic encoding: Some acoustic evi-
dence. In Proceedings of ICSLP (Sydney), volume 7,
pages 3155?3158.
George Yule. 1980. Intonation and Givenness in Spo-
ken Discourse. Studies in Language, pages 271?
286.
736
Proceedings of the COLING/ACL 2006 Main Conference Poster Sessions, pages 25?32,
Sydney, July 2006. c?2006 Association for Computational Linguistics
The Effect of Corpus Size in Combining Supervised and
Unsupervised Training for Disambiguation
Michaela Atterer
Institute for NLP
University of Stuttgart
atterer@ims.uni-stuttgart.de
Hinrich Schu?tze
Institute for NLP
University of Stuttgart
hinrich@hotmail.com
Abstract
We investigate the effect of corpus size
in combining supervised and unsuper-
vised learning for two types of attach-
ment decisions: relative clause attach-
ment and prepositional phrase attach-
ment. The supervised component is
Collins? parser, trained on the Wall
Street Journal. The unsupervised com-
ponent gathers lexical statistics from
an unannotated corpus of newswire
text. We find that the combined sys-
tem only improves the performance of
the parser for small training sets. Sur-
prisingly, the size of the unannotated
corpus has little effect due to the noisi-
ness of the lexical statistics acquired by
unsupervised learning.
1 Introduction
The best performing systems for many tasks in
natural language processing are based on su-
pervised training on annotated corpora such
as the Penn Treebank (Marcus et al, 1993)
and the prepositional phrase data set first de-
scribed in (Ratnaparkhi et al, 1994). How-
ever, the production of training sets is ex-
pensive. They are not available for many
domains and languages. This motivates re-
search on combining supervised with unsu-
pervised learning since unannotated text is in
ample supply for most domains in the major
languages of the world. The question arises
how much annotated and unannotated data
is necessary in combination learning strate-
gies. We investigate this question for two at-
tachment ambiguity problems: relative clause
(RC) attachment and prepositional phrase
(PP) attachment. The supervised component
is Collins? parser (Collins, 1997), trained on
the Wall Street Journal. The unsupervised
component gathers lexical statistics from an
unannotated corpus of newswire text.
The sizes of both types of corpora, anno-
tated and unannotated, are of interest. We
would expect that large annotated corpora
(training sets) tend to make the additional in-
formation from unannotated corpora redun-
dant. This expectation is confirmed in our
experiments. For example, when using the
maximum training set available for PP attach-
ment, performance decreases when ?unanno-
tated? lexical statistics are added.
For unannotated corpora, we would expect
the opposite effect. The larger the unanno-
tated corpus, the better the combined system
should perform. While there is a general ten-
dency to this effect, the improvements in our
experiments reach a plateau quickly as the un-
labeled corpus grows, especially for PP attach-
ment. We attribute this result to the noisiness
of the statistics collected from unlabeled cor-
pora.
The paper is organized as follows. Sections
2, 3 and 4 describe data sets, methods and
experiments. Section 5 evaluates and discusses
experimental results. Section 6 compares our
approach to prior work. Section 7 states our
conclusions.
2 Data Sets
The unlabeled corpus is the Reuters RCV1
corpus, about 80,000,000 words of newswire
text (Lewis et al, 2004). Three different sub-
sets, corresponding to roughly 10%, 50% and
100% of the corpus, were created for experi-
ments related to the size of the unannotated
corpus. (Two weeks after Aug 5, 1997, were
set apart for future experiments.)
The labeled corpus is the Penn Wall Street
Journal treebank (Marcus et al, 1993). We
25
created the 5 subsets shown in Table 1 for ex-
periments related to the size of the annotated
corpus.
unlabeled R
100% 20/08/1996?05/08/1997 (351 days)
50% 20/08/1996?17/02/1997 (182 days)
10% 20/08/1996?24/09/1996 (36 days)
labeled WSJ
50% sections 00?12 (23412 sentences)
25% lines 1 ? 292960 (11637 sentences)
5% lines 1 ? 58284 (2304 sentences)
1% lines 1 ? 11720 (500 sentences)
0.05% lines 1 ? 611 (23 sentences)
Table 1: Corpora used for the experiments:
unlabeled Reuters (R) corpus for attachment
statistics, labeled Penn treebank (WSJ) for
training the Collins parser.
The test set, sections 13-24, is larger than in
most studies because a single section does not
contain a sufficient number of RC attachment
ambiguities for a meaningful evaluation.
which-clauses subset highA lowA total
develop set (sec 00-12) 71 211 282
test set (sec 13-24) 71 193 264
PP subset verbA nounA total
develop set (sec 00-12) 5927 6560 12487
test set (sec 13-24) 5930 6273 12203
Table 2: RC and PP attachment ambigui-
ties in the Penn Treebank. Number of in-
stances with high attachment (highA), low at-
tachment (lowA), verb attachment (verbA),
and noun attachment (nounA) according to
the gold standard.
All instances of RC and PP attachments
were extracted from development and test
sets, yielding about 250 RC ambiguities and
12,000 PP ambiguities per set (Table 2). An
RC attachment ambiguity was defined as a
sentence containing the pattern NP1 Prep NP2
which. For example, the relative clause in Ex-
ample 1 can either attach to mechanism or to
System.
(1) ... the exchange-rate mechanism of the
European Monetary System, which
links the major EC currencies.
A PP attachment ambiguity was defined as
a subtree matching either [VP [NP PP]] or [VP
NP PP]. An example of a PP attachment am-
biguity is Example 2 where either the approval
or the transaction is performed by written con-
sent.
(2) . . . a majority . . . have approved the
transaction by written consent . . .
Both data sets are available for download
(Web Appendix, 2006). We did not use the
PP data set described by (Ratnaparkhi et al,
1994) because we are using more context than
the limited context available in that set (see
below).
3 Methods
Collins parser. Our baseline method for
ambiguity resolution is the Collins parser as
implemented by Bikel (Collins, 1997; Bikel,
2004). For each ambiguity, we check whether
the attachment ambiguity is resolved correctly
by the 5 parsers corresponding to the different
training sets. If the attachment ambiguity is
not recognized (e.g., because parsing failed),
then the corresponding ambiguity is excluded
for that instance of the parser. As a result, the
size of the effective test set varies from parser
to parser (see Table 4).
Minipar. The unannotated corpus is ana-
lyzed using minipar (Lin, 1998), a partial de-
pendency parser. The corpus is parsed and all
extracted dependencies are stored for later use.
Dependencies in ambiguous PP attachments
(those corresponding to [VP NP PP] and [VP
[NP PP]] subtrees) are not indexed. An ex-
periment with indexing both alternatives for
ambiguous structures yielded poor results. For
example, indexing both alternatives will create
a large number of spurious verb attachments
of of, which in turn will result in incorrect high
attachments by our disambiguation algorithm.
For relative clauses, no such filtering is nec-
essary. For example, spurious subject-verb
dependencies due to RC ambiguities are rare
compared to a large number of subject-verb
dependencies that can be extracted reliably.
Inverted index. Dependencies extracted
by minipar are stored in an inverted index
(Witten et al, 1999), implemented in Lucene
(Lucene, 2006). For example, ?john subj
buy?, the analysis returned by minipar for
John buys, is stored as ?john buy john<subj
26
subj<buy john<subj<buy?. All words, de-
pendencies and partial dependencies of a sen-
tence are stored together as one document.
This storage mechanism enables fast on-line
queries for lexical and dependency statistics,
e.g., how many sentences contain the depen-
dency ?john subj buy?, how often does john
occur as a subject, how often does buy have
john as a subject and car as an object etc.
Query results are approximate because double
occurrences are only counted once and struc-
tures giving rise to the same set of dependen-
cies (a piece of a tile of a roof of a house vs.
a piece of a roof of a tile of a house) cannot
be distinguished. We believe that an inverted
index is the most efficient data structure for
our purposes. For example, we need not com-
pute expensive joins as would be required in a
database implementation. Our long-term goal
is to use this inverted index of dependencies
as a versatile component of NLP systems in
analogy to the increasingly important role of
search engines for association and word count
statistics in NLP.
A total of three inverted indexes were cre-
ated, one each for the 10%, 50% and 100%
Reuters subset.
Lattice-Based Disambiguation. Our
disambiguation method is Lattice-Based
Disambiguation (LBD, (Atterer and Schu?tze,
2006)). We formalize a possible attachment
as a triple < R, i,X > where X is (the
parse of) a phrase with two or more possible
attachment nodes in a sentence S, i is one of
these attachment nodes and R is (the relevant
part of a parse of) S with X removed. For
example, the two attachments in Example 2
are represented as the triples:
< approvedi1 the transactioni2 , i1,by consent >,
< approvedi1 the transactioni2 , i2,by consent >.
We decide between attachment possibilities
based on pointwise mutual information, the
well-known measure of how surprising it is to
see R and X together given their individual
frequencies:
MI(< R, i,X >) = log2 P (<R,i,X>)P (R)P (X)
for P (< R, i,X >), P (R), P (X) 6= 0
MI(< R, i,X >) = 0 otherwise
where the probabilities of the dependency
structures < R, i,X >, R and X are estimated
on the unlabeled corpus by querying the in-
0:p
MN:pN
N:pM
N:p
N:pN MN:p
MN:pMN:pMN
MN:pMN
Figure 1: Lattice of pairs of potential attach-
ment site (NP) and attachment phrase (PP).
M: premodifying adjective or noun (upper or
lower NP), N: head noun (upper or lower NP),
p: Preposition.
verted index. Unfortunately, these structures
will often not occur in the corpus. If this is
the case we back off to generalizations of R
and X. The generalizations form a lattice as
shown in Figure 1 for PP attachment. For ex-
ample, MN:pMN corresponds to commercial
transaction by unanimous consent, N:pM to
transaction by unanimous etc. For 0:p we com-
pute MI of the two events ?noun attachment?
and ?occurrence of p?. Points in the lattice in
Figure 1 are created by successive elimination
of material from the complete context R:X.
A child c directly dominated by a parent p
is created by removing exactly one contextual
element from p, either on the right side (the
attachment phrase) or on the left side (the at-
tachment node). For RC attachment, general-
izations other than elimination are introduced
such as the replacement of a proper noun (e.g.,
Canada) by its category (country) (see below).
The MI of each point in the lattice is com-
puted. We then take the maximum over all
MI values of the lattice as a measure of the
affinity of attachment phrase and attachment
node. The intuition is that we are looking for
the strongest evidence available for the attach-
ment. The strongest evidence is often not pro-
vided by the most specific context (MN:pMN
in the example) since contextual elements like
modifiers will only add noise to the attachment
decision in some cases. The actual syntactic
disambiguation is performed by computing the
affinity (maximum over MI values in the lat-
tice) for each possible attachment and select-
ing the attachment with highest affinity. (The
27
default attachment is selected if the two values
are equal.) The second lattice for PP attach-
ment, the lattice for attachment to the verb,
has a structure identical to Figure 1, but the
attachment node is SV instead of MN, where
S denotes the subject and V the verb. So the
supremum of that lattice is SV:pMN and the
infimum is 0:p (which in this case corresponds
to the MI of verb attachment and occurrence
of the preposition).
LBD is motivated by the desire to use as
much context as possible for disambiguation.
Previous work on attachment disambiguation
has generally used less context than in this
paper (e.g., modifiers have not been used for
PP attachment). No change to LBD is neces-
sary if the lattice of contexts is extended by
adding additional contextual elements (e.g.,
the preposition between the two attachment
nodes in RC, which we do not consider in this
paper).
4 Experiments
The Reuters corpus was parsed with minipar
and all dependencies were extracted. Three
inverted indexes were created, corresponding
to 10%, 50% and 100% of the corpus.1 Five
parameter sets for the Collins parser were cre-
ated by training it on the WSJ training sets
in Table 1. Sentences with attachment am-
biguities in the WSJ corpus were parsed with
minipar to generate Lucene queries. (We chose
this procedure to ensure compatibility of query
and index formats.) The Lucene queries were
run on the three indexes. LBD disambigua-
tion was then applied based on the statistics
returned by the queries. LBD results are ap-
plied to the output of the Collins parser by
simply replacing all attachment decisions with
LBD decisions.
4.1 RC attachment
The lattice for LBD in RC attachment is
shown in Figure 2. When disambiguating
an RC attachment, two instances of the
lattice are formed, one for NP1 and one
1In fact, two different sets of inverted indexes were
created, one each for PP and RC disambiguation. The
RC index indexes all dependencies, including ambigu-
ous PP dependencies. Computing the RC statistics
on the PP index should not affect the RC results pre-
sented here, but we didn?t have time to confirm this
experimentally for this paper.
for NP2 in NP1 Prep NP2 RC. Figure 2
shows the maximum possible lattice. If
contextual elements are not present in a
context (e.g., a modifier), then the lattice
will be smaller. The supremum of the lat-
tice corresponds to a query that includes
the entire NP (including modifying adjec-
tives and nouns)2, the verb and its object.
Example: exchange rate<nn<mechanim
&& mechanism<subj<link &&
currency<obj<link.
C:V
[empty]
MC:VC:VO
Mn:V
MN:VO Nf:VO
Mn:VO N:VO MN:V Nf:V
MC:VO
MNf:VO
n:V
n:VO
MNf:V
N:V
Figure 2: Lattice of pairs of potential attach-
ment site NP and relative clause X. M: pre-
modifying adjective or noun, Nf: head noun
with lexical modifiers, N: head noun only, n:
head noun in lower case, C: class of NP, V:
verb in relative clause, O: object of verb in
the relative clause.
To generalize contexts in the lattice, the fol-
lowing generalization operations are employed:
? strip the NP of the modifying adjec-
tive/noun (weekly report ? report)
? use only the head noun of the NP (Catas-
trophic Care Act ? Act)
? use the head noun in lower case (Act ? act)
? for named entities use a hypernym of the NP
(American Bell Telephone Co. ? company)
? strip the object from X (company have sub-
sidiary ? company have)
The most important dependency for disam-
2From the minipar output, we use all adjectives that
modify the NP via the relation mod, and all nouns that
modify the NP via the relation nn.
28
biguation is the noun-verb link, but the other
dependencies also improve the accuracy of
disambiguation (Atterer and Schu?tze, 2006).
For example, light verbs like make and have
only provide disambiguation information when
their objects are also considered.
Downcasing and hypernym generalizations
were used because proper nouns often cause
sparse data problems. Named entity classes
were identified with LingPipe (LingPipe,
2006). Named entities identified as companies
or organizations are replaced with company in
the query. Locations are replaced with coun-
try. Persons block RC attachment because
which-clauses do not attach to person names,
resulting in an attachment of the RC to the
other NP.
query MI
+exchange rate?nn?mechanism 12.2
+mechanism?subj?link +currency?obj?link
+exchange rate?nn?mechanism 4.8
+mechanism?subj?link
+mechanism?subj?link +currency?obj?link 10.2
mechanism?subj?link 3.4
+European Monetary System?subj?link 0
+currency?obj?link
+System?subj?link +currency?obj?link 0
European Monetary System?subj?link 0
System?subj?link 0
+system?subj?link +currency?obj?link 0
system?subj?link 1.2
+company?subj?link +currency?obj?link 0
company?subj?link -1.1
empty 3
Table 3: Queries for computing high attach-
ment (above) and low attachment (below) for
Example 1.
Table 3 shows queries and mutual informa-
tion values for Example 1. The highest values
are 12.2 for high attachment (mechanism) and
3 for low attachment (System). The algorithm
therefore selects high attachment.
The value 3 for low attachment is the de-
fault value for the empty context. This value
reflects the bias for low attachment: the ma-
jority of relative clauses are attached low. If
all MI-values are zero or otherwise low, this
procedure will automatically result in low at-
tachment.3
3We experimented with a number of values (2, 3,
and 4) on the development set. Accuracy of the algo-
rithm was best for a value of 3. The results presented
here differ slightly from those in (Atterer and Schu?tze,
2006) due to a coding error.
Decision list. For increased accuracy, LBD
is embedded in the following decision list.
1. If minipar has already chosen high attach-
ment, choose high attachment (this only oc-
curs if NP1 Prep NP2 is a named entity).
2. If there is agreement between the verb and
only one of the NPs, attach to this NP.
3. If one of the NPs is in a list of person entities,
attach to the other NP.4
4. If possible, use LBD.
5. If none of the above strategies was successful
(e.g. in the case of parsing errors), attach
low.
4.2 PP attachment
The two lattices for LBD applied to PP at-
tachment were described in Section 3 and Fig-
ure 1. The only generalization operation used
in these two lattices is elimination of contex-
tual elements (in particular, there is no down-
casing and named entity recognition). Note
that in RC attachment, we compare affinities
of two instances of the same lattice (the one
shown in Figure 2). In PP attachment, we
compare affinities of two different lattices since
the two attachment points (verb vs. noun) are
different. The basic version of LBD (with the
untuned default value 0 and without decision
lists) was used for PP attachment.
5 Evaluation and Discussion
Evaluation results are shown in Table 4. The
lines marked LBD evaluate the performance
of LBD separately (without Collins? parser).
LBD is significantly better than the baseline
for PP attachment (p < 0.001, all tests are
?2 tests). LBD is also better than baseline
for RC attachment, but this result is not sig-
nificant due to the small size of the data set
(264). Note that the baseline for PP attach-
ment is 51.4% as indicated in the table (upper
right corner of PP table), but that the base-
line for RC attachment is 73.1%. The differ-
ence between 73.1% and 76.1% (upper right
corner of RC table) is due to the fact that for
RC attachment LBD proper is embedded in a
decision list. The decision list alone, with an
4This list contains 136 entries and was semiauto-
matically computed from the Reuters corpus: An-
tecedents of who relative clauses were extracted, and
the top 200 were filtered manually.
29
RC attachment
Train data # Coll. only 100% R 50% R 10% R 0% R
LBD 264 78.4% 78.0% 76.9% 76.1%
50% 251 71.7% 78.5% 78.1% 76.9% 76.1%
25% 250 70.0% 78.0% 77.6% 76.4% 76.4%
5% 238 68.9% 78.2% 77.7% 76.9% 76.1%
1% 245 67.8% 78.8% 78.4% 77.1% 76.7%
0.05% 194 60.8% 76.8% 76.3% 75.8% 73.7%
PP attachment
Train data # Coll. only 100% R 50% R 10% R 0% R
LBD 12203 73.4% 73.4% 73.0% 51.4%
50% 11953 82.8% 73.6% 73.6% 73.2% 51.7%
25% 11950 81.5% 73.6% 73.7% 73.3% 51.7%
5% 11737 77.4% 74.1% 74.2% 73.7% 52.3%
1% 11803 72.9% 73.6% 73.6% 73.2% 51.6%
0.05% 8486 58.0% 73.9% 73.8% 74.0% 52.8%
Table 4: Experimental results. Results for LBD (without Collins) are given in the first lines. #
is the size of the test set. The baselines are 73.1% (RC) and 51.4% (PP). The combined method
performs better for small training sets. There is no significant difference between 10%, 50% and
100% for the combination method (p < 0.05).
unlabeled corpus of size 0, achieves a perfor-
mance of 76.1%.
The bottom five lines of each table evalu-
ate combinations of a parameter set trained
on a subset of WSJ (0.05% ? 50%) and a par-
ticular size of the unlabeled corpus (100% ?
0%). In addition, the third column gives the
performance of Collins? parser without LBD.
Recall that test set size (second column) varies
because we discard a test instance if Collins?
parser does not recognize that there is an am-
biguity (e.g., because of a parse failure). As
expected, performance increases as the size of
the training set grows, e.g., from 58.0% to
82.8% for PP attachment.
The combination of Collins and LBD is con-
sistently better than Collins for RC attach-
ment (not statistically significant due to the
size of the data set). However, this is not
the case for PP attachment. Due to the good
performance of Collins? parser for even small
training sets, the combination is only superior
for the two smallest training sets (significant
for the smallest set, p < 0.001).
The most surprising result of the experi-
ments is the small difference between the three
unlabeled corpora. There is no clear pattern in
the data for PP attachment and only a small
effect for RC attachment: an increase between
1% and 2% when corpus size is increased from
10% to 100%.
We performed an analysis of a sample of in-
correctly attached PPs to investigate why un-
labeled corpus size has such a small effect. We
found that the noisiness of the statistics ex-
tracted from Reuters were often responsible
for attachment errors. The noisiness is caused
by our filtering strategy (ambiguous PPs are
not used, resulting in undercounting), by the
approximation of counts by Lucene (Lucene
overcounts and undercounts as discussed in
Section 3) and by minipar parse errors. Parse
errors are particularly harmful in cases like
the impact it would have on prospects, where,
due to the extraction of the NP impact, mini-
par attaches the PP to the verb. We did
not filter out these more complex ambiguous
cases. Finally, the two corpora are from dis-
tinct sources and from distinct time periods
(early nineties vs. mid-nineties). Many topic-
and time-specific dependencies can only be
mined from more similar corpora.
The experiments reveal interesting dif-
ferences between PP and RC attachment.
The dependencies used in RC disambiguation
rarely occur in an ambiguous context (e.g.,
most subject-verb dependencies can be reli-
ably extracted). In contrast, a large propor-
tion of the dependencies needed in PP dis-
ambiguation (verb-prep and noun-prep depen-
dencies) do occur in ambiguous contexts. An-
other difference is that RC attachment is syn-
tactically more complex. It interacts with
agreement, passive and long-distance depen-
30
dencies. The algorithm proposed for RC ap-
plies grammatical constraints successfully. A
final difference is that the baseline for RC is
much higher than for PP and therefore harder
to beat.5
An innovation of our disambiguation system
is the use of a search engine, lucene, for serv-
ing up dependency statistics. The advantage
is that counts can be computed quickly and
dynamically. New text can be added on an
ongoing basis to the index. The updated de-
pendency statistics are immediately available
and can benefit disambiguation performance.
Such a system can adapt easily to new topics
and changes over time. However, this archi-
tecture negatively affects accuracy. The un-
supervised approach of (Hindle and Rooth,
1993) achieves almost 80% accuracy by using
partial dependency statistics to disambiguate
ambiguous sentences in the unlabeled corpus.
Ambiguous sentences were excluded from our
index to make index construction simple and
efficient. Our larger corpus (about 6 times as
large as Hindle et al?s) did not compensate for
our lower-quality statistics.
6 Related Work
Other work combining supervised and unsu-
pervised learning for parsing includes (Char-
niak, 1997), (Johnson and Riezler, 2000), and
(Schmid, 2002). These papers present inte-
grated formal frameworks for incorporating in-
formation learned from unlabeled corpora, but
they do not explicitly address PP and RC at-
tachment. The same is true for uncorrected
colearning in (Hwa et al, 2003).
Conversely, no previous work on PP and RC
attachment has integrated specialized ambi-
guity resolution into parsing. For example,
(Toutanova et al, 2004) present one of the
best results achieved so far on the WSJ PP
set: 87.5%. They also integrate supervised
and unsupervised learning. But to our knowl-
edge, the relationship to parsing has not been
explored before ? even though application to
parsing is the stated objective of most work on
PP attachment.
5However, the baseline is similarly high for the PP
problem if the most likely attachment is chosen per
preposition: 72.2% according to (Collins and Brooks,
1995).
With the exception of (Hindle and Rooth,
1993), most unsupervised work on PP attach-
ment is based on superficial analysis of the
unlabeled corpus without the use of partial
parsing (Volk, 2001; Calvo et al, 2005). We
believe that dependencies offer a better basis
for reliable disambiguation than cooccurrence
and fixed-phrase statistics. The difference to
(Hindle and Rooth, 1993) was discussed above
with respect to analysing the unlabeled cor-
pus. In addition, the decision procedure pre-
sented here is different from Hindle et al?s.
LBD uses more context and can, in princi-
ple, accommodate arbitrarily large contexts.
However, an evaluation comparing the perfor-
mance of the two methods is necessary.
The LBD model can be viewed as a back-
off model that combines estimates from sev-
eral ?backoffs?. In a typical backoff model,
there is a single more general model to back
off to. (Collins and Brooks, 1995) also present
a model with multiple backoffs. One of its vari-
ants computes the estimate in question as the
average of three backoffs. In addition to the
maximum used here, testing other combina-
tion strategies for the MI values in the lattice
(e.g., average, sum, frequency-weighted sum)
would be desirable. In general, MI has not
been used in a backoff model before as far as
we know.
Previous work on relative clause attachment
has been supervised (Siddharthan, 2002a; Sid-
dharthan, 2002b; Yeh and Vilain, 1998).6
(Siddharthan, 2002b)?s accuracy for RC at-
tachment is 76.5%.7
7 Conclusion
Previous work on specific types of ambiguities
(like RC and PP) has not addressed the in-
tegration of specific resolution algorithms into
a generic statistical parser. In this paper, we
have shown for two types of ambiguities, rel-
ative clause and prepositional phrase attach-
ment ambiguity, that integration into a sta-
tistical parser is possible and that the com-
6Strictly speaking, our experiments were not com-
pletely unsupervised since the default value and the
most frequent attachment were determined based on
the development set.
7We attempted to recreate Siddharthan?s training
and test sets, but were not able to based on the de-
scription in the paper and email communication with
the author.
31
bined system performs better than either com-
ponent by itself. However, for PP attachment
this only holds for small training set sizes. For
large training sets, we could only show an im-
provement for RC attachment.
Surprisingly, we only found a small effect
of the size of the unlabeled corpus on disam-
biguation performance due to the noisiness of
statistics extracted from raw text. Once the
unlabeled corpus has reached a certain size (5-
10 million words in our experiments) combined
performance does not increase further.
The results in this paper demonstrate that
the baseline of a state-of-the-art lexicalized
parser for specific disambiguation problems
like RC and PP is quite high compared to
recent results for stand-alone PP disambigua-
tion. For example, (Toutanova et al, 2004)
achieve a performance of 87.6% for a train-
ing set of about 85% of WSJ. That num-
ber is not that far from the 82.8% achieved
by Collins? parser in our experiments when
trained on 50% of WSJ. Some of the super-
vised approaches to PP attachment may have
to be reevaluated in light of this good perfor-
mance of generic parsers.
References
Michaela Atterer and Hinrich Schu?tze. 2006. A
lattice-based framework for enhancing statisti-
cal parsers with information from unlabeled cor-
pora. In CoNLL.
Daniel M. Bikel. 2004. Intricacies of Collins?
parsing model. Computational Linguistics,
30(4):479?511.
Hiram Calvo, Alexander Gelbukh, and Adam Kil-
garriff. 2005. Distributional thesaurus vs.
WordNet: A comparison of backoff techniques
for unsupervised PP attachment. In CICLing.
Eugene Charniak. 1997. Statistical parsing with
a context-free grammar and word statistics. In
AAAI/IAAI, pages 598?603.
Michael Collins and James Brooks. 1995. Prepo-
sitional attachment through a backed-off model.
In Third Workshop on Very Large Corpora. As-
sociation for Computational Linguistics.
Michael Collins. 1997. Three generative, lexi-
calised models for statistical parsing. In ACL.
Donald Hindle and Mats Rooth. 1993. Structural
ambiguity and lexical relations. Computational
Linguistics, 19(1):103?120.
Rebecca Hwa, Miles Osborne, Anoop Sarkar, and
Mark Steedman. 2003. Corrected co-training
for statistical parsers. In Workshop on the Con-
tinuum from Labeled to Unlabeled Data in Ma-
chine Learning and Data Mining, ICML.
Mark Johnson and Stefan Riezler. 2000. Ex-
ploiting auxiliary distributions in stochastic
unification-based grammars. In NAACL.
David D. Lewis, Yiming Yang, Tony G. Rose, and
Fan Li. 2004. RCV1: A new benchmark collec-
tion for text categorization research. J. Mach.
Learn. Res., 5.
Dekang Lin. 1998. Dependency-based evaluation
of MINIPAR. In Workshop on the Evaluation of
Parsing Systems, Granada, Spain.
LingPipe. 2006. http://www.alias-
i.com/lingpipe/.
Lucene. 2006. http://lucene.apache.org.
Mitchell P. Marcus, Beatrice Santorini, and
Mary Ann Marcinkiewicz. 1993. Building
a large natural language corpus of English:
the Penn treebank. Computational Linguistics,
19:313?330.
Adwait Ratnaparkhi, Jeff Reynar, and Salim
Roukos. 1994. A maximum entropy model for
prepositional phrase attachment. In HLT.
Helmut Schmid. 2002. Lexicalization of proba-
bilistic grammars. In Coling.
Advaith Siddharthan. 2002a. Resolving attach-
ment and clause boundary ambiguities for sim-
plifying relative clause constructs. In Student
Research Workshop, ACL.
Advaith Siddharthan. 2002b. Resolving relative
clause attachment ambiguities using machine
learning techniques and wordnet hierarchies. In
4th Discourse Anaphora and Anaphora Resolu-
tion Colloquium.
Kristina Toutanova, Christopher D. Manning, and
Andrew Y. Ng. 2004. Learning random walk
models for inducing word dependency distribu-
tions. In ICML.
Martin Volk. 2001. Exploiting the WWW as a
corpus to resolve pp attachment ambiguities. In
Corpus Linguistics 2001.
Web Appendix. 2006. http://www.ims.uni-
stuttgart.de/?schuetze/colingacl06/apdx.html.
Ian H. Witten, Alistair Moffat, and Timothy C.
Bell. 1999. Managing Gigabytes: Compressing
and Indexing Documents and Images. Morgan
Kaufman.
Alexander S. Yeh and Marc B. Vilain. 1998. Some
properties of preposition and subordinate con-
junction attachments. In Coling.
32
Proceedings of the 10th Conference on Computational Natural Language Learning (CoNLL-X),
pages 93?100, New York City, June 2006. c?2006 Association for Computational Linguistics
   	
 
 Proceedings of the EACL 2009 Workshop on GEMS: GEometical Models of Natural Language Semantics, pages 17?24,
Athens, Greece, 31 March 2009. c?2009 Association for Computational Linguistics
Unsupervised Classification with Dependency Based Word Spaces
Klaus Rothenh?usler and Hinrich Sch?tze
Institute for Natural Language Processing
University of Stuttgart
Stuttgart, Germany
{Klaus.Rothenhaeusler, Hinrich.Schuetze}@ims.uni-stuttgart.de
Abstract
We present the results of clustering exper-
iments with a number of different evalu-
ation sets using dependency based word
spaces. Contrary to previous results we
found a clear advantage using a parsed
corpus over word spaces constructed with
the help of simple patterns. We achieve
considerable gains in performance over
these spaces ranging between 9 and 13%
in absolute terms of cluster purity.
1 Introduction
Word space models have become a mainstay in the
automatic acquisition of lexical semantic knowl-
edge. The computation of semantic relatedness of
two words in such models is based on their distri-
butional similarity. The most crucial way in which
such models differ is the definition of distribu-
tional similarity: In a regular word space model
the observed distribution concerns the immediate
neighbours of a word within a predefined win-
dow to the left and right (Sch?tze, 1992; Sahlgren,
2006). Early on in the development as an alter-
native models were proposed that relied on the
similarity of the distribution of syntactic relations
(Hindle, 1990; Pad? and Lapata, 2007). More
recently the distribution of the occurrence within
simple patterns defined in the form of regular ex-
pressions that are supposed to capture explicit se-
mantic relations was explored as the basis of distri-
butional similarity (Almuhareb and Poesio, 2004).
Whereas dependency based semantic spaces
have been shown to surpass other word space mod-
els for a number of problems (Pad? and Lapata,
2007; Lin, 1998), for the task of categorisation
simple pattern based spaces have been shown to
perform equally good if not better (Poesio and Al-
muhareb, 2005b; Almuhareb and Poesio, 2005b).
We want to show that dependency based spaces
also fare better in these tasks if the dependency re-
lations used are selected reasonably. At the same
time we want to show that such a system can be
built with freely available components and with-
out the need to rely on the index of a proprietary
search engine vendor.
We propose to use the web acquired data of the
ukWaC (Ferraresi et al, 2008), which is huge but
still manageable and comes in a pre-cleaned ver-
sion with HTML markup removed. It can easily
be fed into a parser like MiniPar which allows for
the subsequent extraction of dependency relations
of different types and complexity. In particular we
work with dependency paths that can reach beyond
direct dependencies as opposed to Lin (1998) but
in the line of Pado and Lapata (2007). In contrast
to the latter, however, different paths that end in
the same word are not generally mapped to the
same dimension in our model. A path in a depen-
dency graph can pass through several nodes and
encompass different relations.
We experimented with two sets of nouns pre-
viously used in the literature for word clustering.
The nouns in both sets are taken from a number
of different WordNet categories. Hence, the task
consists in clustering together the words from the
same category. By keeping the clustering algo-
rithm constant, differences in performance can be
attributed to the differences of the word represen-
tations.
The next section provides a formal description
of our word space model. Section 3 reports on our
clustering experiments with two sets of concepts
used previously to evaluate the categorisation abil-
ities of word spaces. Section 4 discusses these re-
17
sults and draws some conclusions.
2 Word Space Construction
We follow the formalisation and terminology de-
veloped in Pado and Lapata (2007) according to
which a dependency based space is determined by
the sets of its basis elements B and targets T that
form a matrix M = B ? T , a similarity function
S that assigns a real-valued similarity measure to
pairs of elements from T , the association measure
A that captures the strength of the relation between
a target and a basis element, the context selection
function cont, the basis mapping function ? and
the path value function v. Our set of targets is al-
ways a subset of the lemmas output by MiniPar.
The remaining elements are defined in this section.
We use pi to denote a path in a dependency graph
which is conceived of as an undirected graph for
this purpose. So, in general a dependency path has
an upward and downward part where one can have
length zero. All the paths used to define the con-
texts for target words are anchored there, i.e. they
start from the target.
In choosing the context definitions that deter-
mine what dependency paths are used in the con-
struction of the word vectors, we oriented our-
selves at the sets proposed in Pado and Lap-
ata (2007). As Pado and Lapata (2007) achieved
their best results with it we started from their
medium sized set of context definitions, from
which we extracted the appropriate ones for our
experiments and added some that seemed to make
sense for our purposes: As our evaluation sets con-
sist entirely of nouns, we used only context defi-
nitions that start at a noun. Thereby we can en-
sure that only nominal uses are recorded in a word
vector if a target word can have different parts of
speech. The complete set of dependency relations
our context selection function cont comprises is
given in Figure 1 along with an example for each.
We only chose paths that end in an open word
class assuming that they are more informative
about the meaning of a target word. Paths end-
ing in a preposition for instance, as used by
Pado and Lapata (2007), were not considered. For
the same reason we implemented a simple stop
word filter that discards paths ending in a pronoun,
which are assigned the tag N by MiniPar just like
any other noun.
On the other hand we added the relation be-
tween a prepositional complement and the noun it
modifies (appearing as relation IX in Figure 1) as
a close approximation of the pattern used by (Al-
muhareb and Poesio, 2004) to identify attributes
of a concept as detailed in the next section. Path
specifications X and XI are also additions we
made that are thought to gather additional attribute
values to the ones already covered by III.
As a basis mapping function ? we used a gen-
eralisation of the one used by Grefenstette (1994)
and Lin (1998). They map a dependency between
two words to a pair consisting of the relation la-
bel l and the end word of the dependency end(pi).
As we use paths that span more than a single re-
lation, this approach is not directly applicable to
our setup. Instead we use a mapping function that
maps a path to the sequence of edge labels through
which it passes combined with the end word:
?(pi) = (l(pi),end(pi))
where l(?) is a labelling function that returns
the sequence of edge labels for a given path.
With this basis mapping function the nodes or
words respectively through which a path passes
are all neglected except for the node where the
path ends. So, for the noun human the se-
quence human and mouse genome as well as
the sequence human and chimpanzee genome
increase the count for the same basis element
:N:conj:N:*:N:nn:N:genome. Here we
use a path notation of the general form:
(: POS : rel : POS : {word,?})n
where POS is a part of speech, rel a relation and
word a node label, i.e. a lemma, all as produced
by MiniPar. The length of a path is determined by
n and the asterisk (*) indicates that a node label is
ignored by the basis mapping function.
As an alternative we experimented with a lexi-
cal basis mapping function that maps a path to its
end word:
?(pi) = end(pi)
This reduces the number of dimensions consider-
ably and yields semantic spaces that are similar
to window based word spaces. As this mapping
function consistently delivered worse results, we
dropped it from our evaluation.
Considering that (Pad? and Lapata, 2007) only
reported very small differences for different path
valuation functions, we only used a constant valu-
ation of paths:
vconst(pi) = 1
18
(I) the subject of a verb
All humans die.
PreDet
N
Vpre
subj
(II) an object of a verb
Gods from another world created humans
V
N
subj obj
(III) modified by an adjective
Young dogs are like young humans
VBE
Prep
A
N
s pred
pcomp
-n
mod
(IV) linked to another noun via a genitive relation
The human?s eyes glimmered with comprehension
Det
N
N
V
det
subj mod
gen
(V) part of a nominal complex
The human body presents a problem.
Det N
N
V
subj
det
obj
nn
(VI) part of a conjunction
Humans and animals are equally fair game.
N
U N
VBE
s
punc
pred
conj
(VII) the subject of a predicate noun
Humans are the only specie that has sex for pleasure.
N
VBE
N
C
s
det, m
od
pred
rel
subj
(VIII) the subject of a predicate adjective
Humans are fallible.
N
VBE
A
s pred
subj
(IX) the prepositional complement modifying a noun
You must get into the mind of humans.
N Aux
V
Det
N
Prep
N
s aux
det
obj
mod
pcomp
-n
(X) the prepositional complement modifying a
noun that is the subject of a predicate adjective
The nature of humans is corrupt.
N
Prep
N
VBE
A
s
mod
pcomp
-n
pred
(XI) the prepositional complement modifying a noun that is the subject of a predicate noun
Chief diseases of humans are infections.
N
Prep
N
VBE
N
s
mod
pcomp
-n
pred
(XII) relations I-IV and VI-XI above but now with the target as part of a complex noun phrase as shown for
a conjunction relation (VI) in the example
They interrogated him about the human body and reproduction.
Prep
Det N
N
U N
mod
det
pcomp-n
puncnn conj
Figure 1: Context definitions used in the construction of our word spaces. All examples show contexts
for the target human. Greyed out parts are just for illustrative purposes and have no impact on the word
vectors. The examples are slightly simplified versions of sentences found in ukWaC.19
Thus, an occurrence of any path, irrespective of
length or grammatical relations that are involved,
increases the count of the respective basis element
by one.
We implemented three different association
functions, A, to transform the raw frequency
counts and weight the influence of the different co-
occurrences. We worked with an implementation
of the log likelihood ratio (g-Score) as proposed
by Dunning (1993) and two variants of the t-score,
one considering all values (t-score) and one where
only positive values (t-score+) are kept following
the results of Curran and Moens (2002). We also
experimented with different frequency cutoffs re-
moving dimensions that occur very frequently or
very rarely.
3 Evaluation
For all our experiments we used the ukWaC cor-
pus1 to construct the word spaces, which was
parsed using MiniPar. The latter provides lemma
information, which we used as possible target and
context words. The word vectors we built from
this data were represented as pseudo documents in
an inverted index. To our knowledge the experi-
ments described in this paper are the first to work
with a completely parsed version of the ukWaC.
For the evaluation the word vectors for the
test sets were clustered into a predefined number
of clusters corresponding to the number of con-
cept classes from which the words were drawn.
All experiments were conducted with the CLUTO
toolkit (Karypis, 2003) using the repeated bisec-
tions clustering algorithm with global optimisa-
tion and the cosine as a distance measure to main-
tain comparability with related work, e.g. Ba-
roni et al (2008).
As the main evaluation measure we used pu-
rity for the whole set as supplied by CLUTO. For
a clustering solution ? of n clusters and a set of
classes C, purity can be defined as:
purity(?,C) = 1
n
?
k
max
j
|?k ? c j|
where ?k denotes the set of terms in a cluster and
c j the set of terms in a class. This aggregate mea-
sure of purity corresponds to the weighted sum of
purities for the individual clusters, which is de-
fined as the ratio of items in a cluster that belong
to the majority class. The results for the two test
1http://wacky.sslmit.unibo.it
sets we used are described in the following two
subsections.
3.1 Results for 214 nouns from
Almuhareb and Poesio (2004)
The first set we worked with was introduced by
Almuhareb and Poesio (2004) and consists of 214
nouns from 13 different categories in WordNet. In
the original paper the best results were achieved
with vector representations built from concept at-
tributes and their values as identified by simple
patterns. For the identification of attribute values
of a concept C the following pattern was used
?[a|an|the] * C [is|was]?
It will find instances such as an adult human is
identifying adult as a value for an attribute (age)
of [HUMAN] (we use small capitals enclosed in
square brackets to denote a concept). Attributes
themselves are searched with the pattern
?the * of the C [is|was]?
A match for the concept [HUMAN] would be the
dignity of the human is, which yields dignity as
an attribute. These patterns were translated into
queries and submitted to the Google2 search en-
gine.
We compare our dependency based spaces with
the results achieved with the pattern based ap-
proach in Table 1.
association
measure
g-score t-score t-score+
dependency
based space
77.1% 85.5% 96.7%
window based
space
84.1% 82.7% 89.3%
pattern based
space
- - 85.5%
Table 1: Categorisation results for the 214
concepts and 13 classes proposed in Al-
muhareb and Poesio (2004), which is also
the source of the result for the pattern based space.
They only used t-score+. The numbers given are
the best accuracies achieved under the different
settings.
For the window based space we used the best
performing in a free association task with a win-
dow size of six words to each side and all the
2http://www.google.com
20
context accuracy # dimensions
(I) 82.2% 7359
(II) 92.5% 6680
(III) 88.3% 45322
(IV) ? 37231
(V) 82.2% 240157
(VI) 95.3% 93917
(VII) 86.9% 45527
(VIII) 77.1% 5245
(IX) 91.6% 87765
(X) ? 2186
(XI) ? 6967
(XII) 93.0% 188763
Table 2: Clustering results using only one kind of
path specification. For (IV), (X) and (XI) purity
values are missing because vectors for some of the
words could not be built.
words that appeared at least two times as dimen-
sions ignoring stop words. The effective dimen-
sionality of the so built word vectors is 417 837.
The results for the dependency based spaces
were built by selecting all paths without any
frequency thresholds which resulted in a set of
767 119 dimensions.
As can be seen, both window and dependency
based spaces exceed the pattern based space for
certain association measures. But the dependency
space also has a clear advantage over the window
based space. In particular the t-score+ measure
yields very good results. In contrast the g-score
offers the worst results with the t-score retaining
negative values somewhere in between. For our
further experiments we hence used the t-score+ as-
sociation measure.
3.1.1 Further Analysis
We ran a number of experiments to quantify the
impact the different kinds of paths have on the
clustering result. We first built spaces using only
a single kind of path to find out how good each
performs on its own. The result can be found in
Table 2. For some of the words in the evaluation
set no contexts could be found when only one of
the two most complex context specifications (X),
(XI) was used or when the context was reduced to
the genitive relation (IV). Apart from that the re-
sults suggest that even a single type of relation on
its own can prove highly effective. Especially the
conjunctive relation (VI) performs very well with
a purity value of 95.3%.
removed context accuracy
(I) 97.2%
(II) 97.7%
(III) 97.2%
(IV) 97.2%
(V) 98.1%
(VI) 96.3%
(VII) 97.2%
(VIII) 97.2%
(IX) 96.7%
(X) 97.2%
(XI) 97.2%
(XII) 96.7%
Table 3: Clustering results for spaces with one
context specification removed.
To further clarify the role of the different kinds
of contexts, we ran the experiment with word
spaces where we removed each one of the twelve
context specifications in turn. The results as given
in Table 3 are a bit astonishing at first sight: Only
the removal of the conjunctive relation actually
leads to a decrease in performance. All the other
contexts seem to be either redundant ? with per-
formance staying the same when they are removed
? or even harmful ? with performance increasing
once they are removed. Having observed this, we
tried to remove further context specifications and
surprisingly found that the best performance of
98.1% can be reached by only including the con-
junction (VI) and the object (II) relations. The di-
mensionality of these vectors is only a fraction of
the original ones with 100 597.
The result for the best performing dependency
based space listed in the table is almost perfect.
Having a closer look at the results reveals that in
fact only four words are put into a wrong cluster.
These words are: lounge, pain, mouse, oyster.
The first is classified as [BUILDING] instead of
[FURNITURE]. In the case of lounge the misclas-
sification seems to be attributable to the ambiguity
of the word which can either denote a piece of fur-
niture or a waiting room. The latter is apparently
the more prominent sense in the data. In this usage
the word often appears in conjunctions with room
or hotel just like restaurant, inn or clubhouse.
Pain is misclassified as an [ILLNESS] instead
of a [FEELING] which is at least a close miss.
The misclassification of mouse as a [BODY PART]
seems rather odd on the other hand. The reason for
21
it becomes apparent when looking at the most de-
scriptive and discriminating features of the [BODY
PART] cluster: In both lists the highest in the rank-
ing is the dimension :N:mod:A:left, i.e. left
as an adjectival modifier of the word in question.
The prominence of this particular modification is
of course due to the fact that a lot of body parts
come in pairs and that the members of these pairs
are commonly identified by assigning them to the
left or right half of the body. Certainly, the word
mouse enters this cluster not through its sense of
mouse1 as an animal but rather through its sense of
mouse2 as a piece of computer equipment that has
two buttons, which are also referred to as the left
and right one. Unfortunately, MiniPar frequently
resolves left in a wrong way as a modifier of mouse
instead of button.
Finally for oyster which is put into the [EDIBLE
FRUIT] instead of the [ANIMAL] cluster it is con-
spicuous that oyster is the only sea animal in the
evaluation set and consequently it rarely occurs
in conjunctions with the other animals. Conjunc-
tions, however, seem to be the most important fea-
tures for defining all the clusters. Additionally
oyster scores low on a lot of dimensions that are
typical for a big number of the members of the an-
imal cluster, e.g. :N:obj:V:kill.
3.2 Results for 402 words from
Almuhareb and Poesio (2005a)
In Poesio and Almuhareb (2005a) a larger evalu-
ation set is introduced that comprises 402 nouns
sampled from the hierarchies under the 21 unique
beginners in WordNet. The words were also cho-
sen so that candidates from different frequency
bands and different levels of ambiguity were rep-
resented. Further results using this set are reported
in Almuhareb and Poesio (2005b). The best result
was obtained with the attribute pattern alone and
filtering to include only nouns. We tried to assem-
ble word vectors with the same patterns based on
the ukWaC corpus. But even if we included both
patterns, we were only able to construct vectors
for 363 of the 402 words. For 118 of them the
number of occurrences, on which they were based,
was less than ten. This gives an impression of the
size of the index that is necessary for such an ap-
proach. To date such an immense amount of data
is only available through proprietary search engine
providers. This makes a system dependant upon
the availability of an API of such a vendor. In fact
the version of the Google API on which the orig-
inal experiments relied has since been axed. Our
approach circumvents such problems.
We ran analogous experiments to the ones de-
scribed in the previous section on this evaluation
set, now producing 21 clusters. The results given
in Table 4 are for a dependency space without any
frequency thresholds and the complete set of con-
text specifications as defined above. The settings
for the window based space were also the same
(6 words to each side). Again the results achieved
with the t-score+ association were clearly superior
to the others and were used in all the following
experiments. Unsurprisingly, for this more diffi-
cult task the performance is not as good as for the
smaller set but nevertheless the superiority of the
dependency based space is clearly visible with an
absolute increase in cluster purity of 8.2% com-
pared with the pattern based space.
association
measure
g-score t-score t-score+
dependency
based space
67.9% 67.2% 79.1%
window based
space
65.7% 60.7% 67.9%
pattern based
space
- - 70.9%
Table 4: Categorisation results for the 402
concepts and 21 classes proposed in Al-
muhareb and Poesio (2005a) which is also
the source of the result for the pattern based
space. The numbers given are the best accuracies
achieved under the different settings.
3.2.1 Further Analysis
Again we ran further experiments to determine the
impact of the different kinds of relations. The re-
moval of any single context specification leads to
a performance drop with this evaluation set. The
smallest decrease is observed when removing con-
text specification XII. However, as we had seen in
the previous experiment with the smaller set that
only two context specifications suffice to reach
peak performance, we conducted another exper-
iment where we started from the best perform-
ing space constructed from a single context spec-
ification (the conjunction relation, VI) and suc-
cessively added the specification that led to the
biggest performance gain. The crucial results are
22
majority class concepts
solid tetrahedron, salient, ring, ovoid, octahedron, knob, icosahedron, fluting, dome, dodecahedron,
cylinder, cuboid, cube, crinkle, concavity, samba, coco, nonce, divan, ball, stitch, floater, trove,
hoard, mouse
time yesteryear, yesterday, tonight, tomorrow, today, quaternary, period, moment, hereafter, gesta-
tion, future, epoch, day, date, aeon, stretch, snap, throb, straddle, nap
motivation wanderlust, urge, superego, obsession, morality, mania, life, impulse, ethics, dynamic, con-
science, compulsion, plasticity, opinion, acceptance, sensitivity, desire, interest
assets wager, taxation, quota, profit, payoff, mortgage, investment, income, gain, fund, credit, cap-
ital, allotment, allocation, possession, inducement, incentive, disincentive, deterrence, share,
sequestrian, cheque, check, bond, tailor
district village, town, sultanate, suburb, state, shire, seafront, riverside, prefecture, parish, metropolis,
land, kingdom, county, country, city, canton, borough, borderland, anchorage, tribe, nation,
house, fen, cordoba, faro
legal document treaty, statute, rescript, obligation, licence, law, draft, decree, convention, constitution, bill,
assignment, commencement, extension, incitement, caliphate, clemency, venture, dispensation
physical property weight, visibility, temperature, radius, poundage, momentum, mass, length, diameter, deflec-
tion, taper, indentation, droop, corner, concavity
social unit troop , team, platoon, office, legion, league, household, family, department, confederacy, com-
pany, committee, club, bureau, brigade, branch, agency
atmospheric
phenomenon
wind, typhoon, tornado, thunderstorm, snowfall, shower, sandstorm, rainstorm, lightning, hur-
ricane, fog, drizzle, cyclone, crosswind, cloudburst, cloud, blast, aurora, airstream, glow
social occasion wedding, rededication, prom, pageantry, inaugural, graduation, funeral, fundraiser, fiesta, fete,
feast, enthronement, dance, coronation, commemoration, ceremony, celebration, occasion, raf-
fle, beano
monetary unit zloty, yuan, shilling, rupee, rouble, pound, peso, penny, lira, guilder, franc, escudo, drachma,
dollar, dirham, dinar, cent
tree sycamore, sapling, rowan, pine, palm, oak, mangrove, jacaranda, hornbeam, conifer, cinchona,
casuarina, acacia, riel
chemical element zinc, titanium, silver, potassium, platinum, oxygen, nitrogen, neon, magnesium, lithium, iron,
hydrogen, helium, germanium, copper, charcoal, carbon, calcium, cadmium, bismuth, alu-
minium, gold
illness smallpox, plague, meningitis, malnutrition, leukemia, hepatitis, glaucoma, flu, eczema, dia-
betes, cirrhosis, cholera, cancer, asthma, arthritis, anthrax, acne, menopause
feeling wonder, shame, sadness, pleasure, passion, love, joy, happiness, fear, anger, heaviness, cool-
ness, torment, tenderness, suffering, stinging
vehicle van, truck, ship, rocket, pickup, motorcycle, helicopter, cruiser, car, boat, bicycle, automobile,
airplane, aircraft, jag
creator producer, photographer, painter, originator, musician, manufacturer, maker, inventor, farmer,
developer, designer, craftsman, constructor, builder, artist, architect, motivator
pain toothache, soreness, sting, soreness, sciatica, neuralgia, migraine, lumbago, headache, earache,
burn, bellyache, backache, ache, rheumatism, pain
animal zebra, turtle, tiger, sheep, rat, puppy, monkey, lion, kitten, horse, elephant, dog, deer, cow, cat,
camel, bull, bear
game whist, volleyball, tennis, softball, soccer, rugby, lotto, keno, handball, golf, football, curling,
chess, bowling, basketball, baccarat, twister
edible fruit watermelon, strawberry, pineapple, pear, peach, orange, olive, melon, mango, lemon, kiwi,
grape, cherry, berry, banana, apple, oyster, walnut, pistachio, mandarin, lime, fig, chestnut
Figure 2: Optimal clustering for large evaluation set.
contexts used purity
(VI) 73.4%
(VI), (II) 76.6%
(VI), (II), (III) 80.1%
Table 5: Clustering the larger evaluation set with
an increasing number of context specifications.
given in Table 5. As can be seen the object re-
lation is added first again. This time though the
inclusion of adjectival modification brings another
performance increase which is even one per cent
above the result for the space built from all possi-
ble relations. The addition of any further contexts
consistently degrades performance. The clustering
solution thus produced is given in Figure 2. From
the 1 872 698 dimension used in the original space
only 341 214 are retained.
4 Discussion and Conclusion
Our results are counterintuitive at first sight as it
could be expected that a larger number of differ-
ent contexts would increase performance. Instead
we see the best performance with only a very lim-
23
ited set of possible contexts. We suspect that this
behaviour is due to a large amount of correlation
between the different kinds of contexts. The ad-
dition of further contexts beyond a certain point
therefore has no positive effect. As an indication
for this it might be noticed that the three context
specifications that yield the best result for the 402
word set comprise relations with the three main
open word classes. It is to be expected that they
contribute orthogonal information that covers cen-
tral dimensions of meaning. The slight decrease
in performance that can be observed when further
contexts are added is probably due to chance fluc-
tuations and almost certainly not significant; with
significance being hard to determine for any of the
results.
However, it is obviously necessary to cover a
basic variety of features. Patterns which are used
to explicitly track semantic relations on the tex-
tual surface seem to be too restrictive. Informa-
tion accessible from co-occurring verbs for exam-
ple is completely lost. In a regular window based
word space such information is retained and its
performance is competitive with a pattern based
approach. This method is obviously too liberal,
though, if compared to the dependency spaces.
In general we were able to show that seman-
tic spaces are obviously able to capture categori-
cal knowledge about concepts best when they are
built from a syntactically annotated source. This
is true even if the context specification used is not
the most parsimonious. The problem of determin-
ing the right set of contexts is therefore rather an
optimisation issue than a question of using depen-
dency based spaces or not. It is a considerable one,
though, as computations are much cheaper with
vectors of reduced dimensionality, of course.
For the categorisation task the inclusion of more
complex relations reaching over several dependen-
cies does not seem to be helpful considering they
can all be dropped without a decrease in perfor-
mance. As Pado and Lapata (2007) reached better
results in their experiments with a broader set of
context specifications we conclude that the selec-
tion of the kinds of context to include when con-
structing a word space depends largely on the task
at hand.
References
A. Almuhareb and M. Poesio. 2004. Attribute-
based and value-based clustering: An evaluation.
In Dekang Lin and Dekai Wu, editors, Proceedings
of EMNLP 2004, pages 158?165, Barcelona, Spain,
July. Association for Computational Linguistics.
M. Poesio and A. Almuhareb. 2005a. Concept learn-
ing and categorization from the web. In Proceedings
of CogSci2005 - XXVII Annual Conference of the
Cognitive Science Society, pages 103?108, Stresa,
Italy.
A. Almuhareb and M. Poesio. 2005b. Finding at-
tributes in the web using a parser. In Proceedings
of Corpus Linguistics, Birmingham.
Marco Baroni, Stefan Evert, and Alessandro Lenci, ed-
itors. 2008. ESSLLI Workshop on Distributional
Lexical Semantics, Hamburg, August.
J. R. Curran and M. Moens. 2002. Improvements in
automatic thesaurus extraction. In Proceedings of
the ACL-02 workshop on Unsupervised lexical ac-
quisition, pages 59?66, Morristown, NJ, USA. As-
sociation for Computational Linguistics.
T. Dunning. 1993. Accurate methods for the statistics
of surprise and coincidence. Computational Lin-
guistics, 19(1):61?74.
A. Ferraresi, E. Zanchetta, M. Baroni, and S. Bernar-
dini. 2008. Introducing and evaluating ukwac, a
very large web-derived corpus of english. In Pro-
ceedings of the WAC4 Workshop at LREC 2008.
G. Grefenstette. 1994. Explorations in Automatic The-
saurus Discovery. Kluwer Academic Publishers,
Dordrecht.
D. Hindle. 1990. Noun classification from predicate-
argument structures. In Meeting of the Association
for Computational Linguistics, pages 268?275.
G. Karypis. 2003. Cluto: A clustering toolkit. tech-
nical report 02-017. Technical report, University of
Minnesota, November.
D. Lin. 1998. Automatic retrieval and clustering of
similar words. In COLING-ACL, pages 768?774.
S. Pad? and M. Lapata. 2007. Dependency-based con-
struction of semantic space models. Comput. Lin-
guist., 33(2):161?199.
M. Poesio and A. Almuhareb. 2005b. Identifying con-
cept attributes using a classifier. In Proceedings of
the ACL-SIGLEX Workshop on Deep Lexical Acqui-
sition, pages 18?27, Ann Arbor, Michigan, June. As-
sociation for Computational Linguistics.
M. Sahlgren. 2006. The Word Space Model. Ph.D.
thesis, Department of Linguistics, Stockholm Uni-
versity.
H. Sch?tze. 1992. Dimensions of meaning. In Super-
computing ?92: Proceedings of the 1992 ACM/IEEE
conference on Supercomputing, pages 787?796, Los
Alamitos, CA, USA. IEEE Computer Society Press.
24
Proceedings of the NAACL HLT Workshop on Active Learning for Natural Language Processing, pages 9?17,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
On Proper Unit Selection in Active Learning:
Co-Selection Effects for Named Entity Recognition
Katrin Tomanek1? Florian Laws2? Udo Hahn1 Hinrich Schu?tze2
1Jena University Language & Information Engineering (JULIE) Lab
Friedrich-Schiller-Universita?t Jena, Germany
{katrin.tomanek|udo.hahn}@uni-jena.de
2Institute for Natural Language Processing, Universita?t Stuttgart, Germany
{fl|hs999}@ifnlp.org
Abstract
Active learning is an effective method for cre-
ating training sets cheaply, but it is a biased
sampling process and fails to explore large
regions of the instance space in many appli-
cations. This can result in a missed cluster
effect, which signficantly lowers recall and
slows down learning for infrequent classes.
We show that missed clusters can be avoided
in sequence classification tasks by using sen-
tences as natural multi-instance units for label-
ing. Co-selection of other tokens within sen-
tences provides an implicit exploratory com-
ponent since we found for the task of named
entity recognition on two corpora that en-
tity classes co-occur with sufficient frequency
within sentences.
1 Introduction
Active learning (AL) has been shown to be an effec-
tive approach to reduce the amount of data needed
to train an accurate statistical classifier. AL selects
highly informative examples from a pool of unla-
beled data and prompts a human annotator for the
labels of these examples. The newly labeled exam-
ples are added to a training set used to build a statis-
tical classifier. This classifier is in turn used to assess
the informativeness of further examples. Thus, a
select-label-retrain loop is formed that quickly se-
lects hard to classify examples, honing in on the de-
cision boundary (Cohn et al, 1996).
A fundamental characteristic of AL is the fact that
it constitutes a biased sampling process. This is so
? Both authors contributed equally to this work.
by design, but the bias can have an undesirable con-
sequence: partial coverage of the instance space. As
a result, classes or clusters within classes may be
completely missed, resulting in low recall or slow
learning progress. This has been called the missed
cluster effect (Schu?tze et al, 2006). While AL has
been studied for a range of NLP tasks, the missed
cluster problem has hardly been addressed.
This paper studies the missed class effect, a spe-
cial case of the missed cluster effect where complete
classes are overlooked by an active learner. The
missed class effect is the result of insufficient ex-
ploration before or during a mainly exploitative AL
process. In AL approaches where exploration is only
addressed by an initial seed set, poor seed set con-
struction gives rise to the missed class effect.
We focus on the missed class effect in the con-
text of a common NLP task: named entity recogni-
tion (NER). We show that for this task the missed
class effect is avoided by increasing the sampling
granularity from single-instance units (i.e., tokens)
to multi-instance units (i.e., sentences). For AL ap-
proaches to NER, sentence selection recovers better
from unfavorable seed sets than token selection due
to what we call the co-selection effect. Under this
effect, a non-targeted entity class co-occurs in sen-
tences that were originally selected because of un-
certainty on tokens of a different entity class.
The rest of the paper is structured as follows: Sec-
tion 2 introduces the missed class effect in detail.
Experiments which demonstrate the co-selection ef-
fect achieved by sentence selection for NER are de-
scribed in Section 3 and their results presented in
Section 4. We draw conclusions in Section 5.
9
2 The Missed Class Effect
This section first describes the missed class ef-
fect. Then, we discuss several factors influencing
this effect, focusing on co-selection, a natural phe-
nomenon in common NLP applications of AL.
2.1 Sampling bias and misguided AL
The distribution of the labeled data points obtained
with an active learner deviates from the true data
distribution. While this sampling bias is intended
and accounts for the effectiveness of AL, it also
poses challenges as it leads to classifiers that per-
form poorly in some regions, or clusters, of the ex-
ample space. In the literature, this phenomenon has
been described as the missed cluster effect (Schu?tze
et al, 2006; Dasgupta and Hsu, 2008)
In this context, we must distinguish between ex-
ploration and exploitation. By design, AL is a
highly exploitative strategy: regions around decision
boundaries are inspected thoroughly so that decision
boundaries are learned well, but regions far from any
of the initial decision boundaries remain unexplored.
An exploitative sampling approach thus has to be
combined with some kind of exploratory strategy to
make sure the example space is adequately covered.
A common approach is to start an AL process with
an initial seed set that accounts for the exploration
step. However, a seed set which is not represen-
tative of the example space may completely mis-
guide AL ? at least when no other explorative tech-
niques are applied as a remedy. While approaches
to balancing exploration and exploitation (Baram et
al., 2003; Dasgupta and Hsu, 2008; Cebron and
Berthold, 2009) have been discussed, we here fo-
cus on a ?pure? AL scenario where exploration takes
only place in the beginning by a seed set. In sum-
mary, the missed clusters are the result of a sce-
nario where poor exploration is combined with ex-
clusively exploitative sampling.
Why is AL an exploitative sampling strategy? AL
selects data points based on the confidence of the ac-
tive learner. Assume an initial seed set that does not
contain examples of a specific cluster. This leads to
an initial active learner that is mistakenly overconfi-
dent about the class membership of instances in this
missed cluster. Far away from the decision bound-
ary, the active learner assumes a high confidence for
A
B
C
(a)
A
B
C
(b)
Figure 1: Illustration of the missed cluster effect in a 1-
d scenario. Shaded points are contained in the seed set,
vertical lines are final decision boundaries, and dashed
rectangles mark the explored regions
all instances in that cluster, even if they are in fact
misclassified. Consequently, the active learner will
fail to select these instances for long until some re-
direction impulse is received (if at all).
To give an example, let us consider a simple 1-
d toy scenario with examples from three clusters A,
B, and C as shown in Figure 1. In scenario (a), AL
is started from a seed set including one example of
clusters A and B only. In subsequent rounds, AL
will select examples in these clusters only (shown as
the dashed box in the figure). Examples in cluster
C are ignored as they are far from the initial deci-
sion boundary. Eventually, a decision boundary is
fixed as shown by the vertical line which indicates
that this AL process has completely overlooked ex-
amples from cluster C.
Assuming that the examples fall in two classes
X1 = {A ? C} and X2 = {B} the learned clas-
sifier has low recall for class X1 and relatively low
precision for class X2 as it erroneously assigns ex-
amples of cluster C to class X2. In a related sce-
nario with three classes X1 = {A}, X2 = {B}, and
X3 = {C} this would even mean that the classifier
is not at all aware about the third class resulting in
the missed class problem.
A more representative seed set circumvents this
problem. Given a seed set including one example
of each cluster, AL might find a second decision
boundary1 between clusters B and C because it is
now aware of examples from C. Figure 1(b) shows
a possible result of AL on this seed set.
The missed cluster effect can be understood as
the generalized problem. A special case of it is the
1Assuming a classifier that can learn several boundaries.
10
missed class effect as shown in the previous exam-
ple. In general, it has the same causes (insufficient
exploration and misguided exploitation), but is eas-
ier to test. Often we know (at least the number of) all
classes under scrutiny, while we usually cannot as-
sume all clusters in the feature space to be known. In
this paper, we focus on the missed class effect, i.e.,
scenarios where classes are overlooked by a mis-
guided AL process resulting in a slow (active) learn-
ing progress.
2.2 Factors influcencing the missed class effect
AL in a practical scenario is subject to several fac-
tors which mitigate or intensify the missed class ef-
fect described before. In the following, we describe
three such factors, with a special focus on the co-
selection effect, which we claim to significantly mit-
igate the missed class effect in a specific type of NLP
tasks, sequence learning problems such as NER or
POS tagging.
Class imbalance Many studies on AL for NLP
tasks assume that AL is started from a randomly
drawn seed set. Such a seed set can be problem-
atic when the class distribution in the data is highly
skewed. In this case, ?rare? classes might not be
represented in the seed set, increasing the chance to
completely miss out such a class using AL. When
classes are relatively frequent, an active learner ?
even when started from an unfavorable seed set ?
might still mistake an example of one class for an
uncertain example of a different class and conse-
quently select it. Thereby, it can acquire information
about the former class ?by accident? leading to sud-
den and rapid discovery of the newly-found class.
However, in the case of extreme class imbalance this
is very unlikely. Severe class imbalance intensifies
the missed cluster effect.
Similarity of considered classes If, e.g., two of
the classes to be learned, say Xi and Xj , are harder
to discriminate than others, or if the data contains
lots of noise, an active learner is more likely to select
some instances of Xi if at least its ?similar? coun-
terpart Xj was represented in the seed set. Hence,
it may mistake the instances of Xi and Xj before it
has acquired enough information to discriminate be-
tween them. So, under certain situations similarity
of classes can mitigate the missed class effect.
The co-selection effect Many NLP tasks are se-
quence learning problems including, e.g., POS tag-
ging, and named entity recognition. Sequences are
consecutive text tokens constituting linguistically
plausible chunks, e.g., sentences. Algorithms for se-
quence learning obviously work on sequence data,
so respective AL approaches need to select complete
sequences instead of single text tokens (Settles and
Craven, 2008). Furthermore, sentence selection has
been preferred over token selection in other works
with the argument that the manual annotation of sin-
gle, possibly isolated tokens is almost impossible or
at least extremely time-consuming (Ringger et al,
2007; Tomanek et al, 2007).
Within such sequences, instances of different
classes often co-occur. Thus, an active learner that
selects uncertain examples of one class gets exam-
ples of a second class as an unintended, yet pos-
itive side effect. We call this the co-selection ef-
fect. As a result, AL for sequence labeling is not
?pure? exploitative AL, but implicitly comprises an
exploratory aspect which can substantially reduce
the missed class problem. In scenarios where we
cannot hope for such a co-selection, we are much
more likely to have decreased AL performance due
to missed clusters or classes.
3 Experiments
We ran several experiments to investigate how the
sampling granularity, i.e. the size of the selection
unit, influences the missed class effect. AL based
on token selection (T-AL) is compared to AL based
on sentence selection (S-AL). Although our experi-
ments are certainly also subject to the other factors
mitigating the missed class effect (e.g. similarity of
classes), the main focus of the experiments is on the
co-selection effect that we expected to observe in
S-AL. Several scenarios of initial exploration were
simulated by seed sets of different characteristics.
The experiments were run on synthetic and real data
in the context of named entity recognition (NER).
3.1 Classifiers and active learning setup
The active learning approach used for both S-AL
and T-AL is based on uncertainty sampling (Lewis
and Gale, 1994) with the margin metric (Schein and
Ungar, 2007) as uncertainty measure. Let c and c?
11
be the two most likely classes predicted for token
xj with p?c,xj and p?c?,xj being the associated class
probabilities. The per-token margin is calculated as
M = |p?c,xj ? p?c?,xj |.
For T-AL, the sampling granularity is the token,
while in S-AL, complete sentences are selected. For
S-AL, the margins of all tokens in a sentence are
averaged and the aggregate margin is used to select
sentences. We chose this uncertainty measure for S-
AL for better comparison with T-AL. In either case,
examples (tokens or sentences) with a small margin
are preferred for selection. In every iteration, a batch
of examples is selected: 20 sentences for S-AL, 200
tokens for T-AL.
Bayesian logistic regression as implemented in
the BBR classification package (Genkin et al, 2007)
with out-of-the-box parameter settings was used as
base learner for T-AL. For S-AL, a linear-chain
Conditional Random Field (Lafferty et al, 2001) is
employed as implemented in MALLET (McCallum,
2002). Both base learners employ standard features
for NER including the lexical token itself, various
orthographic features such as capitalization, the oc-
currence of special characters like hyphens, and con-
text information in terms of features of neighboring
tokens to the left and right of the current token.
3.2 Data sets
We used three data sets in our experiments. Two of
them (ACE and PBIO) are standard data sets. The
third (SYN) is a synthetic set constructed to have
specific characteristics. For simplicity, we consider
only scenarios with two entity classes, a majority
class (MAJ) and a minority class (MIN). We dis-
carded all other entity annotations originally con-
tained in the corpus assigning the OUTSIDE class.2
The first data set (PBIO) is based on the annota-
tions of the PENNBIOIE corpus for biomedical en-
tity extraction (Kulick et al, 2004). As PENNBIOIE
makes fine-grained and subtle distinctions between
various subtypes of classes irrelevant for this study,
we combined several of the original classes into two
entity classes: The majority class consists of the
three original classes ?gene-protein?, ?gene-generic?,
and ?gene-rna?. The minority class consists of
the original and similar classes ?variation-type? and
2The OUTSIDE class marks that a token is not part of an
named entity.
?variation-event?. All other entity labels were re-
placed by the OUTSIDE class.
The second data set (ACE) is based on the
newswire section of the ACE 2005 Multilingual
Training Corpus (Walker et al, 2006). We chose
the ?person? class as majority class and the ?organi-
zation? class as the minority class. Again, all other
classes are mapped to OUTSIDE.
The synthetic data set (SYN) was constructed by
combining the sentences from the original ACE and
PENNBIOIE corpora. The ?person? class consti-
tutes the minority class, the very similar classes
?malignancy? and ?malignancy-type? were merged
to form the majority class. All other class la-
bels were set to OUTSIDE. SYN?s construction
was motivated by the following characteristics of
the new data set which would make the appear-
ance of the missed class effect very likely for
insufficient exploration scenarios:
(i) absence of inner-sentence entity class correlation
to ensure that sentences contain either mentions of
only a single entity class or no mentions at all.
(ii) marked entity class imbalance between the ma-
jority and minority classes
(iii) dissimilar surface patterns of entity mentions of
the two entity classes with the rationale that class
similarity will be low.
Table 1 summarizes characteristics of the data
sets. While SYN exhibits high imbalance (e.g., 1:9.4
on the token level), PBIO and ACE are moderately
skewed. In PBIO, the number of sentences contain-
ing any entity mention is relatively high compared
to ACE or SYN. For our experiments, the corpora
were randomly split in a pool for AL and a test set
for performance evaluation.
Inner-sentence entity class co-occurrence We
have described co-selection as a potential mitigat-
ing factor for the missed class effect in Section 2.
For this effect to occur, there must be some corre-
lation between the occurrence of entity mentions of
the MAJ class with those from MIN.
Table 2 shows correlation statistics based on the
?2 measure. We found strong correlation in all three
corpora3: For ACE and PBIO, the correlation is pos-
itive; for SYN it is negative so when a sentence in
SYN contains a majority class entity mention, it is
3All correlations are statistically significant (p < 0.01).
12
PBIO ACE SYN
sentences (all) 11,164 2,642 13,804
sentences (MAJ) 7,075 767 5,667
sentences (MIN) 2,156 974 974
MIN-MAJ ratio 1 : 3.3 1 : 1.3 1 : 5.8
tokens (all) 277,053 66,752 343,773
tokens (MAJ) 17,928 2,008 18,959
tokens (MIN) 4,079 1,822 2,008
MIN-MAJ ratio 1 : 4.4 1 : 1.1 1 : 9.4
Table 1: Characteristics of the data sets; ?sentences
(MAJ)?, e.g., specifies the number of sentences contain-
ing mentions of the majority class.
PBIO ACE SYN
?2 132.34 6.07 727
P (MIN |MAJ) 0.26 0.31 0.0
Table 2: Co-occurrence of entity classes in sentences
highly unlikely that it also contains a minority entity.
In fact, it is impossible by construction of the data
set. Further, this table shows the probability that a
sentence containing the majority class also contains
the minority class. As expected, this is exactly 0 for
SYN, but significantly above 0 for PBIO and ACE.
3.3 Seed sets
Selection of an appropriate seed set for the start of an
AL process is important to the success of AL. This is
especially relevant in the case of imbalanced classes
because a typically small random sample will pos-
sibly not contain any example of the rare class. We
constructed different types of seed sets (whose nam-
ing intentionally reflects the use of the entity classes
from Section 3.2) to simulate different scenarios of
ill-managed initial exploration. All seed sets have
a size of 20 sentences. The RANDOM set was ran-
domly sampled, the MAJ set is made of sentences
containing at least one majority class entity, but no
minority class entity. Accordingly, MIN is densely
populated with minority entities. Finally, OUTSIDE
contains only sentences without entity mentions.
One could think of the OUTSIDE and MAJ seed
sets of cases where a random seed set selection has
unluckily produced an especially bad seed set. MIN
serves to demonstrate the opposite case. For each
type of seed set, we sampled ten independent ver-
sions to calculate averages over several AL runs.
3.4 Cost measure
The success of AL is usually measured as reduc-
tion of annotation effort according to some cost mea-
sure. Traditionally, the most common cost measure
considers a unit cost per annotated token, which fa-
vors AL systems that select individual tokens. In
a real annotation setting, however, it is unnatural,
and therefore hard for humans to annotate single,
possibly isolated tokens, leading to bad annotation
quality (Hachey et al, 2005; Ringger et al, 2007).
When providing context, the question arises whether
the annotator can label several tokens present in the
context (e.g., an entire multi-token entity or even
the whole sentence) at little more cost than anno-
tating a single token. Thus, assigning a linear cost
of n to a sentence where n is the sentence?s length
in tokens seems to unfairly disadvantage sentence-
selection AL setups.
However, more work is needed to find a more re-
alistic cost measure. At present there is no other
generally accepted cost measure than unit cost per
token, so we report costs using the token measure.
4 Results
This section presents the results of our experiments
on the missed class effect in two different AL
scenarios, i.e., sentence selection (S-AL) and to-
ken selection (T-AL). The AL runs were stopped
when convergence on the minority class F-score was
achieved. This was done because early AL iterations
before the convergence point are most important and
representative for a real-life scenario where the pool
is extremely large, so that absolute convergence of
the classifier?s performance will never be reached.
The learning curves in Figures 2, 3, and 4 reveal
general characteristics of S-AL compared to T-AL.
For S-AL, the number of tokens on the x-axis is the
total number of tokens in the sentences labeled so
far. While S-AL generally yields higher F-scores, T-
AL converges much earlier when counted in terms
of tokens. The reason for this is that T-AL can se-
lect uncertain data more specifically. In contrast, S-
AL also selects tokens that the classifier can already
classify reliably ? these tokens are selected because
they co-occur in a sentence that also contains an un-
certain token. Whether T-AL is really more efficient
clearly depends on the cost-metric applied (cf. Sec-
13
0 5000 10000 150000.0
0.2
0.4
0.6
0.8
tokens
F?score
MIN classMAJ class
(a) T-AL learning curve, single run
with OUTSIDE seed
0 5000 10000 150000.5
0.6
0.7
0.8
0.9
1.0
tokens
mean
 margin
MIN classMAJ class
(b) T-AL mean margin curve, single
run with OUTSIDE seed
0 5000 10000 150000.0
0.2
0.4
0.6
0.8
tokens
minority
 class F
?score
MIN seedMAJ seedOUTSIDE seedRANDOM seedrandom selection
(c) T-AL learning curves, minority
class, all seeds, 10 runs
0 10000 30000 50000 700000.0
0.2
0.4
0.6
0.8
tokens
per clas
s F?sco
re
MIN classMAJ class
(d) S-AL learning curve, single run
with OUTSIDE seed
0 10000 30000 50000 700000.0
0.2
0.4
0.6
0.8
1.0
tokens
mean
 margin
MIN classMAJ class
(e) S-AL mean margin curve, single
run with OUTSIDE seed
0 10000 30000 50000 700000.0
0.2
0.4
0.6
0.8
tokens
minority
 class F
?score
MIN seedMAJ seedOUTSIDE seedRANDOM seedrandom selection
(f) S-AL learning curves, minority
class, all seeds, 10 runs
Figure 2: Results on SYN corpus for token selection (a,b,c) and sentence selection (d,e,f)
tion 3.4). Since the focus of this paper is on compar-
ing the missed class effect in a sentence and a token
selection AL setting (T-AL and S-AL) we apply the
straight-forward token measure.
4.1 The pathological case
Figure 2 shows results on the SYN corpus for T-AL
(upper row) and S-AL (lower row). Figures 2(a)
and 2(d) show the minority and majority class learn-
ing curves for a single run starting from the OUT-
SIDE seed set, which was particularly problematic
on SYN. (We show single runs to give a better pic-
ture of what happens during the selection process.)
The figures show that for both AL scenarios, the
OUTSIDE seed set caused the active learner to focus
exclusively on the majority class and to completely
ignore the minority class for many AL iterations (al-
most 30,000 tokens for S-AL and over 4,000 tokens
for T-AL). Had we stopped the AL process before
this turning point, the classifier?s performance on
the majority entity class would have been reason-
ably high while the minority class would not have
been learned at all ? which is precisely the defini-
tion of an (initially) missed class.
Figures 2(b) and 2(e) show the corresponding
mean margin plots of these AL runs, indicating the
confidence of the classifier on each class. The mean
margin is calculated as the average margin over to-
kens in the remaining pool, separately for each true
class label.4 As expected, the active learner is over-
confident but wrong on instances of the minority
class (assigning them to the OUTSIDE class, we
assume). Only after some time, margin scores on
minority class tokens start decreasing. This hap-
pens because from time to time minority class ex-
amples are mistakenly considered as majority class
examples with low confidence and thus selected by
accident. Lowered minority class confidence then
causes the selection of further minority class exam-
ples, resulting in a turning point with a steep slope
of the minority class learning curve.
Consequences of seed set selection We compare
the minority class learning curves for all types of
4Note that in a real, non-simulation active learning task, the
true class labels would be unknown.
14
0 5000 10000 15000 200000.0
0.2
0.4
0.6
0.8
tokens
F?score
MIN classMAJ class
(a) T-AL learning curve, single run
with MAJ seed
0 5000 10000 15000 200000.5
0.6
0.7
0.8
0.9
1.0
tokens
mean
 margin
MIN classMAJ class
(b) T-AL mean margin curve, single
run with MAJ seed
0 5000 10000 15000 200000.0
0.2
0.4
0.6
0.8
tokens
minority
 class F
?score
MIN seedMAJ seedOUTSIDE seedRANDOM seedrandom selection
(c) T-AL learning curves, minority
class, all seeds, 10 runs
0 10000 20000 30000 40000 500000.0
0.2
0.4
0.6
0.8
tokens
per clas
s F?sco
re
MIN classMAJ class
(d) S-AL learning curve, single run
with MAJ seed
0 10000 20000 30000 40000 500000.0
0.2
0.4
0.6
0.8
1.0
tokens
mean
 margin
MIN classMAJ class
(e) S-AL mean margin curve, single
run with MAJ seed
0 10000 20000 30000 40000 500000.0
0.2
0.4
0.6
0.8
tokens
minority
 class F
?score
MIN seedMAJ seedOUTSIDE seedRANDOM seedrandom selection
(f) S-AL learning curves, minority
class, all seeds, 10 runs
Figure 3: Results on PBIO corpus for token selection (a,b,c) and sentence selection (d,e,f)
seed sets and for random selection (cf. Figures 2(c)
and 2(f)), now averaged over 10 runs. On S-AL all
but the MIN seed set were inferior to random selec-
tion. Even the commonly used random seed set se-
lection is problematic because the minority class is
so rare that there are random seed sets without any
example of the minority class.
On T-AL, all seed sets are better than random se-
lection. This, however, is because random selec-
tion is an extremely weak baseline for T-AL due to
the token distribution (cf. Table 1). Still, the RAN-
DOM, MAJ, and OUTSIDE seed sets are signifi-
cantly worse than a seed set which covers the minor-
ity class well. Note that the majority class learning
curves are relatively invariant against different seed
sets. The minority class seed set does have some
negative impact on initial learning progress on the
majority class (not shown here), but the impact is
rather small. Because of the higher frequency of
the majority class, the classifier soon finds major-
ity class examples to compensate for the seed set by
chance or class similarity.
4.2 Missed class effect mitigated by co-selection
Results on PBIO corpus On the PBIO corpus,
where minority and majority class entity mentions
naturally co-occur on the sentence level, we get
a different picture. Figure 3 shows the learning
(3(a), 3(d)) and mean margin (3(b), 3(e)) curves for
the MAJ seed set. T-AL still exhibits the missed
class effect on this seed set. The minority class
learning curve again has a delayed slope and high
mean margin scores of minority tokens at the be-
ginning, resulting in insufficient selection and slow
learning. S-AL, on the other hand, does not re-
ally suffer from the missed class effect: minor-
ity class entity mentions are co-selected in sen-
tences which were chosen due to uncertainty on
majority class tokens. Minority class mean mar-
gin scores quickly fall, reinforcing selection for mi-
nority class entities. Learning curves for minority
and majority classes run approximately in parallel.
Figure 3(f) shows that all seed sets perform quite
similar for S-AL. MIN unsurprisingly is a bit better.
With the other seed sets, S-AL performance is com-
15
0 2000 4000 6000 80000.0
0.2
0.4
0.6
0.8
tokens
minority 
class F?
score
MIN seedMAJ seedOUTSIDE seedRANDOM seedrandom selection
(a) T-AL
0 5000 15000 250000.0
0.2
0.4
0.6
0.8
tokens
minority 
class F?
score
MIN seedMAJ seedOUTSIDE seedRANDOM seedrandom selection
(b) S-AL
Figure 4: Minority class learning curves for all seeds on
ACE averaged over 10 runs
parable to random selection. On the PBIO corpus,
random selection is a strong baseline as almost every
sentence contains an entity mention ? which is not
the case for SYN and ACE (cf. Table 1). As there is
no co-selection effect for T-AL, the MAJ and OUT-
SIDE seed sets also here are subject to the missed
class problem (Figure 3(c)), although not as severely
as on the SYN corpus.
Results on ACE corpus Figure 4 shows learning
curves averaged over 10 runs on ACE. Overall, the
missed class effect is less pronounced on ACE com-
pared to PBIO. Still, co-selection avoids a good por-
tion of the missed class effect on S-AL ? all seed
sets yield results much better than random selection
right from the beginning.
On T-AL, the OUTSIDE seed set has a marked
negative effect. However, while different seed
sets still have visible differences in learning perfor-
mance, the magnitude of the effect is smaller than
on PBIO. It is difficult to find the exact reasons
in a non-synthetic, natural language corpus where a
lot of different effects are intermingled. One might
assume higher class similarity between the major-
ity (?persons?) and the minority (?organizations?)
classes on the ACE corpus than, e.g., on the PBIO
corpus. Moreover, there is hardly any imbalance
in frequency between the two entity classes on the
ACE corpus. We briefly discussed such influencing
factors possibly mitigating the missed class effect in
Section 2.2.
4.3 Discussion
To summarize, on a synthetic corpus (SYN) the
missed class effect can be well studied in both
AL scenarios, i.e., S-AL and T-AL. Moving from
a relatively controlled, synthetic corpus (extreme
class imbalance, no inner-sentence co-occurrence
between entity classes, quite different entity classes)
to more realistic corpora, effects generally mix a bit
due to different degrees of class imbalance and prob-
ably higher similarity between entity classes.
Our experiments unveil that co-selection in S-AL
effectively helps avoid dysfunctional classifiers that
insufficiently explore the instance space due to a
disadvantageous seed set. In contrast, AL based
on token-selection (T-AL) cannot recover from in-
sufficient exploration as easy as AL with sentence-
selection and is thus more sensitive to the missed
class effect.
5 Conclusion
We have shown that insufficient exploration in the
initial stages of active learning gives rise to regions
of the sample space that contain missed classes that
are incorrectly classified. This results in low clas-
sification performance and slow learning progress.
Comparing two sampling granularities, tokens vs.
sentences, we found that the missed class effect is
more severe when isolated tokens instead of sen-
tences are selected for labeling.
The missed class problem in sequence classifica-
tion tasks can be avoided using sentences as natural
multi-instance units for selection and labeling. Us-
ing multi-instance units, co-selection of other tokens
within sentences provides an implicit exploratory
component. This solution is effective if classes co-
occur sufficiently within sentences which is the case
for many real-life entity recognition tasks.
While other work has proposed sentence selection
in AL for sequence labeling as a means to ease and
speed up annotation, we have gathered here addi-
tional motivation from the perspective of robustness
of learning. Future work will compare the beneficial
effect introduced by co-selection with other forms of
exploration-enabled active learning.
Acknowledgements
The first and the third author were funded by the
German Ministry of Education and Research within
the StemNet project (01DS001A-C) and by the EC
within the BOOTStrep project (FP6-028099).
16
References
Yoram Baram, Ran El-Yaniv, and Kobi Luz. 2003. On-
line choice of active learning algorithms. In ICML
?03: Proceedings of the 20th International Conference
on Machine Learning, pages 19?26.
Nicolas Cebron and Michael R. Berthold. 2009. Active
learning for object classification: From exploration to
exploitation. Data Mining and Knowledge Discovery,
18(2):283?299.
David A. Cohn, Zoubin Ghahramani, and Michael I. Jor-
dan. 1996. Active learning with statistical models.
Journal of Artificial Intelligence Research, 4:129?145.
Sanjoy Dasgupta and Daniel Hsu. 2008. Hierarchical
sampling for active learning. In ICML ?08: Proceed-
ings of the 25th International Conference on Machine
Learning, pages 208?215.
Alexander Genkin, David D. Lewis, and David Madigan.
2007. Large-scale Bayesian logistic regression for text
categorization. Technometrics, 49(3):291?304.
Ben Hachey, Beatrice Alex, and Markus Becker. 2005.
Investigating the effects of selective sampling on the
annotation task. In CoNLL ?05: Proceedings of the
9th Conference on Computational Natural Language
Learning, pages 144?151.
Seth Kulick, Ann Bies, Mark Liberman, Mark Mandel,
Ryan T. McDonald, Martha S. Palmer, and Andrew Ian
Schein. 2004. Integrated annotation for biomedical
information extraction. In Proceedings of the HLT-
NAACL 2004 Workshop ?Linking Biological Litera-
ture, Ontologies and Databases: Tools for Users?,
pages 61?68.
John D. Lafferty, Andrew McCallum, and Fernando
Pereira. 2001. Conditional random fields: Probabilis-
tic models for segmenting and labeling sequence data.
In ICML ?01: Proceedings of the 18th International
Conference on Machine Learning, pages 282?289.
David D. Lewis and William A. Gale. 1994. A sequential
algorithm for training text classifiers. In Proceedings
of the 17th Annual International ACM SIGIR Confer-
ence on Research and Development in Information Re-
trieval, pages 3?12.
Andrew McCallum. 2002. MALLET: A machine learn-
ing for language toolkit. http://mallet.cs.
umass.edu.
Eric Ringger, Peter McClanahan, Robbie Haertel, George
Busby, Marc Carmen, James Carroll, Kevin Seppi, and
Deryle Lonsdale. 2007. Active learning for part-of-
speech tagging: Accelerating corpus annotation. In
Proceedings of the Linguistic Annotation Workshop at
ACL-2007, pages 101?108.
Andrew Schein and Lyle Ungar. 2007. Active learn-
ing for logistic regression: An evaluation. Machine
Learning, 68(3):235?265.
Hinrich Schu?tze, Emre Velipasaoglu, and Jan Pedersen.
2006. Performance thresholding in practical text clas-
sification. In CIKM ?06: Proceedings of the 15th ACM
International Conference on Information and Knowl-
edge Management, pages 662?671.
Burr Settles and Mark Craven. 2008. An analysis of ac-
tive learning strategies for sequence labeling tasks. In
EMNLP ?08: Proceedings of the 2008 Conference on
Empirical Methods in Natural Language Processing,
pages 1070?1079.
Katrin Tomanek, Joachim Wermter, and Udo Hahn.
2007. An approach to text corpus construction which
cuts annotation costs and maintains reusability of an-
notated data. In Proceedings of the 2007 Joint Confer-
ence on Empirical Methods in Natural Language Pro-
cessing and Computational Natural Language Learn-
ing, pages 486?495.
Christopher Walker, Stephanie Strassel, Julie Medero,
and Kazuaki Maeda. 2006. ACE 2005 Multilin-
gual Training Corpus. Linguistic Data Consortium,
Philadelphia.
17
Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 80?88,
Beijing, August 2010
Self-Annotation for Fine-Grained Geospatial Relation Extraction
Andre Blessing Hinrich Schu?tze
Institute for Natural Language Processing
Universita?t Stuttgart
ner@ifnlp.org
Abstract
A great deal of information on the Web is
represented in both textual and structured
form. The structured form is machine-
readable and can be used to augment the
textual data. We call this augmentation
? the annotation of texts with relations
that are included in the structured data ?
self-annotation. In this paper, we intro-
duce self-annotation as a new supervised
learning approach for developing and im-
plementing a system that extracts fine-
grained relations between entities. The
main benefit of self-annotation is that it
does not require manual labeling. The in-
put of the learned model is a represen-
tation of the free text, its output struc-
tured relations. Thus, the model, once
learned, can be applied to any arbitrary
free text. We describe the challenges for
the self-annotation process and give re-
sults for a sample relation extraction sys-
tem. To deal with the challenge of fine-
grained relations, we implement and eval-
uate both shallow and deep linguistic anal-
ysis, focusing on German.
1 Introduction
In the last years, information extraction has be-
come more important in domains like context-
aware systems (e.g. Nexus (Du?rr et al, 2004)) that
need a rich knowledge base to make the right de-
cisions in different user contexts. Geospatial data
are one of the key features in such systems and
need to be represented on different levels of de-
tail. Data providers do not cover all these lev-
els completely. To overcome this problem, fine-
grained information extraction (IE) methods can
be used to acquire the missing knowledge. We
define fine-grained IE as methods that recognize
entities at a finer grain than standard categories
like person, location, and organization. Further-
more, the quality of the data in context-aware sys-
tems plays an important role and updates by an in-
formation extraction component can increase the
overall user acceptance.
For both issues an information extraction sys-
tem is required that can handle fine-grained rela-
tions, e.g., ?X is a suburb of Y? or ?the river X
is a tributary of Y? ? as opposed to simple con-
tainment. The World Wide Web offers a wealth of
information about geospatial data and can be used
as source for the extraction task. The extraction
component can be seen as a kind of sensor that we
call text senor (Blessing et al, 2006).
In this paper, we address the problem of de-
veloping a flexible system for the acquisition of
relations between entities that meets the above
desiderata. We concentrate on geospatial entities
on a fine-grained level although the approach is
in principle applicable to any domain. We use
a supervised machine learning approach, includ-
ing several features on different linguistic lev-
els, to build our system. Such a system highly
depends on the quality and amount of labeled
data in the training phase. The main contri-
bution of this paper is the introduction of self-
annotation, a novel approach that allows us to
eliminate manual labeling (although training set
creation also involves costs other than labeling).
Self-annotation is based on the fact that Word
Wide Web sites like Wikipedia include, in addi-
80
tion to unstructured text, structured data. We use
structured data sources to automatically annotate
unstructured texts. In this paper, we use German
Wikipedia data because it is a good source for the
information required for our context-aware sys-
tem and show that a system created without man-
ual labeling has good performance.
Our trained model only uses text, not the struc-
tured data (or any other markup) of the input doc-
uments. This means that we can train an informa-
tion extractor on Wikipedia and then apply it to
any text, regardless of whether this text also con-
tains structured information.
In the first part of this paper, we discuss
the challenges of self-annotation including some
heuristics which can easily be adapted to different
relation types. We then describe the architecture
of the extraction system. The components we de-
velop are based on the UIMA (Unstructured In-
formation Management Architecture) framework
(Hahn et al, 2008) and include two linguistic en-
gines (OpenNLP1, FSPar). The extraction task is
performed by a supervised classifier; this classi-
fier is also implemented as a UIMA component
and uses the ClearTK framework. We evaluate our
approach on two types of fine-grained relations.
2 Related work
Jiang (2009) also addresses the issue of super-
vised relation extraction when no large manually
labeled data set is available. They use only a few
seed instances of the target relation type to train
a supervised relation extraction system. However,
they use multi-task transfer learning including a
large amount of labeled instances of other relation
types for training their system. In contrast, our
work eliminates manual labeling by using struc-
tured data to annotate the relations.
Wu and Weld (2007) extract facts from in-
foboxes and link them with their corresponding
representation in the text. They discuss several is-
sues that occur when using infoboxes as a knowl-
edge base, in particular, (i) the fact that infoboxes
are incomplete; and (ii) schema drift. Schema
drift occurs when authors over time use differ-
ent attribute names to model facts or the same
1http://opennlp.sourceforge.net/
attributes are used to model different facts. So
the semantics of the infoboxes changes slightly
and introduces noise into the structured informa-
tion. Their work differs from self-annotation in
that they are not interested in the creation of self-
annotated corpora that can be used as training data
for other tasks. Their goal is to develop methods
that make infoboxes more consistent.
Zhang and Iria (2009) use a novel entity extrac-
tion method to automatically generate gazetteers
from seed lists using Wikipedia as knowledge
source. In contrast to our work they need struc-
tured data for the extraction while our system fo-
cuses on the extraction of information from un-
structured text. Methods that are applicable to
any unstructured text (not just the text in the
Wikipedia) are needed to increase coverage be-
yond the limited number of instances covered in
Wikipedia.
Nothman et al (2009) also annotate
Wikipedia?s unstructured text using struc-
tured data. The type of structured data they use is
hyperlinking (as opposed to infoboxes) and they
use it to derive a labeled named entity corpus.
They show that the quality of the annotation is
comparable to other manually labeled named
entity recognition gold standards. We interpret
their results as evidence that self-annotation can
be used to create high quality gold standards.
3 Task definition
In this section, we describe the annotation task;
give a definition of the relation types covered in
this paper; and introduce the extraction model.
We focus on binary relations between two re-
lation arguments occurring in the same sentence.
To simplify the self-annotation process we restrict
the first argument of the relation to the main en-
tity of the Wikipedia article. As we are building
text sensors for a context aware system, relations
between geospatial entities are of interest. Thus
we consider only relations that use a geospatial
named entity as second argument.
We create the training set by automatically
identifying all correct binary relations in the text.
To this end, we extract the relations from the
structured part of the Wikipedia, the infoboxes.
Then we automatically find the corresponding
81
sentences in the text and annotate the relations
(see section 4). All other not yet marked binary
relations between the main entity and geospatial
entities are annotated as negative samples. The
result of this step is a self-annotated training set.
In the second step of our task, the self-
annotated training set is used to train the extrac-
tion model. The model only takes textual features
as input and can be applied to any free text.
3.1 Classification task and relations used
Our relation extraction task is modeled as a classi-
fication task which considers a pair of named en-
tities and decides whether they occur in the re-
quested relation or not. The classifier uses ex-
tracted features for this decision. Features be-
long to three different classes. The first class con-
tains token-based features and their linguistic la-
bels like part-of-speech, lemma, stem. In the sec-
ond class, we have chunks that aggregate one or
more tokens into complex units. Dependency re-
lations between the tokens are represented in the
third class.
Our classifier is applicable to a wide spectrum
of geospatial relation types. For the purposes of
a focused evaluation, we selected two relations.
The first type contains rivers and the bodies of
water into which they flow. We call it river-
bodyOfWater relation. Our second type is com-
posed of relations between towns and the corre-
sponding suburb. We call this town-suburb rela-
tion.
3.2 Wikipedia as resource
Wikipedia satisfies all corpus requirements for our
task. It contains a lot of knowledge about geospa-
tial data with unstructured (textual) and structured
information. We consider only German Wikipedia
articles because our target application is a German
context aware system. In relation extraction for
German, we arguably face more challenges ? e.g.,
more complex morphology and freer word order ?
than we would in English.
For this work we consider only a subset of the
German Wikipedia. We use all articles that belong
to the following categories: Rivers by country,
Mountains by country, Valleys by country, Islands
by country, Mountain passes by country, Forests
by country and Settlements by country.
For the annotation task we use the structural
content of Wikipedia articles. Most articles be-
longing to the same categories use similar tem-
plates to represent structured information. One
type of template is the infobox, which con-
tains pairs of attributes and their values. These
attribute-value pairs specify a wide range of
geospatial relation types including fine-grained
relations. In this work we consider only the in-
fobox data and the article names from the struc-
tured data.
For context-aware systems fine-grained relation
types are particularly relevant. Such relations are
not represented in resources like DBPedia (Auer
et al, 2007) or Yago (Suchanek et al, 2007) al-
though they also consist of infobox data. Hence,
we have to build our own extraction component
(see section 5.2) when using infoboxes.
4 Self-Annotation
Self-annotation is a two-fold task. First, the struc-
tured data, in our case the infoboxes of Wikipedia
articles, must be analyzed to get al relevant
attribute-value pairs. Then all relevant geospatial
entities are marked and extracted. In a second step
these entities must be matched with the unstruc-
tured data.
In most cases, the extraction of the named en-
tities that correspond to the required relations is
trivial because the values in the infoboxes con-
sist only of one single entity or one single link.
But in some cases the values contain mixed con-
tent which can include links, entities and even
free text. In order to find an accurate extraction
method for those values we have developed sev-
eral heuristics. See section 5.2 for discussion.
The second task links the extracted structured
data to tokens in the textual data. Pattern based
string matching methods are not sufficient to iden-
tify all relations in the text. In many cases, mor-
phological rules need to be applied to identify
the entities in the text. In other cases, the pre-
processed text must be retokenized because the
borders of multi-word expressions are not consis-
tent with the extracted names in step one. One
other issue is that some named entities are a subset
of other named entities (Lonau vs. kleine Lonau;
82
Figure 1: Infobox of the German Wikipedia article
about Gollach.
similar to York vs. New York). We have to use a
longest match strategy to avoid such overlapping
annotations.
The main goal of the self-annotation task is
to reach the highest possible annotation quality.
Thus, only complete extracted relations are used
for the annotation process while incomplete data
are excluded from the training set. This procedure
reduces the noise in the labeled data.
4.1 Example
We use the river-bodyOfWater relation between
the two rivers Gollach and Tauber to describe the
self-annotation steps.
Figure 1 depicts a part of the infobox for the
German Wikipedia article about the river Gollach.
For this relation the attribute Mu?ndung ?mouth? is
relevant. The value contains unstructured infor-
mation (i.e., text, e.g. bei ?at? Bieberehren) and
structured information (the link from Bieberehren
to its Wikipedia page). The relation we want to
extract is that the river Gollach flows into the river
Tauber.
Bieberehrensie
sie
Tauber
Gollach
Gollach Tauber
Sie
Gollach
Tauber
Figure 2: Textual content of the German
Wikipedia article about Gollach. All named enti-
ties which are relevant for the river-bodyOfWater
relation are highlighted. This article contains two
instances for the relation between Gollach and
Tauber.
Figure 2 shows the textual content of the Gol-
lach article. We have highlighted all relevant
named entities for the self-annotation process.
This includes the name of the article and instances
of the pronoun sie referring to Gollach. Our
matching algorithm identifies two sentences as
positive samples for the relation between Gollach
and Tauber:
? (i) Die Gollach ist ein rechter Nebenfluss der
Tauber in Mittel- und Unterfranken. (The
Gollach is a right tributary of the Tauber in
Middle and Lower Franconia.)
? (ii) Schlie?lich mu?ndet sie in Bieberehren
auf 244 m in die Tauber. (Finally, it dis-
charges in Bieberehren at 244 m above MSL
into the Tauber.)
5 Processing
In this section we describe how the self-annotation
method and relation extraction is implemented.
First we introduce the interaction with the
Wikipedia resource to acquire the structured
and unstructured information for the processing
83
pipeline. Second we present the components of
the UIMA pipeline which are used for the relation
extraction task.
5.1 Wikipedia interaction
We use the JWPL API (Zesch et al, 2008) to
pre-process the Wikipedia data. This interface
provides functions to extract structured and un-
structured information from Wikipedia. How-
ever, many Wikipedia articles do not adhere to
valid Wikipedia syntax (missing closing brack-
ets etc.). The API also does not correctly handle
all Wikipedia syntax constructions. We therefore
have enhanced the API for our extraction task to
get high quality data for German Wikipedia arti-
cles.
5.2 Infobox extraction
As discussed in section 4 infoboxes are the key
resource for the self-annotation step. However
the processing of infoboxes that include attribute-
value pairs with mixed content is not trivial.
For each new relation type an initial manual ef-
fort is required. However, in comparison to the
complete annotation of a training corpus, this ef-
fort is small. First the attributes used in the in-
foboxes of the Wikipedia articles relevant for a
specific relation have to be analyzed. The results
of this analysis simplify the choice of the cor-
rect attributes. Next, the used values of these at-
tributes must be investigated. If they contain only
single entries (links or named entities) the extrac-
tion is trivial. However, if they consist of mixed
content (see section 4.1) then specific extraction
methods have to be applied. We investigated dif-
ferent heuristics for the self-annotation process to
get a method that can easily be adapted to new re-
lation types.
Our first heuristic includes a set of rules spec-
ifying the extraction of the values from the in-
foboxes. This heuristic gives an insufficient basis
for the self-annotation task because the rich mor-
phology and free word order in German can not
be modeled with simple rules. Moreover, hand-
crafted rules are arguably not as robust and main-
tainable as a statistical classifier trained on self-
annotated training material.
Our second heuristic is a three step process. In
step one we collect all links in the mixed con-
tent and replace them by a placeholder. In the
second step we tag the remaining content with
the OpenNLP tokenizer to get al named entities.
Both collected lists are then looked up in a lexicon
that contains named entities and the correspond-
ing geospatial classes. This process requires a nor-
malization procedure that includes the application
of morphological methods. The second method
can be easily adapted to new relation types.
5.3 UIMA
The self-annotated corpora are processed by sev-
eral components of the UIMA (Mu?ller et al,
2008) pipeline. The advantage of exchangeable
collection readers is that they seamlessly handle
structured and unstructured data. Another advan-
tage of using UIMA is the possibility to share
components with other research groups. We can
easily exchange different components, like the us-
age of the commonly known OpenNLP process-
ing tools or the FSPar NLP engine (Schiehlen,
2003) (which includes the TreeTagger (Schmid,
1995)). This allows us to experiment with dif-
ferent approaches, e.g., shallow vs. deep analy-
sis. The components we use provide linguistic
analysis on different levels: tokens, morphology,
part of speech (POS), chunking and partial depen-
dency analysis. Figure 4 shows the results after
the linguistic processing of our sample sentence.
For this work only a few annotations are wrapped
as UIMA types: token (incl. lemma, POS), multi-
word, sentence, NP, PP and dependency relations
(labeled edges between tokens). We will intro-
duce our machine learning component in section
5.5. Finally, the CAS consumers allow us to store
extracted facts in a context model.
Figure 3 shows the article about Gollach after
linguistic processing. In the legend all annotated
categories are listed. We highlighted all marked
relations, all references to the article name (re-
ferred to as subject in the figure) and links. After
selection of the Tauber relation, all annotations for
this token are listed in the right panel.
5.4 Coreference resolution
Using anaphora to refer to the main entity is a
common practice of the authors of Wikipedia ar-
84
Figure 3: Screenshot of the UIMA Annotation-
Viewer.
ticles. Coreference resolution is therefore neces-
sary for our annotation task. A shallow linguis-
tic analysis showed that the writing style is simi-
lar throughout Wikipedia articles. Based on this
observation, we empirically investigated some
geospatial articles and came to the conclusion that
a simple heuristic is sufficient for our coreference
resolution problem. In almost all articles, pro-
nouns refer to the main entity of the article. In
addition we include some additional rules to be
able to establish coreference of markables such as
der Fluss ?the river? or der Bach ?the creek? with
the main entity.
5.5 Supervised relation extraction
We use the ClearTK (Ogren et al, 2008) toolkit,
which is also an UIMA component, for the rela-
tion extraction task. It contains wrappers for dif-
ferent machine learning suites. Our initial exper-
iments showed that the MaximumEntropy clas-
sifier achieved the best results for our classifi-
cation task. The toolkit provides additional ex-
tensible feature methods. Because we view self-
annotation and fine-grained named entity recogni-
tion as our main contributions, not feature selec-
tion, we only give a brief overview of the features
we use.
F1 is a window based bag-of-words feature
(window size = 3). It considers lemma and part-
of-speech tag of the tokens. F2 is a phrase based
extractor that uses the parent phrase of both enti-
ties (max 2 levels). F3 is a representation of all
sie
sheSchlie?lichFinally
auf
on
Meter
meter
244
inin
Bieberehren
in
Tauber
die
the
1
2
3
4
1 3 1 2 3 41 2 2
TOP
m?ndenflow
SUBJADV
Figure 4: Dependency parser output of the FSPar
framework.
linguistic effort description
F1 pos-tagging window size 3, LEMMA
F2 chunk-parse parent chunks
F3 dependency-parse dependency paths betw. NEs
Table 1: List of feature types
possible dependency paths between the article?s
main entity and a target entity, where each path
is represented as a feature vector. In most cases,
more than one path is returned by the partial de-
pendency parser (which makes no disambiguation
decisions) and included in the feature representa-
tion. Figure 4 depicts the dependency parser out-
put of our sample sentence. Each pair of square
and circle with the same number corresponds to
one dependency. These different possible depen-
dency combinations give rise to 8 possible paths
between the relation entities Tauber and sie ?she?
although our example sentence is a very simple
sentence.
6 Evaluation
We evaluate the system in two experiments. The
first considers the relation between suburbs and
their parent towns. In the second experiment the
river-bodyOfWater relation is extracted. The ex-
periments are based on the previously described
extracted Wikipedia corpus. For each experiment
a new self-annotated corpus is created that is split
into three parts. The first part (60%) is used as
training corpus. The second part (20%) is used
as development corpus. The remaining 20% is
used for the final evaluation and was not inspected
while we were developing the extraction algo-
rithms.
85
6.1 Metric used
Our gold standard includes all relations of each
article. Our metric works on the level of type
and is independent of how often the same relation
occurs in the article. The metric counts a rela-
tion as true positive (TP) if the system extracted
it at least once. If the relation was not found by
the system a false negative (FN) is counted. A
false positive (FP) is given if the system extracts
a relation between two entities that is not part of
the (infobox-derived) gold standard for the article.
All three measures are used to calculate precision
(P = TPTP+FP ), recall (R = TPTP+FN ), and F1-
score (F1 = 2 P?RP+R ).
6.2 Town-suburb extraction
The town-suburb extractor uses one attribute of
the infobox to identify the town-suburb relation.
There is no schema drift in the infobox data and
the values contain only links. Therefore the self-
annotation works almost perfectly. The only ex-
ceptions are articles without an infobox which
cannot be used for training. However, this is not a
real issue because the amount of remaining data is
sufficient: 9000 articles can be used for this task.
The results in table 2 show that the classifier that
uses F1, F2 and F3 (that is, including the depen-
dency features) performs best.
engine features F1 recall precision
FSPar F1 64.9 79.0% 55.7%
FSPar F1, F2 89.6 90.2% 89.5%
FSPar F1, F2, F3 98.3 98.8% 97.8%
Table 2: Results of different feature combinations
on the test set for town-suburb relation
6.3 River-bodyOfWater extraction
For the extraction of the river-bodyOfWater re-
lation the infobox processing is more difficult.
We have to handle more attributes because there
is schema drift between the different users. It
is hence necessary to merge information coming
from different attribute values. The other diffi-
culty is the usage of mixed contents in the values.
Another main difference to the town-suburb rela-
tion is that the river-bodyOfWater relation is often
not mentioned in the first sentence (which usually
gives a short definition about the the main entity).
Thus, the self-annotation method has to deal with
the more complex sentences that are common later
in the article. This also contributes to a more chal-
lenging extraction task.
Our river-bodyOfWater relation corpus consists
of 3000 self-annotated articles.
Table 3 shows the performance of the extrac-
tor using two different linguistic components as
described in section 5.3. As in the case of town-
suburb extraction the classifier that uses all fea-
tures, including dependency features, performs
best.
engine features F1 recall precision
FSPar F1 51.8% 56.6% 47.8%
FSPar F1,F2 72.1% 68.9% 75.7%
FSPar F1,F2,F3 78.3% 74.1% 83.0%
OpenNLP F1 48.0% 62.8% 38.8%
OpenNLP F1,F2 73.3% 71.7% 74.7%
Table 3: Results of different feature combinations
on the test set for river-bodyOfWater extraction
6.4 Evaluation of self-annotation
To evaluate the quality of self-annotation, we ran-
domly selected one set of 100 self-annotated ar-
ticles from each data set and labeled these sets
manually. These annotations are used to calcu-
late the inter-annotator agreement between the hu-
man annotated and machine annotated instances.
We use Cohen?s ? as measure and get a result of
1.00 for the town-suburb relation. For the river-
bodyOfWater relation we got a ?-value of 0.79,
which also indicates good agreement.
We also use a gazetteer to evaluate the qual-
ity of all town-suburb relations that were extracted
for our self-annotated training set. The accuracy
is nearly perfect (only one single error), which is
good evidence for the high quality of Wikipedia.
Required size of self-annotated training set.
The performance of a supervised system depends
on the size of the training data. In the self-
annotation step a minimum of instances has to be
annotated, but it is not necessary to self-annotate
all available articles.
We reduced the number of articles used in
the training size to test this hypothesis. Reduc-
ing the entire training set of 9000 (respectively,
3000) self-annotated articles to 1000 reduces F1
86
by 2.0% for town-suburb and by 2.4% for river-
bodyOfWater; a reduction to 100 reduces F1 by
8.5% for town-suburb and by 9.3% for river-
bodyOfWater (compared to the 9000/3000 base-
line).
7 Discussion
Wu and Weld (2007) observed schema drift in
their work: Wikipedia authors do not not use in-
fobox attributes in a consistent manner. However,
we did not find schema drift to be a large prob-
lem in our experiments. The variation we found
can easily be handled with a small number of
rules. This can be due to the fact that the qual-
ity of Wikipedia articles improved a lot in the last
years through the introduction of automatic main-
tenance tools like bots2. Nevertheless, the devel-
opment of self-annotation for a new relation type
requires some manual work. The developer has to
check the quality of the extraction relations in the
infoboxes. This can lead to some additional adap-
tation work for the used attributes such as merging
or creating rules. However, a perfect coverage is
not required because the extraction system is only
used for training purposes; we only need to find
a sufficiently large number of positive training in-
stances and do not require exhaustive labeling of
all articles.
It is important to note that considering par-
tially found relations as negative samples has to
be avoided. Wrong negative samples have a gen-
erally unwanted impact on the performance of the
learned extraction model. A developer has to be
aware of this fact. In one experiment, the learned
classifiers were applied to the training data and
returned a number of false positive results ? 40
in case of the river-bodyOfWater relation. 31 of
these errors were not actual errors because the
self-annotation missed some true instances. Nev-
ertheless, the trained model recognizes these sam-
ples as correct; this could perhaps be used to fur-
ther improve the quality of self-annotation.
Manually labeled data also includes noise and
the benefit of self-annotation is substantial when
2See en.wikipedia.org/wiki/Wikipedia:Bots. The edit his-
tory of many articles shows that there is a lot of automatic
maintenance by bots to avoid schema drift.
the aim is to build a fine-grained relation extrac-
tion system in a fast and cheap way.
The difference of the results between OpenNLP
and FSPar engines are smaller than expected.
Although sentence splitting is poorly done by
OpenNLP the effect on the extraction result is
rather low. Another crucial point is that the
lexicon-based named entity recognizer of the FS-
Par engine that was optimized for named entities
used in Wikipedia has no significant impact on the
overall performance. Thus, a basic set of NLP
components with moderate error rates may be suf-
ficient for effective self-annotation.
8 Conclusion
This paper described a new approach to develop-
ing and implementing a complete system to ex-
tract fine-grained geospatial relations by using a
supervised machine learning approach without ex-
pensive manual labeling. Using self-annotation,
systems can be rapidly developed and adapted for
new relations without expensive manual annota-
tion. Only some manual work has to be done
to find the right attributes in the infoboxes. The
matching process between infoboxes and text is
not in all cases trivial and for some attributes ad-
ditional rules have to be modeled.
9 Acknowledgment
This project was funded by DFG as part of Nexus
(Collaborative Research Centre, SFB 627).
References
Auer, So?ren, Christian Bizer, Georgi Kobilarov, Jens
Lehmann, and Zachary Ives. 2007. Dbpedia: A
nucleus for a web of open data. In In 6th Intl Se-
mantic Web Conference, Busan, Korea, pages 11?
15. Springer.
Blessing, Andre, Stefan Klatt, Daniela Nicklas, Stef-
fen Volz, and Hinrich Schu?tze. 2006. Language-
derived information and context models. In Pro-
ceedings of 3rd IEEE PerCom Workshop on Context
Modeling and Reasoning (CoMoRea) (at 4th IEEE
International Conference on Pervasive Computing
and Communication (PerCom?06)).
Du?rr, Frank, Nicola Ho?nle, Daniela Nicklas, Christian
Becker, and Kurt Rothermel. 2004. Nexus?a plat-
form for context-aware applications. In Roth, Jo?rg,
87
editor, 1. Fachgespra?ch Ortsbezogene Anwendun-
gen und Dienste der GI-Fachgruppe KuVS, pages
15?18, Hagen, Juni. Informatik-Bericht der Fer-
nUniversita?t Hagen.
Hahn, Udo, Ekaterina Buyko, Rico Landefeld,
Matthias Mu?hlhausen, Michael Poprat, Katrin
Tomanek, and Joachim Wermter. 2008. An
overview of JCoRe, the JULIE lab UIMA compo-
nent repository. In Proceedings of the LREC?08
Workshop ?Towards Enhanced Interoperability for
Large HLT Systems: UIMA for NLP?, Marrakech,
Morocco, May.
Jiang, Jing. 2009. Multi-task transfer learning for
weakly-supervised relation extraction. In ACL-
IJCNLP ?09: Proceedings of the Joint Conference of
the 47th Annual Meeting of the ACL and the 4th In-
ternational Joint Conference on Natural Language
Processing of the AFNLP: Volume 2, pages 1012?
1020, Morristown, NJ, USA. Association for Com-
putational Linguistics.
Mu?ller, Christof, Torsten Zesch, Mark-Christoph
Mu?ller, Delphine Bernhard, Kateryna Ignatova,
Iryna Gurevych, and Max Mu?hlha?user. 2008. Flex-
ible uima components for information retrieval re-
search. In Proceedings of the LREC 2008 Work-
shop ?Towards Enhanced Interoperability for Large
HLT Systems: UIMA for NLP?, Marrakech, Mo-
rocco, May 31, 2008. 24?27.
Nothman, Joel, Tara Murphy, and James R. Curran.
2009. Analysing wikipedia and gold-standard cor-
pora for ner training. In EACL ?09: Proceedings
of the 12th Conference of the European Chapter
of the Association for Computational Linguistics,
pages 612?620, Morristown, NJ, USA. Association
for Computational Linguistics.
Ogren, Philip V., Philipp G. Wetzler, and Steven
Bethard. 2008. Cleartk: A uima toolkit for sta-
tistical natural language processing. In UIMA for
NLP workshop at Language Resources and Evalua-
tion Conference (LREC).
Schiehlen, Michael. 2003. Combining deep and shal-
low approaches in parsing german. In ACL ?03:
Proceedings of the 41st Annual Meeting on Asso-
ciation for Computational Linguistics, pages 112?
119, Morristown, NJ, USA. Association for Com-
putational Linguistics.
Schmid, Helmut. 1995. Improvements in part-of-
speech tagging with an application to german. In In
Proceedings of the ACL SIGDAT-Workshop, pages
47?50.
Suchanek, Fabian M., Gjergji Kasneci, and Gerhard
Weikum. 2007. Yago: A Core of Semantic Knowl-
edge. In 16th international World Wide Web con-
ference (WWW 2007), New York, NY, USA. ACM
Press.
Wu, Fei and Daniel S. Weld. 2007. Autonomously
semantifying wikipedia. In Proceedings of the Six-
teenth ACM Conference on Information and Knowl-
edge Management, CIKM 2007, Lisbon, Portugal,
November 6-10, 2007, pages 41?50.
Zesch, Torsten, Christof Mu?ller, and Iryna Gurevych.
2008. Extracting Lexical Semantic Knowledge
from Wikipedia and Wiktionary. In Proceedings of
the Conference on Language Resources and Evalu-
ation (LREC).
Zhang, Ziqi and Jose? Iria. 2009. A novel approach to
automatic gazetteer generation using wikipedia. In
People?s Web ?09: Proceedings of the 2009 Work-
shop on The People?s Web Meets NLP, pages 1?9,
Morristown, NJ, USA. Association for Computa-
tional Linguistics.
88
Coling 2010: Poster Volume, pages 614?622,
Beijing, August 2010
A Linguistically Grounded Graph Model for Bilingual Lexicon
Extraction
Florian Laws, Lukas Michelbacher, Beate Dorow, Christian Scheible,
Ulrich Heid, Hinrich Schu?tze
Institute for Natural Language Processing
Universita?t Stuttgart
{lawsfn,michells,dorowbe}@ims.uni-stuttgart.de
Abstract
We present a new method, based on
graph theory, for bilingual lexicon ex-
traction without relying on resources with
limited availability like parallel corpora.
The graphs we use represent linguis-
tic relations between words such as ad-
jectival modification. We experiment
with a number of ways of combining
different linguistic relations and present
a novel method, multi-edge extraction
(MEE), that is both modular and scalable.
We evaluate MEE on adjectives, verbs
and nouns and show that it is superior
to cooccurrence-based extraction (which
does not use linguistic analysis). Finally,
we publish a reproducible baseline to es-
tablish an evaluation benchmark for bilin-
gual lexicon extraction.
1 Introduction
Machine-readable translation dictionaries are an
important resource for bilingual tasks like ma-
chine translation and cross-language information
retrieval. A common approach to obtaining bilin-
gual translation dictionaries is bilingual lexicon
extraction from corpora. Most work has used
parallel text for this task. However, parallel cor-
pora are only available for few language pairs and
for a small selection of domains (e.g., politics).
For other language pairs and domains, monolin-
gual comparable corpora and monolingual lan-
guage processing tools may be more easily avail-
able. This has prompted researchers to investigate
bilingual lexicon extraction based on monolingual
corpora (see Section 2) .
In this paper, we present a new graph-theoretic
method for bilingual lexicon extraction. Two
monolingual graphs are constructed based on syn-
tactic analysis, with words as nodes and relations
(such as adjectival modification) as edges. Each
relation acts as a similarity source for the node
types involved. All available similarity sources
interact to produce one final similarity value for
each pair of nodes. Using a seed lexicon, nodes
from the two graphs can be compared to find a
translation.
Our main contributions in this paper are: (i) we
present a new method, based on graph theory,
for bilingual lexicon extraction without relying
on resources with limited availability like paral-
lel corpora; (ii) we show that with this graph-
theoretic framework, information obtained by lin-
guistic analysis is superior to cooccurrence data
obtained without linguistic analysis; (iii) we ex-
periment with a number of ways of combining dif-
ferent linguistic relations in extraction and present
a novel method, multi-edge extraction, which is
both modular and scalable; (iv) progress in bilin-
gual lexicon extraction has been hampered by the
lack of a common benchmark; we therefore pub-
lish a benchmark and the performance of MEE as
a baseline for future research.
The paper discusses related work in Section 2.
We then describe our translation model (Sec-
tion 3) and multi-edge extraction (Section 4). The
benchmark we publish as part of this paper is de-
scribed in Section 5. Section 6 presents our ex-
perimental results and Section 7 analyzes and dis-
cusses them. Section 8 summarizes.
2 Related Work
Rapp (1999) uses word cooccurrence in a vector
space model for bilingual lexicon extraction. De-
tails are given in Section 5.
Fung and Yee (1998) also use a vector space
approach, but use TF/IDF values in the vector
components and experiment with different vec-
tor similarity measures for ranking the translation
candidates. Koehn and Knight (2002) combine
614
a vector-space approach with other clues such as
orthographic similarity and frequency. They re-
port an accuracy of .39 on the 1000 most frequent
English-German noun translation pairs.
Garera et al (2009) use a vector space model
with dependency links as dimensions instead of
cooccurring words. They report outperforming
a cooccurrence vector model by 16 percentage
points accuracy on English-Spanish.
Haghighi et al (2008) use a probabilistic model
over word feature vectors containing cooccur-
rence and orthographic features. They then use
canonical correlation analysis to find matchings
between words in a common latent space. They
evaluate on multiple languages and report high
precision even without a seed lexicon.
Most previous work has used vector spaces and
(except for Garera et al (2009)) cooccurrence
data. Our approach uses linguistic relations like
subcategorization, modification and coordination
in a graph-based model. Further, we evaluate our
approach on different parts of speech, whereas
some previous work only evaluates on nouns.
3 Translation Model
Our model has two components: (i) a graph repre-
senting words and the relationships between them
and (ii) a measure of similarity between words
based on these relationships. Translation is re-
garded as cross-lingual word similarity. We rank
words according to their similarity and choose the
top word as the translation.
We employ undirected graphs with typed nodes
and edges. Node types represent parts of speech
(POS); edge types represent different kinds of re-
lations. We use a modified version of SimRank
(Jeh and Widom, 2002) as a similarity measure
for our experiments (see Section 4 for details).
SimRank is based on the idea that two nodes
are similar if their neighbors are similar. We ap-
ply this notion of similarity across two graphs. We
think of two words as translations if they appear
in the same relations with other words that are
translations of each other. Figure 1 illustrates this
idea with verbs and nouns in the direct object rela-
tion. Double lines indicate seed translations, i.e.,
known translations from a dictionary (see Sec-
tion 5). The nodes buy and kaufen have the same
house
magazine
book
thought
buy
read
Haus
Zeitschrift
Buch
Gedanke
kaufen
lesen
Figure 1: Similarity through seed translations
objects in the two languages; one of these (maga-
zine ? Zeitschrift) is a seed translation. This re-
lationship contributes to the similarity of buy ?
kaufen. Furthermore, book and Buch are similar
(because of read ? lesen) and this similarity will
be added to buy ? kaufen in a later iteration. By
repeatedly applying the algorithm, the initial sim-
ilarity introduced by seeds spreads to all nodes.
To incorporate more detailed linguistic infor-
mation, we introduce typed edges in addition to
typed nodes. Each edge type represents a linguis-
tic relation such as verb subcategorization or ad-
jectival modification. By designing a model that
combines multiple edge types, we can compute
the similarity between two words based on mul-
tiple sources of similarity. We superimpose dif-
ferent sets of edges on a fixed set of nodes; a node
is not necessarily part of every relation.
The graph model can accommodate any kind of
nodes and relations. In this paper we use nodes
to represent content words (i.e., non-function
words): adjectives (a), nouns (n) and verbs (v).
We extracted three types of syntactic relations
from a corpus: see Table 1.
Nouns participate in two bipartite relations
(amod, dobj) and one unipartite relation (ncrd).
This means that the computation of noun similar-
ities will benefit from three different sources.
Figure 2 depicts a sample graph with all node
and edge types. For the sake of simplicity, a
monolingual example is shown. There are four
nouns in the sample graph all of which are (i)
modified by the adjectives interesting and polit-
ical and (ii) direct objects of the verbs like and
615
relation entities description example
used in this paper
amod a, n adjectival modification a fast car
dobj v, n object subcategorization drive a car
ncrd n, n noun coordination cars and busses
other possible relations
vsub v, n subject subcategorization a man sleeps
poss n, n possessive the child?s toy
acrd a, a adjective coordination red or blue car
Table 1: Relations used in this paper (top) and
possible extensions (bottom).
dobj
amod
ncrd
verb
adjective
noun
like promote
idea
article book
magazine
interesting political
Figure 2: Graph snippet with typed edges
promote. Based on amod and dobj, the four nouns
are equally similar to each other. However, the
greater similarity of article, book, and magazine
to each other can be deduced from the fact that
these three nouns also occur in the relation ncrd.
We exploit this information in the MEE method.
Data and Preprocessing. Our corpus in this
paper is the Wikipedia. We parse all German
and English articles with BitPar (Schmid, 2004)
to extract verb-argument relations. We extract
adjective-noun modification and noun coordina-
tions with part-of-speech patterns based on a
version of the corpus tagged with TreeTagger
(Schmid, 1994). We use lemmas instead of sur-
face forms. Because we perform the SimRank
matrix multiplications in memory, we need to fil-
ter out rare words and relations; otherwise, run-
ning SimRank to convergence would not be feasi-
ble. For adjective-noun pairs, we apply a filter on
pair frequency (? 3). We process noun pairs by
applying a frequency threshold on words (? 100)
and pairs (? 3). Verb-object pairs (the smallest
data set) were not frequency-filtered. Based on
the resulting frequency counts, we calculate asso-
ciation scores for all relationships using the log-
likelihood measure (Dunning, 1993). For noun
pairs, we discard all pairs with an association
score < 3.84 (significance at ? = .05). For all
three relations, we discard pairs whose observed
frequency was smaller than their expected fre-
quency (Evert, 2004, p. 76). As a last step,
we further reduce noise by removing nodes of de-
gree 1. Key statistics for the resulting graphs are
given in Table 2.
We have found that accuracy of extraction is
poor if unweighted edges are used. Using the
log-likelihood score directly as edge weight gives
too much weight to ?semantically weak? high-
frequency words like put and take. We there-
fore use the logarithms of the log-likelihood score
as edge weights in all SimRank computations re-
ported in this paper.
nodes n a v
de 34,545 10,067 2,828
en 22,257 12,878 4,866
edges ncrd amod dobj
de 65,299 417,151 143,906
en 288,889 686,073 510,351
Table 2: Node and edge statistics
4 SimRank
Our work is based on the SimRank graph similar-
ity algorithm (Jeh and Widom, 2002). In (Dorow
et al, 2009), we proposed a formulation of Sim-
Rank in terms of matrix operations, which can be
applied to (i) weighted graphs and (ii) bilingual
problems. We now briefly review SimRank and
its bilingual extension. For more details we refer
to (Dorow et al, 2009).
The basic idea of SimRank is to consider two
nodes as similar if they have similar neighbor-
hoods. Node similarity scores are recursively
computed from the scores of neighboring nodes:
the similarity Sij of two nodes i and j is computed
616
as the normalized sum of the pairwise similarities
of their neighbors:
Sij =
c
|N(i)| |N(j)|
?
k?N(i),l?N(j)
Skl.
where N(i) and N(j) are the sets of i?s and j?s
neighbors. As the basis of the recursion, Sij is set
to 1 if i and j are identical (self-similarity). The
constant c (0 < c < 1) dampens the contribution
of nodes further away. Following Jeh and Widom
(2002), we use c = 0.8. This calculation is re-
peated until, after a few iterations, the similarity
values converge.
For bilingual problems, we adapt SimRank for
comparison of nodes across two graphs A and B.
In this case, i is a node in A and j is a node in B,
and the recursion basis is changed to S(i, j) = 1 if
i and j are a pair in a predefined set of node-node
equivalences (seed translation pairs).
Sij =
c
|NA(i)| |NB(j)|
?
k?NA(i),l?NB(j)
Skl.
Multi-edge Extraction (MEE) Algorithm To
combine different information sources, corre-
sponding to edges of different types, in one Sim-
Rank computation, we use multi-edge extrac-
tion (MEE), a variant of SimRank (Dorow et al,
2009). It computes an aggregate similarity matrix
after each iteration by taking the average similar-
ity value over all edge types T :
Sij =
c
|T |
?
t?T
1
f(|NA,t(i)|)f(|NB,t(j)|)
?
k?NA,t(i),
l?NB,t(j)
Skl.
f is a normalization function (either f = g,
g(n) = n as before or the normalization discussed
in the next section).
While we have only reviewed the case of un-
weighted graphs, the extended SimRank can also
be applied to weighted graphs. (See (Dorow et
al., 2009) for details.) In what follows, all graph
computations are weighted.
Square Root Normalization Preliminary ex-
periments showed that SimRank gave too much
influence to words with few neighbors. We there-
fore modified the normalization function g(n) =
n. To favor words with more neighbors, we want
f to grow sublinearly with the number of neigh-
bors. On the other hand, it is important that,
even for nodes with a large number of neigh-
bors, the normalization term is not much smaller
than |N(i)|, otherwise the similarity computation
does not converge. We use the function h(n) =?n?
?
maxk(|N(k)|). h grows quickly for small
node degrees, while returning values close to the
linear term for large node degrees. This guaran-
tees that nodes with small degrees have less influ-
ence on final similarity scores. In all experiments
reported in this paper, the matrices A?, B? are nor-
malized with f = h (rather than using the stan-
dard normalization f = g). In one experiment,
accuracy of the top-ranked candidate (acc@1) was
.52 for h and .03 for g, demonstrating that the
standard normalization does not work in our ap-
plication.
Threshold Sieving For larger experiments,
there is a limit to scalability, as the similarity ma-
trix fills up with many small entries, which take up
a large amount of memory. Since these small en-
tries contribute little to the final result, Lizorkin et
al. (2008) proposed threshold sieving: an approxi-
mation of SimRank using less space by deleting
all similarity values that are below a threshold.
The quality of the approximation is set by a pa-
rameter ? that specifies maximum acceptable dif-
ference of threshold-sieved similarity and the ex-
act solution. We adapted this to the matrix formu-
lation by integrating the thresholding step into a
standard sparse matrix multiplication algorithm.
We verified that this approximation yields use-
ful results by comparing the ranks of exact and ap-
proximate solutions. We found that for the high-
ranked words that are of interest in our task, siev-
ing with a suitable threshold does not negatively
affect results.
5 Benchmark Data Set
Rapp?s (1999) original experiment was carried out
on newswire corpora and a proprietary Collins
dictionary. We use the free German (280M to-
kens) and English (850M tokens) Wikipedias as
source and target corpora. Reinhard Rapp has
generously provided us with his 100 word test set
617
n a v
training set .61 .31 .08
TS100 .65 .28 .07
TS1000 .66 .14 .20
Table 3: Percentages of POS in test and training
(TS100) and given us permission to redistribute
it. Additionally, we constructed a larger test set
(TS1000) consisting of the 1000 most frequent
words from the English Wikipedia. Unlike the
noun-only test sets used in other studies, (e.g.,
Koehn and Knight (2002), Haghighi et al (2008)),
TS1000 also contains adjectives and verbs. As
seed translations, we use a subset of the dict.cc
online dictionary. For the creation of the sub-
set we took raw word frequencies from Wikipedia
as a basis. We extracted all verb, noun and ad-
jective translation pairs from the original dictio-
nary and kept the pairs whose components were
among the 5,000 most frequent nouns, the 3,500
most frequent adjectives and the 500 most fre-
quent verbs for each language. These numbers are
based on percentages of the different node types
in the graphs. The resulting dictionary contains
12,630 pairs: 7,767 noun, 3,913 adjective and 950
verb pairs. Table 3 shows the POS composition of
the training set and the two test sets. For experi-
ments evaluated on TS100 (resp. TS1000), the set
of 100 (resp. 1000) English words it contains and
all their German translations are removed from the
seed dictionary.
Baseline. Our baseline is a reimplementation
of the vector-space method of Rapp (1999). Each
word in the source corpus is represented as a word
vector, the dimensions of which are words of seed
translation pairs. The same is done for corpus
words in the target language, using the translated
seed words as dimensions. The value of each di-
mension is determined by association statistics of
word cooccurrence. For a test word, a vector is
constructed in the same way. The labels on the
dimensions are then translated, yielding an input
vector in the target language vector space. We
then find the closest corpus word vector in the tar-
get language vector space using the city block dis-
tance measure. This word is taken as the transla-
tion of the test word.
We went to great lengths to implement Rapp?s
method, but omit the details for reasons of space.
Using the Wikipedia/dict.cc-based data set, we
achieve 50% acc@1 when translating words from
English to German. While this is somewhat lower
than the performance reported by Rapp, we be-
lieve this is due to Wikipedia being more hetero-
geneous and less comparable than news corpora
from identical time periods used by Rapp.
Publication. In conjunction with this paper we
publish the benchmark for bilingual lexicon ex-
traction described. It consists of (i) two Wikipedia
dumps from October 2008 and the linguistic re-
lations extracted from them, (ii) scripts to recre-
ate the training and test sets from the dict.cc
data base, (iii) the TS100 and TS1000 test sets,
and (iv) performance numbers of Rapp?s system
and MEE. These can serve as baselines for fu-
ture work. Note that (ii)?(iv) can be used in-
dependently of (i) ? but in that case the effect
of the corpus on performance would not be con-
trolled. The data and scripts are available at
http://ifnlp.org/wiki/extern/WordGraph
6 Results
In addition to the vector space baseline experi-
ment described above, we conducted experiments
with the SimRank model. Because TS100 only
contains one translation per word, but words can
have more than one valid translation, we manu-
ally extended the test set with other translations,
which we verified using dict.cc and leo.org. We
report the results separately for the original test set
(?strict?) and the extended test set in Table 4. We
also experimented with single-edge models con-
sisting of three separate runs on each relation.
The accuracy columns report the percentage of
test cases where the correct translation was found
among the top 1 (acc@1) or top 10 (acc@10)
candidate words found by the translation mod-
els. Some test words are not present in the data at
all; we count these as 0s when computing acc@1
and acc@10. The acc@10 measure is more use-
ful for indicating topical similarity while acc@1
measures translation accuracy.
MRR is Mean Reciprocal Rank of correct trans-
lations: 1n
?n
i
1
ranki (Voorhees and Tice, 1999).
MRR is a more fine-grained measure than acc@n,
618
TS100, strict TS100, extended TS1000
acc@1 acc@10 MRR acc@1 acc@10 MRR acc@1 acc@10 MRR
baseline .50 .67 .56 .54 .70 .60 .33 .56 .41
single .44 .67 .52 .49 .68 .56 .40? .70? .50
MEE .52 .79? .62 .58 .82? .68 .48? .76? .58
Table 4: Results compared to baseline?
e.g., it will distinguish ranks 2 and 10. All MRR
numbers reported in this paper are consistent with
acc@1/acc@10 and support our conclusions.
The results for acc@1, the measure that most
directly corresponds to utility in lexicon extrac-
tion, show that the SimRank-based models out-
perform the vector space baseline ? only slightly
on TS100, but significantly on TS1000. Using the
various relations separately (single) already yields
a significant improvement compared to the base-
line. Using all relations in the integrated MEE
model further improves accuracy. With an acc@1
score of 0.48, MEE outperforms the baseline by
.15 compared to TS1000. This shows that a com-
bination of several sources of information is very
valuable for finding the correct translation.
MEE outperforms the baseline on TS1000 for
all parts of speech, but performs especially well
compared to the baseline for adjectives and verbs
(see Table 5). It has been suggested that vector
space models perform best for nouns and poorly
for other parts of speech. Our experiments seem to
confirm this. In contrast, MEE exhibits good per-
formance for nouns and adjectives and a marked
improvement for verbs.
On acc@10, MEE is consistently better than the
baseline, on both TS100 and TS1000. All three
differences are statistically significant.
6.1 Relation Comparison
Table 5 compares baseline, single-edge and MEE
accuracy for the three parts of speech covered.
Each single-edge experiment can compute noun
similarity; for adjectives and verbs, only amod,
dobj and MEE can be used.
Performance for nouns varies greatly depend-
ing on the relation used in the model. ncrd per-
?We indicate statistical significance at the ? = 0.05 (?)
and 0.01 level (?) when compared to the baseline. We did
not calculate significance for MRR.
forms best, while dobj shows the worst perfor-
mance. We hypothesize that dobj performs badly
because (i) many verbs are semantically non-
restrictive with respect to their arguments, (e.g.,
use, contain or include) and as a result seman-
tically unrelated nouns become similar because
they share the same verb as a neighbor; (ii) light
verb constructions (e.g., take a walk or give an ac-
count) dilute the extracted relations; and (iii) dobj
is the only relation we extracted with a syntac-
tic parser. The parser was trained on newswire
text, a genre that is very different from Wikipedia.
Hence, parsing is less robust than the relatively
straightforward POS patterns used for the other
relations.
Similarly, many semantically non-restrictive
adjectives such as first and new can modify vir-
tually any noun, diluting the quality of the amod
source. We conjecture that ncrd exhibits the best
performance because there are fewer semantically
non-restrictive nouns than non-restrictive adjec-
tives and verbs.
MEE performance for nouns (.45) is signifi-
cantly better than that of the single-edge models.
The information about nouns that is contained in
the verb-object and adjective-noun data is inte-
grated in the model and helps select better trans-
lations. This, however, is only true for the noun
noun adj verb all
TS100 baseline .55 .43 .29 .50
amod .15 .71 - .30
ncrd .34 - - .22
dobj .02 - .43 .04
MEE .45 .71 .43 .52
TS1000 baseline .42 .26 .18 .33
MEE .53 .55 .27 .48
Table 5: Relation comparison, acc@1
619
source acc@1 acc@10
dobj .02 .10
amod .15 .37
amod+dobj .22 .43
ncrd+dobj .32 .65
ncrd .34 .60
ncrd+amod .49 .74
MEE .45 .77
Table 6: Accuracy of sources for nouns
node type, the ?pivot? node type that takes part in
edges of all three types. For adjectives and verbs,
the performance of MEE is the same as that of the
corresponding single-edge model.
We ran three additional experiments each of
which combines only two of the three possible
sources for noun similarity, namely ncrd+amod,
ncrd+dobj and amod+dobj and performed strict
evaluation (see Table 6). We found that in gen-
eral combination increases performance except
for ncrd+dobj vs. ncrd. We attribute this to the
lack of robustness of dobj mentioned above.
6.2 Comparison MEE vs. All-in-one
An alternative to MEE is to use untyped edges in
one large graph. In this all-in-one model (AIO),
we connect two nodes with an edge if they are
linked by any of the different linguistic relations.
While MEE consists of small adjacency matrices
for each type, the two adjacency matrices for AIO
are much larger. This leads to a much denser sim-
ilarity matrix taking up considerably more mem-
ory. One reason for this is that AIO contains simi-
larity entries between words of different parts of
speech that are 0 (and require no memory in a
sparse matrix representation) in MEE.
Since AIO requires more memory, we had to
filter the data much more strictly than before to be
able to run an experiment. We applied the follow-
ing stricter thresholds on relationships to obtain
a small graph: 5 instead of 3 for adjective-noun
MEEsmall AIOsmall
acc@1 .51 .52
acc@10 .72 .75
MRR .62 .59
Table 7: MEE vs. AIO
pairs, and 3 instead of 0 for verb-object pairs,
thereby reducing the total number of edges from
2.1M to 1.4M. We also applied threshold sieving
(see Section 4) with ? = 10?10 for AIO. The re-
sults on TS100 (strict evaluation) are reported in
Table 7. For comparison, MEE was also run on
the smaller graph. Performance of the two models
is very similar, with AIO being slightly better (not
significant). The slight improvement does not jus-
tify the increased memory requirements. MEE is
able to scale to more nodes and edge types, which
allows for better coverage and performance.
7 Analysis and Discussion
Error analysis. We examined the cases where a
reference translation was not at the top of the sug-
gested list of translation candidates. There are a
number of elements in the translation process that
can cause or contribute to this behavior.
Our method sometimes picks a cohyponym of
the correct translation. In many of these cases, the
correct translation is in the top 10 (together with
other words from the same semantic field). For
example, the correct translation of moon, Mond, is
second in a list of words belonging to the semantic
field of celestial phenomena: Komet (comet), Mond
(moon), Planet (planet), Asteroid (asteroid), Stern (star),
Galaxis (galaxy), Sonne (sun), . . . While this behavior
is undesirable for strict lexicon extraction, it can
be exploited for other tasks, e.g. cross-lingual se-
mantic relatedness (Michelbacher et al, 2010).
Similarly, the method sometimes puts the
antonym of the correct translation in first place.
For example, the translation for swift (schnell) is
in second place behind langsam (slow). Based
on the syntactic relations we use, it is difficult to
discriminate between antonyms and semantically
similar words if their syntactic distributions are
similar.
Ambiguous source words also pose a problem
for the system. The correct translation of square
(the geometric shape) is Quadrat. However, 8 out
of its top 10 translation candidates are related to
the location sense of square. The other two are ge-
ometric shapes, Quadrat being listed second. This
is only a concern for strict evaluation, since cor-
rect translations of a different sense were included
in the extended test set.
620
bed is also ambiguous (piece of furniture vs.
river bed). This introduces translation candidates
from the geographical domain. As an additional
source of errors, a number of bed?s neighbors
from the furniture sense have the German transla-
tion Bank which is ambiguous between the furni-
ture sense and the financial sense. This ambiguity
in the target language German introduces spurious
translation candidates from the financial domain.
Discussion. The error analysis demonstrates
that most of the erroneous translations are words
that are incorrect, but that are related, in some ob-
vious way, to the correct translation, e.g. by co-
hyponymy or antonymy. This suggests another
application for bilingual lexicon extraction. One
of the main challenges facing statistical machine
translation (SMT) today is that it is difficult to
distinguish between minor errors (e.g., incorrect
word order) and major errors that are completely
implausible and undermine the users? confidence
in the machine translation system. For example,
at some point Google translated ?sarkozy sarkozy
sarkozy? into ?Blair defends Bush?. Since bilin-
gual lexicon extraction, when it makes mistakes,
extracts closely related words that a human user
can understand, automatically extracted lexicons
could be used to discriminate smaller errors from
grave errors in SMT.
As we discussed earlier, parallel text is not
available in sufficient quantity or for all impor-
tant genres for many language pairs. The method
we have described here can be used in such cases,
provided that large monolingual corpora and ba-
sic linguistic processing tools (e.g. POS tagging)
are available. The availability of parsers is a more
stringent constraint, but our results suggest that
more basic NLP methods may be sufficient for
bilingual lexicon extraction.
In this work, we have used a set of seed trans-
lations (unlike e.g., Haghighi et al (2008)). We
believe that in most real-world scenarios, when
accuracy and reliability are important, seed lexica
will be available. In fact, seed translations can be
easily found for many language pairs on the web.
Although a purely unsupervised approach is per-
haps more interesting from an algorithmic point
of view, the semisupervised approach taken in this
paper may be more realistic for applications.
In this paper, we have attempted to reimplement
Rapp?s system as a baseline, but have otherwise
refrained from detailed comparison with previous
work as far as the accuracy of results is concerned.
The reason is that none of the results published so
far are easily reproducible. While previous publi-
cations have tried to infer from differences in per-
formance numbers that one system is better than
another, these comparisons have to be viewed with
caution since neither the corpora nor the gold stan-
dard translations are the same. For example, the
paper by Haghighi et al (2008) (which demon-
strates how orthography and contextual informa-
tion can be successfully used) reports 61.7% ac-
curacy on the 186 most confident predictions of
nouns. But since the evaluation data sets are not
publicly available it is difficult to compare other
work (including our own) with this baseline. We
simply do not know how methods published so far
stack up against each other.
For this reason, we believe that a benchmark
is necessary to make progress in the area of bilin-
gual lexicon extraction; and that our publication of
such a benchmark as part of the research reported
here is an important contribution, in addition to
the linguistically grounded extraction and the new
graph-theoretical method we present.
8 Summary
We have presented a new method, based on graph
theory, for bilingual lexicon extraction without re-
lying on resources with limited availability like
parallel corpora. We have shown that with this
graph-theoretic framework, information obtained
by linguistic analysis is superior to cooccurrence
data obtained without linguistic analysis. We have
presented multi-edge extraction (MEE), a scalable
graph algorithm that combines different linguis-
tic relations in a modular way. Finally, progress
in bilingual lexicon extraction has been hampered
by the lack of a common benchmark. We publish
such a benchmark with this paper and the perfor-
mance of MEE as a baseline for future research.
9 Acknowledgement
This research was funded by the German Re-
search Foundation (DFG) within the project A
graph-theoretic approach to lexicon acquisition.
621
References
Dorow, Beate, Florian Laws, Lukas Michelbacher,
Christian Scheible, and Jason Utt. 2009. A graph-
theoretic algorithm for automatic extension of trans-
lation lexicons. In EACL 2009 Workshop on Geo-
metrical Models of Natural Language Semantics.
Dunning, Ted. 1993. Accurate methods for the statis-
tics of surprise and coincidence. Computational
Linguistics, 19(1):61?74.
Evert, Stefan. 2004. The Statistics of Word Cooccur-
rences - Word Pairs and Collocations. Ph.D. thesis,
Institut fu?r maschinelle Sprachverarbeitung (IMS),
Universita?t Stuttgart.
Fung, Pascale and Lo Yuen Yee. 1998. An IR ap-
proach for translating new words from nonparallel,
comparable texts. In COLING-ACL, pages 414?
420.
Garera, Nikesh, Chris Callison-Burch, and David
Yarowsky. 2009. Improving translation lexicon
induction from monolingual corpora via depen-
dency contexts and part-of-speech equivalences. In
CoNLL ?09: Proceedings of the Thirteenth Confer-
ence on Computational Natural Language Learn-
ing, pages 129?137, Morristown, NJ, USA. Asso-
ciation for Computational Linguistics.
Haghighi, Aria, Percy Liang, Taylor Berg-Kirkpatrick,
and Dan Klein. 2008. Learning bilingual lexicons
from monolingual corpora. In Proceedings of ACL-
08: HLT, pages 771?779, Columbus, Ohio, June.
Association for Computational Linguistics.
Jeh, Glen and Jennifer Widom. 2002. Simrank: A
measure of structural-context similarity. In KDD
?02, pages 538?543.
Koehn, Philipp and Kevin Knight. 2002. Learning a
translation lexicon from monolingual corpora. In
Proceedings of the ACL-02 Workshop on Unsuper-
vised Lexical Acquisition, pages 9?16.
Lizorkin, Dmitry, Pavel Velikhov, Maxim N. Grinev,
and Denis Turdakov. 2008. Accuracy estimate and
optimization techniques for simrank computation.
PVLDB, 1(1):422?433.
Michelbacher, Lukas, Florian Laws, Beate Dorow, Ul-
rich Heid, and Hinrich Schu?tze. 2010. Building
a cross-lingual relatedness thesaurus using a graph
similarity measure. In Proceedings of the Seventh
conference on International Language Resources
and Evaluation (LREC?10), Valletta, Malta, may.
Rapp, Reinhard. 1999. Automatic identification of
word translations from unrelated English and Ger-
man corpora. In COLING 1999.
Schmid, Helmut. 1994. Probabilistic part-of-speech
tagging using decision trees. In Proceedings of the
International Conference on New Methods in Lan-
guage Processing, pages 44?49.
Schmid, Helmut. 2004. Efficient parsing of highly
ambiguous context-free grammars with bit vectors.
In COLING ?04, page 162.
Voorhees, Ellen M. and Dawn M. Tice. 1999. The
TREC-8 question answering track evaluation. In
Proceedings of the 8th Text Retrieval Conference.
622
Coling 2010: Poster Volume, pages 1104?1112,
Beijing, August 2010
Sentiment Translation through Multi-Edge Graphs
Christian Scheible, Florian Laws, Lukas Michelbacher, and Hinrich Schu?tze
Institute for Natural Language Processing
University of Stuttgart
{scheibcn, lawsfn, michells}@ims.uni-stuttgart.de
Abstract
Sentiment analysis systems can benefit
from the translation of sentiment informa-
tion. We present a novel, graph-based ap-
proach using SimRank, a well-established
graph-theoretic algorithm, to transfer sen-
timent information from a source lan-
guage to a target language. We evaluate
this method in comparison with semantic
orientation using pointwise mutual infor-
mation (SO-PMI), an established unsuper-
vised method for learning the sentiment of
phrases.
1 Introduction
Sentiment analysis is an important topic in com-
putational linguistics that is of theoretical interest
but is also useful in many practical applications.
Usually, two aspects are of importance in senti-
ment analysis. The first is the detection of sub-
jectivity, i.e., whether a text or an expression is
meant to express sentiment at all; the second is the
determination of sentiment orientation, i.e., what
sentiment is to be expressed in a structure that is
considered subjective.
Work on sentiment analysis most often cov-
ers resources or analysis methods in a single lan-
guage, usually English. However, the transfer
of sentiment analysis between languages can be
advantageous by making use of resources for a
source language to improve the analysis of the tar-
get language.
This paper presents an approach to the transfer
of sentiment information between two languages
that does not rely on resources with limited avail-
ability like parallel corpora. It is built around Sim-
Rank, a graph similarity algorithm that has suc-
cessfully been applied to the acquisition of bilin-
gual lexicons (Laws et al, 2010) and semantic
similarity (Michelbacher et al, 2010). It uses
linguistic relations extracted from two monolin-
gual corpora to determine the similarity of words
in different languages. One of the main benefits
of our method is its ability to handle sparse data
about the relations between the languages well
(i.e., a small seed lexicon). Further, we experi-
ment with combining multiple types of linguistic
relations for graph-based translation. Our exper-
iments are carried out using English as a source
language and German as a target language. We
evaluate our method using a hand-annotated set of
German adjectives which we intend to publish.
In the following section, related work is dis-
cussed. Section 3.1 gives an introduction to Sim-
Rank and its application to lexicon induction,
while section 3.2 reviews SO-PMI (Turney, 2002),
an unsupervised baseline method for the genera-
tion of sentiment lexicons. In section 4, we define
our sentiment transfer method which we apply in
experiments in section 5.
2 Related Work
Mihalcea et al (2007) propose two methods for
translating sentiment lexicons. The first method
simply uses bilingual dictionaries to translate an
English sentiment lexicon. A sentence-based clas-
sifier built with this list achieved high precision,
but low recall on a small Romanian test set. The
second method is based on parallel corpora. The
source language in the corpus is annotated with
sentiment information, and the information is then
projected to the target language. Problems arise
due to mistranslations.
Banea et al (2008) use machine translation for
multilingual sentiment analysis. Given a corpus
annotated with sentiment information in one lan-
guage, machine translation is used to produce an
annotated corpus in the target language, by pre-
serving the annotations. The original annotations
1104
can be produced either manually or automatically.
Wan (2009) constructs a multilingual classi-
fier using co-training. In co-training, one classi-
fier produces additional training data for a second
classifier. In this case, an English classifier assists
in training a Chinese classifier.
The induction of a sentiment lexicon is the sub-
ject of early work by Hatzivassiloglou and McK-
eown (1997). They construct graphs from coordi-
nation data from large corpora based on the intu-
ition that adjectives with the same sentiment ori-
entation are likely to be coordinated. For example,
fresh and delicious is more likely than rotten and
delicious. They then apply a graph clustering al-
gorithm to find groups of adjectives with the same
orientation. Finally, they assign the same label to
all adjectives that belong to the same cluster.
Corpus work and bilingual dictionaries are
promising resources for translating sentiment. In
contrast to previous approaches, the work pre-
sented in this paper uses corpora that are not an-
notated with sentiment.
Turney (2002) suggests a corpus-based extrac-
tion method based on his pointwise mutual infor-
mation (PMI) synonymy measure. He assumes
that the sentiment orientation of a phrase can be
determined by comparing its pointwise mutual in-
formation with a positive (excellent) and a nega-
tive phrase (poor). An introduction to this method
is given in Section 3.2.
3 Background
3.1 Lexicon Induction via SimRank
We use the extension of the SimRank (Jeh and
Widom, 2002) node similarity algorithm proposed
by Dorow et al (2009). Given two graphs A and
B, the similarity between two nodes a in A and b
in B is computed in each iteration as:
S(a, b) = c
|NA(a)||NB(b)|
?
k?NA(a),l?NB(b)
S(k, l).
NX(x) is the neighborhood of node x in graph
X . To compute similarities between two graphs,
some initial links between these graphs have to be
given, called seed links. These form the recursion
basis which sets S(a, b) = 1 if there is a seed
link between a and b. At the beginning of each
iteration, all known equivalences between nodes
are reset to 1.
Multi-Edge Extraction (MEE). MEE is an ex-
tension of SimRank that, in each iteration, com-
putes the average node-node similarity of several
different SimRank matrices. In our case, we use
two different SimRank matrices, one for coordi-
nations and one for adjective modification. See
(Dorow et al, 2009) for details. We also used
the node degree normalization function h(n) =?n ??maxk(|N(k)|) (where n is the node de-
gree, and N(k) the degree of node k) to decrease
the harmful effect of high-degree nodes on final
similarity values. See (Laws et al, 2010) for de-
tails.
3.2 SO-PMI
Semantic orientation using pointwise mutual in-
formation (SO-PMI) (Turney, 2002) is an algo-
rithm for the unsupervised learning of semantic
orientation of words or phrases. A word has pos-
itive (resp. negative) orientation if it is associ-
ated with positive (resp. negative) terms more
frequently than with negative (resp. positive)
terms. Association of terms is measured using
their pointwise mutual information (PMI) which
is defined for two words w1 and w2 as follows:
PMI(w1, w2) = log
( p(w1, w2)
p(w1)p(w2)
)
Using PMI, Turney defines SO-PMI for a word
w as
SO-PMI(w) =
log
?
p?P hits(word NEAR p)?
?
n?N hits(n)?
n?N hits(word NEAR n)?
?
p?P hits(p)
hits is a function that returns the number of hits
in a search engine given the query. P is a set of
known positive words, N a set of known negative
words, and NEAR an operator of a search engine
that returns documents in which the operands oc-
cur within a close range of each other.
1105
4 Sentiment Translation
Unsupervised methods like SO-PMI are suitable
to acquire basic sentiment information in a lan-
guage. However, since hand-annotated resources
for sentiment analysis exist in other languages,
it seems plausible to use automatic translation of
sentiment information to leverage these resources.
In order to translate sentiment, we will use multi-
ple sources of information that we represent in a
MEE graph as given in Section 3.1.
In our first experiments (Scheible, 2010), coor-
dinated adjectives were used as the sole training
source. Two adjectives are coordinated if they are
linked with a conjunction like and or but. The
intuition behind using coordinations ? based on
work by Hatzivassiloglou and McKeown (1997)
and Widdows and Dorow (2002) ? was that words
which are coordinated share properties. In partic-
ular, coordinated adjectives usually express sim-
ilar sentiments even though there are exceptions
(e.g., ?The movie was both good and bad?).
In this paper, we focus on using multiple edge
types for sentiment translation. In particular, the
graph we will use contains two types of relations,
coordinations and adjective-noun modification. In
the sentence ?The movie was enjoyable and fun?,
enjoyable and fun are coordinated. In This is an
enjoyable movie, the adjective enjoyable modifies
the noun movie.
We selected these two relation types for two
reasons. First, the two types provide clues for
sentiment analysis. Coordination information is
an established source for sentiment similarity (e.g.
Hatzivassiloglou and McKeown (1997)) while
adjective-noun relations provide a different type
of information on sentiment. For example, nouns
with positive associations (vacation) tend to occur
with positive adjectives and nouns with negative
associations (pain) tend to occur with negative ad-
jectives. Second, we have successfully used these
two types for a similar acquisition task, the acqui-
sition of word-to-word translation pairs (Laws et
al., 2010).
In the resulting graph, adjectives and nouns are
represented as nodes, each containing a word and
its part of speech, and relations are represented as
links which are distinguished by their edge types.
Two graphs, one in the source language and one in
the target language, are needed to translate words
between those languages. Figure 1 shows an ex-
ample for such a setup. Black links in this graph
are coordinations, grey links are seed relations.
In order to calculate sentiment for all nodes in
the target language, we apply the SimRank algo-
rithm to the graphs which gives us similarities be-
tween all nodes in the source graph and all nodes
in the target graph. Using the similarity S(ns, nt)
between a node ns in the source language graph
S and a node nt in the target language graph T ,
the sentiment score (sent(nt)) is the similarity-
weighted average of all sentiment scores in the
target language:
sent(nt) =
?
ns?S
simnorm(ns, nt) sent(ns)
We assume that sentiment scores in the source
language are expressed on a numeric scale. The
normalized similarity simnorm is defined as
simnorm(ns, nt) = S(ns, nt)?
ns?S S(ns, nt)
.
The normalization assures that all resulting sen-
timent values are within [?1, 1], with ?1 being
the most negative sentiment and 1 the most posi-
tive.
5 Experiments
5.1 Data Acquisition
For our experiments, we needed coordination data
to build weighted graphs and a bilingual lexi-
con to define seed relations between those graphs.
Coordinations were extracted from the English
and German versions of Wikipedia1 by applying
pattern-based search using the Corpus Query Pro-
cessor (CQP) (Christ et al, 1999). We annotated
both corpora with parts of speech using the Tree
Tagger (Schmid, 1994). A total of 477,291 En-
glish coordinations and 112,738 German coordi-
nations were collected. A sample of this data is
given in Figure 2. We restrict these experiments
to the use of and/und since other coordinations
1http://www.wikipedia.org/ (01/19/2009)
1106
affordable
delicious
nutritiousjuicy
tasty
healthylovely
schmackhaft
gesundstrange
frisch
wertvoll
nahrhaft angesehen
ertragreich
Figure 1: A German and an English graph with coordinated adjectives including seed links
affordable
delicious
diverse
popularnutritious
inexpensive
original
varied
melodious
rare
strange
juicy
tasty
exotic healthy
tempting
lovely
hearty fragrant
dangerous
beautiful
charming authentic
Figure 2: English sample coordinations (adjectives)
1107
behave differently and might even express dissim-
ilarity (e.g. Was the weather good or bad?).
The seed lexicon was constructed from the
dict.cc dictionary2. While the complete dictionary
contains 30,551 adjective pairs, we reduced the
number of pairs used in the experiments to 1,576.
To produce a smaller seed lexicon which still
makes sense from a semantic point of view, we
used the General Service List (GSL) (West, 1953)
which contains about 2000 words the author con-
sidered central to the English language. More
specifically, a revised list was used3.
SO-PMI needs a larger amount of training data.
Since Wikipedia does not satisfy this need, we
collected additional coordination data from the
web using search result counts from Google. In
Turney?s original paper, he uses the NEAR oper-
ator, which returns documents that contain two
search terms that are within a certain distance of
each other, to collect collocations. Unfortunately,
Google does not support this operator, so instead,
we searched for coordinations using the queries
+ "w and s" and
+ "w und s"
for English and German, respectively. We added
the quotes and the + operator to make sure that
both spelling correction and synonym replace-
ments were disabled.
The original experiments were made for En-
glish, so we had to construct our own set of
seed words. For German, we chose gut (good),
nett (nice), richtig (right), scho?n (beautiful), or-
dentlich (neat), angenehm (pleasant), aufrichtig
(honest), gewissenhaft (faithful), and hervorra-
gend (excellent) as positive seed words, and
schlecht (bad), teuer (expensive), falsch (wrong),
bo?se (evil), feindlich (hostile), verhasst (invidi-
ous), widerlich (disgusting), fehlerhaft (faulty),
and mangelhaft (flawed) as negative ones.
5.2 Sentiment Lexicon
For our experiments, we used two different polar-
ity lexicons. The lexicon of Wilson et al (2005)
contains sentiment annotations for 8,221 words
2http://www.dict.cc
3http://jbauman.com/aboutgsl.html
annotation value
positive 1.0
weakpos 0.5
neutral 0.0
weakneg ?0.5
negative ?1.0
Table 1: Assigned values for Wilson et al set
which are tagged as positive, neutral, or nega-
tive. A few words are tagged as weakneg, imply-
ing weak negativity. These categorial annotations
are mapped to the range [-1,1] using the assign-
ment scheme given in Table 1.
5.3 Human Ratings
In order to manually annotate a test set, we
chose 200 German adjectives that occurred in the
Wikipedia corpus and that were part of a coor-
dination. From these words, we removed those
which we deemed uncommon, too complicated,
or which were mislabeled as adjectives by the tag-
ger. The test set contained 150 adjectives of which
seven were excluded after annotators discarded
them.
We asked 9 native speakers of German to anno-
tate the adjectives. Possible annotations were very
positive, slightly positive, neutral, slightly nega-
tive, or very negative. These categories are the
same as the ones used in the training data.
In order to capture the general sentiment, i.e.,
sentiment that is not related to a specific context,
the judges were asked to stay objective and not
let their personal opinions influence the annota-
tion. However, some words with strong political
implications were annotated by some judges as
non-neutral which led to disagreement beyond the
usual level. Nuklear (nuclear) is an example for
such a word. We measured the agreement of the
judges with Kendall?s coefficient of concordance
(W ) with tie correction (Legendre, 2005), yield-
ing W = 0.674 with a high level of significance
(p < .001); thus, inter-annotator agreement was
high (Landis and Koch, 1977).
5.4 Experimental Setup
Given the relations extracted from Wikipedia, we
built a German and an English graph by setting
1108
Method r
MEE 0.63
MEE-GSL 0.47
SR 0.63
SR-GSL 0.48
SO-PMI 0.58
Table 2: Correlation with human ratings
the weight of each link to the log-likelihood ra-
tio of the two words it connects according to the
corpus frequencies. There are two properties of
the graph transfer algorithm that we intend to in-
vestigate. First, we are interested in the merits of
applying multi edge extraction (MEE) for senti-
ment transfer. Second, we are interested in how
the transfer quality changes when the seed lexi-
con is reduced in size. This way, a sparse data
situation is simulated where large dictionaries are
unavailable. Having these two properties in mind,
four possible setups are evaluated: (i) using the
full seed lexicon with all 30,551 entries, but using
only coordination data (SR), (ii) reducing the seed
lexicon to 1,576 entries from the General Service
List (SR-GSL), (iii) applying MEE by adding ad-
jective modification data (MEE), and (iv) using
MEE with a reduced seed lexicon (MEE-GSL).
SimRank was run for 6 iterations in all experi-
ments. All experiments use the weight function
h as described above. We show that this function
improves similarities and thus lexicon induction
in Laws et al (2010).
Correlation. First, we will examine the correla-
tion between the automatic methods (SO-PMI and
the aforementioned SimRank variations) and the
gold standard as done by Turney in his evaluation.
For this purpose, the human ratings are mapped
to float values following Table 1 and the aver-
age rating over all judges for each word is used.
The correlation coefficients r are given in Table 2.
Judging from these results, the ordering of SR and
MEE matches the human ratings better than SO-
PMI, however it decreases when using any of the
GSL variations instead which can be attributed to
using less data.
Classification. The correct identification of the
classes positive, neutral, and negative is more im-
portant than the correct assignment of values on
a scale since the rank ordering is debatable ? this
becomes apparent when measuring the agreement
of human annotators. Since the assignments made
by the human judges are not unanimous in most
cases, the averages are distributed across the in-
terval [-1,1]; this means that the borders between
the three distinct categories are not clear. Since
there is no standard evaluation for this particu-
lar problem, we need to devise a way to make
the range of the neutral category dynamic. In or-
der to find possible borders, we first assume that
sentiment is distributed symmetrically around 0.
We then define a threshold x which assumes the
values x ? { i20 |0 ? i ? 20}, covering the in-terval [0,0.5]. Since 0.5 is slightly positive, we
do not believe that values above it are plausible.
Then, each word w is positive if its human rating
scoreh(w) ? x, negative if scoreh(w) ? ?x, and
neutral if ?x < scoreh(w) < x. The result of
this process is a gold standard for the three cate-
gories for each of the values for x. The percentiles
of the sizes of those categories are mapped to the
values produced by the automatic methods. For
example, if x = 0.35 means that the top 21% of
all adjectives are in the positive class, the top 21%
of all adjectives as assigned by SO-PMI and the
SimRank varieties are positive as well.
The size of the neutral category increases the
larger x becomes. Thus, high values for x are
unlikely to produce a correct partitioning of the
data. Since slightly positive was defined as 0.5,
we expect the highest plausible value for x to be
below that. The size of the neutral category for
each value of x is given in Table 3. (Recall that
the total size of the set is 143.)
We can then compute the assignment accu-
racy on the positive, neutral, and negative classes,
as well macro- and micro-averages over these
classes.
5.5 Results and Discussion
Figures 3 and 4 show the macro- and micro-
averaged accuracies over the positive, negative,
and neutral class for each automatic method, re-
spectively. Overall, the SimRank variations per-
form better for x in the interval [0, 0.3]. In partic-
ular, MEE has a slightly higher accuracy than SR,
1109
x 0.00 0.05 0.10 0.15 0.20 0.25 0.30 0.35 0.40 0.45 0.50
# neutral 0 13 35 46 56 64 74 82 92 99 99
Table 3: Size of neutral category given x
word (translation) humans SO MEE MEE-GSL SR SR-GSL
chemisch (chemical) 0.00 -20.20 0.185 0.185 0.186 0.184
auferstanden (resurrected) 0.39 -10.96 -0.075 -0.577 -0.057 -0.493
intelligent (intelligent) 0.94 46.59 0.915 0.939 0.834 0.876
versiert (skilled) 0.67 -5.26 0.953 0.447 0.902 0.404
mean -0.04 -9.58 0.003 0.146 0.010 0.142
median 0.00 -15.60 0.110 0.157 0.114 0.157
Table 4: Example adjectives including translation, and their scores
 0
 0.2
 0.4
 0.6
 0.8
 1
 0  0.1  0.2  0.3  0.4  0.5
A
c
c
u
r
a
c
y
x
SO-PMI (macro)MEE (macro)MEE-GSL (macro)SR (macro)SR-GSL (macro)
Figure 3: Macro-averaged Accuracy
 0
 0.2
 0.4
 0.6
 0.8
 1
 0  0.1  0.2  0.3  0.4  0.5
A
c
c
u
r
a
c
y
x
SO-PMI (micro)MEE (micro)MEE-GSL (micro)SR (micro)SR-GSL (micro)
Figure 4: Micro-averaged Accuracy
1110
however, not significantly.
Table 4 shows selected example words with
their scores. These values can be understood bet-
ter together with the means and medians of the
respective methods which are given in the table as
well. These values give us an idea of where we
might expect the neutral point of a particular dis-
tribution of polarities.
Chemisch (chemical) is misclassified by SO-
PMI since it occurs in negative contexts on the
web. SimRank in turn was able to recognize
that most words similar to chemisch are neutral,
the most similar one being its literal translation,
chemical. Auferstanden (resurrected) is an exam-
ple for misclassification by SimRank which hap-
pens because the word is usually coordinated with
words that have negative sentiment, e.g. gestor-
ben (deceased) and gekreuzigt (crucified). This
problem could not be fixed by including adjective-
noun modification data since the coordinations
produced high log-likelihood values which lead to
dead being the most similar word to auferstanden.
Intelligent receives a score close to neutral with
the original (coordination-only) training method,
which could be corrected by applying MEE sim-
ply because the ordering of similar words changes
through the new weighting method. Nouns modi-
fied by intelligent include Leben (life) and Wesen
(being) whose translations are modified by pos-
itive adjectives. Many words, such as versiert
(skilled) are classified more accurately due to the
new weighting method when compared to our pre-
vious experiments (Scheible, 2010) where it re-
ceived a SimRank polarity of only 0.224.
The inclusion of adjective modifications does
not improve the classification results as often as
we had hoped. For some cases (cf. intelligent
mentioned above), the scores do improve, but the
overall impact is limited.
6 Conclusion and Outlook
We were able to show that sentiment translation
with SimRank is able to classify adjectives more
accurately than SO-PMI, an unsupervised base-
line method. We demonstrated that SO-PMI is
outperformed by SimRank when choosing a rea-
sonable region of neutral adjectives. In addition,
we showed that the improvements of SimRank
lead to better accuracy in sentiment translation in
some cases. In future work, we will apply a senti-
ment lexicon generated with SimRank in a senti-
ment classification task for reviews.
The algorithms we compared are different in
their purpose of application. While SO-PMI is
applicable when large corpora are available for a
language, it fails when used in a sparse-data situ-
ation, as noted by Turney (2002). We showed that
despite reducing the seed lexicon for SimRank to
a small fraction of its original size, it still performs
better than SO-PMI.
Currently, our experiments are limited by the
choice of using adjectives for our test set. While
the examination of adjectives is highly important
for sentiment analysis (as shown by Pang et al
(2002) who were able to achieve high accuracy
even when using only adjectives), the application
of our algorithms to a broader set of linguistic
units is an important goal for future work.
Acknowledgments. We are grateful to
Deutsche Forschungsgemeinschaft for fund-
ing this research as part of the WordGraph
project.
References
Banea, Carmen, Rada Mihalcea, Janyce Wiebe, and
Samer Hassan. 2008. Multilingual subjectivity
analysis using machine translation. In Empirical
Methods in Natural Language Processing, pages
127?135.
Christ, O., B.M. Schulze, A. Hofmann, and E. Koenig.
1999. The IMS Corpus Workbench: Corpus Query
Processor (CQP): User?s Manual. University of
Stuttgart, March, 8:1999.
Dorow, Beate, Florian Laws, Lukas Michelbacher,
Christian Scheible, and Jason Utt. 2009. A graph-
theoretic algorithm for automatic extension of trans-
lation lexicons. In Workshop on Geometrical Mod-
els of Natural Language Semantics, pages 91?95.
Hatzivassiloglou, Vasileios and Kathleen R. McKe-
own. 1997. Predicting the semantic orientation of
adjectives. In Proceedings of the 35th Annual Meet-
ing of the Association for Computational Linguis-
tics, pages 174?181.
Jeh, Glen and Jennifer Widom. 2002. Simrank: A
measure of structural-context similarity. In Pro-
ceedings of the Eighth ACM SIGKDD Interna-
1111
tional Conference on Knowledge Discovery and
Data Mining, pages 538?543.
Landis, J.R. and G.G. Koch. 1977. The measurement
of observer agreement for categorical data. Biomet-
rics, 33(1):159?174.
Laws, Florian, Lukas Michelbacher, Beate Dorow,
Christian Scheible, Ulrich Heid, and Hinrich
Schu?tze. 2010. A linguistically grounded graph
model for bilingual lexicon extraction. In Proceed-
ings of the 23nd International Conference on Com-
putational Linguistics.
Legendre, P. 2005. Species associations: the Kendall
coefficient of concordance revisited. Journal of
Agricultural Biological and Environment Statistics,
10(2):226?245.
Michelbacher, Lukas, Florian Laws, Beate Dorow, Ul-
rich Heid, and Hinrich Schu?tze. 2010. Building
a cross-lingual relatedness thesaurus using a graph
similarity measure. In Proceedings of the Seventh
Conference on International Language Resources
and Evaluation.
Mihalcea, Rada, Carmen Banea, and Janyce Wiebe.
2007. Learning multilingual subjective language
via cross-lingual projections. In Proceedings of the
45th Annual Meeting of the Association of Compu-
tational Linguistics, pages 976?983.
Pang, Bo, Lillian Lee, and Shivakumar Vaithyanathan.
2002. Thumbs up? Sentiment classification using
machine learning techniques. In Proceedings of the
2002 Conference on Empirical Methods in Natural
Language Processing, pages 79?86.
Scheible, Christian. 2010. Sentiment translation
through lexicon induction. In Proceedings of the
ACL 2010 Student Research Workshop, Uppsala,
Sweden. Association for Computational Linguis-
tics.
Schmid, Helmut. 1994. Probabilistic part-of-speech
tagging using decision trees. In Proceedings of the
International Conference on New Methods in Lan-
guage Processing.
Turney, Peter. 2002. Thumbs up or thumbs down?
semantic orientation applied to unsupervised classi-
fication of reviews. In Proceedings of 40th Annual
Meeting of the Association for Computational Lin-
guistics, pages 417?424.
Wan, Xiaojun. 2009. Co-training for cross-lingual
sentiment classification. In Proceedings of the Joint
Conference of the 47th Annual Meeting of the ACL
and the 4th International Joint Conference on Natu-
ral Language Processing of the AFNLP, pages 235?
243, Suntec, Singapore, August. Association for
Computational Linguistics.
West, Michael. 1953. A general service list of english
words.
Widdows, Dominic and Beate Dorow. 2002. A graph
model for unsupervised lexical acquisition. InCOL-
ING.
Wilson, Theresa, Janyce Wiebe, and Paul Hoffmann.
2005. Recognizing contextual polarity in phrase-
level sentiment analysis. In Proceedings of Human
Language Technology Conference and Conference
on Empirical Methods in Natural Language Pro-
cessing, pages 347?354, October.
1112
Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 793?803,
Edinburgh, Scotland, UK, July 27?31, 2011. c?2011 Association for Computational Linguistics
A Cascaded Classification Approach to Semantic Head Recognition
Lukas Michelbacher Alok Kothari Martin Forst?
Christina Lioma Hinrich Schu?tze
Institute for NLP
University of Stuttgart
{michells,kotharak,liomaca}@ims.uni-stuttgart.de
?Microsoft
martin.forst@microsoft.com
Abstract
Most NLP systems use tokenization as part
of preprocessing. Generally, tokenizers are
based on simple heuristics and do not recog-
nize multi-word units (MWUs) like hot dog
or black hole unless a precompiled list of
MWUs is available. In this paper, we propose
a new cascaded model for detecting MWUs
of arbitrary length for tokenization, focusing
on noun phrases in the physics domain. We
adopt a classification approach because ? un-
like other work on MWUs ? tokenization re-
quires a completely automatic approach. We
achieve an accuracy of 68% for recognizing
non-compositional MWUs and show that our
MWU recognizer improves retrieval perfor-
mance when used as part of an information re-
trieval system.
1 Introduction
Most NLP systems use tokenization as part of pre-
processing. Generally, tokenizers are based on sim-
ple heuristics and do not recognize multi-word units
(MWUs) like hot dog or black hole. Our long-term
goal is to build MWU-aware tokenizers that are used
as part of the standard toolkit for NLP preprocessing
alongside part-of-speech and named-entity tagging.
We define an MWU as a sequence of words that
has properties that cannot be inferred from the com-
ponent words (cf. e.g. Manning and Schu?tze (1999,
Ch. 5), Sag et al (2002)). The most important
of these properties is non-compositionality, the fact
that the meaning of a phrase cannot be predicted
from the meanings of its component words. For ex-
ample, a hot dog is not a hot animal but a sausage in
a bun and a black hole in astrophysics is a region of
space with special properties, not a dark cavity.
The correct recognition of MWUs is an important
building block of many NLP tasks. For example, in
information retrieval (IR) the query hot dog should
not retrieve documents that only contain the words
hot and dog individually, outside of the phrase hot
dog.
In this study, we focus on noun phrases in the
physics domain. For specialized domains such as
physics, adaptable and reliable MWU recognition
is of particular importance because comprehensive
and up-to-date lists of MWUs are not available
and would have to be created by hand. We chose
noun phrases because domain-specific terminology
is commonly encoded in noun phrase MWUs; other
types of phrases ? e.g., verb constructions ? rarely
give rise to fixed domain-specific multi-word se-
quences that should be treated as a unit.
We cast the task of MWU tokenization as seman-
tic head recognition in this paper. The importance of
syntactic heads for many NLP tasks is generally ac-
cepted. For example, in coreference resolution iden-
tity of syntactic heads is predictive of coreference;
in parse disambiguation, the syntactic head of a noun
phrase is a powerful feature for resolving attachment
ambiguities. However, in all of these cases, the syn-
tactic head is only an approximation of the informa-
tion that is really needed; the underlying assumption
made when using the syntactic head as a substitute
for the entire phrase is that the syntactic head is rep-
resentative of the phrase. This is not the case when
the phrase is non-compositional.
We define the semantic head of a noun phrase as
the non-compositional part of a phrase. Semantic
heads would serve most NLP tasks better than syn-
tactic heads. For example, a coreference resolution
system is misled if it looks at syntactic heads to de-
793
termine possible coreference of a hot dog . . . the dog
in I first ate a hot dog and then fed the dog. This is
not the case for a system that makes the decision
based on the semantic heads hot dog of a hot dog
and dog of the dog.
The specific NLP application we evaluate in this
paper is information retrieval. We will show that se-
mantic head recognition improves the performance
of an information retrieval system.
We introduce a cascaded classification framework
for recognizing semantic heads that allows us to treat
noun phrases of arbitrary length. We use a number
of previously proposed features for recognizing non-
compositionality and semantic heads. In addition,
we compare three features that measure contextual
similarity.
Our main contributions in this paper are as fol-
lows. First, we introduce the notion of semantic
head, in analogy to syntactic head, and propose se-
mantic head recognition as a new component of NLP
preprocessing. Second, we develop a cascaded clas-
sification framework for semantic head recognition.
Third, we investigate the utility of contextual simi-
larity for detecting non-compositionality and show
that it significantly enhances a baseline semantic
head recognizer. However, we also identify a num-
ber of challenges of using contextual similarity in
high-confidence semantic head recognition. Fourth,
we show that our approach to semantic head recog-
nition improves the performance of an IR system.
Section 2 discusses previous work. In Section 3
we introduce semantic heads and present our cas-
caded model for semantic head recognition. In Sec-
tion 4, we describe our data and three different mea-
sures of contextual similarity. Section 5 introduces
the classifier and its features. Section 6 presents
classification results and discussion. Section 7 de-
scribes the information retrieval experiments. In
Section 8 we present our conclusions.
2 Related Work
While there is a large number of publications on
MWUs and collocation extraction, the general prob-
lem of automatic MWU detection for the specific
purpose of tokenization has not been investigated
before to our knowledge.
The classic approach to identifying collocations
and MWUs is to apply statistical association mea-
sures (AMs) to n-grams extracted from a corpus
? often combined with various linguistic heuris-
tics and other filters, resulting in candidate lists.
Choueka (1988) and the XTRACT system (Smadja,
1993) are well-known examples of this approach.
More recent approaches such as Pecina (2010)
and Ramisch et al (2010) combine classifiers with
association measures. Although our approach is
classification-based as well, our data set has a more
realistic size than Pecina (2010)?s (1 billion words
vs 1.5 million words) and we work on noun phrases
of arbitrary length (instead of just bigrams). The
mwetoolkit1 by Ramisch et al (2010) aims to
be a software package for lexicographers and its
features are limited to a small set of association
measures that do not consider marginal frequencies.
Neither of these two studies includes evaluation in
the context of an application.
Lin (1999) defines a decision criterion for non-
compositional phrases based on the change in the
mutual information of a phrase when substituting
one word for a similar one based on an automatically
constructed thesaurus. The method reaches 15.7%
precision and 13.7% recall.
In terms of the extraction of domain-specific
MWUs, cross-language methods have been pro-
posed that make use of the fact that an MWU in one
language might be expressed as a single word in an-
other. Caseli et al (2009) utilize word alignments
in a parallel corpus; Attia et al (2010) exploit the
links between article names of different-language
Wikipedias to search for many-to-one translations.
We did not pursue a cross-language approach be-
cause we strive for a self-contained method of MWU
recognition that operates on a single textual re-
source.
Non-compositionality and distributional se-
mantics. In recent years, a number of studies have
investigated the relationship between distributional
semantics and non-compositionality. These studies
compute the similarity between words and phrases
represented as semantic vectors in a word space
model. A semantic vector of a word is the accumu-
lation of the particular contexts in which the word
1http://sourceforge.net/projects/
mwetoolkit/
794
appears. The underlying idea is similar to Lin?s:
the meaning of a non-compositional phrase some-
how deviates from what one would expect given the
semantic vectors of parts of the phrase. The stan-
dard measure to compare semantic vectors is cosine
similarity. The questions that arise are (i) which
vectors to compare, (ii) how to combine the vectors
of the parts and (iii) from what point on a certain
dissimilarity indicates non-compositionality. To our
knowledge, there are no generally accepted answers
to these questions.
Regarding (i), Schone and Jurafsky (2001) com-
pare the semantic vector of a phrase p and the vec-
tors of its component words in two ways: one in-
cludes the contexts of p in the construction of the
semantic vectors of the parts and one does not. Re-
garding (ii), they suggest weighted or unweighted
sums of the semantic vectors of the parts.
Baldwin et al (2003) investigate semantic decom-
posability of noun-noun compounds and verb con-
structions. They address (i) by comparing the se-
mantic vectors of phrases with the vectors of their
parts individually to detect meaning changes; e.g.,
they compare vice president to vice and president.
We propose a new method that compares phrases
with their alternative phrases, in the spirit of Lin
(1999)?s substitution approach (see Section 4.3).
Our rationale is that context features should be
based on contexts that are syntactically similar to the
phrase in question.
With respect to (iii), the above-mentioned studies
use ad hoc thresholds to separate compositional and
non-compositional phrases but do not offer a princi-
pled decision criterion.2 In contrast, we train a sta-
tistical classifier to learn a decision criterion.
There is a larger body of work concerning non-
compositionality which revolves around the prob-
lem of literal (compositional) vs. non-literal (non-
compositional) usage of idiomatic verb construc-
tions like to break the ice or to spill the beans.
Some studies approach the problem with semantic
vector comparisons in the style of Schone and Ju-
rafsky (2001), e.g Katz and Giesbrecht (2006) and
Cook et al (2007). Other approaches use word-
alignment (e.g. Moiro?n and Tiedemann (2006)) or
2Lin (1999) uses a well-defined criterion but his approach is
not based on vector similarity.
a combination of heuristic and linguistic features
(e.g. Diab and Bhutada (2009), Li and Sporleder
(2010)). Even though there is some methodologi-
cal overlap between our approach and some of the
verb-oriented studies, we believe that verb construc-
tions have properties that are quite different from
noun phrases. For example, our definition of alter-
native vector relies on the fact that most noun phrase
MWUs are fixed and exhibit no syntactic variability.
In contrast, verb constructions are often discontinu-
ous.
The motivation for most work on MWU detec-
tion is lexicography, terminology extraction or the
creation of machine-readable dictionaries. Our mo-
tivation ? tokenization in a preprocessing setting ? is
different from this earlier work.
3 Semantic Heads and Cascaded Model
We cast the task of MWU tokenization as seman-
tic head recognition in this paper. We define the
semantic head of a noun phrase as the largest non-
compositional part of the phrase that contains the
syntactic head. For example, black hole is the se-
mantic head of unusual black hole and afterglow is
the semantic head of bright optical afterglow; in the
latter case syntactic and semantic heads coincide.
Semantic heads would serve most NLP tasks bet-
ter than syntactic heads. The attachment ambiguity
of the last noun phrase in he bought the hot dogs in a
packet can be easily resolved for the semantic head
hot dogs (food is often in a packet), but not as easily
for the syntactic head dogs (dogs are usually not in
packets). Indeed, we will show in Section 7 that se-
mantic head recognition improves the performance
of an IR system.
The semantic head is either a single noun or a non-
compositional noun phrase. In the latter case, the
modifier(s) introduce(s) a non-compositional, un-
predictable shift of meaning; hot shifts the mean-
ing of dog from live animal to food. In contrast,
the compositional meaning shift caused by small
in small dog is transparent. The semantic head al-
ways contains the syntactic head; for compositional
phrases, syntactic head and semantic head are iden-
tical.
To determine the semantic head of a phrase, we
use a cascaded classification approach. The cascade
795
(1) neutron star
(2) unusual black hole
(3) bright optical afterglow
(4) small moment of inertia
Figure 1: Example phrases with modifiers. Peripheral
elements are set in italics, syntactic heads in bold.
comes into play in all aspects of our study: the rat-
ing experiments with human subjects, data extrac-
tion, feature design and classification itself.
We need a cascade because we want to recog-
nize the semantic head in noun phrases of arbitrary
length. The starting point is a phrase of length n:
p = w1 . . . wn. We distinguish between the syntac-
tic head of a phrase and the remaining words, the
modifiers. Figure 1 shows phrases of varying syn-
tactic complexity. The syntactic head is marked in
bold. The model accommodates pre-nominal modi-
fiers as in examples (1) through (3) and post-nominal
modifiers like PPs in example (4).
Among the modifiers, there is a distinguished ele-
ment, the peripheral element u (italicized in the ex-
amples). The remaining words are called the rest
v. We can now represent any phrase p as p = uv.3
The element u is always the outermost modifier. of -
PPs are treated as a single modifier and they take
precedence over pre-nominal modification because
this analysis is dominant in our gold standard data.
This means that in the phrase small moment of iner-
tia, small (and not of inertia) is the peripheral ele-
ment u.
Cascaded classification then operates as shown in
Figure 2. In each iteration, the classifier decides
whether the relation between the current peripheral
element u and the rest v is compositional (C) or non-
compositional (NC). If the relation is NC, process-
ing stops and uv is returned as the semantic head
of p. If the relation is compositional, u is discarded
and classification continues with v as the new input
phrase, which again is represented in the form u?v?.
In case there is no more peripheral element u, i.e.
the new v is a single word, it is returned as the se-
mantic head of p.
Table 1 shows two examples. For the fully com-
positional phrase bright optical afterglow, the pro-
3We use the abstract representation p = uv even though u
can appear after v in the surface form of p.
function recognize semantic head(p)
u? peripheral(p)
v ? rest(p)
while decision(u, v) 6= NC do
u? peripheral(v)
if u = ? then
return v
v ? rest(v)
return uv
Figure 2: Cascaded classification of p
step u v decision
1 bright optical afterglow C
2 optical afterglow C
3 ? afterglow
1 small moment of inertia C
2 of inertia moment NC
Table 1: Cascaded decision processes
cess runs all the way down to the syntactic head af-
terglow which is also the semantic head. In the sec-
ond case, the process stops earlier, in step 2, because
the classifier finds that the relation between moment
and of inertia is NC. This means that the semantic
head of small moment of inertia is moment of iner-
tia.
4 Corpus and Feature Definitions
4.1 Candidate phrases
As our corpus, we use the iSearch collection, a
one billion word collection of documents from the
physics domain (Lykke et al, 2010). We tokenized
the collection by splitting on white space and adding
sentence boundaries and part-of-speech tags to the
output. With part-of-speech information, the iden-
tification of MWU candidates is easy, fast and reli-
able.
We extracted all noun phrases from the collection
that consist of a head noun with up to four modifiers
? almost all domain-specific terminology in our col-
lection is captured by this pattern. The pre-nominal
modifiers can be nouns, proper nouns, adjectives or
cardinal numbers.
The baseline accuracy of a classifier that always
chooses compositionality is very high (> 90%) for
796
V = v V 6= v
U = u O11 O12 = R1
U 6= u O21 O22 = R2
= C1 = C2 = N
Table 2: 2-by-2 contingency tables with observed and
marginal frequencies
phrases of the type [noun] of the/a [noun] (sg.)
(e.g. rest of the paper) and [noun] of [noun] (pl.)
(e.g. series of papers). We therefore restrict post-
nominal modifiers to prepositional phrases with the
word of followed by a non-modified, indefinite, sin-
gular noun, e.g., speed of light or moment of inertia.
Out of all phrases extracted with part-of-speech
patterns, we keep only the ones that appear more of-
ten than 50 times because it is hard to compute re-
liable features for less frequent phrases. All experi-
ments were carried out with lemmatized word forms.
We refer to lemmas as words if not noted otherwise.
4.2 Association measures
Statistical association measures are frequently used
for MWU detection and collocation extraction (e.g.
Schone and Jurafsky (2001), Evert and Krenn
(2001), Pecina (2010)).
We use all measures used by Schone and Jurafsky
(2001) that can be derived from a phrase?s contin-
gency table. These measures are Student?s t-score,
z-score, ?2, pointwise mutual information (MI),
Dice coefficient, frequency, log-likelihood (G2) and
symmetric conditional probability.
We define the AMs in Table 3 based on the no-
tation for the contingency table shown in Table 2
(cf. Evert (2004)). Oij is observed frequency and
Eij = RiCjN expected frequency.
The AMs are designed to deal with two random
variables U and V that traditionally represent single
words. In our model, we use U to represent periph-
eral elements u and V for rests v.
association measure formula
student?s t-score (amt) O11?E11?O11
z-score (amz) O11?E11?E11
chi-square (am?2)
?
i,j
(Oij?Eij)2
Eij
pointwise mutual infor-
mation (amMI ) log
O11
E11
Dice coefficient (amD) 2O11R1+C1
frequency (amf ) O11
log-likelihood (amG2) 2
?
i,j
Oij log OijEij
symmetric conditional
probability (amscp)
O112
R1C1
Table 3: Association measures
4.3 Word space model
As our baseline, we use two methods of compar-
ing semantic vectors: sj1 and sj2, both introduced
by Schone and Jurafsky (2001). They experimented
with variants of sj1 and sj2, but found no large differ-
ences. In addition, we introduce our own approach
alt.
Method sj1 compares the semantic vector of a
phrase p with the sum of the vectors of its parts.
Method sj2 is like sj1, except the contexts of p are
not part of the semantic vectors of the parts. Method
alt compares the semantic vector of a phrase with its
alternative vector. In the definitions below, s repre-
sents a vector similarity measure, w(p) a general se-
mantic vector of a phrase p and w?(wi) the semantic
vector of a partwi of a phrase p that does not include
the contexts of occurrences of wi that were part of p
itself.
sj1 s(w(black hole), w(black) + w(hole))
sj2 s(w(black hole), w?(black) + w?(hole))
alt s(w(black hole),?
u
w(u, hole)); u 6= black
For the third comparison, we build the alternative
vector as follows. For a phrase p = uv with pe-
ripheral element u and rest v, we call the phrase
797
p? = u?v an alternative phrase if the rest v is the
same and u? 6= u. E.g., giant star is an alternative
phrase of neutron star and isolated neutron star is
an alternative of young neutron star. The alterna-
tive vector of p is then the semantic vector that is
computed from the contexts of all of p?s alternative
phrases. The alternative vector is a representation
of the contexts of v except for those modified by u.
This technique bears resemblance to the substitution
approach of Lin (1999). The difference is that he
relies on a similarity thesaurus for substitution and
monitors the change in mutual information for each
substitution individually whereas we substitute with
general alternative modifiers and combine the alter-
native contexts into one vector for comparison.
Previous work has compared the semantic vector
of a phrase with the vectors of its components. Our
approach is more ?head-centric? and only compares
phrases in the same syntactic configuration. Our
question is: Is the typical context of the head hole
if it occurs with a modifier that is not black different
from when it occurs with the modifier black?
We used a bag-of-words model and a window of
?10 words for contexts to create semantic vectors.
We only kept the content words in the window which
we defined as words that are tagged as either a noun,
verb, adjective or adverb. To add information about
the variability of syntactic contexts in which phrases
occur, we add the words immediately before and af-
ter the phrase with positional markers (?1 and +1,
respectively) to the vector. These words were not
subject to the content-word filter. The dimension-
ality of the vectors is then 3V where V is the size
of the vocabulary: V dimensions each for bag-of-
words, left and right syntactic contexts. We did not
include vectors for the stop word of for sj1 and sj2.
4.4 Non-compositionality judgments
Since the domain of the corpus is physics, highly
specialized vocabulary had to be judged. We em-
ployed domain experts as raters (one engineering
and two physics graduate students).
In line with the cascaded model, the raters where
asked to identify the semantic head of each candi-
date phrase. If at least two raters agreed on a seman-
tic head of a phrase we made this choice the seman-
tic head in the gold standard. The final gold standard
comprises 1560 phrases.
We computed raw agreement of each rater with
the gold standard as the percentage of correctly rec-
ognized semantic heads ? this is the task that the
classifier addresses. Agreement is quite high at
86.5%, 88.3% and 88.5% for the three raters. In
addition, we calculated chance-corrected agreement
with Cohen?s ? on the first decision task against the
gold standard (see Section 6). As expected, agree-
ment decreases, but is still substantial at 74.0%,
78.2% and 71.8% for the three raters.
5 Classifier
We use the Stanford maximum entropy classifier for
our experiment.4 We randomly split the data into a
training set of 1300 and a held-out test set of 260
pairs.
We use the eight AMs and the cosine similari-
ties simsj1, simsj2 and simalt described in Sec-
tion 4.3 as features for the classifier. Cosine similar-
ity should be small if a phrase is non-compositional
and large if it is compositional. In other words, if the
contexts of the candidate phrase are too dissimilar to
the contexts of the sum of its parts or to the alterna-
tive phrases, then we suspect non-compositionality.
Feature values are binned into 5 bins. We ap-
plied a log transformation to the four AMs with large
values: amf , amG2 , am?2 and amz . For our ap-
plication there is little difference between statistical
significance at p < .001 and p < .00001. The
log transformation reduces the large gap in magni-
tude between high significance and very high signif-
icance. If co-occurrence of u and v in uv is below
chance, then we set the association scores to 0 since
this is an indication of compositionality (even if it is
highly significant).
Since AMs have been shown to be correlated (e.g.
Pecina (2010)), we first perform feature selection on
the AM features. We tested accuracy of all 2r ? 1
non-empty combinations of the r = 8 AM features
on the task of deciding whether the first decision
during the classification of a phrase was C or NC.
We then selected those AM features that were part
of at least one top 10 result in each fold. Those fea-
tures were amt, amf and amscp.
The main experiment combines these three se-
4http://nlp.stanford.edu/software/
classifier.shtml
798
lected AM features with all possible subsets of con-
text features. We train on the 1300-element training
set and test on the 260-element test set.
6 Results and Discussion
We ran three evaluation modes: dec-1st, dec-all, and
semh. Mode dec-1st only evaluates the first deci-
sion for each phrase; the baseline in this case is .554
since 55.4% of the first decisions are C. In mode
dec-all, we evaluate all decisions that were made in
the course of recognizing the semantic head. This
mode emphasizes the correct recognition of seman-
tic heads in phrases where multiple correct decisions
in a row are necessary. We define the confidence
for multi-decision classification as the product of
the confidence values of all intermediate decisions.
There is no obvious baseline for dec-all because the
number of decisions depends on the classifier ? a
classifier whose first decision on a four-word phrase
is NC makes one decision, another one may make
three. The mode semh evaluates how many semantic
heads were recognized correctly. This mode directly
evaluates the task of semantic head recognition. The
baseline for semh is the tokenizer that always returns
the syntactic head; this baseline is .488.5 Table 4
shows 8? 3 runs, corresponding to the three modes
tested on the AM features (amt, amf , and amscp)
and the eight possible subsets of the three context
features.
For all modes, the best result is achieved with base
AMs combined with the simalt feature; the accura-
cies are .692, .703 and .680. The improvements over
the baselines (for dec-1st and semh) are statistically
significant at p < .01 (binomial test, n = 260).
For semh, accuracy without any context features
is .603; this is significantly better than the .488 base-
line (p < .01). Performance with only the base AM
features is significantly lower than the best context
feature experiment (.680) at p < .01 and signifi-
cantly lower than the worst context feature exper-
iment (.653) at p < .1. However, the differences
between the context feature runs are not significant.
When the semantic head recognizer processes a
phrase, there are four possible results. Result rsemh:
5The baseline could be improved with simple heuristics, e.g.
?uv contains capital letter?? NC. However, this feature only
results in a 2% improvement compared to the baseline.
type freq definition
rsemh 92 sem. head correct (6= synt. head)
rsynth 85 sem. head correct (= synt. head)
r+ 48 sem. head too long
r? 35 sem. head too short
all 260
Table 5: Distribution of result types
the semantic head is correctly recognized and it is
distinct from the syntactic head. Result rsynth: the
semantic head is correctly recognized and it is iden-
tical to the syntactic head. Result r+: the semantic
head is not correctly recognized because the cascade
was stopped too early, i.e., a compositional modifier
that should have been removed was kept. Result r?:
the semantic head is not correctly recognized be-
cause the cascade was stopped too late, i.e., a modi-
fier causing a non-compositional meaning shift was
removed. Table 5 shows the distribution of result
types. It shows that r+ is the more common error:
the classifier more often regards compositional rela-
tions as non-compositional than vice versa.
Table 6 shows the top 20 classifications where
the semantic head was not the same as the syntac-
tic head sorted by confidence in descending order.
In the third column ?phrase . . . ? we list the candi-
dates with semantic heads in bold. The columns to
the right show the predicted semantic head and the
feature values. All five errors in the list are of type
r+.
Two r+ phrases are schematic view and many oth-
ers. The two phrases are clearly compositional and
the classifier failed even though the context feature
points in the direction of compositionality with a
value greater than .5. It can be argued that many oth-
ers is a trivial example that does not require complex
machinery to be identified as compositional, e.g. by
using a stop list. We included it in the analysis since
we want to be able to process arbitrary phrases with-
out additional hand-crafted resources.
Another incorrect classification occurs with the
phrase massive star birth6 for which star birth was
annotated as the semantic head. Here we have a case
where the peripheral element massive does not mod-
6i.e. the birth of a massive star, a certain type of star with
very high mass
799
mode baseline context feature context feature subsets
simalt - ? ? ? ? - - -
simsj1 - - ? ? ? ? - ?
simsj2 - - - ? ? - ? ?
dec-1st .554 .604 .692 .669 .685 .677 .654 .654 .662
dec-all - .615 .703 .681 .696 .688 .666 .669 .675
semh .488 .603 .680 .657 .673 .665 .653 .653 .661
Table 4: Performance for base AM features plus context feature subsets. A ??? indicates the use of the corresponding
context feature.
ify the syntactic head birth but massive star is itself
a complex modifier. In the test set, 5% of the phrases
exhibit structural ambiguities of this type. Our sys-
tem cannot currently deal with this phenomenon.
The remaining r+ phrases are peculiar velocity
and local group. However, Wikipedia lists both
phrases with an individual entry defining the former
as the true velocity of an object, relative to a rest
frame7 and the latter as the group of galaxies that
includes Earth?s galaxy, the Milky Way8. Both def-
initions provide evidence for non-compositionality
since the velocity is not peculiar (as in strange) and
the scope of local is not clear without further knowl-
edge. Arguably, in these cases our method chose a
justifiable semantic head, but the raters disagreed.9
For NLP preprocessing, it is acceptable to sacri-
fice recall and only make high-confidence decisions
on semantic heads. A tokenizer that reliably detects
a subset of MWUs is better than one that recognizes
none. However, our attempts to use the simalt rec-
ognizer (bold in Table 4) in this way were not suc-
cessful. Precision is .680 for confidence > .7 and
does not exceed .770 for higher confidence values.
To understand this effect, we analyzed the distri-
bution of simalt scores. Surprisingly, moderate sim-
ilarity between .4 and .6 is a more reliable indicator
for NC than low similarity < .3. Our intuition for
using distributional semantics in Section 2 was that
low similarity indicates non-compositionality. This
7http://en.wikipedia.org/wiki/Peculiar_
velocity
8http://en.wikipedia.org/wiki/Local_
group
9Further evidence that local group is non-compositional is
the fact that one of the domain experts annotated the phrase as
non-compositional but was overruled by the other two.
does not seem to hold for the lowest similarity val-
ues possibly because they are often extreme cases
in terms of distribution and frequency and then give
rise to unreliable decisions. This means that the con-
text features enhance the overall performance of the
classifier, but they are unreliable and do not support
the high-confidence decisions we need in NLP pre-
processing.
For comparison, the classifier that only uses AM
features achieves 90% precision at 14% recall with
confidence > .7 ? although it has lower overall ac-
curacy than the simalt recognizer. We are still in
the process of analyzing these results and decided to
use the AM-only recognizer for the IR experiment
because it has more predictable performance.
In summary, the results show that, for the recogni-
tion of semantic heads, basic AMs offer a significant
improvement over the baseline. We have shown that
some wrong decisions are defensible even though
the gold standard data suggests otherwise. Context
features further increase performance significantly,
but surprisingly, they are not of clear benefit for
a high-confidence classifier that is targeted towards
recognizing a smaller subset of semantic heads with
high confidence.
7 Information Retrieval Experiment
Typically, IR systems do not process non-
compositional phrases as one semantic entity,
missing out on potentially important information
captured by non-compositionality. This section
illustrates one way of adjusting the retrieval process
so that non-compositional phrases are processed as
semantic entities that may enhance retrieval perfor-
mance. The underlying hypothesis is that, given
800
c. type phrase (semantic head in bold) predicted semantic head amt amf amcp simalt
.99 rsemh ellipsoidal figure of equilibrium ellipsoidal figure of equilibrium 18.03 325 6.23e-01 .219
.99 rsemh point spread function point spread function 95.03 9056 2.33e-01 .529
.99 r+ massive star birth massive star birth 19.99 402 4.81e-03 .134
.98 rsemh high angular resolution imaging high angular resolution imaging 13.07 179 1.27e-03 .173
.98 rsemh integral field spectrograph integral field spectrograph 24.20 586 4.12e-02 .279
.98 r+ local group local group 153.54 24759 8.73e-03 .650
.98 rsemh neutral kaon system neutral kaon system 1.38 108 4.17e-03 .171
.97 rsemh IRAF task IRAF task 49.07 2411 2.96e-02 .517
.92 rsemh easy axis easy axis 44.66 2019 2.79e-03 .599
.89 r+ schematic view schematic view 40.56 1651 8.06e-03 .612
.87 rsemh differential resistance differential resistance 31.71 1034 6.38e-04 .548
.86 rsemh TiO band TiO band 36.84 1372 2.21e-03 .581
.86 r+ many others many others 97.76 9806 6.54e-03 .708
.86 rsemh VLBA observation VLBA observation 43.95 2004 9.35e-04 .648
.85 r+ peculiar velocity peculiar velocity 167.63 28689 2.37e-02 .800
.84 rsemh computation time computation time 43.80 1967 1.35e-03 .657
.83 rsemh Land factor Land factor 21.15 453 6.30e-04 .360
.83 rsemh interference filter interference filter 31.44 1002 1.27e-03 .574
.83 rsemh line formation calculations line formation calculations 14.20 203 1.96e-03 .381
.82 rsemh Wess-Zumino-Witten term Wess-Zumino-Witten term 9.60 94 8.12e-05 .291
Table 6: The 20 most confident classifications where the prediction is semantic head 6= syntactic head. ?c.? = confi-
dence
a query that contains a non-compositional phrase,
boosting the retrieval weight of documents that
contain this phrase will improve overall retrieval
performance.
We do this boosting using Indri?s10 combination
of the language modeling and inference network
approaches (Metzler and Croft, 2004), which al-
lows assigning different degrees of belief to differ-
ent parts of the query. This belief can be drawn from
any suitable external evidence of relevance. In our
case, this source of evidence is the knowledge that
certain query terms constitute a non-compositional
phrase. Under this approach, and using the #weight
and #combine operators for combining beliefs, the
relevance of a documentD to a queryQ is computed
as the probability that D generates Q, P (Q|D):
P (Q|D) =
?
t?Q
P (t|D)
wt
W (W =
?
t?Q
wt) (1)
where t is a term and wt is the belief weight as-
signed to t. The higher wt is, the higher the rank
of documents containing t. In this work, we dis-
10http://www.lemurproject.org/
tinguish between two types of query terms: terms
occurring in non-compositional phrases (Qnc), and
the remaining query terms (Qc). Terms t ? Qnc
receive belief weight wnc and terms t ? Qc belief
weight wc, (wnc + wc = 1 and wnc, wc ? [0, 1]).
To boost the ranking of documents containing non-
compositional phrases, we increase wnc at the ex-
pense of wc. We estimate P (t|D) in Eq. 1 using
Dirichlet smoothing (Zhai and Lafferty, 2002).
We use Indri for indexing and retrieval without
removing stopwords or stemming. This choice is
motivated by two reasons: (i) We do not have a
domain-specific stopword list or stemmer. (ii) Base-
line performance is higher when keeping stopwords
and without stemming, rather than without stop-
words and with stemming.
We use the iSearch collection discussed in Sec-
tion 4. It comprises 453,254 documents and a
set of 65 queries with relevance assessments. To
match documents to queries without any treat-
ment of non-compositionality (baseline run), we
use the Kullback-Leibler language model with
Dirichlet smoothing (KL-Dir) (Zhai and Lafferty,
2002). We applied the preprocessing described
801
run MAP REC P20
baseline 0.0663 770 0.1385
real NC 0.0718 844 0.1538
pseudo NC1 0.0664 788 0.1385
pseudo NC2 0.0658 782 0.1462
pseudo NC3 0.0671 777 0.1477
pseudo NC4 0.0681 807 0.1462
pseudo NC5 0.0670 783 0.1423
Table 7: IR performance without considering non-
compositionality (baseline), versus boosting real and
pseudo non-compositionality (real NC, pseudo NCi).
in Section 4 to the queries and identified non-
compositional phrases with the base AM classifier
from Section 5. Our approach for boosting the
weight of these non-compositional phrases uses
the same retrieval model enhanced with belief
weights as described in Eq. 1 (real NC run). In
addition, we include five runs that boost the weight
of pseudo non-compositional phrases that were
created randomly from the query text (pseudo NC
runs). These pseudo non-compositional phrases
have exactly the same length as the observed non-
compositional phrases for each query. We measure
retrieval performance in terms of mean average
precision (MAP), precision at 20 (P20), and recall
(REC, number of relevant documents retrieved
? total is 2878). For each evaluation measure
separately, we tune the following parameters and
report the best performance: (i) the smoothing
parameter ? of the KL-Dir retrieval model (? ?
{100, 500, 800, 1000, 2000, 3000, 4000, 5000, 8000,
10000}, following Zhai and Lafferty (2002)); (ii)
the belief weights wnc, wc ? {0.1, . . . , 0.9} in steps
of 0.1 while preserving wnc + wc = 1 at all times.
Table 7 displays retrieval performance of our
approach against the baseline and five runs with
pseudo non-compositional phrases. We see a 9.61%
improvement in the number of relevant retrieved
documents over the baseline. MAP and P20 also
show improvements. Our approach is better than
any of the 5 random runs on all three metrics ? the
probability of getting such a good result by chance
is 125 < .05, and thus the improvements are statis-tically significant. On doing a query-wise analysis
of MAP scores, we find that large improvements
over the baseline occur when a non-compositional
phrase aligns with what the user is looking for. The
system seems to retrieve more relevant documents
in that case. E.g., the improvement in MAP is
0.0977 for query #19. The user was looking for
?articles . . . on making tunable vertical cavity sur-
face emitting laser diodes? and laser diodes was
one of the non-compositional phrases recognized.
On the other hand, a decrease in MAP occurs for
non-compositional phrases unrelated to the infor-
mation need. In query #4 the user is looking for
?protein-protein interaction, the surface charge dis-
tribution of these proteins and how this has been in-
vestigated with Electrostatic Force Microscopy? and
though non-compositional phrases such as Force Mi-
croscopy are recognized, these do not reflect the core
information need ?The proteins of interest are the
Avidin-Biotin and IgG-anti-IgG systems?.
8 Conclusion
We have presented an approach to improving to-
kenization in NLP preprocessing that is based on
the notion of semantic head. Semantic heads are
? in analogy to syntactic heads ? the core meaning
units of phrases that cannot be further semantically
decomposed. To perform semantic head recogni-
tion for tokenization, we defined a novel cascaded
model and implemented it as a statistical classifier
that used previously proposed and new context fea-
tures. We have shown that the classifier significantly
outperforms the baseline and that context features
increase performance. We reached an accuracy of
68% and argued that even a semantic head recog-
nizer restricted to high-confidence decisions is use-
ful ? because reliably recognizing a subset of se-
mantic heads is better than recognizing none. We
showed that context features increase the accuracy
of the classifier, but undermine the confidence as-
sessments of the classifier, a result we are still ana-
lyzing. Finally, we showed that even in its prelim-
inary current form the semantic head recognizer is
able to improve the performance of an IR system.
Acknowledgments
This work was funded by DFG projects SFB 732 and
WordGraph. We also thank the anonymous review-
ers for their comments.
802
References
Mohammed Attia, Antonio Toral, Lamia Tounsi, Pavel
Pecina, and Josef van Genabith. 2010. Automatic ex-
traction of arabic multiword expressions. In Proceed-
ings of the 2010 Workshop on Multiword Expressions,
pages 19?27, Beijing, China. Coling 2010 Organizing
Committee.
Timothy Baldwin, Colin Bannard, Takaaki Tanaka, and
Dominic Widdows. 2003. An empirical model of
multiword expression decomposability. In Proceed-
ings of the ACL 2003 Workshop on Multiword Expres-
sions, pages 89?96, Sapporo, Japan. Association for
Computational Linguistics.
Helena Caseli, Aline Villavicencio, Andre? Machado,
and Maria Jose? Finatto. 2009. Statistically-driven
alignment-based multiword expression identification
for technical domains. In Proceedings of the 2009
Workshop on Multiword Expressions, pages 1?8, Sin-
gapore. Association for Computational Linguistics.
Yaacov Choueka. 1988. Looking for needles in a
haystack. In Proceedings of RIAO88, pages 609?623.
Paul Cook, Afsaneh Fazly, and Suzanne Stevenson.
2007. Pulling their weight: Exploiting syntactic forms
for the automatic identification of idiomatic expres-
sions in context. In Proceedings of the 2007 on Mul-
tiword Expressions, pages 41?48, Prague, Czech Re-
public. Association for Computational Linguistics.
Mona Diab and Pravin Bhutada. 2009. Verb noun con-
struction mwe token classification. In Proceedings of
the 2009 Workshop on Multiword Expressions, pages
17?22, Singapore. Association for Computational Lin-
guistics.
Stefan Evert and Brigitte Krenn. 2001. Methods for the
qualitative evaluation of lexical association measures.
In Proceedings of the 39th Annual Meeting on Associ-
ation for Computational Linguistics, pages 188?195.
Association for Computational Linguistics.
Stefan Evert. 2004. The Statistics of Word Cooccur-
rences: Word Pairs and Collocations. Ph.D. thesis, In-
stitut fu?r maschinelle Sprachverarbeitung (IMS), Uni-
versita?t Stuttgart.
Graham Katz and Eugenie Giesbrecht. 2006. Auto-
matic identification of non-compositional multi-word
expressions using latent semantic analysis. In Pro-
ceedings of the 2006 Workshop on Multiword Expres-
sions, pages 12?19, Sydney, Australia. Association for
Computational Linguistics.
Linlin Li and Caroline Sporleder. 2010. Linguistic cues
for distinguishing literal and non-literal usages. In
Coling 2010: Posters, pages 683?691, Beijing, China.
Coling 2010 Organizing Committee.
Dekang Lin. 1999. Automatic identification of non-
compositional phrases. In Proceedings of the 37th
Annual Meeting of the Association for Computational
Linguistics, pages 317?324, College Park, Maryland,
USA. Association for Computational Linguistics.
Marianne Lykke, Birger Larsen, Haakon Lund, and Pe-
ter Ingwersen. 2010. Developing a test collection for
the evaluation of integrated search. In Advances in In-
formation Retrieval, 32nd European Conference on IR
Research, ECIR 2010, Milton Keynes, UK, March 28-
31, 2010. Proceedings, pages 627?630.
Christopher D. Manning and Hinrich Schu?tze. 1999.
Foundations of Statistical Natural Language Process-
ing. The MIT Press, Cambridge, MA.
Donald Metzler and W. Bruce Croft. 2004. Combining
the language model and inference network approaches
to retrieval. Inf. Process. Manage., 40(5):735?750.
B.V. Moiro?n and Jo?rg Tiedemann. 2006. Identify-
ing Idiomatic Expressions Using Automatic Word-
Alignment. In Multi-Word-Expressions in a Multilin-
gual Context, page 33.
Pavel Pecina. 2010. Lexical association measures and
collocation extraction. Language Resources and Eval-
uation, 44(1-2):138?158.
Carlos Ramisch, Aline Villavicencio, and Christian
Boitet. 2010. mwetoolkit: a framework for multiword
expression identification. In Proceedings of the Sev-
enth conference on International Language Resources
and Evaluation (LREC?10), Valletta, Malta.
Ivan A. Sag, Timothy Baldwin, Francis Bond, Ann
Copestake, and Dan Flickinger. 2002. Multiword ex-
pressions: A pain in the neck for nlp. In Proceedings
of the 3rd International Conference on Intelligent Text
Processing and Computational Linguistics, pages 1?
15, Mexico City.
Patrick Schone and Daniel Jurafsky. 2001. Is
knowledge-free induction of multiword unit dictionary
headwords a solved problem? In Proceedings of
the 2001 Conference on Empirical Methods in Natu-
ral Language Processing, pages 100?108, Pittsburgh,
Pennsylvania, USA. Association for Computational
Linguistics.
Frank Smadja. 1993. Retrieving collocations from text:
Xtract. Computational linguistics, 19(1):143?177.
ChengXiang Zhai and John D. Lafferty. 2002. Two-stage
language models for information retrieval. In SIGIR,
pages 49?56. ACM.
803
Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 1546?1556,
Edinburgh, Scotland, UK, July 27?31, 2011. c?2011 Association for Computational Linguistics
Active Learning with Amazon Mechanical Turk
Florian Laws Christian Scheible Hinrich Schu?tze
Institute for Natural Language Processing
Universita?t Stuttgart
{lawsfn, scheibcn}@ims.uni-stuttgart.de
Abstract
Supervised classification needs large amounts
of annotated training data that is expensive to
create. Two approaches that reduce the cost
of annotation are active learning and crowd-
sourcing. However, these two approaches
have not been combined successfully to date.
We evaluate the utility of active learning in
crowdsourcing on two tasks, named entity
recognition and sentiment detection, and show
that active learning outperforms random selec-
tion of annotation examples in a noisy crowd-
sourcing scenario.
1 Introduction
Supervised classification is the predominant tech-
nique for a large number of natural language pro-
cessing (NLP) tasks. The large amount of labeled
training data that supervised classification relies on
is time-consuming and expensive to create, espe-
cially when experts perform the data annotation.
Recently, crowdsourcing services like Amazon Me-
chanical Turk (MTurk) have become available as an
alternative that offers acquisition of non-expert an-
notations at low cost. MTurk is a software service
that outsources small annotation tasks ? called HITs
? to a large group of freelance workers. The cost of
MTurk annotation is low, but a consequence of us-
ing non-expert annotators is much lower annotation
quality. This requires strategies for quality control
of the annotations.
Another promising approach to the data acqui-
sition bottleneck for supervised learning is active
learning (AL). AL reduces annotation effort by set-
ting up an annotation loop where, starting from a
small seed set, only the maximally informative ex-
amples are chosen for annotation. With these an-
notated examples, the classifier is then retrained to
again select more informative examples for further
annotation. In general, AL needs a lot fewer anno-
tations to achieve a desired performance level than
random sampling.
AL has been successfully applied to a number of
NLP tasks such as part-of-speech tagging (Ringger
et al, 2007), parsing (Osborne and Baldridge, 2004),
text classification (Tong and Koller, 2002), senti-
ment detection (Brew et al, 2010), and named entity
recognition (NER) (Tomanek et al, 2007). Until
recently, most AL studies focused on simulating the
annotation process by using already available gold
standard data. In reality, however, human annota-
tors make mistakes, leading to noise in the annota-
tions. For this reason, some authors have questioned
the applicability of AL to noisy annotation scenarios
such as MTurk (Baldridge and Palmer, 2009; Re-
hbein et al, 2010).
AL and crowdsourcing are complementary ap-
proaches: AL reduces the number of annotations
used while crowdsourcing reduces the cost per an-
notation. Combined, the two approaches could sub-
stantially lower the cost of creating training sets.
Our main contribution in this paper is that we
show for the first time that AL is significantly bet-
ter than randomly selected annotation examples in
a real crowdsourcing annotation scenario. Our
experiments directly address two tasks, named en-
tity recognition and sentiment detection, but our
1546
evidence suggests that AL is of general benefit in
crowdsourcing. We also show that the effectiveness
of MTurk annotation with AL can be further en-
hanced by using two techniques that increase label
quality: adaptive voting and fragment recovery.
2 Related Work
2.1 Crowdsourcing
Pioneered by Snow et al (2008), Crowdsourcing,
especially using MTurk, has become a widely used
service in the NLP community. A number of stud-
ies have looked at crowdsourcing for NER. Voyer et
al. (2010) use a combination of expert and crowd-
sourced annotations. Finin et al (2010) annotate
Twitter messages ? short sequences of words ? and
this is reflected in their vertically oriented user in-
terface. Lawson et al (2010) choose an annotation
interface where annotators have to drag the mouse
to select entities. Carpenter and Poesio (2010) ar-
gue that dragging is less convenient for workers than
marking tokens.
These papers do not address AL in crowdsourc-
ing. Another important difference is that previous
studies on NER have used data sets for which no
?linguistic? gold annotation is available. In con-
trast, we reannotate the CoNLL-2003 English NER
dataset. This allows us to conduct a detailed com-
parison of MTurk AL to conventional expert anno-
tation.
2.2 Active Learning with Noisy Labels
Hachey et al (2005) were among the first to in-
vestigate the effect of actively sampled instances
on agreement of labels and annotation time. They
demonstrate applicability of AL when annotators are
trained experts. This is an important result. How-
ever, AL depends on accurate assessments of uncer-
tainty and informativeness and such an accurate as-
sessment is made more difficult if labels are noisy
as is the case in crowdsourcing. For this reason, the
problem of AL performance with noisy labels has
become a topic of interest in the AL community. Re-
hbein et al (2010) investigate AL with human expert
annotators for word sense disambiguation, but do
not find convincing evidence that AL reduces anno-
tation cost in a realistic (non-simulated) annotation
scenario. Brew et al (2010) carried out experiments
on sentiment active learning through crowdsourcing.
However, they use a small set of volunteer labelers
instead of anonymous paid workers.
Donmez and Carbonell (2008) propose a method
to choose annotators from a set of noisy annotators.
However, in a crowdsourcing scenario, it is not pos-
sible to ask specific annotators for a label, as crowd-
sourcing workers join and leave the site. Further-
more, they only evaluate their approach in simula-
tions. We use the actual labels of human annotators
to avoid the risk of unrealistic assumptions when
modeling annotators.
We are not aware of any study that shows that AL
is significantly better than a simple baseline of hav-
ing annotators annotate randomly selected examples
in a highly noisy annotation setting like crowdsourc-
ing. While AL generally is superior to this base-
line in simulated experiments, it is not clear that
this result carries over to crowdsourcing annotation.
Crowdsourcing differs in a number of ways from
simulated experiments: the difficulty and annotation
consistency of examples drawn by AL differs from
that drawn by random sampling; crowdsourcing la-
bels are noisy; and because of the noisiness of labels
statistical classifiers behave differently in simulated
and real annotation experiments.
3 Annotation System
One fundamental design criterion for our annotation
system was the ability to select examples in real time
to support, e.g., the interactive annotation experi-
ments presented in this paper. Thus, we could not
use the standard MTurk workflow or services like
CrowdFlower.1
We therefore designed our own system for anno-
tation experiments. It consists of a two-tiered ap-
plication architecture. The frontend tier is a web
application that serves two purposes. First, the ad-
ministrator can manage annotation experiments us-
ing a web interface and publish annotation tasks as-
sociated with an experiment on MTurk. The front-
end also provides tools for efficient review of the
received answers. Second, the frontend web appli-
cation presents annotation tasks to MTurk workers.
Because we wanted to implement interactive anno-
tation experiments, we used the ?external question?
1http://crowdflower.com/
1547
feature of MTurk. An external question contains
an URL to our frontend web application, which is
queried when a worker views an annotation task.
Our frontend then in turn queries our backend com-
ponent for an example to be annotated and renders it
in HTML.
The backend component is responsible for selec-
tion of an example to be annotated in response to a
worker?s request for an annotation task. The back-
end implements a diverse choice of random and ac-
tive selection strategies as well as the multilabel-
ing strategies described in section 3.2. The backend
component runs as a standalone server and is queried
by the frontend via REST-like HTTP calls.
For the NER task, we present one sentence per
HIT, segmented into tokens, with a select box under-
neath each token containing the classes. The defini-
tion of the classes is based on the CoNLL-2003 an-
notation guidelines (Tjong Kim Sang and De Meul-
der, 2003). Examples were given for every class.
Annotators are forced to make a selection for upper-
case tokens. Lowercase tokens are prelabeled with
?O? (no named entity), but annotators are encour-
aged to change this label if the token is in fact part
of an entity phrase.
For sentiment annotation, we found in prelim-
inary experiments that using simple radio button
selection for the choice of the document label
(positive or negative) leads to a very high
amount of spam submissions, taking the overall clas-
sification accuracy down to around 55%. We then
designed a template that forced annotators to type
the label as well as a randomly chosen word from
the text. Individual label accuracy was around 75%
in this scheme.
3.1 Concurrent example selection
AL works by setting up an interactive annotation
loop where at each iteration, the most informative
example is selected for annotation. We use a pool-
based AL setup where the most informative exam-
ple is selected from a pool of unlabeled examples.
Informativeness is calculated as uncertainty (Lewis
and Gale, 1994) using the margin metric (Schein
and Ungar, 2007). This metric chooses examples for
which the margin of probabilities from the classifier
between the two most probable classes is the small-
est:
Mn = |P? (c1|xn) ? P? (c2|xn)|
Here, xn is the instance to be classified, c1 and c2
are the two most likely classes, and P? the classifier?s
estimate of probability.
For NER, the margins of the tokens are averaged
to get an uncertainty assessment of the sentence. For
sentiment, whole documents are classified, thus un-
certainties can be used directly.
After annotation, the selected example is removed
from the unlabeled pool and, together with its la-
bel(s), added to the set of labeled examples. The
classifier is then retrained on the labeled examples
and the informativeness of the remaining examples
in the pool is re-evaluated.
Depending on the classifier and the sizes of pool
and labeled set, retraining and reevaluation can take
some time. To minimize wait times, traditional AL
implementations select examples in batches of the
n most informative examples. However, batch se-
lection might not give the optimum selection (exam-
ples in a batch are likely to be redundant, see Brinker
(2003)) and wait times can still occur between one
batch and the next.
When performing annotation with MTurk, wait
times are unacceptable. Thus, we perform the re-
training and uncertainty rescoring concurrently with
the annotation user interface. The unlabeled pool is
stored in a priority queue that is ordered according to
the examples? informativeness. The annotation user
interface takes the most informative example from
the pool and presents it to the annotator. The la-
beled example is then inserted into a second queue
that feeds and updates retraining and rescoring pro-
cesses. The pool queue then is resorted according to
the new informativeness. In this way, annotation and
example selection can run in parallel. This is similar
to Haertel et al (2010).
3.2 Adaptive voting and fragment recovery
MTurk labels often have a high error rate. A com-
mon strategy for improving label quality is to ac-
quire multiple labels by different workers for each
example and then consolidate the annotations into
a single label of higher quality. To trade off num-
ber of annotated examples against quality of anno-
tations, we adopt adaptive voting. It uses majority
1548
NER Sentiment
Budget 5820 6931 1130 1756
#train F1 cost/sent w.-accuracy #train F1 #train Acc cost/doc w.-accuracy #train Acc
RS 1 S 5820 59.6 1.00 51.6 ? ? 1130 70.4 1 74.8 ? ?
2 3-v 1624 61.4? 3.58 70.1 ? ? ? ? ? ? ? ?
3 5/4-v 1488 63.0? 3.91 71.6 1774 63.5 450 71.2 2.51 89.6 735 79.2
4 5-v+f 1996 63.6? 2.91 71.8 2385 64.9? ? ? ? ? ? ?
AL 5 S 5820 67.0 1.00 66.5 ? ? 1130 74.8 1 76.0 ? ?
6 3-v 1808 70.0? 3.21 78.8 ? ? ? ? ? ? ? ?
7 5/4-v 1679 70.4? 3.46 79.6 1966 70.6 455 77.4 2.48 89.0 715 81.8
8 5-v+f 2165 70.5 2.68 79.3 2691 71.2 ? ? ? ? ? ?
Table 1: For NER, active learning consistently beats random sampling on MTurk. NER F1 evaluated on
CoNLL test set A. #train = number of sentences in training set, S = single, 3-v = 3-voting, 5/4-voting = 5-
and 4-voting for NER and sentiment resp., +f = using fragments; sentiment budget 1130 for run 1, sentiment
budget 1756 averaged over 2 runs.
voting and is adaptive in the number of repeated an-
notations. For NER, a sentence is first annotated by
two workers. Then majority voting is performed for
each token individually. If there is a majority for ev-
ery token that is greater than an agreement threshold
?, the sentence is accepted with each token labeled
with the majority label. Otherwise additional anno-
tations are requested. A sentence is discarded if the
number of repeated annotations exceeds a discard
threshold d (d-voting).2 We use the same scheme
for sentiment; note that there is just one decision per
HIT in this case, not several as in NER.
For NER, we also use fragment recovery: we sal-
vage tokens with agreeing labels from discarded sen-
tences. We cut the token sequence of a discarded
sentence into several fragments that have agreeing
tokens and discard only those parts that disagree. We
then include these recovered fragments in the train-
ing data just like complete sentences.
Software release. Our active learning framework
used can be downloaded at http://www.ims.
uni-stuttgart.de/?lawsfn/active/.
4 Experiments, Results and Analysis
4.1 Experiments
In our NER experiments, we have workers reanno-
tate the English corpus of the CoNLL-2003 NER
shared task. We chose this corpus to be able to com-
pare crowdsourced annotations with gold standard
2It can take a while in this scheme for annotators to agree
on a final annotation for a sentence. We make tentative labels
of a sentence available to the classifier immediately and replace
them with the final labels once voting is completed.
annotations. A HIT is one sentence and is offered
for a base payment of $0.01. We filtered out answers
that contained unannotated tokens or were obvious
spam (e.g., all tokens labeled as MISC). For test-
ing NER performance, we used a system based on
conditional random fields with standard named en-
tity features including the token itself, orthographic
features like the occurrence of capitalization or spe-
cial characters and context information about the to-
kens to the left/right of the current token.
The sentiment detection task was modeled after a
well-known document analysis setup for sentiment
classification, introduced by Pang et al (2002). We
use their corpus of 1000 positive and 1000 negative
movie reviews and the Stanford maximum entropy
classifier (Manning and Klein, 2003) to predict the
sentiment label of each document d from a unigram
representation of d. We randomly split this corpus
into a test set of 500 reviews and an active learn-
ing pool of 1500 reviews. Each HIT consists of one
document, valued at $0.01.
We compare random sampling (RS) and AL in
combination with the proposed voting and fragment
strategies with different parameters. We want to
avoid rerunning experiments on MTurk over and
over again, but on the other hand, we believe that us-
ing synthetic data for simulations is problematic be-
cause it is difficult to generate synthetic data with a
realistic model of annotator errors. Thus, we logged
a play-by-play record of the annotator interactions
and labels. With this recording, we can then rerun
strategies with different parameters.
We chose voting with at most d = 5 repetitions as
1549
our main reannotation strategy for both random and
active sampling for NER annotation. We use simple
majority voting (? = .5) for NER.
For sentiment, we set d = 4 and minimum agree-
ment ? = .75 because the number of labels is
smaller (2 vs. 5) and so random agreement is more
likely for sentiment.
To get results for 3-voting NER, we take the
recording and discard 5-voting votes not needed in
3-voting. This will result in roughly the same num-
ber of annotated sentences, but at a lower cost. This
simulation of 3-voting is not exactly what would
have happened on MTurk (e.g., the final vote on a
sentence might be different, which then influences
AL example selection), but we will assume that dif-
ferences are rare and simulated and actual results
are similar. The same considerations apply to sin-
gle votes and to the sentiment experiments.
We always compare two strategies for the same
annotation budget. For example, the number of
training sentences in Table 1 differ in the two rel-
evant columns, but all strategies compared use ex-
actly the same annotation budget (5820, 6931, 1130,
and 1756, respectively).
For the single annotation strategy, each interac-
tion record contained only about 40% usable anno-
tations, the rest were repeats. A comparison with
the single annotation strategy over approx. 2000 sen-
tences or 450 documents would not have been mean-
ingful; therefore we chose to run an extra experiment
with the single annotation strategy to match this up
with the budgets of the voting strategies. The re-
sults are presented in two separate columns of Ta-
ble 1 (budgets 6931 and 1756).
4.2 Results
For sentiment detection, worker accuracy or label
quality ? the percentage of correctly annotated doc-
uments ? is 74.8. In contrast, for NER, worker accu-
racy ? the percentage of non-O tokens annotated cor-
rectly ? is only 51.6 (Table 1, line 1). This demon-
strates the challenge of using MTurk for NLP an-
notation tasks. When we use single annotations of
each sentence, NER performance is 59.6 F1 for ran-
dom sampling (line 1). When training with gold la-
bels on the same sentences, the performance is 80.0
(not shown). This means we lose more than 20%
due to poor worker accuracy. Adaptive voting and
fragment recovery manage to recover a small part of
the lost performance (lines 2?4); each of the three
F1 scores is significantly better than the one above
it as indicated by ? (Approximate Randomization
Test (Noreen, 1989; Chinchor et al, 1993) as im-
plemented by Pado? (2006)).
Using AL turns out to be quite successful for NER
performance. For single annotations, NER perfor-
mance is 67.0 (line 5), an improvement of 7.4%
compared to random sampling. Adaptive voting
and fragment recovery again increase worker accu-
racy (lines 6?8) although total improvement of 3.5%
(lines 8 vs. 5) is smaller than 4% for random (lines
4 vs. 1). The learning curves of AL vs. random in
Figure 1 (top left) confirm this good result for AL.
These learning curves are for tokens ? not for sen-
tences ? to show that the reason for AL?s better per-
formance is not that it selects slightly longer sen-
tences than random. In addition, the relative advan-
tage of AL vs random decreases over time, which is
typical of pool-based AL experiments.
We carried out two runs of the same experiment
for sentiment to validate our first positive result since
the difference between the two conditions is not as
large as in NER (Figure 1, top right). After about
300 documents, active learning consistently outper-
forms random sampling. The first AL run performs
better because of higher label quality in the begin-
ning. The overall advantage of AL over random
is lower than for NER because the set of labels is
smaller in sentiment, making the classification task
easier. Second, there is a large amount of simple lex-
ical clues for detecting sentiment (cf. Wilson et al
(2005)). It is likely that some of them can be learned
well through random sampling at first; however, ac-
tive learning can gain accuracy over time because it
selects examples with more difficult clues.
In Figure 1 (bottom), we compare single annota-
tion with adaptive voting. The graphs show F1 as
a function of cost. Adaptive voting trades quantity
of sampled sentences for quality of labels and thus
incurs higher net costs per sentence. This results in
a smaller dataset for a given budget, but this dataset
is still more useful for classifier training. For NER
(Figure 1, bottom left), the single annotation strat-
egy has a faster start; so for small budgets, cover-
ing a somewhat larger portion of the sample space
is beneficial. For larger budgets, however, quality of
1550
0 5000 10000 15000 20000
0.
4
0.
5
0.
6
0.
7
0.
8
Tokens
F?
Sc
or
e
active, 5?voting
random, 5?voting
0 200 400 600
0.
4
0.
5
0.
6
0.
7
0.
8
0.
9
Documents
Ac
cu
ra
cy
active 1
active 2
random 1
random 2
0 1000 2000 3000 4000 5000 6000
0.
4
0.
5
0.
6
0.
7
0.
8
Cost
F?
Sc
or
e
single ann.
3?voting
5?voting
5?voting +frags
0 500 1000 1500 2000
0.
4
0.
5
0.
6
0.
7
0.
8
0.
9
Cost
Ac
cu
ra
cy
single ann.
4?voting
Figure 1: Top: Active learning vs. Random sampling for NER (left) and sentiment (right). Bottom: Active
learning: adaptive voting vs. single annotation for NER (left) and sentiment (right).
the voted labels trumps quantity.
For sentiment (Figure 1, bottom right), results are
similar: voting has no benefit initially, but as find-
ing maximally informative examples to annotate be-
comes harder in later stages of learning, adaptive
voting gains an advantage over single annotations.
The main result of the experiment is that active
learning is better by about 7% F1 than random sam-
pling for NER and by 2.6% accuracy for sentiment
(averaged over two runs at budget 1756). Adaptive
voting further improves AL performance for both
NER and sentiment.
4.3 Annotation time per token
Most AL work assumes constant cost per annotation
unit. This assumption has been questioned because
AL often selects hard examples that take longer to
annotate (Hachey et al, 2005; Settles et al, 2008).
In annotation with MTurk, cost is not a function
of annotation time because workers are paid a fixed
amount per HIT. Nevertheless, annotation time plays
a part in whether workers are willing to work on a
given task for the offered reward. This is particularly
problematic for NER since workers have to examine
each token individually. We therefore investigate
for NER whether the time MTurk workers spend on
annotating sentences differs for random vs. AL.
We first compute median and mean annotation
times and number of tokens per sentence:
sec/sentence tokens/sentence
strategy median mean all required
random 17.2 33.1 15.0 3.4
AL 17.8 33.0 17.7 4.0
We see that most sentences are annotated in a very
short time; but the mean is much larger than the me-
dian because there are outliers of up to eight min-
utes. AL tends to select slightly longer sentences as
1551
0 500 1000 1500 2000
0.
4
0.
5
0.
6
0.
7
0.
8
0.
9
Sentences
F?
Sc
or
e
gold selection, gold labels
MTurk selection, gold labels
MTurk selection, MTurk labels
0 200 400 600
0.
50
0.
60
0.
70
0.
80
Documents
Ac
cu
ra
cy
gold selection, gold labels
MTurk selection, gold labels
MTurk selection, MTurk labels
Figure 3: Performance on gold labels. Left: NER. Right: sentiment (run 1).
0 2 4 6 8 10 13 16 19 23 29
0
10
0
20
0
30
0
40
0
Number of uppercase tokens
An
no
ta
tio
n 
tim
e 
(se
co
nd
s)
random
active
Figure 2: Annotation time vs. # uppercase tokens
well as sentences with slightly more uppercase to-
kens that require annotation.
In a more detailed analysis, we attempt to distin-
guish between (i) the effect of more uppercase (?an-
notation required?) tokens vs. (ii) the effect of ex-
ample difficulty. We fit a linear regression model
to annotation time vs. the number of uppercase to-
kens. For the regression fit, we removed all annota-
tion times > 60 seconds. Such long times indicate
distraction of the worker and are not a reliable mea-
sure of difficulty.
Figure 2 shows the distribution of annotation
times for both cases combined and the fitted models
for each. The model estimated an annotation time of
2.3 secs for each required token for random vs. 2.7
secs for AL. We conclude that the difference in dif-
ficulty between sentences selected by random sam-
pling vs. AL is small, but noticeable.
4.4 Influence of noise on the selection process
While NER performance for AL is much higher than
for random sampling, it is still quite a bit lower than
what is possible on gold labels. In the case of AL,
there are two reasons why this happens: (i) The
noisy labels negatively affect the classifier?s ability
to learn a good model that is used for classifying the
test set. (ii) The noisy labels result in bad interme-
diate models that then select suboptimal examples
to be annotated next. The AL selection process is
?misled? by the noisy examples.
We conduct an experiment to determine the con-
tribution of factors (i) and (ii) to the performance
loss. First, we preserve the sequence of sentences
chosen by our AL experiments on MTurk, with 5-
voting for NER and 4-voting for sentiment but re-
place the noisy worker-provided labels by gold la-
bels. The performance of classifiers trained on this
sequence is the dashed line ?MTurk selection, gold
labels? in Figure 3 for NER (left) and sentiment
(right).
Second, we compare with a traditional simulated
AL experiment with gold labels. Here, the selection
too is controlled by gold labels, so the selection has
a noiseless classifier available for scoring and can
perform optimal uncertainty selection. These are the
1552
1 5 10 50 100 500
0.
0
0.
2
0.
4
0.
6
0.
8
1.
0
Number of sentences
Qu
ali
ty 
(%
 co
rre
ct 
en
tity
 to
ke
n
s)
1 2 5 10 20 50 100 200
0.
0
0.
2
0.
4
0.
6
0.
8
1.
0
Number of documents
Qu
ali
ty 
(%
 co
rre
ct 
do
cu
me
nt 
lab
els
)
Figure 4: Worker accuracy vs. number of HITs. Each point corresponds to one worker (? = active, +
=random sampling; black and grey for different runs). Left: NER. Right: Sentiment.
dotted lines ?gold selection, gold labels? in Figure 3.
We used a batch-mode AL setup for this compari-
son experiment. For a fair comparison, we adjust the
batchsize to be equal to the average staleness of a se-
lected example in concurrent MTurk active learning.
The staleness of an example is defined as the num-
ber of annotations the system has received, but not
yet incorporated in the computation of an example?s
uncertainty score (Haertel et al, 2010).
For our concurrent NER system, the average stal-
eness of an example was about 12 (min: 1, max: 40),
for sentiment it was about 2. The figure for NER is
higher than the number cited by Haertel et al (2010)
because there are more annotators accessing our sys-
tem at the same time via MTurk but not as high for
sentiment since documents are longer and retraining
the sentiment classifier is faster. The average stale-
ness of an example in a batch-mode system is half
the batch size. Thus, we set the batch size of our
comparison system to 25 for NER and to 4 for sen-
timent.
Returning to the two factors introduced above ?
(i) final effect of noise on test set performance vs.
(ii) intermediate effect of noise on example selec-
tion ? we see in Figure 3 that (i) has a large effect
on NER whereas (ii) has a noticeable, but small ef-
fect.3 For example, at 1966 sentences, F1 scores are
3Our comparison unit for NER is the sentence. We can-
not compare on cost here since we do not know what the per-
sentence cost of a ?gold? expert annotation is.
70.6 (MTurk-MTurk), 81.4 (MTurk-gold) and 84.9
(gold-gold). This means that a performance differ-
ence of 10 points F1 has to be attributed to noisy
labels resulting in a worse final classifier (effect i),
and another 3.5 points are lost due to sub-optimal
example selection (effect ii).
For sentiment, the results are different. There is
no clear difference between the three runs. We at-
tribute this to the fact that the quality of the labels
is higher in sentiment than in NER. Our initial ex-
periments on sentiment were all negative (showing
no improvement of AL compared to random) be-
cause label quality was too low. Only after we intro-
duced the template described in Section 3 and used
4-voting with ? = .75 did we get positive results for
AL. This leads to an overall label quality of about
90% (over all runs) which is so high that the differ-
ence to using gold labels is small if present at all.
5 Worker Quality
So far we have assumed that all workers provide
annotations of the same quality. However, this is
not the case. Figure 4 shows plots of worker accu-
racy as a function of worker productivity (number
of annotated examples). Some workers submit only
one or two HITs just to try out the task. For NER,
the majority of workers submit between 5 and 10
sentences, with label qualities between 0.5 and 0.8.
The chance level for correctness is around 0.25 (four
1553
different named entity categories for uppercase to-
kens). For sentiment, most workers submit 1 to 5
documents, with label qualities between 0.5 and 1.
Chance level lies at around 0.5 (for two equally dis-
tributed labels).
While quality for highly productive workers is
mediocre in our experiments, other researchers have
found extremely bad quality for their most prolific
workers (Callison-Burch, 2009). Some of these
workers might be spammers who try to submit an-
swers with automatic scripts. We encountered some
spammers that our heuristics did not detect (shown
in the bottom-right areas of Figure 4, left), but the
voting mechanism was able to mitigate their nega-
tive influence.
Given the large variation in Figure 4, using worker
quality in crowdsourcing for improved training set
creation seems promising. We now test two such
strategies for NER in an oracle setup.
5.1 Blocking low-quality workers
A simple approach is to refuse annotations from
workers that have been determined to provide low
quality answers. We simulated this strategy on NER
data using oracle quality ratings. We chose NER be-
cause of its lower overall label quality. The re-
sults are presented in Figure 5 for random (a) and
AL (b). For random, quality filtering with low cut-
offs helps by removing bad annotations that likely
come from spammers. While the voting strategy
prevented a performance decrease with bad anno-
tations, it needed to expend many extra annotations
for correction. With filtering, these extra annotations
become unnecessary and the system can learn faster.
When low-quality workers are less active, as in the
AL dataset, we find no meaningful performance in-
crease for low cutoffs up to 0.4. For very high cut-
offs (0.7), the beginning of the performance curve
shows that further cost reductions can be achieved.
However, we did not have enough recorded human
annotations available to perform a simulation for the
full budget.
5.2 Trusting high-quality workers
The complementary approach is to take annotations
from highly rated workers at face value and imme-
diately accept them as the correct label, bypassing
the voting procedure. Bypassing saves the cost of
repeated annotation of the same sentence. Figure 5
shows learning curves for two bypass thresholds on
worker quality (measured as proportion of correct
non-O tokens) for random (c) and AL (d). Bypass-
ing performs surprisingly well. We find a steeper
rise of the learning curve, meaning less cost for the
same performance. Not only do we find substantial
cost reductions, but also higher overall performance.
We believe this is because high-quality annotations
can sometimes be voted down by other annotations.
If we can identify high-quality workers and directly
use their annotations, this can be avoided.
These experiments are oracle experiments using
gold data that is normally not available. In future
work, we would like to repeat the experiments using
methods for worker quality estimation (Ipeirotis et
al., 2010; Donmez et al, 2009). For AL, the choice
as to which labels are used (as a result of voting, by-
passing or other) also has an influence on the selec-
tion. However, we had to keep the sequence of the
selected sentences fixed in the simulations reported
above. While our method of sample selection for
AL proved to be quite robust even in the presence
of noise, higher quality labels do have an influence
on the sample selection (see section 4.4), so the im-
provement could be even better than indicated here.
5.3 Differences in quality between AL and
random
The essence of AL is to select examples that are dif-
ficult to classify. As observed in our experiments
on annotation time, this difficulty is reflected in the
amount of time a human needs to work on examples
selected through AL. Another effect to expect from
difficulty could be lower annotation accuracy. We
therefore examined the accuracies for each worker
who contributed to both the AL and the random ex-
periment. We found that in the NER task, the 20
workers in this group had a slightly higher (0.07) av-
erage quality for randomly selected examples. This
difference is low and does not suggest a significant
drop in accuracy for examples selected in AL.
6 Conclusion
We have investigated the use of AL in a real-life
annotation experiment with human annotators in-
stead of traditional simulations with gold labels for
1554
(a) (b) (c) (d)
0 1000 2000 3000 4000
0.
50
0.
55
0.
60
0.
65
0.
70
0.
75
Cost
F?
Sc
or
e
baseline 5?voting
min. quality 0.1
min. quality 0.4
min. quality 0.7
0 1000 2000 3000 4000
0.
50
0.
55
0.
60
0.
65
0.
70
0.
75
Cost
F?
Sc
or
e
baseline 5?voting
min. quality 0.1
min. quality 0.4
min. quality 0.7
0 1000 2000 3000 4000
0.
50
0.
55
0.
60
0.
65
0.
70
0.
75
Cost
F?
Sc
or
e
baseline 5?voting
bypass 0.9
bypass 0.7
0 1000 2000 3000 4000
0.
50
0.
55
0.
60
0.
65
0.
70
0.
75
Cost
F?
Sc
or
e
baseline 5?voting
bypass 0.9
bypass 0.7
Figure 5: Blocking low-quality workers: (a) random, (b) AL. Bypass voting: (c) random, (d) AL.
named entity recognition and sentiment classifica-
tion. The annotation was performed using MTurk in
an AL framework that features concurrent example
selection without wait times. We also evaluated two
strategies, adaptive voting and fragment recovery, to
improve label quality at low additional cost. We find
that even for the relatively high noise levels of anno-
tations gathered with MTurk, AL is successful, im-
proving performance by +6.9 points F1 compared to
random sampling for NER and by +2.6% accuracy
for sentiment. Furthermore, this performance level
is reached at a smaller MTurk cost compared to ran-
dom sampling. Thus AL not only reduces annotation
costs, but also offers an improvement in absolute
performance for these tasks. This is clear evidence
that active learning and crowdsourcing are comple-
mentary methods for lowering annotation cost and
should be used together in training set creation for
natural language processing tasks.
We have also conducted oracle experiments that
show that further performance gains and cost sav-
ings can be achieved by using information about
worker quality. We plan to confirm these results by
using estimates of quality in the future.
7 Acknowledgments
Florian Laws is a recipient of the Google Europe
Fellowship in Natural Language Processing, and
this research is supported in part by his fellowship.
Christian Scheible is supported by the Deutsche
Forschungsgemeinschaft project Sonderforschungs-
bereich 732.
References
Jason Baldridge and Alexis Palmer. 2009. How well
does active learning actually work? Time-based eval-
uation of cost-reduction strategies for language docu-
mentation. In Proceedings of the 2009 Conference on
Empirical Methods in Natural Language Processing,
pages 296?305.
Anthony Brew, Derek Greene, and Pa?draig Cunningham.
2010. Using crowdsourcing and active learning to
track sentiment in online media. In Proceeding of the
2010 conference on ECAI 2010: 19th European Con-
ference on Artificial Intelligence, pages 145?150.
Klaus Brinker. 2003. Incorporating diversity in active
learning with support vector machines. In Proceed-
ings of the Twentieth International Conference on Ma-
chine Learning (ICML 2003), pages 59?66.
Chris Callison-Burch. 2009. Fast, cheap, and cre-
ative: evaluating translation quality using Amazon?s
Mechanical Turk. In Proceedings of the 2009 Confer-
ence on Empirical Methods in Natural Language Pro-
cessing, pages 286?295.
Bob Carpenter and Massimo Poesio. 2010. Models
of data annotation. Tutorial at the seventh interna-
tional conference on Language Resources and Eval-
uation (LREC 2010).
Nancy Chinchor, David D. Lewis, and Lynette
Hirschman. 1993. Evaluating message understanding
systems: an analysis of the third message understand-
ing conference (muc-3). Computational Linguistics,
19(3):409?449.
Pinar Donmez and Jaime G. Carbonell. 2008. Proactive
learning: cost-sensitive active learning with multiple
imperfect oracles. In Proceeding of the 17th ACM con-
ference on Information and knowledge management,
pages 619?628.
Pinar Donmez, Jaime G. Carbonell, and Jeff Schnei-
der. 2009. Efficiently learning the accuracy of la-
1555
beling sources for selective sampling. In Proceedings
of the 15th ACM SIGKDD international conference
on Knowledge discovery and data mining, pages 259?
268.
Tim Finin, William Murnane, Anand Karandikar,
Nicholas Keller, Justin Martineau, and Mark Dredze.
2010. Annotating named entities in twitter data with
crowdsourcing. In Proceedings of the NAACL HLT
2010 Workshop on Creating Speech and Language
Data with Amazon?s Mechanical Turk, pages 80?88.
Ben Hachey, Beatrice Alex, and Markus Becker. 2005.
Investigating the effects of selective sampling on the
annotation task. In CoNLL ?05: Proceedings of the
9th Conference on Computational Natural Language
Learning, pages 144?151.
Robbie Haertel, Paul Felt, Eric K. Ringger, and Kevin
Seppi. 2010. Parallel active learning: Eliminating
wait time with minimal staleness. In Proceedings of
the NAACL HLT 2010 Workshop on Active Learning
for Natural Language Processing, pages 33?41.
Panagiotis G. Ipeirotis, Foster Provost, and Jing Wang.
2010. Quality management on amazon mechanical
turk. In Proceedings of the ACM SIGKDD Workshop
on Human Computation (HCOMP ?10).
Nolan Lawson, Kevin Eustice, Mike Perkowitz, and
Meliha Yetisgen-Yildiz. 2010. Annotating large email
datasets for named entity recognition with mechanical
turk. In Proceedings of the NAACL HLT 2010 Work-
shop on Creating Speech and Language Data with
Amazon?s Mechanical Turk, pages 71?79.
David D. Lewis and William A. Gale. 1994. A sequential
algorithm for training text classifiers. In Proceedings
of the 17th Annual International ACM SIGIR Confer-
ence on Research and Development in Information Re-
trieval, pages 3?12.
Christopher Manning and Dan Klein. 2003. Optimiza-
tion, maxent models, and conditional estimation with-
out magic. In Proceedings of the 2003 Conference
of the North American Chapter of the Association for
Computational Linguistics on Human Language Tech-
nology: Tutorials - Volume 5, pages 8?8.
Eric W. Noreen. 1989. Computer-intensive methods for
testing hypotheses: an introduction. Wiley.
Miles Osborne and Jason Baldridge. 2004. Ensemble-
based active learning for parse selection. In
Daniel Marcu Susan Dumais and Salim Roukos, edi-
tors, HLT-NAACL 2004: Main Proceedings, pages 89?
96.
Sebastian Pado?, 2006. User?s guide to sigf: Signifi-
cance testing by approximate randomisation.
Bo Pang, Lillian Lee, and Shivakumar Vaithyanathan.
2002. Thumbs up? Sentiment classification using
machine learning techniques. In Proceedings of the
2002 Conference on Empirical Methods in Natural
Language Processing (EMNLP), pages 79?86.
Ines Rehbein, Josef Ruppenhofer, and Alexis Palmer.
2010. Bringing active learning to life. In Proceed-
ings of the 23rd International Conference on Compu-
tational Linguistics (Coling 2010), pages 949?957.
Eric Ringger, Peter McClanahan, Robbie Haertel, George
Busby, Marc Carmen, James Carroll, Kevin Seppi, and
Deryle Lonsdale. 2007. Active learning for part-of-
speech tagging: Accelerating corpus annotation. In
Proceedings of the Linguistic Annotation Workshop at
ACL-2007, pages 101?108.
Andrew Schein and Lyle Ungar. 2007. Active learn-
ing for logistic regression: An evaluation. Machine
Learning, 68(3):235?265.
Burr Settles, Mark Craven, and Lewis Friedland. 2008.
Active learning with real annotation costs. In Proceed-
ings of the NIPS Workshop on Cost-Sensitive Learn-
ing, pages 1069?1078.
Rion Snow, Brendan O?Connor, Daniel Jurafsky, and An-
drew Ng. 2008. Cheap and fast ? but is it good?
evaluating non-expert annotations for natural language
tasks. In Proceedings of the 2008 Conference on
Empirical Methods in Natural Language Processing,
pages 254?263.
Erik F. Tjong Kim Sang and Fien De Meulder. 2003.
Introduction to the conll-2003 shared task: language-
independent named entity recognition. In Proceedings
of the seventh conference on Natural language learn-
ing at HLT-NAACL (CoNLL 2003), pages 142?147.
Katrin Tomanek, Joachim Wermter, and Udo Hahn.
2007. An approach to text corpus construction which
cuts annotation costs and maintains reusability of an-
notated data. In Proceedings of the 2007 Joint Confer-
ence on Empirical Methods in Natural Language Pro-
cessing and Computational Natural Language Learn-
ing, pages 486?495.
Simon Tong and Daphne Koller. 2002. Support vec-
tor machine active learning with applications to text
classification. The Journal of Machine Learning Re-
search, 2:45?66.
Robert Voyer, Valerie Nygaard, Will Fitzgerald, and Han-
nah Copperman. 2010. A hybrid model for anno-
tating named entity training corpora. In Proceedings
of the Fourth Linguistic Annotation Workshop, pages
243?246.
Theresa Wilson, Janyce Wiebe, and Paul Hoffmann.
2005. Recognizing contextual polarity in phrase-level
sentiment analysis. In Proceedings of the conference
on Human Language Technology and Empirical Meth-
ods in Natural Language Processing, pages 347?354.
1556
Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 322?332,
Seattle, Washington, USA, 18-21 October 2013. c?2013 Association for Computational Linguistics
Efficient Higher-Order CRFs for Morphological Tagging
Thomas Mu?ller??, Helmut Schmid??, and Hinrich Schu?tze?
?Center for Information and Language Processing, University of Munich, Germany
?Institute for Natural Language Processing , University of Stuttgart, Germany
muellets@cis.lmu.de
Abstract
Training higher-order conditional random
fields is prohibitive for huge tag sets. We
present an approximated conditional random
field using coarse-to-fine decoding and early
updating. We show that our implementation
yields fast and accurate morphological taggers
across six languages with different morpho-
logical properties and that across languages
higher-order models give significant improve-
ments over 1st-order models.
1 Introduction
Conditional Random Fields (CRFs) (Lafferty et al,
2001) are arguably one of the best performing se-
quence prediction models for many Natural Lan-
guage Processing (NLP) tasks. During CRF train-
ing forward-backward computations, a form of dy-
namic programming, dominate the asymptotic run-
time. The training and also decoding times thus
depend polynomially on the size of the tagset and
exponentially on the order of the CRF. This prob-
ably explains why CRFs, despite their outstanding
accuracy, normally only are applied to tasks with
small tagsets such as Named Entity Recognition and
Chunking; if they are applied to tasks with bigger
tagsets ? e.g., to part-of-speech (POS) tagging for
English ? then they generally are used as 1st-order
models.
In this paper, we demonstrate that fast and accu-
rate CRF training and tagging is possible for large
tagsets of even thousands of tags by approximat-
ing the CRF objective function using coarse-to-fine
decoding (Charniak and Johnson, 2005; Rush and
Petrov, 2012). Our pruned CRF (PCRF) model has
much smaller runtime than higher-order CRF mod-
els and may thus lead to an even broader application
of CRFs across NLP tagging tasks.
We use POS tagging and combined POS and
morphological (POS+MORPH) tagging to demon-
strate the properties and benefits of our approach.
POS+MORPH disambiguation is an important pre-
processing step for syntactic parsing. It is
usually tackled by applying sequence prediction.
POS+MORPH tagging is also a good example of a
task where CRFs are rarely applied as the tagsets are
often so big that even 1st-order dynamic program-
ming is too expensive. A workaround is to restrict
the possible tag candidates per position by using ei-
ther morphological analyzers (MAs), dictionaries or
heuristics (Hajic?, 2000). In this paper, however we
show that when using pruning (i.e., PCRFs), CRFs
can be trained in reasonable time, which makes hard
constraints unnecessary.
In this paper, we run successful experiments on
six languages with different morphological prop-
erties; we interpret this as evidence that our ap-
proach is a general solution to the problem of
POS+MORPH tagging. The tagsets in our experi-
ments range from small sizes of 12 to large sizes of
up to 1811. We will see that even for the smallest
tagset, PCRFs need only 40% of the training time of
standard CRFs. For the bigger tagset sizes we can
reduce training times from several days to several
hours. We will also show that training higher-order
PCRF models takes only several minutes longer than
training 1st-order models and ? depending on the
language ? may lead to substantial accuracy im-
322
Language Sentences Tokens POS MORPH POS+MORPH OOV
Tags Tags Tags Rate
ar (Arabic) 15,760 614,050 38 516 516 4.58%
cs (Czech) 38,727 652,544 12 1,811 1,811 8.58%
en (English) 38,219 912,344 45 45 3.34%
es (Spanish) 14,329 427,442 12 264 303 6.47%
de (German) 40,472 719,530 54 255 681 7.64%
hu (Hungarian) 61,034 1,116,722 57 1,028 1,071 10.71%
Table 1: Training set statistics. Out-Of-Vocabulary (OOV) rate is regarding the development sets.
provements. For example in German POS+MORPH
tagging, a 1st-order model (trained in 32 minutes)
achieves an accuracy of 88.96 while a 3rd-order
model (trained in 35 minutes) achieves an accuracy
of 90.60.
The remainder of the paper is structured as fol-
lows: Section 2 describes our CRF implementa-
tion1 and the feature set used. Section 3 sum-
marizes related work on tagging with CRFs, effi-
cient CRF tagging and coarse-to-fine decoding. Sec-
tion 4 describes experiments on POS tagging and
POS+MORPH tagging and Section 5 summarizes
the main contributions of the paper.
2 Methodology
2.1 Standard CRF Training
In a standard CRF we model our sentences using a
globally normalized log-linear model. The proba-
bility of a tag sequence ~y given a sentence ~x is then
given as:
p(~y|~x) =
exp
?
t,i ?i ? ?i(~y, ~x, t)
Z(~?, ~x)
Z(~?, ~x) =
?
~y
exp
?
t,i
?i ? ?i(~y, ~x, t)
Where t and i are token and feature indexes, ?i is
a feature function, ?i is a feature weight and Z is a
normalization constant. During training the feature
weights ? are set to maximize the conditional log-
likelihood of the training data D:
1Our java implementation MarMoT is available at
https://code.google.com/p/cistern/
llD(~?) =
?
(~x,~y)?D
log p(~y|~x,~?)
In order to use numerical optimization we have to
calculate the gradient of the log-likelihood, which is
a vector of partial derivatives ?llD(~?)/??i. For a
training sentence ~x, ~y and a token index t the deriva-
tive wrt feature i is given by:
?i(~y, ~x, t)?
?
~y?
?i(~y
?, ~x, t) p(~y?|~x,~?)
This is the difference between the empirical fea-
ture count in the training data and the estimated
count in the current model ~?. For a 1st-order model,
we can replace the expensive sum over all possible
tag sequences ~y? by a sum over all pairs of tags:
?i(yt, yt+1, ~x, t)?
?
y,y?
?i(y, y
?, ~x, t) p(y, y?|~x,~?)
The probability of a tag pair p(y, y?|~x,~?) can then
be calculated efficiently using the forward-backward
algorithm. If we further reduce the complexity of the
model to a 0-order model, we obtain simple maxi-
mum entropy model updates:
?i(yt, ~x, t)?
?
y
?i(y, ~x, t) p(y|~x,~?)
2.2 Pruned CRF Training
As we discussed in the introduction, we want to de-
code sentences by applying a variant of coarse-to-
fine tagging. Naively, to later tag with nth-order
323
accuracy we would train a series of n CRFs of in-
creasing order. We would then use the CRF of order
n ? 1 to restrict the input of the CRF of order n.
In this paper we approximate this approach, but do
so while training only one integrated model. This
way we can save both memory (by sharing feature
weights between different models) and training time
(by saving lower-order updates).
The main idea of our approach is to create increas-
ingly complex lattices and to filter candidate states
at every step to prevent a polynomial increase in lat-
tice size. The first step is to create a 0-order lat-
tice, which as discussed above, is identical to a se-
ries of independent local maximum entropy models
p(y|~x, t). The models base their prediction on the
current word xt and the immediate lexical context.
We then calculate the posterior probabilities and re-
move states y with p(y|~x, t) < ?0 from the lattice,
where ?0 is a parameter. The resulting reduced lat-
tice is similar to what we would obtain using candi-
date selection based on an MA.
We can now create a first order lattice by adding
transitions to the pruned lattice and pruning with
threshold ?1. The only difference to 0-order prun-
ing is that we now have to run forward-backward
to calculate the probabilities p(y|~x, t). Note that in
theory we could also apply the pruning to transition
probabilities of the form p(y, y?|~x, t); however, this
does not seem to yield more accurate models and is
less efficient than state pruning.
For higher-order lattices we merge pairs of states
into new states, add transitions and prune with
threshold ?i.
We train the model using l1-regularized Stochas-
tic Gradient Descent (SGD) (Tsuruoka et al, 2009).
We would like to create a cascade of increasingly
complex lattices and update the weight vector with
the gradient of the last lattice. The updates, how-
ever, are undefined if the gold sequence is pruned
from the lattice. A solution would be to simply rein-
sert the gold sequence, but this yields poor results
as the model never learns to keep the gold sequence
in the lower-order lattices. As an alternative we per-
form the gradient update with the highest lattice still
containing the gold sequence. This approach is sim-
ilar to ?early updating? (Collins and Roark, 2004)
in perceptron learning, where during beam search
an update with the highest scoring partial hypothe-
1: function GETSUMLATTICE(sentence, ~? )
2: gold-tags? getTags(sentence)
3: candidates? getAllCandidates(sentence)
4: lattice? ZeroOrderLattice(candidates)
5: for i = 1? n do
6: candidates? lattice. prune(?i?1)
7: if gold-tags 6? candidates then
8: return lattice
9: end if
10: if i > 1 then
11: candidates? mergeStates(candidates)
12: end if
13: candidates? addTransitions(candidates)
14: lattice? SequenceLattice(candidates, i)
15: end for
16: return lattice
17: end function
Figure 1: Lattice generation during training
sis is performed whenever the gold candidate falls
out of the beam. Intuitively, we are trying to opti-
mize an nth-order CRF objective function, but ap-
ply small lower-order corrections to the weight vec-
tor when necessary to keep the gold candidate in the
lattice. Figure 1 illustrates the lattice generation pro-
cess. The lattice generation during decoding is iden-
tical, except that we always return a lattice of the
highest order n.
The savings in training time of this integrated ap-
proach are large; e.g., training a maximum entropy
model over a tagset of roughly 1800 tags and more
than half a million instances is slow as we have to
apply 1800 weight vector updates for every token
in the training set and every SGD iteration. In the
integrated model we only have to apply 1800 up-
dates when we lose the gold sequence during fil-
tering. Thus, in our implementation training a 0-
order model for Czech takes roughly twice as long
as training a 1st-order model.
2.3 Threshold Estimation
Our approach would not work if we were to set the
parameters ?i to fixed predetermined values; e.g.,
the ?i depend on the size of the tagset and should
be adapted during training as we start the training
with a uniform model that becomes more specific.
We therefore set the ?i by specifying ?i, the average
number of tags per position that should remain in
the lattice after pruning. This also guarantees sta-
ble lattice sizes and thus stable training times. We
324
achieve stable average number of tags per position
by setting the ?i dynamically during training: we
measure the real average number of candidates per
position ??i and apply corrections after processing a
certain fraction of the sentences of the training set.
The updates are of the form:
?i =
{
+0.1 ? ?i if ??i < ?i
?0.1 ? ?i if ??i > ?i
Figure 2 shows an example training run for Ger-
man with ?0 = 4. Here the 0-order lattice reduces
the number of tags per position from 681 to 4 losing
roughly 15% of the gold sequences of the develop-
ment set, which means that for 85% of the sentences
the correct candidate is still in the lattice. This cor-
responds to more than 99% of the tokens. We can
also see that after two iterations only a very small
number of 0-order updates have to be performed.
2.4 Tag Decomposition
As we discussed before for the very large
POS+MORPH tagsets, most of the decoding time is
spent on the 0-order level. To decrease the number
of tag candidates in the 0-order model, we decode in
two steps by separating the fully specified tag into a
coarse-grained part-of-speech (POS) tag and a fine-
grained MORPH tag containing the morphological
features. We then first build a lattice over POS can-
didates and apply our pruning strategy. In a second
step we expand the remaining POS tags into all the
combinations with MORPH tags that were seen in
the training set. We thus build a sequence of lattices
of both increasing order and increasing tag complex-
ity.
2.5 Feature Set
We use the features of Ratnaparkhi (1996) and Man-
ning (2011): the current, preceding and succeed-
ing words as unigrams and bigrams and for rare
words prefixes and suffixes up to length 10, and
the occurrence of capital characters, digits and spe-
cial characters. We define a rare word as a word
with training set frequency ? 10. We concate-
nate every feature with the POS and MORPH tag
and every morphological feature. E.g., for the word
?der?, the POS tag art (article) and the MORPH
tag gen|sg|fem (genitive, singular, feminine) we
 0
 0.05
 0.1
 0.15
 0.2
 0  1  2  3  4  5  6  7  8  9  10
Unre
acha
ble g
old c
and
idat
es
Epochs
traindev
Figure 2: Example training run of a pruned 1st-order
model on German showing the fraction of pruned gold se-
quences (= sentences) during training for training (train)
and development sets (dev).
get the following features for the current word tem-
plate: der+art, der+gen|sg|fem, der+gen,
der+sg and der+fem.
We also use an additional binary feature, which
indicates whether the current word has been seen
with the current tag or ? if the word is rare ? whether
the tag is in a set of open tag classes. The open tag
classes are estimated by 10-fold cross validation on
the training set: We first use the folds to estimate
how often a tag is seen with an unknown word. We
then consider tags with a relative frequency ? 10?4
as open tag classes. While this is a heuristic, it is
safer to use a ?soft? heuristic as a feature in the lat-
tice than a hard constraint.
For some experiments we also use the output of a
morphological analyzer (MA). In that case we sim-
ply use every analysis of the MA as a simple nom-
inal feature. This approach is attractive because it
does not require the output of the MA and the an-
notation of the treebank to be identical; in fact, it
can even be used if treebank annotation and MA use
completely different features.
Because the weight vector dimensionality is high
for large tagsets and productive languages, we use a
hash kernel (Shi et al, 2009) to keep the dimension-
ality constant.
3 Related Work
Smith et al (2005) use CRFs for POS+MORPH tag-
ging, but use a morphological analyzer for candidate
selection. They report training times of several days
325
and that they had to use simplified models for Czech.
Several methods have been proposed to reduce
CRF training times. Stochastic gradient descent can
be applied to reduce the training time by a factor of 5
(Tsuruoka et al, 2009) and without drastic losses in
accuracy. Lavergne et al (2010) make use of feature
sparsity to significantly speed up training for mod-
erate tagset sizes (< 100) and huge feature spaces.
It is unclear if their approach would also work for
huge tag sets (> 1000).
Coarse-to-fine decoding has been successfully ap-
plied to CYK parsing where full dynamic program-
ming is often intractable when big grammars are
used (Charniak and Johnson, 2005). Weiss and
Taskar (2010) develop cascades of models of in-
creasing complexity in a framework based on per-
ceptron learning and an explicit trade-off between
accuracy and efficiency.
Kaji et al (2010) propose a modified Viterbi algo-
rithm that is still optimal but depending on task and
especially for big tag sets might be several orders of
magnitude faster. While their algorithm can be used
to produce fast decoders, there is no such modifica-
tion for the forward-backward algorithm used during
CRF training.
4 Experiments
We run POS+MORPH tagging experiments on Ara-
bic (ar), Czech (cs), Spanish (es), German (de) and
Hungarian (hu). The following table shows the type-
token (T/T) ratio, the average number of tags of ev-
ery word form that occurs more than once in the
training set (A) and the number of tags of the most
ambiguous word form (A?):
T/T A A?
ar 0.06 2.06 17
cs 0.13 1.64 23
es 0.09 1.14 9
de 0.11 2.15 44
hu 0.11 1.11 10
Arabic is a Semitic language with nonconcate-
native morphology. An additional difficulty is that
vowels are often not written in Arabic script. This
introduces a high number of ambiguities; on the
other hand it reduces the type-token ratio, which
generally makes learning easier. In this paper, we
work with the transliteration of Arabic provided in
the Penn Arabic Treebank. Czech is a highly inflect-
ing Slavic language with a large number of morpho-
logical features. Spanish is a Romance language.
Based on the statistics above we can see that it has
few POS+MORPH ambiguities. It is also the lan-
guage with the smallest tagset and the only language
in our setup that ? with a few exceptions ? does not
mark case. German is a Germanic language and ?
based on the statistics above ? the language with
the most ambiguous morphology. The reason is that
it only has a small number of inflectional suffixes.
The total number of nominal inflectional suffixes for
example is five. A good example for a highly am-
biguous suffix is ?en?, which is a marker for infini-
tive verb forms, for the 1st and 3rd person plural and
for the polite 2nd person singular. Additionally, it
marks plural nouns of all cases and singular nouns
in genitive, dative and accusative case.
Hungarian is a Finno-Ugric language with an ag-
glutinative morphology; this results in a high type-
token ratio, but also the lowest level of word form
ambiguity among the selected languages.
POS tagging experiments are run on all the lan-
guages above and also on English.
4.1 Resources
For Arabic we use the Penn Arabic Tree-
bank (Maamouri et al, 2004), parts 1?3 in
their latest versions (LDC2010T08, LDC2010T13,
LDC2011T09). As training set we use parts 1 and 2
and part 3 up to section ANN20020815.0083. All
consecutive sections up to ANN20021015.0096
are used as development set and the remainder as
test set. We use the unvocalized and pretokenized
transliterations as input. For Czech and Spanish,
we use the CoNLL 2009 data sets (Hajic? et al,
2009); for German, the TIGER treebank (Brants et
al., 2002) with the split from Fraser et al (2013);
for Hungarian, the Szeged treebank (Csendes et al,
2005) with the split from Farkas et al (2012). For
English we use the Penn Treebank (Marcus et al,
1993) with the split from Toutanova et al (2003).
We also compute the possible POS+MORPH tags
for every word using MAs. For Arabic we use the
AraMorph reimplementation of Buckwalter (2002),
for Czech the ?free? morphology (Hajic?, 2001), for
Spanish Freeling (Padro? and Stanilovsky, 2012), for
German DMOR (Schiller, 1995) and for Hungarian
326
Magyarlanc 2.0 (Zsibrita et al, 2013).
4.2 Setup
To compare the training and decoding times we run
all experiments on the same test machine, which fea-
tures two Hexa-Core Intel Xeon X5680 CPUs with
3,33 GHz and 6 cores each and 144 GB of mem-
ory. The baseline tagger and our PCRF implemen-
tation are run single threaded.2 The taggers are im-
plemented in different programming languages and
with different degrees of optimization; still, the run
times are indicative of comparative performance to
be expected in practice.
Our Java implementation is always run with 10
SGD iterations and a regularization parameter of
0.1, which for German was the optimal value out of
{0, 0.01, 0.1, 1.0}. We follow Tsuruoka et al (2009)
in our implementation of SGD and shuffle the train-
ing set between epochs. All numbers shown are av-
erages over 5 independent runs. Where not noted
otherwise, we use ?0 = 4, ?1 = 2 and ?2 = 1.5.
We found that higher values do not consistently in-
crease performance on the development set, but re-
sult in much higher training times.
4.3 POS Experiments
In a first experiment we evaluate the speed and ac-
curacy of CRFs and PCRFs on the POS tagsets.
As shown in Table 1 the tagset sizes range from
12 for Czech and Spanish to 54 and 57 for Ger-
man and Hungarian, with Arabic (38) and English
(45) in between. The results of our experiments are
given in Table 2. For the 1st-order models, we ob-
serve speed-ups in training time from 2.3 to 31 at no
loss in accuracy. For all languages, training pruned
higher-order models is faster than training unpruned
1st-order models and yields more accurate models.
Accuracy improvements range from 0.08 for Hun-
garian to 0.25 for German. We can conclude that
for small and medium tagset sizes PCRFs give sub-
stantial improvements in both training and decod-
ing speed3 and thus allow for higher-order tagging,
2Our tagger might actually use more than one core because
the Java garbage collection is run in parallel.
3Decoding speeds are provided in an appendix submitted
separately.
which for all languages leads to significant4 accu-
racy improvements.
4.4 POS+MORPH Oracle Experiments
Ideally, for the full POS+MORPH tagset we would
also compare our results to an unpruned CRF, but
our implementation turned out to be too slow to do
the required number of experiments. For German,
the model processed ? 0.1 sentences per second
during training; so running 10 SGD iterations on
the 40,472 sentences would take more than a month.
We therefore compare our model against models that
perform oracle pruning, which means we perform
standard pruning, but always keep the gold candi-
date in the lattice. The oracle pruning is applied dur-
ing training and testing on the development set. The
oracle model performance is thus an upper bound for
the performance of an unpruned CRF.
The most interesting pruning step happens at the
0-order level when we reduce from hundreds of can-
didates to just a couple. Table 3 shows the results for
1st-order CRFs.
We can roughly group the five languages into
three groups: for Spanish and Hungarian the dam-
age is negligible, for Arabic we see a small decrease
of 0.07 and only for Czech and German we observe
considerable differences of 0.14 and 0.37. Surpris-
ingly, doubling the number of candidates per posi-
tion does not lead to significant improvements.
We can conclude that except for Czech and Ger-
man losses due to pruning are insignificant.
4.5 POS+MORPH Higher-Order Experiments
One argument for PCRFs is that while they might
be less accurate than standard CRFs they allow to
train higher-order models, which in turn might be
more accurate than their standard lower-order coun-
terparts. In this section, we investigate how big the
improvements of higher-order models are. The re-
sults are given in the following table:
n ar cs es de hu
1 90.90 92.45 97.95 88.96 96.47
2 91.86* 93.06* 98.01 90.27* 96.57*
3 91.88* 92.97* 97.87 90.60* 96.50
4Throughout the paper we establish significance by running
approximate randomization tests on sentences (Yeh, 2000).
327
ar cs es de hu en
n TT ACC TT ACC TT ACC TT ACC TT ACC TT ACC
CRF 1 106 96.21 10 98.95 7 98.51 234 97.69 374 97.63 154 97.05
PCRF 1 5 96.21 4 98.96 3 98.52 7 97.70 12 97.64 5 97.07
PCRF 2 6 96.43* 5 99.01* 3 98.65* 9 97.91* 13 97.71* 6 97.21*
PCRF 3 6 96.43* 6 99.03* 4 98.66* 9 97.94* 14 97.69 6 97.19*
Table 2: POS tagging experiments with pruned and unpruned CRFs with different orders n. For every language the
training time in minutes (TT) and the POS accuracy (ACC) are given. * indicates models significantly better than CRF
(first line).
ar cs es de hu
1 Oracle ?0 = 4 90.97 92.59 97.91 89.33 96.48
2 Model ?0 = 4 90.90 92.45* 97.95 88.96* 96.47
3 Model ?0 = 8 90.89 92.48* 97.94 88.94* 96.47
Table 3: Accuracies for models with and without oracle pruning. * indicates models significantly worse than the oracle
model.
We see that 2nd-order models give improvements for
all languages. For Spanish and Hungarian we see
minor improvements ? 0.1.
For Czech we see a moderate improvement of
0.61 and for Arabic and German we observe sub-
stantial improvements of 0.96 and 1.31. An analysis
on the development set revealed that for all three lan-
guages, case is the morphological feature that bene-
fits most from higher-order models. A possible ex-
planation is that case has a high correlation with syn-
tactic relations and is thus affected by long-distance
dependencies.
German is the only language where fourgram
models give an additional improvement over trigram
models. The reason seem to be sentences with long-
range dependencies, e.g., ?Die Rebellen haben kein
Lo?segeld verlangt? (The rebels have not demanded
any ransom); ?verlangt? (demanded) is a past partic-
ple that is separated from the auxilary verb ?haben?
(have). The 2nd-order model does not consider
enough context and misclassifies ?verlangt? as a fi-
nite verb form, while the 3rd-order model tags it cor-
rectly.
We can also conclude that the improvements for
higher-order models are always higher than the loss
we estimated in the oracle experiments. More pre-
cisely we see that if a language has a low number of
word form ambiguities (e.g., Hungarian) we observe
a small loss during 0-order pruning but we also have
to expect less of an improvement when increasing
the order of the model. For languages with a high
number of word form ambiguities (e.g., German) we
must anticipate some loss during 0-order pruning,
but we also see substantial benefits for higher-order
models.
Surprisingly, we found that higher-order PCRF
models can also avoid the pruning errors of lower-
order models. Here is an example from the German
data. The word ?Januar? (January) is ambiguous: in
the training set, it occurs 108 times as dative, 9 times
as accusative and only 5 times as nominative. The
development set contains 48 nominative instances of
?Januar? in datelines at the end of news articles, e.g.,
?TEL AVIV, 3. Januar?. For these 48 occurrences,
(i) the oracle model in Table 3 selects the correct
case nominative, (ii) the 1st-order PCRF model se-
lects the incorrect case accusative, and (iii) the 2nd-
and 3rd-order models select ? unlike the 1st-order
model ? the correct case nominative. Our interpreta-
tion is that the correct nominative reading is pruned
from the 0-order lattice. However, the higher-order
models can put less weight on 0-order features as
they have access to more context to disambiguate the
sequence. The lower weights of order-0 result in a
more uniform posterior distribution and the nomina-
tive reading is not pruned from the lattice.
4.6 Experiments with Morph. Analyzers
In this section we compare the improvements of
higher-order models when used with MAs. The re-
328
ar cs es de hu en
TT ACC TT ACC TT ACC TT ACC TT ACC TT ACC
SVMTool 178 96.39 935 98.94 64 98.42 899 97.29 2653 97.42 253 97.09
Morfette 9 95.91 6 99.00 3 98.43 16 97.28 30 97.53 17 96.85
CRFSuite 4 96.20 2 99.02 2 98.40 8 97.57 15 97.48 8 96.80
Stanford 29 95.98 8 99.08 7 98.53 51 97.70 40 97.53 65 97.24
PCRF 1 5 96.21* 4 98.96* 3 98.52 7 97.70 12 97.64* 5 97.07*
PCRF 2 6 96.43 5 99.01* 3 98.65* 9 97.91* 13 97.71* 6 97.21
PCRF 3 6 96.43 6 99.03 4 98.66* 9 97.94* 14 97.69* 6 97.19
Table 4: Development results for POS tagging. Given are training times in minutes (TT) and accuracies (ACC).
Best baseline results are underlined and the overall best results bold. * indicates a significant difference (positive or
negative) between the best baseline and a PCRF model.
ar cs es de hu en
SVMTool 96.19 98.82 98.44 96.44 97.32 97.12
Morfette 95.55 98.91 98.41 96.68 97.28 96.89
CRFSuite 95.97 98.91 98.40 96.82 97.32 96.94
Stanford 95.75 98.99 98.50 97.09 97.32 97.28
PCRF 1 96.03* 98.83* 98.46 97.11 97.44* 97.09*
PCRF 2 96.11 98.88* 98.66* 97.36* 97.50* 97.23
PCRF 3 96.14 98.87* 98.66* 97.44* 97.49* 97.19*
Table 5: Test results for POS tagging. Best baseline results are underlined and the overall best results bold. * indicates
a significant difference between the best baseline and a PCRF model.
ar cs es de hu
TT ACC TT ACC TT ACC TT ACC TT ACC
SVMTool 454 89.91 2454 89.91 64 97.63 1649 85.98 3697 95.61
RFTagger 4 89.09 3 90.38 1 97.44 5 87.10 10 95.06
Morfette 132 89.97 539 90.37 63 97.71 286 85.90 540 95.99
CRFSuite 309 89.33 9274 91.10 69 97.53 1295 87.78 5467 95.95
PCRF 1 22 90.90* 301 92.45* 25 97.95* 32 88.96* 230 96.47*
PCRF 2 26 91.86* 318 93.06* 32 98.01* 37 90.27* 242 96.57*
PCRF 3 26 91.88* 318 92.97* 35 97.87* 37 90.60* 241 96.50*
Table 6: Development results for POS+MORPH tagging. Given are training times in minutes (TT) and accuracies
(ACC). Best baseline results are underlined and the overall best results bold. * indicates a significant difference
between the best baseline and a PCRF model.
ar cs es de hu
SVMTool 89.58 89.62 97.56 83.42 95.57
RFTagger 88.76 90.43 97.35 84.28 94.99
Morfette 89.62 90.01 97.58 83.48 95.79
CRFSuite 89.05 90.97 97.60 85.68 95.82
PCRF 1 90.32* 92.31* 97.82* 86.92* 96.22*
PCRF 2 91.29* 92.94* 97.93* 88.48* 96.34*
PCRF 3 91.22* 92.99* 97.82* 88.58* 96.29*
Table 7: Test results for POS+MORPH tagging. Best baseline results are underlined and the overall best results bold.
* indicates a significant difference between the best baseline and a PCRF model.
329
sults are given in the following table:
n ar cs es de hu
1 90.90? 92.45? 97.95? 88.96? 96.47?
2 91.86+ 93.06 98.01? 90.27+ 96.57?
3 91.88+ 92.97? 97.87? 90.60+ 96.50?
MA 1 91.22 93.21 98.27 89.82 97.28
MA 2 92.16+ 93.87+ 98.37+ 91.31+ 97.51+
MA 3 92.14+ 93.88+ 98.28 91.65+ 97.48+
Plus and minus indicate models that are signif-
icantly better or worse than MA1. We can see
that the improvements due to higher-order models
are orthogonal to the improvements due to MAs
for all languages. This was to be expected as
MAs provide additional lexical knowledge while
higher-order models provide additional information
about the context. For Arabic and German the
improvements of higher-order models are bigger
than the improvements due to MAs.
4.7 Comparison with Baselines
We use the following baselines: SVMTool
(Gime?nez and Ma`rquez, 2004), an SVM-based dis-
criminative tagger; RFTagger (Schmid and Laws,
2008), an n-gram Hidden Markov Model (HMM)
tagger developed for POS+MORPH tagging; Mor-
fette (Chrupa?a et al, 2008), an averaged percep-
tron with beam search decoder; CRFSuite (Okazaki,
2007), a fast CRF implementation; and the Stanford
Tagger (Toutanova et al, 2003), a bidirectional Max-
imum Entropy Markov Model. For POS+MORPH
tagging, all baselines are trained on the concatena-
tion of POS tag and MORPH tag. We run SVM-
Tool with the standard feature set and the optimal
c-values ? {0.1, 1, 10}. Morfette is run with the de-
fault options. For CRFSuite we use l2-regularized
SGD training. We use the optimal regularization pa-
rameter ? {0.01, 0.1, 1.0} and stop after 30 itera-
tions where we reach a relative improvement in reg-
ularized likelihood of at most 0.01 for all languages.
The feature set is identical to our model except for
some restrictions: we only use concatenations with
the full tag and we do not use the binary feature that
indicates whether a word-tag combination has been
observed. We also had to restrict the combinations
of tag and features to those observed in the training
set5. Otherwise the memory requirements would ex-
ceed the memory of our test machine (144 GB) for
Czech and Hungarian. The Stanford Tagger is used
5We set the CRFSuite option possible states = 0
as a bidirectional 2nd-order model and trained us-
ing OWL-BFGS. For Arabic, German and English
we use the language specific feature sets and for the
other languages the English feature set.
Development set results for POS tagging are
shown in Table 4. We can observe that Morfette,
CRFSuite and the PCRF models for different orders
have training times in the same order of magnitude.
For Arabic, Czech and English, the PCRF accuracy
is comparable to the best baseline models. For the
other languages we see improvements of 0.13 for
Spanish, 0.18 for Hungarian and 0.24 for German.
Evaluation on the test set confirms these results, see
Table 5.6
The POS+MORPH tagging development set re-
sults are presented in Table 6. Morfette is the fastest
discriminative baseline tagger. In comparison with
Morfette the speed up for 3rd-order PCRFs lies be-
tween 1.7 for Czech and 5 for Arabic. Morfette
gives the best baseline results for Arabic, Spanish
and Hungarian and CRFSuite for Czech and Ger-
man. The accuracy improvements of the best PCRF
models over the best baseline models range from
0.27 for Spanish over 0.58 for Hungarian, 1.91 for
Arabic, 1.96 for Czech to 2.82 for German. The test
set experiments in Table 7 confirm these results.
5 Conclusion
We presented the pruned CRF (PCRF) model for
very large tagsets. The model is based on coarse-to-
fine decoding and stochastic gradient descent train-
ing with early updating. We showed that for mod-
erate tagset sizes of ? 50, the model gives signif-
icant speed-ups over a standard CRF with negligi-
ble losses in accuracy. Furthermore, we showed that
training and tagging for approximated trigram and
fourgram models is still faster than standard 1st-
order tagging, but yields significant improvements
in accuracy.
In oracle experiments with POS+MORPH tagsets
we demonstrated that the losses due to our approx-
imation depend on the word level ambiguity of the
respective language and are moderate (? 0.14) ex-
cept for German where we observed a loss of 0.37.
6Gime?nez and Ma`rquez (2004) report an accuracy of 97.16
instead of 97.12 for SVMTool for English and Manning (2011)
an accuracy of 97.29 instead of 97.28 for the Stanford tagger.
330
We also showed that higher order tagging ? which
is prohibitive for standard CRF implementations ?
yields significant improvements over unpruned 1st-
order models. Analogous to the oracle experiments
we observed big improvements for languages with a
high level of POS+MORPH ambiguity such as Ger-
man and smaller improvements for languages with
less ambiguity such as Hungarian and Spanish.
Acknowledgments
The first author is a recipient of the Google Europe
Fellowship in Natural Language Processing, and this
research is supported in part by this Google Fellow-
ship. This research was also funded by DFG (grant
SFB 732).
References
Sabine Brants, Stefanie Dipper, Silvia Hansen, Wolfgang
Lezius, and George Smith. 2002. The TIGER tree-
bank. In Proceedings of the workshop on treebanks
and linguistic theories.
Tim Buckwalter. 2002. Buckwalter Arabic Morpholog-
ical Analyzer Version 1.0. Linguistic Data Consor-
tium, University of Pennsylvania, 2002. LDC Catalog
No.: LDC2002L49.
Eugene Charniak and Mark Johnson. 2005. Coarse-to-
fine n-best parsing and MaxEnt discriminative rerank-
ing. In Proceedings of ACL.
Grzegorz Chrupa?a, Georgiana Dinu, and Josef van Gen-
abith. 2008. Learning morphology with Morfette. In
Proceedings of LREC.
Michael Collins and Brian Roark. 2004. Incremental
parsing with the perceptron algorithm. In Proceedings
of ACL.
Michael Collins. 2002. Discriminative training meth-
ods for hidden Markov models: Theory and experi-
ments with perceptron algorithms. In Proceedings of
EMNLP.
Do?ra Csendes, Ja?nos Csirik, Tibor Gyimo?thy, and Andra?s
Kocsor. 2005. The Szeged treebank. In Proceedings
of Text, Speech and Dialogue.
Richa?rd Farkas, Veronika Vincze, and Helmut Schmid.
2012. Dependency parsing of Hungarian: Baseline re-
sults and challenges. In Proceedings of EACL.
Alexander Fraser, Helmut Schmid, Richa?rd Farkas, Ren-
jing Wang, and Hinrich Schu?tze. 2013. Knowl-
edge Sources for Constituent Parsing of German, a
Morphologically Rich and Less-Configurational Lan-
guage. Computational Linguistics.
Jesu?s Gime?nez and Lluis Ma`rquez. 2004. Svmtool: A
general POS tagger generator based on Support Vector
Machines. In Proceedings of LREC.
Jan Hajic?, Massimiliano Ciaramita, Richard Johans-
son, Daisuke Kawahara, Maria Anto`nia Mart??, Llu??s
Ma`rquez, Adam Meyers, Joakim Nivre, Sebastian
Pado?, Jan S?te?pa?nek, et al 2009. The CoNLL-2009
shared task: Syntactic and semantic dependencies in
multiple languages. In Proceedings of CoNLL.
Jan Hajic?. 2000. Morphological tagging: Data vs. dictio-
naries. In Proceedings of NAACL.
Jan Hajic?. 2001. Czech ?Free? Morphology. URL
http://ufal.mff.cuni.cz/pdt/Morphology and Tagging.
Nobuhiro Kaji, Yasuhiro Fujiwara, Naoki Yoshinaga, and
Masaru Kitsuregawa. 2010. Efficient staggered de-
coding for sequence labeling. In Proceedings of ACL.
John Lafferty, Andrew McCallum, and Fernando Pereira.
2001. Conditional random fields: Probabilistic mod-
els for segmenting and labeling sequence data. In Pro-
ceedings of ICML.
Thomas Lavergne, Olivier Cappe?, and Franc?ois Yvon.
2010. Practical very large scale CRFs. In Proceed-
ings of ACL.
Mohamed Maamouri, Ann Bies, Tim Buckwalter, and
Wigdan Mekki. 2004. The Penn Arabic treebank:
Building a large-scale annotated Arabic corpus. In
Proceedings of NEMLAR.
Christopher D Manning. 2011. Part-of-speech tagging
from 97% to 100%: Is it time for some linguistics?
In Computational Linguistics and Intelligent Text Pro-
cessing. Springer.
Mitchell P. Marcus, Mary A. Marcinkiewicz, and Beat-
rice Santorini. 1993. Building a large annotated cor-
pus of English: The Penn Treebank. Computational
linguistics.
Naoaki Okazaki. 2007. Crfsuite: A fast implemen-
tation of conditional random fields (CRFs). URL
http://www.chokkan.org/software/crfsuite.
Llu??s Padro? and Evgeny Stanilovsky. 2012. Freeling
3.0: Towards Wider Multilinguality. In Proceedings
of LREC.
Adwait Ratnaparkhi. 1996. A maximum entropy model
for part-of-speech tagging. In EMNLP.
Alexander M. Rush and Slav Petrov. 2012. Vine pruning
for efficient multi-pass dependency parsing. In Pro-
ceedings of NAACL.
Anne Schiller. 1995. DMOR Benutzerhandbuch. Uni-
versita?t Stuttgart, Institut fu?r maschinelle Sprachver-
arbeitung.
Helmut Schmid and Florian Laws. 2008. Estimation of
conditional probabilities with decision trees and an ap-
plication to fine-grained POS tagging. In Proceedings
of COLING.
331
Helmut Schmid. 1994. Probabilistic part-of-speech tag-
ging using decision trees. In Proceedings of NEMLP.
Libin Shen, Giorgio Satta, and Aravind Joshi. 2007.
Guided Learning for Bidirectional Sequence Classifi-
cation. In Proceedings of ACL.
Qinfeng Shi, James Petterson, Gideon Dror, John Lang-
ford, Alex Smola, and S.V.N. Vishwanathan. 2009.
Hash Kernels for Structured Data. J. Mach. Learn.
Res.
Noah A. Smith, David A. Smith, and Roy W. Tromble.
2005. Context-based morphological disambiguation
with random fields. In Proceedings of EMNLP.
Kristina Toutanova, Dan Klein, Christopher D. Manning,
and Yoram Singer. 2003. Feature-rich part-of-speech
tagging with a cyclic dependency network. In Pro-
ceedings of NAACL.
Yoshimasa Tsuruoka, Jun?ichi Tsujii, and Sophia Ana-
niadou. 2009. Stochastic gradient descent training
for L1-regularized log-linear models with cumulative
penalty. In Proceedings of ACL.
David Weiss and Ben Taskar. 2010. Structured predic-
tion cascades. In In Proceedings of AISTATS.
Alexander Yeh. 2000. More accurate tests for the statis-
tical significance of result differences. In Proceedings
of COLING.
Ja?nos Zsibrita, Veronika Vincze, and Richa?rd Farkas.
2013. Magyarlanc 2.0: Szintaktikai elemze?s e?s fel-
gyors??tott szo?faji egye?rtelmu?s??te?s. In IX. Magyar
Sza?m??to?ge?pes Nyelve?szeti Konferencia.
332
Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 669?680,
Seattle, Washington, USA, 18-21 October 2013. c?2013 Association for Computational Linguistics
The Topology of Semantic Knowledge
Jimmy Dubuisson Jean-Pierre Eckmann
De?partement de Physique The?orique and Section de Mathe?matiques
Universite? de Gene`ve
Jimmy.Dubuisson@unige.ch
Christian Scheible
Institut fu?r Maschinelle Sprachverarbeitung
University of Stuttgart
scheibcn@ims.uni-stuttgart.de
Hinrich Schu?tze
Center for Information
and Language Processing
University of Munich
Abstract
Studies of the graph of dictionary definitions
(DD) (Picard et al, 2009; Levary et al, 2012)
have revealed strong semantic coherence of
local topological structures. The techniques
used in these papers are simple and the main
results are found by understanding the struc-
ture of cycles in the directed graph (where
words point to definitions). Based on our ear-
lier work (Levary et al, 2012), we study a dif-
ferent class of word definitions, namely those
of the Free Association (FA) dataset (Nelson
et al, 2004). These are responses by subjects
to a cue word, which are then summarized by
a directed, free association graph.
We find that the structure of this network is
quite different from both the Wordnet and the
dictionary networks. This difference can be
explained by the very nature of free associa-
tion as compared to the more ?logical? con-
struction of dictionaries. It thus sheds some
(quantitative) light on the psychology of free
association.
In NLP, semantic groups or clusters are inter-
esting for various applications such as word
sense disambiguation. The FA graph is tighter
than the DD graph, because of the large num-
ber of triangles. This also makes drift of
meaning quite measurable so that FA graphs
provide a quantitative measure of the seman-
tic coherence of small groups of words.
1 Introduction
The computer study of semantic networks has been
around since the advent of computers (Brunet, 1974)
and has been used to study semantic relations be-
tween concepts and for analyzing semantic data.
Traditionally, a popular lexical database of English
is Wordnet (Miller, 1995; Miller and Fellbaum,
1998), which organizes the semantic network in
terms of graph theory. In contrast to manual ap-
proaches, the automatic analysis of semantically in-
teresting graph structures of language has received
increasing attention. For example, it has become
clear more recently that cycles and triangles play
an important role in semantic networks, see e.g.,
(Dorow et al, 2005). These results suggest that the
underlying semantic structure of language may be
discovered through graph-theoretical methods. This
is in line with similar findings in much wider realms
than NLP (Eckmann and Moses, 2002).
In this paper, we compare two different types
of association networks. The first network is con-
structed from an English dictionary (DD), the sec-
ond from a free association (FA) database (Nelson
et al, 2004). We represent both datasets through
directed graphs. For DD, the nodes are words and
the directed edges point from a word to its defini-
tion(s). For FA, the nodes are again words, and each
cue word has a directed edge to each association it
elicits.
Although the links in these graphs were not con-
structed by following a rational centralized process,
their graph exhibits very specific features and we
concentrate on the study of its topological proper-
ties. We will show that these graphs are quite dif-
ferent in global and local structure, and we inter-
pret this as a reflection of the different nature of
DD vs. FA. The first is an objective set of rela-
669
tions between words and their meaning, as explained
by other words, while the second reveals the nature
of subjective reactions to cue words by individuals.
This matter of fact is reflected by several quantita-
tive differences in the structure of the corresponding
graphs.
The main contribution of this paper is an empiri-
cal analysis of the way semantic knowledge is struc-
tured, comparing two different types of association
networks (DD and FA). We conduct a mathemati-
cal analysis of the structure of the graphs to show
that the way humans express their thoughts exhibits
structural properties in which one can find seman-
tic patterns. We show that a simple graph-based
approach can leverage the information encoded in
free association to narrow down the ambiguity of
meaning, resulting in precise semantic groups. In
particular, we find that the main strongly connected
component of the FA graph (the so-called core) is
very cyclic in nature and contains a large predom-
inance of short cycles (i.e., co-links and triangles).
In contrast to the DD graph, bunches of triangles
form well-delimited lexical fields of collective se-
mantic knowledge. This property may be promising
for downstream tasks. Further, the methods devel-
oped in this paper may be applicable to graph rep-
resentations that occur in other problems such as
word sense disambiguation (e.g., (Heylighen, 2001;
Agirre and Soroa, 2009)) or sentiment polarity in-
duction (Hassan and Radev, 2010; Scheible, 2010).
To show the semantic coherence of these lexi-
cal fields of the FA graph, we perform an exper-
iment with human raters and find that cycles are
strongly semantically connected even when com-
pared to close neighbors in the graph.
The reader might wonder why sets of pairwise
associations can lead to any interesting structure.
One of the deep results in graph theory, (Bolloba?s,
2001), is that in sparse graphs, i.e., in graphs with
few links per node, the number of triangles is ex-
tremely rare. Therefore, if one does find many tri-
angles in a graph, they must be not only a signal
of non-randomness, but carry relevant information
about the domain of research as shown earlier (Eck-
mann and Moses, 2002).
2 The USF FA dataset
This dataset is one of the largest existing databases
of free associations (FA) and has been collected at
the University of South Florida since 1973 by re-
searchers in psychology (Nelson et al, 2004). Over
the years, more than 6?000 participants produced
about 750?000 responses to 5?019 stimulus words.
The procedure for collecting the data is called dis-
crete association task and consists in asking partici-
pants to give the first word that comes to mind (tar-
get) when presented a stimulus word (cue).
For creating the initial set of stimulus words,
the Jenkins and Palermo word association norms
(Palermo and Jenkins, 1964) proved useful but too
limited as they consist of only 200 words. For this
reason, additional words have been regularly added
to the pool of normed words, unfortunately without
well established rules being followed. For instance,
some were selected as potentially interesting cues,
some were added as responses to the first sets of cues
and, some others were collected for supporting new
studies on verbs. We still work with this database,
because of its breadth.
The final pool of stimuli comprises 5?019 words
of which 76% are nouns, 13% adjectives, and 7%
verbs. A word association is said to be normed
when the target is also part of the set of norms, i.e.,
a cue. The USF dataset of free associations con-
tains 72?176 cue-target pairs, 63?619 of which are
normed. As an example, the association puberty-sex
is normed whereas the association puberty-thirteen
is not, because thirteen is not a cue.
3 Mathematical definitions
We collect here those notions we need for the analy-
sis of the data.
A directed graph is a pair G = (V,E) of a set
V of vertices and, a set E of ordered pairs of ver-
tices also called directed edges. For a directed edge
(u, v) ? E, u is called the tail and v the head of
the edge. The number of edges incident to a vertex
v ? V is called the degree of v. The in-degree
(resp. out-degree) of a vertex v is the number of
edge heads (resp. edge tails) adjacent to it. A vertex
with null in-degree is called a source and a vertex
with null out-degree is called a sink.
A directed path is a sequence of vertices such
670
that a directed edge exists between each consecutive
pair of vertices of the graph. A directed graph is
said to be strongly connected, (resp. weakly con-
nected) if for every pair of vertices in the graph,
there exists a directed path (resp. undirected path)
between them. A strongly connected component,
SCC, (resp. weakly connected component, WCC)
of a directed graph G is a maximal strongly con-
nected (resp. weakly connected) subgraph of G.
A directed cycle is a directed path such that its
start vertex is the same as its end vertex. A co-
link is a directed cycle of length 2 and a triangle a
directed cycle of length 3.
The distance between two vertices in a graph is
the number of edges in the shortest path connecting
them. The diameter of a graph G is the greatest
distance between any pair of vertices. The charac-
teristic path length is the average distance between
any two vertices of G.
The density of a directed graph G(V,E) is the
proportion of existing edges over the total number
of possible edges and is defined as:
d = |E|/(|V |(|V | ? 1))
The neighborhoodNi of a vertex vi isNi = {vj :
eij ? E or eji ? E}.
The local clustering coefficient Ci for a vertex vi
corresponds to the density of its neighborhood sub-
graph. For a directed graph, it is thus given by:
Ci =
|{ejk : vj , vk ? Ni, ejk ? E}|
|Ni|(|Ni| ? 1)
The clustering coefficient of a graph G is the aver-
age of the local clustering coefficients of all its ver-
tices.
The efficiency Eff of a directed graph G is an in-
dicator of the traffic capacity of a network. It is the
harmonic mean of the distance between any two ver-
tices of G. It is defined as:
Eff =
1
|V |(|V | ? 1)
?
i 6=j?V
1
dij
The linear correlation coefficient between two
random variables X and Y is defined as:
?(X,Y ) = (E[XY ]? ?X?Y )/(?X?Y )
where ?X and ?X are respectively the mean and
standard deviation of the random variable X .
The linear degree correlation coefficient of a
graph is called assortativity and is expressed as:
?D =
?
xy
xy(exy ? axby)/(?a?b)
where exy is the fraction of all links that connect
nodes of degree x and y and where ax and by are re-
spectively the fraction of links whose tail is adjacent
to nodes with degree x and whose head is adjacent to
nodes with degree y, satisfying the following three
conditions:
?
xy
exy = 1, ax =
?
y
exy, by =
?
x
exy
When ?D is positive, the graph possesses assor-
tative mixing and high-degree nodes tend to con-
nect to other high-degree nodes. On the other hand,
when ?D is negative, the graph features disassorta-
tive mixing and high-degree nodes tend to connect
to low degree nodes.
The intersection graph of sets Ai, i = 1, . . . ,m,
is constructed by representing each setAi as a vertex
vi ? V and adding an edge for each pair of sets with
a non-empty intersection:
E = {(vi, vj) : Ai ?Aj 6= ?}
4 Graph topology analysis
4.1 Graph generation
Our goal being to study the FA network topology,
we first concentrate on the generation of an un-
weighted directed graph. We generate the corre-
sponding graph by adding a directed edge for each
cue-target pair of the dataset. We only consider pairs
whose target was normed in order to avoid overload-
ing the graph with noisy data (e.g., a response mean-
ingful only to a specific participant). The graph has
5?019 vertices and 63?619 edges. It is composed of
a single WCC and 166 SCCs.
For comparison with dictionary definitions (DD),
we construct a graph from the Wordnet2 dictionary
(nouns only), following (Levary et al, 2012). This
graph contains 54?453 vertices and 179?848 edges.
671
4.2 Core extraction
The so-called core was defined previously in (Picard
et al, 2009; Levary et al, 2012) as that subset of
nodes in which a random walker gets trapped after
only a few steps.
The shave algorithm was used in (Levary et al,
2012) to isolate this subset. It consists in recursively
removing the source and sink nodes from a weakly
connected directed graph and permits to get the sub-
graph induced by the union of its strongly connected
components. Note that the dictionary graph (DD)
has no sinks (i.e., words that never get defined) and
that it contains a giant SCC whose size is compara-
ble to the one of the initial graph.
It turns out that the FA graph also contains a giant
SCC, therefore getting the core consists more simply
in extracting the main SCC of the initial graph. We
use Tarjan?s algorithm (Tarjan, 1972) for isolating
the FA core.
4.3 Vertex degree analysis
The FA core has a maximum in-degree of 313, a
maximum out-degree of 33 and an average degree
of 25.42. The in-degree distribution follows a power
law (? = 1.93) and the out-degrees are Poisson-like
distributed with a peak at 14 (Steyvers and Tenen-
baum, 2005; Gravino et al, 2012).
Words having a high in-degree are targets that
tend to be cited more frequently. On the other hand,
words having a high out-degree are cues that evoke
many different targets.
The most evocative cues are, in decreasing order
of out-degree: field (33), body (31), condemn (29),
farmer (29), crisis (28), plan (28), attention (27),
animal (27), and hang (27). Interestingly, the most
cited targets (i.e., targets with highest in-degree) are
in decreasing order: food (313), money (295), water
(271), car (251), good (246), bad (221), work (187),
house (183), school (182), love (179).
4.4 Cycle decomposition of the core
We define the vertex k-cycle multiplicity
(resp. edge k-cycle multiplicity) as the num-
ber of k-cycles a given vertex (resp. edge) belongs
to. We call core-ER the set of Erdo?s-Re?nyi (ER)
random graphs G(n,M) having the same number
of nodes and the same number of edges as the FA
2 4 6 8 10 12 14 1610
1
102
103
104
Cycles length
#
of
sh
or
te
st
cy
cl
es
core
ER
Figure 1: Distribution of shortest cycles lengths in the
core compared to equivalent ER models
One should bear in mind that we only consider the set of short-
est cycles. Thus, a k-cycle is not counted if each of its nodes
belongs to a cycle whose length is < k. Although the num-
ber of 4-shortest cycles is comparable in the core and core-ER
graphs for example, there are in reality far more 4-cycles in the
core (i.e., 42?738 versus 6?517). We see that when considering
shortest cycles, short cycles tend to hide long ones, and, as a
large proportion of nodes in the core belong to 2- and 3-cycles,
many longer cycles do not get counted at all.
core. We start by extracting the 2- and 3-cycles by
using a customized version of Johnson?s algorithm
(Johnson, 1975). The first thing we observe is that
the core has a very high density of short cycles: the
subset of nodes belonging to 2-cycles (i.e., nodes
with 2-cycle multiplicities > 0) cover 95% of the
core vertices and the 3-cycles cover 88% of the
core vertices. The corresponding core-ER graphs
have on average about 100 times fewer 2-cycles and
almost 20 times fewer 3-cycles.
This shows that the core is very cyclic in nature
and that it remains very well connected for short-
length cycles: most vertices of the core indeed be-
long to at least one co-link or triangle.
In order to limit computation times, we only con-
sidered shortest cycles for lengths ? 3 and analyzed
the distribution of the number of shortest cycles
in the core compared to equivalent random graphs.
Whereas there are many more short cycles in the
core, we observe a predominance of 4, 5 and 6-
cycles in core-ER graphs. However, we find again a
slight predominance of long cycles (length between
7 and 15) in the core (see Fig. 1). See (Levary et al,
2012), Fig. 3, where the cycle distribution is very
different, with a minimum at length 5.
672
4.5 Interpretation of cycles
2-cycles are composed of concretely related words
(e.g., drug-coke, destiny-fate, einstein-genius, . . . ).
The vertex with highest 2-cycle multiplicity is music
(22).
Words in 3- and 4-cycles often belong to the
same lexical field. Examples of 3-cycles: protect-
guard-defend or space-universe-star. The vertex
(resp. edge) with highest 3-cycle multiplicity is
car (86) (resp. bad-crime (11)). Examples of 4-
cycles: monster-dracula-vampire-ghost or flu-virus-
infection-sick.
Longer cycles are more difficult to describe: Re-
lations linking words of a given cycle exhibit se-
mantic drift with increasing length (cf. (Levary et
al., 2012)). Examples of 5-cycles: yellow-coward-
chicken-soup-noodles and sleep-relax-music-art-
beauty.
The cumulated set of free associations reflects the
way in which a group of people retrieved its seman-
tic knowledge. As the associated graph is highly
circular, this suggests that this knowledge is not
stored in a hierarchical way (Steyvers and Tenen-
baum, 2005). The large predominance of short cy-
cles in the core may indeed be a natural conse-
quence of the semantic information being acquired
by means of associative learning (Ashcraft and Rad-
vansky, 2009; Shanks, 1995).
4.6 FA core clustering
4.6.1 The walktrap community algorithm
Complex networks are globally sparse but con-
tain locally dense subgraphs. These groups of highly
interconnected vertices are called communities and
convey important properties of the network.
Although the notion of community is difficult to
define formally, the current consensus establishes
that a partition P = {C1, C2, . . . , Ck} of the ver-
tex set of a graph G represents a good community
structure if the proportion of edges inside the Ci is
higher than the proportion of edges between them
(Fortunato, 2010).
Computing such communities in a large graph is
generally computationally expensive (Lancichinetti
and Fortunato, 2009). We use the so-called ?Walk-
trap? community detection algorithm (Pons and Lat-
apy, 2006) for extracting communities from the FA
networks. The idea lying behind this algorithm is
that random walks on a graph will tend to get trapped
in the densely connected subgraphs.
Let P tij be the probability of going from vertex i
to vertex j through a random walk of length t. The
distance between two vertices i and j of the graph is
defined as:
rij(t) =
?
?
?
?
n?
k=1
(P tik ? P
t
jk)
2
d(k)
where d(k) is the degree of vertex k.
One defines the probability P tC,j to go from
community C to vertex j in t steps: P tC,j =?
i?C P
t
ij/|C|, and then the distance is easily gen-
eralized for two communities C1, C2.
The algorithm starts with a partition P1 = {{v} ?
V } of the initial graph into n communities each of
which is a single vertex. At each step, two communi-
ties are chosen and merged according to the criterion
described below and the distances between commu-
nities are updated. The process goes on until we ob-
tain the partition Pn = {V }.
In order to reduce complexity, only adjacent com-
munities are considered for merging. The decision
is then made according to Ward?s method (Everitt
et al, 2001): at each step k, the two communities
that minimize the mean ?k of the squared distances
between each vertex and its community are merged:
?k =
1
n
?
C?Pk
?
i?C
r2iC
4.6.2 Clustering of the core
We first identify the communities of the FA core
using the Walktrap algorithm. We immediately
observe that when the path length parameter in-
creases, the number of identified communities de-
creases (i.e., for a length of 2, we find 35 communi-
ties whereas for a length of 9, we only find 8 com-
munities).
For a path length of 2, the algorithm extracts 35
communities, 7 of which contain more than 100 ver-
tices, 3 of which contain between 100 and 50 ver-
tices and 25 of which contain less than 50 vertices.
We observe that for most small communities (i.e.,
the ones containing less than 50 vertices), there ex-
ists a clear relation between the labels of their ver-
673
tices. Typically, the labels are part of the same lexi-
cal field (e.g., all the planets (except earth) or related
by a common grammatical function (such as why,
where, what, . . . ).
4.6.3 Clustering of the core co-links
We define the k-cycle induced subgraph of a
graph G as the subgraph of G induced by the set
of its vertices with k-cycle multiplicity > 0.
The co-link graph of a graphG(V,E) is the undi-
rected graph obtained by replacing each co-link (i.e.,
2-cycle) of the 2-cycle induced subgraph of G by a
single undirected edge and removing all other edges.
The co-link graph of the FA core has 4?508 ver-
tices and 8?309 edges for a density of 8?10?4. It
is composed of a single weakly connected compo-
nent that can be seen as a projection of the strongest
semantic links from the original graph. Extracting
the co-link graph is thus an efficient way of select-
ing the set of most important semantic links (i.e., the
set of 2-cycles that appear in large predominance in
the core compared to what is found in an equivalent
random graph) while filtering out the noisy or negli-
gible ones.
The sets of communities extracted by the Walk-
trap algorithm exhibit different degrees of granular-
ity depending on the length parameter. For short
paths, a large number of very small communities are
returned (e.g., 923 communities when length equals
2) whereas for longer paths the average size of the
communities increases more and more.
The community detection exhibits thus a far finer
degree of granularity for the core co-links graph than
for the core itself. The size of the communities being
much smaller in average, it is striking to notice to
which extent the words of a given community are
semantically related.
Examples of communities found in the core co-
links graph include (standards, values, morals,
ethics), (hopeless, romantic, worthless, useless),
(thesaurus, dictionary, vocabulary, encyclopedia)
or (molecule, atom, electron, nucleus, proton, neu-
tron).
4.6.4 DD core clustering vs FA core clustering
The clustering of both cores has very different
characteristics: We illustrate the neighborhoods of
conflict for both cases in Fig. 2 and 3.
quarrel
personality
confusion
anger
disagreement
dilemma
fight
war
battle
trouble
conflict
argument
struggle
disagree
schedule
frustration
problem
Figure 2: Neighborhood of conflict in the FA core
The set of words belonging to the neighborhood of conflict are
clearly part of the same lexical field. The high density of co-
links leads to cyclicity and we see that many directed triangles
are present in the local subgraph (e.g., conflict-trouble-fight,
conflict-argument-disagree). We can even find triangles of co-
links that link together words semantically strongly related (e.g.,
fight-war-battle, fight-quarrel-argument). Nodes that are part of
the neighborhood of conflict in both FA and DD are in empty
circles.
On one hand, the words in communities of the DD
core are in most cases either synonyms, e.g., (decla-
ration, assertion, claim) or an instance-of kind of
relation, e.g., (signal, gesture, motion) or (zero, inte-
ger).
On the other hand, communities of the FA core
are generally composed of words belonging to the
same lexical field and sharing the same level of ab-
straction.
Moreover, we notice that it is often difficult to es-
tablish the semantic relation existing between words
of many small communities (i.e., containing less
than 10 words) of the DD core. Two such examples
are: (choice, probate, executor, chosen, certificate,
testator, will) and (numeral, monarchy, monarch,
crown, significance, autocracy, symbol, interpreta-
tion).
The comparison of DD and FA reveals, in a quan-
titative way, fundamental differences between the
two realms. The interesting data are shown in ta-
ble 1.
674
friction
war
conflict
two
disagreement
group
Figure 3: Neighborhood conflict in the DD core
First, we note that the neighborhood has a lower density than
in the FA core. We also see that there is no cycle and there
seems to be a flow going from source nodes to sink nodes. As
it generally happens in the neighborhood subgraphs of the DD
core, source nodes are rather specific words whereas sink nodes
are generic words.
FA core DD core
# vertices 4?843 1?496
# edges 61?544 4?766
density 2.5?10?3 2.1?10?3
avg degree 25.4 6.37
max in-degree 313 59
directed diameter 10 29
characteristic path length 4.26 10.42
efficiency 2.5?10?1 1.2?10?1
clustering coefficient 8.5?10?2 5.1?10?2
assortativity 5.5?10?2 6.1?10?2
Table 1: Comparison FA vs DD
Note that while the FA core is in fact larger than
the DD core, its diameter is smaller. This illustrates
in a beautiful way the nature of free association as
compared to the more neutral dictionary. In par-
ticular, the characteristic path length is smaller in
the FA graph, because humans use generalized event
knowledge (McRae and Matsuki, 2009) in free asso-
ciation, producing semantic shortcuts. For example,
FA contains a direct link mirage?water, whereas
in DD, the shortest path between the two words is
mirage?refraction?wave?water.
5 The Bricks of Meaning
5.1 Extraction of the seed
We already saw that most vertices of the core be-
long to directed 2- and 3-cycles. Whereas 2-cycles
establish strong semantic links (i.e., synonymy or
antonymy relations) and provide cyclicity to the un-
derlying directed graph, we claim that 3-cycles (i.e.,
triangles) form the set of elementary concepts of the
core.
These structues are common to DD and to FA, but
we will see that the links in FA are somehow more
direct than in DD.
We call seed the subgraph of the core induced by
the set V3 of vertices belonging to directed triangles
and shell the subgraph of the core induced by the
set V \V3 (i.e., the set of vertices with a null 3-cycle
multiplicity), see Fig. 4.
Initial graph
core
sh
el
l
se
ed
Figure 4: Composition of the FA graph
The graph of FA contains a giant SCC (the core). The subgraph
of the core induced by the set of nodes belonging to at least one
triangle also forms a giant component we call the ?seed?. The
subgraph of the core induced by the set of nodes not belonging
to any triangle is called the ?shell? and is composed of many
small SCCs, including single vertices. Although the shell has a
low density, its nodes are very well connected to the seed.
The shell contains 530 nodes and 309 edges.
There are 7?035 edges connecting the shell to the
seed. The shell consists of of many small SCCs and
although its average degree is low (1.17), its ver-
tices have on average many (13.27) connections to
the seed.
The seed contains 4?313 vertices (89% of the
core) and 54?197 edges. The first thing to notice
is that it has 100 times more co-links (7?895) and
20 times more triangles (13?119) than an equivalent
random graph. We call shortcuts the 32?773 edges
of the seed that do not belong to 3-cycles, see Fig. 5.
The seed obviously also contains cycles whose
length is greater than 3. One can check that there ex-
ist only 5 basic motifs involving 2 attached triangles
and 1 shortcut for creating 4- and 5-cycles, and that
linking 2 isolated triangles with 2 shortcuts also per-
675
SFigure 5: Shortcut edges between two triangles sharing a
single vertex S
Two triangles can share 0, 1 or 2 vertices. For each of these three
basic motifs, we count the maximum number of shortcut edges
(i.e., edges not belonging to 3-cycles) that can be added. By
linking two triangles, these shortcuts permit to move two basic
semantic units closer together and create longer cycles (i.e., 4,
5, and 6-cycles). Long cycles can be thus considered as group-
ings of basic semantic units. In the case of two triangles sharing
one vertex for example, it is possible to add at most 6 short-
cuts, whereas, for two triangles sharing two vertices, at most 2
shortcuts can be added.
mit to form 4-, 5- and 6-cycles. All longer cycles are
simply made of a juxtaposition of these basic motifs.
Furthermore, there is a limit on the number of
shortcuts that can possibly be added in the seed be-
fore it gets saturated, as all its vertices belong to at
least one triangle. We show that at most 16 shortcuts
can be added between two isolated triangles, at most
6 between 2 triangles sharing 2 vertex and at most 2
between 2 triangles sharing 2 vertices (see Fig. 5).
5.2 The elementary lexical fields
Once the seed is isolated, we go on digging into its
structure. We focus on the arrangements of triangles
as they constitute the set of elementary concepts.
We start by removing all shortcuts from the seed
and convert it then to an undirected graph, in order
to get a homogeneous simplicial 2-complex.
Let t be the graph operator which transforms a
graph G into the intersection graph tG of its 2-
simplices (i.e., triangles sharing an edge). We apply
t to the homogeneous simplicial 2-complex found
previously. The result represents the links between
the basic semantic units of the seed. We call seed-
crux the giant WCC in the intersection graph.
We enumerate the 8?380 maximal cliques of FA
seed-crux and get the list of words composing each
Distance Acc ? KS
original ? 0.404 30
1 74 0.522 42
2 97 0.899 89
? 99 0.899 89
Table 2: Accuracy, ?, and count(p < 0.05) for KS
clique. By removing the ones that are subsets of big-
ger lists, we finally obtain 3?577 lists of words .
These lists of words have a rather small and ho-
mogeneous size (between 4 and 17) and 95% have
a size comprised between 4 and 10. More in-
terestingly, they clearly define well-delimited lexi-
cal fields. We will show this through two experi-
ments in the following sections. A few examples
include (honest, trustworthy, reliable, responsible),
(stress, problem, worry, frustration) and (data, pro-
cess, computer, information).
From a topological perspective, we deduce that
bunches of triangles (i.e., cliques of elementary con-
cepts) span the seed in a homogeneous way. These
bunches form a set of cohesive lexical fields and
constitute essential bricks of semantic knowledge.
5.3 Semantic similarity of the lexical fields
In order to quantify the relative meaning of words
in the lexical fields of the seed-crux, we define the
following semantic similarity metric based on the
Wordnet WUP metric (Wu and Palmer, 1994) for a
given set of words L:
S`(L) = 2
?
wi,wj?L,wi 6=wj
Sw(wi, wj)/(|L|(|L| ? 1))
where Sw(wi, wj) = max
Sk3wiandS`3wj
{wup(Sk, S`)}
and wup is the WUP semantic metric and Sk and S`
are Wordnet synsets.
The average value of S` for the set of cliques of
seed-crux is 0.6 whereas it is only 0.43 for randomly
sampled set of words. This suggests the correspond-
ing lists of words are indeed semantically related.
We will show the strength of this relation in the fol-
lowing experiment with human raters.
5.4 Human evaluation of the lexical fields
To validate our findings, we conducted an empirical
evaluation through human annotators. Starting from
676
the 1?204 4-groups, we designed the following ex-
periment: We corrupt the groups by exchanging one
of the 4 elements with a randomly chosen word at a
distance from the group of 1, 2, and ?infinity? (i.e.,
any word of the whole core). We presented 100 ran-
dom samples for each of the 3 distances as well as
100 unperturbed groups (original) to annotators at
Amazon Mechanical Turk1, asking which word fits
the group the least. Intuitively, the closer the ran-
domly chosen words get to the group, the closer the
distribution of the votes for each sample should be
to the uniform distribution. We collected 10 votes
for each of the 4 problems of 100 random samples.
We calculated accuracy (i.e., the relative frequency
of correctly identified random words) for the 3 ran-
dom confounder experiments and Fleiss? ?. Fur-
ther, we used the Kolmogorov-Smirnov (KS) test for
how uniform the label distribution is, reporting the
relative frequency of samples that are significantly
(p < 0.05) different from the uniform distribution.
The results of this experiment are summarized in Ta-
ble 2 and show clearly that the certainty about the
?odd man out? increases together with the distance.
5.5 Error analysis
If we view our results as a resource for a downstream
task, it is important to know about possible down-
sides. First, we note that there are words which are
not in a triangle and will thus be missing in the in-
tersection graph. This is an indication that the corre-
sponding word is less well embedded contextually,
so conversely, any prediction made about it from the
data may be less reliable. Additionally, semantic
leaps caused by generalized event knowledge may
lead to lesser-connected groups such as (steel, pipe,
lead, copper). Jumps like these may or may not be
desired in a subsequent application.
6 The Case of the EAT FA dataset
The Edinburgh Associative Thesaurus (EAT) (Kiss
et al, 1973) is a large dataset of free associations.
We extract the EAT FA seed-crux with the previ-
ously described methods.
We start by generating the initial graph (23?219
vertices and 325?589 edges), then extract its core
(7?754 vertices and 247?172 edges) and its seed
1http://www.mturk.com
(7?500 vertices and 238?677 edges). It is interest-
ing to notice at this stage that the EAT seed contains
74% of the words belonging to the USF seed. Af-
ter generating the seed-crux which contains 63?363
vertices, 6?825?731 edges, and 342?490 maximal
cliques, we finally obtain 40?998 lists of words.
These lists comprise between 4 and 233 words but
80% of them have a relatively small size between 4
and 20. Although we find exceptions for this graph,
most of the extracted lists again form well-delimited
lexical fields (e.g., (health, resort, spa, bath, salts) or
(god, devil, angel, satan).
Comparing the two association experiments, we
see that the local topologies are quite similar. Both
FA cores have a high density of connected trian-
gles, whereas cycles in the DD graph tend to be
longer and most triangles are isolated. This can be
attributed to the different ways in which DD and FA
are obtained, the former being built rationally by fol-
lowing a humanly-driven process and the latter re-
flecting an implicit collective semantic knowledge.
7 Related Work
A number of metrics like Latent Semantic Analy-
sis (Deerwester et al, 1990) and Word Association
Spaces (Steyvers et al, 2004) have been recently
developed for quantifying the relative meaning of
words. As the topological properties of free associ-
ation graphs reflect key aspects of semantic knowl-
edge, we believe some graph theory metrics could
be used efficiently to derive new ways of measuring
semantic similarity between words.
Topological analysis of the Florida Word Associa-
tions (FA) was started by (Steyvers and Tenenbaum,
2005; Gravino et al, 2012), who extracted global
statistics. We follow the basic methodology of these
studies, but extend their approach. First, we conduct
deeper analyses by examining the neighborhood of
nodes and extracting the statistics of cycles. Second,
we compare the properties of FA and DD graphs.
Word clustering based on graphs has been the sub-
ject of various earlier studies. Close to our work
is (Widdows and Dorow, 2002). These authors rec-
ognize that nearest-neighbor-based clustering of co-
occurrence give rise to semantic groups. This type of
approach has since been applied in various modified
forms, e.g., by (Biemann, 2006) who performs label-
677
propagation based on randomized nearest neighbors,
or Matsuo et al (2006) who perform greedy cluster-
ing. Hierarchical clustering algorithms (e.g., (Jonyer
et al, 2002; Manning et al, 2008)) are related as
well, however, the key difference is that in hierarchi-
cal clustering, the granularity of a cluster is difficult
to determine.
Dorow et al (2005) recognize that triangles form
semantically strongly cohesive groups and apply
clustering coefficients for word sense disambigua-
tion. Their work focuses on undirected graphs of
corpus co-occurrences whereas our work builds on
directed associations. Building on this work, we
take finer topological graph structures into account,
which is one of the main contributions in this paper.
8 Conclusion
The cognitive process of discrete free association be-
ing an epiphenomenon of our semantic memory at
work, the cumulative set of free associations of the
USF dataset can be viewed as the projection of a col-
lective semantic memory.
To analyze the semantic memory, we use the tools
of graph theory, and compare it also to dictionary
graphs. In both cases, triangles play a crucial role
in the local topology and they form the set of ele-
mentary concepts of the underlying graph. We also
show that cohesive lexical fields (taking the form
of cliques of concepts) constitute essential bricks of
meaning, and span the core homogeneously at the
global level; 89% of all words in the core belong
to at least one triangle, and 77% belong to cliques
of triangles containing 4 words (i.e., pairs of trian-
gles sharing an edge or forming tetrahedras). As the
words of a graph of free associations acquire their
meaning from the set of associations they are in-
volved in (Deese, 1962), we go a step further by
examining the neighborhood of nodes and extracting
the statistics of cycles. We further check through hu-
man evaluation that the clustering is strongly related
to meaning, and furthermore, the meaning becomes
measurably more confused as one walks away from
a cluster.
-? -?I call the pairs of triangles sharing an edge
the 2-clovers ;-)
Comparing dictionaries to free association, we
find the free association graph being more concept
driven, with words in small clusters being on the
same level of abstraction. Moreover, we think that
graphs of free associations could find interesting
applications for Word Sense Disambiguation (e.g.,
(Heylighen, 2001; Agirre and Soroa, 2009)), and
could be used for detecting psychological disorders
(e.g., depression, psychopathy) or whether someone
is lying (Hancock et al, 2013; Kent and Rosanoff,
1910).
Finally, we believe that studying the dynamics of
graphs of free associations may be of particular in-
terest for observing the change in meaning of certain
words (Deese, 1967), or more generally to follow the
cultural evolution arising among a social group.
References
Eneko Agirre and Aitor Soroa. 2009. Personalizing
pagerank for word sense disambiguation. In Proceed-
ings of the 12th Conference of the European Chap-
ter of the Association for Computational Linguistics,
EACL ?09, pages 33?41, Stroudsburg, PA, USA. As-
sociation for Computational Linguistics.
Mark H Ashcraft and Gabriel A Radvansky. 2009. Cog-
nition. Pearson Prentice Hall.
Chris Biemann. 2006. Chinese whispers: an efficient
graph clustering algorithm and its application to natu-
ral language processing problems. In Proceedings of
the First Workshop on Graph Based Methods for Natu-
ral Language Processing, TextGraphs-1, pages 73?80,
Stroudsburg, PA, USA. Association for Computational
Linguistics.
Be?la Bolloba?s. 2001. Random graphs, volume 73 of
Cambridge Studies in Advanced Mathematics. Cam-
bridge University Press, Cambridge, second edition.
Etienne Brunet. 1974. Le traitement des faits
linguistiques et stylistiques sur ordinateur. Texte
d?application: Giraudoux, Statistique et Linguistique.
David, J. y Martin, R.(eds.). Paris: Klincksieck, pages
105?137.
Scott Deerwester, Susan T. Dumais, George W Furnas,
Thomas K Landauer, and Richard Harshman. 1990.
Indexing by latent semantic analysis. Journal of the
American society for information science, 41(6):391?
407.
James Deese. 1962. On the structure of associative
meaning. Psychological review, 69:161.
James Deese. 1967. Meaning and change of meaning.
The American psychologist, 22(8):641.
Beate Dorow, Dominic Widdows, Katerina Ling, Jean-
Pierre Eckmann, Danilo Sergi, and Elisha Moses.
678
2005. Using curvature and Markov clustering in
graphs for lexical acquisition and word sense discrim-
ination. In MEANING-2005, 2nd Workshop organized
by the MEANING Project, February 3rd-4th 2005,
Trento, Italy.
Jean-Pierre Eckmann and Elisha Moses. 2002. Curva-
ture of co-links uncovers hidden thematic layers in
the World Wide Web. Proc. Natl. Acad. Sci. USA,
99(9):5825?5829 (electronic).
Brian Everitt, Sabine Landau, and Morven Leese. 2001.
Cluster analysis. 4th Edition. Arnold, London.
Santo Fortunato. 2010. Community Detection in Graphs.
Physics Reports, 486(3):75?174.
Pietro Gravino, Vito DP Servedio, Alain Barrat, and Vit-
torio Loreto. 2012. Complex structures and semantics
in free word association. Advances in Complex Sys-
tems, 15(03n04).
Jeffrey T Hancock, Michael T Woodworth, and Stephen
Porter. 2013. Hungry like the wolf: A word-pattern
analysis of the language of psychopaths. Legal and
Criminological Psychology, 18(1):102?114.
Ahmed Hassan and Dragomir Radev. 2010. Identifying
text polarity using random walks. In Proceedings of
the 48th Annual Meeting of the Association for Com-
putational Linguistics, pages 395?403. Association for
Computational Linguistics.
Francis Heylighen. 2001. Mining associative meanings
from the web: from word disambiguation to the global
brain. In Proceedings of Trends in Special Language
& Language Technology, pages 15?44.
Donald B Johnson. 1975. Finding all the elementary
circuits of a directed graph. SIAM Journal on Com-
puting, 4(1):77?84.
Istvan Jonyer, Diane J Cook, and Lawrence B Holder.
2002. Graph-based hierarchical conceptual clustering.
The Journal of Machine Learning Research, 2:19?43.
Grace H Kent and Aaron J Rosanoff. 1910. A study of
association in insanity. American Journal of Insanity.
George R Kiss, Christine Armstrong, Robert Milroy, and
James Piper. 1973. An associative thesaurus of en-
glish and its computer analysis. The computer and lit-
erary studies, pages 153?165.
Andrea Lancichinetti and Santo Fortunato. 2009. Com-
munity detection algorithms: A comparative analysis.
Physical review E, 80(5):056117.
David Levary, Jean-Pierre Eckmann, Elisha Moses, and
Tsvi Tlusty. 2012. Loops and self-reference in the
construction of dictionaries. Phys. Rev. X, 2:031018.
Christopher D Manning, Prabhakar Raghavan, and Hin-
rich Schu?tze. 2008. Introduction to information re-
trieval, volume 1. Cambridge University Press Cam-
bridge.
Yutaka Matsuo, Takeshi Sakaki, Ko?ki Uchiyama, and
Mitsuru Ishizuka. 2006. Graph-based word cluster-
ing using a web search engine. In Proceedings of
the 2006 Conference on Empirical Methods in Natural
Language Processing, EMNLP ?06, pages 542?550,
Stroudsburg, PA, USA. Association for Computational
Linguistics.
Ken McRae and Kazunaga Matsuki. 2009. People use
their knowledge of common events to understand lan-
guage, and do so as quickly as possible. Language and
linguistics compass, 3(6):1417?1429.
George Miller and Christiane Fellbaum. 1998. WordNet:
An Electronic Lexical database. MIT Press, Cam-
bridge, MA.
George A Miller. 1995. WordNet: a lexical database for
english. Communications of the ACM, 38(11):39?41.
Douglas L Nelson, Cathy L McEvoy, and Thomas A
Schreiber. 2004. The University of South Florida free
association, rhyme, and word fragment norms. Be-
havior Research Methods, Instruments, & Computers,
36(3):402?407.
David S Palermo and James J Jenkins. 1964. Word asso-
ciation norms: Grade school through college. Univer-
sity of Minnesota Press.
Olivier Picard, Alexandre Blondin-Masse?, Stevan Har-
nad, Odile Marcotte, Guillaume Chicoisne, and Yas-
sine Gargouri. 2009. Hierarchies in dictionary defini-
tion space. In Annual Conference on Neural Informa-
tion Processing Systems.
Pascal Pons and Matthieu Latapy. 2006. Computing
communities in large networks using random walks.
In Journal of Graph Algorithms and Applications,
pages 284?293. Springer.
Christian Scheible. 2010. Sentiment translation through
lexicon induction. In Proceedings of the ACL 2010
Student Research Workshop, pages 25?30, Uppsala,
Sweden, July. Association for Computational Linguis-
tics.
David R Shanks. 1995. The psychology of associative
learning, volume 13. Cambridge University Press.
Mark Steyvers and Joshua B Tenenbaum. 2005. The
large-scale structure of semantic networks: Statistical
analyses and a model of semantic growth. Cognitive
Science, 29(1):41?78.
Mark Steyvers, Richard M Shiffrin, and Douglas L Nel-
son. 2004. Word association spaces for predicting
semantic similarity effects in episodic memory. Ex-
perimental cognitive psychology and its applications:
Festschrift in honor of Lyle Bourne, Walter Kintsch,
and Thomas Landauer, pages 237?249.
Robert Tarjan. 1972. Depth-first search and linear graph
algorithms. SIAM journal on computing, 1(2):146?
160.
679
Dominic Widdows and Beate Dorow. 2002. A graph
model for unsupervised lexical acquisition. In Pro-
ceedings of the 19th international conference on Com-
putational linguistics - Volume 1, COLING ?02, pages
1?7, Stroudsburg, PA, USA. Association for Compu-
tational Linguistics.
Zhibiao Wu and Martha Palmer. 1994. Verbs semantics
and lexical selection. In Proceedings of the 32nd an-
nual meeting on Association for Computational Lin-
guistics, pages 133?138. Association for Computa-
tional Linguistics.
680
Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics, pages 276?285,
Avignon, France, April 23 - 27 2012. c?2012 Association for Computational Linguistics
Automatic generation of short informative sentiment summaries
Andrea Glaser and Hinrich Schu?tze
Institute for Natural Language Processing
University of Stuttgart, Germany
glaseraa@ims.uni-stuttgart.de
Abstract
In this paper, we define a new type of
summary for sentiment analysis: a single-
sentence summary that consists of a sup-
porting sentence that conveys the overall
sentiment of a review as well as a convinc-
ing reason for this sentiment. We present a
system for extracting supporting sentences
from online product reviews, based on a
simple and unsupervised method. We de-
sign a novel comparative evaluation method
for summarization, using a crowdsourcing
service. The evaluation shows that our
sentence extraction method performs better
than a baseline of taking the sentence with
the strongest sentiment.
1 Introduction
Given the success of work on sentiment analy-
sis in NLP, increasing attention is being focused
on how to present the results of sentiment analy-
sis to the user. In this paper, we address an im-
portant use case that has so far been neglected:
quick scanning of short summaries of a body of
reviews with the purpose of finding a subset of
reviews that can be studied in more detail. This
use case occurs in companies that want to quickly
assess, perhaps on a daily basis, what consumers
think about a particular product. One-sentence
summaries can be quickly scanned ? similar to
the summaries that search engines give for search
results ? and the reviews that contain interesting
and new information can then be easily identified.
Consumers who want to quickly scan review sum-
maries to pick out a few reviews that are helpful
for a purchasing decision are a similar use case.
For a one-sentence summary to be useful in this
context, it must satisfy two different ?information
needs?: it must convey the sentiment of the re-
view, but it must also provide a specific reason
for that sentiment, so that the user can make an
informed decision as to whether reading the en-
tire review is likely to be worth the user?s time ?
again similar to the purpose of the summary of a
web page in search engine results.
We call a sentence that satisfies these two crite-
ria a supporting sentence. A supporting sentence
contains information on the sentiment as well as
a specific reason for why the author arrived at this
sentiment. Examples for supporting sentences are
?The picture quality is very good? or ?The bat-
tery life is 2 hours?. Non-supporting sentences
contain opinions without such reasons such as ?I
like the camera? or ?This camera is not worth the
money?.
To address use cases of sentiment analysis that
involve quick scanning and selective reading of
large numbers of reviews, we present a simple un-
supervised system in this paper that extracts one
supporting sentence per document and show that
it is superior to a baseline of selecting the sentence
with the strongest sentiment.
One problem we faced in our experiments was
that standard evaluations of summarization would
have been expensive to conduct for this study. We
therefore used crowdsourcing to perform a new
type of comparative evaluation method that is dif-
ferent from training set and gold standard cre-
ation, the dominant way crowdsourcing has been
used in NLP so far.
In summary, our contributions in this paper are
as follows. We define supporting sentences, a new
type of sentiment summary that is appropriate in
situations where both the sentiment of a review
and a good reason for that sentiment need to be
276
conveyed succinctly. We present a simple un-
supervised method for extracting supporting sen-
tences and show that it is superior to a baseline in
a novel crowdsourcing-based evaluation.
In the next section, we describe related work
that is relevant to our new approach. In Section 3
we present the approach we use to identify sup-
porting sentences. Section 4 describes the fea-
ture representation of sentences and the classifi-
cation method. In Section 5 we give an overview
of the crowdsourcing evaluation. Section 6 dis-
cusses our experimental results. In Sections 7 and
8, we present our conclusions and plans for future
work.
2 Related Work
Both sentiment analysis (Pang and Lee, 2008;
Liu, 2010) and summarization (Nenkova and
McKeown, 2011) are important subfields of NLP.
The work most relevant to this paper is work on
summarization methods that addresses the spe-
cific requirements of summarization in sentiment
analysis. There are two lines of work in this vein
with goals similar to ours: (i) aspect-based and
pro/con-summarization and (ii) approaches that
extract summary sentences from reviews.
An aspect is a component or attribute of a
product such as ?battery?, ?lens cap?, ?battery
life?, and ?picture quality? for cameras. Aspect-
oriented summarization (Hu and Liu, 2004;
Zhuang et al 2006; Kim and Hovy, 2006) col-
lects sentiment assessments for a given set of as-
pects and returns a list of pros and cons about ev-
ery aspect for a review or, in some cases, on a
per-sentence basis.
Aspect-oriented summarization and pro/con-
summarization differ in a number of ways from
supporting sentence summarization. First, as-
pects and pros&cons are taken from a fixed in-
ventory. The inventory is typically small and does
not cover the full spectrum of relevant informa-
tion. Second, in its most useful form, aspect-
oriented summarization requires classification of
phrases and sentences according to the aspect they
belong to; e.g., ?The camera is very light? has
to be recognized as being relevant to the aspect
?weight?. Developing a component that assigns
phrases and sentences to their corresponding cat-
egories is time-consuming and has to be redone
for each domain. Any such component will make
mistakes and undetected or incorrectly classified
aspects can result in bad summaries.
Our approach enables us to find strong support-
ing sentences even if the reason given in that sen-
tence does not fit well into the fixed inventory. No
manual work like the creation of an aspect inven-
tory is necessary and there are no requirements on
the format of the reviews such as author-provided
pros and cons.
Aspect-oriented summarization also differs in
that it does not differentiate along the dimension
of quality of the reason given for a sentiment. For
example, ?I don?t like the zoom? and ?The zoom
range is too limited? both give reasons for why a
camera gets a negative evaluation, but only the lat-
ter reason is informative. In our work, we evaluate
the quality of the reason given for a sentiment.
The use case we address in this paper requires
a short, easy-to-read summary. A well-formed
sentence is usually easier to understand than a
pro/con table. It also has the advantage that the
information conveyed is accurately representing
what the user wanted to say ? this is not the case
for a presentation that involves several complex
processing steps and takes linguistic material out
of the context that may be needed to understand it
correctly.
Berend (2011) performs a form of pro/con
summarization that does not rely on aspects.
However, most of the problems of aspect-based
pro/con summarization also apply to this paper:
no differentiation between good and bad reasons,
the need for human labels to train a classifier, and
inferior readability compared to a well-formed
sentence.
Two previous approaches that have attempted
to extract sentences from reviews in the context
of summarization are (Beineke et al 2004) and
(Arora et al 2009). Beineke et al(2004) train
a classifier on rottentomatoes.com summary sen-
tences provided by review authors. These sen-
tences sometimes contain a specific reason for the
overall sentiment of the review, but sometimes
they are just catchy lines whose purpose is to
draw moviegoers in to read the entire review; e.g.,
?El Bulli barely registers a pulse stronger than a
book?s? (which does not give a specific reason for
why the movie does not register a strong pulse).
Arora et al(2009) define two classes of sen-
tences: qualified claims and bald claims. A qual-
ified claim gives the reader more details (e.g.,
?This camera is small enough to fit easily in a
277
coat pocket?) while a bald claim is open to inter-
pretation (e.g., ?This camera is small?). Quali-
fied/bald is a dimension of classification of senti-
ment statements that is to some extent orthogonal
to quality of reason. Qualified claims do not have
to contain a reason and bald claims can contain
an informative reason. For example, ?I didn?t like
the camera, but I suspect it will be a great camera
for first timers? is classified as a qualified claim,
but the sentence does not give a good reason for
the sentiment of the document. Both dimensions
(qualified/bald, high-quality/low-quality reason)
are important and can be valuable components of
a complete sentiment analysis system.
Apart from the definition of the concept of sup-
porting sentence, which we believe to be more ap-
propriate for the application we have in mind than
rottentomatoes.com summary sentences and qual-
ified claims, there are two other important differ-
ences of our approach to these two papers. First,
we directly evaluate the quality of the reasons in a
crowdsourcing experiment. Second, our approach
is unsupervised and does not require manual an-
notation of a training set of supporting sentences.
As we will discuss in Section 5, we propose
a novel evaluation measure for summarization
based on crowdsourcing in this paper. The most
common use of crowdsourcing in NLP is to have
workers label a training set and then train a super-
vised classifier on this training set. In contrast, we
use crowdsourcing to directly evaluate the relative
quality of the automatic summaries generated by
the unsupervised method we propose.
3 Approach
Our approach is based on the following three
premises.
(i) A good supporting sentence conveys both
the review?s sentiment and a supporting fact.
We make this assumption because we want
the sentence to be self-contained. If it only
describes a fact about a product without
evaluation, then it does not on its own ex-
plain which sentiment is conveyed by the ar-
ticle and why.
(ii) Supporting facts are most often expressed by
noun phrases. We call a noun phrase that ex-
presses a supporting fact a keyphrase. We
are not assuming that all important words
in the supporting sentence are nominal; the
verb will be needed in many cases to accu-
rately convey the reason for the sentiment
expressed. However, it is a fairly safe as-
sumption that part of the information is con-
veyed using noun phrases since it is dif-
ficult to convey specific information with-
out using specific noun phrases. Adjectives
are often important when expressing a rea-
son, but frequently a noun is also mentioned
or one would need to resolve a pronoun to
make the sentence a self-contained support-
ing sentence. In a sentence like ?It?s easy
to use? it is not clear what the adjective is
referring to.
(iii) Noun phrases that express supporting facts
tend to be domain-specific; they can be
automatically identified by selecting noun
phrases that are frequent in the domain ? ei-
ther in relative terms (compared to a generic
corpus) or in absolute terms. By making
this assumption we may fail to detect sup-
porting sentences that are worded in an orig-
inal way using ordinary words. However,
in a specific domain there is usually a lot
of redundancy and most good reasons oc-
cur many times and are expressed by similar
words.
Based on these assumptions, we select the sup-
porting sentence in two steps. In the first step, we
determine the n sentences with the strongest sen-
timent within every review by classifying the po-
larity of the sentences (where n is a parameter).
In the second step, we select one of the n sen-
tences as the best supporting sentence by means
of a weighting function.
Step 1: Sentiment Classification
In this step, we apply a sentiment classifier to all
sentences of the review to classify sentences as
positive or negative. We then select the n sen-
tences with the highest probability of conforming
with the overall sentiment of the document. For
example, if the document?s polarity is negative,
we select the n sentences that are most likely to be
negative according to the sentiment classifier. We
restrict the set of n sentences to sentences with the
?right? sentiment because even an excellent sup-
porting sentence is not a good characterization of
278
the content of the review if it contradicts the over-
all assessment given by the review. Only in cases
where there are fewer than n sentences with the
correct sentiment, we also select sentences with
the ?wrong? sentence to obtain a minimum of n
sentences for each review.
Step 2: Weighting Function
Based on premises (ii) and (iii) above, we score
a sentence based on the number of noun phrases
that occur with high absolute and relative fre-
quency in the domain. We only consider sim-
ple nouns and compound nouns consisting of
two nouns in this paper. In general, compound
nouns are more informative and specific. A com-
pound noun may refer to a specific reason even
if the head noun does not (e.g., ?life? vs. ?battery
life?). This means that we need to compute scores
in a way that allows us to give higher weight to
compound nouns than to simple nouns.
In addition, we also include counts of nouns
and compounds in the scoring that do not have
high absolute/relative frequency because fre-
quency heuristics identify keyphrases with only
moderate accuracy. However, theses nouns and
compounds are given a lower weight.
This motivates a scoring function that is a
weighted sum of four variables: number of simple
nouns with high frequency, number of infrequent
simple nouns, number of compound nouns with
high frequency, and number of infrequent com-
pound nouns. High frequency is defined as fol-
lows. Let fdom(p) be the domain-specific abso-
lute frequency of phrase p, i.e., the frequency in
the review corpus, and fwiki(p) the frequency of
p in the English Wikipedia. We view the distribu-
tion of terms in Wikipedia as domain-independent
and define the relative frequency as in Equation 1.
frel(p) =
fdom(p)
fwiki(p)
(1)
We do not consider nouns and compound nouns
that do not occur in Wikipedia for computing
the relative frequency. A noun (resp. compound
noun) is deemed to be of high frequency if it is
one of the k% nouns (resp. compound nouns) with
the highest fdom(p) and at the same time is one of
the k% nouns (resp. compound nouns) with the
highest frel(p) where k is a parameter.
Based on these definitions, we define four dif-
ferent sets: F1 (the set of nouns with high fre-
quency), I1 (the set of infrequent nouns), F2 (the
set of compounds with high frequency), and I2
(the set of infrequent compounds). An infrequent
noun (resp. compound) is simply defined as a
noun (resp. compound) that does not meet the fre-
quency criterion.
We define the score s of a sentence with n to-
kens t1 . . . tn (where the last token tn is a punctu-
ation mark) as follows:
s =
n?1?
i=1
wf2 ? [[(ti, ti+1) ? F2]]
+ wi2 ? [[(ti, ti+1) ? I2]]
+ wf1 ? [[ti ? F1]]
+ wi1 ? [[ti ? I1]]
(2)
where [[?]] = 1 if ? is true and [[?]] = 0 otherwise.
Note that a noun in a compound will contribute to
the overall score in two different summands.
The weights wf2 , wi2 , wf1 , and wi1 are deter-
mined using logistic regression. The training set
for the regression is created in an unsupervised
fashion as follows. From each set of n sentences
(one per review), we select the two highest scor-
ing, i.e., the two sentences that were classified
with the highest confidence. The two classes in
the regression problem are then the top ranked
sentences vs. the sentences at rank 2. Since tak-
ing all sentences turned out to be too noisy, we
eliminate sentence pairs where the top sentence is
better than the second sentence on almost all of
the set counts (i.e., count of members of F1, I1,
F2, and I2). Our hypothesis in setting up this re-
gression was that the sentence with the strongest
sentiment often does not give a good reason. Our
experiments confirm that this hypothesis is true.
The weights wf2 , wi2 , wf1 , and wi1 estimated
by the regression are then used to score sentences
according to Equation 2.
We give the same weight to all keyphrase com-
pounds (and the same weight to all keyphrase
nouns) ? in future work one could attempt to give
higher weights to keyphrases with higher absolute
or relative frequency. In this paper, our goal is to
establish a simple baseline for the task of extrac-
tion of supporting sentences.
After computing the overall weight for each
sentence in a review, the sentence with the highest
weight is chosen as the supporting sentence ? the
sentence that is most informative for explaining
the overall sentiment of the review.
279
4 Experiments
4.1 Data
We use part of the Amazon dataset from Jindal
and Liu (2008). The dataset consists of more than
5.8 million consumer-written reviews of several
products, taken from the Amazon website. For
our experiment we used the digital camera do-
main and extracted 15,340 reviews covering a to-
tal of 740 products. See table 1 for key statistics
of the data set.
Type Number
Brands 17
Products 740
Documents (all) 15,340
Documents (cleaned) 11,624
Documents (train) 9,880
Documents (test) 1,744
Short test documents 147
Long test documents 1,597
Average number of sents 13.36
Median number of sents 10
Table 1: Key statistics of our dataset
In addition to the review text, authors can give
an overall rating (a number of stars) to the prod-
uct. Possible ratings are 5 (very positive), 4 (pos-
itive), 3 (neutral), 2 (negative), and 1 (very nega-
tive). We unify ratings of 4 and 5 to ?positive? and
ratings of 1 and 2 to ?negative? to obtain polarity
labels for binary classification. Reviews with a
rating of 3 are discarded.
4.2 Preprocessing
We tokenized and part-of-speech (POS) tagged
the corpus using TreeTagger (Schmid, 1994). We
split each review into individual sentences by us-
ing the sentence boundaries given by TreeTag-
ger. One problem with user-written reviews is
that they are often not written in coherent En-
glish, which results in wrong POS tags. To ad-
dress some of these problems, we cleaned the
corpus after the tokenization step. We separated
word-punctuation clusters (e.g., word...word) and
removed emoticons, html tags, and all sentences
with three or fewer tokens, many of which were
a result of wrong tokenization. We excluded all
reviews with fewer than five sentences. Short re-
views are often low-quality and do not give good
reasons. The cleaned corpus consists of 11,624
documents. Finally, we split the corpus into train-
ing set (85%) and test set (15%) as shown in Table
1. The average number of sentences of a review is
13.36 sentences, the median number of sentences
is 10.
4.3 Sentiment Classification
We first build a sentence sentiment classifier by
training the Stanford maximum entropy classifier
(Manning and Klein, 2003) on the sentences in the
training set. Sentences occurring in positive (resp.
negative) reviews are labeled positive (resp. neg-
ative). We use a simple bag-of-words representa-
tion (without punctuation characters and frequent
stop words). Propagating labels from documents
to sentences creates a noisy training set because
some sentences have sentiment different from the
sentiment in their documents; however, there is
no alternative because we need per-sentence clas-
sification decisions, but do not have per-sentence
human labels.
The accuracy of the classifier is 88.4% on
?propagated? sentence labels.
We use the sentence classifier in two ways.
First, it defines our baseline BL for extracting
supporting sentences: the baseline simply pro-
poses the sentence with the highest sentiment
score that is compatible with the sentiment of the
document as the supporting sentence.
Second, the sentence classifier selects a subset
of candidate sentences that is then further pro-
cessed using the scoring function in Equation 2.
This subset consists of the n = 5 sentences with
the highest sentiment scores of the ?right? polarity
? that is, if the document is positive (resp. nega-
tive), then the n = 5 sentences with the highest
positive (resp. negative) scores are selected.
4.4 Determining Frequencies and Weights
The absolute frequency of nouns and compound
nouns simply is computed as their token fre-
quency in the training set. For computing the rel-
ative frequency (as described in Section 3, Equa-
tion 1), we use the 20110405 dump of the English
Wikipedia.
In the product review corpora we studied,
the percentage of high-frequency keyphrase com-
pound nouns was higher than that of simple
nouns. We therefore use two different thresh-
olds for absolute and relative frequency. We de-
280
fine F1 as the set of nouns that are in the top
kn = 2.5% for both absolute and relative fre-
quencies; and F2 as the set of compounds that are
in the top kp = 5% for both absolute and rela-
tive frequencies. These thresholds are set to ob-
tain a high density of good keyphrases with few
false positives. Below the threshold there are still
other good keyphrases, but they cannot be sepa-
rated easily from non-keyphrases.
Sentences are scored according to Equation 2.
Recall that the parameters wf2 , wi2 , wf1 , and wi1
are determined using logistic regression. The ob-
tained parameter values (see table 2) indicate the
relative importance of the four different types of
terms. Compounds are the most important term
and even those with a frequency below the thresh-
old kp still provide more detailed information than
simple nouns above the threshold kn; the value of
wi2 is approximately twice the value wf1 for this
reason. Non-keyphrase nouns are least important
and are weighted with only a very small value of
wi1 = 0.01.
Phrase Par Value
keyphrase compounds wf2 1.07
non-keyphrase compounds wi2 0.89
keyphrase nouns wf1 0.46
non-keyphrase nouns wi1 0.01
Table 2: Weight settings
The scoring function with these parameter val-
ues is applied to the n = 5 selected sentences of
the review. The highest scoring sentence is then
selected as the supporting sentence proposed by
our system.
For 1380 of the 1744 reviews, the sentence se-
lected by our system is different from the baseline
sentence; however, there are 364 cases (20.9%)
where the two are the same. Only the 1380 cases
where the two methods differ are included in the
crowdsourcing evaluation to be described in the
next section. As we will show below, our sys-
tem selects better supporting sentences than the
baseline in most cases. So if baseline and our sys-
tem agree, then it is even more likely that the sen-
tence selected by both is a good supporting sen-
tence. However, there could also be cases where
the n = 5 sentences selected by the sentiment
classifier are all bad supporting sentences or cases
where the document does not contain any good
supporting sentences.
5 Comparative Evaluation with Amazon
Mechanical Turk
One standard way to evaluate summarization sys-
tems is to create hand-edited summaries and to
compute some measure of similarity (e.g., word
or n-gram overlap) between automatic and human
summaries. An alternative for extractive sum-
maries is to classify all sentences in the document
with respect to their appropriateness as summary
sentences. An automatic summary can then be
scored based on its ability to correctly identify
good summary sentences. Both of these meth-
ods require a large annotation effort and are most
likely too complex to be outsourced to a crowd-
sourcing service because the creation of manual
summaries requires skilled writers. For the sec-
ond type of evaluation, ranking sentences accord-
ing to a criterion is a lot more time consuming
than making a binary decision ? so ranking the
13 or 14 sentences that a review contains on av-
erage for the entire test set would be a signifi-
cant annotation effort. It would also be difficult
to obtain consistent and repeatable annotation in
crowdsourcing on this task due to its subtlety.
We therefore designed a novel evaluation
methodology in this paper that has a much smaller
startup cost. It is well known that relative judg-
ments are easier to make on difficult tasks than ab-
solute judgments. For example, much recent work
on relevance ranking in information retrieval re-
lies on relative relevance judgments (one docu-
ment is more relevant than another) rather than ab-
solute relevance judgments. We adopt this gen-
eral idea and only request such relative judgments
on supporting sentences from annotators. Unlike
a complete ranking of the sentences (which would
require m(m ? 1)/2 judgments where m is the
length of the review), we choose a setup where
we need to only elicit a single relative judgment
per review, one relative judgment on a sentence
pair (consisting of the baseline sentence and the
system sentence) for each of the 1380 reviews se-
lected in the previous section. This is a manage-
able annotation task that can be run on a crowd-
sourcing service in a short time and at little cost.
We use Amazon Mechanical Turk (AMT) for
this annotation task. The main advantage of AMT
is that cost per annotation task is very low, so that
we can obtain large annotated datasets for an af-
281
Task:
Sentence 1: This 5 meg camera meets all my requirements.
Sentence 2: Very good pictures, small bulk, long battery life.
 
Which sentence gives the more convincing reason? Fill out exactly one field, please.
Please type the blue word of the chosen sentence into the corresponding answer field.
s1 
s2 
If both sentences do not give a convincing reason, type NOTCONV into this answer
field.
X 
Submit
file:///Users/hs0711/example2.html
1 of 1 3/9/12 12:06 PM
Figure 1: AMT interface for annotators
fordable price. The disadvantage is the level of
quality of the annotation which will be discussed
at the end of this section.
5.1 Task Design
We created a HIT (Human Intelligence Task)
template including detailed annotation guidelines.
Every HIT consists of a pair of sentences. One
sentence is the baseline sentence; the other sen-
tence is the system sentence, i.e., the sentence se-
lected by the scoring function. The two sentences
are presented in random order to avoid bias.
The workers are then asked to evaluate the rel-
ative quality of the sentences by selecting one of
the following three options:
1. Sentence 1 has the more convincing reason
2. Sentence 2 has the more convincing reason
3. Neither sentence has a convincing reason
If both sentences contain reasons, the worker
has to compare the two reasons and choose the
sentence with the more convincing reason.
Each HIT was posted to three different workers
to make it possible to assess annotator agreement.
Every worker can process each HIT only once
so that the three assignments are always done by
three different people.
Based on the worker annotations, we compute a
gold standard score for each sentence. This score
is simply the number of times it was rated bet-
ter than its competitor. The score can be 0, 1, 2
or 3. HITs for which the worker chooses the op-
tion ?Neither sentence has a convincing reason?
are ignored when computing sentence scores.
The sentence with the higher score is then se-
lected as the best supporting sentence for the cor-
responding review.
In cases of ties, we posted the sentence pair one
more time for one worker. If one of the two sen-
tences has a higher score after this reposting, we
choose it as the winner. Otherwise we label this
sentence pair ?no decision? or ?N-D?.
5.2 Quality of AMT Annotations
Since our crowdsourcing based evaluation is
novel, it is important to investigate if human an-
notators perform the annotation consistently and
reproducibly.
The Fleiss? ? agreement score for the final
experiment is 0.17. AMT workers only have
the instructions given by the requesters. If they
are not clear enough or too complicated, work-
ers can misunderstand the task, which decreases
the quality of the answers. There are also AMT
workers who spam and give random answers to
tasks. Moreover, ranking sentences according to
the quality of the given reason is a subjective task.
Even if the sentence contains a reason, it might
not be convincing for the worker.
To ensure a high level of quality for our dataset,
282
Experiment # Docs BL SY N-D B=S
1 AMT, first pass 1380 27.4 57.9 14.7 -
2 AMT, second pass 203 46.8 45.8 7.4 -
3 AMT final 1380 34.3 64.6 1.1 -
4 AMT+[B=S] 1744 27.1 51.1 0.9 20.9
Table 3: AMT evaluation results. Numbers are percentages or counts. BL = baseline, SY = system, N-D = no
decision, B=S = same sentence selected by baseline and system
we took some precautions. To force workers to
actually read the sentences and not just click a
few boxes, we randomly marked one word of each
sentence blue. The worker had to type the word
of their preferred sentence into the corresponding
answer field or NOTCONV into the special field if
neither sentence was convincing. Figure 1 shows
our AMT interface design.
For each answer field we have a gold stan-
dard (the words we marked blue and the word
NOTCONV) which enables us to look for spam.
The analysis showed that some workers mistyped
some words, which however only indicates that
the worker actually typed the word instead of
copying it from the task. Some workers submit-
ted inconsistent answers, for instance, they typed
a random word or filled out all three answer fields.
In such cases we reposted this HIT again to re-
ceive a correct answer.
After the task, we counted how often a worker
said that neither sentence is convincing since a
high number indicates that the worker might have
only copied the word for several sentence pairs
without checking the content of the sentences. We
also analyzed the time a worker needed for every
HIT. Since no task was done in less than 10 sec-
onds, the possibility of just copying the word was
rather low.
6 Results and discussion
The results of the AMT experiment are shown in
table 3. As described above, each of the 1380
sentence pairs was evaluated by three workers.
Workers rated the system sentence as better for
57.9% of the reviews, and the baselines sentence
as better for 27.4% of the reviews; for 14.7% of
reviews, the scores of the two sentences were tied
(line 1 of Table 3). The 203 reviews in this cate-
gory were reposted one more time (as described in
Section 5). The responses were almost perfectly
evenly split: about 47% of workers preferred the
baseline system, 46% the system sentence; 7.4%
of the responses were undecided (line 2). Line 3
presents the consolidated results where the 14.7%
ties on line 1 are replaced by the ratings obtained
on line 2 in the second pass.
The consolidated results (line 3) show that our
system is clearly superior to the baseline of se-
lecting the sentence with the strongest sentiment.
Our system selected a better supporting sentence
for 64.6% of the reviews; the baseline selected a
better sentence for 34.3% of the reviews. These
results exclude the reviews where baseline and
system selected the same sentence. If we as-
sume that these sentences are also acceptable sen-
tences (since they score well on the traditional
sentiment metrics as well as on our new con-
tent keyword metric), then our system finds a
good supporting sentence for 72.0% of reviews
(51.1+20.9) whereas the baseline does so for only
48.0% (27.1+20.9).
6.1 Error Analysis
Our error analysis revealed that a significant pro-
portion of system sentences that were worse than
baseline sentences did contain a reason. How-
ever, the baseline sentence also contained a reason
and was rated better by AMT annotators. Exam-
ples (1) and (2) show two such cases. The first
sentence is the baseline sentence (BL) which was
rated better. The system sentence (SY) contains
a similar or different reason. Since rating reasons
is a very subjective task, it is impossible to de-
fine which of these two sentences contains the bet-
ter reason and depends on how the workers think
about it.
(1) BL:The best thing is that everything is just so
easily displayed and one doesn?t need a
manual to start getting the work done.
SY: The zoom is incredible, the video was so
clear that I actually thought of making a
15 min movie.
283
(2) BL:The colors are horrible, indoor shots are
horrible, and too much noise.
SY: Who cares about 8 mega pixels and 1600
iso when it takes such bad quality pic-
tures.
In example (3) the system sentence is an in-
complete sentence consisting of only two noun
phrases. These cut-off sentences are mainly
caused by incorrect usage of grammar and punc-
tuation by the reviewers which results in wrongly
determined sentence boundaries in the prepro-
cessing step.
(3) BL:Gives peace of mind to have it fit per-
fectly.
SY: battery and SD card.
In some cases, the two sentences that were pre-
sented to the worker in the evaluation had a dif-
ferent polarity. This can have two reasons: (i) due
to noisy training input, the classifier misclassified
some of the sentences, and (ii) for short reviews
we also used sentences with the non-conforming
polarity. Sentences with different polarity often
confused the workers and they tended to prefer
the positive sentence even if the negative one con-
tained a more convincing reason as can be seen in
example (4).
(4) BL:It shares same basic commands and
setup, so the learning curve was minimal.
SY: I was not blown away by the image qual-
ity, and as others have mentioned, the
flash really is weak.
A general problem with our approach is that the
weighting function favors sentences with many
noun phrases. The system sentence in example
(5) contains many noun phrases, including some
highly frequent nouns (e.g., ?lens?, ?battery?),
but there is no convincing reason and the baseline
sentence has been selected by the workers.
(5) BL:I have owned my cd300 for about 3 weeks
and have already taken 700 plus pictures.
SY: It has something to do with the lens be-
cause the manual says it only happens to
the 300 and when I called Sony tech sup-
port the guy tried to tell me the battery
was faulty and it wasn?t.
Finally, there are a number of cases where our
assumption that good supporting sentences con-
tain keyphrases is incorrect. For example, sen-
tence (6) does not contain any keyphrases indica-
tive of good reasons. The information that makes
it a good supporting sentence is mainly expressed
using verbs and particles.
(6) I have had an occasional problem with
the camera not booting up and telling me
to turn it off and then on again.
7 Conclusion
In this work, we presented a system that ex-
tracts supporting sentences, single-sentence sum-
maries of a document that contain a convincing
reason for the author?s opinion about a product.
We used an unsupervised approach that extracts
keyphrases of the given domain and then weights
these keyphrases to identify supporting sentences.
We used a novel comparative evaluation method-
ology with the crowdsourcing framework Ama-
zon Mechanical Turk to evaluate this novel task
since no gold standard is available. We showed
that our keyphrase-based system performs better
than a baseline of extracting the sentence with the
highest sentiment score.
8 Future work
Our method failed for some of the about 35% of
reviews where it did not find a convincing reason
because of the noisiness of reviews. Reviews are
user-generated content and contain grammatically
incorrect sentences and are full of typographical
errors. This problem makes it hard to perform pre-
processing steps like part-of-speech tagging and
sentence boundary detection correctly and reli-
ably. We plan to address these problems in fu-
ture work by developing a more robust processing
pipeline.
Acknowledgments
This work was supported by Deutsche
Forschungsgemeinschaft (Sonderforschungs-
bereich 732, Project D7) and in part by the
IST Programme of the European Community,
under the PASCAL2 Network of Excellence,
IST-2007-216886. This publication only reflects
the authors? views.
284
References
Shilpa Arora, Mahesh Joshi, and Carolyn P. Rose?.
2009. Identifying types of claims in online cus-
tomer reviews. In Proceedings of Human Lan-
guage Technologies: The 2009 Annual Conference
of the North American Chapter of the Association
for Computational Linguistics, Companion Volume:
Short Papers, NAACL-Short ?09, pages 37?40,
Stroudsburg, PA, USA. Association for Computa-
tional Linguistics.
Philip Beineke, Trevor Hastie, Christopher Manning,
and Shivakumar Vaithyanathan. 2004. Exploring
sentiment summarization. In Proceedings of the
AAAI Spring Symposium on Exploring Attitude and
Affect in Text: Theories and Applications. AAAI
Press. AAAI technical report SS-04-07.
Ga?bor Berend. 2011. Opinion expression mining by
exploiting keyphrase extraction. In Proceedings of
5th International Joint Conference on Natural Lan-
guage Processing, pages 1162?1170, Chiang Mai,
Thailand, November. Asian Federation of Natural
Language Processing.
Minqing Hu and Bing Liu. 2004. Mining and sum-
marizing customer reviews. In Proceedings of the
Tenth ACM SIGKDD international conference on
Knowledge discovery and data mining, KDD ?04,
pages 168?177, New York, NY, USA. ACM.
Nitin Jindal and Bing Liu. 2008. Opinion spam
and analysis. In WSDM ?08: Proceedings of the
international conference on Web search and web
data mining, pages 219?230, New York, NY, USA.
ACM.
Soo-Min Kim and Eduard Hovy. 2006. Automatic
identification of pro and con reasons in online re-
views. In Proceedings of the COLING/ACL on
Main conference poster sessions, COLING-ACL
?06, pages 483?490, Stroudsburg, PA, USA. Asso-
ciation for Computational Linguistics.
Bing Liu. 2010. Sentiment analysis and subjectivity.
Handbook of Natural Language Processing, 2nd ed.
Christopher Manning and Dan Klein. 2003. Opti-
mization, maxent models, and conditional estima-
tion without magic. In Proceedings of the 2003
Conference of the North American Chapter of the
Association for Computational Linguistics on Hu-
man Language Technology: Tutorials - Volume 5,
NAACL-Tutorials ?03, pages 8?8, Stroudsburg, PA,
USA. Association for Computational Linguistics.
Ani Nenkova and Kathleen McKeown. 2011. Auto-
matic summarization. Foundations and Trends in
Information Retrieval, 5(2-3):103?233.
Bo Pang and Lillian Lee. 2008. Opinion mining and
sentiment analysis. Foundations and Trends in In-
formation Retrieval, 2(1-2):1?135.
Helmut Schmid. 1994. Probabilistic part-of-speech
tagging using decision trees. In Proceedings of the
International Conference on New Methods in Lan-
guage Processing, Manchester, UK.
Li Zhuang, Feng Jing, and Xiao-Yan Zhu. 2006.
Movie review mining and summarization. In Pro-
ceedings of the 15th ACM international conference
on Information and knowledge management, CIKM
?06, pages 43?50, New York, NY, USA. ACM.
285
Knowledge Sources for Constituent Parsing
of German, a Morphologically Rich and
Less-Configurational Language
Alexander Fraser?
Institute for NLP, University of Stuttgart
Helmut Schmid??
Institute for NLP, University of Stuttgart
Richa?rd Farkas?
Institute for NLP, University of Stuttgart
Renjing Wang?
Institute for NLP, University of Stuttgart
Hinrich Schu?tze?
Institute for NLP, University of Stuttgart
We study constituent parsing of German, a morphologically rich and less-configurational
language. We use a probabilistic context-free grammar treebank grammar that has been adapted
to the morphologically rich properties of German by markovization and special features added
to its productions. We evaluate the impact of adding lexical knowledge. Then we examine both
monolingual and bilingual approaches to parse reranking. Our reranking parser is the new state
of the art in constituency parsing of the TIGER Treebank. We perform an analysis, concluding
with lessons learned, which apply to parsing other morphologically rich and less-configurational
languages.
? Institute for Natural Language Processing, University of Stuttgart, Pfaffenwaldring 5b, 70569 Stuttgart,
Germany. E-mail: fraser@ims.uni-stuttgart.de.
?? Institute for Natural Language Processing, University of Stuttgart, Pfaffenwaldring 5b, 70569 Stuttgart,
Germany. E-mail: schmid@ims.uni-stuttgart.de.
? Institute for Natural Language Processing, University of Stuttgart, Pfaffenwaldring 5b, 70569 Stuttgart,
Germany. E-mail: farkas@ims.uni-stuttgart.de.
? Institute for Natural Language Processing, University of Stuttgart, Pfaffenwaldring 5b, 70569 Stuttgart,
Germany.
? Institute for Natural Language Processing, University of Stuttgart, Pfaffenwaldring 5b, 70569 Stuttgart,
Germany.
Submission received: October 1, 2011; revised submission received: May 30, 2012; accepted for publication:
August 3, 2012
? 2013 Association for Computational Linguistics
Computational Linguistics Volume 39, Number 1
1. Introduction
A large part of the methodology for parsing in natural language processing has been
developed for English and a majority of publications on parsing are about parsing
of English. English is a strongly configurational language. Nearly all of the syntactic
information needed by anyNLP application can be obtained by configurational analysis
(e.g., by having a correct constituent parse).
Many other languages of the world are fundamentally different from English in this
respect. At the other end of the configurational?nonconfigurational spectrum we find a
language like Hungarian that has very little fixed structure on the level of the sentence.
Leaving aside the issue of the internal structure of NPs, most sentence-level syntactic
information in Hungarian is conveyed by morphology, not by configuration.
In this paper, we address German, a third type of language that is intermediate
between English and Hungarian. German has strong configurational constraints (e.g.,
main clauses are verb-second) as well as rich derivational and inflectional morphology,
all of which must be modeled for high-quality parsing. German?s intermediate status
raises a number of interesting issues in parsing that are of particular prominence for a
mixed configurational/morphological language, but are?as we will argue?of general
relevance for morphologically rich languages. Partly this is the case because there are
few (if any) languages archetypical of being purely configurational and purely noncon-
figurational (e.g., morphology is also important for English and even Hungarian has
configurational constraints). For lack of a better termwe refer to intermediate languages
as typified by German as MR&LC for morphologically rich and less-configurational.
Part of the motivation for this special issue is that most work on parsing to date
has been done on English, a morphologically simple language. As computational lin-
guistics broadens its focus beyond English it becomes important to take a more general
approach to parsing that can handle languages that are typologically very different from
English. Rich morphology (RM) is one very salient characteristic of a language that
affects the design of parsing methods. We argue that there are two other properties
of languages that are relevant in a discussion of parsing RM languages: syncretism
and configurationality. These two properties are correlated typologically with RM and
should therefore be taken into account when we address parsing RM languages.1
We first define the three properties and explain their relevance for parsing. The
large number of languages for which this correlation holds can be ordered along a
single dimension that can be interpreted as degree of morphological complexity. We
give examples for a number of languages that are positioned at different points on this
scale. Finally, we argue that just as languages that are at the opposite end of the spectrum
from English (prototypical examples of morphological richness like Hungarian) require
parsing methods that can be quite different from those optimal for English, the same
is true for a language like German that is in the middle of the spectrum?and what is
required is in some respects different from what is optimal for one extreme (English) or
the other (Hungarian).
The three correlated properties are rich morphology, syncretism, and configura-
tionality. Morphological richness can be roughly measured by the number of different
morphological forms a word of a particular syntactic category can have; for example,
1 We note, however, that this relationship is not a language universal. It is instead a frequently observed
correlation; for Chinese, for instance, the correlation does not seem to hold as strongly.
58
Fraser et al Knowledge Sources for Parsing German
a typical English noun has two forms (singular and plural), a typical German noun
has eight forms (singular and plural in four different cases), and a typical Hungarian
noun has several hundreds of forms. Syncretism refers to the fact that different mor-
phological forms have identical surface realization; for example, the formMann (?man?
in German) can be the nominative, dative, or accusative singular of Mann depending
on context. Configurationality refers to the degree to which the arrangement of words
and phrases of a particular syntactic function in a sentence is fixed. English is highly
configurational: it has limited flexibility in how the major phrases in a sentence (subject,
verb, direct object, indirect object, etc.) can be ordered. Hungarian and Latin are highly
flexible: Even though there are pragmatic constraints, in principle a large number of
possible orderings are grammatical. German is less configurational. It has some strict
constraints (verb second in main clauses, verb final in subordinate clauses), but also
some properties of a nonconfigurational language; for example, ordering of phrases
in the mittelfeld (the part of the main clause enclosed by the two parts of the verbal
complex) is very flexible.
It is obvious why configurationality and rich morphology are typologically (neg-
atively) correlated. Rich morphology specifies the syntactic role of a phrase in the
sentence, so fixing a position is not required, and many morphologically rich languages
therefore do not fix the position. Conversely, simple morphology gives little specific
information about the role of words and phrases in the sentence. One device often used
by morphologically simple languages to address this problem and reduce widespread
ambiguity is to fix the order of words and phrases in the sentence.
Syncretism has an effect that is similar to simplification of complex morphology.
Simple morphology is unspecific about grammatical function because it uses a small
number of morphological categories. Syncretism is unspecific about grammatical func-
tion because it suffers from a high degree of ambiguity. Even though the number of
different morphological categories is potentially large, syncretic forms conflate many of
these categories, so that these forms are much less helpful in determining grammatical
function than forms in a nonsyncretic language with the same number of categories.
Again, to counteract the communicative difficulties that lack of morphological speci-
ficity would create, stricter constraints on ordering and configuration are often used by
syncretic languages.
We have used English and Hungarian as examples for the extremes and German
for the middle of the spectrum. We now give examples of other languages and their
positions on the scale. Dutch is similar to German in that it also is verb second in main
clauses and verb final in subordinate clauses. The order of arguments in the mittelfeld is
much more restricted than in German, however. At the same time, Dutch morphology
has been much more simplified in the last centuries than German morphology. This
nicely confirms the correlation between RM and configurationality. Thus, Dutch is
positioned between English and German on the scale.
Classical Arabic is somewhat similar to German: The number of different morpho-
logical forms is roughly comparable to German and it allows a number of different
word orders. Modern Standard Arabic speakers rarely mark case, however, at least not
in spontaneous speech. At the same time, Modern Standard Arabic speakers use SVO
order much more frequently and consistently than is the case in Classical Arabic. Thus,
Classical Arabic is roughly at the same position as German on the scale whereas spoken
Modern Standard Arabic may be more comparable to Dutch.
Finally, Modern Greek is a language that is intermediate between German and
Hungarian. It has richer morphology than German, but it has a fair amount of syn-
cretism and therefore more morphological ambiguity than Hungarian. SVO is the
59
Computational Linguistics Volume 39, Number 1
predominant word order in modern Greek, but other word orders can be used. The
order within the noun phrase is more flexible than in German: Adjectives can precede
or follow the noun.
In the examples we have given, the amount of information conveyed by a mor-
phological form is negatively correlated with the amount of information conveyed by
configuration. If morphology conveys a lot of information (due to a large number of
distinctions and the lack of syncretism), then word order is freer and conveys less
information. If morphology conveys less information (due to fewer distinctions or more
syncretism), then configuration is fixed and provides more information to the speaker.
This suggests that RM and configuration are important variables that should be taken
into account in the design of parsing methods. In addition to looking at the extremes of
the spectrum that are exemplified by English andHungarian, we should also investigate
the middle: morphologically (somewhat) rich languages that are less configurational. In
this article, we look at the example of German.
One key question for MR&LC parsing is which type of parsing formalism to adopt,
constituency or dependency. It is a widely held belief that dependency structures are
better suited to represent syntactic analyses for morphologically rich languages because
they allow non-projective structures (the equivalent of discontinuous constituents in
constituency parsing). As Tsarfaty et al (2010) point out, however, this is not the same
as proving that dependency parsers function better than constituency parsers for pars-
ing morphologically rich languages. In fact, most state-of-the-art dependency parsers
(McDonald and Pereira 2006; Hall and Nivre 2008; Seeker et al 2010a) generate purely
projective dependency structures that are optionally transformed into non-projective
structures in a post-processing step. Comparable post-processing techniques have been
used in English constituency parsing (Gabbard, Marcus, and Kulick 2006; Schmid 2006;
Cai, Chiang, and Goldberg 2011) to identify discontinuous constituents andmight work
for other languages, as well.
The overview paper of the Parsing German Shared Task (Ku?bler 2008) reports
higher accuracies for detecting grammatical functions with dependency parsers than
with constituent parsers, but the direct comparison is not fair as it required phrase
boundaries to be correct on the constituent side while the tokens were the unit of
evaluation on the dependency side.2 How to carry out an absolutely fair comparison
of the two representations is still an open research question.3
Constituent parses often provide more information than dependency parses. An
example is the coordination ambiguity in old men and women versus old men and children.
The correct constituent parse for the first expression contains a coordination at the
noun level whereas the parse for the second expression coordinates at the level of
NPs. The dependency structures of both expressions, on the other hand, are usually
identical and thus unable to reflect the fact that oldmodifies women but not children. It is
possible, in principle, to encode the difference in dependency trees (cf. Rambow 2010),
2 This is due to how the evalb tool used to calculate PARSEVAL works. If a constituent is not perfectly
matched, the grammatical function is considered to be wrong, even if there was a partial match (at the
token level). This is not a problem with dependency-based evaluation. For further discussion of the
PARSEVAL metric and dependency-based evaluation see, for example, Rehbein and van Genabith (2007)
and Tsarfaty, Nivre, and Andersson (2012).
3 Two possible solutions are to use TedEval (Tsarfaty, Nivre, and Andersson 2012), or to conduct an analysis
of grammatical functions at the token level in a consistent fashion for both dependency and constituent
parsers. In our case, the latter would require a high quality conversion from the Tiger constituency
representation to a dependency representation, which we hope to implement in future work.
60
Fraser et al Knowledge Sources for Parsing German
for example, by enriching the edge labels, but the constituent representation is simpler
for this phenomenon.
Finally, there are some applications that need constituent parses rather than depen-
dency parses. For instance, many hierarchical statistical machine translation systems
use constituency parses, requiring the output of a dependency parser to be transformed
into a constituent parse.4 We conclude that there is no clear evidence for preferring
dependency parsing over constituency parsing in analyzing languages with RM and
instead argue that research in both frameworks is important.
We view the detailed description of a constituency parsing system for a mor-
phologically rich language, a system that addresses the major problems that arise in
constituency parsing for MR&LC, as one of our main contributions in this paper.
The first problem we address is the proliferation of phrase structure rules in
MR&LC languages. For example, there are a large number of possible orderings of the
phrases in the German mittelfeld, and many orderings are exceedingly rare. A standard
constituency parser cannot estimate probabilities for the corresponding rules reliably.
The solution we adopt here is markovization?complex rules are decomposed
into small unidirectional rules that can be modeled and estimated more reliably than
complex rules. Although markovization in itself is not new, we stress its importance for
MR&LC languages here and present a detailed, reproducible account of how we use it
for German. Markovization combines the best of both worlds for MR&LC languages:
Preferential configurational information can be formalized and exploited by the parser
without incurring too large of a performance penalty due to sparse data problems.
The second problem that needs to be addressed in parsingmanyMR&LC languages
is widespread syncretism. We mainly address syncretism by using a high performance
finite-state automata-based morphological analyzer. Such an analyzer is of obvious
importance for any morphologically rich language because the productivity of mor-
phologically rich languages significantly increases the unknown-word rate in new text
versus morphologically poor languages. So the parser cannot simply memorize the
grammatical properties of words in the Treebank used for training. Instead we incorpo-
rate a complex guesser into our parser that, based on the input from the morphological
analyzer, predicts the grammatical properties of new words and (equally important)
unobserved grammatical properties of known words. With prevailing syncretism, this
task is muchmore complex than in a language where case, gender, number, and so forth,
can be deterministically deduced from morphology.
The morphological analyzer is based on (i) a finite state formalization of German
morphology and (ii) a large lexicon of morphologically analyzed German words. We
refer to these two components together as lexical knowledge. We show that lexical
knowledge is beneficial for parsing performance for an MR&LC language like German.
In addition to lexical knowledge, there is a second important aspect of syncretism
that needs to be addressed in MR&LC languages. Syntactic disambiguation in these
languages must always involve both systems of grammatical encoding, morphology
and configuration, acting together. The most natural way of doing this in a language
like German is to perform this integration of the two knowledge sources directly as part
of parsing.We do this by annotating constituent labels with grammatical functionwhere
appropriate. In contrast with syntactic parses of strongly configurational languages
like English, syntactic parses of German are not useful for most tasks without having
4 We do note, however, that there are a few translation systems which use a dependency representation
directly (e.g., Quirk, Menezes, and Cherry 2005; Shen, Xu, and Weischedel 2008; Tu et al 2010).
61
Computational Linguistics Volume 39, Number 1
grammatical functions indicated. It is not even possible to access the basic subcatego-
rization of the verb (such as determining the subject) without grammatical functions.
We argue that MR&LC languages like German should always be evaluated on labels-
cum-grammatical-function.
Our last main contribution in this paper concerns the fact that we believe that
MR&LC languages give rise to more ambiguity than languages that are predominantly
configurational or morphological. As an example consider the German sentence ?Die
[the] Katze [cat] jagt [hunts] die [the] Schlange [snake].? In German either the cat or the
snake can be the hunter. This type of ambiguity neither occurs in a strongly configu-
rational language like English (where configuration determines grammatical function)
nor in a morphologically rich language like Hungarian that has no or little syncretism
(where morphology determines grammatical function). Although morphology and
configuration in MR&LC languages often work hand in hand for complete disambigua-
tion, there are also many sentences where neither of the two provides the necessary
information for disambiguation. We believe that this distinguishing characteristic of
MR&LC languages makes it necessary to tap additional knowledge sources. In this
paper, we look at two such knowledge sources: monolingual reranking (which captures
global properties of well-formed parses for additional disambiguation) and bilingual
reranking (which exploits parallel text in a different language for disambiguation).
For monolingual reranking, we define a novel set of rich features based on sub-
categorization frames. We compare our compact feature set with a sparse feature set
designed for German previously by Versley and Rehbein (2009). We show that the
richer subcategorization-based framework for monolingual reranking is effective; it has
comparable performance to the sparse feature set?moreover, they complement each
other.
For bilingual reranking, we present our approach to bitext parsing, where a German
parse is found that minimizes syntactic divergence with an automatically generated
parse of its English translation. We pursue this approach for a number of reasons. First,
one limiting factor for syntactic approaches to statistical machine translation is parse
quality (Quirk and Corston-Oliver 2006). Improved parses of bitext should result in
improved machine translation. Second, as more and more texts are available in several
languages, it will be increasingly the case that a text to be parsed is itself part of a
bitext. Third, we hope that the improved parses of bitext can serve as higher quality
training data for improving monolingual parsing using a process similar to self-training
(McClosky, Charniak, and Johnson 2006a).
We show that the three different knowledge sources we use in this paper (lexical
knowledge, monolingual features, and bilingual features) are valuable separately. We
also show that the gain of the two sets of reranking features (monolingual and bilingual)
is additive, suggesting that they capture different types of information.
The resulting parser is currently the best constituent parser for German (with or
without bilingual features). In particular, we show that the baseline parser without
reranking is competitive with the previous state of the art (the Berkeley parser) and
that the re-ranking can add an important gain.
2. Previous Work
Constituent parsing for English is well studied. The best generative constituent parsers
are currently the Brown reranking parser (Charniak and Johnson 2005), the exten-
sion of this parser with self training by McClosky, Charniak, and Johnson (2006b),
and the parser of Petrov and Klein (2007), which is an unlexicalized probabilistic
62
Fraser et al Knowledge Sources for Parsing German
context-free grammar (PCFG) parser with latent feature annotations. Charniak and
Johnson (2005) and Huang (2008) have introduced a significant improvement by
feature-rich discriminative reranking as well.
The number of treebank constituent parsers for German is smaller. Dubey and
Keller (2003) adapted Collins?s (1997) lexicalized parser to German. An unlexicalized
PCFG parser similar to our generative parser was presented by Schiehlen (2004). The
best constituent parser participating in the ACL-08 Workshop on Parsing German
(Ku?bler 2008) was the Berkeley parser (Petrov and Klein 2008). The Stanford parser
was also adapted to German (Rafferty andManning 2008). German dependency parsers
have been developed by Menzel and Schro?der (1998), Duchier and Debusmann (2001),
Hall and Nivre (2008), Henderson et al (2008), and Seeker et al (2010a), to name
a few.
There is also some previous work on German parse reranking. Forst (2007) pre-
sented a reranker for German LFG parsing, and Dreyer, Smith, and Smith (2006) applied
reranking to German dependency parsing. Versley and Rehbein (2009) developed a
reranking method for German constituent parsers. The work by Versley and Rehbein
and by Schiehlen (2004) is closest to ours. Like them, we rerank the unlexicalized BitPar
parser. We also refine treebank labels to increase parsing performance, but add more
information and achieve a larger improvement. We use the monolingual feature set of
Versley and Rehbein in our reranker, but add further monolingual features as well as
bilingual features.
3. Generative Parsing Framework
Our generative parser is an unlexicalized PCFG parser which is based on the BitPar
parser (Schmid 2004). BitPar uses a fast bitvector-based implementation of the well-
known Cocke-Younger-Kasami algorithm and stores the chart as a large bit vector.
This representation is memory efficient and allows full parsing (without search space
pruning) with large treebank grammars. BitPar is also quite fast because the basic
parsing operations are parallelized by means of (single-instruction) and-operations on
bitvectors. BitPar can either be used to compute the most likely parse (Viterbi parse), or
the full set of parses in the form of a parse forest, or the n-best parse trees.
3.1 Grammar
The grammar and lexicon used by our generative parser are extracted from the Tiger2
Treebank (Brants et al 2002). Similar to Johnson (1998) andKlein andManning (2003) we
improve the accuracy of the unlexicalized parser by refining the non-terminal symbols
of the grammar to encode relevant contextual information. This refinement weakens
the strong independence assumptions of PCFGs and improves parsing accuracy. The
extraction of the grammar and lexicon involves the following steps:
1. Discontinuous constituents are eliminated (Section 3.2).
2. Treebank annotations are transformed (Section 3.4) and augmented
(Section 3.5).
3. Grammar rules, lexical rules, and their frequencies are extracted from the
annotated parse trees.
4. The grammar is markovized (Section 3.6).
63
Computational Linguistics Volume 39, Number 1
.
S-TOP
PROPAV-OP-1
Daraus
This-from
VMFIN-HD
kann
can
VP-OC
VP-OC
PROAV-OP
*T*-1
VVPP-HD
gefolgert
concluded
VAINF-HD
werden
be
Figure 1
Projectivized parse tree for the sentence: Daraus kann gefolgert werden [From this can be
concluded].
3.2 Raising for Non-Projectivity
The Tiger2 Treebank that we used in our experiments contains discontinuous con-
stituents. As in other work on German parsing using the Tiger2 Treebank (Dubey
and Keller 2003; Schiehlen 2004; Ku?bler, Hinrichs, and Maier 2006), we eliminated
discontinuous constituents by raising those parts of the discontinuous constituent that
do not contain the head to the child position of an ancestor node of the discontinuous
constituent. Hsu (2010) compared three different Tiger2 conversion schemes and found
raising to be the most effective. The projective parse tree in Figure 1, for instance, is
obtained from a Tiger parse tree where the pronominal adverb Daraus was a dis-
continuous child of the lower VP-OC node.
The parse tree in Figure 1 shows a trace node and coreference indices (similar to
the Penn Treebank annotation style for discontinuous constituents). If slash features
are added to the nodes on the path between the PROAV node and its trace within the
VP, it is possible to restore discontinuous constituents (Schmid 2006). Due to sparse
data problems caused by the added slash features, however, the parsing accuracy
drops by 1.5% compared with the version without slash features (when evaluated on
projectivized parse trees). Traces are recognized with a precision of 53% and a recall of
33%. The correct antecedents are identified with a precision of 48% and a recall of 30%.
These figures indicate that the identification of discontinuous constituents in Tiger parse
trees is a harder task than in English Penn Treebank parses, considering the 84% F-score
for the recognition of empty constituents and the 77% F-score for the identification of
antecedents reported in Schmid (2006) for an analogous approach.
As the example in Figure 1 shows, the precise attachment point of constituents
is often not required: We can simply assume that all constituents appearing at the S
level are dependents of the main verb of the clause. Only for modifiers with scope
ambiguities (e.g., negation particles) is it relevant whether they are attached at the S
or VP level. These considerations suggest that it is better to recognize discontinuous
constituents in a post-processing step as in Johnson (2001), Campbell (2004), and Levy
and Manning (2004). In the rest of the paper, we will only work with parse trees from
which coreference indices and trace nodes have been removed.
3.3 Morphological Features and Grammatical Functions
The Tiger2 Treebank annotates non-terminals not only with syntactic categories but
also with grammatical function labels such as SB (subject), OA (accusative object), or
64
Fraser et al Knowledge Sources for Parsing German
MO (modifier). These labels provide important information that is necessary in order to
derive a semantic representation from a parse. It is not possible to infer the grammatical
role of a constituent from its position in the parse tree alone (as can be done in English,
for instance). Case information is needed in addition in order to help determine the
correct grammatical role. The Tiger2 Treebank provides case, number, degree (positive,
comparative, superlative), and gender information at the part-of-speech (POS) level.
Our parser concatenates the grammatical function labels as well as the case infor-
mation of the POS tags to the base labels similarly to Dubey (2004) and Versley (2005).
Our earlier experiments showed that adding case information increases F-score by 2.1%
absolute. Further enriching the grammar with morphological features, however, hurts
performance. Adding number features decreased F-score by 0.5%. Adding number,
gender, and degree decreased F-score by 1.6%. When grammatical functions are taken
into account in the evaluation, the performance drops by 1.5% when number, gender,
and degree features are incorporated. It seems that the additional information supplied
by the agreement features is not useful enough to outweigh sparse data problems
caused by the more fine-grained label set. Therefore we only use case, but designing
a smoothing procedure allowing us to use number, gender, and degree is interesting
future work.
3.4 Tree Transformations
Similarly to Schiehlen (2004), we automatically augment the Tiger2 annotation with ad-
ditional feature annotations. Our feature annotation set is larger than that of Schiehlen.
In addition to making feature annotations, we also perform some tree transformations
that reduce the complexity of the grammar. In all evaluations, we use the original
(projectivized) Tiger parse trees as gold standard and convert the parse trees generated
by our parser to the same format by undoing the transformations and removing the
additional features. In the rest of this section, we explain the tree transformations that
we used. The following section describes the feature annotations.5
Unary branching rules. The Tiger Treebank avoids unary branching nodes. NPs
and other phrasal categories which dominate just a single node are usually omitted.
The sentence Sie zo?gern [They hesitate], for instance, is analyzed as (S-TOP (PPER-SB
Sie) (VVFIN-HD zo?gern)) without an explicit NP or VP. The lack of unary branching
nodes increases the number of rules because now a rule S-TOP? PPER-SB VVFIN-HD
is needed in addition to the rule S-TOP? NP-SB VVFIN-HD, for instance.
In order to reduce sparse-data problems, we insert unary branching nodes and
transform this parse to (S-TOP (NP-SB (PPER-HD Sie)) (VVFIN-HD zo?gern)) by adding
an NP node with the grammatical function (GF) of the pronoun. The GF of the pronoun,
in turn, is replaced by HD (head). Such unary branching NPs are added on top of nouns
(NN), pronouns (PPER, PDS, PIS, PRELS), cardinals (CARD), and complex proper
names (PN) that are dominated by S, VP, TOP, or DL6 nodes.7 The transformation is
reversible, which allows the original annotation to be restored.
5 Descriptions of the different symbols used in the Tiger annotation scheme are available at
http://www.ims.uni-stuttgart.de/tcl/RESOURCES/CL.html.
6 DL is a discourse level constituent.
7 If a single proper name (NE) forms a noun phrase, we first add a PN node and then an NP node on top.
If a simple noun (NN) with a GF other than NK appears inside of an NP, PP, CNP, CO, or AP, we also add
an NP node on top of it. Similarly, we add a PN node on top of proper names (NE) in the same context.
65
Computational Linguistics Volume 39, Number 1
.
CPP-MO
KON-CD
weder
neither
PP-CJ
APPR-AC
in
in
NE-NK
Berlin
Berlin
KON-CD
noch
nor
PP-CJ
APPR-AC
in
in
NE-NK
Frankfurt
Frankfurt
.
CPP-MO
KON-CD/weder
weder
neither
PP-MO
APPR-AC/in
in
in
NE-HD
Berlin
Berlin
KON-CD/noch
noch
nor
PP-MO
APPR-AC/in
in
in
NE-HD
Frankfurt
Frankfurt
Figure 2
Parse of the phrase weder in Berlin noch in Frankfurt [neither in Berlin nor in Frankfurt] before
and after selective lexicalization of prepositions and conjunctions. This example also shows the
replacement of the grammatical function features CJ and NK discussed in the previous section.
The modified parts are printed in boldface.
By adding a unary-branching NP-SB node, for instance, we introduce an additional
independence assumption, namely, we assume that the expansion of the subject NP is
independent of the other arguments and adjuncts of the verb (a plausible assumption
that is confirmed by a performance improvement).
Elimination of NK. Tiger normally uses the grammatical function HD to mark the
head of a phrase. In case of NPs and PPs, however, the GF of the head is NK (noun
kernel). The same GF is also assigned to the adjectives and determiners of the noun
phrase. We replace NK by HD in order to reduce the set of symbols.8
Elimination of CJ. Tiger annotates each conjunct in a coordination with the spe-
cial grammatical function label CJ. We replace CJ by the grammatical function of the
coordinated phrase. This transformation is also reversible.
3.5 Additional Feature Annotations
Selective lexicalization. We mark the POS tags of the frequent prepositions in [in], von
[from, of], auf [on], durch [through, by means of], unter [under], um [around, at] and
their variants regarding capitalization (e.g., Unter) and incorporation of articles (e.g.,
unters, unterm) with a feature which identifies the preposition. This can be seen as a
restricted form of lexicalization. In the same way, we also ?lexicalize? the coordinating
conjunctions (KON-CD) sowohl [as well], als [as], weder [neither], noch [nor], entweder
[either], and oder [or] if preceded by entweder. Figure 2 shows an example.
Sentence punctuation. If a clause node (S) has a sibling node labeled with POS tag
?$.? that dominates a question mark or exclamation mark, then the clause node and the
POS tag are annotated with quest or excl, so the grammar models different clause types.
8 The original annotation can be restored: HD never occurs in NP or PP children in original Tiger parses.
66
Fraser et al Knowledge Sources for Parsing German
.
CS-RC
S-RC
NP-SB/rel
PRELS-HD-Nom
die
who
NP-OA
NN-HD-ACC
Surfen
surfing
VVFIN-HD
sagen
say
KON-CD
und
and
S-RC/norel/nosubj
NP-OA
NN-HD-Acc
Freiheit
freedom
VVFIN-HD
meinen
mean
Figure 3
Parse of the phrase die Surfen sagen und Freiheit meinen [who say surfing and mean freedom] before
and after annotation with relative clause features. This example also shows the nosubj feature,
which will be discussed later.
Adjunct attachment. Adjuncts often differ with respect to their preferred attach-
ment sites. Therefore, we annotate PPs and adverbials (AVP, ADV, ADJD) with one of
the features N, V, or 0 which indicate a nominal parent (NP or PP), a verbal parent
(VP, S), or anything else, respectively. In case of adverbial phrases (AVP), the label is
propagated to the head child.
Relative clause features. In many relative clauses (S-RC), the relative pronoun
(PRELS, PRELAT, PWAV, PWS) is embedded inside of another constituent. In this case,
all nodes on the path between the pronoun and the clause node are marked with the
feature rel. Furthermore, we add a feature norel to relative clauses if no relative pronoun
is found. Figure 3 shows an example.
Wh features. Similar to the feature rel assigned to phrases that dominate a relative
pronoun, we use a feature wh which is assigned to all NPs and PPs which immediately
dominate a wh-pronoun (PWAT, PWS, PWAV). This feature better restricts the positions
where such NPs and PPs can occur.
Noun sequence feature. If two nouns occur together within a GermanNP (as in drei
Liter Milch [three liters (of) milk] or Ende Januar [end (of) January]), then the first noun
is usually a kind of measure noun. We mark it with the feature seq.
Proper name chunks. Some noun phrases such as Frankfurter Rundschau, Junge
Union, Die Zeit are used as proper names. In this case, the grammatical function of the
NP is PNC. In order to restrict the nouns and adjectives that can occur inside of such
proper name chunks, we mark their POS tags with the feature name.
Predicative APs. Complex adjectival phrases (AP) are either attributively used as
noun modifiers inside of an NP or PP, or predicatively elsewhere. In order to better
model the two types of APs, we mark APs that dominate a predicative adjective (ADJD)
with the feature pred.9
Nominal heads of APs. Sometimes the head of an AP is a noun as in (AP drei
Millionen) Mark [three million Marks] or ein (AP politisch Verfolgter) [a politically
persecuted-person]. We mark these APs with the feature nom.
Year numbers.Years such as 1998 can appear in places where other numbers cannot.
Therefore POS tags of numbers between 1900 and 2019 are marked with year.10
Clause type feature for conjunctions. The type of a subordinate clause and the
subordinating conjunction are highly correlated. German object clauses (S-OC) usually
9 We also mark an AP parent of a node with the label AP-HD/pred in the same way.
10 For some texts, it might be advantageous to use a broader definition of year numbers.
67
Computational Linguistics Volume 39, Number 1
start with dass [that] or ob [whether]; modifier clauses (S-MO) often start with wenn
[if], weil [because], or als [when]. We mark subordinating conjunctions of argument
clauses (S-OC), modifier clauses (S-MO), subject clauses (S-SB), and dislocated clauses
(S-RE) with a feature (OC,MO, SB, or RE) identifying clause type. Without this feature,
argument clauses of nouns, for instance, are often misanalyzed as modifiers of the main
clause.
VP features. VPs that are headed by finite verbs, infinitives, past participles, imper-
atives, and zu infinitives are all used in different contexts. Therefore we mark object VPs
(VP-OC) with a corresponding feature. When parsing the sentence Alle Ra?ume mu?ssen
mehrfach gesa?ubert und desinfiziert werden [all rooms must multiply cleaned and disin-
fected be; all rooms must be ...], this feature allows the parser to correctly coordinate the
two past participle VPs mehrfach gesa?ubert and desinfiziert instead of the past participle
VP mehrfach gesa?ubert and the infinitival VP desinfiziert werden.
Phrases without a head. Some phrases in the Tiger corpus lack a head. This is
frequent in coordinations. All phrases that do not have a child node with one of the
grammatical functions HD, PNC, AC, AVC, NMC, PH, PD, ADC, UC, or DH aremarked
with the feature nohead.
Clauses without a subject. We also mark conjunct clauses with the feature nosubj
if they are neither headed by an imperative nor contain a child node with the gram-
matical function SB (subject) or EP (expletive). This is useful in order to correctly parse
coordinations where the subject is dropped in the second conjunct.
3.6 Markovization
The Tiger Treebank uses rather flat structures where nodes have up to 25 child nodes.
This causes sparse data problems because only some of the possible rules of that length
actually appear in the training corpus. The sparse data problem is solved bymarkoviza-
tion (Collins 1997; Klein and Manning 2003), which splits long rules into a set of shorter
rules. The shorter rules generate the child nodes of the original rule one by one. First,
the left siblings of the head child of the rule are generated from left to right, then the
right siblings are generated from right to left. Finally, the head is generated. Figure 4
shows the markovization of the rule NP? NMNN PP PP.
The auxiliary symbols that are used here encode information about the parent cat-
egory, the head child, and previously generated children. Because all auxiliary symbols
encode the head category, the head is already selected by the first rule, but only later
actually generated by the last rule.
.
NP
NM ?L:NP[NN]NN|NM?
?M:NP[NN]?
?R:NP[NN]PP|PP?
?R:NP[NN]NN|PP?
NN
PP
PP
Figure 4
Markovization of the rule NP? NMNN PP PP.
68
Fraser et al Knowledge Sources for Parsing German
The general form of the auxiliary symbols is ?direction:parent[head]next|previous?
where direction is either L, M, or R, parent is the symbol on the left hand side of the
rule, head is the head on the right hand side of the rule, next is the symbol which will
be generated next, and previous is the symbol that was generated before. Auxiliaries
starting with L generate the children to the left of the head. Auxiliaries starting with
R similarly generate the children to the right of the head and the head itself. The
auxiliary starting with M is used to switch from generating left children to generating
right children. Each rule contains information about the parent, the head, and (usually)
three child symbols (which may include an imaginary boundary symbol). The first rule
encodes the trigram left-boundary NM NN. The second rule is an exception which only
encodes the bigram NM NN. The third rule encodes the trigram PP PP right-boundary.
The last rule is an exception, again, and only encodes NN PP. There is no rule which
covers the trigram consisting of the head and its two immediate neighbors.
Our markovization strategy only transforms rules that occur less than 10 times in
the training data. If one of the auxiliary symbols introduced by the markovization (such
as ?L:NP[NN]NN?NM?) is used less than 20 times (the values of the two thresholds
were optimized on part of the development data) overall, it is replaced by a simpler
symbol ?L:NP[NN]NN? that encodes less context. In this way, we switch from a trigram
model (where the next child depends on the two preceding children) to a bigrammodel
(where it only depends on the preceding child) in order to avoid sparse data problems.
Themethod is similar to the markovization strategy of Klein andManning (2003) except
that they markovize all rules. We simulated their strategy by raising the rule frequency
threshold to a larger value, but obtained worse results. We also tried an alternative
markovization strategy that generates all children left to right (the auxiliary symbols
now lack the direction flag, and the rules cover all possible trigrams), but again obtained
worse results. A disadvantage of our markovization method are spurious ambiguities.
They arise because some of the rules which are not markovized are also covered by
markovization rules.
3.7 Dealing with Unknown Words and Unseen POS Tags
BitPar includes a sophisticated POS guesser that uses several strategies to deal with
unknown words and unseen POS tags of known words. Unknown words are divided
into eight classes11 based on regular expressions that are manually defined. These
classes distinguish between lower-case words, capitalized words, all upper-case words,
hyphenated words, numbers, and so forth. For each word class, BitPar builds a suffix
tree (Weischedel et al 1993; Schmid 1995; Brants 2000) from the suffixes of all words in
the lexicon up to a length of 7. At each node of the suffix tree, it sums up the conditional
POS probabilities (given the word) over all known words with that suffix. By summing
POS probabilities rather than frequencies, all words have the same weight, which is
appropriate here because we need to model the POS probabilities of infrequent words.
BitPar computes POS probability estimates for each node using the sum of probabilities
as a pseudo-frequency for each tag. The estimates are recursively smoothed with the
Witten-Bell method using the smoothed POS probabilities of the parent node as a
backoff probability distribution.12 The suffix trees are pruned by recursively removing
11 We also experimented with more complex classifications, but they failed to improve the results.
12 The number of ?observed? POS tags, which is needed by Witten-Bell smoothing, is defined as the
number of POS tags with a pseudo-frequency larger than 0.5.
69
Computational Linguistics Volume 39, Number 1
leaf nodes whose pseudo-frequency is below 5 or whose weighted information gain13
is below a threshold of 1.
Whenever an unknown word is encountered during parsing, BitPar determines the
word class and obtains the tag probability distribution from the corresponding suffix
tree. BitPar assumes that function words are completely covered by the lexicon and
never guesses function word POS classes for unknown words.
BitPar uses information from the unknown word POS guesser and (if available)
information from an external lexicon (generated by a computational morphology, for
instance, as we will discuss in Section 5.1) in order to predict unobserved POS tags
for known words. First the external lexicon and the lexicon extracted from the training
corpus are merged. Then smoothed probabilities are estimated using Witten-Bell
smoothing with a backoff distribution. The backoff distribution is the average of:
(1) the probability distribution returned by the unknown word POS guesser
if at least one possible POS tag of the word according to the lexicon is an
open-class POS tag,
(2) the average POS probability distribution of all words with exactly the same
set of possible POS tags as the given word14 if at least one of the possible
tags is unseen, and
(3) the prior POS probability distribution if no other word in the lexicon has
the same set of possible POS tags and at least one of the word?s possible
POS tags is unseen.
4. Evaluation of the Generative Parser
As we present each knowledge source, we would like to evaluate it against manually
annotated Treebanks. Our first evaluation shows that our generative parser introduced
in the previous section is comparable with the Berkeley generative parser. Before we
present this comparison in Section 4.1 we discuss evaluating parse accuracy.
In our evaluations, we use the Tiger Treebank (Brants et al 2002) and a small
Europarl Treebank (Pado? and Lapata 2009). We take the first 40,474 sentences of the
Tiger Treebank as training data (Tiger train), the next 5,000 sentences as development
data (Tiger dev), and the last 5,000 sentences as test data (Tiger test). The Europarl
data consists of 662 sentences15 and are either completely used as test data and not di-
vided up or we carried out seven-fold cross-validation experiments with our reranking
models.
All parsers are evaluated on projectivized parse trees. This means that we apply
step 1 of the grammar extraction process described in Section 3.1 to the test parses
and use the result as the gold standard (except for the Pado? set, which is already
projectivized). The test sentences are parsed and the resulting parse trees are converted
13 The weighted information gain is the difference between the entropy of the parent node and the entropy
of the current node, multiplied by the total frequency of the current node and divided by the number of
?observed? POS tags of the current node.
14 A similar pooling of lexicon entries was previously used in the POS tagger of Cutting et al (1992).
15 We use only the sentences in this set which had a single sentence as a translation, so that they could
be used in bilingual reranking, which will be discussed later.
70
Fraser et al Knowledge Sources for Parsing German
to the same format as the gold standard trees by undoing Steps 2, 3, and 4 of Section 3.1.
This conversion involves four steps:
1. Demarkovization removes all the auxiliary nodes introduced by
markovization and raises their children to the next non-auxiliary node.
2. The added unary-branching nodes are eliminated.
3. The original grammatical function labels NK inside of NPs and PPs,
and CJ inside of coordinated phrases, are restored.
4. All feature annotations are deleted.
We use PARSEVAL scores (Black et al 1991) and the standard evaluation tool evalb16
to compare the converted parse trees with the gold standard parse trees using labeled
F-score. We report accuracies for all test sentences and not just sentences of length up to
40. We do not evaluate parsers with gold standard POS tags, but instead automatically
infer them. These considerations make our evaluation setting as close to the real-world
setting as possible.
We report results for evaluations with and without grammatical functions. We
report PARSEVAL scores with grammatical functions inside parentheses after the
results using only basic constituent categories. We believe that grammatical functions
are an important part of the syntactic analysis for any downstream applications in less-
configurational languages such as German because crucial distinctions (e.g., the distinc-
tion between subject and object) are not feasible without them. We should mention that
our results are not directly comparable to previously published results on the Tiger2
corpus (Ku?bler 2008; Versley and Rehbein 2009; Seeker et al 2010b), because each of
the previous studies used different portions of the corpus and there are differences in
the evaluation metric as well. The transformed corpus (in our train, development, and
test split format) and the evaluation scripts we used are available,17 which we hope will
enable direct comparison with our results.
4.1 Comparison of BitPar and Berkeley
The best constituent parser participating in the Parsing German Shared Task (Ku?bler
2008) was the Berkeley parser (Petrov and Klein 2008) and to the best of our knowledge
it has achieved the best published accuracy for German constituency parsing so far.
The Berkeley parser takes an automated approach, in which each constituent symbol is
split into subsymbols applying an expectation-maximization method. We compare our
manually enriched grammar to this automatic approach.
We trained the Berkeley parser on Tiger train using the basic constituent categories
concatenated to the grammatical function labels as starting symbols. We found that it
achieved the best PARSEVAL scores on Tiger dev after the fourth iteration. This model
was used for parsing Tiger dev, Tiger test, and the Europarl corpus.
BitPar achieved 82.51 (72.46), 76.67 (65.61), and 77.13 (66.06), and the Berkeley
parser achieved 82.76 (73.20), 76.37 (65.66), and 75.51 (63.3) on the three corpora,
respectively. In general, these results indicate that these two parsers are competitive.
On the other hand, the fact that the results of the Berkeley parser are much worse than
16 http://nlp.cs.nyu.edu/evalb/, 2008 version.
17 See http://www.ims.uni-stuttgart.de/tcl/RESOURCES/CL.html.
71
Computational Linguistics Volume 39, Number 1
BitPar on the out-of-domain Europarl corpus indicates that it overfits to the domain
of the training corpus (Tiger2). Following a reviewer suggestion, we looked at the
sentences containing many words not occurring in the training data, and observed that
our lexical resource is strongly helpful for these sentences. Another disadvantage of
the automatic approach of the Berkeley parser is that the resulting subsymbols are not
easily interpretable, which can hinder defining features for parse reranking using them.
Based on these considerations, we decided to use BitPar in our reranking experiments.
The combination of the two radically different approaches (linguistically motivated
grammar extensions and automatic symbol splitting) is a rather promising area of
research for improving parsing accuracy, which we plan to address in future work.
5. Impact of Our Lexical Resource
5.1 Integration of SMOR with BitPar
There are a large number of inflectedword forms formanyGerman lemmas. This causes
sparse data problems if some forms are not observed in the training data. BitPar applies
the heuristics described in Section 3.7 to obtain POS probabilities for unseen words.
Although these heuristics seem to work quite well, we expect better results if the parser
has access to information from a morphological analyzer.
We use the German finite-state morphology SMOR (Schmid, Fitschen, and Heid
2004) to provide sets of possible POS tags for all words. SMOR covers inflection, deriva-
tion, and compounding and achieves good coverage in combination with the stem
lexicon IMSLex (Lezius, Dipper, and Fitschen 2000). SMOR is integrated into the parser
in the following way. We create a combined word list from the training and testing
data18 and analyze it with SMOR. The SMOR analyses are then mapped to the POS
tag set used by the parser, and supplied to BitPar as an external lexicon (see Section 3.7).
Consider the example word erlischt [goes out], which did not appear in the train-
ing corpus. SMOR produces the analysis erlo?schen.V.3.Sg.Pres.Ind, which is mapped
to VVFIN-HD and added to the lexicon. Using this entry, BitPar correctly parsed
the sentence Die Anzeige erlischt [The display goes out]. Without using SMOR, the
parser analysed erlischt as a past participle because scht is a frequent past participle
ending.
5.2 Effect on In-Domain and Out-of-Domain Parsing
In order to measure the effect of the integration of a German morphology on parsing
accuracy (see Section 5.1), we tested the BitPar parser on the Tiger data and on Europarl
data. The results are summarized in Table 1. They show that the morphology helps on
out-of-domain data (Europarl), but not so much on in-domain data (Tiger). The POS
tagging accuracy, however, also increases on Tiger data by 0.13%. When grammatical
functions are included in the evaluation, the performance improvement more than
doubles on Europarl data. As a result, we decided to use the finite-state morphology
in the rest of the experiments we conducted.
Table 1 also shows that the Tiger test data is harder to parse than the dev data. We
examined the two subcorpora and found that the test data contains longer sentences
18 Because we are only using the words here, and not their POS labels, this approach is methodologically
sound and could be applied to any unparsed data in the same way.
72
Fraser et al Knowledge Sources for Parsing German
Table 1
Effect of using finite-state morphology on parsing accuracy. The values in parentheses are
labeled F-scores from the evaluation with grammatical functions.
morphology Tiger dev Tiger test Europarl
without 82.51 (72.46) 76.67 (65.61) 76.81 (65.31)
with 82.42 (72.36) 76.84 (65.91) 77.13 (66.06)
difference ?0.09 (?0.10) +0.17 (+0.30) +0.32 (+0.75)
(18.4 vs. 15.3 words on average) and that the ratio of unknown words is higher (10.0%
vs. 7.6%).
6. Parse Reranking
The most successful supervised phrase-structure parsers are feature-rich discriminative
parsers that heavily depend on an underlying PCFG grammar (Charniak and Johnson
2005; Huang 2008). These approaches consist of two stages. At the first stage they apply
a PCFG grammar to extract possible parses. The full set of possible parses cannot be
iterated through in practice, and is usually pruned as a consequence. The n-best list
parsers keep just the 50?100 best parses according to the PCFG. Other methods remove
nodes and edges from the packed parse forest whose posterior probability is under a
pre-defined threshold (Charniak and Johnson 2005).
The task of the second stage is to select the best parse from the set of possible
parses (i.e., rerank this set). These methods use a large feature set (usually a few
million features) (Collins 2000; Charniak and Johnson 2005). The n-best list approaches
can straightforwardly use local and non-local features as well because they decide at
the sentence-level (Charniak and Johnson 2005). Involving non-local features is more
complicated in the forest-based approaches. The conditional random field methods
usually use only local features (Yusuke and Jun?ichi 2002; Finkel, Kleeman, and
Manning 2008). Huang (2008) introduced a beam-search and average perceptron-based
procedure incorporating non-local features in a forest-based approach. His empirical
results show only a minor improvement from incorporating non-local features,
however.
In this study, we experiment with n-best list reranking using a maximum entropy
machine learning model for (re)ranking along with local and non-local features. Our
reranking framework follows Charniak and Johnson (2005). At the first-stage of parsing,
we extract the 100 best parses for a sentence according to BitPar?s probability model.
At parsing time, a weight vector w is given for the feature vectors (which numerically
represent one possible parse) and we select the parse with the highest inner product
of these two vectors. The goal of training is to adjust w. In the maximum entropy
framework, this is achieved by solving the optimization problem of maximizing the
posterior probability of the oracle parse?the parse with the highest F-score.19 Our
method aims to select the oracle, as the gold standard parse is often not present in
the 100-best parses.20 Our preliminary experiments showed that parse candidates close
19 Ties are broken using the PCFG probabilities of the parses.
20 The oracle F-score (i.e., the upper limit of 100-best reranking on the Tiger development corpus) is 90.17.
73
Computational Linguistics Volume 39, Number 1
to the oracle confuse training. Hence during training, we removed all parses whose
F-score is closer than 1.0 to the score of the oracle.21
As we discussed in Section 1, the parsing output of morphologically rich languages
is useful only when it is additionally annotated with grammatical functions. The oracle
parses often change if the grammatical function labels are also taken into consideration
at the PARSEVAL score calculation. Hence slightly different objective functions are used
in the two cases. We will report results achieved by reranking models where the oracle
selection for training agrees with the evaluation metric utilized?that is, we trained
different models (which differ in the oracle selection) for the basic constituent label
evaluation and for the evaluation on grammatical functions.
During training we followed an eight-fold cross validation technique for candidate
extraction (Collins 2000). Here, one-eighth of the training corpus was parsed with a
PCFG extracted from seven-eighths of the data set. This provides realistic training
examples for the reranker as these parses were not seen during grammar extraction. We
used the ranking MaxEnt implementation of MALLET (McCallum 2002) with default
parameters.
7. Monolingual Reranking
7.1 Subcategorization-Based Monolingual Reranking Features
We introduce here several novel subcategorization-based features for monolingual
reranking. For this, we first describe our algorithm for extracting subcategorization
(subcat) information. We use our enriched version of the Tiger2 training set. In order
to extract verbal subcat frames we find all nodes labeled with the category S (clause)
or VP-MO (modifying VP) and extract their arguments. Arguments22 are nodes of the
categories shown in Table 2. The arguments of nouns are obtained by looking for NN
nodes which are either dominated by an NP or a PP, and which take a following node
of category PP, VP-OC, or S-OC as argument.
The feature functions we present are mostly lexicalized. This means we need access
to the head words of the arguments. The argument heads are extracted as follows: As
NP headwe take the last nodewhose function label is either HD,NK, or PH. If this node
is of category NP or PN, we recursively select the head of that constituent. Similarly, the
head of an AP is the last node with functional label HD. If it is an AP, the head is
searched inside of it. In the case of PPs, we extract two heads, namely, the preposition
(or postposition) as well as the nominal head of the PP, which is found using similar
rules as for NPs. We also extract the case of the nominal head.
The extraction of verbal heads is somewhat more complicated. In order to obtain
the correct verbal head of a clause irrespective of the verb position (verb-first, verb-
second, verb-final), we extract all verbs that are dominated by the clause and a possibly
empty sequence of VP-OC or VP-PD (statal passive) nodes and an optional VZ-HD
node. Then we take the first non-finite verb, or alternatively the first finite verb if all
verbs were finite. In order to avoid sparse data problems caused by the many different
inflections of German verbs, we lemmatize the verbs.
21 In Fraser, Wang, and Schu?tze (2009) we used Minimum Error Rate Training. Once we made this change
to maximum entropy the results on small feature sets became similar (details omitted).
22 An exception to this is that if a PP argument dominates a node of category PROAV-PH, it is considered
a PROAV-PH argument. An example is the sentence Er [he] wartet [waits] (PP-OP (PROAV-PH darauf
[for this]), (S-RE dass [that] sie [she] kommt [comes])).
74
Fraser et al Knowledge Sources for Parsing German
Table 2
Arguments used in extracted subcategorization frames.
NP-SB, PN-SB, CNP-SB, S-SB, VP-SB subjects
NP-OA, PN-OA, CNP-OA direct objects
NP-DA, PN-DA, CNP-DA indirect objects
PRF-OA reflexive direct objects
PRF-DA reflexive indirect objects
NP-PD, CNP-PD predicative NPs
ADJD-PD, AP-PD, CAP-PD predicative adjectives
S-OC, CS-OC argument clauses
PP-OP, CPP-OP PP arguments
VP-OC/zu infinitival complement clauses
PROAV-OP pronominal adverbs serving as PP proxies such as
daraus [out of this]
NP-EP expletive subjects
VP-RE, NP-RE VP/NP appearing in expletive constructions
In the case of coordinated phrases, we take the head of the first conjunct. Arguments
are sorted to put them in a well-defined order. An example is that given the correct
parse of the sentence Statt [instead of] Details [details] zu [to] nennen [name], hat [has]
er [he] unverdrossen [assiduously] die [the] ?Erfolgsformel? [formula of success] wiederholt
[repeated], meaning ?instead of naming the details, he assiduously repeated the formula
of success,? we extract the two subcat frames:
VP-MO OBJ:Details VZ-HD:zu:nennen
S-TOP VP-MO SUBJ:er OBJ:Erfolgsformel VVPP-HD:wiederholt
We can now describe our features. The features focus on subcat frames taken from
S nodes (VP-MO is treated as S), and on attachment of prepositions and conjunctions to
nouns. We define conditional probability and mutual information (MI) features.
The two conditional probability features are ProbPrepAttach and ProbAdverb-
Attach, which calculate the probability for each preposition or adverb to be attached
to its governor, given the label of the governor. We estimate this from the training
data as follows, for the example of the PP feature. In the feature scoring, we give
each preposition attachment a score which is the negative log10 of the probabil-
ity p(lex prep|label governor) = f (lex prep, label governor)/f (label governor) (with a
cutoff of 5).
For all of our other monolingual features, we use (negative) pointwise mutual
information: ?log10(p(a, b)/p(a)p(b)) (here we use cutoffs of 5 and ?5).
MI NounP and MI NounConj give an assessment of a preposition or a conjunction
being attached to a noun (given the lexicalized preposition and the lexicalized noun).
For the MI VSubcat feature, we use as a the frame (without lexicalization), and as
b the head verb. p(a) is estimated as the relative frequency of this frame over all frames
extracted from Tiger2 train. MI VSimpleSubcat is a simpler version of MI VSubcat.
PP is excluded from frames because PP is often an adjunct rather than an argument.
For the MI VArg feature, we use as a the argument function and the head word
of the argument (e.g., OBJ:Buch, which is ?book? used as an object). As b we again
use the head verb. The estimate of p(a) is frequency(OBJ:Buch)/(total number of
extracted frames).23 In addition, this feature is refined into individual features for
23 We make the assumption that every frame has an object, but that this object can be NULL.
75
Computational Linguistics Volume 39, Number 1
different kinds of arguments: MI VSubj, MI VObj, MI VIobj, MI VPP, MI VPRF,
MI VS OC, MI VVP, and MI VerbPROAV. As an example, the MI of ?lesen, OBJ:Buch?
(reading, object:Book) would be used for the MI VArg features and for the MI VObj
feature. For functions such as MI VPP which are headed by both a function word (here,
a preposition) and a content word, only the function word is used (and no case).
The last MI feature is MI VParticle. Some German verbs contain a separable parti-
cle, which can also be analyzed as an adverb but will then have a different meaning. For
the sentence ?Und [and] Frau [Mrs.] Ku?nast [(proper name)] bringt [brings] das [that] auch
[also] nicht [not] ru?ber [across],? if ?ru?ber? is analyzed as an adverb, the verb means to
carry/take/bring over [to another physical location], but if it is viewed as a particle, the
sentence means Frau Ku?nast is not able to explain this. The feature MI VParticle helps
with this kind of disambiguation.
7.2 The Versley and Rehbein Feature Set
We also carried out experiments with the feature set of Versley and Rehbein (2009),
which is specially designed for German. It consists of features constructed from the
lexicalized parse tree along with features based on external statistical information.
The features here are local in the sense that their values can be computed at the
constituent in question, its daughters, and its spanning words. All features except
the external statistical information are binary and indicate that a lexicalized pattern is
present in the parse. They were originally designed for forest-based reranking (Versley
and Rehbein 2009). Following Charniak and Johnson (2005) we sum up these local
feature values in the parse tree. Thus our versions count the number of times that a
particular pattern occurs in the entire parse tree.
The patterns used can be further subcategorized into three groups. The wordform-
based patterns are token?POS (e.g., one pattern is ?lesen-VVINF?) and the word class
of the token in question (word class comes from an automatic clustering of words based
on contextual features). The constituent-based patterns are the size of the constituent,
the constituent label, and the right-hand side of the derivational rule applied at the node
in question. The last and biggest group of the pattern features is formed by the bilexical
dependencies. They are based on the head word of the constituent node in question
and its daughters. Versley and Rehbein (2009) have also introduced features that exploit
statistical information gathered from an external data set and aim to resolve PP attach-
ment ambiguity. Mutual information values were gathered on the association between
nouns and immediately following prepositions, as well as between prepositions and
closely following verbs on the DE-WaC corpus (Baroni and Kilgarriff 2006). These
feature values were then used at NP?PP and VP?PP daughter attachments.
A total of 2.7 million features fired in the Tiger train. We ignored features firing
in less than five sentences for computational efficiency, resulting in 117,000 extremely
sparse features.
7.3 Monolingual Reranking Experiments
We rerank 100-best lists from BitPar (Schmid 2004), which uses the grammar extraction
procedure and lexical resources introduced in Section 3. In each of the experiments we
extracted the grammar from the Tiger train and used it to obtain the 100-best parses for
the sentences of the evaluation corpus.
We trained reranking models on the Tiger train as described in Section 6 using our
subcategorization-based features, the Versley09 feature set, and the union of these two
76
Fraser et al Knowledge Sources for Parsing German
Table 3
The PARSEVAL score of monolingual features to rerank the parses of Europarl (seven-way
cross-validation on 662 sentences) and Tiger2 (development and test sets).
Tiger dev Tiger test Europarl CROSS Europarl IN
Baseline 82.42 (72.36) 76.84 (65.91) 77.13 (66.06)
subcat 83.19 (73.63) 77.65 (67.21) 77.23 (66.13) 77.73 (66.95)
Versley09 83.56 (73.89) 78.57 (68.42) 77.82 (66.87) 77.62 (66.05)
subcat+Versley09 84.19 (74.96) 78.86 (69.04) 77.76 (66.84) 77.93 (66.75)
sets. We evaluated the models on Tiger dev, Tiger test, and Europarl. As the domains
of Tiger and Europarl are quite different, besides this cross-domain parser evaluation
(CROSS) we carried out an in-domain (IN) evaluation as well. In the latter we followed
the seven-fold cross-validation approach, that is, the reranking models were trained on
six-sevenths of Europarl. The results are presented in Table 3.
The results presented in Table 3 show that the reranking models achieve an im-
provement over the baseline parser using both our and the Versley09 feature sets. The
Versley09 feature set achieved better results than our monolingual features when a
training dataset with sufficient size is given (Tiger). On the other hand using our 16
rich features (compared with 117,000 sparse features) is more suitable for the settings
where only a limited amount of training instances are available (the training sets consist
of 567 sentences of Europarl in seven-fold cross-validation). The rerankingmodels using
the union of the feature sets obtain close to the sum of the improvements of the two in-
dividual feature sets. The subcategorization features model rich non-local information,
and the fine-grained features capture local distinctions well and the features based on
the Web corpus access additional knowledge.
We performed an experiment adding one feature at a time, and found that the
most effective features were ProbAdverbAttach, MI VPP, MI VPRF, MI VSubj, and
MI VArg. After this the variation caused by numeric instability was too high to see a
consistent incremental gain from the rest of the features. We conclude that these features
can be robustly estimated and have more discriminative power than the others, but we
emphasize that we used all features in our experiments.
Figure 5 shows a parse tree produced by the BitPar parser in which the noun phrase
diese Finanzierung is incorrectly classified as an accusative object. The monolingual
subcategorization features MI VSubcat, MI VSimpleSubcat, and MI VArg enable the
reranker to correctly analyze the noun phrase as a subject and to move it from the VP
level to the S level.
.
S-TOP
PWAV-MO
Woher
where-from
VMFIN-HD
soll
should
VP-OC
NP-OA
PDAT-HD
diese
this
NN-HD
Finanzierung
financing
VVINF-HD
kommen
come
Figure 5
Erroneous parse produced by BitPar that is corrected by monolingual features.
77
Computational Linguistics Volume 39, Number 1
8. Bilingual Reranking
We now present our bilingual reranking framework. This follows our previous work
(Fraser, Wang, and Schu?tze 2009), which defined feature functions for reranking
English parses, but now we will use these same feature functions (and three additional
feature functions introduced to capture phenomena higher in the syntactic tree) to
rerank German parses. The intuition for using this type of bitext projection feature is
that ambiguous structures in one language often correspond to unambiguous structures
in another. Our feature functions are functions on the hypothesized English parse e,
the German parse g, and the word alignment a, and they assign a score (varying
between 0 and infinity) that measures syntactic divergence. The alignment of a sentence
pair is a function that, for each English word, returns a set of German words with
which the English word is aligned. Feature function values are calculated either by
taking the negative log of a probability, or by using a heuristic function which scales
similarly.24
The bilingual feature functions we define are functions that measure differ-
ent types of syntactic divergence between an English parse and a German parse.
Charniak and Johnson (2005) defined the state of the art in discriminative n-best
constituency parsing of English syntax (without the use of self-training). The n-best
output of their generative parser is reranked discriminatively by a reranker. We call
this CJRERANK. We will use an array of feature functions measuring the syntactic
divergence of candidate German parses with the projection of the English parse
obtained from CJRERANK.
In our experiments we use the English text of the parallel Treebank extracted from
the Europarl corpus and annotated by Pado? and Lapata (2009). There are 662 German
sentences that are aligned to single English sentences; this is the set that we use. Due to
the limited number of trees, we perform cross-validation to measure performance.
The basic idea behind our feature functions is that any constituent in a sentence
should play approximately the same syntactic role and have a similar span as the corre-
sponding constituent in a translation. If there is an obvious disagreement, it is probably
caused by wrong attachment or other syntactic mistakes in parsing. Sometimes in
translation the syntactic role of a given semantic constituent changes; we assume that
our model penalizes all hypothesized parses equally in this case.
To determine which features to describe here we conducted a greedy feature addi-
tion experiment (adding one feature at a time), on top of our best monolingual system
(combining both subcat and Versley09 feature sets). All bilingual experiments use all of
the features (not just the features we describe here). Definitions are available.25
BitParLogProb (the only monolingual feature used in the bilingual-only experi-
ment) is the negative log probability assigned by BitPar to the German parse.
8.1 Count Feature Functions
Count feature functions count projection constraint violations.
Feature CrdBin counts binary events involving the heads of coordinated phrases. If
we have a coordination where the English CC is aligned only with a German KON, and
24 A probability of 1 is a feature value of 0, whereas a low probability is a feature value which is
 0.
25 See http://www.ims.uni-stuttgart.de/tcl/RESOURCES/CL.html.
78
Fraser et al Knowledge Sources for Parsing German
Table 4
Other projection features selected; see the previously mentioned Web page25 for precise
definitions.
POSParentPrjWordPerG2E Computes the span difference between all the parent constituents
of POS tags in a German parse and their respective coverage
in the corresponding English parse, measured using percentage
coverage of the sentence in words. The feature value is the sum
of all the differences. The projection direction is from German to
English.
AbovePOSPrjPer Projection direction is from English to German, and measured in
percentage sentence coverage using characters, not words. The
feature value is calculated over all constituents above the POS
level in the English tree.
AbovePOSPrjWord Calculates a length-based difference using words.
POSPar2Prj Only applies when the POS tag?s parent has two children (the
POS tag has only one sibling). Projects from English to German
and calculates a length-based difference in characters.
POSPar2PrjPer Calculates a percentage-based difference based on characters.
POSPar2PrjG2E Like POSPar2Prj except projects from German to English.
POSPar2PrjWordG2E Like POSPar2PrjG2E except uses word-based differences.
both have two siblings, then the value contributed toCrdBin is 1 (indicating a constraint
violation) unless the head of the English left conjunct is aligned with the head of the
German left conjunct and likewise the right conjuncts are aligned.
Feature Q simply captures a mismatch between questions and statements. If a
German sentence is parsed as a question but the parallel English sentence is not, or
vice versa, the feature value is 1; otherwise the value is 0.
Feature S-OC considers that a clausal object (OC) in a German parse should be
projected to a simple declarative clause in English. This feature counts violations.
EngPPinSVP checks whether a PP inside of a S or VP in English attaches to the
same (projected) constituent in German. If an English PP follows immediately a VP or
a single verb, and the whole constituent is labeled ?S? or ?VP,? then the PP should be
identified as governed by the VP. In this case the corresponding German PP should
attach as well to the German VP to which the English VP is projected (attachment in
German can be to the left or to the right). If the governor in German does not turn out to
be a VP or have a tag starting with ?V,? a value of 1 will be added to the feature for this
German parse.
EngLeftSVP checks whether the left sibling of S or VP in English attaches to the
same (projected) constituent in German (where attachment can be left or right). This
feature counts violations.
Span Projection Feature Functions. Span projection features calculate an absolute or
percentage difference between a constituent?s span and the span of its projection. Span
size is measured in characters or words. To project a constituent in a parse, we use the
word alignment to project all word positions covered by the constituent and then look
for the smallest covering constituent in the parse of the parallel sentence.
PPParentPrjWord checks the correctness of PP attachment. It projects all the parents
of PP constituents in an English parse to German, and sums all the span differences. It is
measured in words. In addition to PPParentPrjWord we implement two bonus features,
NonPPWord and NonPPPer. The former simply calculates the number of words that
79
Computational Linguistics Volume 39, Number 1
do not belong to PP phrases in the sentence, and the latter computes the non-PP
proportion in a character-based fashion. These can be thought of as tunable parameters
which adjust PPParentPrjWord to not disfavor large PPs. The other selected projection
features are described in Table 4.
Probabilistic Feature Functions. We use Europarl (Koehn 2005), from which we
extract a parallel corpus of approximately 1.22 million sentence pairs, to estimate
the probabilistic feature functions described in this section.
We describe the feature PTag, despite the fact that it was not selected by the feature
analysis, because several variations (described next) were selected. PTag measures
tagging inconsistency based on estimating the probability for each English word that
it has a particular POS tag, given the aligned German word?s POS tag. To avoid noisy
feature values due to outliers and parse errors, we bound the value of PTag at 5.26 We
use relative frequency to estimate this feature. When an English word is aligned with
two words, estimation is more complex. We heuristically give each English and German
pair one count. The value calculated by the feature function is the geometric mean27 of
the pairwise probabilities.
The feature PTagEParent measures tagging inconsistency based on estimating the
probability that the parent of the English word at position i has a particular tag, given
the aligned German word?s POS label. PTagBiGLeft measures tagging inconsistency
based on estimating the probability for each English word that it has a particular POS
tag, given the aligned German word?s label and the word to the left of the aligned
German word?s label. PTagBiGParent measures tagging inconsistency based on esti-
mating the probability for each English word that it has a particular POS tag, given the
aligned German word?s label and the German word?s parent?s label.
8.2 Bilingual Reranking Experiments
We performed experiments looking at bilingual reranking performance. To train the
parameters of the probabilistic feature functions, we use 1-best parses of the large
Europarl parallel corpus (from CJRERANK and BitPar). We work on the same 100-best
list (of the German sentences in the small Pado? set) as was used in the previous section.
We parse the English sentences of the small Europarl set with CJRERANK; this parse is
used as our bilingual knowledge source. Finally we rerank using the bilingual features
(results in the first row of Table 5).
We then combine the monolingual features with the bilingual features. We rerank
using both the monolingual and the bilingual features together, and the results are
presented in Table 5. The bilingual feature-based reranker achieved 1 percentage point
improvement over the baseline. This advantage was just slightly decreasedwhenmono-
lingual features are also present. This indicates again that themonolingual and bilingual
features can capture different linguistic phenomena and their information content is
rather different. As in the Europarl IN setting, using the large sparse Versley09 feature
set the reranker could not learn a meaningful model from a moderate-sized training
data set.
26 Throughout this paper, assume log(0) = ??.
27 Each English word has the same weight regardless of whether it was aligned with one or with more
German words.
80
Fraser et al Knowledge Sources for Parsing German
Table 5
PARSEVAL scores of bi+monolingual features to rerank the parses of Europarl (seven-way
cross-validation) and the added value of bilingual features over the results achieved by the
corresponding monolingual feature set.
Mono features without bilingual with bilingual added value
NONE 77.13 (66.06) 78.10 (67.12) +0.97 (+1.06)
subcat 77.73 (66.95) 78.54 (67.95) +0.78 (+1.00)
Versley09 77.62 (66.05) 77.71 (66.06) +0.09 (+0.01)
subcat+Versley09 77.93 (66.75) 78.70 (67.45) +0.78 (+0.70)
The parse tree in Figure 6 demonstrates the value of bilingual features. It was
produced by the monolingual reranker and it incorrectly combines the two adverbs aber
and ebenso into an adverbial phrase and places this under the VP. The bilingual reranker
instead attaches the two adverbs separately at the S level. The attachment to the S node
indicates that the two adverbs modify the modal verb kann and not the full verb sagen.
This is triggered by the feature POSPar2Prj.
8.3 Previous Work on Bitext Parsing
Bitext parsing was also addressed by Burkett and Klein (2008). In that work, they use
feature functions defined on triples of (English parse tree, Chinese parse tree, alignment)
which are combined in a log-linear model, much as we do. In later work (Burkett,
Blitzer, and Klein 2010), they developed a unified joint model for solving the same
problem using a weakly synchronized grammar. To train these models they use a small
parallel Treebank that contains gold standard trees for parallel sentences in Chinese
and English, whereas we only require gold standard trees for the language we are
reranking. Another important difference is that Burkett and Klein (2008) use a large
number of automatically generated features (defined in terms of feature generation
templates) whereas we use a small number of carefully designed features that we found
by linguistic analysis of parallel corpora. Burkett, Blitzer, and Klein (2010) use a subset
of the features of Burkett and Klein (2008) for synchronization, along with monolin-
gual parsing and alignment based features. Finally, self-training (McClosky, Charniak,
and Johnson 2006b) is another differentiator of our work. We use probabilities esti-
mated from aligned English CJRERANK parses and German BitPar parses of the large
Europarl corpus in our bilingual feature functions. These feature functions are used to
.
S-TOP
PIS-SB
Man
one
VMFIN-HD
kann
can
VP-OC
AVP-MO
ADV-MO
aber
but
ADV-HD
ebenso
just-as-well
VVINF-HD
sagen
say
,
,
S-OC
KOUS-CP
dass
that
PPER-SB
sie
they
ADJD-PD
anspruchsvoll
demanding
VAFIN-HD
sind
are
Figure 6
Erroneous parse produced by the reranker using only monolingual features, which is corrected
by bilingual features. The sentence means One can, however, just as well say that they are demanding.
81
Computational Linguistics Volume 39, Number 1
improve ranking of German BitPar parses in the held-out test sets, which is a form of
self-training.
Two other interesting studies in this area are those of Fossum and Knight (2008)
and of Huang, Jiang, and Liu (2009). They improve English prepositional phrase at-
tachment using features from a Chinese sentence. Unlike our approach, however, they
do not require a Chinese syntactic parse as the word order in Chinese is sufficient to
unambiguously determine the correct attachment point of the prepositional phrase in
the English sentence without using a Chinese syntactic parse.
We know of no other work that has investigated to what extent monolingual and
bilingual features in parse reranking are complementary. In particular, the work on bi-
text parsing by Burkett and Klein (2008) does not address the question as to whether the
effect of monolingual and bilingual features in parse reranking is (partially) additive.
We demonstrate bilingual improvement for a strong parser of German. Previously,
we showed bilingual improvement for parsing English with an unlexicalized parser
(Fraser, Wang, and Schu?tze 2009), using 34 of the 37 bilingual feature functions we use
in this work.
9. Conclusion
In this paper, we have focused on MR&LC languages like German?languages that
are morphologically rich, but also have a strong configurational component. We have
argued that constituency parsing is, perhaps contrary to conventional wisdom, an ap-
propriate parsing formalism for MR&LC because constituents capture configurational
constraints in a transparent way and because for many applications constituency pars-
ing is preferable to dependency parsing. Our detailed description of a constituency
parsing system for a morphologically rich language, a system that addresses the major
problems that arise in constituency parsing for MR&LC, is one main contribution of this
paper. Two of these problems are rule proliferation and syncretism. We have addressed
rule proliferation bymarkovization and syncretism by (i) deploying a high performance
finite-state-based morphological analyzer that is based on rich lexical knowledge and
(ii) encoding grammatical functions directly as part of the phrase labels. This direct
encoding allows us to directly combine morphological and configurational informa-
tion in parsing and arrive at a maximally disambiguated parse. We argued that this
is the right setup for MR&LC languages because applications must have access to
grammatical functions.
A large part of this paper was concerned with making available and evaluating
additional knowledge sources for improved parsing of the MR&LC language German.
Our motivation was that (as we argued) MR&LC languages have in general higher am-
biguity than purely configurational and purely morphological languages, in particular
with respect to grammatical functions. Apart from the lexical knowledge embedded
in the morphological analyzer, we presented work on two other knowledge sources to
address this type of additional ambiguity: monolingual reranking (which looks at global
sentence-wide constraints for disambiguation) and bitext reranking (which exploits
parallel text for disambiguation). We were able to improve the performance of a strong
baseline parser using these three knowledge sources and we showed that they are
largely complementary: Performance improvements were additive when we used them
together. The resulting parser is currently the best constituent parser for German (with
or without bilingual features).
New languages and even new domains can require new treebanks. To create such
a treebank for a MR&LC language, we would first annotate a small number of gold
82
Fraser et al Knowledge Sources for Parsing German
standard trees, using parallel text with English or another language if such text is
available. Next, wewould consider how to quickly differentiate constituents of the same
type using constituent labels plus grammatical functions, as we outlined in Section 3.
Following this, we would use BitPar to build a parser in the same way as we presented
here, and to determine the optimal level of markovization, which we assume would be
very high with a small number of gold standard training trees. Next, as more trees are
annotated in an active learning framework, we would begin to develop morphological
analysis. We would implement the bilingual framework following this (if we have
access to bitext). Then we would implement basic subcategorization extraction and add
monolingual features. Finally, as more gold standard trees are annotated, the reranking
framework should be constantly retrained. In particular, we expect that the effect of the
knowledge sources we have presented will be much stronger when starting with less
training data.
Our work in this paper will be of use to developers of German syntactic parsers
as we have state-of-the-art performance using linguistically motivated features that are
easy to understand. We also hope that our work can serve as a cookbook of ideas to try
for others working on parsers for other morphologically rich languages.
Acknowledgments
We would like to thank Sandra Ku?bler and
Yannick Versley. We gratefully acknowledge
Deutsche Forschungsgemeinschaft (DFG)
for funding this work (grants SCHU 2246/
6-1Morphosyntax for MT and SFB 732
D4Modular lexicalization of PCFGs). This
work was supported in part by the IST
Programme of the European Community,
under the PASCAL2 Network of Excellence,
IST-2007-216886. This publication only
reflects the authors? views.
References
Baroni, Marco and Adam Kilgarriff. 2006.
Large linguistically processed Web
corpora for multiple languages.
In EACL: Posters & Demonstrations,
pages 87?90, Trento.
Black, E., S. Abney, S. Flickenger,
C. Gdaniec, C. Grishman, P. Harrison,
D. Hindle, R. Ingria, F. Jelinek,
J. Klavans, M. Liberman, M. Marcus,
S. Roukos, B. Santorini, and
T. Strzalkowski. 1991. Procedure for
quantitatively comparing the syntactic
coverage of English grammars. In
Proceedings of the Workshop on Speech
and Natural Language, HLT ?91,
pages 306?311, Pacific Grove, CA.
Brants, Sabine, Stefanie Dipper, Silvia
Hansen, Wolfgang Lezius, and George
Smith. 2002. The TIGER Treebank.
In Proceedings of the Workshop on
Treebanks and Linguistic Theories,
pages 24?41, Sozopol.
Brants, Thorsten. 2000. TnT?a statistical
part-of-speech tagger. In ANLP,
pages 224?231, Seattle, WA.
Burkett, David, John Blitzer, and Dan Klein.
2010. Joint parsing and alignment
with weakly synchronized grammars.
In HLT-NAACL, pages 127?135,
Los Angeles, CA.
Burkett, David and Dan Klein. 2008. Two
languages are better than one (for syntactic
parsing). In EMNLP, pages 877?886,
Honolulu, HI.
Cai, Shu, David Chiang, and Yoav Goldberg.
2011. Language-independent parsing with
empty elements. In ACL, pages 212?216,
Portland, OR.
Campbell, Richard. 2004. Using linguistic
principles to recover empty categories.
In ACL, pages 645?652, Barcelona.
Charniak, Eugene and Mark Johnson. 2005.
Coarse-to-fine n-best parsing and MaxEnt
discriminative reranking. In ACL,
pages 173?180, Ann Arbor, MI.
Collins, Michael. 1997. Three generative,
lexicalized models for statistical parsing.
In ACL, pages 16?23, Madrid.
Collins, Michael. 2000. Discriminative
reranking for natural language parsing.
In ICML, pages 25?70, Stanford, CA.
Cutting, Doug, Julian Kupiec, Jan Pedersen,
and Penelope Sibun. 1992. A practical
part-of-speech tagger. In ANLP,
pages 133?140, Trento.
Dreyer, Markus, David A. Smith, and
Noah A. Smith. 2006. Vine parsing and
minimum risk reranking for speed and
precision. In CoNLL, pages 201?205,
New York, NY.
83
Computational Linguistics Volume 39, Number 1
Dubey, Amit. 2004. Statistical Parsing for
German: Modeling Syntactic Properties
and Annotation Differences. Ph.D. thesis,
Saarland University.
Dubey, Amit and Frank Keller. 2003.
Probabilistic parsing for German using
sister-head dependencies. In ACL,
pages 96?103, Sapporo.
Duchier, Denys and Ralph Debusmann.
2001. Topological dependency trees:
a constraint-based account of linear
precedence. In ACL, pages 180?187,
Toulouse.
Finkel, Jenny Rose, Alex Kleeman, and
Christopher D. Manning. 2008. Efficient,
feature-based, conditional random
field parsing. In ACL, pages 959?967,
Columbus, OH.
Forst, Martin. 2007. Filling statistics
with linguistics?property design
for the disambiguation of German
LFG parses. In Proceedings of the ACL
Workshop on Deep Linguistic Processing,
pages 17?24, Prague.
Fossum, Victoria and Kevin Knight. 2008.
Using bilingual Chinese?English word
alignments to resolve PP-attachment
ambiguity in English. In AMTA,
pages 48?53, Honolulu, HI.
Fraser, Alexander, Renjing Wang,
and Hinrich Schu?tze. 2009. Rich
bitext projection features for parse
reranking. In EACL, pages 282?290,
Athens.
Gabbard, Ryan, Mitchell Marcus, and Seth
Kulick. 2006. Fully parsing the Penn
Treebank. In HLT-NAACL, pages 184?191,
Morristown, NJ.
Hall, Johan and Joakim Nivre. 2008.
A dependency-driven parser for
German dependency and constituency
representations. In Proceedings of the
Workshop on Parsing German, pages 47?54,
Columbus, OH.
Henderson, James, Paola Merlo, Gabriele
Musillo, and Ivan Titov. 2008. A latent
variable model of synchronous parsing
for syntactic and semantic dependencies.
In CoNLL, pages 178?182, Manchester.
Hsu, Yu-Yin. 2010. Comparing conversions
of discontinuity in PCFG parsing. In TLT,
pages 103?113, Tartu.
Huang, Liang. 2008. Forest reranking:
Discriminative parsing with non-local
features. In ACL, pages 586?594,
Columbus, OH.
Huang, Liang, Wenbin Jiang, and
Qun Liu. 2009. Bilingually constrained
(monolingual) shift-reduce parsing.
In EMNLP, pages 1,222?1,231,
Singapore.
Johnson, Mark. 1998. PCFG models
of linguistic tree representations.
Computational Linguistics, 24(4):613?632.
Johnson, Mark. 2001. A simple pattern-
matching algorithm for recovering empty
nodes and their antecedents. In ACL,
pages 136?143, Philadelphia, PA.
Klein, Dan and Christopher D. Manning.
2003. Accurate unlexicalized parsing.
In ACL, pages 423?430, Sapporo.
Koehn, Philipp. 2005. Europarl: a parallel
corpus for statistical machine translation.
InMT Summit X, pages 79?86, Phuket.
Ku?bler, Sandra. 2008. The PaGe 2008 shared
task on parsing German. In Proceedings
of the Workshop on Parsing German,
pages 55?63, Columbus, OH.
Ku?bler, Sandra, Erhard W. Hinrichs, and
Wolfgang Maier. 2006. Is it really that
difficult to parse German? In EMNLP,
pages 111?119, Sydney.
Levy, Roger and Christopher D. Manning.
2004. Deep dependencies from context-free
statistical parsers: Correcting the surface
dependency approximation. In ACL,
pages 327?334, Barcelona.
Lezius, Wolfgang, Stefanie Dipper, and Arne
Fitschen. 2000. IMSLex?representing
morphological and syntactical information
in a relational database. In EURALEX,
pages 133?139, Stuttgart.
McCallum, Andrew Kachites. 2002. Mallet:
A machine learning for language toolkit.
http://mallet.cs.umass.edu.
McClosky, David, Eugene Charniak,
and Mark Johnson. 2006a. Effective
self-training for parsing. In HLT-NAACL,
pages 152?159, Morristown, NJ.
McClosky, David, Eugene Charniak,
and Mark Johnson. 2006b. Reranking
and self-training for parser adaptation.
In COLING-ACL, pages 337?344,
Sydney.
McDonald, Ryan and Fernando Pereira.
2006. Online learning of approximate
dependency parsing algorithms.
In EACL, pages 81?88, Trento.
Menzel, Wolfgang and Ingo Schro?der.
1998. Decision procedures for dependency
parsing using graded constraints.
In COLING-ACL Workshop on Processing
of Dependency-Based Grammars,
pages 78?87, Montreal.
Pado?, Sebastian and Mirella Lapata. 2009.
Cross-lingual annotation projection of
semantic roles. Journal of Artificial
Intelligence Research, 36:307?340.
84
Fraser et al Knowledge Sources for Parsing German
Petrov, Slav and Dan Klein. 2007. Improved
inference for unlexicalized parsing.
In HLT-NAACL, pages 404?411,
Rochester, NY.
Petrov, Slav and Dan Klein. 2008. Parsing
German with latent variable grammars.
In Proceedings of the Workshop on Parsing
German, pages 33?39, Columbus, OH.
Quirk, Chris and Simon Corston-Oliver.
2006. The impact of parse quality on
syntactically-informed statistical
machine translation. In EMNLP,
pages 62?69, Sydney.
Quirk, Chris, Arul Menezes, and
Colin Cherry. 2005. Dependency treelet
translation: Syntactically informed
phrasal SMT. In ACL, pages 271?279,
Oxford.
Rafferty, Anna and Christopher D. Manning.
2008. Parsing three German Treebanks:
Lexicalized and unlexicalized baselines.
In Proceedings of the Workshop on Parsing
German, pages 40?46, Columbus, OH.
Rambow, Owen. 2010. The simple truth
about dependency and phrase structure
representations: an opinion piece.
In HLT-NAACL, pages 337?340,
Los Angeles, CA.
Rehbein, Ines and Josef van Genabith.
2007. Evaluating evaluation measures.
In NODALIDA, pages 372?379, Tartu.
Schiehlen, Michael. 2004. Annotation
strategies for probabilistic parsing in
German. In COLING, pages 390?396,
Geneva.
Schmid, Helmut. 1995. Improvements in
part-of-speech tagging with an application
to German. In Proceedings of the ACL
SIGDAT-Workshop, pages 47?50, Dublin.
Schmid, Helmut. 2004. Efficient parsing
of highly ambiguous context-free
grammars with bit vectors. In COLING,
pages 162?168, Geneva.
Schmid, Helmut. 2006. Trace prediction
and recovery with unlexicalized PCFGs
and slash features. In COLING-ACL,
pages 177?184, Sydney.
Schmid, Helmut, Arne Fitschen, and
Ulrich Heid. 2004. SMOR: A German
computational morphology covering
derivation, composition and inflection.
In LREC, pages 1,263?1,266, Lisbon.
Seeker, Wolfgang, Bernd Bohnet, Lilja
?vrelid, and Jonas Kuhn. 2010a.
Informed ways of improving data-driven
dependency parsing for German. In
COLING: Posters, pages 1,122?1,130,
Beijing.
Seeker, Wolfgang, Ines Rehbein, Jonas Kuhn,
and Josef Van Genabith. 2010b. Hard
constraints for grammatical function
labelling. In ACL, pages 1,087?1,097,
Uppsala.
Shen, Libin, Jinxi Xu, and Ralph Weischedel.
2008. A new string-to-dependency
machine translation algorithm with a
target dependency language model. In
ACL-HLT, pages 577?585, Columbus, OH.
Tsarfaty, Reut, Joakim Nivre, and Evelina
Andersson. 2012. Cross-framework
evaluation for statistical parsing.
In EACL, pages 44?54, Avignon.
Tsarfaty, Reut, Djame? Seddah, Yoav
Goldberg, Sandra Kuebler, Yannick
Versley, Marie Candito, Jennifer Foster,
Ines Rehbein, and Lamia Tounsi. 2010.
Statistical parsing of morphologically
rich languages (SPMRL) what, how and
whither. In Proceedings of the NAACL HLT
2010 First Workshop on Statistical Parsing of
Morphologically-Rich Languages, pages 1?12,
Los Angeles, CA.
Tu, Zhaopeng, Yang Liu, Young-Sook
Hwang, Qun Liu, and Shouxun Lin. 2010.
Dependency forest for statistical machine
translation. In COLING, pages 1,092?1,100,
Beijing.
Versley, Yannick. 2005. Parser evaluation
across text types. In Fourth Workshop on
Treebanks and Linguistic Theories (TLT),
pages 209?220, Barcelona.
Versley, Yannick and Ines Rehbein. 2009.
Scalable discriminative parsing for
German. In IWPT, pages 134?137, Paris.
Weischedel, Ralph, Marie Meteer, Richard
Schwartz, Lance Ramshaw, and Jeff
Palmucci. 1993. Coping with ambiguity
and unknown words through probabilistic
models. Computational Linguistics,
19(2):359?382.
Yusuke, Miyao and Tsujii Jun?ichi. 2002.
Maximum entropy estimation for
feature forests. In HLT, pages 292?297,
San Diego, CA.
85

Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 737?740,
Los Angeles, California, June 2010. c?2010 Association for Computational Linguistics
Bitext-Based Resolution of German Subject-Object Ambiguities
Florian Schwarck Alexander Fraser
Institute for Natural Language Processing
University of Stuttgart
{koehlefn,fraser}@ims.uni-stuttgart.de
Hinrich Schu?tze
Abstract
We present a method for disambiguating syn-
tactic subjects from syntactic objects (a fre-
quent ambiguity) in German sentences taken
from an English-German bitext. We exploit
the fact that subject and object are usually eas-
ily determined in English. We show that a
simple method disambiguates some subject-
object ambiguities in German, while making
few errors. We view this procedure as the first
step in automatically acquiring (mostly) cor-
rect labeled data. We also evaluate using it to
improve a state of the art statistical parser.
1 Introduction
Ambiguity of grammatical role is a problem when
parsing a number of natural languages. In German,
subject-object ambiguities are frequent. The sen-
tence ?Die Maus jagt die Katze? ?the ? mouse ?
chases ? the ? cat? exhibits such an ambiguity. Be-
cause word order is freer in German than in English,
the sentence has two possible meanings: (i) The cat
is chasing the mouse and (ii) the mouse is chasing
the cat. We exploit the fact that such ambiguities are
much less frequent in languages that possess a less
flexible syntax than German. In English, the trans-
lation of the sentence ?Die Maus jagt die Katze? is
not ambiguous. If we have access to this translation,
we can use this information to disambiguate the Ger-
man sentence. The English translation is viewed as
a surrogate for both contextual knowledge from the
text and for world knowledge.
We present a method for disambiguating the sub-
ject and object roles in German sentences. We use
an English-German bitext and exploit the fact that
subject and object roles are rarely ambiguous in En-
glish. Using a new gold standard we created we
show that our method disambiguates a significant
proportion of subject-object ambiguities in German
with high precision. We view this procedure as the
first step in automatically acquiring (mostly) correct
labeled data for training a statistical disambiguator
that can be used on German text (even when no
translation is available). In addition to measuring
algorithm performance directly, we present experi-
ments on improving the disambiguation of BitPar, a
state of the art statistical parser.
2 Algorithm
Data and Word Alignment. We use the aligned
English and German sentences in Europarl (Koehn,
2005) for our experiments. The corpus contains long
and complex sentences. To establish translational
correspondence between parallel sentences we use
GIZA++ (Och and Ney, 2003). Its input is a tok-
enized parallel corpus. We lemmatized the text prior
to aligning it.
Procedure. Figure 1 shows the architecture of
our system. The boxes signify data sets, while the
lines are processes applied to the data sets. The pa-
per presents two applications. The first is the cre-
ation of a set of disambiguated German sentences
(which involves word alignments in the upper right
corner, and the use of parsers in the middle of the
graphic). We also present a reranking of the N -best
parses produced by BitPar (Schmid, 2004), a state of
the art statistical parser (bottom of the graphic).
For processing of German we chose FSPar
737
Figure 1: System Architecture
(Schiehlen, 2003), a fast shallow dependency parser.
FSPar has extensive lexical knowledge which helps
it to find subject-object ambiguities with high accu-
racy, but it does not try to resolve such ambiguities.
The key to our approach is to project syntactic
roles from English text. For English parsing we used
MINIPAR (Lin, 1998).
Based on FSPar?s analysis, all German sentences
with a subject-object ambiguity (about a third) were
selected from EuroParl. The parallel English sen-
tences were parsed with MINIPAR.
Words marked as ambiguous by FSPar were then
processed using our algorithm. If an ambiguous
German word was aligned to an English word that
MINIPAR had (unambiguously) assigned the gram-
matical role of subject or object, then the syntactic
role of the German word was defined by this infor-
mation, see Figure 2.
Figure 2: Disambiguation Algorithm
We used standard heuristics for improving word
alignment (Och and Ney, 2003; Koehn et al, 2003),
but there were many misalignments of ambiguous
German words. In order for the procedure to work,
we require that the German word to be disam-
biguated be aligned to the English subject or object.
For this reason, we implemented second guessing
based on a dictionary that lists for every German
word the 10 most frequently aligned English words
(found using the word alignment of all of Europarl).
If an ambiguous German word was either unaligned
or not aligned to the English subject or object, it was
checked whether a dictionary translation was part of
the parallel sentence and marked as subject or ob-
ject by MINIPAR. If so, this dictionary word was
used for disambiguation.
3 Evaluation
Gold Standard. We had access to a small set of
gold standard parses (Pado? and Lapata, 2009), but
decided to create a larger corpus. We found that FS-
Par had acceptable performance for finding subject-
object ambiguities1. The syntactic roles of words
marked as ambiguous by FSPar were annotated.
Four annotators annotated the syntactic roles in 4000
sentences using a graphical user interface (GUI).
The GUI showed the ambiguous words in context
and gave the annotator four different subject-object
labels to choose from for each ambiguous word:
subject, object, expletive es and none. Because the
syntactic expletive ?es? (English gloss: ?it?) is fre-
quent in German, as in ?es scheint zu regnen? ?it
appears to be raining,? we created a separate label
for expletive ?es?, which is not treated as a subject.2
The statistics are shown in table 1.
1000 sentences were annotated by all four an-
notators. Inter-annotator agreement was sufficient
(? = 0.77 on average (Carletta, 1996)).
Evaluation Measures. The output of our algo-
rithm labels each word that FSPar classified as am-
biguous with one of the three possible labels subject,
1FSPar has a very high precision in detecting subject-object
ambiguities, as can be seen in Table 1 (approximately 0.955,
the sum of two left columns divided by sum of all cells). We
tried to get an idea of recall using the smaller gold standard.
We made conservative assumptions about recall errors which
we manually checked on a small sample, details are omitted.
Using these assumptions led to an estimate for recall of 0.733,
but true recall is likely higher.
2German ?es? is also frequently used as a non-expletive,
where it can take a syntactic role.
738
subj obj expl es none
Annotator1 4152 3210 115 150
Annotator2 4472 3359 92 226
Annotator3 4444 3584 42 155
Annotator4 4027 3595 9 650
Table 1: Annotator decisions on the full gold standard
DE2EN Refined GDFA Intersection
nosg
P 0.8412 0.8381 0.8353 0.8551
R 0.4436 0.3856 0.3932 0.3380
F1 0.5809 0.5282 0.5347 0.4845
sg
P 0.7404 0.7307 0.7310 0.7240
R 0.5564 0.4873 0.4946 0.4528
F1 0.6353 0.5847 0.5900 0.5571
filter-nosg
P 0.9239 0.9203 0.9192 0.9277
R 0.3940 0.3397 0.3461 0.2984
F1 0.5524 0.4962 0.5028 0.4515
filter-sg
P 0.8458 0.8358 0.8369 0.8290
R 0.4839 0.4213 0.4279 0.3898
F1 0.6156 0.5602 0.5662 0.5302
Table 2: Precision, Recall and F1 of the algorithm.
object and no decision3. We use the standard evalua-
tion metrics Precision (P , the percentage of subject
and object labelings in our hypothesis that are cor-
rect), Recall (R, the percentage of subject and ob-
ject labelings in the gold standard that are correctly
labeled in the hypothesis), and balanced F (F1).
4 Experiments
Algorithm Performance. Table 2 shows the perfor-
mance of our algorithm when evaluated against the
manual annotation4. The lines nosg, sg, filter-nosg
and filter-sg denote different configurations of the al-
gorithm: Second guessing (section 2) was (?sg?) or
was not (?nosg?) applied and filtering was (?filter?)
or was not applied. The filter increases precision by
only keeping labels of subjects and objects that oc-
cur in the default order (e.g., the subject is to the
left of the object in the main clause). As an aid to
the user, FSPar presents such a determination of de-
fault order depending on its classification of clause
type5. The columns indicate the heuristic postpro-
3If expletive es or none was annotated, the system is correct
if it does not make a decision.
4Because of problems with BitPar caused by preprocessing
for FSPar, we use 11,279 sentences of the 13,000 annotated.
5Using this determination alone results in P 0.7728 R 0.8206
F 0.7960, very high recall but low precision.
configuration P R F1
1 top-1 (no change) 0.8088 0.8033 0.8060
2 relabeling nosg 0.7998 0.8176 0.8086
3 relabeling filter-nosg 0.8229 0.8344 0.8286
4 reranking nosg 0.8082 0.8123 0.8102
5 reranking filter-nosg 0.8145 0.8143 0.8144
Table 3: Precision, Recall and F1 of changing BitPar de-
cisions, DE2EN alignment
cessing we applied to GIZA++?s alignment. DE2EN
is the 1-to-N alignment calculated using German as
the source language and English as the target lan-
guage (i.e., each English word is linked to exactly
zero or one German words).
As we see in table 2, with the most strict setup,
filter-nosg, the algorithm resolves subject-object
ambiguities with a precision of more than 92%
but the best recall is only 39.4%, obtained using
DE2EN. Second guessing increases recall but leads
to losses in precision. The best precision result with-
out the filter is 85.5%.
Improving BitPar?s Subject-Object Decisions.
For improving BitPar (which always tries to disam-
biguate subject-object), our baseline is the accuracy
of the most probable parse shown in table 3, row 1.
Using the most probable parse from BitPar, we
relabel a word ?subject? or ?object? if our system
indicates to do so. With the algorithm alone we are
able to improve recall (table 3, row 2). When we add
the filter both precision and recall are improved (row
3). This experiment measures the improvement pos-
sible if our syntactic role information were directly
integrated as a hard constraint into a parser (see sec-
tion 5).
We now perform a simple reranking experiment,
using BitPar?s 100-best parses. For each sentence
we choose the parse which agrees with as many of
the subject/object decisions of the algorithm as pos-
sible (once again ignoring words where the algo-
rithm chooses no decision). In case of ties in the
number of agreements, we take the most probable
parse. The results are in rows 4?5. Reranking in-
creases F1 by about 0.8%.
5 Related Work
Syntactic projection has been used to bootstrap tree-
banks in resource poor languages (Yarowsky and
739
Ngai, 2001; Hwa et al, 2005). In contrast with such
work, we are addressing subject-object ambiguity in
German. German parsers have no access to the con-
textual and world knowledge necessary to resolve
this ambiguity.
Work on projecting semantic roles (Pado? and La-
pata, 2009; Fung et al, 2007) requires both syntac-
tic parsing and semantic role labeling and is con-
cerned with filling in the complete information in a
semantic frame. Our approach is simpler and con-
cerned only with syntactic disambiguation, not se-
mantic projection. We focus only on difficult cases
of subject-object ambiguity and although we do not
always make a prediction, we obtain levels of pre-
cision that projection approaches making no use of
knowledge of German syntax cannot achieve.
In bitext parsing, Burkett and Klein (2008) and
Fraser et al (2009) used feature functions defined on
triples of (parse tree in language 1, parse tree in lan-
guage 2, word alignment), combined in a log-linear
model trained to maximize parse accuracy, requir-
ing translated treebanks. We focus only on subject-
object disambiguation in German, and annotated a
new gold standard. We work on sentences that a
partial parser has determined to be ambiguous. Fos-
sum and Knight (2008) and Huang et al (2009) im-
prove English prepositional phrase attachment using
features from an unparsed Chinese sentence. The
latter work integrated the PP-attachment constraint
(detected from the Chinese translation) directly into
an English shift-reduce parser. As we have shown
in the labeling experiment, integrating our subject-
object disambiguation into BitPar could result in fur-
ther increases beyond 100-best reranking.
6 Conclusion
We demonstrated the utility of bitext-based disam-
biguation of grammatical roles. We automatically
created a large corpus of 164,874 disambiguated
subject-object decisions with a precision of over
92%. This corpus will be of use in future research
on syntactic role preferences and for the training
of monolingual subject-object disambiguators. We
presented a prototype application of subject-object
disambiguation through a simple reranking of the
100-best list output by BitPar, and showed a possible
further improvement if integrated in the parser. The
new gold standard, which is publicly available, will
hopefully be useful for work on both monolingual
and bitext-based disambiguation.
Acknowledgments
This work was supported by Deutsche Forschungs-
gemeinschaft grants SFB 732 and MorphoSynt.
References
David Burkett and Dan Klein. 2008. Two languages are
better than one (for syntactic parsing). In EMNLP.
Jean Carletta. 1996. Assessing agreement on classifi-
cation tasks: The kappa statistic. Computational Lin-
guistics, 22.
Victoria Fossum and Kevin Knight. 2008. Using bilin-
gual Chinese-English word alignments to resolve PP-
attachment ambiguity in English. In AMTA.
Alexander Fraser, Renjing Wang, and Hinrich Schu?tze.
2009. Rich bitext projection features for parse rerank-
ing. In EACL.
Pascale Fung, Zhaojun Wu, Yongsheng Yang, and Dekai
Wu. 2007. Learning bilingual semantic frames: Shal-
low semantic parsing vs. semantic role projection. In
TMI.
Liang Huang, Wenbin Jiang, and Qun Liu. 2009.
Bilingually-constrained (monolingual) shift-reduce
parsing. In EMNLP.
Rebecca Hwa, Philip Resnik, Amy Weinberg, Clara
Cabezas, and Okan Kolak. 2005. Bootstrapping
parsers via syntactic projection across parallel texts.
Nat. Lang. Eng., 11(3).
Philipp Koehn, Franz J. Och, and Daniel Marcu. 2003.
Statistical phrase-based translation. In HLT-NAACL.
Philipp Koehn. 2005. Europarl: a parallel corpus for
statistical machine translation. In MT Summit X.
Dekang Lin. 1998. Dependency-based evaluation of
MINIPAR. In Workshop on Eval of Parsing Systems.
Franz J. Och and Hermann Ney. 2003. A systematic
comparison of various statistical alignment models.
Computational Linguistics, 29(1).
Sebastian Pado? and Mirella Lapata. 2009. Cross-lingual
annotation projection of semantic roles. Journal of Ar-
tificial Intelligence Research, 36:307?340.
Michael Schiehlen. 2003. A cascaded finite-state parser
for German. In Research Notes (EACL).
Helmut Schmid. 2004. Efficient parsing of highly am-
biguous context-free grammars with bit vectors. In
COLING.
David Yarowsky and Grace Ngai. 2001. Inducing multi-
lingual POS taggers and NP bracketers via robust pro-
jection across aligned corpora. In NAACL.
740
2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 508?512,
Montre?al, Canada, June 3-8, 2012. c?2012 Association for Computational Linguistics
Active Learning for Coreference Resolution
Florian Laws1 Florian Heimerl2 Hinrich Schu?tze1
1 Institute for Natural Language Processing (IMS)
Universita?t Stuttgart
florian.laws@ims.uni-stuttgart.de
2 Institute for Visualization and Interactive Systems
Universita?t Stuttgart
florian.heimerl@vis.uni-stuttgart.de
Abstract
We present an active learning method for
coreference resolution that is novel in three re-
spects. (i) It uses bootstrapped neighborhood
pooling, which ensures a class-balanced pool
even though gold labels are not available. (ii)
It employs neighborhood selection, a selection
strategy that ensures coverage of both posi-
tive and negative links for selected markables.
(iii) It is based on a query-by-committee selec-
tion strategy in contrast to earlier uncertainty
sampling work. Experiments show that this
new method outperforms random sampling in
terms of both annotation effort and peak per-
formance.
1 Introduction
Coreference resolution (CR) ? the task of determin-
ing if two expressions in natural language text re-
fer to the same real-world entity ? is an important
NLP task. One popular approach to CR is super-
vised classification. This approach needs manually
labeled training data that is expensive to create. Ac-
tive learning (AL) is a technique that can reduce this
cost by setting up an interactive training/annotation
loop that selects and annotates training examples
that are maximally useful for the classifier that is
being trained. However, while AL has been proven
successful for many other NLP tasks, such as part-
of-speech tagging (Ringger et al, 2007), parsing
(Osborne and Baldridge, 2004), text classification
(Tong and Koller, 2002) and named entity recogni-
tion (Tomanek et al, 2007), AL has not been suc-
cessfully applied to coreference resolution so far.
In this paper, we present a novel approach to AL
for CR based on query-by-committee sampling and
bootstrapping and show that it performs better than
a number of baselines.
2 Related work
Coreference resolution. The perhaps most widely
used supervised learning approach to CR is the
mention-pair model (Soon et al, 2001). This model
classifies links (pairs of two mentions) as corefer-
ent or disreferent, followed by a clustering stage that
partitions entities based on the link decisions. Our
AL method is partially based on the class balancing
strategy proposed by Soon et al (2001).
While models other than mention-pair have been
proposed (Culotta et al, 2007), none performs
clearly better as evidenced by recent shared evalu-
ations such as SemEval 2010 (Recasens et al, 2010)
and CoNLL 2011 (Pradhan et al, 2011).
Active learning. The only existing publication
on AL for CR that we are aware of is (Gasperin,
2009). She uses a mention-pair model on a biomed-
ical corpus. The classifier is Naive Bayes and the
AL method uncertainty sampling (Lewis and Gale,
1994). The results are negative: AL is not bet-
ter than random sampling. In preliminary experi-
ments, we replicated this result for our corpus and
our system: Uncertainty sampling is not better than
random sampling for CR. Uncertainty sampling can
fail if uncertainty assessments are too unstable for
successful example selection (cf. Dwyer and Holte
(2007)). This seems to be the case for the decision
trees we use. Naive Bayes is also known to give bad
uncertainty assessments (Domingos and Pazzani,
508
1997). We therefore adopted a query-by-committee
approach combined with a class-balancing strategy.
3 Active learning for CR
The classifier in the mention-pair model is faced
with a severe class imbalance: there are many more
disreferent than coreferent links. To address this im-
balance, we use a neighborhood pool or N-pool as
proposed by Soon et al (2001).
Generation of the N-pool. The neighborhood
of markable x used in N-pooling is defined as the
set consisting of the link between x and its closest
coreferent markable y(x) to the left and all disref-
erent links in between. For a particular markable x,
let y(x) be the closest coreferent markable for x to
the left of x. Between y(x) and x, there are disref-
erent markables zi, so we have a constellation like
y(x), z1, . . . , zn, x. The neighborhood of x is then
the set of links
{(y, x), (z1, x) . . . , (zn, x)}
This set is empty if x does not have a coreferent
markable to the left.
We call the set of all such neighborhoods the N-
pool. The N-pool is a subset of the entire pool of
links.
Bootstrapping the neighborhood. Soon et al
(2001) introduce N-pooling for labeled data. In AL,
no labeled data (or very little of it) is available. In-
stead, we employ the committee of classifiers that
we use for AL example selection for bootstrapping
the N-pool. We query the committee of classifiers
from the last AL iteration and treat a link as coref-
erent if and only if the majority of the classifiers
classifies it as coreferent. We then construct the N-
pool using these bootstrapped labels to determine
the coreferent markables y(x) and then construct the
neighborhoods as described above.
If this procedure yields no coreferent links in an
iteration, we sample links left of randomly selected
markables instead of N-pooling.
Example selection granularity. We use a query-
by-committee approach to AL. The committee con-
sists of 10 instances of the link classifier of the CR
system, each trained on a randomly chosen subset of
the links that have been manually labeled so far.
In each iteration, the N-pool is recomputed and
a small subset of the N-pool is selected for label-
ing. We experiment with two selection granularities.
In neighborhood selection, entire neighborhoods are
selected and labeled in each iteration. We define the
utility of a neighborhood as the average of the vote
entropies (Argamon-Engelson and Dagan, 1999) of
its links.
In link selection, individual links with the highest
utility are selected ? in most cases these will be from
different neighborhoods. Utility is again defined as
vote entropy.
Our hypothesis is that, compared to selection of
individual links, neighborhood selection yields a
more balanced sample that covers both positive and
negative links for a markable. At the same time,
neighborhood selection retains the benefits of AL
sampling: difficult (or highly informative) links are
selected.
4 Experiments
We use the mention-pair CR system SUCRE (Kob-
dani et al, 2011). The link classifier is a deci-
sion tree and the clustering algorithm a variant of
best-first clustering (Ng and Cardie, 2002). SUCRE
results were competitive in SEMEVAL 2010 (Re-
casens et al, 2010). We implemented N-pool boot-
strapping and selection methods on top of the AL
framework of Tomanek et al (2007).
We use the English part of the SemEval-2010 CR
task data set, a subset of OntoNotes 2.0 (Hovy et al,
2006). Training and test set sizes are about 96,000
and 24,000 words. Since we focus on the coref-
erence resolution subtask, we use the true mention
boundaries for the markables.
The pool for example selection is created by pair-
ing every markable with every preceding markable
within a window of 100 markables. This yields a
pool of 1.7 million links, of which only 1.5% are
labeled as coreferent. This drastic class imbalance
necessitates our bootstrapped class-balancing.
We run two baseline experiments for compari-
son: (i) random selection on the entire pool, with-
out any class balancing, and (ii) random selection
from a gold-label-based N-pool. We chose to use
gold neighborhood information for the baseline to
remove the influence of badly predicted neighbor-
509
20,000 links 50,000 links
MUC B3 CEAF mean MUC B3 CEAF mean
(1) random entire pool 49.68 86.07 82.34 72.70 48.81 86.00 82.24 72.34
(2) N-pooling 61.60 85.00 82.85 76.48 62.60 85.99 83.44 77.33
(3) AL link selection 55.65 86.91? 83.67? 75.41 55.84 86.94? 83.70 75.49
(4) neighborhood sel. 63.07? 86.94? 84.42? 78.14? 63.81? 87.11? 84.33? 78.42?
Table 1: Performance of different methods. All measures are F1 measures.
hoods and focus on the performance of random sam-
pling. Hence, this is a very strong random baseline.
The performance with bootstrapped neighborhoods
would likely be lower.
We run 10 runs of each experiment, starting from
10 different seed sets. These seed sets contained 200
links, drawn randomly from the entire pool, for ran-
dom sampling; and 20 neighborhoods for neighbor-
hood selection, with a comparable number of links.
We verified that each seed set contained instances of
both classes.
5 Results
We determine the performance of CR depending on
the number of links used for training. The results
of the experiments are shown in Table 1 and Fig-
ures 1a to 1d. We show results for four coreference
measures: MUC, B3, entity-based CEAF (hence-
forth: CEAF), and the arithmetic mean of MUC, B3
and CEAF (as suggested by the CoNLL-2011 shared
evaluation).
In all four figures, the AL curves have reached a
plateau at 20,000 links. At this point, neighborhood
selection AL (line 4 in Table 1) outperforms random
sampling from the N-pool (line 2) for all coreference
measures, with gains from 1.47 points for MUC to
1.94 points for B3.
At 20,000 links, the N-pooling random baseline
(line 2) has not yet reached maximum performance,
but even at 50,000 links, neighborhood selection AL
still outperforms the baselines. (AL and baseline
performance will eventually converge when most
links from the pool are sampled, but this will hap-
pen much later, since the pool has 1.7 million links
in total).
?Statistically significant at p < .05 compared to baseline 2
using the sign test (N = 10, k ? 9 successes).
Link selection AL (line 3) outperforms the base-
lines for B3 and CEAF, but is performing markedly
worse than the N-pooling random baseline (line 2)
for MUC (due to low recall for MUC) and mean F1.
Link selection yields a CR system that proposes a
lot of singleton entities that are not coreferent with
any other entity. The MUC scoring scheme does not
give credit to singletons at all, thus the lower recall.
Neighborhood selection AL initially has low
MUC, but starts to outperform the baseline at 15,000
links (Figure 1a). For B3 and CEAF, neighborhood
selection AL outperforms the baselines much ear-
lier, at a few 1000 links (Figures 1b and 1c). It thus
shows more robust performance for all evaluation
metrics.
Neighborhood selection AL also performs at least
as well as (for B3) or better than (MUC and CEAF)
link selection AL. Learning curves of neighborhood
selection AL are consistently above the link selec-
tion curves. We therefore consider neighborhood se-
lection AL to be the preferred AL setup for CR.
6 Conclusion
We have presented a new AL method for corefer-
ence resolution. The proposed method is novel in
three respects. (i) It uses bootstrapped N-pooling,
which ensures a class-balanced pool even though
gold labels are not available. (ii) It further improves
class balancing by neighborhood selection, a selec-
tion strategy that ensures coverage of positive and
negative links per markable while still focusing on
selecting difficult links. (iii) It is based on a query-
by-committee selection strategy in contrast to ear-
lier uncertainty sampling work. Experiments show
that this new method outperforms random sampling
in terms of both annotation effort and peak perfor-
mance.
510
Acknowledgments
Florian Laws is a recipient of the Google Europe
Fellowship in Natural Language Processing, and
this research is supported in part by his fellowship.
Florian Heimerl was supported by the Deutsche
Forschungsgemeinschaft as part of the priority pro-
gram 1335 ?Scalable Visual Analytics?.
0 10000 30000 50000
0.3
0.4
0.5
0.6
Links sampled
MU
C.F
Random, entire pool
Random, N?pooling
AL, link selection
AL, neighborhood sel.
(a) Learning curve for MUC
0 10000 30000 50000
0.7
0
0.7
5
0.8
0
0.8
5
0.9
0
Links sampled
B3
.F
Random, entire pool
Random, N?pooling
AL, link selection
AL, neighborhood sel.
(b) Learning curve for B3
0 10000 30000 50000
0.7
0
0.7
5
0.8
0
0.8
5
0.9
0
Links sampled
CE
AF
.
E.F
Random, entire pool
Random, N?pooling
AL, link selection
AL, neighborhood sel.
(c) Learning curve for CEAF
0 10000 30000 50000
0.6
0
0.6
5
0.7
0
0.7
5
0.8
0
Links sampled
CO
NL
L.M
EA
N.F
Random, entire pool
Random, N?pooling
AL, link selection
AL, neighborhood sel.
(d) Learning curve for the mean of the CR measures.
Figure 1: Learning curves for AL and baseline experiments. All measures are F1 measures.
511
References
S. Argamon-Engelson and I. Dagan. 1999. Committee-
based sample selection for probabilistic classifiers.
JAIR, 11:335?360.
A. Culotta, M. Wick, R. Hall, and A. McCallum. 2007.
First-order probabilistic models for coreference reso-
lution. In HLT-NAACL 2007.
Pedro Domingos and Michael J. Pazzani. 1997. On the
optimality of the simple bayesian classifier under zero-
one loss. Mach. Learn., 29(2-3):103?130.
K. Dwyer and R. Holte. 2007. Decision tree instability
and active learning. In ECML.
C. Gasperin. 2009. Active learning for anaphora resolu-
tion. In Proceedings of the NAACL HLT 2009 Work-
shop on Active Learning for Natural Language Pro-
cessing.
E. Hovy, M. Marcus, M. Palmer, L. Ramshaw, and
R. Weischedel. 2006. Ontonotes: The 90% solution.
In HLT-NAACL.
H. Kobdani, H. Schu?tze, M. Schiehlen, and H. Kamp.
2011. Bootstrapping coreference resolution using
word associations. In ACL.
D. Lewis and W. Gale. 1994. A sequential algorithm for
training text classifiers. In SIGIR.
V. Ng and C. Cardie. 2002. Improving machine learning
approaches to coreference resolution. In ACL.
M. Osborne and J. Baldridge. 2004. Ensemble-based
active learning for parse selection. In HLT-NAACL.
S. Pradhan, L. Ramshaw, M. Marcus, M. Palmer,
R. Weischedel, and N. Xue. 2011. Conll-2011 shared
task: Modeling unrestricted coreference in ontonotes.
In CoNLL.
M. Recasens, L. Ma`rquez, E. Sapena, M. A. Mart??,
M. Taule?, V. Hoste, M. Poesio, and Y. Versley. 2010.
Semeval-2010 task 1: Coreference resolution in multi-
ple languages. In Proceedings of the 5th International
Workshop on Semantic Evaluation.
E. Ringger, P. McClanahan, R. Haertel, G. Busby,
M. Carmen, J. Carroll, K. Seppi, and D. Lonsdale.
2007. Active learning for part-of-speech tagging: Ac-
celerating corpus annotation. In Linguistic Annotation
Workshop at ACL-2007.
W. M. Soon, D. Chung, D. Chung Yong Lim, Y. Lim,
and H. T. Ng. 2001. A machine learning approach
to coreference resolution of noun phrases. Computa-
tional Linguistics, 27(4).
K. Tomanek, J. Wermter, and U. Hahn. 2007. An ap-
proach to text corpus construction which cuts annota-
tion costs and maintains reusability of annotated data.
In EMNLP-CoNLL.
S. Tong and D. Koller. 2002. Support vector machine
active learning with applications to text classification.
JMLR, 2:45?66.
512
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 783?792,
Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational Linguistics
Bootstrapping Coreference Resolution Using Word Associations
Hamidreza Kobdani, Hinrich Schu?tze, Michael Schiehlen and Hans Kamp
Institute for Natural Language Processing
University of Stuttgart
kobdani@ims.uni-stuttgart.de
Abstract
In this paper, we present an unsupervised
framework that bootstraps a complete corefer-
ence resolution (CoRe) system from word as-
sociations mined from a large unlabeled cor-
pus. We show that word associations are use-
ful for CoRe ? e.g., the strong association be-
tween Obama and President is an indicator
of likely coreference. Association information
has so far not been used in CoRe because it is
sparse and difficult to learn from small labeled
corpora. Since unlabeled text is readily avail-
able, our unsupervised approach addresses the
sparseness problem. In a self-training frame-
work, we train a decision tree on a corpus that
is automatically labeled using word associa-
tions. We show that this unsupervised system
has better CoRe performance than other learn-
ing approaches that do not use manually la-
beled data.
1 Introduction
Coreference resolution (CoRe) is the process of find-
ing markables (noun phrases) referring to the same
real world entity or concept. Until recently, most ap-
proaches tried to solve the problem by binary classi-
fication, where the probability of a pair of markables
being coreferent is estimated from labeled data. Al-
ternatively, a model that determines whether a mark-
able is coreferent with a preceding cluster can be
used. For both pair-based and cluster-based models,
a well established feature model plays an important
role. Typical systems use a rich feature space based
on lexical, syntactic and semantic knowledge. Most
commonly used features are described by Soon et al
(2001).
Most existing systems are supervised systems,
trained on human-labeled benchmark data sets for
English. These systems use linguistic features based
on number, gender, person etc. It is a challenge to
adapt these systems to new domains, genres and lan-
guages because a significant human labeling effort is
usually necessary to get good performance.
To address this challenge, we pursue an unsuper-
vised self-training approach. We train a classifier
on a corpus that is automatically labeled using asso-
ciation information. Self-training approaches usu-
ally include the use of some manually labeled data.
In contrast, our self-trained system is not trained on
any manually labeled data and is therefore a com-
pletely unsupervised system. Although training on
automatically labeled data can be viewed as a form
of supervision, we reserve the term supervised sys-
tem for systems that are trained on manually labeled
data.
The key novelty of our approach is that we boot-
strap a competitive CoRe system from association
information that is mined from an unlabeled cor-
pus in a completely unsupervised fashion. While
this method is shallow, it provides valuable informa-
tion for CoRe because it considers the actual iden-
tity of the words in question. Consider the pair of
markables (Obama, President). It is a likely coref-
erence pair, but this information is not accessible
to standard CoRe systems because they only use
string-based features (often called lexical features),
named entity features and semantic word class fea-
tures (e.g., from WordNet) that do not distinguish,
783
say, Obama from Hawking.
In our approach, word association information is
used for clustering markables in unsupervised learn-
ing. Association information is calculated as asso-
ciation scores between heads of markables as de-
scribed below. We view association information as
an example of a shallow feature space which con-
trasts with the rich feature space that is generally
used in CoRe.
Our experiments are conducted using the
MCORE system (?Modular COreference REso-
lution?).1 MCORE can operate in three different
settings: unsupervised (subsystem A-INF), super-
vised (subsystem SUCRE (Kobdani and Schu?tze,
2010)), and self-trained (subsystem UNSEL). The
unsupervised subsystem A-INF (?Association
INFormation?) uses the association scores between
heads as the distance measure when clustering
markables. SUCRE (?SUpervised Coreference
REsolution?) is trained on a labeled corpus
(manually or automatically labeled) similar to
standard CoRe systems. Finally, the unsupervised
self-trained subsystem UNSEL (?UNsupervised
SELf-trained?) uses the unsupervised subsystem
A-INF to automatically label an unlabeled corpus
that is then used as a training set for SUCRE.
Our main contributions in this paper are as fol-
lows:
1. We demonstrate that word association informa-
tion can be used to develop an unsupervised
model for shallow coreference resolution (sub-
system A-INF).
2. We introduce an unsupervised self-trained
method (UNSEL) that takes a two-learner two-
feature-space approach. The two learners are
A-INF and SUCRE. The feature spaces are the
shallow and rich feature spaces.
3. We show that the performance of UNSEL is
better than the performance of other unsuper-
vised systems when it is self-trained on the au-
tomatically labeled corpus and uses the lever-
aging effect of a rich feature space.
4. MCORE is a flexible and modular framework
that is able to learn from data with different
1MCORE can be downloaded from ifnlp.org/
?schuetze/mcore.
quality and domain. Not only is it able to deal
with shallow information spaces (A-INF), but
it can also deliver competitive results for rich
feature spaces (SUCRE and UNSEL).
This paper is organized as follows. Related work
is discussed in Section 2. In Section 3, we present
our system architecture. Section 4 describes the ex-
periments and Section 5 presents and discusses our
results. The final section presents our conclusions.
2 Related Work
There are three main approaches to CoRe: super-
vised, semi-supervised (or weakly supervised) and
unsupervised. We use the term semi-supervised for
approaches that use some amount of human-labeled
coreference pairs.
Mu?ller et al (2002) used co-training for coref-
erence resolution, a semi-supervised method. Co-
training puts features into disjoint subsets when
learning from labeled and unlabeled data and tries
to leverage this split for better performance. Ng and
Cardie (2003) use self-training in a multiple-learner
framework and report performance superior to co-
training. They argue that the multiple learner ap-
proach is a better choice for CoRe than the multi-
ple view approach of co-training. Our self-trained
model combines multiple learners (A-INF and SU-
CRE) and multiple views (shallow/rich informa-
tion). A key difference to the work by Mu?ller et al
(2002) and Ng and Cardie (2003) is that we do not
use any human-labeled coreference pairs.
Our basic idea of self-training without human la-
bels is similar to (Kehler et al, 2004), but we ad-
dress the general CoRe problem, not just pronoun
interpretation.
Turning to unsupervised CoRe, Haghighi and
Klein (2007) proposed a generative Bayesian model
with good performance. Poon and Domingos (2008)
introduced an unsupervised system in the framework
of Markov logic. Ng (2008) presented a generative
model that views coreference as an EM clustering
process. We will show that our system, which is
simpler than prior work, outperforms these systems.
Haghighi and Klein (2010) present an ?almost un-
supervised? CoRe system. In this paper, we only
compare with completely unsupervised approaches,
784
not with approaches that make some limited use of
labeled data.
Recent work by Haghighi and Klein (2009), Klen-
ner and Ailloud (2009) and Raghunathan et al
(2010) challenges the appropriateness of machine
learning methods for CoRe. These researchers show
that a ?deterministic? system (essentially a rule-
based system) that uses a rich feature space includ-
ing lexical, syntactic and semantic features can im-
prove CoRe performance. Almost all CoRe systems,
including ours, use a limited number of rules or fil-
ters, e.g., to implement binding condition A that re-
flexives must have a close antecedent in some sense
of ?close?. In our view, systems that use a few ba-
sic filters are fundamentally different from carefully
tuned systems with a large number of complex rules,
some of which use specific lexical information. A
limitation of complex rule-based systems is that they
require substantial effort to encode the large number
of deterministic constraints that guarantee good per-
formance. Moreover, these systems are not adapt-
able (since they are not machine-learned) and may
have to be rewritten for each new domain, genre
and language. Consequently, we do not compare our
performance with deterministic systems.
Ponzetto (2010) extracts metadata from
Wikipedia for supervised CoRe. Using such
additional resources in our unsupervised system
should further improve CoRe performance. Elsner
et al (2009) present an unsupervised algorithm
for identifying clusters of entities that belong to
the same named entity (NE) class. Determining
common membership in an NE class like person is
an easier task than determining coreference of two
NEs.
3 System Architecture
Figure 1 illustrates the system architecture of our
unsupervised self-trained CoRe system (UNSEL).
Oval nodes are data, box nodes are processes. We
take a self-training approach to coreference resolu-
tion: We first label the corpus using the unsuper-
vised model A-INF and then train the supervised
model SUCRE on this automatically labeled train-
ing corpus. Even though we train on a labeled cor-
pus, the labeling of the corpus is produced in a com-
pletely automatic fashion, without recourse to hu-
Unlabeled Data
Unsupervised Model (A-INF)
Automatically Labeled Data
Supervised Model (SUCRE)
Figure 1: System Architecture of UNSEL (Unsupervised
Self-Trained Model).
man labeling. Thus, it is an unsupervised approach.
The MCORE architecture is very flexible; in par-
ticular, as will be explained presently, it can be eas-
ily adapted for supervised as well as unsupervised
settings.
The unsupervised and supervised models have an
identical top level architecture; we illustrate this in
Figure 2. In preprocessing, tokens (words), mark-
ables and their attributes are extracted from the input
text. The key difference between the unsupervised
and supervised approaches is in how pair estimation
is accomplished ? see Sections 3.1 & 3.2 for de-
tails.
The main task in chain estimation is clustering.
Figure 3 presents our clustering method, which is
used for both supervised and unsupervised CoRe.
We search for the best predicted antecedent (with
coreference probability p ? 0.5) from right to left
starting from the end of the document. McEnery et
al. (1997) showed that in 98.68% of cases the an-
tecedent is within a 10-sentence window; hence we
use a window of 10 sentences for search. We have
found that limiting the search to a window increases
both efficiency and effectiveness.
Filtering. We use a feature definition language
to define the templates according to which the fil-
ters and features are calculated. These templates
are hard constraints that filter out all cases that are
clearly disreferent, e.g., (he, she) or (he, they). We
use the following filters: (i) the antecedent of a re-
flexive pronoun must be in the same sentence; (ii)
the antecedent of a pronoun must occur at a distance
of at most 3 sentences; (iii) a coreferent pair of a
noun and a pronoun or of two pronouns must not
785
Input Text Preprocessing Markables Pair Estimation
Markable Chains Chain Estimation Markable Pairs
Figure 2: Common architecture of unsupervised (A-INF) and supervised (SUCRE) models.
Chain Estimation (M1, M2, . . . , Mn)
1. t? 1
2. For each markable Mi: Ci ? {Mi}
3. Proceed through the markables from the end
of the document. For each Mj , consider each
preceding Mi within 10 sentences:
If Pair Estimation(Mi, Mj)>=t: Ci ? Ci?Cj
4. t? t? 0.01
5. If t >= 0.5: go to step 3
Pair Estimation (Mi, Mj):
If Filtering(Mi, Mj)==FALSE then return 0;
else return the probability p (or association
score N ) of markable pair (Mi, Mj) being
coreferent.
Filtering (Mi, Mj):
return TRUE if all filters for (Mi, Mj) are
TRUE else FALSE
Figure 3: MCORE chain estimation (clustering) algo-
rithm (test). t is the clustering threshold. Ci refers to
the cluster that Mi is a member of.
disagree in number; (iv) a coreferent pair of two pro-
nouns must not disagree in gender. These four filters
are used in supervised and unsupervised modes of
MCORE.
3.1 Unsupervised Model (A-INF)
Figure 4 (top) shows how A-INF performs pair esti-
mation. First, in the pair generation step, all possible
pairs inside 10 sentences are generated. Other steps
are separately explained for train and test as follows.
Train. In addition to the filters (i)?(iv) described
above, we use the following filter: (v) If the head
of markable M2 matches the head of the preceding
markable M1, then we ignore all other pairs for M2
in the calculation of association scores.
This additional filter is necessary because an ap-
proach without some kind of string matching con-
straint yields poor results, given the importance of
string matching for CoRe. As we will show below,
even the simple filters (i)?(v) are sufficient to learn
high-quality association scores; this means that we
do not need the complex features of ?determinis-
tic? systems. However, if such complex features are
available, then we can use them to improve perfor-
mance in our self-trained setting.
To learn word association information from an
unlabeled corpus (see Section 4), we compute mu-
tual information (MI) scores between heads of mark-
ables. We defineMI as follows: (Cover and Thomas,
1991)
MI(a, b) =
?
i?{a?,a}
?
j?{b?,b}
P (i, j) log2
P (i, j)
P (i)P (j)
E.g., P (a, b?) is the probability of a pair whose two
elements are a and a word not equal to b.
Test. A key virtue of our approach is that in the
classification of pairs as coreferent/disreferent, the
coreference probability p estimated in supervised
learning plays exactly the same role as the associ-
ation information score N (defined below). For p, it
is important that we only consider pairs with p ? 0.5
as potentially coreferent (see Figure 3). To be able to
impose the same constraint on N , we normalize the
MI scores by the maximum values of the two words
and take the average:
N(a, b) =
1
2
(
MI(a, b)
argmaxxMI(a, x)
+
MI(a, b)
argmaxxMI(x, b)
)
In the above equation, the value of N indicates how
strongly two words are associated. N is normalized
to ensure 0 ? N ? 1. If a or b did not occur, then
we set N =0.
In filtering for test, we use filters (i)?(iv). We then
fetch the MI values and calculate N values. The
clustering algorithm described in Figure 3 uses these
N values in exactly the same way as p: we search for
the antecedent with the maximum association score
786
N greater than 0.5 from right to left starting from
the end of the document.
As we will see below, using N scores acquired
from an unlabeled corpus as the only source of in-
formation for CoRe performs surprising well. How-
ever, the weaknesses of this approach are (i) the fail-
ure to cover pairs that do not occur in the unlabeled
corpus (negatively affecting recall) and (ii) the gen-
eration of pairs that are not plausible candidates for
coreference (negatively affecting precision). To ad-
dress these problems, we train a model on a corpus
labeled by A-INF in a self-training approach.
3.2 Supervised Model (SUCRE)
Figure 4 (bottom) presents the architecture of pair
estimation for the supervised approach (SUCRE).
In the pair generation step for train, we take each
coreferent markable pair (Mi, Mj) without inter-
vening coreferent markables and use (Mi, Mj) as a
positive training instance and (Mi, Mk), i < k < j,
as negative training instances. For test, we generate
all possible pairs within 10 sentences. After filter-
ing, we then calculate a feature vector for each gen-
erated pair that survived filters (i)?(iv).
Our basic features are similar to those described
by Soon et al (2001): string-based features, dis-
tance features, span features, part-of-speech fea-
tures, grammatical features, semantic features, and
agreement features. These basic features are engi-
neered with the goal of creating a feature set that
will result in good performance. For this purpose
we used the relational feature engineering frame-
work which has been presented in (Kobdani et al,
2010). It includes powerful and flexible methods for
implementing and extracting new features. It allows
systematic and fast search of the space of features
and thereby reduces the time and effort needed for
defining optimal features. We believe that the good
performance of our supervised system SUCRE (ta-
bles 1 and 2) is the result of our feature engineering
approach.2
As our classification method, we use a decision
2While this is not the focus of this paper, SUCRE has per-
formance comparable to other state-of-the-art supervised sys-
tems. E.g., B3/MUC F1 are 75.6/72.4 on ACE-2 and 69.4/70.6
on MUC-6 compared to 78.3/66.0 on ACE-2 and 70.9/68.5 on
MUC-6 for Reconcile (Stoyanov et al, 2010)
tree3 (Quinlan, 1993) that is trained on the training
set to estimate the coreference probability p for a
pair and then applied to the test set. Note that, as
is standard in CoRe, filtering and feature calculation
are exactly the same for training and test, but that
pair generation is different as described above.
4 Experimental Setup
4.1 Data Sets
For computing word association, we used a cor-
pus of about 63,000 documents from the 2009 En-
glish Wikipedia (the articles that were larger than
200 bytes). This corpus consists of more than 33.8
million tokens; the average document length is 500
tokens. The corpus was parsed using the Berkeley
parser (Petrov and Klein, 2007). We ignored all sen-
tences that had no parse output. The number of de-
tected markables (all noun phrases extracted from
parse trees) is about 9 million.
We evaluate unsupervised, supervised and self-
trained models on ACE (Phase 2) (Mitchell et al,
2003).4 This data set is one of the most widely
used CoRe benchmarks and was used by the sys-
tems that are most comparable to our approach; in
particular, it was used in most prior work on unsu-
pervised CoRe. The corpus is composed of three
data sets from three different news sources. We give
the number of test documents for each: (i) Broadcast
News (BNEWS): 51. (ii) Newspaper (NPAPER):
17. (iii) Newswire (NWIRE): 29. We report re-
sults for true markables (markables extracted from
the answer keys) to be able to compare with other
systems that use true markables.
In addition, we use the recently published
OntoNotes benchmark (Recasens et al, 2010).
OntoNotes is an excerpt of news from the OntoNotes
Corpus Release 2.0 (Pradhan et al, 2007). The ad-
vantage of OntoNotes is that it contains two parallel
annotations: (i) a gold setting, gold standard manual
annotations of the preprocessing information and (ii)
an automatic setting, automatically predicted anno-
tations of the preprocessing information. The au-
tomatic setting reflects the situation a CoRe system
3We also tried support vector machines and maximum en-
tropy models, but they did not perform better.
4We used two variants of ACE (Phase 2): ACE-2 and
ACE2003
787
Markable Pairs Filtering Association Calculation
Pair Generation Filter Templates Association Information Train/Test
Markable Pairs Filtering Feature Calculation Feature Vectors
Pair Generation Filter Templates Feature Templates Train/Test
Figure 4: Pair estimation in the unsupervised model A-INF (top) and in the supervised model SUCRE (bottom).
faces in reality; in contrast, the gold setting should
be considered less realistic.
The issue of gold vs. automatic setting is directly
related to a second important evaluation issue: the
influence of markable detection on CoRe evaluation
measures. In a real application, we do not have ac-
cess to true markables, so an evaluation on system
markables (markables automatically detected by the
system) reflects actual expected performance better.
However, reporting only CoRe numbers (even for
system markables) is not sufficient either since ac-
curacy of markable detection is necessary to inter-
pret CoRe scores. Thus, we need (i) measures of
the quality of system markables (i.e., an evaluation
of the markable detection subtask) and CoRe per-
formance on system markables as well as (ii) a mea-
sure of CoRe performance on true markables. We
use OntoNotes in this paper to perform such a, in
our view, complete and realistic evaluation of CoRe.
The two evaluations correspond to the two evalua-
tions performed at SemEval-2010 (Recasens et al,
2010): the automatic setting with system markables
and the gold setting with true markables. Test set
size is 85 documents.
In the experiments with A-INF we use Wikipedia
to compute association information and then evalu-
ate the model on the test sets of ACE and OntoNotes.
For the experiments with UNSEL, we use its unsu-
pervised subsystem A-INF (which uses Wikipedia
association scores) to automatically label the train-
ing sets of ACE and OntoNotes. Then for each data
set, the supervised subsystem of UNSEL (i.e., SU-
CRE) is trained on its automatically labeled training
set and then evaluated on its test set. Finally, for
the supervised experiments, we use the manually la-
beled training sets and evaluate on the corresponding
test sets.
4.2 Evaluation Metrics
We report recall, precision, and F1 for MUC (Vilain
et al, 1995), B3 (Bagga and Baldwin, 1998), and
CEAF (Luo, 2005). We selected these three met-
rics because a single metric is often misleading and
because we need to use metrics that were used in
previous unsupervised work.
It is well known that MUC by itself is insuffi-
cient because it gives misleadingly high scores to the
?single-chain? system that puts all markables into
one chain (Luo et al, 2004; Finkel and Manning,
2008). However, B3 and CEAF have a different
bias: they give high scores to the ?all-singletons?
system that puts each markable in a separate chain.
On OntoNotes test, we get B3 = 83.2 and CEAF
= 71.2 for all-singletons, which incorrectly sug-
gests that performance is good; but MUC F1 is 0 in
this case, demonstrating that all-singletons performs
poorly. With the goal of performing a complete eval-
uation, one that punishes all-singletons as well as
single-chain, we use one of the following two com-
binations: (i) MUC and B3 or (ii) MUC and CEAF.
Recasens et al (2010) showed that B3 and CEAF
are highly correlated (Pearson?s r = 0.91). There-
fore, either combination (i) or combination (ii) fairly
characterizes CoRe performance.
5 Results and Discussion
Table 1 compares our unsupervised self-trained
model UNSEL and unsupervised model A-INF to
788
MUC B3 CEAF
BNEWS-ACE-2 Prec. Rec. F1 Prec. Rec. F1 Prec. Rec. F1
1 P&D 68.3 66.6 67.4 70.3 65.3 67.7 ? ? ?
2 A-INF 60.8 61.4 61.1 55.5 69.0 61.5 52.6 52.0 52.3
3 UNSEL 72.5 65.6 68.9 72.5 66.4 69.3 56.7 64.8 60.5
4 SUCRE 86.6 60.3 71.0 87.6 64.6 74.4 56.1 81.6 66.5
NWIRE-ACE-2 Prec. Rec. F1 Prec. Rec. F1 Prec. Rec. F1
5 P&D 67.7 67.3 67.4 74.7 68.8 71.6 ? ? ?
6 A-INF 62.4 57.4 59.8 59.2 62.4 60.7 46.8 52.5 49.5
7 UNSEL 76.2 61.5 68.1 81.5 67.6 73.9 61.5 77.1 68.4
8 SUCRE 82.5 65.7 73.1 85.4 72.3 78.3 63.5 80.6 71.0
NPAPER-ACE-2 Prec. Rec. F1 Prec. Rec. F1 Prec. Rec. F1
9 P&D 69.2 71.7 70.4 70.0 66.5 68.2 ? ? ?
10 A-INF 60.6 56.0 58.2 52.4 60.3 56.0 38.9 44.0 41.3
11 UNSEL 78.6 65.7 71.6 74.0 68.0 70.9 57.6 73.2 64.5
12 SUCRE 82.5 67.0 73.9 80.7 69.5 74.6 58.8 77.1 66.7
BNEWS-ACE2003 Prec. Rec. F1 Prec. Rec. F1 Prec. Rec. F1
13 H&K 68.3 56.8 62.0 ? ? ? 59.9 53.9 56.7
14 Ng 71.4 56.1 62.8 ? ? ? 60.5 53.3 56.7
15 A-INF 60.9 64.9 62.8 50.9 72.5 59.8 53.8 49.4 51.5
16 UNSEL 69.5 65.0 67.1 70.2 65.9 68.0 58.5 64.2 61.2
17 SUCRE 73.9 68.5 71.1 75.4 69.6 72.4 60.1 66.6 63.2
NWIRE-ACE2003 Prec. Rec. F1 Prec. Rec. F1 Prec. Rec. F1
18 H&K 66.2 46.8 54.8 ? ? ? 62.8 49.6 55.4
19 Ng 68.3 47.0 55.7 ? ? ? 60.7 49.2 54.4
20 A-INF 62.7 60.5 61.6 54.8 66.1 59.9 47.7 50.2 49.0
21 UNSEL 64.8 68.6 66.6 61.5 73.6 67.0 59.8 55.1 57.3
22 SUCRE 77.6 69.3 73.2 78.8 75.2 76.9 65.1 74.4 69.5
Table 1: Scores for MCORE (A-INF, SUCRE and UNSEL) and three comparable systems on ACE-2 and ACE2003.
P&D (Poon and Domingos, 2008) on ACE-2; and
to Ng (Ng, 2008) and H&K5 (Haghighi and Klein,
2007) on ACE2003. To our knowledge, these three
papers are the best and most recent evaluation results
for unsupervised learning and they all report results
on ACE-2 and ACE-2003. Results on SUCRE will
be discussed later in this section.
A-INF scores are below some of the earlier unsu-
pervised work reported in the literature (lines 2, 6,
10) although they are close to competitive on two
of the datasets (lines 15 and 20: MUC scores are
equal or better, CEAF scores are worse). Given the
simplicity of A-INF, which uses nothing but asso-
5We report numbers for the better performing Pronoun-only
Salience variant of H&K proposed by Ng (2008).
ciations mined from a large unannotated corpus, its
performance is surprisingly good.
Turning to UNSEL, we see that F1 is always bet-
ter for UNSEL than for A-INF, for all three mea-
sures (lines 3 vs 2, 7 vs 6, 11 vs 10, 16 vs 15, 21
vs 20). This demonstrates that the self-training step
of UNSEL is able to correct many of the errors that
A-INF commits. Both precision and recall are im-
proved with two exceptions: recall of B3 decreases
from line 2 to 3 and from 15 to 16.
When comparing the unsupervised system UN-
SEL to previous unsupervised results, we find that
UNSEL?s F1 is higher in all runs (lines 3 vs 1, 7 vs
5, 11 vs 9, 16 vs 13&14, 21 vs 18&19). The differ-
ences are large (up to 11%) compared to H&K and
789
Ng. The difference to P&D is smaller, ranging from
2.7% (B3, lines 11 vs 9) to 0.7% (MUC, lines 7 vs
5). Given that MCORE is a simpler and more ef-
ficient system than this prior work on unsupervised
CoRe, these results are promising.
In contrast to F1, there is no consistent trend for
precision and recall. For example, P&D is better
than UNSEL on MUC recall for BNEWS-ACE-2
(lines 1 vs 3) and H&K is better than UNSEL on
CEAF precision for NWIRE-ACE2003 (lines 18 vs
21). But this higher variability for precision and re-
call is to be expected since every system trades the
two measures off differently.
These results show that the application of self-
training significantly improves performance. As dis-
cussed in Section 3.1, self-training has positive ef-
fects on both recall and precision. We now present
two simplified examples that illustrate this point.
Example for recall. Consider the markable pair
(Novoselov6,he) in the test set. Its N score is 0 be-
cause our subset of 2009 Wikipedia sentences has
no occurrence of Novoselov. However, A-INF finds
many similar pairs like (Einstein,he) and (Hawk-
ing,he), pairs that have high N scores. Suppose
we represent pairs using the following five fea-
tures: <sentence distance, string match, type of
first markable, type of second markable, number
agreement>. Then (Einstein,he), (Hawking,he) and
(Novoselov,he) will all be assigned the feature vector
<1, No, Proper Noun, Personal Pronoun, Yes>. We
can now automatically label Wikipedia using A-INF
? this will label (Einstein,he) and (Hawking,he) as
coreferent ? and train SUCRE on the resulting train-
ing set. SUCRE can then resolve the coreference
(Novoselov,he) correctly. We call this the better re-
call effect.
Example for precision. Using the same repre-
sentation of pairs, suppose that for the sequence of
markables Biden, Obama, President the markable
pairs (Biden,President) and (Obama,President) are
assigned the feature vectors <8, No, Proper Noun,
Proper Noun, Yes> and <1, No, Proper Noun,
Proper Noun, Yes>, respectively. Since both pairs
have N scores > 0.5, A-INF incorrectly puts the
three markables into one cluster. But as we would
expect, A-INF labels many more markable pairs
6The 2010 physics Nobel laureate.
 10
 20
 30
 40
 50
 60
 70
 80
 100  20000  40000  60000
Pr
ec
., 
R
ec
. a
nd
 F
1
Number of input Wikipedia articles
MUC-Prec.
MUC-Rec.
MUC-F1
Figure 5: MUC learning curve for A-INF.
with the second feature vector (distance=1) as coref-
erent than with the first one (distance=8) in the en-
tire automatically labeled training set. If we now
train SUCRE on this training set, it can resolve such
cases in the test set correctly even though they are
so similar: (Biden,President) is classified as disref-
erent and (Obama,President) as coreferent. We call
this the better precision effect.
Recall that UNSEL has better recall and precision
than A-INF in almost all cases (discussion of Ta-
ble 1). This result shows that better precision and
better recall effects do indeed benefit UNSEL.
To summarize, the advantages of our self-training
approach are: (i) We cover cases that do not occur
in the unlabeled corpus (better recall effect); and (ii)
we use the leveraging effect of a rich feature space
including distance, person, number, gender etc. to
improve precision (better precision effect).
Learning curve. Figure 5 presents MUC scores
of A-INF as a function of the number of Wikipedia
articles used in unsupervised learning. We can see
that a small number of input articles (e.g., 100) re-
sults in low recall and high precision. When we in-
crease the number of input articles, recall rapidly in-
creases and precision rapidly decreases up to about
10,000 articles. Increase and decrease continue
more slowly after that. F1 increases throughout be-
cause lower precision is compensated by higher re-
call. This learning curve demonstrates the impor-
tance of the size of the corpus for A-INF.
Comparison of UNSEL with SUCRE
Table 2 compares our unsupervised self-trained
(UNSEL) and supervised (SUCRE) models with
the recently published SemEval-2010 OntoNotes re-
790
Gold setting + True markables
System MD MUC B3 CEAF
Relax 100 33.7 84.5 75.6
SUCRE2010 100 60.8 82.4 74.3
SUCRE 100 64.3 87.0 80.1
UNSEL 100 63.0 86.9 79.7
Automatic setting + System markables
System MD MUC B3 CEAF
SUCRE2010 80.7 52.5 67.1 62.7
Tanl-1 73.9 24.6 61.3 57.3
SUCRE 80.9 55.7 69.7 66.6
UNSEL 80.9 55.0 69.8 66.3
Table 2: F1 scores for MCORE (SUCRE and UNSEL)
and the best comparable systems in SemEval-2010. MD:
Markable Detection F1 (Recasens et al, 2010).
sults (gold and automatic settings). We compare
with the scores of the two best systems, Relax and
SUCRE20107 (for the gold setting with true mark-
ables) and SUCRE2010 and Tanl-1 (for the automatic
setting with system markables, 89.9% markable de-
tection (MD) F1). It is apparent from this table that
our supervised and unsupervised self-trained mod-
els outperform Relax, SUCRE2010 and Tanl-1. We
should make clear that we did not use the test set for
development to ensure a fair comparison with the
participant systems at SemEval-2010.
Table 1 shows that the unsupervised self-trained
system (UNSEL) does a lot worse than the su-
pervised system (SUCRE) on ACE.8 In contrast,
UNSEL performs almost as well as SUCRE on
OntoNotes (Table 2), for both gold and automatic
settings: F1 differences range from +.1 (Auto-
matic, B3) to ?1.3 (Gold, MUC). We suspect that
this is partly due to the much higher proportion
of singletons in OntoNotes than in ACE-2: 85.2%
(OntoNotes) vs. 60.2% (ACE-2). The low recall of
the automatic labeling by A-INF introduces a bias
for singletons when UNSEL is self-trained. Another
reason is that the OntoNotes training set is about
4 times larger than each of BNEWS, NWIRE and
7It is the first version of our supervised system that took part
in SemEval-2010. We call it SUCRE2010.
8A reviewer observes that SUCRE?s performance is better
than the supervised system of Ng (2008). This may indicate
that part of our improved unsupervised performance in Table 1
is due to better feature engineering implemented in SUCRE.
NPAPER training sets. With more training data,
UNSEL can correct more of its precision and re-
call errors. For an unsupervised approach, which
only needs unlabeled data, there is little cost to cre-
ating large training sets. Thus, this comparison of
ACE-2/Ontonotes results is evidence that in a realis-
tic scenario using association information in an un-
supervised self-trained system is almost as good as
a system trained on manually labeled data.
It is important to note that the comparison of
SUCRE to UNSEL is the most direct comparison
of supervised and unsupervised CoRe learning we
are aware of. The two systems are identical with the
single exception that they are trained on manual vs.
automatic coreference labels.
6 Conclusion
In this paper, we have demonstrated the utility of
association information for coreference resolution.
We first developed a simple unsupervised model for
shallow CoRe that only uses association information
for finding coreference chains. We then introduced
an unsupervised self-trained approach where a su-
pervised model is trained on a corpus that was auto-
matically labeled by the unsupervised model based
on the association information. The results of the ex-
periments indicate that the performance of the unsu-
pervised self-trained approach is better than the per-
formance of other unsupervised learning systems. In
addition, we showed that our system is a flexible and
modular framework that is able to learn from data
with different quality (perfect vs noisy markable de-
tection) and domain; and is able to deliver good re-
sults for shallow information spaces and competitive
results for rich feature spaces. Finally, our frame-
work is the first CoRe system that is designed to sup-
port three major modes of machine learning equally
well: supervised, self-trained and unsupervised.
Acknowledgments
This research was funded by DFG (grant SCHU
2246/4).
We thank Aoife Cahill, Alexander Fraser, Thomas
Mu?ller and the anonymous reviewers for their help-
ful comments.
791
References
Amit Bagga and Breck Baldwin. 1998. Algorithms for
scoring coreference chains. In LREC Workshop on
Linguistics Coreference ?98, pages 563?566.
Thomas M. Cover and Joy A. Thomas. 1991. Elements
of Information Theory. Wiley.
Micha Elsner, Eugene Charniak, and Mark Johnson.
2009. Structured generative models for unsupervised
named-entity clustering. In HLT-NAACL ?09, pages
164?172.
Jenny Rose Finkel and Christopher D. Manning. 2008.
Enforcing transitivity in coreference resolution. In
HLT ?08, pages 45?48.
Aria Haghighi and Dan Klein. 2007. Unsupervised
coreference resolution in a nonparametric bayesian
model. In ACL ?07, pages 848?855.
Aria Haghighi and Dan Klein. 2009. Simple coreference
resolution with rich syntactic and semantic features. In
EMNLP ?09, pages 1152?1161.
Aria Haghighi and Dan Klein. 2010. Coreference resolu-
tion in a modular, entity-centered model. In NAACL-
HLT ?10, pages 385?393.
Andrew Kehler, Douglas E. Appelt, Lara Taylor, and
Aleksandr Simma. 2004. Competitive Self-Trained
Pronoun Interpretation. In HLT-NAACL ?04, pages
33?36.
Manfred Klenner and ?Etienne Ailloud. 2009. Opti-
mization in coreference resolution is not needed: A
nearly-optimal algorithm with intensional constraints.
In EACL, pages 442?450.
Hamidreza Kobdani and Hinrich Schu?tze. 2010. Sucre:
A modular system for coreference resolution. In Se-
mEval ?10, pages 92?95.
Hamidreza Kobdani, Hinrich Schu?tze, Andre Burkovski,
Wiltrud Kessler, and Gunther Heidemann. 2010. Re-
lational feature engineering of natural language pro-
cessing. In CIKM ?10. ACM.
Xiaoqiang Luo, Abe Ittycheriah, Hongyan Jing, Nanda
Kambhatla, and Salim Roukos. 2004. A
Mention-Synchronous Coreference Resolution Algo-
rithm Based on the Bell Tree. In ACL ?04, pages 135?
142.
Xiaoqiang Luo. 2005. On coreference resolution perfor-
mance metrics. In HLT ?05, pages 25?32.
A. McEnery, I. Tanaka, and S. Botley. 1997. Corpus
annotation and reference resolution. In ANARESOLU-
TION ?97, pages 67?74.
Alexis Mitchell, Stephanie Strassel, Mark Przybocki,
JK Davis, George Doddington, Ralph Grishman,
Adam Meyers, Ada Brunstein, Lisa Ferro, and Beth
Sundheim. 2003. ACE-2 version 1.0. Linguistic Data
Consortium, Philadelphia.
Christoph Mu?ller, Stefan Rapp, and Michael Strube.
2002. Applying co-training to reference resolution. In
ACL ?02, pages 352?359.
Vincent Ng and Claire Cardie. 2003. Bootstrapping
coreference classifiers with multiple machine learning
algorithms. In EMNLP ?03, pages 113?120.
Vincent Ng. 2008. Unsupervised models for coreference
resolution. In EMNLP ?08, pages 640?649.
Slav Petrov and Dan Klein. 2007. Improved inference
for unlexicalized parsing. In HLT-NAACL ?07, pages
404?411.
Simone Paolo Ponzetto. 2010. Knowledge Acquisi-
tion from a Collaboratively Generated Encyclopedia,
volume 327 of Dissertations in Artificial Intelligence.
Amsterdam, The Netherlands: IOS Press & Heidel-
berg, Germany: AKA Verlag.
Hoifung Poon and Pedro Domingos. 2008. Joint unsu-
pervised coreference resolution with markov logic. In
EMNLP ?08, pages 650?659.
Sameer S. Pradhan, Eduard Hovy, Mitch Marcus, Martha
Palmer, Lance Ramshaw, and Ralph Weischedel.
2007. Ontonotes: A unified relational semantic rep-
resentation. In ICSC ?07, pages 517?526.
J. Ross Quinlan. 1993. C4.5: Programs for machine
learning. Morgan Kaufmann Publishers Inc., San
Francisco, CA, USA.
Karthik Raghunathan, Heeyoung Lee, Sudarshan Ran-
garajan, Nathanael Chambers, Mihai Surdeanu, Dan
Jurafsky, and Christopher Manning. 2010. A multi-
pass sieve for coreference resolution. In EMNLP ?10,
pages 492?501.
Marta Recasens, Llu??s Ma`rquez, Emili Sapena,
M.Anto`nia Mart??, Mariona Taule?, Ve?ronique Hoste,
Massimo Poesio, and Yannick Versley. 2010.
SemEval-2010 Task 1: Coreference resolution in
multiple languages. In SemEval ?10, pages 70?75.
Wee Meng Soon, Hwee Tou Ng, and Daniel Chung Yong
Lim. 2001. A machine learning approach to coref-
erence resolution of noun phrases. In CL ?01, pages
521?544.
Veselin Stoyanov, Claire Cardie, Nathan Gilbert, Ellen
Riloff, David Buttler, and David Hysom. 2010. Coref-
erence resolution with reconcile. In ACL ?10, pages
156?161.
Marc Vilain, John Burger, John Aberdeen, Dennis Con-
nolly, and Lynette Hirschman. 1995. A model-
theoretic coreference scoring scheme. In MUC6 ?95,
pages 45?52.
792
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 965?975,
Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational Linguistics
Piggyback: Using Search Engines for Robust Cross-Domain
Named Entity Recognition
Stefan Ru?d
Institute for NLP
University of Stuttgart
Germany
Massimiliano Ciaramita
Google Research
Zu?rich
Switzerland
Jens Mu?ller and Hinrich Schu?tze
Institute for NLP
University of Stuttgart
Germany
Abstract
We use search engine results to address a par-
ticularly difficult cross-domain language pro-
cessing task, the adaptation of named entity
recognition (NER) from news text to web
queries. The key novelty of the method is that
we submit a token with context to a search
engine and use similar contexts in the search
results as additional information for correctly
classifying the token. We achieve strong gains
in NER performance on news, in-domain and
out-of-domain, and on web queries.
1 Introduction
As statistical Natural Language Processing (NLP)
matures, NLP components are increasingly used in
real-world applications. In many cases, this means
that some form of cross-domain adaptation is neces-
sary because there are distributional differences be-
tween the labeled training set that is available and
the real-world data in the application. To address
this problem, we propose a new type of features
for NLP data, features extracted from search en-
gine results. Our motivation is that search engine
results can be viewed as a substitute for the world
knowledge that is required in NLP tasks, but that can
only be extracted from a standard training set or pre-
compiled resources to a limited extent. For example,
a named entity (NE) recognizer trained on news text
may tag the NE London in an out-of-domain web
query like London Klondike gold rush as a location.
But if we train the recognizer on features derived
from search results for the sentence to be tagged,
correct classification as person is possible. This is
because the search results for London Klondike gold
rush contain snippets in which the first name Jack
precedes London; this is a sure indicator of a last
name and hence an NE of type person.
We call our approach piggyback and search result-
derived features piggyback features because we pig-
gyback on a search engine like Google for solving a
difficult NLP task.
In this paper, we use piggyback features to ad-
dress a particularly hard cross-domain problem, the
application of an NER system trained on news to
web queries. This problem is hard for two reasons.
First, the most reliable cue for NEs in English, as
in many languages, is capitalization. But queries
are generally lowercase and even if uppercase char-
acters are used, they are not consistent enough to
be reliable features. Thus, applying NER systems
trained on news to web queries requires a robust
cross-domain approach.
News to queries adaptation is also hard because
queries provide limited context for NEs. In news
text, the first mention of a word like Ford is often
a fully qualified, unambiguous name like Ford Mo-
tor Corporation or Gerald Ford. In a short query
like buy ford or ford pardon, there is much less con-
text than in news. The lack of context and capitaliza-
tion, and the noisiness of real-world web queries (to-
kenization irregularities and misspellings) all make
NER hard. The low annotator agreement we found
for queries (Section 5) also confirms this point.
The correct identification of NEs in web queries
can be crucial for providing relevant pages and ads
to users. Other domains have characteristics sim-
ilar to web queries, e.g., automatically transcribed
speech, social communities like Twitter, and SMS.
Thus, NER for short, noisy text fragments, in the
absence of capitalization, is of general importance.
965
NER performance is to a large extent determined
by the quality of the feature representation. Lexical,
part-of-speech (PoS), shape and gazetteer features
are standard. While the impact of different types of
features is well understood for standard NER, fun-
damentally different types of features can be used
when leveraging search engine results. Returning to
the NE London in the query London Klondike gold
rush, the feature ?proportion of search engine results
in which a first name precedes the token of interest?
is likely to be useful in NER. Since using search en-
gine results for cross-domain robustness is a new ap-
proach in NLP, the design of appropriate features is
crucial to its success. A significant part of this paper
is devoted to feature design and evaluation.
This paper is organized as follows. Section 2 dis-
cusses related work. We describe standard NER fea-
tures in Section 3. One main contribution of this
paper is the large array of piggyback features that
we propose in Section 4. We describe the data sets
we use and our experimental setup in Sections 5?6.
The results in Section 7 show that piggyback fea-
tures significantly increase NER performance. This
is the second main contribution of the paper. We dis-
cuss challenges of using piggyback features ? due to
the cost of querying search engines ? and present our
conclusions and future work in Section 8.
2 Related work
Barr et al (2008) found that capitalization of NEs in
web queries is inconsistent and not a reliable cue for
NER. Guo et al (2009) exploit query logs for NER
in queries. This is also promising, but the context
in search results is richer and potentially more infor-
mative than that of other queries in logs.
The insight that search results provide useful ad-
ditional context for natural language expressions is
not new. Perhaps the oldest and best known applica-
tion is pseudo-relevance feedback which uses words
and phrases from search results for query expansion
(Rocchio, 1971; Xu and Croft, 1996). Search counts
or search results have also been used for sentiment
analysis (Turney, 2002), for transliteration (Grefen-
stette et al, 2004), candidate selection in machine
translation (Lapata and Keller, 2005), text similar-
ity measurements (Sahami and Heilman, 2006), in-
correct parse tree filtering (Yates et al, 2006), and
paraphrase evaluation (Fujita and Sato, 2008). The
specific NER application we address is most similar
to the work of Farkas et al (2007), but they mainly
used frequency statistics as opposed to what we view
as the main strength of search results: the ability to
get additional contextually similar uses of the token
that is to be classified.
Lawson et al (2010), Finin et al (2010), and
Yetisgen-Yildiz et al (2010) investigate how to best
use Amazon Mechanical Turk (AMT) for NER. We
use AMT as a tool, but it is not our focus.
NLP settings where training and test sets are from
different domains have received considerable atten-
tion in recent years. These settings are difficult be-
cause many machine learning approaches assume
that source and target are drawn from the same dis-
tribution; this is not the case if they are from differ-
ent domains. Systems applied out of domain typi-
cally incur severe losses in accuracy; e.g., Poibeau
and Kosseim (2000) showed that newswire-trained
NER systems perform poorly when applied to email
data (a drop of F1 from .9 to .5). Recent work in ma-
chine learning has made substantial progress in un-
derstanding how cross-domain features can be used
in effective ways (Ben-David et al, 2010). The de-
velopment of such features however is to a large ex-
tent an empirical problem. From this perspective,
one of the most successful approaches to adaptation
for NER is based on generating shared feature rep-
resentations between source and target domains, via
unsupervised methods (Ando, 2004; Turian et al,
2010). Turian et al (2010) show that adapting from
CoNLL to MUC-7 (Chinchor, 1998) data (thus be-
tween different newswire sources), the best unsuper-
vised feature (Brown clusters) improves F1 from .68
to .79. Our approach fits within this line of work
in that it empirically investigates features with good
cross-domain generalization properties. The main
contribution of this paper is the design and evalu-
ation of a novel family of features extracted from
the largest and most up-to-date repository of world
knowledge, the web.
Another source of world knowledge for NER is
Wikipedia: Kazama and Torisawa (2007) show that
pseudocategories extracted from Wikipedia help for
in-domain NER. Cucerzan (2007) uses Wikipedia
and web search frequencies to improve NE disam-
biguation, including simple web search frequencies
966
BASE: lexical and input-text part-of-speech features
1 WORD(k,i) binary: wk = wi
2 POS(k,t) binary: wk has part-of-speech t
3 SHAPE(k,i) binary: wk has (regular expression) shape regexpi
4 PREFIX(j) binary: w0 has prefix j (analogously for suffixes)
GAZ: gazetteer features
5 GAZ-Bl(k,i) binary: wk is the initial word of a phrase, consisting of l words, whose gaz. category is i
6 GAZ-Il(k,i) binary: wk is a non-initial word in a phrase, consisting of l words, whose gaz. category is i
URL: URL features
7 URL-SUBPART N(w0 is substring of a URL)/N(URL)
8 URL-MI(PER) 1/N(URL-parts)?[[p?URL-parts]] 3MIu(p, PER)?MIu(p,O)?MIu(p,ORG)?MIu(p,LOC)
LEX: local lexical features
9 NEIGHBOR(k) 1/N(k-neighbors)?[[v?k-neighbors]] log[NE-BNC(v, k)/OTHER-BNC(v, k)]
10 LEX-MI(PER,d) 1/N(d-words)?[[v?d-words]] 3MId(v, PER)?MId(v,O)?MId(v,ORG)?MId(v,LOC)
BOW: bag-of-word features
11 BOW-MI(PER) 1/N(bow-words)?[[v?bow-words]] 3MIb(v, PER)?MIb(v,O)?MIb(v,ORG)?MIb(v,LOC)
MISC: shape, search part-of-speech, and title features
12 UPPERCASE N(s0 is uppercase)/N(s0)
13 ALLCAPS N(s0 is all-caps)/N(s0)
14 SPECIAL binary: w0 contains special character
15 SPECIAL-TITLE N(s?1 or s1 in title contains special character)/(N(s?1)+N(s1))
16 TITLE-WORD N(s0 occurs in title)/N(title)
17 NOMINAL-POS N(s0 is tagged with nominal PoS)/N(s0)
18 CONTEXT(k) N(sk is typical neighbor at position k of named entity)/N(s0)
19 PHRASE-HIT(k) N(wk = sk, i.e., word at position k occurs in snippet)/N(s0)
20 ACRONYM N(w?1w0 or w0w1 or w?1w0w1 occur as acronym)/N(s0)
21 EMPTY binary: search result is empty
Table 1: NER features used in this paper. BASE and GAZ are standard features. URL, LEX, BOW and MISC are
piggyback (search engine-based) features. See text for explanation of notation. The definitions of URL-MI, LEX-MI,
and BOW-MI for LOC, ORG and O are analogous to those for PER. For better readability, we write
?
[[x]] for
?
x.
for compound entities.
3 Standard NER features
As is standard in supervised NER, we train an NE
tagger on a dataset where each token is represented
as a feature vector. In this and the following section
we present the features used in our study divided in
groups. We will refer to the target token ? the to-
ken we define the feature vector for ? as w0. Its left
neighbor is w?1 and its right neighbor w1. Table 1
provides a summary of all features.
Feature group BASE. The first class of fea-
tures, BASE, is standard in NER. The binary fea-
ture WORD(k,i) (line 1) is 1 iff wi, the ith word in
the dictionary, occurs at position k with respect to
w0. The dictionary consists of all words in the train-
ing set. The analogous feature for part of speech,
POS(k,t) (line 2), is 1 iff wk has been tagged with
PoS t, as determined by TnT tagger (Brants, 2000).
We also encode surface properties of the word with
simple regular expressions, e.g., x-ray is encoded as
x-x and 9/11 as d/dd (SHAPE, line 3). For these fea-
tures, k ? {?1, 0, 1}. Finally, we encode prefixes
and suffixes, up to three characters long, for w0 (line
4).
Feature group GAZ. Gazetteer features (lines 5
& 6) are an efficient and effective way of building
world knowledge into an NER model. A gazetteer
is simply a list of phrases that belong to a par-
ticular semantic category. We use gazetteers from
(i) GATE (Cunningham et al, 2002): countries,
first/last names, trigger words; (ii) WordNet: the
46 lexicographical labels (food, location, person
etc.); and (iii) Fortune 500: company names. The
two gazetteer features are the binary features GAZ-
Bl(k,i) and GAZ-Il(k,i). GAZ-Bl (resp. GAZ-Il) is 1
967
iff wk occurs as the first (resp. non-initial or internal)
word in a phrase of length l that the gazetteer lists as
belonging to category i where k ? {?1, 0, 1}.
4 Piggyback features
Feature groups URL, LEX, BOW, and MISC are
piggyback features. We produce these by segment-
ing the input text into overlapping trigrams w1w2w3,
w2w3w4, w3w4w5 etc. Each trigram wi?1wiwi+1
is submitted as a query to the search engine. For
all experiments we used the publicly accessible
Google Web Search API.1 The search engine returns
a search result for the query consisting of, in most
cases, 10 snippets,2 each of which contains 0, 1 or
more hits of the search term wi. We then compute
features for the vector representation of wi based on
the snippets. We again refer to the target token and
its neighbors (i.e., the search string) as w?1w0w1.
w0 is the token that is to be classified (PER, LOC,
ORG, or O) and the previous word and the next word
serve as context that the search engine can exploit to
provide snippets in which w0 is used in the same NE
category as in the input text. O is the tag of a token
that is neither LOC, ORG nor PER.
In the definition of the features, we refer to the
word in the snippet that matches w0 as s0, where
the match is determined based on edit distance. The
word immediately to the left (resp. right) of s0 in a
snippet is called s?1 (resp. s1).
For non-binary features, we first calculate real
values and then binarize them into 10 quantile bins.
Feature group URL. This group exploits NE
information in URLs. The feature URL-SUBPART
(line 7) is the fraction of URLs in the search re-
sult containing w0 as a substring. To avoid spurious
matches, we set the feature to 0 if length(w0) ? 2.
For URL-MI (line 8), each URL in the search re-
sult is split on special characters into parts (e.g., do-
main and subdomains). We refer to the set of all
parts in the search result as URL-parts. The value
of MIu(p,PER) is computed on the search results of
the training set as the mutual information (MI) be-
tween (i) w0 being PER and (ii) p occurring as part
of a URL in the search result. MI is defined as fol-
1Now deprecated in favor of the new Custom Search API.
2Less than 0.5% of the queries return fewer than 10 snippets.
lows:
MI(p,PER) =
?
i?{p?,p}
?
j?{ ?PER,PER}
P (i, j) log P (i, j)P (i)P (j)
For example, for the URL-part p = ?staff? (e.g.,
in bigcorp.com/staff.htm), P (staff) is the
proportion of search results that contain a URL
with the part ?staff?, P (PER) is the proportion of
search results where the search token w0 is PER
and P (staff,PER) is the proportion of search results
where w0 is PER and one of the URLs returned by
the search engine has part ?staff?.
The value of the feature URL-MI is the average
difference between the MI of PER and the other
named entities. The feature is calculated in the same
way for LOC, ORG, and O.
Our initial experiments that used binary features
for URL parts were not successful. We then de-
signed URL-MI to integrate all URL information
specific to an NE class into one measurement in
a way that gives higher weight to strong features
and lower weight to weak features. The inner
sum on line 8 is the sum of the three differences
MI(PER) ? MI(O), MI(PER) ? MI(ORG), and
MI(PER)?MI(LOC). Each of the three summands
indicates the relative advantage a URL part p gives
to PER vs O (or ORG and LOC). By averaging over
all URL parts, one then obtains an assessment of the
overall strength of evidence (in terms of MI) for the
NE class in question.
Feature group LEX. These features assess how
appropriate the words occurring in w0?s local con-
texts in the search result are for an NE class.
For NEIGHBOR (line 9), we calculate for each
word v in the British National Corpus (BNC) the
count NE-BNC(v, k), the number of times it oc-
curs at position k with respect to an NE; and
OTHER-BNC(v, k), the number of times it occurs
at position k with respect to a non-NE. We instan-
tiate the feature for k = ?1 (left neighbor) and
k = 1 (right neighbor). The value of NEIGHBOR(k)
is defined as the average log ratio of NE-BNC(v, k)
and OTHER-BNC(v, k), averaged over the set k-
neighbors, the set of words that occur at position k
with respect to s0 in the search result.
In the experiments reported in this paper, we use
a PoS-tagged version of the BNC, a balanced cor-
pus of 100M words of British English, as a model
968
of word distribution in general contexts and in NE
contexts that is not specific to either target or source
domain. In the BNC, NEs are tagged with just one
PoS-tag, but there is no differentiation into subcat-
egories. Note that the search engine could be used
again for this purpose; for practical reasons we pre-
ferred a static resource for this first study where
many design variants were explored.
The feature LEX-MI interprets words occurring
before or after s0 as indicators of named entitihood.
The parameter d indicates the ?direction? of the fea-
ture: before or after. MId(v,PER) is computed on
the search results of the training set as the MI be-
tween (i) w0 being PER and (ii) v occurring close to
s0 in the search result either to the left (d = ?1) or
to the right (d = 1) of s0. Close refers to a window
of 2 words. The value of LEX-MI(PER,d) is then
the average difference between the MI of PER and
the other NEs. The definition for LEX-MI(PER,d)
is given on line 10. The feature is calculated in the
same way for LOC, ORG, and O.
Feature group BOW. The features LEX-MI con-
sider a small window for cooccurrence information
and distinguish left and right context. For BOW fea-
tures, we use a larger window and ignore direction.
Our aim is to build a bag-of-words representation of
the contexts of w0 in the result snippets.
MIb(v,PER) is computed on the search results
of the training set as the MI between (i) w0 being
PER and (ii) v occurring anywhere in the search re-
sult. The value of BOW-MI(PER) is the average dif-
ference between the MI of PER and the other NEs
(line 11). The average is computed over all words
v ? bow-words that occur in a particular search re-
sult. The feature is calculated in the same way for
LOC, ORG, and O.
Feature group MISC. We collect the remaining
piggyback features in the group MISC.
The UPPERCASE and ALLCAPS features (lines
12&13) compute the fraction of occurrences of w0
in the search result with capitalization of only the
first letter and all letters, respectively. We exclude
titles: capitalization in titles is not a consistent clue
for NE status.
The SPECIAL feature (line 14) returns 1 iff any
character of w0 is a number or a special character.
NEs are often surrounded by special characters in
web pages, e.g., Janis Joplin - Summertime. The
SPECIAL-TITLE feature (line 15) captures this by
counting the occurrences of numbers and special
characters in s?1 and s1 in titles of the search result.
The TITLE-WORD feature (line 16) computes the
fraction of occurrences of w0 in the titles of the
search result.
The NOMINAL-POS feature (line 17) calculates
the proportion of nominal PoS tags (NN, NNS, NP,
NPS) of s0 in the search result, as determined by
a PoS tagging of the snippets using TreeTagger
(Schmid, 1994).
The basic idea behind the CONTEXT(k) feature
(line 18) is that the occurrence of words of certain
shapes and with certain parts of speech makes it ei-
ther more or less likely that w0 is an NE. For k = ?1
(the word preceding s0 in the search result), we test
for words that are adjectives, indefinites, posses-
sive pronouns or numerals (partly based on tagging,
partly based on a manually compiled list of words).
For k = 1 (the word following s0), we test for words
that contain numbers and special characters. This
feature is complementary to the feature group LEX
in that it is based on shape and PoS and does not
estimate different parameters for each word.
The feature PHRASE-HIT(?1) (line 19) calculates
the proportion of occurrences of w0 in the search re-
sult where the left neighbor in the snippet is equal
to the word preceding w0 in the search string, i.e.,
k = ?1: s?1 = w?1. PHRASE-HIT(1) is the
equivalent for the right neighbor. This feature helps
identify phrases ? search strings containing NEs are
more likely to occur as a phrase in search results.
The ACRONYM feature (line 20) computes the
proportion of the initials of w?1w0 or w0w1 or
w?1w0w1 occurring in the search result. For ex-
ample, the abbreviation GM is likely to occur when
searching for general motors dealers.
The binary feature EMPTY (line 21) returns 1 iff
the search result is empty. This feature enables the
classifier to distinguish true zero values (e.g., for the
feature ALLCAPS) from values that are zero because
the search engine found no hits.
5 Experimental data
In our experiments, we train an NER classifier on an
in-domain data set and test it on two different out-
of-domain data sets. We describe these data sets in
969
CoNLL trn CoNLL tst IEER KDD-D KDD-T
LOC 4.1 4.1 1.9 11.9 10.6
ORG 4.9 3.7 3.2 8.2 8.3
PER 5.4 6.4 3.8 5.3 5.4
O 85.6 85.8 91.1 74.6 75.7
Table 2: Percentages of NEs in CoNLL, IEER, and KDD.
this section and the NER classifier and the details of
the training regime in the next section, Section 6.
As training data for all models evaluated we used
the CoNLL 2003 English NER dataset, a corpus
of approximately 300,000 tokens of Reuters news
from 1992 annotated with person, location, organi-
zation and miscellaneous NE labels (Sang and Meul-
der, 2003). As out-of-domain newswire evaluation
data3 we use the development test data from the
NIST 1999 IEER named entity corpus, a dataset of
50,000 tokens of New York Times (NYT) and Asso-
ciated Press Weekly news.4 This corpus is annotated
with person, location, organization, cardinal, dura-
tion, measure, and date labels. CoNLL and IEER
are professionally edited and, in particular, properly
capitalized news corpora. As capitalization is ab-
sent from queries we lowercased both CoNLL and
IEER. We also reannotated the lowercased datasets
with PoS categories using the retrained TnT PoS tag-
ger (Brants, 2000) to avoid using non-plausible PoS
information. Notice that this step is necessary as
otherwise virtually no NNP/NNPS categories would
be predicted on the query data because the lower-
case NEs of web queries never occur in properly
capitalized news; this causes an NER tagger trained
on standard PoS to underpredict NEs (1?3% positive
rate).
The 2005 KDD Cup is a query topic categoriza-
tion task based on 800,000 queries (Li et al, 2005).5
We use a random subset of 2000 queries as a source
of web queries. By means of simple regular ex-
pressions we excluded from sampling queries that
looked like urls or emails (? 15%) as they are easy
to identify and do not provide a significant chal-
3A reviewer points out that we use the terms in-domain
and out-of-domain somewhat liberally. We simply use ?differ-
ent domain? as a short-hand for ?different distribution? without
making any claim about the exact nature of the difference.
4nltk.googlecode.com/svn/trunk/nltk data
5www.sigkdd.org/kdd2005/kddcup.html
lenge. We also excluded queries shorter than 10
characters (4%) and longer than 50 characters (2%)
to provide annotators with enough context, but not
an overly complex task. The annotation procedure
was carried out using Amazon Mechanical Turk. We
instructed workers to follow the CoNLL 2003 NER
guidelines (augmented with several examples from
queries that we annotated) and identify up to three
NEs in a short text and copy and paste them into a
box with associated multiple choice menu with the
4 CoNLL NE labels: LOC, MISC, ORG, and PER.
Five workers annotated each query. In a first round
we produced 1000 queries later used for develop-
ment. We call this set KDD-D. We then expanded
the guidelines with a few uncertain cases. In a sec-
ond round, we generated another 1000 queries. This
set will be referred to as KDD-T. Because annota-
tor agreement is low on a per-token basis (? = .30
for KDD-D, ? = .34 for KDD-T (Cohen, 1960)),
we remove queries with less than 50% agreement,
averaged over the tokens in the query. After this
filtering, KDD-D and KDD-T contain 777 and 819
queries, respectively. Most of the rater disagreement
involves the MISC NE class. This is not surprising
as MISC is a sort of place-holder category that is
difficult to define and identify in queries, especially
by untrained AMT workers. We thus replaced MISC
with the null label O. With these two changes, ? was
.54 on KDD-D and .64 on KDD-T. This is sufficient
for repeatable experiments.6
Table 2 shows the distribution of NE types in the
5 datasets. IEER has fewer NEs than CoNLL, KDD
has more. PER is about as prevalent in KDD as
in CoNLL, but LOC and ORG have higher percent-
ages, reflecting the fact that people search frequently
for locations and commercial organizations. These
differences between source domain (CoNLL) and
target domains (IEER, KDD) add to the difficulty
of cross-domain generalization in this case.
6 Experimental setup
Recall that the input features for a token w0 con-
sist of standard NER features (BASE and GAZ) and
features derived from the search result we obtain by
6The two KDD sets, along with additional statistics on an-
notator agreement requested by a reviewer, are available at
ifnlp.org/?schuetze/piggyback11.
970
running a search for w?1w0w1 (URL, LEX, BOW,
and MISC). Since the MISC NE class is not anno-
tated in IEER and has low agreement on KDD in
the experimental evaluation we focus on the four-
class (PER, LOC, ORG, O) NER problem on all
datasets. We use BIO encoding as in the original
CoNLL task (Sang and Meulder, 2003).
ALL LOC ORG PER
CoNLL
c1 l BASE GAZ 88.8? 91.9 77.9 93.0
c2 l GAZ URL BOW MISC 86.4? 90.7 74.0 90.9
c3 l BASE URL BOW MISC 92.3? 93.7 84.8 96.0
c4 l BASE GAZ BOW MISC 91.1? 93.3 82.2 94.9
c5 l BASE GAZ URL MISC 92.7? 94.9 84.5 95.9
c6 l BASE GAZ URL BOW 92.3? 94.2 84.4 95.8
c7 l BASE GAZ URL BOW MISC 93.0 94.9 85.1 96.4
c8 l BASE GAZ URL LEX BOW MISC 92.9 94.7 84.9 96.5
c9 c BASE GAZ 92.9 95.3 87.7 94.6
IEER
i1 l BASE GAZ 57.9? 71.0 37.7 59.9
i2 l GAZ URL LEX BOW MISC 63.8? 76.2 26.0 75.9
i3 l BASE URL LEX BOW MISC 64.9? 71.8 38.3 73.8
i4 l BASE GAZ LEX BOW MISC 67.3 76.7 41.2 74.6
i5 l BASE GAZ URL BOW MISC 67.8 76.7 40.4 75.8
i6 l BASE GAZ URL LEX MISC 68.1 77.2 36.9 77.8
i7 l BASE GAZ URL LEX BOW 66.6? 77.4 38.3 73.9
i8 l BASE GAZ URL LEX BOW MISC 68.1 77.4 36.2 78.0
i9 c BASE GAZ 68.6? 77.3 52.3 73.1
KDD-T
k1 l BASE GAZ 34.6? 48.9 19.2 34.7
k2 l GAZ URL LEX MISC 40.4? 52.1 15.4 50.4
k3 l BASE URL LEX MISC 40.9? 50.0 20.1 48.0
k4 l BASE GAZ LEX MISC 41.6? 55.0 25.2 45.2
k5 l BASE GAZ URL MISC 43.0 57.0 15.8 50.9
k6 l BASE GAZ URL LEX 40.7? 55.5 15.8 42.9
k7 l BASE GAZ URL LEX MISC 43.8 56.4 17.0 52.0
k8 l BASE GAZ URL LEX BOW MISC 43.8 56.5 17.4 52.3
Table 3: Evaluation results. l = text lowercased, c = orig-
inal capitalization preserved. ALL scores significantly
different from the best results for the three datasets (lines
c7, i8, k7) are marked ? (see text).
We use SuperSenseTagger (Ciaramita and Altun,
2006)7 as our NER tagger. It is a first-order con-
ditional HMM trained with the perceptron algo-
7sourceforge.net/projects/supersensetag
rithm (Collins, 2002), a discriminative model with
excellent efficiency-performance trade-off (Sha and
Pereira, 2003). The model is regularized by aver-
aging (Freund and Schapire, 1999). For all models
we used an appropriate development set for choos-
ing the only hyperparameter, T , the number of train-
ing iterations on the source data. T must be tuned
separately for each evaluation because different tar-
get domains have different overfitting patterns.
We train our NER system on an 80% sample of
the CoNLL data. For our in-domain evaluation, we
tune T on a 10% development sample of the CoNLL
data and test on the remaining 10%. For our out-of-
domain evaluation, we use the IEER and KDD test
sets. Here T is tuned on the corresponding develop-
ment sets. Since we do not train on IEER and KDD,
these two data sets do not have training set portions.
For each data set, we perform 63 runs, correspond-
ing to the 26?1 = 63 different non-empty combina-
tions of the 6 feature groups. We report average F1,
generated by five-trial training and evaluation, with
random permutations of the training data. We com-
pute the scores using the original CoNLL phrase-
based metric (Sang and Meulder, 2003). As a bench-
mark we use the baseline model with gazetteer fea-
tures (BASE and GAZ). The robustness of this sim-
ple approach is well documented; e.g., Turian et al
(2010) show that the baseline model (gazetteer fea-
tures without unsupervised features) produces an F1
of .778 against .788 of the best unsupervised word
representation feature.
7 Results and discussion
Table 3 summarizes the experimental results. In
each column, the best numbers within a dataset for
the ?lowercased? runs are bolded (see below for dis-
cussion of the ?capitalization? runs on lines c9 and
i9). For all experiments, we selected a subset of the
combinations of the feature groups. This subset al
ways includes the best results and a number of other
combinations where feature groups are added to or
removed from the optimal combination.
Results for the CoNLL test set show that the 5
feature groups without LEX achieve optimal per-
formance (line c7). Adding LEX improves perfor-
mance on PER, but decreases overall performance
(line c8). Removing GAZ, URL, BOW and MISC
971
from line c7, causes small comparable decreases in
performance (lines c3?c6). These feature groups
seem to have about the same importance in this ex-
perimental setting, but leaving out BASE decreases
F1 by a larger 6.6% (lines c7 vs c2).
The main result for CoNLL is that using piggy-
back features (line c7) improves F1 of a standard
NER system that uses only BASE and GAZ (line
c1) by 4.2%. Even though the emphasis of this pa-
per is on cross-domain robustness, we can see that
our approach also has clear in-domain benefits.
The baseline in line c1 is the ?lowercase? base-
line as indicated by ?l?. We also ran a ?capitalized?
baseline (?c?) on text with the original capitalization
preserved and PoS-tagged in this unchanged form.
Comparing lines c7 and c9, we see that piggyback
features are able to recover all the performance that
is lost when proper capitalization is unavailable. Lin
and Wu (2009) report an F1 score of 90.90 on the
original split of the CoNLL data. Our F1 scores
> 92% can be explained by a combination of ran-
domly partitioning the data and the fact that the four-
class problem is easier than the five-class problem
LOC-ORG-PER-MISC-O.
We use the t-test to compute significance on the
two sets of five F1 scores from the two experiments
that are being compared (two-tailed, p < .01 for t >
3.36).8 CoNLL scores that are significantly different
from line c7 are marked with ?.
For IEER, the system performs best for all six
feature groups (line i8). Runs significantly different
from i8 are marked ?. When URL, LEX and BOW
are removed from the set, performance does not de-
crease, or only slightly (lines i4, i5, i6), indicating
that these three feature groups are least important.
In contrast, there is significant evidence for the im-
portance of BASE, GAZ, and MISC: removing them
decreases performance by at least 1% (lines i2, i3,
i7). The large increase of ORG F1 when URL is
not used is surprising (41.2% on line i4, best per-
formance). The reason seems to be that URL fea-
tures (and LEX to a lesser extent) do not generalize
for ORG. Locations like Madrid in CoNLL are fre-
quently tagged ORG when they refer to sports clubs
like Real Madrid. This is rare in IEER and KDD.
8We make the assumption that the distribution of F1 scores
is approximately normal. See Cohen (1995), Noreen (1989) for
a discussion of how this affects the validity of the t-test.
Compared to standard NER (using feature groups
BASE and GAZ), our combined feature set achieves
a performance that is by more than 10% higher (lines
i8 vs i1). This demonstrates that piggyback features
have robust cross-domain generalization properties.
The comparison of lines i8 and i9 confirms that the
features effectively compensate for the lack of cap-
italization, and perform almost as well as (although
statistically worse than) a model trained on capital-
ized data.
The best run on KDD-D was the run with feature
groups BASE, GAZ, URL, LEX and MISC. On line
k7, we show results for this run for KDD-T and for
runs that differ by one feature group (lines k2?k6,
k8).9 The overall best result (43.8%) is achieved
when using all feature groups (line k8). Omitting
BOW results in the same score for ALL (line k7).
Apparently, the local LEX features already capture
most useful cooccurrence information and looking
at a wider window (as implemented by BOW) is of
limited utility. On lines k2?k6, performance gen-
erally decreases on ALL and the three NE classes
when dropping one of the five feature groups on line
k7. One notable exception is an increase for ORG
when feature group URL is dropped (line k4, 25.2%,
the best performance for ORG of all runs). This is in
line with our discussion of the same effect on IEER.
The key take-away from our results on KDD-T is
that piggyback features are again (as for IEER) sig-
nificantly better than standard feature groups BASE
and GAZ. Search engine based adaptation has an ad-
vantage of 9.2% compared to standard NER (lines
k7 vs k1). An F1 below 45% may not yet be good
enough for practical purposes. But even if additional
work is necessary to boost the scores further, our
model is an important step in this direction.
The low scores for KDD-T are also partially due
to our processing of the AMT data. Our selection
procedure is biased towards short entities whereas
CoNLL guidelines favor long NEs. We can address
this by forcing AMT raters to be more consistent
with the CoNLL guidelines in the future.
We summarize the experimental results as fol-
lows. Piggyback features consistently improve NER
for non-well-edited text when used together with
standard NER features. While relative improve-
9KDD-D F1 values were about 1% higher than for KDD-T.
972
ment due to piggyback features increases as out-
of-domain data become more different from the in-
domain training set, performance declines in abso-
lute terms from .930 (CoNLL) to .681 (IEER) and
.438 (KDD-T).
8 Conclusion
Robust cross-domain generalization is key in many
NLP applications. In addition to surface and linguis-
tic differences, differences in world knowledge pose
a key challenge, e.g., the fact that Java refers to a
location in one domain and to coffee in another. We
have proposed a new way of addressing this chal-
lenge. Because search engines attempt to make op-
timal use of the context a word occurs in, hits shown
to the user usually include other uses of the word in
semantically similar snippets. These snippets can be
used as a more robust and domain-independent rep-
resentation of the context of the word/phrase than
what is available in the input text.
Our first contribution is that we have shown that
this basic idea of using search engines for robust
domain-independent feature representations yields
solid results for one specific NLP problem, NER.
Piggyback features achieved an improvement of F1
of about 10% compared to a baseline that uses BASE
and GAZ features. Even in-domain, we were able
to get a smaller, but still noticeable improvement of
4.2% due to piggyback features. These results are
also important because there are many application
domains with noisy text without reliable capitaliza-
tion, e.g., automatically transcribed speech, tweets,
SMS, social communities and blogs.
Our second contribution is that we address a type
of NER that is of particular importance: NER for
web queries. The query is the main source of in-
formation about the user?s information need. Query
analysis is important on the web because under-
standing the query, including the subtask of NER, is
key for identifying the most relevant documents and
the most relevant ads. NER for domains like Twitter
and SMS has properties similar to web queries.
A third contribution of this paper is the release of
an annotated dataset for web query NER. We hope
that this dataset will foster more research on cross-
domain generalization and domain adaptation ? in
particular for NER ? and the difficult problem of
web query understanding.
This paper is about cross-domain generalization.
However, the general idea of using search to provide
rich context information to NLP systems is applica-
ble to a broad array of tasks. One of the main hurdles
that NLP faces is that the single context a token oc-
curs in is often not sufficient for reliable decisions,
be they about attachment, disambiguation or higher-
order semantic interpretation. Search makes dozens
of additional relevant contexts available and can thus
overcome this bottleneck. In the future, we hope to
be able to show that other NLP tasks can also benefit
from such an enriched context representation.
Future work. We used a web search engine in the
experiments presented in this paper. Latencies when
using one of the three main commercial search en-
gines Bing, Google and Yahoo! in our scenario range
from 0.2 to 0.5 seconds per token. These execution
times are prohibitive for many applications. Search
engines also tend to limit the number of queries per
user and IP address. To gain widespread acceptance
of the piggyback idea of using search results for ro-
bust NLP, we therefore must explore alternatives to
search engines.
In future work, we plan to develop more efficient
methods of using search results for cross-domain
generalization to avoid the cost of issuing a large
number of queries to search engines. Caching will
be of obvious importance in this regard. Another av-
enue we are pursuing is to build a specialized search
system for our application in a way similar to Ca-
farella and Etzioni (2005). While we need good
coverage of a large variety of domains for our ap-
proach to work, it is not clear how big the index
of the search engine must be for good performance.
Conceivably, collections much smaller than those in-
dexed by major search engines (e.g., the Google 1T
5-gram corpus or ClueWeb09) might give rise to fea-
tures with similar robustness properties. It is impor-
tant to keep in mind, however, that one of the key
factors a search engine allows us to leverage is the
notion of relevance which might not be always pos-
sible to model as accurately with other data.
Acknowledgments. This research was funded by
a Google Research Award. We would like to thank
Amir Najmi, John Blitzer, Richa?rd Farkas, Florian
Laws, Slav Petrov and the anonymous reviewers for
their comments.
973
References
Rie Kubota Ando. 2004. Exploiting unannotated cor-
pora for tagging and chunking. In ACL, Companion
Volume, pages 142?145.
Cory Barr, Rosie Jones, and Moira Regelson. 2008. The
linguistic structure of English web-search queries. In
EMNLP, pages 1021?1030.
Shai Ben-David, John Blitzer, Koby Crammer, Alex
Kulesza, Fernando Pereira, and Jennifer Wortman
Vaughan. 2010. A theory of learning from different
domains. Machine Learning, 79:151?175.
Thorsten Brants. 2000. TnT ? A statistical part-of-
speech tagger. In ANLP, pages 224?231.
Michael J. Cafarella and Oren Etzioni. 2005. A search
engine for natural language applications. In WWW,
pages 442?452.
Nancy A. Chinchor, editor. 1998. Proceedings of the
Seventh Message Understanding Conference. NIST.
Massimiliano Ciaramita and Yasemin Altun. 2006.
Broad-coverage sense disambiguation and information
extraction with a supersense sequence tagger. In Pro-
ceedings of the 2006 Conference on Empirical Meth-
ods in Natural Language Processing, pages 594?602.
Jacob Cohen. 1960. A Coefficient of Agreement for
Nominal Scales. Educational and Psychological Mea-
surement, 20(1):37?46.
Paul R. Cohen. 1995. Empirical methods for artificial
intelligence. MIT Press, Cambridge, MA, USA.
Michael Collins. 2002. Discriminative training methods
for hidden Markov models: Theory and experiments
with perceptron algorithms. In EMNLP, pages 1?8.
Silviu Cucerzan. 2007. Large-scale named entity dis-
ambiguation based on Wikipedia data. In EMNLP-
CoNLL, pages 708?716.
Hamish Cunningham, Diana Maynard, Kalina
Bontcheva, and Valentin Tablan. 2002. GATE:
A framework and graphical development environment
for robust NLP tools and applications. In ACL, pages
168?175.
Richa?rd Farkas, Gyo?rgy Szarvas, and Ro?bert Orma?ndi.
2007. Improving a state-of-the-art named entity recog-
nition system using the world wide web. In Industrial
Conference on Data Mining, pages 163?172.
Tim Finin, Will Murnane, Anand Karandikar, Nicholas
Keller, Justin Martineau, and Mark Dredze. 2010.
Annotating named entities in twitter data with crowd-
sourcing. In NAACL HLT 2010 Workshop on Creating
Speech and Language Data with Amazon?s Mechani-
cal Turk, pages 80?88.
Yoav Freund and Robert E. Schapire. 1999. Large mar-
gin classification using the perceptron algorithm. Ma-
chine Learning, 37:277?296.
Atsushi Fujita and Satoshi Sato. 2008. A probabilis-
tic model for measuring grammaticality and similar-
ity of automatically generated paraphrases of predicate
phrases. In COLING, pages 225?232.
Gregory Grefenstette, Yan Qu, and David A. Evans.
2004. Mining the web to create a language model
for mapping between English names and phrases and
Japanese. In Web Intelligence, pages 110?116.
Jiafeng Guo, Gu Xu, Xueqi Cheng, and Hang Li. 2009.
Named entity recognition in query. In SIGIR, pages
267?274.
Jun?ichi Kazama and Kentaro Torisawa. 2007. Exploit-
ing Wikipedia as external knowledge for named entity
recognition. In EMNLP-CoNLL, pages 698?707.
Mirella Lapata and Frank Keller. 2005. Web-based mod-
els for natural language processing. ACM Transac-
tions on Speech and Language Processing, 2(1):1?31.
Nolan Lawson, Kevin Eustice, Mike Perkowitz, and
Meliha Yetisgen-Yildiz. 2010. Annotating large email
datasets for named entity recognition with mechani-
cal turk. In NAACL HLT 2010 Workshop on Creating
Speech and Language Data with Amazon?s Mechani-
cal Turk, pages 71?79.
Ying Li, Zijian Zheng, and Honghua (Kathy) Dai. 2005.
KDD CUP 2005 report: Facing a great challenge.
SIGKDD Explorations Newsletter, 7:91?99.
Dekang Lin and Xiaoyun Wu. 2009. Phrase clustering
for discriminative learning. In Proceedings of the Joint
Conference of the 47th Annual Meeting of the ACL and
the 4th International Joint Conference on Natural Lan-
guage Processing of the AFNLP, pages 1030?1038.
Eric W. Noreen. 1989. Computer-Intensive Methods
for Testing Hypotheses : An Introduction. Wiley-
Interscience.
Thierry Poibeau and Leila Kosseim. 2000. Proper name
extraction from non-journalistic texts. In CLIN, pages
144?157.
J. J. Rocchio. 1971. Relevance feedback in informa-
tion retrieval. In Gerard Salton, editor, The Smart Re-
trieval System ? Experiments in Automatic Document
Processing, pages 313?323. Prentice-Hall.
Mehran Sahami and Timothy D. Heilman. 2006. A web-
based kernel function for measuring the similarity of
short text snippets. In WWW, pages 377?386.
Erik F. Tjong Kim Sang and Fien De Meulder. 2003. In-
troduction to the CoNLL-2003 shared task: Language-
independent named entity recognition. In Proceedings
of CoNLL 2003 Shared Task, pages 142?147.
Helmut Schmid. 1994. Probabilistic part-of-speech tag-
ging using decision trees. In Proceedings of the In-
ternational Conference on New Methods in Language
Processing, pages 44?49.
974
Fei Sha and Fernando Pereira. 2003. Shallow parsing
with conditional random fields. In Proceedings of the
2003 Conference of the North American Chapter of the
Association for Computational Linguistics on Human
Language Technology - Volume 1, pages 134?141.
Joseph Turian, Lev-Arie Ratinov, and Yoshua Bengio.
2010. Word representations: A simple and general
method for semi-supervised learning. In ACL, pages
384?394.
Peter D. Turney. 2002. Thumbs up or thumbs down?
semantic orientation applied to unsupervised classifi-
cation of reviews. In ACL, pages 417?424.
Jinxi Xu and W. Bruce Croft. 1996. Query expansion
using local and global document analysis. In SIGIR,
pages 4?11.
Alexander Yates, Stefan Schoenmackers, and Oren Et-
zioni. 2006. Detecting parser errors using web-based
semantic filters. In EMNLP, pages 27?34.
Meliha Yetisgen-Yildiz, Imre Solti, Fei Xia, and
Scott Russell Halgrim. 2010. Preliminary experience
with Amazon?s Mechanical Turk for annotating medi-
cal named entities. In NAACL HLT 2010 Workshop on
Creating Speech and Language Data with Amazon?s
Mechanical Turk, pages 180?183.
975
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 1516?1525,
Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational Linguistics
Integrating history-length interpolation and classes in language modeling
Hinrich Schu?tze
Institute for NLP
University of Stuttgart
Germany
Abstract
Building on earlier work that integrates dif-
ferent factors in language modeling, we view
(i) backing off to a shorter history and (ii)
class-based generalization as two complemen-
tary mechanisms of using a larger equivalence
class for prediction when the default equiv-
alence class is too small for reliable estima-
tion. This view entails that the classes in a
language model should be learned from rare
events only and should be preferably applied
to rare events. We construct such a model
and show that both training on rare events and
preferable application to rare events improve
perplexity when compared to a simple direct
interpolation of class-based with standard lan-
guage models.
1 Introduction
Language models, probability distributions over
strings of words, are fundamental to many ap-
plications in natural language processing. The
main challenge in language modeling is to estimate
string probabilities accurately given that even very
large training corpora cannot overcome the inherent
sparseness of word sequence data. One way to im-
prove the accuracy of estimation is class-based gen-
eralization. The idea is that even though a particular
word sequence s may not have occurred in the train-
ing set (or too infrequently for accurate estimation),
the occurrence of sequences similar to s can help us
better estimate p(s).
Plausible though this line of reasoning is, the lan-
guage models most commonly used today do not
incorporate class-based generalization. This is par-
tially due to the additional cost of creating classes
and using classes as part of the model. But an
equally important reason is that most models that
integrate class-based information do so by way of a
simple interpolation and achieve only a modest im-
provement in performance.
In this paper, we propose a new type of class-
based language model. The key novelty is that we
recognize that certain probability estimates are hard
to improve based on classes. In particular, the best
probability estimate for frequent events is often the
maximum likelihood estimator and this estimator is
hard to improve by using other information sources
like classes or word similarity. We therefore design a
model that attempts to focus the effect of class-based
generalization on rare events.
Specifically, we propose to employ the same
strategy for this that history-length interpo-
lated (HI) models use. We define HI models
as models that interpolate the predictions of
different-length histories, e.g., p(w3|w1w2) =
?1(w1w2)p?(w3|w1w2) + ?2(w1w2)p?(w3|w2) +
(1 ? ?1(w1w2) ? ?2(w1w2))p?(w3) where p? is a
simple estimate; in this section, we use p? = pML,
the maximum likelihood estimate, as an example.
Jelinek-Mercer (Jelinek and Mercer, 1980) and
modified Kneser-Ney (Kneser and Ney, 1995)
models are examples of HI models.
HI models address the challenge that frequent
events are best estimated by a method close to max-
imum likelihood by selecting appropriate values for
the interpolation weights. For example, if w1w2w3
is frequent, then ?1 will be close to 1, thus ensur-
ing that p(w3|w1w2) ? pML(w3|w1w2) and that the
components pML(w3|w2) and pML(w3), which are
unhelpful in this case, will only slightly change the
reliable estimate pML(w3|w1w2).
1516
The main contribution of this paper is to propose
the same mechanism for class language models. In
fact, we will use the interpolation weights of a KN
model to determine how much weight to give to each
component of the interpolation. The difference to a
KN model is merely that the lower-order distribution
is not the lower-order KN distribution (as in KN),
but instead an interpolation of the lower-order KN
distribution and a class-based distribution. We will
show that this method of integrating history interpo-
lation and classes significantly increases the perfor-
mance of a language model.
Focusing the effect of classes on rare events has
another important consequence: if this is the right
way of using classes, then they should not be formed
based on all events in the training set, but only based
on rare events. We show that doing this increases
performance.
Finally, we introduce a second discounting
method into the model that differs from KN. This
can be motivated by the fact that with two sources
of generalization (history-length and classes) more
probability mass should be allocated to these two
sources than to the single source used in KN. We
propose a polynomial discount and show a signifi-
cant improvement compared to using KN discount-
ing only.
This paper is structured as follows. Section 2
discusses related work. Section 3 reviews the KN
model and introduces two models, the Dupont-
Rosenfeld model (a ?recursive? model) and a top-
level interpolated model, that integrate the KN
model (a history interpolation model) with a class
model. Section 4 details our experimental setup.
Results are presented in Section 5. Based on an
analysis of strengths and weaknesses of Dupont-
Rosenfeld and top-level interpolated models, we
present a new polynomial discounting mechanism
that does better than either in Section 6. Section 7
presents our conclusions.
2 Related work
A large number of different class-based models have
been proposed in the literature. The well-known
model by Brown et al (1992) is a class sequence
model, in which p(u|w) is computed as the prod-
uct of a class transition probability and an emission
probability, p(g(u)|g(w))p(u|g(u)), where g(u) is
the class of u. Other approaches condition the prob-
ability of a class on n-grams of lexical items (as op-
posed to classes) (Whittaker and Woodland, 2001;
Emami and Jelinek, 2005; Uszkoreit and Brants,
2008). In this work, we use the Brown type of
model: it is simpler and has fewer parameters. Mod-
els that condition classes on lexical n-grams could be
extended in a way similar to what we propose here.
Classes have been used with good results in a
number of applications, e.g., in speech recognition
(Yokoyama et al, 2003), sentiment analysis (Wie-
gand and Klakow, 2008), and question answering
(Momtazi and Klakow, 2009). Classes have also
been shown to improve the performance of exponen-
tial models (Chen, 2009).
Our use of classes of lexical n-grams for n > 1
has several precedents in the literature (Suhm and
Waibel, 1994; Kuo and Reichl, 1999; Deligne and
Sagisaka, 2000; Justo and Torres, 2009). The nov-
elty of our approach is that we integrate phrase-level
classes into a KN model.
Hierarchical clustering (McMahon and Smith,
1996; Zitouni and Zhou, 2007; Zitouni and Zhou,
2008) has the advantage that the size of the class to
be used in a specific context is not fixed, but can be
chosen at an optimal level of the hierarchy. There is
no reason why our non-hierarchical flat model could
not be replaced with a hierarchical model and we
would expect this to improve results.
The key novelty of our clustering method is that
clusters are formed based on rare events in the train-
ing corpus. This type of clustering has been applied
to other problems before, in particular to unsuper-
vised part-of-speech tagging (Schu?tze, 1995; Clark,
2003; Reichart et al, 2010). However, the impor-
tance of rare events for clustering in language mod-
eling has not been investigated before.
Our work is most similar to the lattice-based lan-
guage models proposed by Dupont and Rosenfeld
(1997). Bilmes and Kirchhoff (2003) generalize
lattice-based language models further by allowing
arbitrary factors in addition to words and classes.
We use a special case of lattice-based language mod-
els in this paper. Our contributions are that we intro-
duce the novel idea of rare-event clustering into lan-
guage modeling and that we show that the modified
model performs better than a strong word-trigram
1517
symbol denotation
?[[w]] ?w (sum over all unigrams w)
c(wij) count of wij
n1+(?wij) # of distinct w occurring before wij
Table 1: Notation used for Kneser-Ney.
baseline.
3 Models
In this section, we introduce the three models that
we compare in our experiments: Kneser-Ney model,
Dupont-Rosenfeld model, and top-level interpola-
tion model.
3.1 Kneser-Ney model
Our baseline model is the modified Kneser-Ney
(KN) trigram model as proposed by Chen and Good-
man (1999). We give a comprehensive description
of our implementation of KN because the details
are important for the integration of the class model
given below. We use the notation in Table 1.
We estimate pKN on the training set as follows.
pKN(w3|w21) =
c(w31) ? d???(c(w31))
?[[w]] c(w21w)
+?3(w21)pKN(w3|w2)
?3(w21) =
?[[w]] d???(c(w21w))
?[[w]] c(w21w)
pKN(w3|w2) =
n1+(?w32) ? d??(n1+(?w32))
?[[w]] n1+(?w2w)
+?2(w2)pKN(w3)
?2(w2) =
?[[w]] d??(n1+(?w2w))
?[[w]] n1+(?w2w)
pKN(w3) =
{ n1+(?w3)?d?(n1+(?w3))
?
[[w]] n1+(?w) if c(w3) > 0
?1 if c(w3) = 0
?1 =
?[[w]] d?(n1+(?w))
?[[w]] n1+(?w)
The parameters d?, d??, and d??? are the discounts
for unigrams, bigrams and trigrams, respectively, as
defined by Chen and Goodman (1996, p. 20, (26)).
Note that our notation deviates from C&G in that
they use the single symbol D1 for the three different
values d?(1), d??(1), and d???(1) etc.
3.2 Dupont-Rosenfeld model
History-interpolated models attempt to find a good
tradeoff between using a maximally informative his-
tory for accurate prediction of frequent events and
generalization for rare events by using lower-order
distributions; they employ this mechanism recur-
sively by progressively shortening the history.
The key idea of the improved model we will adopt
is that class generalization ought to play the same
role in history-interpolated models as the lower-
order distributions: they should improve estimates
for unseen and rare events. Following Dupont and
Rosenfeld (1997), we implement this idea by lin-
early interpolating the class-based distribution with
the lower order distribution, recursively at each
level. For a trigram model, this means that we in-
terpolate pKN(w3|w2) and pB(w3|w1w2) on the first
backoff level and pKN(w3) and pB(w3|w2) on the
second backoff level, where pB is the (Brown) class
model (see Section 4 for details on pB). We call this
model pDR for Dupont-Rosenfeld model and define
it as follows:
pDR(w3|w21) =
c(w31) ? d???(c(w31))
?[[w]] c(w21w)
+ ?3(w21)[?1(w21)pB(w3|w21)
+(1 ? ?1(w21))pDR(w3|w2)]
pDR(w3|w2) =
n1+(?w32) ? d??(n1+(?w32))
?[[w]] n1+(?w2w)
+ ?2(w2)[?2(w2)pB(w3|w2)
+(1 ? ?2(w2))pDR(w3)]
where ?i(v) is equal to a parameter ?i if the history
(w21 or w2) is part of a cluster and 0 otherwise:
?i(v) =
{
?i if v ? B2?(i?1)
0 otherwise
B1 (resp. B2) is the set of unigram (resp. bigram) his-
tories that is covered by the clusters. We cluster bi-
gram histories and unigram histories separately and
write pB(w3|w1w2) for the bigram cluster model and
pB(w3|w2) for the unigram cluster model. Cluster-
ing and the estimation of these two distributions are
described in Section 4.
1518
The unigram distribution of the Dupont-
Rosenfeld model is set to the unigram distribution
of the KN model: pDR(w) = pKN(w).
The model (or family of models) defined by
Dupont and Rosenfeld (1997) is more general than
our version pDR. Most importantly, it allows a truly
parallel backoff whereas in our model the recursive
backoff distribution pDR is interpolated with a class
distribution pB that is not backed off. We prefer this
version because it makes it easier to understand the
contribution that unique-event vs. all-event classes
make to improved language modeling; the parame-
ters ? are a good indicator of this effect.
An alternative way of setting up the Dupont-
Rosenfeld model would be to interpolate
pKN(w3|w1w2) and pB(w3|w1w2) etc ? but this is
undesirable. The strength of history interpolation is
that estimates for frequent events are close to ML,
e.g., pKN(share|cents a) ? pML(share|cents a) for
our corpus. An ML estimate is accurate for large
counts and we should not interpolate it directly
with pB(w3|w1w2). For pDR, the discount d??? that
is subtracted from c(w1w2w3) is small relative to
c(w1w2w3) and therefore pDR ? pML in this case
(exactly as in pKN).
3.3 Top-level interpolation
Class-based models are often combined with other
models by interpolation, starting with the work by
Brown et al (1992). Since we cluster both unigrams
and bigrams, we interpolate three models:
pTOP(w3|w1w2)
= ?1(w1w2)pB(w3|w1w2) + ?2(w2)pB(w3|w2)
+ (1 ? ?1(w1w2) ? ?2(w2))pKN(w3|w1w2)
where ?1(w1w2) = ?1 if w1w2 ? B2 and 0 other-
wise, ?2(w2) = ?2 if w2 ? B1 and 0 otherwise and
?1 and ?2 are parameters. We call this the top-level
model pTOP because it interpolates the three models
at the top level. Most previous work on class-based
model has employed some form of top-level inter-
polation.
4 Experimental Setup
We run experiments on a Wall Street Journal (WSJ)
corpus of 50M words, split 8:1:1 into training, val-
idation and test sets. The training set contains
256,873 unique unigrams and 4,494,222 unique bi-
grams. Unknown words in validation and test sets
are mapped to a special unknown word u.
We use the SRILM toolkit (Stolcke, 2002) for
clustering. An important parameter of the class-
based model is size |Bi| of the base set, i.e., the total
number of n-grams (or rather i-grams) to be clus-
tered. As part of the experiments we vary |Bi| sys-
tematically to investigate the effect of base set size.
We cluster unigrams (i = 1) and bigrams (i = 2).
For all experiments, |B1| = |B2| (except in cases
where |B2| exceeds the number of unigrams, see be-
low). SRILM does not directly support bigram clus-
tering. We therefore represent a bigram as a hyphen-
ated word in bigram clustering; e.g., Pan Am is rep-
resented as Pan-Am.
The input to the clustering is the vocabulary Bi
and the cluster training corpus. For a particular base
set size b, the unigram input vocabulary B1 is set to
the b most frequent unigrams in the training set and
the bigram input vocabulary B2 is set to the b most
frequent bigrams in the training set.
In this section, we call the WSJ training corpus
the raw corpus and the cluster training corpus the
cluster corpus to be able to distinguish them. We
run four different clusterings for each base set size
(except for the large sets, see below). The cluster
corpora are constructed as follows.
? All-event unigram clustering. The cluster
corpus is simply the raw corpus.
? All-event bigram clustering. The cluster cor-
pus is constructed as follows. A sentence of the
raw corpus that contains s words is included
twice, once as a sequence of the ?s/2? bigrams
?w1?w2 w3?w4 w5?w6 . . . ? and once as a
sequence of the ?(s ? 1)/2? bigrams ?w2?w3
w4?w5 w6?w7 . . . ?.
? Unique-event unigram clustering. The clus-
ter corpus is the set of all sequences of two un-
igrams ? B1 that occur in the raw corpus, one
sequence per line. Each sequence occurs only
once in this cluster corpus.
? Unique-event bigram clustering. The cluster
corpus is the set of all sequences of two bi-
grams ? B2 that occur in the training corpus,
1519
one sequence per line. Each sequence occurs
only once in this cluster corpus.
As mentioned above, we need both unigram and
bigram clusters because we want to incorporate
class-based generalization for histories of lengths 1
and 2. As we will show below this significantly in-
creases performance. Since the focus of this paper is
not on clustering algorithms, reformatting the train-
ing corpus as described above (as a sequence of hy-
phenated bigrams) is a simple way of using SRILM
for bigram clustering.
The unique-event clusterings are motivated by the
fact that in the Dupont-Rosenfeld model, frequent
events are handled by discounted ML estimates.
Classes are only needed in cases where an event was
not seen or was not frequent enough in the train-
ing set. Consequently, we should form clusters not
based on all events in the training corpus, but only
on events that are rare ? because this is the type of
event that classes will then be applied to in predic-
tion.
The two unique-event corpora can be thought
of as reweighted collections in which each unique
event receives the same weight. In practice this
means that clustering is mostly influenced by rare
events since, on the level of types, most events are
rare. As we will see below, rare-event clusterings
perform better than all-event clusterings. This is
not surprising as the class-based component of the
model can only benefit rare events and it is there-
fore reasonable to estimate this component based on
a corpus dominated by rare events.
We started experimenting with reweighted cor-
pora because class sizes become very lopsided in
regular SRILM clustering as the size of the base set
increases. The reason is that the objective function
maximizes mutual information. Highly differenti-
ated classes for frequent words contribute substan-
tially to this objective function whereas putting all
rare words in a few large clusters does not hurt the
objective much. However, our focus is on using
clustering for improving prediction for rare events;
this means that the objective function is counter-
productive when contexts are frequency-weighted as
they occur in the corpus. After overweighting rare
contexts, the objective function is more in sync with
what we use clusters for in our model.
pML maximum likelihood
pB Brown cluster model
pE cluster emission probability
pT cluster transition probability
pKN KN model
pDR Dupont-Rosenfeld model
pTOP top-level interpolation
pPOLKN KN and polynomial discounting
pPOL0 polynomial discounting only
Table 2: Key to probability distributions
It is important to note that the same intu-
ition underlies unique-event clustering that
also motivates using the ?unique-event? dis-
tributions n1+(?w32)/(
?n1+(?w2w)) and
n1+(?w3)/(
?n1+(?w)) for the backoff distri-
butions in KN. Viewed this way, the basic KN
model also uses a unique-event corpus (although a
different one) for estimating backoff probabilities.
In all cases, we set the number of clusters to
k = 512. Our main goal in this paper is to compare
different ways of setting up history-length/class in-
terpolated models and we do not attempt to optimize
k. We settled on a fixed number of k = 512 because
Brown et al (1992) used a total of 1000 classes. 512
unigram classes and 512 bigram classes roughly cor-
respond to this number. We prefer powers of 2 to
facilitate efficient storage of cluster ids (one such
cluster id must be stored for each unigram and each
bigram) and therefore choose k = 512. Clustering
was performed on an Opteron 8214 processor and
took from several minutes for the smallest base sets
to more than a week for the largest set of 400,000
items.
To estimate n-gram emission probabilities pE, we
first introduce an additional cluster for all unigrams
that are not in the base set; emission probabilities
are then estimated by maximum likelihood. Cluster
transition probabilities pT are computed using add-
one smoothing. Both pE and pT are estimated on
the raw corpus. The two class distributions are then
defined as follows:
pB(w3|w1w2) = pT(g(w3)|g(w1w2))pE(w3|g(w3))
pB(w3|w2) = pT(g(w3)|g(w2))pE(w3|g(w3))
where g(v) is the class of the uni- or bigram v.
1520
pDR
all events unique events
|Bi| ?1 ?2 perp. ?1 ?2 perp.
1a 1?104 .20 .40 87.42 .2 .4 87.41
2a 2?104 .20 .50 86.97 .2 .5 86.88
3a 3?104 .10 .40 87.14 .2 .5 86.57
4a 4?104 .10 .40 87.22 .3 .5 86.31
5a 5?104 .05 .30 87.54 .3 .6 86.10
6a 6?104 .01 .30 87.71 .3 .6 85.96
pTOP
all events unique events
|Bi| ?1 ?2 perp. ?1 ?2 perp.
1b 1?104 .020 .03 87.65 .02 .02 87.71
2b 2?104 .030 .04 87.43 .03 .03 87.47
3b 3?104 .020 .03 87.52 .03 .03 87.34
4b 4?104 .010 .04 87.58 .03 .04 87.24
5b 5?104 .003 .03 87.74 .03 .04 87.15
6b 6?104 .000 .02 87.82 .03 .04 87.09
Perplexity of KN model: 88.03
Table 3: Optimal parameters for Dupont-Rosenfeld (left) and top-level (right) models on the validation set and per-
plexity on the validation set. The two tables compare performance when using a class model trained on all events vs a
class model trained on unique events. |B1| = |B2| is the number of unigrams and bigrams in the clusters; e.g., lines 1a
and 1b are for models that cluster 10,000 unigrams and 10,000 bigrams.
Table 2 is a key to the probability distributions we
use.
5 Results
Table 3 shows the performance of pDR and pTOP for a
range of base set sizes |Bi| and for classes trained on
all events and on unique events. Parameters ?i and
?i are optimized on the validation set. Perplexity is
reported for the validation set. All following tables
also optimize on the validation set and report results
on the validation set. The last table, Table 7, also
reports perplexity for the test set.
Table 3 confirms previous findings that classes
improve language model performance. All models
have a perplexity that is lower than KN (88.03).
When comparing all-event and unique-event clus-
terings, a clear tendency is apparent. In all-event
clustering, the best performance is reached for
|Bi| = 20000: perplexity is 86.97 with this base
set size for pDR (line 2a) and 87.43 for pTOP (line
2b). In unique-event clustering, performance keeps
improving with larger and larger base sets; the best
perplexities are obtained for |Bi| = 60000: 85.96
for pDR and 87.09 for pTOP (lines 6a, 6b).
The parameter values also reflect this difference
between all-event and unique-event clustering. For
unique-event results of pDR, we have ?1 ? .2 and
?2 ? .4 (1a?6a). This indicates that classes and his-
tory interpolation are both valuable when the model
is backing off. But for all-event clustering, the val-
ues of ?i decrease: from a peak of .20 and .50 (2a)
to .01 and .30 (6a), indicating that with larger base
sets, less and less value can be derived from classes.
This again is evidence that rare-event clustering is
the correct approach: only clusters derived in rare-
event clustering receive high weights ?i in the inter-
polation.
This effect can also be observed for pTOP: the
value of ?1 (the weight of bigrams) is higher for
unique-event clustering than for all-event clustering
(with the exception of lines 1b&2b). The quality of
bigram clusters seems to be low in all-event cluster-
ing when the base set becomes too large.
Perplexity is generally lower for unique-event
clustering than for all-event clustering: this is the
case for all values of |Bi| for pDR (1a?6a); and for
|Bi| > 20000 for pTOP (3b?6b).
Table 4 compares the two models in two different
conditions: (i) b-: using unigram clusters only and
(ii) b+: using unigram clusters and bigram clusters.
For all events, there is no difference in performance.
However, for unique events, the model that includes
bigrams (b+) does better than the model without bi-
grams (b-). The effect is larger for pDR than for
pTOP because (for unique events) a larger weight for
the unigram model (?2 = .05 instead of ?2 = .04)
apparently partially compensates for the missing bi-
gram clusters.
Table 3 shows that rare-event models do better
than all-event models. Given that training large class
models with SRILM on all events would take sev-
eral weeks or even months, we restrict our direct
1521
pDR pTOP
all unique all unique
?1 ?2 perp. ?1 ?2 perp. ?1 ?2 perp. ?1 ?2 perp.
b- .3 87.71 .5 86.62 .02 87.82 .05 87.26
b+ .01 .3 87.71 .3 .6 85.96 0 .02 87.82 .03 .04 87.09
Table 4: Using both unigram and bigram clusters is better than using unigrams only. Results for |Bi| = 60,000.
pDR pTOP
|Bi| ?1 ?2 perp. ?1 ?2 perp.
1 6?104 0.3 0.6 85.96 0.03 0.04 87.09
2 1?105 0.3 0.6 85.59 0.04 0.04 86.93
3 2?105 0.3 0.6 85.20 0.05 0.04 86.77
4 4?105 0.3 0.7 85.14 0.05 0.04 86.74
Table 5: Dupont-Rosenfeld and top-level models for
|Bi| ? {60000, 100000, 200000, 400000}. Clustering
trained on unique-event corpora.
comparison of all-event and rare-event models to
|Bi| ? 60, 000 in Tables 3-4 and report only rare-
event numbers for |Bi| > 60, 000 in what follows.
As we can see in Table 5, the trends observed in
Table 3 continue as |Bi| is increased further. For
both models, perplexity steadily decreases as |Bi| is
increased from 60,000 to 400,000. (Note that for
|Bi| = 400000, the actual size of B1 is 256,873
since there are only that many words in the training
corpus.) The improvements in perplexity become
smaller for larger base set sizes, but it is reassuring
to see that the general trend continues for large base
set sizes. Our explanation is that the class compo-
nent is focused on rare events and the items that are
being added to the clustering for large base sets are
all rare events.
The perplexity for pDR is clearly lower than that
of pTOP, indicating the superiority of the Dupont-
Rosenfeld model.1
1Dupont and Rosenfeld (1997) found a relatively large im-
provement of the ?global? linear interpolation model ? ptop in
our terminology ? compared to the baseline whereas ptop per-
forms less well in our experiments. One possible explanation is
that our KN baseline is stronger than the word trigram baseline
they used.
6 Polynomial discounting
Further comparative analysis of pDR and pTOP re-
vealed that pDR is not uniformly better than pTOP.
We found that pTOP does poorly on frequent events.
For example, for the history w1w2 = cents a, the
continuation w3 = share dominates. pDR deals well
with this situation because pDR(w3|w1w2) is the dis-
counted ML estimate, with a discount that is small
relative to the 10,768 occurrences of cents a share
in the training set. In the pTOP model on the last line
in Table 5, the discounted ML estimate is multiplied
by 1? .05? .04 = .91, which results in a much less
accurate estimate of pTOP(share|cents a).
In contrast, pTOP does well for productive histo-
ries, for which it is likely that a continuation unseen
in the training set will occur. An example is the his-
tory in the ? almost any adjective or noun can follow.
There are 6251 different words that (i) occur after in
the in the validation set, (ii) did not occur after in
the in the training set, and (iii) occurred at least 10
times in the training set. Because their training set
unigram frequency is at least 10, they have a good
chance of being assigned to a class that captures
their distributional behavior well and pB(w3|w1w2)
is then likely to be a good estimate. For a history
with these properties, it is advantageous to further
discount the discounted ML estimates by multiply-
ing them with .91. pTOP then gives the remaining
probability mass of .09 to words w3 whose proba-
bility would otherwise be underestimated.
What we have just described is already partially
addressed by the KN model ? ?(v) will be rela-
tively large for a productive history like v = in
the. However, it looks like the KN discounts are
not large enough for productive histories, at least not
in a combined history-length/class model. Appar-
ently, when incorporating the strengths of a class-
based model into KN, the default discounting mech-
anism does not reallocate enough probability mass
1522
from high-frequency to low-frequency events. We
conclude from this analysis that we need to increase
the discount values d for large counts.
We could add a constant to d, but one of the ba-
sic premises of the KN model, derived from the as-
sumption that n-gram marginals should be equal to
relative frequencies, is that the discount is larger for
more frequent n-grams although in many implemen-
tations of KN only the cases c(w31) = 1, c(w31) = 2,
and c(w31) ? 3 are distinguished.
This suggests that the ideal discount d(x) in an in-
tegrated history-length/class language model should
grow monotonically with c(v). The simplest way of
implementing this heuristically is a polynomial of
form ?xr where ? and r are parameters. r controls
the rate of growth of the discount as a function of x;
? is a factor that can be scaled for optimal perfor-
mance.
The incorporation of the additional polynomial
discount into KN is straightforward. We use a dis-
count function e(x) that is the sum of d(x) and the
polynomial:
e(x) = d(x) +
{
?xr for x ? 4
0 otherwise
where (e, d) ? {(e?, d?), (e??, d??), (e???, d???)}. This
model is identical to pDR except that d is replaced
with e. We call this model pPOLKN. pPOLKN directly
implements the insight that, when using class-based
generalization, discounts for counts x ? 4 should be
larger than they are in KN.
We also experiment with a second version of the
model:
e(x) = ?xr
This second model, called pPOL0, is simpler and does
not use KN discounts. It allows us to determine
whether a polynomial discount by itself (without us-
ing KN discounts in addition) is sufficient.
Results for the two models are shown in Table 6
and compared with the two best models from Ta-
ble 5, for |Bi| = 400,000, classes trained on unique
events. pPOLKN and pPOL0 achieve a small improve-
ment in perplexity when compared to pDR (line 3&4
vs 2). This shows that using discounts that are larger
than KN discounts for large counts is potentially ad-
vantageous.
?1/?1 ?2/?2 ? r perp.
1 pTOP .05 .04 86.74
2 pDR .30 .70 85.14
3 pPOLKN .30 .70 .05 .89 85.01
4 pPOL0 .30 .70 .80 .41 84.98
Table 6: Results for polynomial discounting compared
to pDR and pTOP. |Bi| = 400,000, clusters trained on
unique events.
perplexity
tb:l model |Bi| val test
1 3 pKN 88.03 88.28
2 3:6a pDR 6?104 ae b+ 87.71 87.97
3 3:6a pDR 6?104 ue b+ 85.96 86.22
4 3:6b pTOP 6?104 ae b+ 87.82 88.08
5 3:6b pTOP 6?104 ue b+ 87.09 87.35
6 4 pDR 6?104 ae b- 87.71 87.97
7 4 pDR 6?104 ue b- 86.62 86.88
8 4 pTOP 6?104 ae b- 87.82 88.08
9 4 pTOP 6?104 ue b- 87.26 87.51
10 5:4 pDR 2?105 ue b+ 85.14 85.39
11 5:4 pTOP 2?105 ue b+ 86.74 86.98
12 6:3 pPOLKN 4?105 ue b+ 85.01 85.26
13 6:4 pPOL0 4?105 ue b+ 84.98 85.22
Table 7: Performance of key models on validation and
test sets. tb:l = Table and line the validation result is taken
from. ae/ue = all-event/unique-event. b- = unigrams only.
b+ = bigrams and unigrams.
The linear interpolation ?p+(1??)q of two dis-
tributions p and q is a form of linear discounting:
p is discounted by 1 ? ? and q by ?. See (Katz,
1987; Jelinek, 1990; Ney et al, 1994). It can thus
be viewed as polynomial discounting for r = 1.
Absolute discounting could be viewed as a form of
polynomial discounting for r = 0. We know of no
other work that has explored exponents between 0
and 1 and shown that for this type of exponent, one
obtains competitive discounts that could be argued
to be simpler than more complex discounts like KN
discounts.
6.1 Test set performance
We report the test set performance of the key mod-
els we have developed in this paper in Table 7. The
experiments were run with the optimal parameters
1523
on the validation set as reported in the table refer-
enced in column ?tb:l?; e.g., on line 2 of Table 7,
(?1, ?2) = (.01, .3) as reported on line 6a of Ta-
ble 3.
There is an almost constant difference between
validation and test set perplexities, ranging from +.2
to +.3, indicating that test set results are consistent
with validation set results. To test significance, we
assigned the 2.8M positions in the test set to 48 dif-
ferent bins according to the majority part-of-speech
tag of the word in the training set.2 We can then
compute perplexity for each bin, compare perplexi-
ties for different experiments and use the sign test for
determining significance. We indicate results that
were significant at p < .05 (n = 48, k ? 32 suc-
cesses) using a star, e.g., 3<? 2 means that test set
perplexity on line 3 is significantly lower than test
set perplexity on line 2.
The main findings on the validation set alo hold
for the test set: (i) Trained on unique events and with
a sufficiently large |Bi|, both pDR and pTOP are bet-
ter than KN: 10<?1, 11<?1. (ii) Training on unique
events is better than training on all events: 3<? 2,
5<?4, 7<?6, 9<?8. (iii) For unique events, using
bigram and unigram classes gives better results than
using unigram classes only: 3<?7. Not significant:
5 < 9. (iv) The Dupont-Rosenfeld model pDR is bet-
ter than the top-level model pTOP: 10<?11. (v) The
model POL0 (polynomial discounting) is the best
model overall: Not significant: 13 < 12. (vi) Poly-
nomial discounting is significantly better than KN
discounting for the Dupont-Rosenfeld model pDR al-
though the absolute difference in perplexity is small:
13<?10.
Overall, pDR and pPOL0 achieve considerable re-
ductions in test set perplexity from 88.28 to 85.39
and 85.22, respectively. The main result of the ex-
periments is that Dupont-Rosenfeld models (which
focus on rare events) are better than the standardly
used top-level models; and that training classes on
unique events is better than training classes on all
events.
2Words with a rare majority tag (e.g., FW ?foreign word?)
and unknown words were assigned to a special class OTHER.
7 Conclusion
Our hypothesis was that classes are a generalization
mechanism for rare events that serves the same func-
tion as history-length interpolation and that classes
should therefore be (i) primarily trained on rare
events and (ii) receive high weight only if it is likely
that a rare event will follow and be weighted in a
way analogous to the weighting of lower-order dis-
tributions in history-length interpolation.
We found clear statistically significant evidence
for both (i) and (ii). (i) Classes trained on unique-
event corpora perform better than classes trained on
all-event corpora. (ii) The pDR model (which ad-
justs the interpolation weight given to classes based
on the prevalence of nonfrequent events following)
is better than top-level model pTOP (which uses a
fixed weight for classes). Most previous work on
class-based models has employed top-level interpo-
lation. Our results strongly suggest that the Dupont-
Rosenfeld model is a superior model.
A comparison of Dupont-Rosenfeld and top-level
results suggested that the KN discount mechanism
does not discount high-frequency events enough.
We empirically determined that better discounts are
obtained by letting the discount grow as a func-
tion of the count of the discounted event and im-
plemented this as polynomial discounting, an ar-
guably simpler way of discounting than Kneser-Ney
discounting. The improvement of polynomial dis-
counts vs. KN discounts was small, but statistically
significant.
In future work, we would like to find a theoreti-
cal justification for the surprising fact that polyno-
mial discounting does at least as well as Kneser-Ney
discounting. We also would like to look at other
backoff mechanisms (in addition to history length
and classes) and incorporate them into the model,
e.g., similarity and topic. Finally, training classes on
unique events is an extreme way of highly weight-
ing rare events. We would like to explore training
regimes that lie between unique-event clustering and
all-event clustering and upweight rare events less.
Acknowledgements. This research was funded
by Deutsche Forschungsgemeinschaft (grant SFB
732). We are grateful to Thomas Mu?ller, Helmut
Schmid and the anonymous reviewers for their help-
ful comments.
1524
References
Jeff Bilmes and Katrin Kirchhoff. 2003. Factored lan-
guage models and generalized parallel backoff. In
HLT-NAACL.
Peter F. Brown, Vincent J. Della Pietra, Peter V. de Souza,
Jennifer C. Lai, and Robert L. Mercer. 1992. Class-
based n-gram models of natural language. Computa-
tional Linguistics, 18(4):467?479.
Stanley F. Chen and Joshua Goodman. 1996. An empir-
ical study of smoothing techniques for language mod-
eling. CoRR, cmp-lg/9606011.
Stanley F. Chen and Joshua Goodman. 1999. An empir-
ical study of smoothing techniques for language mod-
eling. Computer Speech & Language, 13(4):359?393.
Stanley F. Chen. 2009. Shrinking exponential language
models. In HLT/NAACL, pages 468?476.
Alexander Clark. 2003. Combining distributional and
morphological information for part of speech induc-
tion. In EACL, pages 59?66.
Sabine Deligne and Yoshinori Sagisaka. 2000. Statisti-
cal language modeling with a class-based n-multigram
model. Computer Speech & Language, 14(3):261?
279.
Pierre Dupont and Ronald Rosenfeld. 1997. Lattice
based language models. Technical Report CMU-CS-
97-173, Carnegie Mellon University.
Ahmad Emami and Frederick Jelinek. 2005. Random
clustering for language modeling. In ICASSP, vol-
ume 1, pages 581?584.
Frederick Jelinek and Robert L. Mercer. 1980. Inter-
polated estimation of Markov source parameters from
sparse data. In Edzard S. Gelsema and Laveen N.
Kanal, editors, Pattern Recognition in Practice, pages
381?397. North-Holland.
Frederick Jelinek. 1990. Self-organized language mod-
eling for speech recognition. In Alex Waibel and Kai-
Fu Lee, editors, Readings in speech recognition, pages
450?506. Morgan Kaufmann.
Raquel Justo and M. Ine?s Torres. 2009. Phrase classes in
two-level language models for ASR. Pattern Analysis
& Applications, 12(4):427?437.
Slava M. Katz. 1987. Estimation of probabilities from
sparse data for the language model component of a
speech recognizer. IEEE Transactions on Acoustics,
Speech and Signal Processing, 35(3):400?401.
Reinhard Kneser and Hermann Ney. 1995. Im-
proved backing-off for m-gram language modeling. In
ICASSP, volume 1, pages 181?184.
Hong-Kwang J. Kuo and Wolfgang Reichl. 1999.
Phrase-based language models for speech recognition.
In European Conference on Speech Communication
and Technology, volume 4, pages 1595?1598.
John G. McMahon and Francis J. Smith. 1996. Improv-
ing statistical language model performance with auto-
matically generated word hierarchies. Computational
Linguistics, 22:217?247.
Saeedeh Momtazi and Dietrich Klakow. 2009. A word
clustering approach for language model-based sen-
tence retrieval in question answering systems. In ACM
Conference on Information and Knowledge Manage-
ment, pages 1911?1914.
Hermann Ney, Ute Essen, and Reinhard Kneser. 1994.
On structuring probabilistic dependencies in stochastic
language modelling. Computer Speech and Language,
8:1?38.
Roi Reichart, Omri Abend, and Ari Rappoport. 2010.
Type level clustering evaluation: new measures and a
pos induction case study. In Proceedings of the Four-
teenth Conference on Computational Natural Lan-
guage Learning, pages 77?87.
Hinrich Schu?tze. 1995. Distributional part-of-speech
tagging. In EACL 7, pages 141?148.
Andreas Stolcke. 2002. SRILM - An extensible lan-
guage modeling toolkit. In International Conference
on Spoken Language Processing, pages 901?904.
Bernhard Suhm and Alex Waibel. 1994. Towards bet-
ter language models for spontaneous speech. In Inter-
national Conference on Spoken Language Processing,
pages 831?834.
Jakob Uszkoreit and Thorsten Brants. 2008. Distributed
word clustering for large scale class-based language
modeling in machine translation. In Annual Meet-
ing of the Association for Computational Linguistics,
pages 755?762.
E.W.D. Whittaker and P.C. Woodland. 2001. Efficient
class-based language modelling for very large vocab-
ularies. In ICASSP, volume 1, pages 545?548.
Michael Wiegand and Dietrich Klakow. 2008. Opti-
mizing language models for polarity classification. In
ECIR, pages 612?616.
T. Yokoyama, T. Shinozaki, K. Iwano, and S. Furui.
2003. Unsupervised class-based language model
adaptation for spontaneous speech recognition. In
ICASSP, volume 1, pages 236?239.
Imed Zitouni and Qiru Zhou. 2007. Linearly interpo-
lated hierarchical n-gram language models for speech
recognition engines. In Michael Grimm and Kris-
tian Kroschel, editors, Robust Speech Recognition and
Understanding, pages 301?318. I-Tech Education and
Publishing.
Imed Zitouni and Qiru Zhou. 2008. Hierarchical linear
discounting class n-gram language models: A multi-
level class hierarchy approach. In International Con-
ference on Acoustics, Speech, and Signal Processing,
pages 4917?4920.
1525
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 524?528,
Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational Linguistics
Improved Modeling of Out-Of-Vocabulary Words Using Morphological
Classes
Thomas Mu?ller and Hinrich Schu?tze
Institute for Natural Language Processing
University of Stuttgart, Germany
muellets@ims.uni-stuttgart.de
Abstract
We present a class-based language model that
clusters rare words of similar morphology
together. The model improves the predic-
tion of words after histories containing out-
of-vocabulary words. The morphological fea-
tures used are obtained without the use of la-
beled data. The perplexity improvement com-
pared to a state of the art Kneser-Ney model is
4% overall and 81% on unknown histories.
1 Introduction
One of the challenges in statistical language mod-
eling are words that appear in the recognition task
at hand, but not in the training set, so called out-
of-vocabulary (OOV) words. Especially for produc-
tive language it is often necessary to at least reduce
the number of OOVs. We present a novel approach
based on morphological classes to handling OOV
words in language modeling for English. Previous
work on morphological classes in English has not
been able to show noticeable improvements in per-
plexity. In this article class-based language mod-
els as proposed by Brown et al (1992) are used to
tackle the problem. Our model improves perplex-
ity of a Kneser-Ney (KN) model for English by 4%,
the largest improvement of a state-of-the-art model
for English due to morphological modeling that we
are aware of. A class-based language model groups
words into classes and replaces the word transition
probability by a class transition probability and a
word emission probability:
P (w3|w1w2) = P (c3|c1c2) ? P (w3|c3). (1)
Brown et al and many other authors primarily use
context information for clustering. Niesler et al
(1998) showed that context clustering works better
than clusters based on part-of-speech tags. How-
ever, since the context of an OOV word is unknown
and it therefore cannot be assigned to a cluster, OOV
words are as much a problem to a context-based
class model as to a word model. That is why we
use non-distributional features ? features like mor-
phological suffixes that only depend on the shape of
the word itself ? to design a new class-based model
that can naturally integrate unknown words.
In related work, factored language models
(Bilmes and Kirchhoff, 2003) were proposed to
make use of morphological information in highly
inflecting languages such as Finnish (Creutz et al,
2007), Turkish (Creutz et al, 2007; Yuret and Bic?ici,
2009) and Arabic (Creutz et al, 2007; Vergyri et
al., 2004) or compounding languages like German
(Berton et al, 1996). The main idea is to replace
words by sequences of factors or features and to
apply statistical language modeling to the resulting
factor sequences. If, for example, words were seg-
mented into morphemes, an unknown word would
be split into an unseen sequence, which could be rec-
ognized using discounting techniques. However, if
one morpheme, e.g. the stem, is unknown to the sys-
tem, the fundamental problem remains unsolved.
Our class-based model uses a number of features
that have not been used in factored models (e.g.,
shape and length features) and achieves ? in con-
trast to factored models ? good perplexity gains for
English.
524
is capital(w) first character of w is an uppercase letter
is all capital(w) ? c ? w : c is an uppercase letter
capital character(w) ? c ? w : c is an uppercase letter
appears in lowercase(w) ?capital character(w) ? w? ? ?T
special character(w) ? c ? w : c is not a letter or digit
digit(w) ? c ? w : c is a digit
is number(w) w ? L([+ ? ?][0 ? 9] (([., ][0 ? 9])|[0 ? 9]) ?)
not special(w) ?(special character(w) ? digit(w) ? is number(w))
Table 1: Predicates of the capitalization and special character groups. ?T is the vocabulary of the training corpus T ,
w? is obtained from w by changing all uppercase letters to lowercase and L(expr) is the language generated by the
regular expression expr.
2 Morphological Features
The feature vector of a word consists of four parts
that represent information about suffixes, capitaliza-
tion, special characters and word length. For the
suffix group, we define a binary feature for each
of the 100 most frequent suffixes learned on the
training corpus by the Reports algorithm (Keshava,
2006), a general purpose unsupervised morphology
learning algorithm. One additional binary feature is
used for all other suffixes learned by Reports, in-
cluding the empty suffix.
The feature groups capitalization and special
characters are motivated by the analysis shown in
Table 2. Our goal is to improve OOV modeling.
The table shows that most OOV words (f = 0) are
numbers (CD), names (NP), and nouns and adjec-
tives (NN, NNS, JJ). This distribution is similar to
hapax legomena (f = 1), but different from the POS
distribution of all tokens. Capitalization and special
character features are of obvious utility in identify-
ing the POS classes NP and CD since names in En-
glish are usually capitalized and numbers are writ-
ten with digits and special characters such as comma
and period. To capture these ?shape? properties of a
word, we define the features listed in Table 1.
The fourth feature group is length. Short words
often have unusual distributional properties. Exam-
ples are abbreviations and bond credit ratings like
Aaa. To represent this information in the length
part of the vector, we define four binary features for
lengths 1, 2, 3 and greater than 3. The four parts
of the vector (suffixes, capitalization, special char-
acters, length) are weighted equally by normalizing
the subvector of each subgroup to unit length.
We designed the four feature groups to group
word types to either resemble POS classes or to in-
duce an even finer sub-partitioning. Unsupervised
POS clustering is a hard task in English and it is vir-
tually impossible if a word?s context (which is not
available for OOV items) is not taken into account.
For example, there is no way we can learn that ?the?
and ?a? are similar or that ?child? has the same re-
lationship to ?children? as ?kid? does to ?kids?. But
as our analysis in Table 2 shows, part of the benefit
of morphological analysis for OOVs comes from an
appropriate treatment of names and numbers. The
suffix feature group is useful for categorizing OOV
nouns and adjectives because there are very few ir-
regular morphemes like ?ren? in children in English
and OOV words are likely to be regular words.
So even though morphological learning based on
the limited information we use is not possible in gen-
eral, it can be partially solved for the special case of
OOV words. Our experimental results in Section 5
confirm that this is the case. We also testes prefixes
and features based on word stems. However, they
produced inferior clustering solutions.
3 The Language Model
As mentioned before in the literature, e.g. by Mal-
tese and Mancini (1992), class-based models only
outperform word models in cases of insufficient
data. That is why we use a frequency-based ap-
proach and only include words below a certain to-
ken frequency threshold ? in the clustering process.
A second motivation is that the contexts of low fre-
quency words are more similar to the expected con-
texts of OOV words.
Given a training corpus, all words with a fre-
525
tag types tokens
f = 1 f = 0 (OOV)
CD 0.39 0.38 0.05
NP 0.35 0.35 0.14
NN 0.10 0.10 0.17
NNS 0.05 0.06 0.07
JJ 0.05 0.06 0.07
V* 0.04 0.05 0.15
? 0.98 0.99 0.66
Table 2: Proportion of dominant POS for types with train-
ing set frequencies f ? {0, 1} and for tokens. V* consists
of all verb POS tags.
quency below the threshold ? are partitioned into
k clusters using the bisecting k-means algorithm
(Steinbach et al, 2000). The cluster of an OOV
word w can be defined as the cluster whose centroid
is closest to the feature vector of w. The formerly
removed high-frequency words are added as single-
ton clusters to produce a complete clustering. How-
ever, OOV words can only be assigned to the orig-
inal k-means clusters. Over this clustering a class-
based trigram model can be defined, as introduced
by Brown et al (1992). The word transition proba-
bility of such a model is given by equation 1, where
ci denotes the cluster of the word wi. The class
transition probability P (c3|c1c2) is estimated using
the unsmoothed maximum likelihood estimate. The
emission probability is defined as follows:
P (w3|c3) =
?
?
?
?
?
1 if c(w3) > ?
(1 ? ?) c(w3)P
w?c3
c(w) if ??c(w3)>0
? if c(w3) = 0
where c(w) is the frequency of w in the training set.
? is estimated on held-out data. The morphologi-
cal language model is then interpolated with a modi-
fied Kneser-Ney trigram model. In this interpolation
the parameters ? depend on the cluster c2 of the his-
tory word w2, i.e.:
P (w3|w1w2) = ?(c2) ? PM (w3|w1w2)
+ (1 ? ?(c2)) ? PKN (w3|w1w2).
This setup may cause overfitting as every high fre-
quent word w2 corresponds to a singleton class. A
grouping of several words into equivalence classes
could therefore further improve the model; this,
however, is beyond the scope of this article. We es-
timate optimal parameters ?(c2) using the algorithm
described by Bahl et al (1991).
4 Experimental Setup
We compare the performance of the described model
with a Kneser-Ney model and an interpolated model
based on part-of-speech (POS) tags. The relation be-
tween words and POS tags is many-to-many, but we
transform it to a many-to-one relation by labeling
every word ? independent of its context ? with its
most frequent tag. OOV words are treated equally
even though their POS classes would not be known
in a real application. Treetagger (Schmid, 1994) was
used to tag the entire corpus.
The experiments are carried out on a Wall Street
Journal (WSJ) corpus of 50 million words that is
split into training set (80%), valdev (5%), valtst
(5%), and test set (10%). The number of distinct fea-
ture vectors in training set, valdev and validation set
(valdev+valtst) are 632, 466, and 512, respectively.
As mentioned above, the training set is used to learn
suffixes and the maximum likelihood n-gram esti-
mates. The unknown word rate of the validation set
is ? ? 0.028.
We use two setups to evaluate our methods. The
first uses valdev for parameter estimation and valtst
for testing and the second the entire validation set for
parameter estimation and the test set for testing. All
models with a threshold greater or equal to the fre-
quency of the most frequent word type are identical.
We use ? as the threshold to refer to these models.
In a similar manner, the cluster count ? denotes a
clustering where two words are in the same cluster
if and only if their features are identical. This is the
finest possible clustering of the feature vectors.
5 Results
Table 3 shows the results of our experiments. The
KN model yields a perplexity of 88.06 on valtst (top
row). For small frequency thresholds overfitting ef-
fects cause that the interpolated models are worse
than the KN model. We can see that a clustering
of the feature vectors is not necessary as the differ-
ences between all cluster models are small and c?
is the overall best model. Surprisingly, morphologi-
cal clustering and POS classes are close even though
526
? cPOS c1 c50 c100 c?
0 88.06 88.06 88.06 88.06 88.06
1 89.74 89.84 89.73 89.74 89.74
5 89.07 89.36 89.07 89.06 89.07
10 88.59 89.01 88.58 88.57 88.58
50 86.72 87.58 86.69 86.68 86.68
102 85.92 87.06 85.92 85.91 85.89
103 84.43 86.88 84.83 84.77 84.56
104 85.22 87.59 85.89 85.73 85.26
105 86.82 87.99 87.44 87.32 86.79
? 87.31 88.06 87.96 87.92 87.62
? cPOS c1 c50 c100 c?
0 813.50 813.50 813.50 813.50 813.50
1 181.25 206.17 182.78 183.62 184.43
5 152.51 185.54 154.52 152.98 153.83
10 147.48 186.12 149.34 147.98 147.48
50 146.21 203.10 142.21 140.67 140.46
102 149.06 215.54 143.95 142.48 141.67
103 173.91 279.02 164.22 159.04 150.13
104 239.72 349.54 221.42 208.85 180.57
105 317.13 373.98 318.04 297.18 236.90
? 348.76 378.38 366.92 357.80 292.34
Table 3: Perplexities for different frequency thresholds ? and cluster models. In the left table, perplexity is calculated
over all events P (w3|w1w2) of the valtst set. On the right side, the subset of events where w1 or w2 are unknown is
taken into account. The overall best results for class models and POS models are highlighted in bold.
the POS class model uses oracle information to as-
sign the right POS to an unknown word. The optimal
threshold is ? = 103 ? the bolded perplexity values
84.43 and 84.56; that means that only 1.35% of the
word types were excluded from the morphological
clustering (86% of the tokens). The improvement
over the KN model is 4%.
In a second evaluation we reduce the perplexity
calculations to predictions of the form P (w3|w1w2)
where w1 or w2 are OOV words. On such an event
the KN model has to back off to a bigram or even
unigram estimate, which results in inferior predic-
tions and higher perplexity. The perplexity for the
KN model is 813.50 (top row). A first observation
is that the perplexity of model c1 starts at a good
value, but worsens with rising values for ? ? 10.
The reason is the dominance of proper nouns and
cardinal numbers at a frequency threshold of one and
in the distribution of OOV words (cf. Table 2). The
c1 model with ? = 1 is specialized for predicting
words after unknown nouns and cardinal numbers
and two thirds of the unknown words are of exactly
that type. However, with rising ?, other word classes
get a higher influence and different probability dis-
tributions are superimposed. The best morphologi-
cal model c? reduces the KN perplexity of 813.50
to 140.46 (bolded), an improvement of 83%.
As a final experiment, we evaluated our method
on the test set. In this case, we used the entire
validation set for parameter tuning (i.e., valdev and
valtst). The overall perplexity of the KN model is
88.28, the perplexities for the best POS and c? clus-
ter model for ? = 1000 are 84.59 and 84.71 respec-
tively, which corresponds again to an improvement
of 4%. For unknown histories the KN model per-
plexity is 767.25 and the POS and c? cluster model
perplexities at ? = 50 are 150.90 and 144.77. Thus,
the morphological model reduces perplexity by 81%
compared to the KN model.
6 Conclusion
We have presented a new class-based morphological
language model. In an experiment the model outper-
formed a modified Kneser-Ney model, especially in
the prediction of the continuations of histories con-
taining OOV words. The model is entirely unsuper-
vised, but works as well as a model using part-of-
speech information.
Future Work. We plan to use our model for do-
main adaptation in applications like machine trans-
lation. We then want to extend our model to other
languages, which could be more challenging, as cer-
tain languages have a more complex morphology
than English, but also worthwhile, if the unknown
word rate is higher. Preliminary experiments on
German and Finnish show promising results. The
model could be further improved by using contex-
tual information for the word clustering and training
a classifier based on morphological features to as-
sign OOV words to these clusters.
Acknowledgments. This research was funded by
DFG (grant SFB 732). We would like to thank Hel-
mut Schmid and the anonymous reviewers for their
valuable comments.
527
References
Lalit R. Bahl, Peter F. Brown, Peter V. de Souza,
Robert L. Mercer, and David Nahamoo. 1991. A fast
algorithm for deleted interpolation. In Speech Com-
munication and Technology, pages 1209?1212.
Andre Berton, Pablo Fetter, and Peter Regel-Brietzmann.
1996. Compound words in large-vocabulary German
speech recognition systems. In Spoken Language, vol-
ume 2, pages 1165 ?1168 vol.2, October.
Jeff A. Bilmes and Katrin Kirchhoff. 2003. Factored
language models and generalized parallel backoff. In
Human Language Technology, NAACL ?03, pages 4?
6. Association for Computational Linguistics.
Peter F. Brown, Peter V. de Souza, Robert L. Mercer, Vin-
cent J. Della Pietra, and Jenifer C. Lai. 1992. Class-
based n-gram models of natural language. Computa-
tional Linguistics, 18:467?479, December.
Mathias Creutz, Teemu Hirsima?ki, Mikko Kurimo, Antti
Puurula, Janne Pylkko?nen, Vesa Siivola, Matti Var-
jokallio, Ebru Arisoy, Murat Sarac?lar, and Andreas
Stolcke. 2007. Morph-based speech recognition
and modeling of out-of-vocabulary words across lan-
guages. ACM Transactions on Speech and Language
Processing, 5:3:1?3:29, December.
Samarth Keshava. 2006. A simpler, intuitive approach
to morpheme induction. In PASCAL Challenge Work-
shop on Unsupervised Segmentation of Words into
Morphemes, pages 31?35.
Giulio Maltese and Federico Mancini. 1992. An auto-
matic technique to include grammatical and morpho-
logical information in a trigram-based statistical lan-
guage model. In Acoustics, Speech, and Signal Pro-
cessing, volume 1, pages 157 ?160 vol.1, March.
Thomas R. Niesler, Edward W.D. Whittaker, and
Philip C. Woodland. 1998. Comparison of part-of-
speech and automatically derived category-based lan-
guage models for speech recognition. In Acoustics,
Speech and Signal Processing, volume 1, pages 177
?180 vol.1, May.
Helmut Schmid. 1994. Probabilistic part-of-speech tag-
ging using decision trees. In New Methods in Lan-
guage Processing, pages 44?49.
Michael Steinbach, George Karypis, and Vipin Kumar.
2000. A comparison of document clustering tech-
niques. In KDD Workshop on Text Mining.
Dimitra Vergyri, Katrin Kirchhoff, Kevin Duh, and An-
dreas Stolcke. 2004. Morphology-based language
modeling for Arabic speech recognition. In Spoken
Language Processing, pages 2245?2248.
Deniz Yuret and Ergun Bic?ici. 2009. Modeling morpho-
logically rich languages using split words and unstruc-
tured dependencies. In International Joint Conference
on Natural Language Processing, pages 345?348. As-
sociation for Computational Linguistics.
528
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 954?963,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
Sentiment Relevance
Christian Scheible
Institute for Natural Language Processing
University of Stuttgart, Germany
scheibcn@ims.uni-stuttgart.de
Hinrich Schu?tze
Center for Information
and Language Processing
University of Munich, Germany
Abstract
A number of different notions, including
subjectivity, have been proposed for dis-
tinguishing parts of documents that con-
vey sentiment from those that do not. We
propose a new concept, sentiment rele-
vance, to make this distinction and argue
that it better reflects the requirements of
sentiment analysis systems. We demon-
strate experimentally that sentiment rele-
vance and subjectivity are related, but dif-
ferent. Since no large amount of labeled
training data for our new notion of sen-
timent relevance is available, we investi-
gate two semi-supervised methods for cre-
ating sentiment relevance classifiers: a dis-
tant supervision approach that leverages
structured information about the domain
of the reviews; and transfer learning on
feature representations based on lexical
taxonomies that enables knowledge trans-
fer. We show that both methods learn sen-
timent relevance classifiers that perform
well.
1 Introduction
It is generally recognized in sentiment analy-
sis that only a subset of the content of a doc-
ument contributes to the sentiment it conveys.
For this reason, some authors distinguish the
categories subjective and objective (Wilson and
Wiebe, 2003). Subjective statements refer to the
internal state of mind of a person, which cannot be
observed. In contrast, objective statements can be
verified by observing and checking reality. Some
sentiment analysis systems filter out objective lan-
guage and predict sentiment based on subjective
language only because objective statements do not
directly reveal sentiment.
Even though the categories subjective/objective
are well-established in philosophy, we argue that
they are not optimal for sentiment analysis. We in-
stead introduce the notion of sentiment relevance
(S-relevance or SR for short). A sentence or lin-
guistic expression is S-relevant if it contains infor-
mation about the sentiment the document conveys;
it is S-nonrelevant (SNR) otherwise.
Ideally, we would like to have at our disposal
a large annotated training set for our new con-
cept of sentiment relevance. However, such a
resource does not yet exist. For this reason,
we investigate two semi-supervised approaches to
S-relevance classification that do not require S-
relevance-labeled data. The first approach is dis-
tant supervision (DS). We create an initial label-
ing based on domain-specific metadata that we ex-
tract from a public database and show that this
improves performance by 5.8% F1 compared to a
baseline. The second approach is transfer learning
(TL) (Thrun, 1996). We show that TL improves
F1 by 12.6% for sentiment relevance classification
when we use a feature representation based on lex-
ical taxonomies that supports knowledge transfer.
In our approach, we classify sentences as S-
(non)relevant because this is the most fine-grained
level at which S-relevance manifests itself; at the
word or phrase level, S-relevance classification
is not possible because of scope and context ef-
fects. However, S-relevance is also a discourse
phenomenon: authors tend to structure documents
into S-relevant passages and S-nonrelevant pas-
sages. To impose this discourse constraint, we em-
ploy a sequence model. We represent each docu-
ment as a graph of sentences and apply a minimum
cut method.
The rest of the paper is structured as follows.
Section 2 introduces the concept of sentiment rel-
evance and relates it to subjectivity. In Section 3,
we review previous work related to sentiment rel-
evance. Next, we describe the methods applied in
this paper (Section 4) and the features we extract
(Section 5). Finally, we turn to the description and
954
results of our experiments on distant supervision
(Section 6) and transfer learning (Section 7). We
end with a conclusion in Section 8.
2 Sentiment Relevance
Sentiment Relevance is a concept to distinguish
content informative for determining the sentiment
of a document from uninformative content. This
is in contrast to the usual distinction between sub-
jective and objective content. Although there is
overlap between the two notions, they are differ-
ent. Consider the following examples for subjec-
tive and objective sentences:
(1) Subjective example: Bruce Banner, a genet-
ics researcher with a tragic past, suffers a horrible
accident.
(2) Objective example: The movie won a
Golden Globe for best foreign film and an Oscar.
Sentence (1) is subjective because assessments
like tragic past and horrible accident are subjec-
tive to the reader and writer. Sentence (2) is objec-
tive since we can check the truth of the statement.
However, even though sentence (1) has negative
subjective content, it is not S-relevant because it
is about the plot of the movie and can appear in
a glowingly positive review. Conversely, sentence
(2) contributes to the positive opinion expressed
by the author. Subjectivity and S-relevance are
two distinct concepts that do not imply each other:
Generally neutral and objective sentences can be
S-relevant while certain subjective content is S-
nonrelevant. Below, we first describe the annota-
tion procedure for the sentiment relevance corpus
and then demonstrate empirically that subjectivity
and S-relevance differ.
2.1 Sentiment Relevance Corpus
For our initial experiments, we focus on senti-
ment relevance classification in the movie domain.
To create a sentiment-relevance-annotated corpus,
the SR corpus, we randomly selected 125 docu-
ments from the movie review data set (Pang et al,
2002).1 Two annotators annotated the sentences
for S-relevance, using the labels SR and SNR. If no
decision can be made because a sentence contains
both S-relevant and S-nonrelevant linguistic ma-
terial, it is marked as uncertain. We excluded
360 sentences that were labeled uncertain from the
1We used the texts from the raw HTML files since the
processed version does not have capitalization.
evaluation. In total, the SR corpus contains 2759
S-relevant and 728 S-nonrelevant sentences. Fig-
ure 1 shows an excerpt from the corpus. The full
corpus is available online.2
First, we study agreement between human an-
notators. We had 762 sentences annotated for S-
relevance by both annotators with an agreement
(Fleiss? ?) of .69. In addition, we obtained sub-
jectivity annotations for the same data on Amazon
Mechanical Turk, obtaining each label through a
vote of three, with an agreement of ? = .61. How-
ever, the agreement of the subjectivity and rele-
vance labelings after voting, assuming that sub-
jectivity equals relevance, is only at ? = .48.
This suggests that there is indeed a measurable
difference between subjectivity and relevance. An
annotator who we asked to examine the 225 ex-
amples where the annotations disagree found that
83.5% of these cases are true differences.
2.2 Contrastive Classification Experiment
We will now examine the similarities of S-
relevance and an existing subjectivity dataset.
Pang and Lee (2004) introduced subjectivity data
(henceforth P&L corpus) that consists of 5000
highly subjective (quote) review snippets from rot-
tentomatoes.com and 5000 objective (plot) sen-
tences from IMDb plot descriptions.
We now show that although the P&L selection
criteria (quotes, plot) bear resemblance to the def-
inition of S-relevance, the two concepts are differ-
ent.
We use quote as S-relevant and plot as S-
nonrelevant data in TL. We divide both the SR
and P&L corpora into training (50%) and test sets
(50%) and train a Maximum Entropy (MaxEnt)
classifier (Manning and Klein, 2003) with bag-of-
word features. Macro-averaged F1 for the four
possible training-test combinations is shown in Ta-
ble 1. The results clearly show that the classes
defined by the two labeled sets are different. A
classifier trained on P&L performs worse by about
8% on SR than a classifier trained on SR (68.5 vs.
76.4). A classifier trained on SR performs worse
by more than 20% on P&L than a classifier trained
on P&L (67.4 vs. 89.7).
Note that the classes are not balanced in the
S-relevance data while they are balanced in the
subjectivity data. This can cause a misestimation
2http://www.ims.uni-stuttgart.
de/forschung/ressourcen/korpora/
sentimentrelevance/
955
O SNR Braxton is a gambling addict in deep to Mook (Ellen Burstyn), a local bookie.
S SNR Kennesaw is bitter about his marriage to a socialite (Rosanna Arquette), believing his wife
to be unfaithful.
S SR The plot is twisty and complex, with lots of lengthy flashbacks, and plenty of surprises.
S SR However, there are times when it is needlessly complex, and at least one instance the
storytelling turns so muddled that the answers to important plot points actually get lost.
S SR Take a look at L. A. Confidential, or the film?s more likely inspiration, The Usual Suspects
for how a complex plot can properly be handled.
Figure 1: Example data from the SR corpus with subjectivity (S/O) and S-relevance (SR/SNR) annota-
tions
test
P&L SR
tra
in P&L 89.7 68.5
SR 67.4 76.4
Table 1: TL/in-task F1 for P&L and SR corpora
vocabulary fpSR fpSNR
{actor, director, story} 0 7.5
{good, bad, great} 11.5 4.8
Table 2: % incorrect sentences containing specific
words
of class probabilities and lead to the experienced
performance drops. Indeed, if we either balance
the S-relevance data or unbalance the subjectivity
data, we can significantly increase F1 to 74.8%
and 77.9%, respectively, in the noisy label trans-
fer setting. Note however that this step is difficult
in practical applications if the actual label distri-
bution is unknown. Also, in a real practical ap-
plication the distribution of the data is what it is ?
it cannot be adjusted to the training set. We will
show in Section 7 that using an unsupervised se-
quence model is superior to artificial manipulation
of class-imbalances.
An error analysis for the classifier trained on
P&L shows that many sentences misclassified as
S-relevant (fpSR) contain polar words; for exam-
ple, Then, the situation turns bad. In contrast, sen-
tences misclassified as S-nonrelevant (fpSNR) con-
tain named entities or plot and movie business vo-
cabulary; for example, Tim Roth delivers the most
impressive acting job by getting the body language
right.
The word count statistics in Table 2 show this
for three polar words and for three plot/movie
business words. The P&L-trained classifier seems
to have a strong bias to classify sentences with po-
lar words as S-relevant even if they are not, per-
haps because most training instances for the cat-
egory quote are highly subjective, so that there
is insufficient representation of less emphatic S-
relevant sentences. These snippets rarely con-
tain plot/movie-business words, so that the P&L-
trained classifier assigns almost all sentences with
such words to the category S-nonrelevant.
3 Related Work
Many publications have addressed subjectivity in
sentiment analysis. Two important papers that are
based on the original philosophical definition of
the term (internal state of mind vs. external real-
ity) are (Wilson and Wiebe, 2003) and (Riloff and
Wiebe, 2003). As we argue above, if the goal is to
identify parts of a document that are useful/non-
useful for sentiment analysis, then S-relevance is
a better notion to use.
Researchers have implicitly deviated from the
philosophical definition because they were primar-
ily interested in satisfying the needs of a particular
task. For example, Pang and Lee (2004) use a min-
imum cut graph model for review summarization.
Because they do not directly evaluate the results
of subjectivity classification, it is not clear to what
extent their method is able to identify subjectivity
correctly.
In general, it is not possible to know what the
underlying concepts of a statistical classification
are if no detailed annotation guidelines exist and
no direct evaluation of manually labeled data is
performed.
Our work is most closely related to (Taboada
et al, 2009) who define a fine-grained classifica-
tion that is similar to sentiment relevance on the
highest level. However, unlike our study, they
fail to experimentally compare their classification
scheme to prior work in their experiments and
956
to show that this scheme is different. In addi-
tion, they work on the paragraph level. How-
ever, paragraphs often contain a mix of S-relevant
and S-nonrelevant sentences. We use the mini-
mum cut method and are therefore able to incorpo-
rate discourse-level constraints in a more flexible
fashion, giving preference to ?relevance-uniform?
paragraphs without mandating them.
Ta?ckstro?m and McDonald (2011) develop a
fine-grained annotation scheme that includes S-
nonrelevance as one of five categories. However,
they do not use the category S-nonrelevance di-
rectly in their experiments and do not evaluate
classification accuracy for it. We do not use their
data set as it would cause domain mismatch be-
tween the product reviews they use and the avail-
able movie review subjectivity data (Pang and Lee,
2004) in the TL approach. Changing both the do-
main (movies to products) and the task (subjectiv-
ity to S-relevance) would give rise to interactions
that we would like to avoid in our study.
The notion of annotator rationales (Zaidan et
al., 2007) has some overlap with our notion of
sentiment relevance. Yessenalina et al (2010)
use rationales in a multi-level model to integrate
sentence-level information into a document classi-
fier. Neither paper presents a direct gold standard
evaluation of the accuracy of rationale detection.
In summary, no direct evaluation of sentiment
relevance has been performed previously. One
contribution in this paper is that we provide a
single-domain gold standard for sentiment rele-
vance, created based on clear annotation guide-
lines, and use it for direct evaluation.
Sentiment relevance is also related to review
mining (e.g., (Ding et al, 2008)) and sentiment
retrieval techniques (e.g., (Eguchi and Lavrenko,
2006)) in that they aim to find phrases, sentences
or snippets that are relevant for sentiment, either
with respect to certain features or with a focus on
high-precision retrieval (cf. (Liu, 2010)). How-
ever, finding a few S-relevant items with high pre-
cision is much easier than the task we address: ex-
haustive classification of all sentences.
Another contribution is that we show that gen-
eralization based on semantic classes improves S-
relevance classification. While previous work has
shown the utility of other types of feature gen-
eralization for sentiment and subjectivity analysis
(e.g., syntax and part-of-speech (Riloff and Wiebe,
2003)), semantic classes have so far not been ex-
ploited.
Named-entity features in movie reviews were
first used by Zhuang et al (2006), in the form
of feature-opinion pairs (e.g., a positive opinion
about the acting). They show that recognizing plot
elements (e.g., script) and classes of people (e.g.,
actor) benefits review summarization. We follow
their approach by using IMDb to define named
entity features. We extend their work by intro-
ducing methods for labeling partial uses of names
and pronominal references. We address a different
problem (S-relevance vs. opinions) and use differ-
ent methods (graph-based and statistical vs. rule-
based).
Ta?ckstro?m and McDonald (2011) also solve a
similar sequence problem by applying a distantly
supervised classifier with an unsupervised hidden
sequence component. Their setup differs from
ours as our focus lies on pattern-based distant su-
pervision instead of distant supervision using doc-
uments for sentence classification.
Transfer learning has been applied previously in
sentiment analysis (Tan and Cheng, 2009), target-
ing polarity detection.
4 Methods
Due to the sequential properties of S-relevance (cf.
Taboada et al (2009)), we impose the discourse
constraint that an S-relevant (resp. S-nonrelevant)
sentence tends to follow an S-relevant (resp. S-
nonrelevant) sentence. Following Pang and Lee
(2004), we use minimum cut (MinCut) to formal-
ize this discourse constraint.
For a document with n sentences, we create a
graph with n + 2 nodes: n sentence nodes and
source and sink nodes. We define source and
sink to represent the classes S-relevance and S-
nonrelevance, respectively, and refer to them as
SR and SNR.
The individual weight ind(s, x) between a sen-
tence s and the source/sink node x ? {SR,SNR}
is weighted according to some confidence mea-
sure for assigning it to the corresponding class.
The weight on the edge from the document?s
ith sentence si to its j th sentence sj is set to
assoc(si, sj) = c/(j ? i)2 where c is a parame-
ter (cf. (Pang and Lee, 2004)). The minimum cut
is a tradeoff between the confidence of the clas-
sification decisions and ?discourse coherence?.
The discourse constraint often has the effect that
high-confidence labels are propagated over the se-
957
quence. As a result, outliers with low confidence
are eliminated and we get a ?smoother? label se-
quence.
To compute minimum cuts, we use the push-
relabel maximum flow method (Cherkassky and
Goldberg, 1995).3
We need to find values for multiple free param-
eters related to the sequence model. Supervised
optimization is impossible as we do not have any
labeled data. We therefore resort to a proxy mea-
sure, the run count. A run is a sequence of sen-
tences with the same label. We set each param-
eter p to the value that produces a median run
count that is closest to the true median run count
(or, in case of a tie, closest to the true mean run
count). We assume that the optimal median/mean
run count is known. In practice, it can be estimated
from a small number of documents. We find the
optimal value of p by grid search.
5 Features
Choosing features is crucial in situations where
no high-quality training data is available. We are
interested in features that are robust and support
generalization. We propose two linguistic feature
types for S-relevance classification that meet these
requirements.
5.1 Generalization through Semantic
Features
Distant supervision and transfer learning are set-
tings where exact training data is unavailable. We
therefore introduce generalization features which
are more likely to support knowledge transfer. To
generalize over concepts, we use knowledge from
taxonomies. A set of generalizations can be in-
duced by making a cut in the taxonomy and defin-
ing the concepts there as base classes. For nouns,
the taxonomy is WordNet (Miller, 1995) for which
CoreLex (Buitelaar, 1998) gives a set of basic
types. For verbs, VerbNet (Kipper et al, 2008)
already contains base classes.
We add for each verb in VerbNet and for each
noun in CoreLex its base class or basic type as
an additional feature where words tagged by the
mate tagger (Bohnet, 2010) as NN.* are treated as
nouns and words tagged as VB.* as verbs. For ex-
ample, the verb suggest occurs in the VerbNet base
class say, so we add a feature VN:say to the fea-
3using the HIPR tool (www.avglab.com/andrew/
soft.html)
ture representation. We refer to these feature sets
as CoreLex (CX) and VerbNet (VN) features and to
their combination as semantic features (SEM).
5.2 Named Entities
As standard named entity recognition (NER) sys-
tems do not capture categories that are relevant to
the movie domain, we opt for a lexicon-based ap-
proach similar to (Zhuang et al, 2006). We use
the IMDb movie metadata database4 from which
we extract names for the categories <ACTOR>,
<PERSONNEL> (directors, screenwriters, and
composers), and <CHARACTER> (movie charac-
ters). Many entries are unsuitable for NER, e.g.,
dog is frequently listed as a character. We filter
out all words that also appear in lower case in a list
of English words extracted from the dict.cc dictio-
nary.5
A name n can be ambiguous between the cat-
egories (e.g., John Williams). We disambiguate
by calculating the maximum likelihood estimate
of p(c|n) = f(n,c)P
c? f(n,c?)
where c is one of the
three categories and f(n, c) is the number of times
n occurs in the database as a member of cat-
egory c. We also calculate these probabilities
for all tokens that make up a name. While this
can cause false positives, it can help in many
cases where the name obviously belongs to a cat-
egory (e.g., Skywalker in Luke Skywalker is very
likely a character reference). We always inter-
pret a name preceding an actor in parentheses
as a character mention, e.g., Reese Witherspoon
in Tracy Flick (Reese Witherspoon) is an over-
achiever [. . . ] This way, we can recognize charac-
ter mentions for which IMDb provides insufficient
information.
In addition, we use a set of simple rules to prop-
agate annotations to related terms. If a capitalized
word occurs, we check whether it is part of an al-
ready recognized named entity. For example, if
we encounter Robin and we previously encoun-
tered Robin Hood, we assume that the two enti-
ties match. Personal pronouns will match the most
recently encountered named entity. This rule has
precedence over NER, so if a name matches a la-
beled entity, we do not attempt to label it through
NER.
The aforementioned features are encoded as bi-
nary presence indicators for each sentence. This
4www.imdb.com/interfaces/
5dict.cc
958
feature set is referred to as named entities (NE).
5.3 Sequential Features
Following previous sequence classification work
with Maximum Entropy models (e.g., (Ratna-
parkhi, 1996)), we use selected features of adja-
cent sentences. If a sentence contains a feature F,
we add the feature F+1 to the following sentence.
For example, if a <CHARACTER> feature occurs
in a sentence, <CHARACTER+1> is added to the
following sentence. For S-relevance classification,
we perform this operation only for NE features as
they are restricted to a few classes and thus will
not enlarge the feature space notably. We refer to
this feature set as sequential features (SQ).
6 Distant Supervision
Since a large labeled resource for sentiment rele-
vance classification is not yet available, we inves-
tigate semi-supervised methods for creating sen-
timent relevance classifiers. In this section, we
show how to bootstrap a sentiment relevance clas-
sifier by distant supervision (DS) .
Even though we do not have sentiment rele-
vance annotations, there are sources of metadata
about the movie domain that we can leverage for
distant supervision. Specifically, movie databases
like IMDb contain both metadata about the plot,
in particular the characters of a movie, and meta-
data about the ?creators? who were involved in the
production of the movie: actors, writers, direc-
tors, and composers. On the one hand, statements
about characters usually describe the plot and are
not sentiment relevant and on the other hand, state-
ments about the creators tend to be evaluations of
their contributions ? positive or negative ? to the
movie. We formulate a classification rule based
on this observation: Count occurrences of NE fea-
tures and label sentences that contain a majority
of creators (and tied cases) as SR and sentences
that contain a majority of characters as SNR. This
simple labeling rule covers 1583 sentences with
an F1 score of 67.2% on the SR corpus. We call
these labels inferred from NE metadata distant su-
pervision (DS) labels. This is a form of distant
supervision in that we use the IMDb database as
described in Section 5 to automatically label sen-
tences based on which metadata from the database
they contain.
To increase coverage, we train a Maximum En-
tropy (MaxEnt) classifier (Manning and Klein,
2003) on the labels. The MaxEnt model achieves
an F1 of 61.2% on the SR corpus (Table 3, line 2).
As this classifier uses training data that is biased
towards a specialized case (sentences containing
the named entity types creators and characters),
it does not generalize well to other S-relevance
problems and thus yields lower performance on
the full dataset. This distant supervision setup suf-
fers from two issues. First, the classifier only sees
a subset of examples that contain named entities,
making generalization to other types of expres-
sions difficult. Second, there is no way to control
the quality of the input to the classifier, as we have
no confidence measure for our distant supervision
labeling rule. We will address these two issues by
introducing an intermediate step, the unsupervised
sequence model introduced in Section 4.
As described in Section 4, each document is
represented as a graph of sentences and weights
between sentences and source/sink nodes repre-
senting SR/SNR are set to the confidence values
obtained from the distantly trained MaxEnt clas-
sifier. We then apply MinCut as described in the
following paragraphs and select the most confident
examples as training material for a new classifier.
6.1 MinCut Setup
We follow the general MinCut setup described in
Section 4. As explained above, we assume that
creators and directors indicate relevance and char-
acters indicate nonrelevance. Accordingly, we
define nSR to be the number of <ACTOR> and
<PERSONNEL> features occurring in a sentence,
and nSNR the number of <CHARACTER> features.
We then set the individual weight between a sen-
tence and the source/sink nodes to ind(s, x) = nx
where x ? {SR,SNR}. The MinCut parameter c
is set to 1; we wish to give the association scores
high weights as there might be long spans that
have individual weights with zero values.
6.2 Confidence-based Data Selection
We use the output of the base classifier to train su-
pervised models. Since the MinCut model is based
on a weak assumption, it will make many false de-
cisions. To eliminate incorrect decisions, we only
use documents as training data that were labeled
with high confidence. As the confidence measure
for a document, we use the maximum flow value f
? the ?amount of fluid? flowing through the docu-
ment. The max-flow min-cut theorem (Ford and
Fulkerson, 1956) implies that if the flow value
959
Model Features FSR FSNR Fm
1 Majority BL ? 88.3 0.0 44.2
2 MaxEnt (DSlabels) NE 79.8 42.6 61.21
3 DSlabels+MinCut NE 79.6 48.2 63.912
4 DS MaxEnt NE 84.8 46.4 65.612
5 DS MaxEnt NE+SEM 85.2 48.0 66.6124
6 DS CRF NE 83.4 49.5 66.412
7 DS MaxEnt NE+SQ 84.8 49.2 67.01234
8 DS MaxEnt NE+SQ+SEM 84.5 49.1 66.81234
Table 3: Classification results: FSR (S-relevant F1), FSNR (S-nonrelevant F1), and Fm (macro-averaged
F1). Superscript numbers indicate a significant improvement over the corresponding line.
is low, then the cut was found more quickly and
thus can be easier to calculate; this means that the
sentence is more likely to have been assigned to
the correct segment. Following this assumption,
we train MaxEnt and Conditional Random Field
(CRF, (McCallum, 2002)) classifiers on the k%
of documents that have the lowest maximum flow
values f , where k is a parameter which we op-
timize using the run count method introduced in
Section 4.
6.3 Experiments and Results
Table 3 shows S-relevant (FSR), S-nonrelevant
(FSNR) and macro average (Fm) F1 values for dif-
ferent setups with this parameter. We compare the
following setups: (1) The majority baseline (BL)
i.e., choosing the most frequent label (SR). (2) a
MaxEnt baseline trained on DS labels without ap-
plication of MinCut; (3) the base classifier using
MinCut (DSlabels+MinCut) as described above.
Conditions 4-8 train supervised classifiers based
on the labels from DSlabels+MinCut: (4) MaxEnt
with named entities (NE); (5) MaxEnt with NE
and semantic (SEM) features; (6) CRF with NE;
(7) MaxEnt with NE and sequential (SQ) features;
(8) MaxEnt with NE, SQ, and SEM.
We test statistical significance using the approx-
imate randomization test (Noreen, 1989) on doc-
uments with 10,000 iterations at p < .05. We
achieve classification results above baseline using
the MinCut base classifier (line 3) and a consider-
able improvement through distant supervision. We
found that all classifiers using DS labels and Min-
cut are significantly better than MaxEnt trained on
purely rule-based DS labels (line 2). Also, the
MaxEnt models using SQ features (lines 7,8) are
significantly better than the MinCut base classi-
fier (line 3). For comparison to a chain-based se-
quence model, we train a CRF (line 6); however,
the improvement over MaxEnt (line 4) is not sig-
nificant.
We found that both semantic (lines 5,8) and se-
quential (lines 7,8) features help to improve the
classifier. The best model (line 7) performs bet-
ter than MinCut (3) by 3.1% and better than train-
ing on purely rule-generated DS labels (line 2) by
5.8%. However, we did not find a cumulative ef-
fect (line 8) of the two feature sets.
Generally, the quality of NER is crucial in this
task. While IMDb is in general a thoroughly com-
piled database, it is not perfect. For example, all
main characters in Groundhog Day are listed with
their first name only even though the full names
are given in the movie. Also, some entries are in-
tentionally incomplete to avoid spoiling the plot.
The data also contains ambiguities between char-
acters and titles (e.g., Forrest Gump) that are im-
possible to resolve with our maximum likelihood
method. In some types of movies, e.g., documen-
taries, the distinction between characters and ac-
tors makes little sense. Furthermore, ambiguities
like occurrences of common names such as John
are impossible to resolve if there is no earlier full
referring expression (e.g., John Williams).
Feature analysis for the best model using DS
labels (7) shows that NE features are dominant.
This correlation is not surprising as the seed la-
bels were induced based on NE features. Interest-
ingly, some subjective features, e.g., horrible have
high weights for S-nonrelevance, as they are asso-
ciated with non-relevant content such as plot de-
scriptions.
To summarize, the results of our experiments
using distant supervision show that a sentiment
relevance classifier can be trained successfully by
labeling data with a few simple feature rules, with
960
MinCut-based input significantly outperforming
the baseline. Named entity recognition, accom-
plished with data extracted from a domain-specific
database, plays a significant rule in creating an ini-
tial labeling.
7 Transfer Learning
To address the problem that we do not have
enough labeled SR data we now investigate a sec-
ond semi-supervised method for SR classification,
transfer learning (TL). We will use the P&L data
(introduced in Section 2.2) for training. This data
set has labels that are intended to be subjectivity
labels. However, they were automatically created
using heuristics and the resulting labels can be ei-
ther viewed as noisy SR labels or noisy subjectiv-
ity labels. Compared to distant supervision, the
key advantage of training on P&L is that the train-
ing set is much larger, containing around 7 times
as much data.
In TL, the key to success is to find a general-
ized feature representation that supports knowl-
edge transfer. We use a semantic feature gener-
alization method that relies on taxonomies to in-
troduce such features.
We again use MinCut to impose discourse con-
straints. This time, we first classify the data us-
ing a supervised classifier and then use MinCut to
smooth the sequences. The baseline (BL) uses a
simple bag-of-words representation of sentences
for classification which we then extend with se-
mantic features.
7.1 MinCut Setup
We again implement the basic MinCut setup from
Section 4. We set the individual weight ind(s, x)
on the edge between sentence s and class x to the
estimate p(x|s) returned by the supervised classi-
fier. The parameter c of the MinCut model is tuned
using the run count method described in Section 4.
7.2 Experiments and Results
As we would expect, the baseline performance of
the supervised classifier on SR is low: 69.9% (Ta-
ble 4, line 1). MinCut significantly boosts the per-
formance by 7.9% to 77.5% (line 1), a result sim-
ilar to (Pang and Lee, 2004). Adding semantic
features improves supervised classification signif-
icantly by 5.7% (75.6% on line 4). When MinCut
and both types of semantic features are used to-
gether, these improvements are partially cumula-
0.2 0.4 0.6 0.8 1.0
2
4
6
8
10
c
run
 leng
th
77
78
79
80
81
82
F 1
median run countmean run countF1
Figure 2: F1 measure for different values of c.
Horizontal line: optimal median run count. Cir-
cle: selected point.
tive: an improvement over the baseline by 12.6%
to 82.5% (line 4).
We also experiment with a training set where an
artificial class imbalance is introduced, matching
the 80:20 imbalance of SR:SNR in the S-relevance
corpus. After applying MinCut, we find that while
the results for BL with and without imbalances
does not differ significantly. However, models us-
ing CX and VN features and imbalances are ac-
tually significantly inferior to the respective bal-
anced versions. This result suggests that MinCut
is more effective at coping with class imbalances
than artificial balancing.
MinCut and semantic features are successful for
TL because both impose constraints that are more
useful in a setup where noise is a major problem.
MinCut can exploit test set information without
supervision as the MinCut graph is built directly
on each test set review. If high-confidence infor-
mation is ?seeded? within a document and then
spread to neighbors, mistakes with low confidence
are corrected. This way, MinCut also leads to a
compensation of different class imbalances.
The results are evidence that semantic features
are robust to the differences between subjectivity
and S-relevance (cf. Section 2). In the CX+VN
model, meaningful feature classes receive high
weights, e.g., the human class from CoreLex
which contains professions that are frequently as-
sociated with non-relevant plot descriptions.
To illustrate the run-based parameter optimiza-
tion criterion, we show F1 and median/mean run
lengths for different values of c for the best TL
961
Model base classifier MinCutFSR FSNR Fm FSR FSNR Fm
1 BL 81.1 58.6 69.9 87.2 67.8 77.5B
2 CX 82.9 60.1 71.5B 89.0 70.3 79.7BM
3 VN 85.6 62.1 73.9B 91.4 73.6 82.5BM
4 CX+VN 88.3 62.9 75.6B 92.7 72.2 82.5BM
Table 4: Classification results: FSR (S-relevant F1), FSNR (S-nonrelevant F1), and Fm (macro-averaged
F1). B indicates a significant improvement over the BL base classifier (69.9), M over BL MinCut (77.5).
setting (line 4) in Figure 2. Due to differences in
the base classifier, the optimum of c may vary be-
tween the experiments. A weaker base classifier
may yield a higher weight on the sequence model,
resulting in a larger c. The circled point shows the
data point selected through optimization. The op-
timization criterion does not always correlate per-
fectly with F1. However, we find no statistically
significant difference between the selected result
and the highest F1 value.
These experiments demonstrate that S-
relevance classification improves considerably
through TL if semantic feature generalization
and unsupervised sequence classification through
MinCut are applied.
8 Conclusion
A number of different notions, including subjec-
tivity, have been proposed for distinguishing parts
of documents that convey sentiment from those
that do not. We introduced sentiment relevance to
make this distinction and argued that it better re-
flects the requirements of sentiment analysis sys-
tems. Our experiments demonstrated that senti-
ment relevance and subjectivity are related, but
different. To enable other researchers to use this
new notion of S-relevance, we have published the
annotated S-relevance corpus used in this paper.
Since a large labeled sentiment relevance re-
source does not yet exist, we investigated semi-
supervised approaches to S-relevance classifica-
tion that do not require S-relevance-labeled data.
We showed that a combination of different tech-
niques gives us the best results: semantic gener-
alization features, imposing discourse constraints
implemented as the minimum cut graph-theoretic
method, automatic ?distant? labeling based on a
domain-specific metadata database and transfer
learning to exploit existing labels for a related
classification problem.
In future work, we plan to use sentiment rele-
vance in a downstream task such as review sum-
marization.
Acknowledgments
This work was funded by the DFG through the
Sonderforschungsbereich 732. We thank Charles
Jochim, Wiltrud Kessler, and Khalid Al Khatib for
many helpful comments and discussions.
References
Bernd Bohnet. 2010. Top accuracy and fast depen-
dency parsing is not a contradiction. In Proceedings
of the 23rd International Conference on Computa-
tional Linguistics (Coling 2010), pages 89?97, Bei-
jing, China, August. Coling 2010 Organizing Com-
mittee.
P. Buitelaar. 1998. CoreLex: systematic polysemy and
underspecification. Ph.D. thesis, Brandeis Univer-
sity.
B. Cherkassky and A. Goldberg. 1995. On imple-
menting push-relabel method for the maximum flow
problem. Integer Programming and Combinatorial
Optimization, pages 157?171.
X. Ding, B. Liu, and P. S. Yu. 2008. A holistic lexicon-
based approach to opinion mining. In WSDM 2008,
pages 231?240.
K. Eguchi and V. Lavrenko. 2006. Sentiment retrieval
using generative models. In EMNLP 2006, pages
345?354.
L.R. Ford and D.R. Fulkerson. 1956. Maximal flow
through a network. Canadian Journal of Mathemat-
ics, 8(3):399?404.
K. Kipper, A. Korhonen, N. Ryant, and M. Palmer.
2008. A large-scale classification of English verbs.
Language Resources and Evaluation, 42(1):21?40.
B. Liu. 2010. Sentiment analysis and subjectivity.
Handbook of Natural Language Processing, pages
978?1420085921.
C. Manning and D. Klein. 2003. Optimization, maxent
models, and conditional estimation without magic.
In NAACL-HLT 2003: Tutorials, page 8.
962
A.K. McCallum. 2002. Mallet: A machine learning
for language toolkit.
G.A. Miller. 1995. Wordnet: a lexical database for
english. Communications of the ACM, 38(11):39?
41.
E.W. Noreen. 1989. Computer Intensive Methods for
Hypothesis Testing: An Introduction. Wiley.
B. Pang and L. Lee. 2004. A sentimental education:
Sentiment analysis using subjectivity summarization
based on minimum cuts. In ACL 2004, pages 271?
278.
B. Pang, L. Lee, and S. Vaithyanathan. 2002. Thumbs
up?: sentiment classification using machine learning
techniques. In ACL-EMNLP 2002, pages 79?86.
A.M. Popescu and O. Etzioni. 2005. Extracting prod-
uct features and opinions from reviews. In Proceed-
ings of the conference on Human Language Tech-
nology and Empirical Methods in Natural Language
Processing, pages 339?346. Association for Compu-
tational Linguistics.
A. Ratnaparkhi. 1996. A maximum entropy model for
part-of-speech tagging. In Proceedings of the con-
ference on empirical methods in natural language
processing, volume 1, pages 133?142.
E. Riloff and J. Wiebe. 2003. Learning extraction pat-
terns for subjective expressions. In EMNLP 2003,
pages 105?112.
M. Taboada, J. Brooke, and M. Stede. 2009. Genre-
based paragraph classification for sentiment analy-
sis. In SIGdial 2009, pages 62?70.
O. Ta?ckstro?m and R. McDonald. 2011. Discover-
ing fine-grained sentiment with latent variable struc-
tured prediction models. In ECIR 2011, pages 368?
374.
S. Tan and X. Cheng. 2009. Improving SCL model
for sentiment-transfer learning. In ACL 2009, pages
181?184.
S. Thrun. 1996. Is learning the n-th thing any easier
than learning the first? In NIPS 1996, pages 640?
646.
T. Wilson and J. Wiebe. 2003. Annotating opinions in
the world press. In 4th SIGdial Workshop on Dis-
course and Dialogue, pages 13?22.
A. Yessenalina, Y. Yue, and C. Cardie. 2010. Multi-
level structured models for document-level senti-
ment classification. In EMNLP 2010, pages 1046?
1056.
O. Zaidan, J. Eisner, and C. Piatko. 2007. Using anno-
tator rationales to improve machine learning for text
categorization. In NAACL-HLT 2007, pages 260?
267.
L. Zhuang, F. Jing, and X. Zhu. 2006. Movie review
mining and summarization. In CIKM 2006, pages
43?50.
963
Proceedings of the 5th International Workshop on Semantic Evaluation, ACL 2010, pages 92?95,
Uppsala, Sweden, 15-16 July 2010.
c?2010 Association for Computational Linguistics
SUCRE: A Modular System for Coreference Resolution
Hamidreza Kobdani and Hinrich Sch?utze
Institute for Natural Language Processing
University of Stuttgart, Germany
kobdani@ims.uni-stuttgart.de
Abstract
This paper presents SUCRE, a new soft-
ware tool for coreference resolution and
its feature engineering. It is able to sep-
arately do noun, pronoun and full coref-
erence resolution. SUCRE introduces a
new approach to the feature engineering
of coreference resolution based on a rela-
tional database model and a regular feature
definition language. SUCRE successfully
participated in SemEval-2010 Task 1 on
Coreference Resolution in Multiple Lan-
guages (Recasens et al, 2010) for gold
and regular closed annotation tracks of six
languages. It obtained the best results in
several categories, including the regular
closed annotation tracks of English and
German.
1 Introduction
In this paper, we introduce a new software tool
for coreference resolution. Coreference resolution
is the process of finding discourse entities (mark-
ables) referring to the same real-world entity or
concept. In other words, this process groups the
markables of a document into equivalence classes
(coreference entities) so that all markables in an
entity are coreferent.
There are various publicly available systems
that perform coreference resolution, such as
BART (Versley et al, 2008) and GUITAR (Stein-
berger et al, 2007). A considerable engineering
effort is needed for the full coreference resolution
task, and a significant part of this effort concerns
feature engineering. Thus, a system which is able
to extract the features based on a feature defini-
tion language can help the researcher reduce the
implementation effort needed for feature extrac-
tion. Most methods of coreference resolution, if
providing a baseline, usually use a feature set sim-
ilar to (Soon et al, 2001) or (Ng and Cardie, 2002)
and do the feature extraction in the preprocessing
stage. SUCRE has been developed to provide a
more flexible method for feature engineering of
coreference resolution. It has a novel approach to
model an unstructured text corpus in a structured
framework by using a relational database model
and a regular feature definition language to define
and extract the features. Relational databases are
a well-known technology for structured data mod-
eling and are supported by a wide array of soft-
ware and tools. Converting a text corpus to/from
its equivalent relational database model is straight-
forward in our framework.
A regular language for feature definition is a
very flexible method to extract different features
from text. In addition to features defined di-
rectly in SUCRE, it accepts also externally ex-
tracted/generated features. Its modular architec-
ture makes it possible to use any externally avail-
able classification method too. In addition to link
features (features related to a markable pair), it
is also possible to define other kinds of features:
atomic word and markable features. This ap-
proach to feature engineering is suitable not only
for knowledge-rich but also for knowledge-poor
datasets. It is also language independent. The re-
sults of SUCRE in SemEval-2010 Task 1 show the
promise of our framework.
2 Architecture
The architecture of SUCRE has two main parts:
preprocessing and coreference resolution.
In preprocessing the text corpus is converted to
a relational database model. These are the main
functionalities in this stage:
1. Preliminary text conversion
2. Extracting atomic word features
3. Markable detection
92
Column Characteristic
Word Table
Word-ID Primary Key
Document-ID Foreign Key
Paragraph-ID Foreign Key
Sentence-ID Foreign Key
Word-String Attribute
Word-Feature-0 Attribute
Word-Feature-1 Attribute
... Attribute
Word-Feature-N Attribute
Markable Table
Markable-ID Primary Key
Begin-Word-ID Foreign Key
End-Word-ID Foreign Key
Head-Word-ID Foreign Key
Markable-Feature-0 Attribute
Markable-Feature-1 Attribute
... Attribute
Markable-Feature-N Attribute
Links Table
Link-ID Primary Key
First-Markable-ID Foreign Key
Second-Markable-ID Foreign Key
Coreference-Status Attribute
Status-Confidence-Level Attribute
Table 1: Relational Database Model of Text Corpus
4. Extracting atomic markable features
After converting (modeling) the text corpus to
the database, coreference resolution can be per-
formed. Its functional components are:
1. Relational Database Model of Text Corpus
2. Link Generator
3. Link Feature Extractor
4. Learning (Applicable on Train Data)
5. Decoding (Applicable on Test Data)
2.1 Relational Database Model of Text
Corpus
The Relational Database model of text thev cor-
pus is an easy to generate format. Three tables are
needed to have a minimum running system: Word,
Markable and Link.
Table 1 presents the database model of the text
corpus. In the word table, Word-ID is the index
of the word, starting from the beginning of the
corpus. It is used as the primary key to uniquely
identify each token. Document-ID, Paragraph-ID
and Sentence-ID are each counted from the be-
ginning of the corpus, and also act as the foreign
keys pointing to the primary keys of the docu-
ment, paragraph and sentence tables, which are
optional (the system can also work without them).
It is obvious that the raw text as well as any other
format of the corpus can be generated from the
word table. Any word features (Word-Feature-#X
columns) can be defined and will then be added
to the word table in preprocessing. In the mark-
able table, Markable-ID is the primary key. Begin-
Word-ID, End-Word-ID and Head-Word-ID refer
to the word table. Like the word features, the
markable features are not mandatory and in the
preprocessing we can decide which features are
added to the table. In the link table, Link-ID is
the primary key; First-Markable-ID and Second-
Markable-ID refer to the markable table.
2.2 Link Generator
For training, the system generates a positive train-
ing instance for each adjacent coreferent markable
pair and negative training instances for a markable
m and all markables disreferent with m that occur
before m (Soon et al, 2001). For decoding it gen-
erates all the possible links inside a window of 100
markables.
2.3 Link Feature Extractor
There are two main categories of features in
SUCRE: Atomic Features and Link Features
We first explain atomic features in detail and
then turn to link features and the extraction method
we use.
Atomic Features: The current version of
SUCRE supports the atomic features of words
and markables but in the next versions we are
going to extend it to sentences, paragraphs and
documents. An atomic feature is an attribute. For
example the position of the word in the corpus
is an atomic word feature. Atomic word features
are stored in the columns of the word table called
Word-Feature-X.
In addition to word position in the corpus, doc-
ument number, paragraph number and sentence
number, the following are examples of atomic
word features which can be extracted in prepro-
cessing: Part of speech tag, Grammatical Gen-
der (male, female or neutral), Natural Gender
(male or female), Number (e.g. singular, plural or
both), Semantic Class, Type (e.g. pronoun types:
personal, reflexive, demonstrative ...), Case (e.g.
nominative, accusative, dative or genitive in Ger-
man) and Pronoun Person (first, second or third).
Other possible atomic markable features include:
93
number of words in markable, named entity, alias,
syntactic role and semantic class.
For sentences, the following could be extracted:
number of words in the sentence and sentence
type (e.g. simple, compound or complex). For
paragraphs these features are possible: number of
words and number of sentences in the paragraph.
Finally, examples of document features include
document type (e.g. news, article or book), num-
ber of words, sentences and paragraphs in the doc-
ument.
Link Features: Link features are defined over a
pair of markables. For link feature extraction, the
head words of the markables are usually used, but
in some cases the head word may not be a suitable
choice. For example, consider the two markables
the books and a book. In both cases book is the
head word, but to distinguish which markable is
definite and which indefinite, the article must be
taken into account. Now consider the two mark-
ables the university student from Germany and the
university student from France. In this case, the
head words and the first four words of each mark-
able are the same but they can not be coreferent;
this can be detected only by looking at the last
words. Sometimes we need to consider all words
in the two markables, or even define a feature for
a markable as a unit. To cover all such cases
we need a regular feature definition language with
some keywords to select different word combina-
tions of two markables. For this purpose, we de-
fine the following variables. m1 is the first mark-
able in the pair. m1b, m1e and m1h are the first,
last and head words of the first markable in the
pair. m1a refers to all words of the first markable
in the pair. m2, m2b, m2e, m2h and m2a have
the same definitions as above but for the second
markable in the pair.
In addition to the above keywords there are
some other keywords that this paper does not have
enough space to mention (e.g. for accessing the
constant values, syntax relations or roles). The
currently available functions are: exact- and sub-
string matching (in two forms: case-sensitive and
case-insensitive), edit distance, alias, word rela-
tion, markable parse tree path, absolute value.
Two examples of link features are as follows:
? (seqmatch(m1a,m2a) > 0)
&& (m1h.f0 == f0.N )
&& (m2h.f0 == f0.N )
means that there is at least one exact match
between the words of the markables and that
the head words of both are nouns (f0 means
Word-Feature-0, which is part of speech in
our system).
? (abs(m2b.stcnum?m1b.stcnum) == 0)
&& (m2h.f3 == f3.reflexive)
means that two markables are in the same
sentence and that the type of the sec-
ond markable head word is reflexive (f3
means Word-Feature-3, which is morpholog-
ical type in our system).
2.4 Learning
There are four classifiers integrated in SUCRE:
Decision-Tree, Naive-Bayes, Support Vector Ma-
chine (Joachims, 2002) and Maximum-Entropy
(Tsuruoka, 2006).
When we compared these classifiers, the best
results, which are reported in Section 3, were
achieved with the Decision-Tree.
2.5 Decoding
In decoding, the coreference chains are created.
SUCRE uses best-first clustering for this purpose.
It searches for the best predicted antecedent from
right-to-left starting from the end of the document.
3 Results
Table 2 shows the results of SUCRE and the best
competitor system on the test portions of the six
languages from SemEval-2010 Task 1. Four dif-
ferent evaluation metrics were used to rank the
participating systems: MUC (Vilain et al, 1995),
B
3
(Bagga and Baldwin, 1998), CEAF (Luo,
2005) and BLANC (Recasens and Hovy, in prep).
SUCRE has the best results in regular closed
annotation track of English and German (for all
metrics). Its results for gold closed annotation
track of both English and German are the best
in MUC and BLANC scoring metrics (MUC: En-
glish +27.1 German +32.5, BLANC: English +9.5
German +9.0) and for CEAF and B
3
(CEAF: En-
glish -1.3 German -4.8, B
3
: English -2.1 German
-4.8); in comparison to the second ranked sys-
tem, the performance is clearly better in the first
case and slightly better in the second. This re-
sult shows that SUCRE has been optimized in a
way that achieves good results on the four different
scoring metrics. We view this good performance
as a demonstration of the strength of SUCRE: our
94
method of feature extraction, definition and tuning
is uniform and can be optimized and applied to all
languages and tracks.
Results of SUCRE show a correlation between
the MUC and BLANC scores (the best MUC
scores of all tracks and the best BLANC scores in
11 tracks of a total 12), in our opinion this correla-
tion is not because of the high similarity between
MUC and BLANC, but it is because of the bal-
anced scores.
Language ca de en es it nl
System SUCRE (Gold Annotation)
MD-F1 100 100 100 100 98.4 100
CEAF-F1 68.7 72.9 74.3 69.8 66.0 58.8
MUC-F1 56.2 58.4 60.8 55.3 45.0 69.8
B
3
-F1 77.0 81.1 82.4 77.4 76.8 67.0
BLANC 63.6 66.4 70.8 64.5 56.9 65.3
System SUCRE (Regular Annotation)
MD-F1 69.7 78.4 80.7 70.3 90.8 42.3
CEAF-F1 47.2 59.9 62.7 52.9 61.3 15.9
MUC-F1 37.3 40.9 52.5 36.3 50.4 29.7
B
3
-F1 51.1 64.3 67.1 55.6 70.6 11.7
BLANC 54.2 53.6 61.2 51.4 57.7 46.9
System Best Competitor (Gold Annotation)
MD-F1 100 100 100 100 N/A N/A
CEAF-F1 70.5 77.7 75.6 66.6 N/A N/A
MUC-F1 42.5 25.9 33.7 24.7 N/A N/A
B
3
-F1 79.9 85.9 84.5 78.2 N/A N/A
BLANC 59.7 57.4 61.3 55.6 N/A N/A
System Best Competitor (Regular Annotation)
MD-F1 82.7 59.2 73.9 83.1 55.9 34.7
CEAF-F1 57.1 49.5 57.3 59.3 45.8 17.0
MUC-F1 22.9 15.4 24.6 21.7 42.7 8.3
B
3
-F1 64.6 50.7 61.3 66.0 46.4 17.0
BLANC 51.0 44.7 49.3 51.4 59.6 32.3
Table 2: Results of SUCRE and the best competitor system.
Bold F1 scores indicate that the result is the best SemEval
result. MD: Markable Detection, ca: Catalan, de: German,
en:English, es: Spanish, it: Italian, nl: Dutch
4 Conclusion
In this paper, we have presented a new modular
system for coreference resolution. In comparison
with the existing systems the most important ad-
vantage of our system is its flexible method of fea-
ture engineering based on relational database and a
regular feature definition language. There are four
classifiers integrated in SUCRE: Decision-Tree,
Naive-Bayes, SVM and Maximum-Entropy. The
system is able to separately do noun, pronoun and
full coreference resolution. The system uses best-
first clustering. It searches for the best predicted
antecedent from right-to-left starting from the end
of the document.
References
Amit Bagga and Breck Baldwin. 1998. Algorithms for
scoring coreference chains. In In The First Interna-
tional Conference on Language Resources and Eval-
uation Workshop on Linguistics Coreference, pages
563?566.
Thorsten Joachims. 2002. Learning to Classify Text
Using Support Vector Machines, Methods, Theory,
and Algorithms. Kluwer/Springer.
Xiaoqiang Luo. 2005. On coreference resolution per-
formance metrics. In HLT ?05: Proceedings of
the conference on Human Language Technology and
Empirical Methods in Natural Language Process-
ing, pages 25?32, Morristown, NJ, USA. Associa-
tion for Computational Linguistics.
Vincent Ng and Claire Cardie. 2002. Improving ma-
chine learning approaches to coreference resolution.
In Proceedings of the ACL, pages 104?111.
Marta Recasens and Eduard Hovy. in prep. BLANC:
Implementing the Rand Index for Coreference Eval-
uation.
Marta Recasens, Llu??s M`arquez, Emili Sapena,
M.Ant`onia Mart??, Mariona Taul?e, V?eronique Hoste,
Massimo Poesio, and Yannick Versley. 2010.
SemEval-2010 Task 1: Coreference resolution in
multiple languages. In Proceedings of the 5th
International Workshop on Semantic Evaluations
(SemEval-2010), Uppsala, Sweden.
Wee Meng Soon, Hwee Tou Ng, and Daniel
Chung Yong Lim. 2001. A machine learning ap-
proach to coreference resolution of noun phrases. In
Computational Linguistics, pages 521?544.
Josef Steinberger, Massimo Poesio, Mijail A. Kabad-
jovb, and Karel Jezek. 2007. Two uses of anaphora
resolution in summarization. In Information Pro-
cessing and Management, Special issue on Summa-
rization, pages 1663?1680.
Yoshimasa Tsuruoka. 2006. A simple c++ library
for maximum entropy classification. Tsujii labora-
tory, Department of Computer Science, University of
Tokyo.
Yannick Versley, Simone Paolo Ponzetto, Massimo
Poesio, Vladimir Eidelman, Alan Jern, Jason Smith,
and Xiaofeng Yang. 2008. Bart: A modular toolkit
for coreference resolution. In Proceedings of the
46nd Annual Meeting of the Association for Com-
putational Linguistics, pages 9?12.
Marc Vilain, John Burger, John Aberdeen, Dennis Con-
nolly, and Lynette Hirschman. 1995. A model-
theoretic coreference scoring scheme. In MUC6
?95: Proceedings of the 6th conference on Message
understanding, pages 45?52, Morristown, NJ, USA.
Association for Computational Linguistics.
95
Proceedings of the 15th Conference on Computational Natural Language Learning: Shared Task, pages 71?75,
Portland, Oregon, 23-24 June 2011. c?2011 Association for Computational Linguistics
Supervised Coreference Resolution with SUCRE
Hamidreza Kobdani and Hinrich Schu?tze
Institute for Natural Language Processing
University of Stuttgart, Germany
kobdani@ims.uni-stuttgart.de
Abstract
In this paper we present SUCRE (Kobdani
and Schu?tze, 2010) that is a modular coref-
erence resolution system participating in the
CoNLL-2011 Shared Task: Modeling Unre-
stricted Coreference in OntoNote (Pradhan et
al., 2011). The SUCRE?s modular architecture
provides a clean separation between data stor-
age, feature engineering and machine learning
algorithms.
1 Introduction
Noun phrase coreference resolution is the process
of finding markables (noun phrase) referring to the
same real world entity or concept. In other words,
this process groups the markables of a document
into entities (equivalence classes) so that all mark-
ables in an entity are coreferent. Examples of ap-
plications of coreference resolution are Informa-
tion Extraction, Question Answering and Automatic
Summarization.
Coreference is an equivalence relation between
two markables, i.e., it is reflexive, symmetric and
transitive. The first solution that intuitively comes
to mind is binary classification of markable pairs
(links). Therefore at the heart of most existing ap-
proaches there is a binary classifier that classifies
links to coreferent/disreferent. One can also use the
transitive property of coreference relation to build
the entities; this is done using a clustering method.
Our approach in this paper consist of the above
mentioned steps, namely:
1. Classification of links to coreferent/disreferent.
2. Clustering of links which are classified as
coreferent.
This paper is organized as follows. In Section 2,
we present our feature engineering approach. Sec-
tion 3 presents the system architecture. Data set is
described in Section 4. Sections 5 and 6 present re-
sults and conclusions.
2 Feature Engineering
In recent years there has been substantial work on
the problem of coreference resolution. Most meth-
ods present and report on the benchmark data sets
for English. The feature sets they use are based on
(Soon et al, 2001). These features consist of string-
based features, distance features, span features, part-
of-speech features, grammatical features, and agree-
ment features.
We defined a comprehensive set of features based
on previous coreference resolution systems for En-
glish, e.g. (Bengtson and Roth, 2008). In the com-
mon approach to coreference resolution we have
chosen, features are link features, i.e., features are
defined over a pair of markables. For link feature
definition and extraction, the head words of mark-
ables are usually used, but in some cases the head
word is not a suitable choice. For example, con-
sider these two markables: the book and a book, in
both cases book is the head word but to distinguish
which markable is definite and which indefinite ad-
ditional information about the markables has to be
taken into account. Now consider these two mark-
ables: the university students in Germany and the
university students in France in this case the head
words and the first four words of each markable
are the same but they cannot be coreferent, and this
could be detected only by looking at the entire noun
phrase. Some features require complex preprocess-
71
ing or complex definitions. Consider the two mark-
ables the members of parliament and the members of
the European Union. The semantic class ofmembers
is person in the first case and country in the second.
To cover all such cases, we introduced a feature defi-
nition language (Kobdani et al, 2010). With the fea-
ture definition language we will be able to access all
information that is connected to a markable, includ-
ing the first, last and head words of the two mark-
ables; all other words of the two markables; and the
two markables as atomic elements.
After defining new features (new definition from
scratch or definition by combination of existing fea-
tures), we have to evaluate them. In principle, we
could use any figure of merit to evaluate the useful-
ness of a feature or to compare two similar features,
including Gini coefficient, mutual information, and
correlation coefficient. In our current system, ex-
pected information gain (IG) and information gain
ratio (IGR) are used.
As an example, consider the following two fea-
tures, which can be considered different attempts to
formalize the same linguistic property:
1. The noun phrase has a subject role and is def-
inite (e.g. markable begins with a definite arti-
cle)
2. The noun phrase has a subject role and is not
indefinite (e.g. markable begins with an indefi-
nite article)
The information gain ratios of the above men-
tioned features are equal to 0.0026 for the first and
0.0051 for the second one ? this shows that the sec-
ond one is a better choice. We now define IG and
IGR.
The change in entropy from a prior state to a state
that takes some information is the expected informa-
tion gain (Mitchell, 1997):
IG (f) = H (C) ? Hf (C) (1)
Where f is the feature value, C its corresponding
class, and entropy is defined as follows:
H (C) = ?
?
i
P (Ci) log2P (Ci) (2)
Hf (C) =
?
f
|Cf |
|C| H (Cf ) (3)
If a feature takes a large number of distinct values,
the information gain would not be a good measure
for deciding its relevance. In such cases the infor-
mation gain ratio is used instead. The information
gain ratio for a feature is calculated as follows:
IGR (f) = IG (f)
SInf (C) (4)
SInf(C) = ?
?
i
|Ci|
|C| log2
|Ci|
|C| (5)
Equation (4) can be used as an indicator for which
features are likely to improve classification accu-
racy.
3 System Architecture
The architecture of the system has two main parts:
preprocessing and coreference resolution.
In preprocessing the text corpus is converted to
a relational data model. The main purpose of the
relational model in our system is the use of a fea-
ture definition language (Kobdani et al, 2010). Af-
ter modeling the text corpus, coreference resolution
can be performed.
The main steps of the system are presented as fol-
lows.
3.1 Preliminary text conversion
In this step, tokens are extracted from the corpus. In
the CoNLL-2011 Shared Task this step is as simple
as reading each line of the input data set and extract-
ing its corresponding token.
3.2 Atomic attributes of tokens
Atomic features of the tokens are extracted
in this step. The extracted atomic features
are: part of speech, number, pronoun person
(first, second and third), pronoun type (subjec-
tive,,predeterminer,reflexive,objective and posses-
sive), WordNet semantic class and gender.
We use a rather simple method to extract semantic
class of each token from WordNet. We look at the
synonyms of the token and if one of them is in the
predefined keyword set, we take it as its correspond-
ing semantic class. The example of the keywords
are person, time, abstraction, device, human action,
organization, place and animal.
72
3.3 Markable Detection
In this step all noun phrases from the parse tree are
extracted. After clustering step all markables which
are not included in a chain are deleted from the list
of markables. In other word we will not have any
cluster with less than 2 members.
Figure 1 presents the simple markable detection
method which we used in the SUCRE.
3.4 Atomic attributes of markables
In this step, the atomic attributes of the markables
are extracted. In the data set of the CoNLL-2011
shared task the named entity property of a markable
can be used as its atomic attribute.
3.5 Link Generator
For training, the system generates a positive train-
ing instance for an adjacent coreferent markable pair
(m, n) and negative training instances for the mark-
able m and all markables disreferent with m that oc-
cur before n (Soon et al, 2001). For decoding it
generates all the possible links inside a window of
100 markables.
3.6 Link feature definition and extraction
The output of the link generator, which is the list of
the generated links, is the input to the link feature
extractor for creating train and test data sets. To do
this, the feature definitions are used to extract the
feature values of the links (Kobdani et al, 2011).
3.7 Learning
For learning we implemented a decision tree classi-
fier (Quinlan, 1993). To achieve state-of-the-art per-
formance, in addition to decision tree we also tried
support vector machine and maximum entropy that
did not perform better than decision tree.
3.8 Classification and Clustering
In this part, the links inside one document are clas-
sified then the coreference chains are created. We
use best-first clustering for this purpose. It searches
for the best predicted antecedent from right to left
starting from the end of the document. For the docu-
ments with more than a predefined number of mark-
ables we apply a limit for searching. In this way, in
addition to better efficiency, the results also improve.
Markable Detection PSG A (W1, W2, . . . , Wn)
1. A markable M is presented by a set of
three words:
Begin (Mb), End (Me) and Head (Mh).
2. Let DM be the set of detected markables.
3. Let Ti be the node i in the parse tree with
label Li
(if node is a word then Li is equal to Wi).
4. Start from parse tree root Tr:
Find Markables(Tr,Lr,DM )
Find Markables(T ,L,DM )
1. If L is equal to noun phrase, then extract
the markable M :
(a) Set the begin word of the markable:
Mb = Noun Phrase Begin(T ,L)
(b) Set the end word of the markable:
Me = Noun Phrase End(T ,L)
(c) Set the head word of the markable:
Mh = Noun Phrase Head(T ,L)
(d) Add the markable M to the set of de-
tected markables DM .
2. Repeat for all Ti the daughters of T :
Find Markables(Ti,Li,DM )
Noun Phrase Begin(T ,L)
If T has no daughter then return L;
else set Tb to the first daughter of T and return
Noun Phrase Begin(Tb,Lb).
Noun Phrase End(T ,L)
If T has no daughter then return L;
else set Tb to the last daughter of T and return
Noun Phrase End(Tb,Lb).
Noun Phrase Head(T ,L)
If T has no daughter then return L;
else set Th to the biggest noun phrase daughter
of T and return Noun Phrase Head(Th,Lh).
Figure 1: Markable Detection from Parse Tree (all possi-
ble markables) .
73
Automatic Gold
Rec. Prec. F1 Rec. Prec. F1
MD 60.17 60.92 60.55 62.50 61.62 62.06
MUC 54.30 51.84 53.06 57.44 53.15 55.21
B3 71.39 64.68 67.87 74.07 64.39 68.89
CEAFM 46.36 46.36 46.36 47.07 47.07 47.07
CEAFE 35.38 37.26 35.30 35.19 38.44 36.74
BLANC 65.01 64.93 64.97 66.23 65.16 65.67
Table 1: Results of SUCRE on the development data set
for the automatically detected markables. MD: Markable
Detection.
4 Data Sets
OntoNotes has been used for the CoNLL-2011
shared task. The OntoNotes project 1 is to provide
a large-scale, accurate corpus for general anaphoric
coreference. It aims to cover entities and events (i.e.
it is not limited to noun phrases or a limited set of
entity types) (Pradhan et al, 2007).
For training we used 4674 documents containing
a total of 1909175 tokens, 190700 markables and
50612 chains.
SUCRE participated in the closed track of the
shared task. Experiments have been performed for
the two kind of documents, namely, the automati-
cally preprocessed documents and the gold prepro-
cessed documents. In this paper, we report only the
scores on the development data set using the offi-
cial scorer of the shared task. The automatically
preprocessed part consists of 303 documents con-
taining a total of 136257 tokens, 52189 automati-
cally detected markables, 14291 true markables and
3752 chains. The gold preprocessed part consists of
303 documents containing a total of 136257 tokens,
52262 automatically detected markables, 13789 true
markables and 3752 chains.
5 Results
We report recall, precision, and F1 for MUC (Vi-
lain et al, 1995), B3 (Bagga and Baldwin, 1998),
CEAFM /CEAFE (Luo, 2005) and BLANC (Re-
casens et al, 2010).
Table 1 presents results of our system for the
automatically detected markables. It is apparent
from this table that the application of the gold pre-
processed documents slightly improves the perfor-
mance (MD-F1: +1.51; MUC-F1: +2.15; B3-F1:
1http://www.bbn.com/ontonotes/
Automatic Gold
Rec. Prec. F1 Rec. Prec. F1
MUC 58.63 87.88 70.34 60.48 88.25 71.78
B3 57.91 86.47 69.36 59.21 86.25 70.22
CEAFM 59.81 59.81 59.81 60.91 60.91 60.91
CEAFE 70.49 36.43 48.04 71.09 37.73 49.30
BLANC 69.67 76.27 72.34 70.34 76.01 72.71
Table 2: Results of SUCRE on the development data set
for the true markables (i.e. no singletone is included).
+1.02; CEAFM -F1: +0.71; CEAFE-F1: +1.44;
BLANC-F1: +0.70 ).
Table 2 presents results of our system for the true
markables that were all and only part of coreference
chains. Again the results show that the application
of gold preprocessed documents slightly improves
the performance (MUC-F1: +1.44; B3-F1: +0.86;
CEAFM -F1: +1.1; CEAFE-F1: +1.26; BLANC-F1:
+0.37 ).
Comparing the results of tables 1 and 2, there is a
significant difference between the scores on the au-
tomatically detected markables and the scores on the
true markables (e.g. for the automatically prepro-
cessed documents: MUC-F1: +17.28; CEAFM -F1:
+13.45; CEAFE-F1: +12.74; BLANC-F1: +7.37).
No significant improvement in B3 is seen (auto-
matic: +1.49; gold: +1.33). We suspect that this is
partly due to the very sensitive nature of B3 against
the singleton chains. Because in the implementation
of scorer for the CoNLL-2011 shared task the non-
detected key markables are automatically included
into the response as singletons.
6 Conclusion
In this paper, we have presented our system SUCRE
participated in the CoNLL-2011 shared task. We
took a deeper look at the feature engineering of SU-
CRE. We presented the markable detection method
we applied.
We showed that the application of the gold pre-
processed documents improves the performance. It
has been demonstrated that the availability of the
true markables significantly improves the results.
Also it has been shown that the singletons have a
large impact on the B3 scores.
74
References
Amit Bagga and Breck Baldwin. 1998. Algorithms for
scoring coreference chains. In LREC Workshop on
Linguistics Coreference ?98, pages 563?566.
Eric Bengtson and Dan Roth. 2008. Understanding
the value of features for coreference resolution. In
EMNLP ?08, pages 294?303.
Hamidreza Kobdani and Hinrich Schu?tze. 2010. Sucre:
A modular system for coreference resolution. In Se-
mEval ?10, pages 92?95.
Hamidreza Kobdani, Hinrich Schu?tze, Andre Burkovski,
Wiltrud Kessler, and Gunther Heidemann. 2010. Re-
lational feature engineering of natural language pro-
cessing. In CIKM ?10. ACM.
Hamidreza Kobdani, Hinrich Schu?tze, Michael
Schiehlen, and Hans Kamp. 2011. Bootstrap-
ping coreference resolution using word associations.
In Proceedings of the 49th Annual Meeting of the
Association for Computational Linguistics, ACL ?11.
Association for Computational Linguistics.
Xiaoqiang Luo. 2005. On coreference resolution perfor-
mance metrics. In HLT ?05, pages 25?32.
Tom M. Mitchell. 1997. Machine Learning. McGraw-
Hill, New York.
Sameer Pradhan, Lance Ramshaw, Ralph Weischedel,
Jessica MacBride, and Linnea Micciulla. 2007. Unre-
stricted Coreference: Identifying Entities and Events
in OntoNotes. In in Proceedings of the IEEE Inter-
national Conference on Semantic Computing (ICSC),
September 17-19.
Sameer Pradhan, Lance Ramshaw, Mitchell Marcus,
Martha Palmer, Ralph Weischedel, and Nianwen Xue.
2011. Conll-2011 shared task: Modeling unrestricted
coreference in ontonotes. In Proceedings of the Fif-
teenth Conference on Computational Natural Lan-
guage Learning (CoNLL 2011), Portland, Oregon,
June.
J. Ross Quinlan. 1993. C4.5: Programs for machine
learning. Morgan Kaufmann Publishers Inc., San
Francisco, CA, USA.
Marta Recasens, Llu??s Ma`rquez, Emili Sapena,
M.Anto`nia Mart??, Mariona Taule?, Ve?ronique Hoste,
Massimo Poesio, and Yannick Versley. 2010.
SemEval-2010 Task 1: Coreference resolution in
multiple languages. In SemEval ?10, pages 70?75.
Wee Meng Soon, Hwee Tou Ng, and Daniel Chung Yong
Lim. 2001. A machine learning approach to coref-
erence resolution of noun phrases. In CL ?01, pages
521?544.
Marc Vilain, John Burger, John Aberdeen, Dennis Con-
nolly, and Lynette Hirschman. 1995. A model-
theoretic coreference scoring scheme. In MUC6 ?95,
pages 45?52.
75
