NAACL HLT Demonstration Program, pages 1?2,
Rochester, New York, USA, April 2007. c?2007 Association for Computational Linguistics
Demonstration of PLOW: A Dialogue System for One-Shot Task 
Learning
James Allen, Nathanael Chambers, George Ferguson
* 
, Lucian Galescu, Hyuckchul Jung, 
Mary Swift
* 
and William Taysom
Florida Institute for Human and Machine Cognition, Pensacola, FL 32502
*Computer Science Department, University of Rochester, Rochester, NY 14627
Introduction
We describe a system that can learn new 
procedure models effectively from one 
demonstration by the user. Previous work to learn 
tasks through observing a demonstration (e.g., 
Lent & Laird, 2001) has required observing many 
examples of the same task. One-shot learning of 
tasks presents a significant challenge because the 
observed sequence is inherently incomplete ? the 
user only performs the steps required for the 
current situation.  Furthermore, their decision-
making processes, which reflect the control 
structures in the procedure, are not revealed. 
We will demonstrate a system called PLOW 
(Procedural Learning on the Web) that learns task 
knowledge through observation accompanied by a 
natural language ?play-by-play?. Natural 
language (NL) alleviates many task learning 
problems by identifying (i) a useful level of 
abstraction of observed actions; (ii) parameter 
dependencies; (iii) hierarchical structure; (iv) 
semantic relationships between the task and the 
items involved in the actions; and (v) control 
constructs not otherwise observable. Various 
specialized reasoning modules in the system 
communicate and collaborate with each other to 
interpret the user?s intentions, build a task model 
based on the interpretation, and check consistency 
between the learned task and prior knowledge.
The play-by-play approach in NL enables our 
task learning system to build a task with high-
level constructs that are not inferable from 
observed actions alone. In addition to the 
knowledge about task structure, NL also provides 
critical information to transform the observed 
actions into more robust and reliable executable 
forms. Our system learns how to find objects used 
in the task, unifying the linguistic information of 
the objects with the semantic representations of 
the user?s NL descriptions about them.  The 
objects can then be reliably found in dynamic and 
complex environments. See Jung et al(2006) and 
Chambers et al(2006) for more details on the 
PLOW system.
The PLOW System
PLOW learns tasks executable on the web 
involving actions such as navigation, information 
extraction and form filling, and can learn iterative 
steps that operate over lists of objects on pages. 
Figure 1 shows the system during learning a task 
to find publications for a specified author. Upper 
left is the Mozilla browser, in which the user can 
demonstrate action and the system can execute 
actions in a mixed-initiative fashion. The user 
may speak or type to the system (SR output is 
lower right), and PLOW combines knowledge 
from the language and the demonstrated actions to 
produce a parameterized procedure (described in 
generated natural language in the upper right 
corner). Figure 2 shows a complete training 
dialogue in which PLOW learns how to find 
article titles. To save space, simple 
acknowledgments by the system are not shown.
Figure 1: PLOW learning a task
1
Evaluation
The PLOW system was evaluated by independent 
evaluators who considered four task learning 
systems developed in the CALO project. There 
were 16 human subjects who received training on 
each of the systems and who worked through a 
number of successful scripted training sessions 
with each. They were then given ten new 
problems, ranging from slight variations to 
problems they had seen to problems that were 
substantially new. They were free to choose which 
problems to work on and which system to use and 
the resulting tasks learned were tested with 
different settings of the parameters and scored out 
of a total of 4 points based on a complex 
predefined evaluation criteria (not known to the 
developers). The PLOW system did well in the 
test, not only receiving the highest average score 
on tasks learned by a system (figure 3) but also 
was strongly preferred by the users and selected 
more than half the time (figure 4). 
The Demonstration
If we are allowed a presentation we will 
demonstrate PLOW live on a task selected by the 
audience. In addition, we would like to have the 
system available for an extended period of time 
during the conference so that attendees can spend 
time using the system to teach it simple tasks. The 
system runs on a laptop and all that is needed for 
a demo is internet access.
Acknowledgements & References
This work was supported by DARPA grant NBCH-
D-03-0010 under a subcontract from SRI International, 
ONR grant N000140510314, and NSF grant5-28096.
Chambers, N. et al (2006). Using Semantics to Identify 
Web Objects. Proceedings AAAI.
Jung, H., J. Allen, et al (2006). One-Shot Proce-dure 
Learning from Instruction and Observation. FLAIRS, 
Melbourne, FL. 
Lent, M. and Laird, J. (2001)  Learning Procedural 
Knowledge through Observation,  Proc.  of the Intl 
Conf. on Knowledge Capture.
   Figure 3: Average score (out of 4)   Figure 4: User preference for systems (55 trials)
U: Let me show you how to find someone's publications 
U: Go to this webpage 
Navigates to http://scholar.google.com
U: Click on the advanced search link 
Clicks Advanced Scholar Search 
U: Put the author's name here 
Types the name in the Author text field 
U: Click the search button 
Clicks on the Search Scholar button 
U: OK here is the list of results 
Selects the table on page
PLOW: Can you check that I understand the data. 
PLOW: puts up a new window with the results. 
U: OK this is good.
U: this is the article title 
Highlights a title 
PLOW: attempts to find all titles and displays them
U: This is wrong.
PLOW OK
U: This is another article title 
Highlights another correct title 
PLOW: recomputes all the titles and displays them
Switches back to Scholar window
U: OK click next to get more results
Clicks the Next link 
PLOW: When should I stop searching?
U: Get thirty results 
PLOW: OK. I will do that
PLOW: iterates through pages until it gets 30 results
PLOW: I?m done.
U: We're done with the demonstration 
U: Let's practice
PLOW: what is the name of the author?
Enters a new name
PLOW executes the procedure successfully!!
Figure 2: Learning to find titles of an author?s publications (U = user)
2
Proceedings of the ACL Interactive Poster and Demonstration Sessions,
pages 85?88, Ann Arbor, June 2005. c?2005 Association for Computational Linguistics
Two diverse systems built using
generic components for spoken dialogue
(Recent Progress on TRIPS)
James Allen, George Ferguson, Mary Swift, Amanda Stent, Scott Stoness, 
Lucian Galescu, Nathan Chambers, Ellen Campana, and Gregory Aist
University of Rochester
Computer Science Department
UR Comp Sci RC 270226
Rochester NY 14627 USA
{james, ferguson, swift, stoness,
campana, gaist}
@cs.rochester.edu
Institute for
Human and Machine Cognition
40 South Alcaniz St.
Pensacola FL 32502
{lgalescu,nchambers}@ihmc.us
State University of New York at
Stony Brook
1418 Computer Science
Stony Brook University
Stony Brook NY 11794 USA
stent@cs.sunysb.edu
Abstract
This  paper  describes  recent  progress  on  the
TRIPS architecture for developing spoken-lan-
guage dialogue systems.  The interactive poster
session will include demonstrations of two sys-
tems built using TRIPS: a computer purchas-
ing assistant, and an object placement (and ma-
nipulation) task.
1 Introduction
Building a robust spoken dialogue system for a new
task currently requires considerable effort,  includ-
ing  extensive  data  collection,  grammar  develop-
ment, and building a dialogue manager that drives
the  system using its  "back-end" application (e.g.
database query, planning and scheduling). We de-
scribe progress in an effort to build a generic dia-
logue system that  can be rapidly customized to a
wide range of different types of applications, pri-
marily  by  defining a  domain-specific  task  model
and the interfaces to the back-end systems. This is
achieved by  using generic  components  (i.e.,  ones
that apply in any practical domain) for all stages of
understanding  and developing techniques for rapid-
ly customizing the generic components to new do-
mains  (e.g.  Aist,  Allen,  and  Galescu  2004).  To
achieve this goal we have made several innovations,
including (1) developing domain independent mod-
els of  semantic and  contextual  interpretation,  (2)
developing generic  dialogue  management  compo-
nents based on an abstract  model of collaborative
problem solving, and (3) extensively using an ontol-
ogy-mapping system that connects the domain inde-
pendent representations to the representations/query
languages used by the back-end applications,  and
which is used to automatically optimize the perfor-
mance of the system in the specific domain.
2 Theoretical  Underpinnings:  The Prob-
lem-Solving Model of Dialogue
While many have observed that communication
is a specialized form of joint action that happens to
involve language and that dialogue can be viewed
as collaborative problem solving, very few imple-
mented systems have been explicitly based on these
ideas. Theories of speech act interpretation as inten-
tion recognition have been developed  (including ex-
tensive  prior  work  in  TRIPS'  predecessor,  the
TRAINS project), but have been generally consid-
ered impractical for actual systems.  Planning mod-
els  have been more successful  on the  generation
side, and some systems have used the notion of exe-
cuting explicit task models to track and drive the in-
teractions  (e.g.,  Sidner  and  Rich's  COLLAGEN
framework). But collaborative problem solving, and
dialogue in general, is much more general than exe-
cuting tasks. In our applications, in addition to exe-
cuting tasks, we see dialogue that is used to define
the task (i.e., collaborative planning), evaluate the
task (e.g., estimating how long it will take,  com-
paring options,  or  likely effects),    debug a  task
(e.g., identifying and discussing problems and how
to remedy them), learn new tasks (e.g., by demon-
stration and instruction).
85
In the remainder of the paper, we'll first discuss
the methods we've developed for building dialogue
systems using generic components.  We'll then de-
scribe two systems implemented using the TRIPS
architecture that we will demonstrate at the interac-
tive poster session.
3 Generic Methods:  Ontology Mappings
and Collaborative Problem Solving
The goal of our work is to develop generic spoken
dialogue technology that can be rapidly customized
to new applications, tasks and domains. To do this,
we have developed generic domain independent rep-
resentations not only of sentence meaning but also
of the collaborative actions that are performed by
the speech acts as one engages in dialogue. Further-
more, we need to be able to easily connect these
generic representations to a wide range of different
domain specific task models and applications, rang-
ing from data base query systems to state-of-the-art
planning and scheduling systems.  This  paper  de-
scribes  the  approach  we  have  developed  in  the
TRIPS system. TRIPS is now being used in a wide
range of diverse applications, from interactive plan-
ning (e.g., developing evacuation plans), advice giv-
ing  (e.g.,  a  medication  advisor  (Ferguson  et  al.
2002)),  controlling teams of robots,   collaborative
assistance (e.g., an assistant that can help you pur-
chase a computer, as described in this paper), sup-
porting human learning, and most recently having
the computer  learn (or  be  taught)  tasks,  such as
learning to perform tasks on the web.  Even though
the tasks and domains differ dramatically, these ap-
plications use the same set of core understanding
components. 
The key to supporting such a range of tasks and ap-
plications is the use of a general ontology-mapping
system. This allows the developer to express a set
of mapping rules that translate the generic knowl-
edge representation into the specific representations
used by the back-end applications (called the KR
representation).   In  order  to  support  generic dis-
course processing, we represent these mappings as
a chain of simpler transformations. These represen-
tations are thus transformed in several stages. The
first,  using the ontology mapping rules,  maps the
LF representation into an intermediary representa-
tion (AKRL - the abstract KR language) that has a
generic syntax  but  whose content is  expressed in
terms of the KR ontology. The second stage is a
syntactic transformation that occurs at the time that
calls to the back-end applications actually occur so
that  interactions  occur  in  the  representations  the
back-end expects.   In  addition to  using ontology
mapping to  deal  with the representational  issues,
TRIPS is unique in that it uses a generic model of
collaborative problem solving to drive the dialogue
itself  (e.g.  Allen,  Blaylock,  and  Ferguson 2002).
This model forms the basis of a generic component
(the collaboration manager) that supports both in-
tention recognition to identify the intended speech
acts and their content, planning the system's actions
to respond to the user (or that take initiative), and
providing utterance realization goals to the genera-
tion system. To develop this, we have been develop-
ing  a  generic  ontology  of  collaborative  problem
solving acts, which provide the framework for man-
aging  the  dialogue.  The  collaboration  manager
queries a domain-specific task component in order
to  make  decisions  about  interpretations  and  re-
sponses.
4 TRIPS  Spoken  Dialogue  Interface  to
the CALO Purchasing Assistant 
The CALO project is a large multisite effort which
aims  at  building  a  computerized  assistant  that
learns how to help you with day-to-day tasks. The
overarching goal of the CALO project is to 
... create cognitive software systems, that is,
systems that can reason, learn from experi-
ence, be told what to do, explain what they
are doing, reflect on their experience, and re-
spond robustly to surprise (Mark and Per-
rault 2004). 
Within this broad mandate, one of our current areas
of focus is user-system dialogue regarding the task
of purchasing - including eliciting user needs, de-
scribing possibilities, and reviewing & finalizing a
purchase  decision.  (Not  necessarily  as  discrete
stages; these elements may be interleaved as appro-
priate for the specific item(s) and setting.)  Within
the purchasing domain,  we began with computer
purchasing and have branched out to other equip-
ment such as projectors.
How to help with purchasing? The family of tasks
involving purchasing items online, regardless of the
type of item, have a  number of elements in com-
mon. The process of purchasing has some common
86
dialogue elements - reporting on the range of fea-
tures  available,  allowing the user  to specify con-
straints, and so forth.  Also, regarding the goal that
must be reached at the end of the task, the eventual
item must:
Meet requirements.  The item needs to meet some
sort of user expectations. This could be as arbitrary
as a specific part number, or as compositional - and
amenable to machine understanding -  as  a  set  of
physical  dimensions (length,  width,  height,  mass,
etc.) 
Be approved. Either the system will have the au-
thority to approve it (cf. Amazon's one-click order-
ing system), or more commonly the user will review
and confirm the purchase. In an office environment
the approval process may extend to include review
by a supervisor, such as might happen with an item
costing over (say) $1000. 
Be available. (At  one time a  certain  electronics
store in California had the habit of leaving out floor
models of laptops beyond the point where any were
actually available for sale.  (Perhaps to entice the
unwitting customer into an ?upsale?, that is, buying
a  similar  but  more  expensive  computer.))  On  a
more serious note, computer specifications change
rapidly, and so access to online information about
available  computers  (provided  by  other  research
within CALO) would be important in order to en-
sure that the user can actually order the machine he
or she has indicated a preference for.  
At  the interactive poster  session,  we will demon-
strate some of the current spoken dialogue capabili-
ty related to the CALO task of purchasing equip-
ment.  We will demonstrate a number of the aspects
of the system such as initiating a conversation, dis-
cussing specific requirements,  presenting possible
equipment to purchase,  system-initiated reminders
to ask for supervisor approval for large purchases,
and finalizing a decision to purchase. 
Figure 1. Fruit carts display.
87
5 TRIPS  Spoken  Dialogue  Interface  to
choosing,  placing,  painting,  rotating,
and filling (virtual) fruit carts
TRIPS is versatile in its applications, as we've said
previously.  We hope to also demonstrate an inter-
face to  a  system for  using spoken commands to
modifying, manipulating, and placing objects on a
computer-displayed map.  This  system (aka  ?fruit
carts?)  extends  the  TRIPS  architecture  into  the
realm of continuous understanding.  That is, when
state-of-the-art  dialogue systems listen,  they typi-
cally wait for the end of the utterance before decid-
ing what to do.  People on the other hand do not
wait in this way ? they can act on partial informa-
tion as  it  becomes available.   A classic example
comes  from  M.  Tanenhaus  and  colleagues  at
Rochester: when presented with several objects of
various colors and told to ?click on the yel-?, people
will already tend to be looking relatively more at the
yellow object(s) even before the word ?yellow? has
been completed.  To achieve this type of interactivi-
ty with a dialogue system ? at least at the level of
two or three words at a time, if not parts of words ?
imposes some interesting challenges. For example:
1. Information must flow asynchronously between
dialogue components, so that actions can be trig-
gered based on partial utterances even while the
understanding continues
2. There must be reasonable representations of in-
complete information ? not just ?incomplete sen-
tence?,  but  specifying what  is  present  already
and perhaps what may potentially follow
3. Speech  recognition,  utterance  segmentation,
parsing, interpretation, discourse reasoning, and
actions must all be able to happen in real time
The fruit carts system consists of two main compo-
nents:  first,  a  graphical  interface implemented on
Windows  2000  using  the  .NET  framework,  and
connected to  a  high-quality  eyetracker;  second,  a
TRIPS-driven spoken dialogue interface implement-
ed primarily in LISP.   The actions in this domain
are as follows:
1. Select an object (?take the large plain square?)
2. Move it (?move it to central park?)
3. Rotate  it  (?and then turn  it  left  a  bit  ?  that's
good?)
4. Paint it (?and that one needs to be purple?)
5. Fill it (?and there's a grapefruit inside it?)
Figure 1 shows an example screenshot from the
fruit carts visual display. The natural language in-
teraction  is  designed to  handle  various  ways  of
speaking,  including conventional  definite  descrip-
tions (?move the large square to central park?) and
more interactive language such as (?up towards the
flag pole ? right a bit ? more ? um- stop there.?)
6 Conclusion
In this brief paper,  we have described some of
the recent progress on the TRIPS platform.  In par-
ticular we have focused on two systems developed
in TRIPS: a spoken dialogue interface to a mixed-
initiative purchasing assistant, and a spoken inter-
face for exploring continuous understanding in an
object-placement task.  In  both  cases  the  systems
make use of reusable components ? for input and
output  such as  parsing and speech synthesis,  and
also for dialogue functionality such as mapping be-
tween language,  abstract  semantics,  and  specific
representations for each domain.
References 
Aist,  G.  2004.  Speech,  gaze,  and  mouse  data  from
choosing,  placing,  painting,  rotating,  and  filling
(virtual) vending carts. International Committee for
Co-ordination  and  Standardisation  of  Speech
Databases  (COCOSDA)  2004  Workshop,  Jeju  Is-
land, Korea, October 4, 2004. 
Aist, G.S., Allen, J., and Galescu, L. 2004. Expanding
the linguistic coverage of a spoken dialogue system
by mining human-human dialogue for new sentences
with familiar meanings. Member Abstract, 26th An-
nual  Meeting  of  the  Cognitive  Science  Society,
Chicago, August 5-7, 2004. 
James Allen, Nate Blaylock, and George Ferguson. A
problem-solving model for collaborative agents.  In
First International Joint Conference on Autonomous
Agents and Multiagent Systems, Bologna, Italy, July
15-19 2002. 
George  Ferguson,  James  F.  Allen,  Nate  J.  Blaylock,
Donna K. Byron, Nate W. Chambers, Myrsolava O.
Dzikovska, Lucian Galescu, Xipeng Shen, Robert S.
Swier, and Mary D. Swift.  The Medication Advisor
Project: Preliminary Report, Technical Report 776,
Computer  Science  Dept.,  University  of  Rochester,
May 2002. 
Mark,  B.,  and  Perrault,  R.  (principal  investigators).
2004.  Website for Cognitive Assistant  that  Learns
and Organizes. http://www.ai.sri.com/project/CALO
88
Synchronization in an Asynchronous Agent-based Architecture
for Dialogue Systems
Nate Blaylock and James Allen and George Ferguson
Department of Computer Science
University of Rochester
Rochester, New York 14627
USA
{blaylock,james,ferguson}@cs.rochester.edu
Abstract
Most dialogue architectures are ei-
ther pipelined or, if agent-based,
are restricted to a pipelined flow-
of-information. The TRIPS di-
alogue architecture is agent-based
and asynchronous, with several lay-
ers of information flow. We present
this architecture and the synchro-
nization issues we encountered in
building a truly distributed, agent-
based dialogue architecture.
1 Introduction
More and more people are building dia-
logue systems. Architecturally, these sys-
tems tend to fall into two camps: those with
pipelined architectures (e.g., (Lamel et al,
1998; Nakano et al, 1999)), and those with
agent-based architectures (e.g., (Seneff et al,
1999; Stent et al, 1999; Rudnicky et al,
1999)). Agent-based architectures are advan-
tageous because they free up system com-
ponents to potentially act in a more asyn-
chronous manner. However, in practice, most
dialogue systems built on an agent-based ar-
chitecture pass messages such that they are
basically functioning in terms of a pipelined
flow-of-information.
Our original implementation of the TRIPS
spoken dialogue system (Ferguson and Allen,
1998) was such an agent-based, pipelined
flow-of-information system. Recently, how-
ever, we made changes to the system (Allen
et al, 2001a) which allow it to take advan-
tage of the distributed nature of an agent-
based system. Instead of system components
passing information in a pipelined manner
(interpretation ? discourse management ?
generation), we allow the subsystems of in-
terpretation, behavior (reasoning and acting)
and generation to work asynchronously. This
makes the TRIPS system truly distributed
and agent-based.
The driving forces behind these changes are
to provide a framework for incremental and
asynchronous language processing, and to al-
low for a mixed-initiative system at the task
level. We describe these motivations briefly
here.
Incremental Language Processing In a
pipelined (or pipelined flow-of-information)
system, generation does not occur until af-
ter both the interpretation and reasoning pro-
cesses have completed. This constraint is
not present in human-human dialogue as ev-
idenced by the presence of grounding, ut-
terance acknowledgment, and interruptions.
Making interpretation, behavior, and gener-
ation asynchronous allows, for example, the
system to acknowledge a question while it is
still working on finding the answer.
Mixed-initiative Interaction Although
pipelined systems allow the system to take
discourse-level initiative (cf. (Chu-Caroll and
Brown, 1997)), it is difficult to see how they
could allow the system to take task-level ini-
tiative in a principled way. In most systems,
reasoning and action are driven mostly by in-
terpreted input (i.e., they are reactive to the
user?s utterances). In a mixed-initiative sys-
tem, the system?s response should be deter-
mined not only by user input, but also system
goals and obligations, as well as exogenous
        Philadelphia, July 2002, pp. 1-10.  Association for Computational Linguistics.
                  Proceedings of the Third SIGdial Workshop on Discourse and Dialogue,
events. For example, a system with an asyn-
chronous behavior subsystem can inform the
user of a new, important event, regardless of
whether it is tied to the user?s last utterance.
On the other hand, in the extreme version of
pipelined flow-of-control, behavior cannot do
anything until the user says something, which
is the only way to get the pipeline flowing.
The reasons for our changes are described
in further detail in (Allen et al, 2001a). In
this paper, we focus on the issues we encoun-
tered in developing an asynchronous agent-
based dialogue system and their respective so-
lutions, which turn out to be highly related to
the process of grounding.
We first describe the general TRIPS archi-
tecture and information flow and then discuss
the various points of synchronization within
the system. We then discuss what these is-
sues mean in general for the implementation
of an asynchronous agent-based system.
2 TRIPS Architecture
As mentioned above, the TRIPS system1
(Allen et al, 2000; Allen et al, 2001a; Allen
et al, 2001b) is built on an agent-based ar-
chitecture. Unlike many systems, however,
the flow of information within TRIPS is not
pipelined. The architecture and information
flow between components is shown in Fig-
ure 1. In TRIPS, information flows between
the three general areas of interpretation, be-
havior, and generation.
Each TRIPS component is implemented as
a separate process. Information is shared by
passing KQML (Finin et al, 1997) messages
through a central hub, the Facilitator, which
supports message logging and syntax checking
as well as broadcast and selective broadcast
between components.
We first discuss the individual system com-
ponents and their functions. We then de-
scribe the flow of information through the sys-
tem and illustrate it with an example.
1Further details of the TRIPS dia-
logue system can be found at our website:
http://www.cs.rochester.edu/research/cisd/
Behavioral
Agent
Interpretation
Manager
Generation
Manager
Parser
Speech
Planner Scheduler Monitors Events
Task- and Domain-specific
Knowledge Sources Exogenous Event Sources
Response
Planner
GraphicsSpeech
Task
Manager
Reference
Discourse
Context
Interpretation
Generation
Behavior
Task
Interpretation
Requests
Problem-Solving
Acts recognized
from user
Problem-Solving
Acts
to perform
Task
Execution
Requests
Figure 1: The TRIPS Architecture (Allen et
al., 2001a)
2.1 System Components
Figure 1 shows the various components in
the TRIPS system. Components are divided
among three main categories: Interpretation,
Behavior, and Generation. As shown in the
figure, some components straddle categories,
meaning they represent state and provide
services necessary for both sorts of process-
ing. The Interpretation Manager (IM) in-
terprets user input coming from the various
modality processors as it arises. It inter-
acts with Reference to resolve referring ex-
pressions and with the Task Manager (TM)
to perform plan and intention recognition, as
part of the interpretation process. It broad-
casts recognized speech acts and their inter-
pretation as collaborative problem solving ac-
tions (see below), and incrementally updates
the Discourse Context (DC). The Behavioral
Agent (BA) is in some sense the autonomous
?heart? of the agent. It plans system be-
havior based on its own goals and obliga-
tions, the user?s utterances and actions, and
changes in the world state. Actions that re-
quire task- and domain-dependent processing
are performed by the Task Manager. Ac-
tions that involve communication and collab-
oration with the user are sent to the Gener-
ation Manager (GM) in the form of commu-
nicative acts. The GM coordinates planning
the specific content of utterances and display
updates and producing the results. Its behav-
ior is driven by discourse obligations (from the
DC), and the directives it receives from the
BA.
2.1.1 Collaborative Problem Solving
Model
The three main components (IM, BA, GM)
communicate using messages based on a col-
laborative problem solving model of dia-
logue (Allen et al, 2002; Blaylock, 2002).
We model dialogue as collaboration between
agents which are planning and acting. To-
gether, collaborating agents (i.e., dialogue
partners) build and execute plans, deciding on
such things as objectives, recipes, resources,
situations (facts about the world), and so
forth. These are called collaborative problem
solving objects, and are operated on by col-
laborative problem solving acts such as iden-
tity (present as a possibility), evaluate, adopt,
and others. Thus, together, two agents may
decide to adopt a certain objective, or iden-
tify a recipe to use for an objective. The
agreed-upon beliefs, objectives, recipes, and
so forth constitute the collaborative problem
solving state.
Of course, because the agents are au-
tonomous, no agent can single-handedly
change the collaborative problem solving
(CPS) state. Interaction acts are actions that
a single agent performs to attempt to change
the CPS state. The interaction acts are ini-
tiate, continue, complete, and reject. Initi-
ate proposes a new change to the CPS state.
Continue adds new information to the pro-
posal, and complete simply accepts the pro-
posal (bringing about the change), without
adding additional information. Of course,
proposals can be rejected at any time, causing
them to fail.
As an example, the utterance ?Let?s
save the heart-attack victim in Pitts-
ford? in an emergency planning domain
would be interpreted as two interaction
acts: (initiate (identify objective
(rescue person1))) and (initiate
(adopt objective (rescue person1))).
Here the user is proposing that they consider
rescuing person1 as a possible objective to
pursue. He is also proposing that they adopt
it as an objective to plan for.2
Interaction acts are recognized (via inten-
tion recognition) from speech acts. Inter-
action acts and speech acts differ in several
ways. First, speech acts describe a linguistic
level of interaction (ask, tell, etc.), whereas
interaction acts deal with a problem solving
level (adopting objectives, evaluating recipes
and so forth). Also, as shown above, a single
speech act may correspond to many interac-
tion acts.
2.2 Information Flow in the System
There are several paths along which informa-
tion asynchronously flows through the sys-
tem. We discuss information flow at the levels
of problem solving, discourse, and grounding.
The section that follows then gives an exam-
ple of how this proceeds.
2.2.1 Problem Solving Level
The problem solving level describes the ac-
tual underlying task or purposes of the di-
alogue and is based on interaction acts. We
first describe the problem solving information
flow when the user makes an utterance. We
then discuss the case where the system takes
initiative and how this results in an utterance
by the system.
User Utterance Following the diagram in
Figure 1, when a user makes an utterance,
it goes through the Speech Recognizer to the
Parser, which then outputs a list of speech
acts (which cover the input) to the Interpre-
tation Manager (IM). The IM then sends the
speech acts to Reference for resolution.
2Here two interaction acts are posited because of
the ability of the system to react to each separately,
for example completing the first, but rejecting the sec-
ond. Consider the possible response ?No, not right
now.? (accept this as a possible objective, but re-
ject adopting it right now), versus ?The 911 center
in Pittsford is handling that, we don?t have to worry
about it.? (reject this as even a possible objective and
reject adopting it). The scope of this paper precludes
us from giving more detail about multiple interaction
acts.
The IM then sends these speech act hy-
potheses to the Task Manager (TM), which
computes the corresponding interaction acts
for each as well as a confidence score that each
hypothesis is the correct interpretation.
Based on this, the IM then chooses the
best interpretation and broadcasts3 the cho-
sen CPS act(s) in a ?system understood? mes-
sage. The TM receives this message and
updates to the new collaborative problem
solving state which this interpretation en-
tails. The Behavioral Agent (BA) receives the
broadcast and decides if it wants to form any
intentions for action based on the interaction
act.
Assuming the BA decides to act on the
user?s utterance, it sends execution and rea-
soning requests to the TM, which passes them
on to the appropriate back-end components
and returns the result to the BA.
The BA then forms an interaction act based
on this result and sends it to the GM to be
communicated to the user. The GM then gen-
erates text and/or graphical updates based on
the interaction act and utters/presents them
to the user.
In most pipelined and pipelined flow-of-
information systems, the only flow of infor-
mation is at this problem solving level. In
TRIPS, however, there are other paths of in-
formation flow.
System Initiative TRIPS is also capable
of taking initiative. As we stated above, this
initiative originates in the BA and can come
from one of three areas: user utterances, pri-
vate system objectives, or exogenous events.
If the system, say because of an exogenous
event, decides to take initiative and commu-
nicate with the user, it sends an interaction
act to the GM. The GM then, following the
same path as above, outputs content to the
user.
3This is a selective broadcasts to the components
which have registered for such messages.
2.2.2 Discourse Level
The discourse level4 describes information
which is not directly related to the task at
hand, but rather is linguistic in nature. This
information is represented as salience infor-
mation (for Reference) and discourse obliga-
tions (Traum and Allen, 1994).
When the user makes an utterance, the in-
put passes (as detailed above) through the
Speech Recognizer, to the Parser, and then to
the IM, which calls Reference to do resolution.
Based on this reference resolved form, the IM
computes any discourse obligations which the
utterance entails (e.g., if the utterance was
a question, to address or answer it, also, to
acknowledge that it heard the question).
At this point, the IM broadcasts an ?sys-
tem heard? message, which includes incurred
discourse obligations and changes in salience.
Upon receipt of this message, Discourse Con-
text updates its discourse obligations and Ref-
erence updates its salience information.
The GM learns of new discourse obligations
from the Discourse Context and begins to try
to fulfill them, regardless of whether or not
it has heard from the BA about the prob-
lem solving side of things. However, there
are some obligations it will be unable to ful-
fill without knowledge of what is happening
at the problem solving level ? answering or
addressing the question, for example. How-
ever, other obligations can be fulfilled without
problem solving knowledge ? an acknowledg-
ment, for example ? in which case, the GM
produces content to fulfill the discourse obli-
gation.
If the GM receives interaction acts and
discourse obligations simultaneously, it must
produce content which fulfills both problem
solving and discourse needs. Usually, these
interaction acts and discourse obligations are
towards the same objective ? an obligation
to address or answer a question, and an inter-
action act of identifying a situation (commu-
4Although it works in a conceptually similar way,
the current system does not handle discourse level in-
formation flow quite so cleanly as is presented here.
We intend to clean things up and move to this exact
model in the near future.
nicating the answer to the user), for example.
However, because the system has the ability
to take initiative, these interaction acts and
discourse obligations may be disparate ? an
obligation to address or answer a question and
an interaction act to identify and adopt a new
pressing objective, for example. In this case,
the GM must plan content to fulfill the acts
and obligations the best it can ? apologize
for not answering the question and then in-
forming the user, for example. Through this
method, the GM maintains dialogue coher-
ence even though the BA is autonomous.
2.2.3 Grounding Level
The last level of information flow is at the
level that we loosely call grounding (Clark
and Schaefer, 1989; Traum, 1994).5 In
TRIPS, acts and obligations are not accom-
plished and contexts are not updated unless
the user has heard and/or understood the sys-
tem?s utterance.6
Upon receiving a new utterance, the IM
first determines if it contains evidence of the
user having heard and understood the utter-
ance.7 If the user heard and understood, the
IM broadcasts a ?user heard? message which
contains both salience information from the
previous system utterance as well as what dis-
course obligations the system utterance ful-
filled. This message can be used by Reference
to update salience information and by Dis-
course Context to discharge fulfilled discourse
obligations.
It is important that these contexts not be
updated until the system know that the user
heard its last utterance. If the user for ex-
ample, walks away as the system speaks, the
system?s discourse obligations will still not
fulfilled, and salience information will not
5TRIPS only uses a small subset of Traum?s
grounding model. In practice, however, this has not
presented problems thus far.
6The acceptance or rejection of the actual content
of an utterance is handled by our collaborative prob-
lem solving model (Allen et al, 2002; Blaylock, 2002)
and is not further discussed here.
7Hearing and understanding are not currently rec-
ognized separately in the system. For future work, we
would like to extend the system to handle them sepa-
rately (e.g., the case of the user having heard but not
understood).
change.
The GM receives the ?user heard? mes-
sage and also knows which interaction act(s)
the system utterance was presenting. It
then broadcasts a ?user understood? message,
which causes the TM to update the collabo-
rative problem solving state, and the BA to
release any goals and intentions fulfilled by
the interaction act(s).
Again, it is important that these context
updates do not occur until the system has ev-
idence that the user understood its last utter-
ance (for reasons similar to those discussed
above).
This handling of grounding frees the sys-
tem from the assumptions that the user al-
ways hears and understands each utterance.
2.3 An Example
We use here an example from our TRIPS
Medication Advisor domain ((Ferguson et al,
2002)). The Medication Advisor is a project
carried out in conjunction with the Cen-
ter for Future Health at the University of
Rochester.8 The system is designed to help
people (especially the elderly) understand and
manage their prescription medications.
With the huge growth in the number of
pharmaceutical therapies, patients tend to
end up taking a combination of several differ-
ent drugs, each of which has its own charac-
teristics and requirements. For example, each
drug needs to be taken at a certain rate: once
a day, every four hours, as needed, and so on.
Some drugs need to be taken on an empty
stomach, others with milk, others before or
after meals, and so on. Overwhelmed with
this large set of complex interactions many
patients simply do not (or cannot) comply
with their prescribed drug regimen (Claxton
et al, 2001).
The TRIPS Medication Advisor is designed
to help alleviate this problem by giving pa-
tients easy and accessible prescription infor-
mation an management in their own home.
For our example, we assume that a dialogue
between the system and user is in progress,
8http://www.centerforfuturehealth.org
and a number of other topics have been ad-
dressed. At this certain point in the conver-
sation, the system has just uttered ?Thanks,
I?ll try that? and now the user utters the fol-
lowing:
User: ?Can I take an aspirin??
We trace information flow first at the
grounding level, then at the discourse level,
and finally at the problem solving level. This
information flow is illustrated in Figure 2.
Grounding Level The utterance goes
through the Speech Recognizer and Parser to
the IM. As illustrated in Figure 2a, based on
the utterance, the IM recognizes that the user
heard and understood the system?s last ut-
terance, so it sends a ?user heard? message,
which causes the Discourse Context to update
discourse obligations and Reference to update
salience based on the system?s last utterance.
The GM receives the ?user heard? mes-
sage and sends the corresponding ?user un-
derstood? message, containing the interaction
act(s) motivating the system?s last utterance.
Upon receiving this message, the TM updates
the collaborative problem solving state, and
the BA updates its intentions and goals.
Meanwhile ... things have been happening
at the discourse level.
Discourse Level After the IM sends the
?user heard? message, as shown in Figure 2b,
it sends Reference a request to resolve refer-
ences within the user?s utterance. It then rec-
ognizes that the user has asked a question,
which gives the system the discourse obliga-
tions of answering (or addressing) the ques-
tion, as well as acknowledging the question.
The IM then sends a ?system heard?
message, which causes Reference to update
salience and Discourse Context to store the
newly-incurred discourse obligations.
The GM receives the new discourse obliga-
tions, but has not yet received anything from
the BA about problem solving (see below).
Without knowledge of what is happening in
problem solving, the GM is unable to ful-
fill the discourse obligation to answer (or ad-
dress) the question. However, it is able to ful-
fill the obligation of acknowledging the ques-
tion, so, after a certain delay of no response
from the BA, the GM plans content to pro-
duce an acknowledgment, which causes the
avatar9 to graphically show that it is think-
ing, and also causes the system to utter the
following:
System: ?Hang on.?
Meanwhile ... things have been happening
at the problem solving level as well.
Problem Solving Level After it sends the
?system heard? message, as shown in Fig-
ure 2c, the IM computes possible speech acts
for the input. In this case, there are two: a
yes-no question about the ability to take as-
pirin and a request to evaluate the action of
taking aspirin.
These are sent to the TM for intention
recognition. The first case (the yes-no ques-
tion) does not seem to fit the task model well
and receives a low score. (The system prefers
interpretations in which the user wants infor-
mation for a reason and not just for the sake
of knowing something.) The second speech
act is recognized as an initiate of an evalua-
tion of the action of taking aspirin (i.e., the
user wants to evaluate this action with the
system). This hypothesis receives a higher
score.
The IM chooses the second interpretation
and broadcasts a ?system understood? mes-
sage that announces this interpretation. The
TM receives this message and updates its
collaborative problem solving state to reflect
that the user did this interaction act. The
BA receives the message and, as shown in
Figure 2d, decides to adopt the intention of
doing the evaluation and reporting it to the
user. It sends an evaluation request for the ac-
tion of the user taking an aspirin to the TM,
which queries the back-end components (user
knowledge-base and medication knowledge-
base) about what prescriptions the user has
and if any of them interact with aspirin.
9The TRIPSMedication Advisor avatar is a talking
capsule whose top half rotates when it is thinking.
IM Ref
TM BA
GM
User understood (0)
User heard (0)
IM Ref
TM BA
GM
IM Ref
TM BA
GMResolveReply
System heard (1);Obligation to AckObligation to Answer
?Hang on? (2)
Address obligto Ack
System understood (1);CPS Act: evaluate-action
Interpret
Reply
IM Ref
TM BA
GM
?No, you are taking? (3)
Address obligto Answer
Perform PS Act
Result
Inform userof result
S: Thanks, I?ll try that.        (0)U: Can I take an aspirin?     (1)
(a) (b)
(c) (d)
Figure 2: Flow of Information for the Utterance ?Can I take an aspirin?? (a) Grounding Level,
(b) Discourse Level, (c) and (d) Problem-Solving Level
The back-end components report that the
user has a prescription for Celebrex, and that
Celebrex interacts with aspirin. The TM then
reports to the BA that the action is a bad
idea.
The BA then formulates an interaction act
reflecting these facts and sends it to the GM.
The GM then produces the following utter-
ance, which performs the interaction act as
well as fulfills the discourse obligation of re-
sponding to the question.
System: ?No, you are taking Celebrex
and Celebrex interacts with
aspirin.?
3 Synchronization
The architecture above is somewhat idealized
in that we have not yet given the details of
how the components know which context to
interpret messages in and how to ensure that
messages get to components in the right or-
der.
We first illustrate these problems by giving
a few examples. We then discuss the solution
we have implemented.
3.1 Examples of Synchronization
Problems
One of the problems that faces most dis-
tributed systems is that there is no shared
state between the agents. The first problem
with the architecture described in Section 2 is
the lack of context in which to interpret mes-
sages. This is well illustrated by the interpret
request from the IM to the TM.
As discussed above, the IM sends its candi-
date speech acts to the TM, which performs
intention recognition and assigns a score. The
problem is, in which context should the TM
interpret utterances? It cannot simply change
its collaborative problem solving state each
time it performs intention recognition, since it
may get multiple requests from the IM, only
one of which gets chosen to be the official ?in-
terpretation? of the system.
We have stated that the TM updates its
context each time it receives a ?system under-
stood? or ?user understood? message. This
brings up, however, the second problem of
our distributed system. Because all compo-
nents are operating asynchronously (includ-
ing the user, we may add), it is impossible
to guarantee that messages will arrive at a
component in the desired order. This is be-
cause ?desired order? is a purely pragmatic
assessment. Even with a centralized Facili-
tator through which all messages must pass,
the only guarantee is that messages from a
particular component to a particular compo-
nent will arrive in order; i.e., if component A
sends component B three messages, they will
get there in the order that component A sent
them. However, if components A and C each
send component B a message, we cannot say
which will arrive at component B first.
What this means is that the ?current? con-
text of the IM may be very different from that
of the TM. Consider the case where the sys-
tem has just made an utterance and the user
is responding. As we describe above, the first
thing the IM does is check for hearing and un-
derstanding and sends off a ?user heard? mes-
sage. The GM, when it receives this message,
sends the corresponding ?user understood?
message, which causes the TM to update to a
context containing the system?s utterance.
In the meantime, the IM is assuming the
context of the systems last utterance, as it
does interpretation. It then sends off inter-
pret requests to the TM. Now, if the TM re-
ceives an interpret request from the IM be-
fore it receives the ?user understood? message
from the GM, it will try to interpret the in-
put in the context of the user?s last utterance
(as if the user had made two utterance in a
row, without the system saying anything in
between). This situation will give erroneous
results and must be avoided.
3.2 Synchronization Solution
The solution to these problems is, of course,
synchronization: causing components to wait
at certain stages to make sure they are in
the same context. It is interesting to note
that these synchronization points are highly
related to a theory of grounding and common
ground.
To solve the first problem listed above (lack
of context), we have components append con-
text assumptions to the end of each message.
Thus, instead of the IM sending the TM a
request to interpret B, it sends the TM a re-
quest to interpret B in the context of hav-
ing understood A. Likewise, instead of the
IM requesting that Reference resolve D, it re-
quests that Reference resolve D having heard
C. Having messages explicitly contain con-
text assumptions allows components to inter-
pret messages in the correct context.
With this model, context now becomes dis-
crete, incrementing with every ?chunk? of
common ground.10 These common ground
updates correspond exactly to the ?heard?
and ?understood? messages we described
above. Thus, in order to perform a certain
task (reference resolution, intention recogni-
tion, etc.), a component must know in which
common ground context it must be done.
The solution to the second problem (mes-
sage ordering) follows from explicitly listing
context assumptions. If a component receives
a message that is appended with a context
about which the component hasn?t received
an update notice (the ?heard? or ?under-
stood? message), the component simply de-
fers processing of the message until it has re-
ceived the corresponding update message and
can update its context. This ensures that, al-
though messages may not be guaranteed to
arrive in the right order, they will be pro-
cessed in the right context. This provides
the necessary synchronization and allows the
asynchronous system components to work to-
gether in a coherent manner.
4 Discussion
We believe that, in general, this has sev-
eral ramifications for any agent-based, non-
pipelined flow-of-information architecture.
1. Agents which are queried about more
than one hypothesis must keep state for
10For now we treat each utterance as a single
?chunk?. We are interested, however, in moving to
more fine-grained models of dialogue. We believe that
our current architecture will still be useful as we move
to a finer-grained model.
all hypotheses until one is chosen.
2. Agents cannot assume shared context.
Because both the system components
and user are acting asynchronously, it
is impossible in general for any agent to
know what context another agent is cur-
rently in.
3. Agents must be able to defer working on
input. This feature allows them to wait
for synchronization if they receive a mes-
sage to be interpreted in a context they
have not yet reached.
Asynchronous agent-based architectures al-
low dialogue systems to interact with users in
a much richer and more natural way. Unfor-
tunately, the cost of moving to a truly dis-
tributed system is the need to deal with syn-
chronization. Fortunately, for dialogue sys-
tems, models of grounding provide a suitable
and intuitive basis for system synchroniza-
tion.
5 Conclusion and Future Work
In this paper we presented the TRIPS dia-
logue system architecture: an asynchronous,
agent-based architecture, with multiple lay-
ers of flow-of-information. We also discussed
the problems with building this distributed
system. As it turns out, models of ground-
ing provide a foundation for necessary system
synchronization.
For future work we plan to ?clean up? the
model in the ways we have discussed above.
We are also interested in moving to a more in-
cremental model of grounding, where ground-
ing can take place and context can change
within sentence boundaries. Also, we are in-
terested in extending the model to handle
asynchronous issues at the turn-taking level.
For example, what happens to context when
a user barges in while the system is talking, or
if the user and system speak simultaneous for
a time. We believe we will be able to lever-
age our asynchronous model to handle these
cases.
6 Acknowledgments
We would like to thank Amanda Stent, who
was involved with the original formulation of
this architecture. We also wish to thank the
anonymous reviewers for their helpful com-
ments.
This material is based upon work supported
by Department of Education (GAANN) grant
no. P200A000306; ONR research grant no.
N00014-01-1-1015; DARPA research grant
no. F30602-98-2-0133; NSF grant no. EIA-
0080124; and a grant from the W. M. Keck
Foundation.
Any opinions, findings, and conclusions or
recommendations expressed in this material
are those of the authors and do not necessar-
ily reflect the views of the above-mentioned
organizations.
References
J. Allen, D. Byron, M. Dzikovska, G. Ferguson,
L. Galescu, and A. Stent. 2000. An archi-
tecture for a generic dialogue shell. Journal
of Natural Language Engineering special issue
on Best Practices in Spoken Language Dialogue
Systems Engineering, 6(3):1?16, December.
James Allen, George Ferguson, and Amanda
Stent. 2001a. An architecture for more real-
istic conversational systems. In Proceedings of
Intelligent User Interfaces 2001 (IUI-01), pages
1?8, Santa Fe, NM, January.
James F. Allen, Donna K. Byron, Myroslava
Dzikovska, George Ferguson, Lucian Galescu,
and Amanda Stent. 2001b. Towards conversa-
tional human-computer interaction. AI Maga-
zine, 22(4):27?37.
James Allen, Nate Blaylock, and George Fergu-
son. 2002. A problem solving model for col-
laborative agents. In First International Joint
Conference on Autonomous Agents and Multi-
agent Systems, Bologna, Italy, July 15-19. To
appear.
Nate Blaylock. 2002. Managing communica-
tive intentions in dialogue using a collaborative
problem solving model. Technical Report 774,
University of Rochester, Department of Com-
puter Science, April.
Jennifer Chu-Caroll and Michael K. Brown. 1997.
Initiative in collaborative interactions ? its
cues and effects. In S. Haller and S. McRoy,
editors, Working Notes of AAAI Spring 1997
Symposium on Computational Models of Mixed
Initiative Interaction, pages 16?22, Stanford,
CA.
Herbert H. Clark and Edward F. Schaefer. 1989.
Contributing to discourse. Cognitive Science,
13:259?294.
A. J. Claxton, J. Cramer, and C. Pierce. 2001.
A systematic review of the associations be-
tween dose regimens and medication compli-
ance. Clinincal Therapeutics, 23(8):1296?1310,
August.
George Ferguson and James F. Allen. 1998.
TRIPS: An intelligent integrated intelligent
problem-solving assistant. In Proceedings of the
Fifteenth National Conference on Artificial In-
telligence (AAAI-98), pages 567?573, Madison,
WI, July.
George Ferguson, James Allen, Nate Blaylock,
Donna Byron, Nate Chambers, Myroslava
Dzikovska, Lucian Galescu, Xipeng Shen,
Robert Swier, and Mary Swift. 2002. The Med-
ication Advisor project: Preliminary report.
Technical Report 776, University of Rochester,
Department of Computer Science, May.
Tim Finin, Yannis Labrou, and James Mayfield.
1997. KQML as an agent communication lan-
guage. In J. M. Bradshaw, editor, Software
Agents. AAAI Press, Menlo Park, CA.
L. Lamel, S. Rosset, J. L. Gauvain, S. Bennacef,
M. Garnier-Rizet, and B. Prouts. 1998. The
LIMSI ARISE system. In Proceedings of the 4th
IEEE Workshop on Interactive Voice Technol-
ogy for Telecommunications Applications, pages
209?214, Torino, Italy, September.
Mikio Nakano, Noboru Miyazaki, Jun ichi Hira-
sawa, Kohji Dohsaka, and Takeshi Kawabata.
1999. Understanding unsegmented user ut-
terances in real-time spoken dialogue systems.
In Proceedings of the 37th Annual Meeting of
the Association for Computational Linguistics
(ACL-99), pages 200?207.
A. I. Rudnicky, E. Thayer, P. Constantinides,
C. Tchou, R. Shern, K. Lenzo, W. Xu, and
A. Oh. 1999. Creating natural dialogs
in the carnegie mellon communicator system.
In Proceedings of the 6th European Confer-
ence on Speech Communication and Technology
(Eurospeech-99), pages 1531?1534, Budapest,
Hungary, September.
Stephanie Seneff, Raymond Lau, and Joseph Po-
lifroni. 1999. Organization, communication,
and control in the Galaxy-II conversational sys-
tem. In Proceedings of the 6th European Con-
ference on Speech Communication and Tech-
nology (Eurospeech-99), Budapest, Hungary,
September.
Amanda Stent, John Dowding, Jean Mark
Gawron, Elizabeth Owen Bratt, and Robert
Moore. 1999. The CommandTalk spoken dia-
logue system. In Proceedings of the 37th Annual
Meeting of the Association for Computational
Linguistics (ACL-99).
David R. Traum and James F. Allen. 1994.
Discourse obligations in dialogue processing.
In Proceedings of the 32nd Annual Meeting of
the Association for Computational linguistics
(ACL-94), pages 1?8, Las Cruces, New Mexico.
David R. Traum. 1994. A computational theory
of grounding in natural language conversation.
Technical Report 545, University of Rochester,
Department of Computer Science, December.
PhD Thesis.
gency dispatcher, cooperating with the system to 
dynamically allocate resources to and make 
plans for solving problems as they arise in the 
world. The setting, Monroe County, NY, is con- 
siderably more complex than our previous do- 
mains (e.g. Pacifica, TRAINS), and raises new 
issues in knowledge representation a d refer- 
ence. Emergencies include requests for medical 
assistance, car accidents, civil disorder, and 
larger problems such as flooding and snow 
storms. Resources at the user's disposal may 
include road crews, electric crews, ambulances, 
police units and helicopters. Some of the in- 
crease in mixed-initiative interaction comes 
from givi-n~ the_ system more knowledge of the 
tasks being solved. Some comes from the fact 
that the solution to one problem may conflict 
with the solution to another, either because of 
scheduling conflicts, scarce resources, or aspects 
of the physical world (e.g. an ambulance can't go 
down a road that has not been plowed). The 
range of tasks and complexity of the world allow 
for problem solving at different levels of granu- 
larity, making it possible for the system to take 
as much control over the task as the user per- 
mits. 
4. Important  Contr ibut ions  
While a number of robust dialogue systems have 
been built in recent years, they mostly have op- 
erated in domains that require little if any rea- 
soning. Rather, the task is hard-coded into the 
system operation. One of the major goals of the 
TRIPS project has been to develop dialogue 
models and system architectures that support 
conversational interaction in domains where 
complex reasoning systems are required. One 
goal has been to build a fairly generic model in 
which different domains can then be specified 
fairly easily. On this front, we are seeing some 
success as we have now constructed versions of 
TRIPS in three different domains, and TRIPS? 
911 will be the fourth. In developing the system 
for new domains, the bulk of the work by far has 
been in system enhancements rather than in 
developing the domain models. 
The TRIPS-911 domain has forced a rethinking 
of the relationship between dialogue- 
management, problem-solving, the system's 
Figure 1: Monroe County map used in TRIPS-911 
own goal-pursuit and generation. The new ar- 
chitecture is designed to support research into 
mixed-initiative interactions, incremental gen- 
eration of content (in which the user might in- 
tervene before the system completes all it has to 
say), rich reference resolution models, and the 
introduction of plan monitoring and plan repair 
into the suite of plan management operations 
supported. The domain also can support longer 
and richer dialogues than in previous domains. 
More complex domains mean even more com- 
plex dialogues. The complexity arises from 
many factors. First, more complex dialogues 
will involve topic progression, development and 
resumption, and more complex referential phe- 
nomena. On the problem solving front, there will 
be more complex corrections, elaborations and 
modifications--forcing us to develop richer 
discourse models. In addition, the complexity of 
the domain demonstrates a need for better 
grounding behavior and a need for incremental 
dialogue-based generation. 
We have by no means solved these problems. 
Rather we have built a rich testbed, designed and 
implemented a plausible architecture, and have 
constructed an initial system to demonstrate 
basic capabilities in each of the problem areas. 
34 
5. Limitations 
TRIPS-911 is a first attempt at handling a do- 
main of this complexity. As such there are many 
capabilities that people have in such situations 
that are beyond the system's current capabilities. 
Some of the most important are: 
? Scale - we can only handle small domains 
and the existing techniques would not ex- 
tend directly to a realistic size 911 operation. 
To scale up we must face some difficult 
problems including reasoning about quanti- 
ties and aggregates, planning in large-scale 
domains (i.e., the real domains are beyond 
the capabilities of current plan technology), 
and performing intention recognition as the 
number of options increases. In addition, for 
an effective dialogue system, all this must be 
done in real-time. 
? Meta-talk - when faced with complex prob- 
lems, people often first generally discuss the 
problem and possible strategies for solving 
it, and later may explicitly direct attention to 
specific subproblems. The current TRIPS 
system does not support such discussion. 
? Time - in the 911 domain there are at least 
two temporal contexts that can be "used" by 
the conversants: there is the actual time (i.e., 
when they are talking), but there also is the 
time relative to a point of focus in a plan, or 
even simply talking about the past or the 
future. TRIPS-911 can currently interpret 
expressions with respect to the actual time. 
? Interleaved generation - when people are 
discussing complex issues, they often have 
to plan to communicate heir content across 
several different utterances. There is no 
guarantee that the other conversant will not 
"interrupt" (e.g., to clarify, correct, suggest 
alternatives, etc) before the entire content is 
conveyed. This requires a rethinking of cur- 
rent practice in generation to make it incre- 
mental and interactive. 
? True interruptions - people may interrupt the 
system while it is talking. It is unclear at this 
stage what the system should assume was 
conveyed. The strategies of assuming noth- 
ing was conveyed, or that all was conveyed 
have obvious faults. We are pursuing alter- 
natives based on knowing when speech was 
interrupted, but using this ififormation suc- 
cessfully remains adifficult problem. 
References 
Allen, James et al An Architecture for a Generic 
Dialogue Shell, to appear, J. Natural Language 
Engineering, 2000. 
Ferguson, George and J. Allen,-TRIPS: An Integrated 
Intelligent Problem-Solving Assistant, Proc. Na- 
tional Conference on AI (AAAI-98), Madison, WI, 
1998. 
35 
