Description of the HKU Chinese Word Segmentation System 
for Sighan Bakeoff 2005 
Guohong Fu 
Department of Linguistics 
The University of Hong Kong
Pokfulam Road, Hong Kong
ghfu@hkucc.hku.hk
Kang-Kwong Luke 
Department of Linguistics 
The University of Hong Kong
Pokfulam Road, Hong Kong
kkluke@hkusua.hku.hk
Percy Ping-Wai WONG  
Department of Linguistics 
The University of Hong Kong
Pokfulam Road, Hong Kong
wongpw@hkusua.hku.hk
Abstract 
In this paper, we describe in brief our 
system for the Second International 
Chinese Word Segmentation Bakeoff 
sponsored by the ACL-SIGHAN. We 
participated in all tracks at the bakeoff. 
The evaluation results show our system 
can achieve an F measure of 0.940-
0.967 for different testing corpora. 
1 Introduction 
Word segmentation is very important for Chi-
nese text processing, which is aiming at recog-
nizing the implicit word boundaries in plain 
Chinese text. Over the past decades, great pro-
gress has been made with Chinese word 
segmentation technology. However, two 
difficulties still face us while developing a 
practical segmentation system for large open 
applications, i.e. the resolution of ambiguous 
segmentation and the identification of unknown 
or out-of-vocabulary (OOV) words. 
In order to resolve the above two problems, 
we developed a purely statistical Chinese word 
segmentation system using a two-stage strategy. 
We participated in eight tracks at the Second 
International Chinese Word Segmentation 
Bakeoff sponsored by the ACL-SIGHAN, and 
tested our system on different testing corpora. 
The scored results show that our system is effec-
tive for most of ambiguous segmentation and 
unknown words in Chinese text. In this paper, 
we make a summary of this work and give some 
analysis on the results. 
The rest of this paper is organized as follows: 
First in Section 2, we describe in brief a two-
stage strategy for Chinese word segmentation. 
Then in Section 3, we give details about the set-
tings or configuration of our system for different 
testing tracks, particularly the training data and 
the dictionaries used in our system. Finally, we 
report the results of our system at this bakeoff in 
Section 4, and give our conclusions on this work 
in Section 5. 
2 Overview of the System 
In practice, our system works in two major steps 
as follows: 
The first step is a process of known word 
segmentation, which aims to segment an input 
sequence of Chinese characters into a sequence 
of known words that are listed in the system dic-
tionary. In our current system, we apply a 
known word bigram model to perform this task 
(Fu and Luke, 2003; Fu and Luke, 2004; Fu and 
Luke, 2005).  
Actually, known word segmentation is a 
process of disambiguation. Given a Chinese 
character string 
ncccC L21= , there are usually 
multiple possible segmentations of known words 
mwwwW L21=  according to the system diction-
ary. The task of known word segmentation is to 
find a proper segmentation 
mwwwW L21? =  that 
maximizes the probability ?
=
?
m
i
ii wwP
1
1 )|( , i.e. 
?
=
?
?=
m
i
ii
WW
wwPCWPW
1
1 )|(maxarg)|(maxarg?  (1) 
The second step is actually a tagging task on 
the sequence of known words acquired in the 
first step, which intends to detect unknown 
165
words or out-of-vocabulary (OOV) words in the 
input. In this process, each known word yielded 
in the first step will be further assigned a proper 
tag that indicates whether the known word is an 
independent segmented word by itself or a be-
ginning/middle/ending component of an OOV 
word  (Fu and Luke, 2004). In order to improve 
our system, part-of-speech information is also 
introduced in some tracks such as the PKU open 
test and the AS open test. Furthermore, a 
lexicalized HMM tagger is developed to per-
form this task (Fu and Luke, 2004).  
Given a sequence of known words 
nwwwW L21= , the lexicalized HMM tagger 
attempt to find an appropriate sequence of tags 
ntttT L21? =  that maximizes the conditional prob-
ability )|( WTP , namely 
  
?
=
???
?
=
n
i
iiiiii
T
T
twtPtwwP
WTPT
1
111 ),|(),|(maxarg
)|(maxarg?
     (2) 
3 Settings for Different Tracks 
Table 1. Training corpora for different tracks 
Table 1 presents the corpora used to train our 
system for different tracks. In the Academia 
Sinica (AS) open test and the Peking University 
(PKU) open test, our system is trained respec-
tively using the Sinica Corpus (3.0) and the PFR 
Corpus. In all other tests, including all closed 
tests, City University of Hong Kong (CityU) 
open test and Microsoft Research (MSR) open 
test, we trained our system using the relevant 
training corpora provided for the bakeoff.  
Track Training Corpus Word 
counts
AS-O The Sinica Corpus 3.0  5,692,150
AS-C AS corpus for Bakeoff 2005 5,449,698
CityU-O CityU corpus for Bakeoff 2005 1,455,629
CityU-C CityU corpus for Bakeoff 2005 1,455,629
MSR-O MSR corpus for Bakeoff 2005 2,368,391
MSR-C MSR corpus for Bakeoff 2005 2,368,391
PKU-O The PFR Corpus  7,286,960
PKU-C PKU corpus for Bakeoff 2005 1,109,947
Table 2 shows all the dictionaries used in our 
system for different tracks.   
In the closed test, the system dictionaries are 
derived automatically from the relevant training 
corpora for this bakeoff by using the following 
three criteria: (1) Each character in the training 
corpus is taken as an independent entry and col-
lected into the relevant system dictionary. (2) A 
standard Chinese word in the training corpus 
will enter to the relevant dictionary if it has four 
or less Chinese characters within it, and at the 
same time, its counts of occurrence in the corpus 
is observed to be larger than a threshold. In our 
current system, the threshold is set to 10 for the 
AS closed test and 5 for other closed tests. (3) 
For non-standard Chinese words such as nu-
meral expressions, English words and punctua-
tions, if they consist of multiple characters, they 
will be not included in the system dictionary. 
As for the open test, some other dictionaries 
are applied. As can be seen from Table 2, the 
CKIP Lexicon and Chinese Grammar is used in 
both AS and CityU open test, and the Gram-
matical Knowledge-base of Contemporary Chi-
nese developed by the Peking University is 
utilized in both PKU and MSR open test.  
   
Track System dictionary # of 
entries
AS-O The CKIP Lexicon and Chinese Grammar 84K
AS-C Automatically extracted from AS 
corpus for Bakeoff 2005 30K
CityU-O
The CKIP Lexicon and Chinese 
Grammar (without part-of-
speech)
84K
CityU-C Automatically extracted from CityU corpus for Bakeoff 2005 22K
MSR-O 
The Grammatical Knowledge-
base of Contemporary Chinese
(without part-of-speech) 
65K
MSR-C Automatically extracted from MSR corpus for Bakeoff 2005 17K
PKU-O The Grammatical Knowledge-base of Contemporary Chinese 65K
PKU-C Automatically extracted from PKU corpus for Bakeoff 2005 17K
Table 2. System dictionaries for different tracks 
It should be noted that part-of-speech infor-
mation is also utilized in the AS open test and 
the PKU open test, because part-of-speech in-
formation proved to be informative in identify-
ing OOV words in Chinese text (Fu and Luke, 
2004). Therefore, the training corpora for the 
two tests are tagged with part-of-speech, and 
entries in the relevant dictionaries are defined 
with their potential part-of-speech categories.   
166
4 The Scored Results 
In Bakeoff 2005, six measures are employed to 
score the performance of a word segmentation 
system, namely recall (R), precision (P), the 
evenly-weighted F-measure (F), out-of-
vocabulary (OOV) rate for the test corpus, recall 
with respect to OOV words (ROOV) or in-
vocabulary words (Riv).  
In order to achieve a consistent evaluation of 
our system in both the closed test and the open 
test, OOV is defined in this paper as the set of 
words in the test corpus but not occurring in 
both the training corpus and the system diction-
ary. Furthermore, the additional two rates, i.e. 
OOV-C and OOV-D are used to denote the out-
of-vocabulary rate with respect to the training 
corpus and the out-of-vocabulary rate with 
respect to the system dictionary, respectively. At 
the same time, the precision with regard to in-
vocabulary words (Piv) and OOV words (POOV) 
are also computed in this paper to give a more 
complete evalution of our system in unknown 
word identification. 
Track OOV-C OOV-D OOV 
AS-O 0.043 0.096 0.039 
AS-C 0.043 0.096 0.043 
CityU-O 0.074 0.140 0.049 
CityU-C 0.074 0.127 0.074 
MSR-O 0.026 0.076 0.023 
MSR-C 0.026 0.087 0.026 
PKU-O 0.038 0.070 0.033 
PKU-C 0.058 0.091 0.058 
Table 3. OOV rates for different tracks 
Track F R P Riv Piv ROOV POOV
AS-O 0.946 0.955 0.938 0.972 0.947 0.532 0.660
AS-C 0.940 0.947 0.934 0.966 0.949 0.523 0.566
CityU-O 0.941 0.944 0.938 0.962 0.956 0.592 0.599
CityU-C 0.939 0.944 0.933 0.969 0.952 0.626 0.677
MSR-O 0.967 0.969 0.966 0.978 0.977 0.586 0.537
MSR-C 0.962 0.962 0.962 0.972 0.977 0.592 0.499
PKU-O 0.962 0.959 0.965 0.963 0.970 0.835 0.816
PKU-C 0.944 0.943 0.944 0.961 0.958 0.656 0.700
Table 4. Scores for different tracks 
The OOV rates and scores of our system are 
summarized respectively in Table 3 and Table 4. 
The results show that our system can achieve a 
F-measure of 0.940-0.967 for different testing 
corpora while the relevant OOV rates are from 
0.023 to 0.074.  
Although our system has achieved a promis-
ing performance, there is still much to be done 
to improve it. Firstly, our system is purely statis-
tics-based, it cannot yield correct segmentations 
for all non-standard words (NSWs) such as nu-
meral expressions and English strings in Chi-
nese text. Secondly, known word segmentation 
and unknown word identification are taken as 
two independent stages in our system. This 
strategy is obviously simple and more easily 
applicable (Fu and Luke, 2003). Although the 
known word bigram model can partly resolve 
this problem, it is not always effective for some 
complicated strings that contains a mixture of 
ambiguities and unknown words, such as ?31
? and the fragment ?? in the sen-
tence ?	
?.   
5 Conclusions 
This paper presents a two-stage statistical word 
segmentation system for Chinese. We partici-
pated in all testing tracks at the second Sighan 
bakeoff. The scored results show that our system 
can achieve a F-measure of 0.940-0.967 as a 
whole for different corpora. This indicates that 
the proposed system is effective for most am-
biguous segmentations and unknown words in 
Chinese test. For future work, we hope to im-
prove our system by incorporating some pattern 
rules to handle complicated ambiguous frag-
ments and non-standard words in Chinese text. 
References 
Guohong Fu, and Kang-Kwong Luke. 2003. A two-
stage statistical word segmentation system for 
Chinese. In: Proceedings of the 2nd SIGHAN 
Workshop on Chinese Language Processing, Sap-
poro, Japan, 156-159. 
Guohong Fu, and Kang-Kwong Luke. 2004. Chinese 
unknown word identification as known word tag-
ging. In: Proceedings of the Third IEEE Interna-
tional Conference on Machine Learning and 
Cybernetics (ICMLC 2004), Shanghai, China, 
2612-2617. 
Guohong Fu, and Kang-Kwong Luke. 2005. Chinese 
unknown word identification using class-based 
LM. Lecture Notes in Computer Science (IJCNLP 
2004), 3248: 704-713. 
167
   
A two-stage statistical word segmentation system for Chinese 
Guohong Fu 
Dept of Linguistics 
 The University of Hong Kong 
Pokfulam Road, Hong Kong 
ghfu@hkucc.hku.hk 
K.K. Luke 
Dept of Linguistics 
The University of Hong Kong 
Pokfulam Road, Hong Kong 
kkluke@hkusua.hku.hk 
 
Abstract 
In this paper we present a two-stage 
statistical word segmentation system for 
Chinese based on word bigram and word-
formation models. This system was 
evaluated on Peking University corpora at 
the First International Chinese Word 
Segmentation Bakeoff. We also give 
results and discussions on this evaluation. 
1 Introduction 
Word segmentation is very important for Chinese 
language processing, which aims to recognize the 
implicit word boundaries in Chinese text. During 
the past decades, great success has been achieved in 
Chinese word segmentation (Nie, et al 1995; Yao, 
1997; Fu and Wang, 1999; Wang et al 2000; 
Zhang, et al 2002). However, there still remain two 
difficult problems, i.e. ambiguity resolution and 
unknown word (so-called OOV word) identification, 
while developing a practical segmentation system 
for large open applications. 
In this paper, we present a two-stage statistical 
word segmentation system for Chinese. In the first 
stage, we employ word bigram model to segment 
known words (viz. the words included in the system 
dictionary) in input. In the second stage, we develop 
a hybrid algorithm to perform unknown word 
identification incorporating word contextual 
information, word-formation patterns and word 
juncture model.   
The rest of this paper is organized as follows: 
Section 2 presents a word bigram solution for 
known word segmentation. Section 3 describes a 
hybrid approach for unknown word identification. 
In section 4, we report the results of our system at 
the SIGHAN evaluation program, and in the final 
section we give our conclusions on this work.  
2 The first stage: Segmentation of known 
words 
In a sense, known word segmentation is a process 
of disambiguation. In our system, we use word 
bigram language models and Viterbi algorithm 
(1967) to resolve word boundary ambiguities in 
known word segmentation. 
For a particular input Chinese character string 
ncccC L21= , there is usually more than one 
possible segmentation mwwwW L21= according to 
given system dictionary. Word bigram segmentation 
aims to find the most appropriate segmentation 
mwwwW L21? =  that maximizes the probability 
?
=
?
m
i
iir wwP
1
1)|( , i.e.  
    ?
=
?
?=
m
i
iir
W
r
W
wwPCWPW
1
1)|(maxarg)|(maxarg?     (1) 
where )|( 1?iir wwP  is the probability that word lw  
will occur given previous word 1?iw , which can be 
easily estimated from segmented corpus using 
maximum likelihood estimation (MLE), i.e. 
              
)(
)(
)|(
1
1
1
?
?
?
?
i
ii
iir wCount
wwCount
wwP              (2) 
To avoid the problem of data sparseness in MLE, 
here we apply the linear interpolation technique 
(Jelinek and Mercer, 1980) to smooth the estimated 
word bigram probabilities.  
3 The second stage: Unknown word 
identification 
The second stage mainly concerns unknown words 
segmentation that remains unresolved in first stage. 
This section describes a hybrid algorithm for 
unknown word identification, which can incorporate 
word juncture model, word-formation patterns and 
contextual information. To avoid the complicated 
normalization of the probabilities of different 
dimensions, the simple superposition principle is 
also used in merging these models. 
3.1 Word juncture model 
Word juncture model score an unknown word by 
assigning word juncture type. Obviously, most 
unknown words appear as a string of known words 
after segmentation in first stage. Therefore, 
unknown word identification can be viewed as a 
process of re-assigning correct word juncture type 
to each known word pair in input. Given a known 
word string nwwwW L21= , between each word pair 
)11(1 ???+ niww ii  is a word juncture. In general, 
there are two types of junctures in unknown word 
identification, namely word boundary (denoted by 
Bt ) and non-word boundary (denoted by Nt ). 
Let )( 1+ii wwt denote the type of a word juncture 
1+ii ww , and ))(( 1+iir wwtP denote the relevant 
conditional probability, then 
              
)(
))(())((
1
1
1
+
+
+ =
ii
ii
def
iir wwCount
wwtCountwwtP              (3) 
Thus, the word juncture probability )( UCJM wP of a 
particular unknown word jiiU wwww L1+=  
)1( nji ??? can be calculated by  
 ?
?
=
++? ??=
1
111 ))(())(())(()(
j
il
llNrjjBriiBrUCJM cctPwwtPwwtPwP  (4) 
In a sense, word juncture model mirrors the affinity 
of known word pairs in forming an unknown word. 
For a word juncture ),( 1+ii ww , the larger the 
probability ))(( 1+iiNr wwtP , the more likely the two 
words are merged together into one new word. 
3.2 Word-formation patterns 
Word-formation pattern model scores an unknown 
word according to the probability of how each 
internal known word contributes to its formation. In 
general, a known word w  may take one of the 
following four patterns while forming a word: (1) 
w itself is a word. (2) w is the beginning of an 
unknown word. (3) w is at the middle of an 
unknown word. (4) w appears at the end of an 
unknown word.  For convenience, we use S , B , 
M and E  to denote the four patterns respectively. 
Let )(wpttn denote a particular pattern of w  in an 
unknown word and ))(( wpttnPr  denote the relevant 
probability, then  
                
)(
))(())((
wCount
wpttnCountwpttnP
def
r =              (5) 
Obviously, 1))(( =?
pttn
r wpttnP . And  1- ))(( wSPr  is the 
word-formation power of the known word w . 
Let  )( Upttn wP be the overall word-formation 
pattern probability of a certain unknown word 
lU wwww L21=  , then 
               ?
?
=
Ui ww
irUpttn wpttnPwP ))(()(              (6) 
Theoretically speaking, a known word can take any 
pattern while forming an unknown word. But it is 
not even in probability for different known words 
and different patterns. For example, the word ? 
(xing4, nature) is more likely to act as the suffix of 
words. According to our investigation on the 
training corpus, the character? appears at the end 
of a multiword in more than 93% of cases.  
3.3 Hybrid algorithm for unknown word 
identification 
Current algorithm for unknown word identification 
consists of three major components: (1) an 
unknown word extractor firstly extracts a fragment 
of known words nwww L21 that that may have 
unknown words based on the related word-
formation power and word juncture probability and 
its left and right contextual word Lw , Rw from the 
output of the first stage.  (2) A candidate word 
constructor then generates a lattice of all possible 
new segmentations }|{ 21 mUU xxxWW L=  that may 
involve unknown words from the extracted 
fragment. (3) A decoder finally incorporates word 
juncture model )( UWJM WP , word-formation 
patterns )( Upttn WP  and word bigram probability 
)( Ubigram WP to score these candidates, and then 
applies the Viterbi algorithm again to find the best 
new segmentation mU xxxW L21? =  that has the 
maximum score: 
      
}))|()()(({maxarg
)}()()({maxarg?
,,1
1?
=
?
++=
++=
ni
iiriCJMipttn
W
UbigramUCJMUpttn
W
U
xxPxPxP
WPWPWPW
U
U
L
           (7) 
where Lwx =0  and Rn wx =+1 . Let Uw  denote any 
unknown word in the training corpus. If ix  is an 
unknown word, then
)(
)(
)|(
1
1
1
?
?
?
?
=
i
w
Ui
iir xCount
wxCount
xxP U .  
4 Experiments 
Our system participated in both closed and open 
tests on Peking University corpora at the First 
International Chinese Word Segmentation Bakeoff.  
This section reports the results and discussions on 
its evaluation. 
4.1 Measures 
In the evaluation program of the First International 
Chinese Word Segmentation Bakeoff, six measures 
are employed to score the performance of a word 
segmentation system, namely test recall (R), test 
precision (denoted by P), the balanced F-measure 
(F), the out-of-vocabulary (OOV) rate for the test 
corpus, the recall on OOV words (ROOV), and the 
recall on in-vocabulary (Riv) words. OOV is defined 
as the set of words in the test corpus not occurring 
in the training corpus in the closed test, and the set 
of words in the test corpus not occurring in the 
lexicon used in the open test. 
4.2 Experimental lexicons and corpora 
As shown in Table 1, we only used the training data 
from Peking University corpus to train our system 
in both the open and closed tests. As for the 
dictionary, we compiled a dictionary for the closed 
test from the training corpus, which contained 55, 
226 words, and used a dictionary in the open test 
that contained about 65, 269 words.  
 
Items # words in 
lexicon 
# train. 
words  
# test. 
words 
Closed 55,226 1,121,017 17,194 
Open 65,269 1,121,017 17,194 
Table 1: Experimental lexicons and corpora 
4.3 Experimental results and discussion 
 
Items F R P OOV ROOV Riv 
Closed 0.939 0.936 0.942 0.069 0.675 95.5 
Open 0.937 0.933 0.941 0.094 0.762 95.0 
Table 2: Test results on PK corpus 
 
Segmentation speed: There are in all about 28,458 
characters in the test corpus. It takes about 3.21 
and 3.07 seconds in all for our system to perform 
full segmentation (including known word 
segmentation and unknown word identification) on 
the closed and open test corpus respectively, 
running on an ACER notebook (TM632XC-P4M). 
This indicates that our system is able to process 
about 531,925~556,182 characters per minute.  
Results and discussions: The results for the closed 
and open test are presented in Table 2. We can 
draw some conclusions from these results.  
Firstly, the overall performance of our system is 
very stable in both the closed and open tests. As 
shown in Table 2, the out-of-vocabulary (OOV) 
rate is 6.9% in the closed test and 9.4% in the open 
test. However, the overall test F-measure drops by 
only 0.2 percent in the open test, compared with the 
closed test.  
Secondly, our approach can handle most unknown 
words in the input. As can be seen from Table 2, 
the recall on OOV words are 67.5% the closed-test 
and 76.2% in the open-test. Wang et al(2000) and 
Yao (1997) have proposed a character juncture 
model and word-formation patterns for Chinese 
unknown word identification. However, their 
approaches can only work for the unknown words 
that are made up of pure monosyllable character in 
that they are character-based methods. To address 
this problem, we introduce both word juncture 
model and word-based word-formation patterns into 
our system. As a result, our system can deal with 
different unknown words that consist of different 
known words, including monosyllable characters 
and multiword.  
Although our system is effective for most 
ambiguities and unknown words in the input, it has 
its inherent deficiencies. Firstly, to avoid data 
sparseness, we do not differentiate known words 
and unknown words while estimating word juncture 
models and word-formation patterns from the 
training corpus. This simplification may introduce 
some noises into these models for identifying 
unknown words. Our further investigations show 
that the precision on OOV words is still very low, 
i.e. 67.1% for closed test and 72.5% for open test. 
As a result, our system may yield a number of 
mistaken unknown words in the processing. 
Secondly, we regard known word segmentation and 
unknown word identification as two independent 
stages in our system. This strategy is obviously 
simple and more easily applicable. However, it does 
not work while the input contains a mixture of 
ambiguities and unknown words.  For example, 
there was a sentence ?????????? in 
the test corpus, where, the string ???? is a 
fragment mixed with ambiguity and unknown 
words. The correct segmentation should be ??/?
?/, where??(Zhonghang, the Bank of China) is 
a abbreviation of organization name, and ?? 
(Changge) is a place name. Actually, this fragment 
is segmented as?/??/?/ in the first stage of our 
system. However, the unknown word identification 
stage does not have a mechanism to split the word
??(Hangzhang, president) and  finally resulted in 
wrong segmentation.  
5 Conclusions 
This paper presents a two-stage statistical word 
segmentation system for Chinese. In the first stage, 
word bigram model and Viterbi algorithm are 
applied to perform known word segmentation on 
input plain text, and then a hybrid approach is 
employed in the second stage to incorporate word 
bigram probabilities, word juncture model and 
word-based word-formation patterns to detect OOV 
words. The experiments on Peking University 
corpora have shown that the present system based 
on fairly simple word bigram and word-formation 
models can achieve a F-score of 93.7% or above. In 
future work, we hope to improve our strategies on 
estimating word juncture model and word-formation 
patterns and develop an integrated segmentation 
technique that can perform known word 
segmentation and unknown word identification at 
one time. 
Acknowledgments 
We would like to thank all colleagues of the First 
International Chinese Word Segmentation Bakeoff 
for their evaluations of the results and the Institute 
of Computational Linguistics, Peking University for 
providing the experimental corpora. 
References 
Fu, Guohong and Xiaolong Wang. 1999. Unsupervised 
Chinese word segmentation and unknown word 
identification. In: Proceedings of NLPRS?99, 
Beijing, China, 32-37. 
Jelinek, Frederick, and Robert L. Mercer. 1980. 
Interpolated estimation of Markov source parameters 
from sparse data. In: Proceedings of Workshop on 
Pattern Recognition in Practice, Amsterdam, 381-
397. 
Nie, Jian-Yuan, M.-L. Hannan and W.-Y. Jin. 1995. 
Unknown word detection and segmentation of 
Chinese using statistical and heuristic knowledge. 
Communication of COLIPS, 5(1&2): 47-57.  
Viterbi, A.J. 1967. Error bounds for convolutional 
codes and an asymmetrically optimum decoding 
algorithm. IEEE Transactions on Information 
Theory, IT-13(2): 260-269. 
Wang, Xiaolong, Fu Guohong, Danial S.Yeung, James 
N.K.Liu, and Robert Luk. 2000. Models and 
algorithms of Chinese word segmentation. In: 
Proceedings of the International Conference on 
Artificial Intelligence (IC-AI?2000), Las Vegas, 
Nevada, USA, 1279-1284. 
Yao, Yuan. 1997. Statistics Based approaches towards 
Chinese language processing. Ph.D. thesis. National 
University of Singapore. 
Zhang, Hua-Ping, Qun Liu, Hao Zhang, and Xue-Qi 
Cheng. 2002. Automatic recognition of Chinese 
unknown words based on roles tagging. In: 
Proceedings of The First SIGHAN Workshop on 
Chinese Language Processing, Taiwan, 71-77. 
A Morpheme-based Part-of-Speech Tagger for Chinese 
Guohong Fu 
School of Computer Science and Technology 
Heilongjiang University 
Harbin 150080, P.R. China 
ghfu@hotmail.com 
Jonathan J. Webster 
Department of Chinese, Translation and Linguistics 
City University of Hong Kong 
83 Tat Chee Avenue, Hong Kong, P.R. China 
ctjjw@cityu.edu.hk 
 
 
Abstract 
This paper presents a morpheme-based 
part-of-speech tagger for Chinese. It con-
sists of two main components, namely a 
morpheme segmenter to segment each 
word in a sentence into a sequence of mor-
phemes, based on forward maximum 
matching, and a lexical tagger to label each 
morpheme with a proper tag indicating its 
position pattern in forming a word of a 
specific class, based on lexicalized hidden 
Markov models. This system have partici-
pated four closed tracks for POS tagging at 
the Fourth International Chinese Language 
Processing Bakeoff sponsored by the ACL-
SIGHAN.  
1 Introduction 
Part-of-speech (POS) tagging aims to assign each 
word in a sentence with a proper tag indicating its 
POS category. While a number of successful POS 
tagging systems have been available for English 
and many other languages, it is still a challenge to 
develop a practical POS tagger for Chinese due to 
its language-specific issues. Firstly, Chinese words 
do not have a strict one-to-one correspondence be-
tween their POS categories and functions in a sen-
tence. Secondly, an ambiguous Chinese word can 
act as different POS categories in different con-
texts without changing its form. Thirdly, there are 
many out-of-vocabulary (OOV) words in real Chi-
nese text whose POS categories are not defined in 
the dictionary used. All these factors make it much 
more difficult to achieve a high-performance POS 
tagger for Chinese. 
Recent studies in Chinese POS tagging focus on 
statistical or machine learning approaches with 
either characters or words as basic units for tagging 
(Ng and Low, 2004; Fu and Luke, 2006). Very 
little research has been devoted to resolving Chi-
nese POS tagging problems based on morphemes. 
In our system, we prefer morphemes to characters 
or words as tagging units for three reasons. First, 
words are made of morphemes instead of charac-
ters (Wu and Tseng, 1995; Packard, 2000). Sec-
ond, most morphemes are productive in word for-
mation (Baayen, 1989; Sproat and Shih, 2002; Ni-
shimoto, 2003), particularly in the formation of 
morphologically-derived words (MDWs) and 
proper nouns, which are the major source of OOV 
words in Chinese texts. Third, Packard (2000) in-
dicates that Chinese do have morphology. More-
over, morphology proves to be a very informative 
cue for predicting POS categories of Chinese OOV 
words (Tseng et al 2005).  Therefore, we believe 
that a morpheme-based framework would be more 
effective than the character- or word-based ones in 
capturing both word-internal morphological fea-
tures and word-external contextual information for 
Chinese POS disambiguation and unknown word 
guessing (UWG) as well. 
Thus we present a morpheme-based POS tagger 
for Chinese in this paper. It consists of two main 
components, namely a morpheme segmentation 
component for segmenting each word in a sentence 
into a sequence of morphemes, based on the for-
ward maximum matching (FMM) technique, and a 
lexical tagging component for labeling each seg-
mented morpheme with a proper tag indicating its 
position pattern in forming a word of a specific 
type, based on lexicalized hidden Markov models 
(HMMs). Lack of a large morphological knowl-
124
Sixth SIGHAN Workshop on Chinese Language Processing
edge base is a major obstacle to Chinese morpho-
logical analysis (Tseng and Chen, 2002). To over-
come this problem and to facilitate morpheme-
based POS tagging as well, we have also devel-
oped a statistically-based technique for automati-
cally extracting morphemes from POS-tagged cor-
pora. We participated in four closed tracks for POS 
tagging at the Fourth International Chinese Lan-
guage Processing Bakeoff sponsored by the ACL-
SIGHAN and tested our system on different testing 
corpora. In this paper, we also made a summary of 
this work and give some brief analysis on the re-
sults. 
The rest of this paper is organized as follows: 
Section 2 is a brief description of our system. Sec-
tion 3 details the settings of our system for differ-
ent testing tracks and presents the scored results of 
our system at this bakeoff. Finally, we give our 
conclusions in Section 4. 
2 System Description 
2.1 Chinese Morphemes 
In brief, Chinese morphemes can be classified 
into free morphemes and bound morphemes. A 
free morpheme can stand by itself as a word (viz. a 
basic word), whereas a bound morpheme can show 
up if and only if being attached to other 
morphemes to form a word. Free morphemes can 
be subdivided into true free morphemes and 
pseudo free morphemes. A pseudo free morpheme 
??such as  ran2-er2 ?however? can only stand 
alone, while a true free morpheme like ?? 
SHENG-CHAN ?produce? can stand alone by itself 
as a word or occur as parts of other words. Chinese 
affixes include prefixes (e.g. ? fei1 ?non-?, ? 
wei3 ?pseudo?), infixes (e.g. ??  fei1-zhi1) or 
suffixes (e.g. ? xing4 ?-ity?, ?? zhu3-yi4 ?-
ism?), in terms of their positions within a word. 
2.2 Formulation 
To perform morpheme-based Chinese POS tag-
ging, we represent a POS-tagged word in a Chi-
nese sentence as a sequence of lexical chunks with 
the aid of an extended IOB2 tag set (Fu and Luke 
2005). A lexical chunk consists of a sequence of 
constituent morphemes associated with their corre-
sponding lexical chunk tags. A lexical chunk tag 
follows the format T1-T2, indicating the POS cate-
gory T2 of a word and the position pattern T1 of a 
constituent morpheme within the word. As shown 
in Table 1, four position patterns are involved in 
our system, namely O for a single morpheme as a 
word by itself, I for a morpheme inside a word, B 
for a morpheme at the beginning of a word and E 
for a morpheme at the end of a word. 
 
Tag Definition 
Corresponding 
morpheme types 
O A morpheme as a word by itself Free morphemes 
I 
A morpheme inside a 
word 
Free morphemes and 
infixes 
B 
A word-initial mor-
pheme 
Free morphemes and 
prefixes 
E A word-final morpheme Free morphemes and 
suffixes 
Table 1. Extended IOB2 tag set 
2.3 Affix Extraction 
Due to the increasing involvement of affixation in 
Chinese word formation, affixes play a more and 
more important role in Chinese POS tagging. In 
morpheme extraction, affixes are very useful in 
determining whether a given word is derived by 
affixation. To extract affixes from corpora, we 
consider three statistics, i.e. morpheme-position 
frequency )1,( TmCount , morpheme-position 
probability )()1,()1,( mCountTmCountTmMPP =  
and morphological productivity. Following the 
proposal in (Baayen, 1989), the morphological 
productivity of a morpheme m  with a position pat-
tern 1T , denoted as )1,( TmMP , can be defined as 
)1,(
)1,(1)1,(
TmCount
TmnTmMP =                 (1) 
where )1,(1 Tmn  is the number of word types that 
occur only once in the training corpus and at the 
same time, are formed by the morpheme m  with 
the position pattern 1T . 
To estimate the above statistics for affix extrac-
tion, we only take into account the three position 
patterns B, I and E, for prefixes, infixes and suf-
fixes, respectively. Thus we can extract affixes 
from training data with the following three condi-
tions: MPFTHTmCount ?)1,( , MPPTHTmMPP ?)1,(  
and MPTHTmMP ?)1,( , where THMPF, THMPP and 
THMP are three empirically-determined thresholds. 
125
Sixth SIGHAN Workshop on Chinese Language Processing
2.4 Morpheme Extraction 
The goal of morpheme extraction is to identify 
MDWs and proper nouns in training corpora and 
prevent them from getting into the morpheme dic-
tionary for POS tagging. In the present system, the 
following criteria are applied to determine whether 
a word in training data should enter the morpheme 
dictionary. 
Completeness. With a view to the completeness 
of the morpheme dictionary, all characters in train-
ing data will be collected as morphemes. 
Word length. In general, shorter morphemes 
are more productive than longer ones in word for-
mation. As such, the length of a morpheme should 
not exceed four characters. 
Word frequency. By this criterion, a word is 
selected as a morpheme if its frequency of occur-
rences in training data is higher than a given 
threshold. 
MDWs. By this criterion, words formed by 
morphological patterns such as affixation, com-
pounding, reduplication and abbreviation will be 
excluded from the morpheme dictionary. 
Proper nouns. In some training corpora like the 
PKU corpus, some special tags are specified for 
proper nouns. In this case, they will be used to fil-
ter proper nouns during morpheme extraction. 
2.5 Lexicalized HMM Tagger 
As shown in Figure 1, our system works in three 
main steps as follows. 
 
Figure 1. Overall architecture of our system 
 
Morpheme segmentation. In this step, the 
FMM technique is employed to segment each word 
in a sentence to a sequence of morphemes associ-
ated with their position tags within the word.  
Tag candidate generation. In this step, all pos-
sible POS candidates are generated for each word 
in the sentence by consulting the morpheme dic-
tionary with its constitute morphemes and their 
related position patterns. All these candidates are 
stored in a lattice. 
Scoring and Decoding. In this step, the lexical-
ized HMMs are first employed to score each can-
didate in the lattice and the Viterbi decoding algo-
rithm is further used to search an optimal sequence 
of POS tags for the sentence. The details of lexical-
ized HMMs can be seen in (Lee et al 200) and (Fu 
and Luke, 2005).  
3 Evaluation Results 
3.1 System Settings for Different Tracks 
The POS tagging task at the fourth ACL-SIGHAN 
bakeoff consists of five closed tracks. We partici-
pated four of them, namely CKIP, CTB, NCC and 
PKU. Therefore our system is trained only using 
the relevant training corpora provided for the 
bakeoff. Furthermore, the morpheme dictionaries 
for these tracks are also extracted automatically 
from the relevant training data with the method 
presented in Sections 2.3 and 2.4. Table 2 illus-
trated the number of morphemes extracted from 
different training data.  
 
Source Training data (tokens/word types) 
Number of 
morphemes 
CKIP 721551 / 48045 30757 
CTB 642246 / 42133 26330 
NCC 535023 / 45108 28432 
PKU 1116754 / 55178 30085 
Table 2. Number of morphemes extracted from the 
training data for SIGHAN POS tagging bakeoff 
 
3.2 Evaluation Results 
 
Track Total-A IV-R OOV-R MT-R 
CKIP-O 0.9124 0.9549 0.4756 0.8953 
CTB-O 0.9234 0.9507 0.52 0.9051 
NCC-O 0.9395 0.969 0.4086 0.9059 
PKU-C 0.9266 0.9574 0.4386 0.9079 
Table 3. Scores of our system for different tracks 
 
Table 3 presents the scores of our system for dif-
ferent tracks. It should be noted that four measures 
are employed in the 4th ACL-SIGHAN bakeoff to 
Tag Candidate Generation 
Scoring & Decoding 
Morph dictionary 
A segmented sentence 
Morpheme Segmentation 
A POS-tagged sentence 
Lexicalized HMMs 
126
Sixth SIGHAN Workshop on Chinese Language Processing
score the performance of a POS tagging system, 
namely the overall accuracy (Total-A) and the re-
call with respect to in-vocabulary words (IV-R), 
OOV words (OOV-R) or multi-POS words (MT-
R).  
Although our system has achieved a promising 
performance, there is still much to be done to im-
prove it. First, the quality of the morpheme dic-
tionary is of particular importance to morpheme-
based POS tagger. Although the present study pro-
posed a statistical technique to extract morphemes 
from tagged corpora, further exploration is still 
needed on the optimization of this technique to 
acquire a more desirable morpheme dictionary for 
Chinese POS tagging. Second, morphological pat-
terns prove to be informative cues for Chinese POS 
disambiguation and OOV word prediction. How-
ever, such a knowledge base is not publicly avail-
able for Chinese. As such, in the present study we 
only made use of certain surface morphological 
features, namely the position patterns of mor-
phemes in word formation. Future research might 
usefully extend the present method to explore sys-
tematically more precise morphological features, 
including morpheme POS categories and morpho-
syntactic rules for Chinese POS tagging. 
4 Conclusion 
In this paper we have presented a morpheme-based 
POS tagger for Chinese. We participated in four 
closed tracks at the fourth SIGHAN bakeoff. The 
scored results show that our system can achieve an 
overall accuracy of 0.9124-0.9395 for different 
corpora. However, the present system is still under 
development, especially in morphological knowl-
edge acquisition. For future work, we hope to im-
prove our system with a higher quality morpheme 
dictionary and more deep morphological knowl-
edge such as morpheme POS categories and mor-
pho-syntactic rules. 
Acknowledgments 
This study was supported in part by CityU Stra-
tegic Research Grant for fundable CERG (No. 
7001879 & 7002037). 
References 
E. Nishimoto. 2003. Measuring and comparing the pro-
ductivity of Mandarin Chinese suffixes. Computa-
tional Linguistics and Chinese Language Processing, 
8(1): 49-76. 
G. Fu and K.-K. Luke. 2005. Chinese named entity rec-
ognition using lexicalized HMMs. ACM SIGKDD 
Explorations Newsletter, 7(1): 19-25. 
G. Fu and K.-K. Luke. 2006. Chinese POS disambigua-
tion and unknown word guessing with lexicalized 
HMMs. International Journal of Technology and 
Human Interaction, 2(1): 39-50. 
H. Tseng and K.-J. Chen. 2002. Design of Chinese mor-
phological analyzer. In: Proceedings of the 1st 
SIGHAN Workshop on Chinese Language Process-
ing, 1-7. 
H. Tseng, D. Jurafsky, and C. Manning. 2005. Morpho-
logical features help POS tagging of unknown words 
across language varieties. In: Proceedings of the 
Fourth SIGHAN Workshop on Chinese Language 
Processing. 
H.T. Ng and J.K. Low. 2004. Chinese part-of-speech 
tagging: One-at-a-time or all-at-once? Word-based or 
character-based?. In: Proceedings of the 2004 Con-
ference on Empirical Methods in Natural Language 
Processing (EMNLP 2004), Barcelona, Spain, 277-
284. 
J. Packard. 2000. Morphology of Chinese: A linguistic 
and cognitive approach. Cambridge University Press, 
Cambridge, UK. 
R. Sproat and C. Shih. 2002. Corpus-based methods in 
Chinese morphology. In: Proceedings of the 19th In-
ternational Conference on Computational Linguistics 
(COLING 2002), Taipei, Taiwan. 
R.H. Baayen. 1989. A corpus-based study of morpho-
logical productivity: Statistical analysis and psycho-
logical interpretation. Ph.D. thesis, Free University, 
Amsterdam. 
S.-Z. Lee, T.-J. Tsujii, and H.-C. Rim. 2000. Lexical-
ized hidden Markov models for part-of-speech tag-
ging. In: Proceedings of the 18th International Con-
ference on Computational Linguistics (COLING 
2000), Saarbruken, Germany, 481-487. 
Z. Wu, G. Tseng. 1995. ACTS: An automatic Chinese 
text segmentation systems for full text retrieval. 
Journal of the American Society for Information Sci-
ence, 46(2): 83-96. 
 
127
Sixth SIGHAN Workshop on Chinese Language Processing
Coling 2010: Poster Volume, pages 312?319,
Beijing, August 2010
Chinese Sentence-Level Sentiment Classification Based on Fuzzy Sets 
Guohong Fu and Xin Wang 
School of Computer Science and Technology, Heilongjiang University 
ghfu@hlju.edu.cn,wangxincs@hotmail.com 
 
Abstract 
This paper presents a fuzzy set theory 
based approach to Chinese sentence-level 
sentiment classification. Compared with 
traditional topic-based text classification 
techniques, the fuzzy set theory provides 
a straightforward way to model the 
intrinsic fuzziness between sentiment 
polarity classes. To approach fuzzy 
sentiment classification, we first propose 
a fine-to-coarse strategy to estimate 
sentence sentiment intensity. Then, we 
define three fuzzy sets to represent the 
respective sentiment polarity classes, 
namely positive, negative and neutral 
sentiments. Based on sentence sentiment 
intensities, we further build membership 
functions to indicate the degrees of an 
opinionated sentence in different fuzzy 
sets. Finally, we determine sentence-level 
polarity under maximum membership 
principle. We show that our approach can 
achieve promising performance on the 
test set for Chinese opinion analysis pilot 
task at NTCIR-6. 
1 Introduction 
With the explosive growth of the user-generated 
content on the web over the past years, opinion 
mining has been attracting an ever-increasing 
amount of attention from the natural language 
processing community. As a key issue in 
opinions mining, sentiment classification aims to 
classify opinionated documents or sentences as 
expressing positive, negative or neutral opinions, 
and plays a critical role in many opinion mining 
applications such as opinion summarization and 
opinion question answering. 
Although recent years have seen a great 
progress in sentiment analysis, it is still 
challenging to develop a practical sentiment 
classifier for open applications. This is largely 
due to the particularities of subjective languages. 
Unlike factual text, opinion text is usually 
expressed in a more subtle or arbitrary manner 
(Pang and Lee, 2008). Moreover, the sentiment 
orientation of a subjective expression is often 
context, domain and/or even order-dependent 
(Pang and Lee, 2008). This makes it hard to 
explore informative cues for sentiment 
classification. In particular, the final semantic 
orientation of an opinionated sentence often 
depends on the synthetic effects of all sentiment 
units (e.g. sentiment words or phrases) within it. 
Therefore, sentiment granularity selection and 
polarity aggregation are two important factors 
that affect sentiment classification performance. 
In addition, real opinion texts do not contain 
precisely-defined criteria of membership with 
respect to polarity classes. Most current work 
employs supervised machine learning techniques 
like naive Bayesian models and support vector 
machines to perform sentiment classification. 
While they have shown a good performance in 
traditional topic-based text classification tasks 
(Wang, 2006), their applications in sentiment 
classification are far from satisfactory (Pang et 
al., 2002). The reason might be the intrinsic 
fuzziness between sentiment polarity classes. 
Relative to the concept of objective topics like 
sports and politics in traditional text 
classification, the division between positive 
sentiments and negative sentiments is rather 
vague, which does not make clear boundary 
between their conceptual extensions. Such vague 
conceptual extension in sentiment polarity 
inevitably raises another challenge to sentiment 
classification.  
312
To address the above problems, in this paper 
we exploit fuzzy set theory to perform Chinese 
sentiment classification at sentence level. To 
approach this task, we first consider multiple 
sentiment granularities, including sentiment 
morphemes, sentiment words and sentiment 
phrases, and develop a fine-to-coarse strategy for 
computing sentence sentiment intensity. Then, 
we reformulate the three classes of sentiment 
orientations, namely positive, negative and 
neutral sentiments, as three fuzzy sets, 
respectively. To describe the membership of an 
opinion sentence in a special sentiment fuzzy set, 
we further construct membership functions based 
on sentence sentiment intensity, and thus 
determine the final semantic orientation of a 
given opinionated sentence under the principle of 
maximum membership. We show that the 
proposed approach can achieve a promising 
performance on the test set for Chinese opinion 
analysis pilot task at NTCIR-6. 
The remainder of the paper is organized as 
follows: Section 2 provides a brief review of the 
literature on sentiment classification. In Section 3, 
we describe the fine-to-coarse strategy for 
estimating sentiment intensity of opinionated 
sentences. Section 4 details how to apply fuzzy 
set theory in sentiment classification. Section 5 
reports our experimental results on NTCIR-6 
Chinese opinion data. Finally, section 6 
concludes our work and discusses some possible 
directions for future research. 
2 Related Work 
Sentiment classification has been extensively 
studied at different granularity levels. At lexical 
level, Andreevskaia and Bergler (2006) exploit 
an algorithm for extracting sentiment-bearing 
adjectives from the WordNet based on fuzzy 
logic. Following (Turney, 2002), Yuen et al 
(2004) investigate the association between 
polarity words and some strongly-polarized 
morphemes in Chinese, and present a method for 
inferring sentiment orientations of Chinese words. 
More recently, Ku et al (2009) consider eight 
morphological types that constitute Chinese 
opinion words, and develop a machine learning 
based classifier for Chinese word-level sentiment 
classification. They show that using word 
structural features can improve performance in 
word-level polarity classification. At phase level, 
Turney (2002) presents a technique for inferring 
the orientation and intensity of a phrase 
according to its PMI-IR statistical association 
with a set of strongly-polarized seed words. 
More recently, Wilson et al (2009) distinguish 
prior and contextual polarity, and thus describe a 
method to phrase-level sentiment analysis. At 
sentence level, Yu and Hatzivassiloglou (2003) 
propose to classify opinion sentences as positive 
or negative in terms of the main perspective 
being expressed in opinionated sentences. Kim 
and Hovy (2004) try to determine the final 
sentiment orientation of a given sentence by 
combining sentiment words within it. However, 
their system is prone to produce error sentiment 
classification because they only consider 
sentiment words near opinion holders and ignore 
some important words like adversative 
conjunctions. To compute sentiment intensity of 
opinionated sentences, in this study we propose a 
fine-to-coarse strategy, which take into account 
multiple granularity sentiments, from sentiment 
morphemes, sentiment words to sentiment 
phrases, and can thus handle both unknown 
lexical sentiments and contextual sentiments in 
sentiment classification. 
Most recent studies apply machine learning 
techniques to perform sentiment classification. 
Pang et al (2002) attempt three machine learning 
methods, namely naive Bayesian models, 
maximum entropy and support vector machines 
in sentiment classification. They conclude that 
the traditional machine learning methods do not 
perform well enough in sentiment analysis. 
Wilson et al (2009) further employ several 
machine learning algorithms to explore important 
features for contextual polarity identification. 
Different from most existing works that focus on 
traditional text classification techniques, in this 
study we attempt to resolve sentiment 
classification problems under the framework of 
fuzzy set theory. We choose fuzzy set theory 
because it provides a more straightforward way 
to represent the intrinsic fuzziness in sentiment. 
3 Sentence-Level Sentiment Intensity 
In this section, we describe a fine-to-coarse 
strategy to compute sentence-level sentiment 
intensity. After a brief discussion of the 
relationship between Chinese sentiment words 
and their component morphemes in Section 3.1, 
313
we extract a dictionary of sentiment morphemes 
from a sentiment lexicon, and compute their 
opinion scores using a modified chi-square 
technique. Then, we develop two rule-based 
strategies for word-level and phrase-level 
polarity identification, respectively. Finally, we 
calculate the final sentiment intensity of an 
opinionated sentence by summing the opinion 
score of all phrases within it. 
3.1 Sentiment words and morphemes 
As shown in Table 1, Chinese sentiment words 
can be categorized into static polar words and 
dynamic polar words. The polarity of a static 
polar word remains unchanged while a dynamic 
polar word may have different polarity in 
different contexts or domains.  
 
Type Example 
Positive 
?? ?beautiful?, ?? ?gentle? 
Negative 
?? ?beggary?, ???wrong? 
Static 
polar 
word Neutral 
??? ?acceptable? 
Dynamic polar words 
??big?, ??high? 
Table 1. Types of Chinese sentiment words 
 
For a static polar word, its polarity can be 
easily determined by referring to a sentiment 
lexicon. However, a precompiled dictionary 
cannot cover all sentiment words in real text, 
which raises an issue of predicting the polarity of 
out-of-vocabulary (OOV) sentiment words. To 
address this problem, we introduce sentiment 
morphemes. As Table 2 shows, here we consider 
two types of sentiment morphemes, namely 
positive morphemes and negative morphemes.  
 
Morpheme 
types 
Sentiment 
morphemes 
Sentiment words 
composed by sentiment 
morphemes 
??beauty? ?? ?exquisite? 
?? ?graceful? Positive 
morphemes 
??love? ?? ?like? 
?? ?adoration? 
??dirty? ?? ?pollution? 
?? ?corruption? Negative 
morphemes 
??fail? ?? ?corruption? 
?? ?undermine? 
Table 2. Types of Chinese sentiment morphemes 
 
In most cases, the polarity of a sentiment word 
is closely related to the semantic orientation of 
its component morphemes. In other words, word-
level polarity can often be determined by some 
key component sentiment morphemes within 
sentiment words. Take the following three 
sentiment words for example, ?? ?undermine?, 
?? ?corruption?, and ?? ?degenerate?. They 
share a same negative sentiment morpheme ? 
?fail?, and thus have the same negative 
orientation. Based on this observation, here we 
use morpheme-level polarity, rather than a 
sentiment lexicon, to predict the polarity of static 
sentiment words, particularly the OOV sentiment 
words in real text. 
As for dynamic sentiment words, traditional 
lexicon-based methods do not work for their real 
polarity changes with contexts. We will discuss 
the problem of dynamic polarity identification in 
Section 3.4.  
3.2 Identifying morpheme-level polarity 
Sentiment morphemes prove to be helpful in 
dealing with OOV polarity (Ku et al 2009). 
However, there is not a dictionary of sentiment 
morphemes available for sentiment analysis. To 
avoid this, we propose to automatically extract 
sentiment morphemes from some existing 
sentiment lexicon using chi-square (?2) technique. 
Formula (1) presents the ?2 of a morpheme m 
within a sentiment word of category c. 
))()()((
)(),(
2212211122211211
2
211222112
nnnnnnnn
nnnnn
cm
++++
????
=? (1) 
where m denotes a sentiment morpheme. c? 
{positive, negative} denotes the polarity of a 
certain sentiment word w that contain m. n is the 
total number of sentiment words in the lexicon. 
To calculate ?2, we need to construct a 2?2 
contingency table from the sentiment lexicon. As 
shown in Table 3, n11, n12, n21 and n22 denote the 
observed frequencies, respectively. 
 
Polar word w belong to c not belong to c 
contain m n11 n12 
not contain m n21 n22 
Table 3. The 2?2 contingency table for ?2 
 
The traditional ?2 statistics in Formula (1) can 
demonstrate the degree of contributions that a 
sentiment morpheme forms a special group of 
sentiment words. However, it cannot indicate 
whether the morpheme and the sentiment 
category are either positively- or anti-correlated. 
314
Such information is very important for inferring 
word-level polarity from sentiment morphemes. 
To compensate for this deficiency, we modify 
the traditional ?2 by injecting positive correlation 
and anti-correlation. Following (Wang, 2006), 
we introduce the following two rules in 
determining the sign of correlation between the 
sentiment category of words and their component 
sentiment morphemes. 
 If n11?n22-n12?n21>0, the morpheme and 
the sentiment category are positively 
correlated. In this case, a larger ?2 implies 
a higher likelihood that the morpheme 
belongs to the sentiment category. 
 If n11?n22-n12?n21<0, the morpheme and 
the sentiment category are anti-correlated. 
In this case, a larger ?2 value implies a 
higher likelihood that the morpheme does 
not belong to the sentiment category. 
Thus, we obtain a modified ?2 statistics as 
follows. 
))()()((
)()('
2212211122211211
2
21122211
21122211
2
nnnnnnnn
nnnnn
nnnnsign
++++
????
???=? (2) 
With the ?2' statistic, we can build a dictionary 
of sentiment morphemes from a source sentiment 
lexicon, and further determine the polarity of 
each sentiment morpheme using the two rules as 
shown in Definitions 1 and 2.  
Definition 1 (positive sentiment morphemes). 
If the ?2' statistic between a morpheme m and 
positive sentiment words is greater than zero, 
then m can be identified as positive. 
Definition 2 (negative sentiment morphemes). 
If the ?2' statistic between a morpheme m and 
positive sentiment words is smaller than zero, 
then m can be identified as is negative. 
Table 4 illustrates some extracted sentiment 
morphemes and their ?2' values. 
 
Types of morphemes Examples 
?
2
? 
??beautiful? 111.78 
??love? 65.88 Positive morphemes 
??happy? 40.72 
??die? -104.97 
??failed? -45.28 Negative morphemes 
??evil? -72.37 
Table 4. ?2? values of sentiment morphemes 
3.3 Identifying word-level polarity 
To determine word-level polarity, we employ 
morpheme-based rules. First of all, we normalize 
the ?2' value of each sentiment morpheme m into 
[-1, 1] by dividing it with the maximum absolute 
value. Such normalized chi-square, denoted by 
chi(m), is further viewed as the opinion score of 
the sentiment morpheme m. Thus, we can 
determine whether a word is a sentiment or not 
using a simple rule: if a word contains sentiment 
morphemes, it is a sentiment word. Finally, we 
can calculate the opinion score of a word w 
consisting of morphemes mi, (1?i?2)1, using the 
following two rules. 
 If m1 is a negation, e.g. ? ?not? and ? 
?non-?, then Score(w)= -1? chi(m2). 
 If m1 is not a negation morpheme, then 
Score(w)=Sign(chi(mi))?Max(|chi(mi)|). 
Where, Max(|chi(mi)|) is the largest 
absolute value among the opinion scores 
of morphemes within a word w, 
Sign(chi(mi)) denotes the positive or 
negative sign of m, namely ?-? and ?+?. 
3.4 Identifying phrase-level polarity 
To handle contextual polarity, we apply lexical 
polarity to determine the sentiment orientation of 
phrases within an opinionated sentence. Based on 
(Hatzivassiloglou and Wiebe, 2000) and (Turney, 
2002), we consider four types of structures (as 
shown in Table 5) during sentiment phrase 
extraction. To simplify the process, we reduce 
some function words like ? ??s? and ? ?and? 
from the input sentences before extraction in that 
they have no influence on sentiment orientation 
determination, and focus on extracting two 
consecutive words. Different from (Turney, 
2002), we consider phrases with negations as 
their initial words. In this way, we can handle the 
local negation that may reverse polarity. 
 
Phrase structures Examples 
Phases containing a 
adjective ???? ?high success rate? 
Phrases containing a 
verb ?????carefully discuss? 
Phrase containing an 
idiom 
?????? /?intent to 
deceive the public? 
Phrases beginning with 
a negation ?????no evidence? 
Table 5. Structures of opinion phrases 
                                               
1
 For words that contain three or more characters, 
particularly the four-character idioms, their polarity 
can be determined using the second rule. 
315
After opinion phrase extraction, we continue 
to calculate the opinion score of the extracted 
phrases using rules that are similar to (Hu and 
Liu, 2004). Before going to the details of phrase-
level opinion score calculation, we need to give 
some definitions in advance. 
Definition 3 (increased dynamic polar words). 
An increased dynamic polarity word can increase 
the orientation strength of sentiment words that it 
modifies without changing their polarity. For 
example, the word ?  ?serious? in the phrase 
??? ?serious pollution? and the word ? 
?high? in  the phrase ??? ?high benefit?. 
Definition 4 (decreased dynamic polar word). 
A decreased dynamic polarity word can decrease 
the orientation strength of sentiment words that it 
modifies and at the same time, reverse their 
polarity. For example, the word ??? ?little? in 
the phrase??? ?little pollution? and the word 
? ?low? in the phrase ??? ?low benefit?.  
To calculate phrase-level opinion scores, we 
construct a dictionary of dynamic polar words by 
extracting adjectives and verbs that contain a 
single-character seed morpheme like ? ?little? 
from the training corpus. Table 6 illustrates some 
increased and decreased dynamic polar words 
and their signs for changing polarity.  
 
Dynamic 
polar word Example Polarity sign 
Increased 
? ?high?  
?? ?increase? 
?? ?upgrade? 
Sign(increased)=1 
Decreased 
?? ?down? 
?? ?reduce?  
?? ?diminish? 
Sign(decreased)=-1 
Table 6. Dynamic words and their polarity sign 
 
With these dynamic polar words, we can then 
calculate the opinion score of a given opinion 
phrase pi that consists of two words (denoted by 
wj, j?{1,2}), using three rules as follows. 
 If w1 is a negation, e.g. ? ?no? and ?? 
?without?, then Score(pi) =  -1?Score(w2). 
 If pi involves a dynamic word wd, then  
Score(pi) = Sign(wd) ? Score(wj). Where, 
Sign(wd) denotes the polarity sign of 
dynamic words shown in Table 6.  
 Otherwise, Score(pi) = Sign(wj) ? Max 
(|Score(wj)|). Where Max(|Score(wordj)|) 
is the largest absolute value among the 
word-level opinion scores. 
4 Sentence Sentiment Classification 
4.1 Sentiment fuzzy sets and membership 
functions 
As we have mentioned above, sentiment polarity 
is vague with regard to its conceptual extension. 
There is not a clear boundary between the 
concepts of ?positive?, ?neutral? and ?negative?. 
To better handle such intrinsic fuzziness in 
sentiment polarity, we apply the fuzzy set theory 
by (Zadeh, 1965) to sentiment classification. To 
do so, we first redefine sentiment classes as three 
fuzzy sets, and then apply existing fuzzy 
distributions to construct membership functions 
for the three sentiment fuzzy sets.  
In our formulation, all the opinionated 
sentences under discussion are represented as a 
sorted set, denoted by X, in terms of their opinion 
scores. Thus, we have X = [Min(Opinion  
Score(Si)), ?, Max(Opinion Score(Si))]. Where, 
i={1,?,n}, Min(Opinion Score(Si)) and 
Max(Opinion Score(Si)) denotes the respective 
minimum and maximum opinion scores. The 
details of the fuzzy sets and their membership 
functions are given in Definitions 5, 6 and 7, 
respectively. 
Definition 5 (positive sentiment fuzzy set). if X 
is a collection of sentiment opinions (denoted by 
x), then a positive sentiment fuzzy set P~ in X can 
be defined as a set of ordered pairs, namely 
}|))(,{(~ ~ XxxxP P ?= ? , 
where )(~ xP?  denotes the membership function of 
x in P~  that maps X to the membership space M. 
We choose the rise semi-trapezoid distribution 
(Zimmermann, 2001) as the membership 
function of the positive sentiment fuzzy set, 
namely 
            
?
?
?
?
?
?
?
>
??
?
?
<
=
bx
bxa
ab
ax
ax
xP
,1
,
,0
)(~?              (3) 
where x denotes the opinion score of a sentence 
under discussion. The adjustable parameters a 
and b can be defined as a = Min(xi) + ?1(Max(xi) 
- Min(xi)/k) and b = Min(xi) + ?2(Max(xi) - 
Min(xi)/k), respectively. Max(xi) and Min(xi) 
316
denote the respective minimum and maximum 
values within X. ?1, ?2 and k are parameters. Here 
we set ?1= 5.2, ?2 = 5.4, and k = 10. 
Definition 6 (neutral sentiment fuzzy set). if X 
is a collection of sentiment opinions (denoted by 
x), then a neutral sentiment fuzzy set E~ in X can 
be defined as a set of ordered pairs, namely 
}|))(,{(~ ~ XxxxE E ?= ? , 
where )(~ xE?  denotes the membership function of 
x in E~  that maps X to the membership space M. 
As shown in Formula (4), we also select the 
semi-trapezoid distribution (Zimmermann, 2001) 
as the membership function of the neutral 
sentiment fuzzy set. 
         
?
?
?
?
?
?
?
?
?
?
?
?
<?
?
?
<?
<?
?
?
<
=
dx
dxc
cd
xd
cxb
bxa
ab
ax
ax
xE
,0
,
,1
,
,0
)(~?           (4) 
where x denotes the opinion score of a sentence 
under test. a, b, c and d are adjustable parameters 
that can be defined as a = Min(xi) + ?1(Max(xi)- 
Min(xi)/k), b=Min(xi) +m1(Max(xi) - Min(xi)/k), 
c = Min(xi) + m2(Max(xi) - Min(xi)/k) and d= 
Min(xi) + ?2(Max(xi) - Min(xi)/k), respectively. 
Max(xi) and Min(xi) denotes the respective 
minimum and maximum values within X. ?1, ?2, 
m1, m2  and k are parameters, Here  we set ?1 = 
5.2, ?2 = 5.5, m1 = 5.26, m2 = 5.33, and k = 10. 
Definition 7 (negative sentiment fuzzy set). if X 
is a collection of sentiment opinions (denoted by 
x), then a negative sentiment fuzzy set N~ in X 
can be defined as a set of ordered pairs, namely 
}|))(,{(~ ~ XxxxN N ?= ? , 
where )(~ xN?  denotes the membership function 
of x in N~  that maps X to membership space M. 
To represent the membership function of the 
negative sentiment fuzzy set, we employ the drop 
semi-trapezoid distribution (Zimmermann, 2001), 
namely 
           
?
?
?
?
?
?
?
>
??
?
?
<
=
bx
bxa
ab
xb
ax
xN
,0
,
,1
)(~?                (5) 
where x denotes the opinion score of a subjective 
sentence under discussion. The adjustable 
parameters a and b can be defined as a = Min(xi) 
+ ?1(Max(xi) - Min(xi)/k) and b = Min(xi) + 
?2(Max(xi) - Min(xi)/k), respectively. Max(xi) and 
Min(xi) refer to the corresponding minimum and 
maximum values in X. ?1, ?2, and k are 
parameters. Here we set ?1=5.2, ?2=5.3 and k=10. 
4.2 Determining sentence polarity 
Based on the above membership functions, we 
can now calculate the grade of membership of a 
given opinionated sentence in each sentiment 
fuzzy set, and thus determine its polarity under 
the principle of maximum membership. The 
basic idea is as follows: Let ?1, ?2, ?, ?n  be the 
fuzzy sets of X. ?x
0
?X, if 
~ ~
0 01
( ) max{ ( )}k ii nA x A x? ?=
 
then x0 is a membership of the fuzzy set ?k.  
5 Experiments and Results 
To assess the effectiveness of our approach, we 
implemented a classification system for Chinese 
sentence-level sentiment analysis. The system 
involves three main modules, namely a lexical 
analysis module, a subjectivity detection module 
and a sentiment classification module. To 
explore lexical cues for sentiment analysis, the 
morpheme-based chunking technique by (Fu, Kit 
and Webster, 2008) is employed in the lexical 
analysis module to carry out word segmentation 
and part-of-speech tagging tasks. To conform to 
the NTCIR-6 evaluation, a sentiment density-
based naive Bayesian classifier is also embedded 
in the second module to perform opinionated 
sentence detection. The details of this classifier 
can be seen in (Wang and Fu, 2010). To evaluate 
our system, we conducted experiments on the 
NTCIR-6 Chinese opinion data. This section 
reports the experimental results. 
5.1 Experimental setup 
In our experiments, we use the same test set for 
the Chinese opinion analysis tasks at NTCIR-6. 
The basic statistics is presented in Table 7. For 
comparison, the performance is reported in terms 
of the same metrics as used in NTCIR-6. They 
are F-score (F), recall (R), precision (P) under 
the LWK evaluation with lenient standard. 
317
 Item Number 
Topics 32 
Documents 843 
Sentences 11907 
Opinionated sentences under the 
lenient standard 
62% 
Table 7. Basic statistics of the test set for 
Chinese opinion tasks at NTCIR-6 
 
The basic sentiment lexicon used in our 
system contains a total of 17138 sentiment words, 
which is built from the CUHK and NTU 
sentiment lexica by excluding some derived 
opinion words like ??? ?not beautiful?. In 
addition, we also construct a list of 95 dynamic 
polarity words using the method described in 
Section 3.4. 
5.2 Experimental results 
The experiments are designed to examine the 
following two issues: 
(1) As we have discussed above, it is a key 
issue to select a proper granularity for sentiment 
classification. To determine the sentiment 
orientation of an opinionated sentence, we use a 
fine-to-coarse strategy that considers three types 
of sentiment units, namely sentiment morphemes, 
sentiment words and sentiment phrases. 
Therefore, the first intention of our experiments 
is to investigate how the use of different 
sentiment granularity affects the performance of 
Chinese sentence-level sentiment classification. 
To do this, we take the above three sentiment 
granularity as the basic units for computing 
sentence-level sentiment intensity, respectively, 
and examine the relevant sentiment classification 
results.  
(2) To the best of our knowledge, this study 
may be the first attempt to apply the fuzzy set 
theory in Chinese sentiment classification. 
Therefore, our second motivation is to examine 
whether it is feasible to apply fuzzy set theory in 
sentiment classification by comparing our system 
with other public systems for Chinese opinion 
analysis pilot task at NTCIR-6.  
Table 8 presents the experimental results with 
different sentiment granularities. It can be 
observed that the system with word as the basic 
sentiment units slightly performs better than the 
system based on sentiment morphemes. But a 
prominent improvement of performance can be 
obtained after using sentiment phrases. This 
reason may be that under the fine-to-coarse 
framework, sentiment classification based on 
sentiment phrases can handle both internal and 
external contextual sentiment information, and 
can thus result in performance improvement. 
 
Granularity P R F 
Morpheme 0.389 0.480 0.430 
Word 0.393 0.485 0.434 
Phrase 0.415 0.512 0.458 
Table 8. Performance on sentiment classification 
with different sentiment granularity 
 
Table 9 illustrates the comparison of our 
system with the best system for Chinese opinion 
analysis pilot task at NTCIR-6, namely the 
CUHK system (Seki et al, 2007; Xu, Wong and 
Xia, 2007). As can be seen from Table 9, our 
system outperforms the CUHK system by 5 
percents with regard to F-score, showing the 
feasibility of using fuzzy set theory in sentiment 
classification. 
 
System P R F 
CUHK 0.522 0.331 0.405 
Our system 0.415 0.512 0.458 
Table 9. Comparison of our system with the best 
system at NTCIR-6 under lenient standard 
6 Conclusion and Future Work 
In this paper, we have described a fuzzy set 
theory based framework for Chinese sentence-
level sentiment classification. To handle 
unknown polarity and contextual polarity as well, 
we consider three types of sentiment 
granularities, namely sentiment morphemes, 
words and phrases in calculating sentiment 
intensity of opinionated sentenced. Furthermore, 
we define three fuzzy sets to represent polarity 
classes and construct the relevant membership 
functions, respectively. Compared with most 
existing work, the proposed approach provides a 
straightforward way to model the vagueness in 
conceptual division of sentiment polarity. The 
experimental results show that our system 
outperforms the best system for Chinese opinion 
analysis pilot task at NTCIR-6 under the lenient 
evaluation standard.  
The encouraging results of the fuzzy set-based 
approach suggest several possibilities for future 
318
research. Our experiments demonstrate that the 
incorporation of multiple granularity polarity has 
a positive effect on sentiment classification 
performance. To further enhance our system, in 
future we intend to exploit more tailored 
techniques for aggregating multiple-granularity 
polarity within opinionated sentences. Moreover, 
we plan to optimize the proposed membership 
functions for fuzzy sentiment classification. 
Acknowledgments 
The authors would like to thank Chinese 
University of Hong Kong, National Taiwan 
University and NTCIR for their data. This study 
was supported by National Natural Science 
Foundation of China under Grant No.60973081, 
the Returned Scholar Foundation of Educational 
Department of Heilongjiang Province under 
Grant No.1154hz26, and Harbin Innovative 
Foundation for Returnees under Grant 
No.2009RFLXG007, respectively. 
References 
Alina Andreevskaia, and Sabine Bergler. 2006. 
Mining WordNet for a fuzzy sentiment: Sentiment 
tag extraction from WordNet glosses. In 
Proceedings of EACL-06, pages 209-216. 
Xiaowen Ding, Bing Liu, and Philip S. Yu. 2008. A 
holistic lexicon-based approach to opinion mining. 
In Proceedings of the International Conference on 
Web Search and Web Data Mining, pages 231-240. 
Guohong Fu, Chunyu Kit, and Jonathan J. Webster. 
2008. Chinese word segmentation as morpheme-
based lexical chunking. Information Sciences, 
7(1):2282?2296. 
Vasileios Hatzivassiloglou, and Janyce Wiebe. 2000. 
Effects of adjective orientation and gradability on 
sentence subjectivity. In Proceedings of ACL-00, 
pages 299-305. 
Soo-Min Kim, and Eduard Hovy. 2004. Determining 
the sentiment of opinions. In Proceedings of 
COLING-04, pages 1267-1373. 
Lun-Wei Ku, Ting-Hao Huang, and Hsin-Hsi Chen. 
2009. Using morphological and syntactic structures 
for Chinese opinion. In Proceedings of EMNLP-09, 
pages 1260-1269. 
Bo Pang, Lillian Lee, and Shivakumar Vaithyanathan.  
2002. Thumps up? Sentiment classification using 
machine learning techniques. In Proceedings of 
EMNLP-02, pages 79-86. 
Bo Pang, and Lillian Lee. 2008. Opinion mining and 
sentiment analysis. Foundations and Trends in 
Information Retrieval, 2(1-2): 1-135. 
Yohei Seki, David Kirk Evans, Lun-Wei Ku, Hsin-
Hsi Chen, Noriko Kando, and Chin-Yew Lin. 2007. 
Overview of opinion analysis pilot task at NTCIR-
6. In Proceedings of NTCIR-6 Workshop Meeting, 
pages 265-278. 
Peter D. Turney. 2002. Thumbs up or thumbs down? 
Semantic orientation applied to unsupervised 
classification of reviews. In Proceedings of ACL-
03, pages 417-424. 
Xin Wang, and Guohong Fu. 2010. Chinese 
subjectivity detection using a sentiment density-
based na?ve Bayesian classifier. In Proceedings of 
IWWIP-10.  
Yu Wang. 2006. Research on text categorization 
based on decision tree and K-nearest neighbors. 
PhD thesis, Tianjin University. 
Theresa Wilson, Janyce Wiebe, and Paul Hoffmann. 
2009. Recognizing contextual polarity: An 
exploration of features for phrase-level sentiment 
analysis. Computational Linguistics, 35(3):99-433. 
Ruifeng Xu,  Kam-Fai Wong, and Yunqing Xia. 2007. 
Opinmine: Opinion mining system by CUHK for 
NTCIR-06 Pilot Task. In Proceedings of NTCIR-6 
Workshop Meeting, pages 350-357. 
Hong Yu, and Vasileios Hatzivassiloglou. 2003. 
Towards answering opinion questions: Separating 
facts from opinions and identifying the polarity of 
opinion sentences. In Proceedings of EMNLP-03, 
pages 129-136. 
Raymond W.M. Yuen, Terence Y.W. Chan, Tom B.Y. 
Lai, O.Y. Kwong, and Benjamin K.Y. T'sou. 2004. 
Morpheme-based Derivation of Bipolar Semantic 
Orientation of Chinese Words. In Proceedings of 
COLING-04, pages 1008-1014. 
Lotfi A. Zadeh. 1965. Fuzzy sets. Information and 
Control, 8:338-353. 
Hans-J?rgen Zimmermann. 2001. Fuzzy set theory 
and its applications. Kluwer Academic Publishers, 
Norwell, MA, USA. 
319
