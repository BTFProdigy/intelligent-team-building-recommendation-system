BioNLP 2007: Biological, translational, and clinical language processing, pages 181?182,
Prague, June 2007. c?2007 Association for Computational Linguistics
Information Extraction from Patients? Free Form Documentation
Agnieszka Mykowiecka
Institute of Computer Science, PAS
Ordona 21, 01-237 Warszawa, Poland
agn@ipipan.waw.pl
Ma?gorzata Marciniak
Institute of Computer Science, PAS
Ordona 21, 01-237 Warszawa, Poland
mm@ipipan.waw.pl
Abstract
The paper presents two rule-based infor-
mation extraction (IE) from two types of
patients? documentation in Polish. For
both document types, values of sets of at-
tributes were assigned using specially de-
signed grammars.
1 Method/General Assumptions
Various rule-based, statistical, and machine learn-
ing methods have been developed for the purpose
of information extraction. Unfortunately, they have
rarely been tested on Polish texts, whose rich in-
flectional morphology and relatively free word or-
der is challenging. Here, we present results of two
experiments aimed at extracting information from
mammography reports and hospital records of dia-
betic patients.1 Since there are no annotated corpora
of Polish medical text which can be used in super-
vised statistical methods, and we do not have enough
data for weakly supervised methods, we chose the
rule-based extraction schema. The processing pro-
cedure in both experiments consisted of four stages:
text preprocessing, application of IE rules based on
the morphological information and domain lexicons,
postprocessing (data cleaning and structuring), and
conversion into a relational database.
Preprocessing included format unification, data
anonymization, and (for mammography reports) au-
tomatic spelling correction.
The extraction rules were defined as grammars
of the SProUT system, (Droz?dz?yn?ski et al, 2004).
1This work was partially financed by the Polish national
project number 3 T11C 007 27.
SProUT consists of a set of processing components
for basic linguistic operations, including tokeniza-
tion, sentence splitting, morphological analysis (for
Polish we use Morfeusz (Wolin?ski, 2006)) and
gazetteer lookup. The SproUT components are com-
bined into a pipeline that generates typed feature
structures (TFS), on which rules in the form of reg-
ular expressions with unification can operate. Small
specialized lexicons containing both morphologi-
cal and semantic (concept names) information have
been created for both document types.
Extracted attribute values are stored in a rela-
tional database.2 Before that, mammography re-
ports results undergo additional postprocessing ?
grouping together of extracted data. Specially de-
signed scripts put limits that separate descriptions of
anatomical changes, tissue structure, and diagnosis.
More details about mammography IE system can be
found in (Mykowiecka et al, 2005).
2 Document types
For both document types, partial ontologies were de-
fined on the basis of sample data and expert knowl-
edge. To formalize them, we used OWL-DL stan-
dard and the Prote?ge? ontology editor. The excerpt
from the ontology is presented in Fig. 1.
In both cases, the relevant part of the ontology
was translated into a TFS hierarchy. This resulted in
176 types with 66 attributes for the mammography
domain, and 139 types (including 75 drug names)
with 65 attributes for diabetic patients? records.
2This last stage is completed for the diabetes reports while
for mammography it is still under development.
181
BiochemicalData: BloodData: HB1C
Diet
DiseaseOrSymptom
Disease
AutoimmuneDisease
Cancer
Diabetes: Type1, Type2, TypeOther
Symptom
Angiopathy: Macroangiopathy, Microangiopathy
BoodSymptom: Hypoglicaemia
Neuropathy: Autonomic, PeripheralPolineuropathy
UrineSymptom: Acetonuria, Microalbuminuria
Medicine
DiabeticMedicine: Insulin, OralDiabeticMedicine
AnatomicalLocalization
BodyPart
Breast: Subareola, urq, ulq, lrq, llq
BodySide: Left, Right
HistDiagnosis: Benign, Suspicious, Malignant
TissueSpecification: GlandularTissue, FatTissue
Figure 1: A sample of classes
3 Extraction Grammars
The number of rules is highly related to the number
of attributes and possible ways of formulating their
values. The grammar for mammography reports
contains 190 rules; that for hospital records contains
about 100 rules. For the first task, nearly the entire
text is covered by the rules, while for the second,
only a small part of the text is extracted (e.g., from
many blood tests we are interested only in HBA1C).
Polish inflection is handled by using the morpho-
logical analyzer and by inserting the most frequent
morphological forms into the gazetteer. Free word
order is handled either by rules which describe all
possible orderings, or by extracting small pieces of
information which are merged at the postprocessing
stage. Fig. 2 presents a fragment of one mammog-
raphy note and its output. The zp and zk markers are
inserted during the information structuring stage to
represent borders of an anatomical change descrip-
tion. Similar markers are introduced to structure the
tissue description part.
4 Evaluation
The experiments were evaluated on a set of previ-
ously unseen reports. Extraction of the following
structures was evaluated: 1) simple attributes (e.g.
diabetes balance); 2) structured attributes (e.g. lo-
calization); and 3) complex structures (e.g. descrip-
tion of abnormal findings). Evaluation of three se-
lected attributes from both sets is given in Fig. 3.
W obu sutkach rozsiane pojedyncze mikrozwapnienia o charak-
terze ?agodnym. Do?y pachowe prawid?owe. Kontrolna mam-
mografia za rok.
(Within both breasts there are singular benign microcalcifica-
tions. Armpits normal. Next control mammography in a year.)
zp LOC|BODY PART:breast||LOC|L R:left-right
ANAT CHANGE:micro||GRAM MULT:plural
zk DIAGNOSIS RTG:benign
DIAGNOSIS RTG:no susp||LOC D|BODY PART:
armpit||LOC D|L R:left-right
RECOMMENDATION|FIRST:mmg||TIME:year
Figure 2: A fragment of an annotated mammogra-
phy report
The worse results for unbalanced diabetes recogni-
tion were due to an unpredicted expression type.
mammography ? 705 reports
cases precision recall
findings 343 90.76 97.38
block beginnings 299 81.25 97.07
localizations 2189 98.42 99.59
diabetes ? 99 reports
unbalanced diabetes 58 96,67 69,05
diabetic education 39 97,50 97,50
neuropathy 30 100 96,77
Figure 3: Evaluation results for selected attributes
5 Conclusions
Despite the fact that rule based extraction is typi-
cally seen as too time consuming, we claim that in
the case of very detailed information searching, de-
signing rules on the basis of expert knowledge is in
fact a method of a real practical value. In the next
stage, we plan to use our tools for creating anno-
tated corpora of medical texts (manually corrected).
These data can be used to train statistical IE models
and to evaluate other extraction systems.
References
Agnieszka Mykowiecka, Anna Kups?c?, Ma?gorzata
Marciniak. 2005. Rule-based Medical Content Ex-
traction and Classification, Proc. of IIS: IIPWM05.
Advances in Soft Comp., Vol. 31, Springer-Verlag.
Witold Droz?dz?yn?ski and Hans-Ulrich Krieger and Jakub
Piskorski and Ulrich Scha?fer and Feiyu Xu. 2004.
Shallow Processing with Unification and Typed Fea-
ture Structures ? Foundations and Applications. Ger-
man AI Journal KI-Zeitschrift, 01/04.
Marcin Wolin?ski. 2006. Morfeusz ? a Practical Tool for
the Morphological Analysis of Polish, Proc. of IIS:
IIPWM06. Adv. in Soft Comp., Springer-Verlag.
182
Balto-Slavonic Natural Language Processing 2007, June 29, 2007, pages 35?42,
Prague, June 2007. c?2007 Association for Computational Linguistics
Automatic Processing of Diabetic Patients? Hospital Documentation
Ma?gorzata Marciniak
Institute of Computer Science, PAS
Ordona 21, 01-237 Warszawa, Poland
mm@ipipan.waw.pl
Agnieszka Mykowiecka
Institute of Computer Science, PAS
Ordona 21, 01-237 Warszawa, Poland
agn@ipipan.waw.pl
Abstract
The paper presents a rule-based information
extraction (IE) system for Polish medical
texts. We select the most important informa-
tion from diabetic patients? records. Most
data being processed are free-form texts,
only a part is in table form. The work has
three goals: to test classical IE methods on
texts in Polish, to create relational database
containing the extracted data, and to prepare
annotated data for further IE experiments.
1 Introduction
Information extraction from natural language text
has become an important task for NLP applica-
tions during the last years. In the era of huge text
data collections, these methods allows us to per-
form searches within reasonable time. For the pur-
pose of IE, many methods based on very differ-
ent approaches (formal grammars, statistics, artifi-
cial intelligence) have already been elaborated. In
spite of a great number of described experiments,
the invented methods were untested on Polish texts.
Nowadays, there is great interest in statistical and
machine learning methods, e.g. (Bunescu et al,
2003), but applying machine learning techniques to
Polish texts is difficult, as there are hardly any an-
notated Polish data (excluding morpho-syntactic in-
formation which is available). The second obstacle
was the type of chosen data ? relatively low num-
ber of available records with complex text. That is
why we decided to carry out a rule-based IE sys-
tem.1 Below, we present the system selecting infor-
mation from diabetic patients? hospital records writ-
ten in unrestricted Polish. We defined a domain de-
pendent set of rules basing on an expert?s knowl-
edge and tested them on the previously unseen re-
ports. The extracted data were put into a database
allowing for statistical analysis. The other result of
the project, the annotated set of original reports, can
be further used for applying other methods of IE to
these texts.
In our project, we use the SProUT (Shallow Pro-
cessing with Unification and Typed Feature Struc-
tures) system, (Droz?dz?yn?ski et al, 2004). SProUT
is a general purpose platform consisting of a set of
components for basic linguistic operations. Gram-
mar rules are regular expressions on typed feature
structures (TFS) which are results of tokenization
or morphological analysis, as well as information
from the domain lexicon. SProUT differs frommany
other systems in that it allows for unification of TFSs
thus allows more general extraction rules. Analysing
Polish text is possible due to the integration (Pisko-
rski et al, 2004) of Morfeusz, a morphological anal-
yser for Polish (Wolin?ski, 2006).
Although most biomedical IE activities are related
to literature mining and terminology extraction, (e.g.
(Bunescu et al, 2003), (Tveit and Saetre 2005)),
clinical patients record mining is not a new research
goal for all languages, e.g. (Hahn, Romacker and
Schulz, 2002). In (Hripcsak et al, 2002) 24 clini-
cal conditions were extracted from narrative chests
radiographic reports. The task closest to the pre-
1Our first rule-based IE experiment concerned mammogra-
phy reports (Mykowiecka, Kups?c? and Marciniak, 2004).
35
sented here, i.e. searching for information contained
in natural language patients? discharge summaries
was undertaken in project MENELAS (Zweigen-
baum, 1994) and AMBIT (Harkema et al, 2004). In
the last one, the extraction rules were based on both
syntactic (word category) and semantic information
(e.g. latitude-noun or area-noun). 83 radiology re-
ports were processed and descriptions of lung can-
cers extracted and evaluated. The exemplary results
for location were: 61% precision and 92% recall.
Results of our experiment are shown in sections 4
and 6.
2 Domain description
For the purpose of diabetic patients? hospital docu-
mentation analysis, we elaborated a domain model
for the part of data which we are interested in. The
model has been defined on the basis of an expert?s
knowledge and the data i.e. hospital documents. The
model describes information on a patient, hospitali-
sation, diagnosis, tests, treatment and complications.
To formalize it, we used OWL-DL standard and the
Prot?g? ontology editor. A part of the diabetes on-
tology is shown in Fig. 1.
For the purpose of information extraction in
SProUT, the ontology had to be translated (manu-
ally) into a typed feature structures hierarchy. In the
extraction system, the domain model is represented
by typed TFSs. A feature?s (attribute?s) value can be
an atomic type, another TFS, or a list of atomic types
or TFSs. The type hierarchy contains 139 types
with 65 attributes, but as much as 65 types represent
medicine terms.
An example of a structure defined to represent ba-
sic information about a patient?s diabetes is given in
Fig. 2. The structure is of the type diabet_desc_str
and has five attributes. A value of the D_TYPE at-
tribute has the type of d_type_t which is a supertype
for three types of diabetes: first, second, other. The
next attribute HBA1C refers to the results of an im-
portant diabetes monitoring test. Its numerical value
is represented as a string. Next two attributes are
of boolean type and indicate if the illness is uncon-
trolled and if the patient had incidences of hypogly-
caemia. A value of the last attribute DIAB_FROM
is another TFS of type diab_from_str representing
when the diabetes have been diagnosed. This infor-
BiochemicalData
BloodData
HB1C
Diet
DiabetDiet
DiseaseOrSymptom
Disease
Alcoholism
AutoimmuneDisease
Diabetes
DiabetesType1
DiabetesType2
DiabetesTypeOther
Symptom
Angiopathy
Macroangiopathy
Microangiopathy
BoodSymptom
Hypoglicemia
DiabeticFood
Neuropathy
AutonomicNeuropathy
PeripheralPolineuropathy
UrineSymptom
Acetonuria
Microalbuminuria
Hospitalization
Medicine
DiabeticMedicine
Insulin
OralDiabeticMedicine
Treatement
TreatementScheme
Figure 1: Fragment of the ontology
mation can be given in different ways: in words e.g.,
wieloletna ?long-lasting?; as a date ? w 1990 roku
?in the year 1990?; relatively 20 lat temu ?20 years
ago?; or w 20 roku z?ycia ?in the 20th year of life?.
All these types of information demand different rep-
resentation.
?
?
?
?
?
?
diabet_desc_str
D_TYPE d_type_t
HBA1C string
UNCONTROLLED bool_t
HYPOGLYCAEMIA bool_t
DIAB_FROM diab_from_str
?
?
?
?
?
?
Figure 2: Structure of type diabet_desc_str
Every one document we process concerns one pa-
tient?s visit in hospital. A particular visit is identified
(see Fig. 3) by two parameters: ID number within
a year and a year (attribute ID_YEAR). Sometimes
some results of tests are available after the patient
leaves the hospital. In such cases, there are addi-
36
tional hospital documents referring to these visits
described by an attribute CONT: yes ? continua-
tion.
?
?
?
id_str
ID string
ID_YEAR string
CONT bool_t
?
?
?
Figure 3: Visit?s identification structure
The specific structures are defined for represent-
ing the following information:
? identification of a patient?s visit in hospital,
dates when the hospitalisation took place, and
its reasons,
? patient information: identification, age, sex,
weight,
? data about diabetes (see Fig. 2),
? complications,
? other illnesses including autoimmunology and
accompanying illnesses, which may be corre-
lated with diabetes, like hypertension,
? diabetes treatment: recognition of insulin type
and its doses and other oral medications,
? diet: how many calories, and how many meals
are recommended,
? patient?s education, observing of diet, modifi-
cation of treatment or diet.
In order to represent complications we defined
the appropriate hierarchy. It contains three main
types of complications: angiopathy, neuropathy and
diabetic foot. The first two have subtypes. An-
giopathy divides into micro and macroangiopathy,
and neuropathy can be autonomic neuropathy or pe-
ripheral polineuropathy. Micro and macroangiopaty
has further subtypes. One common complication ?
rethinopathy is a subtype of microangiopathy and
has additional attribute, that represents information
about cooccurring maculopathy. Rethinopathy has
also subtypes.
Sometimes it is convenient to recognise more then
one complication through one rule. In this case, re-
sults are represented in a list. For example, the re-
sult of recognition of the following phrase describ-
ing complications z neuropatia? autonomiczna? i ob-
wodowa? ?with autonomic and peripheral neuropa-
thy? is represented in Fig. 4. These two complica-
tions cannot be identified separately, as there is only
one occurrence of the keyword neuropathy.
?
?
?
?
complication_list
FIRST autonomic_neuropathy
REST
[
complication_list
FIRST peripheral_polyneuropathy
REST null
]
?
?
?
?
Figure 4: List of complications
3 Information Extraction
3.1 Domain dictionary ? gazetteer
A domain dictionary contains all forms of the terms
important to the domain terminology. These terms
came from the data set or were introduced into the
lexicon on the basis of a domain expert?s knowl-
edge. The lexicon contains among others all insulin
and oral medication names important in diabetology,
we introduced forms in nominative and genitive (if
such exist) ? only these forms appeared in the doc-
uments. The other group of words in the dictionary
consists of names of diseases and diabetic complica-
tions. They have been introduced into the lexicon in
all forms used in the documents.
In this specific domain lexicon, there are no in-
formation about grammatical categories because it
is not used within the grammar rules. In the dictio-
nary, we have only semantic information about en-
tries. There are two levels of semantic information:
GTYPE ? groups entries with a similar meaning,
and G_CONCEPT connects an entry with its unique
interpretation. The lexicon is rather small ? just
over 200 word forms. In Fig. 5, there is a fragment
of the gazetteer with eight entries. All of them refer
to different types of neuropathy complications.
3.2 Grammar rules
A grammar in SProUT consists of rules, which are
regular expressions over TFSs with functional oper-
ators and coreferences, representing the recognition
37
neuropatia | GTYPE: gaz_comp | G_CONCEPT: neuropathy_t
Neuropatia | GTYPE: gaz_comp | G_CONCEPT: neuropathy_t
Neuropatia? | GTYPE: gaz_comp | G_CONCEPT: neuropathy_t
neuropatia? | GTYPE: gaz_comp | G_CONCEPT: neuropathy_t
obwodowa? | GTYPE: gaz_neuro |
G_CONCEPT: peripheral_polineuropathy
obwodowa | GTYPE: gaz_neuro |
G_CONCEPT: peripheral_polineuropathy
autonomiczna? | GTYPE: gaz_neuro |
G_CONCEPT: autonomic_neuropathy
autonomiczna | GTYPE: gaz_neuro |
G_CONCEPT: autonomic_neuropathy
Figure 5: A fragment of gazetteer
pattern. Output structures are also TFSs. Rules use
three sources of information: tokenization (struc-
tures of type token for recognising, among others,
abbreviations, dates, numbers), morphological anal-
ysis (structures of type morph), and a domain dictio-
nary (gazetteer).
The SProUT grammar described in the paper con-
sists of about 100 extraction rules. Each rule detects
words or phrases describing information presented
in section 2. For example, the rule in Fig. 6 recog-
nises the identification number of a patient?s visit in
hospital. The first line recognises a word from mor-
phological lexicon that has the base form (STEM)
numer ?number? or an abbreviation2 of this word, so
they are recognised as a token with an appropriate
SURFACE form. The next line: token? omits a
dot after the abbreviation, if it is necessary. Next two
lines recognise the keywords with the base forms
ksie?ga (?book?, ?document?) and g??wny (?main?).
Then, the identification number of the document is
recognised by the liczba_nat rule called (via the
@seek operator). The number is unified with the
value of the ID attribute in the output structure. Next
two lines are optional, they recognise a year num-
ber after a slash or a backslash, if this information
is present. If not, the year is fixed, during postpro-
cessing, according to the dates of the patient?s visit
in hospital. In this particular case, the value of the
attribute CONT is no in the output structure. The
rule in Fig. 6 captures, among other, the following
phrases:
? Numer ksie?gi g??wnej 11125/2006
?Number of the main document 11125/2006?
2Abbreviations are not present in the morphological dictio-
nary.
nr_ksiegi :>
(morph & [STEM ?numer?] | token & [SURFACE ?nr?]
| token & [SURFACE ?Nr?])
token ?
morph & [STEM ?ksie?ga?]
morph & [STEM ?g??wny?]
@seek(liczba_nat) & [LICZ #nr]
((token & [TYPE slash] | token & [TYPE back_slash])
@seek (liczba_nat) & [LICZ #nr1])?
->id_str & [ID #nr, ID_YEAR #nr1, CONT no].
Figure 6: Visit?s identification rule
chor_autoimm:>
(morph & [STEM ?choroba?] | morph & [STEM ?zesp???])
gazetteer & [GTYPE gaz_autoimm, G_CONCEPT #type]
->autoimm_dis_str & [AUTOIMM_DISEASE #type].
Figure 7: Autoimmunology disease rule
? nr ksie?gi g??wnej 12354
?nr of the main document 12354?
? Nr. ksie?gi g??wnej 13578
?Nr. of the main document 13578?
The grammar rules recognising the important in-
formation are often relatively simple. There is no
need to use any morphological features in rules, be-
cause we do not have to precisely delimit phrases.
Searched (key) phrases consist very often of words
which are very strongly connected with particular
notions. For example, if we find a phrase stopy
cukrzycowej ?diabetic foot?, it is practically certain
that it concerns a complication. Only base word
forms (values of STEM attribute) from the morpho-
logical analyser output turned out to be necessary
here.
Fig. 7 contains a simple rule recognising autoim-
munology diseases. It seeks for any occurrence of
the following pattern: <disease, autoimmunology-
disease-specification>. The first line of the rule
recognises a word: choroba or zesp?? e.g., ?disease?.
The second line requires an entry from the domain
dictionary which represents an autoimmunology dis-
ease. Its type (variable #type ) is unified with the
value of the attribute AUTOIMM_DISEASE in the
output structure.
3.3 Difficult Issues
Although the results of the program are quite good,
there are some difficult issues which cause errors.
38
We have to cope with negation, which some-
times is difficult to determine. In the following
phrase: bez obecnos?ci retinopatii ?without presence
of retinopathy?, it is not enough to identify the key-
word retinopatii ?retinopathy?, it is necessary to
recognise negation expressed in the form of the neg-
ative preposition bez ?without?. Here, the negation
appeared just before the keyword, and it can be eas-
ily noticed, but sometimes a negation is far from
a keyword, and is difficult to process with shallow
parsing methods. Let us consider the following sen-
tence: Nie stwierdzono p?z?nych powik?an? cukrzycy
o typie mikroangiopatii. ?there were no long-lasting
diabetes complications of microagiopathy type ?. In
this case, the negation nie stwierdzono ?there were
no? is at the beginning of the sentence and the key-
word mikroangiopatii ?microangiopathy? is the last
word of the sentence. The above phrase is recog-
nised with the rule in Fig. 8. It refers to the base
forms of certain words and to the domain lexicon in
order to identify a complication (variable #t). The
same rule recognise, among other, the following
phrases which meaning is the same as the previous
one.
? nie wykryto obecnos?ci p?z?nych powik?an?
cukrzycowych pod postacia? mikroangiopatii,
? nie wyste?puja? p?z?ne powik?ania cukrzycowe o
charakterze mikroangiopatii,
? Nie stwierdzono p?z?nych zmian cukrzycowych
w postaci mikroangiopatii.
In the very similar example: Nie stwierdzono
p?z?nych powik?an? cukrzycy z wyja?tkiem mikroan-
giopatii. ?there were no long-lasting diabetes com-
plications excluding microagiopathy? the case is just
the opposite, and the microangiopathy should be
recognised. So, to properly identify whether a pa-
tient has or hasn?t microangiopathy we have to anal-
yse the whole sentence.
Some problems are caused by keywords which
have different interpretation depending on the con-
text. e.g., mikroalbuminuria refers to a complication
in the phrase wysta?pi?a mikroalbuminuria ?microal-
buminuria appeared? and denotes a test in the phrase
Mikroalbuminuria: 25 mg/dobe? ?Microalbuminuria:
25 mg/day?. In this case we determine the meaning
of an ambiguous notion according to its context.
brak_powiklan :>
morph & [STEM ?nie?] ;; ?no?
(morph & [STEM ?stwierdzic??] | ;; ?recognise?
morph & [STEM ?wystepowac??] |
morph & [STEM ?wykryc??])
(morph & [STEM ?obecnos?c??])?
morph & [STEM ?p?z?ny?] ;; ?long-lasting?
(morph & [STEM ?powik?anie?] | ;; ?complication?
morph & [STEM ?zmiana?])
(morph & [STEM ?cukrzycowy?] | ;; ?diabetes?
morph & [STEM ?cukrzyca?])
(morph & [STEM ?w?] | ;; preposition
morph & [STEM ?pod?] | morph & [STEM ?o?])
(morph & [STEM ?postac??] | ;; ?type?
morph & [STEM ?typ?] | morph & [STEM ?charakter?])
gazetteer & [GTYPE gaz_comp, G_CONCEPT #t]
->no_comp_str & [N_COMP #t].
Figure 8: The rule recognising the lack of a specified
complication
The next thing that should be taken into account,
is that sometimes several pieces of information have
to be recognised with one rule. In the following co-
ordinated phrase: retinopatie? prosta? oka lewego oraz
proliferacyjna oka prawego z makulopatia? w obu
oczach ?nonproliferative rethinopathy in the left eye
and proliferative (rethinopathy) in the right eye with
maculopathy in both eyes? we have to recognise both
types of rethinopathy with maculopathy and create
a list of complications as the output structure, see
Fig. 9. The rule almost entirely refers to notions
from the domain dictionary. It identifies a combi-
nation of notions denoting retinopathy. The domain
dictionary contains both Polish and Latin (words in
this case both languages are used by doctors) refer-
ring to this complication.
In order to recognise precisely given information,
one tends to write complex rules describing entire
phrases instead of separated terms. The crucial prob-
lem for the effectiveness of complex IE rules is that
Polish is a free word language. This greatly in-
creases the number of ways the same idea can be
expressed. Let us consider the following examples:
? Wieloletnia, niekontrolowana cukrzyca typu 2,
long-lasting uncontrolled diabetes type 2,
? Niekontrolowana, wieloletnia cukrzyca typu 2,
? Wieloletnia cukrzyca typu 2, niekontrolowana,
? Cukrzyca wieloletnia typu 2, niekontrolowana.
39
retino_koord1:>
gazetteer & [GTYPE gaz_comp, G_CONCEPT retinopathy_t]
token ?
gazetteer & [GTYPE gaz_retino, G_CONCEPT #r1]
(token){0,2}
(token & [SURFACE ?i?] | token & [SURFACE ?oraz?] |
token &[SURFACE ?et?] | token & [TYPE comma])
(gazetteer &
[GTYPE gaz_comp, G_CONCEPT retinopathy_t])?
token ?
gazetteer & [GTYPE gaz_retino, G_CONCEPT #r2]
(token){0,2}
((token & [SURFACE ?z?] | token & [SURFACE ?cum?] |
token & [SURFACE ?i?] | token & [SURFACE ?oraz?])
gazetteer & [GTYPE gaz_macul, G_CONCEPT yes & #z1 ])?
->
comp_l_str & [ COMP_L complication_list &
[FIRST retinopathy_str &[ RETINOPATHY_T #r1 ,
WITH_MACULOPATHY #z1 ],
REST complication_list &
[FIRST retinopathy_str &[ RETINOPATHY_T #r2 ,
WITH_MACULOPATHY #z1 ],
REST *null* ]]].
Figure 9: Retinopathy coordination rule
All phrases mean: ?Long-lasting, uncontrolled, type
2 diabetes?. Every word of these phrases carries
important information: wieloletni ?long-lasting?,
niekontrolowany ?uncontrolled?, typ 2 ?type 2?. But
they should be identified as important only in con-
text of the keyword cukrzyca ?diabetes?. The only
solution is to recognise the whole phrase through
one rule. So, we ought to predict all possible config-
urations of words and write a lot of rules that identify
subsequent permutations of keywords, which might
be difficult. Thus, some omissions of information
can be caused by insufficient coverage by grammar
rules (see sec. 4).
The information we searched for can be divided
into two types. Many facts were originally written
in the documents in a standardised way, for example
the value of the BMI parameter, or phrases describ-
ing complications. For these parts of information,
the probability of error is rather small and is related
mostly to the occurrence of complicated negation or
coordination. But some of the features can be ex-
pressed in many ways. In this case, the program re-
call can depend on the particular physicians? writing
styles. An example is the information about contin-
uation of diabetes treatment. In this case we have
to identify information about continuation of a treat-
ment (can be expressed in many ways) in the con-
text of a phrases denoting diabetes. This context is
important because, in the texts, there are sometimes
phrases describing continuation of treatment of not
diabetes but other illness. A few samples are given
below:
? Kontynuowano leczenie cukrzycy dotychcza-
sowym systemem wielokrotnych wstrzyknie?c?
?The diabetes treatment was continued on the
same basis of multiple injections?,
? Utrzymano dotychczasowy system wielokrot-
nych wstrzyknie?c? insuliny ?The current system
of multiple insulin injections has been main-
tained?,
? Kontynuowano dotychczasowy schemat
leczenia cukrzycy ?The current schema of
diabetes treatment was continued?,
? Kontynuowano dotychczasowe leczenie
hipotensyjne ?The current treatment of hy-
potension was continued? ? this phrase is not
about diabetes!
A fact that a patient was educated for diabetes is
another example of information which can be ex-
pressed in many ways. Any phrase indicating that
a patient was informed or taught about something
or something was discussed with a patient is inter-
preted as the information about education. We are
not interested in details of education but still we
have to recognise 13 different constructions describ-
ing education.
? Om?wiono z chorym zasady diety, samokon-
troli i adaptacji dawek insuliny ?Diet, self-
control and adaptation of insulin doses were
discussed with the patient?,
? Nauczono chorego pos?ugiwac? sie pompa? in-
sulinowa? i glukometrem. ?The patient was
taught how to use an insulin pump and a glu-
cometer.?,
? W czasie pobytu w Klinice prowadzono
edukacje? chorej dotycza?ca? cukrzycy. ?During
the patient stay in the Clinic, the patient was
educated for diabetes.?,
40
? Po odbyciu szkolenia z zakresu podstawowych
wiadomos?ci o cukrzycy wypisano chora?... ?Af-
ter learning the basic information about dia-
betes, the patient was discharged...?.
4 IE results evaluation
Part of the data was used as a training set, the eval-
uation was made on the other 50 previously unseen
reports. From above 60 attributes, the partial eval-
uation concerned only 7. The evaluated attributes
are of different type: retinopathy is a keyword but
we still deal with the problem of negation and coor-
dination. Words denoting uncontrolled diabetes can
refer not only to diabetes so they should be recog-
nized only in specific contexts. Attributes: educa-
tion and diet modification are represented in the texts
by complex phrases.
Results are presented in Fig. 10. The worst results
were observed for diabetes balance recognition. It
was due to the fact that keywords representing this
information had to be recognised in the context of
the word cukrzyca ?diabetes?, (see 3.3) and some-
times the distance between these words is too far. 4
occurrences of wrongly recognised retinopathywere
caused by the unpredicted negated phrases.
phrases precision recall
uncontrolled
diabetes 61 100 68,85
retinopathy (total) 50 92,5 98
nonproliferative 35 100 100
preproliferative 9 100 88,89
proliferative 5 100 100
unspecified 1 20 100
diabetic education 19 100 94,74
diet modification 1 100 100
Figure 10: IE evaluation of 50 reports
5 Database Organization
The data obtained from the IE system is a huge XML
file. The attribute values included within it were
subsequently introduced into a relational database
which can be searched and analysed. At the database
filling stage some additional postprocessing of data
was done. This concerned, among others, the fol-
lowing problems:
? detection and omission of information of pa-
tient not suffering from diabetes,
? detection and omission of not complete data
(reports not sufficiently filled up with data),
? omission of redundant data and choosing the
most detailed information (e.g. about types of
complications)
? selecting highest levels for blood test results.
The database consists of 20 tables containing all ex-
tracted information about a patient, his/her illness
and the recommended treatment. At the moment, the
database contains 388 hospitalisation descriptions of
387 patients. 254 cases were qualified as diabetes
type 2, 129 as type 1 and 5 as type other. 556 com-
plications for 256 patients and 304 insulin treatment
schemas have been recognised.
6 System Overview and Evaluation
The main aim of the work was creation of a system
that processes diabetic patients? hospital documenta-
tion automatically and inserts the extracted data into
a relational database. The database can be searched
for using SQL queries or a specialized program ded-
icated for doctors which enables queries by exam-
ple. The system architecture is given in Fig. 11. The
processing procedure consisted of four stages:
? text preprocessing including format unification
and data anonymization (Perl scripts),
? information extraction based on the domain
model (Prot?g?), Polish morphological lexicon
(Morfeusz) and the domain lexicon,
? postprocessing: data cleaning and structuring
(Perl scripts),
? insertion data into a relational database (Post-
gres).
The evaluation of the system was done simultane-
ously with IE evaluation on the same set of 50 re-
ports. The results are presented in Fig. 12. The final
recognition of the uncontrolled diabetes was higher
due to repetition of the same information in one doc-
ument.
41
Preprocessing
?
IE
?
Postprocessing
?
Database
Domain Model



+
Morfological Lexiconff
Domain Lexicon
QQk
SQL queries
3
Simple query interface-
Figure 11: System architecture
cases precision recall
uncontolled diabetes 37 100 86,49
retinopathy (total) 22 88 100
nonproliferative 14 100 100
preproliferative 4 100 88,89
proliferative 3 100 100
unspecified 1 25 100
diabetic education 19 100 94,74
diet modification 1 100 100
Figure 12: Overall system evaluation of 50 reports
7 Conclusions
For the chosen domain, the rule-based IE method
seems to be the best one. Learning techniques are
hard to apply due to: a great number of attributes
searched for (in comparison to the amount of avail-
able texts) and their inter connections and crucial de-
pendence on negation and coordination occurrences.
Good precision and recall values make this method
practically usable for information extraction from
free patients? documentation. We plane to use our
tools for creating annotated corpora (manually cor-
rected) which are necessary for training statistical
models.
Of course the portability of the method is poor.
The grammars written for a particular domain can
be developed to cover more facts and details but their
extendibility to another domain is problematic.
Acknowledgment
This work was done in cooperation with Br?dnowski
Hospital in Warsaw and was partially financed by
the Polish national project number 3 T11C 007 27.
References
Razvan Bunescu, Ruifang Ge, Rohit. J. Kate, Raymond
J. Mooney, and Yuk Wah Wong. 2003. Learning to ex-
tract proteins and their interactions from Medline ab-
stracts, Proceedings of ICML-2003 Workshop on Ma-
chine Learning in Bioinformatics, pp. 46-53, Washing-
ton, DC.
Witold Droz?dz?yn?ski, Hans-Ulrich Krieger, Jakub Pisko-
rski, Ulrich Sch?fer and Feiyu Xu. 2004. Shallow Pro-
cessing with Unification and Typed Feature Structures
? Foundations and Applications. German AI Journal
KI-Zeitschrift, 01/04.
Udo Hahn, Martin Romacker and Stefan Schulz. 2002.
Creating knowledge repositories from biomedical re-
ports: The MEDSYNDIKATE text mining system. In
Proceedings PSB 2002, pages 338?349.
Henk Harkema, Andrea Stzer, Rob Gaizauskas, Mark
Hepple, Richard Power and Jeremy Rogers. 2005.
Mining and Modelling Temporal Clinical Data. In:
Proceedings of the UK e-Science All Hands Meeting
2005, Nottingham UK.
George Hripcsak, John Austin, Philip O. Alderson and
Carol Friedman, 2002. Use of Natural Language
Processing to Translate Clinical Information from a
Database of 889,921 Chest Radiographic Reports Ra-
diology.
Agnieszka Mykowiecka, Anna Kups?c?, Ma?gorzata
Marciniak, 2005. Rule-based Medical Content Ex-
traction and Classification, Proceedings of ISMIS
2005, Springer-Verlag.
Jakub Piskorski, Peter Homola, Ma?gorzata Marciniak,
Agnieszka Mykowiecka, Adam Przepi?rkowski and
Marcin Wolin?ski. 2004. Information Extraction for
Polish using the SProUT Platform. In: Proceedings of
ISMIS 2004, Zakopane, pp. 225?236, Springer-Verlag.
Amund Tveit and Rune Saetre, 2005. ProtChew: Auto-
matic Extraction of Protein Names from Biomedical
Literature, Proceedings of the 21st International Con-
ference on Data Engineering Workshops.
Marcin Wolin?ski. 2006. Morfeusz ? a Practical Tool
for the Morphological Analysis of Polish. Procceed-
ings of IIS: IIPWM?06. Advances in Soft Computing,
Springer-Verlag
Roman Yangarber, Winston Lin and Ralph Grishman.
2002. Unsupervised Learning of Generalized Names.
Proceedings of the 19th International Conference on
Computational Linguistics, COLING 2002.
Pierre Zweigenbaum (ed.). 1994. MENELAS: An Ac-
cess System for Medical Records Using Natural Lan-
guage, In: Computer Methods and Programs in
Biomedicine vol. 45.
42
Proceedings of the 2011 Workshop on Biomedical Natural Language Processing, ACL-HLT 2011, pages 92?100,
Portland, Oregon, USA, June 23-24, 2011. c?2011 Association for Computational Linguistics
Towards Morphologically Annotated Corpus
of Hospital Discharge Reports in Polish
Ma?gorzata Marciniak
Institute of Computer Science PAS
ul. J.K. Ordona 21,
01-237 Warszawa, Poland
mm@ipipan.waw.pl
Agnieszka Mykowiecka
Institute of Computer Science PAS
ul. J.K. Ordona 21,
01-237 Warszawa, Poland
agn@ipipan.waw.pl
Abstract
The paper discuses problems in annotating
a corpus containing Polish clinical data with
low level linguistic information. We propose
an approach to tokenization and automatic
morphologic annotation of data that uses ex-
isting programs combined with a set of do-
main specific rules and vocabulary. Finally
we present the results of manual verification
of the annotation for a subset of data.
1 Introduction
Annotated corpora are knowledge resources indis-
pensable to the design, testing and evaluation of
language tools. Medical language differs signifi-
cantly from the everyday language used in newspa-
pers, magazines or fiction. Therefore, general lan-
guage corpora are insufficient when creating tools
for (bio)medical text processing.
There are several biomedical corpora available for
English such as GENIA (Kim et al, 2010) ? the
best known and most used one, containing MED-
LINE abstracts annotated on several levels; BioInfer
(Pyysalo et al, 2007) targeted at protein, gene, and
RNA relationships annotation; or CLEF (Roberts et
al., 2009) containing 20,000 cancer patient records
annotated with clinical relations. Medical corpora
are also collected for lesser spoken languages, e.g.
MEDLEX ? Swedish medical corpus (Kokkinakis,
2006); IATROLEXI project for Greek (Tsalidis et
al., 2007); or Norwegian corpus of patients? histories
(R?st et al, 2008). The paper (Cohen et al, 2005)
contains a survey of 6 biomedical corpora. The au-
thors emphasize the importance of a standard format
and give guidelines for careful annotation and eval-
uation of corpora.
The immediate goal of the paper is to estab-
lish and test a method of annotating Polish clini-
cal data with low level linguistic information, i.e.
token and morpheme descriptions. The research is
done on a relatively small set of data (more than
450,000 tokens) but to gain the experience neces-
sary to create a much larger annotated corpus of Pol-
ish medical texts. We would like to use our cor-
pus to refine and test domain tools for: tagging,
Named Entity Recognition or annotation of nominal
phrases. We have already annotated the corpus with
semantic information (Marciniak and Mykowiecka,
2011) using an existing rule based extraction sys-
tem (Mykowiecka et al, 2009) and performed exper-
iments with machine learning approaches to seman-
tic labeling (Mykowiecka and Marciniak, 2011).
Thus, to enable the realization of various scientific
goals, a detailed and universal morphologic annota-
tion of the corpus was introduced.
The division into tokens is the first level of text
analysis. It is frequently performed without paying
special attention to potential problems, just by di-
viding text on spaces, line breaks and punctuation
marks. In many applications this is quite a satis-
factory solution, but in case of texts that contain a
lot of non-letter characters, using universal tokeniza-
tion rules frequently causes problems. Some exam-
ples, in the case of using the Penn Treebank tok-
enization scheme in annotating the GENIA corpus
were pointed out in (Teteisi and Tsujii, 2006). Jiang
and Zhai (2007) show the importance of tokeniza-
tion strategies in the biomedical domain, and the in-
92
fluence of this process on the results of information
retrieval. Our approach consists of dividing text into
simple tokens which can be grouped at subsequent
levels of analysis using domain specific knowledge.
For languages with rich inflection, like Polish,
morphological annotation is indispensable for fur-
ther text analysis. As there are no Polish taggers
which can analyze medical texts, nor medical lexi-
cons containing inflected forms, we combine a gen-
eral purpose tagger with a set of domain specific
rules referring to a small data induced vocabulary.
A portion of the automatically annotated data was
checked by two linguists to assess data quality. The
results obtained are given in 8. Currently, the entire
dataset is undergoing manual verification.
2 Linguistic Characteristics of Texts
The corpus consists of 460 hospital discharge re-
ports of diabetic patients, collected between the
years 2001 and 2006 in one of Warsaw?s hospi-
tals. These documents are summaries of hospital
treatment and are originally written in MS Word
with spelling correction turned on, so the errors ob-
served are mainly in words that are not included in
the dictionary. The documents are converted into
plain text files to facilitate their linguistic analysis
and corpus construction. Clinical data include infor-
mation serving identification purposes (names and
addresses) which are substituted by symbolic codes
before making the documents accessible for further
analysis. The annonymization task was performed
in order to make the data available for scientific pur-
poses. We plan to inspect the data manually, to re-
move all indirect information enabling a patient?s
identification, and negotiate the terms for making
the corpus publicly available.
Each document is 1.5 ? 2.5 pages long, and be-
gins with the identification information of the pa-
tient and his/her visit in hospital. Next, the follow-
ing information is given in short form: significant
past and current illnesses, diagnoses and patient?s
health at the beginning of the hospitalization. Af-
ter these data, the document describes results of ex-
aminations such as height, weight, BMI and blood
pressure, ophthalmology examinations, blood tests,
lipid profile tests, radiology or ultrasound. This part
of the document may also contain descriptions of at-
tempts to select the best treatment for the patient.
The summary of the document starts from the word
Epikryza ?Discharge abstract?. Its length is about
half a page of text. It contains: data about a pa-
tient?s diabetes, a description of diabetic complica-
tions, and other illnesses, selected examination re-
sults and surgical interventions, information about
education, diet observed, self monitoring, patient?s
reactions, and other remarks. Finally, all recommen-
dations are mentioned, including information about
prescribed diet, insulin treatment (type and doses)
and oral medication.
Most information is given as free-form text, but
the vocabulary of these documents is very spe-
cific, and significantly differs from texts included
in corpora of general Polish like IPIPAN Corpus
(Przepi?rkowski, 2004) or NKJP (National Corpus
of Polish, http://nkjp.pl). The texts con-
tain many dates in different formats, and a lot of
test results with numerical values, whose descrip-
tions are omitted in NKJP. The texts contains also
a lot of medication names, like Cefepime or Ac-
ard not present in any general Polish dictionary.
Some of them are multi-word names like Diaprel
MR, Mono Mack Depot, Mixtard 10. The same
medication can be referred to in different ways de-
pending on international or Polish spelling rules
(e.g. Amitriptylinum and its Polish equivalent Amit-
ryptylina). Polish names could be inflected by cases
(e.g. Amitryptyliny
gen
).
In documents, many diagnoses are written in
Latin. In the following examples the whole phrases
are in Latin: Retinopathia diabetica simplex cum
maculopathia oc. sin. ?simple diabetic retinopathy
with maculopathy of the left eye?; or Laryngitis
chronica. Otitis media purulenta chronica dex.
?Chronic laryngitis. Chronic purulent inflammation
of the middle right ear?. Sometimes foreign expres-
sions are thrown into a Polish sentences: Ascites
duz?a ilos?c? p?ynu w jamie brzusznej mie?dzy pe?tlami
jelit . . . ?Ascites a lot of fluid in abdominal cavity
between intestinal loops . . . ? ? only the first word
is not in Polish.
3 Corpus description
The corpus is annotated with morphological and se-
mantic information. The standard of annotation fol-
93
lows the TEI P5 guidelines advised for annotation of
biomedical corpora, see (Erjavec et al, 2003). Our
corpus format is based on the one accepted for the
NKJP corpus (Przepi?rkowski and Ban?ski, 2009).
According to this scheme, every annotation is de-
scribed in a separate file. Each discharge document
is represented by a catalog containing the following
five files:
? xxx.txt ? plain text of the original annonymized
document;
? xxx.xml ? text of the document (in the form as
in xxx.txt file) divided into numbered sections
which are in turn divided into paragraphs;
? xxx_segm.xml ? token limits and types (29
classes);
? xxx_morph.xml ? morphological information
(lemmas and morphological feature values);
? xxx_sem.xml ? semantic labels and limits.
4 Tokenization
The first level of text analysis is its segmentation
into tokens. In general, most tokens in texts are
lowercase words, words beginning with a capital let-
ter and punctuation marks. The most common (thus
the most important) tokenization problem is then to
decide whether a particular dot ends a sentence or
belongs to the preceding abbreviation (or both). In
some texts there are also many numbers represent-
ing dates, time points, time intervals or various nu-
merical values. For texts in which uniform standards
of expressing these notions are obeyed, recognizing
such complex tokens is much easier and simplifies
further text analysis.
In medical texts the problem of non-word tokens
is harder than in the case of newspapers or novel
content as they constitute a much larger portion
of the text itself. Apart from descriptions of time
(dates, hours, periods of time) there are numbers that
refer to values of different medical tests or medicine
doses and sizes. There are also many specific
names which sometimes contain non-letter charac-
ters (e.g. Na+) as well as locally used abbreviations
and acronyms. An additional difficulty is caused by
the lack of will to obey writing standards. Physi-
cians use different ways of describing dates (e.g.
02.09.2004, 30.09/1.10.2003, 06/01/2004, 14.05.05,
28 .04. 05, 12.05.2005r.) or time (8:00 vs 8.00).
They also do not pay enough attention to punctu-
ation rules and mix Polish and English standards of
writing decimal numbers. In Polish we use a comma
not a dot, but the influence of English results in com-
mon usage of the decimal point. Sometimes both
notations can be found in the same line of text. Fur-
ther, the sequence ?2,3? may mean either ?2.3? or two
separate values: ?2? and ?3?.
Two tools used in the process of constructing
the corpus have embedded tokenizers. The first
one is a part of the information extraction system
SProUT (Droz?dz?yn?ski et al, 2004) which was used
to write grammars identifying semantically impor-
tant pieces of text. The general assumption adopted
while building its tokenizer was ?not to interpret too
much?, which means that tokens are relatively sim-
ple and do not rely on any semantic interpretation.
Their self explanatory names, together with token
examples and their frequencies in the entire input
data set, are listed in table 1.
Two other tokenization modules are embedded in
the TaKIPI tagger used to disambiguate the morpho-
logical descriptions of word forms (Piasecki, 2007).
The first one divides all character sequences into
words and non-words which are assigned the ign la-
bel. The second tokenizer interprets these non-word
sequences and assigns them ttime, tdate, turi (for se-
quences with dots inside) and tsym labels. It also
applies a different identification strategy for token
limits ? for all non-word tokens only a space or a
line break ends a token. Although treating a date
(15.10.2004r) or a range (1500-2000) as one token
is appropriate, in the case of sequences where spaces
are omitted by mistake, the resulting tokens are of-
ten too long (e.g. ?dnia13/14.07.04?, ?iVS-1,5?).
After analyzing the results given by three differ-
ent tokenizers we decided to use the token classes
identified by the SProUT tokenizer and align its
results with the results of the ?simple? TaKIPI to-
kenizer. SProUT tokens which were longer than
TaKIPI tokens, e.g. ?1x2mg?, ?100mg?, ?50x16x18?,
were divided into smaller onces. The changes
introduced to token limits concern those tokens
of the other_symbol type which contain punctua-
tion marks. The other_symbol class comprises se-
quences which do not fit into any other class, i.e.
94
symbols for which separate classes are not defined
(e.g. ?=?) and mixed sequences of letters and digits.
In this latter case a token ends only when a space or
a line break is encountered. The most typical case
when this strategy fails in our data is the sequence
?HbA1c:? as the name of the test according to the
tokenizer rules is classified as an ?other_symbol?
the following colon is not separated. There are
also other similar sequences: ?HbA1c=9,1%:? or
?(HbA1C?. To make the results more uniform we di-
vided these tokens on punctuation characters. This
process resulted in replacing 1226 complex tokens
by 4627 simple ones. Among these newly cre-
ated tokens the most numerous class was lower-
case_word and numbers which were formed after
separating numbers and unit names, e.g. 10g, 100cm
and sequences describing repetitions or sizes, like
2x3, 2mmx5mm. The longest sequence of this kind
was ?ml/min.,GFR/C-G/-37,5ml/min/1,73m2?. This
string was divided into 18 tokens by TAKIPI but fi-
nally represented as 23 tokens in the corpus. Finally,
in the entire data set 465004 tokens (1802864 char-
acters) were identified. The most numerous class
represents numbers ? 18.8% (9% of characters), all
punctuation characters constitute 25% of the total
number of tokens (6.5% characters).
5 Morphological analysies
Morphological annotation was based on the results
obtained by the publicly available Polish POS tag-
ger TaKIPI that cooperates with Morfeusz SIAT
(Wolin?ski, 2006) ? a general-purpose morpholog-
ical analyzer of Polish. For each word, it assigns
all possible interpretations containing: its base form,
part of speech, and complete morphological charac-
terization (e.g. case, gender, number, aspect if rel-
evant). The description is exhaustive and aimed at
further syntactic analyses of texts.
The annotation is done in three steps. In the
first one the documents are analyzed and disam-
biguated by TaKIPI. TaKIPI can be combined with
the Guesser module (Piasecki and Radziszewski,
2007) which suggests tags for words which are not
in the dictionary. We decided to use this module
because otherwise 70600 tokens representing words
and acronyms that occur in the documents would be
assigned an unknown description. The gain from its
Table 1: Token types and number of occurrences
numbers
token class name & examples initial final
all_capital_word: ALT, B, HDL, HM 18369 18416
any_natural_number 85766 87246
apostrophe 14 14
back_slash 7 7
closing_bracket 2661 2663
colon 12426 12427
comma 28799 28831
dot 47261 47269
exclamation_sign 49 49
first_capital_word: Al, Amikacin, Wysokie 43136 43269
hyphen 4720 4725
lowercase_word: antygen, aorta 192305 193368
mixed_word_first_capital: AgHBs, IIo, 513 514
NovoRapid
mixed_word_first_lower: antyHBS, dlAST 989 1003
number_word_first_capital: 200Hz, 14HN 48 0
number_word_first_lower: 100ml, 200r 1kaps 650 0
opening_bracket 3344 3355
other_symbol: (132x60mm), 1,34x3,25, 3161 2868
HbA1c=10,3%,
percentage_tok 4461 4478
question_mark 207 209
quotation 1 1
semicolon 455 455
slash 10340 10353
word_number_first_capital: AST34, B6 1195 1195
word_number_first_lower: mm3, pH6 1865 1854
word_with_hyphen_first_capital: B-hCG, 163 163
Anty-HBs
word_with_hyphen_first_lower: m-ce, p-cia? 402 402
all tokens 463307 465004
usage is however not so evident, as tags and base
forms suggested by Guesser are quite often incor-
rect ? in one test set, only 272 forms out of 1345
were analyzed correctly.
The analyses of TaKIPI results shows that there
are many systematic errors. They can be corrected
globally. An example of such an error is the de-
scription of medication names produced by Guesser.
Their morphologic tags are often correct, but the
problem is with gender assignment in case of mascu-
line forms. In Polish there are three subtypes of mas-
culine gender: personal, animate and inanimate, and
Guesser quite often uses personal masculine gender
instead of the inanimate one while analyzing med-
ication names. The second most common problem
concerns base forms, because all base forms created
by the module are written with a small letter. So in
the case of proper names, all base forms have to be
corrected. Moreover, TaKIPI do not disambiguate
all tags ? certain forms still have more than one pos-
sible description.
95
Thus, to limit the number of manual changes
needed in the final version of the corpus, we post-
process the results with a set of rules (see section 7)
created on the basis of a list of all different token
descriptions. The rules mainly correct the annota-
tions of domain related tokens like acronyms and
units: BMI, HbA1c, RR, USG, Hz or kcal; medi-
cation names e.g. Diaprel, its diaprel base form is
changed into Diaprel; and other domain terms like
dekarboksylazie (?decarboxylase
loc
?) for which the
masculine base form was suggested dekarboksylaz
instead of feminine dekarboksylaza. Moreover, tags
of misspelled tokens and foreign words are assigned
to tokens during this stage and if there is more than
one description attached to a token, then the more
probable in the domain is chosen.
Finally, the morphology analyses are manually
corrected. This is done by two linguists. The re-
sults are compared and corrected by a third annota-
tor. The first results are described in section 8.
6 Tags
For each token, TaKIPI assigns its base form,
POS, and full morphological description. For
example, the token badania that has the base
form badanie ?examination? is classified in all 579
occurrences as a neutral noun. In 566 cases
it is classified as a singular form in genitive
and is assigned the tag subst:sg:gen:n (substan-
tive:singular:genitive:neutral); in 13 cases as a plu-
ral noun including 8 nominative forms, 4 accusative
and even one vocative (unreliable in medical texts).
TaKIPI assigns the unknown tag (ign) to numbers,
so we introduced the number tag to represent nu-
merical values in the corpus. It is assigned to 18.8%
of tokens.
The set of potential morphological tags consists
of more than 4000 elements. In our corpus only 450
different tags are represented, in comparison to over
1000 tags used in the general Polish IPIPAN corpus
(Przepi?rkowski, 2005).
In the rest of this section we describe tags used
for the classification of strings that are not properly
classified by TaKIPI. If no tag described in the sec-
tion suits a token, the tag tsym is assigned to it. In
particular, all patient codes (like d2005_006) have
the tsym tag.
6.1 Errors
Spelling errors in the corpus are left as they are. Mis-
spelled tokens are assigned the base form equal to
the token, and one of the following tags depending
on the type of error:
? err_spell describes misspelled tokens like
bia3ko instead of bia?ko (?protein?). In the cor-
pus we provide additional information with the
corrected input token, its base form and mor-
phological tag.
? err_conj describes concatenations like cukrzy-
cowej2000 (?diabetic2000?). In this case we
add the correct form cukrzycowej 2000 to the
corpus but do not add its description.
? err_disj_f describes the first part of an in-
correctly disjointed word. For example the
word cis?nienie (?pressure?) was divided into
two parts ci and s?nienie, (by chance, both are
valid Polish words).
? err_disj_r describes the second part of the in-
correctly disjointed word.
The last three categories can be supplemented
with spell description if necessary. For example the
token Bylaw is a concatenation of the misspelled
word By?a (?was?) with the preposition w (?in?). This
token has the tag err_conj_spell, and the By?a w
correction is added.
6.2 Abbreviations
There are many abbreviations in the documents.
Some of them are used in general Polish like prof
(?professor?) or dr (?doctor), but there are many ab-
breviations that are specific to the medical domain.
For example in the descriptions of USG examina-
tions the letter t denotes te?tnica (?artery?), while tt
refers to the same word in plural, although usu-
ally there is no number related difference e.g. wit
(?vitamin?) can be used in plural and singular con-
text. Sometimes it is not a single word but the
whole phrase which is abbreviated, e.g. NLPZ is
the acronym of the noun phrase Niesterydowe Leki
PrzeciwZapalne ?Non-Steroidal Anti-Inflammatory
Drugs?, and wpw is an abbreviation of the prepo-
sitional phrase w polu widzenia ?in field of view?.
96
Abbreviations and acronyms obtain the tag acron.
Moreover, it is possible to insert the full form corre-
sponding to them.
Acronyms denoting units obtain the tag unit.
Units in common usage are not explained: mm, kg,
h, but if a unit is typical to the medical domain, its
full form is given (e.g. HBD means tydzien? ciaz?y
?week of pregnancy?).
We also distinguish two tags describing prefixes
and suffixes. The token makro (?macro?) in the
phrase makro i mikroangiopatia (?macro and mi-
croangiopathy?) has the tag prefix, while the suffix
tag describes, for example, the part ma of the string
10-ma which indicates instrumental case of number
10, like in: cukrzyca rozpoznana przed 10-ma laty
(?diabetes diagnosed 10 years ago?).
6.3 Foreign Words
Foreign words receive the foreign tag. This tag can
be elaborated with information on the part of speech,
so for example, Acne has the tag foreign_subst. It
is possible to attach a Polish translation to foreign
words.
7 Correction Rules
Correction rules are created on the basis of a list
of different tokens, their base form, and tags that
occurred in the corpus. Each rule is applied to all
matching form descriptions of tokens in the already
tagged data.
We use the method of global changes because we
want to decrease the number of manual corrections
in the corpus on the final, manual stage. It should
be noted that without context it is impossible to cor-
rect all morphological tags. We can only eliminate
evident errors but we cannot decide, for example,
if a particular description of a token badanie ?ex-
amination? (see section 6) is correct or not. All
these tags can be verified only if we know the con-
text where they occurred. However, quite a lot of
changes can be made correctly in any context, e.g.
changes of gender of a medication name (Lorinden
f
into Lorinden
m3
), or in the prevailing number of
cases, e.g. assigning to zwolnienie the gerund tag
?slowing? (11 occurrences) instead of less frequent
in the texts noun ?sick leave? only one occurrence
(TaKIPI leaves both descriptions).
There are two main types of correction rules of
which syntax is given in (1?2). ?#? is a separator;
the character ?>? indicates the new token description
that is applied to the corpus; after || additional in-
formation can be noted. In case of rule (1) it could
be a text that explains the meaning of acronyms, ab-
breviations or foreign words, while for rule (2), a
corrected token, base form and tag can be given.
This additional information might be used for creat-
ing a corpus without spelling errors, dictionaries of
abbreviations or foreign words used in the medical
domain.
(1) token#base form#tag#>
token#new base form#new tag#
|| ?string? (optionally)
(2) token#base form#tag#>
token#token#error_spell# ||
corr. token#corr. base form#new tag#
The first scheme is useful for changing the base
form or the tag of a token. See example (3) where
the first letter of the base form is capitalized and per-
sonal masculine gender m1 is changed into inani-
mate masculine gender m3.
(3) Insulatard#insulatard#subst:sg:nom:m1#>
Insulatard#Insulatard#subst:sg:nom:m3#
The second scheme is applied to a token grani-
ach ?ridges? (in mountain) that represents the exist-
ing but unreliable word in the medical domain. For
all of its occurrences in our data (3 cases) it is sub-
stituted by granicach ?limits? by the following cor-
rection rule:
(4) graniach#gran?#subst:pl:loc:f#>
granicach#granicach#err_spell# ||
granicach#granica#subst:pl:loc:f#
If there is more than one interpretation left by
TaKIPI, all are mentioned before the character ?>?.
See example (5) where two different base forms are
possible for the token barku and both have the same
tag assigned. The first base form bark (?shoulder?)
is definitely more probable in the medical domain
than the second one barek (?small bar? or ?cocktail
cabinet?), so the rule chooses the first description.
(5) barku#bark#subst:sg:gen:m3##barek#
subst:sg:gen:m3#>barku#bark#subst:sg:gen:m3#
97
Table 2 presents the frequencies of top level mor-
phological classes: directly after running the tagger,
after changing the token limits and after applying au-
tomatic changes. In the last column the number of
different forms in every POS class is presented.
Most part of speech names are self explanatory,
the full list and description of all morphological tags
can be found in (Przepi?rkowski, 2004), the newly
introduced tags are marked with ?. Of all words
(all tags apart form interpunction, number and tsym)
the most numerous groups are nouns (substantive)
? 54% and adjectives ? 15% of wordform occur-
rences.
Table 2: Morpheme types and numbers of occurrences
tagger after tok. final corpus
results change different
POS tag number of tag occurences forms
adj 35305 35041 36848 3576
adv 2323 2323 2437 245
conj 5852 5852 5680 36
prep 29400 29400 26120 71
pron 302 302 142 21
subst 82215 82215 105311 5093
verb forms: 24743 24741 19912 2001
fin 2173 2173 1900 190
ger 9778 9778 4677 423
ppas 5593 5593 6170 551
other 7199 7197 7165 837
qub 4244 4242 2452 67
num 703 703 703 34
ign 160951 163629 0 0
acron? 0 0 30003 678
unit? 0 0 28290 82
prefix? 0 0 13 5
suffix? 0 0 36 6
tsym? 0 0 534 462
interp 115323 116556 116556 21
number? 0 0 87898 1386
err_disj? 0 0 179 129
err_spell? 0 0 560 440
foreign? 0 0 1330 184
total 461361 465004 465004 14537
If we don?t take into account number, tsym and
the punctuation tokens, we have a corpus of 348461
tokens (TW) out of which 78854 (29.81%) were
changed. The most frequent changes concerned in-
troducing domain related unit and acronym classes
(nearly 72% of changes). Quite a number of changes
were responsible for the capitalization of proper
name lemmata. In table 3 the numbers of some other
types of changes are presented.
Table 3: Morphological tag changes
type of change number % % of
of changes TW
base form
capitalization only 6164 13.8 4.12
other 25503 32.34 9.64
POS
to acron & unit 56697 71.90 21.43
to other 10547 13.37 3.99
grammatical features (without acron and unit)
only case 109 0.13 0.04
only gender 1663 2.11 0.62
other 13215 16.75 4.99
Table 4: Manual correction
basic tags all tags
all tokens 8919 8919
without numbers and interp 4972 4972
unchanged 4497 4451
changed 475 521
same changes accepted 226 228
same changes not accepted 1 1
different changes none accepted 4 5
different changes. accepted 1 3 4
different changes. accepted 2 40 42
only 1st annot. changes - accepted 15 48
only 2nd annot. changes - accepted 128 124
only 1nd annot. changes - not accepted 47 47
only 2nd annot. changes - not accepted 0 0
8 Manual Correction
The process of manual correction of the corpus is
now in progress. It is performed using an editor
specially prepared for visualization and facilitation
of the task of correcting the corpus annotation at all
levels. In this section we present conclusions on the
bases of 8 documents corrected by two annotators
(highly experienced linguists). In the case of incon-
sistent corrections the opinion of a third annotator
was taken into account. The process of annotation
checking took about 2x20 hours.
From a total number of 8919 tokens in the dataset,
the verification of 4972 (words, acronyms, units)
was essential, the remaining 3947 tokens represent
numbers, punctuation and tsym tokens. The correc-
tion rules changed the descriptions of 1717 (34%)
tokens, only 87 cases were limited to the change
of a lowercase letter into a capital letter of the
base form. Manual verification left 4497 token de-
scriptions unchanged, while 10.6% of descriptions
were modified (evaluation of TaKIPI by Karwin?ska
and Przepi?rkowski (2009) reports 91.3% accuracy).
Kappa coefficient was equal to 0.983 for part of
98
speech and 0.982 for case assignment (when it is ap-
plicable). The results of manual correction are given
in table 4. The ?basic tags? column gives the number
of changes of the base form and tag, while the ?all
tags? column takes into account all changes, includ-
ing descriptions of the correct word form in case of
spelling errors, explanations of acronyms or units.
More detailed analysis of annotation inconsisten-
cies shows two main sources of errors:
? lack of precision in guidelines resulted in
choosing different base forms in case of
spelling errors and different labeling of cases
with the lack of diacritics which resulted in cor-
rect but not the desired forms;
? some errors were unnoticed by one of the an-
notators (just cost of manual work), e.g. in the
data there are many strings ?W? and ?w? which
may be either acronyms or prepositions.
There are only a few cases that represent real
morphological difficulties, e.g. differentiating adjec-
tives and participles (5 cases among the annotators).
Some examples of different case and gender assign-
ments were also observed. They are mostly errors
consisting in correcting only one feature instead of
two, or a wrong choice of a case for long phrases.
9 Conclusions and Further Work
The problems described in the paper are twofold,
some of them are language independent like tok-
enization, description of: abbreviations, acronyms,
foreign expressions and spelling errors; while the
others are specific for rich-morphology languages.
Our experiment showed that analyzing specialized
texts written in highly inflected language with a gen-
eral purpose morphologic analyzer can give satis-
factory results if it is combined with manually cre-
ated global domain dependent rules. Our rules were
created on the basis of a sorted list of all token de-
scriptions. That allowed us to analyze a group of to-
kens with the same base form e.g. an inflected noun.
Additional information concerning the frequency of
each description, indicated which token corrections
would be important.
Unfortunately, the process of rule creation is time-
consuming (it took about 90 hours to create them).
To speed up the process we postulate to prepare
three sets of tokens for which rules will be created
separately. The first one shall contain tokens which
are not recognized by a morphological analyzer, and
hence requiring transformation rules to be created
for them. The second set shall contain tokens with
more than one interpretation, for which a decision is
necessary. Finally we propose to take into account
the set of frequent descriptions. Infrequent tokens
can be left to the manual correction stage as it is eas-
ier to correct them knowing the context.
At the moment our corpus contains three annota-
tion levels ? segmentation into tokens, morpholog-
ical tags and semantic annotation. After the first
phase of corpus creation we decided to introduce
an additional level of annotation ? extended tok-
enization, see (Marcus Hassler, 2006). Current tok-
enization divides text into simple unstructured frag-
ments. This solution makes it easy to address any
important fragment of a text, but leaves the inter-
pretation of all complex strings to the next levels of
analysis. A new extended tokenization is planned to
create higher level tokens, semantically motivated.
It will allow the annotation of complex strings like:
dates (02.12.2004, 02/12/2004); decimal numbers;
ranges (10 - 15, 10-15); sizes and frequencies (10
x 15, 10x15); complex units (mm/h);abbreviations
with full stops (r. ? rok ?year?); acronyms contain-
ing non-letter characters (K+); complex medication
names (Mono Mack Depot).
Extended tokens can be recognized by rules tak-
ing into account two aspects: specificity of the
domain and problems resulting from careless typ-
ing. In the case of abbreviations and acronyms,
the best method is to use dictionaries, but some
heuristics can be useful too. Electronic dictionar-
ies of acronyms and abbreviations are not available
for Polish, but on the basis of annotated data, a do-
main specific lexicon can be created. Moreover, we
want to test ideas from (Kokkinakis, 2008), the au-
thor presents a method for the application of the
MeSH lexicon (that contains English and Latin data)
to Swedish medical corpus annotation. We will use
a similar approach for acronyms and complex medi-
cation name recognition.
99
References
K. Bretonnel Cohen, Lynne Fox, Philip V. Ogren, and
Lawrence Hunter. 2005. Corpus design for biomed-
ical natural language processing. In Proceedings of
the ACL-ISMB Workshop on Linking Biological Liter-
ature, Ontologies and Databases: Mining Biological
Semantics, pages 38?45, Detroit, June. Association for
Computational Linguistics.
Witold Droz?dz?yn?ski, Hans-Ulrich Krieger, Jakub Pisko-
rski, Ulrich Sch?fer, and Feiyu Xu. 2004. Shallow
Processing with Unification and Typed Feature Struc-
tures ? Foundations and Applications. German AI
Journal KI-Zeitschrift, 01/04.
Toma Erjavec, Yuka Tateisi, Jin dong Kim, Tomoko Ohta,
and Jun ichi Tsujii. 2003. Encoding Biomedical Re-
sources in TEI: the Case of the GENIA Corpus. In
Proceedings of the ACL 2003, Workshop on Natural
Language Processing in Biomedicine, pages 97?104.
Jing Jiang and Chengxiang Zhai. 2007. An Empiri-
cal Study of Tokenization Strategies for Biomedical
Information Retrieval. Information Retrieval, 10(4?
5):341?363.
Danuta Karwan?ska and Adam Przepi?rkowski. 2009. On
the evaluation of two Polish taggers. In The proceed-
ings of Practical Applications in Language and Com-
puters PALC 2009.
Jin-Dong Kim, Tomoko Ohtai, and Jun?ichi Tsujii. 2010.
Multilevel Annotation for Information Extraction In-
troduction to the GENIA Annotation. In Linguis-
tic Modeling of Information and Markup Languages,
pages 125?142. Springer.
Dimitrios Kokkinakis. 2006. Collection, Encoding and
Linguistic Processing of a Swedish Medical Corpus
? The MEDLEX Experience. In Proceedings of the
Fifth International Language Resources and Evalua-
tion (LREC?06), pages 1200?1205.
Dimitrios Kokkinakis. 2008. A Semantically Anno-
tated Swedish Medical Corpus. In Proceedings of the
Sixth International Language Resources and Evalua-
tion (LREC?08), pages 32?38.
Ma?gorzata Marciniak and Agnieszka Mykowiecka.
2011. Construction of a medical corpus based on in-
formation extraction results. Control & Cybernetics.
in preparation.
G?nther Fliedl Marcus Hassler. 2006. Text prepara-
tion throughextended tokenization. Data Mining VII:
Data, Text and Web Mining and their Business Appli-
cations, 37.
Agnieszka Mykowiecka and Ma?gorzata Marciniak.
2011. Automatic semantic labeling of medical texts
with feature structures. In The Text Speech and Dia-
logue Conference 2011 (submitted).
Agnieszka Mykowiecka, Ma?gorzata Marciniak, and
Anna Kups?c?. 2009. Rule-based information extrac-
tion from patients? clinical data. Journal of Biomedi-
cal Informatics, 42:923?936.
Maciej Piasecki and Adam Radziszewski. 2007. Polish
Morphological Guesser Based on a Statistical A Tergo
Index. In 2nd International Symposium Advances
in Artificial Intelligence and Applications (AAIA?07),
wis?a, Poland, pages 247?256.
Maciej Piasecki. 2007. Polish tagger TaKIPI: Rule based
construction and optimisation. Task Quarterly, 11(1?
2):151?167.
Adam Przepi?rkowski and Piotr Ban?ski. 2009. XML
text intechange format in the National Corpus of Pol-
ish. In The proceedings of Practical Applications in
Language and Computers PALC 2009, pages 245?250.
Adam Przepi?rkowski. 2004. Korpus IPI PAN. Wersja
wste?pna / The IPI PAN Corpus: Preliminary version.
IPI PAN.
Adam Przepi?rkowski. 2005. The IPI PAN Corpus
in numbers. In Zygmunt Vetulani, editor, Proc. of
the 2nd Language & Technology Conference, Poznan?,
Poland.
Sampo Pyysalo, Filip Ginter, Juho Heimonen, Jari
Bj?rne, Jorma Boberg, Jouni J?rvinen, and Tapio
Salakoski. 2007. BioInfer: a corpus for information
extraction in the biomedical domain. BMC Bioinfor-
matics, 8.
Angus Roberts, Robert Gaizauskas, Mark Hepple,
George Demetriou, Yikun Guo, Ian Roberts, and An-
drea Setzer. 2009. Building a semantically annotated
corpus of clinical texts. Journal of Biomedical Infor-
matics, 42(5):950?966.
Thomas Brox R?st, Ola Huseth, ?ystein Nytr?, and An-
ders Grimsmo. 2008. Lessons from developing an
annotated corpus of patient histories. Journal of Com-
puting Science and Engineering, 2(2):162?179.
Yuka Teteisi and Jun?ichi Tsujii. 2006. GENIA An-
notation Guidelines for Tokenization and POS Tag-
ging. Technical report, Tsujii Laboratory, University
of Tokyo.
Christos Tsalidis, Giorgos Orphanos, Elena Mantzari,
Mavina Pantazara, Christos Diolis, and Aristides
Vagelatos. 2007. Developing a Greek biomedical cor-
pus towards text mining. In Proceedings of the Corpus
Linguistics Conference (CL2007).
Marcin Wolin?ski. 2006. Morfeusz ? a Practical Tool for
the Morphological Analysis of Polish. In Mieczys?aw
K?opotek, S?awomir Wierzchon?, and Krzysztof Tro-
janowski, editors, IIS:IIPWM?06 Proceedings, Ustron,
Poland, pages 503?512. Springer.
100
Proceedings of the 4th International Workshop on Computational Terminology, pages 33?41,
Dublin, Ireland, August 23 2014.
NPMI driven recognition of nested terms
Ma?gorzata Marciniak
Institute of Computer Science, PAS
Jana Kazimierza 5,
01-248 Warsaw, Poland
mm@ipipan.waw.pl
Agnieszka Mykowiecka
Institute of Computer Science, PAS
Jana Kazimierza 5,
01-248 Warsaw, Poland
agn@ipipan.waw.pl
Abstract
In the paper, we propose a new method of identifying terms nested within candidates for the
terms extracted from domain texts. The list of all terms is then ranked by the process of automatic
term recognition. Our method of identifying nested terms is based on two aspects: grammatical
correctness and normalised pointwise mutual information (NPMI) counted for all bigrams on
the basis of a corpus. NPMI is typically used for recognition of strong word connections but in
our solution we use it to recognise the weakest points within phrases to suggest the best place for
division of a phrase into two parts. By creating only two nested phrases in each step we introduce
a binary hierarchical term structure. In the paper, we test the impact of the proposed nested terms
recognition method applied together with the C-value ranking method to the automatic term
recognition task.
1 Introduction
The Automatic Term Recognition (ATR) task consists in identifying linguistic expressions that refer to
domain concepts. This is usually realised in two steps. In the first one, candidates for terms are identified
in a corpus of domain texts. This step usually consists in identifying grammatically correct phrases by
means of linguistically motivated grammars describing noun phrases in a given language. However,
sometimes no linguistic knowledge is utilised and candidates for terms are just frequent n-grams as in
(Wermter and Hahn, 2005). The second processing step consists in ranking the extracted candidates
and selecting those which are most important for a considered domain. This task is usually based on
statistics.
The ranking procedure can be based on different measures which are characterised as either
?termhood-based? or ?unithood-based?. Kageura and Umino (1996) defined the termhood-based meth-
ods measure as ?the degree that a linguistic unit is related to domain-specific concepts?, i.e. the likelihood
that a phrase is a valid domain term. The unithood-based methods measure the collocation strength of
word sequences, usually with the help of log-likelihood, pointwise mutual information or T-score mea-
sures, described in (Manning and Sch?tze, 1999), while ATR applications based on them are described
in e.g., (Pantel and Lin, 2001), (Sclano and Velardi, 2007). A comparison of these approaches is given
in (Pazienza et al., 2005). Some hybrid solutions to the ATR problem have also been proposed (Vu et al.,
2008) or (Ventura et al., 2014). In the paper (Korkontzelos et al., 2008), the comparison between these
two groups of methods led the authors to the conclusion that the termhood-based methods outperform
the unithood-based ones.
This paper is devoted to the problem of selecting candidates for terms from an annotated domain
corpus. Our approach is based on the C-value method, (Frantzi et al., 2000). An important feature of this
method that attracted our attention was the focus on nested terms. Frantzi et al. (2000) described nested
terms as terms that appear within other longer terms, and may or may not appear by themselves in the
corpus. They show that recognition of nested terms is very important in terms extraction, but they also
give examples when a nested phrase constructed according to the grammar rules is not a term. One of
This work is licensed under a Creative Commons Attribution 4.0 International Licence. Page numbers and proceedings footer
are added by the organisers. Licence details: http://creativecommons.org/licenses/by/4.0/
33
these examples is the phrase real time clock which has two nested phrases: real time and time clock, but
the second one is not a good term. The authors define the C-value measure that is used to rank candidate
terms extracted from a domain corpus, together with their nested terms. It is counted on the basis of the
frequency of the term as a whole phrase in the corpus, its frequency as a nested phrase in other terms,
the number of different phrases in which that nested phrase occurred, and its length. The authors expect
that phrases that aren?t considered as terms should be placed at the end of the list ordered according to
this coefficient value.
We applied the C-value method to extract terminology from a corpus of hospital discharge documents
in Polish. Experiments, where different methods of counting the C-value were tested, are described in
(Marciniak and Mykowiecka, 2014). Unfortunately, a few grammatically correct but semantically odd
phrases were always placed in the top part of the ranking list of terms. Examples of such phrases, placed
among the 200 top positions, are: USG jamy ?USG of cavity? being a nested phrase of the very frequent
phrase USG jamy brzusznej ?USG of abdominal cavity?, infekcja g?rnych dr?g ?infection of upper tract?
or powie?kszony we?ze? ?enlarged node?.
We propose a method that prevents the creation and promotion of such nested phrases to be consid-
ered as terms. The main idea is to use a unithood-based method e.g., Normalised Pointwise Mutual
Information (NPMI) (Bouma, 2009) for driving recognition of nested phrases. Our solution is based on
the division of each considered phrase into only two parts. The places where a phrase is divided must
create nested phrases that are consistent with grammar rules. Usually, there are several possible places
for division of a phrase. From all of them, we choose the weakest point according to NPMI counted for
bigrams on the basis of the whole corpus. So, as a bigram constitutes a strong collocation, it prevents the
phrase from being dividing in this place, and does not usually lead to the creation of semantically odd
nested phrases, of which examples are given above.
The analysed corpus of Polish medical texts is described in Section 2. In the following two sections
we present the method in detail. Then, in Section 5, we describe the comparison of the resulting lists of
terms ranked according to the C-value measure, for two methods of recognition of nested phrases , i.e.:
for all possible phrases fulfilling grammatical rules, and for the method proposed in the paper.
2 Corpus description
The domain corpus consists of 3116 hospital discharge documents gathered at a hospital in Poland.
Texts came from six departments and were written by several physicians of different specialties. The
collected texts were analysed using standard general purpose NLP tools. The morphological tagger Pan-
tera (Aceda
?
nski, 2010), cooperating with the Morfeusz analayser (Woli
?
nski, 2006), was used to divide
the text into tokens and annotate them with morhosyntactic tags. They included a part of speech name
(POS), a base form, as well as case, gender and number information, where they were appropriate. This
information is used by shallow grammars recognising the boundaries of nominal phrases ? term can-
didates and, also,sources for nested phrases. The corpus consists of about 2 million tokens in which a
shallow grammar recognised more than 22 thousand noun phrases.
The corpus contains quite a lot of words unrecognised by Morfeusz as the vocabulary of the clinical
documents significantly differs from general Polish texts. Additionally, the texts are not very well edited
despite the spelling correction tools being usually turned on, so they contain quite a lot of misspelled
words. This results in 22,000 unrecognised tokens (many of them are medications, acronyms and units)
that are not taken into account when nominal phrases are recognised. Consequently, it lowers the number
of phrases, and affects the quality of some of them. In (Marciniak and Mykowiecka, 2011), the problems
of morphological annotation of hospital documents in Polish are presented and the reasons for the many
unrecognised tokens are highlighted.
3 Nested phrases recognition
In this section, we describe the way to create a list of term candidates that takes into account nested
phrases. This task is usually supported by linguistic knowledge that allows for identifying candidates for
terms which are syntactically valid.
34
In the extraction step, we identified complex noun phrases consisting of nouns with adjectival and
nominal modifiers obeying Polish grammar rules (in particular, case, gender and number agreement).
The types of Noun Phrases under consideration can be schematically defined as below:
AdjPhrase Noun AdjPhrase
AdjPhrase Noun
Noun AdjPhrase
NounPhrase NounPhrase-in-genitive
Noun Phrases were extracted from the corpus using a cascade of shallow grammars. As Polish is a
highly inflected language, we operate on simplified base forms of phrases in our computations, con-
sisting of lemmas of subsequent words. This approach, proposed for ATR in Polish in (Marciniak and
Mykowiecka, 2013), allows us to unify forms of phrases in different cases and numbers. For example:
przewlek?e zapalenie gard?a, przewlek?e zapalenia gard?a, przewlek?ego zapalenia gard?a, przewlek?ych
zapalen? gard?a are forms of ?chronic pharyngitis? in nominative singular and plural and genitive in both
numbers.
1
The extracted phrases constitute a foundation for creating the list of term candidates. Then
we add nested phrases, recognised within those phrases, to the list of term candidates. The rules for
identifying nested terms are described in the rest of this section.
3.1 Motivations
The original C-value method (Frantzi et al., 2000) recommends that all grammatical phrases, created
from the maximal phrases identified in a corpus, should be considered as term candidates. But using
this method, we quite frequently obtain nested grammatical subphrases which are syntactically correct,
but semantically odd. One such phrase is infekcja g?rnych dr?g ?infection (of the) upper tract? that is
created from infekcja g?rnych dr?g oddechowych ?infection (of the) upper respiratory tract?.
2
The last
phrase has many different longer phrases in which it is nested, eg: (cze?sta, drobna, ostra, bakteryjna...)
infekcja g?rnych dr?g oddechowych ?(often, minor, acute, bacterial...) infection (of the) upper respiratory
tract?, but it always concerns drogi oddechowe ?respiratory tract?. We observe that the bigram drogi
oddechowe ?respiratory tract? constitutes a strong collocation. So the original phrase shouldn?t be divided
in this place to create a phrase containing the word drogi ?tract? without adding its type, i.e., oddechowe
?respiratory? in this case. Nominal phrases are usually constructed from two parts (except for coordinated
phrases and nouns with more complex subcategoriaztion frames, which usually do not fulfill agreement
constraints in Polish). For nominal phrases from domain corpora, we suggest that the best place for the
division is indicated by the weakest bigram.
After considering patterns of nominal phrases in Polish, we realised that the weakest connections
are usually between two nominal phrases (the last pattern). So, an adjective more likely modifies
the nearest noun and not the whole phrase, as in: prawid?owa
adj
mikroflora
noun
g?rnych
adj
dr?g
noun
oddechowych
adj
?normal microflora (of the) upper respiratory tract?. In this phrase, all the outermost
adjectives are important parts of nominal phrases constructed around their nearest nouns, and it should
be divided into two nominal phrases: prawid?owa mikroflora ?normal microflora? and g?rne drogi odde-
chowe ?upper respiratory tract?. However, it is not the universal rule. Let us consider another example:
cze?ste infekcje g?rnych dr?g oddechowych ?frequent infections (of the) upper respiratory tract?, where
cze?ste ?frequent? modifies the whole phrase. To account for this observation, we may slightly prefer
divisions into two nominal phrases instead of an adjective and a nominal phrase.
3.2 Algorithm
From several methods for counting the strength of bigrams we chose the normalised pointwise mutual
information proposed by Bouma, (2009), as it is less sensitive to occurrence frequency. We were looking
for a method for which the bigram, consisting of a rare and a frequent token, will be high if the rare token
only appears in connection with the frequent token, as, for example, for esowate skrzywienie ?S-shaped
curvature?. The definition of this measure for the ?x y? bigram, where x and y are lemmas of sequence
1
Further in the paper we will use phrases in the nominal case and singular number forms. These forms may differ slightly
from the same phrases being nested ones (in genitive).
2
The word order of the translation is different.
35
tokens, is given in (1), where p(x,y) is a probability of the ?x y? bigram in the considered corpus, and
p(x), p(y) are probabilities of ?x? and ?y? unigrams respectively.
NPMI(x, y) =
(
ln
p(x, y)
p(x)p(y)
)/
? ln p(x, y) (1)
First, we extract all the grammatical phrases from the corpus, taking into account only the maximal
one. Then, for each phrase we identify all places where it can be divided according to grammar rules.
We count NPMI for those and indicate the weakest connection in the phrase. Then, we divide it into two
parts in this position. There are two possible situations: the first, when the phrase is divided into two
nominal phrases; the second, when one phrase is a nominal phrase while the second one is an adjective
phrase. In the first case, we add both parts to the list of term candidates and process the obtained parts of
the phrase in the same way. In the second case, only a nominal phrase is added to the list and only this
phrase is further divided.
nested_phrases (phr)
if length(phr) > 1
find all i positions where phr can be divided according to the grammatic rules
for all i positions
count NPMI(i-th bigram of phr)
sort NPMIs from the lowest to the highest value
j := position with the lowest NPMI
if the j-th position divides phr into two nominal phrases
divide phr into phr1 and phr2 on j-th position
add phr1 and phr2 to the list of nested terms
nested_phrases(phr1)
nested_phrases(phr2)
else
n := position with the lowest NPMI where phr is divided into two nominal phrases
if (120% NPMI(j)) > NPMI (n)
divide phr into phr1 and phr2 on n-th position
add phr1 and phr2 to the list of nested terms
nested_phrases(phr1)
nested_phrases(phr2)
else
if phr is divided on j position into adjective phrase to the left of nominal phrase
cut off the outermost left element from phr
else
cut off the outermost right element from phr
add phr to the list of nested terms
nested_phrases(phr)
Figure 1: Procedure of nested phrases recognition
To take into account the specificity of adjectives in Polish nominal phrases described in 3.1, we decided
to introduce a slight modification to the basic algorithm. If the weakest connection prefers the cutting of
an adjective part from a phrase, we find the nearest place where the phrase is divided into two nominal
phrases. Then, we compare the NPMI value referring to this bigram with 120% (fixed experimentally)
of the lowest NPMI value. If it is still lower, we cut off one outermost element (adjective or adverb)
from this adjectival part of the phrase and add the slightly shorter phrase to the term list. In other case,
we divide the original phrase in that second place into two nominal phrases. The algorithm is given in
Figure 1.
36
The grammatically correct nested phrases The nested phrases divided with help of NPMI
?infection? ?upper? ?tract? ?respiratory? ?infection? ?upper? ?tract? ?respiratory?
infekcja g?rnych dr?g oddechowych infekcja g?rnych dr?g oddechowych
infekcja g?rnych dr?g ?
infekcja infekcja
g?rne drogi oddechowe g?rne drogi oddechowe
g?rne drogi ?
drogi oddechowe drogi oddechowe
drogi drogi
Table 1: The nested phrases for two methods
bigram translation NPMI
infekcja g?rna ?infection upper? 0.65658
g?rna droga ?upper tract? 0.78773
droga oddechowy ?tract respiratory? 0.95089
Table 2: The NPMI value for the bigrams of the phrase: infekcja g?rnych dr?g oddechowych
3.3 Examples
Let us consider examples illustrating the method. We compare nested phrases obtained from the phrase
infekcja g?rnych dr?g oddechowych ?infection (of the) upper respiratory tract? for the two following
methods: creating all grammatically correct nested phrases, and the NPMI driven method. The consid-
ered phrase is constructed according to the following pattern: Noun
j
Adj
i
Noun
i
Adj
i
where indexes
indicate agreement constraints, so a grammatically correct phrase may consist of: Noun
j
Adj
i
Noun
i
,
but can?t be constructed as: Noun
j
Adj
i
. Thus, infekcja g?rnych dr?g ?infection of the upper tract? is
grammatically correct, while infekcja g?rnych ?infection of upper? is not. The phrase can be divided
in one of two places indicated by the ?|? character: infekcja | g?rnych dr?g | oddechowych, ?infection
| upper tract | respiratory?
3
and it is possible to create six grammatically correct phrases, see Table 1.
Applying our method, we first count NPMI for the places of possible divisions. The NPMI value for
two bigrams infekcja g?rny ?infection upper? and droga oddechowy ?tract respiratory? counted for the
corpus described in Section 2 are given in Table 2. The lower value is for the first bigram so the phrase
can be divided into: infekcja ?infection? and g?rne drogi oddechowe ?upper respiratory tract?. Both parts
constitute nominal phrases so the phrase is divided in this place and both parts are added to the list of
term candidates. In the next step only the second phrase can be recursively divided. The weaker connec-
tion is for: g?rny droga ?upper tract?. So the adjective g?rna ?upper? is cut off the phrase and only the
nested phrase drogi oddechowe ?respiratory tract? is accepted as a term candidate. Table 1 contains all
the nested phrases obtained by both methods for the considered phrase. It may be noted that our method,
correctly, does not extract two semantically odd nested phrases from the six obtained by the first method.
Let us consider a phrase where the lowest NPMI indicates division into an adjective and a nominal
phrase: boczne
adj
skrzywienie
noun
kre?gos?upa
noun
?lateral curvature (of the) spine?. The phrase can
be divided in both places: boczne | skrzywienie | kre?gos?upa ?lateral | curvature | spine?. The weakest
connection is for the bigram: boczny skrzywienie ?lateral curvature?, it indicates division into the nominal
phrase skrzywienie kre?gos?upa ?curvature (of the) spine?, and the adjective boczne ?lateral?. The other
place of division causes the phrase to be divided into two nominal phrases. So we compare the NPMI for
skrzywienie kre?gos?up ?curvature spine?, with 120% NPMI boczny skrzywienie ?lateral curvature?, see
Table 3. As the first value is lower than the second one, the method prefers to divide the phrase into
two nominal phrases boczne skrzywienie ?lateral curvature? and kre?gos?up ?spine?. The basic algorithm,
without multiplying NPMI values in some cases by 120%, creates a good term skrzywienie kre?gos?upa
?curvature (of the) spine? instead of two nominal phrases: boczne skrzywienie ?lateral curvature? and
3
The word for word translation.
37
bigram translation NPMI 120% NPMI
boczny skrzywienie ?lateral curvature? 0.67619 0.81143
skrzywienie kre?gos?up ?curvature spine? 0.80151
Table 3: The NPMI value for the bigrams of the phrase: boczne skrzywienie kre?gos?upa
kre?gos?up spine.
There are a few cases when the phrase division driven by the NPMI value prefers cutting off an ad-
jective in the first step instead of dividing it into two nominal phrases, see: oko?oporodowe
adj
uszkodze-
nie
noun
splotu
noun
ramiennego
adj
prawego
adj
?perinatal damage (of) right brachial plexus?. Despite the
fact that oko?oporodowe uszkodzenie splotu ramiennego ?perinatal damage (of) brachial plexus? is a good
term, we would prefer the division into two nominal phrases oko?oporodowe uszkodzenie ?perinatal dam-
age? and splot ramienny prawy ?right brachial plexus?. The last division reflects the internal construction
of the phrase that might be important in an ontology construction task which is one of the intended uses
of the method. In this case, we want to recognise nested phrases representing two concepts which are in
a relationship. The method still (correctly) cuts off the adjective cze?sty ?frequent? from the phrase cze?ste
infekcje g?rnych dr?g oddechowych ?frequent infections (of the) upper respiratory tract?.
4 Terms ordering
To test to what extent our approach to the phrase selection problem influences the ultimate results of the
term selection algorithm, we used the C-value coefficient (Frantzi et al., 2000) to order extracted phrases.
The standard equation for this coefficient is given in (2) where p is the phrase under consideration, freq(p)
is a number of occurrences of this phrase both nested and in isolation, LP is a set of phrases containing
p, r(LP) ? the number of different phrases in LP, and l(p) = log
2
(length(p)).
C-value(p) =
{
l(p) ? (freq(p)?
1
r(LP )
?
lp?LP
freq(lp)), if r(LP ) > 0,
l(p) ? freq(p), if r(LP ) = 0
(2)
The C-value ranking method is focused on deciding which nested phrases should be considered as
terms. It assigns higher values to phrases which, having the same frequency rate, occur more frequently
in isolation or occur in a larger number of different longer phrases, i.e., have different lexical contexts
within a set of initially extracted phrases. To account for the fact that long phrases tend to occur more
rarely than shorter ones, the result is multiplied by the logarithm of the phrase length. If a phrase occurs
only in isolation, its frequency rate defines the C-value. When a phrase occurs only in one context,
its C-value gets the value 0 as it is properly assumed to be incomplete. If a nested phrase occurs in a
lot of different contexts, its chances of constituting a domain term increase. A slight modification of
the method also allows for processing phrases of length 1, which originally all got a 0 value. For this
purpose, for one word phrases, the logarithm of the length (used in the original solution) is replaced with
a non zero constant. In (Barr?n-Cedeno et al., 2009), where this method was applied to Spanish texts,
the authors set it to 1, arguing that if it is lower, one word terms are located too low on the ranking list
(it cannot be greater than 1 for obvious reasons). Our experiments proved that in our data, such a change
results in very many one word elements at the top of the list, we used a 0.1 value as the equivalent of
logarithm of length for one word phrases.
The results obtained using the C-value method depend on the details concerning the way in which
we distinguish different phrases, i.e., how we count r(LP). First, for inflectional languages like Polish,
a method for recognising inflected forms of a multiword phrase has to be established. In our experi-
ment, we used base form sequences for this purpose. Secondly, the way of counting contexts has to be
elaborated. For example, it should be decided, whether red blood cells and white blood cells are two
different contexts for cell or only one. For languages with more relaxed word order, like Polish, the same
phrase can appear in different orders, e.g., liczne krwinki bia?e ?numerous white blood cells? or krwinki
bia?e liczne ?white blood cells numerous?. As the C-value coefficient is drastically different for frequent
phrases which occur in one and in two different contexts, we tried to limit the number of phrase types
38
length all =1 =2 3?5 >5
s-phrases 32809 4918 13442 13984 465
npmi-phrases 28328 4918 11693 11313 393
s&npmi-phrases 26671 4918 10420 10929 404
frequency =1 2-10 11-50 51-100 101-1000 >1000
in isolation 13304 6776 1506 300 415 81
s-phrases 18572 10417 2461 523 704 132
s&npmi-phrases 15210 8296 2002 420 625 118
C-value 0 0<c<1 1?c<5 5?c<10 10 ?c<100 >100
s-phrases 8946 2500 16891 1804 2312 357
s&npmi-phrases 3428 2508 16652 1672 2074 337
Table 4: The number of recognised phrases
total removed lowered
changes all correctly all incorrectly correctly questionable
nmpi/s-phrases 39 39 30 0 - - -
s&nmpi
1
/s-phrases 137 28 26 109 19 73 17
s&nmpi/s-phrases 132 27 27 105 20 70 15
Table 5: The number of correct changes for the first 2000 positions
which differ only in order or are included one in another. We discussed different methods of counting
contexts in (Marciniak and Mykowiecka, 2014) and concluded there that none of the tested ranking pro-
cedures were able to filter out all semantically odd noun phrases from the top of the list of terms. The
best results we obtained taking only the nearest context of a phrase into account, i.e. the closest word to
the left or to the right of a phrase. We used the greater number of these different left and right contexts.
This solution can lower the actual number of contexts, but it prevents us from counting the same context
words placed before and after the phrase twice.
5 Results and evaluation
We applied the C-value method to two sets of term candidates. The first set contains all possible phrases
fulfilling the grammatical rules, while the second one is obtained by the method described in the previous
sections. It is worth noting that we consider contexts of nested phrases only when they are recognised
in phrases by the method. As both methods recognised different numbers of phrases,
4
Table 4 gives
the comparison of their numbers. In this table, s-phrases refers to the baseline solution in which all
grammatically correct nested phrases are taken into account, npmi-phrases refers to the solution obtained
while recognising nested phrases using only NPMI value and s&nmpi-pharses is a name used for the
final solution in which both grammar rules and NPMI values are utilised. Initially, 32809 phrases were
extracted. The number of candidate phrases was significantly lower after applying NPMI selection (by
15%), but some of them were not grammatically correct. When applying both selection criteria we
obtained about 80% of the phrases (only grammatically correct) from the s-phrases set. The reduction
concerned phrases irrespective of their occurrences within texts. As to the distribution of the C-value, it
may be seen that we finally obtained much fewer phrases with a 0 C-value.
In the paper (Marciniak and Mykowiecka, 2014), an evaluation of different aspects of the original C-
value method applied to the same domain corpus is given. In this paper, we want to verify the tendencies
4
The set of phrases recognised by the proposed method is included in that consisting of phrases recognised by the standard
method based on all valid phrases.
39
of changes introduced by the proposed method. To focus on this task, we analysed all phrases that were
included in the top 2000 positions ranked by the first method and whose position was moved below
the 3000 in the final list, see Table 5. This comparison shows that our solution removed 6.6% (132) of
phrases from the top of the list of terms, and 73.5% (97) among them were semantically odd phrases.
We compared the baseline with the version in which, the minimum of NPMI value was always used to
indicate phrase division (s&nmpi
1
) and with the final version, in which the division into two noun phrases
was preferred (i.e. if the NPMI at the division position was not significantly higher than the minimum
inside phrase). In the first case, we observed the elimination of only 39 phrases from the top 2000.
From these sequences, 9 were incorrectly removed from the candidates list. Using both NPMI value and
grammaticality test resulted in 137 changes inside the top 2000. This time, from 28 removed elements
only 2 could be considered correct. In the final solution, all 27 phrases eliminated form the first 2000
were correctly eliminated, while from the remaining 105 phrases, whose positions were significantly
lowered, 70 were not terms. For some phrases it is difficult to judge whether they are domain related
phrases or are rather related to other topics. These cases were labelled as ?questionable? in the table.
As the proposed method does not change the way of counting whole phrases recognised in the corpus,
we cannot expect that every incorrect phrase will be eliminated. For example, the phrase infekcja g?rnych
dr?g ?infection (of the) upper tract? cannot disappear from our list of term candidates, as it occurred three
times as a whole phrase due to a spelling error in the word oddechowy ?respiratory?. We only expect that
its position is similar to the position of this phrase ranked according to the frequency of the whole phrase.
We obtained this required effect. The semantically odd phrase, considered above, changed its position
from 144 to 4374.
The presented results show that integrating NPMI with syntactic rules resulted both in better selection
and ranking of candidates. The final decision to prefer division into two noun phrases had rather small
but positive effects.
6 Conclusion
In the paper, we described a method for recognising nested phrases based on normalised pointwise mutual
information. We proved that the method has a strong tendency not to recognise semantically odd phrases
once they are nested, and allows for the elimination of incorrect unfinished phrases from the top part of
the ranking list. The method can be applied to any language: it requires the existence of a POS tagger and
several rules describing noun phrase structure. Taking into account information on the internal syntactic
structure of terms improved the results.
There are several possible directions for further research. First, we plan to test the method on different
datasets. Then, some extensions of the method are planned. The potentially easiest one concerns the
problem of how to extend the method to take into account more complex phrases (i.e. prepositional
phrases and coordinated phrases) and count NPMI effectively for them. The second problem refers to
longer phrases that are strongly connected but only when all elements appear together. An example of
such a phrase is wyk?adnik stanu zapalnego ?inflammation exponent? where stan zapalny ?inflammation?
can appear in different contexts, but wyk?adnik stanu ?exponent (of the) state? implies the word zapalny
?inflammatory?. The third problem is to explore whether the proposed method provides a good starting
point for recognising pieces of information that should be represented in a domain ontology.
References
Szymon Aceda
?
nski. 2010. A morphosyntactic Brill tagger for inflectional languages. In Hrafn Loftsson, Eir?kur
R?gnvaldsson, and Sigr?n Helgad?ttir, editors, Advances in Natural Language Processing, volume 6233 of
Lecture Notes in Computer Science, pages 3?14. Springer.
Alberto Barr?n-Cedeno, Gerardo Sierra, Patrick Drouin, and Sophia Ananiadou. 2009. An improved automatic
term recognition method for Spanish. In Computational Linguistics and Intelligent Text Processing, LNCS
5449, pages 125?136. Springer.
Gerlof Bouma. 2009. Normalized (pointwise) mutual information in collocation extraction. In From Form to
40
Meaning: Processing Texts Automatically, Proceedings of the Biennial GSCL Conference 2009, volume Nor-
malized, pages 31?40, T?bingen.
Katerina Frantzi, Sophia Ananiadou, and Hideki Mima. 2000. Automatic recognition of multi-word terms: the
C-value/NC-value method. Int. Journal on Digital Libraries, 3:115?130.
Kyo Kageura and Bin Umino. 1996. Method for automatic term recognition. A review. Terminology, 3:2:259?289.
Ioannis Korkontzelos, Ioannis P. Klapaftis, and Suresh Manandhar. 2008. Reviewing and evaluating automatic
term recognition techniques. In Advances in Natural Language Processing, LNAI 5221, volume 5221, pages
248?259. Springer.
Christopher D. Manning and Hinrich Sch?tze. 1999. Foundations of Statistical Natural Language Processing.
MIT Press, Cambridge, MA, USA.
Ma?gorzata Marciniak and Agnieszka Mykowiecka. 2011. Towards Morphologically Annotated Corpus of Hos-
pital Discharge Reports in Polish. In Proceedings of BioNLP 2011, pages 92?100.
Ma?gorzata Marciniak and Agnieszka Mykowiecka. 2013. Terminology extraction from domain texts in Polish.
In R. Bembenik, L. Skonieczny, H. Rybinski, M. Kryszkiewicz, and M. Niezgodka, editors, Intelligent Tools
for Building a Scientific Information Platform. Advanced Architectures and Solutions, volume 467 of Studies in
Computational Intelligence, pages 171?185. Springer-Verlag.
Ma?gorzata Marciniak and Agnieszka Mykowiecka. 2014. Terminology extraction from medical texts in polish.
Journal of Biomedical Semantics, 5:24.
Patrick Pantel and Dekang Lin. 2001. A statistical corpus-based term extractor. In Proceedings of the 14th
Biennial Conference of the Canadian Society on Computational Studies of Intelligence: Advances in Artificial
Intelligence, pages 36?46, London, UK, UK. Springer-Verlag.
Maria T. Pazienza, Marco Pennacchiotti, and Fabio M. Zanzotto. 2005. Terminology Extraction: An Analysis of
Linguistic and Statistical Approaches. In S. Sirmakessis, editor, Knowledge Mining Series: Studies in Fuzziness
and Soft Computing. Springer Verlag.
Francesco Sclano and Paola Velardi. 2007. Termextractor: a web application to learn the shared terminology
of emergent web communities. In Ricardo Jardim-Gon?alves, J?rg P. M?ller, Kai Mertins, and Martin Zelm,
editors, Enterprise Interoperability II, pages 287?290. Springer.
Juan A. Lossio Ventura, Clement Jonquet, Mathieu Roche, and Maguelonne Teisseire. 2014. Towards a mixed
approach to extract biomedical terms from documents. International Journal of Knowledge Discovery in Bioin-
formatics, 4(1).
Thuy Vu, Ai Ti Aw, and Min Zhang. 2008. Term extraction through unithood and termhood unification. In
Proceedings of International Joint Conference on Natural Language Processing.
Joachim Wermter and Udo Hahn. 2005. Massive biomedical term discovery. In Discovery Science, LNCS 3735,
pages 281?293. Springer Verlag.
Marcin Woli?nski. 2006. Morfeusz ? a practical solution for the morphological analysis of Polish. In Intelligent
Information Processing and Web Mining. Proceedings of the International IIS:IIPWM?06 Conference held in
Ustron, Poland. Springer-Verlag.
41
