Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 575?584,
MIT, Massachusetts, USA, 9-11 October 2010. c?2010 Association for Computational Linguistics
Two Decades of Unsupervised POS induction: How far have we come?
Christos Christodoulopoulos
School of Informatics
University of Edinburgh
christos.c@ed.ac.uk
Sharon Goldwater
School of Informatics
University of Edinburgh
sgwater@inf.ed.ac.uk
Mark Steedman
School of Informatics
University of Edinburgh
steedman@inf.ed.ac.uk
Abstract
Part-of-speech (POS) induction is one of the
most popular tasks in research on unsuper-
vised NLP. Many different methods have been
proposed, yet comparisons are difficult to
make since there is little consensus on eval-
uation framework, and many papers evalu-
ate against only one or two competitor sys-
tems. Here we evaluate seven different POS
induction systems spanning nearly 20 years of
work, using a variety of measures. We show
that some of the oldest (and simplest) systems
stand up surprisingly well against more recent
approaches. Since most of these systems were
developed and tested using data from the WSJ
corpus, we compare their generalization abil-
ities by testing on both WSJ and the multi-
lingual Multext-East corpus. Finally, we in-
troduce the idea of evaluating systems based
on their ability to produce cluster prototypes
that are useful as input to a prototype-driven
learner. In most cases, the prototype-driven
learner outperforms the unsupervised system
used to initialize it, yielding state-of-the-art
results on WSJ and improvements on non-
English corpora.
1 Introduction
In recent years, unsupervised learning has become
a hot area in NLP, in large part due to the use of
sophisticated machine learning approaches which
promise to deliver better results than more tradi-
tional methods. Often the new approaches are tested
using part-of-speech (POS) tagging as an example
application, and usually they are shown to perform
better than one or another comparison system. How-
ever, it is difficult to draw overall conclusions about
the relative performance of unsupervised POS tag-
ging systems because of differences in evaluation
measures, and the fact that no paper includes di-
rect comparisons against more than a few other sys-
tems. In this paper, we attempt to remedy that
situation by providing a comprehensive evaluation
of seven different POS induction systems spanning
nearly 20 years of research. We focus specifically
on POS induction systems, where no prior knowl-
edge is available, in contrast to POS disambigua-
tion systems (Merialdo, 1994; Toutanova and John-
son, 2007; Naseem et al, 2009; Ravi and Knight,
2009; Smith and Eisner, 2005), which use a dic-
tionary to provide possible tags for some or all of
the words in the corpus, or prototype-driven sys-
tems (Haghighi and Klein, 2006), which use a small
set of prototypes for each tag class, but no dictio-
nary. Our motivation stems from another part of our
own research, in which we are trying to use NLP
systems on over 50 low-density languages (some of
them dead) where both tagged corpora and language
speakers are mostly unavailable. We therefore de-
sire to use these systems straight out of the box and
to know how well we can expect them to work.
One difficulty in evaluating POS induction sys-
tems is that there is no straightforward way to map
the clusters found by the algorithm onto the gold
standard tags; moreover, some systems are designed
to induce the number of clusters as well as their
contents, so the number of found clusters may not
match either the gold standard or that of another sys-
tem. Nevertheless, most recent papers have used
mapping-based performance measures (either one-
to-one or many-to-one accuracy). Here, we argue
that the entropy-based V-Measure (Rosenberg and
575
Hirschberg, 2007) is more useful in many cases, be-
ing more stable across different numbers of found
and true clusters, and avoiding several of the prob-
lems with another commonly used entropy-based
measure, Variation of Information (Meila?, 2003).
Using V-Measure along with several other evalu-
ation measures, we compare the performance of the
different induction systems on bothWSJ (the data on
which most systems were developed and tested) and
Multext East, a corpus of parallel texts in eight dif-
ferent languages. We find that for virtually all mea-
sures and datasets, older systems using relatively
simple models and algorithms (Brown et al, 1992;
Clark, 2003) work as well or better than systems
using newer and often far more sophisticated and
time-consuming machine learning methods (Gold-
water and Griffiths, 2007; Johnson, 2007; Graca et
al., 2009; Berg-Kirkpatrick et al, 2010). Thus, al-
though these newer methods have introduced po-
tentially useful machine learning techniques, they
should not be assumed to provide the best perfor-
mance for unsupervised POS induction.
In addition to our review and comparison, we in-
troduce a new way to both evaluate and potentially
improve a POS induction system. Our method is
based on the prototype-driven learning system of
Haghighi and Klein (2006), which achieves very
good performance by using a hand-selected list of
prototypes for each syntactic cluster. We instead use
the existing POS induction systems to induce proto-
types automatically, and evaluate the systems based
on the quality of their prototypes. We find that the
oldest system tested (Brown et al, 1992) produces
the best prototypes, and that using these prototypes
as input to Haghighi and Klein?s system yields state-
of-the-art performance on WSJ and improvements
on seven of the eight non-English corpora.
2 POS Induction Systems
We describe each system only briefly; for details,
see the respective papers, cited below. Each system
outputs a set of syntactic clusters C; except where
noted, the target number of clusters |C| must be
specified as an input parameter. Since we are in-
terested in out-of-the-box performance, we use the
default parameter settings for each system, except
for |C|, which is varied in some of our experiments.
The systems are as follows:1
[brown]: Class-based n-grams (Brown et al,
1992). This is the oldest and one of the simplest sys-
tems we tested. It uses a bigram model where each
word type is assigned to a latent class (a hard assign-
ment), and the probability of the corpus w1 . . . wn
is computed as P (w1|c1)
?n
i=2 P (wi|ci)P (ci|ci?1),
where ci is the class of wi. The goal is to opti-
mize the probability of the corpus under this model.
The authors use an approximate search procedure:
greedy agglomerative hierarchical clustering fol-
lowed by a step in which individual word types are
considered for movement to a different class if this
improves the corpus probability.
[clark]: Class-based n-grams with morphology
(Clark, 2003). This system uses a similar model
to the previous one, and also clusters word types
(rather than tokens, as the rest of the systems do).
The main differences between the systems are that
clark uses a slightly different approximate search
procedure, and that he augments the probabilistic
model with a prior that prefers clusterings where
morphologically similar words are clustered to-
gether. The morphology component is implemented
as a single-order letter HMM.
[cw]: Chinese Whispers graph clustering (Bie-
mann, 2006). Unlike the other systems we consider,
this one induces the value of |C| rather than taking
it as an input parameter.2 The system uses a graph
clustering algorithm called Chinese Whispers that is
based on contextual similarity. The algorithm works
in two stages. The first clusters the most frequent
10,000 words (target words) based on their context
statistics, with contexts formed from the most fre-
quent 150-250 words (feature words) that appear ei-
1Implementations were obtained from:
brown: http://www.cs.berkeley.edu/?pliang/
software/brown-cluster-1.2.zip (Percy Liang),
clark: http://www.cs.rhul.ac.uk/home/alexc/
pos2.tar.gz (Alex Clark),
cw: http://wortschatz.uni-leipzig.de/%7Ecbiemann/
software/jUnsupos1.0.zip (Chris Biemann),
bhmm, vbhmm, pr, feat: by request from the authors of the
respective papers.
2Another recent model that induces |C| is the Infinite HMM
(Van Gael et al, 2009). Unfortunately, we were unable to ob-
tain code for the IHMM in time to include it in our analysis.
Van Gael et al (2009) report results of around 59% V-Measure
on WSJ, with 194 induced clusters, which is not as good as the
best system scores in Section 4.
576
ther to the left or right of a target word. The second
stage deals with medium and low frequency words
and uses pairwise similarity scores calculated by the
number of shared neighbors between two words in
a 4-word context window. The final clustering is
a combination of the clusters obtained in the two
stages. While the number of target words, feature
words, and window size are in principle parameters
of the algorithm, they are hard-coded in the imple-
mentation we used and we did not change them.
[bhmm]: Bayesian HMM with Gibbs sampling
(Goldwater and Griffiths, 2007). This system is
based on a standard HMM for POS tagging. It dif-
fers from the standard model by placing Dirichlet
priors over the multinomial parameters defining the
state-state and state-emission distributions, and uses
a collapsed Gibbs sampler to infer the hidden tags.
The Dirichlet hyperparameters ? (which controls the
sparsity of the transition probabilities) and ? (which
controls the sparsity of the emission probabilities)
can be fixed or inferred. We used a bigram version
of this model with hyperparameter inference.
[vbhmm]: Bayesian HMM with variational
Bayes (Johnson, 2007). This system uses the
same bigram model as bhmm, but uses variational
Bayesian EM for inference. We fixed the ? and ?
parameters to 0.1, values that appeared to be reason-
able based on Johnson (2007), and which were also
used by Graca et al (2009).
[pr]: Sparsity posterior-regularization HMM
(Graca et al, 2009). The Bayesian approaches de-
scribed above encourage sparse state-state and state-
emission distributions only indirectly through the
Dirichlet priors. This system, while utilizing the
same bigram HMM, encourages sparsity directly
by constraining the posterior distributions using the
posterior regularization framework (Ganchev et al,
2009). A parameter ? controls the strengths of the
constraints (default = 25). Following Graca et al
(2009), we set ? = ? = 0.1.
[feat]: Feature-based HMM (Berg-Kirkpatrick
et al, 2010). This system uses a model that has the
structure of a standard HMM, but assumes that the
state-state and state-emission distributions are logis-
tic, rather than multinomial. The logistic distribu-
tions allow the model to incorporate local features
of the sort often used in discriminative models. The
default features are morphological, such as character
trigrams and capitalization.
3 Evaluation Measures
One difficulty in comparing POS induction meth-
ods is in finding an appropriate evaluation measure.
Many different measures have been proposed over
the years, but there is still no consensus on which is
best. In addition, some measures with supposed the-
oretical advantages, such as Variation of Information
(VI) (Meila?, 2003) have had little empirical analy-
sis. Our goal in this section is to determine which
of these measures is most sensible for evaluating
the systems presented above. We first describe each
measure before presenting empirical results. Except
for VI, all measures range from 0 to 1, with higher
scores indicating better performance.
[many-to-1]: Many-to-one mapping accuracy
(also known as cluster purity) maps each cluster to
the gold standard tag that is most common for the
words in that cluster (henceforth, the preferred tag),
and then computes the proportion of words tagged
correctly. More than one cluster may be mapped to
the same gold standard tag. This is the most com-
monly used metric across the literature as it is in-
tuitive and creates a meaningful POS sequence out
of the cluster identifiers. However, it tends to yield
higher scores as |C| increases, making comparisons
difficult when |C| can vary.
[crossval]: Cross-validation accuracy (Gao and
Johnson, 2008) is intended to address the problem
with many-to-one accuracy which is that assigning
each word to its own class yields a perfect score. In
this measure, the first half of the corpus is used to
obtain the many-to-one mapping of clusters to tags,
and this mapping is used to compute the accuracy of
the clustering on the second half of the corpus.
[1-to-1]: One-to-one mapping accuracy
(Haghighi and Klein, 2006) constrains the mapping
from clusters to tags, so that at most one cluster can
be mapped to any tag. The mapping is performed
greedily. In general, as the number of clusters
increases, fewer clusters will be mapped to their
preferred tag and scores will decrease (especially
if the number of clusters is larger than the number
of tags, so that some clusters are unassigned and
receive zero credit). Again, this makes it difficult to
577
compare solutions with different values of |C|.
[vi]: Variation of Information (Meila?, 2003) is
an information-theoretic measure that regards the
system output C and the gold standard tags T as two
separate clusterings, and evaluates the amount of in-
formation lost in going from C to T and the amount
of information gained, i.e., the sum of the condi-
tional entropy of each clustering conditioned on the
other. More formally, V I(C, T ) = H(T |C) +
H(C|T ) = H(C)+H(T )? 2I(C, T ), where H(.)
is the entropy function and I(.) is the mutual infor-
mation. VI and other entropy-based measures have
been argued to be superior to accuracy-based mea-
sures such as those above, because they consider
not only the majority tag in each cluster, but also
whether the remainder of the cluster is more or less
homogeneous. Unlike the other measures we con-
sider, lower scores are better (since VI measures the
difference between clusterings in bits).
[vm]: V-Measure (Rosenberg and Hirschberg,
2007) is another entropy-based measure that is de-
signed to be analogous to F-measure, in that it is de-
fined as the weighted harmonic mean of two values,
homogeneity (h, the precision analogue) and com-
pleteness (c, the recall analogue):
h = 1? H(T |C)
H(T )
(1)
c = 1? H(C|T )
H(C)
(2)
VM = (1 + ?)hc
(?h) + c
(3)
As with F-measure, ? is normally set to 1.
[vmb]: V-beta is an extension to V-Measure, pro-
posed by (Vlachos et al, 2009). They noted that
V-Measure favors clusterings where the number of
clusters |C| is larger than the number of POS tags
|T |. To address this issue the parameter ? in equa-
tion 3 is set to |C|/|T | in order adjust the balance
between homogeneity and completeness.
[s-fscore]: Substitutable F-score (Frank et al,
2009). One potential issue with all of the above mea-
sures is that they require a gold standard tagging to
compute. This is normally available during develop-
ment of a system, but if the system is deployed on a
novel language a gold standard may not be available.
In addition, there is the question of whether the gold
standard itself is ?correct?. Recently, Frank et al
(2009) proposed this novel evaluation measure that
requires no gold standard, instead using the concept
of substitutability to evaluate performance. Instead
of comparing the system?s clusters C to gold stan-
dard clusters T , they are compared to a set of clus-
ters S created from substitutable frames, i.e., clus-
ters of words that occur in the same syntactic en-
vironment. Ideally a substitutable frame would be
created by sentences differing in only one word (e.g.
?I want the blue ball.? and ?I want the red ball.?)
and the resulting cluster would contain the words
that change (e.g. [blue, red]). However since it is
almost impossible to find these types of sentences
in real-world corpora, the authors use frames cre-
ated by two words appearing in the corpus with ex-
actly one word between (e.g. the ?- ball). Once the
substitutable clusters have been created, they can be
used to calculate the Precision (SP ), Recall (SR)
and F-score (SF ) of the system?s clustering:
SP =
?
s?S
?
c?C |s ? c|(|s ? c| ? 1)
?
c?C |c|(|c| ? 1)
(4)
SR =
?
s?S
?
c?C |s ? c|(|s ? c| ? 1)
?
s?S |s|(|s| ? 1)
(5)
SF = 2 ? SP ? SR
SP + SR
(6)
3.1 Empirical results
We mentioned a few strengths and weaknesses of
each evaluation method above; in this section we
present some empirical results to expand on these
claims. First, we examine the effects of varying |C|
on the behavior of the evaluation measures, while
keeping the number of gold standard tags the same
(|T | = 45). Results were obtained by training and
evaluating each system on the full WSJ portion of
the Penn Treebank corpus (Marcus et al, 1993). Fig-
ure 1 shows the results from the Brown system for
|C| ranging from 1 to 200; the same trends were ob-
served for all other systems.3 In addition, Table 1
provides results for the two extremes of |C| = 1 (all
words assigned to the same cluster) and |C| equal to
the size of the corpus (a single word per cluster), as
3The results reported in this paper are only a fraction
of the total from our experiments; given the number of
parameters, models and measures tested, we obtained over
15000 results. The full set of results can be found at
http://homepages.inf.ed.ac.uk/s0787820/pos/.
578
Figure 1: Scores for all evaluation measures as a function of the number of clusters returned [model:brown, corpus:wsj,
|C|:{1-200}, |T |:45]. The right-hand y-axis shows VI scores (lower is better); the left-hand y-axis shows percentage
scores for all other measures. The vertical line indicates |T |. Many-to-1 is invisible as it tracks crossval so closely.
measure super random all single
many-to-1 97.85 13.97 13.97 100
crossval 97.59 13.98 13.98 0
1-to-1 97.86 2.42 13.97 0.01
vi 0.35 9.81 4.33 15.82
vm 95.98 0.02 0 35.42
vmb 95.98 0 0 99.99
s-fscore 7.53 0.50 0 0
Table 1: Baseline scores for the different evaluation mea-
sures on the WSJ corpus. For all measures except VI
higher is better.
well as two other baselines (a supervised tagging4
and a random clustering with |C| = 45).
These empirical results confirm that certain mea-
sures favor solutions with many clusters, while oth-
ers prefer fewer clusters. As expected, many-to-1
correlates positively with |C|, rising to almost 85%
with |C| = 200 and reaching 100% when the num-
ber of clusters is maximal (i.e., single). Recall that
crossval was proposed as a possible solution to this
problem, and it does solve the extreme case of sin-
gle, yielding 0% accuracy rather than 100%. How-
ever, it patterns just like many-to-1 for up to 200
clusters, suggesting that there is very little difference
4We used the Stanford Tagger trained on the WSJ corpus:
http://nlp.stanford.edu/software/tagger.shtml.
between the two for any reasonable number of clus-
ters, and we should be wary of using either one when
|C| may vary.
In contrast to these measures are 1-to-1 and vi: for
the most part, they yield worse performance (lower
1-to-1, higher vi) as |C| increases. However, in this
case the trend is not monotonic: there is an initial
improvement in performance before the decrease be-
gins. One might hope that the peak in performance
would occur when the number of clusters is approx-
imately equal to the number of gold standard tags;
however, the best performance for both 1-to-1 and
vi occurs with approximately 25-30 clusters, many
fewer than the gold standard 45.
Next we consider vm and vmb. Interestingly, al-
though vmbwas proposed as a way to correct for the
supposed tendency of vm to increase with increas-
ing |C|, we find that vm is actually more stable than
vmb over different values of |C|. Thus, if the goal
is to compare systems producing different numbers
of clusters (especially important for systems that in-
duce the number of clusters), then vm seems more
appropriate than any of the above measures, which
are more standard in the literature.
Finally, we analyze the behavior of the gold-
standard-independent measure, s-fscore. On the
positive side, this measure assigns scores of 0 to the
579
two extreme cases of all and single, and is relatively
stable across different values of |C| after an initial
increase. It assigns a lower score to the supervised
system than to brown, indicating that words in the
supervised clusters (which are very close to the gold
standard) are actually less substitutable than words
in the unsupervised clusters. This is probably due to
the fact that the gold standard encodes ?pure? syn-
tactic classes, while substitutability also depends on
semantic characteristics (which tend to be picked up
by unsupervised clustering systems as well). An-
other potential problem with this measure is that it
has a very small dynamic range ? while scores as
high as 1 are theoretically possible, in practice they
will never be achieved, and we see that the actual
range of scores observed are all under 20%.
We conclude that there is probably no single eval-
uation measure that is best for all purposes. If a gold
standard is available, then many-to-1 and 1-to-1 are
the most intuitive measures, but should not be used
when |C| is variable, and do not account for differ-
ences in the errors made. While vi has been popular
as an entropy-based alternative to address the latter
problem, its scores are not easy to interpret (being on
a scale of bits) and it still has the problem of incom-
parability across different |C|. Overall, vm seems to
be the best general-purpose measure that combines
an entropy-based score with an intuitive 0-1 scale
and stability over a wide range of |C|.
4 System comparison
Having provided some intuitions about the behav-
ior of different evaluation methods, we move on to
evaluating the various systems presented in Section
2. We first present results for the same WSJ cor-
pus used above. However, because most of the sys-
tems were initially developed on this corpus, and
often evaluated only on it, there is a question of
whether their methods and/or hyperparameters are
overly specific to the domain or to the English lan-
guage. This is a particularly pertinent question since
a primary argument in favor of unsupervised sys-
tems is that they are easier to port to a new language
or domain than supervised systems. To address this
question, we evaluate all the systems as well on the
multilingual Multext East corpus (Erjavec, 2004),
without changing any of the parameter settings. |C|
was set to 45 for all of the experiments reported in
this section. Based on our assessment of evaluation
Figure 2: Performance of the different systems on WSJ,
using three different measures [|C|:45, |T |:45]
system runtime
brown
?
10 min.
clark
?
40 min.
cw
?
10 min.
bhmm
?
4 hrs.
vbhmm
?
10 hrs.
pr
?
10 hrs.*
feat
?
40 hrs.*
Table 2: Runtimes for the different systems on WSJ
[|C|:45]. *pr and feat have multithreading implemen-
tations and ran on 16 cores.
measures above, we report VM scores as the most
reliable measure across different systems and clus-
ter set sizes; to facilitate comparisons with previous
papers, we also report many-to-one and one-to-one
accuracy.
4.1 Results on WSJ
Figure 2 presents results for all seven systems, with
approximate runtimes shown in Table 2. While these
algorithms have not necessarily been optimized for
speed, there is a fairly clear distinction between the
older type-clustering models (brown, clark) and the
graph-based algorithm (cw) on the one hand, and
the newer machine-learning approaches (bhmm,
vbhmm, pr, feat) on the other, with the former be-
ing much faster to run. Despite their faster run-
times and less sophisticated methods, however, these
systems perform surprisingly well in comparison to
the latter group. Even the oldest and perhaps sim-
plest method (brown) outperforms the two BHMMs
and posterior regularization on all measures. Only
580
Figure 3: VM scores for the different systems on English
Multext-East and WSJ-S corpora [|C|:45, |T |:{14,17}]
Figure 4: VM scores for the different systems on the eight
Multext-East corpora [|C|:45, |T |:14]
the very latest approach (feat) rivals clark, show-
ing slightly better performance on two of the three
measures (clark: 71.2, 53.8, 65.5 on many-to-one,
one-to-one, VM; feat: 73.9, 53.3, 67.7). The cw
system returns a total of 568 clusters on this data set,
so the many-to-one and one-to-one measures are not
strictly comparable to the other systems; on VM this
system achieves middling performance.
We note that the two best-performing systems,
clark and feat, are also the only two to use mor-
phological information. Since the clustering algo-
rithms used by brown and clark are quite similar,
the difference in performance between the two can
probably be attributed to the extra information pro-
vided by the morphology. This supports the (unsur-
prising) conclusion that incorporating morphologi-
cal features is generally helpful for POS induction.
4.2 Results on other corpora
We now examine whether either the relative or ab-
solute performance of the different systems holds up
when tested on a variety of different languages. For
these experiments, we used the 1984 portion of the
Multext-East corpus (? 7k sentences), which contains
parallel translations of Orwell?s 1984 in 8 different
languages: Bulgarian[bg], Czech[cs], Estonian[et],
Hungarian[hu], Romanian[ro], Slovene[sl], Ser-
bian[sr] and English[en]. We also included a 7k
sentence version of the WSJ corpus [wsj-s] to help
differentiate effects of corpus size from those of do-
main/language. For the WSJ corpora we experi-
mented with two standardly used tagsets: the orig-
inal PTB 45-tag gold standard and a coarser set of
17 tags previously used by several researchers work-
ing on unsupervised POS tagging (Smith and Eis-
ner, 2005; Goldwater and Griffiths, 2007; Johnson,
2007). For theMultext-East corpus only a coarse 14-
tag tagset was available.5 Finally, to facilitate direct
comparisons of genre while controlling for the size
of both the corpus and the tag set, we also created a
further collapsed 13-tag set for WSJ.6
Figure 3 illustrates the abilities of the different
systems to generalize across different genres of En-
glish text. Comparing the results for the Multext-
East English corpus and the small WSJ corpus with
13 tags (i.e., controlling as much as possible for cor-
pus size and number of gold standard tags), we see
that despite being developed on WSJ, the systems
actually perform better on Multext-East. This is en-
couraging, since it suggests that the methods and
hyperparameters of the algorithms are not strongly
tied to WSJ. It also suggests that Multext-East is in
some sense an easier corpus than WSJ. Indeed, the
distribution of vocabulary items supports this view:
the 100 most frequent words account for 48% of
the WSJ corpus, but 57% of the 1984 novel. It is
also worth pointing out that, although previous re-
searchers have reduced the 45-tag WSJ set to 17 tags
in order to create an easier task for unsupervised
learning (and to decrease training time), reducing
the tag set further to 13 tags actually decreases per-
formance, since some distinctions found by the sys-
5Out of the 14 tags only 11 are shared across all languages.
For details c.f. Appendix B in (Naseem et al, 2009).
6We tried to make the meanings of the tags as similar as
possible between the two corpora; we had to create 13 rather
than 14 WSJ tags for this reason. Our 13-tag set can be found
at http://homepages.inf.ed.ac.uk/s0787820/pos/.
581
tems (e.g., between different types of punctuation)
are collapsed in the gold standard.
Figure 4 gives the results of the different systems
on the various languages.7 Not surprisingly, all the
algorithms perform best on English, often by a wide
margin, suggesting that they are indeed tuned bet-
ter towards English syntax and/or morphology. One
might expect that the two systems with morpho-
logical features (clark and feat) would show less
difference between English and some of the other
languages (all of which have complex morphology)
than the other systems. However, although clark
and feat (along with Brown) are the best perform-
ing systems overall, they don?t show any particular
benefit for the morphologically complex languages.8
One difference between the Multext-East results
and the WSJ results is that on Multext-East, clark
clearly outperforms all the other systems. This is
true for both the English and non-English corpora,
despite the similar performance of clark and feat
on (English) WSJ. This suggests that feat benefits
more from the larger corpus size of WSJ. For the
other languages clarkmay be benefiting from some-
what more general morphological features; feat cur-
rently contains suffix features but no prefix features
(although these could be added).
Overall, our experiments on multiple languages
support our earlier claim that many of the newer
POS induction systems are not as successful as the
older methods. Moreover, these experiments under-
score the importance of testing unsupervised sys-
tems on multiple languages and domains, since both
the absolute and relative performance of systems
may change on different data sets. Ideally, some of
the corpora should be held out as unseen test data
if an effective argument is to be made regarding the
language- or domain-generality of the system.
5 Learning from induced prototypes
We now introduce a final novel method of evaluat-
ing POS induction systems and potentially improv-
ing their performance as well. Our idea is based
7Some results are missing because not all of the corpora
were successfully processed by all of the systems.
8It can be argued that lemmatization would have given a sig-
nificant gain to the performance of the systems in these lan-
guages. Although lemmatization information was included in
the corpus we chose not to use it, maintaining the fully unsu-
pervised nature of this task.
on the prototype-driven learning model of Haghighi
and Klein (2006). This model is unsupervised, but
requires as input a handful of prototypes (canonical
examples) for each word class. The system uses a
log-linear model with features that include the pro-
totype lists as well as morphological features (the
same ones used in feat). Using the most frequent
words in each gold standard class as prototypes, the
authors report 80.5% accuracy (both many-to-one
and one-to-one) on WSJ, considerably higher than
any of the induction systems seen here. This raises
two questions: If we wish to induce prototypes with-
out a tagged corpus or language-specific knowledge,
which induction system will provide the best pro-
totypes (i.e., most similar to the gold standard pro-
totypes)? And, can we use the induced prototypes
as input to the prototype-driven model (h&k) to
achieve better performance than the system the pro-
totypes were extracted from?
To explore these questions, we implemented a
simple heuristic method for inducing prototypes
from the output C of a POS induction system by
selecting a few frequent words in each cluster that
are the most similar to other words in the cluster and
also the most dissimilar to the words in other clus-
ters. For each cluster ci ? C, we retain as candi-
date prototypes the words whose frequency in ci is
at least 90% as high as the word with the highest fre-
quency (in ci). This yields about 20-30 candidates
from each cluster. For each of these, we compute
its average similarity S to the other candidates in its
cluster, and the average dissimilarity D to the candi-
dates in other clusters. Similarity is computed using
the method described by Haghighi and Klein (2006),
which uses SVD on word context vectors and cosine
similarity. Dissimilarity between a pair of words is
computed as one minus the similarity. Finally we
compute the average M = 0.5(S + D), sort the
words by their M scores, and keep as prototypes
the top ten words with M > 0.25 ? maxci(M).
The cutoff threshold results in some clusters having
less than ten prototypes, which is appropriate since
some gold standard categories have very few mem-
bers (e.g., punctuation, determiners).
Using this method, we first tested the various
base+proto systems on the WSJ corpus. Results
in Table 3 show that the brown system produces
the best prototypes. Although not as good as
using prototypes from the gold standard (h&k),
582
system many-to-1 1-to-1 vm
brown 76.1(8.3) 60.7(10.6) 68.8(5.8)
clark 74.5(3.3) 62.1(8.3) 68.6(3.0)
bhmm 71.8(8.6) 56.5(15.0) 65.7(9.5)
vbhmm 68.1(17.9) 67.2(20.7) 67.5(18.3)
pr 71.6(9.2) 60.2(17.0) 67.2(12.4)
feat 69.8(-4.1) 52.0(-1.3) 63.1(-4.6)
h&k 80.2 80.2 75.2
Table 3: Scores on WSJ for our prototype-based POS in-
duction system, with prototypes extracted from each of
the existing systems [|C|:45,|T |:45]. Numbers in paren-
theses are the improvement over the same system without
using the prototype step. Scores in bold indicate the best
performance (improvement) in each column. h&k uses
gold standard prototypes.
corpus brown clark
wsj 68.8(5.8) 68.5(3.0)
wsj-s 62.3(2.7) 67.5(3.6)
en 58.5(1.6) 57.9(-3.3)
bg 53.7(2.3) 50.2(-7.1)
cs 49.9(5.0) 48.0(-4.0)
et 45.8(4.9) 44.4(-1.9)
hu 45.8(0.1) 47.0(-5.7)
ro 53.2(0.8) 52.7(-3.3)
sl 51.2(2.9) 51.7(-4.6)
sr 48.0(2.8) 46.4(-4.9)
Table 4: VM scores for brown+proto and clark+proto
on all corpora. Numbers in parentheses indicate improve-
ment over the base systems.
brown+proto yields a large improvement over
brown, and the highest performance of any system
tested so far. In fact, the brown+proto scores are, to
our knowledge, the best reported results for an un-
supervised POS induction system on WSJ.
Next, we evaluated the two best-performing
+proto systems on Multext-East, as shown in Ta-
ble 4. We see that brown again yields the best
prototypes, and again yields improvements when
used as brown+proto (although the improvements
are not as large as those on WSJ). Interestingly,
clark+proto actually performs worse than clark on
the multilingual data, showing that although induced
prototypes can in principle improve the performance
of a system, not all systems will benefit in all situ-
ations. This suggests a need for additional investi-
gation to determine what properties of an existing
induction system allow it to produce useful proto-
types with the current method and/or to develop a
specialized system specifically targeted towards in-
ducing useful prototypes.
6 Conclusion
In this paper, we have attempted to provide a more
comprehensive review and comparison of evaluation
measures and systems for POS induction than has
been done before. We pointed out that most of the
commonly used evaluation measures are sensitive to
the number of induced clusters, and suggested that
V-measure (which is less sensitive) should be used
as an alternative or in conjunction with the standard
measures. With regard to the systems themselves,
we found that many of the newer approaches actu-
ally perform worse than older methods that are both
simpler and faster. The newer systems have intro-
duced potentially important machine learning tools,
but are not necessarily better suited to the POS in-
duction task specifically.
Since portability is a distinguishing feature for un-
supervised models, we have stressed the importance
of testing the systems on corpora that were not used
in their development, and especially on different lan-
guages. We found that on non-English languages,
Clark?s (2003) system performed best.
Finally, we introduced the idea of evaluating in-
duction systems based on their ability to produce
useful cluster prototypes. We found that the old-
est system (Brown et al, 1992) yielded the best
prototypes, and that using these prototypes gave
state-of-the-art performance on WSJ, as well as im-
provements on nearly all of the non-English corpora.
These promising results suggest a new direction for
future research: improving POS induction by de-
veloping methods targeted towards extracting better
prototypes, rather than focusing on improving clus-
tering of the entire data set.
Acknowledgments
We thank Mark Johnson, Kuzman Ganchev, and
Taylor Berg-Kirkpatrick for providing the imple-
mentations of their models, as well as Stella
Frank, Tom Kwiatkowski, Luke Zettlemoyer and the
anonymous reviewers for their comments and sug-
gestions. This work was supported by an EPSRC
graduate Fellowship, and by ERCAdvanced Fellow-
ship 249520 GRAMPLUS.
583
References
Taylor Berg-Kirkpatrick, Alexandre B. Co?te?, John DeN-
ero, and Dan Klein. 2010. Painless unsupervised
learning with features. In Proceedings of NAACL
2010, pages 582?590, Los Angeles, California, June.
Chris Biemann. 2006. Unsupervised part-of-speech tag-
ging employing efficient graph clustering. In Proceed-
ings of COLING ACL 2006, pages 7?12, Morristown,
NJ, USA.
Peter F. Brown, Vincent J. Della Pietra, Peter V. Desouza,
Jennifer C. Lai, and Robert L. Mercer. 1992. Class-
based n-gram models of natural language. Computa-
tional Linguistics, 18(4):467?479.
Alexander Clark. 2003. Combining distributional and
morphological information for part of speech induc-
tion. In Proceedings of EACL 2003, pages 59?66,
Morristown, NJ, USA.
Tomaz? Erjavec. 2004. MULTEXT-East Version 3:
Multilingual Morphosyntactic Specifications, Lexi-
cons and Corpora. In Fourth International Conference
on Language Resources and Evaluation, LREC?04,
page In print, Paris. ELRA.
Stella Frank, Sharon Goldwater, and Frank Keller. 2009.
Evaluating models of syntactic category acquisition
without using a gold standard. In Proceedings of
CogSci09, July.
Kuzman Ganchev, Joa?o Grac?a, Jennifer Gillenwater, and
Ben Taskar. 2009. Posterior regularization for struc-
tured latent variable models. Technical report, Univer-
sity of Pennsylvania.
Jianfeng Gao and Mark Johnson. 2008. A comparison of
bayesian estimators for unsupervised hidden markov
model pos taggers. In Proceedings of EMNLP 2008,
pages 344?352, Morristown, NJ, USA.
Sharon Goldwater and Tom Griffiths. 2007. A fully
bayesian approach to unsupervised part-of-speech tag-
ging. In Proceedings of ACL 2007, pages 744?751,
Prague, Czech Republic, June.
Joao Graca, Kuzman Ganchev, Ben Taskar, and Fernando
Pereira. 2009. Posterior vs parameter sparsity in latent
variable models. In Y. Bengio, D. Schuurmans, J. Laf-
ferty, C. K. I. Williams, and A. Culotta, editors, Ad-
vances in Neural Information Processing Systems 22,
pages 664?672.
Aria Haghighi and Dan Klein. 2006. Prototype-driven
learning for sequence models. In Proceedings of
NAACL 2006, pages 320?327, Morristown, NJ, USA.
Mark Johnson. 2007. Why doesn?t EM find good HMM
POS-taggers? In Proceedings of EMNLP-CoNLL
2007, pages 296?305, Prague, Czech Republic, June.
M. Marcus, B. Santorini, and M. A. Marcinkiewicz.
1993. Building a large annotated corpus of En-
glish: the Penn Treebank. Computational Linguistics,
19(2):331?330.
Marina Meila?. 2003. Comparing clusterings by the vari-
ation of information. In Learning Theory and Kernel
Machines, pages 173?187.
B. Merialdo. 1994. Tagging English text with a proba-
bilistic model. Computational Linguistics, 20(2):155?
172.
Tahira Naseem, Benjamin Snyder, Jacob Eisenstein, and
Regina Barzilay. 2009. Multilingual part-of-speech
tagging: Two unsupervised approaches. Journal of Ar-
tificial Intelligence Research, 36:341?385.
Sujith Ravi and Kevin Knight. 2009. Minimized mod-
els for unsupervised part-of-speech tagging. In Pro-
ceedings of ACL-IJCNLP 2009, pages 504?512, Sun-
tec, Singapore, August.
Andrew Rosenberg and Julia Hirschberg. 2007. V-
measure: A conditional entropy-based external clus-
ter evaluation measure. In Proceedings of EMNLP-
CoNLL 2007, pages 410?420.
Noah A. Smith and Jason Eisner. 2005. Contrastive esti-
mation: training log-linear models on unlabeled data.
In Proceedings of ACL 2005, pages 354?362, Morris-
town, NJ, USA.
K. Toutanova and M. Johnson. 2007. A Bayesian LDA-
based model for semi-supervised part-of-speech tag-
ging. In Proceedings of NIPS 2007.
Jurgen Van Gael, Andreas Vlachos, and Zoubin Ghahra-
mani. 2009. The infinite HMM for unsupervised PoS
tagging. In Proceedings of EMLNP 2009, pages 678?
687, Singapore, August.
Andreas Vlachos, Anna Korhonen, and Zoubin Ghahra-
mani. 2009. Unsupervised and constrained dirichlet
process mixture models for verb clustering. In Pro-
ceedings of GEMS 2009, pages 74?82, Morristown,
NJ, USA.
584
Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 638?647,
Edinburgh, Scotland, UK, July 27?31, 2011. c?2011 Association for Computational Linguistics
A Bayesian Mixture Model for Part-of-Speech Induction
Using Multiple Features
Christos Christodoulopoulos
School of Informatics
University of Edinburgh
christos.c@ed.ac.uk
Sharon Goldwater
School of Informatics
University of Edinburgh
sgwater@inf.ed.ac.uk
Mark Steedman
School of Informatics
University of Edinburgh
steedman@inf.ed.ac.uk
Abstract
In this paper we present a fully unsupervised
syntactic class induction system formulated as
a Bayesian multinomial mixture model, where
each word type is constrained to belong to a
single class. By using a mixture model rather
than a sequence model (e.g., HMM), we are
able to easily add multiple kinds of features,
including those at both the type level (mor-
phology features) and token level (context and
alignment features, the latter from parallel cor-
pora). Using only context features, our sys-
tem yields results comparable to state-of-the
art, far better than a similar model without the
one-class-per-type constraint. Using the addi-
tional features provides added benefit, and our
final system outperforms the best published
results on most of the 25 corpora tested.
1 Introduction
Research on unsupervised learning for NLP has be-
come widespread recently, with part-of-speech in-
duction, or syntactic class induction, being a partic-
ularly popular task.1 However, despite a recent pro-
liferation of syntactic class induction systems (Bie-
mann, 2006; Goldwater and Griffiths, 2007; John-
son, 2007; Ravi and Knight, 2009; Berg-Kirkpatrick
et al, 2010; Lee et al, 2010), careful compari-
son indicates that very few systems perform better
than some much simpler and quicker methods dating
back ten or even twenty years (Christodoulopoulos
1The task is more commonly referred to as part-of-speech
induction, but we prefer the term syntactic class induction since
the induced classes may not coincide with part-of-speech tags.
et al, 2010). This fact suggests that we should con-
sider which features of the older systems led to their
success, and attempt to combine these features with
some of the machine learning methods introduced
by the more recent systems. We pursue this strat-
egy here, developing a system based on Bayesian
methods where the probabilistic model incorporates
several insights from previous work.
Perhaps the most important property of our model
is that it is type-based, meaning that all tokens of
a given word type are assigned to the same clus-
ter. This property is not strictly true of linguistic
data, but is a good approximation: as Lee et al
(2010) note, assigning each word type to its most
frequent part of speech yields an upper bound ac-
curacy of 93% or more for most languages. Since
this is much better than the performance of cur-
rent unsupervised syntactic class induction systems,
constraining the model in this way seems likely to
improve performance by reducing the number of
parameters in the model and incorporating useful
linguistic knowledge. Both of the older systems
discussed by Christodoulopoulos et al (2010), i.e.,
Clark (2003) and Brown et al (1992), included this
constraint and achieved very good performance rel-
ative to token-based systems. More recently, Lee et
al. (2010) presented a new type-based model, and
also reported very good results.
A second property of our model, which distin-
guishes it from the type-based Bayesian model of
Lee et al (2010), is that the underlying probabilistic
model is a clustering model, (specifically, a multino-
mial mixture model) rather than a sequence model
(HMM). In this sense, our model is more closely re-
638
lated to several non-probabilistic systems that clus-
ter context vectors or lower-dimensional represen-
tations of them (Redington et al, 1998; Schu?tze,
1995; Lamar et al, 2010). Sequence models are
by far the most common method of supervised part-
of-speech tagging, and have also been widely used
for unsupervised part-of-speech tagging both with
and without a dictionary (Smith and Eisner, 2005;
Haghighi and Klein, 2006; Goldwater and Griffiths,
2007; Johnson, 2007; Ravi and Knight, 2009; Lee et
al., 2010). However, systems based on context vec-
tors have also performed well in these latter scenar-
ios (Schu?tze, 1995; Lamar et al, 2010; Toutanova
and Johnson, 2007) and present a viable alternative
to sequence models.
One advantage of using a clustering model rather
than a sequence model is that the features used for
clustering need not be restricted to context words.
Additional types of features can easily be incorpo-
rated into the model and inference procedure using
the same general framework as in the basic model
that uses only context word features. In particu-
lar, we present two extensions to the basic model.
The first uses morphological features, which serve
as cues to syntactic class and seemed to partly ex-
plain the success of two best-performing systems
analysed by Christodoulopoulos et al (2010). The
second extension to our model uses alignment fea-
tures gathered from parallel corpora. Previous work
suggests that using parallel text can improve perfor-
mance on various unsupervised NLP tasks (Naseem
et al, 2009; Snyder and Barzilay, 2008).
We evaluate our model on 25 corpora in 20 lan-
guages that vary substantially in both syntax and
morphology. As in previous work (Lee et al, 2010),
we find that the one-class-per-type restriction boosts
performance considerably over a comparable token-
based model and yields results that are comparable
to state-of-the-art even without the use of morphol-
ogy or alignment features. Including morphology
features yields the best published results on 14 or 15
of our 25 corpora (depending on the measure) and
alignment features can improve results further.
2 Models
Our model is a multinomial mixture model with
Bayesian priors over the mixing weights ? and
? ?
z
? ? f
Z
M
nj
Figure 1: Plate diagram of the basic model with a single
feature per token (the observed variable f ). M , Z, and
nj are the number of word types, syntactic classes z, and
features (= tokens) per word type, respectively.
multinomial class output parameters ?. The model
is defined so that all observations associated with
a single word type are generated from the same
mixing component (syntactic class). In the basic
model, these observations are token-level features;
the morphology model adds type-level features as
well. We begin by describing the simplest version of
our model, where each word token is associated with
a single feature, for example its left context word
(the word that occurs to its left in the corpus). We
then show how to generalise the model to multiple
token-level features and to type-level features.
2.1 Basic model
In the basic model, each word token is represented
by a single feature such as its left context word.
These features are the observed data; the model ex-
plains the data by assuming that it has been gener-
ated from some set of latent syntactic classes. The
ith class is associated with a multinomial parameter
vector ?i that defines the distribution over features
generated from that class, and with a mixing weight
?i that defines the prior probability of that class. ?
and ?i are drawn from symmetric Dirichlet distribu-
tions with parameters ? and ? respectively.
The generative story goes as follows: First, gen-
erate the prior class probabilities ?. Next, for each
639
word type j = 1 . . .M , choose a class assignment zj
from the distribution ?. For each class i = 1 . . . Z,
choose an output distribution over features ?i. Fi-
nally, for each token k = 1 . . . nj of word type j,
generate a feature fjk from ?zj , the distribution as-
sociated with the class that word type j is assigned
to. The model is illustrated graphically in Figure 1
and is defined formally as follows:
? |? ? Dirichlet(?)
zj | ? ? Multinomial(?)
?i |? ? Dirichlet(?)
fjk |?zj ? Multinomial(?zj )
In addition to the variables defined above, we will
use F to refer to the number of different possible
values a feature can take on (so that ? is a Z ? F
matrix). Thus, one way to think of the model is as a
vector-based clustering system, where word type j is
associated with a 1?F vector of feature counts rep-
resenting the features of all nj tokens of j, and these
vectors are clustered into similar classes. The differ-
ence from other vector-based syntactic class induc-
tion systems is in the method of clustering. Here,
we define a Gibbs sampler that samples from the
posterior distribution of the clusters given the ob-
served features; other systems have used various
standard distance-based vector clustering methods.
Some systems also include dimensionality reduction
(Schu?tze, 1995; Lamar et al, 2010) to reduce the
size of the context vectors; we simply use theF most
common words as context features.
2.2 Inference
At inference time we want to sample a syntactic
class assignment z from the posterior of the model.
We use a collapsed Gibbs sampler, integrating out
the parameters ? and ? and sampling from the fol-
lowing distribution:
P (z|f , ?, ?) ? P (z|?)P (f |z, ?). (1)
Rather than sampling the joint class assignment
P (z|f , ?, ?) directly, the sampler iterates over each
word type j, resampling its class assignment zj
given the current assignments z?j of all other word
types. The posterior over zj can be computed as
P (zj | z?j , f , ?, ?)
? P (zj | z?j , ?, ?)P (fj | f?j , z, ?, ?) (2)
where fj are the features associated with word type
j (one feature for each token of j). The first (prior)
factor is easy to compute due to the conjugacy be-
tween the Dirichlet and multinomial distributions,
and is equal to
P (zj = z | z?j , ?) =
nz + ?
n? + Z?
(3)
where nz is the number of types in class z and n?
is the total number of word types in all classes. All
counts in this and the following equations are com-
puted with respect to z?j (e.g., n? = M ? 1).
Computing the second (likelihood) factor is
slightly more complex due to the dependencies be-
tween the different variables in fj that are induced
by integrating out the ? parameters. Consider first a
simple case where word type j occurs exactly twice
in the corpus, so fj contains two features. The prob-
ability of the first feature fj1 is equal to
P (fj1 = f | zj = z, z?j , f?j , ?) =
nf,z + ?
n?,z + F?
(4)
where nf,z is the number of times feature f has been
seen in class z, n?,z is the total number of feature
tokens in the class, and F is the number of different
possible features.
The probability of the second feature fj2 can be
calculated similarly, except that it is conditioned on
fj1 in addition to the other variables, so the counts
for previously observed features must include the
counts due to fj1 as well as those due to f?j . Thus,
the probability is
P (fj2 = f | fj1, zj = z, z?j , f?j , ?)
= nf,z + ?(fj1, fj2) + ?n?,z + 1 + F?
(5)
where ? is the Kronecker delta function, equal to 1
if its arguments are equal and 0 otherwise.
Extending this example to the general case, the
probability of a sequence of features fj is computed
using the chain rule, where the counts used in each
factor are incremented as necessary for each addi-
tional conditioning feature, yielding the following
expression:
P (fj | f?j , zj = z, z?j , ?)
=
?F
k=1
?njk?1
i=0 (njk,z + i + ?)?nj?1
i=0 (n?,z + i + F?)
(6)
640
where njk is the number of instances of feature k in
word type j.2
2.3 Extended models
We can extend the model above in two different
ways: by adding more features at the word token
level, or by adding features at the type level. To add
more token-level features, we simply assume that
each word token generates multiple features, one
feature from each of several different kinds.3 For
example, the left context word might be one kind of
feature and the right context word another. We as-
sume conditional independence between the gener-
ated features given the syntactic class, so each kind
of feature t has its own output parameters ?(t). A
plate diagram of the model with T kinds of features
is shown in Figure 2 (a type-level feature is also in-
cluded in this diagram, as described below).
Due to the independence assumption between the
different kinds of features, the basic Gibbs sampler
is easy to extend to this case by simpling multiplying
in extra factors for the additional kinds of features,
with the prior (Equation 3) unchanged. The likeli-
hood becomes:
P (f (1)j , . . . , f
(T )
j | f
(1...T )
?j , zj = z, z?j , ?)
=
T?
t=1
P (f (t)j | f
(t)
?j , zj = z, z?j , ?) (7)
where each factor in the product is computed using
Equation 6.
In addition to monolingual context features, we
also explore the use of alignment features for those
languages where we have parallel corpora. These
features are extracted for language ? by word-
aligning ? to another language ?? (details of the
alignment procedure are described in Section 3.1).
The features used for each token e in ? are the left
and right context words of the word token that is
aligned to e (if there is one). As with the mono-
lingual context features, we use only the F most fre-
quent words in ?? as possible features.
2One could approximate this likelihood term by assuming
independence between all nj feature tokens of word type j.
This is the approach taken by Lee et al (2010).
3We use the word kind here to avoid confusion with type,
which we reserve for the type-token distinction, which can ap-
ply to features as well as words.
Note that this model with multiple context fea-
tures is deficient: it can generate data that are in-
consistent with any actual corpus, because there is
no mechanism to constrain the left context word
of token ei to be the same as the right context
word of token ei?1 (and similarly with alignment
features). However, deficient models have proven
useful in other unsupervised NLP tasks (Klein and
Manning, 2002; Toutanova and Johnson, 2007). In
particular, Toutanova and Johnson (2007) demon-
strate good performance on unsupervised part-of-
speech tagging (using a dictionary) with a Bayesian
model similar to our own. If we remove the part of
their model that relies on the dictionary (the mor-
phological ambiguity classes), their model is equiv-
alent to our own, without the restriction of one class
per type. We use this token-based version of our
model as a baseline in our experiments.
The final extension to our model introduces type-
level features, specifically morphology features.
The model is illustrated in Figure 2. We assume
conditional independence between the morphology
features and other features, so again we can simply
multiply another factor into the likelihood during in-
ference. There is only one morphological feature per
type, so this factor has the form of Equation 4. Since
frequent words will have many token-level features
contributing to the likelihood and only one morphol-
ogy feature, the morphology features will have a
greater effect for infrequent words (as appropriate,
since there is less evidence from context and align-
ments). As with the other kinds of features, we use
only a limited number Fm of morphology features,
as described below.
3 Experiments
3.1 Experimental setup
We evaluate our models using an increasing level
of complexity, starting with a model that uses only
monolingual context features. We use the F = 100
most frequent words as features, and consider two
versions of this model: one with two kinds of fea-
tures (one left and one right context word) and one
with four (two context words on each side).
For the model with morphology features we ran
the unsupervised morphological segmentation sys-
tem Morfessor (Creutz and Lagus, 2005) to get a
641
??
z
f (1) ?(1) ?(1)
. . . . . . . . .
f (T ) ?(T ) ?(T )m
?(m) ?(m)
M
nj
nj
Z
Z
Z
Figure 2: Plate diagram of the extended model with T
kinds of token-level features (f (t) variables) and a single
kind of type-level feature (morphology, m).
segmentation for each word type in the corpus. We
then extracted the suffix of each word type4 and used
it as a feature type. This process yielded on average
Fm = 110 morphological feature types5. Each word
type generates at most one of these possible features.
If there are overlapping possibilities (e.g. -ingly and
-y) we take the longest possible match.
We also explore the idea of extending the mor-
phology feature space beyond suffixes, by including
features like capitalisation and punctuation. Specif-
ically we use the features described in Haghighi
and Klein (2006), namely initial-capital, contains-
hyphen, contains-digit and we add an extra feature
contains-punctuation.
For the model with alignment features, we fol-
low (Naseem et al, 2009) in using only bidirectional
alignments: using Giza++ (Och and Ney, 2003),
we get the word alignments in both directions be-
tween all possible language pairs in our parallel cor-
pora (i.e., alternating the source and target languages
within each pair). We then use only those align-
ments that are found in both directions. As discussed
4Since Morfessor yields multiple affixes for each word we
concatenated all the suffixes into a single suffix.
5There was large variance in the number of feature types for
each language ranging from 11 in Chinese to more than 350 in
German and Czech.
above, we use two kinds of alignment features: the
left and right context words of the aligned token in
the other language. The feature space is set to the
F = 100 most frequent words in that language.
Instead of fixing the hyperparameters ? and ?, we
used the Metropolis-Hastings sampler presented by
Goldwater and Griffiths (2007) to get updated values
based on the likelihood of the data with respect to
those hyperparameters6. In order to improve conver-
gence of the sampler, we used simulated annealing
with a sigmoid-shaped cooling schedule from an ini-
tial temperature of 2 down to 1. Preliminary experi-
ments indicated that we could achieve better results
by cooling even further (approximating the MAP so-
lution rather than a sample from the posterior), so for
all experiments reported here, we ran the sampler for
a total of 2000 iterations, with the last 400 of these
decreasing the temperature from 1 to 0.66.
Finally, we investigated two different initialisa-
tion techniques: First, we use random class as-
signments to word types (referred to as method 1)
and second, we assign each of the Z most frequent
word types to a separate class and then randomly
distribute the rest of the word types to the classes
(method 2).
3.2 Datasets
Although unsupervised systems should in principle
be language- and corpus-independent, most part-of-
speech induction systems (especially in the early lit-
erature) have been developed on English. Whether
because English is simply an easier language, or be-
cause of bias introduced during development, these
systems? performance is considerably worse in other
languages (Christodoulopoulos et al, 2010)
Since we aim to use our system mostly on non-
English corpora, and ones that are significantly
smaller than the large English treebank corpora, we
developed our models using one of the languages of
the MULTEXT-East corpus (Erjavec, 2004), namely
Bulgarian. The other languages in the corpus were
used during development as a source of word align-
ments, but otherwise were only used for testing final
versions of our models. Since none of the authors
speak any of the languages in the MULTEXT col-
6For simplicity, we tied the ? parameters for the two or four
kinds of context features to the same value, and similarly the ?
parameters for the two kinds of alignment features.
642
lection, we also used the Penn Treebank WSJ cor-
pus (Marcus et al, 1993) for development. Fol-
lowing Christodoulopoulos et al (2010) we created
a smaller version of the WSJ corpus (referred to
as wsj-s) to approximate the size of the corpora in
MULTEXT-East. For comparison to other systems,
we also used the full WSJ at test time.
For further testing, we used the remaining MUL-
TEXT languages, as well as the languages of the
CONNL-X (Buchholz and Marsi, 2006) shared task.
This dataset contains 13 languages, 4 of which
are freely available (Danish, Dutch, Portuguese
and Swedish) and 9 that are used with permission
from the creators of the corpora ( Arabic7, Bul-
garian8, Czech9, German10, Chinese11, Japanese12,
Slovene13, Spanish14, Turkish15 ). Following Lee et
al. (2010) we used only the training sections for each
language.
Finally, to widen the scope of our system, we gen-
erated two more corpora in French16 and Ancient
Greek17, extracting the gold standard parts of speech
from the respective dependency treebanks.
3.3 Baselines
We chose three baselines for comparison. The first
is the basic k-means clustering algorithm, which we
applied to the same feature vectors we extracted for
our system (context + extended morphology), using
a Euclidean distance metric. This provides a very
simple vector-based clustering baseline. The second
baseline is a more recent vector-based syntactic class
induction method, the SVD approach of (Lamar et
al., 2010), which extends Schu?tze (1995)?s original
method and, like ours, enforces a one-class-per-tag
restriction. As a third baseline we use the system of
Clark (2003) since it is a type-level system that mod-
7Part of the Prague Arabic Treebank (Hajic? et al, 2003;
Smrz? and Pajas, 2004)
8Part of the BulTreeBank (Simov et al, 2004).
9Part of the Prague Dep. Treebank (Bo?hmova? et al, 2001)
10Part of the TIGER Treebank (Brants et al, 2002)
11Part of the Sinica Treebank (Keh-Jiann et al, 2003)
12Part of the Tu?bingen Treebank of Spoken Japanese (for-
merly VERMOBIL Treebank - Kawata and Bartels (2000)).
13Part of the Slovene Dep. Treebank (Dz?eroski et al, 2006)
14Part of the Cast3LB Treebank (Civit et al, 2006)
15Part of the METU-Sabanci Treebank (Oflazer et al, 2003).
16French Treebank (Abeille? et al, 2000)
17Greek Dependency Treebank (Bamman et al, 2009)
els morphology and has produced very good results
on multilingual corpora.
4 Results and Analysis
4.1 Development results
Tables 1 and 2 present the results from develop-
ment runs, which were used to decide which fea-
tures to incorporate in the final system. We used V-
Measure (Rosenberg and Hirschberg, 2007) as our
primary evaluation score, but also present many-to-
one matching accuracy (M-1) scores for better com-
parison with previously published results. We chose
V-Measure (VM) as our evaluation score because it
is less sensitive to the number of classes induced by
the model (Christodoulopoulos et al, 2010), allow-
ing us to develop our models without using the num-
ber of classes as a parameter. We fixed the number
of classes in all systems to 45 during development;
note however that the gold standard tag set for Bul-
garian contains only 12 tags, so the results in Ta-
ble 1 (especially the M-1 scores) are not comparable
to previous results. For results using the number of
gold-standard tags refer to Table 4.
The first conclusion that can be drawn from these
results is the large difference between the token-
and type-based versions of our system, which con-
firms that the one-class-per-type restriction is help-
ful for unsupervised syntactic class induction. We
also see that for both languages, the performance of
the model using 4 context words (?2 on each side) is
worse than the 2 context words model. We therefore
used only two context words for all of our additional
test languages (below).
We can clearly see that morphological features
are helpful in both languages; however the extended
features of Haghighi and Klein (2006) seem to help
only on the English data. This could be due to the
fact that Bulgarian has a much richer morphology
and thus the extra features contribute little to the
overall performance of the model.
The contribution of the alignment features on the
Bulgarian corpus (aligned with English) is less sig-
nificant than that of morphology but when com-
bined, the two sets of features yield the best per-
formance. This provides evidence in favor of using
multiple features.
Finally, initialisation method 2 does not yield
643
system ?1 words ?2 words
VM/M-1 VM/M-1
base 58.1 / 70.8 55.4 / 67.6
base(tokens) 48.3 / 62.5 37.0 / 54.4
base(init) 57.6 / 70.1 56.1 / 68.6
+morph 58.3 / 74.9 57.4 / 71.9
+morph(ext) 57.8 / 73.7 57.8 / 70.1
(init)+morph 57.8 / 74.3 57.3 / 69.5
(init)+morph(ext) 58.1 / 74.3 57.2 / 71.3
+aligns(EN) 58.1 / 72.6 56.7 / 71.1
+aligns(EN)+morph 59.0 / 75.4 57.5 / 69.7
Table 1: V-measure (VM) and many-to-one (M-1) results
on the MULTEXT-Bulgarian corpus for various mod-
els using either ?1 or ?2 context words as features.
base: context features only; (tokens): token-based model;
(init): Initialisation method 2?other results use method
1; (ext): Extended morphological features.
system ?1 words ?2 words
VM/M-1 VM/M-1
base 63.3 / 64.3 62.4 / 63.3
base(tokens) 48.6 / 57.8 49.3 / 38.3
base(init) 62.7 / 62.9 62.2 / 62.4
+morph 66.4 / 66.7 65.1 / 67.2
+morph(ext) 67.7 / 72.0 65.6 / 67.0
(init)+morph 64.8 / 66.9 64.2 / 66.0
(init)+morph(ext) 67.4 / 71.3 65.7 / 67.1
Table 2: V-measure and many-to-one results on the wsj-s
corpus for various models, as described in Table 1.
.
consistent improvements over the standard ran-
dom initialisation?if anything, it seems to perform
worse. We therefore use only method 1 in the re-
maining experiments.
4.2 Overall results
Table 3 presents the results on our parallel corpora.
We tested all possible combinations of two lan-
guages to align, and present both the average score
over all alignments, and the score under the best
choice of aligned language.18 Also shown are the
results of adding morphology features to the basic
model (context features only) and to the best align-
ment model for each language. In accord with our
18The choice of language was based on the same test data, so
the ?best-language? results should be viewed as oracle scores.
development results, adding morphology to the ba-
sic model is generally useful. The alignment results
are mixed: on the one hand, choosing the best pos-
sible language to align yields improvements, which
can be improved further by adding morphological
features, resulting in the best scores of all models
for most languages. On the other hand, without
knowing which language to choose, alignment fea-
tures do not help on average. We note, however,
that three out of the seven languages have English
as their best-aligned pair (perhaps due to its better
overall scores), which suggests that in the absence
of other knowledge, aligning with English may be a
good choice.
The low average performance of the alignment
features is disappointing, but there are many pos-
sible variations on our method for extracting these
features that we have not yet tested. For example,
we used only bidirectional alignments in an effort to
improve alignment precision, but these alignments
typically cover less than 40% of tokens. It is pos-
sible that a higher-recall set of alignments could be
more useful.
We turn now to our results on all 25 corpora,
shown in Table 4 along with corpus statistics, base-
line results, and the best published results for each
language (when available). Our system, includ-
ing morphology features in all cases, is listed as
BMMM (Bayesian Multinomial Mixture Model).
We do not include alignment features for the MUL-
TEXT languages since these features only yielded
improvements for the oracle case where we know
which aligned language to choose. Nevertheless, our
MULTEXT scores mostly outperform all other sys-
tems. Overall, we acheive the highest published re-
sults on 14 (VM) or 15 (M-1) of the 25 corpora.
One surprising discovery is the high performance
of the k-means clustering system. Despite its sim-
plicity, it is competitive with the other systems and
in a few cases even achieves the best published re-
sults.
5 Conclusion
We have presented a Bayesian model for syntactic
class induction that has two important properties.
First, it is type-based, assigning the same class to
every token of a word type. We have shown by
644
BASE ALIGNMENTS
Lang. base +morph Avg. Best +morph
VM/M-1 VM/M-1 VM/M-1 VM/M1 VM/M1
Bulgarian 54.4 / 61.5 54.5 / 64.3 53.1 / 60.5 55.2 / 64.5(EN) 55.7 / 66.0
Czech 54.2 / 58.9 53.9 / 64.2 52.6 / 58.4 53.8 / 59.7(EN) 55.4 / 66.4
English 62.9 / 72.4 63.3 / 73.3 62.5 / 72.0 63.2 / 71.9(HU) 63.5 / 73.7
Estonian 52.8 / 63.5 53.3 / 67.4 52.8 / 63.9 53.5 / 65.0(EN) 54.3 / 66.9
Hungarian 53.3 / 60.4 54.8 / 68.2 53.3 / 60.8 53.9 / 61.1(RO) 55.9 / 67.1
Romanian 53.9 / 62.4 52.3 / 61.1 56.2 / 63.7 57.5 / 64.6(ES) 54.5 / 63.4
Slovene 57.2 / 65.9 56.7 / 67.9 54.7 / 64.1 55.9 / 64.4(HU) 56.7 / 67.9
Serbian 49.1 / 56.6 49.0 / 62.0 47.3 / 55.6 48.9 / 59.4(CZ) 48.3 / 60.8
Table 3: V-measure (VM) and many-to-one (M-1) results on the languages in the MULTEXT-East corpus using
the gold standard number of classes shown in Table 4. BASE results use ?1-word context features alone or with
morphology. ALIGNMENTS adds alignment features, reporting the average score across all possible choices of paired
language and the scores under the best performing paired language (in parens), alone or with morphology features.
Language Types Tags k-means SVD2 clark Best Pub. BMMM
WS
J wsj 49,190 45 59.5 / 61.6 58.2 / 64.0 65.6 / 71.2 68.8 / 76.1? 66.1 / 72.8
wsj-s 16,850 45 56.7 / 60.1 54.3 / 60.7 63.8 / 68.8 62.3 / 70.7? 67.7 / 72.0
MU
LT
EX
T-E
ast
Bulgarian 16,352 12 50.3 / 59.3 41.7 / 51.0 55.6 / 66.5 - 54.5 / 64.4
Czech 19,115 12 48.6 / 56.7 35.5 / 50.9 52.6 / 64.1 - 53.9 / 64.2
English 9,773 12 56.5 / 65.4 52.3 / 65.5 60.5 / 70.6 - 63.3 / 73.3
Estonian 17,845 11 45.3 / 55.6 38.7 / 55.3 44.4 / 58.4 - 53.3 / 64.4
Hungarian 20,321 12 46.7 / 53.9 39.8 / 49.5 48.9 / 61.4 - 54.8 / 68.2
Romanian 15,189 14 45.2 / 55.1 42.1 / 52.6 40.9 / 49.9 - 52.3 / 61.1
Slovene 17,871 12 46.9 / 56.2 39.5 / 54.2 54.9 / 69.4 - 56.7 / 67.9
Serbian 18,095 12 41.4 / 47.0 39.1 / 54.6 51.0 / 64.1 - 49.0 / 62.0
Co
NL
L0
6S
har
ed
Ta
sk
Arabic 12,915 20 43.3 / 60.7 27.6 / 49.0 40.6 / 59.8 - 42.4 / 61.5
Bulgarian 32,439 54 53.6 / 65.6 49.0 / 65.3 59.6 / 70.4 - 58.8 / 68.9
Chinese 40,562 15 32.6 / 61.1 24.5 / 54.6 31.8 / 56.7 - 42.6 / 69.4
Czech 130,208 12 - - 47.1 / 65.5 - 48.4 / 65.7
Danish 18,356 25 51.7 / 61.6 40.8 / 57.6 52.7 / 65.3 - / 66.7? 59.0 / 71.1
Dutch 28,393 13 45.3 / 60.5 36.7 / 52.4 52.2 / 67.9 - / 67.3? 54.7 / 71.1
German 72,326 54 58.7 / 67.5 54.1 / 64.2 63.0 / 73.9 - / 68.4? 61.9 / 74.4
Japanese 3,231 80 76.1 / 76.2 74.4 / 75.5 78.6 / 77.4 - 77.4 / 78.5
Portuguese 28,931 22 51.6 / 64.4 45.9 / 63.1 57.4 / 69.2 - / 75.3? 63.9 / 76.8
Slovene 7,128 29 52.6 / 64.2 44.0 / 60.3 53.9 / 63.5 - 49.4 / 56.2
Spanish 16,458 47 59.5 / 69.2 54.8 / 68.2 61.6 / 71.9 - / 73.2? 63.2 / 71.7
Swedish 20,057 41 53.2 / 62.2 47.4 / 59.1 58.9 / 68.7 - / 60.6? 58.0 / 68.2
Turkish 17,563 30 40.8 / 62.8 27.4 / 52.4 36.8 / 58.1 - 40.2 / 58.7
French 49,964 23 48.2 / 68.6 46.3 / 68.5 57.3 / 77.8 - 55.0 / 76.6
A.Greek 15,194 15 38.6 / 44.8 24.2 / 38.5 33.3 / 45.4 - 40.5 / 45.1
Table 4: Final results on 25 corpora in 20 languages, with the number of induced classes equal to the number of gold
standard tags in all cases. k-means and SVD2 models could not produce a clustering in the Czech CoNLL corpus due
its size. Best published results are from ?Christodoulopoulos et al (2010), ?Berg-Kirkpatrick et al (2010) and ?Lee
et al (2010). The latter two papers do not report VM scores. No best published results are shown for the MULTEXT
languages; Christodoulopoulos et al (2010) report results based on 45 tags suggesting that clark performs best on
these corpora.
645
comparison with a token-based version of the model
that this restriction is very helpful. Second, it is
a clustering model rather than a sequence model.
This property makes it easy to incorporate multi-
ple kinds of features into the model at either the to-
ken or the type level. Here, we experimented with
token-level context features and alignment features
and type-level morphology features, showing that
morphology features are helpful in nearly all cases,
and alignment features can be helpful if the aligned
language is properly chosen. Our results even with-
out these extra features are competitive with state-
of-the-art; with the additional features we achieve
the best published results in the majority of the 25
corpora tested.
Since it is so easy to add extra features to our
model, one direction for future work is to explore
other possible features. For example, it could be
useful to add dependency features from an unsuper-
vised dependency parser. We are also interested in
improving our morphology features, either by con-
sidering other ways to extract features during pre-
processing (for example, including prefixes or not
concatenating together all suffixes), or by develop-
ing a joint model for inducing both morphology and
syntactic classes simultaneously. Finally, our model
could be extended by replacing the standard mixture
model with an infinite mixture model (Rasmussen,
2000) in order to induce the number of syntactic
classes automatically.
Acknowledgments
The authors would like to thank Emily Thomforde,
Ioannis Konstas, Tom Kwiatkowski and the anony-
mous reviewers for their comments and suggestions.
We would also like to thank Kiril Simov, Toni Marti,
Tomaz Erjavec, Jess Lin and Kathrin Beck for pro-
viding us with CoNLL data. This work was sup-
ported by an EPSRC graduate Fellowship, and by
ERC Advanced Fellowship 249520 GRAMPLUS.
References
Anne Abeille?, Lionel Cle?ment, and Alexandra Kinyon.
2000. Building a treebank for French. In In Proceed-
ings of the LREC 2000.
David Bamman, Francesco Mambrini, and Gregory
Crane. 2009. An ownership model of annotation: The
Ancient Greek dependency treebank. In TLT 2009-
Eighth International Workshop on Treebanks and Lin-
guistic Theories.
Taylor Berg-Kirkpatrick, Alexandre B. Co?te?, John DeN-
ero, and Dan Klein. 2010. Painless unsupervised
learning with features. In Proceedings of NAACL
2010, pages 582?590, Los Angeles, California, June.
Chris Biemann. 2006. Unsupervised part-of-speech tag-
ging employing efficient graph clustering. In Proceed-
ings of COLING ACL 2006, pages 7?12, Morristown,
NJ, USA.
Alena Bo?hmova?, Jan Hajic?, Eva Hajic?ova?, and Barbora
Hladka?. 2001. The Prague dependency treebank:
Three-level annotation scenario. In Anne Abeille?, ed-
itor, Treebanks: Building and Using Syntactically An-
notated Corpora, pages 103 ? 126. Kluwer Academic
Publishers.
Sabine Brants, Stefanie Dipper, Silvia Hansen, Wolfgang
Lezius, and George Smith. 2002. The TIGER tree-
bank. In Proceedings of the Workshop on Treebanks
and Linguistic Theories, Sozopol.
Peter F. Brown, Vincent J. Della Pietra, Peter V. Desouza,
Jennifer C. Lai, and Robert L. Mercer. 1992. Class-
based n-gram models of natural language. Computa-
tional Linguistics, 18(4):467?479.
Sabine Buchholz and Erwin Marsi. 2006. CoNLL-
X shared task on multilingual dependency parsing.
In Proceedings of the Tenth Conference on Compu-
tational Natural Language Learning, CoNLL-X ?06,
pages 149?164, Stroudsburg, PA, USA. Association
for Computational Linguistics.
Christos Christodoulopoulos, Sharon Goldwater, and
Mark Steedman. 2010. Two decades of unsupervised
POS induction: How far have we come? In Proceed-
ings of the 2010 Conference on Empirical Methods in
Natural Language Processing, pages 575?584, Cam-
bridge, MA, October. Association for Computational
Linguistics.
Montserrat Civit, Ma. Mart??, and Nu?ria Buf??. 2006.
Cat3lb and cast3lb: From constituents to dependen-
cies. In Tapio Salakoski, Filip Ginter, Sampo Pyysalo,
and Tapio Pahikkala, editors, Advances in Natural
Language Processing, volume 4139 of Lecture Notes
in Computer Science, pages 141?152. Springer Berlin
/ Heidelberg.
Alexander Clark. 2003. Combining distributional and
morphological information for part of speech induc-
tion. In Proceedings of EACL 2003, pages 59?66,
Morristown, NJ, USA.
Mathias Creutz and Krista Lagus. 2005. Induc-
ing the morphological lexicon of a natural language
from unannotated text. In In Proceedings of the
International and Interdisciplinary Conference on
646
Adaptive Knowledge Representation and Reasoning
(AKRR?05), volume 5, pages 106?113.
Sas?o Dz?eroski, Tomaz? Erjavec, Nina Ledinek, Petr Pajas,
Zdenek Z?abokrtsky, and Andreja Z?ele. 2006. Towards
a Slovene dependency treebank. In Proceedings Int.
Conf. on Language Resources and Evaluation (LREC).
Tomaz? Erjavec. 2004. MULTEXT-East version 3: Mul-
tilingual morphosyntactic specifications, lexicons and
corpora. In Fourth International Conference on Lan-
guage Resources and Evaluation, (LREC?04), pages
1535 ? 1538, Paris. ELRA.
Sharon Goldwater and Tom Griffiths. 2007. A fully
bayesian approach to unsupervised part-of-speech tag-
ging. In Proceedings of ACL 2007, pages 744?751,
Prague, Czech Republic, June.
Aria Haghighi and Dan Klein. 2006. Prototype-driven
learning for sequence models. In Proceedings of
NAACL 2006, pages 320?327, Morristown, NJ, USA.
Jan Hajic?, Jarmila Panevova?, Zden?ka Ures?ova?, Alevtina
Be?mova?, and Petr Pajas. 2003. PDTVALLEX: cre-
ating a large-coverage valency lexicon for treebank
annotation. In Proceedings of The Second Workshop
on Treebanks and Linguistic Theories, pages 57?68.
Vaxjo University Press.
Mark Johnson. 2007. Why doesn?t EM find good HMM
POS-taggers? In Proceedings of EMNLP-CoNLL
2007, pages 296?305, Prague, Czech Republic, June.
Yasushira Kawata and Julia Bartels. 2000. Stylebook
for the Japanese treebank in VERMOBIL. Technical
report, Universita?t Tu?ubingen.
Chen Keh-Jiann, Chu-Ren Huang, Feng-Yi Chen, Chi-
Ching Luo, Ming-Chung Chang, Chao-Jan Chen, and
Zhao-Ming Gao. 2003. Sinica treebank: Design cri-
teria, representational issues and implementation. In
Anne Abeille?, editor, Treebanks: Building and Us-
ing Syntactically Annotated Corpora, pages 231?248.
Kluwer Academic Publishers.
Dan Klein and Christopher D. Manning. 2002. A gener-
ative constituent-context model for improved grammar
induction. In Proceedings of ACL 40, pages 128?135.
Michael Lamar, Yariv Maron, Mark Johnson, and Elie
Bienenstock. 2010. SVD and clustering for unsuper-
vised POS tagging. In Proceedings of the ACL 2010
Conference Short Papers, pages 215?219, Strouds-
burg, PA, USA. Association for Computational Lin-
guistics.
Yoong Keok Lee, Aria Haghighi, and Regina Barzilay.
2010. Simple type-level unsupervised POS tagging.
In Proceedings of the 2010 Conference on Empirical
Methods in Natural Language Processing, EMNLP
?10, pages 853?861, Stroudsburg, PA, USA. Associ-
ation for Computational Linguistics.
Mitchell Marcus, Beatrice Santorini, and Mary Ann
Marcinkiewicz. 1993. Building a large annotated cor-
pus of English: the Penn Treebank. Computational
Linguistics, 19(2):331?330.
Tahira Naseem, Benjamin Snyder, Jacob Eisenstein, and
Regina Barzilay. 2009. Multilingual Part-of-Speech
tagging: Two unsupervised approaches. Journal of Ar-
tificial Intelligence Research, 36:341?385.
Franz J. Och and Hermann Ney. 2003. A systematic
comparison of various statistical alignment models.
Comput. Linguist., 29(1):19?51.
Kemal Oflazer, Bilge Say, Dilek Z. Hakkani-Tu?r, and
Go?khan Tu?r, 2003. Building A Turkish Treebank,
chapter 1, pages 1?17. Kluwer Academic Publishers.
Carl Rasmussen. 2000. The infinite Gaussian mixture
model. In Advances in Neural Information Processing
Systems 12.
Sujith Ravi and Kevin Knight. 2009. Minimized mod-
els for unsupervised Part-of-Speech tagging. In Pro-
ceedings of ACL-IJCNLP 2009, pages 504?512, Sun-
tec, Singapore, August.
Martin Redington, Nick Chater, and Steven Finch. 1998.
Distributional information: a powerful cue for acquir-
ing syntactic categories. Cognitive Science, 22:425 ?
469.
Andrew Rosenberg and Julia Hirschberg. 2007. V-
measure: A conditional entropy-based external clus-
ter evaluation measure. In Proceedings of EMNLP-
CoNLL 2007, pages 410?420.
Hinrich Schu?tze. 1995. Distributional part-of-speech
tagging. In Proceedings of EACL 7, pages 141?148,
San Francisco, CA, USA.
Kiril Simov, Petya Osenova, Alexander Simov, andMilen
Kouylekov. 2004. Design and implementation of the
Bulgarian HPSG-based treebank. Research on Lan-
guage &amp; Computation, 2(4):495?522.
Noah A. Smith and Jason Eisner. 2005. Contrastive esti-
mation: training log-linear models on unlabeled data.
In Proceedings of ACL 2005, pages 354?362, Morris-
town, NJ, USA.
Otakar Smrz? and Petr Pajas. 2004. Morphotrees of Ara-
bic and their annotation in the TrEd environment. In
Proceedings of the NEMLAR International Conference
on Arabic Language Resources and Tools, pages 38?
41.
Ben Snyder and Regina Barzilay. 2008. Unsupervised
multilingual learning for morphological segmentation.
In Proceedings of ACL.
Kristina Toutanova and Mark Johnson. 2007. A
Bayesian LDA-based model for semi-supervised part-
of-speech tagging. In Proceedings of NIPS 2007.
647
Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 126?134,
Gothenburg, Sweden, April 26-30 2014.
c
?2014 Association for Computational Linguistics
Generalizing a Strongly Lexicalized Parser using Unlabeled Data
Tejaswini Deoskar
1
, Christos Christodoulopoulos
2
, Alexandra Birch
1
, Mark Steedman
1
1
School of Informatics, University of Edinburgh, Edinburgh, EH8 9AB
2
University of Illinois, Urbana-Champaign, Urbana, IL 61801
{tdeoskar,abmayne,steedman}@inf.ed.ac.uk, christod@illinois.edu
Abstract
Statistical parsers trained on labeled data
suffer from sparsity, both grammatical and
lexical. For parsers based on strongly
lexicalized grammar formalisms (such as
CCG, which has complex lexical cate-
gories but simple combinatory rules), the
problem of sparsity can be isolated to
the lexicon. In this paper, we show that
semi-supervised Viterbi-EM can be used
to extend the lexicon of a generative CCG
parser. By learning complex lexical entries
for low-frequency and unseen words from
unlabeled data, we obtain improvements
over our supervised model for both in-
domain (WSJ) and out-of-domain (ques-
tions and Wikipedia) data. Our learnt
lexicons when used with a discriminative
parser such as C&C also significantly im-
prove its performance on unseen words.
1 Introduction
An important open problem in natural language
parsing is to generalize supervised parsers, which
are trained on hand-labeled data, using unlabeled
data. The problem arises because further hand-
labeled data in the amounts necessary to signif-
icantly improve supervised parsers are very un-
likely to be made available. Generalization is also
necessary in order to achieve good performance on
parsing in textual domains other than the domain
of the available labeled data. For example, parsers
trained on Wall Street Journal (WSJ) data suffer a
fall in accuracy on other domains (Gildea, 2001).
In this paper, we use self-training to generalize
the lexicon of a Combinatory Categorial Gram-
mar (CCG) (Steedman, 2000) parser. CCG is a
strongly lexicalized formalism, in which every
word is associated with a syntactic category (sim-
ilar to an elementary syntactic structure) indicat-
ing its subcategorization potential. Lexical en-
tries are fine-grained and expressive, and contain
a large amount of language-specific grammatical
information. For parsers based on strongly lexical-
ized formalisms, the problem of grammar general-
ization can be cast largely as a problem of lexical
extension.
The present paper focuses on learning lexi-
cal categories for words that are unseen or low-
frequency in labeled data, from unlabeled data.
Since lexical categories in a strongly lexicalized
formalism are complex, fine-grained (and far more
numerous than simple part-of-speech tags), they
are relatively sparse in labeled data. Despite per-
forming at state-of-the-art levels, a major source
of error made by CCG parsers is related to unseen
and low-frequency words (Hockenmaier, 2003;
Clark and Curran, 2007; Thomforde and Steed-
man, 2011). The unseen words for which we learn
categories are surprisingly commonplace words of
English; examples are conquered, apprehended,
subdivided, scoring, denotes, hunted, obsessed,
residing, migrated (Wikipedia). Correctly learn-
ing to parse the predicate-argument structures as-
sociated with such words (expressed as lexical cat-
egories in the case of CCG), is important for open-
domain parsing, not only for CCG but indeed for
any parser.
We show that a simple self-training method,
Viterbi-EM (Neal and Hinton, 1998) when used
to enhance the lexicon of a strongly-lexicalized
parser can be an effective strategy for self-training
and domain-adaptation. Our learnt lexicons im-
prove on the lexical category accuracy of two su-
pervised CCG parsers (Hockenmaier (2003) and
the Clark and Curran (2007) parser, C&C) on
within-domain (WSJ) and out-of-domain test sets
(a question corpus and a Wikipedia corpus).
In most prior work, when EM was initialized
based on labeled data, its performance did not im-
prove over the supervised model (Merialdo, 1994;
126
Charniak, 1993). We found that in order for per-
formance to improve, unlabeled data should be
used only for parameters which are not well cov-
ered by the labeled data, while those that are well
covered should remain fixed.
In an additional contribution, we compare two
strategies for treating unseen words (a smoothing-
based, and a part-of-speech back-off method) and
find that a smoothing-based strategy for treat-
ing unseen words is more effective for semi-
supervised learning than part-of-speech back-off.
2 Combinatory Categorial Grammar
Combinatory Categorial Grammar (CCG) (Steed-
man, 2000) is a strongly lexicalized grammar
formalism, in which the lexicon contains all
language-specific grammatical information. The
lexical entry of a word consists of a syntactic cat-
egory which expresses the subcategorization po-
tential of the word, and a semantic interpretation
which defines the compositional semantics (Lewis
and Steedman, 2013). A small number of combi-
natory rules are used to combine constituents, and
it is straightforward to map syntactic categories to
a logical form for semantic interpretation.
For statistical CCG parsers, the lexicon is learnt
from labeled data, and is subject to sparsity due
to the fine-grained nature of the categories. Fig-
ure 1 illustrates this with a simple CCG deriva-
tion. In this sentence, bake is used as a ditransi-
tive verb and is assigned the ditransitive category
S\NP/NP/NP . This category defines the verb syn-
tactically as mapping three NP arguments to a sen-
tence S , and semantically as a ternary relation be-
tween its three arguments, thus providing a com-
plete analysis of the sentence.
[
NNP
John ] [
VBD
baked ] [
NNP
Mary] [
DT
a ] [
NN
cake]
NP S\NP/NP/NP NP NP/N N
> >
S\NP/NP NP
>
S\NP
<
S
?John baked Mary a cake?
Figure 1: Example CCG derivation
For a CCG parser to obtain the correct deriva-
tion above, its lexicon must include the ditransitive
category S\NP/NP/NP for the verb bake. It is not
sufficient to have simply seen the verb in another
context (say a transitive context like ?John baked a
cake?, which is a more common context). This is
in contrast to standard treebank parsers where the
verbal category is simply VBD (past tense verb)
and a ditransitive analysis of the sentence is not
ruled out as a result of the lexical category.
In addition to sparsity related to open-class
words like verbs as in the above example, there are
also missing categories in labeled data for closed-
class words like question words, due to the small
number of questions in the Penn Treebank. In gen-
eral, lexical sparsity for a statistical CCG parser
can be broken down into three types: (i) where a
word is unseen in training data but is present in
test data, (ii) where a word is seen in the train-
ing data but not with the category type required
in the test data (but the category type is seen with
other words) and (iii) where a word bears a cate-
gory type required in the test data but the category
type is completely unseen in the training data.
In this paper, we deal with the first two kinds.
The third kind is more prevalent when the size
of labeled data is comparatively small (although,
even in the case of the English WSJ CCG tree-
bank, there are several attested category types that
are entirely missing from the lexicon, Clark et al.,
2004). We make the assumption here that all cat-
egory types in the language have been seen in the
labeled data. In principle new category types may
be introduced independently without affecting our
semi-supervised process (for instance, manually,
or via a method that predicts new category types
from those seen in labeled data).
3 Related Work
Previous attempts at harnessing unlabeled data to
improve supervised CCG models using methods
like self-training or co-training have been unsat-
isfactory (Steedman et al., 2003, 43-44). Steed-
man et al. (2003) experimented with self-training
a generative CCG parser, and co-training a genera-
tive parser with an HMM-based supertagger. Co-
training (but not self-training) improved the results
of the parser when the seed labeled data was small.
When the seed data was large (the full treebank),
i.e., the supervised baseline was high, co-training
and self-training both failed to improve the parser.
More recently, Honnibal et al. (2009) improved
the performance of the C&C parser on a domain-
adaptation task (adaptation to Wikipedia text) us-
ing self-training. Instead of self-training the pars-
ing model, they re-train the supertagging model,
which in turn affects parsing accuracy. They
obtained an improvement of 1.09% (dependency
127
score) on supertagger accuracy on Wikipedia (al-
though performance on WSJ text dropped) but did
not attempt to re-train the parsing model.
An orthogonal approach for extending a CCG
lexicon using unlabeled data is that of Thomforde
and Steedman (2011), in which a CCG category for
an unknown word is derived from partial parses
of sentences with just that one word unknown.
The method is capable of inducing unseen cate-
gories types (the third kind of sparsity mentioned
in ?2.1), but due to algorithmic and efficiency is-
sues, it did not achieve the broad-coverage needed
for grammar generalisation of a high-end parser. It
is more relevant for low-resource languages which
do not have substantial labeled data and category
type discovery is important.
Some notable positive results for non-CCG
parsers are McClosky et al. (2006) who use a
parser-reranker combination. Koo et al. (2008)
and Suzuki et al. (2009) use unsupervised word-
clusters as features in a dependency parser to get
lexical dependencies. This has some notional sim-
ilarity to categories, since, like categories, clus-
ters are less fine-grained than words but more fine-
grained than POS-tags.
4 Supervised Parser
The CCG parser used in this paper is a re-
implementation of the generative parser of Hock-
enmaier and Steedman (2002) and Hockenmaier
(2003)
1
, except for the treatment of unseen and
low-frequency words.
We use a model (the LexCat model in Hock-
enmaier (2003)) that conditions the generation of
constituents in the parse tree on the lexical cate-
gory of the head word of the constituent, but not on
the head word itself. While fully-lexicalized mod-
els that condition on words (and thus model word-
to-word dependencies) are more accurate than un-
lexicalized ones like the LexCat model, we use
an unlexicalized model
2
for two reasons: first,
1
These generative models are similar to the Collins? head-
based models (Collins, 1997), where for every node, a head is
generated first, and then a sister conditioned on the head. De-
tails of the models are in Hockenmaier and Steedman (2002)
and Hockenmaier 2003:pg 166.
2
A terminological clarification: unlexicalized here refers
to the model, in the sense that head-word information is
not used for rule-expansion. The formalism itself (CCG)
is referred to as strongly-lexicalized, as used in the title of
the paper. Formalisms like CCG and LTAG are consid-
ered strongly-lexicalized since linguistic knowledge (func-
tions mapping words to syntactic structures/semantic inter-
pretations) is included in the lexicon.
our lexicon smoothing procedure (described in the
next section) introduces new words and new cat-
egories for words into the lexicon. Lexical cate-
gories are added to the lexicon for seen and un-
seen words, but no new category types are intro-
duced. Since the LexCat model conditions rule ex-
pansions on lexical categories, but not on words, it
is still able to produce parses for sentences with
new words. In contrast, a fully lexicalized model
would need all components of the grammar to be
smoothed, a task that is far from trivial due to the
resulting explosion in grammar size (and one that
we leave for future work).
Second, although lexicalized models perform
better on in-domain WSJ data (the LexCat model
has an accuracy of 87.9% on Section 23, as op-
posed to 91.03% for the head-lexicalized model
in Hockenmaier (2003) and 91.9% for the C&C
parser), our parser is more accurate on a question
corpus, with a lexical category accuracy of 82.3%,
as opposed to 71.6% and 78.6% for the C&C and
Hockenmaier (2003) respectively.
4.1 Handling rare and unseen words
Existing CCG parsers (Hockenmaier (2003) and
Clark and Curran (2007)) back-off rare and unseen
words to their POS tag. The POS-backoff strategy
is essentially a pipeline approach, where words
are first tagged with coarse tags (POS tags) and
finer tags (CCG categories) are later assigned, by
the parser (Hockenmaier, 2003) or the supertag-
ger (Clark and Curran, 2007). As POS-taggers
are much more accurate than parsers, this strat-
egy has given good performance in general for
CCG parsers, but it has the disadvantage that POS-
tagging errors are propagated. The parser can
never recover from a tagging error, a problem that
is serious for words in the Zipfian tail, where these
words might also be unseen for the POS tagger
and hence more likely to be tagged incorrectly.
This issue is in fact more generally relevant than
for CCG parsers alone?the dependence of parsers
on POS-taggers was cited as one of the problems
in domain-adaptation of parsers in the NAACL-
2012 shared task on parsing the web (Petrov and
McDonald, 2012). Lease and Charniak (2005)
obtained an improvement in the accuracy of the
Charniak (2000) parser on a biomedical domain
simply by training a new POS tagger model.
In the following section, we describe an alter-
native smoothing-based approach to handling un-
128
seen and rare words. This method is less sen-
sitive to POS tagging errors, as described below.
In this approach, in a pre-processing step prior
to parsing, categories are introduced into the lex-
icon for unseen and rare words from the data to
be parsed. Some probability mass is taken from
seen words/categories and given to unseen word
and category pairs. Thus, at parse time, no word is
unseen for the parser.
4.1.1 Smoothing
In our approach, we introduce lexical entries for
words from the unlabeled corpus that are unseen
in the labeled data, and also add categories to ex-
isting entries for rarely seen words. The most gen-
eral case of this would be to assign all known cat-
egories to a word. However, doing this reduces
the lexical category accuracy.
3
A second option,
chosen here, is to limit the number of categories
assigned to the word by using some information
about the word (for instance, its part-of-speech).
Based on the part-of-speech of an unseen word in
the unlabeled or test corpus, we add an entry to the
lexicon of the word with the top n categories that
have been seen with that part-of-speech in the la-
beled data. Each new entry of (w, cat), where w
is a word and cat is a CCG category, is associated
with a count c(w, cat), obtained as described be-
low. Once all (w, cat) entries are added to the lex-
icon along with their counts, a probability model
P (w|cat) is calculated over the entire lexicon.
Our smoothing method is based on a method
used in Deoskar (2008) for smoothing a PCFG
lexicon. Eq. 1 and 2 apply it to CCG entries for
unseen and rare words. In the first step, an out-
of-the-box POS tagger is used to tag the unlabeled
or test corpus (we use the C&C tagger). Counts
of words and POS-tags c
corpus
(w, T ) are obtained
from the tagged corpus. For the CCG lexicon, we
ultimately need a count for a word w and a CCG
category cat. To get this count, we split the count
of a word and POS-tag amongst all categories seen
with that tag in the supervised data in the same
ratio as the ratio of the categories in the super-
vised data. In Eq. 1, this ratio is c
tb
(cat
T
)/c
tb
(T )
where c
tb
(cat
T
) is the treebank count of a cate-
gory cat
T
seen with a POS-tag T , and c
tb
(T ) is the
marginal count of the tag T in the treebank. This
3
For instance, we find that assigning all categories to un-
seen verbs gives a lexical category accuracy of 52.25 %, as
opposed to an accuracy of 65.4% by using top 15 categories,
which gave us the best results, as reported later in Table 3.
ratio makes a more frequent category type more
likely than a rarer one for an unseen word. For ex-
ample, for unseen verbs, it would make the transi-
tive category more likely than a ditransitive one
(since transitives are more frequent than ditran-
sitives). There is an underlying assumption here
that relative frequencies of categories and POS-
tags in the labeled data are maintained in the un-
labeled data, which in fact can be thought of as
a prior while estimating from unlabeled data (De-
oskar et al., 2012).
c
corpus
(w, cat) =
c
tb
(cat
T
)
c
tb
(T )
? c
corpus
(w, T ) (1)
Additionally, for seen but low-frequency words,
we make use of the existing entry in the lexicon.
Thus in a second step, we interpolate the count
c
corpus
(w, cat) of a word and category with the
supervised count of the same c
tb
(w, cat) (if it ex-
ists) to give the final smoothed count of a word and
category c
smooth
(w, cat) (Eq. 2).
c
smooth
(w, cat) = ? ? c
tb
(w, cat) +
(1? ?) ? c
corpus
(w, cat)
(2)
When this smoothed lexicon is used with a
parser, POS-backoff is not necessary since all
needed words are now in the lexicon. Lexical en-
tries for words in the parse are determined not by
the POS-tag from a tagger, but directly by the pars-
ing model, thus making the parse less susceptible
to tagging errors.
5 Semi-supervised Learning
We use Viterbi-EM (Neal and Hinton, 1998) as
the self-training method. Viterbi-EM is an alter-
native to EM where instead of using the model
parameters to find a true posterior from unlabeled
data, a posterior based on the single maximum-
probability (Viterbi) parse is used. Viterbi-EM
has been used in various NLP tasks before and
often performs better than classic EM (Cohen
and Smith, 2010; Goldwater and Johnson, 2005;
Spitkovsky et al., 2010). In practice, a given pars-
ing model is used to obtain Viterbi parses of un-
labeled sentences. The Viterbi parses are then
treated as training data for a new model. This pro-
cess is iterated until convergence.
Since we are interested in learning the lexi-
con, we only consider lexical counts from Viterbi
parses of the unlabeled sentences. Other parame-
ters of the model are held at their supervised val-
ues. We conducted some experiments where we
129
self-trained all components of the parsing model,
which is the usual case of self-training. We ob-
tained negative results similar to Steedman et al.
(2003), where self-training reduced the perfor-
mance of the parsing model. We do not report
them here. Thus, using unlabeled data only to es-
timate parameters that are badly estimated from
labeled data (lexical entries in CCG, due to lexi-
cal sparsity) results in improvements, in contrast
to prior work with semi-supervised EM.
As is common in semi-supervised settings, we
treated the count of each lexical event as the
weighted count of that event in the labeled data
(treebank)
4
and the count from the Viterbi-parses
of unlabeled data. Here we follow Bacchiani et al.
(2006) and McClosky et al. (2006) who show that
count merging is more effective than model inter-
polation.
We placed an additional constraint on the con-
tribution that the unlabeled data makes to the semi-
supervised model?we only use counts (from un-
labeled data) of lexical events that are rarely
seen/unseen in the labeled data. Our reasoning
was that many lexical entries are estimated accu-
rately from the treebank (for example, those re-
lated to function words and other high-frequency
words) and estimation from unlabeled data might
hurt them. We thus had a cut-off frequency (of
words in labeled data) above which we did not
allow the unlabeled counts to affect the semi-
supervised model. In practise, our experiments
turned out to be fairly insensitive to the value of
this parameter, on evaluations over rare or un-
seen verbs. However, overall accuracy would drop
slightly if this cut-off was increased. We experi-
mented with cut-offs of 5, 10 and 15, and found
that the most conservative value (of 5) gave the
best results on in-domain WSJ experiments, and a
higher value of 10 gave the best results for out-of-
domain experiments.
We also conducted some limited experiments
with classical semi-supervised EM, with similar
settings of weighting labeled counts, and using un-
labeled counts only for rare/unseen events. Since
it is a much more computationally expensive pro-
cedure, and most of the results did not come close
to the results of Viterbi-EM, we did not pursue it.
4
The labeled count is weighted in order to scale up the la-
beled data which is usually smaller in size than the unlabeled
data, to avoid swamping the labeled counts with much larger
unlabeled counts.
5.1 Data
Labeled: Sec. 02-21 of CCGbank (Hockenmaier
and Steedman, 2007). In one experiment, we used
Sec. 02-21 minus 1575 sentences that were held
out to simulate test data containing unseen verbs?
see ?6.2 for details.
Unlabeled: For in-domain experiments, we used
sentences from the unlabeled WSJ portion of the
ACL/DCI corpus (LDC93T1, 1993), and the WSJ
portion of the ANC corpus (Reppen et al., 2005),
limited to sentences containing 20 words or less,
creating datasets of approximately 10, 20 and 40
million words each. Additionally, we have a
dataset of 140 million words ? 40M WSJ words
plus an additional 100M from the New York
Times.
For domain-adaptation experiments, we use
two different datasets. The first one consists
of question-sentences ? 1328 unlabeled ques-
tions, obtained by removing the manual annota-
tion of the question corpus from Rimell and Clark
(2008). The second out-of-domain dataset con-
sists of Wikipedia data, approximately 40 million
words in size, with sentence length < 20 words.
5.2 Experimental setup
We ran our semi-supervised method using our
parser with a smoothed lexicon (from ?4.1.1) as
the initial model, on unlabeled data of different
sizes/domains. For comparison, we also ran ex-
periments using a POS-backed off parser (the orig-
inal Hockenmaier and Steedman (2002) LexCat
model) as the initial model. Viterbi-EM converged
at 4-5 iterations. We then parsed various test sets
using the semi-supervised lexicons thus obtained.
In all experiments, the labeled data was scaled to
match the size of the unlabeled data. Thus, the
scaling factor of labeled data was 10 for unlabeled
data of 10M words, 20 for 20M words, etc.
5.3 Evaluation
We focused our evaluations on unseen and low-
frequency verbs, since verbs are the most impor-
tant open-class lexical entries and the most am-
biguous to learn from unlabeled data (approx. 600
categories, versus 150 for nouns). We report lexi-
cal category accuracy in parses produced using our
semi-supervised lexicon, since it is a direct mea-
sure of the effect of the lexicon.
5
We discuss four
5
Dependency recovery accuracy is also used to evaluate
performance of CCG parsers and is correlated with lexical
130
All words All Verbs Unseen
Verbs
SUP 87.76 78.10 52.54
SEMISUP 88.14 78.46 **57.28
SUP
bkoff
87.91 76.08 54.14
SEMISUP
bkoff
87.79 75.68 54.60
Table 1: Lexical category accuracy on TEST-4SEC
**: p < 0.004, McNemar test
experiments below. The first two are on in-domain
(WSJ) data. The last two are on out-of-domain
data ? a question corpus and a Wikipedia corpus.
6 Results
6.1 In-domain: WSJ unseen verbs
Our first testset consists of a concatenation of 4
sections of CCGbank (01, 22, 24, 23), a total of
7417 sentences, to form a testset called TEST-
4SEC. We use all these sections in order to get
a reasonable token count of unseen verbs, which
was not possible with Sec. 23 alone.
Table 1 shows the performance of the smoothed
supervised model (SUP) and the semi-supervised
model (SEMISUP) on this testset. There is a sig-
nificant improvement in performance on unseen
verbs, showing that the semi-supervised model
learns good entries for unseen verbs over and
above the smoothed entry in the supervised lexi-
con. This results in an improvement in the over-
all lexical category accuracy of the parser on all
words, and all verbs.
We also performed semi-supervised training us-
ing a supervised model that treated unseen words
with a POS-backoff strategy SUP
bkoff
. We used
the same settings of cut-off and the same scal-
ing of labeled counts as before. The supervised
backed-off model performs somewhat better than
the supervised smoothed model. However, it did
not improve as much as the smoothed one from
unlabeled data. Additionally, the overall accuracy
of SEMISUP
bkoff
fell below the supervised level,
in contrast to the smoothed model, where overall
numbers improved. This could indicate that the
accuracy of a POS tagger on unseen words, es-
pecially verbs, may be an important bottleneck in
semi-supervised learning.
Low-frequency verbs We also obtain improve-
ments on verbs that are seen but with a low fre-
quency in the labeled data (Table 2). We divided
category accuracy, but a dependency evaluation is more rele-
vant when comparing performance with parsers in other for-
malisms and does not have much utility here.
Freq. Bin 1-5 6-10 11-20
SUP 64.13 75.19 77.6
SEMISUP 66.72 76.21 79.8
Table 2: Seen but rare verbs, TEST-4SEC
verbs occurring in TEST-4SEC into different bins
according to their occurrence frequency in the la-
beled data (bins of frequency 1-5, 6-10 and 11-20).
Semi-supervised training improves over the super-
vised baseline for all bins of low-frequency verbs.
Note that our cut-off frequency for using unlabeled
data is 5, but there are improvements in the 6-10
and 11-20 bins as well, suggesting that learning
better categories for rare words (below the cut-off)
impacts the accuracy of words above the cut-off as
well, by affecting the rest of the parse positively.
6.2 In-domain : heldout unseen verbs
The previous section showed significant improve-
ment in learning categories for verbs that are un-
seen in the training sections of CCGbank. How-
ever, these verbs are in the Zipfian tail, and for this
reason have fairly low occurrence frequencies in
the unlabeled corpus. In order to estimate whether
our method will give further improvements in the
lexical categories for these verbs, we would need
unlabeled data of a much larger size. We there-
fore designed an experimental scenario in which
we would be able to get high counts of unseen
verbs from a similar size of unlabeled data. We
first made a list of N verbs from the treebank and
then extracted all sentences containing them (ei-
ther as verbs or otherwise) from CCGbank training
sections. These sentences form a testset of 1575
sentences, called TEST-HOV (for held out verbs).
The verbs in the list were chosen based on occur-
rence frequency f in the treebank, choosing all
verbs that occurred with a frequency of f = 11.
This number gave us a large enough set and a
good type/token ratio to reliably evaluate and ana-
lyze our semi-supervised models?112 verb types,
with 1115 token occurrences
6
. Since these verbs
are actually mid-frequency verbs in the supervised
data, they have a correspondingly large occurrence
frequency in the unlabeled data, occurring much
more often than true unseen verbs. Thus, the un-
labeled data size is effectively magnified?as far
as these verbs are concerned, the unlabeled data is
approximately 11 times larger than it actually is.
Table 3 shows lexical category accuracy on
6
Selecting a different but close value of f such as f = 10
or f = 12 would have also served this purpose.
131
All Words All Verbs Unseen
Verbs
SUP 87.26 74.55 65.49
SEMISUP 87.78 75.30 *** 70.43
SUP
bkoff
87.58 73.06 67.25
SEMISUP
bkoff
87.52 72.89 68.05
Table 3: Lexical category accuracy in TEST-HOV.
***p<0.0001, McNemar test
55
60
65
70
0 10 20 40 140
Size of Unlabelled Data (in millions of words)
Lexic
al Ca
tegor
y Acc
uracy
 for U
nsee
n Ver
bs
Test:HOVTest:4Sec
Figure 2: Increasing accuracy on unseen verbs
with increasing amounts of unlabeled data.
this testset. The baseline accuracy of the parser
on these verbs is much higher than that on the
truly unseen verbs.
7
The semi-supervised model
(SEMISUP) improves over the supervised model
SUP very significantly on these unseen verbs. We
also see an overall improvement on all verbs (seen
and unseen) in the test data, and in the over-
all lexical category accuracy as well. Again, the
backed-off model does not improve as much as
the smoothed model, and moreover, overall per-
formance falls below the supervised level.
Figure 2 shows the effect of different sizes of
unlabeled data on accuracy of unseen verbs for
the two testsets TEST-HOV and TEST-4SEC . Im-
provements are monotonic with increasing unla-
beled data sizes, up to 40M words. The additional
100M words of NYT also improve the models but
to a lesser degree, possibly due to the difference in
domain. The graphs indicate that the method will
lead to more improvements as more unlabeled data
(especially WSJ data) is added.
7
This could be because verbs in the Zipfian tail have more
idiosyncratic subcategorization patterns than mid-frequency
verbs, and thus are harder for a parser. Another reason is that
they may have been seen as nouns or other parts of speech,
leading to greater ambiguity in their case.
QUESTIONS WIKIPEDIA
All wh All Unseen
words words words words
SUP 82.36 61.77 84.31 79.5
SEMISUP *83.21 63.22 *85.6 80.25
Table 4: Out-of-domain: Questions and
Wikipedia, *p<0.05, McNemar test
6.2.1 Out-of-Domain
Questions The question corpus is not strictly a
different domain (since questions form a differ-
ent kind of construction rather than a different do-
main), but it is an interesting case of adaptation
for several reasons: WSJ parsers perform poorly
on questions due to the small number of questions
in the Penn Treebank/CCGbank. Secondly, unsu-
pervised adaptation to questions has not been at-
tempted before for CCG (Rimell and Clark (2008)
did supervised adaptation of their supertagger).
The supervised model SUP already performs
at state-of-the-art on this corpus, on both overall
scores and on wh(question)-words alone. C&C
and Hockenmaier (2003) get 71.6 and 78.6% over-
all accuracies respectively, and only 33.6 and 50.7
on wh-words alone. To our original unlabeled
WSJ data (40M words), we add 1328 unlabeled
question-sentences from Rimell and Clark, 2008,
scaled by ten, so that each is counted ten times. We
then evaluated on a testset containing questions
(500 question sentences, from Rimell and Clark
(2008)). The overall lexical category accuracy on
this testset improves significantly as a result of the
semi-supervised learning (Table 4). The accuracy
on the question words alone (who, what, where,
when, which, how, whose, whom) also improves
numerically, but by a small amount (the number
of tokens that improve are only 7). This could be
an effect of the small size of the testset (500 sen-
tences, i.e. 500 wh-words).
Wikipedia We obtain statistically significant im-
provements in overall scores over a testset consist-
ing of Wikipedia sentences hand-annotated with
CCG categories (from Honnibal et al. (2009)) (Ta-
ble 4). We also obtained improvements in lexical
category accuracy on unseen words, and on un-
seen verbs alone (not shown), but could not prove
significance. This testset contains only 200 sen-
tences, and counts for unseen words are too small
for significance tests, although there are numeric
improvements. However, the overall improvement
is statistically significantly, showing that adapting
the lexicon alone is effective for a new domain.
132
6.3 Using semi-supervised lexicons with the
C&C parser
To show that the learnt lexical entries may be use-
ful to parsers other than our own, we incorpo-
rate our semi-supervised lexical entries into the
C&C parser to see if it benefits performance. We
do this in a naive manner, as a proof of concept,
making no attempt to optimize the performance
of the C&C parser (since we do not have access
to its internal workings). We take all entries of
unseen words from our best semi-supervised lex-
icon (word, category and count) and add them to
the dictionary of the C&C supertagger (tagdict).
The C&C is a discriminative, lexicalized model
that is more accurate than an unlexicalized model.
Even so, the lexical entries that we learn improve
the C&C parsers performance over and above its
back-off strategy for unseen words. Table 5 shows
the results on WSJ data TEST-4SEC and TEST-
HOV. There were numeric improvements on the
TEST-4SEC test set as shown in Table 5
8
. We ob-
tain significance on the TEST-HOV testset which
has a larger number of tokens of unseen verbs and
entries that were learnt from effectively larger un-
labeled data. We tested two cases: when these
verbs were seen for the POS tagger used to tag
the test data, and when they were unseen for the
POS tagger, and found statistically significant im-
provement for the case when the verbs were un-
seen for the POS tagger
9
, indicating sensitivity to
POS-tagger errors.
6.4 Entropy and KL-divergence
We also evaluated the quality of the semi-
supervised lexical entries by measuring the over-
all entropy and the average Kullback-Leibler (KL)
divergence of the learnt entries of unseen verbs
from entries in the gold testset. The gold entry
for each verb from the TEST-HOV testset was ob-
tained from the heldout gold treebank trees. Su-
pervised (smoothed) and semi-supervised entries
were obtained from the respective lexicons. These
metrics use the conditional probability of a cate-
gory given a word, which is not a factor in the
generative model (which considers probabilities of
8
There were also improvements on the question and
Wikipedia testsets (not shown) (8 and 6 tokens each) but the
size of these testsets is too small for significance.
9
Note that for this testset TEST-HOV, the numbers are the
supertagger?s accuracy, and not the parser?s. We were only
able to retrain the supertagger on training data with TEST-
HOV sentences heldout, but could not retrain the parser, de-
spite consultation with the authors.
TEST-4SEC TEST-HOV
POS-seen POS-unseen
(590) (1134) (1134)
C&C 62.03 (366) 76.71 (870) 72.39 (821)
C&C
(enhanced) 63.89 (377) 77.34 (877) *73.98 (839)
Table 5: TEST-4SEC: Lexical category accuracy of
C&C parser on unseen verbs. Numbers in brackets
are the number of tokens.*p<0.05, McNemar test
words given categories), but provide a good mea-
sure of how close the learnt lexicons are to the gold
lexicon. We find that the average KL divergence
reduces from 2.17 for the baseline supervised en-
tries to 1.40 for the semi-supervised entries. The
overall entropy for unseen verb distributions also
goes down from 2.23 (supervised) to 1.37 (semi-
supervised), showing that semi-supervised distri-
butions are more peaked, and bringing them closer
to the true entropy of the gold distribution (0.93).
7 Conclusions
We have shown that it is possible to learn CCG lex-
ical entries for unseen and low-frequency words
from unlabeled data. When restricted to learning
only lexical entries, Viterbi-EM improved the per-
formance of the supervised parser (both in-domain
and out-of-domain). Updating all parameters of
the parsing model resulted in a decrease in the ac-
curacy of the parser. We showed that the entries
we learnt with an unlexicalized model were accu-
rate enough to also be useful to a highly-accurate
lexicalized parser. It is likely that a lexicalized
parser will provide even better lexical entries. The
lexical entries continued to improve with increas-
ing size of unlabeled data. For the out-of-domain
testsets, we obtained statistically significant over-
all improvements, but we were hampered by the
small sizes of the testsets in evaluating unseen/wh
words.
In future work, we would like to add unseen but
predicted category types to the initial lexicon using
an independent method, and then apply the same
semi-supervised learning to words of these types.
Acknowledgements
We thank Mike Lewis, Shay Cohen and the three
anonymous EACL reviewers for helpful com-
ments. This work was supported by the ERC Ad-
vanced Fellowship 249520 GRAMPLUS.
133
References
Michiel Bacchiani, Michael Riley, Brian Roark, and Richard
Sproat. 2006. MAP adaptation of stochastic grammars.
Computer Speech and Language, 20(1):41?68.
Eugene Charniak. 1993. Statistical Language Learning. MIT
Press.
Stephen Clark and James R. Curran. 2007. Wide-Coverage
Efficient Statistical Parsing with CCG and Log-Linear
Models. Computational Linguistics, 33(4):493?552.
Stephen Clark, Mark Steedman, and James Curran. 2004.
Object-extraction and question-parsing using CCG. In
Proceedings of EMNLP 2004.
Shay Cohen and Noah Smith. 2010. Viterbi Training for
PCFGs: Hardness Results and Competitiveness of Uni-
form Initialization. In Proceedings of ACL 2010.
Michael Collins. 1997. Three generative, lexicalised models
for statistical parsing. In Proceedings of the 35th ACL.
Tejaswini Deoskar. 2008. Re-estimation of Lexical Param-
eters for Treebank PCFGs. In Proceedings of COLING
2008.
Tejaswini Deoskar, Markos Mylonakis, and Khalil Sima?an.
2012. Learning Structural Dependencies of Words in the
Zipfian Tail. Journal of Logic and Computation.
Daniel Gildea. 2001. Corpus Variation and Parser Perfor-
mance. In Proceedings of EMNLP 2001.
Sharon Goldwater and Mark Johnson. 2005. Bias in learning
syllable structure. In Proceedings of CoNLL05.
Julia Hockenmaier. 2003. Data and Models for Statistical
Parsing with Combinatory Categorial Grammar. Ph.D.
thesis, School of Informatics, University of Edinburgh.
Julia Hockenmaier and Mark Steedman. 2002. Generative
Models for Statistical Parsing with Combinatory Catego-
rial Grammar. In ACL40.
Julia Hockenmaier and Mark Steedman. 2007. CCGbank: A
Corpus of CCG Derivations and Dependency Structures
Extracted from the Penn Treebank. Computational Lin-
guistics, 33:355?396.
Matthew Honnibal, Joel Nothman, and James R. Curran.
2009. Evaluating a Statistial CCG Parser on Wikipedia.
In Proceedings of the 2009 Workshop on the People?s Web
Meets NLP, ACL-IJCNLP.
Terry Koo, Xavier Carreras, and Michael Collins. 2008. Sim-
ple Semi-supervised Dependency Parsing. In Proceedings
of ACL-08: HLT , pages 595?603. Association for Com-
putational Linguistics, Columbus, Ohio.
LDC93T1. 1993. LDC93T1. Linguistic Data Consortium,
Philadelphia.
Matthew Lease and Eugene Charniak. 2005. Parsing Biomed-
ical Literature. In R. Dale, K.-F. Wong, J. Su, and
O. Kwong, eds., Proceedings of the 2nd International
Joint Conference on Natural Language Processing (IJC-
NLP?05), vol. 3651 of Lecture Notes in Computer Science,
pages 58 ? 69. Springer-Verlag, Jeju Island, Korea.
Mike Lewis and Mark Steedman. 2013. Combined Distribu-
tional and Logical Semantics. Transactions of the Associ-
ation for Computational Linguistics.
David McClosky, Eugene Charniak, and Mark Johnson.
2006. Effective Self-Training for Parsing. In Proceedings
of HLT-NAACL 2006.
Bernard Merialdo. 1994. Tagging English Text with a Prob-
abilistic Model. Computational Linguistics, 20(2):155?
171.
Radford M. Neal and Geoffrey E. Hinton. 1998. A view of
the EM algorithm that justifies incremental, sparse, and
other variants. In Learning and Graphical Models, pages
355 ? 368. Kluwer Academic Publishers.
Slav Petrov and Ryan McDonald. 2012. Overview of the
2012 Shared Task on Parsing the Web. In First Work-
shop on Syntactic Analysis of Non-Canonical Language
(SANCL) Workshop at NAACL 2012.
Randi Reppen, Nancy Ide, and Keith Suderman. 2005.
LDC2005T35, American National Corpus (ANC) Second
Release. Linguistic Data Consortium, Philadelphia.
Laura Rimell and Stephen Clark. 2008. Adapting a
Lexicalized-Grammar Parser to Contrasting Domains. In
Proceedings of the Conference on Empirical Methods in
Natural Language Processing (EMNLP-08).
Valentin I. Spitkovsky, Hiyan Alshawi, Daniel Jurafsky, and
Christopher D. Manning. 2010. Viterbi Training Improves
Unsupervised Dependency Parsing. In Proceedings of
CoNLL-2010.
Mark Steedman. 2000. The Syntactic Process. MIT
Press/Bradford Books.
Mark Steedman, Steven Baker, Jeremiah Crim, Stephen
Clark, Julia Hockenmaier, Rebecca Hwa, Miles Osbornn,
Paul Ruhlen, and Anoop Sarkar. 2003. Semi-Supervised
Training for Statistical Parsing. Tech. rep., CLSP WS-02.
Jun Suzuki, Hideki Isozaki, Xavier Carreras, and Michael
Collins. 2009. An Empirical Study of Semi-supervised
Structured Conditional Models for Dependency Parsing.
In Proceedings of the 2009 Conference on Empirical
Methods in Natural Language Processing, pages 551?
560. Association for Computational Linguistics, Singa-
pore.
Emily Thomforde and Mark Steedman. 2011. Semi-
supervised CCG Lexicon Extension. In Proceedings of the
Conference on Empirical Methods in Natural Language
Processing, Edinburgh UK.
134
NAACL-HLT Workshop on the Induction of Linguistic Structure, pages 96?99,
Montre?al, Canada, June 3-8, 2012. c?2012 Association for Computational Linguistics
Turning the pipeline into a loop: Iterated unsupervised dependency parsing
and PoS induction
Christos Christodoulopoulos?, Sharon Goldwater?, Mark Steedman?
School of Informatics
University of Edinburgh
?christos.c@ed.ac.uk ?{steedman,sgwater}@inf.ed.ac.uk
1 Motivation
Most unsupervised dependency systems rely on
gold-standard Part-of-Speech (PoS) tags, either di-
rectly, using the PoS tags instead of words, or indi-
rectly in the back-off mechanism of fully lexicalized
models (Headden et al, 2009).
It has been shown in supervised systems that us-
ing a hierarchical syntactic structure model can pro-
duce competitive sequence models; in other words
that a parser can be a good tagger (Li et al, 2011;
Auli and Lopez, 2011; Cohen et al, 2011). This
is unsurprising, as the parser uses a rich set of hi-
erarchical features that enable it to look at a less
localized environment than a PoS tagger which in
most cases relies solely on local contextual features.
However this interaction has not been shown for the
unsupervised setting. To our knowledge, this work
is the first to show that using dependencies for unsu-
pervised PoS induction is indeed useful.
2 Iterated learning
Although most unsupervised systems depend on
gold-standard PoS information, they can also be
used in a fully unsupervised pipeline. One reason for
doing so is to use dependency parsing as an extrinsic
evaluation for unsupervised PoS induction (Headden
et al, 2008). As discussed in that paper (and also by
Klein and Manning (2004)) the quality of the de-
pendencies drops with the use of induced tags. One
way of producing better PoS tags is to use the depen-
dency parser?s output to influence the PoS inducer,
thus turning the pipeline into a loop.
The main difficulty of this approach is to find
a way of incorporating dependency information
into a PoS induction system. In previous work
BMMM
DMV
BMMM
DMV
BMMM
Gen. 0 Gen. 1 Gen. 2
Figure 1: The iterated learning paradigm for induc-
ing both PoS tags and dependencies.
(Christodoulopoulos et al, 2011) we have described
BMMM: a PoS induction system that makes it is
easy to incorporate multiple features either at the
type or token level. For the dependency induction
system we chose the DMV model of Klein and Man-
ning (2004) because of its simplicity and its popular-
ity. Both systems are described briefly in section 3.
Using these two systems we performed an iter-
ated learning experiment. The term is borrowed
from the language evolution literature meaning ?the
process by which the output of one individual?s
learning becomes the input to other individuals?
learning? (Smith et al, 2003). Here we treat the
two systems as the individuals1 that influence each
other in successive generations starting from a run
of the original BMMM system without dependency
information (fig. 1). We start with a run of the basic
BMMM system using just context and morphology
features (generation 0) and use the output to train the
DMV. To complete the first generation, we then use
the induced dependencies as features (as described
in section 4) for a new run of the BMMM system.
As there is no single objective function, this setup
1This is not directly analogous to the language evolution no-
tion of iterated learning; here instead of a single type of indi-
vidual we have two separate systems that learn/model different
representations.
96
does not guarantee that either the quality of PoS tags
or the dependencies will improve after each genera-
tion. However, in practice this iterated learning ap-
proach works well (as we discuss in section 4).
3 Component models
3.1 DMV model
The basic DMV model (Klein and Manning, 2004)
generates dependency trees based on three decisions
(represented by three probability distributions) for a
given head node: whether to attach children in the
left or right direction; whether or not to stop attach-
ing more children in the specific direction given the
adjacency of the child in that direction; and finally
whether to attach a specific child node. The proba-
bility of an entire sentence is the sum of the probabil-
ities of all the possible derivations headed by ROOT.
The DMV model can be seen as (and is equiva-
lent to) a Context Free Grammar (CFG) with only a
few rules from head nodes to generated children and
therefore the model parameters can be estimated us-
ing the Inside-Outside algorithm (Baker, 1979).
3.2 BMMM model
The Bayesian Multinomial Mixture Model
(Christodoulopoulos et al, 2011), illustrated in
figure 2, assumes that all tokens of a given word
type belong to a single syntactic class, and each
type is associated with a number of features (e.g.,
morphological or contextual features), which form
the observed variables. The generative process first
chooses a hidden class z for each word type and then
chooses values for each of the observed features of
that word type, conditioned on the class. Both the
distribution over classes ? and the distributions over
each kind of feature ?(t) are multinomials drawn
from Dirichlet priors ? and ?(t) respectively. A
main advantage of this model is its ability to easily
incorporate features either at the type or token
level; as in Christodoulopoulos et al (2011) we
assume a single type-level feature m (morphology,
drawn from ?(m)) and several token-level features
f
(1)
. . . f
(T ) (e.g., left and right context words and,
in our extension, dependency features).
Inference in the model is performed using a col-
lapsed Gibbs sampler, integrating out the model pa-
rameters and sampling the class label zj for each
?
?
z
f
(1)
?
(1)
?
(1)
. . . . . . . . .
f
(T )
?
(T )
?
(T )
m
?
(m)
?
(m)
M
n
j
n
j
Z
Z
Z
Figure 2: The BMMM with T kinds of token-level
features (f (t) variables) and a single kind of type-
level feature (morphology, m). M is the total num-
ber of word types, Z the number of classes, and nj
the number of tokens of type j.
word type j from the following posterior distribu-
tion:
P (zj | z?j , f , ?, ?)
? P (zj | z?j , ?, ?)P (fj | f?j , z, ?, ?) (1)
where the first factor P (zj) is the prior distribu-
tion over classes (the mixing weights) and the sec-
ond (likelihood) factor P (fj) is the probability given
class zj of all the features associated with word type
j. Since the different kinds of features are assumed
to be independent, the likelihood can be rewritten as:
P (fj | f?j , z, ?, ?) = P (f
(m)
j | f
(m)
?j , z, ?, ?)
?
T?
t=1
P (f (t)j | f
(t)
?j , z, ?) (2)
and, as explained in Christodoulopoulos et al
(2011), the joint probability of all the token level
features of kind t for word type j is computed as:
P (f (t)j | f
(t)
?j , zj = z, z?j , ?)
=
?K(t)
k=1
?njk?1
i=0 (njk,z + i+ ?)
?nj?1
i=0 (n?,z + i+ F?)
(3)
97
30 
35 
40 
45 
50 
55 
60 
65 
0 1 2 3 4 5 6 7 8 9 10 
BMMM M-1 BMMM VM DMV Dir DMV Undir 
(a) Using only directed dependencies as features
30 
35 
40 
45 
50 
55 
60 
65 
0 1 2 3 4 5 6 7 8 9 10 
BMMM M-1 BMMM VM DMV Dir DMV Undir 
(b) Using directed and undirected dependencies as features
Figure 3: Developmental results on WSJ10. The performance of the PoS inducer is shown in terms of
many-to-1 accuracy (BMMM M1) and V-Measure accuracy (BMMM VM) and the performance of the
dependency inducer is shown using directed and undirected dependency accuracy (DMV Dir and DMV
Undir respectively).
where K(t) is the dimensionality of ?(t) and njk is
the number of instances of feature k in word type j.
4 Experimental design
Because the different kinds of features are assumed
to be independent in the BMMM, it is easy to add
more features into the model; this simply increases
the number of factors in equation 2. To incorpo-
rate dependency information, we added a feature for
word-word dependencies. In the model, this means
that for a word type j with nj tokens, we observe nj
dependency features (each being the head of one to-
ken of j). Like all other features, these are assumed
to be drawn from a class-specific multinomial ?(d)z
with a Dirichlet prior ?(d).
Using lexicalized head dependencies introduces
sparsity issues in much the same way contextual in-
formation does. In the case of context words, the
BMMM and most vector-based clustering systems
use a fixed number of most frequent words as fea-
tures; however in the case of dependencies we use
the induced PoS tags of the previous generation as
grouping labels: we aggregate the head dependency
counts of words that have the same PoS tag, so the
dimension of ?(d)z is just the number of PoS tags.
The dependency features are used in tandem with
the features used in the original BMMM system,
namely the 100 most frequent context words (?1
context window), the suffixes extracted from the
Morfessor system (Creutz and Lagus, 2005) and
the extended morphology features of Haghighi and
Klein (2006).
For designing the iterated learning experiments,
we used the 10-word version of the WSJ corpus
(WSJ10) as development data and ran the iterative
learning process for 10 generations. To evaluate the
quality of the induced PoS tags we used the many-
to-1 (M1) and V-Measure (VM) metrics and for the
induced dependencies we used directed and undi-
rected accuracy.
Figure 3a presents the developmental result of the
iterated learning experiments on WSJ10 where only
directed dependencies where used as features. We
can see that although there was some improvement
in the PoS induction score after the first generation,
the rest of the metrics show no significant improve-
ment throughout the experiment.
When we used undirected dependencies as fea-
tures (figure 3b) the improvement over iterations
was substantial: nearly 8.5% increase in M1 and
1.3% in VM after only 5 iterations. We can also see
that the results of the DMV parser are improving as
well: 7% increase in directed and 3.8% in undirected
accuracy. This is to be expected, since as Headden
et al (2008) show, there is a (weak) correlation be-
tween the intrinsic scores of a PoS inducer and the
98
performance of an unsupervised dependency parser
trained on the inducer?s output.
Using the same development set we selected the
remaining system parameters; for the BMMM we
fixed the number of induced classes to the number
of gold-standard PoS tags for each language and
used 500 sampling iterations with annealing. For
the DMV model we used 20 EM iterations. Finally
we used observed that after 5 generations the rate of
improvement seems to level, so for the rest of the
languages we use only 5 learning iterations.
References
Michael Auli and Adam Lopez. 2011. A comparison of
loopy belief propagation and dual decomposition for
integrated CCG supertagging and parsing. In Proceed-
ings of ACL-HLT, pages 470?480.
James K. Baker. 1979. Trainable grammars for speech
recognition. The Journal of the Acoustical Society of
America, 65(S1):S132, June.
Christos Christodoulopoulos, Sharon Goldwater, and
Mark Steedman. 2011. A Bayesian mixture model
for pos induction using multiple features. In Proceed-
ings of EMNLP, pages 638?647, Edinburgh, Scotland,
UK., July.
Shay B. Cohen, Dipanjan Das, and Noah A. Smith. 2011.
Unsupervised structure prediction with non-parallel
multilingual guidance. In Proceedings of EMNLP,
pages 50?61.
Mathias Creutz and Krista Lagus. 2005. Inducing
the morphological lexicon of a natural language from
unannotated text. In In Proceedings of AKRR, vol-
ume 5, pages 106?113.
Aria Haghighi and Dan Klein. 2006. Prototype-driven
learning for sequence models. In Proceedings of
NAACL, pages 320?327.
William P. Headden, David McClosky, and Eugene Char-
niak. 2008. Evaluating unsupervised part-of-speech
tagging for grammar induction. In Proceedings of
COLING, pages 329?336.
William P. Headden, Mark Johnson, and David Mc-
Closky. 2009. Improving unsupervised dependency
parsing with richer contexts and smoothing. In Pro-
ceedings of NAACL, pages 101?109.
Dan Klein and Christopher D. Manning. 2004. Corpus-
based induction of syntactic structure: models of de-
pendency and constituency. In Proceedings of ACL.
Zhenghua Li, Min Zhang, Wanxiang Che, Ting Liu, Wen-
liang Chen, and Haizhou Li. 2011. Joint models for
chinese POS tagging and dependency parsing. In Pro-
ceedings of EMNLP, pages 1180?1191.
Kenny Smith, Simon Kirby, and Henry Brighton. 2003.
Iterated learning: a framework for the emergence of
language. Artif. Life, 9(4):371?386.
99
