Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 322?332,
Edinburgh, Scotland, UK, July 27?31, 2011. c?2011 Association for Computational Linguistics
Universal Morphological Analysis using Structured Nearest Neighbor
Prediction
Young-Bum Kim
University of Wisconsin-Madison
ybkim@cs.wisc.edu
Jo?o V. Gra?a
L2F INESC-ID
Lisboa, Portugal
joao.graca@l2f.inesc-id.pt
Benjamin Snyder
University of Wisconsin-Madison
bsnyder@cs.wisc.edu
Abstract
In this paper, we consider the problem of un-
supervised morphological analysis from a new
angle. Past work has endeavored to design un-
supervised learning methods which explicitly
or implicitly encode inductive biases appropri-
ate to the task at hand. We propose instead
to treat morphological analysis as a structured
prediction problem, where languages with la-
beled data serve as training examples for un-
labeled languages, without the assumption of
parallel data. We define a universal morpho-
logical feature space in which every language
and its morphological analysis reside. We de-
velop a novel structured nearest neighbor pre-
diction method which seeks to find the mor-
phological analysis for each unlabeled lan-
guage which lies as close as possible in the
feature space to a training language. We ap-
ply our model to eight inflecting languages,
and induce nominal morphology with substan-
tially higher accuracy than a traditional, MDL-
based approach. Our analysis indicates that
accuracy continues to improve substantially as
the number of training languages increases.
1 Introduction
Over the past several decades, researchers in the nat-
ural language processing community have focused
most of their efforts on developing text processing
tools and techniques for English (Bender, 2009),
a morphologically simple language. Recently, in-
creasing attention has been paid to the wide variety
of other languages of the world. Most of these lan-
guages still pose severe difficulties, due to (i) their
lack of annotated textual data, and (ii) the fact that
they exhibit linguistic structure not found in English,
and are thus not immediately susceptible to many
traditional NLP techniques.
Consider the example of nominal part-of-speech
analysis. The Penn Treebank defines only four En-
glish noun tags (Marcus et al, 1994), and as a re-
sult, it is easy to treat the words bearing these tags
as completely distinct word classes, with no inter-
nal morphological structure. In contrast, a compara-
ble tagset for Hungarian includes 154 distinct noun
tags (Erjavec, 2004), reflecting Hungarian?s rich in-
flectional morphology. When dealing with such lan-
guages, treating words as atoms leads to severe data
sparsity problems.
Because annotated resources do not exist for most
morphologically rich languages, prior research has
focused on unsupervised methods, with a focus on
developing appropriate inductive biases. However,
inductive biases and declarative knowledge are no-
toriously difficult to encode in well-founded models.
Even putting aside this practical matter, a universally
correct inductive bias, if there is one, is unlikely to
be be discovered by a priori reasoning alone.
In this paper, we argue that languages for which
we have gold-standard morphological analyses can
be used as effective guides for languages lacking
such resources. In other words, instead of treating
each language?s morphological analysis as a de novo
induction problem to be solved with a purely hand-
coded bias, we instead learn from our labeled lan-
guages what linguistically plausible morphological
analyses looks like, and guide our analysis in this
direction.
322
More formally, we recast morphological induc-
tion as a new kind of supervised structured predic-
tion problem, where each annotated language serves
as a single training example. Each language?s noun
lexicon serves as a single input x, and the analysis
of the nouns into stems and suffixes serves as a com-
plex structured label y.
Our first step is to define a universal morpholog-
ical feature space, into which each language and its
morphological analysis can be mapped. We opt for
a simple and intuitive mapping, which measures the
sizes of the stem and suffix lexicons, the entropy of
these lexicons, and the fraction of word forms which
appear without any inflection.
Because languages tend to cluster into well de-
fined morphological groups, we cast our learn-
ing and prediction problem in the nearest neighbor
framework (Cover and Hart, 1967). In contrast to
its typical use in classification problems, where one
can simply pick the label of the nearest training ex-
ample, we are here faced with a structured predic-
tion problem, where locations in feature space de-
pend jointly on the input-label pair (x, y). Finding a
nearest neighbor thus consists of searching over the
space of morphological analyses, until a point in fea-
ture space is reached which lies closest to one of the
labeled languages. See Figure 1 for an illustration.
To provide a measure of empirical validation, we
applied our approach to eight languages with inflec-
tional nominal morphology, ranging in complexity
from very simple (English) to very complex (Hun-
garian). In all but one case, our approach yields
substantial improvements over a comparable mono-
lingual baseline (Goldsmith, 2005), which uses the
minimum description length principle (MDL) as its
inductive bias. On average, our method increases
accuracy by 11.8 percentage points, corresponding
to a 42% decrease in error relative to a supervised
upper bound. Further analysis indicates that accu-
racy improves as the number of training languages
increases.
2 Related Work
In this section, we briefly review prior work on un-
supervised morphological induction, as well as mul-
tilingual analysis in NLP.
Unsupervised Morphological Induction: Unsu-
pervised morphology remains an active area of re-
search (Schone and Jurafsky, 2001; Goldsmith,
2005; Adler and Elhadad, 2006; Creutz and La-
gus, 2005; Dasgupta and Ng, 2007; Creutz and La-
gus, 2007; Poon et al, 2009). Many existing algo-
rithms derive morpheme lexicons by identifying re-
curring patterns in words. The goal is to optimize the
compactness of the data representation by finding a
small lexicon of highly frequent strings, resulting in
a minimum description length (MDL) lexicon and
corpus (Goldsmith, 2001; Goldsmith, 2005). Later
work cast this idea in a probabilistic framework in
which the the MDL solution is equivalent to a MAP
estimate in a suitable Bayesian model (Creutz and
Lagus, 2005). In all these approaches, a locally op-
timal segmentation is identified using a task-specific
greedy search.
Multilingual Analysis: An influential line of prior
multilingual work starts with the observation that
rich linguistic resources exist for some languages
but not others. The idea then is to project linguis-
tic information from one language onto others via
parallel data. Yarowsky and his collaborators first
developed this idea and applied it to the problems of
part-of-speech tagging, noun-phrase bracketing, and
morphology induction (Yarowsky and Wicentowski,
2000; Yarowsky et al, 2000; Yarowsky and Ngai,
2001), and other researchers have applied the idea
to syntactic and semantic analysis (Hwa et al, 2005;
Pad? and Lapata, 2006) In these cases, the existence
of a bilingual parallel text along with highly accurate
predictions for one of the languages was assumed.
Another line of work assumes the existence of
bilingual parallel texts without the use of any super-
vision (Dagan et al, 1991; Resnik and Yarowsky,
1997). This idea has been developed and applied to
a wide variety tasks, including morphological anal-
ysis (Snyder and Barzilay, 2008b; Snyder and Barzi-
lay, 2008a), part-of-speech induction (Snyder et al,
2008; Snyder et al, 2009b; Naseem et al, 2009),
and grammar induction (Snyder et al, 2009a; Blun-
som et al, 2009; Burkett et al, 2010). An even
more recent line of work does away with the as-
sumption of parallel texts and performs joint unsu-
pervised induction for various languages through the
use of coupled priors in the context of grammar in-
323
duction (Cohen and Smith, 2009; Berg-Kirkpatrick
and Klein, 2010).
In contrast to these previous approaches, the
method proposed in this paper does not assume the
existence of any parallel text, but does assume that
labeled data exists for a wide variety of languages, to
be used as training examples for our test language.
3 Structured Nearest Neighbor
We reformulate morphological induction as a super-
vised learning task, where each annotated language
serves as a single training example for our language-
independent model. Each such example consists
of an input-label pair (x, y), both of which contain
complex internal structure: The input x ? X con-
sists of a vocabulary list of all words observed in a
particular monolingual corpus, and the label y ? Y
consists of the correct morphological analysis of all
the vocabulary items in x.1 Because our goal is
to generalize across languages, we define a feature
function which maps each (x, y) pair to a universal
feature space: f : X ? Y ? Rd.
For each unlabeled input language x, our goal is
to predict a complete morphological analysis y ? Y
which maximizes a scoring function on the fea-
ture space, score : Rd ? R. This scoring func-
tion is trained using the n labeled-language exam-
ples: (x, y)1, . . . , (x, y)n, and the resulting predic-
tion rule for unlabeled input x is given by:
y? = argmax
y?Y
score
(
f(x, y)
)
Languages can be typologically categorized by
the type and richness of their morphology. On the
assumption that for each test language, at least one
typologically similar language will be present in the
training set, we employ a nearest neighbor scoring
function. In the standard nearest neighbor classifi-
cation setting, one simply predicts the label of the
closest training example in the input space.2 In our
structured prediction setting, the mapping to the uni-
versal feature space depends crucially on the struc-
ture of the proposed label y, not simply the input
1Technically, the label space of each input, Y , should be
thought of as a function of the input x. We suppress this depen-
dence for notational clarity.
2More generally the majority label of the k-nearest neigh-
bors.
x. We thus generalize nearest-neighbor prediction
to the structured scenario and propose the following
prediction rule:
y? = argmin
y?Y
min
`
? f(x, y)? f(x`, y`) ?, (1)
where the index ` ranges over the training languages.
In words, we predict the morphological analysis y
for our test language which places it as close as pos-
sible in the universal feature space to one of the
training languages `.
Morphological Analysis: In this paper we focus
on nominal inflectional suffix morphology. Consider
the word utiskom in Serbian, meaning impression
with the instrumental case marking. A correct analy-
sis of this word would divide it into a stem (utisak =
impression), a suffix (-om = instrumental case), and
a phonological deletion rule on the stem?s penulti-
mate vowel (..ak#? ..k#).
More generally, as we define it, a morphological
analysis of a word type w consists of (i) a stem t, (ii),
a suffix f , and (iii) a deletion rule d. Either or both
of the suffix and deletion rule can be NULL. We al-
low three types of deletion rules on stems: deletion
of final vowels (..V# ? ..#), deletion of penulti-
mate vowels (..V C# ? ..C#), and removals and
additions of final accent marks (e.g. ..a?# ? ..a#).
We require that stems be at least three characters
long and that suffixes be no more than four. And,
of course, we require that after (1) applying deletion
rule d to stem t, and (2) adding suffix f to the result,
we obtain word w.
Universal Feature Space: We employ a fairly
simple and minimal set of features, all of which
could plausibly generalize across a wide range of
languages. Consider the set of stems T , suffixes F ,
and deletion rules D, induced by the morphological
analyses y of the words x. Our first three features
simply count the sizes of these three sets.
These counting features consider only the raw
number of unique morphemes (and phonological
rules) being used, but not their individual frequency
or distribution. Our next set of features considers
the empirical entropy of these occurrences as dis-
tributed across the lexicon of words x by analysis y.
324
f(x2, y2)
f(x1, y1)
f(x3, y3)
y(t,1)
y(t+1,1)
y(t+1,2)
y(t,2)
y(t,3)
y(t+1,3)
f
?
x, y(0,?)
?
Initialization
Figure 1: Structured Nearest Neighbor Search: The inference procedure for unlabeled test language x, when trained
with three labeled languages, (x1, y1), (x2, y2), (x3, y3). Our search procedure iteratively attempts to find labels for x
which are as close as possible in feature space to each of the training languages. After convergence, the label which is
closest in distance to a training language is predicted, in this case being the label near training language (x3, y3).
For example, if the (x, y) pair consists of the ana-
lyzed words {kiss, kiss-es, hug}, then the empirical
distributions over stems, suffixes, and deletion rules
would be:
? P (t = kiss) = 2/3
? P (t = hug) = 1/3
? P (f = NULL) = 2/3
? P (f = ?es) = 1/3
? P (d = NULL) = 1
The three entropy features are defined as the shan-
non entropies of these stem, suffix, and deletion rule
probabilities: H(t), H(f), H(d).3
Finally, we consider two simple percentage fea-
tures: the percentage of words in x which according
to y are left unsegmented (i.e. have the null suf-
fix, 2/3 in the example above), and the percentage of
segmented words which employ a deletion rule (0 in
the example above). Thus, in total, our model em-
ploys 8 universal morphological features. All fea-
tures are scaled to the unit interval and are assumed
to have equal weight.
3Note that here and throughout the paper, we operate over
word types, ignoring their corpus frequencies.
3.1 Search Algorithm
The main algorithmic challenge for our model lies in
efficiently computing the best morphological analy-
sis y for each language-specific word set x, accord-
ing to Equation 1. Exhaustive search through the
set of all possible morphological analyses is impos-
sible, as the number of such analyses grows expo-
nentially in the size of the vocabulary. Instead, we
develop a greedy search algorithm in the following
fashion (the search procedure is visually depicted in
Figure 1).
At each time-step t, we maintain a set of frontier
analyses {y(t,`)}`, where ` ranges over the traininglanguages. The goal is to iteratively modify each of
these frontier analyses y(t,`) ? y(t+1,`) so that the
location of the training language in universal feature
space ? f(x, y(t+1,`)) ? is as close as possible to
the location of the training language `: f(x`, y`).
After iterating this procedure to convergence, we
are left with a set of analyses {y(`)}`, each of whichapproximates the analyses which yield minimal dis-
tances to a particular training language:
y(`) ? argmin
y?Y
? f(x, y)? f(x`, y`) ? .
We finally select from amongst these analyses and
325
make our prediction:
`? = argmin
`
? f(x, y(`))? f(x`, y`) ?
y? = y(`?)
The main outline of our search algorithm is based
on the MDL-based greedy search heuristic devel-
oped and studied by (Goldsmith, 2005). At a high
level, this search procedure alternates between indi-
vidual analyses of words (keeping the set of stems
and suffixes fixed), aggregate discoveries of new
stems (keeping the suffixes fixed), and aggregate dis-
coveries of new suffixes (keeping stems fixed). As
input, we consider the test words x in our new lan-
guage, and we run the search in parallel for each
training language (x`, y`). For each such test-train
language pair, the search consists of the following
stages:
Stage 0: Initialization
We initially analyze each word w ? x according
to peaks in successor frequency.4 If w?s n-character
prefix w:n has successor frequency > 1 and the sur-
rounding prefixes, w:n?1 and w:n+1 both have suc-
cessor frequency = 1, then we analyze w as a stem-
suffix pair: (w:n, wn+1:).5 Otherwise, we initialize
w as an unsuffixed stem. As this procedure tends to
produce an overly large set of suffixes F , we further
prune F down to the number of suffixes found in
the training language, retaining those which appear
with the largest number of stems. This initialization
stage is carried out once, and afterwards the follow-
ing three stages are repeated until convergence.
Stage 1: Reanalyze each word
In this stage, we reanalyze each word (in random
order). We use the set of stems T and suffixes F
obtained from the previous stage, and don?t permit
the addition of any new items to these lists. In-
stead, we focus on obtaining better analyses of each
word, while also building up a set of phonological
deletion rules D. For each word w ? x, we con-
sider all possible segmentations of w into a stem-
4The successor frequency of a string prefix s is defined as
the number of unique characters that occur immediately after s
in the vocabulary.
5With the restriction that at this stage we only allow suffixes
up to length 5, and stems of at least length 3.
suffix pair (t, f), for which f ? F , and where ei-
ther t ? T or some t? ? T such that t is obtained
from t? using a deletion rule d (e.g. by deleting a
final or penultimate vowel). For each such possi-
ble analysis y?, we compute the resulting location
in feature space f(x, y?), and select the analysis that
brings us closest to our target training language:
y = argminy? ? f(x, y?)? f(x`, y`) ? .
Stage 2: Find New Stems
In this stage, we keep our set of suffixes F and
deletion rules D from the previous stage fixed, and
attempt to find new stems to add to T through an ag-
gregate analysis of unsegmented words. For every
string s, we consider the set of words which are cur-
rently unsegmented, and can be analyzed as a stem-
suffix pair (s, f) for some existing suffix f ? F ,
and some deletion rule d ? D. We then consider
the joint segmentation of these words into a new
stem s, and their respective suffixes. As before, we
choose the segmentation if it brings us closer in fea-
ture space to our target training language.
Stage 3: Find New Suffixes
This stage is exactly analogous to the previous
stage, except we now fix the set of stems T and seek
to find new suffixes.
3.2 A Monolingual Supervised Model
In order to provide a plausible upper bound on per-
formance, we also formulate a supervised monolin-
gual morphological model, using the structured per-
ceptron framework (Collins, 2002). Here we as-
sume that we are given some training sequence of in-
puts and morphological analyses (all within one lan-
guage): (x1, y1), (x2, y2), . . . , (xn, yn). We define
each input xi to be a noun w, along with a morpho-
logical tag z, which specifies the gender, case, and
number of the noun. The goal is to predict the cor-
rect segmentation of w into stem, suffix, and phono-
logical deletion rule: yi = (t, f, d).6
To do so, we define a feature function over input-
label pairs, (x, y), with the following binary feature
templates: (1) According to label yi, the stem is t
6While the assumption of the correct morphological tag as
input is somewhat unrealistic, this model still gives us a strong
upper bound on how well we can expect our unsupervised
model to perform.
326
Type Counts Entropy Percentage
# words # stems # suffs # dels stem entropy suff entropy del entropy unseg deleted
BG 4833 3112 21 8 11.4 2.7 0.9 .45 .29
CS 5836 3366 28 12 11.5 3.2 1.6 .38 .53
EN 4178 3453 3 1 11.7 1.0 0.1 .73 .06
ET 6371 3742 141 5 11.5 5.0 0.2 .31 .04
HU 8051 3746 231 7 11.3 5.8 0.5 .23 .11
RO 5578 3297 23 8 11.5 2.9 1.4 .48 .51
SL 6111 3172 32 6 11.3 3.2 1.5 .33 .56
SR 5849 3178 28 5 11.4 2.9 1.4 .33 .53
Table 1: Corpus statistics for the eight languages. The first four columns give the number of unique word, stem, suffix,
and phonological deletion rule types. The next three columns give, respectively, the entropies of the distributions
of stems, suffixes (including NULL), and deletion rules (including NULL) over word types. The final two columns
give, respectively, the percentage of word types occurring with the NULL suffix, and the number of non-NULL suffix
words which use a phonological deletion rule. Note that the final eight columns define the universal feature space used
by our model. BG = Bulgarian, CS = Czech, EN = English, ET = Estonian, HU = Hungarian, RO = Romanian, SL =
Slovene, SR = Serbian
(one feature for each possible stem). (2) Accord-
ing to label yi, the suffix and deletion rule are (f, d)
(one feature for every possible pair of deletion rules
and suffixes). (3) According to label yi and morpho-
logical tag z, the suffix, deletion rule, and gender
are respectively (f, d,G). (4) According to label yi
and morphological tag z, the suffix, deletion rule,
and case are (f, d, C). (5) According to label yi and
morphological tag z, the suffix, deletion rule, and
number are (f, d,N).
We train a set of linear weights on our fea-
tures using the averaged structured perceptron algo-
rithm (Collins, 2002).
4 Experiments
In this section we turn to experimental findings to
provide empirical support for our proposed frame-
work.
Corpus: To test our cross-lingual model, we ap-
ply it to a morphologically analyzed corpus of eight
languages (Erjavec, 2004). The corpus includes a
roughly 100,000 word English text, Orwell?s novel
?Nineteen Eighty Four,? and its translation into
seven languages: Bulgarian, Czech, Estonian, Hun-
garian, Romanian, Slovene, and Serbian. All the
words in the corpus are tagged with morphologi-
cal stems and a detailed morpho-syntactic analysis.
Although the texts are parallel, we note that par-
allelism is nowhere assumed nor exploited by our
model. See Table 1 for a summary of relevant cor-
pus statistics. As indicated in the table, the raw num-
ber of nominal word types varies quite a bit across
the languages, almost doubling from 4,178 (English)
to 8,051 (Hungarian). In contrast, the number of
stems appearing within these words is relatively sta-
ble across languages, ranging from a minimum of
3,112 (Bulgarian) to a maximum of 3,746 (Hungar-
ian), an increase of just 20%.
In contrast, the number of suffixes across the lan-
guages varies quite a bit. Hungarian and Esto-
nian, both Uralic languages with very complex nom-
inal morphology, use 231 and 141 nominal suffixes,
respectively. Besides English, the remaining lan-
guages employ between 21 and 32 suffixes, and En-
glish is the outlier in the other direction, with just
three nominal inflectional suffixes.
Baselines and Results: As our unsupervised
monolingual baseline, we use the Linguistica pro-
gram (Goldsmith, 2001; Goldsmith, 2005). We ap-
ply Linguistica?s default settings, and run the ?suffix
prediction? option. Our model?s search procedure
closely mirrors the one used by Linguistica, with
the crucial difference that instead of attempting to
greedily minimize description length, our algorithm
instead tries to find the analysis as close as possi-
ble in the universal feature space to that of another
language.
To apply our model, we treat each of the eight
327
Linguistica
Our Model
SupervisedNearest Neighbor Self (oracle) Avg.
Accuracy Distance Accuracy Distance Accuracy Distance
BG 68.7 84.0 (RO) 0.13 88.7 0.03 68.6 3.90 94.7
CS 60.4 82.8 (BG) 0.40 84.5 0.03 66.3 4.05 93.5
EN 81.1 75.8 (BG) 1.29 89.3 0.10 58.3 4.30 93.4
ET 51.2 66.6 (HU) 0.35 80.9 0.03 52.8 4.57 86.5
HU 64.5 69.3 (ET) 0.81 66.5 1.10 68.0 4.94 94.9
RO 65.6 71.0 (CS) 0.11 71.2 0.15 62.3 3.95 89.1
SL 61.1 82.8 (SR) 0.07 85.5 0.04 61.7 3.69 95.4
SR 64.2 79.1 (SL) 0.06 82.2 0.04 63.0 3.71 94.8
avg. 64.6 76.4 0.40 81.1 0.19 62.6 4.14 92.8
Table 2: Prediction accuracy over word types for the Linguistica baseline, our cross-lingual model, and the monolin-
gual supervised perceptron model. For our model, we provide both prediction accuracy and resulting distance to the
training language in three different scenarios: (i) Nearest Neighbor: The training languages include all seven other
languages in our data set, and the predictions with minimal distance to a training language are chosen (the nearest
neighbor is indicated in parentheses). (ii) Self (oracle): Each language is trained to minimize the distance to its own
gold-standard analysis. (iii) Average: The feature values of all seven training languages are averaged together to
create a single objective.
languages in turn as the test language, with the other
seven serving as training examples. For each test
language, we iterate the search procedure for each
training language (performed in parallel), until con-
vergence. The number of required iterations varies
from 6 to 36 (depending on the test-training lan-
guage pair), and each iteration takes no more than 30
seconds of run-time on a 2.4GHz Intel Xeon E5620
processor. We also consider two variants of our
method. In the first (Self (oracle)), we train each
test language to minimize the distance to its own
gold standard feature values. In the second variant
(Avg.), we average the feature values of all seven
training languages into a single objective. As a plau-
sible upper bound on performance, we implemented
the structured perceptron described in Section 3.2.
For each language, we train the perceptron on a ran-
domly selected set of 80% of the nouns, and test on
the remaining 20%.
The prediction accuracy for all models is calcu-
lated as the fraction of word types with correctly
predicted suffixes. See Table 2 for the results. For
all languages other than English (which is a mor-
phological loner in our group of languages), our
model improves over the baseline by a substantial
margin, yielding an average increase of 11.8 abso-
lute percentage points, and a reduction in error rela-
tive to the supervised upper bound of 42%. Some of
the most striking improvements are seen on Serbian
and Slovene. These languages are closely related
to one another, and indeed our model discovers that
they are each others? nearest neighbors. By guiding
their morphological analyses towards one another,
our model achieves a 21 percentage point increase
in the case of Slovene and a 15 percentage point in-
crease in the case of Slovene.
Perhaps unsurprisingly, when each language?s
gold standard feature values are used as its own
target (Self (oracle) in Table 2), performance in-
creases even further, to an average of 81.1%. By the
same token, the resulting distance in universal fea-
ture space between training and test analyses is cut
in half under this variant, when compared to the non-
oracular nearest neighbor method. The remaining
errors may be due to limitations of the search proce-
dure (i.e. getting caught in local minima), or to the
coarseness of the feature space (i.e. incorrect analy-
ses might map to the same feature values as the cor-
rect analysis). Finally, we note that minimizing the
distance to the average feature values of the seven
training languages (Avg. in Table 2) yields subpar
performance and very large distances between be-
tween predicted analyses and target feature values
(4.14 compared to 0.40 for nearest neighbor). This
328
Linguistica
Gold Standard
Our Method
BG
CS
EN
ET
HU
RO
SL
SR
BG
CS
EN
ET
HU
RO
SL
SR
BG
CS
EN
ET
HU
RO
SL
SR
Figure 2: Locations in Feature Space of Linguistica predictions (green squares), gold standard analyses (red tri-
angles), and our model?s nearest neighbor predictions (blue circles). The original 8-dimensional feature space was
reduced to two dimensions using Multidimensional Scaling.
result may indicate that the average feature point be-
tween training languages is simply unattainable as
an analysis of a real lexicon of nouns.
Visualizing Locations in Feature Space: Besides
assessing our method quantitatively, we can also vi-
sualize the the eight languages in universal feature
space according to (i) their gold standard analyses,
(ii) the predictions of our model and (iii) the pre-
dictions of Linguistica. To do so, we reduce the 8-
dimensional features space down to two dimensions
while preserving the distances between the predicted
and gold standard feature vectors, using Multidi-
mensional Scaling (MDS). The results of this anal-
ysis are shown in Figure 2. With the exception of
English, our model?s analyses lie closer in feature
space to their gold standard counterparts than those
of the baseline. It is interesting to note that Serbian
and Slovene, which are very similar languages, have
essentially swapped places under our model?s anal-
ysis, as have Estonian and Hungarian (both highly
inflected Uralic languages). English has (unfortu-
nately) been pulled towards Bulgarian, the second
least inflecting language in our set.
Learning Curves: We also measured the perfor-
mance of our method as a function of the number
of languages in the training set. For each target lan-
guage, we consider all possible training sets of sizes
ranging from 1 to 7 and select the predictions which
bring our test language closest in distance to one of
the languages in the set. We then average the result-
ing accuracy over all training sets of each size. Fig-
ure 3 shows the resulting learning curves averaged
over all test languages (left), as well as broken down
by test language (right). The overall trend is clear:
as additional languages are added to the training set,
test performance improves. In fact, with only one
training language, our method performs worse (on
average) than the Linguistica baseline. However,
with two or more training languages available, our
method achieves superior results.
Accuracy vs. Distance: We can gain some in-
sight into these learning curves if we consider the
relationship between accuracy (of the test language
analysis) and distance to the training language (of
the same predicted analysis). The more training lan-
guages available, the greater the chance that we can
guide our test language into very close proximity to
329
1 2 3 4 5 6 7Number of training languages0.55
0.6
0.65
0.7
0.75
0.8
0.85
BGCSSLSRENROHUET1 2 3 4 5 6 7Number of training languages0.62
0.64
0.66
0.68
0.7
0.72
0.74
0.76 Our ModelLinguistica
Figure 3: Learning curves for our model as the number of training languages increases. The figure on the left shows
the average accuracy of all eight languages for increasingly larger training sets (results are averaged over all training
sets of size 1,2,3,...). The dotted line indicates the average performance of the baseline. The figure on the right shows
similar learning curves, broken down individually for each test language (see Figure 1 for language abbreviations).
one of them. It thus stands to reason that a strong
(negative) correlation between distance and accu-
racy would lead to increased accuracy with larger
training sets. In order to assess this correlation, we
considered all 56 test-train language pairs and col-
lected the resulting accuracy and distance for each
pair. We separately scaled accuracy and distance to
the unit interval for each test language (as some test
languages are inherently more difficult than others).
The resulting plot, shown in Figure 4, shows the ex-
pected correlation: When our test language can be
guided very closely to the training language, the re-
sulting predictions are likely to be good. If not, the
predictions are likely to be bad.
5 Conclusions and Future Work
The approach presented in this paper recasts mor-
phological induction as a structured prediction task.
We assume the presence of morphologically labeled
languages as training examples which guide the in-
duction process for unlabeled test languages. We
developed a novel structured nearest neighbor ap-
proach for this task, in which all languages and their
morphological analyses lie in a universal feature
space. The task of the learner is to search through
the space of morphological analyses for the test lan-
guage and return the result which lies closest to one
0 0.2 0.4 0.6 0.8 1Distance (normalized)
0
0.2
0.4
0.6
0.8
1
Accu
racy 
(norm
alized
)
Figure 4: Accuracy vs. Distance: For all 56 possi-
ble test-train language pairs, we computed test accuracy
along with resulting distance in universal feature space
to the training language. Distance and accuracy are sep-
arately normalized to the unit interval for each test lan-
guage, and all resulting points are plotted together. A
line is fit to the points using least-squares regression.
330
of the training languages. Our empirical findings
validate this approach: On a set of eight different
languages, our method yields substantial accuracy
gains over a traditional MDL-based approach in the
task of nominal morphological induction.
One possible shortcoming of our approach is that
it assumes a uniform weighting of the cross-lingual
feature space. In fact, some features may be far more
relevant than others in guiding our test language to
an accurate analysis. In future work, we plan to in-
tegrate distance metric learning into our approach,
allowing some features to be weighted more heavily
than others. Besides potential gains in prediction ac-
curacy, this approach may shed light on deeper rela-
tionships between languages than are otherwise ap-
parent.
References
Meni Adler and Michael Elhadad. 2006. An un-
supervised morpheme-based hmm for hebrew mor-
phological disambiguation. In Proceedings of the
ACL/CONLL, pages 665?672.
Emily M. Bender. 2009. Linguistically na?ve != lan-
guage independent: why NLP needs linguistic typol-
ogy. In Proceedings of the EACL 2009 Workshop
on the Interaction between Linguistics and Compu-
tational Linguistics, pages 26?32, Morristown, NJ,
USA. Association for Computational Linguistics.
Taylor Berg-Kirkpatrick and Dan Klein. 2010. Phyloge-
netic grammar induction. In Proceedings of the ACL,
pages 1288?1297, Uppsala, Sweden, July. Association
for Computational Linguistics.
P. Blunsom, T. Cohn, and M. Osborne. 2009. Bayesian
synchronous grammar induction. Advances in Neural
Information Processing Systems, 21:161?168.
David Burkett, Slav Petrov, John Blitzer, and Dan Klein.
2010. Learning better monolingual models with unan-
notated bilingual text. In Proceedings of CoNLL.
Shay B. Cohen and Noah A. Smith. 2009. Shared lo-
gistic normal distributions for soft parameter tying in
unsupervised grammar induction. In Proceedings of
the NAACL/HLT.
Michael Collins. 2002. Discriminative training meth-
ods for hidden markov models: theory and experi-
ments with perceptron algorithms. In Proceedings of
EMNLP, pages 1?8.
T. Cover and P. Hart. 1967. Nearest neighbor pattern
classification. Information Theory, IEEE Transactions
on, 13(1):21?27.
Mathias Creutz and Krista Lagus. 2005. Unsupervised
morpheme segmentation and morphology induction
from text corpora using morfessor 1.0. Publications
in Computer and Information Science Report A81,
Helsinki University of Technology.
Mathias Creutz and Krista Lagus. 2007. Unsupervised
models for morpheme segmentation and morphology
learning. ACM Transactions on Speech and Language
Processing, 4(1).
Ido Dagan, Alon Itai, and Ulrike Schwall. 1991. Two
languages are more informative than one. In Proceed-
ings of the ACL, pages 130?137.
Sajib Dasgupta and Vincent Ng. 2007. Unsuper-
vised part-of-speech acquisition for resource-scarce
languages. In Proceedings of the EMNLP-CoNLL,
pages 218?227.
T. Erjavec. 2004. MULTEXT-East version 3: Multi-
lingual morphosyntactic specifications, lexicons and
corpora. In Fourth International Conference on Lan-
guage Resources and Evaluation, LREC, volume 4,
pages 1535?1538.
John Goldsmith. 2001. Unsupervised Learning of the
Morphology of a Natural Language. Computational
Linguistics, 27(2):153?198.
John Goldsmith. 2005. An algorithm for the unsuper-
vised learning of morphology. Technical report, Uni-
versity of Chicago.
R. Hwa, P. Resnik, A. Weinberg, C. Cabezas, and O. Ko-
lak. 2005. Bootstrapping parsers via syntactic projec-
tion across parallel texts. Journal of Natural Language
Engineering, 11(3):311?325.
M.P. Marcus, B. Santorini, and M.A. Marcinkiewicz.
1994. Building a large annotated corpus of En-
glish: The Penn Treebank. Computational linguistics,
19(2):313?330.
Tahira Naseem, Benjamin Snyder, Jacob Eisenstein, and
Regina Barzilay. 2009. Multilingual part-of-speech
tagging: two unsupervised approaches. Journal of Ar-
tificial Intelligence Research, 36(1):341?385.
Sebastian Pad? and Mirella Lapata. 2006. Optimal con-
stituent alignment with edge covers for semantic pro-
jection. In Proceedings of ACL, pages 1161 ? 1168.
Hoifung Poon, Colin Cherry, and Kristina Toutanova.
2009. Unsupervised morphological segmentation with
log-linear models. In Proceedings of Human Lan-
guage Technologies: The 2009 Annual Conference of
the North American Chapter of the Association for
Computational Linguistics, NAACL ?09, pages 209?
217, Stroudsburg, PA, USA. Association for Compu-
tational Linguistics.
Philip Resnik and David Yarowsky. 1997. A perspective
on word sense disambiguation methods and their eval-
uation. In Proceedings of the ACL SIGLEX Workshop
331
on Tagging Text with Lexical Semantics: Why, What,
and How?, pages 79?86.
Patrick Schone and Daniel Jurafsky. 2001. Knowledge-
free induction of inflectional morphologies. In NAACL
?01: Second meeting of the North American Chapter of
the Association for Computational Linguistics on Lan-
guage technologies 2001, pages 1?9, Morristown, NJ,
USA. Association for Computational Linguistics.
Benjamin Snyder and Regina Barzilay. 2008a. Cross-
lingual propagation for morphological analysis. In
Proceedings of the AAAI, pages 848?854.
Benjamin Snyder and Regina Barzilay. 2008b. Unsuper-
vised multilingual learning for morphological segmen-
tation. In Proceedings of the ACL/HLT, pages 737?
745.
Benjamin Snyder, Tahira Naseem, Jacob Eisenstein, and
Regina Barzilay. 2008. Unsupervised multilingual
learning for POS tagging. In Proceedings of EMNLP,
pages 1041?1050.
Benjamin Snyder, Tahira Naseem, and Regina Barzilay.
2009a. Unsupervised multilingual grammar induction.
In Proceedings of the ACL, pages 73?81.
Benjamin Snyder, Tahira Naseem, Jacob Eisenstein, and
Regina Barzilay. 2009b. Adding more languages im-
proves unsupervised multilingual part-of-speech tag-
ging: a Bayesian non-parametric approach. In Pro-
ceedings of the NAACL, pages 83?91.
David Yarowsky and Grace Ngai. 2001. Inducing mul-
tilingual pos taggers and np bracketers via robust pro-
jection across aligned corpora. In Proceedings of the
NAACL, pages 1?8.
David Yarowsky and Richard Wicentowski. 2000. Min-
imally supervised morphological analysis by multi-
modal alignment. In ACL ?00: Proceedings of the
38th Annual Meeting on Association for Computa-
tional Linguistics, pages 207?216, Morristown, NJ,
USA. Association for Computational Linguistics.
David Yarowsky, Grace Ngai, and Richard Wicentowski.
2000. Inducing multilingual text analysis tools via ro-
bust projection across aligned corpora. In Proceedings
of HLT, pages 161?168.
332
Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural
Language Learning, pages 332?343, Jeju Island, Korea, 12?14 July 2012. c?2012 Association for Computational Linguistics
Universal Grapheme-to-Phoneme Prediction Over Latin Alphabets
Young-Bum Kim and Benjamin Snyder
University of Wisconsin-Madison
{ybkim,bsnyder}@cs.wisc.edu
Abstract
We consider the problem of inducing
grapheme-to-phoneme mappings for un-
known languages written in a Latin alphabet.
First, we collect a data-set of 107 languages
with known grapheme-phoneme relationships,
along with a short text in each language. We
then cast our task in the framework of super-
vised learning, where each known language
serves as a training example, and predictions
are made on unknown languages. We induce
an undirected graphical model that learns
phonotactic regularities, thus relating textual
patterns to plausible phonemic interpretations
across the entire range of languages. Our
model correctly predicts grapheme-phoneme
pairs with over 88% F1-measure.
1 Introduction
Written language is one of the defining technologies
of human civilization, and has been independently
invented at least three times through the course of
history (Daniels and Bright, 1996). In many ways
written language reflects its more primary spoken
counterpart. Both are subject to some of the same
forces of change, including human migration, cul-
tural influence, and imposition by empire. In other
ways, written language harkens further to the past,
reflecting aspects of languages long since gone from
their spoken forms. In this paper, we argue that this
imperfect relationship between written symbol and
spoken sound can be automatically inferred from
textual patterns. By examining data for over 100
languages, we train a statistical model to automat-
ically relate graphemic patterns in text to phonemic
sequences for never-before-seen languages.
We focus here on the the alphabet, a writing sys-
tem that has come down to us from the Sumerians.
In an idealized alphabetic system, each phoneme
in the language is unambiguously represented by a
single grapheme. In practice of course, this ideal
is never achieved. When existing alphabets are
melded onto new languages, they must be imper-
fectly adapted to a new sound system. In this paper,
we exploit the fact that a single alphabet, that of the
Romans, has been adapted to a very large variety of
languages.
Recent research has demonstrated the effective-
ness of cross-lingual analysis. The joint analysis of
several languages can increase model accuracy, and
enable the development of computational tools for
languages with minimal linguistic resources. Previ-
ous work has focused on settings where just a hand-
ful of languages are available. We treat the task
of grapheme-to-phoneme analysis as a test case for
larger scale multilingual learning, harnessing infor-
mation from dozens of languages.
On a more practical note, accurately relating
graphemes and phonemes to one another is cru-
cial for tasks such as automatic speech recognition
and text-to-speech generation. While pronunciation
dictionaries and transcribed audio are available for
some languages, these resources are entirely lack-
ing for the vast majority of the world?s languages.
Thus, automatic and generic methods for determin-
ing sound-symbol relationships are needed.
Our paper is based on the following line of rea-
soning: that character-level textual patterns mirror
332
phonotactic regularities; that phonotactic regulari-
ties are shared across related languages and uni-
versally constrained; and that textual patterns for a
newly observed language may thus reveal its under-
lying phonemics. Our task can be viewed as an easy
case of lost language decipherment ? one where the
underlying alphabetic system is widely known.
Nevertheless, the task of grapheme-to-phoneme
prediction is challenging. Characters in the Roman
alphabet can take a wide range of phonemic values
across the world?s languages. For example, depend-
ing on the language, the grapheme ?c? can represent
the following phonemes:1
? /k/ (unvoiced velar plosive)
? /c/ (unvoiced palatal plosive)
? /s/ (unvoiced alveolar fricative)
? /|/ (dental click)
? />dZ/ (affricated voiced postalveolar fricative)
? />tS/ (affricated unvoiced postalveolar fricative)
? />ts/ (affricated unvoiced alveolar fricative)
To make matters worse, the same language may
use a single grapheme to ambiguously represent
multiple phonemes. For example, English orthog-
raphy uses ?c? to represent both /k/ and /s/. Our
task is thus to select a subset of phonemes for each
language?s graphemes. We cast the subset selec-
tion problem as a set of related binary prediction
problems, one for each possible grapheme-phoneme
pair. Taken together, these predictions yield the
grapheme-phoneme mapping for that language.
We develop a probabilistic undirected graphical
model for this prediction problem, where a large set
of languages serve as training data and a single held-
out language serves as test data. Each training and
test language yields an instance of the graph, bound
1For some brief background on phonetics, see Section 2.
Note that we use the term ?phoneme? throughout the paper,
though we also refer to ?phonetic? properties. As we are deal-
ing with texts (written in a roughly phonemic writing system),
we have no access to the true contextual phonetic realizations,
and even using IPA symbols to relate symbols across languages
is somewhat theoretically suspect.
together through a shared set of features and param-
eter values to allow cross-lingual learning and gen-
eralization.
In the graph corresponding to a given language,
each node represents a grapheme-phoneme pair
(g : p). The node is labeled with a binary value to
indicate whether grapheme g can represent phoneme
p in the language. In order to allow coupled label-
ings across the various grapheme-phoneme pairs of
the language, we employ a connected graph struc-
ture, with an automatically learned topology shared
across the languages. The node and edge features
are derived from textual co-occurrence statistics for
the graphemes of each language, as well as general
information about the language?s family and region.
Parameters are jointly optimized over the training
languages to maximize the likelihood of the node la-
belings given the observed feature values. See Fig-
ure 1 for a snippet of the model.
We apply our model to a novel data-set consisting
of grapheme-phoneme mappings for 107 languages
with Roman alphabets and short texts. In this set-
ting, we consider each language in turn as the test
language, and train our model on the remaining 106
languages. Our highest performing model achieves
an F1-measure of 88%, yielding perfect predictions
for over 21% of languages. These results compare
quite favorably to several baselines.
Our experiments lead to several conclusions. (i)
Character co-occurence features alone are not suf-
ficient for cross-lingual predictive accuracy in this
task. Instead, we map raw contextual counts to more
linguistically meaningful generalizations to learn ef-
fective cross-lingual patterns. (ii) A connected graph
topology is crucial for learning linguistically co-
herent grapheme-to-phoneme mappings. Without
any edges, our model yields perfect mappings for
only 10% of test languages. By employing struc-
ture learning and including the induced edges, we
more than double the number of test languages with
perfect predictions. (iii) Finally, an analysis of our
grapheme-phoneme predictions shows that they do
not achieve certain global characteristics observed
across true phoneme inventories. In particular, the
level of ?feature economy? in our predictions is too
low, suggesting an avenue for future research.
333
ph:/ph/
th:/th/
q:/!/
q:/k/x:/>ks/c:/k/w:/w/
c:/s/
Figure 1: A snippet of our undirected graphical model. The binary-valued nodes represent whether a particular
grapheme-phoneme pair is allowed by the language. Sparse edges are automatically induced to allow joint training
and prediction over related inventory decisions.
2 Background and Related Work
In this section, we provide some background on pho-
netics and phoneme inventories. We also review
prior work on grapheme-to-phoneme prediction and
multilingual modeling.
2.1 Phoneme Inventories
The sounds of the world?s languages are produced
through a wide variety of articulatory mechanisms.
Consonants are sounds produced through a partial
or complete stricture of the vocal tract, and can be
roughly categorized along three independent dimen-
sions: (i) Voicing: whether or not oscillation of the
vocal folds accompanies the sound. For example, /t/
and /d/ differ only in that the latter is voiced. (ii)
Place of Articulation: where in the anatomy of the
vocal tract the stricture is made. For example, /p/ is
a bilabial (the lips touching one another) while /k/
is a velar (tongue touching touching the soft palate).
(iii) Manner of Articulation: the manner in which
the airflow is regulated. For example, /m/ is a nasal
(air flowing through the nostrils), while /p/ is a plo-
sive (obstructed air suddenly released through the
mouth).
In contrast, vowels are voiced sounds produced
with an open vocal tract. They are categorized pri-
marily based on the position of the tongue and lips,
along three dimensions: (i) Roundedness: whether
or not the lips are rounded during production of
the sound; (ii) Height: the vertical position of the
tongue; (iii) Backness: how far forward the tongue
lies.
Linguists have noted several statistical regulari-
ties found in phoneme inventories throughout the
world. Feature economy refers to the idea that lan-
guages tend to minimize the number of differenti-
ating characteristics (e.g. different kinds of voic-
ing, manner, and place) that are used to distinguish
consonant phonemes from one another (Clements,
2003). In other words, once an articulatory feature
is used to mark off one phoneme from another, it will
likely be used again to differentiate other phoneme
pairs in the same language. The principle of Maxi-
mal perceptual contrast refers to the idea that the set
of vowels employed by a language will be located
in phonetic space to maximize their perceptual dis-
tances from one another, thus relieving the percep-
tual burden of the listener (Liljencrants and Lind-
blom, 1972). In an analysis of our results, we will
observe that our model?s predictions do not always
follow these principles.
Finally, researchers have noted that languages
exhibit set patterns in how they sequence their
phonemes (Kenstowicz and Kisseberth, 1979). Cer-
tain sequences are forbidden outright by languages,
while others are avoided or favored. While many
of these patterns are language-specific, others seem
more general, either reflecting anatomical con-
334
straints, common language ancestry, or universal as-
pects of the human language system. These phono-
tactic regularities and constraints are mirrored in
graphemic patterns, and as our experiments show,
can be explicitly modeled to achieve high accuracy
in our task.
2.2 Grapheme-to-Phoneme Prediction
Much prior work has gone into developing meth-
ods for accurate grapheme-to-phoneme prediction.
The common assumption underlying this research
has been that some sort of knowledge, usually in the
form of a pronunciation dictionary or phonemically
annotated text, is available for the language at hand.
The focus has been on developing techniques for
dealing with the phonemic ambiguity present both in
annotated and unseen words. For example, Jiampo-
jamarn and Kondrak (Jiampojamarn and Kondrak,
2010) develop a method for aligning pairs of writ-
ten and phonemically transcribed strings; Dwyer
and Kondrak (Dwyer and Kondrak, 2009) develop
a method for accurate letter-to-phoneme conversion
while minimizing the number of training examples;
Reddy and Goldsmith (Reddy and Goldsmith, 2010)
develop an MDL-based approach to finding sub-
word units that align well to phonemes.
A related line of work has grown around the task
of machine transliteration. In this task, the goal is to
automatically transliterate a name in one language
into the written form of another language. Often this
involves some level of phonetic analysis in one or
both languages. Notable recent work in this vein in-
cludes research by Sproat et alproat et al2006)
on transliteration between Chinese and English us-
ing comparable corpora, and Ravi and Knight (Ravi
and Knight, 2009) who take a decipherment ap-
proach to this problem.
Our work differs from all previous work on
grapheme-to-phoneme prediction in that (i) we as-
sume no knowledge for our target language beyond
a small unannotated text (and possibly some region
or language family information), and (ii) our goal
is to construct the inventory of mappings between
the language?s letters and its phonemes (the latter
of which we do not know ahead of time). When a
grapheme maps to more than one phoneme, we do
not attempt to disambiguate particular instances of
that grapheme in words.
A final thread of related work is the task of quan-
titatively categorizing writing systems according to
their levels of phonography and logography (Sproat,
2000; Penn and Choma, 2006). As our data-set
consists entirely of Latin-based writing systems, our
work can be viewed as a more fine-grained compu-
tational exploration of the space of writing systems,
with a focus on phonographic systems with the Latin
pedigree.
2.3 Multilingual Analysis
An influential thread of previous multilingual work
starts with the observation that rich linguistic re-
sources exist for some languages but not others.
The idea then is to project linguistic informa-
tion from one language onto others via parallel
data. Yarowsky and his collaborators first devel-
oped this idea and applied it to the problems of
part-of-speech tagging, noun-phrase bracketing, and
morphology induction (Yarowsky and Wicentowski,
2000; Yarowsky et al2000; Yarowsky and Ngai,
2001), and other researchers have applied the idea
to syntactic and semantic analysis (Hwa et al2005;
Pad? and Lapata, 2006) In these cases, the existence
of a bilingual parallel text along with highly accurate
predictions for one of the languages was assumed.
Another line of work assumes the existence of
bilingual parallel texts without the use of any super-
vision (Dagan et al1991; Resnik and Yarowsky,
1997). This idea has been developed and applied to
a wide variety tasks, including morphological anal-
ysis (Snyder and Barzilay, 2008a; Snyder and Barzi-
lay, 2008b), part-of-speech induction (Snyder et al
2008; Snyder et al2009a; Naseem et al2009), and
grammar induction (Snyder et al2009b; Blunsom
et al2009; Burkett et al2010). An even more re-
cent line of work does away with the assumption of
parallel texts and performs joint unsupervised induc-
tion for various languages through the use of cou-
pled priors in the context of grammar induction (Co-
hen and Smith, 2009; Berg-Kirkpatrick and Klein,
2010).
In contrast to these previous approaches, the
method we propose does not assume the existence
of any parallel text, but instead assumes that labeled
data exists for a wide variety of languages. In this re-
gard, our work most closely resembles recent work
which trains a universal morphological analyzer us-
335
phonemes #lang ent
a /a/ /5/ /A/ /@/ /2/ 106 1.25
c /c/ />dZ/ /k/ /s/ />ts/ />tS/ /|/ 62 2.33
ch /k/ />tS/ /x/ /S/ 39 1.35
e /e/ /i/ /?/ /@/ /E/ 106 1.82
h /-/ /h/ /x/ /?/ /H/ 85 1.24
i /i/ /j/ /I/ 106 0.92
j />dZ/ /h/ /j/ />tS/ /x/ /?/ /Z/ 79 2.05
o /o/ /u/ /6/ /0/ 103 1.47
ph /f/ /ph/ 15 0.64
q /k/ /q/ /!/ 32 1.04
r /r/ /?/ /R/ /?/ /K/ 95 1.50
th /th/ /T/ 15 0.64
u /u/ /w/ /y/ /1/ /U/ /Y/ 104 0.96
v /b/ /f/ /v/ /w/ /B/ 70 1.18
w /u/ /v/ /w/ 74 0.89
x />ks/ /x/ /{/ /S/ 44 1.31
z />dz/ /s/ />ts/ /z/ /T/ 72 0.93
Table 1: Ambiguous graphemes and the set of phonemes
that they may represent among our set of 107 languages.
ing a structured nearest neighbor approach for 8 lan-
guages (Kim et al2011). Our work extends this
idea to a new task and also considers a much larger
set of languages. As our results will indicate, we
found that a nearest neighbor approach was not as
effective as our proposed model-based approach.
3 Data and Features
In this section we discuss the data and features used
in our experiments.
3.1 Data
The data for our experiments comes from three
sources: (i) grapheme-phoneme mappings from an
online encyclopedia, (ii) translations of the Univer-
sal Declaration of Human Rights (UDHR)2, and (iii)
entries from the World Atlas of Language Structures
(WALS) (Haspelmath and Bibiko, 2005).
To start, we downloaded and transcribed im-
age files containing grapheme-phoneme mappings
for several hundred languages from an online en-
2http://www.ohchr.org/en/udhr/pages/introduction.aspx
cyclopedia of writing systems3. We then cross-
referenced the languages with the World Atlas
of Language Structures (WALS) database (Haspel-
math and Bibiko, 2005) as well as the translations
available for the Universal Declaration of Human
Rights (UDHR). Our final set of 107 languages in-
cludes those which appeared consistently in all three
sources and that employ a Latin alphabet. See Fig-
ure 2 for a world map annotated with the locations
listed in the WALS database for these languages, as
well as their language families. As seen from the fig-
ure, these languages cover a wide array of language
families and regions.
We then analyzed the phoneme inventories for the
107 languages. We decided to focus our attention
on graphemes which are widely used across these
languages with a diverse set of phonemic values.
We measured the ambiguity of each grapheme by
calculating the entropy of its phoneme sets across
the languages, and found that 17 graphemes had en-
tropy > 0.5 and appeared in at least 15 languages.
Table 1 lists these graphemes, the set of phonemes
that they can represent, the number of languages in
our data-set which employ them, and the entropy
of their phoneme-sets across these languages. The
data, along with the feature vectors discussed below,
are published as part of this paper.
3.2 Features
The key intuition underlying this work is that
graphemic patterns in text can reveal the phonemes
which they represent. A crucial step in operational-
izing this intuition lies in defining input features that
have cross-lingual predictive value. We divide our
feature set into three categories.
Text Context Features: These features represent
the textual environment of each grapheme in a lan-
guage. For each grapheme g, we consider counts of
graphemes to the immediate left and right of g in the
UDHR text. We define five feature templates, in-
cluding counts of (1) single graphemes to the left of
g, (2) single graphemes to the right of g, (3) pairs of
graphemes to the left of g, (4) pairs of graphemes to
the right of g, and (5) pairs of graphemes surround-
ing g. As our experiments below show, this set of
features on its own performs poorly. It seems that
3http://www.omniglot.com/writing/langalph.htm#latin
336
Figure 2: Map and language families of languages in our data-set
these features are too language specific and not ab-
stract enough to yield effective cross-lingual gener-
alization. Our next set of features was designed to
alleviate this problem.
Phonemic Context Features: A perfect feature-
set would depend on the entire set of grapheme-
to-phoneme predictions for a language. In other
words, we would ideally map all the graphemes in
our text to phonemes, and then consider the plau-
sibility of the resulting phoneme sequences. In
practice, of course, this is impossible, as the set
of possible grapheme-to-phoneme mappings is ex-
ponentially large. As an imperfect proxy for this
idea, we made the following observation: for most
Latin graphemes, the most common phonemic value
across languages is the identical IPA symbol of that
grapheme (e.g. the most common phoneme for g is
/g/, the most common phoneme for t is /t/, etc). Us-
ing this observation, we again consider all contexts
in which a grapheme appears, but this time map the
surrounding graphemes to their IPA phoneme equiv-
alents. We then consider various linguistic prop-
erties of these surrounding ?phonemes? ? whether
they are vowels or consonants, whether they are
voiced or not, their manner and places of articulation
? and create phonetic context features. The process
is illustrated in Figure 3. The intuition here is that
these features can (noisily) capture the phonotactic
context of a grapheme, allowing our model to learn
general phonotactic constraints. As our experiments
below demonstrate, these features proved to be quite
powerful.
Language Family Features: Finally, we consider
features drawn from the WALS database which
capture general information about the language ?
specifically, its region (e.g. Europe), its small lan-
guage family (e.g. Germanic), and its large language
family (e.g. Indo-European). These features al-
low our model to capture family and region specific
phonetic biases. For example, African languages
are more likely to use c and q to represents clicks
than are European languages. As we mention be-
low, we also consider conjunctions of all features.
Thus, a language family feature can combine with
a phonetic context feature to represent a family spe-
cific phonotactic constraint. Interestingly, our exper-
iments below show that these features are not needed
for highly accurate prediction.
3.3 Feature Discretization and Filtering
It is well known that many learning techniques per-
form best when continuous features are binned and
337
Raw Context Features 
 
L1:k =15 
L1:b=3 
L1:g=7 
 
 
Noisy IPA Conversion 
 
   L1:/k/ =15 
 L1:/b/=3 
 L1:/g/=7 
 
 
 
 
 
 
Phonetic Context Features 
 
L1:velar=22 
L1:bilabial=3 
L1:voiced=10 
L1:unvoiced=15 
L1:consonant=25 
 
 
 
Figure 3: Generating phonetic context features. First, character context features are extracted for each grapheme.
The features drawn here give the counts of the character to the immediate left of the grapheme. Next, the contextual
characters are noisily converted to phones using their IPA notation. Finally, phonetic context features are extracted. In
this case, phones /k/ and /g/ combine to give a ?velar? count of 22, while /g/ and /b/ combine to give a ?voiced? count
of 10.
converted to binary values (Dougherty et al1995).
As a preprocessing step, we therefore discretize and
filter the count-based features outlined above. We
adopt the technique of Recursive Minimal Entropy
Partitioning (Fayyad and Irani, 1993). This tech-
nique recursively partitions feature values so as to
minimize the conditional entropy of the labels. Par-
titioning stops when the gain in label entropy falls
below the number of additional bits in overhead
needed to describe the new feature split. This leads
to a (local) minimum description length discretiza-
tion.
We noticed that most of our raw features (espe-
cially the text features) could not achieve even a sin-
gle split point without increasing description length,
as they were not well correlated with the labels. We
decided to use this heuristic as a feature selection
technique, discarding such features. After this dis-
cretization and filtering, we took the resulting binary
features and added their pairwise conjunctions to the
set. This process was conducted separately for each
leave-one-out scenario, without observation of the
test language labels. Table 2 shows the total number
of features before the discretization/filtering as well
as the typical numbers of features obtained after fil-
tering (the exact numbers depend on the training/test
split).
4 Model
Using the features described above, we develop an
undirected graphical model approach to our predic-
Raw Filtered
# Text Features 28,474 1,848
# Phonemic Features 28,948 7,799
# Family Features 66 32
Total 57,488 9,679
Table 2: Number of features in each category before
and after discretization/filtering. Note that the pair-wise
conjunction features are not included in these counts.
tion task. Corresponding to each training language is
an instance of our undirected graph, labeled with its
true grapheme-phoneme mapping. We learn weights
over our features which optimally relate the input
features of the training languages to their observed
labels. At test-time, the learned weights are used to
predict the labeling of the held-out test language.
More formally, we assume a set of graph nodes
1, ...,m with edges between some pairs of nodes
(i, j). Each node corresponds to a grapheme-
phoneme pair (g : p) and can be labeled with a bi-
nary value. For each training language ?, we observe
a text x(?) and a binary labeling of the graph nodes
y(?). For each node i, we also obtain a feature vector
fi(x(?)), by examining the language?s text and ex-
tracting textual and noisy phonetic patterns (as de-
tailed in the previous section). We obtain similar
feature vectors for edges (i, j): gjk(x(?)). We then
parameterize the probability of each labeling using
a log-linear form over node and edge factors:4
4The delta function ?(p) evaluates to 1 when predicate p is
338
logP
(
y(?)|x(?)
)
=
?
i
?i ?
[
fi(x(?)) ?(y(?)i = 1)
]
+
?
j,k
?jk1 ?
[
gjk(x(?)) ?(y
(?)
j = 1 ? y
(?)
k = 1)
]
+
?
j,k
?jk2 ?
[
gjk(x(?)) ?(y
(?)
j = 1 ? y
(?)
k = 0)
]
+
?
j,k
?jk3 ?
[
gjk(x(?)) ?(y
(?)
j = 0 ? y
(?)
k = 1)
]
? logZ(x(?), ?)
The first term sums over nodes i in the graph. For
each i, we extract a feature vector fi(x(?)). If the
label of node i is 1, we take the dot product of the
feature vector and corresponding parameters, other-
wise the term is zeroed out. Likewise for the graph
edges j, k: we extract a feature vector, and depend-
ing on the labels of the two vertices yj and yk, take
a dot product with the relevant parameters. The final
term is a normalization constant to ensure that the
probabilities sum to one over all possible labelings
of the graph.
Before learning our parameters, we first automat-
ically induce the set of edges in our graph, using
the PC graph structure learning algorithm (Spirtes
et al2000). This procedure starts with a fully con-
nected undirected graph structure, and iteratively re-
moves edges between nodes that are conditionally
independent given other neighboring nodes in the
graph according to a statistical independence test
over all training languages. In our graphs we have
75 nodes, and thus 2,775 potential edges. Run-
ning the structure learning algorithm on our data
yields sparse graphs, typically consisting of about
50 edges. In each leave-one-out scenario, a single
structure is learned for all languages.
Once the graph structure has been induced, we
learn parameter values by maximizing the L2-
penalized conditional log-likelihood over all train-
ing languages:5
L(?) =
?
?
logP
(
y(?)|x(?)
)
? C||?||2
true, and to 0 when p is false.
5In our experiments, we used an L2 penalty weight of .5
for node features and .1 for edge features. Similar results are
observed for a wide range of values.
The gradient takes the standard form of a difference
between expected and observed feature counts (Laf-
ferty et al2001). Expected counts, as well as
predicted assignments at test-time, are computed
using loopy belief propagation (Murphy et al
1999). Numerical optimization is performed using
L-BFGS (Liu and Nocedal, 1989).
5 Experiments
In this section, we describe the set of experiments
performed to evaluate the performance of our model.
Besides our primary undirected graphical model, we
also consider several baselines and variants, in or-
der to assess the contribution of our model?s graph
structure as well as the features used. In all cases,
we perform leave-one-out cross-validation over the
107 languages in our data-set.
5.1 Baselines
Our baselines include:
1. A majority baseline, where the most common
binary value is chosen for each grapheme-
phoneme pair,
2. two linear SVM?s, one trained using the dis-
cretized and filtered features described in Sec-
tion 3.2, and the other using the raw continuous
features,
3. a Nearest Neighbor classifier, which chooses
the closest training language for each
grapheme-phoneme pair in the discretized
feature space, and predicts its label, and
4. a variant of our model with no edges between
nodes (essentially reducing to a set of indepen-
dent log-linear classifiers).
5.2 Evaluation
We report our results using three evaluation metrics
of increasing coarseness.
1. Phoneme-level: For individual grapheme-
phoneme pairs (e.g. a:/5/, a:/2/, c:/k/, c:/tS/)
our task consists of a set of binary predic-
tions, and can thus be evaluated in terms of
precision, recall, and F1-measure. We report
micro-averages of these quantities across all
339
Phoneme Grapheme Language
Precision Recall F1 Accuracy Accuracy
MAJORITY 80.47 57.47 67.06 55.54 2.8
SVM CONTINUOUS 79.87 64.48 79.87 59.07 3.74
SVM DISCRETE 90.55 78.27 83.97 70.78 8.41
NEAREST NEIGHBOR 85.35 79.43 82.28 67.97 2.8
MODEL: NO EDGES 89.35 82.05 85.54 73.96 10.28
FULL MODEL 91.06 83.98 87.37 78.58 21.5
MODEL: NO FAMILY 92.43 84.67 88.38 80.04 19.63
MODEL: NO TEXT 89.58 81.43 85.31 75.86 15.89
MODEL: NO PHONETIC 86.52 74.19 79.88 69.6 9.35
Table 3: The performance of baselines and variants of our model, evaluated at the phoneme-level (binary predictions),
whole-grapheme accuracy, and whole-language accuracy.
grapheme-phoneme pairs in all leave-one-out
test languages.
2. Grapheme-level: We also report grapheme-
level accuracy. For this metric, we con-
sider each grapheme g and examine its pre-
dicted labels over all its possible phonemes:
(g : p1), (g : p2), ..., (g : pk). If all k binary
predictions are correct, then the grapheme?s
phoneme-set has been correctly predicted. We
report the percentage of all graphemes with
such correct predictions (micro-averaged over
all graphemes in all test language scenarios).
3. Language-level: Finally, we assess language-
wide performance. For this metric, we re-
port the percentage of test languages for which
our model achieves perfect predictions on all
grapheme-phoneme pairs, yielding a perfect
mapping.
5.3 Results
The results for the baselines and our model are
shown in Table 3. The majority baseline yields 67%
F1-measure on the phoneme-level binary prediction
task, with 56% grapheme accuracy, and about 3%
language accuracy.
Using undiscretized raw count features, the SVM
improves phoneme-level performance to about 80%
F1, but fails to provide any improvement on
grapheme or language performance. In contrast, the
SVM using discretized and filtered features achieves
performance gains in all three categories, achieving
71% grapheme accuracy and 8% language accuracy.
The nearest neighbor baseline achieves performance
somewhere in between the two SVM variants.
The unconnected version of our model achieves
similar, though slightly improved performance over
the discretized SVM. Adding the automatically in-
duced edges into our model leads to significant
gains across all three categories. Phoneme-level
F1 reaches 87%, grapheme accuracy hits 79%, and
language accuracy more than doubles, achieving
22%. It is perhaps not surprising that the biggest
relative gains are seen at the language level: by
jointly learning and predicting an entire language?s
grapheme-phoneme inventory, our model ensures
that language-level coherence is maintained.
Recall that three sets of features are used by our
models. (1) language family and region features,
(2) textual context features, and (3) phonetic context
features. We now assess the relative merits of each
set by considering our model?s performance when
the set has been removed. Table 3 shows several
striking results from this experiment. First, it ap-
pears that dropping the region and language family
features actually improves performance. This result
is somewhat surprising, as we expected these fea-
tures to be quite informative. However, it appears
that whatever information they convey is redundant
when considering the text-based feature sets. We
next observe that dropping the textual context fea-
tures leads to a small drop in performance. Finally,
we see that dropping the phonetic context features
seriously degrades our model?s accuracy. Achieving
robust cross-linguistic generalization apparently re-
quires a level of feature abstraction not achieved by
340
character-level context features alone.
6 Global Inventory Analysis
In the previous section we saw that our model
achieves relatively high performance in predicting
grapheme-phoneme relationships for never-before-
seen languages. In this section we analyze the pre-
dicted phoneme inventories and ask whether they
display the statistical properties observed in the
gold-standard mappings.
As outlined in Section 2, consonant phonemes can
be represented by the three articulatory features of
voicing, manner, and place. The principle of fea-
ture economy states that phoneme inventories will
be organized to minimize the number of distinct ar-
ticulatory features used in the language, while max-
imizing the number of resulting phonemes. This
principle has several implications. First, we can
measure the economy index of a consonant system
by computing the ratio of the number of conso-
nantal phonemes to the number of articulatory fea-
tures used in their production: #consonants#features (Clements,
2003). The higher this value, the more economical
the sound system.
Secondly, for each articulatory dimension we can
calculate the empirical distribution over values ob-
served across the consonants of the language. Since
consonants are produced as combinations of the
three articulatory dimensions, the greatest number
of consonants (for a given set of utilized feature
values) will be produced when the distributions are
close to uniform. Thus, we can measure how eco-
nomical each feature dimension is by computing the
entropy of its distribution over consonants. For ex-
ample, in an economical system, we would expect
roughly half the consonants to be voiced, and half to
be unvoiced.
Table 4 shows the results of this analysis. First,
we notice that the average entropy of voiced vs. un-
voiced consonants is nearly identical in both cases,
close to the optimal value. However, when we ex-
amine the dimensions of place and manner, we no-
tice that the entropy induced by our model is not as
high as that of the true consonant inventories, imply-
ing a suboptimal allocation of consonants. In fact,
when we examine the economy index (ratio of con-
sonants to features), we indeed find that ? on aver-
H
(v
oi
ce
)
H
(p
la
ce
)
H
(m
an
ne
r)
Ec
on
om
y
In
de
x
True 0.9739 2.7355 2.4725 1.6536
Predicted 0.9733 2.6715 2.4163 1.6337
Table 4: Measures of feature economy applied to the pre-
dicted and true consonant inventories (averaged over all
107 languages).
age ? our model?s predictions are not as economi-
cal as the gold standard. This analysis suggests that
we might obtain a more powerful predictive model
by taking the principle of feature economy into ac-
count.
7 Conclusions
In this paper, we considered a novel problem: that
of automatically relating written symbols to spo-
ken sounds for an unknown language using a known
writing system ? the Latin alphabet. We constructed
a data-set consisting of grapheme-phoneme map-
pings and a short text for over 100 languages. This
data allows us to cast our problem in the supervised
learning framework, where each observed language
serves as a training example, and predictions are
made on a new language. Our model automatically
learns how to relate textual patterns of the unknown
language to plausible phonemic interpretations us-
ing induced phonotactic regularities.
Acknowledgments
This work is supported by the NSF under grant IIS-
1116676. Any opinions, findings, or conclusions are
those of the authors, and do not necessarily reflect
the views of the NSF.
References
Taylor Berg-Kirkpatrick and Dan Klein. 2010. Phyloge-
netic grammar induction. In Proceedings of the ACL,
pages 1288?1297, Uppsala, Sweden, July. Association
for Computational Linguistics.
P. Blunsom, T. Cohn, C. Dyer, and M. Osborne. 2009.
A gibbs sampler for phrasal synchronous grammar in-
duction. In Proceedings of the Joint Conference of the
341
47th Annual Meeting of the ACL and the 4th Interna-
tional Joint Conference on Natural Language Process-
ing of the AFNLP: Volume 2-Volume 2, pages 782?
790. Association for Computational Linguistics.
David Burkett, Slav Petrov, John Blitzer, and Dan Klein.
2010. Learning better monolingual models with unan-
notated bilingual text. In Proceedings of CoNLL.
G.N. Clements. 2003. Feature economy in sound sys-
tems. Phonology, 20(3):287?333.
Shay B. Cohen and Noah A. Smith. 2009. Shared lo-
gistic normal distributions for soft parameter tying in
unsupervised grammar induction. In Proceedings of
the NAACL/HLT.
Ido Dagan, Alon Itai, and Ulrike Schwall. 1991. Two
languages are more informative than one. In Proceed-
ings of the ACL, pages 130?137.
P.T. Daniels and W. Bright. 1996. The world?s writing
systems, volume 198. Oxford University Press New
York, NY.
James Dougherty, Ron Kohavi, and Mehran Sahami.
1995. Supervised and unsupervised discretization of
continuous features. In ICML, pages 194?202.
K. Dwyer and G. Kondrak. 2009. Reducing the anno-
tation effort for letter-to-phoneme conversion. In Pro-
ceedings of the Joint Conference of the 47th Annual
Meeting of the ACL and the 4th International Joint
Conference on Natural Language Processing of the
AFNLP: Volume 1-Volume 1, pages 127?135. Associ-
ation for Computational Linguistics.
Usama M Fayyad and Keki B Irani. 1993. Multi-interval
discretization of continuous-valued attributes for clas-
sification learning. In Proceedings of the International
Joint Conference on Uncertainty in AI, pages 1022?
1027.
M. Haspelmath and H.J. Bibiko. 2005. The world atlas
of language structures, volume 1. Oxford University
Press, USA.
R. Hwa, P. Resnik, A. Weinberg, C. Cabezas, and O. Ko-
lak. 2005. Bootstrapping parsers via syntactic projec-
tion across parallel texts. Journal of Natural Language
Engineering, 11(3):311?325.
S. Jiampojamarn and G. Kondrak. 2010. Letter-phoneme
alignment: An exploration. In Proceedings of the 48th
Annual Meeting of the Association for Computational
Linguistics, pages 780?788. Association for Computa-
tional Linguistics.
M.J. Kenstowicz and C.W. Kisseberth. 1979. Generative
phonology. Academic Press San Diego, CA.
Young-Bum Kim, Jo?o Gra?a, and Benjamin Snyder.
2011. Universal morphological analysis using struc-
tured nearest neighbor prediction. In Proceedings of
the 2011 Conference on Empirical Methods in Natu-
ral Language Processing, pages 322?332, Edinburgh,
Scotland, UK., July. Association for Computational
Linguistics.
John D. Lafferty, AndrewMcCallum, and Fernando C. N.
Pereira. 2001. Conditional random fields: Probabilis-
tic models for segmenting and labeling sequence data.
In Proceedings of the Eighteenth International Con-
ference on Machine Learning, pages 282?289.
J. Liljencrants and B. Lindblom. 1972. Numerical simu-
lation of vowel quality systems: the role of perceptual
contrast. Language, pages 839?862.
D.C. Liu and J. Nocedal. 1989. On the limited memory
bfgs method for large scale optimization. Mathemati-
cal programming, 45(1):503?528.
K.P. Murphy, Y. Weiss, and M.I. Jordan. 1999. Loopy
belief propagation for approximate inference: An em-
pirical study. In Proceedings of the Fifteenth confer-
ence on Uncertainty in artificial intelligence, pages
467?475. Morgan Kaufmann Publishers Inc.
Tahira Naseem, Benjamin Snyder, Jacob Eisenstein, and
Regina Barzilay. 2009. Multilingual part-of-speech
tagging: two unsupervised approaches. Journal of Ar-
tificial Intelligence Research, 36(1):341?385.
Sebastian Pad? and Mirella Lapata. 2006. Optimal con-
stituent alignment with edge covers for semantic pro-
jection. In Proceedings of ACL, pages 1161 ? 1168.
G. Penn and T. Choma. 2006. Quantitative methods for
classifying writing systems. In Proceedings of the Hu-
man Language Technology Conference of the NAACL,
Companion Volume: Short Papers, pages 117?120.
Association for Computational Linguistics.
S. Ravi and K. Knight. 2009. Learning phoneme map-
pings for transliteration without parallel data. In Pro-
ceedings of Human Language Technologies: The 2009
Annual Conference of the North American Chapter of
the Association for Computational Linguistics, pages
37?45. Association for Computational Linguistics.
S. Reddy and J. Goldsmith. 2010. An mdl-based ap-
proach to extracting subword units for grapheme-to-
phoneme conversion. In Human Language Technolo-
gies: The 2010 Annual Conference of the North Amer-
ican Chapter of the Association for Computational
Linguistics, pages 713?716. Association for Compu-
tational Linguistics.
Philip Resnik and David Yarowsky. 1997. A perspective
on word sense disambiguation methods and their eval-
uation. In Proceedings of the ACL SIGLEX Workshop
on Tagging Text with Lexical Semantics: Why, What,
and How?, pages 79?86.
B. Snyder and R. Barzilay. 2008a. Unsupervised multi-
lingual learning for morphological segmentation. Pro-
ceedings of ACL-08: HLT, pages 737?745.
Benjamin Snyder and Regina Barzilay. 2008b. Cross-
lingual propagation for morphological analysis. In
Proceedings of the AAAI, pages 848?854.
342
Benjamin Snyder, Tahira Naseem, Jacob Eisenstein, and
Regina Barzilay. 2008. Unsupervised multilingual
learning for POS tagging. In Proceedings of EMNLP,
pages 1041?1050.
B. Snyder, T. Naseem, J. Eisenstein, and R. Barzilay.
2009a. Adding more languages improves unsuper-
vised multilingual part-of-speech tagging: A bayesian
non-parametric approach. In Proceedings of Human
Language Technologies: The 2009 Annual Conference
of the North American Chapter of the Association for
Computational Linguistics, pages 83?91. Association
for Computational Linguistics.
Benjamin Snyder, Tahira Naseem, and Regina Barzilay.
2009b. Unsupervised multilingual grammar induc-
tion. In Proceedings of the ACL, pages 73?81.
P. Spirtes, C.N. Glymour, and R. Scheines. 2000. Cau-
sation, prediction, and search, volume 81. The MIT
Press.
R. Sproat, T. Tao, and C.X. Zhai. 2006. Named entity
transliteration with comparable corpora. In Proceed-
ings of the 21st International Conference on Compu-
tational Linguistics and the 44th annual meeting of the
Association for Computational Linguistics, pages 73?
80. Association for Computational Linguistics.
R.W. Sproat. 2000. A computational theory of writing
systems. Cambridge Univ Pr.
David Yarowsky and Grace Ngai. 2001. Inducing mul-
tilingual pos taggers and np bracketers via robust pro-
jection across aligned corpora. In Proceedings of the
NAACL, pages 1?8.
David Yarowsky and Richard Wicentowski. 2000. Min-
imally supervised morphological analysis by multi-
modal alignment. In ACL ?00: Proceedings of the
38th Annual Meeting on Association for Computa-
tional Linguistics, pages 207?216, Morristown, NJ,
USA. Association for Computational Linguistics.
David Yarowsky, Grace Ngai, and Richard Wicentowski.
2000. Inducing multilingual text analysis tools via ro-
bust projection across aligned corpora. In Proceedings
of HLT, pages 161?168.
343
Proceedings of NAACL-HLT 2013, pages 1196?1205,
Atlanta, Georgia, 9?14 June 2013. c?2013 Association for Computational Linguistics
Optimal Data Set Selection: An Application to Grapheme-to-Phoneme
Conversion
Young-Bum Kim and Benjamin Snyder
University of Wisconsin-Madison
{ybkim,bsnyder}@cs.wisc.edu
Abstract
In this paper we introduce the task of unla-
beled, optimal, data set selection. Given a
large pool of unlabeled examples, our goal is
to select a small subset to label, which will
yield a high performance supervised model
over the entire data set. Our first proposed
method, based on the rank-revealing QR ma-
trix factorization, selects a subset of words
which span the entire word-space effectively.
For our second method, we develop the con-
cept of feature coverage which we optimize
with a greedy algorithm. We apply these
methods to the task of grapheme-to-phoneme
prediction. Experiments over a data-set of 8
languages show that in all scenarios, our selec-
tion methods are effective at yielding a small,
but optimal set of labelled examples. When
fed into a state-of-the-art supervised model for
grapheme-to-phoneme prediction, our meth-
ods yield average error reductions of 20% over
randomly selected examples.
1 Introduction
Over the last 15 years, supervised statistical learning
has become the dominant paradigm for building nat-
ural language technologies. While the accuracy of
supervised models can be high, expertly annotated
data sets exist for a small fraction of possible tasks,
genres, and languages. The would-be tool builder
is thus often faced with the prospect of annotating
data, using crowd-sourcing or domain experts. With
limited time and budget, the amount of data to be an-
notated might be small, especially in the prototyping
stage, when the exact specification of the prediction
task may still be in flux, and rapid prototypes are
desired.
In this paper, we propose the problem of unsuper-
vised, optimal data set selection. Formally, given
a large set X of n unlabeled examples, we must
select a subset S ? X of size k  n to label.
Our goal is to select such a subset which, when
labeled, will yield a high performance supervised
model over the entire data set X . This task can be
thought of as a zero-stage version of active learn-
ing: we must choose a single batch of examples to
label, without the benefit of any prior labelled data
points. This problem definition avoids the practical
complexity of the active learning set-up (many it-
erations of learning and labeling), and ensures that
the labeled examples are not tied to one particular
model class or task, a well-known danger of active
learning (Settles, 2010). Alternatively, our methods
may be used to create the initial seed set for the ac-
tive learner.
Our initial testbed for optimal data set selec-
tion is the task of grapheme-to-phoneme conver-
sion. In this task, we are given an out-of-vocabulary
word, with the goal of predicting a sequence of
phonemes corresponding to its pronunciation. As
training data, we are given a pronunciation dic-
tionary listing words alongside corresponding se-
quences of phones, representing canonical pronun-
ciations of those words. Such dictionaries are used
as the final bridge between written and spoken lan-
guage for technologies that span this divide, such as
speech recognition, text-to-speech generation, and
speech-to-speech language translation. These dic-
tionaries are necessary: the pronunciation of words
1196
continues to evolve after their written form has been
fixed, leading to a large number of rules and ir-
regularities. While large pronunciation dictionaries
of over 100,000 words exist for several major lan-
guages, these resources are entirely lacking for the
majority of the world?s languages. Our goal is to
automatically select a small but optimal subset of
words to be annotated with pronunciation data.
The main intuition behind our approach is that
the subset of selected data points should efficiently
cover the range of phenomena most commonly ob-
served across the pool of unlabeled examples. We
consider two methods. The first comes from a line
of research initiated by the numerical linear algebra
community (Golub, 1965) and taken up by computer
science theoreticians (Boutsidis et al, 2009), with
the name COLUMN SUBSET SELECTION PROBLEM
(CSSP). Given a matrixA, the goal of CSSP is to se-
lect a subset of k columns whose span most closely
captures the range of the full matrix. In particu-
lar, the matrix A? formed by orthogonally project-
ing A onto the k-dimensional space spanned by the
selected columns should be a good approximation
to A. By defining AT to be our data matrix, whose
rows correspond to words and whose columns corre-
spond to features (character 4-grams), we can apply
the CSSP randomized algorithm of (Boutsidis et al,
2009) on A to obtain a subset of k words which best
span the entire space of words.
Our second approach is based on a notion of fea-
ture coverage. We assume that the benefit of seeing
a feature f in a selected word bears some positive
relationship to the frequency of f in the unlabeled
pool. However, we further assume that the lion?s
share of benefit accrues the first few times that we
label a word with feature f , with the marginal util-
ity quickly tapering off as more such examples have
been labeled. We formalize this notion and provide
an exact greedy algorithm for selecting the k data
points with maximal feature coverage.
To assess the benefit of these methods, we ap-
ply them to a suite of 8 languages with pronunci-
ation dictionaries. We consider ranges from 500
to 2000 selected words and train a start-of-the-art
grapheme-to-phoneme prediction model (Bisani and
Ney, 2008). Our experiments show that both meth-
ods produce significant improvements in prediction
quality over randomly selected words, with our fea-
ture coverage method consistently outperforming
the randomized CSSP algorithm. Over the 8 lan-
guages, our method produces average reductions in
error of 20%.
2 Background
Grapheme-to-phoneme Prediction The task of
grapheme-to-phoneme conversion has been consid-
ered in a variety of frameworks, including neural
networks (Sejnowski and Rosenberg, 1987), rule-
based FSA?s (Kaplan and Kay, 1994), and pronun-
ciation by analogy (Marchand and Damper, 2000).
Our goal here is not to compare these methods, so
we focus on the probabilistic joint-sequence model
of Bisani and Ney (2008). This model defines a
joint distribution over a grapheme sequence g ? G?
and a phoneme sequence ? ? ??, by way of an
unobserved co-segmentation sequence q. Each co-
segmentation unit qi is called a graphone and con-
sists of an aligned pair of zero or one graphemes and
zero or one phonemes: qi ? G?{}???{}.1 The
probability of a joint grapheme-phoneme sequence
is then obtained by summing over all possible co-
segmentations:
P (g,?) =
?
q?S(g,?)
P (q)
where S(g,?) denotes the set of all graphone se-
quences which yield g and ?. The probability of a
graphone sequence of length K is defined using an
h-order Markov model with multinomial transitions:
P (q) =
k+1?
i=1
P (qi|qi?h, . . . , qi?1)
where special start and end symbols are assumed for
qj<1 and qk+1, respectively.
To deal with the unobserved co-segmentation se-
quences, the authors develop an EM training regime
that avoids overfitting using a variety of smoothing
and initialization techniques. Their model produces
state-of-the-art or comparable accuracies across a
1The model generalizes easily to graphones consisting of
more than one grapheme or phoneme, but in both (Bisani and
Ney, 2008) and our initial experiments we found that the 01-to-
01 model always performed best.
1197
wide range of languages and data sets.2 We use the
publicly available code provided by the authors.3 In
all our experiments we set h = 4 (i.e. a 5-gram
model), as we found that accuracy tended to be flat
for h > 4.
Active Learning for G2P Perhaps most closely
related to our work are the papers of Kominek and
Black (2006) and Dwyer and Kondrak (2009), both
of which use active learning to efficiently bootstrap
pronunciation dictionaries. In the former, the au-
thors develop an active learning word selection strat-
egy for inducing pronunciation rules. In fact, their
greedy n-gram selection strategy shares some of
the some intuition as our second data set selection
method, but they were unable to achieve any accu-
racy gains over randomly selected words without ac-
tive learning.
Dwyer and Kondrak use a Query-by-Bagging
active learning strategy over decision tree learn-
ers. They find that their active learning strategy
produces higher accuracy across 5 of the 6 lan-
guages that they explored (English being the ex-
ception). They extract further performance gains
through various refinements to their model. Even
so, we found that the Bisani and Ney grapheme-to-
phoneme (G2P) model (Bisani and Ney, 2008) al-
ways achieved higher accuracy, even when trained
on random words. Furthermore, the relative gains
that we observe using our optimal data set selection
strategies (without any active learning) are much
larger than the relative gains of active learning found
in their study.
Data Set Selection and Active Learning
Eck et al(2005) developed a method for train-
ing compact Machine Translation systems by
selecting a subset of sentences with high n-gram
coverage. Their selection criterion essentially cor-
responds to our feature coverage selection method
using coverage function cov2 (see Section 3.2). As
our results will show, the use of a geometric feature
discount (cov3) provided better results in our task.
Otherwise, we are not aware of previous work
2We note that the discriminative model of Jiampojamarn and
Kondrak (2010) outperforms the Bisani and Ney model by an
average of about 0.75 percentage points across five data sets.
3http://www-i6.informatik.rwth-aachen.
de/web/Software/g2p.html
proposing optimal data set selection as a general re-
search problem. Of course, active learning strategies
can be employed for this task by starting with a small
random seed of examples and incrementally adding
small batches. Unfortunately, this can lead to data-
sets that are biased to work well for one particular
class of models and task, but may otherwise perform
worse than a random set of examples (Settles, 2010,
Section 6.6). Furthermore the active learning set-
up can be prohibitively tedious and slow. To illus-
trate, Dwyer and Kondrak (2009) used 190 iterations
of active learning to arrive at 2,000 words. Each
iteration involves bootstrapping 10 different sam-
ples, and training 10 corresponding learners. Thus,
in total, the underlying prediction model is trained
1,900 times. In contrast, our selection methods are
fast, can select any number of data points in a sin-
gle step, and are not tied to a particular prediction
task or model. Furthermore, these methods can be
combined with active learning in selecting the initial
seed set.
Unsupervised Feature Selection Finally, we note
that CSSP and related spectral methods have been
applied to the problem of unsupervised feature se-
lection (Stoppiglia et al, 2003; Mao, 2005; Wolf and
Shashua, 2005; Zhao and Liu, 2007; Boutsidis et al,
2008). These methods are related to dimensionality
reduction techniques such as Principal Components
Analysis (PCA), but instead of truncating features in
the eigenbasis representation (where each feature is
a linear combination of all the original features), the
goal is to remove dimensions in the standard basis,
leading to a compact set of interpretable features. As
long as the discarded features can be well approxi-
mated by a (linear) function of the selected features,
the loss of information will be minimal.
Our first method for optimal data-set creation ap-
plies a randomized CSSP approach to the transpose
of the data matrix, AT . Equivalently, it selects the
optimal k rows ofA for embedding the full set of un-
labeled examples. We use a recently developed ran-
domized algorithm (Boutsidis et al, 2009), and an
underlying rank-revealing QR factorization (Golub,
1965).
1198
(a) (b) (c)
Figure 1: Various versions of the feature coverage function. Panel (a) shows cov1 (Equation 5). Panel (b) shows cov2
(Equation 6). Panel (c) shows cov3 (Equation 7) with discount factor ? = 1.2.
3 Two Methods for Optimal Data Set
Selection
In this section we detail our two proposed methods
for optimal data set selection. The key intuition is
that we would like to pick a subset of data points
which broadly and efficiently cover the features of
the full range of data points. We assume a large pool
X of n unlabeled examples, and our goal is to se-
lect a subset S ? X of size k  n for labeling.
We assume that each data point x ? X is a vec-
tor of m feature values. Our first method applies to
any real or complex feature space, while our second
method is specialized for binary features. We will
use the (n ? m) matrix A to denote our unlabeled
data: each row is a data point and each column is
a feature. In all our experiments, we used the pres-
ence (1) or absence (0) of each character 4-gram as
our set of features.
3.1 Method 1: Row Subset Selection
To motivate this method, first consider the task of
finding a rank k approximation to the data matrix A.
The SVD decomposition yields:
A = U?V T
? U is (n ? n) orthogonal and its columns form
the eigenvectors of AAT
? V is (m?m) orthogonal and its columns form
the eigenvectors of ATA
? ? is (n?m) diagonal, and its diagonal entries
are the singular values ofA (the square roots of
the eigenvalues of both AAT and ATA).
To obtain a rank k approximation to A, we start by
rewriting the SVD decomposition as a sum:
A =
??
i=1
?iuivTi (1)
where ? = min(m,n), ?i is the ith diagonal entry of
?, ui is the ith column of U , and vi is the ith column
of V . To obtain a rank k approximation to A, we
simply truncate the sum in equation 1 to its first k
terms, yielding Ak. To evaluate the quality of this
approximation, we can measure the Frobenius norm
of the residual matrix ||A ? Ak||F .4 The Eckart-
Young theorem (Eckart and Young, 1936) states that
Ak is optimal in the following sense:
Ak = argmin
A? s.t. rank(A?)=k
||A? A?||F (2)
In other words, truncated SVD gives the best rank
k approximation to A in terms of minimizing the
Frobenius norm of the residual matrix. In CSSP,
the goal is similar, with the added constraint that the
approximation to A must be obtained by projecting
onto the subspace spanned by a k-subset of the orig-
inal rows of A.5 Formally, the goal is to produce a
(k ?m) matrix S formed from rows of A, such that
||A?AS+S||F (3)
4The Frobenius norm ||M ||F is defined as the entry-wise L2
norm:
??
i,j m
2
ij
5Though usually framed in terms of column selection, we
switch to row selection here as our goal is to select data points
rather than features.
1199
is minimized over all
(n
k
)
possible choices for S.
Here S+ is the (m ? k) Moore-Penrose pseudo-
inverse of S, and S+S gives the orthogonal projec-
tor onto the rowspace of S. In other words, our goal
is to select k data points which serve as a good ap-
proximate basis for all the data points. Since AS+S
can be at most rank k, the constraint considered here
is stricter than that of Equation 1, so the truncated
SVD Ak gives a lower bound on the residual.
Boutsidis et al(2009) develop a randomized algo-
rithm that produces a submatrix S (consisting of k
rows of A) which, with high probability, achieves a
residual bound of:
||A?AS+S||F ? O(k
?
log k)||A?Ak||F (4)
in running time O(min{mn2,m2n}). The algo-
rithm proceeds in three steps: first by computing the
SVD of A, then by randomly sampling O(k log k)
rows of A with importance weights carefully com-
puted from the SVD, and then applying a determin-
istic rank-revealing QR factorization (Golub, 1965)
to select k of the sampled rows. To give some in-
tuition, we now provide some background on rank
revealing factorizations.
Rank revealing QR / LQ (RRQR) Every real
(n?m) matrix can be factored asA = LQ, whereQ
is (m?m) orthogonal and L is (n?m) lower trian-
gular.6 It is important to notice that in this triangular
factorization, each successive row of A introduces
exactly one new basis vector from Q. We can thus
represent row i as a linear combination of the first
i? 1 rows along with the ith row of Q.
A rank-revealing factorization is one which dis-
plays the numerical rank of the matrix ? defined to
be the singular value index r such that
?r  ?r+1 = O()
for machine precision . In the case of the LQ
factorization, our goal is to order the rows of A
such that each successive row has decreasing rep-
resentational importance as a basis for the future
rows. More formally, If there exists a row permu-
tation ? such that ?A has a triangular factorization
6We replace the standard upper triangular QR factorization
with an equivalent lower triangular factorization LQ to focus
intuition on the rowspace of A.
Language Training Test Total
Dutch 11,622 104,589 116,211
English 11209 100891 112100
French 2,748 24,721 27,469
Frisian 6,198 55,778 61,976
German 4,942 44,460 49,402
Italian 7,529 79,133 86,662
Norwegian 4,172 37,541 41,713
Spanish 3,150 28,341 31,491
Table 1: Pronunciation dictionary size for each of the lan-
guages.
?A = LQ with L =
[
L11 0
L21 L22
]
, where the small-
est singular value of L11 is much greater than the
spectral norm of L22, which is itself almost zero:
?min(L11) ||L22||2 = O()
then we say that ?A = LQ is a rank-revealing LQ
factorization. Both L11 and L22 will be lower tri-
angular matrices and if L11 is (r ? r) then A has
numerical rank r (Hong and Pan, 1992).
Implementation In our implementation of the
CSSP algorithm, we first prune away 4-gram fea-
tures that appear in fewer than 3 words, then com-
pute the SVD of the pruned data matrix using
the PROPACK package,7 which efficiently handles
sparse matrixes. After sampling k log k words from
A (with sampling weights calculated from the top-k
singular vectors), we form a submatrix B consist-
ing of the sampled words. We then use the RRQR
implementation from ACM Algorithm 782 (Bischof
and Quintana-Ort??, 1998) (routine DGEQPX) to
compute ?B = LQ. We finally select the first k
rows of ?B as our optimal data set. Even for our
largest data sets (English and Dutch), this entire pro-
cedure runs in less than an hour on a 3.4Ghz quad-
core i7 desktop with 32 GB of RAM.
3.2 Method 2: Feature Coverage Maximization
In our previous approach, we adopted a general
method for approximating a matrix with a subset of
rows (or columns). Here we develop a novel objec-
tive function with the specific aim of optimal data set
selection. Our key assumption is that the benefit of
7http://soi.stanford.edu/?rmunk/PROPACK/
1200
seeing a new feature f in a selected data point bears
a positive relationship to the frequency of f in the
unlabeled pool of words. However, we further as-
sume that the lion?s share of benefit accrues quickly,
with the marginal utility quickly tapering off as we
label more and more examples with feature f . Note
that for this method, we assume a boolean feature
space.
To formalize this intuition, we will define the cov-
erage of a selected (k ?m) submatrix S consisting
of rows ofA, with respect to a feature index j. For il-
lustration purposes, we will list three alternative def-
initions:
cov1(S; j) = ||sj ||1 (5)
cov2(S; j) = ||aj ||1 I
(
||sj ||1 > 0
)
(6)
cov3(S; j) = ||aj ||1 ?
||aj ||1
?||sj ||1
I
(
||sj ||1 < ||aj ||1
)
(7)
In all cases, sj refers the jth column of S, aj refers
the jth column of A, I(?) is a 0-1 indicator function,
and ? is a scalar discount factor.8
Figure 1 provides an intuitive explanation of these
functions: cov1 simply counts the number of se-
lected data points with boolean feature j. Thus, full
coverage (||aj ||: the entire number of data points
with the feature) is only achieved when all data
points with the feature are selected. cov2 lies at the
opposite extreme. Even a single selected data point
with feature j triggers coverage of the entire feature.
Finally, cov3 is designed so that the coverage scales
monotonically as additional data points with feature
j are selected. The first selected data point will cap-
ture all but 1? of the total coverage, and each further
selected data point will capture all but 1? of what-
ever coverage remains. Essentially, the coverage for
a feature scales as a geometric series in the number
of selected examples having that feature.
To ensure that the total coverage (?|aj ||1) is
achieved when all the data points are selected, we
add an indicator function for the case of ||cj ||1 =
||aj ||1 .9
8Chosen to be 5 in all our experiments. We experimented
with several values between 2 and 10, without significant dif-
ferences in results.
9Otherwise, the geometric coverage function would con-
verge to ||aj || only as ||cj || ? ?.
500 Words 2000 Words
RAND CSSP FEAT RAND CSSP FEAT
Dut 48.2 50.8 59.3 69.8 75.0 77.8
Eng 25.4 26.5 29.5 40.3 40.1 42.8
Fra 66.9 69.2 72.1 81.2 82.0 84.8
Fri 42.7 48.0 53.6 62.2 65.3 68.5
Ger 55.2 58.6 65.0 74.2 78.6 80.8
Ita 80.6 82.8 82.8 85.3 86.1 86.8
Nor 48.1 49.5 55.0 66.1 69.9 71.6
Spa 90.7 96.8 95.0 98.1 98.4 99.0
avg 57.2 60.3 64.0 72.2 74.4 76.5
Table 2: Test word accuracy across the 8 languages for
randomly selected words (RAND), CSSP matrix subset
selection (CSSP), and Feature Coverage Maximization
(FEAT). We show results for 500 and 2000 word train-
ing sets.
Setting our feature coverage function to cov3, we
can now define the overall feature coverage of the
selected points as:
coverage(S) =
1
||A||1
?
j
cov3(S; j) (8)
where ||A||1 is the L1 entrywise matrix norm,?
i,j |Aij |, which ensures that 0 ? coverage(S) ?
1 with equality only achieved when S = A, i.e.
when all data points have been selected.
We provide a brief sketch of our optimization al-
gorithm: To pick the subset S of k words which
optimizes Equation 8, we incrementally build opti-
mal subsets S? ? S of size k? < k. At each stage,
we keep track of the unclaimed coverage associated
with each feature j:
unclaimed(j) = ||aj ||1 ? cov3(S?; j)
To add a new word, we scan through the pool of re-
maining words, and calculate the additional cover-
age that selecting word w would achieve:
?(w) =
?
feature j in w
unclaimed(j)
(
? ? 1
?
)
We greedily select the word which adds the most
coverage, remove it from the pool, and update the
unclaimed feature coverages. It is easy to show that
this greedy algorithm is globally optimal.
1201
500 1000 1500 200025
30
35
40
English
500 1000 1500 2000
70
75
80
85 French
500 1000 1500 200090
92
94
96
98
Spanish
500 1000 1500 200080
8182
8384
8586
87 Italian
500 1000 1500 2000
55
60
65
70
75
80
German
500 1000 1500 2000
50
55
60
65
70
Norwegian
500 1000 1500 2000
5055
6065
7075
80 Dutch
500 1000 1500 2000
45
50
55
60
65
70 Frisian
Feat Coverage RRQR Random
Figure 2: Test word accuracy across the 8 languages for (1) feature coverage, (2) CSSP matrix subset selection, (3)
and randomly selected words.
4 Experiments and Analysis
To test the effectiveness of the two proposed data
set selection methods, we conduct grapheme-to-
phoneme prediction experiments across a test suite
of 8 languages: Dutch, English, French, Frisian,
German, Italian, Norwegian, and Spanish. The data
was obtained from the PASCAL Letter-to-Phoneme
Conversion Challenge,10 and was processed to
match the setup of Dwyer and Kondrak (2009).
The data comes from a range of sources, includ-
ing CELEX for Dutch and German (Baayen et al,
1995), BRULEX for French (Mousty et al, 1990),
CMUDict for English,11 the Italian Festival Dictio-
nary (Cosi et al, 2000), as well as pronunciation dic-
tionaries for Spanish, Norwegian, and Frisian (orig-
inal provenance not clear).
As Table 1 shows, the size of the dictionaries
ranges from 31,491 words (Spanish) up to 116,211
words (Dutch). We follow the PASCAL challenge
training and test folds, treating the training set as our
pool of words to be selected for labeling.
Results We consider training subsets of sizes 500,
1000, 1500, and 2000. For our baseline, we train the
10http://pascallin.ecs.soton.ac.uk/
Challenges/PRONALSYL/
11http://www.speech.cs.cmu.edu/cgi-bin/
cmudict
G2P model (Bisani and Ney, 2008) on randomly se-
lected words of each size, and average the results
over 10 runs. We follow the same procedure for
our two data set selection methods. Figure 2 plots
the word prediction accuracy for all three meth-
ods across the eight languages with varying training
sizes, while Table 2 provides corresponding numer-
ical results. We see that in all scenarios the two data
set selection strategies fare better than random sub-
sets of words.
In all but one case, the feature coverage method
yields the best performance (with the exception of
Spanish trained with 500 words, where the CSSP
yields the best results). Feature coverage achieves
average error reduction of 20% over the randomly
selected training words across the different lan-
guages and training set sizes.
Coverage variants We also experimented with
the other versions of the feature coverage function
discussed in Section 3.2 (see Figure 1). While cov1
tended to perform quite poorly (usually worse than
random), cov2 ? which gives full credit for each
feature the first time it is seen ? yields results just
slightly worse than the CSSP matrix method on av-
erage, and always better than random. In the 2000
word scenario, for example, cov2 achieves average
accuracy of 74.0, just a bit below the 74.4 accuracy
of the CSSP method. It is also possible that more
1202
RAND CSSP FEAT SVD
Fra 0.66 0.62 0.65 0.51
Fry 0.75 0.72 0.75 0.6
Ger 0.71 0.67 0.71 0.55
Ita 0.64 0.61 0.67 0.49
Nor 0.7 0.61 0.64 0.5
Spa 0.65 0.67 0.68 0.53
avg 0.69 0.65 0.68 0.53
Table 3: Residual matrix norm across 6 languages for
randomly selected words (RAND), CSSP matrix subset
selection (CSSP), feature coverage maximization (FEAT),
and the rank k SVD (SVD). Lower is better.
RAND CSSP FEAT
Dut 0.66 0.72 0.81
Eng 0.52 0.58 0.69
Fra 0.68 0.74 0.81
Fry 0.7 0.79 0.84
Ger 0.68 0.74 0.81
Ita 0.79 0.84 0.9
Nor 0.7 0.79 0.84
Spa 0.67 0.75 0.8
avg 0.68 0.74 0.81
Table 4: Feature coverage across the 8 languages for ran-
domly selected words (RAND), CSSP matrix subset selec-
tion (CSSP), and feature coverage maximization (FEAT).
Higher is better.
careful tuning of the discount factor ? of cov3 would
yield further gains.
Optimization Analysis Both the CSSP and fea-
ture coverage methods have clearly defined objec-
tive functions ? formulated in Equations 3 and 8,
respectively. We can therefore ask how well each
methods fares in optimizing either one of the two
objectives.
First we consider the objective of the CSSP al-
gorithm: to find k data points which can accurately
embed the entire data matrix. Once the data points
are selected, we compute the orthogonal projection
of the data matrix onto the submatrix, obtaining an
approximation matrix A?. We can then measure the
residual norm as a fraction of the original matrix
norm:
||A? A?||F
||A||F
(9)
As noted in Section 3.1, truncated SVD minimizes
the residual over all rank k matrices, so we can com-
CSSP FEAT FEAT-SLS
fettered internationalization rating
exceptionally underestimating overs
gellert schellinger nation
daughtry barristers scherman
blowed constellations olinger
harmonium complementing anderson
cassini bergerman inter
rupees characteristically stated
tewksbury heatherington press
ley overstated conner
Table 5: Top 10 words selected by CSSP, feature cov-
erage (FEAT), and feature coverage with stratified length
sampling (FEAT-SLS)
pare our three methods ? random selections, CSSP,
and feature coverage ? all of which select k exam-
ples as a basis, against the lower bound given by
SVD. Table 3 shows the result of this analysis for
k = 2000 (Note that we were unable to compute
the projection matrices for English and Dutch due
to the size of the data and memory limitations). As
expected, SVD fares the best, with CSSP as a some-
what distant second. On average, feature coverage
seems to do a bit better than random.
A similar analysis for the feature coverage objec-
tive function is shown in Table 4. Unsurprisingly,
this objective is best optimized by the feature cov-
erage method. Interestingly though, CSSP seems
to perform about halfway between random and the
feature coverage method. This makes some sense,
as good basis data points will tend to have frequent
features, while at the same time being maximally
spread out from one another. We also note that
the poor coverage result for English in Table 4 mir-
rors its overall poor performance in the G2P predic-
tion task ? not only are the phoneme labels unpre-
dictable, but the input data itself is wild and hard to
compress.
Stratified length sampling As Table 5 shows,
the top 10 words selected by the feature coverage
method are mostly long and unusual, averaging 13.3
characters in length. In light of the potential an-
notation burden, we developed a stratified sampling
strategy to ensure typical word lengths. Before se-
lecting each new word, we first sample a word length
according to the empirical word length distribution.
We then choose among words of the sampled length
1203
according to the feature coverage criterion. This re-
sults in more typical words of average length, with
only a very small drop in performance.
5 Conclusion and Future Work
In this paper we proposed the task of optimal data
set selection in the unsupervised setting. In contrast
to active learning, our methods do not require re-
peated training of multiple models and iterative an-
notations. Since the methods are unsupervised, they
also avoid tying the selected data set to a particular
model class (or even task).
We proposed two methods for optimally select-
ing a small subset of examples for labeling. The
first uses techniques developed by the numerical lin-
ear algebra and theory communities for approximat-
ing matrices with subsets of columns or rows. For
our second method, we developed a novel notion
of feature coverage. Experiments on the task of
grapheme-to-phoneme prediction across eight lan-
guages show that our method yields performance
improvements in all scenarios, averaging 20% re-
duction in error. For future work, we intend to apply
the data set selection strategies to other NLP tasks,
such as the optimal selection of sentences for tag-
ging and parsing.
Acknowledgments
The authors thank the reviewers and acknowledge
support by the NSF (grant IIS-1116676) and a re-
search gift from Google. Any opinions, findings, or
conclusions are those of the authors, and do not nec-
essarily reflect the views of the NSF.
References
RH Baayen, R. Piepenbrock, and L. Gulikers. 1995.
The celex lexical database (version release 2)[cd-rom].
Philadelphia, PA: Linguistic Data Consortium, Uni-
versity of Pennsylvania.
Maximilian Bisani and Hermann Ney. 2008. Joint-
sequence models for grapheme-to-phoneme conver-
sion. Speech Communication, 50(5):434?451, 5.
C.H. Bischof and G. Quintana-Ort??. 1998. Algorithm
782: codes for rank-revealing qr factorizations of
dense matrices. ACM Transactions on Mathematical
Software (TOMS), 24(2):254?257.
C. Boutsidis, M.W. Mahoney, and P. Drineas. 2008. Un-
supervised feature selection for principal components
analysis. In Proceeding of the 14th ACM SIGKDD
international conference on Knowledge discovery and
data mining, pages 61?69.
C. Boutsidis, M. W. Mahoney, and P. Drineas. 2009.
An improved approximation algorithm for the column
subset selection problem. In Proceedings of the twen-
tieth Annual ACM-SIAM Symposium on Discrete Al-
gorithms, pages 968?977. Society for Industrial and
Applied Mathematics.
P. Cosi, R. Gretter, and F. Tesser. 2000. Festival
parla italiano. Proceedings of GFS2000, Giornate del
Gruppo di Fonetica Sperimentale, Padova.
K. Dwyer and G. Kondrak. 2009. Reducing the anno-
tation effort for letter-to-phoneme conversion. In Pro-
ceedings of the ACL, pages 127?135. Association for
Computational Linguistics.
Matthias Eck, Stephan Vogel, and Alex Waibel. 2005.
Low cost portability for statistical machine translation
based on n-gram coverage. In Proceedings of the Ma-
chine Translation Summit X.
C. Eckart and G. Young. 1936. The approximation of
one matrix by another of lower rank. Psychometrika,
1(3):211?218.
G. Golub. 1965. Numerical methods for solving lin-
ear least squares problems. Numerische Mathematik,
7(3):206?216.
Yoo Pyo Hong and C-T Pan. 1992. Rank-revealing
factorizations and the singular value decomposition.
Mathematics of Computation, 58(197):213?232.
S. Jiampojamarn and G. Kondrak. 2010. Letter-phoneme
alignment: An exploration. In Proceedings of the
ACL, pages 780?788. Association for Computational
Linguistics.
R.M. Kaplan and M. Kay. 1994. Regular models of
phonological rule systems. Computational linguistics,
20(3):331?378.
J. Kominek and A. W. Black. 2006. Learning pronunci-
ation dictionaries: language complexity and word se-
lection strategies. In Proceedings of the NAACL, pages
232?239. Association for Computational Linguistics.
K.Z. Mao. 2005. Identifying critical variables of prin-
cipal components for unsupervised feature selection.
Systems, Man, and Cybernetics, Part B: Cybernetics,
IEEE Transactions on, 35(2):339?344.
Y. Marchand and R.I. Damper. 2000. A multistrategy ap-
proach to improving pronunciation by analogy. Com-
putational Linguistics, 26(2):195?219.
P. Mousty, M. Radeau, et al 1990. Brulex. une base de
donne?es lexicales informatise?e pour le franc?ais e?crit et
parle?. L?anne?e psychologique, 90(4):551?566.
T.J. Sejnowski and C.R. Rosenberg. 1987. Parallel net-
works that learn to pronounce english text. Complex
systems, 1(1):145?168.
1204
Burr Settles. 2010. Active learning literature survey.
Technical Report TR1648, Department of Computer
Sciences, University of Wisconsin-Madison.
H. Stoppiglia, G. Dreyfus, R. Dubois, and Y. Oussar.
2003. Ranking a random feature for variable and fea-
ture selection. The Journal of Machine Learning Re-
search, 3:1399?1414.
L. Wolf and A. Shashua. 2005. Feature selection for un-
supervised and supervised inference: The emergence
of sparsity in a weight-based approach. The Journal
of Machine Learning Research, 6:1855?1887.
Z. Zhao and H. Liu. 2007. Spectral feature selection for
supervised and unsupervised learning. In Proceedings
of the ICML, pages 1151?1157.
1205
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 1527?1536,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
Unsupervised Consonant-Vowel Prediction over Hundreds of Languages
Young-Bum Kim and Benjamin Snyder
University of Wisconsin-Madison
{ybkim,bsnyder}@cs.wisc.edu
Abstract
In this paper, we present a solution to one
aspect of the decipherment task: the pre-
diction of consonants and vowels for an
unknown language and alphabet. Adopt-
ing a classical Bayesian perspective, we
performs posterior inference over hun-
dreds of languages, leveraging knowledge
of known languages and alphabets to un-
cover general linguistic patterns of typo-
logically coherent language clusters. We
achieve average accuracy in the unsuper-
vised consonant/vowel prediction task of
99% across 503 languages. We further
show that our methodology can be used
to predict more fine-grained phonetic dis-
tinctions. On a three-way classification
task between vowels, nasals, and non-
nasal consonants, our model yields unsu-
pervised accuracy of 89% across the same
set of languages.
1 Introduction
Over the past centuries, dozens of lost languages
have been deciphered through the painstaking
work of scholars, often after decades of slow
progress and dead ends. However, several impor-
tant writing systems and languages remain unde-
ciphered to this day.
In this paper, we present a successful solution
to one aspect of the decipherment puzzle: auto-
matically identifying basic phonetic properties of
letters in an unknown alphabetic writing system.
Our key idea is to use knowledge of the phonetic
regularities encoded in known language vocabu-
laries to automatically build a universal probabilis-
tic model to successfully decode new languages.
Our approach adopts a classical Bayesian per-
spective. We assume that each language has
an unobserved set of parameters explaining its
observed vocabulary. We further assume that
each language-specific set of parameters was itself
drawn from an unobserved common prior, shared
across a cluster of typologically related languages.
In turn, each cluster derives its parameters from
a universal prior common to all language groups.
This approach allows us to mix together data from
languages with various levels of observations and
perform joint posterior inference over unobserved
variables of interest.
At the bottom layer (see Figure 1), our
model assumes a language-specific data generat-
ing HMM over words in the language vocabulary.
Each word is modeled as an emitted sequence of
characters, depending on a corresponding Markov
sequence of phonetic tags. Since individual letters
are highly constrained in their range of phonetic
values, we make the assumption of one-tag-per-
observation-type (e.g. a single letter is constrained
to be always a consonant or always a vowel across
all words in a language).
Going one layer up, we posit that the language-
specific HMM parameters are themselves drawn
from informative, non-symmetric distributions
representing a typologically coherent language
grouping. By applying the model to a mix of lan-
guages with observed and unobserved phonetic se-
quences, the cluster-level distributions can be in-
ferred and help guide prediction for unknown lan-
guages and alphabets.
We apply this approach to two small decipher-
ment tasks:
1. predicting whether individual characters in
an unknown alphabet and language represent
vowels or consonants, and
2. predicting whether individual characters in
an unknown alphabet and language represent
vowels, nasals, or non-nasal consonants.
For both tasks, our approach yields considerable
1527
success. We experiment with a data set consist-
ing of vocabularies of 503 languages from around
the world, written in a mix of Latin, Cyrillic, and
Greek alphabets. In turn for each language, we
consider it and its alphabet ?unobserved? ? we
hide the graphic and phonetic properties of the
symbols ? while treating the vocabularies of the
remaining languages as fully observed with pho-
netic tags on each of the letters.
On average, over these 503 leave-one-language-
out scenarios, our model predicts consonant/vowel
distinctions with 99% accuracy. In the more chal-
lenging task of vowel/nasal/non-nasal prediction,
our model achieves average accuracy over 89%.
2 Related Work
The most direct precedent to the present work is
a section in Knight et al (2006) on universal pho-
netic decipherment. They build a trigram HMM
with three hidden states, corresponding to conso-
nants, vowels, and spaces. As in our model, indi-
vidual characters are treated as the observed emis-
sions of the hidden states. In contrast to the present
work, they allow letters to be emitted by multiple
states.
Their experiments show that the HMM trained
with EM successfully clusters Spanish letters into
consonants and vowels. They further design a
more sophisticated finite-state model, based on
linguistic universals regarding syllable structure
and sonority. Experiments with the second model
indicate that it can distinguish sonorous conso-
nants (such as n, m, l, r) from non-sonorous con-
sonants in Spanish. An advantage of the linguis-
tically structured model is that its predictions do
not require an additional mapping step from unin-
terpreted hidden states to linguistic categories, as
they do with the HMM.
Our model and experiments can be viewed as
complementary to the work of Knight et al, while
also extending it to hundreds of languages. We
use the simple HMM with EM as our baseline. In
lieu of a linguistically designed model structure,
we choose an empirical approach, allowing poste-
rior inference over hundreds of known languages
to guide the model?s decisions for the unknown
script and language.
In this sense, our model bears some similarity
to the decipherment model of Snyder et al (2010),
which used knowledge of a related language (He-
brew) in an elaborate Bayesian framework to de-
cipher the ancient language of Ugaritic. While the
aim of the present work is more modest (discover-
ing very basic phonetic properties of letters) it is
also more widely applicable, as we don?t required
detailed analysis of a known related language.
Other recent work has employed a simi-
lar perspective for tying learning across lan-
guages. Naseem et al (2009) use a non-parametric
Bayesian model over parallel text to jointly learn
part-of-speech taggers across 8 languages, while
Cohen and Smith (2009) develop a shared logis-
tic normal prior to couple multilingual learning
even in the absence of parallel text. In simi-
lar veins, Berg-Kirkpatrick and Klein (2010) de-
velop hierarchically tied grammar priors over lan-
guages within the same family, and Bouchard-
C?t? et al (2013) develop a probabilistic model of
sound change using data from 637 Austronesian
languages.
In our own previous work, we have developed
the idea that supervised knowledge of some num-
ber of languages can help guide the unsupervised
induction of linguistic structure, even in the ab-
sence of parallel text (Kim et al, 2011; Kim and
Snyder, 2012)1. In the latter work we also tack-
led the problem of unsupervised phonemic predic-
tion for unknown languages by using textual reg-
ularities of known languages. However, we as-
sumed that the target language was written in a
known (Latin) alphabet, greatly reducing the dif-
ficulty of the prediction task. In our present case,
we assume no knowledge of any relationship be-
tween the writing system of the target language
and known languages, other than that they are all
alphabetic in nature.
Finally, we note some similarities of our model
to some ideas proposed in other contexts. We
make the assumption that each observation type
(letter) occurs with only one hidden state (con-
sonant or vowel). Similar constraints have been
developed for part-of-speech tagging (Lee et al,
2010; Christodoulopoulos et al, 2011), and the
power of type-based sampling has been demon-
strated, even in the absence of explicit model con-
straints (Liang et al, 2010).
3 Model
Our generative Bayesian model over the ob-
served vocabularies of hundreds of languages is
1We note that similar ideas were simultaneously proposed
by other researchers (Cohen et al, 2011).
1528
1529
For example, the cluster Poisson parameter over
vowel observation types might be ? = 9 (indi-
cating 9 vowel letters on average for the cluster),
while the parameter over consonant observation
types might be ? = 20 (indicating 20 consonant
letters on average). These priors will be distinct
for each language cluster and serve to characterize
its general linguistic and typological properties.
We pause at this point to review the Dirich-
let distribution in more detail. A k?dimensional
Dirichlet with parameters ?1 ...?k defines a distri-
bution over the k ? 1 simplex with the following
density:
f(?1 ... ?k|?1 ... ?k) ?
?
i
??i?1i
where ?i > 0, ?i > 0, and?i ?i = 1. The Dirich-
let serves as the conjugate prior for the Multino-
mial, meaning that the posterior ?1...?k|X1...Xn is
again distributed as a Dirichlet (with updated pa-
rameters). It is instructive to reparameterize the
Dirichlet with k + 1 parameters:
f(?1 ... ?k|?0, ??1 ... ??k) ?
?
i
??0?
?
i?1
i
where ?0 = ?i ?i, and ??i = ?i/?0. In this
parameterization, we have E[?i] = ??i. In other
words, the parameters ??i give the mean of the dis-
tribution, and ?0 gives the precision of the dis-
tribution. For large ?0 ? k, the distribution is
highly peaked around the mean (conversely, when
?0 ? k, the mean lies in a valley).
Thus, the Dirichlet parameters of a language
cluster characterize both the average HMMs of in-
dividual languages within the cluster, as well as
how much we expect the HMMs to vary from
the mean. In the case of emission distribu-
tions, we assume symmetric Dirichlet priors? i.e.
one-parameter Dirichlets with densities given by
f(?1 ...?k|?) ?
? ?(??1)i . This assumption is nec-
essary, as we have no way to identify characters
across languages in the decipherment scenario,
and even the number of consonants and vowels
(and thus multinomial/Dirichlet dimensions) can
vary across the languages of a cluster. Thus, the
mean of these Dirichlets will always be a uniform
emission distribution. The single Dirichlet emis-
sion parameter per cluster will specify whether
this mean is on a peak (large ?) or in a valley
(small ?). In other words, it will control the ex-
pected sparsity of the resulting per-language emis-
sion multinomials.
In contrast, the transition Dirichlet parameters
may be asymmetric, and thus very specific and
informative. For example, one cluster may have
the property that CCC consonant clusters are ex-
ceedingly rare across all its languages. This prop-
erty would be expressed by a very small mean
??CCC ? 1 but large precision ?0. Later we shall
see examples of learned transition Dirichlet pa-
rameters.
3.3 Cluster Generation
The generation of the cluster parameters (Algo-
rithm 1) defines the highest layer of priors for our
model. As Dirichlets lack a standard conjugate
prior, we simply use uniform priors over the in-
terval [0, 500]. For the cluster Poisson parameters,
we use conjugate Gamma distributions with vague
priors.3
4 Inference
In this section we detail the inference proce-
dure we followed to make predictions under our
model. We run the procedure over data from
503 languages, assuming that all languages but
one have observed character and tag sequences:
w1, w2, . . . , t1, t2, . . . Since each character type w
is assumed to have a single tag category, this is
equivalent to observing the character token se-
quence along with a character-type-to-tag map-
ping tw. For the target language, we observe only
character token sequence w1, w2, . . .
We assume fixed and known parameter val-
ues only at the cluster generation level. Unob-
served variables include (i) the cluster parameters
?, ?, ?, (ii) the cluster assignments z, (iii) the per-
language HMM parameters ?, ? for all languages,
and (iv) for the target language, the tag tokens
t1, t2, . . . ? or equivalently the character-type-to-
tag mappings tw ? along with the observation
type-counts Nt.
4.1 Monte Carlo Approximation
Our goal in inference is to predict the most likely
tag tw,? for each character type w in our target lan-
guage ? according to the posterior:
f (tw,? |w, t??)
=
?
f (t?, z, ?, ? |w, t??) d? (1)
3(1,19) for consonants, (1,10) for vowels, (0.2, 15) for
nasals, and (1,16) for non-nasal consonants.
1530
where ? = (t?w,?, z, ?, ?), w are the observed
character sequences for all languages, t?? are the
character-to-tag mappings for the observed lan-
guages, z are the language-to-cluster assignments,
and ? and ? are all the cluster-level transition and
emission Dirichlet parameters.
Sampling values (t?, z, ?, ?)Nn=1 from the inte-
grand in Equation 1 allows us to perform the stan-
dard Monte Carlo approximation:
f (tw,? = t |w, t??)
? N?1
N?
n=1
I (tw,? = t in sample n) (2)
To maximize the Monte Carlo posterior, we sim-
ply take the most commonly sampled tag value
for character type w in language ?. Note that
we leave out the language-level HMM parame-
ters (?, ?) as well as the cluster-level Poisson pa-
rameters ? from Equation 1 (and thus our sample
space), as we can analytically integrate them out
in our sampling equations.
4.2 Gibbs Sampling
To sample values (t?, z, ?, ?) from their poste-
rior (the integrand of Equation 1), we use Gibbs
sampling, a Monte Carlo technique that constructs
a Markov chain over a high-dimensional sample
space by iteratively sampling each variable condi-
tioned on the currently drawn sample values for
the others, starting from a random initialization.
The Markov chain converges to an equilibrium
distribution which is in fact the desired joint den-
sity (Geman and Geman, 1984). We now sketch
the sampling equations for each of our sampled
variables.
Sampling tw,?
To sample the tag assignment to character w in
language ?, we need to compute:
f (tw,? |w, t?w,?, t??, z, ?, ?) (3)
? f (w?, t?, N? | ?k, ?k,Nk??) (4)
where N? are the types-per-tag counts implied by
the mapping t?, k is the current cluster assignment
for the target language (z? = k), ?k and ?k are the
cluster parameters, andNk?? are the types-per-tag
counts for all languages currently assigned to the
cluster, other than language ?.
Applying the chain rule along with our model?s
conditional independence structure, we can further
re-write Equation 4 as a product of three terms:
f(N?|Nk??) (5)
f(t1, t2, . . . |?k) (6)
f(w1, w2, . . . |N?, t1, t2, . . . , ?k) (7)
The first term is the posterior predictive distribu-
tion for the Poisson-Gamma compound distribu-
tion and is easy to derive. The second term is the
tag transition predictive distribution given Dirich-
let hyperparameters, yielding a familiar Polya urn
scheme form. Removing terms that don?t depend
on the tag assignment t?,w gives us:
?
t,t?
(
?k,t,t? + n(t, t?)
)[n?(t,t?)]
?
t
(?
t? ?k,t,t? + n(t)
)[n?(t)]
where n(t) and n(t, t?) are, respectively, unigram
and bigram tag counts excluding those containing
character w. Conversely, n?(t) and n?(t, t?) are,
respectively, unigram and bigram tag counts only
including those containing character w. The no-
tation a[n] denotes the ascending factorial: a(a +
1) ? ? ? (a+n?1). Finally, we tackle the third term,
Equation 7, corresponding to the predictive dis-
tribution of emission observations given Dirichlet
hyperparameters. Again, removing constant terms
gives us:
?[n(w)]k,t?
t? N?,t??
[n(t?)]
k,t?
where n(w) is the unigram count of character w,
and n(t?) is the unigram count of tag t, over all
characters tokens (including w).
Sampling ?k,t,t?
To sample the Dirichlet hyperparameter for cluster
k and transition t ? t?, we need to compute:
f(?k,t,t? |t, z)
? f(t, z|?z,t,t?)
= f(tk|?z,t,t?)
where tk are the tag sequences for all languages
currently assigned to cluster k. This term is a pre-
dictive distribution of the multinomial-Dirichlet
compound when the observations are grouped into
multiple multinomials all with the same prior.
Rather than inefficiently computing a product of
Polya urn schemes (with many repeated ascending
1531
factorials with the same base), we group common
terms together and calculate:
?
j=1(?k,t,t? + k)n(j,k,t,t
?)
?
j=1(
?
t?? ?k,t,t?? + k)n(j,k,t)
where n(j, k, t) and n(j, k, t, t?) are the numbers
of languages currently assigned to cluster k which
have more than j occurrences of unigram (t) and
bigram (t, t?), respectively.
This gives us an efficient way to compute un-
normalized posterior densities for ?. However, we
need to sample from these distributions, not just
compute them. To do so, we turn to slice sam-
pling (Neal, 2003), a simple yet effective auxiliary
variable scheme for sampling values from unnor-
malized but otherwise computable densities.
The key idea is to supplement the variable
x, distributed according to unnormalized density
p?(x), with a second variable u with joint density
defined as p(x, u) ? I(u < p?(x)). It is easy
to see that p?(x) ? ? p(x, u)du. We then itera-
tively sample u|x and x|u, both of which are dis-
tributed uniformly across appropriately bounded
intervals. Our implementation follows the pseudo-
code given in Mackay (2003).
Sampling ?k,t
To sample the Dirichlet hyperparameter for cluster
k and tag t we need to compute:
f(?k,t|t,w, z,N)
? f(w|t, z, ?k,t,N)
? f(wk|tk, ?k,t,Nk)
where, as before, tk are the tag sequences for
languages assigned to cluster k, Nk are the tag
observation type-counts for languages assigned
to the cluster, and likewise wk are the char-
acter sequences of all languages in the cluster.
Again, we have the predictive distribution of
the multinomial-Dirichlet compound with multi-
ple grouped observations. We can apply the same
trick as above to group terms in the ascending fac-
torials for efficient computation. As before, we
use slice sampling for obtaining samples.
Sampling z?
Finally, we consider sampling the cluster assign-
ment z? for each language ?. We calculate:
f(z? = k|w, t,N, z??, ?, ?)
? f(w?, t?, N?|?k, ?k,Nk??)
= f(N?|Nk??)f(t?|?k)f(w?|t?, N?, ?k)
The three terms correspond to (1) a standard pre-
dictive distributions for the Poisson-gamma com-
pound and (2) the standard predictive distribu-
tions for the transition and emission multinomial-
Dirichlet compounds.
5 Experiments
To test our model, we apply it to a corpus of 503
languages for two decipherment tasks. In both
cases, we will assume no knowledge of our tar-
get language or its writing system, other than that
it is alphabetic in nature. At the same time, we
will assume basic phonetic knowledge of the writ-
ing systems of the other 502 languages. For our
first task, we will predict whether each character
type is a consonant or a vowel. In the second task,
we further subdivide the consonants into two ma-
jor categories: the nasal consonants, and the non-
nasal consonants. Nasal consonants are known to
be perceptually very salient and are unique in be-
ing high frequency consonants in all known lan-
guages.
5.1 Data
Our data is drawn from online electronic transla-
tions of the Bible (http://www.bible.is,
http://www.crosswire.org/index.
jsp, and http://www.biblegateway.
com). We have identified translations covering
503 distinct languages employing alphabetic
writing systems. Most of these languages (476)
use variants of the Latin alphabet, a few (26)
use Cyrillic, and one uses the Greek alphabet.
As Table 1 indicates, the languages cover a very
diverse set of families and geographic regions,
with Niger-Congo languages being the largest
represented family.4 Of these languages, 30 are
either language isolates, or sole members of their
language family in our data set.
For our experiments, we extracted unique word
types occurring at least 5 times from the down-
loaded Bible texts. We manually identified vowel,
nasal, and non-nasal character types. Since the let-
ter ?y? can frequently represent both a consonant
and vowel, we exclude it from our evaluation. On
average, the resulting vocabularies contain 2,388
unique words, with 19 consonant characters, two
2 nasal characters, and 9 vowels. We include the
data as part of the paper.
4In fact, the Niger-Congo grouping is often considered
the largest language family in the world in terms of distinct
member languages.
1532
Language Family #lang
Niger-Congo 114
Austronesian 67
Oto-Manguean 41
Indo-European 39
Mayan 34
Quechuan 17
Afro-Asiatic 17
Uto-Aztecan 16
Altaic 16
Trans-New Guinea 15
Nilo-Saharan 14
Sino-Tibetan 13
Tucanoan 9
Creole 8
Chibchan 6
Maipurean 5
Tupian 5
Nakh-Daghestanian 4
Uralic 4
Cariban 4
Totonacan 4
Mixe-Zoque 3
Jivaroan 3
Choco 3
Guajiboan 2
Huavean 2
Austro-Asiatic 2
Witotoan 2
Jean 2
Paezan 2
Other 30
Table 1: Language families in our data set. The
Other category includes 9 language isolates and
21 language family singletons.
5.2 Baselines and Model Variants
As our baseline, we consider the trigram HMM
model of Knight et al (2006), trained with EM. In
all experiments, we run 10 random restarts of EM,
and pick the prediction with highest likelihood.
We map the induced tags to the gold-standard tag
categories (1-1 mapping) in the way that maxi-
mizes accuracy.
We then consider three variants of our model.
The simplest version, SYMM, disregards all in-
formation from other languages, using simple
symmetric hyperparameters on the transition and
emission Dirichlet priors (all hyperparameters set
to 1). This allows us to assess the performance of
Model Cons vs Vowel C vs V vs N
Al
l EM 93.37 74.59SYMM 95.99 80.72
MERGE 97.14 86.13
CLUST 98.85 89.37
Iso
lat
es EM 94.50 74.53SYMM 96.18 78.13
MERGE 97.66 86.47
CLUST 98.55 89.07
No
n-L
ati
n EM 92.93 78.26
SYMM 95.90 79.04
MERGE 96.06 83.78
CLUST 97.03 85.79
Table 2: Average accuracy for EM baseline and
model variants across 503 languages. First panel:
results on all languages. Second panel: results for
30 isolate and singleton languages. Third panel:
results for 27 non-Latin alphabet languages (Cyril-
lic and Greek). Standard Deviations across lan-
guages are about 2%.
our Gibbs sampling inference method for the type-
based HMM, even in the absence of multilingual
priors.
We next consider a variant of our model,
MERGE, that assumes that all languages reside in
a single cluster. This allows knowledge from the
other languages to affect our tag posteriors in a
generic, language-neutral way.
Finally, we consider the full version of our
model, CLUST, with 20 language clusters. By al-
lowing for the division of languages into smaller
groupings, we hope to learn more specific param-
eters tailored for typologically coherent clusters of
languages.
6 Results
The results of our experiments are shown in Ta-
ble 2. In all cases, we report token-level accuracy
(i.e. frequent characters count more than infre-
quent characters), and results are macro-averaged
over the 503 languages. Variance across languages
is quite low: the standard deviations are about 2
percentage points.
For the consonant vs. vowel prediction task,
all tested models perform well. Our baseline, the
EM-based HMM, achieves 93.4% accuracy. Sim-
ply using our Gibbs sampler with symmetric priors
boosts the performance up to 96%. Performance
1533
1534
Figure 4: Inferred Dirichlet transition hyperparameters for bigram CLUST on three-way classification
task with four latent clusters. Row gives starting state, column gives target state. Size of red blobs are
proportional to magnitude of corresponding hyperparameters.
Language Family Portion #langs Ent.
Indo-European
0.38 26 2.26
0.24 41 3.19
0.21 38 3.77
Quechuan 0.89 18 0.61
Mayan 0.64 33 1.70
Oto-Manguean 0.55 31 1.99
Maipurean 0.25 8 2.75
Tucanoan 0.2 45 3.98
Uto-Aztecan 0.4 25 2.85
Altaic 0.44 27 2.76
Niger-Congo
1 2 0.00
0.78 23 1.26
0.74 27 1.05
0.68 22 1.22
0.67 33 1.62
0.5 18 2.21
0.24 25 3.27
Austronesian
0.91 22 0.53
0.71 21 1.51
0.24 17 3.06
Table 3: Plurality language families across 20
clusters. The columns indicate portion of lan-
guages in the plurality family, number of lan-
guages, and entropy over families.
with a bigram HMM with four language clus-
ters. Examining just the first row, we see that
the languages are partially grouped by their pref-
erence for the initial tag of words. All clus-
ters favor languages which prefer initial conso-
nants, though this preference is most weakly ex-
pressed in cluster 3. In contrast, both clusters
2 and 4 have very dominant tendencies towards
consonant-initial languages, but differ in the rel-
ative weight given to languages preferring either
vowels or nasals initially.
Finally, we examine the relationship between
the induced clusters and language families in Ta-
ble 3, for the trigram consonant vs. vowel CLUST
model with 20 clusters. We see that for about
half the clusters, there is a majority language fam-
ily, most often Niger-Congo. We also observe
distinctive clusters devoted to Austronesian and
Quechuan languages. The largest two clusters are
rather indistinct, without any single language fam-
ily achieving more than 24% of the total.
8 Conclusion
In this paper, we presented a successful solution
to one aspect of the decipherment task: the predic-
tion of consonants and vowels for an unknown lan-
guage and alphabet. Adopting a classical Bayesian
perspective, we develop a model that performs
posterior inference over hundreds of languages,
leveraging knowledge of known languages to un-
cover general linguistic patterns of typologically
coherent language clusters. Using this model, we
automatically distinguish between consonant and
vowel characters with nearly 99% accuracy across
503 languages. We further experimented on a
three-way classification task involving nasal char-
acters, achieving nearly 90% accuracy.
Future work will take us in several new direc-
tions: first, we would like to move beyond the as-
sumption of an alphabetic writing system so that
we can apply our method to undeciphered syllabic
scripts such as Linear A. We would also like to
extend our methods to achieve finer-grained reso-
lution of phonetic properties beyond nasals, con-
sonants, and vowels.
Acknowledgments
The authors thank the reviewers and acknowledge support by
the NSF (grant IIS-1116676) and a research gift fromGoogle.
Any opinions, findings, or conclusions are those of the au-
thors, and do not necessarily reflect the views of the NSF.
1535
References
Taylor Berg-Kirkpatrick and Dan Klein. 2010. Phylogenetic
grammar induction. In Proceedings of the ACL, pages
1288?1297. Association for Computational Linguistics.
Alexandre Bouchard-C?t?, David Hall, Thomas L Griffiths,
and Dan Klein. 2013. Automated reconstruction of
ancient languages using probabilistic models of sound
change. Proceedings of the National Academy of Sci-
ences, 110(11):4224?4229.
Christos Christodoulopoulos, Sharon Goldwater, and Mark
Steedman. 2011. A Bayesian mixture model for part-
of-speech induction using multiple features. In Proceed-
ings of EMNLP, pages 638?647. Association for Compu-
tational Linguistics.
Shay B Cohen and Noah A Smith. 2009. Shared logistic nor-
mal distributions for soft parameter tying in unsupervised
grammar induction. In Proceedings of NAACL, pages 74?
82. Association for Computational Linguistics.
Shay B Cohen, Dipanjan Das, and Noah A Smith. 2011. Un-
supervised structure prediction with non-parallel multilin-
gual guidance. In Proceedings of EMNLP, pages 50?61.
Association for Computational Linguistics.
Stuart Geman and Donald Geman. 1984. Stochastic relax-
ation, Gibbs distributions, and the Bayesian restoration of
images. Pattern Analysis and Machine Intelligence, IEEE
Transactions on, (6):721?741.
Young-Bum Kim and Benjamin Snyder. 2012. Univer-
sal grapheme-to-phoneme prediction over latin alphabets.
In Proceedings of EMNLP, pages 332?343, Jeju Island,
South Korea, July. Association for Computational Lin-
guistics.
Young-Bum Kim, Jo?o V Gra?a, and Benjamin Snyder.
2011. Universal morphological analysis using structured
nearest neighbor prediction. In Proceedings of EMNLP,
pages 322?332. Association for Computational Linguis-
tics.
Kevin Knight, Anish Nair, Nishit Rathod, and Kenji Yamada.
2006. Unsupervised analysis for decipherment problems.
In Proceedings of COLING/ACL, pages 499?506. Associ-
ation for Computational Linguistics.
Yoong Keok Lee, Aria Haghighi, and Regina Barzilay. 2010.
Simple type-level unsupervised POS tagging. In Proceed-
ings of EMNLP, pages 853?861. Association for Compu-
tational Linguistics.
Percy Liang, Michael I Jordan, and Dan Klein. 2010. Type-
based MCMC. In Proceedings of NAACL, pages 573?581.
Association for Computational Linguistics.
David JC MacKay. 2003. Information Theory, Inference and
Learning Algorithms. Cambridge University Press.
Tahira Naseem, Benjamin Snyder, Jacob Eisenstein, and
Regina Barzilay. 2009. Multilingual part-of-speech tag-
ging: Two unsupervised approaches. Journal of Artificial
Intelligence Research, 36(1):341?385.
Radford M Neal. 2003. Slice sampling. Annals of statistics,
31:705?741.
Benjamin Snyder, Regina Barzilay, and Kevin Knight. 2010.
A statistical model for lost language decipherment. In
Proceedings of the ACL, pages 1048?1057. Association
for Computational Linguistics.
1536
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 637?642,
Baltimore, Maryland, USA, June 23-25 2014.
c?2014 Association for Computational Linguistics
Training a Korean SRL System with Rich Morphological Features
Young-Bum Kim, Heemoon Chae, Benjamin Snyder and Yu-Seop Kim*
University of Wisconsin-Madison, Hallym University*
{ybkim, hmchae21, bsnyder}@cs.wisc.edu, yskim01@hallym.ac.kr
*
Abstract
In this paper we introduce a semantic role
labeler for Korean, an agglutinative lan-
guage with rich morphology. First, we
create a novel training source by semanti-
cally annotating a Korean corpus contain-
ing fine-grained morphological and syn-
tactic information. We then develop a su-
pervised SRL model by leveraging mor-
phological features of Korean that tend
to correspond with semantic roles. Our
model also employs a variety of latent
morpheme representations induced from a
larger body of unannotated Korean text.
These elements lead to state-of-the-art per-
formance of 81.07% labeled F1, represent-
ing the best SRL performance reported to
date for an agglutinative language.
1 Introduction
Semantic Role Labeling (SRL) is the task of auto-
matically annotating the predicate-argument struc-
ture in a sentence with semantic roles. Ever since
Gildea and Jurafsky (2002), SRL has become an
important technology used in applications requir-
ing semantic interpretation, ranging from infor-
mation extraction (Frank et al, 2007) and ques-
tion answering (Narayanan and Harabagiu, 2004),
to practical problems including textual entailment
(Burchardt et al, 2007) and pictorial communica-
tion systems (Goldberg et al, 2008).
SRL systems in many languages have been
developed as the necessary linguistic resources
become available (Taul?e et al, 2008; Xue and
Palmer, 2009; B?ohmov?a et al, 2003; Kawahara et
al., 2002). Seven languages were the subject of the
CoNLL-2009 shared task in syntactic and seman-
tic parsing (Haji?c et al, 2009). These languages
can be categorized into three broad morphological
types: fusional (4), analytic (2), and one aggluti-
native language.
Paul 
 
 studies 
 
 mathematics 
 
 with 
 
 Jane 
 
 at 
 
 a 
 
 library
Poleun 
 
 doseogwaneseo 
 
Jeingwa 
 
suhageull 
 
 gongbuhanda
Figure 1: English (SVO) and Korean (SOV) words
alignment. The subject, verb, and object are high-
lighted as red, blue, and green, respectively. Also,
prepositions and suffixes are highlighted as purple.
Bj?orkelund et al (2009) report an average la-
beled semantic F1-score of 80.80% across these
languages. The highest performance was achieved
for the analytic language group (82.12%), while
the agglutinative language, Japanese, yielded the
lowest performance (76.30%). Agglutinative lan-
guages such as Japanese, Korean, and Turkish are
computationally difficult due to word-form spar-
sity, variable word order, and the challenge of us-
ing rich morphological features.
In this paper, we describe a Korean SRL system
which achieves 81% labeled semantic F1-score.
As far as we know, this is the highest accuracy
obtained for Korean, as well as any agglutinative
language. Figure 1 displays a English/Korean sen-
tence pair, highlighting the SOV word order of Ko-
rean as well as its rich morphological structure.
Two factors proved crucial in the performance of
our SRL system: (i) The analysis of fine-grained
morphological tags specific to Korean, and (ii) the
use of latent stem and morpheme representations
to deal with sparsity. We incorporated both of
these elements in a CRF (Lafferty et al, 2001) role
labeling model.
Besides the contribution of this model and SRL
system, we also report on the creation and avail-
ability of a new semantically annotated Korean
corpus, covering over 8,000 sentences. We used
this corpus to develop, train, and test our Korean
SRL model. In the next section, we describe the
process of corpus creation in more detail.
637
2 A Semantically Annotated Korean
Corpus
We annotated predicate-argument structure of
verbs in a corpus from the Electronics and
Telecommunications Research Institute of Korea
(ETRI).
1
Our corpus was developed over two
years using a specialized annotation tool (Song et
al., 2012), resulting in more than 8,000 semanti-
cally annotated sentences. As much as possible,
annotations followed the PropBank guidelines for
English (Bonial et al, 2010).
We view our work as building on the efforts of
the Penn Korean PropBank (PKPB).
2
Our corpus
is roughly similar in size to the PKPB, and taken
together, the two Korean corpora now total about
half the size of the Penn English PropBank. One
advantage of our corpus is that it is built on top of
the ETRI Korean corpus, which uses a richer Ko-
rean morphological tagging scheme than the Penn
Korean Treebank. Our experiments will show that
these finer-grained tags are crucial for achieving
high SRL accuracy.
All annotations were performed by two people
working in a team. At first, each annotator as-
signs semantic roles independently and then they
discuss to reduce disagreement of their annotation
results. Initially, the disagreement rate between
two annotators was about 14%. After 4 months
of this process, the disagreement rate fell to 4%
through the process of building annotation rules
for Korean. The underlying ETRI syntactically-
annotated corpus contains the dependency tree
structure of sentences with morpho-syntactic tags.
It includes 101,602 multiple-clause sentences with
21.66 words on average.
We encountered two major difficulties during
annotation. First, the existing Korean frame files
from the Penn Korean PropBank include 2,749
verbs, covering only 13.87% of all the verbs in the
ETRI corpus. Secondly, no Korean PropBanking
guidelines have previously been published, lead-
ing to uncertainty in the initial stages of annota-
tion. These uncertainties were gradually resolved
through the iterative process of resolving inter-
annotator disagreements.
Table 1 shows the semantic roles considered in
our annotated corpus. Although these are based on
the general English PropBank guidelines (Bonial
et al, 2010), they also differ in that we used only
1
http://voice.etri.re.kr/db/db pop.asp?code=88
2
http://catalog.ldc.upenn.edu/LDC2006T03
Roles Definition Rate
ARG0 Agent 10.02%
ARG1 Patient 26.73%
ARG2
Start point /
Benefactive
5.18%
ARG3 Ending point 1.10%
ARGM-ADV Adverbial 1.26%
ARGM-CAU Cause 1.17%
ARGM-CND Condition 0.36%
ARGM-DIR Direction 0.35%
ARGM-DIS Discourse 28.71%
ARGM-EXT Extent 4.50%
ARGM-INS Instrument 1.04%
ARGM-LOC Locative 4.51%
ARGM-MNR Manner 8.72%
ARGM-NEG Negation 0.26%
ARGM-PRD Predication 0.27%
ARGM-PRP Purpose 0.77%
ARGM-TMP Temporal 5.05%
Table 1: Semantic roles in our annotated corpus.
4 numbered arguments from ARG0 to ARG3 in-
stead of 5 numbered arguments. We thus consider
17 semantic roles in total. Four of them are num-
bered roles, describing the essential arguments of
a predicate. The other roles are called modifier
roles that play more of an adjunct role.
We have annotated semantic roles by following
the PropBank annotation guideline (Bonial et al,
2010) and by using frame files of the Penn Korean
PropBank built by Palmer et al (2006). The Prop-
Bank and our corpus are not exactly compatible,
because the former is built on constituency-based
parse trees, whereas our corpus uses dependency
parses.
More importantly, the tagsets of these corpora
are not fully compatible. The PKPB uses much
coarser morpho-syntactic tags than the ETRI
corpus. For example, the PCA tag in PKPB used
for a case suffix covers four different functioning
tags used in our corpus. Using coarser suffix
tags can seriously degrade SRL performance, as
we show in Section 6, where we compare the
performance of our model on both the new corpus
and the older PKPB.
638
3 Previous Work
Korean SRL research has been limited to domesti-
cally published Korean research on small corpora.
Therefore, the most direct precedent to the present
work is a section in Bj?orkelund et al (2009) on
Japanese SRL. They build a classifier consisting
of 3 stages: predicate disambiguation, argument
identification, and argument classification.
They use an L
2
-regularized linear logistic re-
gression model cascaded through these three
stages, achieving F1-score of 80.80% on average
for 7 languages (Catalan, Chinese, Czech, English,
German, Japanese and Spanish). The lowest re-
ported performance is for Japanese, the only ag-
glutinative language in their data set, achieving
F1-score of 76.30%. This result showcases the
computational difficulty of dealing with morpho-
logically rich agglutinative languages. As we dis-
cuss in Section 5, we utilize these same features,
but also add a set of Korean-specific features to
capture aspects of Korean morphology.
Besides these morphological features, we also
employ latent continuous and discrete morpheme
representations induced from a larger body of
unannotated Korean text. As our experiments be-
low show, these features improve performance by
dealing with sparsity issues. Such features have
been useful in a variety of English NLP mod-
els, including chunking, named entity recogni-
tion (Turian et al, 2010), and spoken language un-
derstanding (Anastasakos et al, 2014). Unlike the
English models, we use individual morphemes as
our unit of analysis.
4 Model
For the semantic role task, the input is a sentence
consisting of a sequence of words x = x
1
, . . . , x
n
and the output is a sequence of corresponding se-
mantic tags y = y
1
, . . . , y
n
. Each word con-
sists of a stem and some number of suffix mor-
phemes, and the semantic tags are drawn from the
set {NONE, ARG?, . . . , ARGM-TMP}. We model
the conditional probability p(y|x) using a CRF
model:
Z(x)
?1
x
?
i=1
exp
?
m
?
m
f
m
(y
i?1
, y
i
, x, i),
where f
m
(y
i?1
, y
i
, x, i) are the feature functions.
These feature functions include transition features
that identify the tag bigram (y
i?1
, y
i
), and emis-
sion features that combine the current semantic tag
(y
i
) with instantiated feature templates extracted
from the sentence x and its underlying morpho-
logical and dependency analysis. The function
Z is the normalizing function, which ensures that
p(y|x) is a valid probability distribution. We used
100 iteration of averaged perceptron algorithm to
train the CRF.
5 Features
We detail the feature templates used for our ex-
periments in Table 2. These features are catego-
rized as either general features, Korean-specific
features, or latent morpheme representation fea-
tures. Korean-specific features are built upon the
morphological analysis of the suffix agglutination
of the current word x
i
.
Korean suffixes are traditionally classified into
two groups called Josa and Eomi. Josa is used
to define nominal cases and modify other phrases,
while Eomi is an ending of a verb or an adjective
to define a tense, show an attitude, and connect
or terminate a sentence. Thus, the Eomi and Josa
categorization plays an important role in signaling
semantic roles. Considering the functions of Josa
and Eomi, we expect that numbered roles are rele-
vant to Josa while modifier roles are more closely
related to Eomi. The one exception is adverbial
Josa, making the attached phrase an adverb that
modifies a verb predicate.
For all feature templates, ?A-? or ?P-? are used
respectively to signify that the feature corresponds
to the argument in question (x
i
), or rather is de-
rived from the verbal predicate that the argument
depends on.
General features: We use and modify 18 fea-
tures used for Japanese from the prior work of
Bj?orkelund et al (2009), excluding SENSE, PO-
SITION, and re-ranker features.
? Stem: a stem without any attachment. For
instance, the first word Poleun at the Figure 1
consists of a stem Pol plus Josa eun.
? POS Lv1: the first level (coarse classifi-
cation) of a POS tag such as noun, verb,
adjective, or adverb.
639
Feature Description
A-Stem, P-Stem Stem of an argument and a predicate
A-POS Lv1, P-POS Lv1 Coarse-grained POS of A-Stem and P-Stem
A-POS Lv2, P-POS Lv2 Fine-grained POS of A-Stem and P-Stem
A-Case, P-Case Case of A-Stem and P-Stem
A-LeftmostChildStem Stem of the leftmost child of an argument
A-LeftSiblingStem Stem of the left sibling of an argument
A-LeftSiblingPOS Lv1 Coarse-grained POS of A-LeftSiblingStem
A-LeftSiblingPOS Lv2 Fine-grained POS of A-LeftSiblingStem
A-RightSiblingPOS Lv1 Coarse-grained POS of a stem of the right sibling of an argument
A-RightSiblingPOS Lv2 Fine-grained POS of a stem of the right sibling of an argument
P-ParentStem Stem of a parent of a predicate
P-ChildStemSet Set of stems of children of a predicate
P-ChildPOSSet Lv1 Set of coarse POS of P-ChildStemSet
P-ChildCaseSet Set of cases of P-childStemSet
A-JosaExist If 1, Josa exists in an argument, otherwise 0.
A-JosaClass Linguistic classification of Josa
A-JosaLength Number of morphemes consisting of Josa
A-JosaMorphemes Each morpheme consisting of Josa
A-JosaIdenity Josa of an argument
A-EomiExist If 1, Eomi exists in an argument, otherwise 0.
A-EomiClass Lv1 Linguistic classification of Eomi
A-EomiClass Lv2 Another linguistic classification of Eomi
A-EomiLength Number of morphemes consisting of Eomi
A-EomiMorphemes Each morpheme consisting of Eomi
A-EomiIdentity Eomi of an argument
A-StemRepr Stem representation of an argument
A-JosaRepr Josa representation of an argument
A-EomiRepr Eomi representation of an argument
Table 2: Features used in our SRL experiments. Features are grouped as General, Korean-specific, or
Latent Morpheme Representations. For the last group, we employ three different methods to build them:
(i) CCA, (ii) deep learning, and (iii) Brown clustering.
? POS Lv2: the second level (fine classifica-
tion) of a POS tag. If POS Lv1 is noun, ei-
ther a proper noun, common noun, or other
kinds of nouns is the POS Lv2.
? Case: the case type such as SBJ, OBJ, or
COMP.
The above features are also applied to some depen-
dency children, parents, and siblings of arguments
as shown in Table 2.
Korean-specific features: We have 11 different
kinds of features for the Josa (5) and Eomi (6). We
highlight several below:
? A-JosaExist: an indicator feature checking
any Josa whether or not exists in an argument.
It is set to 1 if any Josa exists, otherwise 0.
? A-JosaClass: the linguistic classification of
Josa with a total of 8 classes. These classes
are adverbial, auxiliary, complemental, con-
nective, determinative, objective, subjective,
and vocative.
? A-JosaLength: the number of morphemes
consisting of Josa. At most five morphemes
are combined to consist of one Josa in our
data set.
? A-JosaMorphemes: Each morpheme com-
posing the Josa.
? A-JosaIdentity: The Josa itself.
? A-EomiClass Lv1: the linguistic classifica-
tion of Eomi with a total of 14 classes. These
14 classes are adverbial, determinative, coor-
dinate, exclamatory, future tense, honorific,
imperative, interrogative, modesty, nominal,
normal, past tense, petitionary, and subordi-
nate.
? A-EomiClass Lv2: Another linguistic classi-
fication of Eomi with a total of 4 classes. The
four classes are closing, connection, prefinal,
and transmutation. The EomiClass Lv1 and
Lv2 are combined to display the characteris-
tic of Eomi such as ?Nominal Transmutation
Eomi?, but not all combinations are possible.
640
Corpus Gen Gen+Kor
Gen+Kor+LMR
CCA Deep Brown All
PKPB 64.83% 75.17% 75.51% 75.43% 75.55% 75.54%
Our annotated corpus 66.88% 80.33% 80.88% 80.84% 80.77% 81.07%
PKPB + our annotated corpus 64.86% 78.61% 79.32% 79.44% 78.91% 79.20%
Table 3: Experimental F1-score results on every experiment. Abbreviation on features are Gen: general
features, Kor: Korean specific features, LMR: latent morpheme representation features.
Latent morpheme representation features: To
alleviate the sparsity, a lingering problem in NLP,
we employ three kinds of latent morpheme repre-
sentations induced from a larger body of unsuper-
vised text data. These are (i) linear continuous rep-
resentation through Canonical Correlation Analy-
sis (Dhillon et al, 2012), (ii) non-linear contin-
uous representation through Deep learning (Col-
lobert and Weston, 2008), and (iii) discrete rep-
resentation through Brown Clustering (Tatu and
Moldovan, 2005).
The first two representations are 50 dimensional
continuous vectors for each morpheme, and the
latter is a set of 256 clusters over morphemes.
6 Experiments and Results
We categorized our experiments by the scenarios
below, and all results are summarized in Table 3.
The F1-score results were investigated for each
scenario. We randomly divided our data into 90%
training and 10% test sets for all scenarios.
For latent morpheme representations, we used
the Donga news article corpus.
3
The Donga cor-
pus contains 366,636 sentences with 25.09 words
on average. The Domain of this corpus cov-
ers typical news articles such as health, entertain-
ment, technology, politics, world and others. We
ran Kokoma Korean morpheme analyzer
4
on each
sentence of the Donga corpus to divide words into
morphemes to build latent morpheme representa-
tions.
1st Scenario: We first tested on general features
in previous work (2nd column in Table 3). We
achieved 64.83% and 66.88% on the PKPB and
our corpus. When the both corpora were com-
bined, we had 64.86%.
2nd Scenario: We then added the Korean-
specific morphological features to signify its ap-
3
http://www.donga.com
4
http://kkma.snu.ac.kr/
propriateness in this scenario. These features in-
creased greatly performance improvements (3rd
column in Table 3). Although both the PKPB
and our corpus had improvements, the improve-
ments were the most notable on our corpus. This
is because PKPB POS tags might be too coarse.
We achieved 75.17%, 80.33%, and 78.61% on the
PKPB, our corpus, and the combined one, respec-
tively.
3rd Scenario: This scenario is to reveal the ef-
fects of the different latent morpheme represen-
tations (4-6th columns in Table 3). These three
representations are from CCA, deep learning, and
Brown clustering. The results gave evidences that
all representations increased the performance.
4th Scenario: We augmented our model with all
kinds of features (the last column in Table 3). We
achieved our best F1-score of 81.07% over all sce-
narios on our corpus.
7 Conclusion
For Korean SRL, we semantically annotated a
corpus containing detailed morphological annota-
tion. We then developed a supervised model which
leverages Korean-specific features and a variety
of latent morpheme representations to help deal
with a sparsity problem. Our best model achieved
81.07% in F1-score. In the future, we will con-
tinue to build our corpus and look for the way to
use unsupervised learning for SRL to apply to an-
other language which does not have available cor-
pus.
Acknowledgments
We thank Na-Rae Han and Asli Celikyilmaz for
helpful discussion and feedback. This research
was supported by the Basic Science Research Pro-
gram of the Korean National Research Foundation
(NRF), and funded by the Korean Ministry of Ed-
ucation, Science and Technology (2010-0010612).
641
References
Tasos Anastasakos, Young-Bum Kim, and Anoop Deo-
ras. 2014. Task specific continuous word represen-
tations for mono and multi-lingual spoken language
understanding. In Proceedings of the IEEE Interna-
tional Conference on Acoustics, Speech, and Signal
Processing (ICASSP).
Anders Bj?orkelund, Love Hafdell, and Pierre Nugues.
2009. Multilingual semantic role labeling. In Pro-
ceedings of the Thirteenth Conference on Compu-
tational Natural Language Learning: Shared Task,
pages 43?48. Association for Computational Lin-
guistics.
Alena B?ohmov?a, Jan Haji?c, Eva Haji?cov?a, and Barbora
Hladk?a. 2003. The prague dependency treebank. In
Treebanks, pages 103?127. Springer.
Claire Bonial, Olga Babko-Malaya, Jinho D Choi, Jena
Hwang, and Martha Palmer. 2010. Propbank an-
notation guidelines. Center for Computational Lan-
guage and Education Research, CU-Boulder.
Aljoscha Burchardt, Nils Reiter, Stefan Thater, and
Anette Frank. 2007. A semantic approach to tex-
tual entailment: system evaluation and task analy-
sis. In Proceedings of the ACL-PASCAL Workshop
on Textual Entailment and Paraphrasing, RTE ?07,
pages 10?15, Stroudsburg, PA, USA. Association
for Computational Linguistics.
Ronan Collobert and Jason Weston. 2008. A unified
architecture for natural language processing: Deep
neural networks with multitask learning. In Pro-
ceedings of the 25th international conference on
Machine learning, pages 160?167. ACM.
Paramveer Dhillon, Jordan Rodu, Dean Foster, and
Lyle Ungar. 2012. Two step cca: A new spec-
tral method for estimating vector models of words.
arXiv preprint arXiv:1206.6403.
Anette Frank, Hans-Ulrich Krieger, Feiyu Xu, Hans
Uszkoreit, Berthold Crysmann, Brigitte J?org, and
Ulrich Sch?afer. 2007. Question answering from
structured knowledge sources. Journal of Applied
Logic, 5(1):20 ? 48. Questions and Answers: Theo-
retical and Applied Perspectives.
Daniel Gildea and Daniel Jurafsky. 2002. Automatic
labeling of semantic roles. Computational linguis-
tics, 28(3):245?288.
Andrew B Goldberg, Xiaojin Zhu, Charles R Dyer,
Mohamed Eldawy, and Lijie Heng. 2008. Easy
as abc?: facilitating pictorial communication via
semantically enhanced layout. In Proceedings of
the Twelfth Conference on Computational Natural
Language Learning, pages 119?126. Association for
Computational Linguistics.
Jan Haji?c, Massimiliano Ciaramita, Richard Johans-
son, Daisuke Kawahara, Maria Ant`onia Mart??, Llu??s
M`arquez, Adam Meyers, Joakim Nivre, Sebastian
Pad?o, Jan
?
St?ep?anek, et al 2009. The conll-2009
shared task: Syntactic and semantic dependencies
in multiple languages. In Proceedings of the Thir-
teenth Conference on Computational Natural Lan-
guage Learning: Shared Task, pages 1?18. Associa-
tion for Computational Linguistics.
Daisuke Kawahara, Sadao Kurohashi, and K?oiti
Hasida. 2002. Construction of a japanese relevance-
tagged corpus. In LREC.
John Lafferty, Andrew McCallum, and Fernando CN
Pereira. 2001. Conditional random fields: Prob-
abilistic models for segmenting and labeling se-
quence data.
Srini Narayanan and Sanda Harabagiu. 2004. Ques-
tion answering based on semantic structures. In
Proceedings of the 20th international conference on
Computational Linguistics, COLING ?04, Strouds-
burg, PA, USA. Association for Computational Lin-
guistics.
Martha Palmer, Shijong Ryu, Jinyoung Choi, Sinwon
Yoon, and Yeongmi Jeon. 2006. Korean propbank.
Linguistic data consortium.
Hye-Jeong Song, Chan-Young Park, Jung-Kuk Lee,
Min-Ji Lee, Yoon-Jeong Lee, Jong-Dae Kim, and
Yu-Seop Kim. 2012. Construction of korean se-
mantic annotated corpus. In Computer Applications
for Database, Education, and Ubiquitous Comput-
ing, pages 265?271. Springer.
Marta Tatu and Dan Moldovan. 2005. A seman-
tic approach to recognizing textual entailment. In
Proceedings of the conference on Human Language
Technology and Empirical Methods in Natural Lan-
guage Processing, pages 371?378. Association for
Computational Linguistics.
Mariona Taul?e, Maria Ant`onia Mart??, and Marta Re-
casens. 2008. Ancora: Multilevel annotated corpora
for catalan and spanish. In LREC.
Joseph Turian, Lev Ratinov, and Yoshua Bengio. 2010.
Word representations: a simple and general method
for semi-supervised learning. In Proceedings of the
48th Annual Meeting of the Association for Compu-
tational Linguistics, pages 384?394. Association for
Computational Linguistics.
Nianwen Xue and Martha Palmer. 2009. Adding se-
mantic roles to the chinese treebank. Natural Lan-
guage Engineering, 15(01):143?172.
642
