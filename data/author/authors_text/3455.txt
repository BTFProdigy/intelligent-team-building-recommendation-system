HMMand CRF Based Hybrid Model for Chinese Lexical Analysis
	
				
	
Proceedings of the Fifth SIGHAN Workshop on Chinese Language Processing, pages 72?78,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Hybrid Models 
for Chinese Named Entity Recognition 
 
Lishuang Li, Tingting Mao, Degen Huang, Yuansheng Yang 
Department of Computer Science and Engineering 
Dalian University of Technology 
116023 Dalian, China 
{computer, huangdg, yangys}@dlut.edu.cn  
maotingting1007@sohu.com 
 
Abstract 
This paper describes a hybrid model and 
the corresponding algorithm combining 
support vector machines (SVMs) with 
statistical methods to improve the per-
formance of SVMs for the task of Chi-
nese Named Entity Recognition (NER). 
In this algorithm, a threshold of the dis-
tance from the test sample to the hyper-
plane of SVMs in feature space is used to 
separate SVMs region and statistical 
method region. If the distance is greater 
than the given threshold, the test sample 
is classified using SVMs; otherwise, the 
statistical model is used. By integrating 
the advantages of two methods, the hy-
brid model achieves 93.18% F-measure 
for Chinese person names and 91.49% F-
measure for Chinese location names. 
1 Introduction 
Named entity (NE) recognition is a fundamental 
step to many language processing tasks such as 
information extraction (IE), question answering 
(QA) and machine translation (MT). On its own, 
NE recognition can also provide users who are 
looking for person or location names with quick 
information. Palma and Day (1997) reported that 
person (PER), location (LOC) and organization 
(ORG) names are the most difficult sub-tasks as 
compared to other entities as defined in Message 
Understanding Conference (MUC). So we focus 
on the recognition of PER, LOC and ORG enti-
ties. 
Recently, machine learning approaches are 
widely used in NER, including the hidden 
Markov model (Zhou and Su, 2000; Miller and 
Crystal, 1998), maximum entropy model 
(Borthwick, 1999), decision tree (Qin and Yuan, 
2004), transformation-based learning (Black and 
Vasilakopoulos, 2002), boosting (Collins, 2002; 
Carreras et al, 2002), support vector machine 
(Takeuchi and Collier, 2002; Yu et al, 2004; 
Goh et al, 2003), memory-based learning (Sang, 
2002). SVM has given high performance in vari-
ous classification tasks (Joachims, 1998; Kudo 
and Matsumoto, 2001). Goh et al (2003) pre-
sented a SVM-based chunker to extract Chinese 
unknown words. It obtained higher F-measure 
for person names and organization names. 
Like other classifiers, the misclassified testing 
samples by SVM are mostly near the decision 
plane (i.e., the hyperplane of SVM in feature 
space). In order to increase the accuracy of SVM, 
we propose a hybrid model combining SVM 
with a statistical approach for Chinese NER, that 
is, in the region near the decision plane, statisti-
cal method is used to classify the samples instead 
of SVM, and in the region far away from the de-
cision plane, SVM is used. In this way, the mis-
classification by SVM near the decision plane 
can be decreased significantly. A higher F-
measure for Chinese NE recognition can be 
achieved. 
In the following sections, we shall describe 
our approach in details. 
2 Recognition of Chinese Named Entity 
Using SVM 
Firstly, we segment and assign part-of-speech 
(POS) tags to words in the texts using a Chinese 
lexical analyzer. Secondly, we break segmented 
words into characters and assign each character 
its features. Lastly, a model based on SVM to 
identify Chinese named entities is set up by 
choosing a proper kernel function. 
In the following, we will exemplify the person 
names and location names to illustrate the identi-
fication process. 
72
2.1 Support Vector Machines 
Support Vector Machines first introduced by 
Vapnik (1996) are learning systems that use a 
hypothesis space of linear functions in a high 
dimensional feature space, trained with a learn-
ing algorithm from optimization theory that im-
plements a learning bias derived from statistical 
theory. SVMs are based on the principle of struc-
tural risk minimization. Viewing the data as 
points in a high-dimensional feature space, the 
goal is to fit a hyperplane between the positive 
and negative examples so as to maximize the 
distance between the data points and the hyper-
plane. 
Given training examples: 
},1,1{,)},,(),...,,(),,{( 2211 +???= iill ynRxyxyxyxS  (1) 
ix is a feature vector (n dimension) of the i-th 
sample.  is the class (positive(+1) or nega-
tive(-1) class) label of the i-th sample. l  is the 
number of the given training samples. SVMs find 
an ?optimal? hyperplane:  to sepa-
rate the training data into two classes. The opti-
mal hyperplane can be found by solving the fol-
lowing quadratic programming problem (we 
leave the details to Vapnik (1998)): 
iy
0)( =+ bwx
.,...,2,1,0    ,0           tosubject
),(
2
1
               max
1 11
licy
Kyy   
ii
l
1i
i
l
i
l
j
jjii
l
i
i
=???=?
????
?
? ??
=
= ==
ji xx    (2) 
The function  is called 
kernel function, is the mapping from pri-
mary input space to feature space. Given a test 
example, its label y is decided by the following 
function: 
)()()( jiji xxx,x ???=K
)(x?
.]),(sgn[)( ?
?
+?=
sv
ii bKyf
ix
i xxx            (3) 
Basically, SVMs are binary classifiers, and 
can be extended to multi-class classifiers in order 
to solve multi-class discrimination problems. 
There are two popular methods to extend a bi-
nary classification task to that of K classes: one 
class vs. all others and pairwise. Here, we em-
ploy the simple pairwise method. This idea is to 
build  classifiers considering all 
pairs of classes, and final decision is given by 
their voting. 
2/)1( ?? KK
2.2 Recognition of Chinese Person Names 
Based on SVM 
We use a SVM-based chunker, YamCha (Kudo 
and Masumoto, 2001), to extract Chinese person 
names from the Chinese lexical analyzer. 
1) Chinese Person Names Chunk Tags 
We use the Inside/Outside representation for 
proper chunks: 
I    Current token is inside of a chunk. 
O   Current token is outside of any chunk. 
B   Current token is the beginning of a chunk. 
A chunk is considered as a Chinese person 
name in this case. Every character in the training 
set is given a tag classification of B, I or O, that 
is, },,{ OIByi ? . Here, the multi-class decision 
method pairwise is selected. 
2) Features Extraction for Chinese Person 
Names 
Since Chinese person names are identified 
from the segmented texts, the mistakes of word 
segmentation can result in error identification of 
person names. So we must break words into 
characters and extract features for every charac-
ter. Table 1 summarizes types of features and 
their values. 
 
Type of feature Value 
POS tag n-B, v-I, p-S 
Whether a character 
is a surname  Y or N 
Character surface form of the  character itself 
The frequency of a 
character in person 
names table 
Y or N 
Previous BIO tag B-character, I-character, O-character 
Table 1. Summary of Features and Their Values 
The POS tag from the output of lexical analy-
sis is subcategorized to include the position of 
the character in the word. The list of POS tags is 
shown in Table 2. 
 
POS tag Description of the position of the character in a word 
<POS>-S One-character word 
<POS>-B first character in a multi-character word 
<POS>-I intermediate character in a multi-character word 
<POS>-E last character in a multi-character word 
Table 2.  POS Tags in A Word 
If the character is a surname, the value is as-
signed to Y, otherwise assigned to N. 
The ?character? is surface form of the charac-
ter in the word. 
We extract all person names in January 1998 
of the People?s Daily to set up person names ta-
ble and calculate the frequency of every charac-
73
ter (F) of person names table in the training cor-
pus. The frequency of F is defined as 
,)(
  F of number  total  the
names person of character a as F of number the
FP = (4) 
if P(F) is greater than the given threshold, the 
value is assigned to Y, otherwise assigned to N.  
We also use previous BIO-tags as features. 
Whether a character is inside a person name or 
not, it depends on the context of the character. 
Therefore, we use contextual information of two 
previous and two successive characters of the 
current character as features. 
Figure 1 shows an example of features extrac-
tion for the i-th character. When training, the fea-
tures of the character ?Min? contains all the fea-
tures surrounded in the frames. If the same sen-
tence is used as testing, the same features are 
used. 
 
Position 
Character 
POS tags 
The frequency of a character in 
the person names table 
Previous BIO tags 
-2    -1    0   +1   +2
Jiang  Ze   Min  zhu  xi
n-S  n-B  n-E  n-B  n-E
Y     Y    Y    N    N
B     I    I    O    O
  i 
Whether the character 
is a surname 
Y     N    N    N    Y
 Figure 1.  An example of features extraction
 
3) Choosing Kernel Functions 
Here, we choose polynomial kernel functions: 
to build an optimal 
separating hyperplane. 
d
ii xxxxK ]1)[(),( +?=
2.3 Recognition of Chinese Location Names 
Based on SVM 
The identification process of location names is 
the same as that of person names except for the 
features extraction. Table 3 summarizes types of 
features and their values of location names ex-
traction. 
Type of feature Value 
POS tag n-B, v-I, p-S 
Whether a character 
appears in location names 
characteristic table 
Y or N 
Character surface form of the character itself 
Previous BIO tag 
B-character, I-
character, O-
character 
Table 3. Summary of Features and Their Values 
The location names characteristic table is set 
up in advance, and it includes the characters or 
words expressing the characteristics of location 
names such as ?sheng (province)?, ?shi (city)?, 
?xian (county)?etc. If the character is in the loca-
tion names characteristic table, the value is as-
signed to Y, otherwise assigned to N. 
3 Statistical Models 
Many statistical models for NER have been pre-
sented (Zhang et al, 1992; Huang et al, 2003 
etc). In this section, we proposed our statistical 
models for Chinese person names recognition 
and Chinese location names recognition.     
3.1 Chinese Person Names 
We define a function to evaluate the person name 
candidate PN. The evaluated function Total-
Probability(PN) is composed of two parts: the 
lexical probability LP(PN) and contextual prob-
ability CP(PN) based on POS tags. 
),()1()()( PNCPPNLPPNbilityTotalProba ??+?=  (5) 
where PN is the evaluated person name and ? is 
the balance cofficient. 
1) lexical probability LP(PN)    
We establish the surname table (SurName) and 
the first name table (FirstName) from the 
students of year 1999 in a university (containing 
9986 person names). 
Suppose PN=LF1F2, where L is the surname 
of the evaluated person name PN, Fi (i=1,2) is the 
i-th first name of the evaluated person name PN. 
The probability of the surname Pl(L) is defined 
as 
,
)(
)(
)(
0
0?
?
=
SurNamey
l
l
l yP
LP
LP                                (6) 
where ,  is the 
number of L as the single or multiple surname of 
person names in the SurName. 
)2)((log)( 20 += LNLPl )(LN
The probability of the first name Pf(F) is 
defined as 
,
)(
)(
)(
0
0
?
?
=
FirstNamey
f
f
f yP
FP
FP                                 (7) 
where ,  is the 
number of F in the FirstName. 
)2)((log)( 20 += FNFPf )(FN
The lexical probability of the person name PN 
is defined as 
 ,)FLFif(PN       FP   FPCLPPNLP
)LFif(PN                                FPLPPNLP
ffbl
fl
2121
11
))()(()()(
)()()(
=+??=
=?=  (8) 
74
where Cb is the balance cofficient between the 
single name and the double name. Here, 
Cb=0.844 (Huang et al, 2001). 
2) contextual probability based on POS tags 
CP(PN) 
Chinese person names have characteristic 
contexual POS tags in real Chinese texts, for 
example, in the phrase ?dui Zhangshuai shuo 
(say to Zhangshuai)?, the POS tag before the 
person name ?Zhangshuai? is prepnoun and verb 
occurs after the person name. We define the 
bigram contextual probability CP(PN) of the 
person name PN as the following equation: 
CP(PN)= ,),,(
TotalPOS
rposPNlposPersonPOS ><            (9) 
where lpos is the POS tag of the character before 
PN (called POS forward), rpos is the POS tag of 
the character after PN (called POS backward), 
and is the number 
of PN as a pereson name whose POS forward is 
lpos and POS backward is rpos in training corpus. 
 is the total number of the contexual 
POS tags of every person name in the whole 
training corpus. 
),,( >< rposPNlposPersonPOS
TotalPOS
3.2 Chinese Location Names 
We also define a function to evaluate the location 
name candidate LN. The evaluated function To-
talProbability(LN) is composed of two parts: the 
lexical probability LP (LN) and contextual prob-
ability CP (LN) based on POS tags. 
),()1()()( LNCPLNLPLNbilityTotalProba ??+?= (10) 
where LN is the evaluated location name and?  is 
the balance cofficient. 
1) lexical probability LP (LN) 
Suppose LN=F0F+S, F+=F1?Fn, (i=1,?,n), 
where F0 is the first character of the evaluated 
location name LN, F+ is the middle characters of 
the evaluated location name LN, S is the last 
character of the evaluated location name LN. 
The probability of the first character of the 
evaluated location name is defined as )( 0FPh
,
)(
)(
)(
00
00
0 FP
FP
FP
h
h
h ?=                                      (11) 
where ,  is the 
number of F
)2)((log)( 0200 += FCFPh )( 0FC
0 as the first character of location 
names in the Chinese Location Names Record.  
)2)((log)( 0200 +?=? FCFPh ,  is the total 
number of F
)( 0FC ?
0 in the Chinese Location Names 
Record. 
The probability of the middle character of the 
evaluated location name is defined as )( +FPf
,
)(
)(
)(
1
?
=
+
?=
n
i if
if
f FP
FP
FP                                  (12) 
where ,  is the 
number of F
)2)((log)( 2 += iif FCFP )( iFC
i as the i-th middle character of loca-
tion names in the Chinese Location Names Re-
cord. 
)2)((log)( 2 +?=? iif FCFP ,  is the total 
number of F
)( iFC ?
i in the Chinese Location Names 
Record. 
 The probability of the last character of the 
evaluated location name is defined as )(SPl
,
)(
)(
)(
SP
SP
SP
l
l
l ?=                                          (13) 
where ,  is the 
number of  S as the last character of location 
names in the Chinese Location Names Record. 
)2)((log)( 2 +?= SCSPl )(SC
)2)((log)( 2 +?=? SCSPl , )(SC ?  is the total number 
of S in the Chinese Location Names Record. 
The lexical probability of the location name 
LN is defined as                           
),(/))()()(( 0 LNLenSPFPFPLN lfh ++= +    (14) 
where Len(LN) is the length of the evaluated lo-
cation name LN.                                 
2) contextual probability based on POS tags CP 
(LN) 
Location names also have characteristic 
contexual POS tags in real Chinese texts, for 
example, in the phrase ?zai Chongqing shi 
junxing (to be held in Chongqing)?, the POS tag 
before the location name ?Chongqing?is 
prepnoun and verb occurs after the location name. 
We define the bigram contextual probability 
CP(LN) of the location name LN similar to that 
of the person name PN in equation (9), where PN 
is replaced with LN. 
4 Recognition of Chinese Named Entity 
Using Hybrid Model 
Analyzing the classification results (obtained by 
sole SVMs described in section 2) between B 
and I, B and O, I and O respectively, we find that 
the error is mainly caused by the second classifi-
cation. The samples which attribute to B class 
are misclassified to O class, which leads to B 
class vote?s diminishing and the corresponding 
named entities are lost. Therefore the Recall is 
lower. In the meantime, the number of the mis-
classified samples whose function distances to 
the hyperplane of SVM in feature space are less 
than 1 can reach over 83% of the number of total 
misclassified samples. That means the misclassi-
75
fication of a classifier is occurred in the region of 
two overlapping classes. Considering this fact, 
we can expect to improve SVM using the follow-
ing hybrid model. 
The hybrid model includes the following 
procedure: 
1) compute the distance from the test sample 
to the hyperplane of SVM in feature space.  
2) compare the distance with given threshold. 
The algorithm of hybrid model can be de-
scribed as follows: 
Suppose T is the testing set, 
(1) if  ??T , select Tx? , else stop; 
(2) compute  ?
=
+?= l
i
ii bxxKyxg
1
),()(
(3) if ?>)(xg , output 
, else use the statistic 
models and output the returned results. 
[ ]1,0??
))(sgn()( xgxf =
(4) , repeat(1) { }xTT ??
5 Experiments 
Our experimental results are all based on the 
corpus of Peking University. 
5.1 Extracting Chinese Person Names 
We use 180 thousand characters corpus of year 
1998 from the People?s Daily as the training cor-
pus and extract other sentences (containing 1526 
Chinese person names) as testing corpus to con-
duct an open test experiment. The results are ob-
tained as follows based on different models. 
1) Based on Sole SVM 
An experiment is carried out to recognize Chi-
nese person names based on sole SVM by the 
method as described in Section 2. The Recall, 
Precision and F-measure using different number 
of degree of polynomial kernel function are 
given in Table 4. The best result is obtained 
when d=2. 
 
 Recall Precision F-measure 
d=1 87.22% 94.26% 90.61% 
d=2 87.16% 96.10% 91.41% 
d=3 84.67% 95.14% 89.60% 
Table 4. Results for Person Names Extraction 
Based on Sole SVM 
 
2) Using Hybrid Model 
As mentioned in section 4, the test samples 
which attribute to B class are misclassified to O 
class and therefore the Recall for person names 
extraction from sole SVM is lower. So we only 
deal with the test samples (B class and O class) 
whose function distances to the hyperplane of 
SVM in feature space (i.e. g(x)) is between 0 and 
? . We move class-boundary learned by SVM 
towards the O class, that is, the O class samples 
are considered as B class in that area. 93.64% of 
the Chinese person names in testing corpus are 
recalled when ? =0.9 (Here, ?  also represents 
how much the boundary is moved). However, a 
number of non-person names are also identified 
as person names wrongly and the Precision is 
decreased correspondingly. Table 5 shows the 
Recall and Precision of person names extraction 
with different ? .  
 
 Recall Precision F-measure
? =1 93.05% 75.17% 83.16% 
? =0.9 93.64% 81.75% 87.29% 
? =0.8 93.51% 85.91% 89.55% 
? =0.7 93.05% 88.31% 90.62% 
? =0.6 92.39% 90.21% 91.29% 
? =0.5 91.81% 91.87% 91.84% 
? =0.4 91.02% 93.28% 92.13% 
? =0.3 90.56% 95.05% 92.75% 
? =0.2 90.03% 95.48% 92.68% 
? =0.1 88.66% 95.82% 92.10% 
Table 5. Results for Person Names Extraction 
with Different ?  
 
We use the evaluated function TotalProbabil-
ity(PN) as described in section 3 to filter the 
wrongly recalled person names using SVM. We 
tune?  in equation (5) to obtain the best results. 
The results based on the hybrid model with dif-
ferent ?  are listed in Table 6 (when d=2). We 
can observe that the result is best when ? =0.4. 
Table 7 shows the results based on the hybrid 
model with different ?  when =0.4. We can 
observe that the Recall rises and the Precision 
drops on the whole when 
?
?  increases. The syn-
thetic index F-measures are improved when ?  is 
between 0.1 and 0.8 compared with sole SVM. 
The best result is obtained when ? =0.3. The Re-
call and the F-measure increases 3.27% and 
1.77% respectively. 
 
 Recall Precision F-measure
? =0.1 90.37% 95.76% 92.99% 
? =0.2 90.37% 96.03% 93.11% 
? =0.3 90.43% 96.03% 93.15% 
? =0.4 90.43% 96.10% 93.18% 
? =0.5 90.63% 95.76% 93.13% 
? =0.6 90.43% 95.97% 93.12% 
76
? =0.7 90.43% 95.90% 93.09% 
? =0.8 90.43% 95.90% 93.09% 
? =0.9 90.37% 95.90% 93.05% 
Table 6. Results for Person Names Extraction 
Based on The Hybrid Model with Different?  
 
 Recall Precision F-measure
? =1 92.53% 84.96% 88.58% 
? =0.9 93.05% 88.81% 90.88% 
? =0.8 92.86% 90.95% 91.89% 
? =0.7 92.46% 92.04% 92.25% 
? =0.6 91.93% 93.22% 92.58% 
? =0.5 91.48% 94.26% 92.85% 
? =0.4 90.76% 95.25% 92.95% 
? =0.3 90.43% 96.10% 93.18% 
? =0.2 90.04% 96.15% 92.99% 
? =0.1 88.73% 96.23% 92.32% 
Table 7. Results for Person Names Extraction 
Based on The Hybrid Model ( =0.4) ?
5.2 Extracting Chinese Location Names 
We use 1.5M characters corpus of year 1998 
from the People?s Daily as the training corpus 
and extract sentences of year 2000 from the Peo-
ple?s Daily (containing 2919 Chinese location 
names) as testing corpus to conduct an open test 
experiment. The results are obtained as follows 
based on different models. 
1) Based on Sole SVM 
The Recall, Precision and F-measure using 
different number of degree of polynomial kernel 
function are given in Table 8. The best result is 
obtained when d=2. 
 
 Recall Precision F-measure 
d=1 84.66% 91.95% 88.16% 
d=2 86.69% 93.82% 90.12% 
d=3 86.27% 94.23% 90.07% 
Table 8. Results for Location Names Extraction 
Based on Sole SVM 
 
2) Using Hybrid Model 
The results for Chinese location names extrac-
tion based on the hybrid model are listed in Ta-
ble 9 (when d=2; ? =0.2 in equation (10)). We 
can observe that the Recall rises and the Preci-
sion drops on the whole when ?  increases. The 
synthetic index F-measures are improved when 
?  is between 0.1 and 0.7 compared with sole 
SVM. The best result is obtained when ? =0.3. 
The Recall increases 3.55%, the Precision de-
creases 1.05% and the F-measure increases 
1.37%. 
 Recall Precision F-measure
? =1 90.75% 83.00% 86.71% 
? =0.9 90.85% 85.33% 88.01% 
? =0.8 91.42% 87.42% 89.37% 
? =0.7 91.65% 89.05% 90.33% 
? =0.6 91.75% 90.38% 91.06% 
? =0.5 91.32% 90.98% 91.15% 
? =0.4 90.66% 91.87% 91.26% 
? =0.3 90.24% 92.77% 91.49% 
? =0.2 89.10% 93.28% 91.15% 
? =0.1 87.83% 93.38% 90.52% 
Table 9. Results for Location Names Extraction 
Based on The Hybrid Model (?=0.2) 
6 Comparison with other work 
The same corpus was also tested using statistics-
based approach to identify Chinese person names 
(Huang et al 2001) and location names (Huang 
and Yue, 2003). In their systems, lexical reliabil-
ity and contextual reliability were used to iden-
tify person names and location names calculated 
from statistical information drawn from a train-
ing corpus. The results of our models and the 
statistics-based methods (Huang 2001; Huang 
2003) are shown in Table 10 for comparison. We 
can see that the Recall and F-measure in our 
method all increase a lot.  
 
 Recall Precision F-measure
Our 
models 90.10% 96.15% 93.03%Person 
names Huang
(2001) 88.62% 92.37% 90.46%
Our 
models 90.24% 92.77% 91.49%Location 
names Huang
(2003) 86.86% 91.48% 89.11%
Table 10. Results of Our Method and Huang 
(2001; 2003) for Comparison 
7 Conclusions and Future work 
We recognize Chinese named entities using a 
hybrid model combining support vector ma-
chines with statistical methods. The model inte-
grates the advantages of two methods and the 
experimental results show that it can achieve 
higher F-measure than the sole SVM and indi-
vidual statistical approach. 
  Future work includes optimizing statistical 
models, for example, we can add the probability 
information of Chinese named entities in real 
texts to compute lexical probability, and we can 
77
also use trigram models to compute contextual 
probability. 
The hybrid model is expected to extend to for-
eign names in transliteration to obtain improved 
results by sole SVMs. The identification of trans-
literated names by SVMs has been completed (Li 
et al, 2004). The future work includes: set up 
statistical models for transliterated names and 
combine statistical models with SVMs to identify 
transliterated names. 
References 
William J. Black and Argyrios Vasilakopoulos. 2002. 
Language Independent Named Entity Classifica-
tion by Modified Transformation-based Learning 
and by Decision Tree Induction. The 6th Confer-
ence on Natural Language Learning, Taipei. 
Andrew Eliot Borthwick. 1999. A Maximum Entropy 
Approach to Named Entity Recognition. PhD Dis-
sertation. New York University. 
Xavier Carreras, Lluis Marquez, and Lluis Padro. 
2002. Named Entity Extraction Using AdaBoost. 
The 6th Conference on Natural Language Learning, 
Taipei. 
Michael Collins. 2002. Ranking Algorithms for 
Named-entity Extraction: Boosting and the Voted 
Perceptron. Proceedings of the 40th Annual Meet-
ing of the Association for Computational Linguis-
tics (ACL-2002), Philadelphia, 489-496. 
Chooi-Ling Goh, Masayuki Asahara and Yuji Ma-
tsumoto. 2003. Chinese Unknown Word Identifica-
tion Based on Morphological Analysis and Chunk-
ing. The Companion Volume to the Proceedings of 
41st Annual Meeting of the Association for Compu-
tational Linguistics (ACL-2003), Sapporo, 197-200. 
De-Gen Huang, Yuan-Sheng Yang, and Xing Wang. 
2001. Identification of Chinese Names Based on 
Statistics. Journal of Chinese Information Process-
ing, 15(2): 31-37. 
De-Gen Huang and Guang-Ling Yue. 2003. Identifi-
cation of Chinese Place Names Based on Statistics. 
Journal of Chinese Information Processing, 17(2): 
46-52. 
Thorsten Joachims. 1998. Text Categorization with 
Support Vector Machines: Learning with Many 
Relevant Features. In Proceedings of the European 
Conference on Machine Learning, 1398:137-142. 
Taku Kudo and Yuji Matsumoto. 2001. Chunking 
with Support Vector Machines. In Proceedings of 
NAACL 2001. 
Li-Shuang Li, Chun-Rong Chen, De-Gen Huang and 
Yuan-Sheng Yang. 2004. Identifying Pronuncia-
tion-Translated Names from Chinese Texts Based 
on Support Vector Machines. Advances in Neural 
Networks-ISNN 2004, Lecture Notes in Computer 
Science, Berlin Heidelberg, 3173: 983-988. 
Scott Miller and Michael Crystal. 1998. BBN: De-
scription of the SIFT System as Used for MUC-7. 
Proceedings of 7th Message Understanding Con-
ference, Washington. 
David D. Palmer. 1997. A Trainable Rule-Based Al-
gorithm for Word Segmentation. In Proc of 35th of 
ACL & 8th conf. of EACL, 321-328. 
Wen Qin and Chun-Fa Yuan. 2004. Identification of 
Chinese Unknown Word Based on Decision Tree. 
Journal of Chinese Information Processing, 18(1): 
14-19. 
Erik Tjong Kim Sang. 2002. Memory-based Named 
Entity Recognition. The 6th Conference on Natural 
Language Learning, Taipei. 
Koichi Takeuchi and Nigel Collier. 2002. Use of 
Support Vector Machines in Extended Named En-
tity Recognition. The 6th Conference on Natural 
Language Learning, Taipei. 
Vladimir N. Vapnik. 1995. The Nature of Statistical 
Learning Theory. Springer-Verlag, Berlin. 
Vladimir N. Vapnik. 1998. Statistical Learning The-
ory. John Wiley & Sons, New York. 
Ying Yu, Xiao-Long Wang, Bing-Quan Liu, and Hui 
Wang. 2004. Efficient SVM-based Recognition of 
Chinese Personal Names. High Technology Letters, 
10(3): 15-18. 
Jun-Sheng Zhang, Shun-De Chen, Ying Zheng, Xian-
Zhong Liu and Shu-Jin Ke. 1992. Large-Corpus-
Based Methods for Chinese Personal Name. Jour-
nal of Chinese Information Processing, 6(3): 7-15. 
Guo-Dong Zhou and Jian Su. 2002. Named Entity 
Recognition Using an HMM-based Chunk Tagger. 
Proceedings of the 40th Annual Meeting of the 
ACL, Philadelphia, 473-480. 
 
 
78
Coling 2010: Poster Volume, pages 472?480,
Beijing, August 2010
Mining Large-scale Comparable Corpora from Chinese-English 
News Collections 
Degen Huang1                Lian Zhao2                Lishuang Li 3               Haitao Yu4 
Department of Computer Science and Technology 
Dalian University of Technology 
1huangdg@dlut.edu.cn              3lils@dlut.edu.cn 
2zhaolian@mail.dlut.edu.cn        4gengshenspirit@163.com 
 
Abstract 
In this paper, we explore a CLIR-based  
approach to construct large-scale Chi-
nese-English comparable corpora, which 
is valuable for translation knowledge 
mining. The initial source and target 
document sets are crawled from news 
website and standardized uniformly. 
Keywords are extracted from the source 
document firstly, and then the extracted 
keywords are translated and combined as 
query words through certain criteria to 
retrieve against the index created using 
target document set. Meanwhile, the 
mapping correlations between source and 
target documents are developed accord-
ing to the value of similarity calculated 
by the retrieval tool. Two methods are 
evaluated to filter the comparable docu-
ment pairs so as to ensure the quality of 
the comparable corpora. Experimental re-
sults indicate that our approach is effec-
tive on the construction of Chinese-
English comparable corpora. 
1 Introduction 
Parallel corpora are key resource for statistical 
machine translation, in which machine learning 
techniques are used to learn translation knowl-
edge. Sufficient data is necessary for the data-
driven approaches to estimate the model parame-
ters reliably. However, as Munteanu (2006) 
stated, beyond a few resource-rich language pairs 
such as English-Chinese or English-French and a 
small number of contexts like parliamentary de-
                                                 
 This work was supported by Microsoft Research Asia. 
bates or legal texts, parallel corpora remain a 
scarce resource, despite the proposition of auto-
mated methods to collect parallel corpora from 
the Web. Researches on comparable corpora are 
motivated by the scarcity of parallel corpora. 
Compared with parallel corpora, comparable 
corpora are more abundant, up-to-date and ac-
cessible. 
Comparable corpora are defined as pairs of 
monolingual corpora selected according to the 
same set of criteria, but in different languages or 
language varieties. When creating comparable 
corpora, the key process is to align the source 
document with relevant target documents. Early 
work by Braschler and Sc?uble (1998) employed 
content descriptors and publication dates to align 
German and Italian news stories. Resnik (1999) 
mined comparable corpora on the assumption 
that the pages which are comparable of each 
other share a similar structure (headers, para-
graphs, etc.) when text is presented in many lan-
guages in the Web. Tao and Zhai (2005) acquired 
comparable bilingual text corpora based on the 
observation that terms that are translations of 
each other or share the same topic tend to co-
occur in the comparable corpora at the 
same/similar time periods. Recently, Talvensaari 
et al (2007) introduced a CLIR-based approach 
to align two document collections with different 
languages. All the target documents were in-
dexed with Lemur. Then appropriate keywords 
were extracted from the source language docu-
ments and translated into the target language as 
query words to retrieve similar target documents. 
As we know, the problems may vary with the 
language of documents when using CLIR-based 
approach to construct comparable corpora, such 
as keyword extraction, out-of-vocabulary key-
word translation and so on. This paper is a fur-
ther endeavor to CLIR-based approach for com- 
472
 
Figure 1. The general architecture of comparable corpora construction 
parable corpora construction. We focus on the 
construction of Chinese-English comparable cor-
pora, explore and address the issues during the 
construction. Experimental results show that our 
method is better through a rough comparison 
with Talvensaari et al (2007) and it also outper-
forms our reconstruction of Tao and Zhai (2005) 
in respect to the quality of comparable corpora. 
This paper is organized as follows. In section 2, 
the general architecture of our system is de-
scribed, and each module is illuminated in detail. 
Section 3 reports and analyzes the experimental 
results followed by conclusions in section 4.  
2 System Architecture 
Figure 1 shows the general architecture of our 
comparable corpora construction system. It con-
sists of two components: component I and com-
ponent II. Component I is mainly composed by a 
web crawler, which is used to harvest source and 
target documents from selected web sites. We 
can get the final source and target document sets 
through content extraction and noise filtering. 
The core of the system is component II, which 
aligns a source document with target documents 
having comparable contents. It implements on 
the two document sets generated by component I. 
Component II is composed of three modules: 
keyword extraction, keyword translation, and 
retrieval & filtering. The methods for three mod-
ules are detailed respectively. 
2.1 Keyword Extraction 
A keyword is described as a meaningful and sig-
nificant expression containing one or more words. 
Appropriate keywords briefly describe the theme 
of a document. In this paper, keywords are 
viewed as basic units of search indexes in order 
to retrieve closely related documents. Generally, 
phrases can capture the main idea of a document 
more effectively, inasmuch as they have more 
information than single words (an independent 
linguistic unit after word segmentation for Chi-
nese). 
Existing approaches for keyword extraction 
could be distinguished into two main categories: 
supervised or unsupervised methods. Supervised 
machine learning algorithms were widely used in 
keyword extraction such as Na?ve Bayes (Frank 
et al, 1999; Witten et al, 1999), SVM (Zhang et 
al., 2006), CRF (Zhang et al, 2008), etc. These 
approaches had excellent stability. However, it 
was difficult for us to construct a big-enough 
golden annotated corpus to train a good classifier, 
especially for news web pages. Unsupervised 
methods hinged on evaluating various features to 
select keywords, such as word frequency (Luhn, 
1957), word co-occurrence (Matsuo and Ishizuka, 
2004), and TF*IDF (Li et al, 2007). The inher-
ent problem in these methods was that most of 
their work came in the judgment whether a can-
didate was a keyword or not, but they had not 
paid sufficient attention to the identification of 
phrase candidates. Wan and Xiao (2008) pro-
posed a method for keyphrase extraction from 
single document. However, it simply combined 
the adjacent candidate words to a multi-word 
phrase. 
Based on the above observation, our approach 
for keyword extraction focuses more on the con-
struction of phrasal candidates. It is mainly based 
on MWE (Multi-Word Expression) extraction 
together with relevant word ranking method. 
473
MWE is a special lexical unit including com-
pound terms, idioms and collocations, etc. The 
process of keyword extraction in this paper 
mainly depends on the following stages. 
Stage 1: The generation of phrasal candidates 
(1) The extraction of MWEs from the preproc-
essed document 
Document preprocessing is a procedure of 
morphological analysis including segmentation 
and part of speech tagging for Chinese. The 
method based on the marginal probabilities de-
tailed in (Luo and Huang, 2009) is adopted in 
this part. 
We extract MWEs using LocalMaxs selection 
algorithm together with a relevance measure cal-
culation method (FSCP) proposed by Silva et al 
(1999). Suffix arrays and related structures in 
(Aires et al, 2008) are used to compute the FSCP 
value so as to raise efficiency. And the initial col-
lection of MWEs named G for the document is 
generated after filtered by stopword list. 
(2) The acquisition of new MWEs through the 
modification for segmentation 
As a matter of fact, the results of segmentation 
for the document usually have some errors espe-
cially for out-of-vocabulary (OOV) words which 
are segmented to single Chinese characters in 
most cases. Inaccurate segmentation leads to 
some faults for keyword extraction. As stated in 
(Liu et al, 2007), OOV words can be identified 
by the method of MWEs extraction mentioned 
above. Therefore, we modify the segmentation 
like this: any MWE in G is merged to one word 
if it only consists of single Chinese characters 
and its frequency > freq. The changes before and 
after merging are shown in Table 1. Because the 
method of MWE extraction is based on statistical 
techniques, so low frequency of MWE will result 
in poor performance. But large value for freq 
means that very few MEWs can satisfy the fre-
quency restriction. In our experiments, we set 
freq=2. The extraction process is called again to 
identify MWEs from the document with modi-
fied segmentation. Consequently, new collection 
of MWEs is acquired. 
Additionally, some simple rules are defined 
according to language features to filter MWEs. 
In this paper, our method is tailored to extract 
keywords from news web pages which contain 
some special symmetric marks like ??, ??. The 
words in a specially marked area are usually im-
portant to the document. So we extract words 
within each paired marks and view them as a 
MWE on the condition that it contains two or 
more than two words. All of the MWEs are 
viewed as phrasal candidates and filtered by 
stopword list. 
Stage 2: The generation of single words candi-
dates 
Our method also generates single word candi-
dates with the account that both phrase and sin-
gle word can be served as a keyword. The proc-
ess of single word selection is independent of 
MWE extraction. The candidate words are re-
stricted to nouns, verbs, strings (like WTO) and 
merged words as discussed in the previous stage. 
But the word will be removed if it only appears 
once in the document or is contained in the 
stopword list. 
Stage 3: Keyword selection based on candidates 
ranking 
As for MWE candidates, we calculate the 
weight for them using Formula 1 which refers to 
the formula used to sort NP phrases in (Brace-
well et al, 2008). But the weight of len is re-
duced. 
1
( ) log(
1 ( ))
MWE
len
ii
Weight MWE len f
tf w
len =
= + +
??        (1) 
Where len is the length of MWE (in number of 
words); fMWE is the frequency of the MWE within 
in the document; tf(wi) is the frequency of word 
wi. The following rules are used to rank MWEs: 
MWE Segmentation before merging 
Segmentation 
after merging 
Pos  
before merging 
Pos  
after merging 
? ? ?/ ?/ ?/ ?/ ?/ ??/ ?/ ?/ 
?/ ?/ ??/ ?/  
??/ ?/ ?/ 
?/n ?/n ?/n ?/n 
?/vl ??/n ?/us 
?/a 
?/n ?/n ??/oov 
?/vl ??/n ?/us  
?/a 
? ? ?/ ?/ ??/ ?/ ??/ ??/ ?/ ? /jb ? /n ?? /b ?/n 
??/oov ??/b  
?/n 
Table 1. Changes before and after merging 
474
(a) more frequent MWEs are ranked higher; (b) 
MWEs with larger weight are ranked higher. In 
order to avoid redundancy, we remove the re-
dundant MWEs with lower rank. 
Single word candidates are ranked as follows: 
(a) the single word w with larger TF*IDF value 
is ranked higher; (b) the pos score for w in de-
scending order is: named entity, merged words, 
nouns, strings, verbs. In the end, top-a MWEs 
and top-b single words are chosen to form the 
keyword set of the document. 
Stage 4: Parameters evaluation and experimental 
results 
The max number of keywords extracted from 
each document is limited to ten (a+b=10) and we 
run our approach on the dataset which include 
one hundred Chinese documents from the corpus 
of NTCIR-5 since they are also news articles. 
For evaluation of the results, the keywords ex-
tracted by our method are compared with the 
manually extracted keywords (at most ten key-
words are assigned to each document). The F-
measure is used as evaluation metric. It is de-
fined like this: F=(P+R)/2; P=nummatch/numsystem; 
R=nummatch/nummanual. Where nummatch is the 
count of keywords extracted by our method 
matching with manually extracted keywords; 
numsystem is the count of keywords extracted by 
our method; nummanual is the count of keywords 
assigned by human. 
Figure 2 shows the performance curves for our 
extraction method. In this figure, a ranges from 0 
to 10 while b is 10 to 0. It performs best when a 
= 4 and b = 6. So the two values are adopted in 
this paper. 
 
Figure 2. F-measure varies with the value of a 
We test our approach on another dataset which 
also contains one hundred documents. In the ex-
periments, the max number of keywords is set to 
ten. Table 2 shows the results of keyword extrac-
tion under three different conditions respectively. 
(A) Only extracts single words as keywords 
while just MWEs with (B). (C) The method pre-
sented in this paper which makes a proper com-
bination of MWEs and single words. 
 P R F 
A (single words)  24.2% 28.5% 26.4% 
B (MWEs)  18.1% 23.0% 20.6% 
C (A+B)  34.2% 43.6% 38.9% 
Table 2. Keyword extraction results 
2.2 Keyword Translation 
As for keyword translation, there are three main 
approaches: translation based on dictionary, par-
allel corpora and machine translation. Dictionary 
based approach is adopted in our system by tak-
ing the acquisition of translation resource into 
account. 
Word Sense Disambiguation (WSD) and OOV 
problem are the main difficulties in CLIR (Cross 
Language Information Retrieval) task. A typical 
bilingual dictionary will provide a set of alterna-
tive translations for a given keyword, so how to 
choose the optimal translation is called Word 
Sense Disambiguation. Actually some keywords 
can not be found and translated due to the cover-
age limitation of a bilingual dictionary, which is 
called OOV problem. 
In this paper, the keyword is given up if its 
size of translations gained from the bilingual dic-
tionary is larger than two for the convenience of 
WSD. Additionally, both of the translations are 
treated as synonyms and equal weight is assigned 
to them when retrieval. 
To address the OOV problem, researchers pro-
posed methods using snippets returned by a 
search engine. For example, Wang et al (2004) 
introduced a statistics-based approach called 
SCPCD to mine translations from the returned 
snippets. Different from (Wang et al, 2004), 
Zhou et al (2007) used a pattern-based approach 
to analyze the mixed-languages snippets. 
Leveraging on previous work, we analyze the 
co-occurrence mode of the OOV term and the 
corresponding translation in the returned snippets. 
Table 3 shows the typical co-occurrence modes 
collected during experiments, where the English 
words in bold are the corresponding translations 
of the underlined Chinese OOV terms. From Ta-
ble 3, we can see the translations in number 1, 2 
and 3 are included in the symmetric symbols, 
like bracket, quotation marks. However, the 
475
Serial  
number Segments extracted from the returned snippets 
1 ???????????????The Bridges of Madison County??????? 
2 ???????????The Bridges of Madison County?-52???... 
3 ???????????????cowboy diplomacy????????????? 
4 ...???????????? Bayesian Network for Data Mining-????... 
5 ?????????????The End of Cowboy Diplomacy????????? 
6 ?????????. The Bridges of Madison County. Forrest Gump ????... 
Table 3. Chinese OOV and the corresponding translation in returned snippets 
translations in number 4, 5, and 6 are embedded 
in the partial sentence while there are noise Eng-
lish words. In order to get the correct translation, 
the partial sentence needs to be segmented. By 
above analysis, we integrate the SCPCD method 
and the pattern-based method so as to extract 
more correct translations. The SCPCD method 
can be used to determine the boundaries for 
OOVs like number 4, 5, and 6; while pattern-
based method makes use of the symmetric sym-
bols like number 1, 2 and 3. Table 4 shows the 
experimental results for OOV translation meth-
ods. The average top-n inclusion rate is adopted 
as a metric. For a set of test OOV terms, its top-n 
inclusion rate is defined as the percentage of the 
OOVs whose translations can be found in the 
first n extracted translations. 
 Pattern SCPCD Pattern + SCPCD 
Top-1 40.0% 49.2% 68.1% 
Top-3 41.5% 55.4% 70.2% 
Table 4. The performance comparison of differ-
ent OOV translation methods 
The test dataset used is the Chinese topic 
terms in CLIR task of NTCIR-5. The search en-
gine is Google. The bilingual dictionary used by 
us is LDC_CE_DICT 2.0. And we only adapt the 
pattern with symmetric symbols, which has the 
highest precision proposed by Cao et al (2007). 
2.3 Retrieval and Filtering 
The process of retrieval is to construct the align-
ment relationship between source and target 
document pairs. It is a core module in our system 
since the quality of comparable corpora is greatly 
influenced by alignment level which depends on 
the relevance between document pairs. Our in-
tention here is to retrieve high relevant target 
documents for the source documents. Open-
source toolkit Indri is introduced to assist the 
retrieval process. Indri is a part of the Lemur pro-
ject1. On the basis of Lemur, it combines infer-
ence networks with language modeling. And it?s 
widely adopted by institution for scientific re-
search since it is effective, flexible, usable and 
powerful. So it is employed by us to retrieve re-
lated documents. A query for each source docu-
ment is formed by the translated keywords with 
Indri query language and then run against the 
target collection. 
The essential of alignment is to compare the 
similarity between source and target document 
pairs. In order to reduce the workload of compar-
ing, Pooling method is applied to assist the com-
paring process. We choose the top r documents 
returned by Indri retrieval system to build the 
related document pool. And g (g<=r) documents 
in the pool are selected to form the alignment 
document pairs together with the source docu-
ment. In our experiments, we set r=10 and g=1. 
In the process of alignment, three features are 
used to filter the alignment pairs for the sake of 
pruning the low relevant pairs. The first is publi-
cation date contained in documents. The second 
is similarity calculated by Indri between the 
query and the target document when retrieval. 
The last is KSD (Keyword similarity between 
document pairs) which is defined by our system. 
In this paper, we propose two methods to filter 
the alignment pairs by using various features. 
(1) DSF filtering 
This method depends on two features: date 
and similarity. At first, we give a priority to the 
target documents that have the closest date to the 
source document during the top-r documents 
searching. A date-window size d is defined to 
measure the date difference. We set d=1 in this 
paper. That is to say, the target documents with 
                                                 
1 Lemur toolkit is developed by Carnegie Mellon University 
and University of Massachusetts. The open source code is 
available at http://www.lemurproject.org. 
476
exactly the same date as the source document, 
and one day earlier or later are considered to be 
closest. Then, we select g documents with larger 
similarity from the related document pool. Fi-
nally, we rank all of the alignment pairs with the 
score of similarity and set a similarity threshold s 
to filter further. It should be noted that there are 
n ? g alignment pairs, where n is the number of 
source documents having non-empty related 
document pool. 
(2) DSKF filtering 
This method utilizes all of the features: date, 
similarity and KSD. As for KSD, it integrates 
two factors. One is NTK, namely the number of 
translated keywords appeared in the target 
document, since the target document is more 
similar to the source document as increasing of 
NTK. The other is FIS, namely frequency infor-
mation score. Inspired by paper (Tao and Zhai, 
2005), we use the score of FIS to measure the 
correlations between the keywords in source 
document and translated keywords in target 
document which represent the matching for 
source and target document pair. We define ds as 
the source document, dt as the target document, 
ks as the set of keywords extracted from ds, kts as 
the set of translated keywords. Formula 2 is used 
to compute the score of FIS: 
 1 ( 25( , ) ( )
25( , ) ( ) / ( ( , )))
ktsLen
FIS i s ii
i t i i i
Score BM x d IDF x
BM y d IDF y norm Dif x y
=
= ? ?
?
?  (2) 
Where, ktsLen is the size of kts, yi is an element 
in kts, xi is the element in ks while yi is the trans-
lation of xi. Moreover, BM25(w, d) is the normal-
ized frequency of word w in document d. It has 
been considered as one of the most effective 
matching functions for retrieval. IDF stands for 
Inverse Document Frequency which is also 
commonly used in information retrieval. Dif(x, y) 
is defined as the difference between BM25(x, ds) 
and BM25(y, dt). Formula 2 penalizes large dif-
ference due to the conditions like this: any key-
word in source document appears many times 
while its translation appears rarely in target 
document. The process of its normalization is run 
by Formula 3 which makes the score less sensi-
tive to the absolute value: 
1, 1( ) ,
scorenorm score score else
<?
= ??       (3) 
Furthermore, the final KSD score is got by 
simply adding the normalized scores of NTK and 
FIS which are dealt with Formula 3. Actually, the 
two filtering methods differ principally in the last 
step. DSKF sorts all of the alignment pairs ac-
cording to the KSD score while it is similarity in 
DSF. We also set a KSD threshold k for DSKF 
method to filter further. The values for s and k 
will be investigated in the following experiments. 
3 Experiments 
In this section, we first introduce how to acquire 
the source and target document sets. Then our 
system is tested on the two sets. The experimen-
tal results are reported and analyzed finally. 
3.1 Experiment Setup 
To test the effectiveness of the proposed system, 
large-scale of Chinese and English news web 
pages are crawled respectively from XinHuaNet 
and used as the document resource. The reasons 
for choosing news pages are: 
(1) Many websites, like portal website, news 
agency, government and so on, provide large-
scale news reports. At the same time, a large pro-
portion of the reports can be crawled politely, so 
document acquisition is relatively easy. 
(2) The news pages include various contents, 
such as politics, economy, sports, so the corpora 
made up of news pages can avoid the limitations 
of domain-specific corpora. 
All the news pages are processed uniformly. 
The core content of each web page crawled is 
extracted and several tags describing the headline 
and publication date are added. Meanwhile, the 
original contents are kept with no change. Table 
5 shows the basic information of document sets.  
Year Number of source documents 
Number of target 
documents 
2003 23747 3390 
2004 25660 2943 
2005 47333 11578 
2006 28572 25320 
2007 25036 25247 
2008 14021 24292 
2009 7476 10887 
Total 171845 103657 
Table 5. The composition of source document set 
and target document set 
3.2 Results and Discussion 
The quality of comparable corpora highly de-
477
pends on the alignment level between source and 
target document pairs. Braschler and Sc?uble 
(1998) used five levels of relevance to assess the 
alignments as follows: 
(1) Same story. The two documents deal with 
the same event.  
(2) Related story. The two documents deal 
with the same event or topic from a slightly dif-
ferent viewpoint. Alternatively, the other docu-
ment may concern the same event or topic, but 
the topic is only a part of a broader story or the 
article is comprised of multiple stories. 
(3) Shared aspect. The documents deal with 
related events. They may share locations or per-
sons. 
(4) Common terminology. The events or topics 
are not directly related, but the documents share 
a considerable amount of terminology. 
(5) Unrelated. The similarities between the 
documents are slight or nonexistent.  
We randomly select 500 source documents 
published in 2009 as the test dataset. Experi-
ments with different parameters are constructed 
based on this dataset. The quality of each align-
ment pair is manually assessed using the five-
level relevance as discussed above. What should 
to be pointed out is that parameter s and k are not 
absolute values, but percentile rank level in our 
work. For instance, k = 10 means that we only 
choose the alignment pairs whose KSD score 
rank in top ten percent among all of the results. 
Table 6 shows the results filtered by DSF 
method with different values of s (s1 < s2 < s3 < 
s4 ). Table 7 shows the results filtered by DSKF 
method with various values of k (k1 < k2 < k3 < 
k4). In order to evaluate the results conveniently, 
two standards are established: (a) the number of 
high relevant pairs created, which is the count of 
document pairs in Level 1 and 2; (b) the quality 
of the whole alignments, that is to say the per-
centage of alignment pairs with Level 1 and 2. 
Seen from Table 6 and 7, DSKF is better than 
DSF by considering the two standards. Com-
pared with DSF, more high relevant pairs are left 
filtered by DSKF when they have the same total 
number of pairs. In other words, the DSKF 
method is more powerful to make high relevant 
pairs in higher rank so as to reduce alignment 
pairs which are rarely relevant. Therefore, DSKF 
is adopted in our system. Taking the first crite-
rion into account, we give up the parameter k1, k2. 
Parameter k4 is not the best considering the sec-
ond criterion. Ultimately, k3 is chosen as the final 
value for k. At this point, the number of align-
ment pairs in Level 1 and 2 is close to the maxi-
mum. Meanwhile, the percentage of high align-
ments reaches 68.5%. 
Among the surveyed related work, Talvensaari 
et al (2007) created Swedish-English compara-
ble corpora based on CLIR techniques and its 
framework of construction is similar to ours. 
However, the two systems are different in the 
following aspects: 
s1 = 10 s2 = 30 s3 =50 s4 =70 Level Number % Number % Number % Number % 
Leve1 1 23 46.9% 54 36.5% 83 33.5% 96 27.7% 
Level 2 18 36.7% 43 29.1% 62 25.0% 81 23.3% 
Level 3 4 8.2% 21 14.2% 40 16.1% 57 16.4% 
Level 4 4 8.2% 19 12.8% 41 16.5% 60 17.3% 
Level 5 0 0.0% 11 7.4% 22 8.9% 53 15.3% 
Total 49 100% 148 100% 248 100% 347 100% 
Table 6. The distribution results filtered by DSF with different s parameters 
k1 = 10 k2 = 30 k3 = 50 k4 =70 Level Number % Number % Number % Number % 
Level 1 33 67.3% 78 52.7% 93 37.5% 98 28.2% 
Level 2 15 30.6% 52 35.1% 77 31.0% 89 25.6% 
Level 3 1 2.0% 9 6.1% 37 14.9% 62 17.9% 
Level 4 0 0.0% 9 6.1% 34 13.7% 60 17.3% 
Level 5 0 0.0% 0 0.0% 7 2.8% 38 11.0% 
Total 49 100% 148 100% 248 100% 347 100% 
Table 7. The distribution results filtered by DSKF with different k parameters 
478
(1) The language is different. We focus on 
building comparable corpora of Chinese-English 
while they were Swedish-English. 
(2) A series of sub problems are different due 
to language difference. As for keyword extrac-
tion, we propose a method to select both key 
phrases and single words, while they used RATF 
(Relative Average Term Frequency) method. For 
OOV problem, we combine the SCPCD method 
with the pattern-based method to extract OOV 
translations from snippets returned by a search 
engine. However, the classified s-gram matching 
technique was utilized by Talvensaari et al (2007) 
to translate OOV words. 
(3) Talvensaari et al (2007) filtered their 
alignment pairs mainly depending on date and 
similarity, while we introduce new feature KSD 
to extend the original feature set. 
Talvensaari et al (2007) also randomly chose 
500 source documents and assessed the quality 
of alignments using the same five-level relevance. 
In addition to this, we implement the method 
of Tao and Zhai (2005) which is a purely statisti-
cal-based and language independent approach. 
The source and target documents published in 
2009 are employed to test the method. The same 
sample as our system including 500 Chinese 
documents is chosen to make a further compari-
son with our work. We align each source docu-
ment with one target document through the 
BM25Corr model in (Tao and Zhai, 2005). The 
alignment pairs are ranked according to mapping 
scores calculated by the BM25Corr model. And 
we select the top N (N = 248) alignment pairs for 
the benefit of comparison.  
Table 8 shows the distribution results for the 
three systems. As illustrated in Table 8, we can 
roughly conclude that our approach creates more 
alignment pairs with the same number of source 
documents when compared with Talvensaari et al 
(2007). Meanwhile, the percentage of high rele-
vant document pairs is larger.  
Likewise, our system outperforms BM25Corr 
in that it aligns more high relevant documents 
pairs when they use the same sample of test cor-
pora and create the same total number of pairs. 
Obviously, the quality of comparable corpora 
gained by our system is better than BM25Corr. 
All the experimental results and analysis men-
tioned above indicate that our method is effective 
to create alignment pairs. Up to now, both the 
source and target documents published in 2007-
2009 years are used to build comparable corpora 
through our proposed system. It includes 23102 
alignment pairs after filtered by DSKF. 
Talvensaari et al  
(2007) 
Our System 
(DSKF filtering) 
BM25Corr 
(Top N = 248) Level 
Number % Number % Number % 
Level 1 21 21.6% 93 37.5% 1 0.4% 
Level 2 20 20.6% 77 31.0% 2 0.8% 
Level 3 33 34.0% 37 14.9% 3 1.2% 
Level 4 19 19.6% 34 13.7% 5 2.0% 
Level 5 4 4.1% 7 2.8% 237 95.6% 
Total 97 100% 248 100% 248 100% 
Table 8. The distribution results for Talvensaari et al (2007), Our System, and BM25Corr 
4 Conclusions 
In this paper, we propose a CLIR-based approach 
to create large-scale Chinese-English comparable 
corpora. Firstly, we harvest the original source 
and target document sets from news website us-
ing open-source crawler. Then the core content 
of each document is extracted through discrimi-
nating noise contents. Next, we delve into the 
approaches of problems such as keyword extrac-
tion and OOV translation followed by the proc-
ess of retrieval to develop mapping correlations 
between source and target documents. Finally, 
three features as publication date, similarity 
score and KSD value are used to filter the 
aligned document pairs. Experimental results 
show that our approach is effective to mine Chi-
nese-English document pairs with comparable 
contents. In the future, we will optimize the ap-
proach for every module in the construction of 
comparable corpora for the sake of improving 
the performance of the whole system. What?s 
more, it will be worth consideration to mine 
mappings between terms which can be served as 
a feature for the process of developing mappings 
between document pairs in turn. 
479
References 
Aires, Jos?, Gabriel Lopes, and Joaquim Ferreira 
Silva. 2008. Efficient Multi-word Expressions Ex-
tractor Using Suffix Arrays and Related Structures. 
In Proceeding of the 2nd ACM workshop on Imp-
roving non english web searching, pp. 1-8.  
Bracewell, David B., Fuji Ren, and Shingo Kuroiwa. 
2008. Mining News Sites to Create Special Do-
main News Collections. International Journal of 
Computational Intelligence, 4(1): 56-63. 
Braschler, Martin, and Peter Sc?uble. 1998. Multilin-
gual Information Retrieval Based on Document 
Alignment Techniques. In Proceedings of the 2nd 
European Conference on Research and Advanced 
Technology for Digital Libraries, pp. 183-197. 
Cao Guihong, Jianfeng Gao, and Jianyun Nie. A Sys-
tem to Mine Large-Scale Bilingual Dictionaries 
from Monolingual Web pages. 2007. In Proceed-
ings of Machine Translation Summit XI, pp. 57-64. 
Frank, Eibe, Gordon W. Paynter, and Ian H. Witten. 
1999. Domain-Specific Keyphrase Extraction. In 
Proceedings of the 16th International Joint Con-
ference on Artificial Intelligence, pp. 668-673. 
Li Juanzi, Qi?na Fan, and Kuo Zhang. 2007. Keyword 
Extraction Based on tf/idf for Chinese News 
Document. Wuhan University Journal of Natural 
Sciences, 12(5): 917-921. 
Liu Tao, Bingquan Liu, Xiaolong Wang, and Minghui 
Li. 2007. The Effectiveness Study of Local Maxi-
mum Feature for Chinese Unknown Word Identifi-
cation. Journal of Chinese Language and Comput-
ing, 17(1): 15-26. 
Luhn, Hans Peter. 1957. A Statistical Approach to 
Mechanized Encoding and Searching of Literary 
Information. IBM Journal of Research and Devel-
opment, 1(4): 309-317. 
Luo Yanyan, and Degen Huang. 2009. Chinese Word 
Segmentation Based on the Marginal Probabilities 
Generated by CRFs. Journal of Chinese Informa-
tion Processing, 23(5): 3-8. 
Matsuo, Yutaka and Mitsuru Ishizuka. 2004. Key-
word Extraction from a Single Document Using 
Word Co-occurrence Statistical Information. Inter-
national Journal on Artificial Intelligence Tools, 
13(1): 157-169. 
Munteanu, Dragos Stefan. 2006. Exploiting Compa-
rable Corpora. Doctoral Thesis. UMI Order 
No.3257825. University of Southern California. 
Resnik, Philip. 1999. Mining the web for bilingual 
text. In Proceedings of the 37th Annual Meeting of 
the Association for Computational Linguistics, pp. 
527-534. 
Silva, Joaquim Ferreira, Ga?l Dias, Sylvie Guillor?, 
and Jos? Gabriel Pereira Lopes. 1999. Using Lo-
calMaxs Algorithm for the Extraction of Contigu-
ous and Non-contiguous Multiword Lexical Units. 
In Proceedings of the 9th Portuguese Conference 
on Artificial Intelligence, pp. 113-132. 
Talvensaari, Tuomas, Jorma Laurikkala, Kalervo 
J?rvelin, Martti Juhola and Heikki Keskustalo. 
2007. Creating and Exploiting a Comparable Cor-
pus in Cross-Language Information Retrieval. 
ACM Transactions on Information Systems, 
25(1):1-21. 
Tao Tao, and Chengxiang Zhai. 2005. Mining Com-
parable Bilingual Text Corpora for Cross-
Language Information Integration. In Proceedings 
of the 11th ACM SIGKDD international conference 
on Knowledge discovery in data mining, pp. 691-
696. 
Wan Xiaojun, and Jianguo Xiao. 2008. CollabRank: 
Towards a Collaborative Approach to Single-
Document Keyphrase Extraction. In Proceeding of 
the 22nd International Conference on Computa-
tional Linguistics, pp. 969-976. 
Wang Jenq Haur, Jie Wen Teng, Pu Jen Cheng, Wen 
Hsiang Lu, and Lee Feng Chien. 2004. Translating 
Unknown Cross-Lingual Queries in Digital Librar-
ies using a Web-based Approach. In Proceedings 
of the 4th ACM/IEEE-CS joint Conference on Digi-
tal Libraries, pp. 108-116. 
Witten, Ian H., Gordon W. Paynter, Eibe Frank, Carl 
Gutwin, and Craig G. Nevill-Manning. 1999. KEA: 
Practical automatic keyphrase extraction. In Pro-
ceedings of the 4th ACM Conference on Digital Li-
braries, pp. 254-255.  
Zhang Chengzhi, Huilin Wang, Yao Liu, Dan Wu, Yi 
Liao, and Bo Wang. 2008. Automatic Keyword 
Extraction from Documents Using Conditional 
Random Fields. Journal of Computational Infor-
mation Systems, 4(3): 1169-1180. 
Zhang Kuo, Hui Xu, Jie Tang, and Juanzi Li. 2006. 
Keyword Extraction Using Support Vector Ma-
chines. In Proceedings of the 7th International 
Conference on Web-Age Information Management, 
pp. 85-96. 
Zhou Dong, Mark Truran, Tim Brailsford, and Helen 
Ashman. 2007. NTCIR-6 Experiments Using Pat-
tern Matched Translation Extraction. In Proceed-
ings of 6th NTCIR Workshop Meeting, pp. 145-151. 
480
Proceedings of the Fourteenth Conference on Computational Natural Language Learning: Shared Task, pages 106?113,
Uppsala, Sweden, 15-16 July 2010. c?2010 Association for Computational Linguistics
 
 
Exploiting Multi-Features to Detect Hedges and Their Scope in 
Biomedical Texts
 
Huiwei Zhou1, Xiaoyan Li2, Degen Huang3, Zezhong Li4, Yuansheng Yang5 
Dalian University of Technology 
Dalian, Liaoning, China 
{1zhouhuiwei, 3huangdg, 5yangys}@dlut.edu.cn 
2lixiaoyan@mail.dlut.edu.cn 
4lizezhonglaile@163.com 
Abstract 
In this paper, we present a machine learning 
approach that detects hedge cues and their 
scope in biomedical texts. Identifying hedged 
information in texts is a kind of semantic 
filtering of texts and it is important since it 
could extract speculative information from 
factual information. In order to deal with the 
semantic analysis problem, various evidential 
features are proposed and integrated through a 
Conditional Random Fields (CRFs) model. 
Hedge cues that appear in the training dataset 
are regarded as keywords and employed as an 
important feature in hedge cue identification 
system. For the scope finding, we construct a 
CRF-based system and a syntactic 
pattern-based system, and compare their 
performances. Experiments using test data 
from CoNLL-2010 shared task show that our 
proposed method is robust. F-score of the 
biological hedge detection task and scope 
finding task achieves 86.32% and 54.18% in 
in-domain evaluations respectively. 
1. Introduction 
Identifying sentences in natural language texts 
which contain unreliable or uncertain information 
is an increasingly important task of information 
extraction since the extracted information that 
falls in the scope of hedge cues cannot be 
presented as factual information. Szarvas et al 
(2008) report that 17.69% of the sentences in the 
abstracts section of the BioScope corpus and 
22.29% of the sentences in the full papers section 
contain hedge cues. Light et al (2004) estimate 
that 11% of sentences in MEDLINE abstracts 
contain speculative fragments. Szarvas (2008) 
reports that 32.41% of gene names mentioned in 
the hedge classification dataset described in 
Medlock and Briscoe (2007) appear in a 
speculative sentence. Many Wikipedia articles 
contain a specific weasel tag which mark 
sentences as non-factual (Ganter and Strube, 
2009). 
There are some Natural Language Processing 
(NLP) researches that demonstrate the benefit of 
hedge detection experimentally in several 
subjects, such as the ICD-9-CM coding of 
radiology reports and gene named Entity 
Extraction (Szarvas, 2008), question answering 
systems (Riloff et al, 2003), information 
extraction from biomedical texts (Medlock and 
Briscoe, 2007). 
The CoNLL-2010 Shared Task (Farkas et al, 
2010) ?Learning to detect hedges and their scope 
in natural language text? proposed two tasks 
related to speculation research. Task 1 aimed to 
identify sentences containing uncertainty and 
Task 2 aimed to resolve the in-sentence scope of 
hedge cues. We participated in both tasks. 
In this paper, a machine learning system is 
constructed to detect sentences in texts which 
contain uncertain or unreliable information and to 
find the scope of hedge cues. The system works 
in two phases: in the first phase uncertain 
sentences are detected, and in the second phase 
in-sentence scopes of hedge cues are found. In the 
uncertain information detecting phase, hedge 
cues play an important role. The sentences that 
contain at least one hedge cue are considered as 
uncertain, while sentences without cues are 
considered as factual. Therefore, the task of 
uncertain information detection can be converted 
into the task of hedge cue identification. Hedge 
cues that appear in the training dataset are 
collected and used as keywords to find hedges. 
Furthermore, the detected keywords are 
employed as an important feature in hedge cue 
identification system. In addition to keywords, 
various evidential features are proposed and 
integrated through a machine learning model. 
Finding the scope of a hedge cue is to determine 
at sentence level which words are affected by the 
106
  
hedge cue. In the scope finding phase, we 
construct a machine learning-based system and a 
syntactic pattern-based system, and compare their 
performances. 
For the learning algorithm, Conditional random 
fields (CRFs) is adopted relying on its flexible 
feature designs and good performance in 
sequence labeling problems as described in 
Lafferty et al (2001). The main idea of CRFs is 
to estimate a conditional probability distribution 
over label sequences, rather than over local 
directed label sequences as with Hidden Markov 
Models (Baum and Petrie, 1966) and Maximum 
Entropy Markov Models (McCallum et al, 
2000). 
Evaluation is carried out on the CoNLL-2010 
shared task (Farkas et al, 2010) dataset in which 
sentences containing uncertain information are 
annotated. For the task of detecting uncertain 
information, uncertain cues are annotated. And 
for the task of finding scopes of hedge cues, 
hedge cues and their scope are annotated as 
shown in sentence (a): hedge cue indicate that, 
and its scope indicate that dhtt is widely 
expressed at low levels during all stages of 
Drosophila development are annotated. 
 
(a)Together, these data <xcope 
id="X8.74.1"><cue ref="X8.74.1" 
type="speculation">indicate that</cue> dhtt 
is widely expressed at low levels during all 
stages of Drosophila development</xcope>. 
2. Related Work 
In the past few years, a number of studies on 
hedge detection from NLP perspective have been 
proposed. Elkin et al (2005) exploited 
handcrafted rule-based negation/uncertainty 
detection modules to detect the negation or 
uncertainty information. However, their detection 
modules were hard to develop due to the lack of 
standard corpora that used for evaluating the 
automatic detection and scope resolution. Szarvas 
et al (2008) constructed a corpus annotated for 
negations, speculations and their linguistic scopes. 
It provides a common resource for the training, 
testing and comparison of biomedical NLP 
systems. 
Medlock and Briscoe (2007) proposed an 
automatic classification of hedging in biomedical 
texts using weakly supervised machine learning. 
They started with a very limited amount of 
annotator-labeled seed data. Then they iterated 
and acquired more training seeds without much 
manual intervention. The best classifier using 
their model achieved 0.76 precision/recall 
break-even-point (BEP). Further, Medlock 
(2008) illuminated the hedge identification task 
including annotation guidelines, theoretical 
analysis and discussion. He argued for separation 
of the acquisition and classification phases in 
semi-supervised machine learning method and 
presented a probabilistic acquisition model. In 
probabilistic model he assumed bigrams and 
single terms as features based on the intuition that 
many hedge cues are bigrams and single terms 
and achieves a peak performance of around 0.82 
BEP.  
Morante and Daelemans (2009) presented a 
meta-learning system that finds the scope of 
hedge cues in biomedical texts. The system 
worked in two phases: in the first phase hedge 
cues are identified, and in the second phase the 
full scopes of these hedge cues are found. The 
performance of the system is tested on three 
subcorpora of the BioScope corpus. In the hedge 
finding phase, the system achieves an F-score of 
84.77% in the abstracts subcorpus. In the scope 
finding phase, the system with predicted hedge 
cues achieves an F-score of 78.54% in the 
abstracts subcorpus. 
The research on detecting uncertain 
information is not restricted to analyze 
biomedical documents. Ganter and Strube (2009) 
investigated Wikipedia as a source of training 
data for the automatic hedge detection using word 
frequency measures and syntactic patterns. They 
showed that the syntactic patterns worked better 
when using the manually annotated test data, 
word frequency and distance to the weasel tag 
was sufficient when using Wikipedia weasel tags 
themselves. 
3. Identifying Hedge Cues 
Previous studies (Light et al, 2004) showed that 
the detection of hedging could be solved 
effectively by looking for specific keywords 
which were useful for deciding whether a 
sentence was speculative. Szarvas (2008) reduces 
the number of keyword candidates without 
excluding helpful keywords for hedge 
classification. Here we also use a simple 
keyword-based hedge cue detection method. 
3.1 Keyword-based Hedge Cue Detection 
In order to recall as many hedge cues as possible, 
107
  
all hedge cues that appear in the training dataset 
are used as keywords. Hedge cues are represented 
by one or more tokens. The list of all hedge cues 
in the training dataset is comprised of 143 cues. 
90 hedge cues are unigrams, 24 hedge cues are 
bigrams, and the others are trigrams, four-grams 
and five-grams. Besides, hedge cues that appear 
in the training dataset and their synonyms in 
WordNet 1  are also selected as keywords for 
hedge cue detection. The complete list of them 
contains 438 keywords, 359 of which are 
unigrams. Many tokens appear in different grams 
cues, such as possibility appears in five-grams 
cue cannot rule out the possibility, four-gram cue 
cannot exclude the possibility, trigrams cue raise 
the possibility and unigram cue possibility. To 
find the complete cues, keywords are matched 
through a maximum matching method (MM) (Liu 
et al, 1994). For example, though indicate and 
indicate that are both in keywords list, indicate 
that is extracted as a keyword in sentence (a) 
through MM. 
3.2 CRF-based Hedge Cue Detection 
Candidate cues are extracted based on keywords 
list in keyword-based hedge cue detection stage. 
But the hedge cue is extremely ambiguous, so 
CRFs are applied to correct the false 
identification results that occurred in the 
keyword-based hedge cue detection stage. The 
extracted hedge cues are used as one feature for 
CRFs-based hedge cue detection. 
A CRF identifying model is generated by 
applying a CRF tool to hedge cue labeled 
sequences. Firstly, hedge cue labeled sentences 
are transformed into a set of tokenized word 
sequences with IOB2 labels: 
 
B-cue Current token is the beginning of a 
hedge cue 
I-cue Current token is inside of  a hedge cue 
O Current token is outside of any hedge 
cue  
 
For sentence (a) the system assigns the B-cue 
tag to indicate, the I-cue tag to that and the O tag 
to the rest of tokens as shown in Figure1. 
The hedge cues that are found by 
keyword-based method is also given IOB2 labels 
feature as shown in Figure1. 
                                                          
1
 Available at http://wordnet.princeton.edu/ 
 
 
 
 
 
 
 
 
 
 
Text 
? 
these 
data 
indicate 
that 
dhtt 
is 
... 
Keyword Labels Feature 
...       
O   
O 
B 
I 
O 
O 
...                            
Cue Labels  
...       
O   
O 
B-cue 
I-cue 
O 
O 
...                          
 
Figure 1: Example of Cues labels and Keywords 
labels Feature 
 
Diverse features including keyword feature are 
employed to our CRF-based hedge cue detection 
system. 
 
(1) Word Features 
? Word (i) (i=-n, ?, ?2, ?1, 0, +1, +2, ?, +n) 
Where Word (0) is the current word, Word (-1) 
is the first word to the left, Word (1) is the first 
word to the right, etc. 
 
(2) Stem Features 
The motivation for stemming in hedge 
identification is that distinct morphological forms 
of hedge cues are used to convey the same 
semantics (Medlock, 2008). In our method, 
GENIA Tagger2 (Tsuruoka et al, 2005) is applied 
to get stem features. 
? Stem (i) (i=-n, ?, ?2, ?1, 0, +1, +2, ?, +n) 
where Stem (0) is the stem for the current word, 
Stem(-1) is the first stem to the left, Stem (1) is the 
first stem to the right, etc. 
 
(3) Part-Of-Speech Features  
Since most of hedge cues in the training dataset 
are verbs, auxiliaries, adjectives and adverbs. 
Therefore, Part-of-Speech (POS) may provide 
useful evidence about the hedge cues and their 
boundaries. GENIA Tagger is also used to 
generate this feature.  
? POS (i) (i=-n, ?, ?2, ?1, 0, +1, +2, ?, +n) 
where POS (0) is the current POS, POS (-1) is 
the first POS to the left, POS (1) is the first POS 
to the right, etc. 
 
(4) Chunk Features 
Some hedge cues are chunks consisting of more 
than one token. Chunk features may contribute to 
the hedge cue boundaries. We use GENIA 
Tagger to get chunk features for each token. The 
                                                          
2
 Available at 
http://www-tsujii.is.s.u-tokyo.ac.jp/GENIA/tagger/ 
108
  
chunk features include unigram, bigram, and 
trigram types, listed as follows: 
? Chunk (i) (i=-n, ?, ?2, ?1, 0, +1, +2, ?, +n) 
? Chunk (i?1)+Chunk(i) (i =?1,0,+1,+2) 
? Chunk (i?2) + Chunk (i?1)+Chunk (i) (i= 
0,+1,+2) 
where Chunk (0) is the chunk label for the 
current word, Chunk (?1) is the chunk label for 
the first word to the left , Chunk (1) is the chunk 
label for the first word to the right, etc. 
 
(5) Keyword Features 
Keyword labels feature is an important feature. 
? Keyword (i) (i=-n, ?, ?2, ?1, 0, +1, +2, ?, 
+n) 
where Keyword (0) is the current keyword label, 
Keywords (-1) is the keyword label for the first 
keyword to the left, Keywords (1) is the keyword 
label for the first keyword to the right, etc. 
Feature sets can be easily redefined by 
changing the window size n. The relationship of 
the window size and the F-score observed in our 
experiments will be reported in Section 5. 
4. Hedge Scope Finding 
In this task, a CRFs classifier is applied to predict 
for all the tokens in the sentence whether a token 
is the first token of the scope sequence (F-scope), 
the last token of the scope sequence (L-scope), or 
neither (None). For sentence (a) in Section 1, the 
classifier assigns F-scope to indicate, L-scope to 
benchmarks, and None to the rest of the tokens. 
Only sentences that assigned cues in the first 
phase are selected for hedge scope finding. 
Besides, a syntactic pattern-based system is 
constructed, and compared with the CRF-based 
system. 
4.1 CRF-based System 
The features that used in CRF-based hedge cue 
detection systems are also used for scope finding 
except for the keyword features. The features are: 
 
(1) Word Features 
? Word (i) (i=-n, ?, ?2, ?1, 0, +1, +2, ?, +n) 
 
(2) Stem Features 
? Stem (i) (i=-n, ?, ?2, ?1, 0, +1, +2, ?, +n) 
(3) Part-Of-Speech Features  
? POS (i) (i=-n, ?, ?2, ?1, 0, +1, +2, ?, +n) 
 
(4) Chunk Features 
The chunk features include unigram, bigram, 
and trigram types, listed as follows: 
 
? Chunk (i) (i=-n, ?, ?2, ?1, 0, +1, +2, ?, +n) 
? Chunk (i?1)+Chunk(i) (i =?1,0,+1,+2) 
? Chunk (i?2) + Chunk (i?1)+Chunk (i) (i= 
0,+1,+2) 
 
(5) Hedge cues Features 
Hedge cues labels that are doped out in Task 1 
are selected as an important feature. 
 
? Hedge cues (i) (i=-n, ?, ?2, ?1, 0, +1, +2, ?, 
+n) 
where Hedge cues (0) is the cue label for the 
current word, Hedge cues (?1) is the cue label for 
the first word to the left , Hedge cues (1) is the 
cue label for the first word to the right, etc. 
The scope of the sequence must be consistent 
with the hedge cues. That means that the number 
of the F-scope and L-scope must be the same with 
the hedge cues. However, sometimes their 
number predicted by classifier is not same. 
Therefore, we need to process the output of the 
classifier to get the complete sequence of the 
scope. The following post processing rules are 
adapted. 
 
? If the number of F-scope, L-scope and hedge 
cue is the same, the sequence will start at the 
token predicted as F-scope, and end at the 
token predicted as L-scope. 
? If one token has been predicted as F-scope 
and none has been predicted as L-scope, the 
sequence will start at the token predicted as 
F-scope and end at the end of the sentence. 
Since when marking the scopes of keywords, 
linguists always extend the scope to the biggest 
syntactic unit possible. 
? If one token has been predicted as L-scope 
and none has been predicted as F-scope, the 
sequence will start at the hedge cue and end at 
the token predicted as L-scope. Since scopes 
must contain their cues. 
? If one token has been predicted as F-scope 
and more than one has been predicted as 
L-scope, the sequence will end at the first token 
predicted as L-scope. Statistics from prediction 
on CoNLL-2010 Shared Task evaluation data 
show that 20 sentences are in this case. And the 
scope of 6 sentences extends to the first 
L-scope, and the scope of 3 sentences end at 
the last L-scope, the others are predicted 
mistakenly. Our system prediction and 
gold-standard annotation are shown in sentence 
(b1) and (b2) respectively. 
109
  
 
(b1) our system annotation: 
dRas85DV12 <xcope id="X3.64.1"><cue 
ref="X3.64.1" type="speculation">may</cue> 
be more potent than dEGFR?</xcope> because 
dRas85DV12 can activate endogenous PI3K 
signaling</xcope> [16]. 
 
(b2) gold-standard annotation: 
dRas85DV12 <xcope id="X3.64.1"><cue 
ref="X3.64.1" 
type="speculation">may</cue> be more 
potent than dEGFR?</xcope> because 
dRas85DV12 can activate endogenous PI3K 
signaling [16]. 
 
? If one token has been predicted as L-scope 
and more than one has been predicted as 
F-scope, the sequence will start at the first 
token predicted as F-scope. 
? If an L-scope is predicted before an F-scope, 
the sequence will start at the token predicted as 
F-scope, and finished at the end of the sentence.  
4.2 Syntactic Pattern-based System 
Hedge scopes usually can be determined on the 
basis of syntactic patterns dependent on the cue. 
Therefore, a syntactic pattern-based system is 
also implemented for hedge scope finding. When 
the sentence is predicted as uncertain, the toolkit 
of Stanford Parser3 (Klein and Manning, 2003) is 
utilized to parse the sentence into a syntactic tree, 
which can release a lot of information about the 
grammatical structure of sentences that is 
beneficial for the finding of hedge scope. For 
sentence (c) the Stanford Parser gives the 
syntactic tree as showed in Figure 2. 
 
(c) This <xcope id="X*.*.*"><cue ref="X*.*.*" 
type="speculation"> may </cue> represent a 
viral illness</xcope>. 
It is obvious to see from the syntactic tree, all 
the words of the parsed sentence concentrate at 
the places of leaves. We use the following rules to 
find the scope. 
? If the tag of the word is ?B-cue?, it is predicted 
as F-scope. 
? If the POS of the hedge cue is verbs and 
auxiliaries, the L-scope is signed at the end of the 
clause. 
? If the POS of the hedge cue is attributive 
                                                          
3
 Available at 
http://nlp.stanford.edu/software/lex-parser.shtml  
adjectives, the L-scope is signed at the following 
noun phrase.  
? If the POS of the hedge cue is prepositions, the 
L-scope is signed at the following noun phrase. 
? If none of the above rules apply, the scope of a 
hedge cue starts with the hedge cue and ends at 
the following clause. 
 
 
 
Figure 2: Syntactic tree parsed by Stanford 
Parser 
5. Experiments and Discussion 
We evaluate our method using CoNLL-2010 
shared task dataset. The evaluation of uncertain 
information detection task is carried out using the 
sentence-level F-score of the uncertainty class. 
As mentioned in Section 1, Task 1 is converted 
into the task of hedge cues identification. 
Sentences can be classified as certain or uncertain 
according to the presence or absence of a few 
hedge cues within the sentences. In task of 
finding in-sentence scopes of hedge cues, a scope 
is correct if all the tokens in the sentence have 
been assigned the correct scope class for a 
specific hedge signal. 
5.1 Detecting Uncertain Information 
In the CoNLL-2010 Shared Task 1, our 
in-domain system obtained the F-score of 85.77%. 
Sentence-level results of in-domain systems 
under the condition n=3 (window size) are 
summarized in Table 1.  
 
System Prec. Recall F-score 
Keyword-based 41.15 99.24 58.18 
CRF-based system 
(without keyword 
features) 
88.66 80.13 84.18 
CRF-based system 
+ keyword features 
86.21 84.68 85.44 
CRF-based system 86.49 85.06 85.77 
110
  
+ keyword features 
+ MM 
 
Table 1: Official in-domain results for Task 1 
(n=3) 
 
The keyword-based system extracts hedge cues 
through maximum matching method (MM). As 
can be seen in Table 1, the system achieves a high 
recall (99.24%). This can be explained that 
almost all of the hedge cues in the test dataset are 
in the keywords list. However, it also brings 
about the low precision since not all potential 
speculative keywords convey real speculation. So 
the keyword-based method can be combined with 
our CRF-based method to get better performance. 
All the CRF-based systems in Table 1 
significantly outperform the keyword-based 
system, since the multi-features achieve a high 
precision. And the result with keyword features is 
better than the result without it. The keyword 
features improve the performance by recalling 39 
true positives. In addition, further improvement is 
achieved by using Maximum Matching method 
(MM). 
In the test dataset, there should be a few hedge 
cues not in the training dataset. And the 
additional resources besides the manually labeled 
data are allowed for in-domain predictions. 
Therefore, the synonyms of the keywords can be 
used for in-domain systems. The synonyms of the 
keywords are added to the keywords list, and are 
expected to improve detecting performance. The 
synonyms are obtained from WordNet. 
Table 2 shows the relationship between the 
window size and the sentence-level results. This 
table shows the results with and without 
synonyms. Generally, the results with synonyms 
are better than the results without them. With 
respect to window size, the wider the window 
size, the better precision can be achieved. 
However, large window size leads to low recall 
which is probably because of data sparse. The 
best F-score 86.32 is obtained when the window 
size is +/-4. 
 
Window 
size 
Synonym
s 
 
Prec. Recall F-score 
without 
synonyms 
85.27 86.46 85.86 1 
with 
synonyms 
85.66 86.20 85.93 
without 
synonyms 
86.35 85.70 86.02 2 
with 86.14 84.94 85.53 
without 
synonyms 
86.49 85.06 85.77 3 
with 
synonyms 
86.69 84.94 85.81 
without 
synonyms 
86.34 84.81 85.57 4 
with 
synonyms 
87.21 85.44 86.32 
 
Table 2: Sentence-level results relative to 
synonyms and window size for speculation 
detection 
5.2 Finding Hedge Scope 
In the CoNLL-2010 Shared Task 2, our 
in-domain system obtained the F-score of 44.42%. 
Table 3 shows the scope finding results. For 
in-domain scope finding system, we use the 
hedge cues extracted by the submitted CRF-based 
in-domain system (the best result 85.77 in Table 
1). The result of the syntactic pattern-based 
system is not ideal probably due to the syntactic 
parsing errors and limited annotation rules. 
 
System Prec. Recall F-score 
syntactic pattern-based 44.31 42.59 43.45 
CRF-based 45.32 43.56 44.42 
 
Table 3: Official in-domain results for Task 2 
 
Through analyzing the false of our scope 
finding system, we found that many of our false 
scope were caused by such scope as sentence (d1) 
shows. Our CRF-based system signed the 
L-scope to the end of sentence mistakenly. The 
incorrectly annotation of our system and 
gold-standard annotation are shown in sentence 
(d1) and (d2) respectively. So an additional rule is 
added to our CRF-based system to correct the 
L-scope. The rule is: 
? If one token has been predicted as L-scope, 
and if the previous token is ?)?, or ?]?, the 
L-scope will be modified just before the 
paired token ?(? or ?[?. 
 
(d1) The incorrectly predicted version: 
These factors were <cue ref="X1.178.1" 
type="speculation">presumed</cue> to be 
pathogenic</xcope> (85).  
(d2) Gold-standard annotation: 
These factors were <cue ref="X1.178.1" 
type="speculation">presumed</cue> to be 
pathogenic (85) </xcope>. 
 
111
  
F-score is reached to 51.83 by combining this 
additional rule with the submitted CRF-based 
in-domain system as shown in Table 4. 
 
TP FP FN Prec. Recall F-score 
525 468 508 52.87 50.82 51.83 
 
Table 4: Official in-domain results for Task 2 
 
Several best results of Task 1 are exploited to 
investigate the relationship between the window 
size and the scope finding results. From the 
results of Table 5, we can see that the case of n=4 
gives the best precision, recall and F-score. And 
the case of n=2 and the case of n=3 based on the 
same task 1 system have a very similar score. 
With respect to the different systems of Task 1, in 
principle, the higher the F-score of Task 1, the 
better the performance of Task 2 can be expected. 
However, the result is somewhat different from 
the expectation. The best F-score of Task 2 is 
obtained under the case F-score (task 1) =86.02. 
This indicates that it is not certain that Task 2 
system based on the best Task 1 result gives the 
best scope finding performance.  
 
F-score 
(Task 1) 
Window 
size 
Prec. Recall F-score 
86.32 4 
3 
2 
54.32 
52.59 
52.90 
51.69 
50.05 
50.34 
52.98 
51.29 
51.59 
86.02 4 
3 
2 
54.85 
53.13 
53.13 
52.57 
50.92 
50.92 
53.68 
52.00 
52.00 
85.86 4 
3 
2 
54.19 
52.50 
52.50 
52.57 
50.92 
50.92 
53.37 
51.70 
51.70 
 
Table 5: Scope finding results relative to the 
results of task 1 and window size 
 
In the case that scopes longer than n (window 
size) words, the relevant cue will thus not fall into 
the +/-n word window of the L-scope and all 
hedge cue features will be O tag. The hedge cue 
features will be useless for detecting L-scopes. 
Taking into account the importance of hedge cue 
features, the following additional features are 
also incorporated to capture hedge cue features. 
 
? Distance to the closest preceding hedge cue 
? Distance to the closest following hedge cue 
? Stem of the closest preceding hedge cue 
? Stem of the closest following hedge cue  
? POS of the closest preceding hedge cue 
? POS of the closest following hedge cue 
 
Table 6 shows the results when the additional 
hedge cue features are used. The results with 
additional hedge cue feature set are constantly 
better than the results without them. In most of 
cases, the improvement is significant. The best 
F-score 54.18% is achieved under the case 
F-score (task 1) =86.02 and n=4. 
 
F-score 
(Task 1) 
Window 
size 
Prec. Recall F-score 
86.32 4 
3 
2 
54.73 
54.22 
53.41 
52.08 
51.60 
50.82 
53.37 
52.88 
52.08 
86.02 4 
3 
2 
55.35 
54.75 
53.94 
53.05 
52.47 
51.69 
54.18 
53.58 
52.79 
85.86 4 
3 
2 
54.49 
53.79 
53.09 
52.86 
52.18 
51.50 
53.66 
52.97 
52.29 
 
Table 6: Scope finding results relative to the 
results of Task 1 and window size with additional 
cue features 
 
The upper-bound results of CRF-based system 
assuming gold-standard annotation of hedge cues 
are show in Table 7. 
 
TP FP FN Prec. Recall F-score 
618 427 415 59.14 59.83 59.48 
 
Table 7: Scope finding result with gold-standard 
hedge signals 
 
A comparative character analysis of syntactic 
pattern-based method and CRF-based method 
will be interesting, which can provide insights 
leading to better methods in the future. 
6. Conclusion 
In this paper, we have exploited various useful 
features evident to detect hedge cues and their 
scope in biomedical texts. For hedge detection 
task, keyword-based system is integrated with 
CRF-based system by introducing keyword 
features to CRF-based system. Our experimental 
results show that the proposed method improves 
the performance of CRF-based system by the 
additional keyword features. Our system has 
achieved a state of the art F-score 86.32% on the 
sentence-level evaluation. For scope finding task, 
112
  
two different systems are established: CRF-based 
and syntactic pattern-based system. CRF-based 
system outperforms syntactic pattern-based 
system due to its evidential features. 
In the near future, we will improve the hedge 
cue detection performance by investigating more 
implicit information of potential keywords. On 
the other hand, we will study on how to improve 
scope finding performance by integrating 
CRF-based and syntactic pattern-based scope 
finding systems. 
References 
Leonard E. Baum, and Ted Petrie. 1966. Statistical 
inference for probabilistic functions of finite state 
Markov chains. Annals of Mathematical 
Statistics, 37(6):1554?1563. 
Peter L. Elkin, Steven H. Brown, Brent A. Bauer, 
Casey S. Husser, William Carruth, Larry R. 
Bergstrom, and Dietlind L. Wahner-Roedler. 2005. 
A controlled trial of automated classification of 
negation from clinical notes. BMC Medical 
Informatics and Decision Making, 5(13). 
Rich?rd Farkas, Veronika Vincze, Gy?rgy M?ra, J?nos 
Csirik, and Gy?rgy Szarvas. 2010. The 
CoNLL-2010 Shared Task: Learning to Detect 
Hedges and their Scope in Natural Language Text. 
In Proceedings of CoNLL-2010: Shared Task, 
2010, pages 1?12. 
Viola Ganter, and Michael Strube. 2009. Finding 
hedges by chasing weasels: Hedge detection using 
wikipedia tags and shallow linguistic features. In 
Proceedings of the ACL-IJCNLP 2009 
Conference Short Papers, pages 173?176. 
Dan Klein, and Christopher D. Manning. 2003. 
Accurate unlexicalized parsing. In Proceedings of 
the 41st Meeting of the Association for 
Computational Linguistics, pages 423?430. 
John Lafferty, Andrew McCallum, and Fernando 
Pereira. 2001. Conditional random fields: 
Probabilistic models for segmenting and labeling 
sequence data. In Proceedings of the Eighteenth 
International Conference on Machine 
Learning, pages 282?289. 
Marc Light, Xin Ying Qiu, and Padmini Srinivasan. 
2004. The language of bioscience: facts, 
speculations, and statements in between. In 
HLT-NAACL 2004 Workshop: BioLINK 2004, 
Linking Biological Literature, Ontologies and 
Databases, pages 17?24. 
Yuan Liu, Qiang Tan, and Kunxu Shen. 1994. The 
word segmentation rules and automatic word 
segmentation methods for Chinese information 
processing. QingHua University Press and 
GuangXi Science and Technology Press. 
Andrew McCallum, Dayne Freitag, and Fernando 
Pereira. 2000. Maximum entropy Markov models 
for information extraction and segmentation. In 
Proceedings of ICML 2000, pages 591?598. 
Ben Medlock. 2008. Exploring hedge identification in 
biomedical literature. Journal of Biomedical 
Informatics, 41(4):636?654. 
Ben Medlock, and Ted Briscoe. 2007. Weakly 
supervised learning for hedge classification in 
scientific literature. In Proceedings of ACL-07, 
pages 992?999. 
Roser Morante, and Walter Daelemans. 2009. 
Learning the scope of hedge cues in biomedical 
texts. In Proceedings of the Workshop on 
BioNLP, ACL 2009, pages 28?36. 
Ellen Riloff, Janyce Wiebe, and Theresa Wilson. 2003. 
Learning subjective nouns using extraction pattern 
bootstrapping. In Proceedings of the 7th 
Conference on Computational Natural 
Language Learning, pages 25?32. 
Gy?rgy Szarvas. 2008. Hedge classification in 
biomedical texts with a weakly supervised selection 
of keywords. In Proceedings of ACL: HLT, pages 
281?289. 
Gy?rgy Szarvas, Veronika Vincze, Rich?rd Farkas, 
and J?nos Csirik. 2008. The BioScope corpus: 
biomedical texts annotated for uncertainty, negation 
and their scopes. In Proceedings of BioNLP 2008: 
Current Trends in Biomedical Natural 
Language, pages 38?45. 
Yoshimasa Tsuruoka, Yuka Tateishi, Jin-Dong Kim, 
Tomoko Ohta, John McNaught, Sophia Ananiadou, 
Jun?ichi Tsujii. 2005. Developing a robust 
part-of-speech tagger for biomedical text. In 
Advances in Informatics 2005, pages 382?392.
 
113
HMM Revises Low Marginal Probability by CRF  
for Chinese Word Segmentation? 
 
Degen Huang, Deqin Tong, Yanyan Luo 
Department of Computer Science and Engineering 
Dalian University of Technology 
huangdg@dlut.edu.cn, {tongdeqin, ziyanluoyu}@gmail.com 
                                                        
?
 The work described in this paper is supported by Microsoft Research Asia Funded Project. 
Abstract 
This paper presents a Chinese word 
segmentation system for CIPS-SIGHAN 
2010 Chinese language processing task. 
Firstly, based on Conditional Random 
Field (CRF) model, with local features 
and global features, the character-based 
tagging model is designed. Secondly, 
Hidden Markov Models (HMM) is used 
to revise the substrings with low marginal 
probability by CRF. Finally, confidence 
measure is used to regenerate the result 
and simple rules to deal with the strings 
within letters and numbers. As is well 
known that character-based approach has 
outstanding capability of discovering 
out-of-vocabulary (OOV) word, but ex-
ternal information of word lost. HMM 
makes use of word information to in-
crease in-vocabulary (IV) recall. We par-
ticipate in the simplified Chinese word 
segmentation both closed and open test 
on all four corpora, which belong to dif-
ferent domains. Our system achieves bet-
ter performance. 
1 Introduction  
Chinese Word Segmentation (CWS) has wit-
nessed a prominent progress in the first four 
SIGHAN Bakeoffs. Since Xue (2003) used 
character-based tagging, this method has at-
tracted more and more attention. Some previous 
work (Peng et al, 2004; Tseng et al, 2005; Low 
et al, 2005) illustrated the effectiveness of using 
characters as tagging units, while literatures 
(Zhang et al, 2006; Zhao and Kit, 2007a; Zhang 
and Clark, 2007) focus on employing lexical 
words or subwords as tagging units. Because the 
word-based models can capture the word-level 
contextual information and IV knowledge. Be-
sides, many strategies are proposed to balance 
the IV and OOV performance (Wang et al, 
2008).  
CRF has been widely used in sequence label-
ing tasks and has a good performance (Lafferty 
et al, 2001). Zhao and Kit (2007b; 2008) at-
tempt to integrate global information with local 
information to further improve CRF-based tag-
ging method of CWS, which provides a solid 
foundation for strengthening CRF learning with 
unsupervised learning outcomes.  
In order to increase the accuracy of tagging 
using CRF, we adopt the strategy, which is: if the 
marginal probability of characters is lower than a 
threshold, the modified component based on 
HMM will be trigged; combining the confidence 
measure the results will be regenerated.  
2 Our word segmentation system 
In this section, we describe our system in more 
details. Three modules are included in our sys-
tem: a basic character-based CRF tagger, HMM 
which revises the substrings with low marginal 
probability and confidence measure which com-
bines them to regenerate the result. In addition, 
we also use some rules to deal with the strings 
within letters and numbers. 
2.1 Character-based CRF tagger 
Tag Set A 6-tag set is adopted in our system. It 
includes six tags: B, B2, B3, M, E and S. Here, 
Tag B and E stand for the first and the last posi-
tion in a multi-character word, respectively. S 
stands for a single-character word. B2 and B3 
stand for the second and the third position in a 
multi-character word. M stands for the fourth or 
more rear position in a multi-character word 
with more than four characters. The 6-tag set is 
proved to work more effectively than other tag 
sets in improving the segmentation performance 
of CRFs by Zhao et al (2006). 
Feature templates In our system, six n-gram 
templates, namely, C
-1, C0, C1, C-1C0, C0C1, 
C
-1C1 are selected as features, where C stands for 
a character and the subscripts -1, 0 and 1 stand 
for the previous, current and next character, re-
spectively. Furthermore, another one is character 
type feature template T
-1T0T1. We use four 
classes of character sets which are predefined as: 
class N represents numbers, class L represents 
non-Chinese letters, class P represents punctua-
tion labels and class C represents Chinese char-
acters.  
Except for the character feature, we also em-
ploy global word feature templates. The basic 
idea of using global word information for CWS 
is to inform the supervised learner how likely it 
is that the subsequence can be a word candidate. 
The accessor variety (AV) (Feng et al, 2005) is 
opted as global word feature, which is integrated 
into CRF successfully in literatures (Zhao and 
Kit, 2007b; Zhao and Kit, 2008). The AV value 
of a substring s  is defined as: 
                       
{ }( ) min ( ), ( )av avAV s L s R s=     (1) 
Where the left and right AV values ( )avL s  
and ( )avR s  are defined, respectively, as the 
number of its distinct predecessors and the 
number of its distinct successors. 
Multiple feature templates are used to repre-
sent word candidates of various lengths identi-
fied by the AV criterion. Meanwhile, in order to 
alleviate the sparse data problem, we follow the 
feature function definition for a word candidate 
s  with a score ( )AV s  in Zhao and Kit (2008), 
namely: 
( )nf s t= , 12 ( ) 2t tAV s +? <     (2) 
In order to improve the efficiency, all candi-
dates longer than five characters are given up. 
The AV features of word candidates can?t di-
rectly be utilized to direct CRF learning before 
being transferred to the information of characters. 
So we only choose the one with the greatest AV 
score to activate the above feature function for 
that character. 
In the open test, we only add another feature 
of ?FRE?, the basic idea of which is if a string 
matches a word in an existing dictionary, it may 
be a clue that the string is likely a true word. 
Then more word boundary information can be 
obtained, which may be helpful for CRF learn-
ing on CWS. The dictionary we used is 
downloaded from the Internet? and consists of 
108,750 words with length of one to four char-
acters. We get FRE features similar to the AV 
features. 
2.2 HMM revises substrings with low mar-
ginal probability 
The MP (short for marginal probability) of each 
character labeled with one of the six tags can be 
got separately through the basic CRF tagger. Here, 
B replaces ?B? and ?S? , and I represents other 
tags (?B2?, ?B3?, ?M?, ?E?). So each character has 
corresponding new MP as defined in formula (3) 
and (4). 
                                 
( )S B
B
t
P PP
P
+
= ?
                (3)                           
                           
2 3
( )B B M E
I
t
P P P P
P
P
+ + +
= ?
        (4)                 
Where { }2 3, , , , ,t S B B B M E?  and tP can be 
calculated by using forward-backward algorithm 
and more details are in Lafferty et al (2001).  
A low confident word refers to a word with 
word boundary ambiguity which can be reflected 
by the MP of the first character of a word. That 
is, it?s a low confident word if the MP of the first 
character of the word is lower than a threshold 
?  (it?s an empirical value and can be obtained 
by experiments). After getting the new MP, all 
these low confident candidate words are recom-
bined with their direct predecessors until the 
occurrence of a word that the MP of its first 
character is above the threshold ? , and then a 
new substring is generated for post processing.  
Then, we use class-based HMM to re-segment 
the substrings mentioned above. Given a word 
                                                        
?http://ccl.pku.edu.cn/doubtfire/Course/Chinese%20Inform
ation%20Processing/Source_Code/Chapter_8/Lexicon_full.
zip 
wi, a word class ci is the word itself. Let W  be 
the word sequence, let C  be its class sequence, 
and let 
#W be the segmentation result with the 
maximum likelihood. Then, a class-based HMM 
model (Liu, 2004) can be got. 
 
# arg max ( )
W
W P W=  
= arg max ( | ) ( )
W
P W C P C  
= 
1 2
1
... 1
arg max '( | ) ( | )
m
m
i i i i
w w w i
p w c P c c
?
=
?  
= 
1 2
1
... 1
arg max ( | )
m
m
i i
w w w i
P c c
?
=
?             (5) 
Where 1( | )i iP c c ?  indicates the transitive 
probability from one class to another and it can 
be obtained from training corpora. 
The word boundary of results from HMM is 
also represented by tag ?B? and ?I? which mean-
ing are the same as mentioned in above. 
2.3 Confidence measure and post process-
ing for final result 
There are two segmentation results for substrings 
with low MP candidates after reprocessing using 
HMM. Analyzing experiments data, we find 
wrong tags labeled by CRF are mainly: OOV 
words in test data, IV words and incorrect words 
recognized by CRF. Rectifying the tags with 
lower MP simply may produce an even worse 
performance in some case. For example, some 
OOV words are recognized correctly by CRF but 
with low MP. So, we can?t accept the revised 
results completely. A confidence measure ap-
proach is used to resolve this problem. Its calcu-
lation is defined as: 
                          
(1 )
o oC C C
P P P?= + ?
           (6)                           
oC
P is the MP of the character as ?I?, ?  is the 
premium coefficient. Based on the new value, a 
threshold t  was used, if the value was lower 
than t , the original tag ?I? will be rejected and 
changed into the tag ?B? which is labeled by 
HMM. 
At last, we use a simple rule to post-process the 
result directed at the strings that containing letters, 
numbers and punctuations. If the punctuation (not 
all punctuations) is half-width and the string be-
fore or after are composed of letters and numbers, 
combine all into a string as a whole. For an ex-
ample, ?.?, ?/?, ?:?, ?%? and ?\? are usually recog-
nized as split tokens. So, it needs handling addi-
tionally.  
3 Experiments results and analysis 
We evaluate our system on the corpora given by 
CIPS-SIGHAN 2010. There are four test corpora 
which belong to different domains. The details 
are showed in table 1. 
 
Domain Testing Data OOV rate 
A 149K 0.069 
B 165K 0.152 
C 151K 0.110 
D 157K 0.087 
 Table 1. Test corpora details 
A, B, C and D represent literature, computer 
science, medical science and finance, respec-
tively. 
3.1 Closed test 
The rule for the closed test in Bakeoff is that no 
additional information beyond training corpora is 
allowed. Following the rule, the closed test is 
designed to compare our system with other CWS 
systems. Five metrics of SIGHAN Bakeoff are 
used to evaluate the segmentation results: F-score 
(F), recall (R), precision (P), the recall on IV 
words (RIV) and the recall on OOV words (Roov). 
The closed test results are presented in table 2. 
 
Domain R P F Roov R?IV 
0.932 0.936 0.934 0.662 0.952 
A 
0.940 0.942 0.941 0.649 0.961 
0.950 0.948 0.949 0.831 0.971 
B 
0.953 0.950 0.951 0.827 0.975 
0.934 0.932 0.933 0.751 0.957 
C 
0.942 0.936 0.939 0.750 0.965 
0.955 0.957 0.956 0.837 0.966 
D 
0.959 0.960 0.959 0.827 0.972 
Table 2. Evaluation closed results on all data sets 
                                                        
?
 In order to analyze our results, we got value of RIV from 
the organizers because it can?t be obtained from the scoring 
system on http://nlp.ict.ac.cn/demo/CIPS-SIGHAN2010/#. 
In each domain, the first line shows the results 
of our basic CRF segmenter and the second one 
shows the final results dealt with HMM through 
confidence measure, which make it clear that 
using the confidence measure can improve the 
overall F-score by increasing value of R and P. 
 
Domain ID R P F Roov RIV 
5 0.945 0.946 0.946 0.816 0.954 
our 0.940 0.942 0.941 0.649 0.961 A 
12 0.937 0.937 0.937 0.652 0.958 
our 0.953 0.950 0.951 0.827 0.975 
11 0.948 0.945 0.947 0.853 0.965 B 
12 0.941 0.940 0.940 0.757 0.974 
our 0.942 0.936 0.939 0.750 0.965 
18 0.937 0.934 0.936 0.761 0.959 C 
5 0.940 0.928 0.934 0.761 0.962 
our 0.959 0.960 0.959 0.827 0.972 
12 0.957 0.956 0.957 0.813 0.971 D 
9 0.956 0.955 0.956 0.857 0.965 
Table 3. Comparison our closed results with the top three in all test sets
Next, we compare it with other top three sys-
tems. From the table 3 we can see that our system 
achieves better performance on closed test. In 
contrast, the values of RIV of our method are su-
perior to others?, which contributes to the model 
we use. Whether the features of AV for charac-
ter-based CRF tagger or HMM revising, they all 
make good use of word information of training 
corpora. 
3.2 Open test 
In the open test, the only additional source we 
use is the dictionary mentioned above. We get 
one first and two third best. Our result is showed 
in table 4. Compared with closed test, the value 
of RIV is increased in all test corpora. But we 
only get the higher value of F in domain of lit-
erature. The reasons will be analyzed as follows: 
In the open test, the OOV words are split into 
pieces because our model may be more depend-
ent on the dictionary information. Consequently, 
we get higher value of R but lower P. The train-
ing corpora are the same as closed test, but it is 
different that FRE features are added. The addi-
tional features enhance the original information 
of IV words, so the value of RIV is improved to 
some extent. However, they have side effects for 
OOV segmentation. We will continue to solve 
this problem in the future work. 
 
Domain R P F Roov RIV 
0.956 0.947 0.952 0.636 0.980 
A 
0.958 0.953 0.955 0.655 0.981 
0.943 0.921 0.932 0.716 0.985 
B 
0.948 0.929 0.939 0.735 0.986 
0.947 0.915 0.931 0.659 0.983 
C 
0.951 0.92 0.935 0.67 0.986 
0.962 0.948 0.955 0.760 0.981 
D 
0.964 0.95 0.957 0.763 0.983 
Table 4. Evaluation open results on all test sets 
4 Conclusions and future work 
In this paper, a detailed description on a Chinese 
segmentation system is presented. Based on 
intermediate results from a CRF tagger, which 
employs local features and global features, we 
use class-based HMM to revise the substrings 
with low marginal probabilities. Then, a confi-
dence measure is introduced to combine the two 
results. Finally, we post process the strings 
within letters, numbers and punctuations using 
simple rules. The results above show that our 
system achieves the state-of-the-art performance. 
The MP plays the important role in our method 
and HMM revises some errors identified by CRF. 
Besides, the word features are proved to be in-
formative cues in obtaining high quality MP. 
Therefore, our future work will focus on how to 
make CRF generate more reliable MP of char-
acters, including exploring other word informa-
tion or more unsupervised segmentation infor-
mation. 
 
References 
Feng Haodi, Kang Chen, Chuyu Kit, Xiaotie Deng. 
2005. Unsupervised segmentation of Chinese cor-
pus using accessor variety, In: Natural Language 
Processing IJCNLP, pages 694-703, Sanya, China. 
Lafferty John, Andrew McCallum and Fernando 
Pereira. 2001. Conditional Random Fields: prob-
abilistic models for segmenting and labeling se-
quence data, In: Proceedings of ICML-18, pages 
282-289, Williams College, USA. 
Liu Qun, Huaping Zhang, Hongkui Yu and Xueqi 
Chen. 2004. Chinese lexical analysis using cas-
caded Hidden Markov Model, Journal of computer 
research and development 41(8): 1421-1429. 
Low Kiat Jin, Hwee Tou Ng and Wenyuan Guo. 2005. 
A Maximum Entropy Approach to Chinese Word 
Segmentation. In: Proceedings of the Fourth 
SIGHAN Workshop on Chinese Language Proc-
essing, pages 161-164, Jeju Island, Korea. 
Peng Fuchun, Fangfang Feng and Andrew McCallum. 
2004. Chinese segmentation and new word detec-
tion using Conditional Random Fields, In: COL-
ING 2004, pages 562-568, Geneva, Switzerland. 
Tseng Huihsin, Pichuan Chang et al 2005. A Condi-
tional Random Field Word Segmenter for SIGHAN 
Bakeoff 2005. In: Proceedings of the Fourth 
SIGHAN Workshop on Chinese Language Proc-
essing, pages 168-171, Jeju Island, Korea. 
Wang Zhenxing, Changning Huang and Jingbo Zhu. 
2008. Which perform better on in-vocabulary word 
segmentation: based on word or character? In: 
Processing of the Sixth SIGHAN Workshop on 
Chinese Language Processing, pages 61-68, Hy-
derabad, India. 
Xue Nianwen. 2003. Chinese word segmentation as 
character tagging, Computational Linguistics and 
Chinese Language Processing 8(1): 29-48. 
Zhang Yue and Stephen Clark. 2007. Chinese Seg-
mentation with a Word-Based Perceptron Algo-
rithm. In: Proceedings of the 45th Annual Meeting 
of the Association for Computational Linguistics, 
pages 840-847, Prague, Czech Republic. 
Zhang Ruiqiang, Genichiro Kikui and Eiichiro Sumita. 
2006. Subword-based  tagging  by  Conditional 
Random Fields for Chinese word segmentation, In: 
Proceedings  of  the  Human  Language  
Technology Conference of the NAACL, pages 
193-196, New York, USA. 
Zhao Hai, Changning Huang, Mu Li and Baoliang Lu. 
2006. Effective tag set selection in Chinese word 
segmentation via Conditional Random Field mod-
eling, In: PACLIC-20, pages 87-94, Wuhan, China. 
Zhao Hai and Chunyu Kit. 2007a. Effective subse-
quence based tagging for Chinese word segmenta-
tion, Journal of Chinese Information Processing 
21(5): 8-13. 
Zhao Hai and Chunyu Kit. 2007b. Incorporating 
global information into supervised learning for 
Chinese word segmentation, In: PACLING-2007, 
pages 66-74, Melbourne, Australia. 
Zhao Hai and Chunyu Kit. 2008. Unsupervised seg-
mentation helps supervised learning of character 
tagging for word segmentation and named entity 
recognition, In: Proceedings of the Six SIGHAN 
Workshop on Chinese Language Processing, pages 
106-111, Hyderabad, India. 
 
  DLUT: Chinese Personal Name Disambiguation with Rich 
Features 
Dongliang Wang 
Department of Computer Science 
and Engineering, Dalian University 
of Technology 
wdl129@163.com 
Degen Huang 
Department of Computer Science 
and Engineering, Dalian University 
of Technology 
huangdg@dlut.edu.cn 
 
Abstract 
In this paper we describe a person clus-
tering system for a given document set 
and report the results we have obtained 
on the test set of Chinese personal name 
(CPN) disambiguation task of CIPS-
SIGHAN 2010. This task consists of 
clustering a set of Xinhua news docu-
ments that mention an ambiguous CPN 
according to named entity in reality. 
Several features including named entities 
(NE) and common nouns generated from 
the documents and a variety of rules are 
employed in our system. This system 
achieves F = 86.36% with B_Cubed 
scoring metrics and F = 90.78% with pu-
rity_based metrics. 
1 Introduction 
As the amount of web information expands at an 
ever more rapid pace, extraction of information 
for specific named entity is more and more im-
portant. Usually there are named-entity ambigu-
ity in web data, for example more than one per-
son use a same name, therefore it is difficult to 
decide which document refers to a specific 
named entity. 
The goal of CPN disambiguation is to cluster-
ing input Xinhua news corpus by the entity each 
document refers to. The new documents which 
span a time of fourteen years are extracted on 
web. 
As description of CPN disambiguation task of 
CIPS-SIGHAN 2010, Chinese personal name 
disambiguation is potentially more challenging 
due to the need for word segmentation, which 
could introduce errors that can in large part be 
avoided in the English task.  
In this paper we employ a CPN disambigua-
tion system that extracts NE and common nouns 
from the input corpus as features, and then com-
putes the similarity of each two documents in the 
corpus based on feature vector. Hierarchical Ag-
glomerative Clustering (HAC) algorithm (AK 
Jain et al, 1999) is used to implement clustering. 
After a great deal of analysis of news corpus, 
we constitute several rules, the experiments 
show that these rules can improve the result of 
this task. 
The remainder of this paper is organized as 
follows. Section 2 introduces the preprocessing 
of test corpus, and in section 3 we present the 
methodology of our system. In section 4 we pre-
sent the experimental results and give a conclu-
sion in section 5. 
2 Preprocessing 
In this step, we mainly complete the works as 
follows. 
Firstly, corpuses including a given name 
string are in different files, one document one 
file. In order to convenient for processing, we 
combine these documents into one file, distin-
guish them with document id.  
Secondly, some news corpuses have several 
subtitles but usually only part of them including 
focused name string, the others are noise of dis-
ambiguate of focused named entity, for example 
a news about sports may contain several subti-
tles about basketball, swimming, race and so on. 
These noises are removed from the corpus by us. 
Lastly, there is a lack of date-line in a few 
documents; in general, these data-lines are rec-
ognized as part of text, they can be recognized 
through simple matching method. Because data-
lines have consistent format as ????**?*?
??. 
3 Methodology 
The system follows a procedure include: word 
segmentation, the detection of ambiguous ob-
jects, feature extractions, computation of docu-
ment similarity and clustering. 
First, the text is segmented by a word segmen-
tation system explored by Luo and Huang 
(2009). The second step is extract all features 
from segmented text, all features are put into 
two feature vectors: NE vector and common 
noun vector. Then we will compute the distance 
between corresponding vectors of each two 
documents, the standard SoftTFIDF (Chen and 
Martin, 2007) are employed to compute the dis-
tance between two feature vectors. Lastly, we 
use the HAC algorithm for clustering of docu-
ments. 
3.1 Word Segmentation 
Word segmentation is a base and difficult work 
of natural language processing (NLP) and a 
precondition of feature extraction. In this paper, 
the word segmentation system explored by Luo 
and Huang (2009) are employed to do this work. 
This system training on the corpus of 2000?s 
?People?s Daily?. In addition, this system can 
recognize named entities including personal 
name, location name and organization name. We 
can extract these NEs by part-of-speech (POS) 
directly. 
3.2 The Detection of Ambiguous Entities 
Given a name string, the documents can be di-
vided into three groups: 
(1) Documents which contain names that are 
exactly match the query name string. 
(2) Documents which contain names that have 
a substring exactly match the query name string. 
(3) Documents which contain the query name 
string that is not personal name. 
After word segmentation, all personal names 
are labeled by system, when we find one per-
sonal name or its substring match the query 
name string; we will cluster this document ac-
cording to the name. If we failure all over the 
document, it?s considered that this document 
belong to category (3), it will be discarded. 
The ambiguous personal name in a document 
may refer to multiple entities, for example a 
news about party of namesakes, but this is a very 
small probability event, so we assume that all 
mentions in one document refers to the same 
entity, viz. ?one person one document?. 
Although we assume that ?one person one 
document?, the same personal name may occur 
more than once. Some times the word segmenta-
tion system will give the same personal name 
different labels in one document, for example a 
personal name ????? may be recognized as 
???? and ????? in different sentence in 
one document. Suppose that P1, P2, ? , Pn are  
recognized names that match the query name 
string, T1, T2, ? , Tn are the corresponding oc-
cur times.  We use the following method to en-
sure the final needed personal name: 
(1) If Ti > Tj for j = 1, 2, ?, i-1, i+1, ? , n, 
Pi is selected as the final needed personal name, 
else go to step (2). 
(2) Define S = { T1, T2, ? , Tn }, E1 = {T11, 
T12, ?, T1m}, E2 = S ? E1 satisfying T11 = T12 
= ...= T1m, E1 ?  S and Ti > Tj (Ti ?  E1, Tj ?  E2). 
Fi shows the word before Pi and Bi after Pi. For 
each Ti ?  E1, connect Fi, Ti and Bi into a new 
string named Ri, we can get R = {R11, R12, ?, 
R1m} corresponding to E1, the longest common 
substring of R are considered the final needed 
personal name. 
3.3 Features 
We define local sentence as sentences which 
contain the query name string, the features ex-
tracted from local sentences named local fea-
tures. Otherwise, all sentences except local sen-
tences in a document are named global sentences; 
the features extracted from global sentences are 
global features. The reason to distinguish them is 
because they have different contribution to simi-
larity computation. Local features are generally 
considered more important than global features, 
therefore a high weight should be given to local 
features. 
Named entities are important information 
about focused name. In this paper, NEs include 
personal names, location names and organization 
names. Location name and organization name 
usually indicate the region and department of 
focused name, and personal names usually have 
high co-occurrence rate, for example ????? 
and ???? are two names of table tennis players, 
so they always appear in a same news document 
about table tennis. The NE features which have 
been tagged by segmentation system can be ex-
tracted from the document directly. 
We also consider the features of common 
nouns. Semantically independent common nouns 
such as person?s job and person?s hobby etc usu-
ally include some useful information about the 
ambiguous object. We attempt to capture these 
noun features and use them as elements in fea-
ture vector. 
Location names in data-line. The location 
name in the data-line indicates the place the 
news had occurred, if two documents have the 
same date-line location name, and then there is a 
good chance that these two documents refer the 
same person.  
Appellation of query name. Appellation usu-
ally demonstrate a person?s identity, for example, 
if the appellation of the query name is ????, it 
shows that he or she is a journalist. As location 
names in data-line, if two query names have the 
same appellation, the possibility of them refer to 
the same person increased. The word segmenta-
tion system doesn?t clearly marked out appella-
tion but marked as common noun. In generally, 
appellations appear neighbor in front of name, so 
we collect the common nouns neighbor front of 
query names as their appellations. 
So far, we have developed four feature vec-
tors: local NE vector, local common noun vector, 
global NE vector and global common noun vec-
tor. Given feature vectors, we need to find a way 
to learn the similarity matrix. In this paper, we 
choose the standard TF-IDF method to calculate 
the similarity matrix. Location name in date-line 
and appellation of query name will be used in 
rule method without similarity calculation. 
3.4 Similarity Matrix 
Given a pair of feature vectors consisting of NEs 
or common nouns, we need to choose a similar-
ity scheme to calculate the similarity matrix. The 
standard TF-IDF method is introduced here, then 
a little change for Chinese string. 
Standard TF-IDF: Given a pair of vector S 
and T, S = (s1, s2, ?, sn), T = (t1, t2, ?, tm). 
Here, si (i = 1, ?, n) and tj (j = 1, ?, m) are NE 
or common noun. We define: 
}),(,,;{
);;(
?
?
>???
=
vwdistTvSww
TSCLOSE
    (1) 
Where dist(w;v) is the Jaro-Winkler dis-
tance function (Winkler, 1999), which will 
be introduced later. 
);(max);( vwdistTwD Tv?=              (2) 
Then the standard TF-IDF SoftTFIDF is com-
puted as: 
=),( TSSoftTFIDF  
),(*),(*),();;( TwDTwVSwVTSCLOSEw? ? ? (3) 
? ?
=
Sw
SwV
SwVSwV
2'
'
),(
),(),(            (4) 
)log(*)1log(),(
,
'
wSw IDFTFSwV +=    (5) 
Where SwTF ,  is the frequency of substring 
w in S, and wIDF is the inverse of the fraction 
of documents in the corpus that contain w . Sup-
pose Nt is total number of documents, Nw is total 
number of documents which contain w . Then 
wIDF  computed as: 
w
t
N
NIDF =?                          (6) 
The Jaro-Winkler distance Jw of two given 
strings s1 and s2 as shown in formula (7), l is 
the length of common prefix at the start of the 
string up to a maximum of 4 characters, p is a 
constant scaling factor for how much the score is 
adjusted upwards for having common prefixes, 
the value for p is 0.1. 
)1( jjw dlpdd ?+=                    (7) 
)|2||1|( m
tm
s
m
s
md j
?
++=              (8) 
In formula (8) m is the number of matching 
characters, t is the number of transpositions. In 
order to be consistent with the English strings, a 
Chinese character is seen as two English charac-
ters. 
Corresponding to four feature vectors, we can 
calculate the four similarities: S(gNE), S(gCN), 
S(lNE), S(lCN). The similarity between two 
documents (DS) is computed as: 
 
2
)()(
*)1(
2
)()(
*
gCNSlCNSgNESlNES
DS
+
?+
+
=
??                                                                         
(9) 
As time is tight, we just give ?  a value of 0.8 
with out experiment because we consider NEs 
have stronger instructions. 
3.5 Clustering 
Clustering is a key work of this task, it is very 
important to choose a clustering algorithm. Here 
we use HAC algorithm to do clustering. HAC 
algorithm is an unsupervised clustering algo-
rithm, which can be described as follows: 
(1) Initialization. Every document is re-
garded as a separate class. 
(2) Repetition. Computing the similarity of 
each of the two classes, merge the two classes 
whose similarity are the highest and higher than 
the threshold value of ?  into a new class. 
(3) Termination. Repeat step (2) until all 
classes don?t satisfy the clustering condition. 
Suppose document class F = {f1, f2, ?, fn} 
and K = {k1, k2, ?, km}, fi and kj are documents 
in class F and class K, then the similarity be-
tween F and K is: 
nm
kfS
KJS ji ji
*
),(
),( ,?=                  (9) 
If two documents have different query name, 
obviously they refer to different person, only 
documents which have same query name will be 
clustered. Before clustering, several rules are 
afforded to improve the clustering condition. 
These rules are generally applicable to news 
corpus. 
(1) If two documents have the same query 
name and both of them are reporter, and both 
date-lines have the same location name, then 
combine the two documents into one class. 
(2) If two documents have the same query 
name and another same personal name, then 
combine the two documents into one class. 
(3) If two documents have the same query 
name and both date-lines have the same location 
name, then double the similarity, else halve the 
similarity. 
(4) If two documents have the same query 
name and both personal names have the same 
appellation, then double the similarity, else halve 
the similarity. 
4 Evaluation 
In order to prove the validity of the rule ap-
proach, a group of experiments are performed on 
the train set of Chinese personal name disam-
biguation task of CIPS-SIGHAN 2010. The re-
sult is shown in Table 1. R1 is the result without 
rules, and R2 shows the accuracy after adding 
the rules. 
The system performance on the test set of 
CPN disambiguation task of CIPS-SIGHAN 
2010 is F = 90.78% evaluated with P_IP evalua-
tion, and F = 86.36% with B_Cubed evaluation. 
The accuracy is shown in Table 2. 
 
 
B_Cubed Precision Recall F 
R1 70.56 86.77 74.74 
R2 78.05 84.99 79.60 
P_IP Purity Inverse 
Purity 
F 
R1 77.22 90.48 81.20 
R2 82.92 88.30 84.29 
Table 1. Experimental results for system with 
rules and without rules on training set 
 
B_Cubed Precision Recall F 
 82.96 91.33 86.36 
P_IP Purity Inverse 
Purity 
F 
 87.94 94.21 90.78 
Table 2. The results on test set 
5  Conclusion 
We described our system that disambiguates 
Chinese personal names in Xinhua corpus. We 
mainly focus on extracting rich features from 
documents and computing the similarity of each 
two documents. Several rules are introduced to 
improve the accuracy and have proved effective. 
References 
Anil K. Jain, M. Narasimha Murty, and Patrick J. 
Flynn. 1999. Data clustering: A review. ACM 
Computing Surveys, 31(3): 264-323. 
Bradley Malin. 2005. Unsupervised Name Disam-
biguation via Network Similarity. In proceedings 
SIAM Conference on Data Mining, 2005. 
Chen Ying, James Martin. 2007. CU-COMSEM: Ex-
ploring Rich Features for Unsupervised Web Per-
sonal Name Disambiguation. In proceedings of 
Semeval 2007, Association for Computational 
Linguistics, 2007. 
Chen Ying, Sophia Y. M. Lee and Churen Huang. 
2009. PolyUHK:A Robust Information Extraction 
System for Web Personal Names. In proceedings 
of Semeval 2009, Association for Computational 
Linguistics, 2009. 
Gusfield, Dan. 1997. Algorithms on Strings, Trees 
and Sequences. Cambridge University Press, 
Cambridge, UK 
Javier Artiles, J. Gonzalo and S. Sekine. WePS2 
Evaluation Campaign: Overview of the Web Peo-
ple Search Clustering Task. In proceedings of Se-
meval 2009, Association for Computational Lin-
guistics, 2009. 
Luo Yanyan, Degen Huang. 2009. Chinese word seg-
mentation based on the marginal probabilities 
Generated by CRFs. Journal of Chinese Informa-
tion Processing, 23(5): 3-8. 
Octavian Popescu, B. Magnini. 2007. IRST-BP: Web 
People Search Using Name Entities. In proceed-
ings of Semeval 2007, Association for Computa-
tional Linguistics, 2007. 
William E. Winkler. 1999. The state of record linkage 
and current research problems. Statistics of In-
come Division, Internal Revenue Service Publica-
tion R99/04. 
 
Proceedings of the 15th Conference on Computational Natural Language Learning: Shared Task, pages 66?70,
Portland, Oregon, 23-24 June 2011. c?2011 Association for Computational Linguistics
Combining Syntactic and Semantic Features by SVM for Unrestricted
Coreference Resolution
Huiwei Zhou1, Yao Li2, Degen Huang3, Yan Zhang4, Chunlong Wu5, Yuansheng Yang6
Dalian University of Technology
Dalian, Liaoning, China
{1zhouhuiwei,3huangdg,6yangys}@dlut.edu.cn
2tianshanyao@mail.dlut.edu.cn
4zhangyan zyzy@yeah.net
5wuchunlong@gmail.com
Abstract
The paper presents a system for the CoNLL-
2011 share task of coreference resolution. The
system composes of two components: one for
mentions detection and another one for their
coreference resolution. For mentions detec-
tion, we adopted a number of heuristic rules
from syntactic parse tree perspective. For
coreference resolution, we apply SVM by ex-
ploiting multiple syntactic and semantic fea-
tures. The experiments on the CoNLL-2011
corpus show that our rule-based mention iden-
tification system obtains a recall of 87.69%,
and the best result of the SVM-based corefer-
ence resolution system is an average F-score
50.92% of the MUC, B-CUBED and CEAFE
metrics.
1 Introduction
Coreference resolution, defined as finding the dif-
ferent mentions in a document which refer to the
same entity in reality, is an important subject in Nat-
ural Language Processing. In particular, coreference
resolution is a critical component of information ex-
traction systems (Chinchor and Nancy, 1998; Sund-
heim and Beth, 1995) and a series of coreference
resolution tasks have been introduced and evaluated
from MUC (MUC-6, 1995). Some machine learning
approaches have been applied to coreference resolu-
tion (Soon et al, 2001; Ng and Cardie, 2002; Bengt-
son and Roth, 2008; Stoyanov et al, 2009). Soon
et al(2001) use a decision tree classifier to decide
whether two mentions in a document are coreferen-
t. Bergsma and Lin (2006) exploit an effective fea-
ture of gender and number to a pronoun resolution
system and improve the performance significantly,
which is also appeared in our feature set. Howev-
er, automatic coreference resolution is a hard task
since it needs both syntactic and semantic knowl-
edge and some intra-document knowledge. To im-
prove the performance further, many deep knowl-
edge resources like shallow syntactic and seman-
tic knowledge are exploited for coreference resolu-
tion (Harabagiu et al, 2001; McCallum and Well-
ner, 2004; Denis and Baldridge, 2007; Ponzetto and
Strube, 2005; Versley, 2007; Ng, 2007). In order to
make use of more syntactic information, Kong et al
(2010) employ a tree kernel to anaphoricity determi-
nation for coreference resolution and show that ap-
plying proper tree structure in corefernce resolution
can achieve a good performance.
The CoNLL-2011 Share Task (Pradhan et
al., 2011) ?Modeling Unrestricted Coreference in
OntoNotes? proposes a task about unrestricted
coreference resolution, which aims to recognize
mentions and find coreference chains in one docu-
ment. We participate in the closed test.
In this paper, we exploit multi-features to a
coreference resolution system for the CONLL-2011
Share Task, including flat features and a tree struc-
ture feature. The task is divided into two steps in
our system. In the first step, we adopt some heuristic
rules to recognize mentions which may be in a coref-
erence chain; in the second step, we exploit a num-
ber of features to a support vector machine (SVM)
classifier to resolute unrestricted coreference. The
experiments show that our system gets a reasonable
result.
The rest of the paper is organized as follows. In
66
Section 2, we describe in detail how our system does
the work of coreference resolution, including how
we recognize mentions and how we mark the coref-
erence chains. The experimental results are dis-
cussed in Section 3. Finally in Section 4, we give
some conclusion.
2 The Coreference Resolution System
The task of coreference resolution is divided into
two steps in our system: mentions detection and
coreference resolution. In the first step, we use some
heuristic rules to extract mentions which may re-
fer to an entity. In the second step, we make up
mention-pairs with the mentions extracted in the
first step, and then classify the mention-pairs in-
to two groups with an SVM model: Coreferent or
NotCoreferent. Finally we get several coreference
chains in a document according to the result of clas-
sification. Each coreference chain stands for one en-
tity.
2.1 Rule-based Identification of Mentions
The first step for coreference resolution is to identify
mentions from a sequence of words. We have tried
the machine-learning method detecting the bound-
ary of a mention. But the recall cannot reach a high
level, which will lead to bad performance of coref-
erence resolution. So we replace it with a rule-based
method. After a comprehensive study, we find that
mentions are always relating to pronouns, named en-
tities, definite noun phrases or demonstrative noun
phrases. So we adopt the following 5 heuristic rules
to extract predicted mentions:
1. If a word is a pronoun, then it is a mention.
2. If a word is a possessive pronoun or a posses-
sive, then the smallest noun phrase containing
this word is a mention.
3. If a word string is a named entity, then it is a
mention.
4. If a word string is a named entity, then the s-
mallest noun phrase containing it is a mention.
5. If a word is a determiner (a, an, the, this, these,
that, etc.), then all the noun phrase beginning
with this word is a mention.
2.2 Coreference Resolution with
Multi-Features
The second step is to mark the coreference chain us-
ing the model trained by an SVM classifier. We ex-
tract the marked mentions from the training data and
take mention-pairs in one document as instances to
train the SVM classifier like Soon et al(2001) . The
mentions with the same coreference id form the pos-
itive instances while those between the nearest posi-
tive mention-pair form the negative instance with the
second mention of the mention-pair.
The following features are commonly used in
NLP processes, which are also used in our system:
? i-NamedEntity/j-NamedEntity: the named en-
tity the mention i/j belongs to
? i-SemanticRole/j-SemanticRole: the semantic
role the mention i/j belongs to which
? i-POSChain/j-POSChain: the POS chain of the
mention i/j
? i-Verb/j-Verb: the verb of the mention i/j
? i-VerbFramesetID/j-VerbFramesetID: the verb
frameset ID of the mention i/j, which works to-
gether with i/j-Verb
All the 5 kinds of features above belong to a sin-
gle mention. For mention-pairs, there are another 4
kinds of features as below:
? StringMatch: after cutting the articles, 1 if the
two mentions can match completely, 2 if one is
a substring of the other, 3 if they partly match,
4 else.
? IsAlias: after cutting the articles, 1 if one men-
tion is the name alias or the abbreviation of the
other one, 0 else
? Distance: it is the number of sentences between
two mentions, 0 if the two mentions are from
one sentenci-Verb/j-Verb: the verb of the men-
tion i/j
? SpeakerAgreement: 1 if both the speakers of
the two mentions are unknown, 2 if both the
two mentions come from the same speaker, 3 if
the mentions comes from different speakers.
67
All of the 14 simple and effective features above
are applied in the baseline system, which use the
same method with our system. But coreference res-
olution needs more features to make full use of the
intra-documental knowledge, so we employ the fol-
lowing 3 kinds of features to our system to catch
more information about the context.
? i-GenderNumber/j-GenderNumber (GN): 7
values: masculine, feminine, neutral, plu-
ral, ?rst-person singular, ?rst-person plural,
second-person.
? SemanticRelation (SR): the semantic relation
in WordNet between the head words of the t-
wo mentions: synonym, hyponym, no relation,
unknown.
? MinimumTree (MT): a parse tree represents the
syntactic structure of a sentence, but corefer-
ence resolution needs the overall context in a
document. So we add a super root to the forest
of all the parse trees in one document, and then
we get a super parse tree. The minimum tree
(MT) of a mention-pair in a super parse tree is
the minimum sub-tree from the common par-
ent mention to the two mentions, just like the
method uesd by Zhou(2009). And the similari-
ty of two trees is calculated using a convolution
tree kernel (Collins and Duffy, 2001), which
counts the number of common sub-trees.
We try all the features in our system, and get some
interesting results which is given in Experiments and
Results Section.
3 Experiments and Results
Our experiments are all carried out on CONLL-2011
share task data set (Pradhan et al, 2007).
The result of mention identification in the first
step is evaluated through mention recall. And the
performance of coreference resolution in the second
step is measured using the average F1-measures of
MUC, B-CUBED and CEAFE metrics (Recasens et
al., 2010). All the evaluations are implemented us-
ing the scorer downloaded from the CONLL-2011
share task website 1 .
1http://conll.bbn.com/index.php/software.html
3.1 Rule-based Identification of Mentions
The mention recall of our system in the mention i-
dentification step reaches 87.69%, which can result
in a good performance of the coreference resolution
step. We also do comparative experiments to inves-
tigate the effect of our rule-based mention identifica-
tion. The result is shown in Table 1. The CRF-based
method in Table 1 is to train a conditional random
field (CRF) model with 6 basic features, including
Word, Pos, Word ID, Syntactic parse label, Named
entity, Semantic role.
Method Recall Precision F-score
Rule-based 87.69 32.16 47.06
CRF-based 59.66 50.06 54.44
Table 1: comparative experiments of CRF-based and
rule-based methods of mention identification(%)
Table 1 only shows one kind of basic machine-
learning methods performs not so well as our rule-
based method in recall measure in mention iden-
tification, but the F1-measure of the CRF-based
method is higher than that of the rule-based method.
In our system, the mention identification step should
provide as many anaphoricities as possible to the
coreference resolution step to avoid losing corefer-
ent mentions, which means that the higher the recal-
l of mention identification is, the better the system
performs.
3.2 Coreference Resolution with
Multi-Features
In the second step of our system, SVM-LIGHT-
TK1.2 implementation is employed to coreference
resolution. We apply the polynomial kernel for
the flat features and the convolution tree kernel for
the minimum tree feature to the SVM classifier, in
which the parameter d of the polynomial kernel is
set to 3 (polynomial (a ? b + c)d) and the combin-
ing parameter r is set to 0.2 (K = tree? forest ?
kernel ? r + vector ? kernel). All the other pa-
rameters are set to the default value. All the exper-
iments are done on the broadcast conversations part
of CoNLL-2011 corpus as the calculating time of
SVM-LIGHT-TK1.2 is so long.
Experimental result using the baseline method
with the GenderNumber feature added is shown in
68
d=? MUC B3 CEAFE AVE
2 47.49 61.14 36.15 48.26
3 51.37 62.82 38.26 50.82
Table 2: parameter d in polynomial kernel in coreference
resolution using the baseline method with the GN fea-
ture(%)
Talbe 2. The result shows that the parameter d in
polynomial kernel plays an important role in our
coreference resolution system. The score when d is
3 is 2.56% higher than when d is 2, but the running
time becomes longer, too.
r=? MUC B3 CEAFE AVE
1 31.41 45.08 22.72 33.07
0.25 34.15 46.87 23.63 34.88
0 51.37 62.82 38.26 50.82
Table 3: combining parameter r (K = tree ? forest ?
kernel ? r + vector? kernel) in coreference resolution
using the baseline with the GN and MT features(%)
In Table 3, we can find that the lower the combin-
ing parameter r is, the better the system performs,
which indicates that the MT feature plays a negative
role in our system. There are 2 possible reasons for
that: the MT structure is not proper for our coref-
erence resolution system, or the simple method of
adding a super root to the parse forest of a document
is not effective.
Method MUC B3 CEAFE AVE
baseline 42.19 58.12 33.6 44.64
+GN 51.37 62.82 38.26 50.82
+GN+SR 49.61 64.18 38.13 50.64
+GN 50.97 62.53 37.96 50.49
+SEMCLASS
Table 4: effect of GN and SR features in coreference res-
olution using no MT feature (%)
Table 4 shows the effect of GenderNumber fea-
ture and SemanticRelation feature, and the last item
is the method using the SemanticClassAgreement-
Feature (SEMCLASS) used by (Soon et al, 2001)
instead of the SR feature of our system. The GN fea-
ture significantly improves the performance of our
system by 6.18% of the average score, which may
be greater if we break up the gender and number
feature into two features. As the time limits, we
haven?t separated them until the deadline of the pa-
per. The effect of the SR feature is not as good as
we think. The score is lower than the method with-
out SR feature, but is higher than the method using
SEMCLASS feature. The decreasing caused by S-
R feature may be due to that the searching depth in
WordNet is limited to one to shorten running time.
To investigate the performance of the second step,
we do an experiment for the SVM-based corefer-
ence resolution using just all the anaphoricities as
the mention collection input. The result is shown in
Table 5. As the mention collection includes no in-
correct anaphoricity, any mistake in coreference res-
olution step has double effect, which may lead to a
relatively lower result than we expect.
MUC B3 CEAFE AVE
65.55 58.77 39.96 54.76
Table 5: using just all the anaphoricities as the mention
collection input in coreference resolution step (%)
In the three additional features, only the GN fea-
ture significantly improves the performance of the
coreference resolution system, the result we finally
submitted is to use the baseline method with GN fea-
ture added. The official result is shown in Table 6.
The average score achieves 50.92%.
MUC B3 CEAFE AVE
48.96 64.07 39.74 50.92
Table 6: official result in CoNLL-2011 Share Task using
baseline method with GN feature added (%)
4 Conclusion
This paper proposes a system using multi-features
for the CONLL-2011 share task. Some syntactic and
semantic information is used in our SVM-based sys-
tem. The best result (also the official result) achieves
an average score of 50.92%. As the MT and S-
R features play negative roles in the system, future
work will focus on finding a proper tree structure
for the intra-documental coreference resolution and
combining the parse forest of a document into a tree
to make good use of the convolution tree kernel.
69
References
A. McCallum and B. Wellner. 2004. Conditional models
of identity uncertainty with application to noun coref-
erence. In Advances in Neural Information Processing
Systems (NIPS), 2004.
Chinchor, Nancy A. 1998. Overview of MUC-7/MET-2.
In Proceedings of the Seventh Message Understanding
Conference (MUC-7).
Eric Bengtson, Dan Roth. 2008. Understanding the Val-
ue of Features for Coreference Resolution Proceed-
ings of the 2008 Conferenceon Empirical Methods in
Natural Language Processing, pages294C303.
Fang Kong, Guodong Zhou, Longhua Qian, Qiaoming
Zhu. 2010. Dependency-driven Anaphoricity Deter-
mination for Coreference Resolution Proceedings of
the 23rd International Conferenceon Computational
Linguistics (Coling2010), pages599C607.
Guodong Zhou, Fang Kong. 2009. Global Learning of
Noun Phrase Anaphoricity in Coreference Resolution
via Label Propagation. In Proceedings of the 2009
Coreference on Empirical Methods in Natural Lan-
guage Processing, pages 978-986, 2009.
M. Collins, N.Duffy. 2001. Convolution Kernels for Nat-
ural Language Resolution NIPS? 2001.
Marta Recasens, Llu?s Mrquez, Emili Sapena, M. Antnia
Mart?, Mariona Taul, Vronique Hoste, Massimo Poe-
sio, Yannick Versley 2010. SemEval-2010 Task 1:
Coreference Resolutionin Multiple Languages In Pro-
ceeding SemEval 2010 Proceedings of the 5th Interna-
tional Workshop on Semantic Evaluation, 2010.
MUC-6. 1995. Coreference task definition (v2.3, 8 Sep
95) In Proceedings of the Sixth Message Understand-
ing Conference (MUC-6), pages 335-344.
P.Denis, J.Baldridge. 2007. Joint determination of
anaphoricity and coreference resolution using integer
programming. In Proceedings of HLT/NAACL, 2007.
V. Ng and C. Cardie. 2002. Improving machine learning
approaches to coreference resolution. In Proceedings
of ACL, 2002.
V. Ng. 2007. Shallow semantics for coreference resolu-
tion. In Proceedings of IJCAI, 2007.
Veselin Stoyanov, Nathan Gilbert, Claire Cardie, Ellen
Riloff. 2009. Conundrums in Noun Phrase Corefer-
ence Resolution: Making Sense of the State-of-the-Art
Proceeding ACL ?09 Proceedings of the Joint Confer-
ence of the 47th Annual Meeting of the ACL.
W.Soon,H.Ng,and D.Lim. 2001. A machine learning ap-
proach to coreference resolution of noun phrase. Com-
putational Linguistics, 27(4):521-544,2001.
S. M.Harabagiu,R.C.Bunescu,and S.J. Maiorano. 2001.
Text and knowledge mining for coreference resolution.
In Proceedings of NAACL, 2001.
S.Ponzetto, M.Strube. 2005. Semantic role labeling for
coreference resolution. In Proceedings of EACL, Italy,
April 2005.
Sameer Pradhan, Lance Ramshaw, Mitchell Marcus,
Martha Palmer, Ralph Weischedel, Nianwen Xue.
2011. CoNLL-2011 Shared Task: Modeling Unre-
stricted Coreference in OntoNotes. Proceedings of the
Fifteenth Conference on Computational Natural Lan-
guage Learning (CoNLL 2011).
Sameer S. Pradhan, Lance Ramshaw, Ralph Weischedel,
Jessica MacBride, Linnea Micciulla. 2007. Unre-
stricted Coreference: Identifying Entities and Events
in OntoNotes. In International Conference on Seman-
tic Computing, 2007.
Shane Bergsma, Dekang Lin. 2006. Bootstrapping Path-
Based Pronoun Resolution. In Proceedings of the 21st
International Conference on Computational Linguis-
tics, 2006.
Sundheim, Beth M. 1995. Overview of results of the
MUC-6 evaluation. In Proceedings of the Sixth Mes-
sage Understanding Conference (MUC-6), pages 13-
31.
Y.Versley. 2007. Antecedent selection techniques for
high-recall coreference resolution. In Proceedings of
EMNLP/CoNLL, 2007.
70
Proceedings of the BioNLP Shared Task 2013 Workshop, pages 109?115,
Sofia, Bulgaria, August 9 2013. c?2013 Association for Computational Linguistics
Improving Feature-Based Biomedical Event Extraction System by In-
tegrating Argument Information 
Lishuang Li, Yiwen Wang, Degen Huang 
School of Computer Science and Technology 
Dalian University of Technology 
116023 Dalian, China 
lilishuang314@163.com yeevanewong@gmail.com 
huangdg@dlut.edu.cn 
 
Abstract 
We describe a system for extracting biomedi-
cal events among genes and proteins from 
biomedical literature, using the corpus from 
the BioNLP?13 Shared Task on Event Extrac-
tion. The proposed system is characterized by 
a wide array of features based on dependency 
parse graphs and additional argument informa-
tion in the second trigger detection. Based on 
the Uturku system which is the best one in the 
BioNLP?09 Shared Task, we improve the per-
formance of biomedical event extraction by 
reducing illegal events and false positives in 
the second trigger detection and the second ar-
gument detection. On the development set of 
BioNLP?13, the system achieves an F-score of 
50.96% on the primary task. On the test set of 
BioNLP?13, it achieves an F-score of 47.56% 
on the primary task obtaining the 5th place in 
task 1, which is 1.78 percentage points higher 
than the baseline (following the Uturku sys-
tem), demonstrating that the proposed method 
is efficient. 
1 Introduction 
Extracting knowledge from unstructured text is 
one of the most important goals of Natural Lan-
guage Processing and Artificial Intelligence. Re-
sources in the internet are expanding at an expo-
nential speed, especially in the biomedical do-
main. Due to the astronomical growth of biomed-
ical scientific literature, it is very important and 
urgent to develop automatic methods for know-
ledge extraction system. 
In the past few years, most researchers in the 
field of Biomedical Natural Language Processing 
focused on extracting information with simple 
structure, such as named entity recognition 
(NER), protein-protein interactions (PPIs) (Air-
ola et al, 2008; Miwa et al, 2009) and disease-
gene association (Chun et al, 2006). While PPIs 
concern the flat relational schemas with no 
nested structures, bio-molecular events describe 
the detailed behavior of bio-molecules, which 
capture the biomedical phenomena from texts 
well. The BioNLP?09 shared task (Kim et al, 
2009) provides the first entry to bio-event extrac-
tion. As described in BioNLP?09, a bio-event 
consists of a trigger and one or more arguments, 
where a trigger is a contiguous textual string con-
taining one or more tokens and an argument is a 
participant (event or protein) with a correspond-
ing type. For example, in the snippet ?interferon 
regulatory factor 4 gene expression?, the event 
trigger is ?expression? which is tagged by the 
event type ?Gene_expression? and the event ar-
gument is ?interferon regulatory factor 4?. Not-
ably, bio-events may have arbitrary arguments 
and even contain other events as arguments, re-
sulting in nested events. 
The complex event structure makes this task 
particularly attractive, drawing initial interest 
from many researchers. Bj?rne et al's (2009) 
system (referred to hereinafter as Uturku system) 
was the best pipeline system in BioNLP?09, 
achieving an F-score of 51.95% on the test data 
sets. After that, Miwa et al (2010a, 2010b) com-
pared different parsers and dependency represen-
tations on bio-event extraction task and obtained 
an F-score of 57.79% on development data sets 
and 56.00% on test data sets with parser ensem-
ble. In contrast to the pipeline system which di-
vided the event process into three stages, triggers 
detection, arguments detection and post 
processing, Poon and Vanderwende?s (2010) and 
Riedel et al?s (2009) joint models combined 
trigger recognition and argument detection by 
using a Markov logic network learning approach. 
After the BioNLP?09, the Genia event task (Bi-
oNLP?11 task 1, hereafter) in the BioNLP?11 
Shared Task (Kim et al, 2011) introduced a 
same event extraction task on a new dataset. 
There were still some pipeline systems applied to 
Genia task 1, e.g. Bj?rne et al?s (2011) system 
and Quirk et al?s (2011) system. To the best of 
109
our knowledge, Miwa et al?s (2012) pipeline 
system incorporating domain adaptation and co-
reference resolution, is the best biomedical event 
extraction system on BioNLP'11 task 1 so far. 
The Genia event extraction task (BioNLP'13 
task 1, hereafter) (Kim et al, 2013) in Bi-
oNLP'13 Shared Task is consistent with the Ge-
nia task in BioNLP'11 Shared task. Nevertheless, 
BioNLP'13 task 1 focuses on event extraction 
from full texts while BioNLP?11 task 1 contains 
abstracts and full texts. Furthermore, the corefe-
rence resolution task separated from event ex-
traction task in BioNLP'11 is integrated to Bi-
oNLP'13 task 1, and there are more event types 
in the BioNLP'13 task 1 than those in BioNLP'11 
task 1. The BioNLP?13 shared task contains 
three parts, the training corpus, the development 
corpus and the test corpus. The training corpus 
consists of 10 full texts containing 2792 events. 
The development corpus for optimizing the pa-
rameters involves 10 full texts containing 3184 
events, while the test corpus is composed of 14 
full texts including 3301 events. To avoid the 
researchers optimizing parameters on the test 
corpus, it is not published, and we have the per-
mission to combine the training corpus and the 
development corpus as training set. However, we 
extend BioNLP'13 training set by adding the ab-
stracts of training set and development set in Bi-
oNLP'11 task 1 rather than merging the devel-
opment set of BioNLP'13 into the training set.  
Our system generally follows the Uturku sys-
tem reported by Bj?rne et al (2009), and uses a 
simple but efficient way to reduce the cascading 
errors. The Uturku system was a pipeline of trig-
ger detection, argument detection and post-
processing. Each of its components was simple  
to implement by reducing event extraction task 
into independent classification of triggers and 
arguments. Moreover, the Uturku system devel-
oped rich features and made extensive use of 
syntactic dependency parse graphs, and the rules 
in the post-processing step were efficient and 
simple. However, the stages of the pipeline in-
troduced cascading errors, meaning that the trig-
ger missed in the trigger detection would never 
be recalled in the following stages. By changing 
the pipeline and adding argument information in 
trigger detection, we construct a model for ex-
tracting complex events using rich features and 
achieve better performance than the baseline sys-
tem implemented according to Bj?rne et al's 
(2009) paper. 
2 Our Event Extraction System  
Fig.1 shows the overall architecture of the pro-
posed system. Since 97% of all annotated events 
are fully contained within a single sentence, our 
system deals with one sentence at a time, which 
does not incur a large performance penalty but 
greatly reduces the size and complexity of the 
machine learning problems (Bj?rne et al, 2009). 
The system?s components are different from 
those of the Uturku system by adding a second 
trigger detection component and a second edge 
detection component (argument detection). Trig-
ger detection component is used to recognize the 
trigger words that signify the event, and edge 
detection component is used to identify the ar-
guments that undergo the change. Semantic post-
processing component generates events consis-
tent with the restrictions on event argument types 
and combinations defined in the shared task. 
 
Input data
Sentence splitting
Token zation
Parsing
First Trigger 
detection
(multi-class SVM)
First Edge detection
(multi-class SVM)
Second Trigger 
detection
(multi-class SVM)
Second Edge 
detection
(multi-class SVM)
Semantic
 post-processing
(Rule based)
Output data
 
Figure 1. The flow chart of our system. 
In the following sections, we present the im-
plementation for these stages in our biomedical 
event extraction system in detail and evaluate our 
system on the BioNLP?13 data sets. 
2.1 Trigger Detection 
nuclear extracts showed decreased or absent p65 protein levels
Neg_reg Pro
Theme
 
Figure 2. An example of the trigger consisting of two 
head tokens 
Trigger detection assigns each token an event 
class or a negative class (if the token is not a 
trigger). The head token is chosen when the real 
trigger consists of several tokens, which does not
110
Type Feature 
Primary features The token 
Part-Of-Speech of the token 
Base form 
The rest part of the token, getting rid of the stem word 
Token feature Token has a capital letter 
Token has a first letter of the sentence 
Token has a number 
Token has a symbol like ?-?,?/?,?\? 
N-grams (n = 2, 3) of characters 
Govern and Dependent feature Dependency type 
Part-Of-Speech (POS) of the other token 
Combine the POS and the dependency type 
The word form of the other token 
Frequency features Number of named entities in the sentence 
Bag-of-word counts of token texts in the sentence 
Shortest path Token features of the token in the path 
N-grams of dependencies (n =2, 3, 4) 
N-grams of words (base form + POS) (n =2, 3, 4) 
N-grams of consecutive words (base form + POS) representing 
Governor-dependent relationships (n =1, 2, 3) 
Table 1: Features for the first trigger detection 
Type Feature 
Path feature The token in the path 
The POS of the token in the path 
The dependency type of edges in the path 
(all these features are combined with direction, length and the entity type) 
Table 2: Added feature for the second trigger detection 
incur performance penalty with the approximate 
span matching/approximate recursive matching 
mode (Kim et al, 2009).  Two head tokens may 
be chosen from one trigger when the trigger con-
sists of two appositives. For example, for the 
snippets ?decreased or absent p65 protein le-
vels?, both ?decreased? and ?absent? are the 
head token of the trigger ?decreased or absent?, 
shown in Fig 2. Rich features are extracted for 
the first trigger detection, shown in Table 1. 
To remove the erroneous events and correct 
the event type assigned in the first trigger detec-
tion, a second trigger detection is added in our 
system. Thus the second trigger detection is dif-
ferent from the first one. Uturku system shows 
that the trigger information improves the edge 
detection because of the constraints on the type 
of arguments. Naturally, the edge information is 
helpful for trigger detection with the same reason. 
As a result, this method can improve the preci-
sion of trigger performance. 
In order to leverage the argument information, 
we explore a lot of features of the edges which 
are the arguments detected in the first edge de-
tection. The edge information concerns the fea-
tures of the edges attached to the token. In the 
second trigger detection, we add all the path fea-
tures between the candidate trigger and argu-
ments attached to the candidate trigger detected 
in the first edge detection. These features contain 
the entity information of the argument, the de-
pendency path between the trigger and the argu-
ment and so on. Specially, the added features 
cannot contain any trigger type information ob-
tained in the first trigger detection, or the added 
features cannot do any help. The reason is that 
SVM classifier will classify samples only relying 
on the label feature if it is in the feature set. The 
added features are shown in Table 2. 
 
 
111
Type Features 
N-grams N-grams of consecutive tokens(n=2,3,4) in the path 
N-grams of vertex walks 
Terminal node feature Token feature of the terminal nodes 
The entity type of the terminal nodes 
Re-normalized confidences of all event class 
Frequency feature The length of the path 
The number of entities in the sentence 
Edges feature in the path Dependency type of the edges in the path 
The POS of the tokens in the path 
The tokens in the path 
Table 3: Features for edge detection 
2.2 Edge Detection 
Similar to the trigger detector, the edge detector 
is based on a multi-class SVM classifier. An 
edge is from a trigger to a trigger or from a trig-
ger to a protein. The edge detector classifies each 
candidate edge as a theme, a cause, or a negative 
denoting the absence of an edge between the two 
nodes in the given direction. The features in edge 
detection are shown in Table 3. As the trigger 
information is helpful in edge detection, the ter-
minal node feature contains it. Additionally?the 
first edge detection is completely the same as the 
second one, that is, they share the same features 
and machine learning strategy. 
2.3 Semantic Post-processing 
After the trigger detection and edge detection, 
the biomedical event cannot be produced directly. 
Some simple events may be attached with sever-
al proteins, and complex events may form circles. 
We develop a custom rule-based method to gen-
erate events that are consistent with the restric-
tions on event argument types and combinations 
defined in the shared task. For details, Bj?rne et 
al.?s (2009) paper can be referred to. 
3 Tools and Component Combination  
We use the support vector machine (SVM) mul-
ti-class classifier (Crammer and Singer (2002),  
Tsochantaridis et al (2004)) in the trigger detec-
tion and edge detection. Besides, the dependency 
parser used in our system is McClosky-Charniak 
domain-adapted parser (McClosky and  Charniak 
(2008)) and the dependency parse was provided 
in the share task1 . To optimize the precision-
recall trade-off, we introduce ? that decreases the 
classifier confidence score given to the negative 
                                                 
1 http://2013.bionlp-st.org/supporting-resources 
trigger class as formula (1) as the Uturku system 
does (2009).  
score = score-(1-?)*abs(score)       (1) 
where abs(score) means the absolute value of 
score and ??[0,1]. 
4 Evaluations and Discussion 
4.1 Evaluations 
Firstly, our system is evaluated on the develop-
ment set. Table 4 compares the performance be-
tween our system and the baseline. The baseline 
is implemented based on Bj?rne et al?s (2009) 
paper. Compared to baseline, the precision of our 
system is 6.08 percentage points higher while the 
recall increases 0.91 percentage points. From 
Table 4 we can see that our system is 2.85 F-
score higher than the baseline system. 
 
 Recall  Precision F-score 
Baseline  43.15 54.37 48.12 
Ours 44.06 60.45 50.97 
Table 4: Performance comparison on the development 
set using approximate span and recursive matching 
Secondly, the performance of our system is 
evaluated on the test data set with online evalua-
tion2. Table 5 shows the results for the baseline 
and the proposed system with argument informa-
tion to evaluate the importance of argument in-
formation. Integrating argument information, our 
system archives 1.78% F-score improvement. 
Compared to the baseline, the performance for 
complex events is very encouraging with about 
7.5 percentage points improvement in the Phos-
phorylation events, 1.77 percentage points im-
provement in the regulation events, 2.91 percen- 
                                                 
2 http://bionlp-st.dbcls.jp/GE/2013/eval-test/ 
112
Event type # Our system Baseline 
R/P/F-score R/P/F-score 
Gene_expression 619 77.54/82.76/80.07 79.48/78.10/78.78 
Transcription 101 49.50/65.79/56.50 53.47/62.79/57.75 
Protein_catabolism 14 78.57/55.00/64.71 78.57/45.83/57.89 
Localization 99 35.35/89.74/50.72 38.38/84.44/52.78 
=[SIMPLE ALL]= 833 69.15/80.56/74.42 71.43/75.80/73.55 
Binding 333 40.84/44.16/42.43 42.64/44.65/43.63 
Protein_modification 1 0.00/0.00/0.00 0.00/0.00/0.00 
Phosphorylation 160 75.00/77.42/76.19 69.38/68.10/68.73 
Ubiquitination 30 0.00/0.00/0.00 0.00/0.00/0.00 
Acetylation 0 0.00/0.00/0.00 0.00/0.00/0.00 
Deacetylation 0 0.00/0.00/0.00 0.00/0.00/0.00 
=[PROT-MOD ALL]= 191 62.83/77.42/69.36 58.12/68.10/62.71 
Regulation 288 15.28/42.72/22.51 14.58/35.90/20.74 
Positive_regulation 1130 29.20/44.47/35.26 26.11/42.51/32.35 
Negative_regulation 526 26.81/41.47/32.56 25.10/35.11/29.27 
=[REGULATION ALL]= 1944 26.49/43.46/32.92 24.13/39.51/29.96 
==[EVENT TOTAL]== 3301 40.81/57.00/47.56 39.90/53.69/45.78 
Table 5: Approximate span matching/approximate recursive matching on test data set. 
Th(E1)
Triggering of the human interleukin-6 gene by interferon-gamma and tumor necrosis factor-alpha 
Binding Pro Pro Pro
Th(E2) Th(E1)Th(E2)
  
(a) Th(E1)
Triggering of the human interleukin-6 gene by interferon-gamma and tumor necrosis factor-alpha 
Pos-Reg Pro Pro Pro
Cause(E2) Cause(E1)Th(E2)
 
(b) 
Figure 3: (a) A result of a fragment using the first trigger detection. (b) A result of a fragment using the second 
trigger detection. 
tage points improvement in the positive regula-
tion events and 3.29 percentage points increase 
in the negative regulation events, but not much 
loss in other events. As a consequence, the total 
F-score of our system is 47.56%, 1.78 percentage 
points higher than the baseline system and ob-
tains the 5th place in BioNLP'13 task 1. 
4.2 Discussion 
Our system achieves better performance than the 
baseline thanks to the second trigger detection. 
The second trigger detection improves the per-
formance of event extraction in two ways. Firstly, 
the triggers that cannot form events are directly 
deleted, and therefore the corresponding errone-
ous events are deleted. Secondly, since the erro-
neous triggers are deleted or the triggers recog-
nized in the first trigger detection are given the 
right types in the second trigger detection, the 
corresponding arguments are reconstructed to 
form right events. Fig.3 shows an example. In 
the first trigger detection, the trigger ?triggering? 
is recognized as the illegal type of ?binding? so 
that ?interferon-gamma? and ?tumor necrosis 
factor-alpha? are illegally detected as theme ar-
guments of ?triggering?, resulting in erroneous 
events. However, in the second trigger detection, 
113
?triggering? is correctly revised as the type of 
positive regulation, so the arguments are recon-
structed, which makes the positive regulation 
events (E1 and E2) right. As a result, the preci-
sion of event detection increases as well as the 
recall. 
The proposed method is an efficient way to 
reduce cascading errors in pipeline system. 
Moreover, Riedel and McCallum (2011) pro-
posed a dual decomposition-based model, anoth-
er efficient method to get around cascading er-
rors. Following Riedel et al?s (2011) paper, we 
implement a dual decomposition-based system 
using the same features in our system. Table 6 
shows the performance comparison on the devel-
opment set of BioNLP?09 between our system 
and dual decomposition-based system. The com-
parison indicates that the proposed method is 
comparable to the stat-of-the-art systems.  
 
 Recall  Precision F-score 
Dual Decom-
position 
50.08 63.66 56.06 
Ours 53.88 59.67 56.63 
Table 6: Performance comparison on the development 
set of BioNLP?09 using approximate span and recur-
sive matching based on different methods 
5 Conclusions 
We proposed a simple but effective method to 
improve event extraction by boosting the trigger 
detection. The added edge information in the 
second trigger detection improves the perfor-
mance of trigger detection. Features from the 
dependency parse graphs are the main features 
we use for event extraction. 
The future work includes: the first trigger de-
tection should classify a token into three classes: 
simple event type, complex event type and none 
event type; discovering some more helpful edge 
features in the second trigger detection; solving 
coreference problem with coreference resolution 
approach. Besides, the dual decomposition-based 
method will be improved and further compared 
with the pipeline system. 
 
Acknowledgments 
 
This work is supported by grant from the Nation-
al Natural Science Foundation of China (no. 
61173101, 61173100). 
References  
Antti Airola, Sampo Pyysalo, Jari Bj?rne, Tapio Pa-
hikkala, Filip Ginter, and Tapio Salakoski. 2008. 
All-paths graph kernel for protein-protein interac-
tion extraction with evaluation of cross-corpus 
learning. BMC Bioinformatics, 9(Suppl 11):S2. 
Chris Quirk, Pallavi Choudhury, Michael Gamon, and 
Lucy Vanderwend. 2011. MSR-NLP Entry in Bi-
oNLP Shared Task 2011. In Proceedings of the Bi-
oNLP 2011 Workshop Companion Volume for 
Shared Task, Portland, Oregon, June. Association 
for Computational Linguistics. 
David McClosky and Eugene Charniak. 2008. Self-
training for biomedical parsing. In Proceedings of 
ACL-08: HLT, Short Papers, pages 101?104. Asso-
ciation for Computational Linguistics. 
Hoifung Poon, Lucy Vanderwende. 2010. Joint Infe-
rence for Knowledge Extraction from Biomedical 
Literature. In Proceedings of the North American 
Chapter of the Association for Computational Lin-
guistics-Human Language Technologies 2010 con-
ference. 
Hong-Woo Chun, Yoshimasa Tsuruoka, Jin-Dong 
Kim, Rie Shiba, Naoki Nagata, Teruyoshi Hishiki, 
and Jun?ichi Tsujii. 2006. Extraction of gene-
disease relations from medline using domain dic-
tionaries and machine learning. In Proceedings of 
the Pacific Symposium on Biocomputing (PSB?06), 
pages 4?15. 
Ioannis Tsochantaridis, Thomas Hofmann, Thorsten 
Joachims, and Yasemin Altun. 2004. Support vec-
tor machine learning for interdependent and struc-
tured output spaces. In Proceedings of the Twenty-
first International Conference on Machine Learn-
ing (ICML?04), pages 104?111. ACM. 
Jari Bj?rne, Juho Heimonen, Filip Ginter, Antti Airola, 
Tapio Pahikkala, and Tapio Salakoski. 2009. Ex-
tracting complex biological events with rich graph-
based feature sets. In Proceedings of the BioNLP 
2009 Workshop Companion Volume for Shared 
Task, pages 10?18, Boulder, Colorado, June. Asso-
ciation for Computational Linguistics.  
Jari Bj?rne and Tapio Salakoski. 2011. Generalizing 
biomedical event extraction. In Proceedings of the 
BioNLP 2011 Workshop Companion Volume for 
Shared Task, Portland, Oregon, June. Association 
for Computational Linguistics. 
Jin-Dong Kim, Tomoko Ohta, Sampo Pyysalo, Yo-
shinobu Kano, and Junichi Tsujii. 2009. Overview 
of BioNLP?09 Shared Task on event extraction. In 
Proceedings of the NAACL-HLT 2009 Workshop 
on Natural Language Processing in Biomedicine 
(BioNLP?09). ACL. 
Jin-Dong Kim, Sampo Pyysalo, Tomoko Ohta, Robert 
Bossy, and Jun?ichi Tsujii. 2011. Overview of Bi-
114
oNLP Shared Task 2011. In Proceedings of the Bi-
oNLP 2011 Workshop Companion Volume for 
Shared Task, Portland, Oregon, June. Association 
for Computational Linguistics. 
Jin-Dong Kim, Yue Wang and Yamamoto Yasunori. 
2013. The Genia Event Extraction Shared Task, 
2013 Edition - Overview. In Proceedings of the Bi-
oNLP Shared Task 2013 Workshop, Sofia, Bulgaria, 
Aug. Association for Computational Linguistics. 
Koby Crammer and Yoram Singer. 2002. On the al-
gorithmic implementation of multiclass kernel-
based vector machines. Journal of Machine Learn-
ing Research, 2:265?292. 
Makoto Miwa, Rune S?tre, Yusuke Miyao, and-
Jun?ichi Tsujii. 2009. A rich feature vector for 
protein?protein interaction extraction from mul-
tiple corpora. In EMNLP?09: Proceedings of the 
2009 Conference on Empirical Methods in Natu-
ral Language Processing, pages 121?130, Morris-
town, NJ, USA. Association for Computational 
Linguistics. 
Makoto Miwa, Sampo Pyysalo, Tadayoshi Hara, and 
Jun?ichi Tsujii. 2010a . A comparative study of 
syntactic parsers for event extraction. In Proceed-
ings of BioNLP?10  p. 37?45. 
Makoto Miwa, Sampo Pyysalo, Tadayoshi Hara, and 
Jun?ichi Tsujii. 2010b. Evaluating dependency re-
presentation for event extraction. In Proceedings of 
the 23rd International Conference on Computa-
tional Linguistics, COLING ?10, Association for 
Computational Linguistics, 2010; p. 779?787. 
Makoto Miwa, Paul Thompson, and Sophia Anania-
dou. 2012. Boosting automatic event extraction 
from the literature using domain adaptation and co-
reference resolution. Bioinformatics.  
Sebastian Riedel, Hong-Woo Chun, Toshihisa Taka-
gi,and Jun?ichi Tsujii. 2009. A Markov logic ap-
proach to bio-molecular event extraction. In Bi-
oNLP?09: Proceedings of the Workshop on BioNLP, 
pages 41-49, Morristown, NJ, USA. Association 
for Computational Linguistics. 
Sebastian Riedel and Andrew McCallum. 2011. Ro-
bust Biomedical Event Extraction with Dual De-
composition and Minimal Domain Adaptation. In 
Proceedings of the BioNLP 2011 Workshop Com-
panion Volume for Shared Task, Portland, Oregon, 
June. Association for Computational Linguistics. 
115
