Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 150?154,
Baltimore, Maryland, USA, June 23-25 2014.
c?2014 Association for Computational Linguistics
Simple extensions for a reparameterised IBM Model 2
Douwe Gelling
Department of Computer Science
The University of Sheffield
d.gelling@shef.ac.uk
Trevor Cohn
Computing and Information Systems
The University of Melbourne
t.cohn@unimelb.edu.au
Abstract
A modification of a reparameterisation of
IBM Model 2 is presented, which makes
the model more flexible, and able to model
a preference for aligning to words to either
the right or left, and take into account POS
tags on the target side of the corpus. We
show that this extension has a very small
impact on training times, while obtain-
ing better alignments in terms of BLEU
scores.
1 Introduction
Word alignment is at the basis of most statistical
machine translation. The models that are gener-
ally used are often slow to train, and have a large
number of parameters. Dyer et al (2013) present
a simple reparameterization of IBM Model 2 that
is very fast to train, and achieves results similar to
IBM Model 4.
While this model is very effective, it also has
a very low number of parameters, and as such
doesn?t have a large amount of expressive power.
For one thing, it forces the model to consider
alignments on both sides of the diagonal equally
likely. However, it isn?t clear that this is the case,
as for some languages an alignment to earlier or
later in the sentence (above or below the diagonal)
could be common, due to word order differences.
For example, when aligning to Dutch, it may be
common for one verb to be aligned near the end of
the sentence that would be at the beginning in En-
glish. This would mean most of the other words in
the sentence would also align slightly away from
the diagonal in one direction. Figure 1 shows an
example sentence in which this happens. Here, a
circle denotes an alignment, and darker squares
are more likely under the alignment model. In
this case the modified Model 2 would simply make
both directions equally likely, where we would re-
ally like for only one direction to be more likely.
Hij had de man gezien
He
had
seen
the
man
Figure 1: Visualization of aligned sentence pair in
Dutch and English, darker shaded squares have a
higher alignment probability under the model, a
circle indicates a correct alignment. The English
sentence runs from bottom to top, the Dutch sen-
tence left to Right.
In some cases it could be that the prior probability
for a word alignment should be off the diagonal.
Furthermore, it is common in word alignment to
take word classes into account. This is commonly
implemented for the HMM alignment model as
well as Models 4 and 5. Och and Ney (2003) show
that for larger corpora, using word classes leads
to lower Alignment Error Rate (AER). This is not
implemented for Model 2, as it already has an
alignment model that is dependent on both source
and target length, and the position in both sen-
tences, and adding a dependency to word classes
would make the the Model even more prone to
overfitting than it already is. However, using the
reparameterization in (Dyer et al, 2013) would
leave the model simple enough even with a rela-
tively large amount of word classes.
Figure 2 shows an example of how the model
extensions could benefit word alignment. In the
example, all the Dutch words have a different
150
Hij had de man gezien
He
had
seen
the
man
Figure 2: Visualization of aligned sentence pair in
Dutch and English, darker shaded squares have a
higher alignment probability under the model, a
circle indicates a correct alignment. The English
sentence runs from bottom to top, the Dutch sen-
tence left to Right.
word class, and so can have different gradients for
alignment probability over the english words. If
the model has learned that prepositions and nouns
are more likely to align to words later in the sen-
tence, it could have a lower lambda for both word
classes, resulting in a less steep slope. If we also
split lambda into two variables, we can get algn-
ment probabilities as shown above for the Dutch
word ?de?, where aligning to one side of the diag-
onal is made more likely for some word classes.
Finally, instead of just having one side of the di-
agonal less steep than the other, it may be useful
to instead move the peak of the alignment prob-
ability function off the diagonal, while keeping it
equally likely. In Figure 2, this is done for the past
participle ?gezien?.
We will present a simple model for adding the
above extensions to achieve the above (splitting
the parameter, adding an offset and conditioning
the parameters on the POS tag of the target word)
in section 2, results on a set of experiments in sec-
tion 3 and present our conclusions in section 4.
2 Methods
We make use of a modified version of Model 2,
from Dyer et al (2013), which has an alignment
model that is parameterised in its original form
solely on the variable ?. Specifically, the proba-
bility of a sentence e given a sentence f is given
as:
m
?
i=1
n
?
j=0
?(a
i
|i,m, n) ? ?(e
i
|f
a
i
)
here, m is the length of the target sentence e, n
the same for source sentence f , ? is the alignment
model and ? is the translation model. In this pa-
per we are mainly concerned with the alignment
model ?. In the original formulation (with a minor
tweak to ensure symmetry through the center), this
function is defined as:
?(a
i
= j|i,m, n) =
?
?
?
?
?
?
?
p
0
j = 0
(1? p
0
) ?
e
h(i,j,m,n)
Z(i,m,n)
0 < j ? n
0 otherwise
where, h(?) is defined as
h(i, j,m, n) = ??
?
?
?
?
i
m+ 1
?
j
n+ 1
?
?
?
?
and Z
?
(i,m, n) is
?
n
j
?
=1
e
?h(i,j
?
,m,n)
, i.e. a
normalising function. Like the original Model 2
(Brown et al, 1993), this model is trained us-
ing Expectation-Maximisation. However, it is not
possible to directly update the ? parameter during
training, as it cannot be computed analytically. In-
stead, a gradient-based approach is used during the
M-step.
Two different optimisations are employed, the
first of which is used for calculating Z
?
. This
function forms a geometric series away from the
diagonal (for each target word), which can be
computed efficiently for each of the directions
from the diagonal. The second is used during the
M-step when computing the derivative, and is very
similar, but instead of using a geometric series, an
arithmetico-geometric series is used.
In order to allow the model to have a different
parameter above and below the diagonal, the only
change needed is to redefine h(?) to use a different
parameter for ? above and below the diagonal. We
denote these parameters as ? and ? for below and
above the diagonal respectively. Further, the offset
is denoted as ?.
we change the definition of h(?) to the following
instead:
151
h(i, j,m, n) =
?
?
?
?
?
?
?
?
?
??
?
?
?
?
i
m+ 1
?
j
n+ 1
+ ?
?
?
?
?
j <= j
?
??
?
?
?
?
i
m+ 1
?
j
n+ 1
+ ?
?
?
?
?
otherwise
j
?
is the point closest to or on the diagonal here,
calculated as:
max(min(b
i ? (n+ 1)
m+ 1
+ ? ? (n+ 1)c, n), 0)
Here, ? can range from ?1 to 1, and thus the
calculation for the diagonal j
?
is clamped to be in
a valid range for alignments.
As the partition function (Z(?)) used in (Dyer et
al., 2013) consists of 2 calculations for each tar-
get position i, one for above and one for below the
diagonal, we can simply substitute ? for the geo-
metric series calculations in order to use different
parameters for each:
s
?
(e
?h(i,j
?
,m,n)
, r) + s
n??
(e
?h(i,j
?
,m,n)
, r)
where j
?
is j
?
+ 1.
2.1 Optimizing the Parameters
As in the original formulation, we need to use
gradient-based optimisation in order to find good
values for ?, ? and ?. Unfortunately, optimizing
? would require taking the derivative of h(?), and
thus the derivative of the absolute value. This is
unfortunately undefined when the argument is 0,
however we work around this by choosing a sub-
gradient of 0 at that point. This means the steps we
take do not always improve the objective function,
but in practice the method works well.
The first derivative of L with respect to ? at a
single target word becomes:
?
?
L =
j
?
?
k=1
p(a
i
= k|e
i
, f,m, n)h(i, k,m, n)
?
j
?
?
l=1
?(l|i,m, n)h(i, l,m, n)
And similar for finding the first derivative with
respect to ?, but summing from j
?
to n instead.
The first derivative with respect to ? then, is:
?
?
L =
n
?
k=1
p(a
i
= k|e
i
, f,m, n)h
?
(i, k,m, n)
?
j
?
?
l=1
?(l|i,m, n)h
?
(i, l,m, n)
Where h
?
(?) is the first derivative of h(?) with
respect to ?. For obtaining this derivative, the
arithmetico-geometric series (Fernandez et al,
2006) was originally used as an optimization, and
for the gradient with respect to omega a geometric
series should suffice, as an optimization, as there
is no conditioning on the source words. This is
not done in the current work however, so timing
results will not be directly comparable to those
found in (Dyer et al, 2013).
Conditioning on the POS of the target words
then becomes as simple as using a different ?, ?,
and ? for each POS tag in the input, and calculat-
ing a separate derivative for each of them, using
only the derivatives at those target words that use
the POS tag. A minor detail is to keep a count of
alignment positions used for finding the derivative
for each different parameter, and normalizing the
resulting derivatives with those counts, so the step
size can be kept constant across POS tags.
3 Empirical results
The above described model is evaluated with ex-
periments on a set of 3 language pairs, on which
AER scores and BLEU scores are computed. We
use similar corpora as used in (Dyer et al, 2013):
a French-English corpus made up of Europarl ver-
sion 7 and news-commentary corpora, the Arabic-
English parallel data consisting of the non-UN
portions of the NIST training corpora, and the
FBIS Chinese-English corpora.
The models that are compared are the original
reparameterization of Model 2, a version where ?
is split around the diagonal (split), one where pos
tags are used, but ? is not split around the diagonal
(pos), one where an offset is used, but parameters
aren?t split about the diagonal (offset), one that?s
split about the diagonal and uses pos tags (pos &
split) and finally one with all three (pos & split &
offset). All are trained for 5 iterations, with uni-
form initialisation, where the first iteration only
the translation probabilities are updated, and the
other parameters are updated as well in the sub-
sequent iterations. The same hyperparameters are
152
Model Fr-En Ar-En Zh-En
Tokens 111M 46M 17.3M
(after) 110M 29.0M 10.4M
average 1.64 0.76 0.27
Model 4 15.5 6.3 2.2
Table 1: Token counts and average amount of time
to train models (and separately training time for
Model 4) on original corpora in one direction in
hours, by corpus.
used as in (Dyer et al, 2013), with stepsize for up-
dates to ? and ? during gradient ascent is 1000,
and that for ? is 0.03, decaying after every gradi-
ent descent step by 0.9, using 8 steps every iter-
ation. Both ? and ? are initialised to 6, and ? is
initialised to 0. For these experiments the pos and
pos & split use POS tags generated using the Stan-
ford POS tagger (Toutanova and Manning, 2000),
using the supplied models for all of the languages
used in the experiments. For comparison, Model
4 is trained for 5 iterations using 5 iterations each
of Model 1 and Model 3 as initialization, using
GIZA++ (Och and Ney, 2003).
For the comparisons in AER, the corpora are
used as-is, but for the BLEU comparisons, sen-
tences longer than 50 words are filtered out. In
Table 2 the sizes of the corpora before filtering are
listed, as well as the time taken in hours to align
the corpora for AER. As the training times for
the different versions barely differ, only the aver-
age is displayed for the models here described and
Model 4 training times are given for comparison.
Note that the times for the models optimizing only
? and ?, and the model only optimizing ? still cal-
culate the derivatives for the other parameters, and
so could be made to be faster than here displayed.
For both the BLEU and AER results, the align-
ments are generated in both directions, and sym-
metrised using the grow-diag-final-and heuristic,
which in preliminary tests had shown to do best in
terms of AER.
The results are given in Table 2. These scores
were computed using the WMT2012 data as gold
standard. The different extensions to the model
make no difference to the AER scores for Chinese-
English, and actually do slightly worse for French-
English. In both cases, Model 4 does better than
the models introduced here.
Model Fr-En Zh-En
Original 16.3 42.5
Split 16.8 42.5
Pos 16.6 42.5
Offset 16.8 42.5
Pos & Split 16.8 42.5
Pos & Split & Offset 16.7 42.5
Model 4 11.2 40.5
Table 2: AER results on Chinese-English and
French-English data sets
Model Fr-En Ar-En Zh-En
Original 25.9 43.8 32.8
Split 25.9 43.2 32.8
Pos 25.9 43.9 32.9
Offset 26.0 43.9 32.8
Pos & Split 26.0 44.1 33.2
Pos & Split & Offset 26.0 44.2 33.3
Model 4 26.8 43.9 32.4
Table 3: BLEU results on Chinese-English and
French-English data sets
For the comparisons of translation quality, the
models are trained up using a phrase-based trans-
lation system (Koehn et al, 2007) that used the
above listed models to align the data. Language
models were augmented with data outside of the
corpora for Chinese-English (200M words total)
and Arabic-English (100M words total). Test sets
for Chinese are MT02, MT03, MT06 and MT08,
for Arabic they were MT05, MT06 and MT08, and
for French they were the newssyscomb2009 data
and the newstest 2009-2012 data.
The results are listed in Table 3
1
. BLEU scores
for Arabic-English and Chinese-English are com-
puted with multiple references, while those for
French-English are against a single reference. Al-
though the different models made little difference
in AER, there is quite a bit of variation in the
BLEU scores between the different models. In
all cases, the models conditioned on POS tags
did better than the original model, by as much
as 0.5 BLEU points. For Arabic-English as well
as Chinese-English, the full model outperformed
1
The difference in these results compared to those re-
ported in Dyer et al (2013) is due to differences in corpus
size, and the fact that a different translation model is used.
153
Model 4, in the case of Chinese-English by 0.9
BLEU points.
The low impact of the split and offset models
are most likely due to the need to model all align-
ments in the corpus. The distributions can?t skew
too far to aligning to one direction, as that would
lower the probability of a large amount of align-
ments. This is reflected in the resulting parame-
ters ?, ? and ? that are estimated, as the first two
do not differ much from the parameters estimated
when both are kept the same, and the second tends
to be very small.
As for the Pos model, it seems that only vary-
ing the symmetrical slope for the different POS
tags doesn?t capture the differences between dis-
tributions for POS tags. For example, the ? and
? parameters can differ quite a lot in the Pos &
Split model when compared to the Pos model, with
one side having a much smaller parameter and the
other a much larger parameter for a given POS tag
in the first model, and the single parameter being
closer to the model average for the same POS tag
in the second model.
The low variation in results between the differ-
ent models for French-English might be explained
by less word movement when translating between
these languages, which could mean the original
model is sufficient to capture this behaviour.
4 Conclusion
We have shown some extensions to a reparame-
terized IBM Model 2, allowing it to model word
reordering better. Although these models don?t
improve on the baseline in terms of AER, they
do better than the original in all three languages
tested, and outperform M4 in two of these lan-
guages, at little cost in terms of training time. Fu-
ture directions for this work include allowing for
more expressivity of the alignment model by using
a Beta distribution instead of the current exponen-
tial model.
5 Acknowledgments
Dr Cohn is the recipient of an Australian Re-
search Council Future Fellowship (project number
FT130101105).
References
Peter F. Brown, Vincent J. Della Pietra, Stephen
A. Della Pietra, and Robert. L. Mercer. 1993.
The mathematics of statistical machine translation:
Parameter estimation. Computational Linguistics,
19:263?311.
Chris Dyer, Victor Chahuneau, and Noah A Smith.
2013. A simple, fast, and effective reparameteri-
zation of ibm model 2. In Proceedings of NAACL-
HLT, pages 644?648.
P. A. Fernandez, T. Foregger, and J. Pahikkala. 2006.
Arithmetico-geometric series.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, Chris Dyer, Ond?rej Bojar, Alexandra
Constantin, and Evan Herbst. 2007. Moses: Open
source toolkit for statistical machine translation. In
Proceedings of the 45th Annual Meeting of the ACL
on Interactive Poster and Demonstration Sessions,
ACL ?07, pages 177?180, Stroudsburg, PA, USA.
Association for Computational Linguistics.
Franz Josef Och and Hermann Ney. 2003. A sys-
tematic comparison of various statistical alignment
models. Comput. Linguist., 29:19?51, March.
Kristina Toutanova and Christopher D. Manning.
2000. Enriching the knowledge sources used in a
maximum entropy part-of-speech tagger. In Pro-
ceedings of the 2000 Joint SIGDAT Conference on
Empirical Methods in Natural Language Process-
ing and Very Large Corpora: Held in Conjunction
with the 38th Annual Meeting of the Association
for Computational Linguistics - Volume 13, EMNLP
?00, pages 63?70, Stroudsburg, PA, USA. Associa-
tion for Computational Linguistics.
154
NAACL-HLT Workshop on the Induction of Linguistic Structure, pages 39?46,
Montre?al, Canada, June 3-8, 2012. c?2012 Association for Computational Linguistics
Using Senses in HMM Word Alignment
Douwe Gelling and Trevor Cohn
Department of Computer Science
University of Sheffield, UK
{d.gelling,t.cohn}@sheffield.ac.uk
Abstract
Some of the most used models for statis-
tical word alignment are the IBM models.
Although these models generate acceptable
alignments, they do not exploit the rich in-
formation found in lexical resources, and as
such have no reasonable means to choose bet-
ter translations for specific senses.
We try to address this issue by extending the
IBM HMM model with an extra hidden layer
which represents the senses a word can take,
allowing similar words to share similar output
distributions. We test a preliminary version of
this model on English-French data. We com-
pare different ways of generating senses and
assess the quality of the alignments relative to
the IBM HMM model, as well as the gener-
ated sense probabilities, in order to gauge the
usefulness in Word Sense Disambiguation.
1 Introduction
Modern machine translation is dominated by statis-
tical methods, most of which are trained on word-
aligned parallel corpora (Koehn et al, 2007; Koehn,
2004), which need to be generated separately. One
of the most commonly used methods to generate
these word alignments is to use the IBM models 1-5,
which generate one-directional alignments.
Although the IBM models perform well, they fail
to take into account certain situations. For exam-
ple, if an alignment between two words f1 and e1 is
considered, and f1 is an uncommon translation for
e1, the translation probability will be low. It might
happen, that an alignment to a different nearby word
is preferred by the model. Consider for example
the situation where f1 is ?taal? (Dutch, meaning lan-
guage), and e1 is ?tongue?. The translation probabil-
ity for this may be low, as ?tongue? usually translates
as ?tong?, meaning the body part. In this case the
preference of the alignment model may dominate,
leading to the wrong alignment.
Moreover, the standard tools for word alignment
fail to make use of the lexical resources that already
exist, and which could contribute useful information
for the task. In particular, the ontology defined in
WordNet (Miller, 1995) could be put to good use.
Intuitively, the translation of a word should depend
on the sense of the word being used. The current
work seeks to explore this idea, by explicitly mod-
eling the senses in the translation process. It does
so, by modifying the HMM alignment model to in-
clude synsets as an intermediate stage of translation.
This would facilitate sharing of translation distribu-
tions between words with similar senses that should
generate the correct sense. In terms of the example
above, one of the senses for ?tongue? will share the
translation distribution with ?language?, for which
we will have more relevant translation probabilities.
As well as performing word alignment this model
can be used to generate sense annotations on one
side of a parallel corpus, given an alignment, or even
generate sense annotations while aligning a corpus.
Thus, the model could learn to align a corpus and
do WSD at the same time. In this paper, the effect
the usage of senses has on alignment is investigated,
and the potential usefulness of the model for WSD
is explored. In the next section related work is dis-
cussed, after which in section 3 the current model is
39
discussed.
In section 4 the evaluation of the model is dis-
cussed, in two parts. In the first part, the model is
evaluated for English-French on gold standard man-
ually aligned data and compared to the results of the
base HMM model. In the second part, the model is
qualitatively evaluated by inspecting the senses and
associated output distributions of selected words.
2 Previous Work
Although most researchers agree that Word Sense
Disambiguation (WSD) is a useful field, it hasn?t
been shown to consistently help in related tasks. Ma-
chine Translation is no exception, and whether or
not WSD systems can improve performance of MT
systems is debated. Furthermore, it is unclear how
parallel corpuses can be exploited for WSD systems.
In this section we will present a brief overview of re-
lated work.
(Carpuat and Wu, 2007) report an improvement
in translation quality by incorporating a WSD sys-
tem directly in a phrase-based translation system.
This is in response to earlier work done, where in-
corporating the output of a traditional WSD system
gave disappointing results (Carpuat and Wu, 2005).
The WSD task is redefined, to be similar to choosing
the correct phrasal translation for a word, instead of
choosing a sense from a sense inventory. This sys-
tem is trained on the same data as the SMT system
is.
The output of this model is incorporated into the
machine translation system by providing the WSD
probabilities for a phrase translation as extra features
in a log-linear model (Carpuat and Wu, 2007). This
system consistently outperforms the baseline system
(the same system, but without WSD component), on
multiple metrics, which seems to indicate that WSD
can make a useful contribution to machine transla-
tion. However, the way the system is set up, it could
also be viewed as a way of incorporating translation
probabilities of other systems into the phrase-based
translation model.
(Chan and Ng, 2007) introduce a system very sim-
ilar to that of (Carpuat and Wu, 2007), but as ap-
plied to hierarchical phrase-based translation. They
demonstrate modest improvements in BLEU score
over the unmodified system, as well as some qualita-
tive improvements in the output. Here again, the ar-
gument could be made that what is being done is not
strictly word sense disambiguation, but augmenting
the translation system with extra features for some
of the phrase translations.
In (Tufis? et al, 2004) parallel corpora and aligned
WordNets are exploited for WSD. This is done, by
word aligning the parallel texts, and then for ev-
ery aligned pair, generating a set of wordnet sense
codes (ILI codes, or interlingual index codes) for ei-
ther word, corresponding to the possible senses that
word can take. As the wordnets for both languages
are linked, if the ILI code of a sense is the same, the
sense should be sufficiently similar. Thus, the in-
tersection of both sets of ILI is taken to find an ILI
code that is common to both pairs. If such a code is
found, it represents the sense index of both words.
Otherwise, the closest ILI code to the two most sim-
ilar ILI codes is found, and that is taken as the sense
for the word. The current work however only uses
a lexical resource for one of the languages, and as
such has fewer places to fail, and less demanding
requirements.
Other similar work includes that in (Ng et al,
2003), where a sense-annotated corpus was automat-
ically generated from a parallel corpus. This is done
by word-aligning the parallel corpus, and then find-
ing the senses according to WordNet given a list of
nouns. Two senses are lumped together if they are
translated into the same chinese word. The selec-
tion of correct translations is done manually. Only
those occurrences of the chosen nouns that translate
to one of the chosen chinese words are considered
sense-tagged by the translation.
Although similar in approach to what the current
system would do, this system uses a much more sim-
ple approach to generate sense annotations and it de-
pends on a previously word-aligned corpus, whereas
the current approach would integrate alignment and
sense-tagging, whis may give a higher accuracy.
3 Senses Model
The current model is based on the HMM alignment
model (Vogel et al, 1996), as it is a less complex
model than IBM models 3 and above, but still finds
acceptable alignments. The HMM alignment model
is defined as a HMM model, where the observed
40
e a2a1
fmf1 f2
am
Figure 1: Diagram of HMM model. Arrows indicate
dependencies, grey nodes indicate known values, white
nodes indicate hidden variables.
variables are the words of a sentence in the French
language f, and the hidden variables are alignments
to words in the English sentence e, or to a null state.
See figure 1 for a diagram of the standard HMM
model. Under this model, French words can align to
at most 1 English word. The transition probability
is not dependent on the english words themselves,
but on the size of jumps between alignments and the
length of the English sentence. The probability of
the French sentence given the English sentence is:
Pr(f|e) =
?
a
J?
j=1
p(fj |eaj )p(aj |aj?1, I) (1)
Here, f and e denote the French and English sen-
tences, which have lengths J and I respectively, and
a denotes an alignment of these two sentences. So,
the states in the HMM assign a number from the
range [0, I] to each of the positions j in the French
sentence, effectively assigning one English word eaj
to each French word fj , or a NULL translation e0.
The term p(fj |eaj ) is the translation probability of a
pair of words, and p(aj |aj?1, I) gives the transition
probability in the HMM.
Here, i is the current state of the HMM, and i? is
the previous state of the HMM, each being an index
into the English sentence and p(aj |aj?1, I) is de-
fined as the probability of the gap between i and i?.
So, if in an alignment French word 2 is aligned to the
3rd English word, and the next French Word (3) is
aligned to the 5th English word, p(aj |aj?1, I) isn?t
modelled directly as p(5|3, I), but as p(5? 3|I).
To implement a dependency on senses in the
model an extra hidden layer is added to the HMM
model, representing the senses. The probability of a
s1 s2
e a2a1
fmf1 f2
am
sm
Figure 2: Diagram of SHMM model, with senses gener-
ated by the English words. Arrows indicate dependen-
cies, grey nodes indicate known values, white nodes in-
dicate hidden variables.
french word then depends on the generated sense,
the probability of which depends on the English.
The possible senses for a given English word is con-
strained by an external source, such as WordNet.
The probability under the model of a french sen-
tence f given an English sentence e thus becomes:
Pr(f|e) =
?
a
J?
j=1
p?(fj |eaj )p(aj |aj?1, I) (2)
where
p?(fj |eaj ) =
K?
k=1
p(fj |sk)p(sk|eaj ) (3)
Here, K is the number of senses that english word
associated with this translation pair. The senses will
be constrained either by the English word eaj or by
the French word fj depending on which language
the sense inventory is taken from. The first case,
with senses constrained by the English, will be de-
noted with SHMM1, and the second with SHMM2.
In this work, only SHMM1 is used.
If the amount of senses defined for each word is
exactly 1 and this sense is different for each word,
the model reduces to the HMM model (see Figure
2). However, if the sense inventory is defined such
that for two different words with a sense that is sim-
ilar, the same sense can be used, the model is able
to use translation probabilities drawn from observa-
tions from both these words together. For example,
41
in SHMM1, the words ?small? and ?little? may have
the same sense listed in the sense inventory, which
allows the model to learn a translation distribution to
the French words that both these words often align
to.
For training this model, as with the IBM models,
Expectation-Maximization and initialisation are key.
The more complex IBM models are initialised from
simpler versions, so the complex models can start
out with reasonable estimates, which allow it to find
good alignments. Here, too, the same steps are used.
The HMM model is initialised from Model 1, as de-
scribed in citevogel:1996. From this, the SHMM
models can be initialised.
For the SHMM1, given a translation probability
for a french word given an english word under the
HMM, p(f |e), and a list of valid senses for that
english word e, an equal portion of that translation
probability is given to the new translation probabil-
ity depending on the sense. This is done for all trans-
lation probabilities, and the translation table is then
normalised. Probability of a sense given an english
word is initialised to a uniform distribution over the
valid senses.
For the SHMM2, the probability of french words
given a sense is set to uniform over the words for
which the sense is valid, and the probability of the
sense given the english word is calculated analogous
to the probability of the french word given the sense
in the first case.
After initialisation, the expectation-maximisation
algorithm can be used for training, as with the HMM
model, using the forward-backward algorithm to
find the posterior probabilities of the alignments. As
the senses can be summed out during this phase, the
algorithm can be used as-is, and afterwards the pro-
portion of the partial count that should be assigned
to each sense can be found. By summing out over
the relevant senses and words, the two parts c(fj |qk)
and c(qk|ei) can then be found.
3.1 Generating Senses for Words
In order to be able to use this model, an inventory
of senses is needed for every word in the corpus, for
one of the languages. The most obvious source for
this is the English Wordnet (Miller, 1995), as it has
a large inventory of senses. Note that, in this doc-
ument, the words senses and synsets are used inter-
changeably.
The process of obtaining this inventory is ex-
plained from the viewpoint of using English Word-
Net, but the same basic conditions apply for any
other lexicon, or language. The inventory of senses
is obtained through the WordNet corpus in NLTK
1, which automatically stems the words that synsets
are sought for.
In this model, two senses (synsets) are function-
ally equivalent, if the list of words that have them
in their senselist is the same for both senses. That
is to say, if the partial counts that will be added to
either of the senses will be the same, there is no way
of distinguishing between the two senses under this
model. For example, in WordNet 3.0, among the
synsets listed for the word ?small?, there are 3 that
have as constituent words only ?small? and ?little?.
These 3 synsets would be functionally equivalent for
our purposes. When this occurs, the senses that are
equivalent are collated under one name, so that it?s
possible to find out which senses a particular sense
is made up of.
At this point, there will be some words with only
a sense that is unique to that word (such as those
words that were not in the lexicon, which get a newly
made sense), some words with only shared senses
and some with a mix. We might want to enforce one
of a few distinct options:
? All words have exactly 1 unique sense, and per-
haps a few shared ones (?synthesis? condition)
? Some words have a unique sense, some don?t
(?merge? condition)
? No words have unique senses if they have at
least 1 shared sense (?none? condition)
These conditions are generated by first finding the
filtered list of senses for each word. At this point,
some words have only unique senses, either because
they didn?t occur in WordNet, or because WordNet
only listed unique senses for that word (the ?merge?
condition. The ?synth? condition is made, by finding
all words that have only shared senses, and adding a
new sense, that is unique to that word. The ?none?
1http://www.nltk.org/
42
1 2 3 4 5Number of iterations0.15
0.20
0.25
0.30
0.35
0.40
0.45
0.50
AER
Model 1HMMSHMM (none)SHMM (merge)SHMM (synth)
Figure 3: AER scores for Model 1, HMM, and 3 SHMM variations trained for 5 iterations each, lower is better.
condition then is found by doing the opposite: re-
moving all unique senses from words that also have
shared senses.
Under each of these 3 conditions, the model might
work slightly differently. Under the ?synthesis? con-
dition, it may generate the translation probabilities
either directly, as in the HMM (which is what hap-
pens for any word with only 1 sense, which is unique
for that word), or from the shared probabilities,
through the senses. In the other models, the model
is increasingly forced to use the shared translation
probabilities.
4 Evaluation
We will evaluate the early results of this model
against the HMM and Model 1 results, and will do
a qualitative analysis of the distribution over senses
and French words that the model obtains, in order
to find out if reasonable predictions for senses are
made.
The sense HMM model will be evaluated using
the three sense inventories suggested in subsection
3.1. The dataset used was a 1 million sentence
aligned English-French corpus, taken from the Eu-
roparl corpus (Koehn, 2005). The data was to-
kenised, length limited to a maximum length of 50,
and lowercased. The results are evaluated on the test
set from the ACL 2005 shared task, using Alignment
Error Rate. The models are all trained for 5 itera-
tions, and a pruning threshold is employed that re-
moves probabilities from the translation tables if it
is below 1.0 ? 10?6.
The results of training models based on senses
generated in the 3 ways listed above is shown in
Figure 3. The three SHMM models are compared
against Model 1, and the standard HMM model,
each of which is trained for 5 iterations. The HMM
model is initialised from Model 1, and the SHMM
models initialised from the HMM model. As the fig-
ure shows, the AER score for the last two iterations
of the HMM model is very similar to the scores that
the three variations of the SHMM model attain. The
scores for the three HMM models range from 0.185
to 0.192
A possible reason for this performance is that the
models didn?t have enough sharing going on be-
tween the senses. The corpus contains 70700 unique
words. Looking at the amount of senses that are
found in the ?none? condition, meaning that all of the
WordNet senses share output probabilities, there are
17194 words that have at least one of these senses
listed, and there are 27120 distinct senses available
in that setting. For the other 53500 senses, no shar-
ing is going on whatsoever.
In the ?merge? and ?synth? conditions, there are
more senses taken from WordNet (for a total from
WordNet of 33133), but these don?t add any shar-
43
Sense Definition P (s|e) Most likely French words in order
severe.s.06 very bad in degree or
extent
0.4861 graves, se?ve`res, des, se?ve`re, grave, de, grave-
ment, une, se?rieuses, les
severe.s.04 unsparing and un-
compromising in
discipline or judg-
ment
0.2358 graves, se?ve`res, des, se?ve`re, grave, de, grave-
ment, une, se?rieuses, les
dangerous.s.02 causing fear or anx-
iety by threatening
great harm
0.1177 grave, des, graves, les, se?rieux, tre`s,
se?rieuses, une, importantes, se?rieuse
austere.s.01 severely simple 0.1148 graves, des, grave, se?ve`re, se?ve`res, tre`s, forte-
ment, forte, rigoureuses, situation
hard.s.04 very strong or vigor-
ous
0.035 dur, plus, importants, des, se?ve`res, durement,
son, une, difficile, tre`s
severe.s.01 intensely or ex-
tremely bad or
unpleasant in degree
or quality
0.01055 terrible, terribles, des, grave, les, mauvais,
dramatique, cette, aussi, terriblement
Table 1: Senses for the word ?severe? in the ?none? version of the SHMM model, their WordNet definition, the proba-
bility of the sense for the word severe, and the most likely French words for the senses given in order of likelihood.
ing. It might be then, that the model has insuffi-
cient opportunity to share output distributions, caus-
ing it to behave much as the HMM alignment model.
Another possibility is, that the senses insufficiently
well-defined, and share probabilities between words
that are too dissimilar, negating any positive effect
this may have and possibly pushing the model to-
wards less sharing. We will suggest possibilities for
dealing with this in section 5.
Regardless of the performance of the model in
word alignment, if the model learns probabilities for
senses that are reasonable, it can be used as a word
sense disambiguation system for parallel corpora,
with the candidate senses being made up from the
senses out of WordNet. Those words not listed in
WordNet, are treated as being monosemous words
in this context. The ?merge? and ?none? conditions
are most useful for this: if a WSD system chooses a
sense that is not linked to a WordNet sense, it is not
clearly defined which sense is meant here.
In order to find out if the model makes sensi-
ble distinctions between different senses, we have
picked a random polysemous word, and looked at
the senses associated with it in the ?none? condition.
The word that was chosen is ?severe?. It has 6 pos-
Sense Associated English words
severe.s.06 (only has basic 3 senses)
severe.s.04 spartan
dangerous.s.02 dangerous, grave, graver,
gravest, grievous, life-
threatening, serious
austere.s.01 austere, stark, starker, starkest,
stern
hard.s.04 hard, harder, hardest
severe.s.01 terrible, wicked
Table 2: Senses for the word ?severe? in the ?none? ver-
sion of the SHMM model and the English words apart
from ?severe?, ?severer? and ?severest? that have the sense
in their senselist
sible senses, listed by main word and definition in
Table 1, along with the probability of the senses,
p(s|e), and the 10 most likely French words for the
senses.
As the table shows, the two most likely senses are
quite similar. In fact, because words are stemmed
before looking up suitable senses, all senses have at
least the following 3 words associated with them:
?severe?, ?severer? and ?severest?. The words that
44
Sense Definition P(s?e) Most likely French words in order
rigorous.s.01 rigidly accurate; al-
lowing no deviation
from a standard
0.8962 rigoureuse, rigoureux, une, rigueur,
rigoureuses, des, un, stricte, strict, strictes
rigorous.s.02 demanding strict at-
tention to rules and
procedures
0.1038 des, strictes, rigoureux, stricte, se?ve`res,
rigoureuses, stricts, rigoureuse, une, se?ve`re
Table 3: Senses for the word ?rigorous? in the ?none? version of the SHMM model, their WordNet definition, the
probability of the senses of the word ?rigorous?, and the most likely French words for the senses given in order of
likelihood.
cause the differences between the senses are listed
in table 2. It can be seen that the only difference
between severe.s.04 and severe.s.06 is the addition
of the word ?spartan? for the first. As ?spartan? only
occurs 67 times in the corpus, versus 484 for severe,
it is possible that they are so similar, because the
counts for ?spartan? get overshadowed.
For the other senses however, the most likely
translations vary quite a bit. The sense ?hard.s.04?,
meaning very strong or vigorous, also includes
translations to ?plus? and ?dur?, which seems more
likely given the sense. Given these translation prob-
abilities though, it should at least be possible to dis-
tinguish between different senses of the word severe,
given that it?s aligned to a different french word.
One more example is listed in table 3, showing
the probabilities for two different senses, and their
most likely translations. The most likely sense for
rigorous under the model is in the sense of ?allowing
no deviation from a standard?. This is the only of the
two senses that can translate to ?rigueur? in french,
literally rigor. The other sense, meaning ?demand-
ing strict attention to rules and procedures?, is more
likely to translate to ?strictes?, ?stricte? and ?se?ve`res?,
which reflects the WordNet definition.
The difference in contributing English words be-
tween these two senses can be found in Table 4. In-
terestingly, the three forms of the word strict are as-
sociated with the sense rigorous.s.01, even though
the naive translations of these words into French are
more likely for rigorous.s.02. Even so, the results
match the WordNet definitions better.
These results show that useful translations are
found, and the corresponding senses can be learned
as well. For sense discrimination in parallel cor-
puses then, this model shows potential, and for
Sense Associated English words
rigorous.s.01 rigorous strict stricter
strictest
rigorous.s.02 rigorous stringent tight
tighter tightest
alignment good alignments can be found, even with
better abstraction in the model.
5 Conclusion
The results have shown that this may be a useful way
to incorporate senses in a word alignment system.
While the alignment results in themselves weren?t
significantly better, alignment probabilities to senses
have been shown to be generated, which make it pos-
sible to distinguish between different senses. This
could open the door to automatically sense annotat-
ing parallel corpora, using a predefined set of senses.
At this early point, several options lay open to
improve upon the results so far. To improve the
alignment results, more encompassing senses may
be generated, for example by integrating similar
synsets. At the same time, the list of synsets for
each word may be improved upon, by filtering out
very unlikely senses for a word.
It should also be possible to employ an already ex-
isting WSD system to annotate the parallel corpus,
and use the counts of the annotated senses to better
initialise the senses, rather than starting out assum-
ing all are equaly likely for a given word. This may
be used as well to initialise the translation probabil-
ities for senses.
45
References
Marine Carpuat and Dekai Wu. 2005. Word sense disam-
biguation vs. statistical machine translation. In Pro-
ceedings of the 43rd Annual Meeting on Association
for Computational Linguistics, ACL ?05, pages 387?
394, Stroudsburg, PA, USA. Association for Compu-
tational Linguistics.
Marine Carpuat and Dekai Wu. 2007. Improving statisti-
cal machine translation using word sense disambigua-
tion. In In The 2007 Joint Conference on Empirical
Methods in Natural Language Processing and Compu-
tational Natural Language Learning (EMNLP-CoNLL
2007, pages 61?72.
Yee Seng Chan and Hwee Tou Ng. 2007. Word sense
disambiguation improves statistical machine transla-
tion. In In 45th Annual Meeting of the Association
for Computational Linguistics (ACL-07, pages 33?40.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran, Richard
Zens, Chris Dyer, Ondr?ej Bojar, Alexandra Con-
stantin, and Evan Herbst. 2007. Moses: open source
toolkit for statistical machine translation. In Proceed-
ings of the 45th Annual Meeting of the ACL on Inter-
active Poster and Demonstration Sessions, ACL ?07,
pages 177?180, Stroudsburg, PA, USA. Association
for Computational Linguistics.
Philipp Koehn. 2004. Pharaoh: a beam search decoder
for phrase-based statistical machine translation mod-
els. In Proceedings of AMTA 2004 (Conference of the
Association for Machine Translation in the Americas),
volume 3265, pages 115?124. Springer.
P. Koehn. 2005. Europarl: A Parallel Corpus for Sta-
tistical Machine Translation. In Machine Translation
Summit X, pages 79?86, Phuket, Thailand.
George A. Miller. 1995. Wordnet: A lexical database for
english. Communications of the ACM, 38:39?41.
Hwee Tou Ng, Bin Wang, and Yee Seng Chan. 2003. Ex-
ploiting parallel texts for word sense disambiguation:
an empirical study. In Proceedings of the 41st Annual
Meeting on Association for Computational Linguistics
- Volume 1, ACL ?03, pages 455?462, Stroudsburg,
PA, USA. Association for Computational Linguistics.
Dan Tufis?, Radu Ion, and Nancy Ide. 2004. Fine-grained
word sense disambiguation based on parallel corpora,
word alignment, word clustering and aligned word-
nets. In Proceedings of the 20th international con-
ference on Computational Linguistics, COLING ?04,
Stroudsburg, PA, USA. Association for Computational
Linguistics.
Stephan Vogel, Hermann Ney, and Christoph Tillmann.
1996. Hmm-based word alignment in statistical trans-
lation. In Proceedings of the 16th conference on
Computational linguistics - Volume 2, COLING ?96,
pages 836?841, Stroudsburg, PA, USA. Association
for Computational Linguistics.
46
NAACL-HLT Workshop on the Induction of Linguistic Structure, pages 64?80,
Montre?al, Canada, June 3-8, 2012. c?2012 Association for Computational Linguistics
The PASCAL Challenge on Grammar Induction
Douwe Gelling and Trevor Cohn
Department of Computer Science
University of Sheffield, UK
{d.gelling,t.cohn}@sheffield.ac.uk
Phil Blunsom
Department of Computer Science
University of Oxford, UK
Phil.Blunsom@cs.ox.ac.uk
Joa?o Grac?a
L2F Spoken Language Systems Laboratory
INESC ID Lisboa, Portugal
joao.graca@l2f.inesc-id.pt
Abstract
This paper presents the results of the PASCAL
Challenge on Grammar Induction, a compe-
tition in which competitors sought to predict
part-of-speech and dependency syntax from
text. Although many previous competitions
have featured dependency grammars or parts-
of-speech, these were invariably framed as
supervised learning and/or domain adaption.
This is the first challenge to evaluate unsuper-
vised induction systems, a sub-field of syntax
which is rapidly becoming very popular. Our
challenge made use of a 10 different treebanks
annotated in a range of different linguistic for-
malisms and covering 9 languages. We pro-
vide an overview of the approaches taken by
the participants, and evaluate their results on
each dataset using a range of different evalua-
tion metrics.
1 Introduction
Inducing grammatical structure from text has long
been fundamental problem in Computational Lin-
guistics and Natural Language Processing. In re-
cent years interest has grown, spurred by advances
in unsupervised statistical modelling and machine
learning. The task has relevance to cognitive scien-
tists and linguists attempting to gauge the learnabil-
ity of natural language by human children, and also
natural language processing researchers who seek
syntactic representations for languages with few lin-
guistic resources.
Grammar learning has been popular in previous
challenges. For example the CoNLL shared tasks in
2006 and 2007 (Buchholz and Marsi, 2006; Nivre
et al, 2007) involved supervised learning of de-
pendency parsers across a wide range of different
languages. Our challenge has many similarities to
these, in that we focus on dependency grammars,
however we seek to evaluate unsupervised algo-
rithms only using syntactically annotated data for
evaluation and not for training. Additionally we also
consider the related task of part-of-speech (POS) in-
duction, and the next logical challenge: the joint
task of POS and dependency induction. Other re-
lated challenges can be found in the formal gram-
mar community (e.g., the Omphalos1 competition)
in which competitors seek to learn synthetic lan-
guages. In contrast we seek to model natural lan-
guage text, which entails many different challenges.
Research into unsupervised grammar and POS in-
duction holds considerable promise, although cur-
rent approaches are still a long way from solving
the general problem. For example, the majority of
recent research into dependency grammar induction
has adopted the evaluation setting of Klein and Man-
ning (2004) who learn grammars on strings of POS
tags, rather than on words themselves. One aim of
this challenge is to popularise the more difficult and
ambitious task of inducing grammars directly from
text, which can be viewed as integrating the POS and
grammar induction tasks. A second aim is to foster
grammar and POS induction research across a wider
variety of languages, and improving the standard of
evaluation.
We have collated data from existing treebanks in
a variety of different languages, domains and lin-
guistic formalisms. This gives a diverse range of
1See http://www.irisa.fr/Omphalos
64
data upon which to test induction algorithms, yield-
ing a deeper insight into their strengths and short-
comings. One key problem in grammar induction
research is how to evaluate the models? predictions
given that often many different analyses are linguis-
tically plausible, e.g., the choice of whether deter-
miners or nouns should head noun phrases, or how to
represent coordination. Simply comparing against a
single gold standard often results in poor reported
performance because the model has discovered a
different analysis to that used when annotating the
treebank. For this reason it has been popular to use
lenient measures for comparing predicted trees to
the treebank gold standard trees, such as undirected
accuracy and the neutral edge distance (Schwartz et
al., 2011). As well as evaluating using these popular
metrics, we also propose a new method of evaluation
which is also lenient in that it rewards different types
of linguistically plausible output, but requires con-
sistency in the output, something the previous meth-
ods cannot do.
The paper is organised as follows. Section 2 de-
scribes the tasks and our data format and section 3
outlines the different treebanks used for the chal-
lenge. The baselines, our own benchmark systems
and the competitors entries are described in section
5. In section 6 we present and analyse the results
for the three different tracks. Finally we conclude in
section 7.
2 Task Definition
The three tracks of the WILS challenge are de-
scribed below. First we describe the data format for
the submissions common to the three tracks (POS
induction, Dependency induction, and jointly induc-
ing both), and then the three tracks are described
along with the respective evaluation metrics.
2.1 Data format
All datasets were presented in a file format similar to
that used in the CoNLL tasks, but with slight mod-
ifications. In particular the last two columns are re-
moved, as no projective head or projective depen-
dency relations were used, and an extra POS column
was inserted at column 6 to accommodate the Uni-
versal POS tagset (Petrov et al, 2011). Each line in a
file then either consists of 9 columns, separated by a
tab character, or is an empty line. Empty lines sepa-
rate sentences, and all other lines give the annotation
for a single token in the sentence as follows:
1. ID: Token counter, gives the index of current
word in the sentence. Indexing starts at 1.
2. FORM: Surface form of the token in the sen-
tence.
3. LEMMA: Stemmed form of the word form if
available.
4. CPOSTAG: Coarse-grained POS tag.
5. POSTAG: Fine-grained POS tag, or CPOSTAG
again if not available.
6. UPOSTAG: Universal POS tag, based on the
POSTAG and CPOSTAG.
7. FEATS: List of syntactic / morphological fea-
tures, separated by a vertical pipe (|).
8. HEAD: Syntactic head of the token, with 0 in-
dicating the root node.
9. DEPREL: The general type of the dependency
relation, e.g., subject.
In this setup, the LEMMA, FEATS and DEPREL
columns are optional, in which case an underscore
( ) will be used as a placeholder. Each treebank
was split into training, development and testing par-
titions. The HEAD and DEPREL entries were only
supplied for the development and the final testing
sets,2 but not for the training partition. The com-
petitors were encouraged to develop their unsuper-
vised entries on the union of the three partitions, and
make sparse use of the development set, i.e., for san-
ity checking more than model fitting in order to min-
imise the extent of supervision.
2.2 POS induction
In the POS induction track, participants developed
systems to induce the Part-of-Speech (POS) classes
for each word in the testing corpus. In order to train
the systems, the same training and development sets
were used as for the other tracks. These corpora in-
cluded manually supplied POS tags for each token,
2For the initial test set these fields were omitted.
65
which were not to be used for training, only evalua-
tion. Participants submitted predicted tags for each
token, which were scored against the gold-standard.
For evaluation, we used 4 different metrics. The
first is the many-to-one metric (M-1) (also known
as cluster purity), which is widely used for cluster
evaluation as well as evaluation of POS induction.
This metric assigns each word cluster to its most
common tag, and then measures the proportion of
correctly tagged words. The second metric is the
one-to-one mapping (1-1), a constrained version of
Many-to-one mapping in which each predicted tag
is associated with only one gold-standard tag and
vice versa (Haghighi and Klein, 2006). Word clus-
ters are assigned greedily to tags, and in the event
of there being more word classes than tags, some
word classes will be left unassigned. Another met-
ric that was used is Variation of information (VI)
(Meila, 2003), which is based the conditional en-
tropy of between the two different clusterings (John-
son, 2007). Lastly, we use the V-measure (VM) met-
ric (Rosenberg and Hirschberg, 2007), which is an-
other entropy-based measure, but defined in terms of
a F score to balance precision and recall terms (we
use equal weighting of the two factors). Please see
Christodoulopoulos et al (2010) for further details
about these metrics.3 For these metrics, a higher
score is better, with the exception of VI.
For all these metrics, the induced tags are eval-
uated against the universal pos tags, as this means
there are a consistent number of tags across the lan-
guages. Using these metrics, the results will vary as
a result of predicting a different number of tags (in
particular, more tags will mean a higher score for M-
1, and the converse is true for 1-1). However, using
the universal POS tags, we think will make results
less sensitive to large differences in POS inventory
between languages (such as for the Dutch dataset).
2.3 Dependency induction
For the Dependency induction track, the training
data consisted of the original treebank data, but
without dependency annotations. A development set
was also provided, which included the dependency
annotations, but this was meant mainly as a way to
3Thanks to Christos Christodoulopoulos for sharing his im-
plementation of the POS induction metrics, which we have used
in our evaluation.
verify systems, as we mean to minimise the amount
of supervision in the task. The participants were
later supplied with test sets for which the systems
could generate predictions. Only after the predic-
tions were submitted were the fully annotated test
sets released.
The dependency inductions were evaluated on
3 metrics: directed accuracy, undirected accuracy
and Neutral Edge Detection (NED) (Schwartz et
al., 2011). Directed accuracy is the ratio of cor-
rectly predicted dependencies (including direction)
over total amount of predicted dependencies. Undi-
rected accuracy is much the same, but also considers
a predicted dependency correct if the direction of the
dependency is reversed (e.g. if the predicted depen-
dency is not A ? B, but B ? A). Lastly, the NED
metric is a variant of undirected accuracy that also
rewards cases where an edge-flip occurs, meaning
that the predicted parent of a token is actually the
grandparent of that same token in the gold-standard
data. Note that before evaluating with these metrics
punctuation was removed from all sentences, and
any child words under a punctuation node were re-
attached to their nearest ancestor that wasn?t punc-
tuation.
The final ?joint? task consisted of inducing depen-
dency structure from only the tokens in the corpus,
without recourse to the gold POS tags. Where POS
is predicted (e.g., in a pipeline), we included these
in our general POS evaluation. The induced depen-
dency trees were evaluated with the same metrics as
in the dependency induction track, but are consid-
ered separately. We expect these systems to have
lower scores overall due to the lack of gold-standard
POS tags.
3 Treebanks
We selected a number of different treebanks for use
in the challenge, aiming to represent a wide range
of different languages, dialects and genres of text.
In total we used ten different treebanked corpora
in nine different languages. For the practical rea-
sons of simplifying the administration of the chal-
lenge and allowing the data to be reused in future re-
search, we chose corpora with licences allowing ei-
ther free redistribution, or those held by the Linguis-
66
tic Data Consortium (LDC).4 Many of these datasets
have been used before in dependency grammar or
part-of-speech research, particularly the shared tasks
at CoNLL 2006 and 2007. For the purpose of
the competition, we have updated these datasets to
include any annotation updates or additional data,
where available. It is important for unsupervised ap-
proaches to have sufficient amounts of data, espe-
cially given the common sentence length limitations
imposed by most dependency grammar models. As
described in section 2, we have included an extra
field for the universal part-of-speech (UPOS) using
Petrov et al (2011)?s automatic conversion tool.5
Below we describe the different treebanks used,
and the conversion process into our data format for
the purpose of the competition. Please see Table 1
for statistics on each of the treebanks.
Dependency treebanks We used the following
dependency treebanks: Arabic The Prague Ara-
bic Dependency Treebank V1 (Hajic? et al, 2004).6
Basque The Basque 3lb dependency treebank
(Aduriz et al, 2003). Czech The Prague Depen-
dency Treebank 2.0 (Bo?hmova? et al, 2001).7 Dan-
ish The Copenhagen Dependency Treebank ver-
sion 2 (Buch-Kromann et al, 2007). English The
CHILDES US/Brown subcorpus (Sagae et al,
2007). Slovene The jos500k Treebank (Erjavec et
al., 2010). 8 Swedish The Talbanken treebank
(Nivre et al, 2006). The conversion of each of these
treebanks was quite straightforward as they were al-
ready annotated for dependencies. Moreover, many
of these corpora had been used previously in the
CoNLL 2006 and 2007 shared tasks, and therefore
we were able to reuse this data and/or their conver-
sion scripts. In the case of Arabic and Swedish we
used the exact same data, simply converting from
CoNLL dependency format into our own format (re-
moving redundant columns and adding a UPOS col-
umn). While many of the other corpora had also
4In the following corpus descriptions, when not otherwise
specified the corpus is freely available for research purposes.
5http://code.google.com/p/
universal-pos-tags
6LDC catalogue number LDC2004T23.
7LDC catalogue number LDC2006T01.
8For the shared task, the annotation was converted to english
using the tables found at the JOS website: http://nl.ijs.
si/jos/msd/html-en/index.html
been used previously, our data is different, making
use of subsequent corrections to these treebanks and
additional annotated data now available.
First language acquisition provides an important
motivation for grammar induction research, conse-
quently we have included data from the CHILDES
database of child-directed speech. We use the
Brown sub-corpus, a longitudinal study of parent-
child interactions for three children aged between 18
months and 5 years old. The corpus has been man-
ually annotated with syntactic dependencies (Sagae
et al, 2007) and morphology. From this we take all
child-directed utterances, extracting word, morphol-
ogy, part-of-speech and dependency markup, and
developed our own conversion into UPOS. Our test-
ing and development sets were drawn from the first
15 Eve files which were manually annotated for de-
pendency structure. The rest of the corpus, which
had not been manually annotated for syntax, was
merged to form the training set.
Phrase-structure treebanks As well as depen-
dency treebanks, we used three different phrase-
structure treebanks: The Dutch Alpino treebank
(Bouma et al, 2000), the English Penn Treebank
V3 (Marcus et al, 1993),9 and the Portuguese Flo-
resta Sinta?(c)tica treebank (Afonso et al, 2002). As
these treebanks do not explicitly mark dependen-
cies, we automatically extracted these using head
finding heuristics. Thankfully the difficult work
of creating such scripts has already been done as
part of the CoNLL shared tasks. We have reused
their scripts to create dependency representations of
these treebanks, before converting into our file for-
mat and augmenting with UPOS annotation. In the
case of Dutch, we have reused the same CoNLL
2006 data; note that this dataset includes predicted
part-of-speech rather than gold standard annotation
(Buchholz and Marsi, 2006). For the Portuguese,
we used the same Bosque 7.3 sub-corpus10 from
CoNLL 2006, additionally including in our training
set the recently-annotated Selva 1.0 subcorpus.
The Penn Treebank is the most common data set
in parsing and grammar induction. We have patched
9LDC catalogue number LDC99T42.
10An updated version of this corpus is available, however
the file format had changed significantly and we were unable
to adapt the conversion scripts in time for the competition.
67
ar cs da en-childes en-ptb eu nl pt sl sv
annotation d d d d p d p p d d
Training data
Tokens 106.6k 1.2M 68.5k 312.8k 1.1M 124.7k 192.2k 196.4k 193k 184.6k
Sentences 2.8k 68.5k 3.6k 57.4k 45.4k 9.1k 13k 8.7k 9.4k 10.7k
Tokens/sent 38.4 17.1 18.8 5.5 23.9 13.7 14.8 22.6 20.5 17.3
CPOSTAG 15 12 25 31 31 16 13 16 13 41
POSTAG 21 61 141 76 45 50 300 22 31 41
FEATS 22 75 338 29 0 269 310 146 46 0
Development data
Tokens 5.1k 159k 17k 25.3k 32.9k 12.6k 2.9k 10.3k 20.2k 6.9k
Sentences 139 9.3k 1k 5k 1.3k 1k 386 400 1k 389
Tokens/sent 36.8 17.1 17 5.1 24.4 12.5 7.4 25.8 20.2 17.6
% New words 27.5 26 49.8 9.8 11.4 46.1 18.8 27.5 38.7 13.8
Test data
Tokens 5.1k 173.6k 14.7k 28.4k 56.7k 14.3k 5.6k 5.9k 22.6k 5.7k
Sentences 131 10.1k 1k 5.2k 2.4k 1.1k 386 288 1k 389
Tokens/sent 39.1 17.1 14.7 5.4 23.5 12.7 14.5 20.4 22.6 14.5
% New words 24.3 25.3 43.7 9 12.1 51.5 40.5 25.2 37.1 34.6
Table 1: Properties of the treebanks. We report the linguistic annotation method (dependency vs. phrase-structure),
the size of each treebank, the number of types for the different granularities of part-of-speech tags and morphological
features (note that UPOS has a fixed set of 12 tags), and the proportion of word types that were not present in training.
the treebank to include NP-internal structure using
Vadas and Curran?s annotations (Vadas and Cur-
ran, 2007), which was then converted to dependency
structures using the penn-converter11 script
(Johansson and Nugues, 2007). This tool has a num-
ber of options controlling the linguistic decisions
in converting from phrase-structure to dependency
trees, e.g., the treatment of coordination. We ex-
tracted five versions of the treebank, each encoding
each different sets of linguistic assumptions (Tsar-
faty et al, 2011).12 These are denoted default, old-
LTH, CoNLL-2007, functional and lexical; for the
main results we used the standard options, we also
report separately evaluations using each of the five
variants. The treebank was partitioned into training
(sections 0-22), development (sec. 24) and testing
sets (sec. 23).
4 Baselines and Benchmarks
A number of standard baselines and previously pub-
lished benchmark systems were implemented for
each task in order to place the submitted systems in
context.
11http://nlp.cs.lth.se/software/treebank_
converter
12Note that Tsarfaty et al (2011) also propose an evalua-
tion metric for comparing dependency trees, which we have not
used. Note however that it could, in principle, be used for simi-
lar evaluations.
The standard baseline for grammar induction
models is to assume either left branching or right
branching analyses (LB, RB). These capture the ten-
dency for languages to favour one attachment direc-
tion over another. The most frequently cited and
extended model for dependency induction is DMV
(Klein and Manning, 2004). We provide results for
this model trained on each of the coarse (DMVc), fine
(DMVp), and universal (DMVu) POS tag sets, all ini-
tialised with the original harmonic initialiser. As a
further baseline we also evaluated the dependency
trees resulting from directly using the harmonic ini-
tialiser without any training (H).
As a strong benchmark we include the results of
the non-parametric Bayesian model previously pub-
lished in Blunsom and Cohn (2010) (BC). The stated
results are for the unlexicalised model described in
that paper where the final analysis is formed by
choosing the maximum marginal probability depen-
dency links estimated from forty independent Gibbs
sampler runs.
For part-of-speech tagging we include results
from an implementation of the Brown word clus-
tering algorithm (Brown et al, 1992) (Bc,p,u), and
the mkcls tool written by Franz Och (Och, 1999)
(MKc,p,u). Both of these benchmarks were trained
with the number of classes matching the number
in the gold standard of each of the tagsets in turn:
coarse (c), fine (p), and universal (u). A notable
68
property of both of these word class models is that
they enforce a one-tag-per-type restriction that en-
sures there is a one-to-one mapping between word
types and classes.
For POS tagging we also provide benchmark re-
sults from two previously published models. The
first of these is the Pitman-Yor HMM model de-
scribed in (Blunsom and Cohn, 2011), which in-
corporates ta one-tag-per-type restriction (BC). This
model was trained with the same number of tags as
in the gold standard fine tag set for each corpus. The
second benchmark is the HMM with Sparsity Con-
straints trained using Posterior Regularization (PR)
described in (Grac?a et al, 2011). In this model
the HMM emission probabilitiy distribution are esti-
mated using small Maximum Entropy models (fea-
tures set described in the original paper). The mod-
els were trained for 200 iterations of PR using both
the same number of hidden states as the coarse Gc
and universal Gu gold standard. All parameters were
set to the values described in the original paper.
5 Submissions
The shared task received submissions covering a di-
verse range of approaches to the dependency and
part-of-speech induction challenges. Encouragingly
all of these submissions made significant departures
from the benchmark HMM and DMV approaches
which have dominated the published literature on
these tasks in recent years. The submissions were
characterised by varied choices of model structure,
parameterisation, regularisation, and the degree to
which light supervision was provided through con-
straints or the use of labelled tuning data. In the fol-
lowing sections we summarise the approaches taken
by the systems submitted for each task.
5.1 Part-of-Speech Induction
The part-of-speech induction challenge received two
submission, (Chrupa?a, 2012; Christodoulopoulos et
al., 2012). Both of these submissions based their in-
duction systems on LDA inspired models for cluster-
ing word types by the contexts in which they appear.
Notably, the strongest of the provided benchmarks
and the two submissions modelled part-of-speech
tags at the type level, thus restricting all tokens of
a given word type to share the same tag. Though
clearly out of step with the gold standard tagging,
this one-tag-per-type restriction has previously been
shown to be a crude but effective way of regularising
models towards a good solution. Below we sum-
marise the approach of each submission, identified
by the surname of the first author on the submitted
system description.
Chrupa?a (2012) employed a two stage approach
to inducing part-of-speech tags. The first stage used
an LDA style probabilistic model to induce a dis-
tribution over possible tags for a given word type.
These distributions were then hierarchically clus-
tered and the final tags selected using the prefix of
the path from the root node to the word type in the
cluster tree. The length of the prefixes, and thus the
number of tags, was tuned on the labelled develop-
ment data.
The system of Christodoulopoulos et al (2012)
was based upon an LDA type model which included
both contexts and other conditionally independent
features (Christodoulopoulos et al, 2011). This base
system was then iterated with a DMV system and
with the resultant dependencies being repeatedly fed
back into the POS model as features. This submis-
sion is notable for being one of the first to attempt
joint POS and dependency induction rather than tak-
ing a pipeline approach.
5.2 Dependency Induction
The dependency parsing task saw a variety of ap-
proaches with only a couple based on the previously
dominant DMV system. Two forms of light super-
vision were popular, the first being the inclusion of
pre-specified constraints or rules for allowable de-
pendency links, and the second being the tuning of
model parameters or selecting between competing
models on the labelled development data. Obviously
the merits of such supervision would depend on the
desired application for the induced parser. The di-
rect comparison of models which include a form of
universal prior syntactic information with those that
don?t does permit interesting development linguistic
questions to be explored in future.
Bisk and Hockenmaier (2012) chose to induce a
restricted form of Combinatory Categorial Grammar
(CCG), the parses of which were then mapped to
dependency structures. Restrictions on head-child
dependencies were encoded in the allowable cate-
69
gories for each POS tag and the heads of sentences.
Key features of their approach were a maximum
likelihood objective function and an iterative proce-
dure for generating composite categories from sim-
ple ones. Such composite categories allow the pa-
rameterisation of larger units than just head-child
dependencies, improving over the more limited con-
ditioning of DMV.
Marac?ek and Z?abokrtsky? (2012) introduced a
number of novel features in their dependency induc-
tion submission. Wikipedia articles were used to
quantify the reducibility of word types, the degree
to which the word could be removed from a sen-
tence and grammaticality maintained. This metric
was then used, along with a model of child fertil-
ity and dependency distance, within a probabilistic
model. Inference was performed by using a local
Gibbs sampler to approximate the marginal distribu-
tion over head-child links.
S?gaard (2012) presented two model-free heuris-
tic algorithms. The first was based on heuristically
adding dependency edges based on rules such as ad-
jacency, function words, and morphology. The re-
sulting structure is then run through a PageRank al-
gorithm and another heuristic is used to select a tree
from the resulting ranked dependency edges. The
second approach takes the universal rules of Naseem
et al (2010) but rather than estimating a probabilis-
tic model with these rules, a rule based heuristic is
used to select a parse rather. This second model-free
approach in particular provides a strong baseline for
probabilistic models built upon hand-specified de-
pendency rules.
Tu (2012) described a system based on an ex-
tended DMV model. Their work focussed on the
exploration of multiple forms of regularisation, in-
cluding Dirichlet priors and posterior regularisation,
to favour both sparse conditional distributions and
low ambiguity in the induced parse charts. While
many previous works have included sparse priors
on the conditional head-child distributions the ad-
ditional regularisation of the ambiguity over parse
trees is a novel and interesting addition. The la-
belled development sets were employed to both se-
lect between models employing different regularisa-
tion, and to tune model parameters.
5.3 POS and Dependency Induction
There was only a single submission for the task of
inducing dependencies without gold standard part-
of-speech tags supplied. Christodoulopoulos et al
(2012) submitted the same joint tagging and DMV
system used for the POS induction task to the depen-
dency induction task. Results on the development
data indicated that this iterated joint training had a
significant benefit for the induced tags and a smaller
benefit for the dependency structures induced.
6 Results
The main results for the three tasks are shown in Ta-
bles 2, 3, and 4, for the POS induction, dependency
induction and joint tasks, respectively.13 We now
present a detailed analysis of each of the three tasks.
6.1 POS induction
The main evaluation results for the POS induc-
tion task are shown in Table 2, which compares
the induced clusters against the gold universal tags
(UPOS).14 Given the diversity of scenarions used by
each system (e.g. number of hidden states, tuning
on development data) a direct comparison between
the systems can only be illustrative. A first obser-
vation is that depending on the particular evaluation
metric employed the ranking of the systems changes
substantially, for instance the Gu system is the best
using the 1-1 and VI metric but is the worst of the en-
tries (excepting the baselines) when using the other
two metrics. Focusing on the VM metric, which
was shown empirically not to have low bias with re-
spect to the word classes (Christodoulopoulos et al,
2010), the best entry is the BC system which has the
best performance in 9 out of 10 entries followed by
the CGS and the C system. Note that this ranking
holds also for the comparison against fine POS tags,
shown in Table 7.
An interesting aspect is that almost all systems
beat the strong Brown (B) and mkcls (MK) base-
line across the different metrics when we restrict
our attention to the cases where the same number
13Additional tables of results are in the appendix, and fur-
ther results are online at http://wiki.cs.ox.ac.uk/
InducingLinguisticStructure.
14See also Table 7 for the comparison against the fine POS
tags; we base our analysis on UPOS instead as this tag set has a
fixed size irrespective of the treebank.
70
M-1
testset BC CGS Cc Cp Gc Gu Bc Bp Bu MKc MKp MKu
arabic 83.80 N/A 83.33 83.33 65.05 66.61 66.20 69.89 66.22 71.08 72.72 68.03
basque 80.85 79.54 86.67 86.67 77.37 73.88 74.73 77.49 73.32 74.80 78.63 71.40
czech 83.10 66.78 72.27 77.97 N/A N/A 60.85 75.57 60.42 65.43 79.35 57.16
danish 81.44 77.76 84.13 84.92 68.16 53.78 72.12 79.77 47.09 72.26 82.59 53.07
dutch 80.75 70.13 74.04 76.11 63.37 57.64 57.99 84.17 57.31 68.18 84.78 63.04
en-childes 90.36 85.42 91.50 91.50 N/A N/A 82.65 89.70 70.12 86.27 91.44 75.63
en-ptb 86.73 81.93 78.11 84.35 77.14 71.10 77.29 80.88 63.74 79.99 83.88 63.34
portuguese 81.69 77.38 80.38 81.90 75.54 74.35 70.07 74.25 67.60 70.79 72.90 68.08
slovene 70.81 65.31 75.53 75.92 67.94 59.96 61.58 68.93 58.32 58.43 65.69 50.36
swedish 78.61 80.45 79.60 79.60 69.91 58.79 71.69 71.69 57.55 76.45 76.45 57.30
averages 81.82 76.08 80.56 82.23 70.56 64.51 69.52 77.23 62.17 72.37 78.84 62.74
1-1
testset BC CGS Cc Cp Gc Gu Bc Bp Bu MKc MKp MKu
arabic 53.67 N/A 39.44 39.44 39.83 55.52 40.55 33.57 43.31 51.54 40.24 51.58
basque 36.10 36.03 47.15 47.15 47.09 54.70 32.61 20.53 40.62 34.80 27.28 37.65
czech 31.82 49.30 30.49 27.20 N/A N/A 46.19 26.66 45.10 43.70 24.48 39.25
danish 42.54 42.77 31.67 31.04 39.95 45.58 36.04 17.74 39.19 43.89 22.18 44.23
dutch 42.79 56.15 43.10 39.62 56.45 45.37 48.18 21.36 43.12 55.99 21.32 54.09
en-childes 38.79 42.57 43.76 43.76 N/A N/A 40.78 35.54 57.71 43.45 32.00 59.18
en-ptb 41.55 39.57 43.86 31.56 42.07 51.70 39.79 33.90 46.50 40.55 36.22 51.17
portuguese 59.66 47.45 35.90 35.50 46.50 56.08 51.15 42.68 51.58 44.28 35.38 46.31
slovene 39.02 53.04 33.18 32.50 50.90 48.50 46.83 40.16 42.28 40.34 39.32 40.58
swedish 42.38 32.44 26.45 26.45 34.99 54.92 27.56 27.56 51.34 35.82 35.82 43.60
averages 42.83 44.37 37.50 35.42 44.72 51.55 40.97 29.97 46.07 43.44 31.42 46.76
VM
testset BC CGS Cc Cp Gc Gu Bc Bp Bu MKc MKp MKu
arabic 61.75 N/A 51.27 51.27 44.81 47.07 39.93 42.43 39.92 47.47 43.91 44.49
basque 42.17 41.52 43.04 43.04 40.86 40.05 34.85 33.33 36.08 36.32 34.35 33.42
czech 52.26 45.31 40.22 39.20 N/A N/A 38.56 42.90 37.46 41.70 46.03 37.34
danish 56.57 54.63 52.46 52.32 47.26 41.96 47.89 44.37 35.13 50.52 48.17 39.96
dutch 56.96 53.35 54.87 52.90 48.57 45.80 43.34 49.33 43.67 51.37 50.11 47.20
en-childes 64.53 62.32 62.76 62.76 N/A N/A 58.87 60.31 57.06 62.76 60.92 60.51
en-ptb 60.73 57.99 53.14 52.09 55.10 52.54 54.76 55.08 48.04 56.81 57.29 48.46
portuguese 64.17 58.41 52.54 52.32 55.96 58.14 52.09 53.18 50.32 52.48 50.87 50.18
slovene 51.15 51.29 46.60 46.50 50.98 45.98 44.49 45.80 38.61 36.79 43.43 36.43
swedish 57.05 54.21 47.08 47.08 48.89 45.73 45.87 45.87 40.84 49.77 49.77 42.83
averages 56.73 53.23 50.40 49.95 49.05 47.16 46.06 47.26 42.71 48.60 48.48 44.08
VI
testset BC CGS Cc Cp Gc Gu Bc Bp Bu MKc MKp MKu
arabic 2.48 N/A 3.70 3.70 3.39 2.98 3.78 3.94 3.53 3.31 3.82 3.30
basque 3.82 3.44 3.98 3.98 3.25 2.82 3.92 4.98 3.45 3.79 4.76 3.58
czech 3.83 3.41 4.92 5.77 N/A N/A 3.70 4.76 3.69 3.63 4.53 3.83
danish 3.36 3.34 4.31 4.38 3.78 3.46 3.86 5.43 3.79 3.64 4.90 3.59
dutch 3.56 3.13 3.28 3.71 3.30 3.44 3.66 5.22 3.60 3.26 5.15 3.39
en-childes 2.81 2.86 3.06 3.06 N/A N/A 3.13 3.34 2.59 2.84 3.33 2.50
en-ptb 3.18 3.28 3.67 4.36 3.34 3.03 3.46 3.62 3.36 3.36 3.52 3.28
portuguese 2.47 2.83 3.96 4.09 2.96 2.62 3.19 3.36 3.10 3.21 3.52 3.15
slovene 3.62 3.14 4.80 4.86 3.16 3.30 3.61 4.09 3.73 4.15 4.33 3.99
swedish 3.31 3.68 4.98 4.98 3.90 3.32 4.46 4.46 3.70 4.07 4.07 3.62
averages 3.24 3.23 4.07 4.29 3.39 3.12 3.68 4.32 3.45 3.53 4.19 3.42
Table 2: Results for the POS induction task, showing one-to-one, many-to-one, VM and VI scores, measured against
the gold UPOS tags. Each system is shown in a column, where the title is an acronym of the authors? last names, or
else the name of a benchmark system (B is the Brown clusterer and MK is mkcls). The superscripts c, p and u denote
different applications of the same method with a number of word classes set to equal the true number of coarse tags,
full tags or universal tags, respectively, for each treebank.
71
of hidden states are used (the exception being the G
system which occasionally under-performed against
MK). Interestingly the assumption of one-tag-per-
word, made by all but the G system, works very
well in practice leading to consistently strong re-
sults. This suggests that dealing with word ambigu-
ity is still an unresolved issue in unsupervised POS
induction.
Comparing the performance of the systems for
different languages, as expected the languages for
which we have a larger corpora (English CHILDES
and PTB and Czech) tend to result in systems with
better accuracies. An interesting future question is
how do the propose methods scale when training on
really large corpora (e.g., wikipedia) both in terms
of performance (accuracy) but also in the resources
they required.
Finally, the wild divergences in the system rank-
ings when considering the different evaluation met-
rics calls for some sort of external evaluation using
the induced clusters as features to other end sys-
tems, for instance semi-supervised tagging. The
main question is if there will be a definitive ranking
between systems for a diverse set of tasks, or if on
the contrary the effectiveness of the output of each
system will vary according to the task at hand.
6.2 Dependency induction
The main evaluation results for the dependency task
are shown in Table 3. From this we make several
observations.15 Firstly, for almost all the corpora
the participants systems have outperformed the sim-
ple baselines, and by a significant margin. There
are three exceptions to this: for Arabic, Basque and
Danish the left or right-branching baselines outper-
forms most or all of the competitors. This may in-
dicate that these languages are inherently difficult,
or may simply be a consequence of these three lan-
guages having the least data of all of our corpora.
Basque and Dutch proved to be the hardest of the
treebanks, with the lowest overall scores, and the
CHILDES (English) and Portuguese were the eas-
iest. The reasons for this are not immediately clear,
15Table 3 evaluates against the full test sets, however it is
traditional to present results for short sentences mirroring the
common training setup. See Tables 8 and 9 for results over
sentences with 10 words or fewer, excluding punctuation. Note
that our analysis is based on the results for the full test set.
although we speculate that Basque is difficult due to
its dissimilarity from other European languages, and
therefore may not match the assumptions underly-
ing models developed primarily on English. Dutch
is difficult as its annotation was non-projective, and
it has a very large set of POS tags, while CHILDES
is made easier due to its extremely short and simple
sentences.
In terms of declaring a ?winner?, it is clear that
Tu?s system ranks best under directed accuracy and
NED, and a very narrow second (to the organisers?
submission, BC) for undirected accuracy. Moreover
Tu?s system was a consistent performed across all
corpora, with no single result well below the results
of the other participants. Note that the three different
metrics often predict the same winner across the dif-
ferent treebanks, however there are some large dis-
crepancies, such as Portuguese and Dutch where the
directed and undirected accuracy metrics concur, but
NED produces a very different ranking. It is unclear
which metric should be trusted more than another;
this could only be assessed by correlating these met-
rics with some form of secondary evaluation, such
as in a task based setting or obtaining human gram-
maticality judgements.16
The benchmark systems include DMV (Klein and
Manning, 2004), which has historical importance
in terms of being the first research systems to out-
perform simple baselines for dependency induction,
and also the model upon which most recent depen-
dency induction research is based, including many
of the competitors in the competition. We ob-
serve that in most cases the competitors have out-
performed the DMV models, in many cases by a
large margin. In all cases DMV improved over
its initialisation condition (the harmonic initialiser),
although often this improvement was only slight,
underscoring the importance of good initialisation.
The effect of inducing DMV grammars from var-
ious different granularity of POS tags made little
difference in most cases, although for Dutch17 and
the English PTB there change was more dramatic.
16It was our intention to include a task-based evaluation for
machine translation, but this proved impractical for the compe-
tition due to the volumes of data that we would require each
participant to process.
17Note that for Dutch the full POS tags were not gold stan-
dard, but were system predictions.
72
Directed
testset BC BH MZc MZp MZu S1 S2 Tu DMVc DMVp DMVu H LB RB
arabic 61.1 47.2 15.7 64.8 64.8 47.2 54.6 66.7 46.3 45.4 50.0 42.6 9.3 64.8
basque 56.3 50.3 28.0 30.3 27.2 33.3 22.3 58.6 46.3 43.2 31.3 21.8 34.3 24.4
czech 50.0 48.5 61.3 57.5 57.7 45.5 51.2 59.0 30.1 31.2 31.8 24.7 28.9 34.3
danish 46.2 49.3 60.2 51.3 61.4 56.9 60.5 60.8 47.2 50.2 35.3 36.4 18.7 49.2
dutch 50.5 50.8 37.0 49.5 38.4 38.9 50.0 51.7 48.7 39.7 49.1 35.1 34.0 39.5
en-childes 48.1 62.2 56.8 47.2 51.8 50.5 53.5 56.0 51.7 51.9 39.0 31.7 36.0 23.3
en-ptb 72.1 73.7 58.9 67.4 52.2 44.8 61.0 74.7 31.7 44.7 30.6 35.2 40.4 19.9
portuguese 54.3 76.3 63.6 59.9 44.3 47.7 71.1 55.7 27.1 37.2 26.9 31.1 28.1 37.7
slovene 65.8 53.9 42.1 51.4 39.2 39.7 50.3 67.7 35.7 37.2 35.6 25.7 35.9 14.7
swedish 65.8 66.7 61.4 63.7 70.8 48.2 72.0 76.5 44.2 44.2 45.8 39.1 33.2 31.3
averages 57.0 57.9 48.5 54.3 50.8 45.3 54.7 62.7 40.9 42.5 37.5 32.3 29.9 33.9
Undirected
testset BC BH MZc MZp MZu S1 S2 Tu DMVc DMVp DMVu H LB RB
arabic 57.3 29.7 57.6 62.0 58.7 48.0 58.4 59.3 41.8 42.0 43.7 41.2 61.7 63.9
basque 58.0 47.2 43.3 45.0 43.2 47.5 24.3 53.3 48.1 47.7 40.3 37.6 53.9 53.1
czech 59.0 45.0 57.8 54.3 55.5 49.3 55.8 61.4 46.2 46.7 45.3 38.5 51.5 52.3
danish 60.8 50.7 60.7 56.1 60.3 56.6 60.5 61.6 55.1 54.1 51.6 46.0 58.7 59.9
dutch 61.0 45.0 47.5 51.5 48.9 46.8 51.4 54.6 52.2 45.0 52.2 37.2 50.1 50.8
en-childes 63.5 68.4 67.2 59.9 61.4 62.0 62.4 66.9 63.8 64.0 57.5 49.0 50.0 49.9
en-ptb 66.2 58.1 49.7 57.6 48.2 49.5 58.8 62.1 43.1 53.1 43.0 36.2 51.7 51.5
portuguese 56.6 72.4 61.4 61.9 49.8 52.6 66.9 61.4 44.3 48.1 43.6 41.2 55.7 56.8
slovene 58.1 47.9 45.2 49.1 44.5 42.4 53.5 61.8 42.1 40.6 42.1 32.5 40.8 41.1
swedish 70.0 58.5 58.8 59.3 60.4 53.5 65.2 66.9 51.1 51.1 53.3 44.5 53.0 53.2
averages 61.0 52.3 54.9 55.7 53.1 50.8 55.7 60.9 48.8 49.2 47.3 40.4 52.7 53.2
NED
testset BC BH MZc MZp MZu S1 S2 Tu DMVc DMVp DMVu H LB RB
arabic 63.6 37.3 59.2 67.1 63.2 56.5 65.8 64.1 48.9 48.0 48.8 47.5 62.7 69.0
basque 69.6 55.8 51.5 55.6 53.4 58.8 38.0 65.8 57.6 57.1 51.5 49.3 67.2 59.1
czech 71.0 55.7 70.2 65.2 67.3 63.2 69.7 71.6 53.2 52.9 54.1 47.6 56.3 68.6
danish 72.0 63.1 72.9 69.5 73.5 65.9 71.8 76.4 64.8 63.5 58.9 53.5 61.6 71.5
dutch 71.6 58.6 68.6 72.0 69.7 60.6 63.8 66.9 63.5 54.5 63.5 46.9 55.1 67.0
en-childes 80.9 79.6 82.8 74.1 72.7 77.1 83.2 80.4 78.1 78.3 77.5 67.2 61.0 75.2
en-ptb 75.2 69.8 69.4 73.8 67.2 64.1 71.6 69.8 49.8 67.0 49.6 44.8 53.9 68.1
portuguese 67.5 79.8 75.6 75.7 71.7 66.9 78.2 80.4 62.1 66.6 61.3 51.8 57.3 75.4
slovene 64.4 60.7 56.9 58.9 57.1 55.9 66.7 68.7 49.2 47.3 49.2 38.9 43.8 56.6
swedish 80.1 70.9 73.2 73.8 72.7 66.7 77.0 77.1 64.0 64.0 62.0 56.0 56.5 71.0
averages 71.6 63.1 68.0 68.6 66.9 63.6 68.6 72.1 59.1 59.9 57.6 50.3 57.6 68.1
Table 3: Directed accuracy, undirected accuracy and NED results for the dependency task (using supplied POS). The
first column (BC) is our benchmark system, the next seven are participants systems, and the remaining columns consist
of the DMV benchmark and various simple baselines. The superscripts c, p and u denote which type of POS was used,
and S1 and S2 denote two different submissions for S?gaard (2012).
Overall the full POS tagset lead to the best perfor-
mance over the coarse and universal tags (consider-
ing undirected accuracy or NED), which is to be ex-
pected as there is considerably more syntactic infor-
mation contained in the full POS. This must be bal-
anced against the additional model complexity from
expanding its parameter space, which may explain
why the difference in performance differences are so
small. The same pattern can also be seen in Marac?ek
and Z?abokrtsky? (2012)?s submission, whose system
using full POS (Mp) outperformed their other vari-
ants.
6.3 Joint task
As we had only one submission for the joint prob-
lem of POS and dependency induction, there are
few conclusions we can draw for this joint task (see
Table 4 for the results, and Table 9 for the short
sentence evaluation). Compared to the dependency
induction task using gold standard POS, as shown
in Table 3, the accuracy for the joint models are
lower. Interestingly, the DMV model performs best
when using the same number of word clusters as
there are POS tags, mirroring the findings reported
73
directed
testset CGS DMVc DMVp DMVu
arabic N/A 35.3 44.4 34.2
basque 24.5 27.5 25.1 28.7
czech 24.7 19.9 33.2 20.0
danish 21.4 23.3 31.9 10.0
dutch 15.1 20.6 33.7 20.5
en-childes 29.9 38.6 42.2 40.3
en-ptb 21.5 22.5 23.3 17.2
portuguese 19.7 28.5 28.0 17.1
slovene 19.2 13.9 11.5 14.4
swedish 23.6 26.4 26.4 20.5
averages 22.2 25.7 30.0 22.3
undirected
testset CGS DMVc DMVp DMVu
arabic N/A 45.5 52.5 45.0
basque 43.5 46.4 47.3 47.0
czech 38.9 37.5 50.9 38.5
danish 51.4 52.2 48.8 37.3
dutch 40.3 41.9 48.6 40.8
en-childes 54.9 59.2 60.8 58.1
en-ptb 43.4 45.4 48.8 39.4
portuguese 45.5 51.8 52.7 39.8
slovene 32.8 33.3 36.7 32.8
swedish 45.6 48.9 48.9 40.3
averages 44.0 46.2 49.6 41.9
NED
testset CGS DMVc DMVp DMVu
arabic N/A 53.4 57.6 53.3
basque 55.9 55.6 54.4 54.7
czech 51.2 49.3 63.4 51.5
danish 61.7 60.3 60.4 46.3
dutch 47.2 57.5 56.8 55.2
en-childes 78.2 77.7 78.1 76.5
en-ptb 53.9 60.2 63.5 47.5
portuguese 50.0 69.4 70.8 57.9
slovene 40.7 38.7 47.5 40.3
swedish 54.5 65.4 65.4 54.3
averages 54.8 58.8 61.8 53.8
Table 4: Directed, undirected and NED accuracy results
for evaluating the predicted dependency structures in the
joint task (i.e., not using supplied POS tags). The first
column is the participant?s system and the next three are
DMV models trained on the Brown word clusters (see
section 6.1).
above with gold standard tags. The best joint sys-
tem was the DMVp model, which only marginally
under-performed the equivalent DMV model trained
on gold POS. This is an encouraging finding, sug-
gesting that word clusters are able to represent im-
portant POS distinctions to inform deeper syntactic
processing.
6.4 Analysis
Until now we have adopted the standard metrics in
dependency evaluation: namely directed head at-
tachment accuracy, and its more lenient counter-
parts, undirected accuracy and NED. The latter met-
rics reward structures that almost match the gold
standard tree, by way of rewarding child-parent
edges that are predicted in the reverse direction, i.e.,
attaching the child as the parent (NED takes this fur-
ther, by also rewarding the grandparent-child edge
when this occurs). This allows some degree of flexi-
bility when considering various contentious linguis-
tic decisions such as whether a preposition should
head a preposition phrase, or the head of the child
noun-phrase. This added leniency comes at a price,
as shown in Table 3 where the undirected accuracy
and NED results are considerably higher than di-
rected accuracy, and display less spread of values
(look in particular at the random trees, Ra). Is is
unclear that the predicted trees are truly predicting
linguistically plausible structures, but instead that
the differences are due largely to chance. Moreover,
systems that predict linguistic phenomena inconsis-
tently between sentences or across types of related
phenomena are rewarded under these lenient met-
rics.
For these reasons we also consider a different,
less permissive, evaluation method, using multiple
references of the treebank where each is annotated
with different styles of dependency. As described
in section 2, we processed the Penn treebank five
times with different options to the LTH conversion
tool. This affected the treatment of coordination,
preposition phrases, subordinate clauses, infinitival
clauses etc. Next we compare the directed accu-
racy of the systems against these five different ?gold
standard? references, which are displayed in Table 5,
alongside the maximum score for each system. Note
that most systems performed well against the stan-
dard, conll2007 and functional references but poorly
against the lexical and oldLTH references.18 Con-
sidering the latter two references, a different system
would be selected as the highest performing, namely
Bisk and Hockenmaier (2012) (BH) over Blunsom
and Cohn (2010) (BC) which wins in the other cases.
18The common difference here is that the latter two refer-
ences do not treat prepositions as heads of PPs.
74
This evaluation method rewards many different lin-
guistically plausible structures, but in such a way
that the predictions must be consistent between dif-
ferent sentences in the testing set, and in their treat-
ment of related linguistic phenomena. One caveat
is that this method can only be used when there
are many references, although in many cases differ-
ent outputs can be generated automatically, e.g., by
adjusting head-finding heuristics in converting be-
tween phrase-structure to dependency trees.
The previous analysis has rated each system in
terms of overall performance against treebank trees,
however this doesn?t necessarily mean that the pre-
dictions of the best ranked system will be the most
useful ones in a task-based setting. Take the ex-
ample of information extraction, in which a central
problem is to identify the arguments (subject, object
etc) of a given verb. This setting gives rise to some
types of dependency edges being more valuable than
others. We present comparative results for the Penn
treebank in Table 6 showing the directed accuracy
for different types of dependency relations. Observe
that there is a wide spread of accuracies for predict-
ing the head word of the sentence (ROOT), and simi-
larly for verbs? subject and object arguments. These
scores are similar to the scores for the local modi-
fiers shown, such as NMOD which describe the ar-
guments of a noun. This is surprising as noun edges
tend to be much shorter than for the arguments to a
verb, and thus should be easier to predict. Also in-
teresting are the spread of results for the CC edges
(these link a coordinating conjunction to its head),
suggesting that the systems learn to represent coor-
dination in very different ways to the method used
in the reference.
Figure 1 illustrates the directed accuracy over dif-
ferent lengths of dependency edge. For all systems
the accuracy diminishes with edge length, however
some fall at a much faster rate. The two best systems
(Tu, BC) have similar overall accuracy results, but
it is clear that Tu does better on short edges while
BC does better on longer ones. The same pattern
was also observed when considering the average ac-
curacy over all treebanks (not shown), although the
systems? results were closer together.
system ROOT SBJ OBJ PRD NMOD COORD CC
Tu 71.0 64.8 53.7 49.4 56.9 36.8 11.4
LB 17.8 40.1 15.3 18.0 41.9 27.7 9.7
BC 74.9 65.7 53.0 50.2 56.8 36.3 71.4
DMVc 17.0 11.7 16.0 31.3 27.8 25.7 9.2
DMVu 17.6 9.3 16.4 25.0 27.8 25.7 8.6
BH 67.5 55.3 44.9 45.6 58.6 27.6 62.7
Mu 29.3 42.4 38.8 51.8 34.5 30.5 33.0
R 12.9 9.4 16.1 21.1 12.1 15.7 2.7
Mc 60.7 47.4 39.9 45.8 36.5 33.9 44.3
RB 17.9 12.4 26.2 36.5 15.3 25.4 1.1
H 19.4 29.3 12.2 22.2 17.3 20.9 10.3
DMVp 54.7 42.0 30.7 30.1 28.9 25.4 24.3
S2 45.2 41.9 44.2 49.8 39.7 25.4 63.8
Mp 67.8 54.3 49.6 59.4 47.7 37.7 49.7
S1 43.1 47.9 36.3 46.7 27.9 23.5 7.6
Table 6: Directed accuracy results on the Penn treebank,
stratified by dependency relation. For clarity, only 9 im-
portant relation types are shown. The vertical bars sepa-
rate different groups of relations, from left to right, relat-
ing to the main verb, general modifiers and coordination.
7 Conclusion
This challenge set out to evaluate the state-of-the-
art in part-of-speech and dependency grammar in-
duction, promoting research in this field and, im-
portantly, providing a fair means of evaluation. The
participants submissions used a wide variety of dif-
ferent approaches, many of which we shown to
have improved over competitive benchmark sys-
tems. While the results were overall very positive,
it is fair to say that the tasks of part-of-speech and
grammar induction are still very much open chal-
lenges, and that there is still considerable room for
improvement. The data submitted to this evaluation
campaign will provide a great resource for devising
new methods of evaluation, and we plan to pursue
this avenue in future work, in particular task-based
evaluation such as in an information extraction or
machine translation setting.
8 Acknowledgements
This challenge was funded by the PASCAL 2 (Pat-
tern Analysis, Statistical Modelling and Compu-
tational Intelligence) European Network of Excel-
lence. We would also like to thank the treebank
providers for allowing us to use their resources, as-
sisting us in converting these into our desired for-
mat, and helping to resolve various questions. In
particular, special thanks to Zdenek Zabokrtsky and
Jan (Czech and Arabic), Tomaz Erjavec (Slovene),
and Eckhard Bick and Diana Santos (Portuguese).
We are also indebted to the organisers of the previ-
75
testset BC BH MZc MZp MZu S1 S2 Tu DMVc DMVp DMVu H LB RB
conll2007 54.9 51.7 40.4 49.2 36.8 32.2 41.7 54.2 20.9 33.2 20.4 18.0 30.1 20.3
functional 59.6 52.4 41.5 47.4 36.2 30.6 40.0 58.5 20.9 37.2 20.6 19.3 29.2 23.7
lexical 40.6 41.9 28.5 37.3 24.8 27.7 35.5 39.5 23.5 23.1 23.0 14.4 33.1 10.1
oldLTH 41.4 43.6 28.8 37.8 24.6 28.6 36.1 39.5 22.3 23.7 21.8 14.3 32.0 10.7
standard 56.0 50.4 41.0 50.3 37.5 32.8 42.5 55.5 22.3 33.5 21.8 18.4 31.4 20.4
best 59.6 52.4 41.5 50.3 37.5 32.8 42.5 58.5 23.5 37.2 23.0 19.3 33.1 23.7
Table 5: Directed accuracy results measured against different conversions of the Penn Treebank into dependency trees.
l l
l
l
l
l l
l l
2 4 6 8
10
20
30
40
50
60
70
edge length
direc
ted a
ccura
cy (%)
l TuBCBHMZ?pS?2DMV?p
Figure 1: Directed accuracy on the Penn treebank strat-
ified by dependency length. For clarity only a subset of
the systems are shown, and edges of length 10 or more
were omitted.
ous CoNLL 2006 and 2007 competitions, who con-
tributed significant efforts into collating so many
treebanks and developing treebank conversion tools,
making our job much easier than it would other-
wise have been. Thanks to Sebastian Reidel, Joakim
Nivre and Sabine Buchholz for promptly answer-
ing our questions. We would like to thank the
LDC, who allowed their licenced data to be used
free of charge by the competitors, and Ilya Ahtaridis
who administered the licencing and corpus distribu-
tion. Thanks also to Valentin Spitkovski and Chris-
tos Christodoulopoulos who kindly provided us with
their evaluation scripts, and finally, the participants
themselves for taking part.
References
I. Aduriz, M. J. Aranzabe, J. M. Arriola, A. Atutxa,
A. Diaz de Ilarraza, A. Garmendia, and M. Oronoz.
2003. Construction of a Basque dependency treebank.
In Proceedings of the 2nd Workshop on Treebanks and
Linguistic Theories (TLT).
Susana Afonso, Eckhard Bick, Renato Haber, and Di-
ana Santos. 2002. Floresta sinta?(c)tica: a treebank
for Portuguese. In Proceedings of the Third Interna-
tional Conference on Language Resources and Evalu-
ation (LREC 2002), pages 1698?1703, May.
Yonatan Bisk and Julia Hockenmaier. 2012. Induction of
linguistic structure with combinatory categorial gram-
mars. In Proceedings of the NAACL-HLT 2012 Work-
shop on Inducing Linguistic Structure Shared Task,
June.
Phil Blunsom and Trevor Cohn. 2010. Unsupervised in-
duction of tree substitution grammars for dependency
parsing. In Proceedings of the 2010 Conference on
Empirical Methods on Natural Language Processing
(EMNLP), pages 1204?1213, Cambridge, MA, USA.
Phil Blunsom and Trevor Cohn. 2011. A hierarchical
Pitman-Yor process HMM for unsupervised part of
speech induction. In Proceedings of the 49th Annual
Meeting of the Association for Computational Linguis-
tics: Human Language Technologies, pages 865?874,
Portland, Oregon, USA, June.
Alena Bo?hmova?, Jan Hajic?, Eva Hajic?ova?, and Barbora
Hladka?. 2001. The Prague Dependency Treebank: A
Three-Level Annotation Scenario. In Anne Abeille?,
editor, Treebanks: Building and Using Syntactically
Annotated Corpora, pages 103?127. Kluwer Aca-
demic Publishers.
Gosse Bouma, Gertjan van Noord, and Robert Malouf.
2000. Alpino: Wide coverage computational analysis
of Dutch. In Proceedings of Computational Linguis-
tics in the Netherlands (CLIN 2000), pages 45?59.
Peter F. Brown, Peter V. deSouza, Robert L. Mercer, Vin-
cent J. Della Pietra, and Jenifer C. Lai. 1992. Class-
based n-gram models of natural language. Comput.
Linguist., 18:467?479, December.
Matthias Buch-Kromann, Ju?rgen Wedekind,
and Jakob Elming. 2007. The Copen-
hagen Danish-English dependency tree-
bank. http://code.google.com/p/
copenhagen-dependency-treebank.
Sabine Buchholz and Erwin Marsi. 2006. CoNLL-X
shared task on multilingual dependency parsing. In
76
Proceedings of the Tenth Conference on Computa-
tional Natural Language Learning (CoNLL-X), pages
149?164.
Christos Christodoulopoulos, Sharon Goldwater, and
Mark Steedman. 2010. Two decades of unsupervised
POS induction: how far have we come? In Proceed-
ings of the 2010 Conference on Empirical Methods
in Natural Language Processing, EMNLP ?10, pages
575?584.
Christos Christodoulopoulos, Sharon Goldwater, and
Mark Steedman. 2011. A Bayesian mixture model
for PoS induction using multiple features. In Proceed-
ings of the 2011 Conference on Empirical Methods in
Natural Language Processing, pages 638?647, Edin-
burgh, Scotland, UK., July.
Christos Christodoulopoulos, Sharon Goldwater, and
Mark Steedman. 2012. Turning the pipeline into
a loop: Iterated unsupervised dependency parsing
and pos induction. In Proceedings of the NAACL-
HLT 2012 Workshop on Inducing Linguistic Structure
Shared Task, June.
Grzegorz Chrupa?a. 2012. Hierarchical clustering
of word class distributions. In Proceedings of the
NAACL-HLT 2012 Workshop on Inducing Linguistic
Structure Shared Task, June.
Tomaz? Erjavec, Darja Fis?er, Simon Krek, and Nina
Ledinek. 2010. The JOS linguistically tagged corpus
of Slovene. In Proceedings of the Seventh Interna-
tional Conference on Language Resources and Evalu-
ation (LREC?10).
Joa?o Grac?a, Kuzman Ganchev, Lu??sa Coheur, Fernando
Pereira, and Benjamin Taskar. 2011. Controlling
complexity in part-of-speech induction. J. Artif. Intell.
Res. (JAIR), 41:527?551.
Aria Haghighi and Dan Klein. 2006. Prototype-driven
learning for sequence models. In Proceedings of
the main conference on Human Language Technology
Conference of the North American Chapter of the As-
sociation of Computational Linguistics (HLT-NAACL
?06), pages 320?327.
Jan Hajic?, Otakar Smrz?, Petr Zema?nek, Jan S?naidauf, and
Emanuel Bes?ka. 2004. Prague Arabic dependency
treebank: Development in data and tools. In Proceed-
ings of the NEMLAR International Conference on Ara-
bic Language Resources and Tools, pages 110?117.
Richard Johansson and Pierre Nugues. 2007. Extended
constituent-to-dependency conversion for English. In
Proceedings of the 16th Nordic Conference of Compu-
tational Linguistics (NODALIDA 2007).
Mark Johnson. 2007. Why doesn?t EM find good hmm
pos-taggers. In Proceedings of the 2007 Joint Confer-
ence on Empirical Methods in Natural Language Pro-
cessing and Computational Natural Language Learn-
ing, EMNLP-CoNLL ?07, pages 296?305.
Dan Klein and Christopher D. Manning. 2004. Corpus-
based induction of syntactic structure: models of de-
pendency and constituency. In ACL ?04: Proceedings
of the 42nd Annual Meeting on Association for Com-
putational Linguistics, page 478.
David Marac?ek and Zdene?k Z?abokrtsky?. 2012. Unsuper-
vised dependency parsing using reducibility and fertil-
ity features. In Proceedings of the NAACL-HLT 2012
Workshop on Inducing Linguistic Structure Shared
Task, June.
Mitchell P Marcus, Mary Ann Marcinkiewicz, and Beat-
rice Santorini. 1993. Building a large annotated cor-
pus of English: the Penn treebank. Computational
Linguistics, 19(2):313?330.
Marina Meila. 2003. Comparing Clusterings by the Vari-
ation of Information. Learning Theory and Kernel
Machines, pages 173?187.
Tahira Naseem, Harr Chen, Regina Barzilay, and Mark
Johnson. 2010. Using universal linguistic knowl-
edge to guide grammar induction. In Proceedings of
the 2010 Conference on Empirical Methods in Natural
Language Processing, pages 1234?1244, October.
Joakim Nivre, Jens Nilsson, and Johan Hall. 2006. Tal-
banken05: A Swedish treebank with phrase structure
and dependency annotation. In Proceedings of the fifth
international conference on Language Resources and
Evaluation (LREC2006).
Joakim Nivre, Johan Hall, Sandra Ku?bler, Ryan Mc-
Donald, Jens Nilsson, Sebastian Riedel, and Deniz
Yuret. 2007. The CoNLL 2007 shared task on depen-
dency parsing. In Proceedings of the CoNLL Shared
Task Session of EMNLP-CoNLL 2007, pages 915?932,
June.
Franz Josef Och. 1999. An efficient method for deter-
mining bilingual word classes. In Proceedings of the
ninth conference on European chapter of the Associa-
tion for Computational Linguistics, pages 71?76.
Slav Petrov, Dipanjan Das, and Ryan T. McDonald.
2011. A universal part-of-speech tagset. CoRR,
abs/1104.2086.
Andrew Rosenberg and Julia Hirschberg. 2007. V-
measure: A conditional entropy-based external cluster
evaluation measure. In Proceedings of the 2007 Joint
Conference on Empirical Methods in Natural Lan-
guage Processing and Computational Natural Lan-
guage Learning (EMNLP-CoNLL), pages 410?420.
K. Sagae, E. Davis, A. Lavie, B. MacWhinney, and
S. Wintner. 2007. High-accuracy annotation and pars-
ing of CHILDES transcripts. In Proceedings of the
ACL-2007 Workshop on Cognitive Aspects of Compu-
tational Language Acquisition., June.
Roy Schwartz, Omri Abend, Roi Reichart, and Ari Rap-
poport. 2011. Neutralizing linguistically problematic
77
annotations in unsupervised dependency parsing eval-
uation. In Proceedings of the 49th Annual Meeting
of the Association for Computational Linguistics: Hu-
man Language Technologies, pages 663?672.
Anders S?gaard. 2012. Two baselines for unsupervised
dependency parsing. In Proceedings of the NAACL-
HLT 2012 Workshop on Inducing Linguistic Structure
Shared Task, June.
Reut Tsarfaty, Joakim Nivre, and Evelina Andersson.
2011. Evaluating dependency parsing: Robust and
heuristics-free cross-annotation evaluation. In Pro-
ceedings of the 2011 Conference on Empirical Meth-
ods in Natural Language Processing, pages 385?396,
Edinburgh, UK, July.
Kewei Tu. 2012. Combining the sparsity and unambi-
guity biases for grammar induction. In Proceedings of
the NAACL-HLT 2012 Workshop on Inducing Linguis-
tic Structure Shared Task, June.
David Vadas and James R. Curran. 2007. Adding noun
phrase structure to the Penn Treebank. In Proceed-
ings of the 45th Annual Meeting of the Association for
Computational Linguistics (ACL-07), pages 240?247,
June.
Appendix
Directed
testset CGS DMVc DMVp DMVu
arabic 36.1 42.6 51.9 49.1
basque 28.4 28.9 27.1 30.2
czech 33.1 28.6 38.2 28.3
danish 27.9 36.4 38.4 18.2
dutch 31.0 39.0 41.1 40.3
en-childes 31.2 40.8 44.3 42.1
en-ptb 22.7 25.1 23.1 23.1
portuguese 26.7 38.4 34.5 31.1
slovene 26.3 20.6 19.2 22.6
swedish 29.0 30.9 30.9 26.5
averages 29.3 33.1 34.9 31.1
Undirected
testset CGS DMVc DMVp DMVu
arabic 58.3 52.8 58.3 61.1
basque 49.3 49.2 50.5 50.0
czech 48.7 45.9 57.2 47.9
danish 56.3 60.9 57.0 43.7
dutch 47.0 53.6 57.2 53.8
en-childes 56.3 61.0 62.7 59.9
en-ptb 50.7 52.9 54.1 46.9
portuguese 51.8 61.1 59.4 51.6
slovene 40.5 41.9 45.3 41.2
swedish 52.5 57.1 57.1 48.6
averages 51.1 53.6 55.9 50.5
NED
testset CGS DMVc DMVp DMVu
arabic 62.0 63.0 66.7 67.6
basque 67.4 62.8 62.5 62.3
czech 65.1 60.7 72.0 64.0
danish 72.0 72.4 73.2 60.3
dutch 57.7 64.9 65.0 64.7
en-childes 79.9 79.6 79.9 78.4
en-ptb 67.9 73.4 74.3 63.7
portuguese 58.2 80.7 79.5 72.6
slovene 55.7 52.3 59.4 51.9
swedish 64.8 78.4 78.4 65.7
averages 65.1 68.8 71.1 65.1
Table 9: Evaluation of the joint task on the dependency
output using a maximum sentence length of 10.
78
M-1
testset BC CGS Cc Cp Gc Gu Bc Bp Bu MKc MKp MKu
arabic 75.06 N/A 79.00 79.00 62.20 62.24 61.81 64.60 61.55 65.69 67.82 63.23
basque 71.58 69.20 75.23 75.23 65.97 56.37 64.37 68.58 62.17 63.59 68.22 60.49
czech 74.84 61.53 66.97 76.00 N/A N/A 55.60 72.51 55.01 60.38 73.38 51.96
danish 56.48 55.41 70.28 71.62 49.24 35.32 50.21 66.50 33.57 49.40 61.60 35.02
dutch 80.72 70.13 74.04 76.08 63.37 57.64 57.99 83.76 57.31 68.18 84.64 63.04
en-childes 84.23 77.57 85.35 85.35 N/A N/A 76.34 85.11 59.75 77.55 86.39 59.55
en-ptb 78.26 72.26 62.86 73.46 63.15 56.32 65.10 68.10 48.76 70.31 73.96 47.91
portuguese 76.00 72.05 75.47 77.13 68.40 65.86 65.52 69.61 61.84 64.63 66.81 62.95
slovene 67.29 59.78 72.18 72.71 63.95 55.23 54.85 63.68 52.15 54.08 59.31 45.40
swedish 66.20 67.86 73.55 73.55 60.10 48.43 61.21 61.21 47.51 64.39 64.39 46.04
averages 73.07 67.31 73.49 76.01 62.05 54.68 61.30 70.37 53.96 63.82 70.65 53.56
1-1
testset BC CGS Cc Cp Gc Gu Bc Bp Bu MKc MKp MKu
arabic 50.90 N/A 39.15 39.15 41.49 53.92 39.89 34.23 40.63 50.53 41.98 50.27
basque 42.45 46.25 52.38 52.38 48.91 45.55 41.29 33.49 50.80 43.27 35.73 43.43
czech 31.45 48.24 32.18 31.74 N/A N/A 43.12 33.55 41.94 38.93 28.33 35.31
danish 43.08 43.64 32.17 31.77 40.56 34.83 33.48 30.87 26.33 38.92 30.59 32.95
dutch 43.22 55.85 43.26 39.98 56.45 45.37 48.13 21.88 43.10 55.86 22.42 54.04
en-childes 64.10 63.62 64.50 64.50 N/A N/A 59.96 56.87 59.75 63.43 53.40 57.68
en-ptb 57.63 56.02 45.52 41.35 48.95 53.15 55.43 49.60 47.57 54.10 51.80 45.43
portuguese 59.71 50.18 36.13 35.38 54.42 60.08 49.57 45.00 48.25 46.57 38.37 45.10
slovene 42.62 50.66 33.23 32.59 56.55 50.30 44.97 44.34 40.62 41.24 40.01 38.93
swedish 48.76 40.54 34.07 34.07 38.21 46.32 36.12 36.12 44.57 41.90 41.90 38.05
averages 48.39 50.55 41.26 40.29 48.19 48.69 45.20 38.59 44.36 47.48 38.45 44.12
VM
testset BC CGS Cc Cp Gc Gu Bc Bp Bu MKc MKp MKu
arabic 61.59 N/A 52.95 52.95 46.99 47.18 40.75 43.16 40.40 47.95 45.09 45.14
basque 53.83 51.34 54.45 54.45 49.34 44.26 44.18 45.46 45.37 45.02 44.90 42.76
czech 56.80 50.22 45.06 46.76 N/A N/A 42.50 49.93 41.51 45.15 51.38 39.62
danish 61.57 59.00 63.39 63.62 53.35 43.20 51.83 58.38 33.46 52.52 58.44 39.46
dutch 57.82 53.94 55.01 53.40 48.99 46.26 44.08 50.49 44.37 52.02 51.33 47.99
en-childes 80.17 76.59 78.18 78.18 N/A N/A 73.67 76.47 65.44 76.14 76.87 68.25
en-ptb 71.44 68.12 59.90 61.31 63.90 60.04 63.79 63.64 52.96 66.40 66.50 54.33
portuguese 67.49 60.37 54.61 54.74 58.91 59.58 53.30 54.99 50.26 53.15 52.67 50.76
slovene 54.80 52.13 51.85 51.88 52.99 48.55 45.33 48.33 40.13 39.25 45.73 38.68
swedish 61.52 58.23 56.09 56.09 55.02 48.69 51.76 51.76 43.39 54.28 54.28 44.51
averages 62.70 58.88 57.15 57.34 53.69 49.72 51.12 54.26 45.73 53.19 54.72 47.15
VI
testset BC CGS Cc Cp Gc Gu Bc Bp Bu MKc MKp MKu
arabic 2.65 N/A 3.76 3.76 3.47 3.19 3.96 4.12 3.73 3.49 3.96 3.48
basque 3.65 3.50 3.78 3.78 3.45 3.36 4.09 4.79 3.67 4.00 4.72 3.83
czech 3.80 3.49 4.96 5.48 N/A N/A 3.92 4.57 3.91 3.85 4.46 4.17
danish 3.76 3.85 4.07 4.08 4.29 4.53 4.54 4.91 5.24 4.45 4.78 4.85
dutch 3.53 3.14 3.31 3.72 3.33 3.47 3.68 5.16 3.61 3.26 5.07 3.39
en-childes 1.86 2.12 2.11 2.11 N/A N/A 2.39 2.32 2.59 2.17 2.31 2.47
en-ptb 2.69 2.90 3.67 4.03 3.16 3.08 3.24 3.41 3.66 3.05 3.19 3.50
portuguese 2.40 2.90 4.01 4.11 2.97 2.74 3.35 3.46 3.35 3.40 3.63 3.37
slovene 3.65 3.40 4.65 4.68 3.34 3.48 3.92 4.23 4.03 4.38 4.51 4.25
swedish 3.36 3.78 4.57 4.57 3.89 3.65 4.45 4.45 4.11 4.17 4.17 4.07
averages 3.13 3.23 3.89 4.03 3.49 3.44 3.75 4.14 3.79 3.62 4.08 3.74
Table 7: One to one, Many to one, VM and VI scores of POS induction results evaluated against fine POS tags (c.f.,
Table 2 which used UPOS).
79
Directed
testset BC BH MZc MZp MZu S1 S2 Tu DMVc DMVp DMVu H LB RB
arabic 61.1 47.2 15.7 64.8 64.8 47.2 54.6 66.7 46.3 45.4 50.0 42.6 9.3 64.8
basque 56.3 50.3 28.0 30.3 27.2 33.3 22.3 58.6 46.3 43.2 31.3 21.8 34.3 24.4
czech 50.0 48.5 61.3 57.5 57.7 45.5 51.2 59.0 30.1 31.2 31.8 24.7 28.9 34.3
danish 46.2 49.3 60.2 51.3 61.4 56.9 60.5 60.8 47.2 50.2 35.3 36.4 18.7 49.2
dutch 50.5 50.8 37.0 49.5 38.4 38.9 50.0 51.7 48.7 39.7 49.1 35.1 34.0 39.5
en-childes 48.1 62.2 56.8 47.2 51.8 50.5 53.5 56.0 51.7 51.9 39.0 31.7 36.0 23.3
en-ptb 72.1 73.7 58.9 67.4 52.2 44.8 61.0 74.7 31.7 44.7 30.6 35.2 40.4 19.9
portuguese 54.3 76.3 63.6 59.9 44.3 47.7 71.1 55.7 27.1 37.2 26.9 31.1 28.1 37.7
slovene 65.8 53.9 42.1 51.4 39.2 39.7 50.3 67.7 35.7 37.2 35.6 25.7 35.9 14.7
swedish 65.8 66.7 61.4 63.7 70.8 48.2 72.0 76.5 44.2 44.2 45.8 39.1 33.2 31.3
averages 57.0 57.9 48.5 54.3 50.8 45.3 54.7 62.7 40.9 42.5 37.5 32.3 29.9 33.9
Undirected
testset BC BH MZc MZp MZu S1 S2 Tu DMVc DMVp DMVu H LB RB
arabic 69.4 59.3 59.3 69.4 69.4 59.3 65.7 67.6 52.8 53.7 55.6 54.6 61.1 69.4
basque 65.6 59.5 49.8 50.1 48.4 54.3 32.8 66.4 60.1 58.1 48.5 42.2 56.4 53.9
czech 65.9 59.9 69.2 66.6 67.6 62.3 63.5 70.1 51.6 51.2 50.5 47.2 54.2 56.9
danish 67.9 63.5 70.6 64.4 71.1 67.9 70.7 70.2 65.0 64.3 60.0 56.4 59.7 63.9
dutch 63.2 59.9 57.5 63.3 58.0 58.5 58.5 60.5 62.7 56.7 62.9 51.1 56.3 60.7
en-childes 65.3 70.6 69.4 62.4 63.7 64.3 63.6 69.1 65.7 66.1 59.5 51.2 51.2 51.0
en-ptb 79.4 79.2 65.9 72.5 62.4 62.5 75.1 78.8 53.8 65.3 53.2 52.0 58.8 54.9
portuguese 66.3 81.9 71.6 70.2 62.3 65.8 78.5 72.1 54.0 60.9 54.3 53.3 56.7 63.8
slovene 70.6 63.7 56.3 59.1 55.1 54.8 63.7 72.0 46.4 53.1 46.3 44.9 45.5 46.0
swedish 82.3 73.5 70.1 71.1 75.4 66.5 77.3 83.7 64.5 64.5 66.1 59.2 59.2 59.5
averages 69.6 67.1 64.0 64.9 63.3 61.6 64.9 71.0 57.7 59.4 55.7 51.2 55.9 58.0
NED
testset BC BH MZc MZp MZu S1 S2 Tu DMVc DMVp DMVu H LB RB
arabic 78.7 68.5 66.7 75.9 75.9 68.5 72.2 71.3 61.1 63.0 63.0 63.0 64.8 75.9
basque 77.9 68.6 62.9 65.2 64.2 70.1 55.0 79.5 69.3 69.0 63.8 58.2 73.2 64.0
czech 79.9 72.6 81.8 78.6 79.9 76.0 78.1 81.2 62.9 61.7 63.6 60.3 63.2 73.8
danish 81.7 76.3 83.2 78.4 84.3 77.3 84.4 85.0 77.9 75.8 69.5 67.8 66.2 77.5
dutch 71.0 71.8 77.1 78.4 76.8 72.6 68.5 71.0 73.2 64.6 73.0 60.5 64.4 71.0
en-childes 82.4 81.6 84.5 76.7 74.8 79.0 84.9 82.5 79.9 80.4 79.9 70.1 63.2 76.9
en-ptb 86.7 89.4 87.5 88.4 84.0 78.7 87.1 84.3 64.0 80.7 64.2 64.3 65.2 75.0
portuguese 78.2 90.7 87.8 87.8 87.5 82.9 91.9 90.2 75.6 81.7 76.5 67.7 60.9 82.4
slovene 79.3 76.8 70.8 72.8 70.4 68.1 78.9 79.8 59.4 62.2 59.4 55.8 54.3 62.2
swedish 91.7 85.8 83.1 85.6 87.1 80.8 87.6 92.1 76.1 76.1 76.4 75.3 67.2 79.1
averages 80.8 78.2 78.5 78.8 78.5 75.4 78.9 81.7 69.9 71.5 68.9 64.3 64.3 73.8
Table 8: Evaluation of the dependency task using a maximum sentence length of 10. See also Table 3 which presents
the same results with no length restriction.
80
Proceedings of the ACL 2014 Workshop on Language Technologies and Computational Social Science, pages 13?17,
Baltimore, Maryland, USA, June 26, 2014.
c?2014 Association for Computational Linguistics
Extracting Socioeconomic Patterns from the News: Modelling Text and
Outlet Importance Jointly
Vasileios Lampos
1
, Daniel Preot?iuc-Pietro
2
, Sina Samangooei
3
,
Douwe Gelling
2
, and Trevor Cohn
4
1
Department of Computer Science, University College London ? v.lampos@ucl.ac.uk
2
Department of Computer Science, The University of Sheffield ? {d.preotiuc,d.gelling}@shef.ac.uk
3
Electronics and Computer Science, University of Southampton ? ss@ecs.soton.ac.uk
4
Computing and Information Systems, The University of Melbourne ? t.cohn@unimelb.edu.au
Abstract
Information from news articles can be used
to study correlations between textual dis-
course and socioeconomic patterns. This
work focuses on the task of understanding
how words contained in the news as well as
the news outlets themselves may relate to
a set of indicators, such as economic senti-
ment or unemployment rates. The bilinear
nature of the applied regression model fa-
cilitates learning jointly word and outlet
importance, supervised by these indicators.
By evaluating the predictive ability of the
extracted features, we can also assess their
relevance to the target socioeconomic phe-
nomena. Therefore, our approach can be
formulated as a potential NLP tool, partic-
ularly suitable to the computational social
science community, as it can be used to in-
terpret connections between vast amounts
of textual content and measurable society-
driven factors.
1 Introduction
Vast amounts of user-generated content on the Inter-
net as well as digitised textual resources allow us to
study text in connection to real world events across
large intervals of time. Over the last decade, there
has been a shift in user news consumption starting
with a move from offline to online sources (Lin
et al., 2005); in more recent years user-generated
news have also become prominent. However, tra-
ditional news outlets continue to be a central refer-
ence point (Nah and Chung, 2012) as they still have
the advantage of being professionally authored, al-
leviating the noisy nature of citizen journalism for-
mats.
Here, we present a framework for analysing so-
cioeconomic patterns in news articles. In contrast
to prior approaches, which primarily focus on the
textual contents, our analysis shows how Machine
Learning methods can be used to gain insights into
the interplay between text in news articles, the news
outlets and socioeconomic indicators. Our experi-
ments are performed on a set of EU-related news
summaries spanning over 8 years, with the inten-
tion to study two basic economic factors: EU?s
unemployment rate and Economic Sentiment Index
(ESI) (European Commision, 1997). To determine
connections between the news, the outlets and the
indicators of interest, we formulate our learning
task as bilinear text-based regression (Lampos et
al., 2013).
Approaches to learning the correlation of news,
or text in general, with real world indicators have
been performed in both unsupervised and super-
vised settings. For example, Flaounas et al. (2010)
uncover interesting patterns in EU?s Mediasphere,
whereas Schumaker and Chen (2009) demonstrate
that news articles can predict financial indicators.
Conversely, Bentley et al. (2014) show that emo-
tions in the textual content of books reflect back
on inflation and unemployment rates during the
20th century. Recently, Social Media text has been
intensively studied as a quicker, unobtrusive and
cheaper alternative to traditional surveys. Applica-
tion areas include politics (O?Connor et al., 2010),
finance (Bollen and Mao, 2011), health (Lampos
and Cristianini, 2012; Paul and Dredze, 2011) or
psychology (De Choudhury et al., 2013; Schwartz
et al., 2013).
In this paper, we apply a modified version of a
bilinear regularised regression model (BEN) pro-
posed for the task of voting intention inference
from Twitter content (Lampos et al., 2013). The
main characteristic of BEN is the ability of mod-
elling word frequencies as well as individual user
importance in a joint optimisation task. By apply-
ing it in the context of supervised news analysis,
we are able to visualise relevant discourse to a par-
ticular socioeconomic factor, identifying relevant
words together with important outlets.
13
2 Data
We compiled a data set by crawling summaries
on news articles written in English language, pub-
lished by the Open Europe Think Tank.
1
The press
summaries are daily aggregations of news items
about the EU or member countries with a focus
on politics; the news outlets used to compile each
summary are listed below the summary?s text. The
site is updated every weekday, with the major news
being covered in a couple of paragraphs, and other
less prevalent issues being mentioned in one para-
graph to as little as one sentence. The news sum-
maries were first published on February 2006; we
collected all of them up to mid-November 2013,
creating a data set with the temporal resolution of
1913 days (or 94 months).
The text was tokenised using the NLTK li-
brary (Bird et al., 2009). News outlets with fewer
than 5 mentions were removed, resulting in a total
of 435 sources. Each summary contains on average
14 news items, with an average of 3 news sources
per item; where multiple sources were present, the
summary was assigned to all the referenced news
outlets. After removing stop words, we ended up
with 8, 413 unigrams and 19, 045 bigrams; their
daily occurrences were normalised using the total
number of news items for that day.
For the purposes of our supervised analysis, we
use the response variables of ESI and unemploy-
ment rate across the EU. The monthly time series
of these socioeconomic indicators were retrieved
from Eurostat, EU?s statistical office (see the red
lines in Fig. 1a and 1b respectively). ESI is a com-
posite indicator often seen as an early predictor for
future economic developments (Gelper and Croux,
2010). It consists of five confidence indicators with
different weights: industrial (40%), services (30%),
consumer (20%), construction (5%) and retail trade
(5%). The unemployment rate is a seasonally ad-
justed ratio of the non employed persons over the
entire EU labour force.
2
3 Models
A common approach to regression arises through
the application of generalised linear models. These
models use a feature vector inputx and aim to build
a linear function of x for predicting a response
1
http://www.openeurope.org.uk/Page/
PressSummary/en/
2
http://epp.eurostat.ec.europa.
eu/statistics_explained/index.php/
Unemployment_statistics
variable y:
f(x) = x
T
w + ? where x,w ? R
m
. (1)
The objective is to find an f , which minimises a
model-dependent loss function (e.g. sum squared
error), optionally subject to a regularisation penalty
?; `
2
-norm regularisation (ridge regression) pe-
nalises high weights (Hoerl and Kennard, 1970),
while `
1
-norm regularisation (lasso) encourages
sparse solutions (Tibshirani, 1994). Sparsity is de-
sirable for avoiding overfitting, especially when
the dimensionality m is larger than the number of
training examples n (Hastie et al., 2009). Elastic
Net formulates a combination of `
1
and `
2
-norm
regularisation defined by the objective:
{w
?
, ?
?
} =argmin
w,?
n
?
i=1
(x
T
i
?w + ? ? y
i
)
2
+ ?
EN
(w, ?) ,
(2)
where ? denotes the regularisation parameters (Zou
and Hastie, 2005); we refer to this model as LEN
(Linear Elastic Net) in the remainder of the script.
In the context of voting intention inference from
Twitter content, Lampos et al. (2013) extended
LEN to a bilinear formulation, where a set of two
vector weights are learnt: one for words (w) and
one for users (u). This was motivated by the ob-
servation that only a sparse set of users may have
predictive value. The model now becomes:
f(X) = u
T
Xw + ? , (3)
where X is a matrix of word ? users frequencies.
The bilinear optimisation objective is formulated
as:
{w
?
,u
?
, ?
?
} =argmin
w,u,?
n
?
i=1
(
u
T
X
i
w + ? ? y
i
)
2
+ ?
EN
(w, ?
1
) + ?
EN
(u, ?
2
) ,
(4)
where X
i
is the word ? user frequency matrix, and
?
1
, ?
2
are the word and user regularisation param-
eters. This can be treated as a biconvex learning
task and be solved by iterating over two convex
processes: fixingw and learning u, and vice versa
(Lampos et al., 2013). Regularised regression on
both user and word spaces allows for an automatic
selection of the most important words and users,
performing at the same time an improved noise
filtering.
14
In our experiments, news outlets and socioeco-
nomic indicators replace users and voting intention
in the previous model formulation. To ease the in-
terpretation of the outputs, we further impose a
positivity constraint on the outlet weights u, i.e.
min(u) ? 0; this makes the model more restric-
tive, but, in our case, did not affect the prediction
performance. We refer to this model as BEN (Bi-
linear Elastic Net).
4 Experiments
Both models are applied to the news summaries
data set with the aim to predict EU?s ESI and rate
of unemployment. The predictive capability of the
derived models, assessed by their respective infer-
ence performance, is used as a metric for judging
the degree of relevance between the learnt model
parameters ? word and outlet weights ? and the
response variable. A strong predictive performance
increases confidence on the soundness of those pa-
rameters.
To match input with the monthly temporal reso-
lution of the response variables, we compute the
mean monthly term frequencies for each outlet.
Evaluation is performed via a 10-fold validation,
where each fold?s training set is based on a mov-
ing window of p = 64 contiguous months, and the
test set consists of the following q = 3 months;
formally, the training and test sets for fold i are
based on months {q(i? 1) + 1, ..., q(i? 1) + p}
and {q(i? 1) + p+ 1, ..., q(i? 1) + p+ q} re-
spectively. In this way, we emulate a scenario
where we always train on past and predict future
points.
Performance results for LEN and BEN are pre-
sented in Table 1; we show the average Root Mean
Squared Error (RMSE) as well as an error rate
(RMSE over ?(y)) across folds to allow for a bet-
ter interpretation. BEN outperforms LEN in both
tasks, with a clearer improvement when predict-
ing ESI. Predictions for all folds are depicted in
Fig. 1a and 1b together with the actual values. Note
that reformulating the problem into a multi-task
learning scenario, where ESI and unemployment
are modelled jointly did not improve inference per-
formance.
The relatively small average error rates (< 8.8%)
make meaningful a further analysis of the model?s
outputs. Due to space limitations, we choose to fo-
cus on the most recent results, depicting the models
derived in the 10th fold. Following the example of
Schwartz et al. (2013), we use a word cloud visu-
ESI Unemployment
LEN 9.253 (9.89%) 0.9275 (8.75%)
BEN 8.209 (8.77%) 0.9047 (8.52%)
Table 1: 10-fold validation average RMSEs (and
error rates) for LEN and BEN on ESI and unem-
ployment rates prediction.
2007 2008 2009 2010 2011 2012 20130
50
100
 
 
actualpredictions
(a) ESI
2007 2008 2009 2010 2011 2012 20130
5
10
 
 
actualpredictions
(b) Unemployment
Figure 1: Time series of ESI and unemployment
together with BEN predictions (smoothed using a
3-point moving average).
alisation, where the font size is proportional to the
derived weights by applying BEN, flipped terms de-
note negative weights and colours are determined
by the frequency of use in the corpus (Fig. 2). Word
clouds depict the top-60 positively and negatively
weighted n-grams (120 in total) together with the
top-30 outlets; bigrams are separated by ? ?.
5 Discussion and Future Work
Our visualisations (Fig. 2) present various inter-
esting insights into the news and socioeconomic
features being explored, serving as a demonstra-
tion of the potential power of the proposed mod-
elling. Firstly, we notice that in the word cloud,
the size of a feature (BEN?s weight) is not tightly
connected with its colour (frequency in the corpus).
Also, the word clouds suggest that mostly different
terms and outlets are selected for the two indicators.
For example, ?sky.it? is predominant for ESI but
not for unemployment, while the opposite is true
for ?hedgefundsreview.com?. Some of the words
selected for ESI reflect economical issues, such as
?stimulus? and ?spending?, whereas key politicians
15
(a) ESI
(b) Unemployment
Frequency
Word
Outlet
Weight
a
a
Polarity
Yes
Yes
+
-
Figure 2: Word clouds for words and outlets visualising the outputs of BEN.
like ?david cameron? and ?berlusconi?, are major
participants in the word cloud for unemployment.
In addition, the visualisations show a strong neg-
ative relationship between unemployment and the
terms ?food?, ?russia? and ?agriculture?, but no such
relationship with respect to ESI. The disparity of
these selections is evidence for our framework?s
capability to highlight features of lesser or greater
importance to a given socioeconomic time series.
The exact interpretation of the selected words and
outlets is, perhaps, context-dependent and beyond
the scope of this work.
In this paper, we presented a framework for per-
forming a supervised analysis on news. An impor-
tant factor for this process is that the bilinear nature
of the learning function allows for a joint selection
of important words and news outlets. Prediction
performance is used as a reference point for de-
termining whether the extracted outputs (i.e. the
model?s parameters) encapsulate relevant informa-
tion regarding to the given indicator. Experiments
were conducted on a set of EU-related news sum-
maries and the supervising socioeconomic factors
were the EU-wide ESI and unemployment. BEN
outperformed the linear alternative (LEN), produc-
ing error rates below 8.8%.
The performance of our framework motivates
several extensions to be explored in future work.
Firstly, the incorporation of additional textual fea-
tures may improve predictive capability and allow
for richer interpretations of the term weights. For
example, we could extend our term vocabulary us-
ing n-grams with n > 2, POS tags of words and
entities (people, companies, places, etc.). Further-
more, multi-task learning approaches as well as
models which incorporate the regularised learning
of weights for different countries might give us fur-
ther insights into the relationship between news,
geographic location and socioeconomic indicators.
Most importantly, we plan to gain a better under-
standing of the outputs by conducting a thorough
analysis in collaboration with domain experts.
16
Acknowledgements
VL acknowledges the support from the EPSRC
IRC project EP/K031953/1. DPP, SS, DG and TC
were supported by EU-FP7-ICT project n.287863
(?TrendMiner?).
References
R. Alexander Bentley, Alberto Acerbi, Paul Ormerod,
and Vasileios Lampos. 2014. Books average previ-
ous decade of economic misery. PLoS ONE, 9(1).
Steven Bird, Ewan Klein, and Edward Loper. 2009.
Natural Language Processing with Python. O?Reilly
Media.
Johan Bollen and Huina Mao. 2011. Twitter mood as a
stock market predictor. IEEE Computer, 44(10):91?
94.
Munmun De Choudhury, Scott Counts, and Eric
Horvitz. 2013. Social media as a measurement tool
of depression in populations. In Proceedings of ACM
WebSci?13, pages 47?56.
European Commision. 1997. The joint harmonised EU
programme of business and consumer surveys. Euro-
pean economy: Reports and studies.
Ilias Flaounas, Marco Turchi, Omar Ali, Nick Fyson,
Tijl De Bie, Nick Mosdell, Justin Lewis, and Nello
Cristianini. 2010. The Structure of the EU Medias-
phere. PLoS ONE, 5(12), 12.
Sarah Gelper and Christophe Croux. 2010. On the con-
struction of the European Economic Sentiment Indi-
cator. Oxford Bulletin of Economics and Statistics,
72(1):47?62.
Trevor Hastie, Robert Tibshirani, and Jerome Friedman.
2009. The Elements of Statistical Learning: Data
Mining, Inference, and Prediction. Springer.
Arthur E. Hoerl and Robert W. Kennard. 1970. Ridge
regression: biased estimation for nonorthogonal prob-
lems. Technometrics, 12:55?67.
Vasileios Lampos and Nello Cristianini. 2012. Now-
casting events from the Social Web with statistical
learning. ACM TIST, 3(4):72:1?72:22.
Vasileios Lampos, Daniel Preot?iuc-Pietro, and Trevor
Cohn. 2013. A user-centric model of voting inten-
tion from Social Media. In Proceedings of ACL?13,
pages 993?1003.
Carolyn Lin, Michael B. Salwen, Bruce Garrison, and
Paul D. Driscoll. 2005. Online news as a functional
substitute for offline news. Online news and the pub-
lic, pages 237?255.
Seungahn Nah and Deborah S. Chung. 2012. When cit-
izens meet both professional and citizen journalists:
Social trust, media credibility, and perceived journal-
istic roles among online community news readers.
Journalism, 13(6):714?730.
Brendan O?Connor, Ramnath Balasubramanyan,
Bryan R. Routledge, and Noah A. Smith. 2010.
From tweets to polls: linking text sentiment to
public opinion time series. In Proceedings of AAAI
ICWSM?10, pages 122?129.
Michael J. Paul and Mark Dredze. 2011. You
Are What You Tweet: Analyzing Twitter for Public
Health. In Proceedings of AAAI ICWSM?11, pages
265?272.
Robert P. Schumaker and Hsinchun Chen. 2009. Tex-
tual analysis of stock market prediction using break-
ing financial news: the AZFin text system. ACM
TOIS, 27(2):12:1?12:19.
H. Andrew Schwartz, Johannes C. Eichstaedt, Mar-
garet L. Kern, Lukasz Dziurzynski, Stephanie M. Ra-
mones, Megha Agrawal, Achal Shah, Michal Kosin-
ski, David Stillwell, Martin E. P. Seligman, and
Lyle H. Ungar. 2013. Personality, Gender, and
Age in the Language of Social Media: The Open-
Vocabulary Approach. PLoS ONE, 8(9).
Robert Tibshirani. 1994. Regression shrinkage and
selection via the lasso. JRSS: Series B, 58:267?288.
Hui Zou and Trevor Hastie. 2005. Regularization and
variable selection via the elastic net. JRSS: Series B,
67(2):301?320.
17
