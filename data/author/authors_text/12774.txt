CoNLL 2008: Proceedings of the 12th Conference on Computational Natural Language Learning, pages 89?96
Manchester, August 2008
An Incremental Bayesian Model for Learning Syntactic Categories
Christopher Parisien, Afsaneh Fazly and Suzanne Stevenson
Department of Computer Science
University of Toronto
Toronto, ON, Canada
[chris,afsaneh,suzanne]@cs.toronto.edu
Abstract
We present an incremental Bayesian model for
the unsupervised learning of syntactic cate-
gories from raw text. The model draws infor-
mation from the distributional cues of words
within an utterance, while explicitly bootstrap-
ping its development on its own partially-
learned knowledge of syntactic categories.
Testing our model on actual child-directed
data, we demonstrate that it is robust to noise,
learns reasonable categories, manages lexical
ambiguity, and in general shows learning be-
haviours similar to those observed in children.
1 Introduction
An important open problem in cognitive science and
artificial intelligence is how children successfully
learn their native language despite the lack of explicit
training. A key challenge in the early stages of lan-
guage acquisition is to learn the notion of abstract
syntactic categories (e.g., nouns, verbs, or determin-
ers), which is necessary for acquiring the syntactic
structure of language. Indeed, children as young as
two years old show evidence of having acquired a
good knowledge of some of these abstract categories
(Olguin and Tomasello, 1993); by around six years of
age, they have learned almost all syntactic categories
(Kemp et al, 2005). Computational models help to
elucidate the kinds of learning mechanisms that may
be capable of achieving this feat. Such studies shed
light on the possible cognitive mechanisms at work
in human language acquisition, and also on potential
means for unsupervised learning of complex linguis-
tic knowledge in a computational system.
Learning the syntactic categories of words has
been suggested to be based on the morphological and
phonological properties of individual words, as well
c
? 2008. Licensed under the Creative Commons
Attribution-Noncommercial-Share Alike 3.0 Unported li-
cense (http://creativecommons.org/licenses/by-nc-sa/3.0/).
Some rights reserved.
as on the distributional information about the con-
texts in which they appear. Several computational
models have been proposed that draw on one or more
of the above-mentioned properties in order to group
words into discrete unlabeled categories. Most ex-
isting models only intend to show the relevance of
such properties to the acquisition of adult-like syn-
tactic categories such as nouns and verbs; hence, they
do not necessarily incorporate the types of learning
mechanisms used by children (Schu?tze, 1993; Red-
ington et al, 1998; Clark, 2000; Mintz, 2003; Onnis
and Christiansen, 2005). For example, in contrast to
the above models, children acquire their knowledge
of syntactic categories incrementally, processing the
utterances they hear one at a time. Moreover, chil-
dren appear to be sensitive to the fact that syntactic
categories are partially defined in terms of other cat-
egories, e.g., nouns tend to follow determiners, and
can be modified by adjectives.
We thus argue that a computational model should
be incremental, and should use more abstract cate-
gory knowledge to help better identify syntactic cat-
egories. Incremental processing also allows a model
to incorporate its partially-learned knowledge of cat-
egories, letting the model bootstrap its development.
To our knowledge, the only incremental model of
category acquisition that also incorporates bootstrap-
ping is that of Cartwright and Brent (1997). Their
template-based model, however, draws on very spe-
cific linguistic constraints and rules to learn cate-
gories. Moreover, their model has difficulty with the
variability of natural language data.
We address these shortcomings by developing an
incremental probabilistic model of syntactic category
acquisition that uses a domain-general learning algo-
rithm. The model also incorporates a bootstrapping
mechanism, and learns syntactic categories by look-
ing only at the general patterns of distributional sim-
ilarity in the input. Experiments performed on actual
(noisy) child-directed data show that an explicit boot-
strapping component improves the model?s ability to
89
learn adult-like categories. The model?s learning tra-
jectory resembles some relevant behaviours seen in
children, and we also show that the categories that
our model learns can be successfully used in a lexical
disambiguation task.
2 Overview of the Computational Model
We adapt a probabilistic incremental model of un-
supervised categorization (i.e., clustering) proposed
by Anderson (1991). The original model has been
used to simulate human categorization in a variety
of domains, including the acquisition of verb argu-
ment structure (Alishahi and Stevenson, 2008). Our
adaptation of the model incorporates an explicit boot-
strapping mechanism and a periodic merge of clus-
ters, both facilitating generalization over input data.
Here, we explain the input to our model (Section 2.1),
the categorization model itself (Section 2.2), how we
estimate probabilities to facilitate bootstrapping (Sec-
tion 2.3), and our approach for merging similar clus-
ters (Section 2.4).
2.1 Input Frames
We aim to learn categories of words, and we do this
by looking for groups of similar word usages. Thus,
rather than categorizing a word alone, we categorize a
word token with its context from that usage. The ini-
tial input to our model is a sequence of unannotated
utterances, that is, words separated by spaces. Before
being categorized by the model, each word usage in
the input is processed to produce a frame that con-
tains the word itself (the head word of the frame) and
its distributional context (the two words before and
after it). For example, in the utterance ?I gave Josie
a present,? when processing the head word Josie, we
create the following frame for input to the categoriza-
tion system:
feature w
?2
w
?1
w
0
w
+1
w
+2
I gave Josie a present
where w
0
denotes the head word feature, and w
?2
,
w
?1
, w
+1
, w
+2
are the context word features. A con-
text word may be ?null? if there are fewer than two
preceding or following words in the utterance.
2.2 Categorization
Using Anderson?s (1991) incremental Bayesian cat-
egorization algorithm, we learn clusters of word us-
ages (i.e., the input frames) by drawing on the overall
similarity of their features (here, the head word and
the context words). The clusters themselves are not
predefined, but emerge from similarities in the input.
More formally, for each successive frame F in the
input, processed in the order of the input words, we
place F into the most likely cluster, either from the
K existing clusters, or a new one:
BestCluster(F ) = argmax
k
P (k|F ) (1)
where k = 0, 1, ..,K, including the new cluster
k = 0. Using Bayes? rule, and dropping P (F ) from
the denominator, which is constant for all k, we find:
P (k|F ) =
P (k)P (F |k)
P (F )
? P (k)P (F |k) (2)
The prior probability of k, P (k), is given by:
P (k) =
cn
k
(1? c) + cn
, 1 ? k ? K (3)
P (0) =
1? c
(1? c) + cn
(4)
where n
k
is the number of frames in k, and n is
the total number of frames observed at the time of
processing frame F . Intuitively, a well-entrenched
(large) cluster should be a more likely candidate for
categorization than a small one. We reserve a small
probability for creating a new cluster (Eq. 4). As the
model processes more input overall, it should become
less necessary to create new clusters to fit the data, so
P (0) decreases with large n. In our experiments, we
set c to a large value, 0.95, to further increase the
likelihood of using existing clusters.1
The probability of a frame F given a cluster k,
P (F |k), depends on the probabilities of the features
in F given k. We assume that the individual fea-
tures in a frame are conditionally independent given
k, hence:
P (F |k) = P
H
(w
0
|k)
?
i?{?2,?1,+1,+2}
P (w
i
|k) (5)
where P
H
is the head word probability, i.e., the like-
lihood of seeing w
0
as a head word among the frames
in cluster k. The context word probability P (w
i
|k) is
the likelihood of seeing w
i
in the ith context position
of the frames in cluster k. Next, we explain how we
estimate each of these probabilities from the input.
2.3 Probabilities and Bootstrapping
For the head word probability P
H
(w
0
|k), we use a
smoothed maximum likelihood estimate (i.e., the pro-
portion of frames in cluster k with head word w
0
).
For the context word probability P (w
i
|k), we can
form two estimates. The first is a simple maximum
likelihood estimate, which enforces a preference for
creating clusters of frames with the same context
words. That is, head words in the same cluster will
1The prior P (k) is equivalent to the prior in a Dirichlet pro-
cess mixture model (Sanborn et al, 2006), commonly used for
sampling clusters of objects.
90
tend to share the same adjacent words. We call this
word-based estimate P
word
.
Alternatively, we may consider the likelihood of
seeing not just the context word w
i
, but similar words
in that position. For example, if w
i
can be used as a
noun or a verb, then we want the likelihood of seeing
other nouns or verbs in position i of frames in cluster
k. Here, we use the partial knowledge of the learned
clusters. That is, we look over all existing clusters
k
?
, estimate the probability that w
i
is the head word
of frames in k?, then estimate the probability of using
the head words from those other clusters in position i
in cluster k. We refer to this category-based estimate
as P
cat
:
P
cat
(w
i
|k) =
?
k
?
P
H
(w
i
|k
?
)P
i
(k
?
|k) (6)
where P
i
(k
?
|k) is the probability of finding usages
from cluster k? in position i given cluster k. To sup-
port this we record the categorization decisions the
model has made. When we categorize the frames of
an utterance, we get a sequence of clusters for that
utterance, which gives additional information to sup-
plement the frame. We use this information to esti-
mate P
i
(k
?
|k) for future categorizations, again using
a smoothed maximum likelihood formula.
In contrast to the P
word
estimate, the estimate in
Eq. (6) prefers clusters of frames that use the same
categories as context. While some of the results of
these preferences will be the same, the latter approach
lets the model make second-order inferences about
categories. There may be no context words in com-
mon between the current frame and a potential clus-
ter, but if the context words in the cluster have been
found to be distributionally similar to those in the
frame, it may be a good cluster for that frame.
We equally weight the word-based and the
category-based estimates for P (w
i
|k) to get the like-
lihood of a context word; that is:
P (w
i
|k) ?
1
2
P
word
(w
i
|k) +
1
2
P
cat
(w
i
|k) (7)
This way, the model sees an input utterance simulta-
neously as a sequence of words and as a sequence of
categories. It is the P
cat
component, by using devel-
oping category knowledge, that yields the bootstrap-
ping abilities of our model.
2.4 Generalization
Our model relies heavily on the similarity of word
contexts in order to find category structure. In nat-
ural language, these context features are highly vari-
able, so it is difficult to draw consistent structure from
the input in the early stages of an incremental model.
When little information is available, there is a risk of
incorrectly generalizing, leading to clustering errors
which may be difficult to overcome. Children face
a similar problem in early learning, but there is ev-
idence that they may manage the problem by using
conservative strategies (see, e.g., Tomasello, 2000).
Children may form specific hypotheses about each
word type, only later generalizing their knowledge to
similar words. Drawing on this observation, we form
early small clusters specific to the head word type,
then later aid generalization by merging these smaller
clusters. By doing this, we ensure that the model only
groups words of different types when there is suffi-
cient evidence for their contextual similarity.
Thus, when a cluster has been newly created, we
require that all frames put into the cluster share the
same head word type.2 When clusters are small, this
prevents the model from making potentially incorrect
generalizations to different words. Periodically, we
evaluate a set of reasonably-sized clusters, and merge
pairs of clusters that have highly similar contexts (see
below for details). If the model decides to merge two
clusters with different head word types?e.g., one
cluster with all instances of dog, and another with
cat?it has in effect made a decision to generalize.
Intuitively, the model has learned that the contexts
in the newly merged cluster apply to more than one
word type. We now say that any word type could be
a member of this cluster, if its context is sufficiently
similar to that of the cluster. Thus, when categoriz-
ing a new word token (represented as a frame F ),
our model can choose from among the clusters with
a matching head word, and any of these ?generalized?
clusters that contain mixed head words.
Periodically, we look through a subset of the clus-
ters to find similar pairs to merge. In order to limit
the number of potential merges to consider, we only
examine pairs of clusters in which at least one cluster
has changed since the last check. Thus, after pro-
cessing every 100 frames of input, we consider the
clusters used to hold those recent 100 frames as can-
didates to be merged with another cluster. We only
consider clusters of reasonable size (here, at least 10
frames) as candidates for merging. For each candi-
date pair of clusters, k
1
and k
2
, we first evaluate a
heuristic merge score that determines if the pair is
appropriate to be merged, according to some local
criteria, i.e., the size and the contents of the candi-
date clusters. For each suggested merge (a pair whose
merge score exceeds a pre-determined threshold), we
then look at the set of all clusters, the global evidence,
to decide whether to accept the merge.
The merge score combines two factors: the en-
trenchment of the two clusters, and the similarity of
2However, a word type may exist in several clusters (e.g., for
distinct noun and verb usages), thus handling lexical ambiguity.
91
their context features. The entrenchment measure
identifies clusters that contain enough frames to show
a significant trend. We take a sigmoid function over
the number of frames in the clusters, giving a soft
threshold approaching 0 for small clusters and 1 for
large clusters. The similarity measure identifies pairs
of clusters with similar distributions of word and cat-
egory contexts. Given two clusters, we measure the
symmetric Kullback-Leibler divergence for each cor-
responding pair of context feature probabilities (in-
cluding the category contexts P
i
(k
?
|k), 8 pairs in to-
tal), then place the sum of those measures on another
sigmoid function. The merge score is the sum of the
entrenchment and similarity measures.
Since it is only a local measure, the merge score is
not sufficient on its own for determining if a merge
is appropriate. For each suggested merge, we thus
examine the likelihood of a sample of input frames
(here, the last 100 frames) under two states: the set
of clusters before the merge, and the set of clusters if
the merge is accepted. We only accept a merge if it
results in an increase in the likelihood of the sample
data. The likelihood of a sample set of frames, S ,
over a set of clusters, K, is calculated as in:
P (S) =
?
F?S
?
k?K
P (F |k)P (k) (8)
3 Evaluation Methodology
To test our proposed model, we train it on a sample of
language representative of what children would hear,
and evaluate its categorization abilities. We have
multiple goals in this evaluation. First, we determine
the model?s ability to discover adult-level syntactic
categories from the input. Since this is intended to be
a cognitively plausible learning model, we also com-
pare the model?s qualitative learning behaviours with
those of children. In the first experiment (Section 4),
we compare the model?s categorization with a gold
standard of adult-level syntactic categories and exam-
ine the effect of the bootstrapping component. The
second experiment (Section 5) examines the model?s
development of three specific parts of speech. De-
velopmental evidence suggests that children acquire
different syntactic categories at different ages, so we
compare the model?s learning rates of nouns, verbs,
and adjectives. Lastly, we examine our model?s abil-
ity to handle lexically ambiguous words (Section 6).
English word forms commonly belong to more than
one syntactic category, so we show how our model
uses context to disambiguate a word?s category.
In all experiments, we train and test the model us-
ing the Manchester corpus (Theakston et al, 2001)
from the CHILDES database (MacWhinney, 2000).
The corpus contains transcripts of mothers? conver-
sations with 12 British children between the ages of
1;8 (years;months) and 3;0. There are 34 one-hour
sessions per child over the course of a year. The age
range of the children roughly corresponds with the
ages at which children show the first evidence of syn-
tactic categories.
We extract the mothers? speech from each of the
transcripts, then concatenate the input of all 12 chil-
dren (all of Anne?s sessions, followed by all of Aran?s
sessions, and so on). We remove all punctuation. We
spell out contractions, so that each token in the input
corresponds to only one part-of-speech (PoS) label
(noun, verb, etc.). We also remove single-word ut-
terances and utterances with a single repeated word
type, since they contain no distributional informa-
tion. We randomly split the data into development
and evaluation sets, each containing approximately
683,000 tokens. We use the development set to fine-
tune the model parameters and develop the experi-
ments, then use the evaluation set as a final test of
the model. We further split the development set into
about 672,000 tokens (about 8,000 types) for training
and 11,000 tokens (1,300 types) for validation. We
split the evaluation set comparably, into training and
test subsets. All reported results are for the evaluation
set. A conservative estimate suggests that children
are exposed to at least 1.5 million words of child-
directed speech annually (Redington et al, 1998), so
this corpus represents only a small portion of a child?s
available input.
4 Experiment 1: Adult Categories
4.1 Methods
We use three separate versions of the categorization
model, in which we change the components used to
estimate the context word probability, P (w
i
|k) (as
used in Eq. (5), Section 2.2). In the word-based
model, we estimate the context probabilities using
only the words in the context window, by directly
using the maximum-likelihood P
word
estimate. The
bootstrap model uses only the existing clusters to es-
timate the probability, directly using the P
cat
esti-
mate from Eq. (6). The combination model uses an
equally-weighted combination of the two probabili-
ties, as presented in Eq. (7).
We run the model on the training set, categoriz-
ing each of the resulting frames in order. After every
10,000 words of input, we evaluate the model?s cate-
gorization performance on the test set. We categorize
each of the frames of the test set as usual, treating the
text as regular input. So that the test set remains un-
seen, the model does not record these categorizations.
4.2 Evaluation
The PoS tags in the Manchester corpus are too fine-
grained for our evaluation, so for our gold standard
92
we map them to the following 11 tags: noun, verb,
auxiliary, adjective, adverb, determiner, conjunction,
negation, preposition, infinitive to, and ?other.? When
we evaluate the model?s categorization performance,
we have two different sets of clusters of the words in
the test set: one set resulting from the gold standard,
and another as a result of the model?s categorization.
We compare these two clusterings, using the adjusted
Rand index (Hubert and Arabie, 1985), which mea-
sures the overall agreement between two clusterings
of a set of data points. The measure is ?corrected for
chance,? so that a random grouping has an expected
score of zero. This measure tends to be very con-
servative, giving values much lower than an intuitive
percentage score. However, it offers a useful relative
comparison of overall cluster similarity.
4.3 Results
Figure 1 gives the adjusted Rand scores of the three
model variants, word-based, bootstrap, and combi-
nation. Higher values indicate a better fit with the
gold-standard categorization scheme. The adjusted
Rand score is corrected for chance, thus providing a
built-in baseline measure. Since the expected score
for a random clustering is zero, all three model vari-
ants operate at above-baseline performance.
As seen in Figure 1, the word-based model gains
an early advantage in the comparison, but its per-
formance approaches a plateau at around 200,000
words of input. This suggests that while simple
word distributions provide a reliable source of infor-
mation early in the model?s development, the infor-
mation is not sufficient to sustain long-term learn-
ing. The bootstrap model learns much more slowly,
which is unsurprising, given that it depends on hav-
ing some reasonable category knowledge in order to
develop its clusters?leading to a chicken-and-egg
problem. However, once started, its performance im-
proves well beyond the word-based model?s plateau.
These results suggest that on its own, each compo-
nent of the model may be effectively throwing away
useful information. By combining the two models,
the combination model appears to gain complemen-
tary benefits from each component, outperforming
both. The word-based component helps to create a
base of reliable clusters, which the bootstrap compo-
nent uses to continue development.
After all of the training text, the combination
model uses 411 clusters to categorize the test tokens
(compared to over 2,000 at the first test point). While
this seems excessive, we note that 92.5% of the test
tokens are placed in the 25 most populated clusters.3
3See www.cs.toronto.edu/?chris/syncat for examples.
0 1 2 3 4 5 6
x 105
0
0.05
0.1
0.15
0.2
Training set size (words)
R a
dj
Combination
Word?based
Bootstrap
Figure 1: Adjusted Rand Index of each of three mod-
els? clusterings of the test set, as compared with the
PoS tags of the test data.
5 Experiment 2: Learning Trends
A common trend observed in children is that differ-
ent syntactic categories are learned at different rates.
Children appear to have learned the category of nouns
by 23 months of age, verbs shortly thereafter, and
adjectives relatively late (Kemp et al, 2005). Our
goal in this experiment is to look for these specific
trends in the behaviour of our model. We thus simu-
late an experiment where a child uses a novel word?s
linguistic context to infer its syntactic category (e.g.,
Tomasello et al, 1997). For our experiment, we ran-
domly generate input frames with novel head words
using contexts associated with nouns, verbs, and ad-
jectives, then examine the model?s categorization in
each case. We expect that our model should approxi-
mate the developmental trends of children, who tend
to learn the category of ?noun? before ?verb,? and both
of these before ?adjective.?
5.1 Methods
We generate new input frames using the most com-
mon syntactic patterns in the training data. For each
of the noun, verb, and adjective categories (from the
gold standard), we collect the five most frequent PoS
sequences in which these are used, bounded by the
usual four-word context window. For example, the
Adjective set includes the sequence ?V Det Adj N
null?, where the sentence ends after the N. For each
of the three categories, we generate each of 500 input
frames by sampling one of the five PoS sequences,
weighted by frequency, then sampling words of the
right PoS from the lexicon, also weighted by fre-
quency. We replace the head word with a novel word,
forcing the model to use only the context for cluster-
ing. Since the context words are chosen at random,
most of the word sequences generated will be novel.
This makes the task more difficult, rather than sim-
ply sampling utterances from the corpus, where rep-
93
etitions are common. While a few of the sequences
may exist in the training data, we expect the model
to mostly use the underlying category information to
cluster the frames.
We intend to show that the model uses context to
find the right category for a novel word. To evaluate
the model?s behaviour, we let it categorize each of
the randomly generated frames. We score each frame
as follows: if the frame gets put into a new cluster,
it earns score zero. Otherwise, its score is the pro-
portion of frames in the chosen cluster matching the
correct part of speech (we use a PoS-tagged version
of the training corpus; for example, a noun frame put
into a cluster with 60% nouns would get 0.6). We re-
port the mean score for each of the noun, verb, and
adjective sets. Intuitively, the matching score indi-
cates how well the model recognizes that the given
contexts are similar to input it has seen before. If the
model clusters the novel word frame with others of
the right type, then it has formed a category for the
contextual information in that frame.
We use the full combination model (Eq. (7)) to
evaluate the learning rates of individual parts of
speech. We run the model on the training subset of
the evaluation corpus. After every 10,000 words of
input, we use the model to categorize the 1,500 con-
text frames with novel words (500 frames each for
noun, verb, and adjective). As in experiment 1, the
model does not record these categorizations.
5.2 Results
Figure 2 shows the mean matching scores for each
of the tested parts of speech. Recall that since the
frames each use a novel head word, a higher match-
ing score indicates that the model has learned to cor-
rectly recognize the contexts in the frames. This does
not necessarily mean that the model has learned sin-
gle, complete categories of ?noun,? ?verb,? and ?ad-
jective,? but it does show that when the head word
gives no information, the model can generalize based
on the contextual patterns alone. The model learns
to categorize novel nouns better than verbs until late
in training, which matches the trends seen in children.
Adjectives progress slowly, and show nearly no learn-
ing ability by the end of the trial. Again, this appears
to reflect natural behaviour in children, although the
effect we see here may simply be a result of the over-
all frequency of the PoS types. Over the entire corpus
(development and evaluation), 35.4% of the word to-
kens are nouns and 24.3% are verbs, but only 2.9%
are tagged as adjectives. The model, and similarly a
child, may need much more data to learn adjectives
than is available at this stage.
The scores in Figure 2 tend to fluctuate, partic-
ularly for the noun contexts. This fluctuation cor-
responds to periods of overgeneralization, followed
0 1 2 3 4 5 6
x 105
0
0.05
0.1
0.15
0.2
0.25
Training set size (words)
M
at
ch
in
g 
sc
or
e
Nouns
Verbs
Adjectives
Figure 2: Comparative learning trends of noun, verb,
and adjective patterns.
by recovery (also observed in children; see, e.g.,
Tomasello, 2000). When the model merges two clus-
ters, the contents of the resulting cluster can initially
be quite heterogeneous. Furthermore, the new cluster
is much larger, so it becomes a magnet for new cate-
gorizations. This results in overgeneralization errors,
giving the periodic drops seen in Figure 2. While our
formulation in Section 2.4 aims to prevent such er-
rors, they are likely to occur on occasion. Eventually,
the model recovers from these errors, and it is worth
noting that the fluctuations diminish over time. As the
model gradually improves with more input, the dom-
inant clusters become heavily entrenched, and incon-
sistent merges are less likely to occur.
6 Experiment 3: Disambiguation
The category structure of our model allows a single
word type to be a member of multiple categories. For
example, kiss could belong to a category of predom-
inantly noun usages (Can I have a kiss?) and also
to a category of verb usages (Kiss me!). As a result,
the model easily represents lexical ambiguity. In this
experiment, inspired by disambiguation work in psy-
cholinguistics (see, e.g., MacDonald, 1993), we ex-
amine the model?s ability to correctly disambiguate
category memberships.
6.1 Methods
Given a word that the model has previously seen as
various different parts of speech, we examine how
well the model can use that ambiguous word?s con-
text to determine its category in the current usage.
For example, by presenting the word kiss in sepa-
rate noun and verb contexts, we expect that the model
should categorize kiss as a noun, then as a verb, re-
spectively. We also wish to examine the effect of the
target word?s lexical bias, that is, the predominance of
a word type to be used as one category over another.
As with adults, if kiss is mainly used as a noun, we
expect the model to more accurately categorize the
94
N V N V N V N V N V N V
0
0.1
0.2
0.3
0.4
Po
S 
pr
op
or
tio
n 
in 
ch
os
en
 cl
us
te
rs
Nouns
Verbs
Context:
Noun only Noun biased Equibiased Verb biased Verb only Novel wordWord bias:
Figure 3: Syntactic category disambiguation. Shown are the proportions of nouns and verbs in the chosen
clusters for ambiguous words used in either noun (N) or verb (V) contexts.
word in a noun context than in a verb context.
We focus on noun/verb ambiguities. We artificially
generate input frames for noun and verb contexts as
in experiment 2, with the following exceptions. To
make the most use of the context information, we al-
low no null words in the input frames. We also ensure
that the contexts are distinctive enough to guide dis-
ambiguation. For each PoS sequence surrounding a
noun (e.g., ?V Det head Prep Det?), we ensure that
over 80% of the instances of that pattern in the cor-
pus are for nouns, and likewise for verbs.
We test the model?s disambiguation in six con-
ditions, with varying degrees of lexical bias. Un-
ambiguous (?noun/verb only?) conditions test words
seen in the corpus only as nouns or verbs (10 words
each). ?Biased? conditions test words with a clear
bias (15 with average 93% noun bias; 15 with aver-
age 84% verb bias). An ?equibiased? condition uses 4
words of approximately equal bias, and a novel word
condition provides an unbiased case.
For the six sets of test words, we measure the ef-
fect of placing each of these words in both noun and
verb contexts. That is, each word in each condition
was used as the head word in each of the 500 noun
and 500 verb disambiguating frames. For example,
we create 500 frames where book is used as a noun,
and 500 frames where it is used as a verb. We then
use the fully-trained ?combination? model (Eq. (7)) to
categorize each frame. Unlike in the previous experi-
ment, we do not let the model create new clusters. For
each frame, we choose the best-fitting existing clus-
ter, then examine that cluster?s contents. As in ex-
periment 2, we measure the proportions of each PoS
of the frames in this cluster. We then average these
measures over all tested frames in each condition.
6.2 Results
Figure 3 presents the measured PoS proportions for
each of the six conditions. For both the equibias and
novel word conditions, we see that the clusters cho-
sen for the noun context frames (labeled N) contain
more nouns than verbs, and the clusters chosen for
the verb context frames (V) contain more verbs than
nouns. This suggests that although the model?s past
experience with the head word is not sufficiently in-
formative, the model can use the word?s context to
disambiguate its category. In the ?unambiguous? and
the ?biased? conditions, the head words? lexical biases
are too strong for the model to overcome.
However, the results show a realistic effect of the
lexical bias. Note the contrasts from the ?noun only?
condition, to the ?noun biased? condition, to ?equibi-
ased? (and likewise for the verb biases). As the lex-
ical bias weakens, the counter-bias contexts (e.g., a
noun bias with a verb context) show a stronger ef-
fect on the chosen clusters. This is a realistic effect
of disambiguation seen in adults (MacDonald, 1993).
Strongly biased words are more difficult to categorize
in conflict with their bias than weakly biased words.
7 Related Work
Several existing computational models use distribu-
tional cues to find syntactic categories. Schu?tze
(1993) employs co-occurrence statistics for common
words, while Redington et al (1998) build word dis-
tributional profiles using corpus bigram counts. Clark
(2000) also builds distributional profiles, introducing
an iterative clustering method to better handle am-
biguity and rare words. Mintz (2003) shows that
even very simple three-word templates can effec-
tively define syntactic categories. Each of these mod-
els demonstrates that by using the kinds of simple in-
formation to which children are known to be sensi-
tive, syntactic categories are learnable. However, the
specific learning mechanisms they use, such as the
hierarchical clustering methods of Redington et al
(1998), are not intended to be cognitively plausible.
In contrast, Cartwright and Brent (1997) propose
95
an incremental model of syntactic category acquisi-
tion that uses a series of linguistic preferences to find
common patterns across sentence-length templates.
Their model presents an important incremental al-
gorithm which is very effective for discovering cat-
egories in artificial languages. However, the model?s
reliance on templates limits its applicability to tran-
scripts of actual spoken language data, which contain
high variability and noise.
Recent models that apply Bayesian approaches
to PoS tagging are not incremental and assume a
fixed number of tags (Goldwater and Griffiths, 2007;
Toutanova and Johnson, 2008). In syntactic cate-
gory acquisition, the true number of categories is un-
known, and must be inferred from the input.
8 Conclusions and Future Directions
We have developed a computational model of syn-
tactic category acquisition in children, and demon-
strated its behaviour on a corpus of naturalistic child-
directed data. The model is based on domain-general
properties of feature similarity, in contrast to earlier,
more linguistically-specific methods. The incremen-
tal nature of the algorithm contributes to a substantial
improvement in psychological plausibility over pre-
vious models of syntactic category learning. Further-
more, due to its probabilistic framework, our model
is robust to noise and variability in natural language.
Our model successfully uses a syntactic bootstrap-
ping mechanism to build on the distributional proper-
ties of words. Using its existing partial knowledge
of categories, the model applies a second level of
analysis to learn patterns in the input. By making
few assumptions about prior linguistic knowledge,
the model develops realistic syntactic categories from
the input data alone. The explicit bootstrapping com-
ponent improves the model?s ability to learn adult cat-
egories, and its learning trajectory resembles relevant
behaviours seen in children. Using the contextual
patterns of individual parts of speech, we show dif-
ferential learning rates across nouns, verbs, and ad-
jectives that mimic child development. We also show
an effect of a lexical bias in category disambiguation.
The algorithm is currently only implemented as an
incremental process. However, comparison with a
batch version of the algorithm, such as by using a
Gibbs sampler (Sanborn et al, 2006), would help us
further understand the effect of incrementality on lan-
guage fidelity.
While we have only examined the effects of learn-
ing categories from simple distributional information,
the feature-based framework of our model could eas-
ily be extended to include other sources of informa-
tion, such as morphological and phonological cues.
Furthermore, it would also be possible to include se-
mantic features, thereby allowing the model to draw
on correlations between semantic and syntactic cate-
gories in learning.
Acknowledgments
We thank Afra Alishahi for valuable discussions,
and the anonymous reviewers for their comments.
We gratefully acknowledge the financial support of
NSERC of Canada and the University of Toronto.
References
Alishahi, A. and S. Stevenson 2008. A computational
model for early argument structure acquisition. Cog-
nitive Science, 32(5).
Anderson, J. R. 1991. The adaptive nature of human cate-
gorization. Psychological Review, 98(3):409?429.
Cartwright, T. A. and M. R. Brent 1997. Syntactic catego-
rization in early language acquisition: formalizing the
role of distributional analysis. Cognition, 63:121?170.
Clark, A. 2000. Inducing syntactic categories by context
distribution clustering. In CoNLL2000, pp. 91?94.
Goldwater, S. and T. L. Griffiths 2007. A fully bayesian
approach to unsupervised part-of-speech tagging. In
Proc. of ACL2007, pp. 744?751.
Hubert, L. and P. Arabie 1985. Comparing partitions.
Journal of Classification, 2:193?218.
Kemp, N., E. Lieven, and M. Tomasello 2005. Young chil-
dren?s knowledge of the ?determiner? and ?adjective?
categories. J. Speech Lang. Hear. R., 48:592?609.
MacDonald, M. C. 1993. The interaction of lexical and
syntactic ambiguity. J. Mem. Lang., 32:692?715.
MacWhinney, B. 2000. The CHILDES Project: Tools for
analyzing talk, volume 2: The Database. Lawrence Erl-
baum, Mahwah, NJ, 3 edition.
Mintz, T. H. 2003. Frequent frames as a cue for gram-
matical categories in child directed speech. Cognition,
90:91?117.
Olguin, R. and M. Tomasello 1993. Twenty-five-month-
old children do not have a grammatical category of verb.
Cognitive Development, 8:245?272.
Onnis, L. and M. H. Christiansen 2005. New beginnings
and happy endings: psychological plausibility in com-
putational models of language acquisition. CogSci2005.
Redington, M., N. Chater, and S. Finch 1998. Distribu-
tional information: A powerful cue for acquiring syn-
tactic categories. Cognitive Science, 22(4):425?469.
Sanborn, A. N., T. L. Griffiths, and D. J. Navarro 2006. A
more rational model of categorization. CogSci2006.
Schu?tze, H. 1993. Part of speech induction from scratch.
In Proc. of ACL1993, pp. 251?258.
Theakston, A. L., E. V. Lieven, J. M. Pine, and C. F. Row-
land 2001. The role of performance limitations in the
acquisition of verb-argument structure: an alternative
account. J. Child Lang., 28:127?152.
Tomasello, M. 2000. Do young children have adult syn-
tactic competence? Cognition, 74:209?253.
Tomasello, M., N. Akhtar, K. Dodson, and L. Rekau 1997.
Differential productivity in young children?s use of
nouns and verbs. J. Child Lang., 24:373?387.
Toutanova, K. and M. Johnson 2008. A Bayesian LDA-
based model for semi-supervised part-of-speech tag-
ging. In NIPS2008.
96
Proceedings of the ACL 2011 Workshop on Relational Models of Semantics (RELMS 2011), pages 72?80,
Portland, Oregon, USA, June 23, 2011. c?2011 Association for Computational Linguistics
Incorporating Coercive Constructions into a Verb Lexicon 
Claire Bonial*, Susan Windisch Brown*, Jena D. Hwang*, Christopher Parisien**, 
Martha Palmer* and Suzanne Stevenson** 
*Department of Linguistics, University of Colorado at Boulder 
**Department of Computer Science, University of Toronto 
{Claire.Bonial, Susan.Brown, hwangd, Martha.Palmer}@colorado.edu 
{chris, suzanne}@cs.toronto.edu 
 
 
Abstract 
We take the first steps towards augmenting a lexical 
resource, VerbNet, with probabilistic information 
about coercive constructions. We focus on CAUSED-
MOTION as an example construction occurring with 
verbs for which it is a typical usage or for which it 
must be interpreted as extending the event semantics 
through coercion, which occurs productively and adds 
substantially to the relational semantics of a verb. 
However, through annotation we find that VerbNet 
fails to accurately capture all usages of the 
construction. We use unsupervised methods to 
estimate  probabilistic measures from corpus data for 
predicting usage of the construction across verb 
classes in the lexicon and evaluate against VerbNet. 
We discuss how these methods will form the basis for 
enhancements for VerbNet supporting more accurate 
analysis of the relational semantics of a verb across 
productive usages. 
1 Introduction  
Automatic semantic analysis has been very successful 
when taking a supervised learning approach on data 
labeled with sense tags and semantic roles (e.g., see 
M?rquez et al, 2008). Underlying these recent successes 
are lexical resources, such as PropBank (Palmer et al, 
2005), VerbNet (Kipper et al, 2008), and FrameNet 
(Baker et al, 1998; Fillmore et al, 2002), which encode 
the relational semantics of numerous lexical items, 
especially verbs. However, because authors and speakers 
use verbs productively in previously unseen ways, 
semantic analysis systems must not be limited to direct 
extrapolation from previously seen usages licensed by 
static lexical resources (cf. Pustejovsky & Jezek, 2008). 
To achieve more accurate semantic analyses, we must 
augment such resources with knowledge of the 
extensibility of verbs. 
Central to verb extensibility is the process of semantic 
and syntactic coercion. Coercion allows a verb to be used 
in ?atypical? contexts that extend its relational semantics, 
thereby enabling expression of a novel concept, or simply 
more fluid expression of a complex concept. For 
example, consider a strictly intransitive action verb such 
as blink. This verb may instead be used in a construction 
with an object, as in She blinked the snow off her lashes, 
leading to an interpretation of the verb in which the object 
is causally affected and changes location (the CAUSED-
MOTION construction; Goldberg, 1995). This type of 
constructional coercion is common in language and 
underlies much extensibility of verb usages. 
Understanding such coercive processes thus has 
significant impact on how we should represent 
knowledge about verbs in a lexical resource. 
Importantly, constructional coercion is not an all-or-
nothing process ? a word must be semantically and 
syntactically compatible in some respects with a context 
in order for its use to be extended to that context, but the 
restrictions on compatibility are not hard-and-fast rules 
(Langacker, 1987; Kay & Fillmore, 1999; Goldberg, 
2006; Goldberg, to appear). Gradience of compatibility 
plays an important role in coercion, suggesting that a 
probabilistic approach may be necessary for encoding 
knowledge of constructional coercion in a verb lexicon 
(cf. Lapata & Lascarides, 2003). 
Our hypothesis here is that, due to this gradient process 
of productivity, existing verb lexicons do not adequately 
capture the actual patterns of use of extensible 
constructions. In this paper, we focus on the CAUSED-
MOTION (CM) construction as an initial test case. We first 
annotate the classes of an extensive verb lexicon, 
VerbNet, as to whether the CM construction is allowed 
for all, some, or none of the verbs in the class, noting 
additionally whether it is a typical or coerced usage. We 
find that many of the classes that allow the construction 
for at least some verbs do not include the CM frame in 
their definition, indicating a significant shortcoming in the 
relational knowledge encoded in the lexicon. Next, we 
72
develop probabilistic measures for determining to what 
degree a class is likely to admit the CM construction. We 
then test our measures over corpus data, manually 
annotated for use of the CM construction. Finally, we 
present preliminary work on automatic techniques for 
calculating the proposed measures in an unsupervised 
way, to avoid the need for expensive manual annotation. 
This work forms the preliminary steps toward empirically 
augmenting VerbNet?s predictive capabilities concerning 
the event semantics of verbs in coercible constructions. 
2 Extensible Constructions and VerbNet 
Construction grammar has much insight to offer on the 
topic of productivity and on the resulting statistical 
patterns and gradience of usages (e.g., Langacker, 1987; 
Kay & Fillmore, 1999; Goldberg, 2006). A construction 
is formally defined to be any pairing of linguistic form 
(e.g., a syntactic frame) and meaning. Words can be used 
in constructions to the extent that their lexical semantics is 
compatible with ? or can be coerced to be compatible 
with ? the semantic constraints on the construction. 
It is this notion of constructional coercion, and degree 
of coercibility, that accounts for the richness of usages 
that go beyond those thought of as typical or definitional 
for a verb: by coercing a verb not normally associated 
with a particular frame to occur in it, the meaning of the 
event can take on additional properties not considered a 
core part of the verb?s semantics. For example, in the case 
of the sentence discussed above, She blinked the snow off 
her lashes, it is not the verb but rather the CM 
construction itself that licenses the direct object and adds 
the notion of ?motion causally affecting the object? to the 
event semantics. Amongst other examples of well-known 
constructional coercions are: (1) The CAUSE-RECEIVE 
construction has the syntactic form of NP-V-NP-NP. For 
example, in Bob painted Sally a picture, the simple 
transitive verb paint gains the CAUSE TO RECEIVE sense, 
in which Sally is the recipient and the picture is the 
transferred item. (2) The WAY construction has the form 
of NP-V-[POSS way]-PP. For example, in Frank found 
his way to New York, the construction allows the verb 
find to gain a motion reading (i.e., ?Frank traveled to New 
York?) that would not otherwise be allowed (e.g., *Frank 
found to New York).  
Recognizing such extensions to the relational 
semantics of verbs is very important for accurate 
semantic interpretation in NLP. However, precise 
specifications for capturing the notion of coercible 
constructions, such as are needed for a computational 
resource, have heretofore been lacking. 
2.1 VerbNet & Knowledge of Constructions 
Computational verb lexicons are key to supporting NLP 
systems aimed at semantic interpretation. Verbs express 
the semantics of an event being described as well as the 
relational information among participants in that event, 
and project the syntactic structures that encode that 
information. Verbs are also highly variable, displaying a 
rich range of semantic and syntactic behavior. 
Verb classifications help NLP systems to deal with 
this complexity by organizing verbs into groups that 
share core semantic and syntactic properties. For 
example, VerbNet (derived from Levin?s [1993] work, 
Kipper et al, 2008) is widely used for a number of 
semantic processing tasks, including semantic role 
labeling (Swier and Stevenson, 2004), the creation of 
semantic parse trees (Shi and Mihalcea, 2005), and 
implicit argument resolution (Gerber and Chai, 2010). 
The detailed semantic predicates listed with each 
VerbNet class also have the potential to contribute to text-
specific semantic representations and, thereby, to tasks 
requiring inferencing (Zaenen et al, 2008; Palmer et al, 
2009). 
VerbNet identifies semantic roles and syntactic 
patterns characteristic of the verbs in each class makes 
explicit the connections between the syntactic patterns 
and the underlying semantic relations that can be inferred 
for all members of the class. Each syntactic frame in a 
class has a corresponding semantic representation that 
details the semantic relations between event participants 
across the course of the event. For example, one of the 
characteristic patterns listed for the Pour class is a 
CAUSED-MOTION pattern, which accounts for sentences 
like She poured water from the pitcher into the bowl. This 
is represented in VerbNet as follows: 
Syntactic representation: 
NP V NP PP PP 
Agent V Theme Source Location 
Semantic representation: 
MOTION (DURING(E), THEME)  
NOT (PREP (START(E), THEME, LOCATION)) 
PREP (START(E), THEME, SOURCE) 
PREP (END(E), THEME, LOCATION) 
CAUSE (AGENT, E) 
This representation details connections between the 
syntax and semantics using the semantic roles as links, 
indicating that the Agent is the Subject NP and has 
CAUSED the Event, and that the Theme is the Object NP 
and has a new LOCATION at the end of the event. These 
types of inferences provide the foundation for deep 
semantic analysis of text.  
73
However, the specifications in VerbNet (as in other 
predicate lexicons, such as FrameNet, Baker et al, 1998; 
Fillmore et al, 2002) are seen as definitional ? they are 
restricted to the core usages of the verbs that are valid for 
all verbs in the class. However, as noted above, people 
often use verbs productively, in ways that go beyond the 
boundaries of the verb class structure. It is important to 
correctly identify these productive usages when they 
occur, since they may be explicitly adding crucial 
inferences. If a construction is not recognized in the form 
of a syntactic frame in VerbNet, such inferences are not 
possible, greatly reducing VerbNet?s utility and coverage. 
For example, creative uses of a verb, such as She blinked 
the snow off her lashes, would have no corresponding 
frame in blink?s class, the Hiccup class.  It contains one 
intransitive frame: 
 NP V 
Agent V 
  
 
BODY_PROCESS (E, AGENT) 
INVOLUNTARY (E, AGENT) 
 
Sentences that coerce the meaning of blink to fit with a 
CM event would currently be misanalysed. One option 
might be to augment the Hiccup class with the CM frame 
from the Pour class, which would ensure that such 
sentences would be analyzed more accurately. However, 
given the productive nature of constructional coercion 
and its widespread applicability, the approach of adding 
any possible pattern to each class is not appropriate: this 
would undermine the definitional distinctions between 
classes and greatly lessen their usefulness.  
Complicating the issue is the phenomenon of regular 
sense extensions (Dang et al, 1998), where what once 
may have been coercion has become entrenched and is 
now seen as a different sense of the verb. For example, 
the verbs in the Push class express the general meaning of 
exerting force on an object, such as She pushed on the 
wall. Often, the exertion of force moves the object, which 
can be expressed in a CM construction such as She 
pushed the box across the room. VerbNet accounts for 
this regular sense extension by including most of the Push 
verbs in the Carry class as well, which has the CM 
construction as one of its frames. Deciding when to 
include a verb in another class based on regular sense 
extensions, when to add a frame for a construction to a 
class, or when to reject the frame as a defining part of a 
class, is made difficult by the graded nature of matches 
between verbs and a construction. Our goal is to maintain 
the advantages of the class structure of VerbNet while 
enhancing it with a graded view of the applicability of a 
construction for each class. Noting the applicability of a 
construction will enable the inclusion of its appropriate 
semantic predicates, and the inferencing over them, 
which are currently not supported. 
3 Our Proposal: Constructional Profiles 
We aim to augment VerbNet with knowledge of 
constructions that are likely to be used extensibly with a 
range of verbs. Such extensible constructions will be core 
usages for some classes (such as the CM for the Pour 
class, as noted above) but will be less characteristic of the 
fundamental semantics of other verb classes (such as CM 
for the Hiccup class). We propose to identify such a 
construction and its varying roles in the different classes 
by using relevant statistics over usages of verbs in a 
corpus ? what we call a constructional profile. 
A constructional profile is a probabilistic assessment 
of the usage of a particular construction by the verbs in a 
class. We developed the following three measures to 
capture the relevant behavior, with the goal of providing 
both type- and token-based views of the behavior of a 
verb class with respect to a target construction: 
P1 Ptype(X|C): probability that a verb type in class C is 
attested in construction X 
P1 gives a type-based assessment, indicating how 
widespread the use of the construction is across the 
verb types in the class. For example, if 8 out of 10 
members of a class appear with the construction, we 
might estimate P1 as 0.8. 
P2 Ptoken(X|C): probability that the instances of a typical 
verb in class C occur in construction X 
P2 gives a token-based assessment, indicating, for a 
typical verb in the class, the relative amount of usage of 
the construction among all usages of the verb. For 
example, to estimate this, we might average across all 
verbs in the class, the percentage of tokens in this 
construction. 
P3: Ptoken(X|X-verbs-in-C): same as P2 but considering 
only verbs that have been attested in construction X 
P3 is the same as P2, but looking only at those verbs in 
the class that have an attested usage of the construction, 
removing verbs without attested usages. 
We hypothesize that these measures will have high 
values for those classes for which the construction should 
be definitional; very low values for those classes that are 
not compatible with the construction; and varying values 
for those classes that allow coerced usages to a greater or 
lesser extent. 
Although these probabilities are intuitively very 
simple, estimating them from corpus data poses a 
significant challenge. Since a construction is a pairing of 
form with meaning, recognizing the use of a particular 
74
construction is not simply a matter of determining the 
syntactic pattern of the usage; rather, certain semantic 
properties and relations must co-occur with the syntactic 
pattern. Earlier work has shown that a supervised learning 
method was able to discriminate potential usages of the 
CM construction given training sentences manually 
labeled as either CM or not (Hwang et al, 2010). Here, 
we aim instead to identify usages of the CM construction, 
but without requiring an expensive manual annotation 
effort. That is, we seek an unsupervised method for 
estimating the probabilities in P1?P3 above. 
We approach this goal in steps as follows. First, we 
examine all the classes in VerbNet to see which allow the 
CM construction (Section 4). This anno-tation reveals 
shortcomings in VerbNet?s representa-tion (classes that 
allow the CM construction but do not list it) and also 
provides a gold standard with which to evaluate our 
method of identifying an exten-sible construction using 
our constructional profiles. Second, we use the manually 
annotated CM construction data from Hwang et al 
(2010) to estimate probabilities P1?P3 using maximum 
likelihood formulations (Section 5). An analysis of the 
predictive power of these constructional profile measures 
shows a good match with the distinctions made in the 
human annotation of the classes. Thus, our annotation 
based constructional profile measures show promise for 
identifying relevant behaviors of the construction across 
the classes. Third, we explore automatic methods for 
estimating the constructional profile measures without the 
need for manual annotations (Section 6). We use a 
hierarchical Bayesian model that learns verb classes from 
corpus data to provide unsupervised estimates of the 
constructional profiles, which also exhibit the relevant 
distinctions across the classes. 
4 Annotating the VerbNet Resource  
We begin with a manual examination of the resource and 
a thorough annotation of the status of each class with 
respect to the CM construction. This effort reveals a 
number of shortcomings in VerbNet, and the need for 
developing methods that can support the extension of 
VerbNet to better reflect the coercive uses of 
constructions across the classes. The annotation described 
here also forms the basis for the evaluation in the 
following sections of our new probabilistic measures, by 
motivating hypotheses about the expected patterns of use 
of the CM construction across the classes. 
4.1 Annotation Guidelines and Results 
The first goal of our manual annotation of VerbNet 
classes was to determine which classes currently 
represent CM in one of their frames. To this end, we 
identified which classes contain the following frame:  
NP [Agent/Cause]-V-NP [Patient/Theme]- 
PP [Source/Destination/Recipient/Location]  
These frames correspond to classes such as Slide, with its 
frame NP-V-NP-PP.Destination: Carla slid the books to 
the floor. We also examined classes with the patterns NP-
V-NP-PP.Oblique, NP-V-NP-PP. Theme2, and NP-V-
NP-PP.Patient2. In these classes, annotators had to judge 
whether the final PP was compatible with CM. For 
example, the Breathe class contains the frame NP-V-
NP.Theme-PP.Oblique, The dragon breathed fire on 
Mary, which is compatible with CM; whereas the same 
basic frame in the Other_cos class is not: NP V NP 
PP.Oblique, The summer sun tanned her skin to a golden 
bronze. 
In addition, we annotated which classes were 
potentially compatible with CM for either all verbs in the 
class or only some verbs. The "some" classification has 
the drawback that it may be applied to classes with very 
different proportions of compatible verbs; while suitable 
for our exploratory work here, we plan to make finer 
distinctions in the future. A secondary determination was 
whether or not the class was compatible with CM as part 
of its core semantics, or if it was compatible with CM 
because it was coercible into the construction. A verb was 
considered ?compatible with CM? and ?not coerced? if 
the verb could be used in the CM construction and its 
semantics, as reflected in VerbNet?s semantic predicates, 
involved a CAUSE predicate in combination with another 
predicate such as CONTACT, TRANSFER, (EN)FORCE, 
EMIT, TAKE_IN (predicates potentially involving 
movement along some path). For example, although CM 
is not already included as a frame for the Bend class 
containing the verb fold, the semantics of this class 
include CAUSE and CONTACT, and the verb can be used 
in a CM construction: She folded the note into her 
journal. Therefore, this class would have been considered 
?compatible with CM? but ?not coerced?. Conversely, a 
verb was considered ?compatible with CM? and 
?coerced? if the verb could be used in the CM 
construction, yet its semantics, again as reflected in 
VerbNet, did not involve CAUSE and MOVEMENT 
ALONG A PATH (e.g., the verb wiggle of the 
Body_internal_motion class: She wiggled her foot out of 
the boot). 
In summary, as presented in the table below, we 
annotated each class according to whether (1) the CM 
construction was already represented in VerbNet for this 
class, (2) the construction was possible for all, some, or 
75
none of the verbs in that class, and (3) the verbs of any 
class compatible with CM were coerced into the 
construction or not. The classification for (3) was made 
regardless of whether ?all? verbs or only ?some? were 
compatible with CM. This determination was made 
uniformly for a class: there were no classes in which only 
certain CM-compatible verbs were considered ?coerced?.  
VN class example  
[# of classes like this] 
CM in 
VN 
CM is 
possible 
CM is 
coerced 
Banish [50] Yes All No 
Nonverbal_Expression [2] Yes All Yes 
Cheat [6] Yes Some No 
Exhale [18] No All No 
Hiccup [30] No All Yes 
Fill [46] No Some No 
Wish [54] No Some Yes 
Matter [64] No None N/A 
Notably, we identified 206 classes where at least some of 
the verbs in that class are compatible with the CM 
construction; however, VerbNet currently only 
recognizes the CM construction in 58 classes. There were 
several classes of interest: First, although it may seem 
unusual that CM is represented in 6 classes where we 
found that only ?some? verbs were compatible with CM 
(e.g., Cheat class), these were cases where only more 
restricted subclasses are compatible with CM, and this 
syntactic frame is listed for that subclass. This suggests 
subclasses may provide a more precise characterization 
of which verbs are compatible with a construction.  
Secondly, we identified 18 classes in which all verbs 
were compatible with CM without coercion; thus, these 
classes could likely be improved by the addition of the 
CM syntactic frame. Additionally, we found 30 classes in 
which all verbs are coercible into the CM construction; 
however, the actual likelihood of a verb in those classes 
occurring in a CM construction remains to be 
investigated in the following sections. Like those classes 
where it was determined that only ?some? verbs are 
compatible with CM, usefully incorporating the CM 
construction into classes that require coercion relies on 
accurately determining the probability that verbs in those 
classes will actually appear in the CM construction.  
For those classes in which ?all? verbs are compatible 
with CM, our intuition was that some aspect of the verb?s 
semantics either inherently includes or allows the verb to 
be coerced into the CM construction. Conversely, for 
those classes in which no verbs are compatible with CM, 
presumably some aspect of the verb?s semantics is 
logically incompatible with CM. Although pinpointing 
precisely what aspect of a verb?s semantics makes it 
compatible with CM may not be possible, we can 
investigate whether or not our intuitions are supported by 
examining the actual frequencies of CM constructions for 
given verbs or a given class.  
4.2 Hypotheses  
Using these annotations, we were able to develop two 
simple hypotheses. 
Hypothesis 1: We expect the constructional profile 
measures for the CM construction in a given corpus to be 
highest for those classes in which all verbs were found to 
be compatible with CM; lower for classes in which only 
some verbs were found to be compatible; and lowest for 
classes in which no verbs were found to be compatible. 
Hypothesis 2: We expect the constructional profile 
measures for the CM construction in a given corpus to be 
highest for verbs that fall into classes where CM is not 
considered coerced (for either some or all of the verbs in 
the class); lower for verbs that fall into classes in which 
the CM construction only works through coercion (for 
either some or all of the verbs in the class); and lowest for 
verbs that fall into classes in which no verbs are 
compatible with CM.  
To investigate Hypothesis 1, we grouped the annotated 
classes according to whether all, some, or no verbs in the 
class are compatible with CM: 
 Class example # of classes 
Allowed by All Bring, Carry 106 
Allowed by Some Appoint, Lodge 100 
Allowed by None Try, Own 64 
To investigate Hypothesis 2, we did a second grouping 
of the classes according to whether CM is not coerced, 
CM is coerced, or CM is simply not compatible with the 
class. This second grouping did not distinguish whether 
CM was compatible with ?all? or ?some? of the verbs in 
a given class. 
 Class example # of classes 
Not Coerced Put, Throw 120 
Coerced Floss, Wink 86 
Not Compatible Differ 64 
5 Evaluation using Constructional Profiles 
5.1 Annotated data description 
Our research uses the data annotated for Hwang et al 
(2010), in which 1800 instances in the form NP-V-NP-
PP were identified in the Wall Street Journal portion of 
the Penn Treebank II (Marcus et al, 1994). Each instance 
76
of the data was single annotated with one of the two 
labels: CM or non-CM. The annotation guidelines were 
based on the CM analysis of Goldberg (1995). 
Our analysis began with the same data but adopted a 
slightly narrower definition of CM. We diverged from 
the Hwang et al (2010) study in the following two ways: 
(1) sentences where the object NP is an item that is 
created by the event denoted by the verb were not 
considered CM (e.g., Mr. Pilson scribbled a frighteningly 
large figure on a slip of paper, where the figure is created 
through the scribbling event); and (2) sentences in which 
movement is prevented were not considered CM (e.g., 
He kept her at arm?s length). In agreement with Hwang 
et al, our annotation included both metaphorical senses 
(e.g., [It] cast a shadow over world oil markets) and 
literal senses (e.g., The company moved the employees to 
New York) of CM. Our annotation using the narrower 
guidelines resulted in 85.8% agreement with the original 
annotation.1  The distribution of labels in our data is 
21.8% for CM and 78.2% for NON-CM. 
5.2 Annotated data description 
Using statistics over the manually annotated data, we 
calculate maximum likelihood estimates of the three 
constructional profile measures introduced in Section 3, 
as follows. First, let the probability that a verb v is used in 
the CM construction be estimated as: 
P(CM|v,C) = 
#(CM usages of     ) 
#(CM+non-CM usages of    ) 
That is, P(CM|v,C) is estimated as the relative frequency 
of the CM construction for v out of all annotated usages 
of v that are labeled as class C. Now let CCM be all verbs v 
in C with at least one usage annotated as CM; i.e.: 
    *      |  (  |   )    + 
Then we calculate estimates of P1?P3 as: 
P1: Ptype(CM|C) = |CCM |/|C| 
This measure indicates how widespread the use of CM is 
across the verb types in the class. 
P2: Ptoken(CM|C) =,?  (  |   )   - | |?  
The average over all verbs v in C of P(CM|v,C) 
This indicates the relative amount of usage of CM among 
all usages of the verbs in the class.  
P3: Ptoken(CM|v,C) = [?  (  |   ))- |   |       
The average over all verbs v in CCM of P(CM|v,C) 
P3 narrows the P2 measure to only those verbs in the 
                                                          
1We found that 34.0% of the disagreements were directly due to 
the changes in annotation resulting from our two new criteria. 
class for which there is an attested usage of CM. 
5.3 Analysis of the Constructional Profiles 
The tables below provide a summary of the profile 
measures P1-P3 for the groups of VerbNet classes as 
defined in section 4.2. For each group listed, we report 
the averages of P1-P3 over all classes in the group where 
at least one verb in the class occurred in the data 
manually annotated for CM usage. 
 P1 P2 P3 
CM Allowed by All 0.413 0.323 0.437 
CM Allowed by Some 0.087 0.078 0.224 
CM Not Allowed 0.055 0.055 0.083 
As seen here, the constructional profile measures over 
CM in the data corroborate our Hypothesis 1 (Section 
4.2). All three measures on average are highest for the 
classes that fall into the ?all allowed? group, next highest 
for those in the ?some allowed? group, and lowest for the 
?not allowed? classes.  
 P1 P2 P3 
CM Non-Coerced 0.354 0.274 0.418 
CM Coerced 0.091 0.091 0.185 
CM Not Allowed2 0.056 0.056 0.083 
Furthermore, the second table here confirms our 
expectations for Hypothesis 2 (Section 4.2). Again, all 
three measures on average are highest for classes that fall 
into the ?non-coerced? group, next highest for classes in 
the ?coerced? group (in which the construction is 
achievable only through coercion), and lowest for the 
?not allowed? group.  
Thus, our two hypotheses are borne out, showing that 
our constructional profile measures, when estimated over 
manually annotated data, can be useful in capturing 
important distinctions among classes of verbs with regard 
to their usage in an extensible construction such as CM. 
6 Automatic Creation of Constructional 
Profiles Using a Bayesian Model  
Manually annotating a corpus for usages of a con-
struction can be prohibitively expensive, so we also 
investigate the use of automatic methods to estimate 
constructional profile measures. By using a hierarchi-cal 
Bayesian model (HBM) that acquires latent prob-abilistic 
verb classes from corpus data, we provide unsupervised 
                                                          
2 Note the non-zero values result from actual CM verb usages in 
the data belonging to classes believed to be not compatible with 
CM by VerbNet expert annotators. 
77
estimates of the constructional profiles. 
6.1 Overview of Model and Data 
We use the HBM of Parisien & Stevenson (2011), a 
model that automatically acquires probabilistic 
knowledge about verb argument structure and verb 
classes from large-scale corpora. The model is based on a 
large body of research in nonparametric Bayesian topic 
modeling (e.g., Teh et al, 2004), a robust method of 
discovering syntactic and semantic structure in very large 
datasets. For each verb encountered in a corpus, the 
model provides an estimate of the verb?s expected overall 
pattern of usage. By using latent probabilistic verb classes 
to influence these expected usage patterns, the model can, 
for example, estimate the probability that a verb like blink 
might occur in a CM construction, even if no such 
attested usages appear in the corpus. 
In this preliminary study, we use the corpus data from 
Parisien & Stevenson (2011), since the model has been 
trained and evaluated on this data. As that study was 
aimed at modeling facts of child language acquisition, it 
uses child-directed speech from the Thomas corpus 
(Lieven et al, 2009), part of the CHILDES database 
(MacWhinney, 2000). In this preliminary study, we use 
their development dataset containing approx. 170,000 
verb usages, covering approx. 1,400 verb types. (We 
reserve the test set for future experiments.) For each verb 
usage in the input, a number of features are automatically 
extracted that indicate the number and type of syntactic 
arguments occurring with the verb and general semantic 
properties of the verb. The semantic features are drawn 
from the set of VerbNet semantic predicates, such as 
CAUSE, MOTION, and CONTACT. These are automatically 
extracted from all classes compatible with the verb (with 
no sense disambiguation). 
6.2 Measures for Constructional Profiles 
Using the argument structure constructions, verb usage 
patterns and classes learned by the model, we estimate 
the three constructional profile measures in Section 3, as 
follows. First, we note that since the constructions 
acquired by the model are probabilistic in nature, a 
particular CM instance may be a partial match to more 
than one of the model?s constructions.  
For each verb in the input, we consider the likelihood 
of use of the CM construction to be the likelihood of a 
contrived frame intended to capture the important 
properties of a CM usage. FCM is a usage taking a direct 
object and a prepositional phrase, and including the 
semantic features CAUSE and MOTION, with all other 
semantic features left unspecified. For a given verb v, we 
estimate the likelihood of this CM usage, over all 
constructions in the model, as follows: 
 (   | )  ? (   | ) (
 
 | ) 
Here, P(FCM |k) is the likelihood of the CM usage FCM 
being an instance of the probabilistic construction k, and 
P(k|v) is the likelihood that verb v occurs with 
construction k. These component probabilities are 
estimated using the probability distributions acquired by 
the model and averaged over 100 samples from the 
Markov Chain Monte Carlo simulation, as described in 
Parisien & Stevenson (2011). 
Now, we let CCM be the set of verbs in VerbNet class 
C where the expected likelihood of a CM usage is non-
negligible (akin to the set of verbs with attested usage in 
Section 5.2): 
CCM = {v C | P(FCM|v)>? } 
where ? is a small threshold, here 0.0001. Note that since 
v is not disambiguated for class in our data, all usages of v 
contribute to this estimate. 
The estimates of P1-P3 are comparable to those in 
Section 5.2. The difference is that since we are un-able to 
disambiguate individual usages of the verbs, each usage 
of v is considered to belong to all possible classes C of 
which v is a member. P1 is estimated as before; P2 and 
P3 are averages of P(FCM|v). 
6.3 Analysis of the Constructional Profiles 
The tables below provide a summary of the profile 
estimates P1-P3 for the groups of VerbNet classes as 
given in Section 4.2. For each group listed, we report the 
averages of P1-P3 over all classes in the group where at 
least one of the verbs in the class occurred in the training 
input to the model. 
 P1 P2 P3 
All allowed 0.569 0.0180 0.0250 
Some allowed 0.449 0.0106 0.0192 
Not allowed 0.363 0.0044 0.0079 
These profile measures align with the hypotheses in 
Section 4.2 and with the measures based on manually 
annotated data in Section 5.2. The estimates are high-est 
for classes where all verbs permit the CM con-struction, 
second highest for classes where only some permit it, and 
lowest for classes that do not permit it. 
 P1 P2 P3 
CM non-coerced 0.546 0.0178 0.0260 
CM coerced 0.458 0.0095 0.0167 
CM not allowed 0.363 0.0044 0.0079 
78
Again, the overall patterns of the profile measures align 
with Sections 4.2 and 5.2. The profile estimates are 
highest for classes annotated to be non-coerced usages of 
CM, second highest for coerced classes, and lowest for 
?not allowed?.  
The measures show the overall differences among 
classes in the different groups (for both groupings) ? i.e., 
the average behavior among classes in the different 
groups varies as we predicted.  This indicates that the 
measures are tapping into aspects of construction usage 
that are relevant to making the desired distinctions in 
VerbNet, and validates the use of automatic 
techniques.  However, there is a substantial amount of 
variability in these measures across the classes, so we also 
consider how well the estimates can predict the 
appropriate group for individual classes. That is, can we 
automatically predict whether the CM construction can 
be used by all, some, or none of the verbs in a given verb 
class, and can we predict whether such usages are 
coerced? 
We consider the P3 measure as it provides the best 
separation among the class groupings. The tables below 
report precision (P), recall (R) and F-measures (F) for 
each group, where ?all? and ?some? have been collapsed. 
For exploratory purposes, we pick P3 = 0.006 as the 
value that optimizes F-measures of this classification. 
Future work will explore more principled means for 
setting these thresholds. 
 P R F 
CM allowed 0.880 0.742 0.806 
CM not allowed 0.407 0.636 0.497 
Only a 2-way distinction can be made reliably for the 
allowed grouping. The F-score of over 80% for the 
?allowed? label is very promising. The low precision for 
the ?not allowed? case suggests that the model can?t 
generalize sufficiently due to sparse data. 
 P R F 
CM non-coerced 0.691 0.491 0.574 
CM coerced 0.461 0.417 0.438 
CM not allowed 0.406 0.709 0.517 
We use thresholds of P3 = 0.021 to separate non-coerced 
from coerced classes, and P3 = 0.007 to separate coerced 
from not allowed classes. The model estimates show 
moderate success in distinguishing classes with coerced 
vs. non-coerced usage of the CM construction. However, 
our measures simply cannot distinguish non-occurrence 
due to semantic incompatibility from non-occurrence due 
to chance, given the expected low frequency of a novel 
coerced use of a construction.  To separate the allowed 
cases into whether they are coerced or not requires a 
more detailed assessment of the semantic compatibility of 
the class, which means looking at finer-grained features 
of verb usages that are indicative of the semantic 
predicates compatible with the particular construction.  
Moreover, this kind of assessment likely needs to be 
applied on a verb-specific (and not just class-specific) 
level, in order to identify those verbs out of a potentially 
coercible class that are indeed coercible (i.e., identifying 
the coercible verbs in a class labeled as "some allowed"). 
7 Conclusion 
Our investigation demonstrates that VerbNet does not 
currently represent the CM construction for all verbs or 
verb classes that are compatible with this construction, 
and the existing static representation of verbs is 
inadequate for analyzing extensions of verb meaning 
brought about by coercion. The utility of VerbNet would 
be greatly enhanced by an improved representation of 
constructions: specifically, the incorporation of 
probabilities that verbs in a given (sub)class would occur 
in a particular construction, and whether this constitutes a 
regular sense extension. This addition to VerbNet would 
increase the resource?s coverage of syntactic frames that 
are compatible with a given verb, and therefore enable 
appropriate inferences when coercion occurs. We have 
made preliminary steps towards developing this 
probabilistic distribution over both verb instances and 
classes, based on a large corpus. Unsupervised methods 
for estimating the probabilities achieve an F-score of over 
80% in distinguishing the classes that allow the target 
construction. However, making distinctions among 
coerced and non-coerced cases will require us to go 
beyond these class-based probabilities to finer-grained, 
corpus-based assessments of a verb?s semantic 
compatibility with a coercible construction.  
To move beyond these preliminary findings, we must 
therefore shift our focus to the behavior of individual 
verbs. Additionally, to reduce the impact of errors 
resulting from low-frequency verbs and classes, we plan 
to expand our research to more data, specifically the 
OntoNotes TreeBank data (Weischedel et al, 2011). 
Finally, to achieve our ultimate goal of creating a lexicon 
that can flexibly account for a variety of constructions, we 
will examine other constructions as well. While 
determining the set of coercible constructions in a 
language is itself a topic of current research, we propose 
initially to include the widely recognized CAUSE-
RECEIVE and WAY constructions in addition to CM. 
79
References  
Baker, Collin F., Charles J. Fillmore, and John B. Lowe. 1998. 
The Berkeley FrameNet Project. Proceedings of the 17th 
International Conference on Computational Linguistics 
(COLING/ACL-98), pp. 86?90, Montreal. 
Dang, HoaTrang, Karin Kipper, Martha Palmer, and Joseph 
Rosenzweig. 1998. Investigating regular sense extensions 
based on intersective Levin classes. Proceedings of 
COLING-ACL98, pp. 293?299. 
Fillmore, Charles J., Christopher R. Johnson, and Miriam R.L. 
Petruck. 2002. Background to FrameNet. International 
Journal of Lexicography, 16(3):235-250.  
Gerber, Matthew, and Joyce Y. Chai. 2010. Beyond 
NomBank: A study of implicit arguments for nominal 
predicates. Proceedings of the 48th Annual Meeting of the 
Association of Computational Linguistics, pp. 1583?1592, 
Uppsala, Sweden, July. 
Goldberg, A. E. 1995. Constructions: A construction 
grammar approach to argument structure. Chicago: 
University of Chicago Press. 
Goldberg, A. E. 2006. Constructions at work: The nature of 
generalization in language. Oxford: Oxford University 
Press. 
Goldberg, A. E. To appear. Corpus evidence of the viability of 
statistical preemption. Cognitive Linguistics. 
Hwang Jena D., Rodney D. Nielsen and Martha Palmer. 2010. 
Towards a domain-independent semantics: Enhancing 
semantic representation with construction grammar. 
Proceedings of Extracting and Using Constructions in 
Computational Linguistic Workshop, held with NAACL 
HLT 2010, Los Angeles, June. 
Kay, P., and C. J. Fillmore. 1999. Grammatical constructions 
and linguistic generalizations: The What's X Doing Y? 
construction. Language, 75:1?33. 
Kipper, Karin, Anna Korhonen, Neville Ryant, and Martha 
Palmer. 2008. A large-scale classification of English verbs. 
Language Resources and Evaluation Journal, 42:21?40. 
Langacker, R. W. 1987. Foundations of cognitive grammar: 
Theoretical prerequisites. Stanford, CA: Stanford 
University Press. 
Lapata, M., and A. Lascarides. 2003. Detecting novel 
compounds: The role of distributional evidence. 
Proceedings of the 11th Conference of the European 
Chapter of the Association for Computational 
Linguistics(EACL03), pp.235?242. Budapest, Hungary. 
Levin, B. 1993.English Verb Classes and Alternations: A 
Preliminary Investigation. Chicago: Chicago University 
Press.  
 
 
Lieven, E., D. Salomo, and M. Tomasello. 2009. Two-year-
old children?s production of multiword utterances: A 
usage-based analysis. Cognitive Linguistics 20(3):481?507. 
MacWhinney, B. 2000.The CHILDES Project: Tools for 
analyzing talk (3rd ed., Vol. 2: The Database). Erlbaum. 
M?rquez, L., X. Carreras, K. Litkowski, and S. Stevenson. 
2008. Semantic role labeling: An introduction to the special 
issue. Computational Linguistics, 34(2): 145?159. 
Martha Palmer, Jena D. Hwang, Susan Windisch Brown, 
Karin Kipper Schuler and Arrick Lanfranchi. 2009. 
Leveraging lexical resources for the detection of event 
relations. Proceedings of the AAAI 2009 Spring 
Symposium on Learning by Reading, Stanford, CA, March. 
Palmer, Martha, Daniel Gildea, and Paul Kingsbury.2005. 
The Proposition Bank: An annotated corpus of semantic 
roles. Computational Linguistics, 31(1):71?106. 
Parisien, Christopher, and Suzanne Stevenson. 2011. To 
appear in Proceedings of the 33rd Annual Meeting of the 
Cognitive Science Society, Boston, MA, July. 
Pustejovsky, J., and E. Jezek. 2008. Semantic coercion in 
language: Beyond distributional analysis. Italian Journal of 
Linguistics/RivistaItaliana di Linguistica 20(1): 181?214. 
Shi, Lei, and Rada Mihalcea. 2005. Putting pieces together: 
Combining FrameNet, VerbNet and WordNet for robust 
semantic parsing. Proceedings of the 6th International 
Conference on Intelligent Text Processing and 
Computational Linguistics, Mexico City, Mexico. 
Swier, R., and S. Stevenson. 2004. Unsupervised semantic 
role labeling. Proceedings of the 2004 Conf. on Empirical 
Methods in Natural Language Processing, pp. 95?102, 
Barcelona, Spain. 
Teh, Y. W., M. I. Jordan, M. J.Beal, and D. M.Blei.2006. 
Hierarchical Dirichlet processes. Jrnl of the American 
Statistical Asscn, 101(476): 1566?1581. 
Weischedel, R., E. Hovy, M. Marcus, M. Palmer, .R. Belvin, 
S. Pradan, L. Ramshaw and N. Xue. 2011.OntoNotes: A 
Large Training Corpus for Enhanced Processing. In Part 1: 
Data Acquisition and Linguistic Resources of The 
Handbook of Natural Language Processing and Machine 
Translation: Global Automatic Language Exploitation, 
Eds.: Joseph Olive, Caitlin Christianson, John McCary. 
Springer Verlag, pp. 54-63. 
Zaenen, A., C. Condoravdi, and D. G. Bobrow. 2008. The 
encoding of lexical implications in VerbNet. Proceedings 
of LREC 2008, Morocco, May. 
80
