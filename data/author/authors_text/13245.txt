Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 172?180,
Los Angeles, California, June 2010. c?2010 Association for Computational Linguistics
Unsupervised Modeling of Twitter Conversations
Alan Ritter?
Computer Sci. & Eng.
University of Washington
Seattle, WA 98195
aritter@cs.washington.edu
Colin Cherry?
National Research Council Canada
Ottawa, Ontario, K1A 0R6
Colin.Cherry@nrc-cnrc.gc.ca
Bill Dolan
Microsoft Research
Redmond, WA 98052
billdol@microsoft.com
Abstract
We propose the first unsupervised approach
to the problem of modeling dialogue acts in
an open domain. Trained on a corpus of
noisy Twitter conversations, our method dis-
covers dialogue acts by clustering raw utter-
ances. Because it accounts for the sequential
behaviour of these acts, the learned model can
provide insight into the shape of communica-
tion in a new medium. We address the chal-
lenge of evaluating the emergent model with a
qualitative visualization and an intrinsic con-
versation ordering task. This work is inspired
by a corpus of 1.3 million Twitter conversa-
tions, which will be made publicly available.
This huge amount of data, available only be-
cause Twitter blurs the line between chatting
and publishing, highlights the need to be able
to adapt quickly to a new medium.
1 Introduction
Automatic detection of dialogue structure is an im-
portant first step toward deep understanding of hu-
man conversations. Dialogue acts1 provide an
initial level of structure by annotating utterances
with shallow discourse roles such as ?statement?,
?question? and ?answer?. These acts are useful in
many applications, including conversational agents
(Wilks, 2006), dialogue systems (Allen et al, 2007),
dialogue summarization (Murray et al, 2006), and
flirtation detection (Ranganath et al, 2009).
Dialogue act tagging has traditionally followed an
annotate-train-test paradigm, which begins with the
?This work was conducted at Microsoft Research.
1Also called ?speech acts?
design of annotation guidelines, followed by the col-
lection and labeling of corpora (Jurafsky et al, 1997;
Dhillon et al, 2004). Only then can one train a tag-
ger to automatically recognize dialogue acts (Stol-
cke et al, 2000). This paradigm has been quite suc-
cessful, but the labeling process is both slow and
expensive, limiting the amount of data available for
training. The expense is compounded as we con-
sider new methods of communication, which may
require not only new annotations, but new annota-
tion guidelines and new dialogue acts. This issue be-
comes more pressing as the Internet continues to ex-
pand the number of ways in which we communicate,
bringing us e-mail, newsgroups, IRC, forums, blogs,
Facebook, Twitter, and whatever is on the horizon.
Previous work has taken a variety of approaches
to dialogue act tagging in new media. Cohen et al
(2004) develop an inventory of dialogue acts specific
to e-mail in an office domain. They design their in-
ventory by inspecting a large corpus of e-mail, and
refine it during the manual tagging process. Jeong et
al. (2009) use semi-supervised learning to transfer
dialogue acts from labeled speech corpora to the In-
ternet media of forums and e-mail. They manually
restructure the source act inventories in an attempt
to create coarse, domain-independent acts. Each ap-
proach relies on a human designer to inject knowl-
edge into the system through the inventory of avail-
able acts.
As an alternative solution for new media, we pro-
pose a series of unsupervised conversation models,
where the discovery of acts amounts to clustering
utterances with similar conversational roles. This
avoids manual construction of an act inventory, and
allows the learning algorithm to tell us something
about how people converse in a new medium.
172
There is surprisingly little work in unsupervised
dialogue act tagging. Woszczyna and Waibel (1994)
propose an unsupervised Hidden Markov Model
(HMM) for dialogue structure in a meeting schedul-
ing domain, but model dialogue state at the word
level. Crook et al (2009) use Dirichlet process mix-
ture models to cluster utterances into a flexible num-
ber of acts in a travel-planning domain, but do not
examine the sequential structure of dialogue.2
In contrast to previous work, we address the prob-
lem of discovering dialogue acts in an informal,
open-topic domain, where an unsupervised learner
may be distracted by strong topic clusters. We also
train and test our models in a new medium: Twit-
ter. Rather than test against existing dialogue inven-
tories, we evaluate using qualitative visualizations
and a novel conversation ordering task, to ensure our
models have the opportunity to discover dialogue
phenomena unique to this medium.
2 Data
To enable the study of large-data solutions to di-
alogue modeling, we have collected a corpus of
1.3 million conversations drawn from the micro-
blogging service, Twitter. 3 To our knowledge,
this is the largest corpus of naturally occurring chat
data that has been available for study thus far. Sim-
ilar datasets include the NUS SMS corpus (How
and Kan, 2005), several IRC chat corpora (Elsner
and Charniak, 2008; Forsyth and Martell, 2007),
and blog datasets (Yano et al, 2009; Gamon et al,
2008), which can display conversational structure in
the blog comments.
As it characterizes itself as a micro-blog, it should
not be surprising that structurally, Twitter conversa-
tions lie somewhere between chat and blogs. Like
blogs, conversations on Twitter occur in a public en-
vironment, where they can be collected for research
purposes. However, Twitter posts are restricted to be
no longer than 140 characters, which keeps interac-
tions chat-like. Like e-mail and unlike IRC, Twit-
ter conversations are carried out by replying to spe-
cific posts. The Twitter API provides a link from
each reply to the post it is responding to, allowing
2The Crook et al model should be able to be combined with
the models we present here.
3Will be available at http://www.cs.washington.
edu/homes/aritter/twitter_chat/
1 2 3 4 5
0
2
4
6
8
10
12
14
log length
log 
freq
uen
cy
Figure 1: Conversation length versus frequency
accurate thread reconstruction without requiring a
conversation disentanglement step (Elsner and Char-
niak, 2008). The proportion of posts on Twitter that
are conversational in nature are somewhere around
37% (Kelly, 2009).
To collect this corpus, we crawled Twitter using
its publicly available API. We monitored the public
timeline4 to obtain a sample of active Twitter users.
To expand our user list, we also crawled up to 10
users who had engaged in dialogue with each seed
user. For each user, we retrieved all posts, retain-
ing only those that were in reply to some other post.
We recursively followed the chain of replies to re-
cover the entire conversation. A simple function-
word-driven filter was used to remove non-English
conversations.
We crawled Twitter for a 2 month period during
the summer of 2009. The resulting corpus consists
of about 1.3 million conversations, with each con-
versation containing between 2 and 243 posts. The
majority of conversations on Twitter are very short;
those of length 2 (one status post and a reply) ac-
count for 69% of the data. As shown in Figure 1, the
frequencies of conversation lengths follow a power-
law relationship.
While the style of writing used on Twitter is
widely varied, much of the text is very similar to
SMS text messages. This is likely because many
users access Twitter through mobile devices. Posts
are often highly ungrammatical, and filled with
spelling errors. In order to illustrate the spelling
variation found on Twitter, we ran the Jcluster word
clustering algorithm (Goodman, 2001) on our cor-
4http://twitter.com/public_timeline pro-
vides the 20 most recent posts on Twitter
173
coming comming
enough enought enuff enuf
be4 b4 befor before
yuhr yur your yor ur youur yhur
msgs messages
couldnt culdnt cldnt cannae cudnt couldent
about bou abt abour abut bowt
Table 1: A sample of Twitter spelling variation.
pus, and manually picked out clusters of spelling
variants; a sample is displayed in Table 1.
Twitter?s noisy style makes processing Twitter
text more difficult than other domains. While mov-
ing to a new domain (e.g. biomedical text) is a chal-
lenging task, at least the new words found in the
vocabulary are limited mostly to verbs and nouns,
while function words remain constant. On Twit-
ter, even closed-class words such as prepositions and
pronouns are spelled in many different ways.
3 Dialogue Analysis
We propose two models to discover dialogue acts in
an unsupervised manner. An ideal model will give
insight into the sorts of conversations that happen
on Twitter, while providing a useful tool for later
processing. We first introduce the summarization
technology we apply to this task, followed by two
Bayesian extensions.
3.1 Conversation model
Our base model structure is inspired by the con-
tent model proposed by Barzilay and Lee (2004)
for multi-document summarization. Their sentence-
level HMM discovers the sequence of topics used
to describe a particular type of news event, such as
earthquakes. A news story is modeled by first gen-
erating a sequence of hidden topics according to a
Markov model, with each topic generating an ob-
served sentence according to a topic-specific lan-
guage model. These models capture the sequential
structure of news stories, and can be used for sum-
marization tasks such as sentence extraction and or-
dering.
Our goals are not so different: we wish to dis-
cover the sequential dialogue structure of conversa-
tion. Rather than learning a disaster?s location is
followed by its death toll, we instead wish to learn
that a question is followed by an answer. An initial
a
0
a
1
a
2
w
0,j
w
1,j
w
2,j
W
0
W
1
W
2
C
k
Figure 2: Conversation Model
a
0
a
1
a
2
w
0,j
w
1,j
w
2,j
W
0
W
1
W
2
C
k
s
0,j
s
1,j
s
2,j
?
k
?
E
pi
k
Figure 3: Conversation + Topic Model
conversation model can be created by simply apply-
ing the content modeling framework to conversation
data. We rename the hidden states acts, and assume
each post in a Twitter conversation is generated by
a single act.5 During development, we found that a
unigram language model performed best as the act
emission distribution.
The resulting conversation model is shown as a
plate diagram in Figure 2. Each conversation C is
a sequence of acts a, and each act produces a post,
represented by a bag of words shown using the W
plates. The number of acts available to the model
is fixed; we experimented with between 5 and 40.
Starting with a random assignment of acts, we train
our conversation model using EM, with forward-
backward providing act distributions during the ex-
pectation step. The model structure in Figure 2 is
5The short length of Twitter posts makes this assumption
reasonable.
174
sadly no. some pasta bake, but coffee and pasta bake is not a
contender for tea and toast... .
yum! Ground beef tacos? We ?re grilling out. Turkey dogs for
me, a Bubba Burger for my dh, and combo for the kids.
ha! They gotcha! You had to think about Arby?s to write that tweet.
Arby?s is conducting a psychlogical study. Of roast beef.
Rumbly tummy soon to be tamed by Dominos for lunch! Nom
nom nom!
Table 2: Example of a topical cluster discovered by
the EM Conversation Model.
similar to previous HMMs for supervised dialogue
act recognition (Stolcke et al, 2000), but our model
is trained unsupervised.
3.2 Conversation + Topic model
Our conversations are not restricted to any partic-
ular topic: Twitter users can and will talk about
anything. Therefore, there is no guarantee that our
model, charged with discovering clusters of posts
that aid in the prediction of the next cluster, will nec-
essarily discover dialogue acts. The sequence model
could instead partition entire conversations into top-
ics, such as food, computers and music, and then pre-
dict that each topic self-transitions with high proba-
bility: if we begin talking about food, we are likely
to continue to do so. Since we began with a content
model, it is perhaps not surprising that our Conversa-
tion Model tends to discover a mixture of dialogue
and topic structure. Several high probability posts
from a topic-focused cluster discovered by EM are
shown in Table 2. These clusters are undesirable, as
they have little to do with dialogue structure.
In general, unsupervised sentence clustering tech-
niques need some degree of direction when a par-
ticular level of granularity is desired. Barzilay and
Lee (2004) mask named entities in their content
models, forcing their model to cluster topics about
earthquakes in general, and not instances of specific
earthquakes. This solution is not a good fit for Twit-
ter. As explained in Section 2, Twitter?s noisiness
resists off-the-shelf tools, such as named-entity rec-
ognizers and noun-phrase chunkers. Furthermore,
we would require a more drastic form of prepro-
cessing in order to mask all topic words, and not
just alter the topic granularity. During development,
we explored coarse methods to abstract away con-
tent while maintaining syntax, such as replacing to-
kens with either parts-of-speech or automatically-
generated word clusters, but we found that these ap-
proaches degrade model performance.
Another approach to filtering out topic informa-
tion leaves the data intact, but modifies the model
to account for topic. To that end, we adopt a Latent
Dirichlet Allocation, or LDA, framework (Blei et al,
2003) similar to approaches used recently in sum-
marization (Daume? III and Marcu, 2006; Haghighi
and Vanderwende, 2009). The goal of this extended
model is to separate content words from dialogue in-
dicators. Each word in a conversation is generated
from one of three sources:
? The current post?s dialogue act
? The conversation?s topic
? General English
The extended model is shown in Figure 3.6 In addi-
tion to act emission and transition parameters, the
model now includes a conversation-specific word
multinomial ?k that represents the topic, as well as a
universal general English multinomial ?E . A new
hidden variable, s determines the source of each
word, and is drawn from a conversation-specific dis-
tribution over sources pik. Following LDA conven-
tions, we place a symmetric Dirichlet prior over
each of the multinomials. Dirichlet concentration
parameters for act emission, act transition, conver-
sation topic, general English, and source become the
hyper-parameters of our model.
The multinomials ?k, pik and ?E create non-local
dependencies in our model, breaking our HMM dy-
namic programing. Therefore we adopt Gibbs sam-
pling as our inference engine. Each hidden vari-
able is sampled in turn, conditioned on a complete
assignment of all other hidden variables through-
out the data set. Again following LDA convention,
we carry out collapsed sampling, where the various
multinomials are integrated out, and are never ex-
plicitly estimated. This results in a sampling se-
quence where for each post we first sample its act,
and then sample a source for each word in the post.
The hidden act and source variables are sampled ac-
cording to the following transition distributions:
6This figure omits hyperparameters as well as act transition
and emission multinomials to reduce clutter. Dirichlet priors are
placed over all multinomials.
175
Ptrans(ai|a?i, s,w) ?
P (ai|a?i)
Wi?
j=1
P (wi,j |a, s,w?(i,j))
Ptrans(si,j |a, s?(i,j),w) ?
P (si,j |s?(i,j))P (wi,j |a, s,w?(i,j))
These probabilities can be computed analogously to
the calculations used in the collapsed sampler for a
bigram HMM (Goldwater and Griffiths, 2007), and
those used for LDA (Griffiths and Steyvers, 2004).
Note that our model contains five hyperparame-
ters. Rather than attempt to set them using an ex-
pensive grid search, we treat the concentration pa-
rameters as additional hidden variables and sample
each in turn, conditioned on the current assignment
to all other variables. Because these variables are
continuous, we apply slice sampling (Neal, 2003).
Slice sampling is a general technique for drawing
samples from a distribution by sampling uniformly
from the area under its density function.
3.3 Estimating Likelihood on Held-Out Data
In Section 4.2 we evaluate our models by comparing
their probability on held-out test conversations. As
computing this probability exactly is intractable in
our model, we employ a recently proposed Chibb-
style estimator (Murray and Salakhutdinov, 2008;
Wallach et al, 2009). Chibb estimators estimate the
probability of unseen data, P (w) by selecting a high
probability assignment to hidden variables h?, and
taking advantage of the following equality which
can be easily derived from the definition of condi-
tional probability:
P (w) =
P (w,h?)
P (h?|w)
As the numerator can be computed exactly, this re-
duces the problem of estimating P (w) to the eas-
ier problem of estimating P (h?|w). Murray and
Salakhutdinov (2008) provide an unbiased estimator
for P (h?|w), which is calculated using the station-
ary distribution of the Gibbs sampler.
3.4 Bayesian Conversation model
Given the infrastructure necessary for the Conver-
sation+Topic model described above, it is straight-
forward to also implement a Bayesian version of
of the conversation model described in Section 3.1.
This amounts to replacing the add-x smoothing of
dialogue act emission and transition probabilities
with (potentially sparse) Dirichlet priors, and replac-
ing EM with Gibbs sampling. There is reason to
believe that integrating out multinomials and using
sparse priors will improve the performance of the
conversation model, as improvements have been ob-
served when using a Bayesian HMM for unsuper-
vised part-of-speech tagging (Goldwater and Grif-
fiths, 2007).
4 Experiments
Evaluating automatically discovered dialogue acts
is a difficult problem. Unlike previous work, our
model automatically discovers an appropriate set of
dialogue acts for a new medium; these acts will
not necessarily have a close correspondence to di-
alogue act inventories manually designed for other
corpora. Instead of comparing against human anno-
tations, we present a visualization of the automati-
cally discovered dialogue acts, in addition to mea-
suring the ability of our models to predict post order
in unseen conversations. Ideally we would evaluate
performance using an end-use application such as a
conversational agent; however as this is outside the
scope of this paper, we leave such an evaluation to
future work.
For all experiments we train our models on a set of
10,000 randomly sampled conversations with con-
versation length in posts ranging from 3 to 6. Note
that our implementations can likely scale to larger
data by using techniques such as SparseLDA (Yao
et al, 2009). We limit our vocabulary to the 5,000
most frequent words in the corpus.
When using EM, we train for 100 iterations, eval-
uating performance on the test set at each iteration,
and reporting the maximum. Smoothing parameters
are set using grid search on a development set.
When performing inference with Gibbs Sam-
pling, we use 1,000 samples for burn-in and take
10 samples at a lag of 100. Although using multi-
ple samples introduces the possibility of poor results
due to ?act drift?, we found this not to be a problem
in practice; in fact, taking multiple samples substan-
tially improved performance during development.
Recall that we infer hyperparameters using slice
176
sampling. The concentration parameters chosen in
this manner were always sparse (< 1), which pro-
duced a moderate improvement over an uninformed
prior.
4.1 Qualitative Evaluation
We are quite interested in what our models can tell
us about how people converse on Twitter. To vi-
sualize and interpret our competing models, we ex-
amined act-emission distributions, posts with high-
confidence acts, and act-transition diagrams. Of
the three competing systems, we found the Conver-
sation+Topic model by far the easiest to interpret:
the 10-act model has 8 acts that we found intuitive,
while the other 2 are used only with low probabil-
ity. Conversely, the Conversation model, whether
trained by EM or Gibbs sampling, suffered from
the inclusion of general terms and from the confla-
tion of topic and dialogue. For example, the EM-
trained conversation model discovered an ?act? that
was clearly a collection of posts about food, with no
underlying dialogue theme (see Table 2).
In the remainder of this section, we reproduce
our visualization for the 10-act Conversation+Topic
model. Word lists summarizing the discovered dia-
logue acts are shown in Table 3. For each act, the
top 40 words are listed in order of decreasing emis-
sion probability. An example post, drawn from the
set of highest-confidence posts for that act, is also
included. Figure 4 provides a visualization of the
matrix of transition probabilities between dialogue
acts. An arrow is drawn from one act to the next
if the probability of transition is above 0.15.7 Note
that a uniform model would transition to each act
with probability 0.10. In both Table 3 and Figure 4,
we use intuitive names in place of cluster numbers.
These are based on our interpretations of the clus-
ters, and are provided only to benefit the reader when
interpreting the transition diagram.8
From inspecting the transition diagram (Figure 4),
one can see that the model employs three distinct
acts to initiate Twitter conversations. These initial
acts are quite different from one another, and lead to
7After setting this threshold, two Acts were cut off from the
rest of the graph (had no incoming edges), and were therefore
removed
8In some cases, the choice in name is somewhat arbitrary,
ie: answer versus response, reaction versus comment.
Figure 4: Transitions between dialogue acts. See
table 3 for word lists and example posts for each act
different sets of possible responses. We discuss each
of these in turn.
The Status act appears to represent a post in which
the user is broadcasting information about what they
are currently doing. This can be seen by the high
amount of probability mass given to words like I
and my, in addition to verbs such as go and get, as
well as temporal nouns such as today, tomorrow and
tonight.
The Reference Broadcast act consists mostly of
usernames and urls.9 Also prominent is the word rt,
which has special significance on Twitter, indicating
that the user is re-posting another user?s post. This
act represents a user broadcasting an interesting link
or quote to their followers. Also note that this node
transitions to the Reaction act with high probability.
Reaction appears to cover excited or appreciative re-
sponses to new information, assigning high proba-
bility to !, !!, !!!, lol, thanks, and haha.
Finally Question to Followers represents a user
asking a question to their followers. The presence
of the question mark and WH question words indi-
cate a question, while words like anyone and know
indicate that the user is asking for information or an
opinion. Note that this is distinct from the Question
act, which is in response to an initial post.
Another interesting point is the alternation be-
9As part of the preprocessing of our corpus we replaced all
usernames and urls with the special tokens -usr- and -url-.
177
Status I . to ! my , is for up in ... and going was today so at go get back day got this am but Im now tomorrow night work
tonight off morning home had gon need !! be just getting
I just changed my twitter page bkgornd and now I can?t stop looking at it, lol!!
Question to Followers ? you is do I to -url- what -usr- me , know if anyone why who can ? this or of that how does - : on your are need
any rt u should people want get did have would tell
anyone using google voice? just got my invite, should i?? don?t know what it is? -url- for the video and break
down
Reference Broadcast -usr- ! -url- rt : -usr-: - ? my the , is ( you new ? ? !! ) this for at in follow of on ? lol u are twitter your thanks via
!!! by :) here 2 please check
rt -usr-: -usr- word that mac lip gloss give u lock jaw! lol
Question ? you what ! are is how u do the did your that , lol where why or ?? hey about was have who it in so haha on
doing going know good up get like were for there :) can
DWL!! what song is that??
Reaction ! you I :) !! , thanks lol it haha that love so good too your thank is are u !!! was for :d me -usr- ? hope ? my 3 omg
... oh great hey awesome - happy now aww
sweet! im so stoked now!
Comment you I . to , ! do ? it be if me your know have we can get will :) but u that see lol would are so want go let up well
need - come ca make or think them
why are you in tx and why am I just now finding out about it?! i?m in dfw, till I get a job. i?ll have to come to
Htown soon!
Answer . I , you it ? that ? is but do was he the of a they if not would know be did or does think ) like ( as have what in are
- no them said who say ?
my fave was ?keeping on top of other week?
Response . I , it was that lol but is yeah ! haha he my know yes you :) like too did well she so its ... though do had no - one
as im thanks they think would not good oh
nah im out in maryland, leaving for tour in a few days.
Table 3: Word lists and example posts for each Dialogue Act. Words are listed in decreasing order of
probability given the act. Example posts are in italics.
tween the personal pronouns you and I in the acts
due to the focus of conversation and speaker. The
Status act generates the word I with high probability,
whereas the likely response state Question generates
you, followed by Response which again generates I.
4.2 Quantitative Evaluation
Qualitative evaluations are both time-consuming
and subjective. The above visualization is useful for
understanding the Twitter domain, but it is of little
use when comparing model variants or selecting pa-
rameters. Therefore, we also propose a novel quan-
titative evaluation that measures the intrinsic qual-
ity of a conversation model by its ability to predict
the ordering of posts in a conversation. This mea-
sures the model?s predictive power, while requiring
no tagged data, and no commitment to an existing
tag inventory.
Our test set consists of 1,000 randomly selected
conversations not found in the training data. For
each conversation in the test set, we generate all
n! permutations of the posts. The probability of
each permutation is then evaluated as if it were an
unseen conversation, using either the forward algo-
rithm (EM) or the Chibb-style estimator (Gibbs).
Following work from the summarization community
(Barzilay and Lee, 2004), we employ Kendall?s ? to
measure the similarity of the max-probability per-
mutation to the original order.
The Kendall ? rank correlation coefficient mea-
sures the similarity between two permutations based
on their agreement in pairwise orderings:
? =
n+ ? n?
(n
2
)
where n+ is the number of pairs that share the same
order in both permutations, and n? is the number
that do not. This statistic ranges between -1 and +1,
where -1 indicates inverse order, and +1 indicates
identical order. A value greater than 0 indicates a
positive correlation.
Predicting post order on open-domain Twitter
conversations is a much more difficult task than on
topic-focused news data (Barzilay and Lee, 2004).
We found that a simple bigram model baseline does
very poorly at predicting order on Twitter, achieving
only a weak positive correlation of ? = 0.0358 on
our test data as compared with 0.19-0.74 reported by
Barzilay and Lee on news data.
Note that ? is not a perfect measure of model qual-
ity for conversations; in some cases, multiple order-
178
5 10 15 20 25 30 35 40
EM ConversationConversation+TopicBayesian Conversation
# acts
tau
0.0
0.1
0.2
0.3
0.4
Figure 5: Performance at conversation ordering task.
ings of the same set of posts may form a perfectly
acceptable conversation. On the other hand, there
are often strong constraints on the type of response
we might expect to follow a particular dialogue act;
for example, answers follow questions. We would
expect an effective model to use these constraints to
predict order.
Performance at the conversation ordering task
while varying the number of acts for each model is
displayed in Figure 5. In general, we found that us-
ing Bayesian inference outperforms EM. Also note
that the Bayesian Conversation model outperforms
the Conversation+Topic model at predicting conver-
sation order. This is likely because modeling conver-
sation content as a sequence can in some cases help
to predict post ordering; for example, adjacent posts
are more likely to contain similar content words. Re-
call though that we found the Conversation+Topic
model to be far more interpretable.
Additionally we compare the likelihood of these
models on held out test data in Figure 6. Note that
the Bayesian methods produce models with much
higher likelihood.10 For the EM models, likelihood
tends to decrease on held out test data as we increase
the number of hidden states, due to overfitting.
5 Conclusion
We have presented an approach that allows the
unsupervised induction of dialogue structure from
naturally-occurring open-topic conversational data.
10Likelihood of the test data is estimated using the Chibb
Style estimator described in (Murray and Salakhutdinov, 2008;
Wallach et al, 2009). This method under-estimates likelihood
in expectation. The maximum likelihood (EM) likelihoods are
exact.
5 10 15 20 25 30 35 40
EM ConversationConversation+TopicBayesian Conversation
# acts
negat
ive lo
g like
lihood
3100
003
1500
032
0000
3250
003
3000
033
5000
3400
00
Figure 6: Negative log likelihood on held out test
data (smaller values indicate higher likelihood).
By visualizing the learned models, coherent patterns
emerge from a stew of data that human readers find
difficult to follow. We have extended a conversa-
tion sequence model to separate topic and dialogue
words, resulting in an interpretable set of automat-
ically generated dialogue acts. These discovered
acts have interesting differences from those found
in other domains, and reflect Twitter?s nature as a
micro-blog.
We have introduced the task of conversation or-
dering as an intrinsic measure of conversation model
quality. We found this measure quite useful in
the development of our models and algorithms, al-
though our experiments show that it does not nec-
essarily correlate with interpretability. We have di-
rectly compared Bayesian inference to EM on our
conversation ordering task, showing a clear advan-
tage for Bayesian methods.
Finally, we have collected a corpus of 1.3 million
Twitter conversations, which we will make available
to the research community, and which we hope will
be useful beyond the study of dialogue. In the fu-
ture, we wish to scale our models to the full corpus,
and extend them with more complex notions of dis-
course, topic and community. Ultimately, we hope
to put the learned conversation structure to use in the
construction of a data-driven, conversational agent.
Acknowledgements
We are grateful to everyone in the NLP and TMSN
groups at Microsoft Research for helpful discussions
and feedback. We thank Oren Etzioni, Michael Ga-
mon, Mausam and Fei Wu, and the anonymous re-
viewers for helpful comments on a previous draft.
179
References
James Allen, Nathanael Chambers, George Ferguson,
Lucian Galescu, Hyuckchul Jung, Mary Swift, and
William Taysom. 2007. Plow: a collaborative task
learning agent. In Proceedings of AAAI.
Regina Barzilay and Lillian Lee. 2004. Catching the
drift: Probabilistic content models, with applications
to generation and summarization. In Proceedings of
HLT-NAACL, pages 113?120.
David M. Blei, Andrew Y. Ng, and Michael I. Jordan.
2003. Latent dirichlet alocation. J. Mach. Learn.
Res., 3:993?1022.
William W. Cohen, Vitor R. Carvalho, and Tom M.
Mitchell. 2004. Learning to classify email into
?speech acts?. In Proceedings of EMNLP.
Nigel Crook, Ramon Granell, and Stephen Pulman.
2009. Unsupervised classification of dialogue acts us-
ing a Dirichlet process mixture model. In Proceedings
of SIGDIAL, pages 341?348.
Hal Daume? III and Daniel Marcu. 2006. Bayesian query-
focused summarization. In Proceedings of ACL.
Rajdip Dhillon, Sonali Bhagat, Hannah Carvey, and Eliz-
abeth Shriberg. 2004. Meeting recorder project: Dia-
log act labeling guide. Technical report, International
Computer Science Institute.
Micha Elsner and Eugene Charniak. 2008. You talking
to me? A corpus and algorithm for conversation dis-
entanglement. In Proceedings of ACL-HLT.
Eric N. Forsyth and Craig H. Martell. 2007. Lexical and
discourse analysis of online chat dialog. In Proceed-
ings of ICSC.
Michael Gamon, Sumit Basu, Dmitriy Belenko, Danyel
Fisher, Matthew Hurst, and Arnd Christian Knig.
2008. Blews: Using blogs to provide context for news
articles. In Proceedings of ICWSM.
Sharon Goldwater and Tom Griffiths. 2007. A fully
bayesian approach to unsupervised part-of-speech tag-
ging. In Proceedings of ACL, pages 744?751.
Joshua T. Goodman. 2001. A bit of progress in language
modeling. Technical report.
T. L. Griffiths and M. Steyvers. 2004. Finding scientific
topics. Proc Natl Acad Sci, 101 Suppl 1:5228?5235.
Aria Haghighi and Lucy Vanderwende. 2009. Exploring
content models for multi-document summarization. In
Proceedings of HLT-NAACL, pages 362?370.
Yijue How and Min-Yen Kan. 2005. Optimizing pre-
dictive text entry for short message service on mobile
phones. In Proceedings of HCII.
Minwoo Jeong, Chin-Yew Lin, and Gary Geunbae Lee.
2009. Semi-supervised speech act recognition in
emails and forums. In Proceedings of EMNLP, pages
1250?1259.
Dan Jurafsky, Liz Shriberg, and Debra Biasca. 1997.
Switchboard swbd-damsl shallow-discourse-function
annotation coders manual, draft 13. Technical report,
University of Colorado Institute of Cognitive Science.
Ryan Kelly. 2009. Pear analytics twitter study. Whitepa-
per, August.
Iain Murray and Ruslan Salakhutdinov. 2008. Evalu-
ating probabilities under high-dimensional latent vari-
able models. In Proceedings of NIPS, pages 1137?
1144.
Gabriel Murray, Steve Renals, Jean Carletta, and Johanna
Moore. 2006. Incorporating speaker and discourse
features into speech summarization. In Proceedings of
HLT-NAACL, pages 367?374.
Radford M. Neal. 2003. Slice sampling. Annals of
Statistics, 31:705?767.
Rajesh Ranganath, Dan Jurafsky, and Dan Mcfarland.
2009. It?s not you, it?s me: Detecting flirting and
its misperception in speed-dates. In Proceedings of
EMNLP, pages 334?342.
Andreas Stolcke, Noah Coccaro, Rebecca Bates, Paul
Taylor, Carol Van Ess-Dykema, Klaus Ries, Eliza-
beth Shriberg, Daniel Jurafsky, Rachel Martin, and
Marie Meteer. 2000. Dialogue act modeling for
automatic tagging and recognition of conversational
speech. Computational Linguistics, 26(3):339?373.
Hanna M. Wallach, Iain Murray, Ruslan Salakhutdinov,
and David M. Mimno. 2009. Evaluation methods for
topic models. In Proceedings of ICML, page 139.
Yorick Wilks. 2006. Artificial companions as a new kind
of interface to the future internet. In OII Research Re-
port No. 13.
M. Woszczyna and A. Waibel. 1994. Inferring linguistic
structure in spoken language. In Proceedings of IC-
SLP.
Tae Yano, William W. Cohen, and Noah A. Smith. 2009.
Predicting response to political blog posts with topic
models. In Proceedings of NAACL, pages 477?485.
Limin Yao, David Mimno, and Andrew McCallum.
2009. Efficient methods for topic model inference on
streaming document collections. In Proceedings of
KDD, pages 937?946.
180
Proceedings of NAACL-HLT 2013, pages 416?425,
Atlanta, Georgia, 9?14 June 2013. c?2013 Association for Computational Linguistics
Learning to Relate Literal and Sentimental Descriptions of Visual Properties
Mark Yatskar
Computer Science & Engineering
University of Washington
Seattle, WA
my89@cs.washington.edu
Svitlana Volkova
Center for Language and Speech Processing
Johns Hopkins University
Baltimore, MD
svitlana@jhu.edu
Asli Celikyilmaz
Conversational Understanding Sciences
Microsoft
Mountain View, CA
asli@ieee.org
Bill Dolan
NLP Group
Microsoft Research
Redmond, WA
billdol@microsoft.edu
Luke Zettlemoyer
Computer Science & Engineering
University of Washington
Seattle, WA
lsz@cs.washington.edu
Abstract
Language can describe our visual world at
many levels, including not only what is lit-
erally there but also the sentiment that it in-
vokes. In this paper, we study visual language,
both literal and sentimental, that describes the
overall appearance and style of virtual char-
acters. Sentimental properties, including la-
bels such as ?youthful? or ?country western,?
must be inferred from descriptions of the more
literal properties, such as facial features and
clothing selection. We present a new dataset,
collected to describe Xbox avatars, as well as
models for learning the relationships between
these avatars and their literal and sentimen-
tal descriptions. In a series of experiments,
we demonstrate that such learned models can
be used for a range of tasks, including pre-
dicting sentimental words and using them to
rank and build avatars. Together, these re-
sults demonstrate that sentimental language
provides a concise (though noisy) means of
specifying low-level visual properties.
1 Introduction
Language can describe varied aspects of our visual
world, including not only what is literally there but
also the social, cultural, and emotional sentiment it
invokes. Recently, there has been a growing effort
to study literal language that describes directly ob-
servable properties, such as object color, shape, or
This is a light tan young man
with short and trim haircut. He
has straight eyebrows and large
brown eyes. He has a neat and
trim appearance.
State of mind: angry, upset,
determined. Likes: country
western, rodeo. Occupation:
cowboy, wrangler, horse trainer.
Overall: youthful, cowboy.
Figure 1: (A) Literal avatar descriptions and (B) sen-
timental descriptions of four avatar properties, in-
cluding possible occupations and interests.
category (Farhadi et al, 2009; Mitchell et al, 2010;
Matuszek et al, 2012). Here, we add a focus on
sentimental visual language, which compactly de-
scribes more subjective properties such as if a person
looks determined, if a resume looks professional, or
if a restaurant looks romantic. Such models enable
many new applications, such as text editors that au-
tomatically select properties including font, color, or
text alignment to best match high level descriptions
such as ?professional? or ?artistic.?
416
In this paper, we study visual language, both lit-
eral and sentimental, that describes the overall ap-
pearance and style of virtual characters, like those in
Figure 1. We use literal language as feature norms, a
tool used for studying semantic information in cog-
nitive science (Mcrae et al, 2005). Literal words,
such ?black? or ?hat,? are annotated for objects to in-
dicate how people perceive visual properties. Such
feature norms provide our gold-standard visual de-
tectors, and allow us to focus on learning to model
sentimental language, such as ?youthful? or ?goth.?
We introduce a new corpus of descriptions of
Xbox avatars created by actual gamers. Each avatar
is specified by 19 attributes, including clothing and
body type, allowing for more than 1020 possibil-
ities. Using Amazon Mechanical Turk,1 we col-
lected literal and sentimental descriptions of com-
plete avatars and many of their component parts,
such as the cowboy hat in Figure 1(B). In all, there
are over 100K descriptions. To demonstrate poten-
tial for learning, we also report an A/B test which
shows that native speakers can use sentimental de-
scriptions to distinguish the labeled avatars from
random distractors. This new data will enable study
of the relationships between the co-occurring literal
and sentimental text in a rich visual setting.2
We describe models for three tasks: (i) classify-
ing when words match avatars, (ii) ranking avatars
given a description, and (iii) constructing avatars to
match a description. Each model includes literal part
descriptions as feature norms, enabling us to learn
which literal and sentinel word pairs best predict
complete avatars.
Experiments demonstrate the potential for jointly
modeling literal and sentimental visual descriptions
on our new dataset. The approach outperforms sev-
eral baselines and learns varied relationships be-
tween the sentimental and literal descriptions. For
example, in one experiment ?nerdy student? is pre-
dictive of an avatar with features indicating its shirt
is ?plaid? and glasses are ?large? and faces that are
not ?bearded.? We also show that individual sen-
timental words can be predicted but that multiple
avatars can match a single sentimental description.
Finally, we use our model to build complete avatars
1www.mturk.com
2Data available at http://homes.cs.washington.
edu/?my89/avatar.
and show that we can accurately predict the senti-
mental terms annotators ascribe to them.
2 Related Work
To the best of our knowledge, our focus on learn-
ing to understand visual sentiment descriptions is
novel. However, visual sentiment has been stud-
ied from other perspectives. Jrgensen (1998) pro-
vides examples which show that visual descriptions
communicate social status and story information in
addition to literal object and properties. Tousch et
al. (2012) draw the distinction between ?of-ness?
(objective and concrete) and ?about-ness? (subjec-
tive and abstract) in image retrieval, and observe
that many image queries are abstract (for example,
images about freedom). Finally, in descriptions of
people undergoing emotional distress, Fussell and
Moss (1998) show that literal descriptions co-occur
frequently with sentimental ones.
There has been significant work on more lit-
eral aspects of grounded language understand-
ing, both visual and non-visual. The Words-
Eye project (Coyne and Sproat, 2001) generates
3D scenes from literal paragraph-length descrip-
tions. Generating literal textual descriptions of vi-
sual scenes has also been studied, including both
captions (Kulkarni et al, 2011; Yang et al, 2011;
Feng and Lapata, 2010) and descriptions (Farhadi
et al, 2010). Furthermore, Chen and Dolan (2011)
collected literal descriptions of videos with the
goal of learning paraphrases while Zitnick and
Parikh (2013) describe a corpus of descriptions for
clip art that supports the discovery of semantic ele-
ments of visual scenes.
There has also been significant recent work on au-
tomatically recovering visual attributes, both abso-
lute (Farhadi et al, 2009) and relative (Kovashka et
al., 2012), a challenge that we avoid having to solve
with our use of feature norms (Mcrae et al, 2005).
Grounded language understanding has also re-
ceived significant attention, where the goal is to
learn to understand situated non-visual language
use. For example, there has been work on learning
to execute instructions (Branavan et al, 2009; Chen
and Mooney, 2011; Artzi and Zettlemoyer, 2013),
provide sports commentary (Chen et al, 2010), un-
derstand high level strategy guides to improve game
417
Figure 2: The number of assets per category and ex-
ample images from the hair, shirt and hat categories.
play (Branavan et al, 2011; Eisenstein et al, 2009),
and understand referring expression (Matuszek et
al., 2012).
Finally, our work is similar in spirit to sentiment
analysis (Pang et al, 2002), emotion detection from
images and speech (Zeng et al, 2009), and metaphor
understanding (Shutova, 2010a; Shutova, 2010b).
However, we focus on more general visual context.
3 Data Collection
We gathered a large number of natural language de-
scriptions from Mechanical Turk (MTurk). They in-
clude: (1) literal descriptions of specific facial fea-
tures, clothing or accessories and (2) high level sub-
jective descriptions of human-generated avatars.3
Literal Descriptions We showed annotators a sin-
gle image of clothing, a facial feature or an acces-
sory and asked them to produce short descriptions.
Figure 2 shows the distribution over object types.
We restricted descriptions to be between 3 and 15
words. In all, we collected 33.2K descriptions and
had on average 7 words per descriptions. The ex-
ample annotations with highlighted overlapping pat-
terns are in Table 1.
Sentimental Descriptions We also collected 1913
gamer-created avatars from the web. The avatars
were filtered to contain only items from the set of
665 for which we gathered literal descriptions. The
gender distribution is 95% male.
3(2) also has phrases describing emotional reactions. We
also collected (3) multilingual literal, (4) relative literal and (5)
comprehensive full-body descriptions. We do not use this data,
but it will be included in the public release.
LITERAL DESCRIPTIONS
full-sleeved executive blue shirt
blue , long-sleeved button-up shirt
mens blue button dress shirt with dark blue stripes
multi-blue striped long-sleeve button-up dress
shirt with cuffs and breast pocket
Table 1: Literal descriptions of shirt in Figure 2.
To gather high level sentimental descriptions, an-
notators were presented with an image of an avatar
and asked to list phrases in response to the follow
different aspects:
- State of mind of the avatar.
- Things the avatar might care about.
- What the avatar might do for a living.
- Overall appearance of the avatar.
6144 unique vocabulary items occurred in these
descriptions, but only 1179 occurred more than 10
times. Figure 1 (B) shows an avatar and its corre-
sponding sentimental descriptions.
Quality Control All annotations in our dataset are
produced by non-expert annotators. We relied on
manual spot checks to limit poor annotations. Over
time, we developed a trusted crowd of annotators
who produced only high quality annotations during
the earliest stage of data collection.
4 Feasibility
Our hypothesis is that sentimental language does not
uniquely identify an avatar, but instead summarizes
or otherwise describes its overall look. In general,
there is a trade off between concise and precise de-
scriptions. For example, given a single word you
might be able to generally describe the overall look
of an avatar, but a long, detailed, literal description
would be required to completely specify their ap-
pearance.
To demonstrate that the sentimental descriptions
we collected are precise enough to be predictive
of appearance, we conducted an experiment that
prompts people to judge when avatars match de-
scriptions. We created an A/B test where we show
English speakers two avatars and one sentimental
description. They were asked to select which avatar
is better matched by the description and how dif-
ficult they felt, on a scale from 1 to 4, it was to
judge. For 100 randomly selected descriptions, we
418
 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 1.1 1.2  
1
 1.5
 2
 2.5
 3
 3.5
data 
diffic
ulty l
ess t
han X
Kapp
a vs 
Cum
ulativ
e Dif
ficult
y
game
r is m
ajorit
y lab
el kapp
a
portio
n of d
ata
Figure 3: Judged task difficulty versus agreement,
gamer avatar preference, and percentage of data cov-
ered. The difficulty axis is cumulative.
asked 5 raters to compare the gamer avatars to ran-
domly generated ones (where each asset is selected
independently according to a uniform distribution).
Figure 3 shows a plot of Kappa and the percent of
the time a majority of the raters selected the gamer
avatar. The easiest 20% of the data pairs had the
strongest agreement, with kappa=.92, and two thirds
of the data has kappa = .70. While agreement falls
off to .52 for the full data set, the gamer avatar re-
mains the majority judgment 81% of the time.
The fact that random avatars are sometimes pre-
ferred indicates that it can be difficult to judge sen-
timental descriptions. Consider the avatars in Fig-
ure 4. Neither conforms to a clear sentimental de-
scription based on the questions we asked. The
right one is described with conflicting words and
the words describing the left one are very general
(like ?dumb?). This corresponds to our intuition that
while many avatars can be succinctly summarized
with our questions, some would be more easily de-
scribed using literal language.
5 Tasks and Evaluation
We formulate three tasks to study the feasibility of
learning the relationship between sentimental and
literal descriptions. In this section, we first define
the space of possible avatars, followed by the tasks.
Avatars Figure 5 summarizes the notation we will
develop to describe the data. An avatar is defined by
a 19 dimensional vector ~a where each position is an
State of mind:
playful, happy;
Likes: sex
Occupation: hobo
Overall: dumb
State of mind: content, humble, satisfied,
peaceful, relaxed, calm. Likes: fashion,
friends, money, cars, music, education.
Occupation: teacher, singer, actor,
performer, dancer, computer engineer.
Overall: nerdy, cool, smart, comfy,
easygoing, reserved
Figure 4: Avatars rated as difficult.
index into a list of possible items~i. Each dimension
represents a position on the avatar, for example, hat
or nose. Each possible item is called an asset and
is associated with a set of positions it can fill. Most
assets take up exactly one position, while there are
a few cases where assets take multiple positions.4
An avatar ~a is valid if all of its mandatory positions
are filled, and no two assets conflict on a position.
Mandatory positions include hair, eyes, ears, eye-
brows, nose, mouth, chin, shirt, pants, and shoes.
All other positions are optional. We refer to this set
of valid ~a as A. Practically speaking, if an avatar is
not valid, it cannot be reliably rendered graphically.
Each item i is associated with the literal descrip-
tions ~di ? D where D is the set of literal descrip-
tions. Furthermore, every avatar~a is associated a list
of sentimental query words ~q, describing subjective
aspects of an avatar.5
Sentimental Word Prediction We first study in-
dividual words. The word prediction task is to de-
cide whether a given avatar can be described with a
4For example, long sleeve shirts cover up watches, so they
take up both shirt and wristwear positions. Costumes tend to
span many more positions, for example there a suit that takes
up shirt, pants, wristwear and shoes positions.
5We do not distinguish which prompt (e.g., ?state of mind?
or ?occupation?) a word in ~q came from, although the vocabu-
laries are relatively disjoint.
419
Figure 5: Avatars, queries, items, literal descriptions.
particular sentimental word q?. We evaluate perfor-
mance with F-score.
Avatar Ranking We also consider an avatar re-
trieval task, where the goal is to rank the set of
avatars in our data, ?j=1...n ~aj , according to which
one best matches a sentimental description, ~qi. As
an automated evaluation, we report the average per-
centile position assigned to the true ~ai for each ex-
ample. However, in general, many different avatars
can match each ~qi, an interesting phenomena we will
further study with human evaluation.
Avatar Generation Finally, we consider the prob-
lem of generating novel, previously unseen avatars,
by selecting a set of items that best embody some
sentimental description. As with ranking, we aim to
construct the avatar ~ai that matches each sentimen-
tal description ~qi. We evaluate by considering the
item overlap between ~ai and the output avatar ~a?,
discounting for empty positions:6
f =
?| ~a?|
j=1 I( ~a
?
j = ~aij)
max(numparts( ~a?), numparts(~ai))
, (1)
where numparts returns the number of non-empty
avatar positions. The score is a conservative measure
because some items are significantly more visually
salient than others. For instance, shirts and pants oc-
cupy a large portion of the physical realization of the
avatar, while rings are small and virtually unnotice-
able. We additionally perform a human evaluation
in Section 8 to better understand these challenges.
6Optional items are infrequently used. Therefore not pre-
dicting them at all offers a strong baseline. Yet doing this
demonstrates nothing about an algorithm?s ability to predict
items which contribute to the sentimental qualities of an avatar.
6 Methods
We present two different models: one that considers
words in isolation and another that jointly models
the query words. This section defines the models
and how we learn them.
6.1 Independent Sentimental Word Model
The independent word model (S-Independent) as-
sumes that each word independently describes the
avatar. We construct a separate linear model for each
word in the vocabulary.
To train these model, we transform the data to
form a binary classification problem for each word,
where the positive data includes all avatars the word
was seen with, (q, ~ai, 1) for all i and q ? ~qi, and the
rest are negative, (q, ~ai, 0) for all i and q /? ~qi.
We use the following features:
? an indicator feature for the cross product of a
sentiment query word q, a literal description
word w ? D, and the avatar position index j
(for example, q = ?angry? with w = ?pointy?
and j = eyebrows):
I(q ? ~qi, w ? ~daij , j)
? a bias feature for keeping a position empty:
I(q ? ~qi, aij = empty, j)
These features will allow the model to capture
correlations between our feature norms which pro-
vide descriptions of visual attributes, like black, and
sentimental words, like gothic.
420
S-Independent is used for both word prediction
and ranking. For prediction, we train a linear model
using averaged binary perceptron. For ranking, we
try to rank all positive instances above negative in-
stances. We use an averaged structured perceptron
to train the ranker (Collins, 2002). To rank with re-
spect to an entire query ~qi, we sum the scores of each
word q ? ~qi.
6.2 Joint Sentimental Model
The second approach (S-Joint) jointly models the
query words to learn the relationships between lit-
eral and sentimental words with score s:
s(~a|~q,D) =
|~a|?
i=1
|~q|?
j=1
?T f(~ai, ~qj , ~dai)
Where every word in the query has a separate factor
and every position is treated independently subject
to the constraint that ~a is valid. The feature function
f uses the same features as the word independent
model above.
This model is used for ranking and generation.
For ranking, we try to rank the avatar ai for query
qi above all other avatars in the candidate set. For
generation, we try to score ai above all other valid
avatars given the query qi. In both cases, we train
with averaged structured perceptron (Collins, 2002)
on the original data, containing query, avatar pairs
(~qi, ~ai).
7 Experimental Setup
Random Baseline For the ranking and avatar gen-
eration tasks, we report random baselines. For rank-
ing, we randomly order the avatars. In the genera-
tion case, we select an item randomly for every posi-
tion. This baseline does not generate optional assets
because they are rare in the real data.
Sentimental-Literal Overlap (SL-Overlap) We
also report a baseline that measures the overlap be-
tween words in the sentiment query ~qi and words in
the literal asset descriptions D. In generation, for
each position in the avatar, ~ai, SL-Overlap selects
the item whose literal description has the most words
in common with ~qi. If no item had overlap with the
query, we backoff to a random choice. In the case of
ranking, it orders avatars by the sum over every po-
sition of the number of words in common between
Word F-Score Precision Recall N
happi 0.84 0.89 0.78 149
student 0.78 0.82 0.74 129
friend 0.76 0.84 0.70 153
music 0.74 0.89 0.63 148
confid 0.74 0.82 0.76 157
sport 0.69 0.62 0.76 76
casual 0.63 0.6 0.67 84
youth 0.6 0.57 0.64 88
waitress 0.59 0.42 1 5
smart 0.57 0.54 0.6 88
fashion 0.54 0.54 0.54 70
monei 0.54 0.52 0.56 76
cool 0.54 0.52 0.56 84
relax 0.53 0.52 0.56 90
game 0.51 0.44 0.62 61
musician 0.51 0.44 0.61 66
parti 0.51 0.43 0.62 58
content 0.5 0.47 0.53 75
friendli 0.49 0.42 0.6 56
smooth 0.49 0.4 0.63 57
Table 2: Top 20 words (stemmed) for classification.
N is the number of occurances in the test set.
the literal description and the query, ~qi. This base-
line tests the degree to which literal and sentimental
descriptions overlap lexically.
Feature Generation For all models that use lexi-
cal features, we limited the number of words. 6144
unique vocabulary items occur in the query set, and
3524 in the literal description set. There are over
400 million entries in the full set of features that in-
clude the cross product of these sets with all possible
avatar positions, as described in Section 6. Since this
would present a challenge for learning, we prune in
two ways. We stem all words with a Porter stemmer.
We also filter out all features which do not occur at
least 10 times in our training set. The final model
has approximately 700k features.
8 Results
We present results for the tasks described in Sec-
tion 5 with the appropriate models from Section 6.
8.1 Word Prediction Results
The goal of our first experiment is to study when
individual sentiment words can be accurately pre-
dicted. We computed sentimental word classifica-
tion accuracy for 1179 word classes with 10 or more
421
Algorithm Percentile Rank
S-joint 77.3
S-independant 73.5
SL-overlap 60.4
Random 48.8
Table 3: Automatic evaluation of ranking. The aver-
age percentile that a test avatar was ranked given its
sentimental description.
mentions. Table 2 shows the top 20 words ordered
by F-score.7 Many common words can be predicted
with relatively high accuracy. Words with strong
individual cues like happy (a smiling mouth), and
confidence (wide eyes) and nerdi (particular glasses)
can be predicted well.
The average F-score among all words was .085.
33.2% of words have an F-score of zero. These zeros
include words like: unusual, bland, sarcastic, trust,
prepared, limber, healthy and poetry. Some of these
words indicate broad classes of avatars (e.g., unusual
avatars) and others indicate subtle modifications to
looks that without other words are not specific (e.g.,
a prepared surfer vs. a prepared business man). Fur-
thermore, evaluation was done assuming that when
a word is not mentioned, it is should be predicted as
negative. This fails to account for the fact that peo-
ple do not mention everything that?s true, but instead
make choices about what to mention based on the
most relevant qualities. Despite these difficulties,
the classification performance shows that we can ac-
curately capture usage patterns for many words.
8.2 Ranking Results
Ranking allows us to test the hypothesis that multi-
ple avatars are valid for a high level description. Fur-
thermore, we consider the differences between S-
Joint and S-Independent, showing that jointly mod-
elings all words improves ranking performance.
Automatic Evaluation The results are shown in
Table 3. Both S-Independent and S-Joint outperform
the SL-overlap baseline. SL-Overlap?s poor perfor-
mance can be attributed to low direct overlap be-
tween sentimental words and literal words. S-Joint
also outperforms the S-Independent.
7Accuracy numbers are inappropriate in this case because
the number of negative instances, in most cases, is far larger
than the number of positive ones.
Inspection of the parameters shows that S-Joint
does better than S-Independent in modeling words
that only relate to a subset of body positions. For
example, in one case we found that for the word
?puzzled? nearly 50% of the weights were on fea-
tures that related to eyebrows and eyes. This type
of specialization was far more pronounced for S-
Joint. The joint nature of the learning allows the fea-
tures for individual words to specialize for specific
positions. In contrast, S-Independent must indepen-
dently predict all parts for every word.
Human Evaluation We report human relevancy
judgments for the top-5 returned results from S-
Joint. On average, 56.2% were marked to be rele-
vant. This shows that S-Joint is performing better
than automatic numbers would indicate, confirming
our intuition that there is a one-to-many relationship
between a sentimental description and avatars. Sen-
timental descriptions, while having significant sig-
nal, are not exact. These results also indicate that
relying on automatic measures of accuracy that as-
sume a single reference avatar underestimates per-
formance. Figure 6 shows the top ranked results
returned by S-Joint for a sentimental description
where the model performs well.
8.3 Generation Results
Finally we evaluate three models for avatar genera-
tion: Random, SL-Overlap and S-Joint using auto-
matic measures and human evaluation.
Automatic Evaluation Table 4 presents results
for automatic evaluation. The Random baseline per-
forms badly, on average assigning items correctly to
less than 1 position in the generated avatar. The SL-
Overlap baseline improves, but still performs quite
poorly. The S-Joint model performs significantly
better, correctly guessing 2-3 items for each output
avatar. However, as we will see in the manual eval-
uation, many of the non-matching parts it produces
are still a good fit for the query.
Human Evaluation As before, there are many
reasonable avatars that could match as well as the
reference avatars. Therefore, we also evaluated gen-
eration with A/B tests, much like in Section 4. An-
notators were asked to judge which of two avatars
better matched a sentimental description. They
422
pensive,confrontational; music,socializing; musician,bar tending,club owner; smart,cool.
Figure 6: A sentimental description paired with the highest ranked avatars found by S-Joint.
Model Overlap
Random 0.041
SL-Overlap 0.049
S-Joint 0.126
Table 4: Automatic generation evaluation results.
The item overlap metric is defined in Section 5.
Kappa Majority Random Sys.
SL-Overlap 0.20 0.25 0.34 0.32
S-Joint 0.52 0.90 0.07 0.81
Gamer 0.52 0.81 0.08 0.77
Table 5: Human evaluation of automatically gener-
ated avatars. Majority represents the percentage of
time the system output is preferred by a majority of
raters. Random and System (Sys.) indicate the per-
centage of time each was preferred.
could rate System A or System B as better, or re-
port that they were equal or that neither matched
the description. We consider two comparisons: SL-
Overlap vs. Random and S-Joint vs Random. Five
annotators performed each condition, rating 100 ex-
amples with randomly ordered avatars.
We report the results for human evaluation includ-
ing kappa, majority judgments, and a distribution
over judgments in Table 5. The SL-Overlap baseline
is indistinguishable from a random avatar. This con-
trasts with the ranking case, where this simple base-
line showed improvement, indicating that generation
is a much harder problem. Furthermore, agreement
is low; people felt the need to make a choice but
were not consistent.
We also see in Table 5 that people prefer the S-
Joint model outputs to random avatars as often as
they prefer gamer to random. While this does not
necessarily imply that S-Joint creates gamer-quality
avatars, it indicates substantial progress by learning
a mapping between literal and sentimental words.
Qualitative Results Table 6 presents the highest
and lowest weighted features for different sentimen-
tal query words. Figure 7 shows four descriptions
that were assigned high quality avatars.
In general, many of the weaker avatars had as-
pects of the descriptions but lacked such distinctive
overall looks. This was especially true when the
descriptions contained seemingly contradictory in-
formation. For example, one avatar was described
as being both nerdy and popular. We generated a
look that had aspects of both of these descriptions,
including a head that contained both conservative el-
ements (like glasses) and less conservative elements
(like crazy hair and earrings). However, the combi-
nation would not be described as nerdy or popular,
because of difficult to predict global interactions be-
tween the co-occurring words and items. This is an
important area for future work.
9 Conclusions
We explored how visual language, both literal and
sentimental, maps to the overall physical appearance
and style of virtual characters. While this paper fo-
cused on avatar design, our approach has implica-
tions for a broad class of natural language-driven
423
Ambition; business,
fashion, success;
salesman; smooth,
professional.
Capable, confident, firm; heavy metal,
extreme sports, motorcycles; engineer,
mechanic, machinist; aggressive,
strong, protective.
Stressed, bored,
discontent; emo music;
works at a record store;
goth, dark, drab.
Happy, content, confident,
home, career, family,
secretary,student,
classy,clean,casual
Figure 7: Avatars automatically generated with the S-Joint model.
Sentiment positive features negative features
happi mouth:thick, mouth:smilei, mouth:make, mouth:open mouth:tight, mouth:emotionless, mouth:brownish, mouth:attract
gothic shoes:brown, shirt:black, pants:hot, shirt:band shirt:half, shirt:tight, pants:sexi, hair:brownish
retro eyebrows:men, eyebrows:large, hair:round, pants:light eyebrows:beauti, pants:side; eyebrows:trim, pants:cut
beach pants:yello, pants:half, nose:narrow, pants:white shirt:brown, shirt:side; shoes:long, pants:jean
Table 6: Most positive and negative features for a word stem. A feature is [position]:[literal word].
dialog scenarios. In many situations, a user may
be perfectly able to formulate a high-level descrip-
tion of their intent (?Make my resume look cleaner?
?Buy me clothes for a summer wedding,? or ?Play
something more danceable?) while having little or
no understanding of the complex parameter space
that the underlying software must manipulate in or-
der to achieve this result.
We demonstrated that these high-level sentimen-
tal specifications can have a strong relationship to
literal aspects of a problem space and showed that
sentimental language is a concise, yet noisy, way
of specifying high level characteristics. Sentimen-
tal language is an unexplored avenue for improving
natural language systems that operate in situated set-
tings. It has the potential to bridge the gap between
lay and expert understandings of a problem domain.
Acknowledgments
This work is partially supported by the DARPA
CSSG (N11AP20020) and the NSF (IIS-1115966).
The authors would like to thank Chris Brockett,
Noelle Sophy, Rico Malvar for helping with collect-
ing and processing the data. We would also like
to thank Tom Kwiatkowski and Nicholas FitzGer-
ald and the anonymous reviewers for their helpful
comments.
References
Yoav Artzi and Luke Zettlemoyer. 2013. Weakly su-
pervised learning of semantic parsers for mapping in-
structions to actions. Transactions of the Association
for Computational Linguistics, 1(1):49?62.
SRK Branavan, H. Chen, L.S. Zettlemoyer, and R. Barzi-
lay. 2009. Reinforcement learning for mapping in-
structions to actions. In Proceedings of the Joint Con-
ference of the 47th Annual Meeting of the ACL and
the 4th International Joint Conference on Natural Lan-
guage Processing of the AFNLP, pages 82?90.
SRK Branavan, David Silver, and Regina Barzilay. 2011.
Learning to win by reading manuals in a monte-carlo
framework. In Proceedings of the 49th Annual Meet-
ing of the Association for Computational Linguistics:
Human Language Technologies-Volume 1, pages 268?
277.
David L. Chen and William B. Dolan. 2011. Collecting
highly parallel data for paraphrase evaluation. In Pro-
ceedings of the 49th Annual Meeting of the Association
for Computational Linguistics, pages 190?200.
D.L. Chen and R.J. Mooney. 2011. Learning to interpret
natural language navigation instructions from observa-
424
tions. In Proceedings of the 25th AAAI Conference on
Artificial Intelligence (AAAI-2011), pages 859?865.
David L. Chen, Joohyun Kim, and Raymond J. Mooney.
2010. Training a multilingual sportscaster: Using per-
ceptual context to learn language. Journal of Artificial
Intelligence Research, 37:397?435.
Michael Collins. 2002. Discriminative training meth-
ods for hidden markov models: theory and experi-
ments with perceptron algorithms. In Proceedings of
the ACL-02 conference on Empirical methods in natu-
ral language processing, pages 1?8.
B. Coyne and R. Sproat. 2001. Wordseye: an automatic
text-to-scene conversion system. In Proceedings of the
28th annual conference on Computer graphics and in-
teractive techniques, pages 487?496.
J. Eisenstein, J. Clarke, D. Goldwasser, and D. Roth.
2009. Reading to learn: Constructing features from
semantic abstracts. In Proceedings of the 2009 Con-
ference on Empirical Methods in Natural Language
Processing, pages 958?967.
Ali Farhadi, Ian Endres, Derek Hoiem, and David
Forsyth. 2009. Describing objects by their attributes.
In Proceedings of the IEEE Computer Society Confer-
ence on Computer Vision and Pattern Recognition.
Ali Farhadi, Mohsen Hejrati, Mohammad Amin Sadeghi,
Peter Young, Cyrus Rashtchian, Julia Hockenmaier,
and David Forsyth. 2010. Every picture tells a story:
generating sentences from images. In Proceedings of
the 11th European conference on Computer Vision,
ECCV?10, pages 15?29.
Yansong Feng and Mirella Lapata. 2010. Topic models
for image annotation and text illustration. In Human
Language Technologies: The 2010 Annual Conference
of the North American Chapter of the Association for
Computational Linguistics, pages 831?839.
Susan R Fussell and Mallie M Moss. 1998. Figura-
tive language in emotional communication. Social and
cognitive approaches to interpersonal communication,
page 113.
Corinne Jrgensen. 1998. Attributes of images in describ-
ing tasks. Information Processing & Management,
34(23):161 ? 174.
Adriana Kovashka, Devi Parikh, and Kristen Grauman.
2012. Whittlesearch: Image search with relative at-
tribute feedback. In Computer Vision and Pattern
Recognition (CVPR), pages 2973?2980.
G. Kulkarni, V. Premraj, S. Dhar, S. Li, Y. Choi, A.C.
Berg, and T.L. Berg. 2011. Baby talk: Understanding
and generating simple image descriptions. In Com-
puter Vision and Pattern Recognition (CVPR), pages
1601?1608.
Cynthia Matuszek, Nicholas FitzGerald, Luke Zettle-
moyer, Liefeng Bo, and Dieter Fox. 2012. A Joint
Model of Language and Perception for Grounded At-
tribute Learning. In Proc. of the 2012 International
Conference on Machine Learning.
Ken Mcrae, George S. Cree, Mark S. Seidenberg, and
Chris Mcnorgan. 2005. Semantic feature production
norms for a large set of living and nonliving things.
Behavior Research Methods, 37(4):547?559.
Margaret Mitchell, Kees van Deemter, and Ehud Reiter.
2010. Natural reference to objects in a visual domain.
In Proceedings of the 6th International Natural Lan-
guage Generation Conference, INLG ?10, pages 95?
104.
B. Pang, L. Lee, and S. Vaithyanathan. 2002. Thumbs
up?: sentiment classification using machine learning
techniques. In Proceedings of the ACL-02 conference
on Empirical methods in natural language processing,
pages 79?86.
Ekaterina Shutova. 2010a. Automatic metaphor inter-
pretation as a paraphrasing task. In Human Language
Technologies: The 2010 Annual Conference of the
North American Chapter of the Association for Com-
putational Linguistics, HLT ?10, pages 1029?1037.
Ekaterina Shutova. 2010b. Models of metaphor in nlp.
In Proceedings of the 48th Annual Meeting of the Asso-
ciation for Computational Linguistics, ACL ?10, pages
688?697.
Anne-Marie Tousch, Stphane Herbin, and Jean-Yves Au-
dibert. 2012. Semantic hierarchies for image annota-
tion: A survey. Pattern Recognition, 45(1):333 ? 345.
Yezhou Yang, Ching Lik Teo, Hal Daume? III, and Yian-
nis Aloimonos. 2011. Corpus-guided sentence gen-
eration of natural images. In Empirical Methods in
Natural Language Processing.
Z. Zeng, M. Pantic, G.I. Roisman, and T.S. Huang. 2009.
A survey of affect recognition methods: Audio, vi-
sual, and spontaneous expressions. Pattern Analy-
sis and Machine Intelligence, IEEE Transactions on,
31(1):39?58.
C Lawrence Zitnick and Devi Parikh. 2013. Bringing
semantics into focus using visual abstraction. In Com-
puter Vision and Pattern Recognition (To Appear).
425
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 1669?1679,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
Lightly Supervised Learning of Procedural Dialog Systems
Svitlana Volkova
CLSP
Johns Hopkins University
Baltimore, MD
svitlana@jhu.edu
Pallavi Choudhury, Chris Quirk, Bill Dolan
NLP Group
Microsoft Research
Redmond, WA
pallavic,chrisq,
billdol@microsoft.com
Luke Zettlemoyer
Computer Science and Engineering
University of Washington
Seattle, WA
lsz@cs.washington.edu
Abstract
Procedural dialog systems can help users
achieve a wide range of goals. However,
such systems are challenging to build,
currently requiring manual engineering of
substantial domain-specific task knowl-
edge and dialog management strategies. In
this paper, we demonstrate that it is pos-
sible to learn procedural dialog systems
given only light supervision, of the type
that can be provided by non-experts. We
consider domains where the required task
knowledge exists in textual form (e.g., in-
structional web pages) and where system
builders have access to statements of user
intent (e.g., search query logs or dialog
interactions). To learn from such tex-
tual resources, we describe a novel ap-
proach that first automatically extracts task
knowledge from instructions, then learns a
dialog manager over this task knowledge
to provide assistance. Evaluation in a Mi-
crosoft Office domain shows that the indi-
vidual components are highly accurate and
can be integrated into a dialog system that
provides effective help to users.
1 Introduction
Procedural dialog systems aim to assist users
with a wide range of goals. For example, they
can guide visitors through a museum (Traum et
al., 2012; Aggarwal et al, 2012), teach students
physics (Steinhauser et al, 2011; Dzikovska et
al., 2011), or enable interaction with a health care
U: ?I want to add page numbers and a title?
S: ?Top or Bottom of the page??
U: ?Top?
S: ?Please select page design from the tem-
plates? (*System shows drop down menu*)
U: *User selects from menu*
S: ?Enter header or footer content?
U: ?C.V.?
S: ?Task completed.?
Figure 1: An example dialog interaction between
a system (S) and user (U) that can be automatically
achieved by learning from instructional web page
and query click logs.
system (Morbini et al, 2012; Rizzo et al, 2011).
However, such systems are challenging to build,
currently requiring expensive, expert engineering
of significant domain-specific task knowledge and
dialog management strategies.
In this paper, we present a new approach for
learning procedural dialog systems from task-
oriented textual resources in combination with
light, non-expert supervision. Specifically, we as-
sume access to task knowledge in textual form
(e.g., instructional web pages) and examples of
user intent statements (e.g., search query logs or
dialog interactions). Such instructional resources
are available in many domains, ranging from
recipes that describe how to cook meals to soft-
ware help web pages that describe how to achieve
goals by interacting with a user interface.1
1ehow.com,wikianswers.com
1669
There are two key challenges: we must (1)
learn to convert the textual knowledge into a us-
able form and (2) learn a dialog manager that pro-
vides robust assistance given such knowledge. For
example, Figure 1 shows the type of task assis-
tance that we are targeting in the Microsoft Office
setting, where the system should learn from web
pages and search query logs. Our central contribu-
tion is to show that such systems can be built with-
out the help of knowledge engineers or domain ex-
perts. We present new approaches for both of our
core problems. First, we introduce a method for
learning to map instructions to tree representations
of the procedures they describe. Nodes in the tree
represent points of interaction with the questions
the system can ask the user, while edges represent
user responses. Next, we present an approach that
uses example user intent statements to simulate di-
alog interactions, and learns how to best map user
utterances to nodes in these induced dialog trees.
When combined, these approaches produce a com-
plete dialog system that can engage in conversa-
tions by automatically moving between the nodes
of a large collection of induced dialog trees.
Experiments in the Windows Office help do-
main demonstrate that it is possible to build an
effective end-to-end dialog system. We evaluate
the dialog tree construction and dialog manage-
ment components in isolation, demonstrating high
accuracy (in the 80-90% range). We also conduct
a small-scale user study which demonstrates that
users can interact productively with the system,
successfully completing over 80% of their tasks.
Even when the system does fail, it often does so in
a graceful way, for example by asking redundant
questions but still reaching the goal within a few
additional turns.
2 Overview of Approach
Our task-oriented dialog system understands user
utterances by mapping them to nodes in dialog
trees generated from instructional text. Figure 2
shows an example of a set of instructions and the
corresponding dialog tree. This section describes
the problems that we must solve to enable such in-
teractions, and outlines our approach for each.
Knowledge Acquisition We extract task knowl-
edge from instructional text (e.g., Figure 2, left)
that describes (1) actions to be performed, such
as clicking a button, and (2) places where input
is needed from the user, for example to enter the
contents of the footer or header they are trying to
create. We aim to convert this text into a form that
will enable a dialog system to automatically assist
with the described task. To this end, we construct
dialog trees (e.g., Figure 2, right) with nodes to
represent entire documents (labeled as topics t),
nodes to represent user goals or intents (g), and
system action nodes (a) that enable execution of
specific commands. Finally, each node has an as-
sociated system action as, which can prompt user
input (e.g., with the question ?Top or bottom of
the page??) and one or more user actions au that
represent possible responses. All nodes connect
to form a tree structure that follows the workflow
described in the document. Section 3 presents a
scalable approach for inducing dialog trees.
Dialog Management To understand user intent
and provide task assistance, we need a dialog man-
agement approach that specifies what the system
should do and say. We adopt a simple approach
that at all times maintains an index into a node in
a dialog tree. Each system utterance is then simply
the action as for that node. However, the key chal-
lenge comes in interpreting user utterances. After
each user statement, we must automatically up-
date our node index. At any point, the user can
state a general goal (e.g., ?I want to add page num-
bers?), refine their goal (e.g., ?in a footer?), or both
(e.g.,?I want to add page numbers in the footer?).
Users can also change their goals in the process of
completing the tasks.
We develop a simple classification approach
that is robust to these different types of user behav-
ior. Specifically, we learn classifiers that, given the
dialog interaction history, predict how to pick the
next tree node from the space of all nodes in the di-
alog trees that define the task knowledge. We iso-
late two specific cases, classifying initial user ut-
terances (Section 4) and classifying all subsequent
utterances (Section 5). This approach allows us to
isolate the difference in language for the two cases,
and bias the second case to prefer tree nodes near
the current one. The resulting approach allows for
significant flexibility in traversing the dialog trees.
Data and Evaluation We collected a large set of
such naturally-occurring web search queries that
resulted in a user click on a URL in the Microsoft
Office help domain.2 We found that queries longer
that 4-5 words often resembled natural language
utterances that could be used for dialog interac-
2http://office.microsoft.com
1670
Figure 2: An example instructional text paired with a section of the corresponding dialog tree.
tions, for example how do you add borders, how
can I add a footer, how to insert continuous page
numbers, and where is the header and footer.
We also collected instructional texts from the
web pages that describe how to solve 76 of the
most pressing user goals, as indicated by query
click log statistics. On average 1,000 user queries
were associated with each goal. To some extent
clickthroughs can be treated as a proxy for user
frustration; popular search targets probably repre-
sent user pain points.
3 Building Dialog Trees from
Instructions
Our first problem is to convert sets of instructions
for user goals to dialog trees, as shown in Figure
2. These goals are broadly grouped into topics
(instructional pages). In addition, we manually
associate each node in a dialog tree with a train-
ing set of 10 queries. For the 76 goals (246 in-
structions) in our data, this annotation effort took
a single annotator a total of 41 hours. Scaling this
approach to the entire Office help domain would
require a focused annotation effort. Crucially,
though, this annotation work can be carried out by
non-specialists, and could even be crowdsourced
(Bernstein et al, 2010).
Problem Definition As input, we are given in-
structional text (p1 . . . pn), comprised of topics
(t1 . . . tn) describing:
(1) high-level user intents (e.g., t1 ? ?add and for-
mat page numbers?)
(2) goals (g1, . . . , gk) that represent more spe-
cific user intents (e.g., g1 ? ?add header or
footer content to a preformatted page number
design?, g2 ? ?place the page number in the
side margin of the page?).
Given instructional text p1 . . . pn and queries
q1 . . . qm per topic ti, our goals are as follows:
Figure 3: Relationships between user queries and
OHP with goals, instructions and dialog trees.
- for every instructional page pi extract a topic
ti and a set of goals g1 . . . gk;
- for every goal gj for a topic ti, extract a set of
instructions i1 . . . il;
- from topics, goals and instructions, construct
dialog trees f1 . . . fn (one dialog tree per
topic). Classify instructions to user interac-
tion types thereby identifying system action
nodes a1s . . . als. Transitions between these
nodes are the user actions a1u . . . alu.
Figure 2 (left) presents an example of a topic
extracted from the help page, and a set of goals
and instructions annotated with user action types.
In the next few sections of the paper, we out-
line an overall system component design demon-
strating how queries and topics are mapped to the
dialog trees in Figure 3. The figure shows many-
to-one relations between queries and topics, one-
to-many relations between topics and goals, goals
and instructions, and one-to-one relations between
topics and dialog trees.
User Action Classification We aim to classify
instructional text (i1 . . . il) for every goal gj in the
decision tree into four categories: binary, selec-
tion, input or none.
Given a single instruction i with category au,
we use a log-linear model to represent the distri-
1671
bution over the space of possible user actions. Un-
der this representation, the user action distribution
is defined as:
p(au|i, ?) =
e???(au,i)?
a?u e
???(au,i) , (1)
where ?(au, i) ? Rn is an n-dimensional fea-
ture representation and ~? is a parameter vector we
aim to learn. Features are indicator functions of
properties of the instructions and a particular class.
For smoothing we use a zero mean, unit variance
Gaussian prior (0, 1) that penalizes ~? for drifting
too far from the mean, along with the following
optimization function:
log p(Au, ?|I) = log p(Au|I, ?)? log p(?) =
=
?
au,i?(Au,I)
p(au|i, ?)?
?
i
(? ? ?i)2
2?2i
+ k
(2)
We use L-BFGS (Nocedal and Wright, 2000) as
an optimizer.
Experimental Setup As described in Section 2,
our dataset consists of 76 goals grouped into 30
topics (average 2-3 goals per topic) for a total of
246 instructions (average 3 instructions per goal).
We manually label all instructions with user ac-
tion au categories. The distribution over cate-
gories is binary=14, input=23, selection=80 and
none=129. The data is skewed towards the cat-
egories none and selection. Many instruction do
not require any user input and can be done auto-
matically, e.g., ?On the Insert tab, in the Header
and Footer group, click Page Number?. The ex-
ample instructions with corresponding user action
labels are shown in Figure 2 (left) . Finally, we di-
vide the 246 instructions into 2 sets: 80% training
and 20% test, 199 and 47 instructions respectively.
Results We apply the user action type classifi-
cation model described in the Eq.1 and Eq.2 to
classify instructions from the test set into 4 cate-
gories. In Table 1 we report classification results
for 2 baselines: a majority class and heuristic-
based approach, and 2 models with different fea-
ture types: ngrams and ngrams + stems. For a
heuristic baseline, we use simple lexical clues to
classify instructions (e.g., X or Y for binary, select
Y for selection and type X, insert Y for input). Ta-
ble 1 summarizes the results of mapping instruc-
tional text to user actions.
Features # Features Accuracy
Baseline 1: Majority ? 0.53
Baseline 2: Heuristic ? 0.64
Ngrams 10,556 0.89
Ngrams + Stems 12,196 0.89
Table 1: Instruction classification results.
Building the Dialog Trees Based on the classi-
fied user action types, we identify system actions
a1s . . . als which correspond to 3 types of user ac-
tions a1s . . . als (excluding none type) for every goal
in a topic ti. This involved associating all words
from an instruction il with a system action als. Fi-
nally, for every topic we automatically construct a
dialog tree as shown in Figure 2 (right). The dia-
log tree includes a topic t1 with goals g1 . . . g4, and
actions (user actions au and system actions as).
Definition 1. A dialog tree encodes a user-system
dialog flow about a topic ti represented as a di-
rected unweighted graph fi = (V,E) where top-
ics, goals and actions are nodes of correspond-
ing types {t1 . . . tn}, {g1 . . . gk}, {a1 . . . al} ? V .
There is a hierarchical dependency between topic,
goal and action nodes. User interactions are
represented by edges ti ? {g1 . . . gk}, a1u =
(gj , a1) . . . alu = (ak?1, ak) ? E.
For example, in the dialog tree in Figure 2 there
is a relation t1 ? g4 between the topic t1 ?add
and format page numbers? and the goal g4 ?in-
clude page of page X of Y with the page number?.
Moreover, in the dialog tree, the topic level node
has one index i ? [1..n], where n is the number
of topics. Every goal node includes information
about its parent (topic) node and has double index
i.j, where j ? [1..k]. Finally, action nodes include
information about their parent (goal) and grand-
parent (topic) nodes and have triple index i.j.z,
where z ? [1..l].
4 Understanding Initial Queries
This section presents a model for classifying ini-
tial user queries to nodes in a dialog tree, which
allows for a variety of different types of queries.
They can be under-specified, including informa-
tion about a topic only (e.g., ?add or delete page
numbers?); partially specified, including informa-
tion about a goal (e.g., ?insert page number?); or
over-specified, including information about an ac-
tion ( e.g., ?page numbering at bottom page?.)
1672
Figure 4: Mapping initial user queries to the nodes
on different depth in a dialog tree.
Problem Definition Given an initial query, the
dialog system initializes to a state s0, searches for
the deepest relevant node given a query, and maps
the query to a node on a topic ti, goal gj or action
ak level in the dialog tree fi, as shown in Figure 4.
More formally, as input, we are given automati-
cally constructed dialog trees f1 . . . fn for instruc-
tional text (help pages) annotated with topic, goal
and action nodes and associated with system ac-
tions as shown in Figure 2 (right). From the query
logs, we associate queries with each node type:
topic qt, goal qg and action qa. This is shown in
Figure 2 and 4. We join these dialog trees repre-
senting different topics into a dialog network by
introducing a global root. Within the network,
we aim to find (1) an initial dialog state s0 that
maximizes the probability of state given a query
p(s0|q, ?); and (2) the deepest relevant node v ? V
on topic ti, goal gj or action ak depth in the tree.
Initial Dialog State Model We aim to predict
the best node in a dialog tree ti, gj , al ? V based
on a user query q. A query-to-node mapping is en-
coded as an initial dialog state s0 represented by a
binary vector over all nodes in the dialog network:
s0 = [t1, g1.1, g1.2, g1.2.1 . . . , tn, gn.1, gn.1.1].
We employ a log-linear model and try to maxi-
mize initial dialog state distribution over the space
of all nodes in a dialog network:
p(s0|q, ?) =
e
?
i ?i?i(s0,q)
?
s?0 e
?
i ?i?i(s?0,q)
, (3)
Optimization follows Eq. 2.
We experimented with a variety of features.
Lexical features included query ngrams (up to 3-
grams) associated with every node in a dialog tree
with removed stopwords and stemming query un-
igrams. We also used network structural features:
Accuracy
Features Topic Goal Action
Random 0.10 0.04 0.04
TFIDF 1Best 0.81 0.21 0.45
Lexical (L) 0.92 0.66 0.63
L + 10TFIDF 0.94 0.66 0.64
L + 10TFIDF + PO 0.94 0.65 0.65
L + 10TFIDF + QO 0.95 0.72 0.69
All above + QHistO 0.96 0.73 0.71
Table 2: Initial dialog state classification results
where L stands for lexical features, 10TFIDF - 10
best tf-idf scores, PO - prompt overlap, QO - query
overlap, and QHistO - query history overlap.
tf-idf scores, query ngram overlap with the topic
and goal descriptions, as well as system action
prompts, and query ngram overlap with a history
including queries from parent nodes.
Experimental Setup For each dialog tree,
nodes corresponding to single instructions were
hand-annotated with a small set of user queries,
as described in Section 3. Approximately 60% of
all action nodes have no associated queries3 For
the 76 goals, the resulting dataset consists of 972
node-query pairs, 80% training and 20% test.
Results The initial dialog state classification
model of finding a single node given an initial
query is described in Eq. 3.
We chose two simple baselines: (1) randomly
select a node in a dialog network and (2) use a tf-
idf 1-best model.4 Stemming, stopword removal
and including top 10 tf-idf results as features led
to a 19% increase in accuracy on an action node
level over baseline (2). Adding the following fea-
tures led to an overall 26% improvement: query
overlap with a system prompt (PO), query overlap
with other node queries (QO), and query overlap
with its parent queries (QHistO) .
We present more detailed results for topic, goal
and action nodes in Table 2. For nodes deeper in
the network, the task of mapping a user query to an
action becomes more challenging. Note, however,
that the action node accuracy numbers actually un-
3There are multiple possible reasons for this: the soft-
ware user interface may already make it clear how to accom-
plish this intent, the user may not understand that the software
makes this fine-grained option available to them, or their ex-
perience with search engines may lead them to state their in-
tent in a more coarse-grained way.
4We use cosine similarity to rank all nodes in a dialog
network and select the node with the highest rank.
1673
derstate the utility of the resulting dialog system.
The reason is that even incorrect node assignments
can lead to useful system performance. As long
as a misclassification results being assigned to a
too-high node within the correct dialog tree, the
user will experience a graceful failure: they may
be forced to answer some redundant questions, but
they will still be able to accomplish the task.
5 Understanding Query Refinements
We also developed a classifier model for mapping
followup queries to the nodes in a dialog network,
while maintaining a dialog state that summarizes
the history of the current interaction.
Problem Definition Similar to the problem def-
inition in Section 4, we are given a network of di-
alog trees f1 . . . fn and a query q?, but in addition
we are given the previous dialog state s, which
contains the previous user utterance q and the last
system action as. We aim to find a new dialog
state s? that pairs a node from the dialog tree with
updated history information, thereby undergoing a
dialog state update.
We learn a linear classifier that models
p(s?|q?, q, as, ?), the dialog state update distribu-
tion, where we constrain the new state s? to contain
the new utterance q? we are interpreting. This dis-
tribution models 3 transition types: append, over-
ride and reset.
Definition 2. An append action defines a dialog
state update when transitioning from a node to its
children at any depth in the same dialog tree e.g.,
ti ? gi.j (from a topic to a goal node), gi.j ?
ai.j.z (from a goal to an action node) etc.
Definition 3. An override action defines a dialog
state update when transitioning from a goal to its
sibling node. It could also be from an action node5
to another in its parent sibling node in the same di-
alog tree e.g., gi.j?1 ? gi.j (from one goal to an-
other goal in the same topic tree), ai.j.z ? ai.?j.z
(from an action node to another action node in a
different goal in the same dialog tree) etc.
Definition 4. A reset action defines a dialog state
update when transitioning from a node in a current
dialog tree to any other node at any depth in a
dialog tree other than the current dialog tree e.g.,
ti ? t?i, (from one topic node to another topic
5A transition from ai.j.z must be to a different goal or an
action node in a different goal but in the same dialog tree.
(a) Updates from topic node ti
(b) Updates from goal node gj
(c) Updates from action node al
Figure 5: Information state updates: append, reset
and override updates based on Definition 2, 3 and
4, respectively, from topic, goal and action nodes.
node) ti ? g?i.j (from a topic node to a goal node
in a different topic subtree), etc.
The append action should be selected when the
user?s intent is to clarify a previous query (e.g.,
?insert page numbers? ? ?page numbers in the
footer?). An override action is appropriate when
the user?s intent is to change a goal within the
same topic (e.g., ?insert page number? ?change
page number?). Finally, a reset action should be
used when the user?s intent is to restart the dialog
(e.g., ?insert page x of y? ? ?set default font?).
We present more examples for append, override
and reset dialog state update actions in Table 3.
1674
Previous Utterance, q User Utterance, q? Transition Update Action, a
inserting page numbers qt1 add a background ti ? t?i 2, reset-T, reset
how to number pages qt2 insert numbers on pages in margin ti ? si.j 1.4, append-G, append
page numbers qt3 set a page number in a footer ti ? ai.j.z 1.2.1, append-A, append
page number a document qt4 insert a comment ti ? g?i.j 21.1, reset-G, reset
page number qt5 add a comment ?redo? ti ? a?i.j.z 21.2.1, reset-A, reset
page x of y qg1 add a border gi.j ? t?i 6, reset-T, resetformat page x of x qg2 enter text and page numbers gi.j ? gi.?j 1.1, override-G, overrideenter page x of y qg3 page x of y in footer gi.j ? ai.j.z 1.3.1, append-A, appendinserting page x of y qg4 setting a default font gi.j ? g?i.j 6.1, reset-G, resetshowing page x of x qg5 set default font and style gi.j ? a?i.j.z 6.4.1, reset-A, resetpage numbers bottom qa1 make a degree symbol ai.j.z ? t?i 13, reset-T, reset
numbering at bottom page qa2 insert page numbers ai.j.z ? gi.?j 1.1, override-G, override
insert footer page numbers qa3 page number design ai.j.z?1 ? ai.j.z 1.2.2, append-A, append
headers page number qa4 comments in document ai.j.z ? g?i.j 21.1, reset-G, reset
page number in a footer qa5 changing initials in a comment ai.j.z ? a?i.j.z 21.2.1, reset-A, reset
Table 3: Example q and q? queries for append, override and reset dialog state updates.
Figure 5 illustrates examples of append, over-
ride and reset dialog state updates. All transitions
presented in Figure 5 are aligned with the example
q and q? queries in Table 3.
Dialog State Update Model We use a log-linear
model to maximize a dialog state distribution over
the space of all nodes in a dialog network:
p(s?|q?, q, as?) =
e
?
i ?i?i(s?,q?,as,q)
?
s?? e
?
i ?i?i(s??,q?,as,q)
, (4)
Optimization is done as described in Section 3.
Experimental Setup Ideally, dialog systems
should be evaluated relative to large volumes of
real user interaction data. Our query log data,
however, does not include dialog turns, and so we
turn to simulated user behavior to test our system.
Our approach, inspired by recent work (Schatz-
mann et al, 2006; Scheffler and Young, 2002;
Georgila et al, 2005), involves simulating dialog
turns as follows. To define a state s we sam-
ple a query q from a set of queries per node v
and get a corresponding system action as for this
node; to define a state s?, we sample a new query
q? from another node v? ? V, v 6= v? which
is sampled using a prior probability biased to-
wards append: p(append)=0.7, p(override)=0.2,
p(reset)=0.1. This prior distribution defines a dia-
log strategy where the user primarily continues the
current goal and rarely resets.
We simulate 1100 previous state and new query
pairs for training and 440 pairs for testing. The
features were lexical, including word ngrams,
stems with no stopwords; we also tested network
structure, such as:
- old q and new q? query overlap (QO);
- q? overlap with a system prompt as (PO);
- q? ngram overlap with all queries from the old
state s (SQO);
- q? ngram overlap with all queries from the
new state s? (S?QO);
- q? ngram overlap with all queries from the
new state parents (S?ParQO).
Results Table 4 reports results for dialog state
updates for topic, goal and action nodes. We also
report performance for two types of dialog updates
such as: append (App.) and override (Over.).
We found that the combination of lexical and
query overlap with the previous and new state
queries yielded the best accuracies: 0.95, 0.84 and
0.83 for topic, goal and action node level, respec-
tively. As in Section 4, the accuracy on the topic
level node was highest. Perhaps surprisingly, the
reset action was perfectly predicted (accuracy is
100% for all feature combinations, not included
in figure). The accuracies for append and override
actions are also high (append 95%, override 90%).
Features Topic Goal Action App. Over.
L 0.92 0.76 0.78 0.90 0.89
L+Q 0.93 0.80 0.80 0.92 0.83
L+P 0.93 0.80 0.79 0.91 0.85
L+Q+P 0.94 0.80 0.80 0.93 0.85
L+SQ 0.94 0.82 0.81 0.93 0.85
L+S?Q 0.93 0.80 0.80 0.91 0.90
L+S?+ParQ 0.94 0.80 0.80 0.91 0.86
L+Q+S?Q 0.94 0.81 0.81 0.91 0.88
L+SQ+S?Q 0.95 0.84 0.83 0.94 0.88
Table 4: Dialog state updates classification ac-
curacies where L stands for lexical features, Q -
query overlap, P - prompt overlap, SQ - previous
state query overlap, S?Q - new state query overlap,
S?ParQ - new state parent query overlap.
1675
6 The Complete Dialog System
Following the overall setup described in Section 2,
we integrate the learned models into a complete
dialog system. To evaluate the quality of the in-
teractions with this system, we performed a small
scale user study, as described here.
Experimental Setup We randomly chose 8
goals, across topics, from the 76 used to train our
models. For each goal, six users were shown a Mi-
crosoft Word document before and after the goal
was executed. Users were allowed to interact with
the system in an unconstrained manner providing a
total of 48 interactions. If at any point they thought
the system response was irrelevant to their intent,
they could either rephrase or start over.
User Study We objectively measured task com-
pletion and the quality of individual system utter-
ances. We also asked the users to rate the system
(on a scale of 1-5, with 5 being best) on various
measures, including whether the system asked rel-
evant questions. Table 5 summarizes the results.
In general, users were able to complete the tasks
and were reasonably satisfied.
Discussion Figure 6 shows four example di-
alogs. User utterances ranged from single key-
words to rich, sentence-like goal descriptions. For
the shorter inputs, the system was generally able
to identify a small cohort of high-probability in-
tents. For longer inputs, especially with explicitly
specified parameter settings, the system was usu-
ally able to elicit whatever additional information
was required to complete the task. The majority of
errors fell into two categories when the system:
I. missed key information in the input and
asked redundant questions ? Figure 6 (C);
II. failed to correctly identify the intent, and
asked an irrelevant question ? Figure 6 (D).
Table 5 shows the frequency of each error type.
Objective Measures Observed Rates
User achieved task 83%
System asked irrelevant questions 19%
System asked redundant questions 18%
User Satisfaction Score Std.Dev.
Overall satisfaction 3.42 0.49
Questions were relevant 4.17 0.41
Ambiguity was managed 4.00 0.89
Table 5: User study results.
U: ?I want a page number and title at the bot-
tom of the page?
S: ?Choose a page number design from the
gallery of designs?
U: *User selects design*
S: ?To insert header or footer content before the
page number, type the content, and then press
TAB to position the content?
U: ?Professional Resume?
S: ?Task Completed?
(A)
U: ?Alphabetize bullets?
S: ?Do you want to sort a list alphabetically?
U: ?Sort a list?
S: ?Select the text in a bulleted or numbered
list?
U: *User selection*
S: ?Ascending or Descending??
U: ?Ascending?
S: ?Task Completed?
(B)
U: ?I want the background color to be yellow?
S: ?Choose a color for the document?
U: ?Yellow ?
S: ?Task completed?
(C)
U: ?I want to change the font face and size?
S: ?Choose the default font?
U: ?No, reduce the size of the text?
S: *System Restarts*
(D)
Figure 6: Four example dialogs from the user
study, including cases that (A and B) complete
successfully, (C) have a redundant question, and
(D) fail to recognize the user intent.
7 Related work
To the best of our knowledge, this paper presents
the first effort to induce full procedural dialog sys-
tems from instructional text and query click logs.
Grounded Language Learning There has been
significant interest in grounded language learn-
ing. Perhaps the most closely related work
learns to understand instructions and automati-
1676
cally complete the tasks they describe (Branavan
et al, 2009; Vogel and Jurafsky, 2010; Kush-
man et al, 2009; Branavan et al, 2010; Artzi and
Zettlemoyer, 2013). However, these approaches
did not model user interaction. There are also
many related approaches for other grounded lan-
guage problems, including understanding game
strategy guides (Branavan et al, 2011), model-
ing users goals in a Windows domain (Horvitz
et al, 1998), learning from conversational inter-
action (Artzi and Zettlemoyer, 2011), learning
to sportscast (Chen and Mooney, 2011), learning
from event streams (Liang et al, 2009), and learn-
ing paraphrases from crowdsourced captions of
video snippets (Chen and Dolan, 2011).
Dialog Generation from Text Similarly to Pi-
wek?s work (2007; 2010; 2011), we study extract-
ing dialog knowledge from documents (mono-
logues or instructions). However, Piwek?s ap-
proach generates static dialogs, for example to
generate animations of virtual characters having a
conversation. There is no model of dialog man-
agement or user interaction, and the approach does
not use any machine learning. In contrast, to the
best of our knowledge, we are the first to demon-
strate it is possible to learn complete, interactive
dialog systems using instructional texts (and non-
expert annotation).
Learning from Web Query Logs Web query
logs have been extensively studied. For example,
they are widely used to represent user intents in
spoken language dialogs (Tu?r et al, 2011; Celiky-
ilmaz et al, 2011; Celikyilmaz and Hakkani-Tur,
2012). Web query logs are also used in many other
NLP tasks, including entity linking (Pantel et al,
2012) and training product and job intent classi-
fiers (Li et al, 2008).
Dialog Modeling and User Simulation Many
existing dialog systems learn dialog strategies
from user interactions (Young, 2010; Rieser and
Lemon, 2008). Moreover, dialog data is often lim-
ited and, therefore, user simulation is commonly
used (Scheffler and Young, 2002; Schatzmann et
al., 2006; Georgila et al, 2005).
Our overall approach is also related to many
other dialog management approaches, including
those that construct dialog graphs from dialog data
via clustering (Lee et al, 2009), learn information
state updates using discriminative classification
models (Hakkani-Tur et al, 2012; Mairesse et al,
2009), optimize dialog strategy using reinforce-
ment learning (RL) (Scheffler and Young, 2002;
Rieser and Lemon, 2008), or combine RL with
information state update rules (Heeman, 2007).
However, our approach is unique in the use of in-
ducing task and domain knowledge with light su-
pervision to assist the user with many goals.
8 Conclusions and Future Work
This paper presented a novel approach for au-
tomatically constructing procedural dialog sys-
tems with light supervision, given only textual re-
sources such as instructional text and search query
click logs. Evaluations demonstrated highly accu-
rate performance, on automatic benchmarks and
through a user study.
Although we showed it is possible to build com-
plete systems, more work will be required to scale
the approach to new domains, scale the complex-
ity of the dialog manager, and explore the range of
possible textual knowledge sources that could be
incorporated. We are particularly interested in sce-
narios that would enable end users to author new
goals by writing procedural instructions in natural
language.
Acknowledgments
The authors would like to thank Jason Williams
and the anonymous reviewers for their helpful
comments and suggestions.
References
Priti Aggarwal, Ron Artstein, Jillian Gerten, An-
thanasios Katsamanis, Shrikanth Narayanan, Angela
Nazarian, and David R. Traum. 2012. The twins
corpus of museum visitor questions. In Proceedings
of LREC.
Yoav Artzi and Luke Zettlemoyer. 2011. Learning
to recover meaning from unannotated conversational
interactions. In NIPS Workshop In Learning Seman-
tics.
Yoav Artzi and Luke Zettlemoyer. 2013. Weakly su-
pervised learning of semantic parsers for mapping
instructions to actions. Transactions of the Associa-
tion for Computational Linguistics, 1(1):49?62.
Michael S. Bernstein, Greg Little, Robert C. Miller,
Bjo?rn Hartmann, Mark S. Ackerman, David R.
Karger, David Crowell, and Katrina Panovich.
2010. Soylent: a word processor with a crowd in-
side. In Proceedings of ACM Symposium on User
Interface Software and Technology.
1677
S. R. K. Branavan, Harr Chen, Luke S. Zettlemoyer,
and Regina Barzilay. 2009. Reinforcement learning
for mapping instructions to actions. In Proceedings
of ACL.
S. R. K. Branavan, Luke S. Zettlemoyer, and Regina
Barzilay. 2010. Reading between the lines: learn-
ing to map high-level instructions to commands. In
Proceedings of ACL.
S. R. K. Branavan, David Silver, and Regina Barzi-
lay. 2011. Learning to win by reading manuals in
a monte-carlo framework. In Proceedings of ACL.
Asli Celikyilmaz and Dilek Hakkani-Tur. 2012. A
joint model for discovery of aspects in utterances.
In Proceedings of ACL.
Asli Celikyilmaz, Dilek Hakkani-Tu?r, and Gokhan Tu?r.
2011. Mining search query logs for spoken language
understanding. In Proceedings of ICML.
David L. Chen and William B. Dolan. 2011. Collect-
ing highly parallel data for paraphrase evaluation. In
Proceedings of ACL.
David L. Chen and Raymond J. Mooney. 2011. Learn-
ing to interpret natural language navigation instruc-
tions from observations. In Proceedings of AAAI.
Myroslava Dzikovska, Amy Isard, Peter Bell, Jo-
hanna D. Moore, Natalie B. Steinhauser, Gwen-
dolyn E. Campbell, Leanne S. Taylor, Simon Caine,
and Charlie Scott. 2011. Adaptive intelligent tuto-
rial dialogue in the beetle ii system. In Proceedings
of AIED.
Kallirroi Georgila, James Henderson, and Oliver
Lemon. 2005. Learning user simulations for infor-
mation state update dialogue systems. In Proceed-
ings of Eurospeech.
Dilek Hakkani-Tur, Gokhan Tur, Larry Heck, Ashley
Fidler, and Asli Celikyilmaz. 2012. A discrimi-
native classification-based approach to information
state updates for a multi-domain dialog system. In
Proceedings of Interspeech.
Peter Heeman. 2007. Combining Reinforcement
Learning with Information-State Update Rules. In
Proceedings of ACL.
Eric Horvitz, Jack Breese, David Heckerman, David
Hovel, and Koos Rommelse. 1998. The Lumiere
project: Bayesian user modeling for inferring the
goals and needs of software users. In Proceedings
of Uncertainty in Artificial Intelligence.
Nate Kushman, Micah Brodsky, S. R. K. Branavan,
Dina Katabi, Regina Barzilay, and Martin Rinard.
2009. WikiDo. In ACM HotNets.
Cheongjae Lee, Sangkeun Jung, Kyungduk Kim, and
Gary Geunbae Lee. 2009. Automatic agenda graph
construction from human-human dialogs using clus-
tering method. In Proceedings of NAACL.
Xiao Li, Ye-Yi Wang, and Alex Acero. 2008. Learn-
ing query intent from regularized click graphs. In
Proceedings of SIGIR.
Percy Liang, Michael I. Jordan, and Dan Klein. 2009.
Learning semantic correspondences with less super-
vision. In Proceedings of ACL-IJCNLP.
F. Mairesse, M. Gasic, F. Jurcicek, S. Keizer, B. Thom-
son, K. Yu, and S. Young. 2009. Spoken lan-
guage understanding from unaligned data using dis-
criminative classification models. In Proceedings of
Acoustics, Speech and Signal Processing.
Fabrizio Morbini, Eric Forbell, David DeVault, Kenji
Sagae, David R. Traum, and Albert A. Rizzo. 2012.
A mixed-initiative conversational dialogue system
for healthcare. In Proceedings of SIGDIAL.
Jorge Nocedal and Stephen J. Wright. 2000. Numeri-
cal Optimization. Springer.
Patric Pantel, Thomas Lin, and Michael Gamon. 2012.
Mining entity types from query logs via user intent.
In Proceedings of ACL.
Paul Piwek and Svetlana Stoyanchev. 2010. Generat-
ing expository dialogue from monologue: Motiva-
tion, corpus and preliminary rules. In Proceedings
of NAACL.
Paul Piwek and Svetlana Stoyanchev. 2011. Data-
oriented monologue-to-dialogue generation. In Pro-
ceedings of ACL, pages 242?247.
Paul Piwek, Hugo Hernault, Helmut Prendinger, and
Mitsuru Ishizuka. 2007. T2d: Generating dialogues
between virtual agents automatically from text. In
Proceedings of Intelligent Virtual Agents.
Verena Rieser and Oliver Lemon. 2008. Learning ef-
fective multimodal dialogue strategies from wizard-
of-oz data: Bootstrapping and evaluation. In Pro-
ceedings of ACL.
A. Rizzo, Kenji Sagae, E. Forbell, J. Kim, B. Lange,
J. Buckwalter, J. Williams, T. Parsons, P. Kenny,
David R. Traum, J. Difede, and B. Rothbaum. 2011.
Simcoach: An intelligent virtual human system for
providing healthcare information and support. In
Proceedings of ITSEC.
Jost Schatzmann, Karl Weilhammer, Matt Stuttle, and
Steve Young. 2006. A survey of statistical user sim-
ulation techniques for reinforcement-learning of dia-
logue management strategies. Knowledge Engineer-
ing Review, 21(2).
Konrad Scheffler and Steve Young. 2002. Automatic
learning of dialogue strategy using dialogue simula-
tion and reinforcement learning. In Proceedings of
Human Language Technology Research.
Natalie B. Steinhauser, Gwendolyn E. Campbell,
Leanne S. Taylor, Simon Caine, Charlie Scott, My-
roslava Dzikovska, and Johanna D. Moore. 2011.
1678
Talk like an electrician: Student dialogue mimick-
ing behavior in an intelligent tutoring system. In
Proceedings of AIED.
David R. Traum, Priti Aggarwal, Ron Artstein, Susan
Foutz, Jillian Gerten, Athanasios Katsamanis, Anton
Leuski, Dan Noren, and William R. Swartout. 2012.
Ada and grace: Direct interaction with museum vis-
itors. In Proceedings of Intelligent Virtual Agents.
Go?khan Tu?r, Dilek Z. Hakkani-Tu?r, Dustin Hillard, and
Asli C?elikyilmaz. 2011. Towards unsupervised spo-
ken language understanding: Exploiting query click
logs for slot filling. In Proceedings of Interspeech.
Adam Vogel and Dan Jurafsky. 2010. Learning to fol-
low navigational directions. In Proceedings of ACL.
Steve Young. 2010. Cognitive user interfaces. In IEEE
Signal Processing Magazine.
1679
