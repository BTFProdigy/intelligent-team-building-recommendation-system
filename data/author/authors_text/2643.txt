Compi l ing  Language Mode ls  f rom a L ingu is t i ca l ly  Mot ivated  
Un i f i ca t ion  Grammar  
Manny Rayner t>, Beth Ann Hockey t, Frankie James t 
Elizabeth Owen Bratt ++, Sharon Goldwater ++ and Jean Mark Gawron ~ 
tResea.rch Inst i tute for 
Advanced Computer  Science 
Mail Stop 19-39 
NASA Ames Research Center 
Moffett Field, CA 94035-1000 
Abstract 
Systems now exist which are able to con:pile 
unification gralmnars into language models that 
can be included in a speech recognizer, but it 
is so far unclear whether non-trivial linguisti- 
cally principled gralnlnars can be used for this 
purpose. We describe a series of experiments 
which investigate the question empirica.lly, by 
incrementally constructing a grammar and dis- 
covering what prot)lems emerge when succes- 
sively larger versions are compiled into finite 
state graph representations and used as lan- 
guage models for a medium-vocabulary recog- 
nition task. 
1 Introduction ~ 
Construction of speech recognizers for n:ediuln- 
vocabulary dialogue tasks has now becolne an 
important I)ractical problem. The central task 
is usually building a suitable language model, 
and a number of standard methodologies have 
become established. Broadly speaking, these 
fall into two main classes. One approach is 
to obtain or create a domain corpus, and froln 
it induce a statistical anguage model, usually 
some kind of N-gram grammar; the alternative 
is to manually design a grammar which specifies 
the utterances the recognizer will accept. There 
are many theoretical reasons to prefer the first 
course if it is feasible, but in practice there is of- 
ten no choice. Unless a substantial domain cor- 
pus is available, the only method that stands a 
chance of working is hand-construction f an ex- 
i The majority of the research reported was performed 
at I{IACS under NASA Cooperative Agreement~ Number 
NCC 2-1006. The research described in Section 3 was 
supported by the Defense Advanced Research Projects 
Agency under Con~racl~ N66001-94 C-6046 with the 
Naval Command, Control, and Ocean Surveillance Cen- 
ter. 
SRI International  
333 Ravenswood Ave 
Menlo Park, CA 94025 
*netdecisions 
Well ington House 
East Road 
Cambr idge CB1 1BH 
England 
plicit grammar based on the grammar-writer's 
intuitions. 
If the application is simple enough, experi- 
ence shows that good grammars of this kind 
can be constructed quickly and efficiently using 
commercially available products like ViaVoice 
SDK (IBM 1999) or the Nuance Toolkit (Nu- 
ance 1999). Systems of this kind typically al- 
low specification of some restricted subset of the 
class of context-free grammars, together with 
annotations that permit the grammar-writer to
associate selnantic values with lexical entries 
and rules. This kind of framework is fl:lly ad- 
equate for small grammars. As the gran:mars 
increase in size, however, the limited expres- 
sive power of context-free language notation be- 
conies increasingly burdensome. The grainn:a,r 
tends to beconie large and unwieldy, with many 
rules appearing in multiple versions that con- 
stantly need to be kept in step with each other. 
It represents a large developn:ent cost, is hard 
to maintain, and does not usually port well to 
new applications. 
It is tempting to consider the option of mov- 
ing towards a :::ore expressive grammar tbrmal- 
isln, like unification gramnm.r, writing the orig- 
inal grammar in unification grammar form and 
coml)iling it down to the context-free notation 
required by the underlying toolkit. At least 
one such system (Gemilfi; (Moore ct al 1997)) 
has been implemented and used to build suc- 
cessful and non-trivial applications, most no- 
tably ComnmndTalk (Stent ct al 1999). Gem- 
ini accepts a slightly constrained version of the 
unification grammar formalism originally used 
in the Core Language Engine (Alshawi 1992), 
and compiles it into context-free gran:nmrs in 
the GSL formalism supported by the Nuance 
Toolkit. The Nuance Toolkit con:piles GSL 
gran:mars into sets of probabilistic finite state 
670 
gra.phs (PFSGs), which form the final bmguage 
model. 
The relative success of the Gemilfi system 
suggests a new question. Ulfification grammars 
ha.re been used many times to build substantial 
general gramlnars tbr English and other na.tu- 
ra\[ languages, but the language model oriented 
gra.mln~rs o far developed fi)r Gemini (includ- 
ing the one for ColnmandTalk) have a.ll been 
domain-sl)ecific. One naturally wonders how 
feasible it is to take yet another step in the di- 
rection of increased genera.lity; roughly, what 
we want to do is start with a completely gen- 
eral, linguistically motivated gramma.r, combine 
it with a domain-specific lexicon, and compile 
the result down to a domain-specitic context- 
free grammar that can be used as a la.nguage 
model. If this 1)tetra.mine can be rea.lized, it is 
easy to believe that the result would 1)e a.n ex- 
tremely useful methodology tbr rapid construc- 
tion of la.nguage models. It is i lnportant o note 
tha.t there are no obvious theoretical obstacles 
in our way. The clailn that English is context- 
free has been respectable since a.t least the early 
8(Is (Pullum and Gazda.r 1982) 'e, and the idea. 
of using unification grammar as a. compact wa 5, 
of tel)resenting an ulMerlying context-fl'e~e, lan- 
guage is one of the main inotivations for GPSG 
(Gazdar et al1985) and other formalislns based 
on it. The real question is whether the goal is 
practically achievable, given the resource limi- 
tations of current technology. 
In this l)a.1)er, we describe work aimed at the 
target outlined above, in which we used the 
Gemini system (described in more detail in Sec- 
tion 2) to a.ttempt o compile a. va.riety of lin- 
guistically principled unification gralnlna.rs into 
la.ngua.ge lnodels. Our first experiments (Sec- 
tion 3) were pertbrmed on a. large pre-existing 
unification gramlna.r. These were unsuccessful, 
for reasons that were not entirely obvious; in 
order to investigate the prol)lem more system- 
atically, we then conducted a second series of 
experilnents (Section 4), in which we increlnen- 
tally 1)uilt up a smMler gra.lnlna.r. By monitor- 
ing; the behavior of the compilation process and 
the resulting langua.ge model as the gra.lmnar~s 
2~1e m'e aware l, hal, this claim is most~ 1)robably not 
l;rue for natural languages ill gelmraI (lh'csnall cl al 
1987), but furl~hcr discussion of t.his point is beyond I.he 
scope of t, llC paper. 
cover~ge was expanded, we were a.ble to iden- 
tit~ the point a,t which serious problems began 
to emerge (Section 5). In the fina.1 section, we 
summarize and suggest fltrther directions. 
2 Tile Genfini Language Model  
Compi le r  
To lnake the paper nlore self-contained, this sec- 
tion provides some background on the method 
used by Gemini to compile unifica.tion grain- 
mars into CFGs, and then into language mod- 
els. The ha.sic idea. is the obvious one: enu- 
mera.te all possible instantiations of the feal;ures 
in the grammar rules and lexicon entries, and 
thus tra.nsform esch rule and entry in the ()rig- 
inal unification grammar into a set of rules in 
the derived CFG. For this to be possible, the 
relevant fe~ttul'es Inust be constrained so that 
they can only take values in a finite predefined 
range. The finite range restriction is inconve- 
nient for fea.tures used to build semantic repre- 
sentations, and the tbrmalism consequently dis- 
tinguishes syntactic and semantic features; se- 
lmmtic features axe discarded a.t the start of the 
compilation process. 
A naive iml)lelnentation of the basic lnethod 
would be iml)raetical for any but the small- 
est a.nd simplest grammars, and considera.ble 
ingemfity has been expended on various opti- 
mizations. Most importantly, categories axe ex- 
panded in a demand-driven fa.shion, with infer  
lnatiotl being percolated 1)oth t)otton>up (from 
the lexicon) and top-down (fl'om the grammar's 
start symbol). This is done in such a. way 
that potentially valid colnl)inations of feature 
instantiations in rules are successively filtered 
out if they are not licensed by the top-down 
and bottom-ul) constra.ints. Ranges of feature 
values are also kept together when possible, so 
that sets of context-free rules produced by the 
mdve algorithm may in these cases be merged 
into single rules. 
By exploiting the structure of the gram- 
mar a.nd lexicon, the demand-driven expansion 
lnethod can often effect substa.ntial reductions 
in the size of the derived CFG. (For the type 
of grammar we consider in this paper, the re- 
duction is typically by ,~ fa.etor of over 102?). 
The downside is that even an app~trently slnall 
cha.nge in the syntactic t>atures associated with 
a. rule may have a large eIfect on the size of 
671 
the CFG, if it opens up or blocks an impor- 
tant percolation path. Adding or deleting lexi- 
con entries can also have a significant effect on 
the size of the CFG, especially when there are 
only a small number of entries in a given gram- 
matical category; as usual, entries of this type 
behave from a software ngineering standpoint 
like grammar ules. 
The language model compiler also performs 
a number of other non-trivial transformations. 
The most important of these is related to the 
fact that Nuance GSL grammars are not al- 
lowed to contain left-recursive rules, and left- 
recursive unification-grammar rules must con- 
sequently be converted into a non-left-recursive 
fort::. Rules of this type do not however occur 
in the gramlnars described below, and we conse- 
quently omit further description of the method. 
3 Initial Experiments 
Our initial experiments were performed on a 
recent unification grammar in the ATIS (Air 
Travel Information System) domain, developed 
as a linguistically principled grammar with a 
domain-specific lexicon. This grammar was 
cre~ted for an experiment COl::t)aring cover- 
age and recognition performance of a hand- 
written grammar with that of a.uto:::atically de- 
rived recognition language models, as increas- 
ing amounts of data from the ATIS corpus 
were made available for each n:ethod. Exam- 
ples of sentences covered by this gralnlnar are 
"yes", "on friday", "i want to fly from boston 
to denver on united airlines on friday septem- 
ber twenty third", "is the cheapest one way 
fare from boston to denver a morning flight", 
and "what flight leaves earliest from boston to 
san francisco with the longest layover in den- 
ver". Problems obtaining a working recognition 
grammar from the unification grammar ended 
our original experiment prematurely, and led 
us to investigate the factors responsible for the 
poor recognition performance. 
We explored several ikely causes of recogni- 
tion trouble: number of rules, ::umber of vocab- 
ulary items, size of node array, perplexity, and 
complexity of the grammar, measured by aver- 
age and highest number of transitions per graph 
in the PFSG form of the grammar. 
We were able to in:mediately rule out sim- 
ple size metrics as the cause of Nuance's diffi- 
culties with recognition. Our smallest air travel 
grammar had 141 Gemini rules and 1043 words, 
producing a Nuance grammar with 368 rules. 
This compares to the Con:mandTalk grammar, 
which had 1231 Gemini rules and 1771 words, 
producing a Nuance gran:n:ar with 4096 rules. 
To determine whether the number of the 
words in the grammar or the structure of 
the phrases was responsible for the recognition 
problems, we created extreme cases of a Word+ 
grammar (i.e. a grammar that constrains the 
input to be any sequence of the words in the 
vocabulary) and a one-word-per-category gram- 
mar. We found that both of these variants 
of our gralmnar produced reasonable recogni- 
tion, though the Word+ grammar was very in- 
accurate. However, a three-words-per-category 
grammar could not produce snccessflfl speech 
recognition. 
Many thature specifications can lnake a gram- 
mar ::tore accurate, but will also result in a 
larger recognition grammar due to multiplica- 
tion of feature w~lues to derive the categories 
of the eontext-fl'ee grammar. We experimented 
with various techniques of selecting features to 
be retained in the recognition grammar. As de- 
scribed in the previous ection, Gemini's default 
method is to select only syntactic features and 
not consider semantic features in the recogni- 
tion grammar. We experimented with selecting 
a subset of syntactic features to apply and with 
applying only se:nantic sortal features, and no 
syntactic features. None of these grammars pro- 
duced successful speech recognition. 
/.Fro::: these experiments, we were unable to 
isolate any simple set of factors to explain which 
grammars would be problematic for speech 
recognition. However, the numbers of transi- 
tions per graph in a PFSG did seem suggestive 
of a factor. The ATIS grammar had a high of 
1184 transitions per graph, while the semantic 
grammar of CommandTalk had a high of 428 
transitions per graph, and produced very rea- 
sonable speech recognition. 
Still, at; the end of these attempts, it beca.me 
clear that we did not yet know the precise char- 
acteristic that makes a linguistically motivated 
grammar intractable for speech recognition, nor 
the best way to retain the advantages of the 
hand-written grammar approach while provid- 
ing reasonable speech recognition. 
672 
4 Incrementa l  Grammar  
Deve lopment  
In our second series of experiments, we in- 
crelnenta.lly developed a. new grammar front 
s('ra.tch. The new gra.mma.r is basica.lly a s('a.led- 
down and a.dapted version of tile Core Lan- 
guage Engine gramme\ for English (Puhnan 
1!)92; Rayner 1993); concrete development work 
a.nd testing were organized a.round a. speech in- 
terfa c(; to a. set; of functionalities oflhred by a 
simple simula,tion of the Space Shuttle (Rather, 
Hockey gll(l James 2000). Rules and lexical 
entries were added in sma.ll groups, typically 
2-3 rules or 5 10 lexical entries in one incre- 
ment. After each round of exl)a.nsion , we tested 
to make sure that the gramlnar could still 1)e 
compiled into a. usa.bh; recognizer, a.nd a.t sev- 
ere.1 points this suggested changes in our iln- 
1)\]ementation strategy. The rest of this section 
describes tile new grmmnar in nlore detail. 
4.1 Overv iew of  Ru les  
The current versions of the grammar and lexi- 
con contain 58 rules a.nd 30J. Ulfinflectesl entries 
respectively. They (:over the tbllowing phenom- 
el i  :~IZ 
1. Top-level utl;er~tnces: declarative clauses, 
WH-qtlestions, Y-N questions, iml)erat;ives, 
etlil)tical NPs and I)Ps, int(;rject.ions. 
~.. / \ ]  9 \,~ H-lnovement of NPs and PPs. 
3. The fbllowing verb types: intr~nsi- 
tive, silnple transitive, PP con:plen-mnt, 
lnodaJ/a.uxiliary, -ing VP con-q)len:ent, par- 
ticleq-NP complement, sentential comple- 
lnent, embedded question complement. 
4. PPs: simple PP, PP with postposition 
("ago")~ PP lnodifica,tion of VP and NP. 
5. Relat;ive clauses with both relative NP pro- 
1101111 ("tit(; telnperature th,tt I measured )
and relative PP ("the (loci: where I am"). 
6. Numeric determiners, time expressions, 
and postmodification of NP 1)y nun:eric ex- 
pressions. 
7. Constituent conjunction of NPs and 
cl~ulses. 
Tilt following examl)le sentences illustrate 
current covera,ge: 3 '-. , ':how ~d)out scenario 
three.?", "wha, t is the temperature?", "mea- 
sure the pressure a,t flight deck", "go to tile 
crew ha.tch a.nd (:lose it", "what were ten:per- 
a.tttt'e a, nd pressure a.t iifteen oh five?", "is the 
telnpera.ture going ttp'. ~', "do the fi?ed sensors 
sa.y tha.t the pressure is decreasing. , "find out 
when the pressure rea.ched fifteen p s i . . . .  wh~t 1 
is the pressure that you mea.sured?", "wha.t is 
the tempera.lure where you a.re?", ?~(:a.n you find 
out when the fixed sensors ay the temperature 
at flight deck reached thirty degrees celsius?". 
4.2 Unusua l  Features  o f  the  Grammar  
Most of the gramn:~u', as already sta.ted, is 
closely based on the Core Language Eng!ne 
gra.nlnla.r. \?e briefly sllnllna.rize the main di- 
vergences between the two gramnlars. 
4.2.1 I nvers ion  
The new gramlna, r uses a. novel trea.tment of 
inversion, which is p~trtly designed to simplify 
the l)l'ocess of compiling a, fea,ture gl'anllna, r into 
context-free form. The CLE grammar's trea.t- 
l l tent of invers ion uses a, movement account, in 
which the fronted verb is lnoved to its notional 
pla.ce in the VP through a feature. So, tbr 
example, the sentence "is pressure low?" will 
in the origina.1 CLE gramma.r ha.re the phrase- 
structure 
::\[\[iS\]l" \ [p ressure \ ]N / ,  \[\[\]V \[IO\V\]AI),\]\]V'\]'\],'g" 
in whk:h the head of th(, VP is a V gap coin- 
dexed with tile fronted main verb 1,~ . 
Our new gra.mn:ar, in contrast, hal:dles in- 
version without movement, by making the con> 
bination of inverted ver\]) and subject into a. 
VBAR constituent. A binary fea.ture invsubj  
picks o:ll; these VBARs, a.nd there is a. question- 
forma,tion rule of tilt form 
S --> VP : E invsub j=y\ ]  
Continuing the example, the new gram- 
mar a.ssigns this sentence tilt simpler phrase- 
structure 
"\[\[\[is\] v \[press:ire\] N*'\] v .A .  \[\[low\] J\] V.\] S" 
4.2.2 Sorta l  Const ra in ts  
Sortal constra,ints are coded into most gr~un:nnr 
rules as synta.ctic features in a straight-forward 
lna.nner, so they are available to the compilation 
673 
process which constructs the context-free gram- 
mar, ~nd ultimately tile language model. The 
current lexicon allows 11 possible sortal values 
tbr nouns, and 5 for PPs. 
We have taken the rather non-standard step 
of organizing tile rules for PP modification so 
that a VP or NP cannot be modified by two 
PPs  of the same sortal type. The principal mo- 
tivation is to tighten the language model with 
regard to prepositions, which tend to be pho- 
netically reduced and often hard to distinguish 
from other function words. For example, with- 
out this extra constraint we discovered that an 
utterance like 
measure temperature at flight deck 
and lower deck 
would frequently be misrecognized as 
measure temperature at flight deck in 
lower deck 
5 Exper iments  with Incremental  
G r am 111 ar  S 
Our intention when developing the new gram- 
mar was to find out just when problems began 
to emerge with respect to compilation of tan- 
gm~ge models. Our initial hypothesis was that 
these would l)robably become serious if the rules 
for clausal structure were reasonably elaborate; 
we expected that the large number of possible 
ways of combining modal and auxiliary verbs, 
question forlnation, movement, and sentential 
complements would rapidly combine to produce 
an intractably loose language model. Interest- 
ingly, this did not prove to be the case. In- 
stead, the rules which appear to be the primary 
ca.use of difficulties are those relating to relative 
clauses. We describe the main results in Sec- 
tion 5.1; quantitative results on recognizer per- 
tbrmance are presented together in Section 5.2. 
5.1 Main Findings 
We discovered that addition of the single rule 
which allowed relative clause modification of an 
NP had a dr~stic effect on recognizer perfor- 
lnance. The most obvious symptoms were that 
recognition became much slower and the size of 
the recognition process much larger, sometimes 
causing it to exceed resource bounds. The false 
reject rate (the l)roportion of utterances which 
fell below the recognizer's mininmnl confidence 
theshold) also increased substantially, though 
we were surprised to discover no significant in- 
crea.se in the word error rate tbr sentences which 
did produce a recognition result. To investi- 
gate tile cause of these effects, we examined the 
results of perfornfing compilation to GSL and 
PFSG level. The compilation processes are such 
that symbols retain mnemonic names, so that it 
is relatively easy to find GSL rules and gral)hs 
used to recognize phrases of specified gralnmat- 
ical categories. 
At the GSL level, addition of the relative 
clause rule to the original unification grammar 
only increased the number of derived Nuance 
rules by about 15%, from 4317 to 4959. The av- 
erage size of the rules however increased much 
more a. It, is easiest o measure size at the level of 
PFSGs, by counting nodes and transitions; we 
found that the total size of all the graphs had in- 
creased from 48836 nodes and 57195 tra.nsitions 
to 113166 nodes and 140640 transitions, rather 
more than doubling. The increase was not dis- 
tributed evenly between graphs. We extracted 
figures for only the graphs relating to specific 
grammatical categories; this showed that, the 
number of gra.1)hs fbr NPs had increased from 
94 to 258, and lnoreover that the average size 
of each NP graph had increased fronl 21 nodes 
and 25.5 transitions to 127 nodes and 165 tra.nsi- 
tions, a more than sixfold increase. The graphs 
for clause (S) phrases had only increased in 
number froln 53 to 68. They ha.d however also 
greatly increased in average size, from 171 nodes 
and 212 transitions to 445 nodes and 572 tran- 
sitions, or slightly less than a threefold increase. 
Since NP and S are by far the most important 
categories in the grammar, it is not strange that 
these large changes m~tke a great difference to 
the quality of the language model, and indi- 
rectly to that of speech recognition. 
Colnparing the original unification gramlnar 
and the compiled CSL version, we were able to 
make a precise diagnosis. The problem with the 
relative clause rules are that they unify feature 
values in the critical S and NP subgralnlnars; 
this means that each constrains the other, lead- 
ing to the large observed increase in the size 
and complexity of the derived Nuance grammar. 
aGSL rules are written in all notat ion which allows 
disjunction and Klccne star. 
674 
Specifically, agreement ilffbrmation and sortal 
category are shared between the two daugh- 
ter NPs in the relative clause modification rule, 
which is schematically as follows: 
Igp: \[agr=A, sort=S\]  --+ 
NP: \[agr=A, sort=S\] 
REL:\[agr=A, sort=S\]  
These feature settings ~re needed in order to get 
tile right alternation in pairs like 
the robot that *measure/measures 
the teml)erature \[agr\] 
the *deck/teml)era.ture tha.t you 
measured \[sort\] 
We tested our hypothesis by colnlnenting ()lit 
the agr and sor t  features in the above rule. 
This completely solves the main 1)robh;in of ex- 
1)lesion in the size of the PFSG representation; 
tile new version is only very slightly larger than 
tile one with no relative clause rule (50647 nodes 
and 59322 transitions against 48836 nodes and 
57195 transitions) Most inL1)ortantty, there is 
no great increase in the number or average size 
of the NP and S graphs. NP graphs increase in 
number froin 94 to 130, and stay constant in a.v- 
era ge size.; S graphs increase in number f}om 53 
to 64, and actually decrease, in aa;erage size to 
13,5 nodes and 167 transitions. Tests on st)eech 
(l~t;a. show that recognition quality is nea~rly :lie 
sa.me as for the version of the recognizer which 
does not cover relative clauses. Although speed 
is still significantly degraded, the process size 
has been reduced sufficiently that the 1)roblen:s 
with resource bounds disappear. 
It would be rea.sonal)le 1:o expect tim: remov- 
ing the explosion in the PFSG ret)resentation 
would result in mL underconstrained language 
model for the relative clause paxt of the gram- 
mar, causing degraded 1)erformance on utter- 
ances containing a, relative clause. Interestingly, 
this does not appear to hapl)en , though recog- 
nition speed under the new grammar is signif- 
icaatly worse for these utterances COml)ared to 
utterances with no relative clause. 
5.2 Recogn i t ion  Resu l ts  
This section summarizes our empirical recog- 
nition results. With the help of the Nuance 
Toolkit batchrec  tool, we evah:ated three ver- 
sions of the recognizer, which differed only with 
respect to tile language model, no_re ls  used 
the version of the language model derived fl'onI a 
granLn:a.r with the relative clause rule removed; 
re l s  is the version derived from the fltll gram- 
lnar; and un l inked  is the colnl)romise version, 
which keeps the relative clause rule but removes 
the critical features. We constructed a corpus 
of 41 utterances, of mean length 12.1 words. 
The utterances were chosen so that the first, 31 
were within the coverage of all three versions 
of the grammar; the last 10 contained relative 
clauses, and were within the coverage of re :s  
and un: inked but :tot of no_rels .  Each utter- 
anee was recorded by eight different subjects, 
none of whom had participated in development 
of the gra.mmar or recognizers. Tests were run 
on a dual-processor SUN Ultra60 with 1.5 GB 
of RAM. 
The recognizer was set, to reject uttera.nces if 
their a.ssociated confidence measure fell under 
the default threshold. Figures 1 and 2 sum- 
marize the re.suits for the first 31 utterances 
(no relative clauses) and the last 10 uttera:Lces 
(relative clauses) respectively. Under '?RT', 
we give inean recognition speed (averaged over 
subjects) e?pressed as a multiple of real time; 
'PRe.j' gives the false reject rate, the :heart l)er - 
centage of utterances which were reiected ue to 
low confidence measures; 'Me:n' gives the lnean 
1)ercentage of uttera.nces which fhiled due to the. 
recognition process exceeding inemory resource 
bounds; and 'WER,' gives the mean word er- 
ror rate on the sentences that were neither re- 
jected nor failed due to resource bound prob- 
lems. Since the distribution was highly skewed, 
all mea.ns were calculated over the six subjects 
renm.i:fing after exclusion of the extreme high 
and low values. 
Looking first at Figure 1, we see that re l s  is 
clearly inferior to no_re ls  on tile subset of the 
corpus which is within the coverage of both ver- 
sions: nea.rly twice as many utterances are re- 
jected due to low confidence values or resource 
1)roblems, and recognition speed is about five 
times slower, un l inked  is in contrast :tot sig- 
nificantly worse than no_re ls  in terms of recog- 
nition performance, though it is still two and a 
half times slower. 
Figure 2 compares re l s  and un l inked on the 
utterances containing a relative clause. It seems 
reasona.ble to say that recognition performance 
675 
I C4ran"nar I I FR .i I IWER 1 
no_rels 1.04 9.0% - 6.0% 
re l s  4.76 16.1% 1.1% 5.7% 
un l inked  2.60 9.6% - 6.5% 
Figure 1: Evaluation results for 31 utterances 
not containing relative clauses, averaged across 
8 subjects excluding extreme values. 
Grammar xRT FRej Men: WER\ ]  
re l s  4.60 26.7% 1.6% 3.5%\] 
un l inked 5.29 20.0% - 5.4%J 
Figure 2: Evaluation results for i0 utter~mces 
containing relative clauses, averaged across 8 
subjects excluding extreme values. 
is comparable for the two versions: rels has 
lower word error rate, but also rqjects more 
utterances. Recognition speed is marginally 
lower for unl inked,  though it is not clear to us 
whether the difference is significant given the 
high variability of the data. 
6 Conc lus ions  and  Fur ther  
D i rec t ions  j 
We found the results presented above surpris- 
ing and interesting. When we 1)egal: our pro- 
gramme of attempting to compile increasingly 
larger linguistically based unification grammars 
into language models, we had expected to see a 
steady combinatorial increase, which we guessed 
would be most obviously related to complex 
clause structure. This did not turn out to be the 
case. Instead, the serious problems we encoun- 
tered were caused by a small number of crit- 
ical rules, of which the one for relative clause 
modification was by the far the worst. It was 
not immediately obvious how to deal with the 
problem, but a careful analysis revealed a rea- 
sonable con:promise solution, whose only draw- 
back was a significant but undisastrous degra- 
dation in recognition speed. 
It seems optimistic to hope that the rela- 
tive clause problem is the end of the story; the 
obvious way to investigate is by continuing to 
expand the gramlnar in the same incremental 
fashion, and find out what happens next. We 
intend to do this over the next few months, and 
expect in due course to be able to l)resent fur- 
ther results. 
References  
H. Alshawi. 1992. The Core Language Engine. 
Cambridge, Massachusetts: The MIT Press. 
J. Bresnan, R.M. Kapla.n, S. Peters and A. Za- 
enen. Cross-Serial Dependencies in Dutch. 
1987. In W. J. Savitch et al(eds.), The For- 
real Complexity of Natural Languagc, Reidel, 
Dordrecht, pages 286-319. 
G. Gazdar, E. Klein, G. Pullum and I. Sag. 
1985. Generalized Phrase Structure Gram- 
mar Basil Blackwell. 
IBM. 1999. ViaVoice SDK tbr Windows, ver- 
sion 1.5. 
R. Moore, J. Dowding, H. Bratt, J.M. Gawron, 
Y. Gorfl:, and A. Cheyer. 1997. Com- 
mandTalk: A Spoken-Language Interface 
tbr Battlefield Simulations. Proceedings 
of the Fifth Conference on Applied Nat- 
uraI Languagc Processing, pages 1-7, 
Washington, DC. Available online from 
http ://www. ai. sri. com/natural-language 
/project s/arpa-sl s / commandt alk. html. 
Nuance Communications. 1999. Nuance Speech 
Recognition System Developer's Manv, aI, Ver- 
sion 6.2 
G. Pullum and G. Gazdar. 1982. Natural Lan- 
guages and Context-Free Languages. Lin- 
guistics and Philosophy, 4, pages 471-504. 
S.G. Puhnan. 1992. Unification-Based Synta.c- 
tic Analysis. In (Alshawi 1992) 
M. Rayner. 1993. English Linguistic Coverage. 
In M.S. Agn~s et al 1993. Spoken Language 
Translator: First Year Report. SRI Techni- 
cal Report CRC-043. Available online from 
http ://www. sri. com. 
M. Rayner, B.A. Hockey and F. James. 2000. 
Turning Speech into Scripts. To appear in 
P~vceedings of the 2000 AAAI Spring Sym- 
posium on Natural Language Dialogues with 
Practical Robotic Devices 
A. Stent, J. Dowding, J.M. Gawron, E.O. 
Bratt, and R. Moore. 1999. The Coin- 
mandTalk Spoken Dialogue System. P'rv- 
cecdings of the 37th Annual Meeting of the 
ACL, pages 183-190. Available online from 
ht tp  ://www. a i .  s r i .  com/natura l - language 
/p ro jec t  s /a rpa-s  :s  / commandt a:k.  html. 
676 
Building a Robust Dialogue System with Limited Data * 
Sharon  J .  Go ldwater ,  E l i zabeth  Owen Brat t ,  Jean  Mark  Gawron ,  and  John  Dowdingt  
, .  SR I  In ternat iona l  
333 Ravenswood Avenue 
Men lo  Park ,  CA 94025 
{goldwater, owen, gawron, dowding} @ai.sri.cora 
Abst ract  
We describe robustness techniques used in the Com- 
mandTalk system at: the recognition level, the pars- 
ing level, and th~ dia16gue level, and how these were 
influenced by the lack of domain data. We used 
interviews with subject matter experts (SME's) to 
develop a single grammar for recognition, under- 
standing, and generation, thus eliminating the need 
for a robust parser. We broadened the coverage of 
the recognition grammar by allowing word insertions 
and deletions, and we implemented clarification and 
correction subdialogues to increase robustness at the 
dialogue level. We discuss the applicability of these 
techniques to other domains. 
1 I n t roduct ion  
Three types of robustness must be considered when 
designing a dialogue system. First, there is robust- 
ness at the recognition level. When plentiful data 
is available, a robust n-gram language model can be 
produced, but when data is limited, producing a ro- 
bust language model for recognition can be prob- 
lematic. Second, there is robustness at the level 
of the parser. Robust parsing is often achieved by 
combining a full parser with a partial parser and 
fragment-combining rules, but even then some utter- 
ances may be correctly recognized, only to be parsed 
incorrectly or not at all. Finally, there is robustness 
at the dialogue level. Utterances may be uninter- 
pretable within the context of the dialogue due to 
errors on the part of either the system or the user, 
and the dialogue manager should be able to handle 
such problems gracefully. 
Our CommandTalk dialogue system was designed 
for a highly specialized omain with little available 
data, so finding ways to build a robust system with 
* This research was supported by the Defense Advanced Re- 
search Projects Agency under Contract N66001-94-C-6046 
with the Space and Naval Warfare Systems Center. The views 
and conclusions contained in this document are those of the 
authors and should not be interpreted asnecessarily repre- 
senting the official policies, either express or implied, of the 
Defense Advanced Research Projects Agency of the U.S. Gov- 
ernment. 
? Currently affiliated with GO.corn 
limited data was a major concern. In this paper, 
we discuss our methods and their applicability to 
other domains. Section 2 gives a brief overview of 
the CommandTalk system. In Section 3, we discuss 
the approach we took to building recognition, under- 
standing, and generaffon models for CommandTalk, 
and how it relates to the first two types of robustness 
mentioned. Section 4 discusses additional robust- 
ness techniques at the recognizer level, and Section 5 
describes dialogue-level robustness techniques. Sec- 
tion 6 discusses the applicability of our methods to 
other domains. 
2 CommandTa lk  
CommandTalk is a spoken-language interface to the 
ModSAF (Modular Semi-Automated Forces) battle- 
field simulator, developed with the goal of allow- 
ing military commanders to interact with simulated 
forces in a manner as similar as possible to the way 
they would command actual forces. CommandTalk 
allows the use of ordinary English commands and 
mouse gestures to 
? Create forces and control measures (points and 
lines) 
? Assign missions to forces 
? Modify missions during execution 
? Control ModSAF system functions, such as the 
map display 
? Get information about the state of the simula- 
tion 
CommandTalk consists of a number of indepen- 
dent, cooperating agents interacting through SRI's 
Open Agent Architecture (OAA) (Martin et al, 
1998). OAA uses a facilitator agent that plans and 
coordinates interactions among agents during dis- 
tributed computation. An introduction to the basic 
CommandTalk agents can be found in Moore et al 
(1997). CommandTalk's dialogue component is de- 
scribed in detail in Stent et al (1999), and its use 
of linguistic and situational context is described in 
Dowding et al (1999). 
61 
3 The  One-Grammar  Approach  
In a domain with limited data, the inability to col- 
lect a sufficient corpus for training a statistical lan- 
guage model can be a significant problem. For 
CommandTalk, we did not create a statistical lan- 
guage model. Instead, with information gathered 
from interviews of subject matter experts (SME's), 
we developed a handwritten grammar using Gemini 
(Dowding et al, 1993), a unification-based gram- 
mar formalism. We used this unification grammar 
for both natural language understanding and gener- 
ation, and, using a grammar compiler we developed, 
compiled it into a context-free form suitable for the 
speech recognizer as well. 
The effe~s_ of this single-grammar pproach on 
the robustness of the CommandTalk system were 
twofold. On the negative side, we presumably ended 
up with a recognition language model with less cov- 
erage than a statistical model would have had. Our 
attempts to deal with this are discussed in the next 
section. On the positive side, we eliminated the 
usual discrepancy incoverage between the recognizer 
and the natural language parser. This was advanta- 
geous, since no fragment-combining or other parsing 
robustness techniques were needed. 
Our approach ad other advantages a well. Any 
changes we made to the understanding grammar 
were automatically reflected in the recognition and 
generation grammars, making additions and modifi- 
cations efficient. Also, anecdotal evidence suggests 
that the language used by the system often influ- 
ences the language used by speakers, o maintaining 
consistency between the input and output of the sys- 
tem is desirable. 
4 Ut terance-Leve l  Robustness  
It is difficult o write a grammar that is constrained 
enough to be useful without excluding some rea- 
sonable user utterances. To alleviate this prob- 
lem, we modified the speech recognition grammar 
and natural language parser to allow certain "close- 
to-grammar" utterances. Utterances with inserted 
words, such as Center on Checkpoint 1 now or zoom 
way out (where Center on Checkpoint 1 and zoom 
out are grammatical) were permitted by allowing 
the recognizer to skip unknown words. We also al- 
lowed utterances with deleted words, as long as those 
words did not contribute to the semantics of the ut- 
terance as determined by the Gemini semantic rules 
constraining logical forms. For example, a user could 
say, Set speed, 40 kph rather than Set speed to 40 kph. 
The idea behind these modifications was to allow ut- 
terances with a slightly broader ange of wordings 
than those in the grammar, but with essentially the 
same meanings: 
We began by testing the effects of these modi- 
fications on in-grammar utterances, to ensure that 
Time, CPURT 
SRR 
AWER 
SER 
Non-Robust Robust 
0.664 : 1.05 
2.56% 1.70% 
1.68% 2.94% 
10.00% ~ 12.07% 
Table 1: In-Grammar Recognition Results 
they did not significantly decr egse recognition per- 
formance. We used a small test corpus of approxi- 
mately 800 utterances read by SRI employees. We 
collected four measures of performance: 
? Recognition time, measured, in multiples of 
CPU real time (CPURT). A recognition time 
of lxCPURT means that on,our CPU (a Sun 
Ultra2), recognition took exactly as~ long as the 
duration of the utterance. : 
? Sentence reject rate (SRR).' The percentage of 
sentences that the recognizer rejects. 
? Adjusted word error rate (A:WER). The per- 
centage of words in non:rejected sentences that 
are misrecognized. 
? Sentence rror rate (SER). The percentage of 
sentences in which some sort of error occurred, 
either a complete rejection or misrecognized 
word. 
Several parameters affected the results, most no- 
tably the numerical penalties assigned for inserting 
or deleting words, and the pruning threshold of the 
recognizer. Raising the pruning threshold caused 
both reject and error rates to go down, but slowed 
recognition. Lowering the penalties caused rejection 
rates to go down, but word and Sentence rror rates 
to go up, since some sentences which had been re- 
jected were now recognized partially correctly, and 
some sentences which had been recognized correctly 
now included some errors. Lowering the penalties 
also led to slower recognition. 
Table 1 shows recognition results for the non- 
robust and robust versions 0f the recognition gram- 
mar on in-grammar utterances: Th e pruning thresh- 
old is the same for both versions and the insertion 
and deletion penalties are set to intermediate val- 
ues. Recognition times for the robust grammar are 
about 60% slower than those of the control gram- 
mar, but still at acceptable l vels. Reject and error 
rates are fairly close for the two grammars. Overall, 
adding robustness to the recognition grammar did 
not severely penalize in-grammar recognition per- 
formance. 
We had very little out-of-grammar data for Com- 
mandTalk, and finding subjects in this highly spe- 
cialized domain would have been difficult and ex- 
pensive. To test our robustness techniques on out- 
62 
of-grammar utterances, we decided to port them 
to another domain with easily accessible users and 
data; namely, the ATIS air travel domain. We wrote 
a small grammar covering part of the ATIS data 
and ,compiled it into a recognition grammar using 
the same techniques as in CommandTalk. Unfortu- 
nately, we were unable to carry out any experiments, 
because the recognition grammar we derived yielded 
recognition times that were so slow as to be imprac- 
tical. We discuss these results further in Section 6. 
5 Diaiogue-Level Robustness 
To be considered robust at the dialogue level, a sys- 
tem must be able to deal with situations where an 
utterance is recognized and parsed, but cannot be in- 
terpreted withi~4he current system state or dialogue 
context. In addition~it must be easy for the user to 
correct faulty interpretations on the part of the sys- 
tem. Contextual interpretation problems may occur 
for a variety of reasons, including misrecognitions, 
incorrect reference resolution, and confusion or in- 
completeness on the part of the user. 
The CommandTalk dialogue manager maintains 
a Stack to ~keep 'track of the current discourse con- 
text and uses small finite-state machines to represent 
different~ types of subdialogues. Below we illustrate 
some types of  subdialogues and other techniques 
which provide robustness at the dialogue level. Note 
that for each utterance, we write what the system 
recognizes, not what the user actually says. 
5.1 Correction Subdlalogues 
Sx? 1: 
U 1 :Create a CEV at 76 53 
S 2 ?. 
U 3 Put Objective Golf here <click> 
S 4 ? I will locate Objective Golf at FQ 
? 658 583 
U 5 I said Objective Alpha 
S 6 ? I will locate Objective Alpha at FQ 
658 853 
Allowing the user to correct full or partial utterances 
can remedy interpretation problems caused by mis- 
recognitions, incorrect reference resolution, or user 
error. 
In Example 1, the system responds to the user's 
first utterance by producing a rising tone, illustrated 
by the ? symbol, to indicate successful interpreta- 
tion and execution of the command, in this case cre- 
ation of a CEV, a type of vehicle. (Unsuccessful 
interpretation is indicated by a falling tone, illus- 
trated by the ? symbol.) In utterances 3 through 
6, a misrecognition causes the system to perform 
the wrong behavior. The user initiates a correction 
subdialogue, and the system goes on to correctly re- 
interpret he full utterance. 
5.2 Impl ic i t  Conf i rmat ion  
Ex. 2: 
U 7 CEV, advance to Checkpoint 1 
S 8 ? 100All will proceed to Checkpoint 
1 
Use of implicit confirmation in combination with 
correction subdialogues makes it easy to correct 
faulty interpretations as soon as possible by alert- 
ing the user to possible sources of error. 
In utterances 7 and 8, the system must resolve 
the user's reference, "CEV", to a particular unit. 
It therefore choes the user's command using the 
CEV's unique call sign. This makes explicit the sys- 
tem's interpretation of the user's utterance, giving 
the user a chance to correct he system if necessary. 
Note that utterance 4 also contains an implicit 
confirmation, since the system has resolved the 
user's gesture to a set of coordinates. 
5.3 C lar i f i ca t ion  Subd ia logues  
Ex.  3: 
U 9 
S 10 
U 11 
Create a CEV at 72 69 
? 
CEV, conduct crater breach facing- 
south 
S 12 ? There are two CEV's. Do you mean 
100All or 100A12? 
U 13 A12 
S 14 ? Which point should 100A12 conduct 
crater breach oriented south at? 
U 15 Checkpoint 2 
S 16 ? There is no Checkpoint 2. Which 
point should 100A12 conduct crater 
breach oriented south at? 
U 17 I meant Checkpoint 1 
S 18 ? 100A12 will conduct crater breach 
oriented south at Checkpoint 1 
Clarification subdialogues are generally initiated by 
the system as a result of errors or incomplete com- 
mands on the part of the user. 
Example 3 illustrates three different ypes of prob- 
lems that can be corrected by system questions. 
First, the user's reference to "CEV" in utterance 
11 is ambiguous, so the system asks a question to 
determine which CEV the user is referring to. Next, 
the system asks the user to supply a missing piece 
of information that is required to carry out the com- 
mand. Finally, when the user makes an error by 
referring to a point that doesn't exist, the system 
prompts for a correction. 
6 Discussion and Conclusions 
CommandTalk is an example of a successful and ro- 
bust dialogue system in a domain with limited ac- 
63 
cess to both data and subjects. The pre-dialogue 
version of CommandTalk was used in the STOW 
(Synthetic Theater of War) '97 ACTD (Advanced 
Concept Technology Demonstration) exercise, an in- 
tensive 48-hour continuous military simulation by 
all four U.S. military services, and received high 
praise. The dialogue portion of the system has in- 
creased CommandTalk's usefulness and robustness. 
Nevertheless, everal questions remain, not the least 
of which is whether the robustness techniques used 
for CommandTalk can be successfully transferred to 
other domains. 
We have no doubt that our methods for adding ro- 
bustness at the dialogue level can and should be im- 
plemented in other domains, but this is not as clear 
for our parsing a-nd recognition robustness methods. 
The one-grammar approach is key to our elimi- 
nating the necessity for robust parsing, renders a 
large corpus for generating a recognition model un- 
necessary, and has other advantages as well. Yet 
our experience in the ATIS domain suggests that 
further research into this approach is needed. Our 
ATIS grammar is based on a grammar of general 
English and has a very different structure from that 
of CommandTalk's semantic grammar, but we were 
unable to isolate the factor or factors responsible for 
its poor recognition performance. Recent research 
(Rayner et al, 2000) suggests that it may be pos- 
sible to compile a useful recognition model from a 
general English unification grammar if the gram- 
mar is constructed carefully and a few compromises 
are made. We also believe that using an appropri- 
ate grammar approximation algorithm to reduce the 
complexity of the recognition model may prove fruit- 
ful. This would reintroduce some discrepancy be- 
tween the recognition and understanding language 
models, but maintain the other advantages of the 
one-grammar pproach. 
In either case, the effectiveness of our recognition 
robustness techniques remains an open question. We 
know they have no significant negative impact on in- 
grammar ecognition, but whether they are helpful 
in recognizing and~ more importantly, interpreting 
out-of-grammar utterances is unknown. We have 
been unable to evaluate them so far in the Com- 
mandTalk or any other domain, although we hope 
to do so in the future. 
Another possible solution to the problem of 
producing a workable robust recognition grammar 
would return to a statistical approach rather than 
using word insertions and deletions. Stolcke and 
Segal (1994) describe a method for combining a 
context-free grammar with an n-gram model gen- 
erated from a small corpus of a few hundred utter- 
ances to create a more accurate n-gram model. This 
method would provide a robust recognition model 
based on the context-free grammar compiled from 
64 
our unification grammar. We would'still have to 
write only one grammar for the system, it would still 
influence the recognition model, and we could still 
be sure that the system would never say anything it 
couldn't recognize. This approach Would require us- 
ing robust parsing methods, but might be the best 
solution for other domains if compiling a practical 
recognition grammar proves too difficult. 
Despite the success of the CommandTalk system, 
it is clear that more investigation is called for to 
determine how best to develop dialogue systems in 
domains with limited data. Researchers must de- 
termine which types of unification grammars can be 
compiled into practical recognition grammars using 
existing technology, whether grammar approxima- 
tions or other techniques can produce good results 
for a broader range of grammars, whether allow- 
ing word insertions and deletions is an effective ro- 
bustness technique, orwhether we should use other 
methods altogether. 
Re ferences  
J. Dowding, J. Gawron, D. Appelt, L. Cherny, 
R. Moore, and D. Moran. 1993. Gemini: A Natu- 
ral Language System for Spoken Language Under- 
standing. In Proceedings of the Thirty-First An- 
nual Meeting of the ACL, Columbus, OH. Associ- 
ation for Computational Linguistics. 
J. Dowding, E. Owen Bratt, and S. Goldwater. 
1999. Interpreting Language in Context in Com- 
mandTalk. In Communicative Agents: The Use 
of Natural Language in Embodied Systems, pages 
63-67. 
D. Martin, A. Cheyer, and D. Moran. 1998. Build- 
ing Distributed Software Systems with the Open 
Agent Architecture. In Proceedings of the Third 
International Conference on the Practical Appli- 
cation of Intelligent Agents and Multi-Agent Tech- 
nology, Blackpool, Lancashire, UK. The Practical 
Application Company Ltd. 
R. Moore, J. Dowding, H. Bratt, J. Gawron, 
Y. Gorfu, and A. Cheyer. 1997. CommandTalk: 
A Spoken-Language Interface for Battlefield Sim- 
ulations. In Proceedings of the Fifth Conference 
on Applied Natural Language Processing, pages 
1-7, Washington, DC. Association for Computa- 
tional Linguistics. 
M. Rayner, B. A. Hockey, F. James, E. Owen Bratt, 
S. Goldwater, and J. M. Gawron. 2000. Compil- 
ing Language Models from a Linquistically Moti- 
vated Unification Grammar. Submitted to COL- 
ING '00. 
A. Stent, J. Dowding, J. Gawron, E. Owen Bratt, 
and R. Moore. 1999. The CommandTalk Spoken 
Dialogu.e System. In Proceedings of the 37th An- 
nual Meeting of the A CL. Association of Compu- 
tational Linguistics. 
A. Stolcke and J. Segal. 1994. Precise N-Gram 
Probabilities from Stochastic Context-free Gram- 
mar.: In Proceedings of the 32nd Annual Meeting 
off :the ~Association for Computational Linguistics, 
pages 74~-79, 
65 
Practical Issues in Compiling Typed Unification Grammars for Speech
Recognition
John Dowding Beth Ann Hockey
RIACS RIALIST Group
NASA Ames Research Center
Moffett Field, CA 94035
jdowding@riacs.edu
bahockey@riacs.edu
Jean Mark Gawron
Dept. of Linguistics
San Diego State University
San Diego, CA
gawron@mail.sdsu.edu
Christopher Culy
SRI International
333 Ravenswood Avenue
Menlo Park, CA 94025
culy@ai.sri.com
Abstract
Current alternatives for language mod-
eling are statistical techniques based
on large amounts of training data, and
hand-crafted context-free or finite-state
grammars that are difficult to build
and maintain. One way to address
the problems of the grammar-based ap-
proach is to compile recognition gram-
mars from grammars written in a more
expressive formalism. While theoreti-
cally straight-forward, the compilation
process can exceed memory and time
bounds, and might not always result in
accurate and efficient speech recogni-
tion. We will describe and evaluate two
approaches to this compilation prob-
lem. We will also describe and evalu-
ate additional techniques to reduce the
structural ambiguity of the language
model.
1 Introduction
Language models to constrain speech recogni-
tion are a crucial component of interactive spo-
ken language systems. The more varied the lan-
guage that must be recognized, the more critical
good language modeling becomes. Research in
language modeling has heavily favored statisti-
cal approaches (Cohen 1995, Ward 1995, Hu et
al. 1996, Iyer and Ostendorf 1997, Bellegarda
1999, Stolcke and Shriberg 1996) while hand-
coded finite-state or context-free language models
dominate the commercial sector (Nuance 2001,
SpeechWorks 2001, TellMe 2001, BeVocal 2001,
HeyAnita 2001, W3C 2001). The difference re-
volves around the availability of data. Research
systems can achieve impressive performance us-
ing statistical language models trained on large
amounts of domain-targeted data, but for many
domains sufficient data is not available. Data may
be unavailable because the domain has not been
explored before, the relevant data may be con-
fidential, or the system may be designed to do
new functions for which there is no human-human
analog interaction. The statistical approach is un-
workable in such cases for both the commercial
developers and for some research systems (Moore
et al 1997, Rayner et al 2000, Lemon et al
2001, Gauthron and Colineau 1999). Even in
cases for which there is no impediment to col-
lecting data, the expense and time required to col-
lect a corpus can be prohibitive. The existence
of the ATIS database (Dahl et al 1994) is no
doubt a factor in the popularity of the travel do-
main among the research community for exactly
this reason.
A major problem with grammar-based finite-
state or context-free language models is that they
can be tedious to build and difficult to maintain,
as they can become quite large very quickly as
the scope of the grammar increases. One way
to address this problem is to write the gram-
mar in a more expressive formalism and gener-
ate an approximation of this grammar in the for-
mat needed by the recognizer. This approach
has been used in several systems, CommandTalk
(Moore et al 1997), RIALIST PSA simula-
tor (Rayner et al 2000), WITAS (Lemon et al
2001), and SETHIVoice (Gauthron and Colin-
eau 1999). While theoretically straight-forward,
this approach is more demanding in practice, as
each of the compilation stages contains the po-
tential for a combinatorial explosion that will ex-
ceed memory and time bounds. There is also no
guarantee that the resulting language model will
lead to accurate and efficient speech recognition.
We will be interested in this paper in sound ap-
proximations (Pereira and Wright 1991) in which
the language accepted by the approximation is
a superset of language accepted by the original
grammar. While we conceed that alternative tech-
niques that are not sound (Black 1989, (Johnson
1998, Rayner and Carter 1996) may still be useful
for many purposes, we prefer sound approxima-
tions because there is no chance that the correct
hypothesis will be eliminated. Thus, further pro-
cessing techniques (for instance, N-best search)
will still have an opportunity to find the optimal
solution.
We will describe and evaluate two compilation
approaches to approximating a typed unification
grammar with a context-free grammar. We will
also describe and evaluate additional techniques
to reduce the size and structural ambiguity of the
language model.
2 Typed Unification Grammars
Typed Unification Grammars (TUG), like HPSG
(Pollard and Sag 1994) and Gemini (Dowding et
al. 1993) are a more expressive formalism in
which to write formal grammars1. As opposed to
atomic nonterminal symbols in a CFG, each non-
terminal in a TUG is a complex feature structure
(Shieber 1986) where features with values can be
attached. For example, the rule:
s[]   np:[num=N] vp:[num=N]
can be considered a shorthand for 2 context free
rules (assuming just two values for number):
s
 
np singular vp singular
s
 
np plural vp plural
1This paper specifically concerns grammars written in
the Gemini formalism. However, the basic issues involved in
compiling typed unification grammars to context-free gram-
mars remain the same across formalisms.
This expressiveness allows us to write grammars
with a small number of rules (from dozens to a
few hundred) that correspond to grammars with
large numbers of CF rules. Note that the approx-
imation need not incorporate all of the features
from the original grammar in order to provide a
sound approximation. In particular, in order to de-
rive a finite CF grammar, we will need to consider
only those features that have a finite number of
possible values, or at least consider only finitely
many of the possible values for infinitely valued
features. We can use the technique of restriction
(Shieber 1985) to remove these features from our
feature structures. Removing these features may
give us a more permissive language model, but it
will still be a sound approximation.
The experimental results reported in this pa-
per are based on a grammar under development
at RIACS for a spoken dialogue interface to a
semi-autonomous robot, the Personal Satellite
Assistant (PSA). We consider this grammar to be
medium-sized, with 61 grammar rules and 424
lexical entries. While this may sound small, if
the grammar were expanded by instantiating vari-
ables in all legal permutations, it would contain
over 	 context-free rules.
3 The Compilation Process
We will be studying the compilation process
to convert typed unification grammars expressed
in Gemini notation into language models for
use with the Nuance speech recognizer (Nuance,
2001). We are using Nuance in part because it
supports context-free language models, which is
not yet industry standard.2 Figure 1 illustrates the
stages of processing: a typed unification grammar
is first compiled to a context-free grammar. This
is in turn converted into a grammar in Nuance?s
Grammar Specification Language (GSL), which
is a form of context-free grammar in a BNF-like
notation, with one rule defining each nonterminal,
and allowing alternation and Kleene closure on
the right-hand-side. Critically, the GSL must not
contain any left-recursion, which must be elimi-
nated before the GSL representation is produced.
2The standard is moving in the direction of context-
free language models, as can be seen in the draft standard
for Speech Recognition Grammars being developed by the
World Wide Web Consortium (W3C 2001).
Context Free Grammar
TUG to CFG Compiler
nuance_compiler
GSL Grammar
CFG to GSL Conversion
Recognition System
        Package
Typed Unification Grammar (TUG)
Figure 1: Compilation Process
The GSL representation is then compiled into a
Nuance package with the nuance compiler.
This package is the input to the speech recognizer.
In our experience, each of the compilation stages,
as well as speech recognition itself, has the po-
tential to lead to a combinatorial explosion that
exceeds practical memory or time bounds.
We will now describe implementations of the
first stage, generating a context-free grammar
from a typed unification grammar, by two differ-
ent algorithms, one defined by Kiefer and Krieger
(2000) and one by Moore and Gawron, described
in Moore (1998) The critical difficulty for both
of these approaches is how to select the set of
derived nonterminals that will appear in the final
CFG.
3.1 Kiefer&Krieger?s Algorithm
The algorithm of Kiefer&Krieger (K&K) divides
this compilation step into two phases: first, the
set of context-free nonterminals is determined by
iterating a bottom-up search until a least fixed-
point is reached; second, this least fixed-point is
used to instantiate the set of context-free produc-


 for each l 
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 296?301,
Baltimore, Maryland, USA, June 23-25 2014.
c
?2014 Association for Computational Linguistics
Improving sparse word similarity models with asymmetric measures
Jean Mark Gawron
San Diego State University
gawron@mail.sdsu.edu
Abstract
We show that asymmetric models based on
Tversky (1977) improve correlations with
human similarity judgments and nearest
neighbor discovery for both frequent and
middle-rank words. In accord with Tver-
sky?s discovery that asymmetric similarity
judgments arise when comparing sparse
and rich representations, improvement on
our two tasks can be traced to heavily
weighting the feature bias toward the rarer
word when comparing high- and mid-
frequency words.
1 Introduction
A key assumption of most models of similarity is
that a similarity relation is symmetric. This as-
sumption is foundational for some conceptions,
such as the idea of a similarity space, in which
similarity is the inverse of distance; and it is deeply
embedded into many of the algorithms that build
on a similarity relation among objects, such as
clustering algorithms. The symmetry assumption
is not, however, universal, and it is not essential
to all applications of similarity, especially when it
comes to modeling human similarity judgments.
Citing a number of empirical studies, Tversky
(1977) calls symmetry directly into question, and
proposes two general models that abandon sym-
metry. The one most directly related to a large
body of word similarity work that followed is what
he calls the ratio model, which defines sim(a, b)
as:
f(A ? B)
f(A ? B) + ?f(A\B) + ?f(B\A)
(1)
Here A and B represent feature sets for the objects
a and b respectively; the term in the numerator is a
function of the set of shared features, a measure of
similarity, and the last two terms in the denomina-
tor measure dissimilarity: ? and ? are real-number
weights; when ? 6= ?, symmetry is abandoned.
To motivate such a measure, Tversky presents
experimental data with asymmetric similarity re-
sults, including similarity comparisons of coun-
tries, line drawings of faces, and letters. Tversky
shows that many similarity judgment tasks have
an inherent asymmetry; but he also argues, fol-
lowing Rosch (1975), that certain kinds of stimuli
are more naturally used as foci or standards than
others. Goldstone (in press) summarizes the re-
sults succinctly: ?Asymmetrical similarity occurs
when an object with many features is judged as
less similar to a sparser object than vice versa; for
example, North Korea is judged to be more like
China than China is [like] North Korea.? Thus,
one source of asymmetry is the comparison of
sparse and dense representations.
The relevance of such considerations to word
similarity becomes clear when we consider that
for many applications, word similarity measures
need to be well-defined when comparing very fre-
quent words with infrequent words. To make this
concrete, let us consider a word representation
in the word-as-vector paradigm (Lee, 1997; Lin,
1998), using a dependency-based model. Sup-
pose we want to measure the semantic similarity
of boat, rank 682 among the nouns in the BNC
corpus studied below, which has 1057 nonzero
dependency features based on 50 million words
of data, with dinghy, rank 6200, which has only
113 nonzero features. At the level of the vec-
tor representations we are using, these are events
of very different dimensionality; that is, there are
ten times as many features in the representation of
boat as there are in the representation of dinghy. If
in Tversky/Rosch terms, the more frequent word
is also a more likely focus, then this is exactly
the kind of situation in which asymmetric similar-
ity judgments will arise. Below we show that an
296
asymmetric measure, using ? and ? biased in fa-
vor of the less frequent word, greatly improves the
performance of a dependency-based vector model
in capturing human similarity judgments.
Before presenting these results, it will be help-
ful to slightly reformulate and slightly generalize
Tversky?s ratio model. The reformulation will al-
low us to directly draw the connection between
the ratio model and a set of similarity measures
that have played key roles in the similarity litera-
ture. First, since Tversky has primarily additive f
in mind, we can reformulate f(A ? B) as follows
f(A ? B) =
?
f?A?B
wght(f) (2)
Next, since we are interested in generalizing from
sets of features, to real-valued vectors of features,
w
1
, w
2
, we define
?
SI
(w
1
, w
2
) =
?
f?w
1
?w
2
SI(w
1
[f ], w
2
[f ]).
(3)
Here SI is some numerical operation on real-
number feature values (SI stands for shared infor-
mation). If the operation is MIN and w
1
[f ] and
w
2
[f ] both contain the feature weights for f , then
?
f?A?B
wght(f)= ?
MIN
(w
1
, w
2
)
=
?
f?w
1
?w
2
MIN(w
1
[f ], w
2
[f ]),
so with SI set to MIN, Equation (3) includes Equa-
tion (2) as a special case. Similarly, ?(w
1
, w
1
)
represents the summed feature weights of w
1
, and
therefore,
f(w
1
\w
2
) = ?(w
1
, w
1
) ? ?(w
1
, w
2
)
In this generalized form, then, (1) becomes
?(w
1
,w
2
)
?(w
1
,w
2
)+?[?(w
1
,w
1
)??(w
1
,w
2
)]+?[?(w
2
,w
2
)??(w
1
,w
2
)]
=
?(w
1
,w
2
)
??(w
1
,w
1
)+??(w
2
,w
2
)+?(w
1
,w
2
)?(?+?)?(w
1
,w
2
)
(4)
Thus, if ? + ? = 1, Tversky?s ratio model be-
comes simply:
sim(w
1
, w
2
) =
?(w
1
,w
2
)
??(w
1
,w
1
)+(1??)?(w
2
,w
2
)
(5)
The computational advantage of this reformula-
tion is that the core similarity operation ?(w
1
, w
2
)
is done on what is generally only a small number
of shared features, and the ?(w
i
, w
i
) calculations
(which we will call self-similarities), can be com-
puted in advance. Note that sim(w
1
, w
2
) is sym-
metric if and only if ? = 0.5. When ? > 0.5,
sim(w
1
, w2) is biased in favor of w
1
as the refer-
ent; When ? < 0.5, sim(w
1
, w2) is biased in favor
of w
2
.
Consider four similarity functions that have
played important roles in the literature on similar-
ity:
DICE PROD(w
1
, w
2
) =
2?w
1
?w
2
?w
1
?
2
+?w
2
?
2
DICE
?
(w
1
, w
2
) =
2?
?
f?w
1
?w
2
min(w
1
[f ], w
2
[f ])
?
w
1
[f ]+
?
w
2
[f ]
LIN(w
1
, w
2
) =
?
f?w
1
?w
2
w
1
[f ]+ w
2
[f ]
?
w
1
[f ]+
?
w
2
[f ]
COS(w
1
, w
2
) = DICE PROD applied
to unit vectors
(6)
The function DICE PROD is not well known in the
word similarity literature, but in the data mining
literature it is often just called Dice coefficient, be-
cause it generalized the set comparison function
of Dice (1945). Observe that cosine is a special
case of DICE PROD. DICE
?
was introduced in Cur-
ran (2004) and was the most successful function
in his evaluation. Since LIN was introduced in Lin
(1998); several different functions have born that
name. The version used here is the one used in
Curran (2004).
The three distinct functions in Equation 6 have
a similar form. In fact, all can be defined in terms
of ? functions differing only in their SI operation.
Let ?
SI
be a shared feature sum for operation SI,
as defined in Equation (3). We define the Tversky-
normalized version of ?
SI
, written T
SI
, as:
1
T
SI
(w
1
, w
2
) =
2 ? ?
SI
(w
1
, w
2
)
?
SI
(w
1
, w
1
) + ?
SI
(w
2
, w
2
)
(7)
Note that T
SI
is just the special case of Tversky?s
ratio model (5) in which ? = 0.5 and the similarity
measure is symmetric.
We define three SI operations ?
PROD
2
, ?
MIN
, and
?
AVG
as follows:
SI ?
SI
(w
1
, w
2
)
PROD
?
f?w
1
?w
2
w
1
[f ] ? w
2
[f ]
AVG
?
f?w
1
?w
2
w
1
[f ]+w
2
[f ]
2
MIN
?
f?w
1
?w
2
MIN(w
1
[f ], w
2
[f ])
1
Paralleling (7) is Jaccard-family normalization:
?
JACC
(w
1
, w
2
) =
?(w
1
, w
2
)
?(w
1
, w
1
) + ?(w
2
, w
2
)? ?(w
1
, w
2
)
It is easy to generalize the result from van Rijsbergen (1979)
for the original set-specific versions of Dice and Jaccard, and
show that all of the Tversky family functions discussed above
are monotonic in Jaccard.
2
?
PROD
, of course, is dot product.
297
This yields the three similarity functions cited
above:
DICE PROD(w
1
, w
2
) =T
PROD
(w
1
, w
2
)
DICE
?
(w
1
, w
2
) =T
MIN
(w
1
, w
2
)
LIN(w
1
, w
2
) =T
AVG
(w
1
, w
2
)
(8)
Thus, all three of these functions are special cases
of symmetric ratio models. Below, we investigate
asymmetric versions of all three, which we write
as T
?,SI
(w
1
, w
2
), defined as:
?
SI
(w
1
, w
2
)
? ? ?
SI
(w
1
, w
1
) + (1 ? ?) ? ?
SI
(w
2
, w
2
)
(9)
Following Lee (1997), who investigates a different
family of asymmetric similarity functions, we will
refer to these as ?-skewed measures.
We also will look at a rank-biased family of
measures:
R
?,SI
(w
1
, w
2
) = T
?,SI
(w
h
, w
l
)
wherew
l
= argmin
w?{w
1
,w
2
}
Rank(w)
w
h
= argmax
w?{w
1
,w
2
}
Rank(w)
(10)
Here, T
?,SI
(w
h
, w
l
) is as defined in (9), and the ?-
weighted word is always the less frequent word.
For example, consider comparing the 100-feature
vector for dinghy to the 1000 feature vector for
boat: if ? is high, we give more weight to the pro-
portion of dinghy?s features that are shared than
we give to the proportion of boat?s features that
are shared.
In the following sections we present data show-
ing that the performance of a dependency-based
similarity system in capturing human similarity
judgments can be greatly improved with rank-
bias and ?-skewing. We will investigate the three
asymmetric functions defined above.
3
We argue
that the advantages of rank bias are tied to im-
proved similarity estimation when comparing vec-
tors of very different dimensionality. We then
turn to the problem of finding a word?s nearest
semantic neighbors. The nearest neighbor prob-
lem is a rather a natural ground in which to try
out ideas on asymmetry, since the nearest neigh-
bor relation is itself not symmetrical. We show
that ?-skewing can be used to improve the quality
of nearest neighbors found for both high- and mid-
frequency words.
3
Interestingly, Equation (9) does not yield an asymmetric
version of cosine. Plugging unit vectors into the ?-skewed
version of DICE PROD still leaves us with a symmetric func-
tion (COS), whatever the value of ?.
2 Systems
1. We parsed the BNC with the Malt Depen-
dency parser (Nivre, 2003) and the Stanford
parser (Klein and Manning, 2003), creating
two dependency DBs, using basically the de-
sign in Lin (1998), with features weighted by
PMI (Church and Hanks, 1990).
2. For each of the 3 rank-biased similarity sys-
tems (R
?,SI
) and cosine, we computed corre-
lations with human judgments for the pairs
in 2 standard wordsets: the combined Miller-
Charles/Rubenstein-Goodenough word sets
(Miller and Charles, 1991; Rubenstein and
Goodenough, 1965) and the Wordsim 353
word set (Finkelstein et al, 2002), as well as
to a subset of the Wordsim set restricted to
reflect semantic similarity judgments, which
we will refer to as Wordsim 201.
3. For each of 3 ?-skewed similarity systems
(T
?,SI
) and cosine, we found the nearest
neighbor from among BNC nouns (of any
rank) for the 10,000 most frequent BNC
nouns using the the dependency DB created
in step 2.
4. To evaluate of the quality of the nearest
neighbors pairs found in Step 4, we scored
them using the Wordnet-based Personalized
Pagerank system described in Agirre (2009)
(UKB), a non distributional WordNet based
measure, and the best system in Table 1.
3 Human correlations
Table 1 presents the Spearman?s correlation
with human judgments for Cosine, UKB, and
our 3 ?-skewed models using Malt-parser
based vectors applied to the combined Miller-
Charles/Rubenstein-Goodenough word sets, the
Wordsim 353 word set, and the Wordsim 202
word set.
The first of each of the column pairs is a sym-
metric system, and the second a rank-biased vari-
ant, based on Equation (10). In all cases, the bi-
ased system improves on the performance of its
symmetric counterpart; in the case of DICE
?
and
DICE PROD, that improvement is enough for the
biased system to outperform cosine, the best of
the symmetric distributionally based systems. The
value .97 was chosen for ? because it produced the
best ?-system on the MC/RG corpus. That value
298
MC/RG Wdsm201 Wdsm353
? = .5 ? = .97 ? = .5 ? = .97 ? = .5 ? = .97
Dice DICE PROD .59 .71 .50 .60 .35 .44
LIN .48 .62 .42 .54 .29 .39
DICE
?
.58 .67 .49 .58 .34 .43
Euc Cosine .65 NA .56 NA .41 NA
WN UKB WN .80 NA .75 NA .68 NA
Table 1: System/Human correlations. Above the line: MALT Parser-based systems
0.5 0.6 0.7 0.8 0.9 1.0
? value
0.34
0.36
0.38
0.40
0.42
c
o
r
r
e
l
a
t
i
o
n
Figure 1: Scores monotonically increase with ?
is probably probably an overtrained optimum. The
point is that ?-skewing always helps: For all three
systems, the improvement shown in raising ? from
.5 to whatever the optimum is is monotonic. This
is shown in Figure 1. Table 2 shows very simi-
lar results using the Stanford parser, demonstrat-
ing the pattern is not limited to a single parsing
model.
In Table 3, we list the pairs whose reranking
on the MC/RG dataset contributed most to the im-
provement of the ? = .9 system over the default
? = .5 system. In the last column an approxi-
mation of the amount of correlation improvement
provided by that pair (?):
4
Note the 3 of the 5
items contributing the most improvement this sys-
tem were pairs with a large difference in rank.
Choosing ? = .9, weights recall toward the rarer
word. We conjecture that the reason this helps is
Tversky?s principle: It is natural to use the sparser
4
The approximation is based on the formula for comput-
ing Spearman?s R with no ties. If n is the number of items,
then the improvement on that item is:
6 ? [(baseline ? gold)
2
? (test? gold)
2
]
n ? (n
2
? 1)
Word 1 Rank Word 2 Rank ?
automobile 7411 car 100 0.030
asylum 3540 madhouse 14703 0.020
coast 708 hill 949 0.018
mound 3089 stove 2885 0.017
autograph 10136 signature 2743 0.009
Table 3: Pairs contributing the biggest improve-
ment, MC/RG word set
representation as the focus in the comparison.
4 Nearest neighbors
Figure 2 gives the results of our nearest neighbor
study on the BNC for the case of DICE PROD. The
graphs for the other two ?-skewed systems are
nearly identical, and are not shown due to space
limitations. The target word, the word whose
nearest neighbor is being found, always receives
the weight 1 ? ?. The x-axis shows target word
rank; the y-axis shows the average UKB simi-
larity scores assigned to nearest neighbors every
50 ranks. All the systems show degraded nearest
neighbor quality as target words grow rare, but at
lower ranks, the ? = .04 nearest neighbor system
fares considerably better than the symmetric ? =
.50 system; the line across the bottom tracks the
score of a system with randomly generated near-
est neighbors. The symmetric DICE PROD sys-
tem is as an excellent nearest neighbor system at
high ranks but drops below the ? = .04 system at
around rank 3500. We see that the ? = .8 system
is even better than the symmetric system at high
ranks, but degrades much more quickly.
We explain these results on the basis of the prin-
ciple developed for the human correlation data: To
reflect natural judgments of similarity for compar-
isons of representations of differing sparseness, ?
should be tipped toward the sparser representation.
Thus, ? = .80 works best for high rank tar-
get words, because most nearest neighbor candi-
299
MC/RG Wdsm201 Wdsm353
? = .5 opt opt ? ? = .5 opt opt ? ? = .5 opt opt ?
DICE PROD .65 .70 .86 .42 .57 .99 .36 .44 .98
LIN .58 .68 .90 .41 .56 .94 .30 .41 .99
DICE
?
.60 .71 .91 .43 .53 .99 .32 .43 .99
Table 2: System/Human correlations for Stanford parser systems
0 2000 4000 6000 8000 10000
Word rank
0.000
0.005
0.010
0.015
0.020
0.025
0.030
0.035
A
v
g
 
s
i
m
i
l
a
r
i
t
y
alpha04
alpha_50
alpha_80
random
Figure 2: UKB evaluation scores for nearest
neighbor pairs across word ranks, sampled every
50 ranks.
dates are less frequent, and ? = .8 tips the bal-
ance toward the nontarget words. On the other
hand, when the target word is a low ranking word,
a high ? weight means it never receives the high-
est weight, and this is disastrous, since most good
candidates are higher ranking. Conversely, ? =
.04 works better.
5 Previous work
The debt owed to Tversky (1977) has been made
clear in the introduction. Less clear is the debt
owed to Jimenez et al (2012), which also pro-
poses an asymmetric similarity framework based
on Tversky?s insights. Jimenez et al showed the
continued relevance of Tversky?s work.
Motivated by the problem of measuring how
well the distribution of one word w
1
captures the
distribution of another w
2
, Weeds and Weir (2005)
also explore asymmetric models, expressing sim-
ilarity calculations as weighted combinations of
several variants of what they call precision and re-
call. Some of their models are also Tverskyan ratio
models. To see this, we divide (9) everywhere by
?(w
1
, w
2
):
T
SI
(w
1
, w
2
) =
1
???(w
1
,w
1
)
?(w
1
,w
2
)
+
(1??)??(w
2
,w
2
)
?(w
1
,w
2
)
If the SI is MIN, then the two terms in the de-
nominator are the inverses of what W&W call
difference-weighted precision and recall:
PREC(w
1
, w
2
) =
?
MIN
(w
1
,w
2
)
?
MIN
(w
1
,w
1
)
REC(w
1
, w
2
) =
?
MIN
(w
1
,w
2
)
?
MIN
(w
2
,w
2
)
,
So for T
MIN
, (9) can be rewritten:
1
?
PREC(w
1
,w
2
)
+
1??
REC(w
1
,w
2
)
That is, T
MIN
is a weighted harmonic mean of
precision and recall, the so-called weighted F-
measure (Manning and Schu?tze, 1999). W&W?s
additive precision/recall models appear not to be
Tversky models, since they compute separate
sums for precision and recall from the f ? w
1
?
w
2
, one using w
1
[f ], and one using w
2
[f ].
Long before Weed and Weir, Lee (1999) pro-
posed an asymmetric similarity measure as well.
Like Weeds and Weir, her perspective was to cal-
culate the effectiveness of using one distribution as
a proxy for the other, a fundamentally asymmetric
problem. For distributions q and r, Lee?s ?-skew
divergence takes the KL-divergence of a mixture
of q and r from q, using the ? parameter to define
the proportions in the mixture.
6 Conclusion
We have shown that Tversky?s asymmetric ratio
models can improve performance in capturing
human judgments and produce better nearest
neighbors. To validate these very preliminary
results, we need to explore applications compat-
ible with asymmetry, such as the TOEFL-like
synonym discovery task in Freitag et al (2005),
and the PP-attachment task in Dagan et al (1999).
Acknowledgments
This work reported here was supported by NSF
CDI grant # 1028177.
300
References
E. Agirre, E. Alfonseca, K. Hall, J. Kravalova,
M. Pasca, and A. Soroa. 2009. A study on similar-
ity and relatedness using distributional and wordnet-
based approaches. In Proceedings of NAACL-HLT
09, Boulder, Co.
K.W. Church and P. Hanks. 1990. Word association
norms, mutual information, and lexicography. Com-
putational linguistics, 16(1):22?29.
J.R. Curran. 2004. From Distributional to Semantic
Similarity. Ph.D. thesis, University of Edinburgh.
College of Science and Engineering. School of In-
formatics.
I. Dagan, L. Lee, and F.C.N. Pereira. 1999. Similarity-
based models of word cooccurrence probabilities.
Machine Learning, 34(1):43?69.
L.R. Dice. 1945. Measures of the amount of ecologic
association between species. Ecology, 26(3):297?
302.
L. Finkelstein, E. Gabrilovich, Yossi Matias, Ehud
Rivlin, Zach Solan, Gadi Wolfman, and Eytan Rup-
pin. 2002. Placing search in context: The concept
revisited. ACM Transactions on Information Sys-
tems, 20(1):116?131.
D. Freitag, M. Blume, J. Byrnes, E. Chow, S. Kapadia,
R. Rohwer, and Z.Wang. 2005. New experiments in
distributional representations of synonymy. In Pro-
ceedings of the Ninth Conference on Computational
Natural Language Learning, pages 25?32. Associa-
tion for Computational Linguistics.
R. L. Goldstone. in press. Similarity. In R.A. Wilson
Wilson and F. C. Keil, editors, MIT Encylcopedia of
Cognitive Sciences. MIT Press, Cambridge, MA.
S. Jimenez, C. Becerra, and A. Gelbukh. 2012. Soft
cardinality: A parameterized similarity function for
text comparison. In Proceedings of the First Joint
Conference on Lexical and Computational Seman-
tics, pages 449?453. Association for Computational
Linguistics.
D. Klein and Christopher D. Manning. 2003. Fast ex-
act inference with a factored model for natural lan-
guage parsing. In Advances in Neural Information
Processing Systems 15 (NIPS 2002), pages 3?10,
Cambridge, MA. MIT Press.
L. Lee. 1997. Similarity-based approaches to natural
language processing. Ph.D. thesis, Harvard Univer-
sity.
L. Lee. 1999. Measures of distributional similarity.
In Proceedings of the 37th annual meeting of the
Association for Computational Linguistics on Com-
putational Linguistics, pages 25?32. Association for
Computational Linguistics.
D. Lin. 1998. Automatic retrieval and clustering of
similar words. In Annual Meeting-Association for
Computational Linguistics, volume 36, pages 768?
774. Association for Computational Linguistics.
C.D. Manning and H. Schu?tze. 1999. Foundations of
statistical natural language processing. MIT Press,
Cambridge.
G.A. Miller and W.G. Charles. 1991. Contextual cor-
relates of semantic similarity. Language and Cogni-
tive Processes, 6(1):1?28.
J. Nivre. 2003. An efficient algorithm for projective
dependency parsing. In Proceedings of the 8th Inter-
national Workshop on Parsing Technologies (IWPT
03), pages 149?160.
E. Rosch and C. B. Mervis. 1975. Family resem-
blances: Studies in the internal structure of cate-
gories. Cognitive psychology, 7(4):573?605.
H. Rubenstein and J.B. Goodenough. 1965. Contex-
tual correlates of synonymy. Communications of the
ACM, 8:627?633.
A. Tversky. 1977. Features of similarity. Psychologi-
cal Review, 84:327?352.
C. J. van Rijsbergen. 1979. Information retrieval.
Butterworth-Heinemann, Oxford.
J. Weeds and D. Weir. 2005. Co-occurrence retrieval:
A flexible framework for lexical distributional simi-
larity. Computational Linguistics, 31(4):439?475.
301
