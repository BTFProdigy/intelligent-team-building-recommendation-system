Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 464?471,
Prague, Czech Republic, June 2007. c?2007 Association for Computational Linguistics
Statistical Machine Translation for Query Expansion in Answer Retrieval
Stefan Riezler, Alexander Vasserman, Ioannis Tsochantaridis, Vibhu Mittal and Yi Liu
Google Inc., 1600 Amphitheatre Parkway, Mountain View, CA 94043
{riezler|avasserm|ioannis|vibhu|yliu}@google.com
Abstract
We present an approach to query expan-
sion in answer retrieval that uses Statisti-
cal Machine Translation (SMT) techniques
to bridge the lexical gap between ques-
tions and answers. SMT-based query ex-
pansion is done by i) using a full-sentence
paraphraser to introduce synonyms in con-
text of the entire query, and ii) by trans-
lating query terms into answer terms us-
ing a full-sentence SMT model trained on
question-answer pairs. We evaluate these
global, context-aware query expansion tech-
niques on tfidf retrieval from 10 million
question-answer pairs extracted from FAQ
pages. Experimental results show that SMT-
based expansion improves retrieval perfor-
mance over local expansion and over re-
trieval without expansion.
1 Introduction
One of the fundamental problems in Question An-
swering (QA) has been recognized to be the ?lexi-
cal chasm? (Berger et al, 2000) between question
strings and answer strings. This problem is mani-
fested in a mismatch between question and answer
vocabularies, and is aggravated by the inherent am-
biguity of natural language. Several approaches have
been presented that apply natural language process-
ing technology to close this gap. For example, syn-
tactic information has been deployed to reformu-
late questions (Hermjakob et al, 2002) or to re-
place questions by syntactically similar ones (Lin
and Pantel, 2001); lexical ontologies such as Word-
net1 have been used to find synonyms for question
words (Burke et al, 1997; Hovy et al, 2000; Prager
et al, 2001; Harabagiu et al, 2001), and statisti-
cal machine translation (SMT) models trained on
question-answer pairs have been used to rank can-
didate answers according to their translation prob-
abilities (Berger et al, 2000; Echihabi and Marcu,
2003; Soricut and Brill, 2006). Information retrieval
(IR) is faced by a similar fundamental problem of
?term mismatch? between queries and documents.
A standard IR solution, query expansion, attempts to
increase the chances of matching words in relevant
documents by adding terms with similar statistical
properties to those in the original query (Voorhees,
1994; Qiu and Frei, 1993; Xu and Croft, 1996).
In this paper we will concentrate on the task of
answer retrieval from FAQ pages, i.e., an IR prob-
lem where user queries are matched against docu-
ments consisting of question-answer pairs found in
FAQ pages. Equivalently, this is a QA problem that
concentrates on finding answers given FAQ docu-
ments that are known to contain the answers. Our
approach to close the lexical gap in this setting at-
tempts to marry QA and IR technology by deploy-
ing SMT methods for query expansion in answer
retrieval. We present two approaches to SMT-based
query expansion, both of which are implemented in
the framework of phrase-based SMT (Och and Ney,
2004; Koehn et al, 2003).
Our first query expansion model trains an end-
to-end phrase-based SMT model on 10 million
question-answer pairs extracted from FAQ pages.
1http://wordnet.princeton.edu
464
The goal of this system is to learn lexical correla-
tions between words and phrases in questions and
answers, for example by allowing for multiple un-
aligned words in automatic word alignment, and dis-
regarding issues such as word order. The ability to
translate phrases instead of words and the use of a
large language model serve as rich context to make
precise decisions in the case of ambiguous transla-
tions. Query expansion is performed by adding con-
tent words that have not been seen in the original
query from the n-best translations of the query.
Our second query expansion model is based on
the use of SMT technology for full-sentence para-
phrasing. A phrase table of paraphrases is extracted
from bilingual phrase tables (Bannard and Callison-
Burch, 2005), and paraphrasing quality is improved
by additional discriminative training on manually
created paraphrases. This approach utilizes large
bilingual phrase tables as information source to ex-
tract a table of para-phrases. Synonyms for query
expansion are read off from the n-best paraphrases
of full queries instead of from paraphrases of sep-
arate words or phrases. This allows the model to
take advantage of the rich context of a large n-gram
language model when adding terms from the n-best
paraphrases to the original query.
In our experimental evaluation we deploy a
database of question-answer pairs extracted from
FAQ pages for both training a question-answer
translation model, and for a comparative evalua-
tion of different systems on the task of answer re-
trieval. Retrieval is based on the tfidf framework
of Jijkoun and de Rijke (2005), and query expan-
sion is done straightforwardly by adding expansion
terms to the query for a second retrieval cycle. We
compare our global, context-aware query expansion
techniques with Jijkoun and de Rijke?s (2005) tfidf
model for answer retrieval and a local query expan-
sion technique (Xu and Croft, 1996). Experimen-
tal results show a significant improvement of SMT-
based query expansion over both baselines.
2 Related Work
QA has approached the problem of the lexical gap
by various techniques for question reformulation,
including rule-based syntactic and semantic refor-
mulation patterns (Hermjakob et al, 2002), refor-
mulations based on shared dependency parses (Lin
and Pantel, 2001), or various uses of the Word-
Net ontology to close the lexical gap word-by-word
(Hovy et al, 2000; Prager et al, 2001; Harabagiu
et al, 2001). Another use of natural language pro-
cessing has been the deployment of SMT models on
question-answer pairs for (re)ranking candidate an-
swers which were either assumed to be contained
in FAQ pages (Berger et al, 2000) or retrieved by
baseline systems (Echihabi and Marcu, 2003; Sori-
cut and Brill, 2006).
IR has approached the term mismatch problem by
various approaches to query expansion (Voorhees,
1994; Qiu and Frei, 1993; Xu and Croft, 1996).
Inconclusive results have been reported for tech-
niques that expand query terms separately by adding
strongly related terms from an external thesaurus
such as WordNet (Voorhees, 1994). Significant
improvements in retrieval performance could be
achieved by global expansion techniques that com-
pute corpus-wide statistics and take the entire query,
or query concept (Qiu and Frei, 1993), into account,
or by local expansion techniques that select expan-
sion terms from the top ranked documents retrieved
by the original query (Xu and Croft, 1996).
A similar picture emerges for query expansion
in QA: Mixed results have been reported for word-
by-word expansion based on WordNet (Burke et
al., 1997; Hovy et al, 2000; Prager et al, 2001;
Harabagiu et al, 2001). Considerable improvements
have been reported for the use of the local context
analysis model of Xu and Croft (1996) in the QA
system of Ittycheriah et al (2001), or for the sys-
tems of Agichtein et al (2004) or Harabagiu and
Lacatusu (2004) that use FAQ data to learn how to
expand query terms by answer terms.
The SMT-based approaches presented in this pa-
per can be seen as global query expansion tech-
niques in that our question-answer translation model
uses the whole question-answer corpus as informa-
tion source, and our approach to paraphrasing de-
ploys large amounts of bilingual phrases as high-
coverage information source for synonym finding.
Furthermore, both approaches take the entire query
context into account when proposing to add new
terms to the original query. The approaches that
are closest to our models are the SMT approach of
Radev et al (2001) and the paraphrasing approach
465
web pages FAQ pages QA pairs
count 4 billion 795,483 10,568,160
Table 1: Corpus statistics of QA pair data
of Duboue and Chu-Carroll (2006). None of these
approaches defines the problem of the lexical gap
as a query expansion problem, and both approaches
use much simpler SMT models than our systems,
e.g., Radev et al (2001) neglect to use a language
model to aid disambiguation of translation choices,
and Duboue and Chu-Carroll (2006) use SMT as
black box altogether.
In sum, our approach differs from previous work
in QA and IR in the use SMT technology for query
expansion, and should be applicable in both areas
even though experimental results are only given for
the restricted domain of retrieval from FAQ pages.
3 Question-Answer Pairs from FAQ Pages
Large-scale collection of question-answer pairs has
been hampered in previous work by the small sizes
of publicly available FAQ collections or by restricted
access to retrieval results via public APIs of search
engines. Jijkoun and de Rijke (2005) nevertheless
managed to extract around 300,000 FAQ pages
and 2.8 million question-answer pairs by repeatedly
querying search engines with ?intitle:faq?
and ?inurl:faq?. Soricut and Brill (2006) could
deploy a proprietary URL collection of 1 billion
URLs to extract 2.3 million FAQ pages contain-
ing the uncased string ?faq? in the url string. The
extraction of question-answer pairs amounted to a
database of 1 million pairs in their experiment.
However, inspection of the publicly available Web-
FAQ collection provided by Jijkoun and de Rijke2
showed a great amount of noise in the retrieved
FAQ pages and question-answer pairs, and yet the
indexed question-answer pairs showed a serious re-
call problem in that no answer could be retrieved for
many well-formed queries. For our experiment, we
decided to prefer precision over recall and to attempt
a precision-oriented FAQ and question-answer pair
extraction that benefits the training of question-
answer translation models.
2http://ilps.science.uva.nl/Resources/WazDah/
As shown in Table 1, the FAQ pages used in our
experiment were extracted from a 4 billion page
subset of the web using the queries ?inurl:faq?
and ?inurl:faqs? to match the tokens ?faq? or
?faqs? in the urls. This extraction resulted in 2.6
million web pages (0.07% of the crawl). Since not
all those pages are actually FAQs, we manually la-
beled 1,000 of those pages to train an online passive-
aggressive classificier (Crammer et al, 2006) in a
10-fold cross validation setup. Training was done
using 20 feature functions on occurrences question
marks and key words in different fields of web
pages, and resulted in an F1 score of around 90%
for FAQ classification. Application of the classifier
to the extracted web pages resulted in a classification
of 795,483 pages as FAQ pages.
The extraction of question-answer pairs from this
database of FAQ pages was performed again in a
precision-oriented manner. The goal of this step
was to extract url, title, question, and answers fields
from the question-answer pairs in FAQ pages. This
was achieved by using feature functions on punc-
tuations, HTML tags (e.g., <p>, <BR>), listing
markers (e.g., Q:, (1)), and lexical cues (e.g.,
What, How), and an algorithm similar to Joachims
(2003) to propagate initial labels across similar text
pieces. The result of this extraction step is a database
of about 10 million question answer pairs (13.3
pairs per FAQ page). A manual evaluation of 100
documents, containing 1,303 question-answer pairs,
achieved a precision of 98% and a recall of 82% for
extracting question-answer pairs.
4 SMT-Based Query Expansion
Our SMT-based query expansion techniques are
based on a recent implementation of the phrase-
based SMT framework (Koehn et al, 2003; Och and
Ney, 2004). The probability of translating a foreign
sentence f into English e is defined in the noisy chan-
nel model as
argmax
e
p(e|f) = argmax
e
p(f|e)p(e) (1)
This allows for a separation of a language model
p(e), and a translation model p(f|e). Translation
probabilities are calculated from relative frequencies
of phrases, which are extracted via various heuris-
tics as larger blocks of aligned words from best word
466
alignments. Word alignments are estimated by mod-
els similar to Brown et al (1993). For a sequence of
I phrases, the translation probability in equation (1)
can be decomposed into
p(f Ii |e
I
i ) =
I?
i=1
p(fi|ei) (2)
Recent SMT models have shown significant im-
provements in translation quality by improved mod-
eling of local word order and idiomatic expressions
through the use of phrases, and by the deployment
of large n-gram language models to model fluency
and lexical choice.
4.1 Question-Answer Translation
Our first approach to query expansion treats the
questions and answers in the question-answer cor-
pus as two distinct languages. That is, the 10 million
question-answer pairs extracted from FAQ pages are
fed as parallel training data into an SMT training
pipeline. This training procedure includes various
standard procedures such as preprocessing, sentence
and chunk alignment, word alignment, and phrase
extraction. The goal of question-answer translation
is to learn associations between question words and
synonymous answer words, rather than the trans-
lation of questions into fluent answers. Thus we
did not conduct discriminative training of feature
weights for translation probabilities or language
model probabilities, but we held out 4,000 question-
answer pairs for manual development and testing of
the system. For example, the system was adjusted
to account for the difference in sentence length be-
tween questions and answers by setting the null-
word probability parameter in word alignment to
0.9. This allowed us to concentrate the word align-
ments to a small number of key words. Furthermore,
extraction of phrases was based on the intersection
of alignments from both translation directions, thus
favoring precision over recall also in phrase align-
ment.
Table 2 shows unique translations of the query
?how to live with cat allergies? on the phrase-level,
with corresponding source and target phrases shown
in brackets. Expansion terms are taken from phrase
terms that have not been seen in the original query,
and are highlighted in bold face.
4.2 SMT-Based Paraphrasing
Our SMT-based paraphrasing system is based on the
approach presented in Bannard and Callison-Burch
(2005). The central idea in this approach is to iden-
tify paraphrases or synonyms at the phrase level by
pivoting on another language. For example, given
a table of Chinese-to-English phrase translations,
phrasal synonyms in the target language are defined
as those English phrases that are aligned to the same
Chinese source phrases. Translation probabilities for
extracted para-phrases can be inferred from bilin-
gual translation probabilities as follows: Given an
English para-phrase pair (trg, syn), the probability
p(syn|trg) that trg translates into syn is defined
as the joint probability that the English phrase trg
translates into the foreign phrase src, and that the
foreign phrase src translates into the English phrase
syn. Under an independence assumption of those
two events, this probability and the reverse transla-
tion direction p(trg|syn) can be defined as follows:
p(syn|trg) = max
src
p(src|trg)p(syn|src) (3)
p(trg|syn) = max
src
p(src|syn)p(trg|src)
Since the same para-phrase pair can be obtained
by pivoting on multiple foreign language phrases, a
summation or maximization over foreign language
phrases is necessary. In order not to put too much
probability mass onto para-phrase translations that
can be obtained from multiple foreign language
phrases, we maximize instead of summing over src.
In our experiments, we employed equation (3)
to infer for each para-phrase pair translation model
probabilities p?(syn|trg) and p??(trg|syn) from
relative frequencies of phrases in bilingual tables.
In contrast to Bannard and Callison-Burch (2005),
we applied the same inference step to infer also
lexical translation probabilities pw(syn|trg) and
pw?(trg|syn) as defined in Koehn et al (2003) for
para-phrases. Furthermore, we deployed features for
the number of words lw, number of phrases c? , a
reordering score pd , and a score for a 6-gram lan-
guage model pLM trained on English web data. The
final model combines these features in a log-linear
model that defines the probability of paraphrasing a
full sentence, consisting of a sequence of I phrases
467
qa-translation (how, how) (to, to) (live, live) (with, with) (cat, pet) (allergies, allergies)
(how, how) (to, to) (live, live) (with, with) (cat, cat) (allergies, allergy)
(how, how) (to, to) (live, live) (with, with) (cat, cat) (allergies, food)
(how, how) (to, to) (live, live) (with, with) (cat, cats) (allergies, allergies)
paraphrasing (how, how) (to live, to live) (with cat, with cat) (allergies, allergy)
(how, ways) (to live, to live) (with cat, with cat) (allergies, allergies)
(how, how) (to live with, to live with) (cat, feline) (allergies, allergies)
(how to, how to) (live, living) (with cat, with cat) (allergies, allergies)
(how to, how to) (live, life) (with cat, with cat) (allergies, allergies)
(how, way) (to live, to live) (with cat, with cat) (allergies, allergies)
(how, how) (to live, to live) (with cat, with cat) (allergies, allergens)
(how, how) (to live, to live) (with cat, with cat) (allergies, allergen)
Table 2: Unique n-best phrase-level translations of query ?how to live with cat allergies?.
as follows:
p(synI1|trg
I
1) = (
I?
i=1
p?(syni|trgi)
?? (4)
? p??(trgi|syni)
???
? pw(syni|trgi)
?w
? pw?(trgi|syni)
?w?
? pd(syni, trgi)
?d)
? lw(syn
I
1)
?l
? c?(syn
I
1)
?c
? pLM (syn
I
1)
?LM
For estimation of the feature weights ~? defined
in equation (4) we employed minimum error rate
(MER) training under the BLEU measure (Och,
2003). Training data for MER training were taken
from multiple manual English translations of Chi-
nese sources from the NIST 2006 evaluation data.
The first of four reference translations for each Chi-
nese sentence was taken as source paraphrase, the
rest as reference paraphrases. Discriminative train-
ing was conducted on 1,820 sentences; final evalua-
tion on 2,390 sentences. A baseline paraphrase table
consisting of 33 million English para-phrase pairs
was extracted from 1 billion phrase pairs from three
different languages, at a cutoff of para-phrase prob-
abilities of 0.0025.
Query expansion is done by adding terms intro-
duced in n-best paraphrases of the query. Table 2
shows example paraphrases for the query ?how to
live with cat allergies? with newly introduced terms
highlighted in bold face.
5 Experimental Evaluation
Our baseline answer retrieval system is modeled af-
ter the tfidf retrieval model of Jijkoun and de Ri-
jke (2005). Their model calculates a linear com-
bination of vector similarity scores between the
user query and several fields in the question-answer
pair. We used the cosine similarity metric with
logarithmically weighted term and document fre-
quency weights in order to reproduce the Lucene3
model used in Jijkoun and de Rijke (2005). For
indexing of fields, we adopted the settings that
were reported to be optimal in Jijkoun and de
Rijke (2005). These settings comprise the use of
8 question-answer pair fields, and a weight vec-
tor ?0.0, 1.0, 0.0, 0.0, 0.5, 0.5, 0.2, 0.3? for fields or-
dered as follows: (1) full FAQ document text, (2)
question text, (3) answer text, (4) title text, (5)-(8)
each of the above without stopwords. The second
field thus takes takes wh-words, which would typ-
ically be filtered out, into account. All other fields
are matched without stopwords, with higher weight
assigned to document and question than to answer
and title fields. We did not use phrase-matching or
stemming in our experiments, similar to Jijkoun and
de Rijke (2005), who could not find positive effects
for these features in their experiments.
Expansion terms are taken from those terms
in the n-best translations of the query that have
not been seen in the original query string. For
paraphrasing-based query expansion, a 50-best list
of paraphrases of the original query was used.
For the noisier question-answer translation, expan-
sion terms and phrases were extracted from a 10-
3http://lucene.apache.org
468
S2@10 S2@20 S1,2@10 S1,2@20
baseline tfidf 27 35 58 65
local expansion 30 (+ 11.1) 40 (+ 14.2) 57 (- 1) 63 (- 3)
SMT-based expansion 38 (+ 40.7) 43 (+ 22.8) 58 65
Table 3: Success rate at 10 or 20 results for retrieval of adequate (2) or material (1) answers; relative change
in brackets.
best list of query translations. Terms taken from
query paraphrases were matched with the same field
weight vector ?0.0, 1.0, 0.0, 0.0, 0.5, 0.5, 0.2, 0.3? as
above. Terms taken from question-answer trans-
lation were matched with the weight vector
?0.0, 1.0, 0.0, 0.0, 0.5, 0.2, 0.5, 0.3?, preferring an-
swer fields over question fields. After stopword
removal, the average number of expansion terms
produced was 7.8 for paraphrasing, and 3.1 for
question-answer translation.
The local expansion technique used in our exper-
iments follows Xu and Croft (1996) in taking ex-
pansion terms from the top n answers that were re-
trieved by the baseline tfidf system, and by incorpo-
rating cooccurrence information with query terms.
This is done by calculating term frequencies for ex-
pansion terms by summing up the tfidf weights of
the answers in which they occur, thus giving higher
weight to terms that occur in answers that receive
a higher similarity score to the original query. In
our experiments, expansion terms are ranked accord-
ing to this modified tfidf calculation over the top 20
answers retrieved by the baseline retrieval run, and
matched a second time with the field weight vector
?0.0, 1.0, 0.0, 0.0, 0.5, 0.2, 0.5, 0.3? that prefers an-
swer fields over question fields. After stopword re-
moval, the average number of expansion terms pro-
duced by the local expansion technique was 9.25.
The test queries we used for retrieval are taken
from query logs of the MetaCrawler search en-
gine4 and were provided to us by Valentin Jijk-
oun. In order to maximize recall for the comparative
evaluation of systems, we selected 60 queries that
were well-formed natural language questions with-
out metacharacters and spelling errors. However, for
one third of these well-formed queries none of the
five compared systems could retrieve an answer. Ex-
amples are ?how do you make a cornhusk doll?,
4http://www.metacrawler.com
?what is the idea of materialization?, or ?what does
8x certified mean?, pointing to a severe recall prob-
lem of the question-answer database.
Evaluation was performed by manual labeling of
top 20 answers retrieved for each of 60 queries for
each system by two independent judges. For the sake
of consistency, we chose not to use the assessments
provided by Jijkoun and de Rijke. Instead, the judges
were asked to find agreement on the examples on
which they disagreed after each evaluation round.
The ratings together with the question-answer pair
id were stored and merged into the retrieval results
for the next system evaluation. In this way consis-
tency across system evaluations could be ensured,
and the effort of manual labeling could be substan-
tially reduced. The quality of retrieval results was
assessed according to Jijkoun and de Rijke?s (2005)
three point scale:
? adequate (2): answer is contained
? material (1): no exact answer, but important in-
formation given
? unsatisfactory (0): user?s information need is
not addressed
The evaluation measure used in Jijkoun and de
Rijke (2005) is the success rate at 10 or 20 an-
swers, i.e., S2@n is the percentage of queries with
at least one adequate answer in the top n retrieved
question-answer pairs, and S1,2@n is the percentage
of queries with at least one adequate or material an-
swer in the top n results. This evaluation measure ac-
counts for improvements in coverage, i.e., it rewards
cases where answers are found for queries that did
not have an adequate or material answer before. In
contrast, the mean reciprocal rank (MRR) measure
standardly used in QA can have the effect of prefer-
ring systems that find answers only for a small set
of queries, but rank them higher than systems with
469
(1) query: how to live with cat allergies
local expansion (-): allergens allergic infections filter plasmacluster rhinitis introduction effective replacement
qa-translation (+): allergy cats pet food
paraphrasing (+): way allergens life allergy feline ways living allergen
(2) query: how to design model rockets
local expansion (-): models represented orientation drawings analysis element environment different structure
qa-translation (+): models rocket
paraphrasing (+): missiles missile rocket grenades arrow designing prototype models ways paradigm
(3) query: what is dna hybridization
local expansion (-): instructions individual blueprint characteristics chromosomes deoxyribonucleic information biological
genetic molecule
qa-translation (+): slides clone cdna sitting sequences
paraphrasing (+): hibridization hybrids hybridation anything hibridacion hybridising adn hybridisation nothing
(4) query: how to enhance competitiveness of indian industries
local expansion (+): resources production quality processing established investment development facilities institutional
qa-translation (+): increase industry
paraphrasing (+): promote raise improve increase industry strengthen
(5) query: how to induce labour
local expansion (-): experience induction practice imagination concentration information consciousness different meditation
relaxation
qa-translation (-): birth industrial induced induces
paraphrasing (-): way workers inducing employment ways labor working child work job action unions
Table 4: Examples for queries and expansion terms yielding improved (+), decreased (-), or unchanged (0)
retrieval performance compared to retrieval without expansion.
higher coverage. This makes MRR less adequate for
the low-recall setup of FAQ retrieval.
Table 3 shows success rates at 10 and 20 retrieved
question-answer pairs for five different systems. The
results for the baseline tfidf system, following Jijk-
oun and de Rijke (2005), are shown in row 2. Row
3 presents results for our variant of local expansion
by pseudo-relevance feedback (Xu and Croft, 1996).
Results for SMT-based expansion are given in row 4.
A comparison of success rates for retrieving at least
one adequate answer in the top 10 results shows rel-
ative improvements over the baseline of 11.1% for
local query expansion, and of 40.7% for combined
SMT-based expansion. Success rates at top 20 re-
sults show similar relative improvements of 14.2%
for local query expansion, and of 22.8% for com-
bined SMT-based expansion. On the easier task of
retrieving a material or adequate answer, success
rates drop by a small amount for local expansion,
and stay unchanged for SMT-based expansion.
These results can be explained by inspecting a few
sample query expansions. Examples (1)-(3) in Ta-
ble 4 illustrate cases where SMT-based query expan-
sion improves results over baseline performance, but
local expansion decreases performance by introduc-
ing irrelevant terms. In (4) retrieval performance is
improved over the baseline for both expansion tech-
niques. In (5) both local and SMT-based expansion
introduce terms that decrease retrieval performance
compared to retrieval without expansion.
6 Conclusion
We presented two techniques for query expansion in
answer retrieval that are based on SMT technology.
Our method for question-answer translation uses a
large corpus of question-answer pairs extracted from
FAQ pages to learn a translation model from ques-
tions to answers. SMT-based paraphrasing utilizes
large amounts of bilingual data as a new informa-
tion source to extract phrase-level synonyms. Both
SMT-based techniques take the entire query context
into account when adding new terms to the orig-
inal query. In an experimental comparison with a
baseline tfidf approach and a local query expansion
technique on the task of answer retrieval from FAQ
pages, we showed a significant improvement of both
SMT-based query expansion over both baselines.
Despite the small-scale nature of our current ex-
perimental results, we hope to apply the presented
techniques to general web retrieval in future work.
Another task for future work is to scale up the ex-
traction of question-answer pair data in order to
provide an improved resource for question-answer
translation.
470
References
Eugene Agichtein, Steve Lawrence, and Luis Gravano.
2004. Learning to find answers to questions on
the web. ACM Transactions on Internet Technology,
4(2):129?162.
Colin Bannard and Chris Callison-Burch. 2005. Para-
phrasing with bilingual parallel corpora. In Proceed-
ings of (ACL?05), Ann Arbor, MI.
Adam L. Berger, Rich Caruana, David Cohn, Dayne Fre-
itag, and Vibhu Mittal. 2000. Bridging the lexical
chasm: Statistical approaches to answer-finding. In
Proceedings of SIGIR?00, Athens, Greece.
Peter F. Brown, Stephen A. Della Pietra, Vincent J. Della
Pietra, and Robert L. Mercer. 1993. The mathemat-
ics of statistical machine translation: Parameter esti-
mation. Computational Linguistics, 19(2):263?311.
Robin B. Burke, Kristian J. Hammond, and Vladimir A.
Kulyukin. 1997. Question answering from
frequently-asked question files: Experiences with the
FAQ finder system. AI Magazine, 18(2):57?66.
Koby Crammer, Ofer Dekel, Joseph Keshet, Shai Shalev-
Shwartz, and Yo-ram Singer. 2006. Online passive-
agressive algorithms. Machine Learning, 7:551?585.
Pablo Ariel Duboue and Jennifer Chu-Carroll. 2006. An-
swering the question you wish they had asked: The im-
pact of paraphrasing for question answering. In Pro-
ceedings of (HLT-NAACL?06), New York, NY.
Abdessamad Echihabi and Daniel Marcu. 2003. A
noisy-channel approach to question answering. In
Proceedings of (ACL?03), Sapporo, Japan.
Sanda Harabagiu and Finley Lacatusu. 2004. Strategies
for advanced question answering. In Proceedings of
the HLT-NAACL?04 Workshop on Pragmatics of Ques-
tion Answering, Boston, MA.
Sanda Harabagiu, Dan Moldovan, Marius Pas?ca, Rada
Mihalcea, Mihai Surdeanu, Ra?zvan Bunescu, Roxana
G??rju, Vasile Rus, and Paul Mora?rescu. 2001. The
role of lexico-semantic feedback in open-domain tex-
tual question-answering. In Proceedings of (ACL?01),
Toulouse, France.
Ulf Hermjakob, Abdessamad Echihabi, and Daniel
Marcu. 2002. Natural language based reformulation
resource and web exploitation for question answering.
In Proceedings of TREC-11, Gaithersburg, MD.
Eduard Hovy, Laurie Gerber, Ulf Hermjakob, Michael
Junk, and Chin-Yew Lin. 2000. Question answering
in webclopedia. In Proceedings of TREC 9, Gaithers-
burg, MD.
Abraham Ittycheriah, Martin Franz, and Salim Roukos.
2001. IBM?s statistical question answering system. In
Proceedings of TREC 10, Gaithersburg, MD.
Valentin Jijkoun and Maarten de Rijke. 2005. Retrieving
answers from frequently asked questions pages on the
web. In Proceedings of the Tenth ACM Conference on
Information and Knowledge Management (CIKM?05),
Bremen, Germany.
Thorsten Joachims. 2003. Transductive learning
via spectral graph partitioning. In Proceedings of
ICML?03, Washington, DC.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In Proceed-
ings of (HLT-NAACL?03), Edmonton, Cananda.
Dekang Lin and Patrick Pantel. 2001. Discovery of infer-
ence rules for question answering. Journal of Natural
Language Engineering, 7(3):343?360.
Franz Josef Och and Hermann Ney. 2004. The align-
ment template approach to statistical machine transla-
tion. Computational Linguistics, 30(4):417?449.
Franz Josef Och. 2003. Minimum error rate training
in statistical machine translation. In Proceedings of
(HLT-NAACL?03), Edmonton, Cananda.
John Prager, Jennifer Chu-Carroll, and Krysztof Czuba.
2001. Use of wordnet hypernyms for answering what-
is questions. In Proceedings of TREC 10, Gaithers-
burg, MD.
Yonggang Qiu and H. P. Frei. 1993. Concept based query
expansion. In Proceedings of SIGIR?93, Pittsburgh,
PA.
Dragomir R. Radev, Hong Qi, Zhiping Zheng, Sasha
Blair-Goldensohn, Zhu Zhang, Weigo Fan, and John
Prager. 2001. Mining the web for answers to natu-
ral language questions. In Proceedings of (CIKM?01),
Atlanta, GA.
Radu Soricut and Eric Brill. 2006. Automatic question
answering using the web: Beyond the factoid. Journal
of Information Retrieval - Special Issue on Web Infor-
mation Retrieval, 9:191?206.
Ellen M. Voorhees. 1994. Query expansion using
lexical-semantic relations. In Proceedings of SI-
GIR?94, Dublin, Ireland.
Jinxi Xu and W. Bruce Croft. 1996. Query expansion
using local and global document analysis. In Proceed-
ings of SIGIR?96, Zurich, Switzerland.
471
Query-Relevant Summarization using FAQs
Adam Berger Vibhu O. Mittal
School of Computer Science Just Research
Carnegie Mellon University 4616 Henry Street
Pittsburgh, PA 15213 Pittsburgh, PA 15213
aberger@cs.cmu.edu mittal@justresearch.com
Abstract
This paper introduces a statistical model for
query-relevant summarization: succinctly
characterizing the relevance of a document
to a query. Learning parameter values for
the proposed model requires a large collec-
tion of summarized documents, which we
do not have, but as a proxy, we use a col-
lection of FAQ (frequently-asked question)
documents. Taking a learning approach en-
ables a principled, quantitative evaluation
of the proposed system, and the results of
some initial experiments?on a collection
of Usenet FAQs and on a FAQ-like set
of customer-submitted questions to several
large retail companies?suggest the plausi-
bility of learning for summarization.
1 Introduction
An important distinction in document summarization
is between generic summaries, which capture the cen-
tral ideas of the document in much the same way that
the abstract of this paper was designed to distill its
salient points, and query-relevant summaries, which
reflect the relevance of a document to a user-specified
query. This paper discusses query-relevant summa-
rization, sometimes also called ?user-focused summa-
rization? (Mani and Bloedorn, 1998).
Query-relevant summaries are especially important
in the ?needle(s) in a haystack? document retrieval
problem: a user has an information need expressed
as a query (What countries export smoked
salmon?), and a retrieval system must locate within
a large collection of documents those documents most
likely to fulfill this need. Many interactive retrieval
systems?web search engines like Altavista, for
instance?present the user with a small set of candi-
date relevant documents, each summarized; the user
must then perform a kind of triage to identify likely
relevant documents from this set. The web page sum-
maries presented by most search engines are generic,
not query-relevant, and thus provide very little guid-
ance to the user in assessing relevance. Query-relevant
summarization (QRS) aims to provide a more effective
characterization of a document by accounting for the
user?s information need when generating a summary.
Search for
relevant
documents
  
 
 
 

Summarize
documents
relative to
Q
? 	 
    
?      
?      Headline Generation Based on Statistical Translation
Michele Banko
Computer Science Department
Johns Hopkins University
Baltimore, MD 21218
banko@cs.jhu.edu
Vibhu O. Mittal
Just Research
4616 Henry Street
Pittsburgh, PA 15213
mittal@justresearch.com
Michael J. Witbrock
Lycos Inc.
400-2 Totten Pond Road
Waltham, MA 023451
mwitbrock@lycos.com
Abstract
Extractive summarization techniques
cannot generate document summaries
shorter than a single sentence, some-
thing that is often required. An ideal
summarization system would under-
stand each document and generate an
appropriate summary directly from the
results of that understanding. A more
practical approach to this problem re-
sults in the use of an approximation:
viewing summarization as a problem
analogous to statistical machine trans-
lation. The issue then becomes one of
generating a target document in a more
concise language from a source docu-
ment in a more verbose language. This
paper presents results on experiments
using this approach, in which statisti-
cal models of the term selection and
term ordering are jointly applied to pro-
duce summaries in a style learned from
a training corpus.
1 Introduction
Generating effective summaries requires the abil-
ity to select, evaluate, order and aggregate items
of information according to their relevance to
a particular subject or for a particular purpose.
Most previous work on summarization has fo-
cused on extractive summarization: selecting text
spans - either complete sentences or paragraphs
? from the original document. These extracts are
Vibhu Mittal is now at Xerox PARC, 3333 Coyote
Hill Road, Palo Alto, CA 94304, USA. e-mail: vmit-
tal@parc.xerox.com; Michael Witbrock?s initial work on
this system was performed whilst at Just Research.
then arranged in a linear order (usually the same
order as in the original document) to form a sum-
mary document. There are several possible draw-
backs to this approach, one of which is the fo-
cus of this paper: the inability to generate co-
herent summaries shorter than the smallest text-
spans being considered ? usually a sentence, and
sometimes a paragraph. This can be a problem,
because in many situations, a short headline style
indicative summary is desired. Since, in many
cases, the most important information in the doc-
ument is scattered across multiple sentences, this
is a problem for extractive summarization; worse,
sentences ranked best for summary selection of-
ten tend to be even longer than the average sen-
tence in the document.
This paper describes an alternative approach to
summarization capable of generating summaries
shorter than a sentence, some examples of which
are given in Figure 1. It does so by building sta-
tistical models for content selection and surface
realization. This paper reviews the framework,
discusses some of the pros and cons of this ap-
proach using examples from our corpus of news
wire stories, and presents an initial evaluation.
2 Related Work
Most previous work on summarization focused
on extractive methods, investigating issues such
as cue phrases (Luhn, 1958), positional indi-
cators (Edmundson, 1964), lexical occurrence
statistics (Mathis et al, 1973), probabilistic mea-
sures for token salience (Salton et al, 1997), and
the use of implicit discourse structure (Marcu,
1997). Work on combining an information ex-
traction phase followed by generation has also
been reported: for instance, the FRUMP sys-
tem (DeJong, 1982) used templates for both in-
1: time -3.76 Beam 40
2: new customers -4.41 Beam 81
3: dell computer products -5.30 Beam 88
4: new power macs strategy -6.04 Beam 90
5: apple to sell macintosh users -8.20 Beam 86
6: new power macs strategy on internet -9.35 Beam 88
7: apple to sell power macs distribution strategy -10.32 Beam 89
8: new power macs distribution strategy on internet products -11.81 Beam 88
9: apple to sell power macs distribution strategy on internet -13.09 Beam 86
Figure 1: Sample output from the system for a variety of target summary lengths from a single
input document.
formation extraction and presentation. More
recently, summarizers using sophisticated post-
extraction strategies, such as revision (McKeown
et al, 1999; Jing and McKeown, 1999; Mani et
al., 1999), and sophisticated grammar-based gen-
eration (Radev and McKeown, 1998) have also
been presented.
The work reported in this paper is most closely
related to work on statistical machine transla-
tion, particularly the ?IBM-style? work on CAN-
DIDE (Brown et al, 1993). This approach
was based on a statistical translation model that
mapped between sets of words in a source lan-
guage and sets of words in a target language, at
the same time using an ordering model to con-
strain possible token sequences in a target lan-
guage based on likelihood. In a similar vein,
a summarizer can be considered to be ?translat-
ing? between two languages: one verbose and the
other succinct (Berger and Lafferty, 1999; Wit-
brock and Mittal, 1999). However, by definition,
the translation during summarization is lossy, and
consequently, somewhat easier to design and ex-
periment with. As we will discuss in this paper,
we built several models of varying complexity;1
even the simplest one did reasonably well at sum-
marization, whereas it would have been severely
deficient at (traditional) translation.
1We have very recently become aware of related work
that builds upon more complex, structured models ? syn-
tax trees ? to compress single sentences (Knight and Marcu,
2000); our work differs from that work in (i) the level of
compression possible (much more) and, (ii) accuracy possi-
ble (less).
3 The System
As in any language generation task, summariza-
tion can be conceptually modeled as consisting
of two major sub-tasks: (1) content selection, and
(2) surface realization. Parameters for statistical
models of both of these tasks were estimated from
a training corpus of approximately 25,000 1997
Reuters news-wire articles on politics, technol-
ogy, health, sports and business. The target docu-
ments ? the summaries ? that the system needed
to learn the translation mapping to, were the head-
lines accompanying the news stories.
The documents were preprocessed before
training: formatting and mark-up information,
such as font changes and SGML/HTML tags, was
removed; punctuation, except apostrophes, was
also removed. Apart from these two steps, no
other normalization was performed. It is likely
that further processing, such as lemmatization,
might be useful, producing smaller and better lan-
guage models, but this was not evaluated for this
paper.
3.1 Content Selection
Content selection requires that the system learn a
model of the relationship between the appearance
of some features in a document and the appear-
ance of corresponding features in the summary.
This can be modeled by estimating the likelihood
of some token appearing in a summary given that
some tokens (one or more, possibly different to-
kens) appeared in the document to be summa-
rized. The very simplest, ?zero-level? model for
this relationship is the case when the two tokens
00.05
0.1
0.15
0.2
0.25
0.3
0.35
0.4
0 2 4 6 8 10 12
Pr
op
or
tio
n 
of
 d
oc
um
en
ts
Length in words
Summary lengths
headlines
Figure 2: Distribution of Headline Lengths for
early 1997 Reuters News Stories.
in the document and the summary are identical.
This can be computed as the conditional proba-
bility of a word occurring in the summary given
that the word appeared in the document:
 
	

 	 Multi-Document Summarization By Sentence Extraction 
Jade Goldstein* Vibhu Mittal t Jaime Carbonell* Mark Kantrowitzt 
jade@cs.cmu.edu mittal@jprc.com jgc@cs.cmu.edu mkant@jprc.com 
*Language Technologies Institute 
Carnegie Mellon University 
Pittsburgh, PA 15213 
U.S.A. 
tJust Research 
4616 Henry Street 
Pittsburgh, PA 15213 
U.S.A. 
Abstract 
This paper discusses a text extraction approach to multi- 
document summarization that builds on single-document 
summarization methods by using additional, available in-, 
formation about the document set as a whole and the 
relationships between the documents. Multi-document 
summarization differs from single in that the issues 
of compression, speed, redundancy and passage selec- 
tion are critical in the formation of useful summaries. 
Our approach addresses these issues by using domain- 
independent techniques based mainly on fast, statistical 
processing, a metric for reducing redundancy and maxi- 
mizing diversity in the selected passages, and a modular 
framework to allow easy parameterization for different 
genres, corpora characteristics and user requirements. 
1 Introduction 
With the continuing growth of online information, it 
has become increasingly important to provide improved 
mechanisms to find and present extual information ef- 
fectively. Conventional IR systems find and rank docu- 
ments based on maximizing relevance to the user query 
(Salton, 1970; van Rijsbergen, 1979; Buckley, 1985; 
Salton, 1989). Some systems also include sub-document 
relevance assessments and convey this information to the 
user. More recently, single document summarization sys- 
tems provide an automated generic abstract or a query- 
relevant summary (TIPSTER, 1998a). i However, large- 
scale IR and summarization have not yet been truly in- 
tegrated, and the functionality challenges on a summa- 
rization system are greater in a true IR or topic-detection 
context (Yang et al, 1998; Allan et al, 1998). 
Consider the situation where the user issues a search 
query, for instance on a news topic, and the retrieval sys- 
tem finds hundreds of closely-ranked documents in re- 
sponse. Many of these documents are likely to repeat 
much the same information, while differing in certain 
i Most of these were based on statistical techniques applied to var- 
ious document entities; examples include frait, 1983; Kupiec et al, 
1995; Paice, 1990, Klavans and Shaw, 1995; MeKeown et al, 1995; 
Shaw, 1995; Aon? et al, 1997; Boguraev and Kennedy, 1997; Hovy 
and Lin, 1997; Mitra et al, 1997; Teufel and Moens, 1997; Barzilay 
and Elhadad, 1997; Carbonell and Goldstein, 1998; Baldwin and Mor- 
tbn, 1998; Radev and McKeown, 1998; Strzalkowski etal., 1998). 
parts. Summaries of the individual documents would 
help, but are likely to be very similar to each other, un- 
less the summarization system takes into account other 
summaries that have already been generated. Multi- 
document summarization - capable of summarizing ei- 
ther complete documents sets, or single documents in the 
context of previously summarized ones - are likely to 
be essential in such situations. Ideally, multi-document 
summaries should contain the key shared relevant infor- 
mation among all the documents only once, plus other 
information unique to some of the individual documents 
that are directly relevant to the user's query. 
Though many of the same techniques used in single- 
document summarization can also be used in multi- 
document summarization, there are at least four signif- 
icant differences: 
1. The degree of redundancy in information contained 
within a group of topically-related articles is much 
higher than the degree of redundancy within an arti- 
cle, as each article is apt to describe the main point 
as well as necessary shared background. Hence 
anti-redundancy methods are more crucial. 
2. A group of articles may contain a temporal dimen- 
sion, typical in a stream of news reports about an 
unfolding event. Here later information may over- 
ride earlier more tentative or incomplete accounts. 
3. The compression ratio (i.e. the size of the summary 
with respect o the size of the document set) will 
typically be much smaller for collections of dozens 
or hundreds of topically related documents than 
for single document summaries. The SUMMAC 
evaluation (TIPSTER, 1998a) tested 10% compres- 
sion summaries, but in our work summarizing 200- 
document clusters, we find that compression to the 
1% or 0.1% level is required. Summarization be- 
comes significantly more difficult when compres- 
sion demands increase. 
4. The co-reference problem in summarization 
presents even greater challenges for multi- 
document han for single-document summariza- 
tion (Baldwin and Morton, 1998). 
This paper discusses an approach to multi-document 
summarization that builds on previous work in single- 
40 
I 
i 
l 
i 
i 
I 
! 
I 
i 
i 
l 
I 
I 
! 
I 
I 
! 
I, 
I 
document summarization by using additional, available 
information about the document set as a whole, the re- 
lationships between the documents, as well as individual 
documents. 
2 Background and Related Work 
Generating an effective summary requires the summa- 
rizer to select, evaluate, order and aggregate items of 
information according to their relevance to a particular 
subject or purpose. These tasks can either be approx- 
imated by IR techniques or done in greater depth with 
fuller natural language processing. Most previous work 
in summarization has attempted todeal with the issues by 
focusing more on a related, but simpler, problem. With 
text-span deletion the system attempts o delete "less im- 
portant" spans of text from the original document; the 
text that remains is deemed a summary. Work on auto- 
mated document summarization by text span extraction 
dates back at least to work at IBM in the fifties (Luhn, 
1958). Most of the work in sentence xtraction applied 
statistical techniques (frequency analysis, variance anal- 
ysis, etc.) to linguistic units such as tokens, names, 
anaphora, etc. More recently, other approaches have 
investigated the utility of discourse structure (Marcu, 
1997), the combination of information extraction and 
language generation (Klavans and Shaw, 1995; McKe- 
own et al, 1995), and using machine learning to find 
patterns in text (Teufel and Moens, 1997; Barzilay and 
Elhadad, 1997; Strzalkowski et al, 1998). 
Some of these approaches tosingle document summa- 
rization have been extended to deal with multi-document 
summarization (Mani and Bloedern, 1997; Goldstein and 
Carbonell, 1998; TIPSTER, 1998b; Radev and McKe- 
own, 1998; Mani and Bloedorn, 1999; McKeown et al, 
.!999; Stein et al, 1999). These include comparing tem- 
plates filled in by extracting information - using special- 
ized, domain specific knowledge sources - from the doc- 
"ument, and then generating natural language summaries 
from the templates (Radev and McKeown, 1998), com-- 
? paring named-entities - extracted using specialized lists 
- between documents and selecting the most relevant 
section (TIPSTER, 1998b), finding co-reference chains 
in the document set to identify common sections of inter- 
est (TIPSTER, 1998b), or building activation etworks 
of related lexical items (identity mappings, synonyms, 
hypernyms, etc.) to extract text spans from the document 
set (Mani and Bloedern, 1997). Another system (Stein et 
al., 1999) creates a multi-document summary from mul- 
tiple single document summaries, an approach that can 
be sub-optimal in some cases, due to the fact that the 
process of generating the final multi-document summary 
takes as input he individual summaries and not the com- 
plete documents. (Particularly if the single-document 
summaries can contain much overlapping information.) 
The Columbia University system (McKeown et al, 1999) 
creates amulti-document summary using machine learn- 
ing and statistical techniques to identify similar sections 
41 
and language generation to reformulate the summary. 
The focus of our approach is a multi-document system 
that can quickly summarize large clusters of similar doc- 
uments (on the order of thousands) while providing the 
key relevant useful information or pointers to such in- 
formation. Our system (1) primarily uses only domain- 
independent techniques, based mainly on fast, statistical 
processing, (2) explicitly deals with the issue of reducing 
redundancy without eliminating potential relevant infor- 
mation, and (3) contains parameterized modules, so that 
different genres or corpora characteristics an be taken 
into account easily. 
3 Requirements for Multi-Document 
Summarization 
There are two types of situations in which multi- 
document summarization would be useful: (1) the user 
is faced with a collection of dis-similar documents and 
wishes to assess the information landscape contained in 
the collection, or (2) there is a collection of topically- 
related ocuments, extracted from a larger more diverse 
collection as the result of a query, or a topically-cohesive 
cluster. In the first case, if the collection is large enough, 
it only makes ense to first cluster and categorize the doc- 
uments (Yang et al, 1999), and then sample from, or 
summarize ach cohesive cluster. Hence, a "summary" 
would constitute of a visualization of the information 
landscape, where features could be clusters or summaries 
thereof. In the second case, it is possible to build a syn- 
thetic textual summary containing the main point(s) of 
the topic, augmented with non-redundant background in- 
formation and/or query-relevant elaborations. This is the 
focus of our work reported here, including the necessity 
to eliminate redundancy among the information content 
of multiple related ocuments. 
Users' information seeking needs and goals vary 
tremendously. When a group of three people created a
multi-document summarization of 10 articles about he 
Microsoft Trial from a given day, one summary focused 
on the details presented in court, one on an overall gist 
of the day's events, and the third on a high level view of 
the goals and outcome of the trial. Thus, an ideal multi- 
document summarization would be able to address the 
different levels of detail, which is difficult without natu- 
ral language understanding. An interface for the summa- 
rization system needs to be able to permit he user to en- 
ter information seeking oals, via a query, a background 
interest profile and/or a relevance feedback mechanism. 
Following is a list of requirements for multi-document 
summarization: 
? clustering: The ability to cluster similar documents 
and passages to find related information. 
? coverage: The ability to find and extract he main 
points across documents. 
? anti-redundancy: The ability to minimize redun- 
dancy between passages in the summary. 
*. summary cohesion criteria: The ability to combine 
text passages in a useful manner for the reader.-This 
may include: 
- document ordering: All text segments of high- 
est ranking document, hen all segments from 
the next highest ranking document, etc. 
- news-story principle (rank ordering):present 
the most relevant and diverse information first 
so that the reader gets the maximal information 
content even if they stop reading the summary. 
- topic-cohesion: Group together the passages 
by topic clustering using passage similarity cri- 
teria and present the information by the cluster" 
centroid passage rank. 
- t ime line ordering: Text passages ordered 
based on the occurrence of events in time. 
* coherence: Summaries generated should be read- 
able and relevant to the user. 
. context: Include sufficient context so that the sum- 
mary is understandable to the reader. 
? identification of source inconsistencies: Articles of- 
ten have errors (such as billion reported as million, 
etc.); multi-document summarization must be able 
to recognize and report source inconsistencies. 
? summary updates: A new multi-document summary 
must take into account previous ummaries in gen- 
erating new summaries. In such cases, the system 
needs to be able to track and categorize vents. 
? effective user interfaces: 
- Attributability: The user needs to be able to 
easily access the source of a given passage. 
This could be the single document summary. 
- Relationship: The user needs to view related 
passages to the text passage shown, which can 
highlight source inconsistencies. 
- Source Selection: The user needs to be able to 
,- select or eliminate various sources. For exam- 
ple, the user may want to eliminate information 
from some less reliable foreign news reporting 
sources. 
- Context: The user needs to be able to zoom 
in on the context surrounding the chosen pas- 
sages. 
- Redirection: The user should be able to high- 
light certain parts of the synthetic summary 
and give a command to the system indicating 
that these parts are to be weighted heavily and 
that other parts are to be given a lesser weight. 
4 Types of Multi-Document Summarizers 
In the previous ection we discussed the requirements 
for a multi-document summarization system. Depend- 
ing on a user's information seeking goals, the user may 
want to create summaries that contain primarily the com- 
mon portions of the documents (their intersection) or an 
overview of the entire cluster of documents (a sampling. 
of the space that the documents span). A user may also 
want to have a highly readable summary, an overview of 
pointers (sentences or word lists) to further information, 
? or a combination of the two. Following is a list of  var- 
ious methods of creating multi-document summaries by 
extraction: 
1. Summary from Common Sections of Documents: 
Find the important relevant parts that the cluster of 
documents have in common (their intersection) and 
use that as a summary. 
2. Summary from Common Sections and Unique Sec- 
tions of Documents: Find the important relevant 
parts that the cluster of documents have in common 
and the relevant parts that are unique and use that as 
a summary. 
3. Centroid Document Summary: Create a single doc- 
ument summary from the centroid ocument in the 
? cluster. 
4. Centroid Document plus Outliers Summary: Cre- 
ate a single document summary from the centroid 
document in the cluster and add some representa- 
tion from outlier documents (passages or keyword 
extraction) to provide a fuller coverage of the docu- 
ment set. 2 
5. Latest Document plus Outliers Summary: Create 
a single document summary from the latest time 
stamped ocument in the cluster (most recent in- 
formation) and add some representation f outlier 
documents o provide a fuller coverage of the docu- 
ment set. 
6. Summary from Common Sections and Unique Sec- 
tions of Documents with Time Weighting Factor: 
Find the important relevant parts that the cluster of 
documents have in common and the relevant parts 
that are unique and weight all the information by 
the time sequence of the documents in which they 
appear and use the result as a summary. This al- 
lows the more recent, often updated information to 
be more likely to be included in the summary. 
There are also much more complicated types of sum- 
mary extracts which involve natural anguage process- 
ing and/or understanding. These types of summaries in- 
clude: (1) differing points of view within the document 
collection, (2) updates of information within the doc- 
ument collection, (3) updates of information from the 
document collection with respect o an already provided 
summary, (4) the development of an event or subtopic of 
2This is similar to the approach ofTextwise fHPSTER, 1998b), 
whose multi-document summary consists of the most relevant para- 
graph and specialized word lists. 
42 
I 
I 
I 
I 
l 
I 
I 
I 
I 
I 
I 
i 
an event (e.g., death tolls) over time, and (5) a compara- 
tive development of an event. 
Naturally, an ideal multi-document summary would 
include a natural language generation component to cre- 
ate cohesive readable summaries (Radev and McKeown, 
1998; McKeown et al, 1999). Our current focus is on 
the extraction of the relevant passages. 
5 System Design 
In the previous ections we discussed the requirements 
and types of multi-document summarization systems. 
This section discusses our current implementation of
a multi-document summarization system which is de- 
signed to produce summaries that emphasize "relevant 
novelty." Relevant novelty is a metric for minimizing re- 
dundancy and maximizing both relevance and diversity. 
A first approximation tomeasuring relevant novelty is to 
measure relevance and novelty independently and pro- 
vide a linear combination as the metric. We call this lin- 
ear combination "marginal relevance" .-- i.e., a text pas- 
sage has high marginal relevance if it is both relevant to 
the query and useful for a summary, while having mini- 
mal similarity to previously selected passages. Using this 
metric one can maximize marginal relevance in retrieval 
and summarization, hence we label our method "maxi- 
mal marginal relevance" (MMR) (Carboneli and Gold- 
stein, 1998). 
The Maximal Marginal Relevance Multi-Document 
(MMR-MD) metric is defined in Figure 1. Sirnl and 
Sire2 cover some of the properties that we discussed in 
Section 3. 3 
: For Sirnl, the first term is the cosine similarity metric 
for query and document. The second term computes a
coverage score for the passage by whether the passage 
is in one or more clusters and the size of the cluster. 
The third term reflects the information content of the pas- 
.sage by taking into account both statistical and linguis- 
tic features for summary inclusion (such as query expan- 
.sion, position of the passage in the document and pres- 
ence/absence of named-entities in the passage). The final 
term indicates the temporal sequence of the document in 
the collection allowing for more recent information to 
have higher weights. 
For Sire2, the first term uses the cosine similarity met- 
ric to compute the similarity between the passage and 
previously selected passages. (This helps the system to 
minimize the possibility of including passages similar to 
ones already selected.) The second term penalizes pas- 
sages that are part of clusters from which other passages 
have already been chosen. The third term penalizes doc- 
uments from which passages have already been selected; 
however, the penalty is inversely proportional to docu- 
ment length, to allow the possibility of longer documents 
3Sirnn and Sirn2 as previously defined in MMR for single- 
document summarization contained only the first term of each equa- 
tion: 
43 
contributing more passages. These latter two terms allow 
for a fuller coverage of the clusters and documents. 
Given the above definition, MMR-MD incrementally 
computes the standard relevance-ranked list- plus some 
additional scoring factors - when the parameter A= 1, and 
computes a maximal diversity ranking among the pas- 
sages in the documents when A=0. For intermediate val- 
ues of A in the interval \[0,1 \], a linear combination of both 
criteria is optimized. In order to sample the information 
space in the general vicinity of the query, small values of 
can be used; to focus on multiple, potentially overlap- 
ping or reinforcing relevant passages, A can be set to a 
value closer to 1. We found that a particularly effective 
search strategy for document retrieval is to start with a 
small A (e.g., A = .3) in order to understand the informa- 
tion space in the region of the query, and then to focus 
on the most important parts using a reformulated query 
(possibly via relevance feedback) and a larger value of 
(e.g., A = .7) (Carboneli and Goldstein, 1998). 
Our multi-document summarizer works as follows: 
? Segment he documents into passages, and index 
them using inverted indices (as used by the IR 
engine). Passages may be phrases, sentences, n- 
sentence chunks, or paragraphs. 
? Identify the passages relevant o the query using 
cosine similarity with a threshold below which the 
passages are discarded. 
? Apply the MMR-MD metric as defined above. De- 
pending on the desired length of the summary, se- 
lect a number of passages to compute passage re- 
dundancy using the cosine similarity metric and use 
the passage similarity scoring as a method of clus- 
tering passages. Users can select he number of pas- 
sages or the amount of compression. 
? Reassemble the selected passages into a summary 
document using one of the summary-cohesion cri- 
teria (see Section 3). 
The results reported in this paper are based on the use 
of the SMART search engine (Buckley, 1985) to compute 
cosine similarities (with a SMART weighting of lnn  for 
both queries and passages), stopwords eliminated from 
the indexed ata and stemming turned on. 
6 Discussion 
The TIPSTER evaluation corpus provided several sets of 
topical clusters to which we applied MMR-MD summa- 
rization. As an example, consider a set of 200 apartheid- 
related news-wire documents from the Associated Press 
and the Wall Street Journal, spanning the period from 
1988 to 1992. We used the TIPSTER provided topic de- 
scription as the query. These 200 documents were on 
an average 31 sentences in length, with a total of 6115 
sentences. We used the sentence as our summary unit. 
Generating a summary 10 sentences long resulted in a 
MMR-MD ~ Arg max \[A(Siml (Pii, Q, Cij, Di, D)) - (1 - A) max Sirn2 (Pij, Pnm, C, S, Di))\] 
Pij ER\S t - P,=.. ES 
Sire1 (P,.j, Q, Cij, Di, D) = wl *(Pij'Q)+w2*coverage(Pij, Cij)+wa*content(Pij)+w4*tirne_sequenee(Di, D) 
Sim2 ( Pij, Pare, C, S, Di ) = tOa * ( f f  i j  " Pnm) + rob * clusters_selected( (7ij, S) + we * documents_selected( Di , S) 
~ov~r~ge(Pi~,C) = ~ wk * Ikl 
kECi./ 
eonlent(Pij) = ~ wtvp,(W) 
WEPij 
tirnesiarap( D,,a=tim, ) - timestamp( Di ) 
time_sequ_ence ( Di, D) = timestamp( Dmaxtime ) - tiraestamp( D,nintime ) 
clusters_selected(C~, S) = IC~ n L.J cv=l 
v,w:P,,,~ES 
documents_selected(Di, S) = ~ = 
where 
Sire1 is the similarity metric for relevance ranking 
Sim~ is the anti-redundancy metric 
D is a document collection 
P is the passages from the documents in that collection (e.g., ~ j  is passage j from document Di) 
Q is a query or user profile 
R = IR(D, P, Q, 8), i.e., the ranked list of passages from documents retrieved by an IR system, given D, P, Q and a 
' relevance threshold O, below which it will not retrieve passages (O can be degree of match or number of passages) 
._5" is the subset of passages in R already selected 
R\S  is the set difference, i.e., the set of as yet unselected passages in R 
' C is the set of passage clusters for the set of documents 
(7vw is the subset of clusters of (7 that contains passage Pvw 
(7~ is the subset of clusters that contain passages from document D~ 
Ikl is the number of passages in the individual cluster k
IC~,~ N Cijl is the number of clusters in the intersection of (7,,,nand(Tij 
wi..are weights for the terms, which can be optimized 
W is a word in the passage/~j 
type is a particular type of word, e.g., city name 
IOil is the length of document i. 
Figure l: Definition of multi-document summarization algorithm - MMR-MD 
i 
I 
I 
I 
i 
I 
! 
I 
I 
I 
i 
! 
i 
sentence compression ratio of 0.2% and a character com- 
pression of 0.3%, approximately two orders of magni- 
tude different with compression ratios used in single doc- 
ument summarization. The results of summarizing this 
document set with a value of A set to I (effectively query 
relevance, but no MMR-MD) and A set to 0.3 (both query 
relevance and MMR-MD anti-redundancy) are shown in 
Figures 2 and 3 respectively. The summary in Figure 2 
clearly illustrates the need for reducing redundancy and 
maximizing novel information. 
Consider for instance, the summary shown in Figure 2. 
The fact that the ANC is fighting to overthrow the gov- 
44 
i. wsJg10204-0176:1 CAPE TOWN, South Africa - President EW. de Klerk's proposal to repeal the major pillars 
of apartheid rew a generally positive response from black leaders, but African National Congress leader Nelson 
Mandela called on the international community to continue conomic sanctions against South Africa until the 
government takes further steps. 
2. AP880803-0082:25 Three Canadian anti-apartheid groups issued a statement urging the government to sever 
diplomatic and economic links with South Africa and aid the African National Congress, the banned group fighting 
the white-dominated government in South Africa. 
3. AP880803-0080:25 Three Canadian anti-apartheid groups issued a statement urging the government to sever 
diplomatic and economic links with South Africa and aid the African National Congress, the banned group fighting 
the white-dominated government in South Africa. 
4. AP880802-0165:23 South Africa says the ANC, the main black group fighting to overthrow South Africa's white 
government, has seven major military bases in Angola, and the Pretoria government wants those bases closed 
down. 
5. AP880212-0060:14 ANGOP quoted the Angolan statement as saying the main causes of confict in the region 
are South Africa's "illegal occupation" of Namibia, South African attacks against its black-ruled neighbors and 
its alleged creation of armed groups to carry out "terrorist a~tivities" in those countries, and the denial of political 
rights to the black majority in South Africa. 
6. AP880823-0069:17 The ANC is the main guerrilla group fighting to overthrow the South African government 
and end apartheid, the system of racial segregation i which South Africa's black majority has no vote in national 
affairs. 
7. AP880803-0158:26 South Africa says the ANC, the main black group fighting to overthrow South Africa's white- 
led government, has seven major military bases in Angola, and it wants those bases closed down. 
8. AP880613-0126:15 The ANC is fighting to topple the South African government and its policy of apartheid, 
under which the nation's 26 million blacks have no voice in national affairs and the 5 million whites control the 
economy and dominate government. 
9. AP880212-0060:13 The African National Congress i the main rebel movement fighting South Africa's white-led 
government and SWAPO is a black guerrilla group fighting for independence for Namibia, which is administered 
by South Africa. 
I0. WSJ870129-0051:1 Secretary of State George Shultz, in a meeting with Oliver Tambo, head of the African 
National Congress, voiced concerns about Soviet influence on the black South African group and the ANC's use 
of violence in the struggle against apartheid. 
Figure 2: Sample multi-document summary with A = 1, news-story-principle ordering (rank order) 
? ernment is mentioned seven times (sentences #2,-#4,#6- 
#9),"which constitutes 70% of the sentences in the sum- 
mary. Furthermore, sentence #3 is an exact duplicate of  
sentence #2, and sentence #7 is almost identical to sen- 
tence #4. In contrast, the summary in Figure 3, generated 
using MMR-MD with a value of A set to 0.3 shows sig- 
nificant improvements in eliminating redundancy. The 
fact that the ANC is fighting to overthrow the govern- 
ment is mentioned only twice (sentences #3,#7), and one 
of these sentences has additional information in it. The 
new summary retained only three of  the sentences from 
the earlier summary. 
Counting clearly distinct propositions in both cases, 
yields a 60% greater information content for the MMR- 
MD case, though both summaries are equivalent in 
length. 
When these 200 documents were added to a set of 4 
other topics of 200 documents, yielding a document-set 
with 1000 documents, the query relevant multi-document 
summarization system produced exactly the same re- 
suits. 
We are currently working on constructing datasetsfor 
experimental evaluations of multi-document summariza- 
tion. In order to construct these data sets, we attempted 
to categorize user's information seeking goals for multi- 
document summarization (see Section 3). As can be seen 
in Figure 2, the standard IR technique of using a query to 
extract relevant passages i no longer sufficient for multi- 
document summarization due to redundancy. In addi- 
tion, query relevant extractions cannot capture temporal 
sequencing. The data sets will allow us to measure the 
effects of these, and other features, on multi-document 
summarization quality. 
Specifically, we are constructing sets of 10 documents, 
? which either contain a snapshot of  an event from mul- 
tiple sources or the unfoldment of an event over time. 
45 
I 
I 1. WSJ870129-0051 1 Secretary of State George Shultz, in a meeting with Oliver Tambo, head of the African Na- 
tional Congress, voiced concerns about Soviet influence on the black South African group and the ANC's use of 
violence in the struggle against apartheid. 
2. wsJgg0422-0133 44 (See related story: "ANC: Apartheid' s Foes - The Long Struggle: The ANC Is Banned, 
But It Is in the Hearts of a Nation's Blacks - -  In South Africa, the Group Survives Assassinations, Government 
Crackdowns n The Black, Green and Gold" - WSJ April 22, 1988) 
3. AP880803-0158 26 South Africa says the ANC, the main black group fighting to overthrow South Africa's white- 
led government, has seven major military bases in Angola, and it wants those bases closed own. 
4. AP880919-0052  But activist clergymen from South Africa said the pontiff should have spoken out more force- 
fully against their white-minority government's policies of apartheid, under which 26 million blacks have no say 
in national affairs. 
5. AP890821-0092 10 Besides ending the emergency and lifting bans on anti- apartheid groups and individual ac- 
tivists, the Harare summit's conditions included the removal of all troops from South Africa's black townships, 
releasing all political prisoners and ending political trials and executions, and a government commitment tofree 
political discussion. 
6. wsJg00503-0041 1  Pretoria and the ANC remain'far ap~t ontheir vision s for a post-apartheid South Africa: 
The ANC wants a simple one-man, one-vote majority rule system, while the government claims that will lead to 
black domination and insists on constitutional protection of the rights of minorities, including the whites. 
7. WSJ900807-0037 1 JOHANNESBURG, South Africa - The African National Congress uspended its 30-year 
armed struggle against he whiie minority government, clearing the way for the start of negotiations over a new 
constitution based on black-white power sharing. 
8. WSJ900924-011920 The African National Congress, South Africa's main black liberation group, forged its sanc- 
tions strategy as a means of pressuring the government toabandon white-minority rule. 
9. WSJ910702-0053 36 At a, meeting in South Africa this week, the African National Congress, the major black 
group, is expected to take a tough line again st the white-rnn government. 
10. wsJg10204-01761 CAPE TOWN, South Africa - President EW. de Klerk's proposal to repeal the major pillars 
of apartheid rew a generally positive response from black leaders, but African National Congress leader Nelson 
Mandela called on the international community to continue conomic sanctions against South Africa until the 
government takes further steps. 
Figure 3: Sample multi-document summary with A = 0.3, time-line ordering 
From these sets we are performing two types of exper- 
iments. In the first, we are examining how users put 
sentences into pre-defined clusters and how they create 
sentence based multi-document summaries. The result 
will also serve as a gold standard for system generated 
summaries - do our systems pick the same summary sen- 
tences as humans and are they picking sentences from 
the same clusters as humans? The second type Of exper- 
iment is designed to determine how users perceive the 
output summary quality. In this experiment, users are 
asked to rate the output sentences from the summarizer 
as good, okay or bad. For the okay or bad sentences, 
they are asked to provide a summary sentence from the 
document set that is "better", i.e., that makes a better set 
of  sentences to represent the information content of  the 
document set. We are comparing our proposed summa- 
rizer #6 in Section 4 to summarizer #1, the common por- 
tions of  the document sets with no anti-redundancy and 
summarizer #3, single document summary of  a centroid 
document using our single document summarizer (Gold- 
stein et al, 1999). 
7 Conc lus ions  and  Future  Work  
This paper presented a statistical method of  generating 
extraction based multi-document summaries. I t  builds 
upon previous work in single-document summarization 
and takes into account some of the major differences be- 
tween single-document and multi-document summariza- 
tion: (i) the need to carefully eliminate redundant infor- 
mation from multiple documents, and achieve high com- 
pression ratios, (ii) take into account information about 
document and passage similarities, and weight different 
passages accordingly, and (iii) take temporal information 
into account. 
Our approach differs from others in several ways: it 
is completely domain-independent, is based mainly on 
fast, statistical processing, it attempts to maximize the 
novelty of the information being selected, and different 
46 
I 
I 
I 
I 
I 
I 
! 
I 
! 
I 
! 
! 
I 
i 
I 
I 
! 
I 
! 
I 
I 
! 
I 
I 
genres or corpora characteristics an be taken into ac- 
count easily. Since our system is not based on the use of 
sophisticated natural language understanding or informa- 
tion extraction techniques, ummaries lack co-reference 
resolution, passages may be disjoint from one another, 
and in some cases may have false implicature. 
In future work, we will integrate work on multi- 
document summarization with work on clustering to pro- 
vide summaries for clusters produced by topic detection 
and tracking. We also plan to investigate how to gen- 
erate coherent temporally based event summaries. We 
will also investigate how users can effectively use multi- 
document summarization through interactive interfaces 
to browse and explore large document sets. 
References 
James Allan, Jaime Carbonell, George Doddington,, 
Jonathan Yamron, and Yiming Yang. 1998. Topic de- 
tection and tracking pilot study: Final report. In Pro- 
ceedings of the DARPA Broadcast News Transcription 
and Understanding Workshop. 
Chinatsu Aone, M. E. Okurowski, J. Gorlinsky, and 
B. Larsen. 1997. A scalable summarization sys- 
tem using robust NLP. In Proceedings of the 
ACL'97/EACL'97 Workshop on Intelligent Scalable 
Text Summarization, pages 66-73, Madrid, Spain. 
Breck Baldwin and Thomas S. Morton. 1998. Dy- 
namic coreference-based summarization. I Proceed- 
ings of the Third Conference on Empirical Methods in 
Natural Language Processing (EMNLP-3), Granada, 
Spain, June. 
Regina Barzilay and Michael Elhadad. 1997. Using lex- 
ical chains for text summarization. In Proceedings of 
the ACL'97/EACL'97 Workshop on Intelligent Scal- 
able Text Summarization, pages 10-17, Madrid, Spain. 
Branimir Boguraev and Chris Kennedy. 1997. Salience 
based content characterization f text documents. In 
Proceedings of the ACL'97/EACL'97 Workshop on 
Intelligent Scalable Text Summarization, pages 2-9,. 
Madrid, Spain. 
Chris Buckley. 1985. Implementation f the SMART in- 
formation retrieval system. Technical Report TR 85- 
686, Cornell University. 
Jaime G. Carbonell and Jade Goldstein. 1998. The 
use of MMR, diversity-based reranking for reordering 
documents and producing summaries. In Proceedings 
of SIGIR-98, Melbourne, Australia, August. 
Jade Goldstein and Jaime Carbonell. 1998. The use 
of mmr and diversity-based reranking in document 
reranking and summarization. In Proceedings of the 
14th Twente Workshop on Language Technology in 
Multimedia Information Retrieval, pages 152-166, 
Enschede, the Netherlands, December. 
Jade Goldstein, Mark Kantrowitz, Vibhu O. Mittal, and 
? Jaime G. Carbonell. 1999. Summarizing Text Doc- 
uments: Sentence Selection and Evaluation Metrics. 
Irf Proceedings of the 22nd International ACM SIGIR 
Conference on Research and Development in Informa- 
tion Retrieval (S1G1R-99), pages 121-128, Berkeley, 
CA. 
Eduard Hovy and Chin-Yew Lin. 1997. Automated text 
summarization i SUMMARIST. In ACUEACL-97 
Workshop on Intelligent Scalable Text Summarization, 
pages 18-24, Madrid, Spain, July. 
Judith L. Klavans and James Shaw. 1995. Lexical se- 
mantics in summarization. I  Proceedings of the First 
Annual Workshop of the IFIP Working Group FOR 
NLP and KR, Nantes, France, April. 
Julian M. Kupiec, Jan Pedersen, and Francine Chen. 
1995. A trainable document summarizer. In Proceed- 
ings of the 18th Annual Int. ACM/SIG1R Coaference 
on Research and Development in IR, pages 68-73, 
Seattle, WA, July. 
P. H. Luhn. 1958. Automatic reation of literature ab- 
stracts. IBM Journal, pages 159-165. 
Inderjeet Mani and Eric Bloedern. 1997. Multi- 
document summarization by graph search and merg- 
ing. In Proceedings of AAA1-97, pages 622--628. 
AAAI. 
Inderjeet Mani and Eric Bloedom. 1999. Summarizing 
similarities and differences among related ocuments. 
Information Retrieval, 1:35-67. 
Daniel'Marcu. 1997. From discourse structures to text 
summaries. In Proceedings of the ACL'97/EACL'97 
Workshop on Intelligent Scalable Text Summarization, 
pages 82-88, Madrid, Spain. 
Kathleen McKeown, Jacques Robin, and Karen Kukich. 
1995. Designing and evaluating a new revision-based 
model for summary generation. Info. Proc. and Man- 
agement, 31 (5). 
Kathleen McKeown, Judith Klavans, Vasileios Hatzivas- 
siloglou, Regina Barzilay, and Eleazar Eskin. 1999. 
Towards Multidocument Summarization by Reformu- 
lation: Progress and Prospects. In Proceedings of 
AAAI-99, pages 453--460, Orlando, FL, July. 
Mandar Mitra, Amit Singhal, and Chris Buckley. 1997. 
Automatic text summarization by paragraph extrac- 
tion. In ACL/EACL-97 Workshop on Intelligent Scal- 
able Text Summarization, pages 31-36, Madrid, Spain, 
July. 
Chris D. Paice. 1990. Constructing literature abstracts 
by computer: Techniques and prospects. Info. Proc. 
and Management, 26:171-186. 
Dragomir Radev and Kathy McKeown. 1998. Generat- 
ing natural language summaries from multiple online 
sources. Compuutational Linguistics. 
Gerald Salttm. 1970. Automatic processing of foreign 
language docuemnts. Journal of American Society for 
Information Sciences, 21:187-194. 
Gerald Salton. 1989. Automatic Text Processing: The 
Transformation, Analysis, and Retrieval of Informa- 
tion by Computer. Addison-Wesley. 
47 
James Shaw. 1995. Conciseness through aggregation i  
text generation. In Proceedings of 33rd Association 
for Computational Linguistics, pages 329-331. 
Gees C. Stein, Tomek Strzalkowski, and G. Bowden 
Wise. 1999. Summarizing Multiple Documents Us- 
ing Text Extraction and Interactive Clustering. In Pro- 
ceedings of PacLing-99: The Pacific Rim Conference 
on Computational Linguistics, pages 200-208, Water- 
loo, Canada. 
Tomek Strzalkowski, Jin Wang, and Bowden Wise. 
1998. A robust practical text summarization system. 
In AAAI Intelligent Text Summarization Workshop, 
pages 26-30, Stanford, CA, March. 
J. I. Tait. 1983. Automatic Summarizing of English 
Texts. Ph.D. thesis, University of Cambridge, Cam- 
bridge, UK. 
Simone Teufel and Marc Moens. 1997. Sentence x- 
traction as a classification task. In ACL/EACL-97 
Workshop on Intelligent Scalable Text Summarization, 
pages 58-65, Madrid, Spain, July. 
TIPSTER. 1998a. Tipster text phase III 18-month work- 
shop notes, May. Fairfax, VA. 
TIPSTER. 1998b. Tipster text phase III 24-month work- 
shop notes, October. Baltimore, MD. 
Charles J. van Rijsbergen. 1979. Information Retrieval. 
Butterworths, London. 
Yiming Yang, Tom Pierce, and Jaime 13. Carbonell. 
1998. A study on retrospective and on-line event de- 
tection. In Proceedings of the 21th Ann lnt ACM SI- 
G1R Conference on Research and Development inIn- 
formation Retrieval (SIGIR'98), pages 28-36. 
:Yiming Yang, Jaime G. Carbonell, Ralf D. Brown, 
Tom Pierce, Brian T. Archibald, and Xin Liu. 1999. 
Learning approaches for topic detection and tracking 
. news events. IEEE Intelligent Systems, Special Issue 
on Applications of Intelligent Information Retrieval, 
14(4):32-43, July/August. 
48 
