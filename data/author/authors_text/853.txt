Robust Segmentation of Japanese Text into a Lattice for Parsing 
Gary Kaemarcik, Chris Brockett, Hisami Suzuki 
M icrosofl Research 
One Microsoft Way 
Redmond WA, 98052 USA 
{ garykac,chrisbkt, hisamis }@in i croso ft.com 
Abstract 
We describe a segmentation component that 
utilizes minimal syntactic knowledge to produce a
lattice of word candidates for a broad coverage 
Japanese NL parser. The segmenter is a finite 
state morphological nalyzer and text normalizer 
designed to handle the orthographic variations 
characteristic of written Japanese, including 
alternate spellings, script variation, vowel 
extensions and word-internal parenthetical 
material. This architecture differs from con- 
ventional Japanese wordbreakers in that it does 
not attempt to simultaneously attack the problems 
of identifying segmentation candidates and 
choosing the most probable analysis. To minimize 
duplication of effort between components and to 
give the segmenter greater fi'eedom to address 
orthography issues, the task of choosing the best 
analysis is handled by the parser, which has access 
to a much richer set of linguistic information. By 
maximizing recall in the segmenter and allowing a 
precision of 34.7%, our parser currently achieves a
breaking accuracy of ~97% over a wide variety of 
corpora. 
Introduction 
The task of segmenting Japanese text into word 
units (or other units such as bunsetsu (phrases)) 
has been discussed at great length in Japanese NL 
literature (\[Kurohashi98\], \[Fuchi98\], \[Nagata94\], 
et al). Japanese does not typically have spaces 
between words, which means that a parser must 
first have the input string broken into usable units 
before it can analyze a sentence. Moreover, a 
variety of issues complicate this operation, most 
notably that potential word candidate records may 
overlap (causing ambiguities for the parser) or 
there may be gaps where no suitable record is 
found (causing a broken span). 
These difficulties are commonly addressed using 
either heuristics or statistical methods to create a 
model for identifying the best (or n-best) sequence 
of records for a given input string. This is 
typically done using a connective-cost model 
(\[Hisamitsu90\]), which is either maintained 
laboriously by hand, or trained on large corpora. 
Both of these approaches uffer fiom problems. 
Handcrafted heuristics may become a maintenance 
quagmire, and as \[Kurohashi98\] suggests in his 
discussion of the JUMAN scgmenter, statistical 
models may become increasingly fi'agile as the 
system grows and eventually reach a point where 
side effects rule out fiwther improvements. The 
sparse data problem commonly encountered in 
statistical methods is exacerbated in Japanese by 
widespread orthographic variation (see ?3). 
Our system addresses these pitfalls by assigning 
completely separate roles to the segmeuter and the 
parser to allow each to delve deeper into the 
complexities inherent in its tasks. 
Other NL systems (\[Kitani93\], \[Ktu'ohashi98\]) 
have separated the segmentation and parsing 
components. However, these dual-level systems 
are prone to duplication of effort since mauy 
segmentation ambiguities cannot be resolved 
without invoking higher-level syntactic or 
semantic knowledge. Our system avoids this 
duplication by relaxing the requirement that the 
segmenter identify the best path (or even n-best 
paths) through the lattice of possible records. The 
segmenter is responsible only for ensuring that a 
correct set of records is present in its output. It is 
the filnction of the parsing component to select he 
best analysis from this lattice. With tiffs model, 
our system achieves roughly 97% recall/precision 
(see \[Suzuki00\] for more details). 
1 System Overview 
Figure shows a simple block diagram of our 
Natural Language Understanding system for 
Japanese, the goal of which is to robustly produce 
syntactic and logical forms that allow automatic 
390 
Word Segmentation 
l{ 
\[ Dcrivational .,\sscmhl~ I 
Syntactic Analysis \] 
\[ \[,ogical Form \] 
,0 ( )rthograph.v 
l.exicon 
Syntax 
I.cxicon 
% 
Figure 1: Block diagram of Japanese NL system 
extraction of semantic relationships (see 
\[Richardson98\]) and support other lirlguistic 
projects like information retrieval, NL interfaces 
and dialog systems, auto-.summarization and 
machine translation. 
The segmenter is the frst level of' processing. This 
is a finite-state morphological nalyzer esponsible 
for generating all possible word candidates into a 
word lattice. It has a custom lexicon (auto: 
matically derived from the main lexicon to ensure 
consistency) that is designed to facilitate the 
identification of orfllographic variants. 
Records representing words and morphemes are 
handed off by the segmenter to the derivational 
assembly component, which uses syntax-like rules 
to generate additional derived forms that are then 
used by the parser to create syntax trees and logical 
forms. Many of the techniques here are similar to 
what we use in our Chinese NI., system (see 
\[Wu98\] for more details). 
The parser (described exterisively in \[Jensen93\]) 
generates syntactic representatioris arm logical 
forms. This is a bottomoup chart parser with 
binary rnles within the Augnmnted Phrase 
Structure Grammar formalism. The grammar rules 
are language--specific while the core engine is 
shared among 7 languages (Chinese, Japanese, 
Korean, English, French, German, Spanish). The 
Japanese parser is described in \[Suzuki00\]. 
2 Recall vs? Precision 
In this architecture, data is fed forward from one 
COlnponent to the next; hence, it is crucial that the 
base components (like the segmenter) generate a
minimal number of omission errors. 
Since segmentation errors may affect subsequent 
components, it is convenient to divide these errors 
into two types: recoverable and non-recoverable. 
A ram-recoverable error is one that prevents the 
syntax (or any downstream) component from 
arriving at a correct analysis (e.g., a missing 
record). A recoverable rror is one that does not 
interfere with the operation of following 
components. An example of the latter is the 
inchision of an extra record. This extra record 
does not (theoretically) prevent the parser from 
doing its lob (although in practice it may since it 
eonsun les  resot l rces) .  
Using standard definitions of recall (R) and 
precision (P): 
*~ Jr R - Seg~,,,.,.,.,., p = Seg~,,,.,.~.~., 
7bg,,,,,/ &g,,,,,,i 
where Segcor~ec t and .<,egmxal are the number q/" "'cotwect" 
and total number o/'segments returned by the segmentet; 
and "\['agto~a I is the total Jlttmber of "correct" segments 
fi'om a tagged corpus, 
we can see that recall measures non-recoverable 
errors and precision measures recoverable rrors. 
Since our goal is to create a robust NL system, it 
behooves us to maximize recall (i.e., make very 
few non-recoverable errors) in open text while 
keeping precision high enough that the extra 
records (recoverable errors) do not interfere with 
the parsing component. 
Achieving near-100% recall might initially seem to 
be a relatively straightforward task given a 
sufficiently large lexicon - simply return every 
possible record that is found in the input string, in 
practice, tile mixture of scripts and flexible 
orthography rules of Japanese (in addition to the 
inevitable non-lexicalized words) make the task of 
identifying potential lexical boundaries an 
interesting problem in its own right. 
3 Japanese Orthographic Variation 
Over tile centuries, Japanese has evolved a 
complex writing system that gives tile writer a 
great deal of flexibility when composing text. 
Four scripts are in common use (kanji, hiragana, 
katakana and roman), and can co-occur within 
lexical entries (as shown ill Table 1). 
Some mixed-script entries could be handled as 
syntactic ompounds, for example, ID ~a---1-" /at 
dii kaado="ID card'7 could be derived fl'om 
1DNotJN + 79-- I ~ NOUN. tlowever, many such items 
are preferably treated as lexical entries because 
391 
i!i \[~ ~ ' \[atarashii ,: "'new "\] 
Kanji-I l iragana I~LII~J \[ho~(vtnn'ui = "'mammal"\] 
~ 7/" "7 :./\[haburashi -~ "'toothbrush "\] 
. . . . . . . . . . . . . . . . . . .  . . . . . . . . .  
K'}n.!i-<~!P!\]!! ...................... E ( !S  ' ! t ( !Z . /~( :GS tm! ' i  t? ::c:(';S's:vstet!* :'1 ........ 
12 ) J \[lmtmL, atsu - "December"/ 
Kallji-Synlbol v ,{'~ \ [gcmma sen = "'~amma rays "\] 
.i-3 1" 4" t/ \[otoim - "'toilet "'\] 
Mixed kana 
............................. \[ -/")3 hl~?!,,!! ? .;;(9 c?f~,,,~/5' ;7/ ........... 
II3 )-~ - -  b" \[aidtt kaado = "lP card"\] 
Kana-Alpha ..t .y-t~ .>"-5;-V - -RNA \[messe~gaa RA'A = 
................................................................. T ie~t~:s~'~lxe~ ~U.:I. '7 .................................. 
7, J- U 2/-)'- ~) 2, 90 \[suloronchiumu 90 - 
"'Strontiunt 90 "\] 
Kana-Symbol I~ 2_ 7~ 40 ? \[hoeru yonjuu do = "'roaring 
............................................................................ fo t : ! !~, , ; .7  ......................................................... 
~i~ b ~" ~, \[keshigomu - "'eraser "1 
a >,I- 21" ~ e/ ~) ~: \[artiSt kentauri set : 
Other mixed 
"'Alpha Centauri "\] 
\[. ~: ~ \[togaki = "'stage directions"\] 
Table 1: Mixed-script lexical entries 
they have non-compositional syntactic or semantic 
attributes. 
In addition, many Japanese verbs and adjectives 
(and words derived from them) have a variety of 
accepted spellings associated with okurigana, 
optional characters representing inflectional 
endings. For example, the present ense of e)j b ~;?; 
~- (kiriotosu = "to prune ") can be written as any 
of: ~)Ji:~~J --, ~;)J b .."?,:t, ?:JJTf; & ,~, ~JJb.~s&~-, ~ ~:; ~s ~ I 
or even ~ ~) ~'g ~ 4-, -~ 9 7~;-~-. 
Matters become even more complex when one 
script is substituted for another at the word or sub- 
word level. This can occur for a var iety o f  
reasons: to replace a rare or d i f f i cu l t  ka@ (.~?~ 
\[rachi= "kMnap"\] instead of f,):~); to highlight a 
word in a sentence ( ~ >" t2 -h, o ~_ *) \[henna 
kakkou = "strange appearance '7); or to indicate a 
particular, often technical, sense (7 Y o -c \[watatte 
="crossing over"\] instead of iA~o-c, to emphasize 
the domain-specific sense of "connecting 2 
groups" in Go literature). 
More colloquial writing allows for a variety of 
contracted tbrms like ~ t~j~.\-~  ~ t~ + !=t \[ore~ 
tacha = ore-tachi + wa = "we'" + TOPIC\] and 
phonological inutations as in ~d.~:--~- = -d'4 \ [dee- 
su ~ desu = "is "\]. 
This is only a sampling of the orthographic issues 
present in Japanese. Many of these variations pose 
serious sparse-data problems, and lexicalization of 
all variants is clearly out of the questioi1. 
II.\]'~., ~lJ < . .~  lt,l~L~l~;'~q ?gJ \[H/ikc,kkoku "'every 
repeat moment "'\] 
characters Ill ~ ~., e L I~, .~ HI :!:~ til-!~ U ~ ' \[kaigaishu 
"'diligent "'\] 
distribution of  t: " '>" v~- ~ t:":#v\]- \[huleo "vMeo"\] voicina nl,qrks 
halt\vidth & 
lhllwidth 
composite 
symbols 
F M b2J~" ~ FM )/ZJ~ \[I"M housml ~ "FM 
broadcast "'\] 
;~ (i'?" ~U, "-~ 5" 4, "V <e" :>, 2, \[daivaguramu : 
"'diagram 'J 
;~; "-~ . . . . .  L" 2/ 1- \[paasento =: "percent :;\] 
r tz  , .  . . /~ ,  . 
"'incorporated 'i\] 
N\] ~ 2 8 FI \[n!jmthactH niciu = "28 'j' day of the 
month "\] 
Table 2: Character type normalizations 
4 Segmenter Design 
Given the broad long-term goals for' the overall 
system, we address the issues of recall/precision 
and orthographic variation by narrowly defining 
the responsibilities of the segmenter as: 
(i) Maximize recall 
(2) Normalize word variants 
4.1 Maxinf fze Recall  
Maximal recall is imperative, Any recall mistake 
lnade in the segmenter prevents the parser from 
reaching a successful analysis. Since the parser in 
our NL system is designed to handle ambiguous 
input in the fbrm of a word lattice of potentially 
overlapping records, we can accept lower precision 
if that is what is necessary to achieve high recall? 
Conversely, high precision is specifically not a 
goal for the segmenter. While desirable, high 
precision may be at odds with the primary goal of 
maximizing recall. Note that the lower bound for 
precision is constrained by the lexicon. 
4?2 Normal ize word variants 
Given tile extensive amount of orthographic 
variability present in Japanese, some form of 
normalization into a canonical form is a pre- 
requisite for any higher-.order linguistic processing. 
The segmenter performs two basic kinds of 
nomlalization: \[,emmatization f inflected forms 
and Orthographic Norlnalization. 
392 
,~kur ieana . . . .  n),: g.). z~ -+ i,J,: ~- ~),~ :~ \[lhkmuk~ :: "drafty"/  ;5'./~ <% J)-tJ- ; '3_ \[11i1:i:;5,.,7~-\]\[ ~", ,. ,,,,.. ~ ' >\]-~J-'"o kammwaseru = 7o  
. . . . . . . . . . . . . . . . . . .  ~ !o{ ~ c!i ~s /./!,a!!{,6! : ::e ,!:e!l?!el<, 71 . . . .  . . . . . . . . . . . . . . . . . . . . . . . . . . .  engage (gear.w i' 
a ml l .~ ' l lmor i  - "'~111 non-s tandard  tA'ct) ::~ "~ ~-0~ \]'- \[onnanoko :: "gir l"/  .;.'/. <) ~, l) \[ ~u:<;'~-\]\[ ~ [:~-). {,. ~') \] estimate "' 
script +~:t , - t  - "+ 0 :4  7, :~ \[d,s'uko : "d i sco" /  I I )  ) :2  I" \ [ l i ' , ;4 -  l l l \ ]) :<fi4 - - i :DZ i D: a/ih, kaado2:" lDc iml  '" 
? D, I\] "+ " "~ Jl \[tkka,4etsu :: "one month" /  . . . . . . . . . . . . . . . .  
9:0  var iants  kagi tahako "'smf/'f --I, I!:~ " \['~\] \[ktl.~'uml~aseki = "Kasunli~aseki "\] 
numera ls  fi~r 5 i\[i~ .+ ti\]!i~ \[gorm : "'Olympics "\] 
kalt j i  1 ),, ~ ")v \[hitori : "one person"/  
:t.; i Z - -  ~ ,t~, k.  ~ -~J { 2 b ' ~5 /~ \[oniisan = "'older 
vovcel 
extens ions  brother "'\] 
-7 -,- 4' \]" 4 "+ 7 -," 4" I" \[/'aito 'ftght "'\] 
'Y - /  4- ~- 9 "../-i~ i<.( ~- 9 "-./\[batorm - "'vloti~F7 
ka lakana  
-)" 9 :E- - -  Iv ..+ 9 ~ . O ? :E- --" F' \ [aramoode "'h la v ari all Is too& "'\] 
IPL~(!rt)2_ ~ + ll~),t 2. ".'~ \[haeru = "'to shine "7 
in l ine yomi  ~ ~_lt(f:l: ~o ~I> ") )\]~(i "+l/'~ ~t JL?\[ \[hachuurui 
D,3 '/, < :~' II~';L D ', ~ II:t:~'i 0:: 'J "< -~ I toDac~'o" 
/g;'~; V, 1-{{:/~ D;\]\[!i,~:l '  '1 na~ai "'a long visit" 
Table 4: Orthography lattices 
lexical entry. Examples are given in "fable 3. Two 
cases of special interest are okurigana and inline 
yomi/kanji normalizations. The okurigana 
normalization expands shortened forms into fully 
specified forms (i.e., fornls with all optional 
................................. 7"~'!~t!?C'\] ..................................................................................... characters present). Tile yomi/kanji handling takes 
in l ine kanj i  l Yo (N)~: i i l2d  -'~ tgs ~ft~ \[~essh~rul =: "'rodent"/ 
Table 3: Script normalizations 
4.Z 1 Lemmatization 
LEMMATIZATION iri Japanese is the same as that 
for any language with inflected forms - a lemma, 
or dictionary form, is returned along with the 
inflection attributes. So, a form like ~:~-7~ \[tabeta 
= "ate "J would retum a lemma of f,~ ~<~; \[taberu = 
"'eat"\] along with a PAST attribute. 
Contracted forms are expanded and lemmatized 
individually, so that f,~ ~<-<o ~:~ .> o ?= \[tabe~ 
tecchatta = "has ectten and gone'7 is returned as: 
f~  ~-Z. 7 0 G\[-RUND -F (, x < GERUND -F L. +E ") PASr  
\ [taberu: "eat" + iku--++go" F s \ ]T imaz l=. . iSpE(7\ [ . ' \ ] .  
4..2.2 Orthographic Normalizatio,, 
ORTIIOGRAPttlC NORMALIZATION smoothes out 
orthographic variations o that words are returned 
in a standardized form. This facilitates lexical 
lookup and allows tile system to map the variant 
representations to a single lexicon entry. 
We distinguish two classes of orqthographic 
normalization: character type normalization and 
script normalization. 
CI IARAC' IER  TYPE  NORMAI . IZAT ION takes tile 
various representations allowed by the Unicode 
specification and converts them into a single 
consistent form. Table 2 summarizes this class of 
normalization. 
SCR. I I ' T  NORMAI,IZAI'ION rewrites the word so that 
it conforms to tile script and :~pelling used in the 
infixed parenthetical material and normalizes it out 
(after using the parenthetical infommtion to verify 
segmentation accuracy). 
5 Lexicon Structures 
Several special lexicon structures were developed 
to support hese features. Tile most significant is 
an orthography lattice* that concisely encapsulates 
all orthographic variants for each lexicon entry and 
implicitly specifies the normalized form. This has 
the advantage of compactness and facilitates 
lexicon maintenance since lexicographic inform- 
ation is stored in one location. 
The orthography lattice stores kana inforrnation 
about each kanji or group of kanji in a word. For 
example, the lattice far the verb Y~:-<~D \[taberu = 
"eat'7 is \[~:#_\]-<~, because the first character 
(ta) can be written as either kanji 5~ or kana 1=. A 
richer lattice is needed for entries with okurigana 
variants~ like LJJ 0 i'~:~ 4 \[kiriotosu = "'to prune "\] 
cited earlier: commas separate each okurigana 
grouping. The lattice for kiriotosu is \[OJ:~, 0 \]\[i"#: 
~,  E \]~j-. Table 4 contains more lattice examples. 
Enabling all possible variants can proliferate 
records and confiise the analyzer (see \[Kurohashi 
94\]). We therefore suppress pathological variants 
that cause confusion with more common words 
and constructions. For example, f:L-t,q- \[n~gai = "a 
long visit'7 never occurs as I.~ ~' since this is 
ambiguous with the highly fi'equent adjective ~-~v, 
/nasal - "l<mg'7. Likewise, a word like !t 
' Not to be confiised with the word lattice, which is the 
set of records passed fi'om the segmenter tothe parser. 
393 
\[nihon = ",Aq)an "7 is constrained to inhibit invalid 
variants like 124< which cause confusion with: {c 
I'OSl' + # NOUN \ [ t I i : : I ' . - tRT IC I . I : "  + /1on  = "book  " \ ] .  
We default to enabling all possible orthographies 
for each ennT and disable only those that are 
required. This saves US from having to update the 
lexicon whenever we encounter a novel 
orthographic variant since the lattice anticipates all 
possible variants. 
6 Unknown Words 
Unknown words pose a significant recall problem 
in languages that don't place spaces between 
words. The inability to identify a word in the input 
stream of characters can cause neighboring words 
to be misidentified. 
We have divided this problem space into six 
categories: variants of lexical entries (e.g., 
okurigana variations, vowel extensions, et al); 
non-lexiealized proper nouns; derived forms; 
foreign Ioanwords; mimetics; and typographical 
errors. This allows us to devise focused heuristics 
to attack each class of unfound words. 
The first category, variants of lexical entries, has 
been addressed through the script normalizations 
discussed earlier. 
Non-lexicalized proper nouns and derived words, 
which account for the vast majority of unfound 
words, are handled in the derivational assembly 
component. This is where compounds like -: ~ >i 
x ~':, ffuransugo = "French (language)"\] are 
assembled from their base components ;1 5~ J x 
\[furansu : "France "\] and at~ \[go = "language "J. 
Unknown foreign Ioanwords are identified by a 
simple maximal-katakana heuristic that returns the 
longest run of katakana characters. Despite its 
simplicity, this algorithm appears to work quite 
reliably when used in conjunction with the other 
mechanisms in our system. 
Mimetic words in Japanese tend to follow simple 
ABAB or ABCABC patterns in hiragana or 
katakana, so we look for these patterns and 
propose them as adverb records. 
The last category, typographical errors, remains 
mostly the subject for future work. Currently, we 
only address basic : (kanji) ~-~ -: (katakana) and 
i-, (hiragana) +~ : ' -  (katakana) substitutions. 
50% 
40% 
30% 
20% 
"10% 
0% 
15 25 35 45 55 65 75 85 95 105 115 
- - -~ Japanese  =-~t-=Chinese \]
? . . . . .  72 .27_~_z z zs?  27 ~ 7 ~Lz77 ~z ~25z ~ 2 7~ . . . . . . . .  
Figure 2: Worst-case segmenter precision (y-axis) versus 
sentence length (x-axis - in characters) 
7 Eva|uation 
Our goal is to improve the parser coverage by 
improving the recall in the segmenter. Evaluation 
of this component is appropriately conducted in the 
context of its impact on the entire system, 
Z 1 Parser Evaluation 
Running on top of our segmenter, our current 
parsing system reports ~71% coverage + (i.e, input 
strings for which a complete and acceptable 
sentential parse is obtained), and -,97% accuracy 
for POS labeled breaking accuracy? A full 
description of these results is given in \[Suzuki00\]. 
Z 2 Segmenter Evaluatkm 
Three criteria are relevant to segmenter per- 
formance: recall precision and speed. 
Z Z 1 Recall 
Analysis of a randonlly chosen set of tagged 
sentences gives a recall of 99.91%. This result is 
not surprising since maxindzing recall was a 
prinlary focus of our efforts. 
The breakdown of the recall errors is as follows: 
missing proper nouns = 47%, missing nouns = 
15%.. missing verbs/adjs = 15%, orthographic 
idiosyncrasies = 15%, archaic inflections = 8%. 
It is worth noting that for derived forms (those that 
Tested on a 15,000 sentence blind, balanced corpus. 
See \[SuzuldO0\] fordetails. 
394 
3000 \ [ i  
2000 I 
1 
<':> ,# ,~, ?> e <# ~,~, e e @,, ,+>,e,e 
Figure 3: Characters/second (y~axis) vs. sentence 
length (x-axis) for se~<ginenter alone (upper curve) 
and our NL system as a whole (lower curve) 
are tiandled in the derivational assembly corn-. 
ponent), tim segmenter is considered correct as 
long as it produces the necessary base records 
needed to build the derived fom-t. 
ZZ2 Precision 
Since we focused our effbrts on maximizing recall,, 
a valid concern is the impact of the extra records 
on the parser, that is, the effect of lower segmenter 
precision oll the system as a whole. 
Figure 2 shows the baselirie segrnenter precision 
plotted against sentence length using the 3888 
tagged sentences ~: For compaiison~ data for 
Chinese ~is included. These are baseline vahles in 
the sense they represent the riumber of records 
looked up in the lexicon without application of ariy 
heuristics to suppress invalid records. Thus, these 
mnnbers represent worst--case segmenter p ecision. 
The baseline precisior, for the Japariese segmenter 
averages 24.8%, whicl-i means that a parser would 
need to discard 3 records for each record it used in 
the final parse. TMs value stays fairly constant as 
the sentence length increases. The baseline 
precision for Chir, ese averages 37.1%. The 
disparity between the Japanese and Chinese worst- 
case scenario is believed to reflect the greater 
ambiguity inherent in the Japanese v<'riting system, 
owing to orthographic w~riation and the use of a 
syllabic script. 
++ The " <,<," o .~ t<%~,% was obtained by usin,,the results of the 
parser on untagged sentences. 
39112 sentences tagged in a sirnilar fashion using our 
Chinese NI,P system. 
100% 
70% "-:-: 5 ~ ::.::~::,~::-5.. ,'i ,': ~ -"r.'-~,'~7,:'s~'-,.: ~ :  .~ ~ ::,~ x;K< ~ 
50% 
40% ~ ~ - . ---..-~ 
30% ::::::::::::::::::::::::::::::::::::::::::::::::::::::: 
20% :::i:!)?i:~:~)}ii!:i\]::{i)~:,x::i!illii.:i!:'-.~!!~\]:!21{7-i\[.g{:!:'7:7:~::?. . . . . . . . . . . . . . . . . . . . . .  ,~< ............ 
10% !::::::ii'::::ii!i'ii{f!}{ii'.".'::iii::::ii 
0% ~'~S 2 
15 25 35 45 55 65 75 85 95 105 115 125 135 
\ [BSegmenter  \ [ \ ]Lex ica l  E IDer iv  BOther  El Parser  
Figure 4: Percentage oftime spent in each component (y- 
axis) vs. sentence l ngth x-axis) 
Using conservative pruning heuristics, we are able 
to bring the precision tip to 34.7% without 
affecting parser recall. Primarily, these heuristics 
work by suppressing the hiragana form of shork 
ambiguous words (like ~ \[ki="tree, air, .slJirit, 
season, record, yellow,... '7, which is normally 
written using kanji to identify the intended sense). 
Z2..3 Speed 
Another concern with lower precision values has to 
do with performance measured in terms of speed. 
Figure 3 summarizes characters-per.-second per- 
formance of the segmentation component and our 
NL system as a whole (irmluding the segmentation 
component). As expected, the system takes more 
time for longer senterlces. Crucially, however, the 
system slowdowri s shown to be roughly linear, 
Figure 4 shows how nluch time is spent in each 
component during sentence analysis. As the sen- 
tence length increases, lexical lookup+ derivational 
morphology and '+other" stay approximately con- 
starit while the percentage of time spent in the 
parsing component increases. 
Table 5 compares parse time performance for 
tagged and untagged sentences. This table 
qnantifies the potential speed improvement that the 
parser could realize if the segmenter precision was 
improved. Cohunn A provides baseline lexical 
lookup and parsing times based on untagged input. 
Note that segmenter time is not given this table 
because it would not be comparable to tile hypothetical 
segmenters devised for columns P, and C. 
395 
A 
Lexical processing 7.66 s 
Parsing 3.480 s
Other 4. 95 s 
Total 25.336 s 
Overall 
Percent Lexical 
Improvement I'arsing 
Other 
B c 
2.5 0 s 2.324 s
8.865 s 7. 79 s 
3.620 s 3.5 9 s 
4.995 s 3.022 s
40.82% 48.60% 
67.24% 69.66% 
34.24% 46.74% 
3.7 % 6. % 
Table 5: Summary of performance (speed) 
experiment where untagged input (A) is compared 
with space-broken i put (B) and space-broken i put 
with POS tags (C). 
Columns B and C give timings based on a 
(hypothetical) segmenter that correctly identifies 
all word botmdaries (B) and one that identifies all 
word boundaries and POS (C) 1'I". C represents the 
best-case parser performance since it assumes 
perfect precision and recall in the segmenter. The 
bottom portion of Table ,5 restates these 
improvements a percentages. 
This table suggests that adding conservative 
pruning to enhance segmenter precision may 
improve overall system performance. It also 
provides a metric for evaluating the impact of 
heuristic rule candidates. The parse-time 
improvemeuts from a rule candidate can be 
weighed against the cost of implementing this 
additional code to determine the overall benefit o 
the entire system. 
8 Future 
Planued near-term enhancements include adding 
context-sensitive h uristic rules to the segmenter as 
appropriate. In addition to the speed gains 
quantified in Table 5, these heuristics can also be 
expected to improve parser coverage by reducing 
resource requiremeuts. 
Other areas for improvement are unfotmd word 
models, particularly typographical error detection, 
and addressing the issue of probabilities as they 
apply to orthographic variants. Additionally, we 
are experimenting with various lexicon formats to 
more efficiently support Japanese. 
tt For the hypothetical segmenters, our segmenter was 
modified to return only the records consistent with a 
tagged input set. 
9 Conclusion 
The complexities involved in segmenting Japanese 
text make it beneficial to treat this task 
independently from parsing. These separate tasks 
are each simplified, thcilitating the processing of a 
wider range of phenomenon specific to their 
respective domains. The gains in robustness 
greatly outweigh the impact on parser performance 
caused by the additional records. Our parsing 
results demonstrate that this compartmentalized 
approach works well, with overall parse times 
increasing linearly with sentence length. 
10 References 
\[Fuchi98\] Fuchi,T., Takagi,S., "Japanese 
Morphological Analyzer using Word Co-occurrence", 
ACL/COLING 98, pp409-4 3, 998. 
\[Hisamitsu90\] Hisamitsu,T., Nitta, Y., 
Morphological Analyis by Minimum Connective-Cost 
Method", SIGNLC 90-8, IEICE pp 7-24, 990 (in 
Japanese). 
\[Jensen93\] Jensen,K., Heidorn,G., Richardson,S, 
(eds.) "Natural Language Processing: The PLNLP 
Approach", Kluwer, Boston, 993. 
\[Kitani93\] Kitani,T., Mitamura,T., "A Japanese 
Preprocessor for Syntactic and Semantic Parsing", 9 th 
Conference on AI in Applications, pp86-92, 993. 
\[Kurohashi94\] Kurohashi,S., Nakamura,Y., 
Matsumoto,Y., Nagao,M., "hnprovements of Japanese 
Morphological Analyzer JUMAN", SNLR, pp22-28, 
994. 
\[Kurohashi98\] Kurohashi,S., Nagao,M., "Building a 
Japanese Parsed Corpus while hnproving the Parsing 
System", First LREC Proceedings, pp7 9-724, 998. 
\[Nagata94\] Nagata,M., "A Stochastic Japanese 
Morphological Analyzer Using a Forward-DP 
Backward-A* N-Best Search Algorithm", COL1NG, 
pp20-207, 994. 
\[Richardson98\] Richardson,S.D., Dolan,W.B., 
Vanderwende,L., "MindNet: Acquiring and Structuring 
Semantic Information from Text", COLING/ACL 98, 
pp 098- 02, 998. 
\[Suzuki00\] Suzuki,H., Brockett,C., Kacmarcik,G., 
"Using a broad-coverage parser for word-breaking in 
Japanese", COLING 2000. 
\[Wu98\] Wu,A., Zixin,J., "Word Segmentation in 
Sentence Analysis", Microsoft Technical Report MSR- 
TR-99- 0, 999. 
396 
Using a Broad-Coverage Parser for Word-Breaking in Japanese 
Hisami Suzuki, Chris Brockett and Gary Kacmarcik 
Microsoft Research 
One Microsoft Way 
Redmond WA 98052 USA 
{ hisamis, chrisbkt, garykac }@ microsoft.corn 
Abstract 
We describe a method of word segmentation i
Japanese in which a broad-coverage parser selects 
the best word sequence while producing a syntactic 
analysis. This technique is substantially different 
from traditional statistics- or heuristics-based 
models which attempt o select the best word 
sequence before handing it to the syntactic 
component. By breaking up the task of finding the 
best word sequence into the identification of words 
(in the word-breaking component) and the selection 
of the best sequence (a by-product of parsing), we 
have been able to simplify the task of each 
component and achieve high accuracy over a wide 
varicty of data. Word-breaking accuracy of our 
system is currently around 97-98%. 
1. Introduction 
Word-breaking is an unavoidable and crucial first 
step toward sentence analysis in Japanese. In a 
sequential model of word-breaking and syntactic 
analysis without a feedback loop, the syntactic 
analyzer assumes that the results of word-breaking 
are correct, so for the parse to be successful, the 
input from the word-breaking component must 
include all words needed for a desired syntactic 
analysis. Previous approaches to Japanese word 
segmentation have relied on heuristics- or 
statistics-based models to find the single most 
likely sequence of words for a given string, which 
can then be passed to the syntactic omponent for 
further processing. The most common 
heuristics-based approach utilizes a connectivity 
matrix between parts-of-speech and word 
probabilities. The most likely analysis can be 
obtained by searching for the path with the 
minimum connective cost (Hisamitsu and Nitta 
1990), often supplemented by additional heuristic 
devices such as the longest-string-match or the 
least-number-of-bunsetsu (phrase). Despite its 
popularity, the connective cost method has a major 
disadvantage in that hand-tuning is not only 
labor-intensive but also unsafe, since adjusting the 
cost for one string may cause another to break. 
Various heuristic (e.g. Kurohashi and Nagao 1998) 
and statistical (e.g. Takeuchi and Matsumoto 1997) 
augmentations of the minimum connective cost 
method have been proposed, bringing 
segmentation accuracy up to around 98-99% (e.g. 
Kurohashi and Nagao 1998, Fuchi and Takagi 
1998). 
Fully stochastic language models (e.g. Nagata 
1994), on the other hand, do not allow such manual 
cost manipulation and precisely for that reason, 
improvements in segmentation accuracy are harder 
to achieve. Attaining a high accuracy using fully 
stochastic methods is particularly difficult for 
Japanese due to the prevalence of orthographic 
variants (a word can be spelled in many different 
ways by combining different character sets), which 
exacerbates the sparse data problem. As a result, 
the performance of stochastic models is usually not 
as good as the heuristics-based language models. 
The best accuracy reported for statistical methods 
to date is around 95% (e.g. Nagata 1994). 
Our approach contrasts with the previous 
approaches in that the word-breaking component 
itself does not perform the selection of the best 
segmentation analysis at all. Instead, the 
word-breaker returns all possible words that span 
the given string in a word lattice, and the best word 
sequence is determined by applying the syntactic 
rules for building parse trees. In other words, there 
is no task of selecting the best segmentation per se; 
the best word-breaking analysis is merely a 
concomitant of the best syntactic parse. We 
demonstrate hat a robust, broad-coverage parser 
can be implemented irectly on a word lattice 
input and can be used to resolve word-breaking 
ambiguities effectively without adverse 
performance effects. A similar model of 
word-breaking is reported for the problem of 
Chinese word segmentation (Wu and Jiang 1998), 
but the amount of ambiguity that exists in the word 
822 
lattice is nmch larger in Japanese, which requires a
different treatment. In the l'ollowing, we first 
describe the word-breaker and the parser in more 
detail (Section 2); we then report the results of 
segmentation accuracy (Section 3) and the results 
of related experinaents a sessing the effects of the 
segmentation ambiguities in the word lattice to 
parsing (Section 4). In Conclusion, we discuss 
implications for future research. 
2. Using a broad-coverage parser for 
word-breaking 
The word-breaking and syntactic components 
discussed in the current study are implelnented 
within a broad-coverage, multi-purpose natural 
hmguage understanding system being developed at 
Microsoft Research, whose ultimate goal is to 
achieve deep Selnantic understanding of natural 
language I. A detailed escription of the system is 
found in Heidom (in press). Though we focus on 
the word-breaking and syntactic components in 
this paper, the syntactic analysis is by no means 
the final goal of the system; rather, a parse tree is 
considered to be an approxilnate first step toward a 
more useful meaning representation. We also aim 
at being truly broad-coverage, i.e., returning useful 
analyses irrespective of the genre or the subject 
matter of the input text, be it a newspaper articlc or 
a piece of e-mail. For the proposed model of 
word-breaking to work well, the following 
properties of the parser are particularly important. 
? The bottom-up chart parser creates syntactic 
analyses by building incrementally arger phrases 
fl'om individual words and phrases (Jensen et al 
1993). The analyses that span the entire input 
string are the complete analyses, and the words 
used in that analysis constitutes the word-breaking 
analysis for the string. Incorrect words returned by 
the word-breaker are filtered out by the syntactic 
rules, and will not make it into the final complete 
parse. 
? All the grammar rules, written in the 
formalism of Augmented Phrase Structure 
Grammar (lteidorn 1975), are binary, a feature 
crucial for dealing with free word-order and 
i Japanese is one of the seven languages under 
development in our lab, along with Chinese, English, 
French, German, Korean and Spanish. 
missing constituents (Jensen 1987). Not only has 
the rule formalism proven to be indispensable for 
parsing a wide range of English texts, it is all tile 
more critical 1'o1 parsing Japanese, as the free 
word-order and missing constituents are the norm 
for Japanese sentences. 
? There is very little semantic dependency in the 
grammar rules, which is essential if the grammar is
to be domain-independent. However, the grammar 
rules are elaborately conditioned on morphological 
and syntactic l'eatums, enabling much finer-grained 
parsing analyses than just relying on a small 
number of basic parts-of speech (POS). This gives 
the grammar the power to disambiguate multiple 
word analyses in the input lattice. 
13ecause we do not utilize semantic information, 
we perforln no selnantically motivated attachlnent 
of phrases during parsing. Instead, we parse them 
into a default analysis, which can then be expanded 
and disambiguatcd at later stages of processing 
using a large semantic knowledge base 
(Richardson 1997, Richardson et al 1998). One of 
the goals o1' Ihis paper is to show that the syntactic 
information alone can resolve the ambiguities in 
the word lattice sufficiently well to select the best 
breaking analysis in the absence of elaborate 
semantic information. Figure 1 (see Appendix) 
shows the default attachment of the relative clause 
to the closest NP. Though this structure may be 
semantically implausible, the word-breaking 
analysis is correct. 
The word-breaking colnponent of out" system is 
described in detail in Kacmarcik et al (2000). For 
the lmrpose of robust parsing, the component is 
expected to solve the following two problems: 
? Lemmatization: Find possible words in the 
input text using a dictionary and its inflectional 
morphology, and return the dictionary entry forms 
(lemmas). Note that multiple lemmas are often 
possible for a given inflected form (e.g. surface 
form /o,~z -(- (kalte) could be an inflected form of 
the verbs /9~3 (kau "buy"), /0~o (katu "win")or 
/o,~ (karu "trim"), in which case all these forms 
must be returned. The dictionary the word-breaker 
uses has about 70,000 unique entries. 
? Orthography norlnalization: Identify and 
norlnalize orthographic variants. This is a 
non-trivial task in Japanese, as words can be 
spelled using any colnbination of the tout" chanmter 
823 
4.3 Parser precision 
An initial concern in implementing the present 
model was that parsing ambiguous input might 
proliferate syntactic analyses. In theory, the 
number of analyses might grow exponentially as 
the input sentence length increased, making the 
reliable ranking of parse results unmanageable. In 
practice, however, pathological proliferation of 
syntactic analyses is not a problem s. Figure 4 
tallies the average number of parses obtained in 
relation to sentence l ngth for all successful parses 
in the 5,000-sentence t st corpus (corpus A in 
Table 1). There were 4,121 successful parses in the 
corpus, corresponding to 82.42% coverage. From 
Figure 4, we can see that the number of parses 
does increase as the sentence grows longer, but the 
increment is linear and the slope is very moderate. 
Even in the highest-scoring range, the mean 
number of parses is only 2.17. Averaged over all 
sentence lengths, about 68% of the successfully 
parsed sentences receive only one parse, and 22% 
receive two parses. Only about 10% of sentences 
receive more than 2 analyses. From these results 
we conclude that the overgeneration f parse trees 
is not a practical concern within our approach. 
3 
"6 2 
E 
==t 
1-10 11- 21- 31- 41- 51- 61- 71- 81- 91- >101 
20 30 40 50 60 70 80 90 100 
sentence length (in char) 
Figure 4. Average number ol'parses for corpus A 
(5,000 sentences) 
4.4 Performance 
A second potential concern was performance: 
would the increased number of records in the chart 
cause unacceptable degradation of system speed? 
5 A similar observation is made by Charniak et al 
(forthcoming), who find that the number ot' final parses 
caused by additional POS tags is far less than the 
theoretical worst case in reality. 
This concern also proved unfounded in practice. In 
another experiment, we evaluated the processing 
speed of the system by measuring the time it takes 
per character in the input sentence (in 
milliseconds) relative to the sentence length. The 
results are given in Figure 5. This figure shows 
that the processing time per-character grows 
moderately as the sentence grows longer, due to 
the increased number of intermediate analyses 
created during the parsing. But the increase is 
linear, and we interpret these results as indicating 
that our approach is fully viable and realistic in 
terms of processing speed, and robust against input 
sentence length. The current average parsing time 
for our 15,000-sentence corpus (with average 
sentence length of 49.02 characters) is 23.09 
sentences per second on a Dell 550MHz Pentium 
III machine with 512MB of RAM. 
1.5 
1.4 
1.3 
1.2 
1.1 
1 
0.9 
0.8 
0.7 
15 25 35 45 55 65 75 85 95 105 115 125 135 
sentence length (in char) 
Figure 5. Processing speed on a 15,000-sentence corpus 
5. Conclusion 
We have shown that a practical, broad-coverage 
parser can be implemented without requiring the 
word-breaking component to return a single 
segmentation a alysis, and that it can at the same 
time achieve high accuracy in POS-labeled 
word-breaking. Separating the tasks of word 
ident~/'l'cation a d best sequence selection offers 
flexibility in enhancing both recall and precision 
without sacrificing either at the cost of the other. 
Our results show that morphological nd syntactic 
information alone can resolve most word-breaking 
ambiguities. Nonetheless, some ambiguities 
require semantic and contextual information. For 
example, the following sentence allows two parses 
corresponding to two word-breaking analyses, of 
which the first is semantically preferred: 
826 
(1) ocha-ni haitte-irtt arukaroido 
tea-in contain-ASP alkaloid 
"the alkaloid contained in lea" 
(2) ocha-ni-ha itte-iru arukatwido 
tea-in-TOP go-ASP alkaloid 
? ? the alkaloid that has gone to the tea" 
Likewise, the sentence below allows two different 
interpretations of the morpheme de, either as a 
locative marker (1) or as a copula (2). Both 
interpretations are syntactically and semantically 
wflid; only contextual information can resolve the 
ambiguity. 
(1) minen-ha isuraeru-de aru 
next year-TOP Israel-LOC be-held 
"It will be held in Israel next year". 
(2) rainen-ha isuraeru de-artt 
next year-TOP Israel be-PP, ES 
"It will be Israel next year". 
In both these sentences, we create syntactic trees 
for all syntactically valid interpretations, leaving 
the ambiguity intact. Such ambiguities can only be 
resolved with semantic and contextual information 
eventually made available by higher processing 
components. This will be Ihe focus of our ongoing 
rese.arclt. 
Acknowledgements 
We: would like to thank Mari Bmnson and Kazuko 
Robertshaw for annotating corpora for target 
word-breaking and POS tagging. We also thank 
the anonymous reviewers and the members of the 
MSR NLP group for their comments on the earlier 
version of the paper. 
References  
Charniak, Eugene, Glenn Carroll, John Adcock, Antony 
Cassandra, Yoshihiko Gotoh, Jeremy Katz, Michael 
Littmaa, and John McCann. Forthcoming. Taggers l'or 
parsers. To appear in Artificial Intelligence. 
Fuchi, Takeshi and Shinichiro Takagi. 1998. Japanese 
Morphological Analyzer Using Word Co-Occurrence 
-JTAG-. Proceeding of ACL-COLING: 409-413. 
Gamon, Michael, Carmen Lozano, Jessie Pinkham and 
Tom Reutter. 1997. Practical Experience with 
Grammar Sharing in Multilingual NLP. Jill Burstcin 
and Chmdia Leacock (eds.), From Research to 
Commercial Applications: Making NLP Work in 
Practice (Proceedings of a Workshop Sponsored by 
the Association for Computational Linguistics). 
pp.49-56. 
Heidorn, George. 1975. Attgmented Phrase Structure 
Grammars. In B.L. Webber and R.C. Schank (eds.), 
Theoretical Issues in Natural Language Processing. 
ACL 1975: 1-5. 
Heidorn, George. In press. Intelligent Writing 
Assistance. To appear in Robert Dale, Hermann Moisl 
and Harold Seiners (eds.), lfandbook of Natural 
Language Processing. Chapter 8. 
Hisamitsu, T. and Y. Nitta. 1990. Morphological 
Analysis by Minimum Connective-Cost Method. 
Technical Report, SIGNLC 90-8. IEICE pp.17-24 (in 
Japanese). 
Jensen, Karen. 1987. Binary Rules and Non-Binary 
Trees: Breaking Down the Concept o1' Phrase 
Structure. In Alexis Manasler-Ramer (ed.), 
Mathematics of Language. Amsterdam: John 
Benjamins Publishing. pp.65-86. 
Jensen, Karen, George E. Hektorn and Stephen D. 
Richardson (eds.). 1993. Natural Language 
Processing: The PLNLP approach. Kluwer: Boston. 
Kacmareik, Gary, Chris Brockett and Hisami St, zuki. 
2000. Robust Segmentation of Japanese Text into a 
l,attice for Parsing. Proceedings of COLING 2000. 
Kurohashi, Sadao and Makoto Nagao. 1998. Building a 
Japanese Parsed Corpus While hnproving the Parsing 
System. First LREC Proceedings: 719-724. 
Murakami, J. and S. Sagayama. 1992. Hidden Markov 
Model Applied to Morphological Analysis. lPSJ 3: 
161 - 162 (in Japanese). 
Nagata, Masaaki. 1994. A Stochastic Japanese 
Morphological Analyzer Using a Forward-DP 
Backward-A* N-Best Search Algorithm. Proceedings 
o1' COLING '94:201-207. 
Richardson, Stephen D. 1997. Determining Similarity 
and Inferring Relatkms in a Lexical Knowledge Base. 
Ph.D. dissertation. The City University of New York. 
Richardson, Stephen D., William B. Dolan and Lucy 
Vanderwende. 1998. MindNet: acquiring and 
structuring semantic information f l 'om text. 
Proceedings of COLING-ACL OS: 1098-1102. 
Taket,chi, Koichi and Yuji Matsumoto. 1997. HMM 
Parameter Learning for Japanese Morphological 
Analyzer. IPSJ: 38-3 (in Japanese). 
Wu, Andi and Zixin Jiang. 1998. Word Segmentation in 
Sentence Analysis. Technical Report, MSR-TR-99-10. 
Microsoft Reseamh. 
827 
Appendix 
"(It) has been annoyed by the successive interventions by the neighboring country". 
~b~O<" VERB1 
~t~ NOUN1 
~t~ VERB2 
:::::::: O< ~ VERB3 
::: : : : : : : : : : : : : :  ~ NOUN2 
: : : : : : : : : : : : : : : : : : : : : : : :  ~ NOUN3 
: : : : : : : : : : : : : : : : : : : : : : : :  ~ PRONI 
: : : : : : : : : : : : : : : : : : : : : : : :  ~ POSPI 
: : : : : : : : : : : : : : : : : : : : : : : : : : : :  ~ NOUN4 
: : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : :  \[C~ NOUN5 
: : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : :  \[: VERB4 
: : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : :  \[C POSP2 
: : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : :  ~ VERB5 
: : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : :  ~NOUN6 
: : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : :  ~ VERB~ 
: : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : :  ~ IJl 
: : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : :  ~ POSP3 
: : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : :  ~ VERB7 
: : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : :  ~ Ia2 
: : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : :  ~ POSP4 
: : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : :  ~NOUN7 
: : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : :  ~ VERB8 
: : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : :  ~ CONJI 
: : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : :  b%~NOUN8 
: : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : :  L%~ VERB9 
DECL I  NP I  NP2  RELCL I  VERB1*  "~%O?"  
NOUN2*  "~"  
PP I  POSP I*  "~"  
NOUN4*  "=~\]~" 
PP2 POSP2*  "\[C" 
VERBS* "~&~"  
AUXPI  VERB9*  " ~ "  
CHAR1 "o" 
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  
(successive) 
(neighboring country) 
(GEN) 
(intervention) 
(by) 
(annoyed) 
(be) 
Figure 1. Example of ambiguous attachment. RELCL I  can syntactically modify either NOUN2 or NOUNa. 
NOUN4 (non-local attachment) is the semantically correct choice. Shown above the parse tree is the 
input word lattice returned from the word-breaker. 
94~-~l~:{~,,~,W~\[7_-tk~bvEl,~Teo "Classical Thai literature is based on tradition and history". 
F ITTED1 NP I  
NP2  
NOUN1*  "9-1" i~  ~ "  (classical Thai literature) 
PPI POSP i* " \ [~"  (TOP IC)  
UP3 NOUn2 * "{~"  (tradition) 
posp2 * ,, k" ,, (and) 
NP4 NOUN3 * "~"  (history) 
PP2 POSP3 * "17-" (on) 
VPI  VERB1*  ":6~'-5t VC"  (based) 
AUXPI  VERB2 * ,, I, xT~ ,, (be) 
CHAR1 " o " 
Figure 2. Example of an incomplete parse with correct word-breaking results. 
828 
Unsupervised Construction of Large Paraphrase Corpora: 
Exploiting Massively Parallel News Sources 
Bill DOLAN, Chris QUIRK, and Chris BROCKETT 
Natural Language Processing Group, Microsoft Research  
One Microsoft Way 
Redmond, WA 90852, USA 
{billdol,chrisq,chrisbkt}@microsoft.com 
 
Abstract 
We investigate unsupervised techniques for 
acquiring monolingual sentence-level 
paraphrases from a corpus of temporally and 
topically clustered news articles collected from 
thousands of web-based news sources. Two 
techniques are employed: (1) simple string edit 
distance, and (2) a heuristic strategy that pairs 
initial (presumably summary) sentences from 
different news stories in the same cluster. We 
evaluate both datasets using a word alignment 
algorithm and a metric borrowed from machine 
translation. Results show that edit distance data 
is cleaner and more easily-aligned than the 
heuristic data, with an overall alignment error 
rate (AER) of 11.58% on a similarly-extracted 
test set.  On test data extracted by the heuristic 
strategy, however, performance of the two 
training sets is similar, with AERs of 13.2% 
and 14.7% respectively. Analysis of 100 pairs 
of sentences from each set reveals that the edit 
distance data lacks many of the complex lexical 
and syntactic alternations that characterize 
monolingual paraphrase. The summary 
sentences, while less readily alignable, retain 
more of the non-trivial alternations that are of 
greatest interest learning paraphrase 
relationships.    
 
1 Introduction 
The importance of learning to manipulate 
monolingual paraphrase relationships for 
applications like summarization, search, and dialog 
has been highlighted by a number of recent efforts 
(Barzilay & McKeown 2001; Shinyama et al 
2002; Lee & Barzilay 2003; Lin & Pantel 2001). 
While several different learning methods have 
been applied to this problem, all share a need for 
large amounts of data in the form of pairs or sets of 
strings that are likely to exhibit lexical and/or 
structural paraphrase alternations. One approach1 
                                                    
1
 An alternative approach involves identifying anchor 
points--pairs of words linked in a known way--and 
collecting the strings that intervene. (Shinyama, et al 
2002; Lin & Pantel 2001). Since our interest is in 
that has been successfully used is edit distance, a 
measure of similarity between strings. The 
assumption is that strings separated by a small edit 
distance will tend to be similar in meaning: 
   
The leading indicators measure the economy? 
The leading index measures the economy?. 
 
Lee & Barzilay (2003), for example, use Multi-
Sequence Alignment (MSA) to build a corpus of 
paraphrases involving terrorist acts.  Their goal is 
to extract sentential templates that can be used in 
high-precision generation of paraphrase alter-
nations within a limited domain.  
 Our goal here is rather different: our interest lies 
in constructing a monolingual broad-domain 
corpus of pairwise aligned sentences. Such data 
would be amenable to conventional statistical 
machine translation (SMT) techniques (e.g., those 
discussed in Och & Ney 2003).2 In what follows 
we compare two strategies for unsupervised 
construction of such a corpus, one employing 
string similarity and the other associating sentences 
that may overlap very little at the string level. We 
measure the relative utility of the two derived 
monolingual corpora in the context of word 
alignment techniques developed originally for 
bilingual text.   
We show that although the edit distance corpus is 
well-suited as training data for the alignment 
algorithms currently used in SMT, it is an 
incomplete source of information about paraphrase 
relations, which exhibit many of the characteristics 
of comparable bilingual corpora or free 
translations. Many of the more complex 
alternations that characterize monolingual 
paraphrase, such as large-scale lexical alternations 
and constituent reorderings, are not readily 
                                                                                 
learning sentence level paraphrases, including major 
constituent reorganizations, we do not address this 
approach here.  
2
 Barzilay & McKeown (2001) consider the 
possibility of using SMT machinery, but reject the 
idea because of the noisy, comparable nature of their 
dataset. 
captured by edit distance techniques, which 
conflate semantic similarity with formal similarity.  
We conclude that paraphrase research would 
benefit by identifying richer data sources and 
developing appropriate learning techniques.  
2 Data/Methodology 
Our two paraphrase datasets are distilled from a 
corpus of news articles gathered from thousands of 
news sources over an extended period. While the 
idea of exploiting multiple news reports for 
paraphrase acquisition is not new, previous efforts 
(for example, Shinyama et al 2002; Barzilay and 
Lee 2003) have been restricted to at most two news 
sources. Our work represents what we believe to 
be the first attempt to exploit the explosion of news 
coverage on the Web, where a single event can 
generate scores or hundreds of different articles 
within a brief period of time. Some of these articles 
represent minor rewrites of an original AP or 
Reuters story, while others represent truly distinct 
descriptions of the same basic facts.  The massive 
redundancy of information conveyed with widely 
varying surface strings is a resource begging to be 
exploited. 
Figure 1 shows the flow of our data collection 
process. We begin with sets of pre-clustered URLs 
which point to news articles on the Web, 
representing thousands of different news sources. 
The clustering algorithm takes into account the full 
text of each news article, in addition to temporal 
cues, to produce a set of topically and temporally 
related articles. Our method is believed to be 
independent of the specific clustering technology 
used. The story text is isolated from a sea of 
advertisements and other miscellaneous text 
through use of a supervised HMM.  
Altogether we collected 11,162 clusters in an 8-
month period, assembling 177,095 articles with an 
average of 15.8 articles per cluster.  The clusters 
are generally coherent in topic and focus. Discrete 
events like disasters, business announcements, and 
deaths tend to yield tightly focused clusters, while 
ongoing stories like the SARS crisis tend to 
produce less focused clusters. While exact 
duplicate articles are filtered out of the clusters, 
many slightly-rewritten variants remain. 
 
2.1 Extracting Sentential Paraphrases 
Two separate techniques were employed to 
extract likely pairs of sentential paraphrases from 
these clusters. The first used string edit distance, 
counting the number of lexical deletions and 
insertions needed to transform one string into 
another. The second relied on a discourse-based 
heuristic, specific to the news genre, to identify 
likely paraphrase pairs even when they have little 
superficial similarity. 
 
3 Levenshtein Distance 
A simple edit distance metric (Levenshtein 
1966) was used to identify pairs of sentences 
within a cluster that are similar at the string level.  
First, each sentence was normalized to lower case 
and paired with every other sentence in the cluster. 
Pairings that were identical or differing only by 
punctuation were rejected, as were those where the 
shorter sentence in the pair was less than two thirds 
the length of the longer, this latter constraint in 
effect placing an upper bound on edit distance 
relative to the length of the sentence. Pairs that had 
been seen before in either order were also rejected. 
Filtered in this way, our dataset yields 139K non-
identical sentence pairs at a Levenshtein distance 
of n ? 12. 3  Mean Levenshtein distance was 5.17, 
and mean sentence length was 18.6 words. We will 
refer to this dataset as L12. 
 
3.1.1 First sentences  
The second extraction technique was 
specifically intended to capture paraphrases which 
might contain very different sets of content words, 
word order, and so on. Such pairs are typically 
used to illustrate the phenomenon of paraphrase, 
but precisely because their surface dissimilarity 
renders automatic discovery difficult, they have 
generally not been the focus of previous 
computational approaches.  
In order to automatically identify sentence pairs 
of this type, we have attempted to take advantage 
of some of the unique characteristics of the dataset. 
The topical clustering is sufficiently precise to 
ensure that, in general, articles in the same cluster 
overlap significantly in overall semantic content. 
Even so, any arbitrary pair of sentences from 
different articles within a cluster is unlikely to 
exhibit a paraphrase relationship: 
 
The Phi-X174 genome is short and compact. 
This is a robust new step that allows us to make much 
larger pieces. 
 
To isolate just those sentence pairs that represent 
likely paraphrases without requiring significant 
string similarity, we exploited a common 
journalistic convention: the first sentence or two of 
                                                    
3A maximum Levenshtein distance of 12 was selected 
for the purposes of this paper on the basis of 
experiments with corpora extracted at various edit 
distances.  
a newspaper article typically summarize its 
content. One might reasonably expect, therefore, 
that initial sentences from one article in a cluster 
will be paraphrases of the initial sentences in other 
articles in that cluster. This heuristic turns out to be 
a powerful one, often correctly associating 
sentences that are very different at the string level: 
 
In only 14 days, US researchers have created an 
artificial bacteria-eating virus from synthetic 
genes. 
An artificial bacteria-eating virus has been made from 
synthetic genes in the record time of just two weeks. 
 
Also consider the following example, in which 
related words are obscured by different parts of 
speech: 
   
Chosun Ilbo, one of South Korea's leading newspapers, 
said North Korea had finished developing a new 
ballistic missile last year and was planning to 
deploy it.  
The Chosun Ilbo said development of the new missile, 
with a range of up to %%number%% kilometres 
(%%number%% miles), had been completed and 
deployment was imminent. 
 
A corpus was produced by extracting the first 
two sentences of each article, then pairing these 
across documents within each cluster. We will 
refer to this collection as the F2 corpus.  The 
combination of the first-two sentences heuristic 
plus topical article clusters allows us to take 
advantage of meta-information implicit in our 
corpus, since clustering exploits lexical 
information from the entire document, not just the 
few sentences that are our focus. The assumption 
that two first sentences are semantically related is 
thus based in part on linguistic information that is 
external to the sentences themselves. 
Sometimes, however, the strategy of pairing 
sentences based on their cluster and position goes 
astray. This would lead us to posit a paraphrase 
relationship where there is none: 
 
Terence Hope should have spent most of yesterday in 
hospital performing brain surgery. 
A leading brain surgeon has been suspended from work 
following a dispute over a bowl of soup. 
 
To prevent too high an incidence of unrelated 
sentences, one string-based heuristic filter was 
found useful: a pair is discarded if the sentences do 
not share at least 3 words of 4+ characters. This 
constraint succeeds in filtering out many unrelated 
pairs, although it can sometimes be too restrictive, 
excluding completely legitimate paraphrases:   
 
There was no chance it would endanger our planet, 
astronomers said. 
NASA emphasized that there was never danger of a 
collision. 
 
An additional filter ensured that the word count 
of the shorter sentence is at least one-half that of 
the longer sentence. Given the relatively long 
sentences in our corpus (average length 18.6 
words), these filters allowed us to maintain a 
degree of semantic relatedness between sentences. 
Accordingly, the dataset encompasses many 
paraphrases that would have been excluded under a 
more stringent edit-distance threshold, for 
example, the following non-paraphrase pair that 
contain an element of paraphrase:  
 
A staggering %%number%% million Americans have 
been victims of identity theft in the last five years , 
according to federal trade commission survey out 
this week.  
In the last year alone, %%number%% million people 
have had their identity purloined. 
 
Nevertheless, even after filtering in these ways,  
a significant amount of unfiltered noise remains in 
the F2 corpus, which consisted of 214K sentence 
pairs. Out of a sample of 448 held-out sentence 
pairs, 118 (26.3%) were rated by two independent 
human evaluators as sentence-level paraphrases, 
while 151 (33.7%) were rated as partial 
paraphrases. The remaining ~40% were assessed as 
 
News article clusters: URLs
Download URLs,
Isolate content (HMM),
Sentence separate
Textual content of articles
Select and filter
first sentence pairs
Approximately parallel
monolingual corpus
 
 
 
Figure 1. Data collection 
 
 
unrelated. 4   Thus, although the F2 data set is 
nominally larger than the L12 data set, when the 
noise factor is taken into account, the actual 
number of full paraphrase sentences in this data set 
is estimated to be in the region of 56K sentences, 
with a further estimated 72K sentences containing 
some paraphrase material that might be a potential 
source of alignment.  
Some of these relations captured in this data can 
be complex. The following pair, for example, 
would be unlikely to pass muster on edit distance 
grounds, but nonetheless contains an inversion of 
deep semantic roles, employing different lexical 
items.    
 
The Hartford Courant reported %%day%% that Tony 
Bryant said two friends were the killers.  
A lawyer for Skakel says there is a claim that the 
murder was carried out by two friends of one of 
Skakel's school classmates, Tony Bryan. 
 
The F2 data also retains pairs like the following 
that involve both high-level semantic alternations 
and long distance dependencies:  
 
Two men who robbed a jeweller's shop to raise funds 
for the Bali bombings were each jailed for 
%%number%% years by Indonesian courts today.  
An Indonesian court today sentenced two men to 
%%number%% years in prison for helping 
finance last year's terrorist bombings in Bali by 
robbing a jewelry store. 
 
These examples do not by any means exhaust 
the inventory of complex paraphrase types that are 
commonly encountered in the F2 data. We 
encounter, among other things, polarity 
alternations, including those involving long-
distance dependencies, and a variety of distributed 
paraphrases, with alignments spanning widely 
separated elements. 
 
3.2 Word Error Alignment Rate 
An objective scoring function was needed to 
compare the relative success of the two data 
collection strategies sketched in 2.1.1 and 2.1.2. 
Which technique produces more data? Are the 
types of data significantly different in character or 
utility? In order to address such questions, we used 
word Alignment Error Rate (AER), a metric 
borrowed from the field of statistical machine 
translation (Och & Ney 2003). AER measures how 
accurately an automatic algorithm can align words 
in corpus of parallel sentence pairs, with a human-
                                                    
4
  This contrasts with 16.7% pairs assessed as 
unrelated in a 10,000 pair sampling of the L12 data.   
tagged corpus of alignments serving as the gold 
standard. Paraphrase data is of course monolingual, 
but otherwise the task is very similar to the MT 
alignment problem, posing the same issues with 
one-to-many, many-to-many, and one/many-to-
null word mappings. Our a priori assumption was 
that the lower the AER for a corpus, the more 
likely it would be to yield learnable information 
about paraphrase alternations.  
We closely followed the evaluation standards 
established in Melamed (2001) and Och & Ney 
(2000, 2003). Following Och & Ney?s 
methodology, two annotators each created an 
initial annotation for each dataset, subcategorizing 
alignments as either SURE (necessary) or POSSIBLE 
(allowed, but not required). Differences were then 
highlighted and the annotators were asked to 
review these cases.  Finally we combined the two 
annotations into a single gold standard in the 
following manner: if both annotators agreed that an 
alignment should be SURE, then the alignment was 
marked as sure in the gold-standard; otherwise the 
alignment was marked as POSSIBLE. 
To compute Precision, Recall, and Alignment 
Error Rate (AER) for the twin datasets, we used 
exactly the formulae listed in Och & Ney (2003).  
Let A be the set of alignments in the comparison, S 
be the set of SURE alignments in the gold standard, 
and P be the union of the SURE and POSSIBLE 
alignments in the gold standard.  Then we have:  
 
||
||precision
A
PA ?
=   
 
||
||
  recall
S
SA ?
=
 
 
||
||AER
SA
SAPA
+
?+?
=  
 
 
We held out a set of news clusters from our 
training data and randomly extracted two sets of 
sentence pairs for blind evaluation. The first is a 
set of 250 sentence pairs extracted on the basis of 
an edit distance of 5 ? n ? 20, arbitrarily chosen to 
allow a range of reasonably divergent candidate 
pairs. These sentence pairs were checked by an 
independent human evaluator to ensure that they 
contained paraphrases before they were tagged for 
alignments. The second set comprised 116 
sentence pairs randomly selected from the set of 
first-two sentence pairs. These were likewise hand-
vetted by independent human evaluators. After an 
initial training pass and refinement of the linking 
specification, interrater agreement measured in 
terms of AER5 was 93.1% for the edit distance test 
set versus 83.7% for the F2 test set, suggestive of 
the greater variability in the latter data set.  
3.3 Data Alignment  
Each corpus was used as input to the word 
alignment algorithms available in Giza++ (Och & 
Ney 2000).  Giza++ is a freely available 
implementation of IBM Models 1-5 (Brown et al 
1993) and the HMM alignment (Vogel et al 1996), 
along with various improvements and 
modifications motivated by experimentation by 
Och & Ney (2000).  Giza++ accepts as input a 
corpus of sentence pairs and produces as output a 
Viterbi alignment of that corpus as well as the 
parameters for the model that produced those 
alignments.  
While these models have proven effective at the 
word alignment task (Mihalcea & Pedersen 2003), 
there are significant practical limitations in their 
output. Most fundamentally, all alignments have 
either zero or one connection to each target word. 
Hence they are unable to produce the many-to-
many alignments required to identify 
correspondences with idioms and other phrasal 
chunks. 
To mitigate this limitation on final mappings, 
we follow the approach of Och (2000): we align 
once in the forward direction and again in the 
backward direction.  These alignments can 
subsequently be recombined in a variety of ways, 
                                                    
5
 The formula for AER given here and in Och & Ney 
(2003) is intended to compare an automatic alignment 
against a gold standard alignment. However, when 
comparing one human against another, both comparison 
and reference distinguish between SURE and POSSIBLE 
links. Because the AER is asymmetric (though each 
direction differs by less than 5%), we have presented the 
average of the directional AERs. 
such as union to maximize recall or intersection to 
maximize precision. Och also documents a method 
for heuristically recombining the unidirectional 
alignments intended to balance precision and 
recall. In our experience, many alignment errors 
are present in one side but not the other, hence this 
recombination also serves to filter noise from the 
process. 
4 Evaluation 
Table 1 shows the results of training translation 
models on data extracted by both methods and then 
tested on the blind data. The best overall 
performance, irrespective of test data type, is 
achieved by the L12 training set, with an 11.58% 
overall AER on the 250 sentence pair edit distance 
test set (20.88% AER for non-identical words). 
The F2 training data is probably too sparse and, 
with 40% unrelated sentence pairs, too noisy to 
achieve equally good results; nevertheless the gap 
between the results for the two training data types 
is dramatically narrower on the F2 test data. The 
nearly comparable numbers for the two training 
data sets, at 13.2% and 14.7% respectively, suggest 
that the L12 training corpus provides no 
substantive advantage over the F2 data when tested 
on the more complex test data. This is particularly 
striking given the noise inherent in the F2 training 
data. 
5 Analysis/Discussion 
To explore some of the differences between the 
training sets, we hand-examined a random sample 
of sentence pairs from each corpus type. The most 
common paraphrase alternations that we observed 
fell into the following broad categories: 
 
? Elaboration: Sentence pairs can differ in total 
information content, with an added word, 
phrase or clause in one sentence that has no 
Training Data Type: L12 F2 L12 F2 
Test Data Type: 250 Edit Dist 250 Edit Dist 116 F2 Heuristic 116 F2 Heuristic 
Precision   87.46% 86.44% 85.07% 84.16% 
Recall      89.52% 82.64% 88.70% 86.55% 
AER         11.58% 15.41% 13.24% 14.71% 
Identical word precision   89.36% 88.79% 92.92% 93.41% 
Identical word recall      89.50% 83.10% 93.49% 92.47% 
Identical word AER         10.57% 14.14% 6.80% 7.06% 
Non-Identical word precision   76.99% 71.86% 60.54% 53.69% 
Non-Identical word recall      90.22% 69.57% 59.50% 50.41% 
Non-Identical word AER         20.88% 28.57% 39.81% 47.46% 
 
 
Table 1.  Precision, recall, and alignment error rates (AER) for F2 and L12 
 
counterpart in the other (e.g. the NASDAQ /  
the tech-heavy NASDAQ). 
? Phrasal: An entire group of words in one 
sentence alternates with one word or a phrase 
in the other.  Some are non-compositional 
idioms (has pulled the plug on / is dropping 
plans for); others involve different phrasing 
(electronically / in electronic form, more than 
a million people / a massive crowd). 
? Spelling: British/American sources system-
atically differ in spellings of common words 
(colour / color); other variants also appear 
(email / e-mail). 
? Synonymy:  Sentence pairs differ only in one 
or two words (e.g. charges / accusations), 
suggesting an editor?s hand in modifying a 
single source sentence. 
? Anaphora: A full NP in one sentence  
corresponds to an anaphor in the other (Prime 
Minister Blair / He). Cases of NP anaphora 
(ISS / the Atlanta-based security company) are 
also common in the data, but in quantifying 
paraphrase types we restricted our attention to 
the simpler case of pronominal anaphora.  
? Reordering: Words, phrases, or entire 
constituents occur in different order in two 
related sentences, either because of major 
syntactic differences (e.g. topicalization, voice 
alternations) or more local pragmatic choices 
(e.g. adverb or prepositional phrase placement).  
 
These categories do not cover all possible 
alternations between pairs of paraphrased 
sentences; moreover, categories often overlap in 
the same sequence of words. It is common, for 
example, to find instances of clausal Reordering 
combined with Synonymy. 
Figure 2 shows a hand-aligned paraphrase pair 
taken from the F2 data. This pair displays one 
Spelling alternation (defence / defense), one 
Reordering (position of the ?since? phrase), and 
one example of Elaboration (terror attacks occurs 
in only one sentence).    
To quantify the differences between L12 and F2, 
we randomly chose 100 sentence pairs from each 
dataset and counted the number of times each 
phenomenon was encountered. A given sentence 
pair might exhibit multiple instances of a single 
phenomenon, such as two phrasal paraphrase 
changes or two synonym replacements.  In this 
case all instances were counted. Lower-frequency 
changes that fell outside of the above categories 
were not tallied: for example, the presence or 
absence of a definite article (had authority / had 
the authority) in Figure 2 was ignored.  After 
summing all alternations in each sentence pair, we 
calculated the average number of occurrences of 
each paraphrase type in each data set.  The results 
are shown in Table 2. 
Several major differences stand out between the 
two data sets.  First, the F2 data is less parallel, as 
evidenced by the higher percentage of Elaborations 
found in those sentence pairs. Loss of parallelism, 
however, is offset by greater diversity of 
paraphrase types encountered in the F2 data. 
Phrasal alternations are more than 4x more 
common, and Reorderings occur over 20x more 
frequently.   Thus while string difference methods 
may produce relatively clean training data, this is 
achieved at the cost of filtering out common (and 
interesting) paraphrase relationships. 
 
6 Conclusions and Future Work 
Edit distance identifies sentence pairs that 
exhibit lexical and short phrasal alternations that 
can be aligned with considerable success. Given a 
large dataset and a well-motivated clustering of 
documents, useful datasets can be gleaned even 
without resorting to more sophisticated techniques 
 
 
Figure 2.   Sample human-aligned paraphrase 
 
 L12 F2 
Elaboration 0.83 1.3 
Phrasal 0.14 0.69 
Spelling 0.12 0.01 
Synonym 0.18 0.25 
Anaphora 0.1 0.13 
Reordering 0.02 0.41 
 
 
Table 2.  Mean number of instances of 
paraphrase phenomena per sentence 
 
(such as Multiple Sequence Alignment, as 
employed by Barzilay & Lee 2003).  
However, there is a disparity between the kinds 
of paraphrase alternations that we need to be able 
to align and those that we can already align well 
using current SMT techniques. Based solely on the 
criterion of word AER, the L12 data would seem to 
be superior to the F2 data as a source of paraphrase 
knowledge.  Hand evaluation, though, indicates 
that many of the phenomena that we are interested 
in learning may be absent from this L12 data. 
String edit distance extraction techniques involve 
assumptions about the data that are inadequate, but 
achieve high precision.  Techniques like our F2 
extraction strategies appear to extract a more 
diverse variety of data, but yield more noise.  We 
believe that an approach with the strengths of both 
methods would lead to significant improvement in 
paraphrase identification and generation.   
In the near term, however, the relatively similar 
performances of F2 and L12-trained models on the 
F2 test data suggest that with further refinements, 
this more complex type of data can achieve good 
results. More data will surely help. 
One focus of future work is to build a classifier 
to predict whether two sentences are related 
through paraphrase. Features might include edit 
distance, temporal/topical clustering information, 
information about cross-document discourse 
structure, relative sentence length, and synonymy 
information. We believe that this work has 
potential impact on the fields of summarization, 
information retrieval, and question answering.   
Our ultimate goal is to apply current SMT 
techniques to the problems of paraphrase 
recognition and generation. We feel that this is a 
natural extension of the body of recent 
developments in SMT; perhaps explorations in 
monolingual data may have a reciprocal impact. 
The field of SMT, long focused on closely aligned 
data, is only now beginning to address the   kinds 
of problems immediately encountered in 
monolingual paraphrase (including phrasal 
translations and large scale reorderings).  
Algorithms to address these phenomena will be 
equally applicable to both fields. Of course a 
broad-domain SMT-influenced paraphrase solution 
will require very large corpora of sentential 
paraphrases. In this paper we have described just 
one example of a class of data extraction 
techniques that we hope will scale to this task. 
Acknowledgements 
We are grateful to the Mo Corston-Oliver, Jeff 
Stevenson and Amy Muia of the Butler Hill Group 
for their work in annotating the data used in the 
experiments. We have also benefited from 
discussions with Ken Church, Mark Johnson, 
Daniel Marcu and Franz Och. We remain, 
however, responsible for all content.  
References  
R. Barzilay and K. R. McKeown. 2001. Extracting 
Paraphrases from a parallel corpus. In Proceedings of 
the ACL/EACL. 
R. Barzilay and  L. Lee. 2003. Learning to Paraphrase: 
an unsupervised approach using multiple-sequence 
alignment. In Proceedings of HLT/NAACL. 
P. Brown, S. A. Della Pietra, V.J. Della Pietra and R. L. 
Mercer. 1993. The Mathematics of Statistical 
Machine Translation. Computational Linguistics, 
19(2): 263-311. 
V. Levenshtein. 1966. Binary codes capable of 
correcting deletions, insertions, and reversals. Soviet 
Physice-Doklady, 10:707-710. 
D. Lin and P. Pantel. 2001. DIRT - Discovery of 
Inference Rules from Text. In Proceedings of ACM 
SIGKDD Conference on Knowledge Discovery and 
Data Mining. 
I. D. Melamed. 2001. Empirical Methods for Exploiting 
Parallel Texts.  MIT Press.  
R. Mihalcea and T. Pedersen. 2003 An Evaluation 
Exercise for Word Alignment. In Proceedings of the 
Workshop on Building and Using Parallel Texts: 
Data Driven Machine Translation and Beyond. May 
31, 2003. Edmonton, Canada. 
F. Och and H. Ney. 2000. Improved Statistical 
Alignment Models.  In Proceedings of the 38th 
Annual Meeting of the ACL, Hong Kong, China. 
F. Och and H. Ney. 2003. A Systematic Comparison of 
Various Statistical Alignment Models.  
Computational Linguistics, 29(1):19-52. 
Y. Shinyama, S. Sekine and K. Sudo. 2002. Automatic 
Paraphrase Acquisition from News Articles.  In 
Proceedings of NAACL-HLT. 
S. Vogel, H. Ney and C. Tillmann. 1996. HMM-Based 
Word Alignment in Statistical Translation. In 
Proceedings of the Annual Meeting of the ACL, 
Copenhagen, Denmark.  
 
 
 
Support Vector Machines for Paraphrase Identification  
and Corpus Construction 
Chris Brockett and William B. Dolan 
Natural Language Processing Group 
Microsoft Research 
One Microsoft Way, Redmond, WA 98502, U.S.A. 
{chrisbkt, billdol}@microsoft.com 
Abstract 
The lack of readily-available large cor-
pora of aligned monolingual sentence 
pairs is a major obstacle to the devel-
opment of Statistical Machine Transla-
tion-based paraphrase models. In this 
paper, we describe the use of annotated 
datasets and Support Vector Machines 
to induce larger monolingual para-
phrase corpora from a comparable cor-
pus of news clusters found on the 
World Wide Web.  Features include: 
morphological variants; WordNet 
synonyms and hypernyms; log-
likelihood-based word pairings dy-
namically obtained from baseline sen-
tence alignments; and formal string 
features such as word-based edit dis-
tance. Use of this technique dramati-
cally reduces the Alignment Error Rate 
of the extracted corpora over heuristic 
methods based on position of the sen-
tences in the text.  
1 Introduction 
Paraphrase detection?the ability to determine 
whether or not two formally distinct strings are 
similar in meaning?is increasingly recognized 
as crucial to future applications in multiple 
fields including Information Retrieval, Question 
Answering, and Summarization. A growing 
body of recent research has focused on the prob-
lems of identifying and generating paraphrases, 
e.g., Barzilay & McKeown (2001), Lin & Pantel 
(2002), Shinyama et al (2002), Barzilay & Lee 
(2003), and Pang et al (2003). One promising 
approach extends standard Statistical Machine 
Translation (SMT) techniques (e.g., Brown et al, 
1993; Och & Ney, 2000, 2003) to the problems 
of monolingual paraphrase identification and 
generation. Finch et al (2004) have described 
several MT based paraphrase systems within the 
context of improving machine translation output. 
Quirk et al (2004) describe an end-to-end para-
phrase identification and generation system us-
ing GIZA++ (Och & Ney, 2003) and a 
monotone decoder to generate information-
preserving paraphrases.  
As with conventional SMT systems, SMT-
based paraphrase systems require extensive 
monolingual parallel training corpora. However, 
while translation is a common human activity, 
resulting in large corpora of human-translated 
bilingual sentence pairs being relatively easy to 
obtain across multiple domains and language 
pairs, this is not the case in monolingual para-
phrase, where naturally-occurring parallel data 
are hard to come by. The paucity of readily 
available monolingual parallel training corpora 
poses a formidable obstacle to the development 
of SMT-based paraphrase systems.   
The present paper describes the extraction of 
parallel corpora from clustered news articles 
using annotated seed corpora and an SVM clas-
sifier, demonstrating that large parallel corpora 
can be induced by a classifier that includes mor-
phological and synonymy features derived from 
both static and dynamic resources.  
2 Background 
Two broad approaches have dominated the lit-
erature on constructing paraphrase corpora. One 
1
approach utilizes multiple translations of a sin-
gle source language text, where the source lan-
guage text guarantees semantic equivalence in 
the target language texts (e.g., Barzilay & 
McKeown, 2001; Pang et al, 2003). Such cor-
pora are of limited availability, however, since 
multiple translations of the same document are 
uncommon in non-literary domains.  
The second strain of corpora construction in-
volves mining paraphrase strings or sentences 
from news articles, with document clustering 
typically providing the topical coherence neces-
sary to boost the likelihood that any two arbi-
trary sentences in the cluster are paraphrases. In 
this vein, Shinyama et al (2002) use named en-
tity anchors to extract paraphrases within a nar-
row domain. Barzilay & Lee (2003) employ 
Multiple Sequence Alignment (MSA, e.g., 
Durbin et al, 1998) to align strings extracted 
from closely related news articles. Although the 
MSA approach can produce dramatic results, it 
is chiefly effective in extracting highly templatic 
data, and appears to be of limited extensibility to 
broad domain application (Quirk et al 2004).  
Recent work by Dolan, et al (2004) describes 
the construction of broad-domain corpora of 
aligned paraphrase pairs extracted from news-
cluster data on the World Wide Web using two 
heuristic strategies: 1) pairing sentences based 
on a word-based edit distance heuristic; and 2) a 
naive text-feature-based heuristic in which the 
first two sentences of each article in a cluster are 
cross-matched with each other, their assumption 
being that the early sentences of a news article 
will tend to summarize the whole article and are 
thus likely to contain the same information as 
other early sentences of other articles in the 
cluster. The word-based edit distance heuristic 
yields pairs that are relatively clean but offer 
relatively minor rewrites in generation, espe-
cially when compared to the MSA model of 
(Barzilay & Lee, 2003). The text-based heuristic, 
on the other hand, results in a noisy ?compara-
ble? corpus: only 29.7% of sentence pairs are 
paraphrases, resulting in degraded performance 
on alignment metrics. This latter technique, 
however, does afford large numbers of pairings 
that are widely divergent at the string level; cap-
turing these is of primary interest to paraphrase 
research. In this paper, we use an annotated cor-
pus and an SVM classifier to refine the output of 
this second heuristic in an attempt to better iden-
tify sentence pairs containing richer paraphrase 
material, and minimize the noise generated by 
unwanted and irrelevant data. 
3 Constructing a Classifier 
3.1 Sequential Minimal Optimization 
Although any of a number of machine learning 
algorithms, including Decision Trees, might be 
equally applicable here, Support Vector Ma-
chines (Vapnik, 1995) have been extensively 
used in text classification  problems and with 
considerable success (Dumais 1998; Dumais et 
al., 1998; Joachims 2002). In particular, SVMs 
are known to be robust in the face of noisy train-
ing data. Since they permit solutions in high di-
mensional space, SVMs lend themselves readily 
to bulk inclusion of lexical features such as 
morphological and synonymy information. 
For our SVM, we employed an off-the-shelf 
implementation of the Sequential Minimal Op-
timization (SMO) algorithm described in Platt 
(1999). 1  SMO offers the benefit of relatively 
short training times over very large feature sets, 
and in particular, appears well suited to handling 
the sparse features encountered in natural lan-
guage classification tasks. SMO has been de-
1 The pseudocode for SMO may be found in the appendix of Platt (1999)
Edit Distance
(e ? 12) 
San Jose Medical Center announced 
Wednesday that it would close its 
doors by Dec. 1, 2004. 
San Jose Medical Center has an-
nounced that it will close its 
doors by Dec. 1, 2004. 
First Two 
Sentences 
The genome of the fungal pathogen 
that causes Sudden Oak Death has 
been sequenced by US scientists
Researchers announced Thursday 
they've completed the genetic 
blueprint of the blight-causing 
culprit responsible for Sudden Oak 
Death
Table 1.  Paraphrase Examples Identified by Two Heuristics  
2
ployed a variety of text classification tasks (e.g., 
Dumais 1998; Dumais et al, 1998). 
3.2 Datasets 
To construct our corpus, we collected news arti-
cles from news clusters on the World Wide Web. 
A database of 13,127,938 candidate sentence 
pairs was assembled from 9,516,684 sentences 
in 32,408 clusters collected over a 2-year period, 
using simple heuristics to identify those sen-
tence pairs that were most likely to be para-
phrases, and thereby prune the overall search 
space.  
Word-based Levenshtein edit distance 
of 1 < e ? 20; and a length ratio 
> 66%; OR
Both sentences in the first three 
sentences of each file; and length 
ratio > 50%. 
From this database, we extracted three data-
sets. The extraction criteria, and characteristics 
of these datasets are given in Table 2. The data 
sets are labled L(evenshtein) 12, F(irst) 2 and 
F(irst) 3 reflecting their primary selection char-
acteristics. The L12 dataset represents the best 
case achieved so far, with Alignment Error 
Rates beginning to approach those reported for 
alignment of closely parallel bilingual corpora. 
The F2 dataset was constructed from the first 
two sentences of the corpus on the same as-
sumptions as those used in Dolan et al (2004). 
To avoid conflating the two data types, however, 
sentence pairs with an edit distance of 12 or less 
were excluded. Since this resulted in a corpus 
that was significantly smaller than that desirable 
for exploring extraction techniques, we also cre-
ated a third data set, F3 that consisted of the 
cross-pairings of the first three sentences of each 
article in each cluster, excluding those where 
the edit distance is e ? 12.   
3.3 Training Data 
Our training data consisted of 10,000 sentence 
pairs extracted from randomly held-out clusters 
and hand-tagged by two annotators according to 
whether in their judgment (1 or 0) the sentence 
pairs constituted paraphrases. The annotators 
were presented with the sentences pairs in isola-
tion, but were informed that they came from 
related document sets (clusters). A conservative 
interpretation of valid paraphrase was adopted: 
if one sentence was a superstring of the other, 
e.g., if a clause had no counterpart in the other 
sentence, the pair was counted as a non-
paraphrase. Wherever the two annotators dis-
agreed, the pairs were classed as non-
paraphrases. The resultant data set contains 2968 
positive and 7032 negative examples.  
3.4 Features 
Some 264,543 features, including overt lexical 
pairings, were in theory available to the classi-
fier. In practice, however, the number of dimen-
sions used typically fell to less than 1000 after 
the lowest frequency features are eliminated (see 
Table 4.) The main feature classes were: 
String Similarity Features: All sentence pairs 
were assigned string-based features, includ-
ing absolute and relative length in words, 
number of shared words, word-based edit 
distance, and lexical distance, as measured 
by converting the sentences into alphabet-
ized strings of unique words and applying 
word based edit distance. 
Morphological Variants: Another class of 
features was co-ocurrence of morphological 
variants in sentence pairs. Approximately 
490,000 sentences in our primary datasets 
were stemmed using a rule-based stemmer, 
to yield a lexicon of 95,422 morphologically 
variant word pairs. Each word pair was 
treated as a feature. Examples are: 
orbit|orbital
orbiter|orbiting
WordNet Lexical Mappings: Synonyms and 
hypernyms were extracted from WordNet, 
L12 F2 F3 
Corpus size 253,725 51,933 235,061 
Levenshtein 
edit distance 1 < e ? 12 e > 12 e > 12 
Sentence range 
in article All First two First three
Length 5 < n < 30 5 < n < 30 5 < n < 30
Length ratio 66% 50% 50% 
Shared words 3 3 3 
Table 2. Characteristics of L(evenshtein) 12, 
F(irst) 2, and F(irst) 3 Data 
3
(http://www.cogsci.princeton.edu/~wn/;
Fellbaum, 1998), using the morphological 
variant lexicon from the 490,000 sentences 
as keywords. The theory here is that as addi-
tional paraphrase pairs are identified by the 
classifier, new information will ?come 
along for the ride,? thereby augmenting the 
range of paraphrases available to be learned. 
A lexicon of 314,924 word pairs of the fol-
lowing form created. Only those pairs iden-
tified as occurring in either training data or 
the corpus to be classified were included in 
the final classifier. 
operation|procedure
operation|work
Word Association Pairs: To augment the 
above resources, we dynamically extracted 
from the L12 corpus a lexicon of 13001 
possibly-synonymous word pairs using a 
log-likelihood algorithm described in Moore 
(2001) for machine translation. To minimize 
the damping effect of the overwhelming 
number of identical words, these were de-
leted from each sentence pair prior to proc-
essing; the algorithm was then run on the 
non-identical residue as if it were a bilingual 
parallel corpus.  
To deploy this data in the SVM feature set, 
a cutoff was arbitrarily selected that yielded 
13001 word pairs. Some exemplars (not 
found in WordNet) include:
straight|consecutive
vendors|suppliers
Fig. 1 shows the distribution of word pair-
ings obtained by this method on the L12 
corpus in comparison with WordNet. Ex-
amination of the top-ranked 1500 word 
pairs reveals that 46.53% are found in 
WordNet and of the remaining 53.47%, 
human judges rated 56% as good, yielding 
an overall ?goodness score? of 76.47%. 
Judgments were by two independent raters. 
For the purposes of comparison, we auto-
matically eliminated pairs containing trivial 
substring differences, e.g., spelling errors, 
British vs. American spellings, singu-
lar/plural alternations, and miscellaneous 
short abbreviations. All pairs on which the 
raters disagreed were discarded. Also dis-
carded were a large number of partial 
phrasal matches of the ?reported|according? 
and ?where|which? type, where part of a 
phrase (?according to?, ?in which?) was 
missing. Although viewed in isolation these 
do not constitute valid synonym or hyper-
rnym pairs, the ability to identify these par-
tial matchings is of central importance 
within an SMT-framework of paraphrase 
alignment and generation. These results 
suggest, among other things, that dynami-
cally-generated lexical data of this kind 
might be useful in increasing the coverage 
of hand-built synonymy resources. 
Composite Features: From each of the lexi-
cal feature classes, we derived a set of more 
abstract features that summarized the fre-
quency with which each feature or class of 
features occurred in the training data, both 
independently, and in correlation with others.  
These had the effect of performing normali-
zation for sentence length and other factors. 
Some examples are: 
No_of_List_2_Words (i.e., the 
count of Wordnet matches)
30.00%
35.00%
40.00%
45.00%
50.00%
55.00%
60.00%
500
1000
1500
2000Word Pairs
Not in Wordnet
In WordNet
Fig. 1.  WordNet Coverage in Word Associa-
tion Output 
4
External_Matches_2_LED (i.e,, 
the ratio of total lexical matches to 
Levenshtein edit distance.) 
4 Evaluation
4.1 Methodology 
Evaluation of paraphrase recognition within an 
SMT framework is highly problematic, since no 
technique or data set is standardly recognized. 
Barzilay & Lee (2003) and Quirk et al (2004) 
use human evaluations of end-to-end generation, 
but these are not very useful here, since they add 
an additional layer of uncertainty into the 
evaluation, and depend to a significant extent on 
the quality and functionality of the decoder.  
Dolan & Brockett (2005) report extraction pre-
cision of 67% using a similar classifier, but with 
the explicit intention of creating a corpus that 
contained a significant number of naturally-
occuring paraphrase-like negative examples. 
Since our purpose in the present work  is non-
application specific corpus construction, we ap-
ply an automated technique that is widely used 
for reporting intermediate results in the SMT 
community, and is being extended in other fields 
such as summarization (Daum? and Marcu, 
forthcoming), namely word-level alignment us-
ing an off-the-shelf implementation of the SMT 
system GIZA++ (Och & Ney, 2003). Below, we 
use Alignment Error Rate (AER), which is in-
dicative of how far the corpus is from providing 
a solution under a standard SMT tool. This al-
lows the effective coverage of an extracted cor-
pus to be evaluated efficiently, repeatedly 
against a single standard, and at little cost after 
the initial tagging. Further, if used as an objec-
tive function, the AER technique offers the 
prospect of using hillclimbing or other optimiza-
tion techniques for non-application-specific cor-
pus extraction. 
To create the test set, two human annotators 
created a gold standard word alignment on held 
out data consisting of 1007 sentences pairs. Fol-
lowing the practice of Och & Ney (2000, 2003), 
the annotators each created an initial annotation, 
categorizing alignments as either SURE (neces-
sary) or POSSIBLE (allowed, but not required). In 
the event of differences, annotators were asked 
to review their choices. First pass inter-rater 
agreement was 90.28%, climbing to 94.43% on 
the second pass. Finally we combined the anno-
tations into a single gold standard as follows: if 
both annotators agreed that an alignment was 
SURE, it was tagged as SURE in the gold-
standard; otherwise it was tagged as POSSIBLE.
To compute Precision, Recall, and Alignment 
Error Rate (AER), we adhere to the formulae 
listed in Och & Ney (2003). Let A be the set of 
alignments in the comparison, S be the set of 
SURE alignments in the gold standard, and P be 
the union of the SURE and POSSIBLE alignments 
in the gold standard:  
||
||precision
A
PA?
= ; ||
||
recall
S
SA?
=
||
||||AER
SA
SAPA
+
?+?
=
4.2 Baselines 
Evaluations were performed on the heuristi-
cally-derived L12, F2, and F3 datasets using the 
above formulation. Results are shown in Table 3.  
L12 represents the best case, followed respec-
tively by F3 and F2.  AERs were also computed 
separately for identical (Id) and non-identical 
(Non-Id) word mappings in order to be able to  
Corpus 
Size
(pairs) 
Precision Recall AER Id AER Non Id AER 
L12 ~254 K 87.42% 87.66% 12.46% 11.57% 21.25% 
F2 ~52 K 85.56% 83.31% 15.57% 13.19% 39.08% 
F3 ~235K 86.53% 81.57% 15.99% 14.24% 33.83% 
10K Trained ~24 K 86.93% 87.24% 12.92% 11.69% 24.70% 
MSR Trained  ~50 K 86.76% 86.39% 13.42% 11.92% 28.31% 
Table 3.  Precision, Recall and Alignment Error Rates 
5
drill down on the extent to which new non-
identical mappings are being learned from the 
data. A high Id error rate can be considered in-
dicative of noise in the data. The score that we 
are most interested in, however, is the Non-Id 
alignment error rate, which can be considered 
indicative of coverage as represented by the 
Giza++ alignment algorithm?s ability to learn 
new mappings from the training data. It will be 
observed that the F3 dataset non-Id AER is 
smaller than that of the F2 dataset: it appears 
that more data is having the desired effect.  
Following accepted SMT practice, we added 
a lexicon of identical word mappings to the 
training data, since Giza++ does not directly 
model word identity, and cannot easily capture 
the fact that many words in paraphrase sentence 
may translate as themselves. We did not add in 
word pairs derived from word association data 
or other supplementary resources that might 
help resolve matches between unlike but seman-
tically similar words.  
4.3 Training on the 10K Data 
We trained an SVM on the 10 K training set 
employing 3-fold cross-validation on the train-
ing set itself.  Validation errors were typically in 
the region of 16-17%. Linear kernels with de-
fault parameters (tolerance=1e-3; margin size 
computed automatically; error probability=0.5) 
were employed throughout. Applying the SVM 
to the F3 data, using 946 features encountered in 
the training data with frequency > 4, this classi-
fier yielded a set of 24588 sentence pairs, which 
were then aligned using Giza++.   
The alignment result is shown in Table 3. The 
?10K Trained? row represents the results of ap-
plying Giza++ to the data extracted by the SVM. 
Non-identical word AER, at 24.70%, shows a 
36.9% reduction in the non-identical word AER 
over the F2 dataset (which is approximately 
double the size), and approximately 28% over 
the original F3 dataset. This represents a huge 
improvement in the quality of the data collected 
by using the SVM and is within striking distance 
of the score associated with the L12 best case. 
The difference is especially significant when it 
is considered that the newly constructed corpus 
is less than one-tenth the size of the best-case 
corpus. Table 5 shows sample extracted sen-
tences. 
To develop insights into the relative contribu-
tions of the different feature classes, we omitted 
some feature classes from several runs. The re-
sults were generally indistinguishable, except 
for non-Id AER, shown in Table 4, a fact that 
may be taken to indicate that string-based fea-
tures such as edit distance still play a major role.  
Eliminating information about morphological 
alternations has the largest overall impact, pro-
ducing a degradation of a 0.94 in on Non-Id 
AER. Of the three feature classes, removal of 
WordNet appears to have the least impact, 
showing the smallest change in Non-Id AER.  
When the word association algorithm is ap-
plied to the extracted ~24K-sentence-pair set, 
degradation in word pair quality occurs signifi-
cantly earlier than observed for  the L12 data; 
after removing ?trivial? matches, 22.63% of 
word pairs in the top ranked 800 were found in 
Wordnet, while 25.3% of the remainder were 
judged to be ?good? matches. This is equivalent 
to an overall ?goodness score? of 38.25%. The 
rapid degradation of goodness may be in part 
attributable to the smaller corpus size yielded by 
the classifier. Nevertheless, the model learns 
many valid new word pairs. Given enough data 
with which to bootstrap, it may be possible to do 
away with static resources such as Wordnet, and 
rely entirely on dynamically derived data.   
4.4 Training on the MSR Training Set 
By way of comparison, we also explored appli-
cation of the SVM to the training data in the 
MSR Paraphrase corpus. For this purpose we 
used the 4076-sentence-pair ?training? section 
of the MSR corpus, comprising 2753 positive 
and 1323 negative examples. The results at de-
fault parameter settings are given in Table 3, 
with respect to all features that were observed to 
occur with frequency greater than 4. Although 
the 49914 sentence pairs yielded by using the 
    Dimensions Non Id AER
All (fq > 4) 946 24.70
No Lexical Pairs 230 25.35 
No Word  
Association 470 25.35 
No WordNet  795 25.24 
No Morphology 813 25.64 
Table 4.  Effect of Eliminating Feature Classes 
on 10K Training Set 
6
MSR Paraphrase Corpus is nearly twice that of 
the 10K training set, AER performance is meas-
urably degraded. Nevertheless, the MSR-trained 
corpus outperforms the similar-sized F12, yield-
ing a reduction in Non-Id AER of a not insig-
nificant 16%.   
The fact that the MSR training data does not 
perform as well as the 10 K training set probably 
reflects its derivative nature, since it was origi-
nally constructed with data collected using the 
10K training set, as described in Dolan & 
Brockett (2005). The performance of the MSR 
corpus is therefore skewed to reflect the biases 
inherent in its original training, and therefore 
exhibits the performance degradation commonly 
associated with bootstrapping. It is also a sig-
nificantly smaller training set, with a higher 
proportion of negative examples than in typical 
in real world data. It will probably be necessary 
to augment the MSR training corpus with further 
negative examples before it can be utilized ef-
fectively for training classifiers. 
5 Discussion and Future Work 
These results show that it is possible to use ma-
chine learning techniques to induce a corpus of 
likely sentential paraphrase pairs whose align-
ment properties measured in terms of AER ap-
proach those of a much larger, more 
homogeneous dataset collected using a string-
edit distance heuristic. This result supports the 
idea that an abstract notion of paraphrase can be 
captured in a high dimensional model.  
Future work will revolve around optimizing 
classifiers for different domains, corpus types 
and training sets. It seems probable that the ef-
fect of the 10K training corpus can be greatly 
augmented by adding sentence pairs that have 
been aligned from multiple translations using 
the techniques described in, e.g., Barzilay & 
McKeown (2001) and Pang et al (2003).  
6 Conclusions
We have shown that supervised machine 
learning techniques such as SVMs can signifi-
cantly expand available paraphrase corpora, and 
achieve a reduction of noise as measured by 
AER on non-identical words.  
Although from the present research has fo-
cused on ?ready-made? news clusters found on 
the web, nothing in this paper depends on the 
availability of such clusters. Given standard 
clustering techniques, the approach that we have 
described for inductive classifier learning should 
in principle be applicable to any flat corpus 
which contains multiple sentences expressing 
similar content. We expect also that the tech-
niques described here could be extended to iden-
tify bilingual sentence pairs in comparable 
corpora, helping automate the construction of 
corpora for machine translation. 
The ultimate test of paraphrase identification 
technologies lies in applications. These are 
likely to be in fields such as extractive multi-
document summarization where paraphrase de-
tection might eliminate sentences with compara-
ble content and Question Answering, for both 
identifying sentence pairs with comparable con-
tent and generating unique new text. Such prac-
young female chimps learn skills 
earlier , spend more time studying 
and tend to do better than young 
male chimpanzees - at least when it 
comes to catching termites . 
young female chimpanzees are better stu-
dents than males , at least when it 
comes to catching termites , according 
to a study of wild chimps in tanzania 's 
gombe national park . Paraphrase
(accepted)  a %%number%% -year-old girl was 
arrested , handcuffed and taken 
into custody on charges of stealing 
a rabbit and a small amount of 
money from a neighbor 's home . 
sheriff 's deputies in pasco county , 
fla. , this week handcuffed and ques-
tioned a %%number%% -year-old girl who 
was accused of stealing a rabbit 
and  %%money%%  from a neighbor 's 
home . 
Non-
Paraphrase
(rejected) 
roy moore , the chief justice of 
alabama , installed the two-ton 
sculpture in the rotunda of his 
courthouse in montgomery , and has 
refused to remove it . 
the eight associate justices of alabama 
's supreme court voted unani-
mously  %%day%%  to overrule moore and 
comply with u.s. district judge myron 
thompson 's order to remove the monu-
ment . 
Table 5.  Sample Pairs Extracted and Rejected by the SVM Trained on the 10K Corpus 
7
tical applications will only be possible once 
large corpora are available to permit the devel-
opment of robust paraphrase models on the scale 
of the best SMT models. We believe that the 
corpus construction techniques that we have de-
scribed here represent an important contribution 
to this goal.      
Acknowledgements 
We would like to thank Monica Corston-Oliver, 
Jeff Stevenson, Amy Muia and Margaret Salome 
of Butler Hill Group LLC for their assistance in 
annotating and evaluating our data. This paper 
has also benefited from feedback from several 
anonymous reviewers. All errors and omissions 
are our own. 
References  
Regina Barzilay and Katherine. R. McKeown. 2001. 
Extracting Paraphrases from a parallel corpus. In 
Proceedings of the ACL/EACL.
Regina Barzilay and  Lillian Lee. 2003. Learning to 
Paraphrase; an unsupervised approach using mul-
tiple-sequence alignment. In Proceedings of 
HLT/NAACL 2003.
P. Brown, S. A. Della Pietra, V.J. Della Pietra and R. 
L. Mercer. 1993. The Mathematics of Statistical 
Machine Translation. Computational Linguistics,
Vol. 19(2): 263-311. 
Hal Daum? III and Daniel Marcu. (forthcoming)  
Induction of Word and Phrase Alignments for 
Automatic Document Summarization. To appear 
in Computational Linguistics.
William. B. Dolan, Chris Quirk, and Chris Brockett. 
2004. Unsupervised Construction of Large Para-
phrase Corpora: Exploiting Massively Parallel 
News Sources. Proceedings of COLING 2004,
Geneva, Switzerland.  
William B. Dolan and Chris Brockett. 2005. Auto-
matically Constructing a Corpus of Sentential 
Paraphrases. In Proceedings of The Third Interna-
tional Workshop on Paraphrasing (IWP2005), 
Jeju, Republic of Korea.
Susan Dumais. 1998. Using SVMs for Text Catego-
rization. IEEE Intelligent Systems, Jul.-Aug. 1998: 
21-23 
Susan Dumais, John Platt, David Heckerman, Me-
hran Sahami. 1998. Inductive learning algorithms 
and representations for text categorization. In Pro-
ceedings of the Seventh International Conference 
on Information and Knowledge Management. 
Richard Durbin, Sean R. Eddy, Anders Krogh, and 
Graeme Mitchison. 1998. Biological sequence 
analysis: Probabilistic models of proteins and nu-
cleic acids. Cambridge University Press.  
Christiane Fellbaum (ed.). 1998. WordNet: An Elec-
tronic Lexical Database. The MIT Press. 
Andrew Finch, Taro Watanabe, Yasuhiro Akiba and 
Eiichiro Sumita. 2004. Paraphrasing as Machine 
Translation. Journal of Natural Language Proc-
essing, 11(5), pp 87-111. 
Thorsten Joachims. 2002.  Learning to Classify Text 
Using Support Vector Machines: Methods, Theory, 
and Algorithms. Kluwer Academic Publishers, 
Boston/Dordrecht/London. 
Microsoft Research Paraphrase Corpus. 
http://research.microsoft.com/research/downloads/
default.aspx 
Robert C. Moore. 2001.  Towards a Simple and Ac-
curate Statistical Approach to Learning Transla-
tion Relationships among Words. In Proceedings 
of the Workshop on Data-Driven Machine Trans-
lation, ACL 2001. 
Franz Joseph Och and H. Ney. 2000. Improved Sta-
tistical Alignment Models.  In Proceedings of the 
38th Annual Meeting of the ACL, Hong Kong, 
China, pp 440-447. 
Franz Joseph Och and Hermann Ney. 2003. A Sys-
tematic Comparison of Various Statistical Align-
ment Models.  Computational Linguistics, 29 (1): 
19-52. 
Bo Pang, Kevin Knight, and Daniel Marcu. 2003. 
Syntax-based Alignment of Multiple Translations: 
Extracting Paraphrases and Generating New Sen-
tences. In Proceedings of NAACL-HLT.
John C. Platt. 1999. Fast Training of Support Vector 
Machines Using Sequential Minimal Optimization. 
In Bernhard Sch?lkopf, Christopher J. C. Burges 
and Alexander J. Smola (eds.). 1999.  Advances in 
Kernel Methods: Support Vector Learning. The 
MIT Press, Cambridge, MA. 185-208. 
Quirk, Chris, Chris Brockett, and William B. Dolan. 
2004. Monolingual Machine Translation for Para-
phrase Generation, In Proceedings of the 2004 
Conference on Empirical Methods in Natural 
Language Processing, 25-26 July 2004, Barcelona 
Spain, pp. 142-149. 
Bernhard Sch?lkopf and Alexander J. Smola. 2002.  
Learning with Kernels: Support Vector Machines, 
Regularization, Optimization, and Beyond. The 
MIT Press, Cambridge, MA. 
Y. Shinyama, S. Sekine and K. Sudo 2002. Auto-
matic Paraphrase Acquisition from News Articles.
In Proceedings of NAACL-HLT. 
Vladimir N. Vapnik. 1995. The Nature of Statistical 
Learning Theory. Springer-Verlag, New York. 
8
Automatically Constructing a Corpus of Sentential Paraphrases 
William B. Dolan and Chris Brockett 
Natural Language Processing Group 
Microsoft Research 
Redmond, WA, 98052, USA 
{billdol,chrisbkt}@microsoft.com
Abstract 
An obstacle to research in automatic 
paraphrase identification and genera-
tion is the lack of large-scale, publicly-
available labeled corpora of sentential 
paraphrases. This paper describes the 
creation of the recently-released Micro-
soft Research Paraphrase Corpus, 
which contains 5801 sentence pairs, 
each hand-labeled with a binary judg-
ment as to whether the pair constitutes 
a paraphrase. The corpus was created 
using heuristic extraction techniques in 
conjunction with an SVM-based classi-
fier to select likely sentence-level para-
phrases from a large corpus of topic-
clustered news data. These pairs were 
then submitted to human judges, who 
confirmed that 67% were in fact se-
mantically equivalent. In addition to 
describing the corpus itself, we explore 
a number of issues that arose in defin-
ing guidelines for the human raters. 
1 Introduction 
The Microsoft Research Paraphrase Corpus 
(MSRP), available for download at 
http://research.microsoft.com/research/nlp/msr_
paraphrase.htm, consists of 5801 pairs of sen-
tences, each accompanied by a binary judgment 
indicating whether human raters considered the 
pair of sentences to be similar enough in mean-
ing to be considered close paraphrases. This data 
has been published for the purpose of encourag-
ing research in areas relating to paraphrase and 
sentential synonymy and inference, and to help 
establish a discourse on the proper construction 
of paraphrase corpora for training and evalua-
tion.  It is hoped that by releasing this corpus, 
we will stimulate the publication of similar cor-
pora by others and help move the field toward 
adoption of a shared dataset that will permit use-
ful comparisons of results across research efforts.    
 
2 Motivation
The success of Statistical Machine Translation 
(SMT) has sparked a successful line of investi-
gation that treats paraphrase acquisition and 
generation essentially as a monolingual machine 
translation problem (e.g., Barzilay & Lee, 2003; 
Pang et al, 2003; Quirk et al, 2004; Finch et al, 
2004). However, a lack of standardly-accepted 
corpora on which to train and evaluate models is 
a major stumbling block to the successful appli-
cation of SMT models or other machine learning 
algorithms to paraphrase tasks.  Since para-
phrase is not apparently a common ?natural? 
task?under normal circumstances people do not 
attempt to create extended paraphrase texts?the 
field lacks a large readily identifiable dataset 
comparable to, for example, the Canadian Han-
sard corpus in SMT that can serve as a standard 
against which algorithms can be trained and 
evaluated.  
What paraphrase data is currently available is 
usually too small to be viable for either training 
or testing, or exhibits narrow topic coverage, 
limiting its broad-domain applicability. One 
class of paraphrase data that is relatively widely 
available is multiple translations of sentences in 
a second language. These, however, tend to be 
rather restricted in their domain (e.g. the ATR 
English-Chinese paraphrase corpus, which con-
9
sists of translations of travel phrases (Zhang & 
Yamamoto, 2002)), are limited to short hand-
crafted predicates (e.g. the ATR Japanese-
English corpus (Shirai, et al, 2002)), or exhibit 
quality problems stemming from insufficient 
command of the target language by the transla-
tors of the documents in question, e.g. the Lin-
guistic Data Consortium?s Multiple-Translation 
Chinese Corpus (Huang et al, 2002).  Multiple 
translations of novels, such as those used in 
(Barzilay & McKeown, 2001) provide a rela-
tively limited dataset to work with, and ? since 
these usually involve works that are out of copy-
right ?  usually exhibit older styles of language 
that have little in common with modern lan-
guage resources or application requirements.   
Likewise, the data made available by (Barzi-
lay & Lee, 2003: http://www.cs.cornell.edu/ 
Info/Projects/NLP/statpar.html), while invalu-
able in understanding and evaluating their re-
sults, is too limited in size and domain coverage 
to serve as either training or test data. 
Attempting to evaluate models of paraphrase 
acquisition and generation under limitations can 
thus be an exercise in frustration. Accordingly, 
we have tried to create a reasonably large corpus 
of naturally-occurring, non-handcrafted sentence 
pairs, along with accompanying human judg-
ments, that can be used as a resource for training 
or testing purposes. Since the search space for 
identifying any two sentence pairs occurring ?in 
the wild? is huge, and provides far too many 
negative examples for humans to wade through, 
clustered news articles were used to constrain 
the initial search space to data that was likely to 
yield paraphrase pairs.   
3 Source Data 
The Microsoft Research Paraphrase Corpus 
(MSRP) is distilled from a database of 
13,127,938 sentence pairs, extracted from 
9,516,684 sentences in 32,408 news clusters 
collected from the World Wide Web over a 2-
year period, The methods and assumptions used 
in building this initial data set are discussed in 
Quirk et al (2004) and Dolan et al (2004). Two 
heuristics based on shared lexical properties and 
sentence position in the document were em-
ployed to construct the initial database:  
Word-based Levenshtein edit distance 
of 1 < e    20; and a length ratio 
> 66%; OR
Both sentences in the first three 
sentences of each file; and length 
ratio > 50%. 
Within this initial dataset we were able to 
automatically identify the names of both authors 
and copyright holders of 61,618 articles.1  Limit-
ing ourselves only to sentences found in those 
articles, we further narrowed the range of candi-
date pairs using the following criteria: 
The number of words in both sentences 
in words is 5 ?  n ? 40; 
The two sentences shared at least 
three words in common; 
The length of the shorter of the two 
sentences, in words, is at least 
66.6% that of the longer; and
The two sentences had a bag-of-words 
lexical distance of e ? 8 edits.
This enabled us extract a set of 49,375 initial 
candidate sentence pairs whose author was 
known,  The purpose of these heuristics was 
two-fold: 1) to narrow the search space for sub-
sequent application of the classifier algorithm 
and human evaluation, and 2) to ensure at least 
some diversity among the sentences. In particu-
lar, we sought to exclude the large number of 
sentence pairs whose differences might be at-
tributable only to typographical errors, variance 
between British and American spellings, and 
minor editorial variations. Lexical distance was 
computed by constructing an alphabetized list of 
unique vocabulary items from each of the sen-
tences and measuring the number of insertions 
and deletions. Note that the number of sentence 
pairs collected in this first pass was relatively 
small compared with the overall size of the data-
set; the requirement of author identification sig-
nificantly circumscribed the available dataset.   
1
 Author identification was performed on the basis of pat-
tern matching datelines and other textual information.  We 
made a strong effort to ensure correct attribution. 
10
4 Constructing a Classifier 
4.1 Sequential Minimal Optimization 
To extract candidate pairs from this ~49K list, 
we used a Support Vector Machine. (Vapnik, 
1995), in this case an implementation of the Se-
quential Minimal Optimization (SMO) algo-
rithm described in Platt (1999),2  which has been 
shown to be useful in text classification tasks 
(Dumais 1998; Dumais et al, 1998). 
4.2 Training Set 
A separate set of 10,000 sentence pairs had 
previously been extracted from randomly held-
out clusters and hand-tagged by two annotators 
according to whether the sentence pairs consti-
tuted paraphrases. This yielded a set of 2968 
positive examples and 7032 negative examples. 
The sentences represented a random mixture of 
held out sentences; no attempt was made to 
match their characteristics to those of the candi-
date data set.  
4.3 Classifiers 
In the classifier we restricted the feature set to a 
small set of feature classes. The main classes are 
given below. More details can be found in 
Brockett and Dolan (2005). 
String Similarity Features: Absolute and rela-
tive length in words, number of shared 
words, word-based edit distance, and bag-
of-words-based lexical distance. 
Morphological Variants: A morphological 
variant lexicon consisting of 95,422 word 
pairs was created using a hand-crafted 
stemmer. Each pair is then treated as a 
feature in the classifier.  
WordNet Lexical Mappings: 314,924 word 
synonyms and hypernym pairs were ex-
tracted from WordNet, (Fellbaum, 1998; 
http://www.cogsci.princeton.edu/~wn/). 
Only pairs identified as occurring in either 
training data or the corpus to be classified 
were included in the final classifier.  
2
 The pseudocode for SMO may be found in the appendix 
of Platt (1999) 
Encarta Thesaurus: 125,054 word synonym 
pairs were extracted from the Encarta The-
saurus (Rooney, 2001). 
Composite Features: Additional, more ab-
stract features summarized the frequency 
with which each feature or class of features 
occurred in the training data, both inde-
pendently, and in correlation with other fea-
tures or feature classes.  
4.4 Results of Applying the Classifier  
Since our purpose was not to evaluate the poten-
tial effectiveness of the classifier itself, but to 
identify a reasonably large set of both positive 
and plausible ?near-miss? negative examples, 
the classifier was applied with output probabili-
ties deliberately skewed towards over-
identification, i.e., towards Type 1 errors, as-
suming non-paraphrase (0) as null hypothesis.  
This yielded 20,574 pairs out the initial 49,375-
pair data set, from which 5801 pairs were then 
further randomly selected for human assessment. 
5 Human Evaluation  
The 5801 sentences selected by the classifier as 
likely paraphrase pairs were examined by two 
independent human judges. Each judge was 
asked whether the two sentences could be con-
sidered ?semantically equivalent?. Disagree-
ments were resolved by a 3rd judge, with the 
final binary judgment reflecting the majority 
vote.3 After resolving differences between raters, 
3900 (67%) of the original pairs were judged 
?semantically equivalent?. 
5.1 Semantic Divergence 
In many instances, the two sentences judged 
?semantically equivalent? in fact diverge seman-
tically to at least some degree. For instance, both 
judges considered the following two to be para-
phrases: 
3
 This annotation task was carried out by an independent 
company, the Butler Hill Group, LLC. Monica Corston-
Oliver directed the effort, with Jeff Stevenson, Amy Muia, 
and David Rojas acting as raters.  
11
Charles O. Prince, 53, was named as 
Mr. Weill?s successor. 
Mr. Weill?s longtime confidant, 
Charles O. Prince, 53, was named 
as his successor. 
If a full paraphrase relationship can be de-
scribed as ?bidirectional entailment?, then the 
majority of the ?equivalent? pairs in this dataset 
exhibit ?mostly bidirectional entailments?, with 
one sentence containing information that differs 
from or is not contained in the other. Our deci-
sion to adopt this relatively loose tagging crite-
rion was ultimately a practical one: insisting on 
complete sets of bidirectional entailments would 
have limited the dataset to pairs of sentences 
that are practically identical at the string level, 
as in the following examples.  
The euro rose above US$1.18, the 
highest price since its January 
1999 launch. 
The euro rose above $1.18 the high-
est level since its launch in 
January 1999. 
However, without a carefully con-
trolled study, there was little 
clear proof that the operation ac-
tually improves people?s lives. 
But without a carefully controlled 
study, there was little clear 
proof that the operation improves 
people?s lives. 
Such pairs are commonplace in the raw data, 
reflecting the tendency of news agencies to pub-
lish and republish the same articles, with editors 
introducing small and often inexplicable 
changes (is ?however? really better than ?but??) 
along the way. The resulting alternations are 
useful sources of information about synonymy 
and local syntactic changes, but our goal was to 
produce a richer type of corpus; one that pro-
vides information about the large-scale alterna-
tions that typify complex paraphrases.4
4
 Recall that in an effort to focus on sentence pairs that are 
not simply trivial variants of some original single source, 
we restricted our original dataset by removing all pairs with 
a minimum word-based Levenshtein distance of ? 8. 
5.2 Complex Alternations 
Some sentence pairs in the news data capture 
complex and full paraphrase alternations: 
Wynn paid $23.5 million for Re-
noir?s ?In the Roses (Madame Leon 
Clapisson)? at a Sotheby auction 
on Tuesday 
Wynn nabbed Renoir?s ?In the Roses 
(Madame Leon Clapisson)? for $23.5 
on Tuesday at Sotheby?s
Far more frequently, however, interesting 
paraphrases in the data are accompanied by at 
least minor differences in content: 
David Gest has sued his estranged 
wife Liza Minelli for %MONEY% mil-
lion for beating him when she was 
drunk
Liza Minelli?s estranged husband is 
taking her to court for %MONEY% 
million after saying she threw a 
lamp at him and beat him in 
drunken rages 
It quickly became clear, that in order to col-
lect significant numbers of sentential paraphrase 
pairs, our standards for what constitutes ?seman-
tic equivalence? would have to be relaxed.  
5.3 Rater Instructions 
Raters were told to use their best judgment in 
deciding whether 2 sentences, at a high level, 
?mean the same thing?. Under our relatively 
loose definition of semantic equivalence, any 2 
of the following sentences would have qualified 
as ?paraphrases?, despite obvious differences in 
information content: 
The genome of the fungal pathogen 
that causes Sudden Oak Death has 
been sequenced by US scientists 
Researchers announced Thursday 
they've completed the genetic 
blueprint of the blight-causing 
culprit responsible for sudden oak 
death
Scientists have figured out the 
complete genetic code of a viru-
lent pathogen that has killed tens 
12
of thousands of California native 
oaks
The East Bay-based Joint Genome In-
stitute said Thursday it has un-
raveled the genetic blueprint for 
the diseases that cause the sudden 
death of oak trees 
Several classes of named entities were re-
placed by generic tags in sentences presented to 
the raters, so that ?Tuesday? be-
came %%DAY%%, ?$10,000? became 
?%%MONEY%%, and so on. In the released 
version of the dataset, however, these place-
holders were replaced by the original strings. 
After a good deal of trial-and-error, some 
specific rating criteria were developed and in-
cluded in a tagging specification. For the most 
part, though, the degree of mismatch allowed 
before the pair was judged ?non-equivalent? was 
left to the discretion of the individual rater: did a 
particular set of asymmetries alter the meanings 
of the sentences so much that they could not be 
regarded as paraphrases? The following sen-
tences, for example, were judged ?not equiva-
lent? despite some significant content overlap: 
The Gerontology Research Group said 
Slough was born on %DATE%, making 
her %NUMBER% years old at the time 
of her death. 
?[Mrs. Slough?] is the oldest liv-
ing American as of the time she 
died, L. Stephen Coles, Executive 
Director of the Gerontology Re-
search Group, said %DATE%. 
The tagging task was ill-defined enough that 
we were surprised at how high inter-rater 
agreement was (averaging 84%). The Kappa 
score of 62 is good, but low enough to be indica-
tive of the difficulty of the rating task.  We be-
lieve that with more practice and discussion 
between raters, agreement on the task could be 
improved. 
Interestingly, a series of experiments aimed 
at making the judging task more concrete re-
sulted in uniformly degraded inter-rater agree-
ment. Providing a checkbox to allow judges to 
specify that one sentence fully entailed another, 
for instance, left the raters frustrated, slowed 
down the tagging, and had a negative impact on 
agreement. Similarly, efforts to identify classes 
of syntactic alternations that would not count 
against an ?equivalent? judgment resulted, in 
most cases, in a collapse in inter-rater agreement. 
After completing hundreds of judgments, the 
raters themselves were asked for suggestions as 
to what checkboxes or instructions might im-
prove tagging speed and accuracy. In the end, 
few generalizations seemed useful in streamlin-
ing the task; each pair is sufficiently idiosyn-
cratic that that common sense has to take 
precedence over formal guidelines. 
In a few cases, firm tagging guidelines were 
found to be useful. One example was the treat-
ment of pronominal and NP anaphora. Raters 
were instructed to treat anaphors and their full 
forms as equivalent, regardless of how great the 
disparity in length or lexical content between the 
two sentences. (Often these correspondences are 
extremely interesting, and in sufficient quantity 
would provide interesting fodder for learning 
models of anaphora.) 
SCC argued that Lexmark was trying 
to shield itself from competition? 
The company also argued that Lex-
mark was trying to squash competi-
tion?
But Secretary of State Colin Powell 
brushed off this possibil-
ity %%day%%. 
Secretary of State Colin Powell 
last week ruled out a non-
aggression treaty.
Note that many of the 33% of sentence pairs 
judged to be ?not equivalent? still overlap sig-
nificantly in information content and even word-
ing. These pairs reflect a range of relationships, 
from pairs that are completely unrelated seman-
tically, to those that are partially overlapping, to 
those that are almost-but-not-quite semantically 
equivalent.  
6 Discussion
Given that MSRP reflects both the initial heuris-
tics and the SVM methodology that was em-
ployed to identify paraphrase candidates for 
human evaluation, it is also limited by that tech-
nology. The 67% ratio of positive to negative 
judgments is a reasonably reliable indicator of 
the precision of our technique--though it should 
13
be recalled that parameters were deliberately 
distorted to yield imprecise results that included 
positive and a large number of ?near-miss? 
negatives.  Coverage is hard to estimate reliably. 
we calculate that fewer than 30% of the pairs in 
a set of matched first-two sentences extracted 
from clustered news data, after application of 
simple heuristics, are paraphrases (Dolan et al, 
2004). It seems reasonable to assume that the 
reduction to 10% seen in the initial data set still 
leaves many valid paraphrase pairs uncaptured 
in the corpus. The need to limit the corpus to 
those sentences for which authorship can be 
verified, and more specifically. to no more than 
a single sentence extracted from each article. 
further constrains the coverage in ways whose 
consequences are not yet known. In addition, the 
three-shared-words heuristic further guarantees 
that an entire class of paraphrases in which no 
words are shared in common have been ex-
cluded from the data. It has been observed that 
the mean lexical overlap in the corpus is a rela-
tively high 0.7 (Weeds et al 2005), suggesting 
that more lexically divergent examples will be 
needed.  In these respects, as Wu (2005) points 
out, the corpus is far from distributionally neu-
tral. This is a matter that we hope to remedy in 
the future, since in many ways this excluded set 
of pairs is the most interesting of all.  
The above limitations, together with its rela-
tively small size, perhaps make the MRSP inap-
propriate for direct use as a training corpus. We 
show separately that the results of training a 
classifier on the present corpus may be inferior 
to other training sets, though better than crude 
string or text-based heuristics (Brockett & Dolan, 
2005). We expect that the utility of the corpus 
will stem primarily from its use as a tool for 
evaluating paraphrase recognition algorithms. It 
has already been applied in this way by Corley 
& Mihalcea (2005) and Wu (2005).  
7 A Virtual Super Corpus? 
Although larger than any other non-translation-
based labeled paraphrase corpus currently pub-
licly available, MSRP is tiny compared with the 
huge bilingual parallel corpora publicly avail-
able within the Machine Translation community, 
for example, the Canadian Hansards, the Hong 
Kong Parliamentary corpus, or the United Na-
tions documents. It is improbable that we will 
ever encounter a ?naturally occurring? para-
phrase corpus on the scale of any of these bilin-
gual corpora.  Moreover, whatever extraction 
technique is employed to identify paraphrases in 
other kinds of data will be apt to reflect the im-
plicit biases of the methodology employed.   
Here we would like to put forward a proposal.  
The paraphrase research community might be 
able to construct a ?virtual paraphrase corpus? 
that would be adequately large for both training 
and testing purposes and minimize selectional 
biases. This could be achieved in something like 
the following manner. Research groups could 
compile their own labeled paraphrase corpora, 
applying whatever learning techniques they 
choose to select their initial data. If enough in-
terested groups were to release a sufficiently 
large number of reasonably-sized corpora, it 
might be possible to achieve some sort consen-
sus, in a manner analogous to the division of the 
Penn Treebank into sections, whereby classifiers 
and other tools are conventionally trained on one 
subset of corpora, and tested against another 
subset. Though this would present issues of its 
own, it would obviate many of the problems of 
extraction bias inherent in automated extraction, 
and allow better cross comparison across sys-
tems.   
8 Future Directions 
For our part we plan to expand the MSRP, 
both by extending the number of sentence pairs, 
and also improving the balance of positive and 
negative examples. We anticipate using multiple 
classifiers to reduce inherent biases in candidate 
corpus selection, and with better author identifi-
cation to ensure proper attribution, to be able to 
draw on a larger dataset for consideration by our 
judges.  
In future releases we expect to make avail-
able more information about individual evalua-
tor judgments. Burger & Ferro (2005) have 
suggested that this data may allow researchers 
greater freedom to construct models based on 
the judgments of specific judges or combina-
tions of judges, permitting more fine-grained use 
of the corpus.  
One further issue that we will also be at-
tempting to address is the need to provide a bet-
ter metric for corpus coverage and quality. Until 
reliable metrics can be established for end-to-
14
end paraphrase tasks?these will probably need 
to be application specific?the Alignment Error 
Rate strategy that was successfully applied in 
early development of machine translation sys-
tems (Och & Ney, 2000, 2003) offers a useful 
intermediate representation of the coverage and 
precision of a corpus and extraction techniques. 
Though fullscale reliability studies have yet to 
be performed, the AER technique is already 
finding application in other fields such as sum-
marization (Daum? & Marcu, forthcoming). We 
expect to be able to provide a reasonably large 
corpus of word-aligned paraphrase sentences in 
the near future that we hope will serve as some 
sort of standard by which corpus extraction 
techniques can be measured and compared in a 
uniform fashion.   
One other path that we are concurrently ex-
ploring is collection and validation of para-
phrase data by volunteers on the web. Some 
initial efforts using game formats for elicitation 
are presented in Chklovski (2005) and Brockett 
& Dolan (2005). It is our hope that web volun-
teers will prove a useful source of colloquial 
paraphrases of written text, and?if paraphrase 
identification can be effectively embedded in the 
game?of paraphrase judgments.   
9 Conclusion
We have used heuristic techniques and a classi-
fier to automatically create a corpus of 5801 
?naturally occurring? (non-constructed) sentence 
pairs, labeled according to whether, in the judg-
ment of our evaluators, the sentences ?mean the 
same thing? or not.  To our knowledge, MSRP 
constitutes the largest currently-available broad-
domain corpus of paraphrase pairs that does not 
have its origins in translations from another lan-
guage.  We hope that others will utilize it, find it 
useful, and provide feedback when it is not.  
The methodology that we have described for 
extracting this corpus is readily adaptable by 
others, and is not limited to news clusters, but 
can be readily extended to any flat corpus con-
taining a large number of semantically similar 
sentences on which topic-based document clus-
tering is possible. We have shown that by allow-
ing a statistical learning algorithm to constrain 
the search space, it is possible to identify a man-
ageable-sized candidate corpus on the basis of 
which human judges can label sentence pairs for 
paraphrase content quickly and in a cost effec-
tive manner. We hope that others will follow our 
example.   
Acknowledgements 
We would like to thank Monica Corston-Oliver, 
Jeff Stevenson, Amy Muia and David Rojas of 
Butler Hill Group LLC for their assistance in 
annotating the Microsoft Research Paraphrase 
Corpus and in preparing the seed data used for 
training. This paper has also benefited from 
feedback from several anonymous reviewers. 
All errors and omissions are our own. 
References  
Regina Barzilay and Katherine. R. McKeown. 2001. 
Extracting Paraphrases from a parallel corpus. In 
Proceedings of the ACL/EACL.
Regina Barzilay and  Lillian Lee. 2003. Learning to 
Paraphrase; an unsupervised approach using mul-
tiple-sequence alignment. In Proceedings of 
HLT/NAACL 2003.
Chris Brockett and William B. Dolan. 2005. Support 
Vector Machines for Paraphrase Identification and 
Corpus Construction. In Proceedings of The Third 
International Workshop on Paraphrasing 
(IWP2005), Jeju, Republic of Korea. 
Chris Brockett and William B. Dolan. 2005. Echo 
Chamber: A Game for Eliciting a Colloquial Para-
phrase Corpus. AAAI 2005 Spring Symposium, 
Knowledge Collection from Volunteer Contribu-
tors (KCVC05). Stanford, CA. March 21-23, 2005. 
P. Brown, S. A. Della Pietra, V.J. Della Pietra and R. 
L. Mercer. 1993. The Mathematics of Statistical 
Machine Translation. Computational Linguistics,
Vol. 19(2): 263-311.  
John Burger and Lisa Ferro. 2005.  Generating an 
Entailment Corpus from News Headlines.  In Pro-
ceedings of the ACL Workshop on Empirical 
Modeling of Semantic Equivalence and Entailment.
pp 49-54. 
Timothy Chklovski. 2005 1001 Paraphrases: Incent-
ing Responsible Contributions in Collecting Para-
phrases from Volunteers. AAAI 2005 Spring 
Symposium, Knowledge Collection from Volunteer 
Contributors (KCVC05). Stanford, CA. March 21-
23, 2005. 
Courtney Courley and Rada Mihalcea. 2005. Measur-
ing the Semantic Similarity of Texts.  In Proceed-
ings of the ACL Workshop on Empirical Modeling 
of Semantic Equivalence and Entailment. Pp 13-
18.
Hal Daum? III and Daniel Marcu. (forthcoming)  
Induction of Word and Phrase Alignments for 
15
Automatic Document Summarization. To appear 
in Computational Linguistics.
William. B. Dolan, Chris Quirk, and Chris Brockett. 
2004. Unsupervised Construction of Large Para-
phrase Corpora: Exploiting Massively Parallel 
News Sources. Proceedings of COLING 2004,
Geneva, Switzerland.  
Susan Dumais. 1998. Using SVMs for Text Catego-
rization. IEEE Intelligent Systems, Jul.-Aug. 1998: 
21-23 
Susan Dumais, John Platt, David Heckerman, Meh-
ran Sahami. 1998. Inductive learning algorithms 
and representations for text categorization. In Pro-
ceedings of the Seventh International Conference 
on Information and Knowledge Management. 
Christiane Fellbaum (ed.). 1998. WordNet: An Elec-
tronic Lexical Database. The MIT Press. 
Andrew Finch, Taro Watanabe, Yasuhiro Akiba and 
Eiichiro Sumita. 2004. Paraphrasing as Machine 
Translation. Journal of Natural Language Proc-
essing, 11(5), pp 87-111. 
Pascale Fung and Percy Cheung. 2004. Multi-level 
Bootstrapping for Extracting Parallel Sentences 
from a Quasi-Comparable Corpus. In Proceedings 
of Coling 2004, 1051-1057. 
Shudong Huang, David Graff, and George Dodding-
ton (eds.) 2002. Multiple-Translation Chinese 
Corpus. Linguistic Data Consortium. 
Thorsten Joachims. 2002.  Learning to Classify Text 
Using Support Vector Machines: Methods, Theory, 
and Algorithms. Kluwer Academic Publishers, 
Boston/Dordrecht/London. 
V. Levenshtein. 1966. Binary codes capable of cor-
recting deletions, insertions, and reversals. Soviet 
Physice-Doklady 10: 707-710. 
Microsoft Research Paraphrase Corpus. 
http://research.microsoft.com/research/downloads/
default.aspx 
Franz Joseph Och and H. Ney. 2000. Improved Sta-
tistical Alignment Models.  In Proceedings of the 
38th Annual Meeting of the ACL, Hong Kong, 
China, pp 440-447. 
Franz Joseph Och and Hermann Ney. 2003. A Sys-
tematic Comparison of Various Statistical Align-
ment Models.  Computational Linguistics, 29 (1): 
19-52. 
Bo Pang, Kevin Knight, and Daniel Marcu. 2003. 
Syntax-based Alignment of Multiple Translations: 
Extracting Paraphrases and Generating New Sen-
tences. In Proceedings of NAACL-HLT.
John C. Platt. 1999. Fast Training of Support Vector 
Machines Using Sequential Minimal Optimization. 
In Bernhard Sch?lkopf, Christopher J. C. Burges 
and Alexander J. Smola (eds.). 1999.  Advances in 
Kernel Methods: Support Vector Learning. The 
MIT Press, Cambridge, MA. 185-208. 
Chris Quirk, Chris Brockett, and William B. Dolan. 
2004. Monolingual Machine Translation for Para-
phrase Generation, In Proceedings of the 2004 
Conference on Empirical Methods in Natural 
Language Processing, 25-26 July 2004, Barcelona 
Spain, pp. 142-149. 
Kathy Rooney (ed.) 2001. Encarta Thesaurus.
Bloomsbury Publishing. 
Satoshi Shirai, Kazuhide Yamamoto, Francis Bond & 
Hozumi Tanaka. 2002. Towards a thesaurus of 
predicates. In Proceedings of LREC 2002 (Third 
International Conference on Language Resources 
and Evaluation), (May 29-31, 2002). Vol.6, pp. 
1965-1972. 
Vladimir N. Vapnik. 1995. The Nature of Statistical 
Learning Theory. Springer-Verlag, New York. 
Julie Weeds, David Weir and Bill Keller. 2005. The 
Distributional Similarity of Subparses. In Pro-
ceedings of the ACL Workshop on Empirical 
Modeling of Semantic Equivalence and Entailment.
pp 7-12.  
Dekai Wu. 2005. Recognizing Paraphrases and Tex-
tual Entailment using Inversion Transduction 
Grammars. In Proceedings of the ACL Workshop 
on Empirical Modeling of Semantic Equivalence 
and Entailment. Pp 25-30. 
Yujie Zhang and Kazuhide Yamamoto.  2002 Para-
phrasing of Chinese Utterances.  Proceedings of 
Coling 2002, pp.1163-1169. 
16
Using Contextual Speller Techniques and Language Modeling for 
ESL Error Correction 
Michael Gamon*, Jianfeng Gao*, Chris Brockett*, Alexandre Klementiev+, William 
B. Dolan*, Dmitriy Belenko*, Lucy Vanderwende* 
 
*Microsoft Research 
One Microsoft Way 
Redmond, WA 98052 
{mgamon,jfgao,chrisbkt,billdol, 
dmitryb,lucyv}@microsoft.com 
+Dept. of Computer Science 
University of Illinois 
Urbana, IL 61801 
klementi@uiuc.edu 
 
 
Abstract 
We present a modular system for detection 
and correction of errors made by non-
native (English as a Second Language = 
ESL) writers. We focus on two error types: 
the incorrect use of determiners and the 
choice of prepositions. We use a decision-
tree approach inspired by contextual 
spelling systems for detection and 
correction suggestions, and a large 
language model trained on the Gigaword 
corpus to provide additional information to 
filter out spurious suggestions. We show 
how this system performs on a corpus of 
non-native English text and discuss 
strategies for future enhancements. 
1 Introduction 
English is today the de facto lingua franca for 
commerce around the globe. It has been estimated 
that about 750M people use English as a second 
language, as opposed to 375M native English 
speakers (Crystal 1997), while as much as 74% of 
writing in English is done by non-native speakers. 
However, the errors typically targeted by 
commercial proofing tools represent only a subset 
of errors that a non-native speaker might make. For 
example, while many non-native speakers may 
encounter difficulty choosing among prepositions, 
this is typically not a significant problem for native 
speakers and hence remains unaddressed in 
proofing tools such as the grammar checker in 
Microsoft Word (Heidorn 2000). Plainly there is an 
opening here for automated proofing tools that are 
better geared to the non-native users.  
One challenge that automated proofing tools 
face is that writing errors often present a semantic 
dimension that renders it difficult if not impossible 
to provide a single correct suggestion. The choice 
of definite versus indefinite determiner?a 
common error type among writers with a Japanese, 
Chinese or Korean language background owing to 
the lack of overt markers for definiteness and 
indefiniteness?is highly dependent on larger 
textual context and world knowledge. It seems 
desirable, then, that proofing tools targeting such 
errors be able to offer a range of plausible 
suggestions, enhanced by presenting real-world 
examples that are intended to inform a user?s 
selection of the most appropriate wording in the 
context1. 
2 Targeted Error Types 
Our system currently targets eight different error 
types: 
1. Preposition presence and choice: 
In the other hand, ... (On the other hand ...) 
2. Definite and indefinite determiner presence 
and choice: 
 I am teacher... (am a teacher) 
3. Gerund/infinitive confusion: 
I am interesting in this book. (interested in) 
4. Auxiliary verb presence and choice: 
My teacher does is a good teacher (my teacher 
is...) 
                                                 
1 Liu et al 2000 take a similar approach, retrieving 
example sentences from a large corpus. 
449
5. Over-regularized verb inflection: 
 I writed a letter (wrote) 
6. Adjective/noun confusion: 
 This is a China book (Chinese book) 
7. Word order (adjective sequences and nominal 
compounds): 
I am a student of university (university student) 
8. Noun pluralization: 
 They have many knowledges (much knowledge) 
In this paper we will focus on the two most 
prominent and difficult errors: choice of 
determiner and prepositions. Empirical 
justification for targeting these errors comes from 
inspection of several corpora of non-native writing. 
In the NICT Japanese Learners of English (JLE) 
corpus (Izumi et al 2004), 26.6% of all errors are 
determiner related, and about 10% are preposition 
related, making these two error types the dominant 
ones in the corpus. Although the JLE corpus is 
based on transcripts of spoken language, we have 
no reason to believe that the situation in written 
English is substantially different. The Chinese 
Learners of English Corpus (CLEC, Gui and Yang 
2003) has a coarser and somewhat inconsistent 
error tagging scheme that makes it harder to isolate 
the two errors, but of the non-orthographic errors, 
more than 10% are determiner and number related. 
Roughly 2% of errors in the corpus are tagged as 
preposition-related, but other preposition errors are 
subsumed under the ?collocation error? category 
which makes up about 5% of errors. 
3 Related Work 
Models for determiner and preposition selection 
have mostly been investigated in the context of 
sentence realization and machine translation 
(Knight and Chander 1994, Gamon et al 2002,  
Bond 2005, Suzuki and Toutanova 2006, 
Toutanova and Suzuki 2007). Such approaches 
typically rely on the fact that preposition or 
determiner choice is made in otherwise native-like 
sentences. Turner and Charniak (2007), for 
example, utilize a language model based on a 
statistical parser for Penn Tree Bank data. 
Similarly, De Felice and Pulman (2007) utilize a 
set of sophisticated syntactic and semantic analysis 
features to predict 5 common English prepositions. 
Obviously, this is impractical in a setting where 
noisy non-native text is subjected to proofing. 
Meanwhile, work on automated error detection on 
non-native text focuses primarily on detection of 
errors, rather than on the more difficult task of 
supplying viable corrections (e.g., Chodorow and 
Leacock, 2000). More recently,  Han et al (2004, 
2006) use a maximum entropy classifier to propose 
article corrections in TESOL essays, while Izumi 
et al (2003) and Chodorow et al (2007) present 
techniques of automatic preposition choice 
modeling. These more recent efforts, nevertheless, 
do not attempt to integrate their methods into a 
more general proofing application designed to 
assist non-native speakers when writing English. 
Finally, Yi et al (2008) designed a system that 
uses web counts to determine correct article usage 
for a given sentence, targeting ESL users. 
4 System Description 
Our system consists of three major components: 
1. Suggestion Provider (SP) 
2. Language Model (LM) 
3. Example Provider (EP) 
The Suggestion Provider contains modules for 
each error type discussed in section 2. Sentences 
are tokenized and part-of-speech tagged before 
they are presented to these modules. Each module 
determines parts of the sentence that may contain 
an error of a specific type and one or more possible 
corrections. Four of the eight error-specific 
modules mentioned in section 2 employ machine 
learned (classification) techniques, the other four 
are based on heuristics. Gerund/infinitive 
confusion and auxiliary presence/choice each use a 
single classifier. Preposition and determiner 
modules each use two classifiers, one to determine 
whether a preposition/article should be present, 
and one for the choice of preposition/article. 
All suggestions from the Suggestion Provider 
are collected and passed through the Language 
Model. As a first step, a suggested correction has 
to have a higher language model score than the 
original sentence in order to be a candidate for 
being surfaced to the user. A second set of 
heuristic thresholds is based on a linear 
combination of class probability as assigned by the 
classifier and language model score. 
The Example Provider queries the web for 
exemplary sentences that contain the suggested 
correction. The user can choose to consult this 
information to make an informed decision about 
the correction. 
450
4.1 Suggestion Provider Modules for 
Determiners and Prepositions 
The SP modules for determiner and preposition 
choice are machine learned components. Ideally, 
one would train such modules on large data sets of 
annotated errors and corrected counterparts. Such a 
data set, however, is not currently available. As a 
substitute, we are using native English text for 
training, currently we train on the full text of the 
English Encarta encyclopedia (560k sentences) and 
a random set of 1M sentences from a Reuters news 
data set. The strategy behind these modules is 
similar to a contextual speller as described, for 
example, in (Golding and Roth 1999). For each 
potential insertion point of a determiner or 
preposition we extract context features within a 
window of six tokens to the right and to the left. 
For each token within the window we extract its 
relative position, the token string, and its part-of-
speech tag. Potential insertion sites are determined 
heuristically from the sequence of POS tags. Based 
on these features, we train a classifier for 
preposition choice and determiner choice. 
Currently we train decision tree classifiers with the 
WinMine toolkit (Chickering 2002). We also 
experimented with linear SVMs, but decision trees 
performed better overall and training and 
parameter optimization were considerably more 
efficient. Before training the classifiers, we 
perform feature ablation by imposing a count 
cutoff of 10, and by limiting the number of features 
to the top 75K features in terms of log likelihood 
ratio (Dunning 1993). 
We train two separate classifiers for both 
determiners and preposition: 
? decision whether or not a 
determiner/preposition should be present 
(presence/absence or pa classifier) 
? decision which determiner/preposition is 
the most likely choice, given that a 
determiner/preposition is present (choice 
or ch classifier) 
In the case of determiners, class values for the ch 
classifier are a/an and the. Preposition choice 
(equivalent to the ?confusion set? of a contextual 
speller) is limited to a set of 13 prepositions that 
figure prominently in the errors observed in the 
JLE corpus: about, as, at, by, for, from, in, like, of, 
on, since, to, with, than, "other" (for prepositions 
not in the list). 
The decision tree classifiers produce probability 
distributions over class values at their leaf nodes. 
For a given leaf node, the most likely 
preposition/determiner is chosen as a suggestion. If 
there are other class values with probabilities 
above heuristically determined thresholds2, those 
are also included in the list of possible suggestions. 
Consider the following example of an article-
related error: 
I am teacher from Korea. 
As explained above, the suggestion provider 
module for article errors consists of two classifiers, 
one for presence/absence of an article, the other for 
article choice. The string above is first tokenized 
and then part-of-speech tagged: 
0/I/PRP   1/am/VBP   2/teacher/NN   3/from/IN   
4/Korea/NNP   5/./.  
Based on the sequence of POS tags and 
capitalization of the nouns, a heuristic determines 
that there is one potential noun phrase that could 
contain an article: teacher. For this possible article 
position, the article presence/absence classifier 
determines the probability of the presence of an 
article, based on a feature vector of pos tags and 
surrounding lexical items: 
p(article + teacher) = 0.54 
Given that the probability of an article in this 
position is higher than the probability of not having 
an article, the second classifier is consulted to 
provide the most likely choice of article: 
p(the) = 0.04 
p(a/an) = 0.96 
Given  this probability distribution, a correction 
suggestion I am teacher from Korea -> I am a 
teacher from Korea is generated and passed on to 
evaluation by the language model component. 
4.2 The Language Model 
The language model is a 5-gram model trained 
on the English Gigaword corpus (LDC2005T12). 
In order to preserve (singleton) context information 
as much as possible, we used interpolated Kneser-
Ney smoothing (Kneser and Ney 1995) without 
count cutoff. With a 120K-word vocabulary, the 
trained language model contains 54 million 
bigrams, 338 million trigrams, 801 million 4-grams 
                                                 
2 Again, we are working on learning these thresholds 
empirically from data. 
451
and 12 billion 5-grams.  In the example from the 
previous section, the two alternative strings  of the 
original user input and the suggested correction are 
scored by the language model: 
I am teacher from Korea. score = 0.19 
I am a teacher from Korea. score = 0.60 
The score for the suggested correction is 
significantly higher than the score for the original, 
so the suggested correction is provided to the user. 
4.3 The Example Provider 
In many cases, the SP will produce several 
alternative suggestions, from which the user may 
be able to pick the appropriate correction reliably. 
In other cases, however, it may not be clear which 
suggestion is most appropriate. In this event, the 
user can choose to activate the Example Provider 
(EP) which will then perform a web search to 
retrieve relevant example sentences illustrating the 
suggested correction. For each suggestion, we 
create an exact string query including a small 
window of context to the left and to the right of the 
suggested correction. The query is issued to a 
search engine, and the retrieved results are 
separated into sentences. Those sentences that 
contain the string query are added to a list of 
example candidates.  The candidates are then 
ranked by two initially implemented criteria: 
Sentence length (shorter examples are preferred in 
order to reduce cognitive load) and context overlap 
(sentences that contain additional words from the 
user input are preferred). We have not yet 
performed a user study to evaluate the usefulness 
of the examples provided by the system. Some 
examples of usage that we retrieve are given below 
with the query string in boldface: 
Original: I am teacher from Korea. 
Suggestion: I am a teacher from Korea. 
All top 3 examples: I am a teacher.  
Original: So Smokers have to see doctor more often 
than non-smokers. 
Suggestion: So Smokers have to see a doctor more 
often than non-smokers. 
Top 3 examples: 
1. Do people going through withdrawal have 
to see a doctor? 
2. Usually, a couple should wait to see a 
doctor until after they've tried to get 
pregnant for a year. 
3. If you have had congestion for over a 
week, you should see a doctor. 
Original: I want to travel Disneyland in March. 
Suggestion: I want to travel to Disneyland in 
March. 
Top 3 examples: 
1. Timothy's wish was to travel to 
Disneyland in California. 
2. Should you travel to Disneyland in 
California or to Disney World in 
Florida? 
3. The tourists who travel to Disneyland in 
California can either choose to stay in 
Disney resorts or in the hotel for 
Disneyland vacations. 
5 Evaluation 
We perform two different types of evaluation on 
our system. Automatic evaluation is performed on 
native text, under the assumption that the native 
text does not contain any errors of the type targeted 
by our system. For example, the original choice of 
preposition made in the native text would serve as 
supervision for the evaluation of the preposition 
module. Human evaluation is performed on non-
native text, with a human rater assessing each 
suggestion provided by the system. 
5.1 Individual SP Modules 
For evaluation, we split the original training data 
discussed in section 4.1 into training and test sets 
(70%/30%). We then retrained the classifiers on 
this reduced training set and applied them to the 
held-out test set. Since there are two models, one 
for preposition/determiner presence and absence 
(pa), and one for preposition/determiner choice 
(ch), we report combined accuracy numbers of the 
two classifiers. Votes(a) stands for the counts of 
votes for class value = absence from pa, votes(p) 
stands for counts of votes for presence from pa. 
Acc(pa) is the accuracy of the pa classifier, acc(ch) 
the accuracy of the choice classifier. Combined 
accuracy is defined as in Equation 1. 
 
 
??? ?? ? ?????(?) + ??? ?? ? ??? ?? ? ?????(?)
????? ?????
 
Equation 1: Combined accuracy of the 
presence/absence and choice models 
452
The total number of cases in the test set is 
1,578,342 for article correction and 1,828,438 for 
preposition correction. 
5.1.1 Determiner choice 
Accuracy of the determiner pa and ch models 
and their combination is shown in Table 1. 
Model pa ch combined 
Accuracy 89.61% 85.97% 86.07% 
Table 1: Accuracy of the determiner pa, ch, and 
combined models. 
The baseline is 69.9% (choosing the most 
frequent class label none). The overall accuracy of 
this module is state-of-the-art compared with 
results reported in the literature (Knight and 
Chander 1994, Minnen et al 2000, Lee 2004, 
Turner and Charniak 2007). Turner and Charniak 
2007 obtained the best reported accuracy to date of 
86.74%, using a Charniak language model 
(Charniak 2001) based on a full statistical parser 
on the Penn Tree Bank. These numbers are, of 
course, not directly comparable, given the different 
corpora. On the other hand, the distribution of 
determiners is similar in the PTB (as reported in 
Minnen et al 2000) and in our data (Table 2). 
 PTB Reuters/Encarta 
mix 
no determiner 70.0% 69.9% 
the 20.6% 22.2% 
a/an 9.4% 7.8% 
Table 2: distribution of determiners in the Penn 
Tree Bank and in our Reuters/Encarta data. 
Precision and recall numbers for both models on 
our test set are shown in Table 3 and Table 4. 
Article 
pa classifier 
precision recall 
presence 84.99% 79.54% 
absence 91.43% 93.95% 
Table 3: precision and recall of the article pa 
classifier. 
Article  
ch classifier 
precision Recall 
the 88.73% 92.81% 
a/an 76.55% 66.58% 
Table 4: precision and recall of the article ch 
classifier. 
5.1.2 Preposition choice 
The preposition choice model and the combined 
model achieve lower accuracy than the 
corresponding determiner models, a result that can 
be expected given the larger choice of candidates 
and hardness of the task. Accuracy numbers are 
presented in Table 5. 
Model pa ch combined 
Accuracy 91.06%% 62.32% 86.07% 
Table 5:Accuracy of the preposition pa, ch, and 
combined models. 
The baseline in this task is 28.94% (using no 
preposition). Precision and recall numbers are 
shown in Table 6 and Table 7. From Table 7 it is 
evident that prepositions show a wide range of 
predictability. Prepositions such as than and about 
show high recall and precision, due to the lexical 
and morphosyntactic regularities that govern their 
distribution. At the low end, the semantically more 
independent prepositions since and at show much 
lower precision and recall numbers. 
 
Preposition  
pa classifier 
precision recall 
presence 90.82% 87.20% 
absence 91.22% 93.78% 
Table 6: Precision and recall of the preposition pa 
classifier. 
Preposition 
ch classifier 
precision recall 
other 53.75% 54.41% 
in 55.93% 62.93% 
for 56.18% 38.76% 
of 68.09% 85.85% 
on 46.94% 24.47% 
to 79.54% 51.72% 
with 64.86% 25.00% 
at 50.00% 29.67% 
by 42.86% 60.46% 
as 76.78% 64.18% 
from 81.13% 39.09% 
since 50.00% 10.00% 
about 93.88% 69.70% 
than 95.24% 90.91% 
Table 7: Precision and recall of the preposition ch 
classifier. 
 
453
Chodorow et al (2007) present numbers on an 
independently developed system for detection of 
preposition error in non-native English. Their 
approach is similar to ours in that they use a 
classifier with contextual feature vectors.  The 
major differences between the two systems are the 
additional use of a language model in our system 
and, from a usability perspective, in the example 
provider module we added to the correction 
process. Since both systems are evaluated on 
different data sets3, however, the numbers are not 
directly comparable. 
5.2 Language model Impact 
The language model gives us an additional piece 
of information to make a decision as to whether a 
correction is indeed valid. Initially, we used the 
language model as a simple filter: any correction 
that received a lower language model score than 
the original was filtered out. As a first approxi-
mation, this was an effective step: it reduced the 
number of preposition corrections by 66.8% and 
the determiner corrections by 50.7%, and increased 
precision dramatically. The language model alone, 
however, does not provide sufficient evidence: if 
we produce a full set of preposition suggestions for 
each potential preposition location and rank these 
suggestions by LM score alone, we only achieve 
58.36% accuracy on Reuters data. 
Given that we have multiple pieces of 
information for a correction candidate, namely the 
class probability assigned by the classifier and the 
language model score, it is more effective to 
combine these into a single score and impose a 
tunable threshold on the score to maximize 
precision. Currently, this threshold is manually set 
by analyzing the flags in a development set. 
5.3 Human Evaluation 
A complete human evaluation of our system would 
have to include a thorough user study and would 
need to assess a variety of criteria, from the 
accuracy of individual error detection and 
corrections to the general helpfulness of real web-
based example sentences. For a first human 
evaluation of our system prototype, we decided to 
                                                 
3 Chodorow et al (2007) evaluate their system on 
proprietary student essays from non-native students, 
where they achieve 77.8% precision at 30.4% recall for 
the preposition substitution task. 
simply address the question of accuracy on the 
determiner and preposition choice tasks on a 
sample of non-native text.  
For this purpose we ran the system over a 
random sample of sentences from the CLEC 
corpus (8k for the preposition evaluation and 6k 
for the determiner evaluation). An independent 
judge annotated each flag produced by the system 
as belonging to one of the following categories: 
? (1) the correction is valid and fixes the 
problem 
? (2) the error is correctly identified, but 
the suggested correction does not fix it 
? (3) the original and the rewrite are both 
equally good 
? (4) the error is at or near the suggested 
correction, but it is a different kind of 
error (not having to do with 
prepositions/determiners) 
? (5) There is a spelling error at or near 
the correction 
? (6) the correction is wrong, the original 
is correct 
Table 8 shows the results of this human 
assessment for articles and prepositions. 
 
Articles (6k 
sentences) 
Prepositions 
(8k 
sentences) 
count ratio count ratio 
(1) correction is 
valid 
240 55% 165 46% 
(2) error identified, 
suggestion does 
not fix it 
10 2% 17 5% 
(3) original and 
suggestion equally 
good 
17 4% 38 10% 
(4) misdiagnosis 65 15% 46 13% 
(5) spelling error 
near correction 
37 8% 20 6% 
(6) original correct 70 16% 76 21% 
Table 8: Article and preposition correction 
accuracy on CLEC data. 
The distribution of corrections across deletion, 
insertion and substitution operations is illustrated 
in Table 9. The most common article correction is 
insertion of a missing article. For prepositions, 
substitution is the most common correction, again 
an expected result given that the presence of a 
454
preposition is easier to determine for a non-native 
speaker than the actual choice of the correct 
preposition. 
 deletion insertion substitution 
Articles 8% 79% 13% 
Prepositions 15% 10% 76% 
Table 9: Ratio of deletion, insertion and 
substitution operations. 
6 Conclusion and Future Work 
Helping a non-native writer of English with the 
correct choice of prepositions and 
definite/indefinite determiners is a difficult 
challenge. By combining contextual speller based 
methods with language model scoring and 
providing web-based examples, we can leverage 
the combination of evidence from multiple 
sources. 
The human evaluation numbers presented in the 
previous section are encouraging. Article and 
preposition errors present the greatest difficulty for 
many learners as well as machines, but can 
nevertheless be corrected even in extremely noisy 
text with reasonable accuracy. Providing 
contextually appropriate real-life examples 
alongside with the suggested correction will, we 
believe, help the non-native user reach a more 
informed decision than just presenting a correction 
without additional evidence and information. 
The greatest challenge we are facing is the 
reduction of ?false flags?, i.e. flags where both 
error detection and suggested correction are 
incorrect. Such flags?especially for a non-native 
speaker?can be confusing, despite the fact that the 
impact is mitigated by the set of examples which 
may clarify the picture somewhat and help the 
users determine that they are dealing with an 
inappropriate correction. In the current system we 
use a set of carefully crafted heuristic thresholds 
that are geared towards minimizing false flags on a 
development set, based on detailed error analysis. 
As with all manually imposed thresholding, this is 
both a laborious and brittle process where each 
retraining of a model requires a re-tuning of the 
heuristics. We are currently investigating a learned 
ranker that combines information from language 
model and classifiers, using web counts as a 
supervision signal. 
7 Acknowledgements 
We thank Claudia Leacock (Butler Hill Group) for 
her meticulous analysis of errors and human 
evaluation of the system output, as well as for 
much invaluable feedback and discussion. 
References 
Bond, Francis. 2005.  Translating the Untranslatable: A 
Solution to the Problem of Generating English 
Determiners. CSLI Publications. 
Charniak, Eugene. 2001. Immediate-head parsing for 
language models. In Proceedingsof the 39th Annual 
Meeting of the Association for Computational 
Linguistics, pp 116-123. 
Chickering, David Maxwell. 2002. The WinMine 
Toolkit.  Microsoft Technical Report 2002-103. 
Chodorow, Martin, Joel R. Tetreault and Na-Rae Han. 
2007. Detection of Grammatical Errors Involving 
Prepositions. In Proceedings of the 4th ACL-SIGSEM 
Workshop on Prepositions, pp 25-30. 
Crystal, David. 1997.  Global English. Cambridge 
University Press. 
Rachele De Felice and Stephen G Pulman. 2007. 
Automatically acquiring models of preposition use. 
Proceedings of the ACL-07 Workshop on 
Prepositions. 
Dunning, Ted. 1993. Accurate Methods for the Statistics 
of Surprise and Coincidence. Computational 
Linguistics, 19:61-74. 
Gamon, Michael, Eric Ringger, and Simon Corston-
Oliver. 2002. Amalgam: A machine-learned 
generation module. Microsoft Technical Report, 
MSR-TR-2002-57. 
Golding, Andrew R. and Dan Roth. 1999. A Winnow 
Based Approach to Context-Sensitive Spelling 
Correction. Machine Learning, pp. 107-130. 
Gui, Shicun and Huizhong Yang (eds.). 2003. Zhongguo 
Xuexizhe Yingyu Yuliaohu. (Chinese Learner English 
Corpus). Shanghai Waiyu Jiaoyu Chubanshe.. 
Han, Na-Rae., Chodorow, Martin and Claudia Leacock. 
2004. Detecting errors in English article usage with a 
maximum entropy classifier trained on a large, 
diverse corpus. Proceedings of the 4th international 
conference on language resources and evaluation, 
Lisbon, Portugal. 
 
 
455
Han, Na-Rae. Chodorow, Martin., and Claudia Leacock. 
(2006). Detecting errors in English article usage by 
non-native speakers. Natural Language Engineering, 
12(2), 115-129. 
Heidorn, George. 2000. Intelligent Writing Assistance. 
In Robert Dale, Herman Moisl, and Harold Somers 
(eds.). Handbook of Natural Language Processing.  
Marcel Dekker.  pp 181 -207. 
Izumi, Emi, Kiyotaka Uchimoto and Hitoshi Isahara. 
2004. The NICT JLE Corpus: Exploiting the 
Language Learner?s Speech Database for Research 
and Education. International Journal of the 
Computer, the Internet and Management 12:2, pp 
119 -125. 
Kneser, Reinhard. and Hermann Ney. 1995. Improved 
backing-off for m-gram language modeling. 
Proceedings of the IEEE International Conference 
on Acoustics, Speech, and Signal Processing, volume 
1. 1995. pp. 181?184. 
Knight, Kevin and Ishwar Chander. 1994. Automatic 
Postediting of Documents. Proceedings of the 
American Association of Artificial Intelligence, pp 
779-784. 
Lee, John. 2004. Automatic Article Restoration. 
Proceedings of the Human Language Technology 
Conference of the North American Chapter of the 
Association for Computational Linguistics, pp. 31-
36. 
Liu, Ting, Mingh Zhou, JianfengGao, Endong Xun, and 
Changning Huan. 2000. PENS: A Machine-Aided 
English Writing System for Chinese Users. 
Proceedings of ACL 2000, pp 529-536. 
Minnen, Guido, Francis Bond and Ann Copestake. 
2000. Memory-Based Learning for Article 
Generation. Proceedings of the Fourth Conference 
on Computational Natural Language Learning and 
of the Second Learning Language in Logic 
Workshop, pp 43-48. 
Suzuki, Hisami and Kristina Toutanova. 2006. Learning 
to Predict Case Markers in Japanese. Proceedings of 
COLING-ACL, pp. 1049-1056. 
Toutanova, Kristina and Hisami Suzuki. 2007 
Generating Case Markers in Machine Translation.  
Proceedings of NAACL-HLT. 
Turner, Jenine and Eugene Charniak. 2007. Language 
Modeling for Determiner Selection. In Human 
Language Technologies 2007: The Conference of the 
North American Chapter of the Association for 
Computational Linguistics; Companion Volume, 
Short Papers, pp 177-180. 
Yi, Xing, Jianfeng Gao and William B. Dolan. 2008. 
Web-Based English Proofing System for English as a 
Second Language Users. To be presented at IJCNLP 
2008. 
456
A machine learning approach
to the automatic evaluation of machine translation
Simon Corston-Oliver, Michael Gamon and Chris Brockett
Microsoft Research
One Microsoft Way
Redmond WA 98052, USA
{simonco, mgamon, chrisbkt}@microsoft.com
Abstract
We present a machine learning
approach to evaluating the well-
formedness of output of a machine
translation system, using classifiers that
learn to distinguish human reference
translations from machine translations.
This approach can be used to evaluate
an MT system, tracking improvements
over time; to aid in the kind of failure
analysis that can help guide system
development; and to select among
alternative output strings. The method
presented is fully automated and
independent of source language, target
language and domain.
1 Introduction
Human evaluation of machine translation (MT)
output is an expensive process, often
prohibitively so when evaluations must be
performed quickly and frequently in order to
measure progress. This paper describes an
approach to automated evaluation designed to
facilitate the identification of areas for
investigation and improvement. It focuses on
evaluating the wellformedness of output and
does not address issues of evaluating content
transfer.
Researchers are now applying automated
evaluation in MT and natural language
generation tasks, both as system-internal
goodness metrics and for the assessment of
output. Langkilde and Knight (1998), for
example, employ n-gram metrics to select
among candidate outputs in natural language
generation, while Ringger et al (2001) use n-
gram perplexity to compare the output of MT
systems. Su et al (1992), Alshawi et al (1998)
and Bangalore et al (2000) employ string edit
distance between reference and output sentences
to gauge output quality for MT and generation.
To be useful to researchers, however,
assessment must provide linguistic information
that can guide in identifying areas where work is
required. (See Nyberg et al, 1994 for useful
discussion of this issue.)
The better the MT system, the more its
output will resemble human-generated text.
Indeed, MT might be considered a solved
problem should it ever become impossible to
distinguish automated output from human
translation. We have observed that in general
humans can easily and reliably categorize a
sentence as either machine- or human-generated.
Moreover, they can usually justify their
decision. This observation suggests that
evaluation of the wellformedness of output
sentences can be treated as a classification
problem: given a sentence, how accurately can
we predict whether it has been translated by
machine? In this paper we cast the problem of
MT evaluation as a machine learning
classification task that targets both linguistic
features and more abstract features such as n-
gram perplexity.
2 Data
Our corpus consists of 350,000 aligned Spanish-
English sentence pairs taken from published
computer software manuals and online help
documents. We extracted 200,000 English
sentences for building language models to
evaluate per-sentence perplexity. From the
remainder of the corpus, we extracted 100,000
aligned sentence pairs. The Spanish sentences in
this latter sample were then translated by the
Microsoft machine translation system, which
was trained on documents from this domain
(Richardson et al, 2001). This yielded a set of
200,000 English sentences, one half of which
were English reference sentences, and the other
half of which were MT output. (The Spanish
sentences were not used in building or
evaluating the classifiers). We split the 200,000
English sentences 90/10, to yield 180,000
sentences for training classifiers and 20,000
sentences that we used as held-out test data.
Training and test data were evenly divided
between reference English sentences and
Spanish-to-English translations.
3 Features
The selection of features used in our
classification task was motivated by failure
analysis of system output. We were particularly
interested in those linguistic features that could
aid in qualitative analysis, as we discuss in
section 5. For each sentence we automatically
extracted 46 features by performing a syntactic
parse using the Microsoft NLPWin natural
language processing system (Heidorn, 2000) and
language modeling tools. The features extracted
fall into two broad categories:
(i) Perplexity measures were extracted using the
CMU-Cambridge Statistical Language Modeling
Toolkit (Clarkson and Rosenfeld, 1997). We
calculated two sets of values: lexicalized trigram
perplexity, with values discretized into deciles
and part of speech (POS) trigram perplexity. For
the latter we used the following sixteen POS
tags: adjective, adverb, auxiliary, punctuation,
complementizer, coordinating conjunction,
subordinating conjunction, determiner,
interjection, noun, possessor, preposition,
pronoun, quantifier, verb, and other.
(ii) Linguistic features fell into several
subcategories: branching properties of the parse;
function word density, constituent length, and
other miscellaneous features
We employed a selection of features to
provide a detailed assessment of the branching
properties of the parse tree. The linguistic
motivation behind this was twofold. First, it had
become apparent from failure analysis that MT
system output tended to favor right-branching
structures over noun compounding. Second, we
hypothesized that translation from languages
whose branching properties are radically
different from English (e.g. Japanese, or a verb-
second language like German) might pollute the
English output with non-English characteristics.
For this reason, assessment of branching
properties is a good candidate for a language-
pair independent measure. The branching
features we employed are given below. Indices
are scalar counts; other measures are normalized
for sentence length.
? number of right-branching nodes across
all constituent types
? number of right-branching nodes for
NPs only
? number of left-branching nodes across
all constituent types
? number of left-branching nodes for NPs
only
? number of premodifiers across all
constituent types
? number of premodifiers within NPs only
? number of postmodifiers across all
constituent types
? number of postmodifiers within NPs
only
? branching index across all constituent
types, i.e. the number of right-branching
nodes minus number of left-branching
nodes
? branching index for NPs only
? branching weight index: number of
tokens covered by right-branching
nodes minus number of tokens covered
by left-branching nodes across all
categories
? branching weight index for NPs only
? modification index, i.e. the number of
premodifiers minus the number of
postmodifiers across all categories
? modification index for NPs only
? modification weight index: length in
tokens of all premodifiers minus length
in tokens of all postmodifiers across all
categories
? modification weight index for NPs only
? coordination balance, i.e. the maximal
length difference in coordinated
constituents
We considered the density of function words,
i.e. the ratio of function words to content words,
because of observed problems in WinMT
output. Pronouns received special attention
because of frequent problems detected in failure
analysis. The density features are:
? overall function word density
? density of determiners/quantifiers
? density of pronouns
? density of prepositions
? density of punctuation marks,
specifically commas and semicolons
? density of auxiliary verbs
? density of conjunctions
? density of different pronoun types: Wh,
1st, 2nd, and 3rd person pronouns
We also measured the following constituent
sizes:
? maximal and average NP length
? maximal and average AJP length
? maximal and average PP length
? maximal and average AVP length
? sentence length
On a lexical level, the presence of out of
vocabulary (OOV) words is frequently caused
by the direct transfer of source language words
for which no translation could be found. The
top-level syntactic template, i.e. the labels of the
immediate children of the root node of a
sentence, was also used, as was subject-verb
disagreement. The final five features are:
? number of OOV words
? the presence of a word containing a non-
English letter, i.e. an extended ASCII
character. This is a special case of the
OOV problem.
? label of the root node of the sentence
(declarative, imperative, question, NP,
or "FITTED" for non-spanning parses)
? sentence template, i.e. the labels of the
immediate children of the root node.
? subject-verb disagreement
4 Decision Trees
We used a set of automated tools to construct
decision trees (Chickering et al, 1997) based on
the features extracted from the reference and
MT sentences. To avoid overfitting, we
specified that nodes in the decision tree should
not be split if they accounted for fewer than fifty
cases. In the discussion below we distinguish the
perplexity features from the linguistic features.
4.1 Decision trees built using all
training data
Table 1 gives the accuracy of the decision trees,
when trained on all 180,000 training sentences
and evaluated against the 20,000 held-out test
sentences. Since the training data and test data
contain an even split between reference human
translations and machine translations, the
baseline for comparison is 50.00%. As Table 1
shows, the decision trees dramatically
outperform this baseline. Using only perplexity
features or only linguistic features yields
accuracy substantially above this baseline.
Combining the two sets of features yields the
highest accuracy, 82.89%.
Features used Accuracy (%)
All features 82.89
Perplexity features only 74.73
Linguistic features only 76.51
Table 1 Accuracy of the decision trees
Notably, most of the annotated features
were selected by the decision tree tools. Two
features were found not to be predictive. The
first non-selected feature is the presence of a
word containing an extended ASCII character,
suggesting that general OOV features were
sufficient and subsume the effect of this
narrower feature. Secondly, subject-verb
disagreement was also not predictive, validating
the consistent enforcement of agreement
constraints in the natural language generation
component of the MT system. In addition, only
eight of approximately 5,200 observed sentence
templates turned out to be discriminatory.
For a different use of perplexity in
classification, see Ringger et al (2001) who
compare the perplexity of a sentence using a
language model built solely from reference
translations to the perplexity using a language
model built solely from machine translations.
The output of such a classifier could be used as
an input feature in building decision trees.
Effect of training data size
66
67
68
69
70
71
72
73
74
75
76
77
78
79
80
81
82
83
84
0
10
,00
0
20
,00
0
30
,00
0
40
,00
0
50
,00
0
60
,00
0
70
,00
0
80
,00
0
90
,00
0
10
0,0
00
11
0,0
00
12
0,0
00
13
0,0
00
14
0,0
00
15
0,0
00
16
0,0
00
17
0,0
00
18
0,0
00
Training cases
A
v
g
be
st
ac
cu
ra
cy
All features Perplexity only Linguistic only
Figure 1 Accuracy with varying amounts of training data
4.2 Varying the amount of training data
For our experiments, we had access to several
hundred thousand sentences from the target
domain. To measure the effect of reducing the
size of the training data set on the accuracy of
the classifier, we built classifiers using samples
of the training data and evaluating against the
same held-out sample of 20,000 sentences. We
randomly extracted ten samples containing the
following numbers of sentences: {1,000, 2,000,
3,000, 4,000, 5,000, 6,000, 12,000, 25,000,
50,000, 100,000, 150,000}. Figure 1 shows the
effect of varying the size of the training data.
The data point graphed is the average accuracy
over the ten samples at a given sample size, with
error bars showing the range from the least
accurate decision tree at that sample size to the
most accurate.
As Figure 1 shows, the models built using
only perplexity features do not benefit from
additional training data. The models built using
linguistic features, however, benefit
substantially, with accuracy leveling off after
150,000 training cases. With only 2,000 training
cases, the classifiers built using all features
range in accuracy from 75.06% to 78.84%,
substantially above the baseline accuracy of
50%.
5 Discussion
As the results in section 4 show, it is possible to
build classifiers that can distinguish human
reference translations from the output of a
machine translation system with high accuracy.
We thus have an automatic mechanism that can
perform the task that humans appear to do with
ease, as noted in section 1. The best result, a
classifier with 82.89% accuracy, is achieved by
combining perplexity calculations with a set of
finer-grained linguistic features. Even with as
few as 2,000 training cases, accuracy exceeded
75%. In the discussion below we consider the
advantages and possible uses of this automatic
evaluation methodology.
5.1 Advantages of the approach
Once an appropriate set of features has been
selected and tools to automatically extract those
features are in place, classifiers can be built and
evaluated quickly. This overcomes the two
problems associated with traditional manual
evaluation of MT systems: manual evaluation is
both costly and time-consuming. Indeed, an
automated approach is essential when dealing
with an MT system that is under constant
development in a collaborative research
environment. The output of such a system may
change from day to day, requiring frequent
feedback to monitor progress.
The methodology does not crucially rely on
any particular set of features. As an MT system
matures, more and more subtle cues might be
necessary to distinguish between human and
machine translations. Any linguistic feature that
can be reliably extracted can be proposed as a
candidate feature to the decision tree tools.
The methodology is also not sensitive to the
domain of the training texts. All that is needed
to build classifiers for a new domain is a
sufficient quantity of aligned translations.
5.2 Possible applications of the
approach
The classifiers can be used for evaluating a
system overall, providing feedback to aid in
system development, and in evaluating
individual sentences.
Evaluating an MT system overall
Evaluating the accuracy of the classifier against
held-out data is equivalent to evaluating the
fluency of the MT system. As the MT system
improves, its output will become more like the
human reference translations. To measure
improvement over time, we would hold the set
of features constant and build and evaluate new
classifiers using the human reference
translations and the output of the MT system at a
given point in time. Using the same set of
features, we expect the accuracy of the
classifiers to go down over time as the MT
output becomes more like human translations.
Feedback to aid system development
Our primary interest in evaluating an MT system
is to identify areas that require improvement.
This has been the motivation for using linguistic
features in addition to perplexity measures.
From the point of view of system development,
perplexity is a rather opaque measure. This can
be viewed as both a strength and a weakness. On
the one hand, it is difficult to tune a system with
the express goal of causing perplexity to
improve, rendering perplexity a particularly
good objective measurement. On the other hand,
given a poor perplexity score, it is not clear how
to improve a system without additional failure
analysis.
We used the DNETVIEWER tool (Heckerman
et al, 2000), a visualization tool for viewing
decision trees and Bayesian networks, to explore
the decision trees and identify problem areas in
our MT system. In one visualization, shown in
Figure 2, DNETVIEWER allows the user to adjust
a slider to see the order in which the features
were selected during the heuristic search that
guides the construction of decision trees. The
most discriminatory features are those which
cause the MT translations to look most awful, or
are characteristics of the reference translations
that ought to be emulated by the MT system. For
the coarse model shown in Figure 2, the distance
between pronouns (nPronDist) is the highest
predictor, followed by the number of second
person pronouns (n2ndPersPron), the number of
function words (nFunctionWords), and the
distance between prepositions (nPrepDist).
Using DNETVIEWER we are able to explore
the decision tree, as shown in Figure 3. Viewing
the leaf nodes in the decision tree, we see a
probability distribution over the possible states
of the target variable. In the case of the binary
classifier here, this is the probability that a
sentence will be a reference translation. In
Figure 3, the topmost leaf node shows that
p(Human translation) is low. We modified
DNETVIEWER so that double-clicking on the leaf
node would display reference translations and
MT sentences from the training data. We display
a window showing the path through the decision
tree, the probability that the sentence is a
reference translation given that path, and the
sentences from the training data identified by the
features on the path. This visualization allows
the researcher to view manageable groups of
similar problem sentences with a view to
identifying classes of problems within the
groups. A goal for future research is to select
additional linguistic features that will allow us to
pinpoint problem areas in the MT system and
thereby further automate failure analysis.
Figure 2 Using the slider to view the best predictors
Figure 3 Examining sentences at a leaf node in the decision tree
Figure 4 Examining sentences at a leaf node in the decision tree
Decision trees are merely one form of
classifier that could be used for the automated
evaluation of an MT system. In preliminary
experiments, the accuracy of classifiers using
support vector machines (SVMs) (Vapnik, 1998;
Platt et al, 2000) exceeded the accuracy of the
decision tree classifiers by a little less than one
percentage point using a linear kernel function,
and by a slightly greater margin using a
polynomial kernel function of degree three. We
prefer the decision tree classifiers because they
allow a researcher to explore the classification
system and focus on problem areas and
sentences. We find this method for exploring the
data more intuitive than attempting to visualize
the location of sentences in the high-
dimensional space of the corresponding SVM.
Evaluating individual sentences
In addition to system evaluation and failure
analysis, classifiers could be used on a per-
sentence basis to guide the output of an MT
system by selecting among multiple candidate
strings. If no candidate is judged sufficiently
similar to a human reference translation, the
sentence could be flagged for human post-
editing.
6 Conclusion
We have presented a method for evaluating the
fluency of MT, using classifiers based on
linguistic features to emulate the human ability
to distinguish MT from human translation. The
techniques we have described are system- and
language-independent. Possible applications of
our approach include system evaluation, failure
analysis to guide system development, and
selection among alternative possible outputs.
We have focused on structural aspects of a
text that can be used to evaluate fluency. A full
evaluation of MT quality would of course need
to include measurements of idiomaticity and
techniques to verify that the semantic and
pragmatic content of the source language had
been successfully transferred to the target
language.
Acknowledgements
Our thanks go to Eric Ringger and Max
Chickering for programming assistance with the
tools used in building and evaluating the
decision trees, and to Mike Carlson for help in
sampling the initial datasets. Thanks also to
John Platt for helpful discussion on parameter
setting for the SVM tools, and to the members
of the MSR NLP group for feedback on the uses
of the methodology presented here.
References
Alshawi, H., S. Bangalore, and S. Douglas. 1998.
Automatic acquisition of hierarchical transduction
models for machine translation. In Proceedings of
the 36th Annual Meeting of the Association for
Computational Linguistics, Montreal Canada, Vol.
I: 41-47.
Bangalore, S., O. Rambow, and S. Whittaker. 2000.
Evaluation Metrics for Generation. In Proceedings
of the International Conference on Natural
Language Generation (INLG 2000), Mitzpe
Ramon, Israel. 1-13.
Chickering, D. M., D. Heckerman, and C. Meek.
1997. A Bayesian approach to learning Bayesian
networks with local structure. In Geiger, D. and P.
Punadlik Shenoy (Eds.), Uncertainty in Artificial
Intelligence: Proceedings of the Thirteenth
Conference. 80-89.
Clarkson, P. and R. Rosenfeld. 1997. Statistical
Language Modeling Using the CMU-Cambridge
Toolkit. Proceedings of Eurospeech97. 2707-
2710.
Heckerman, D., D. M. Chickering, C. Meek, R.
Rounthwaite, and C. Kadie. 2000. Dependency
networks for inference, collaborative filtering and
data visualization. Journal of Machine Learning
Research 1:49-75.
Heidorn, G. E., 2000. Intelligent writing assistance.
In R. Dale, H. Moisl and H. Somers (Eds.).
Handbook of Natural Language Processing. New
York, NY. Marcel Dekker. 181-207.
Langkilde, I., and K. Knight. 1998. Generation that
exploits corpus-based statistical knowledge. In
Proceedings of the 36th Annual Meeting of the
Association for Computational Linguistics, and
17th International Conference on Computational
Linguistics, Montreal, Canada. 704-710.
Nyberg, E. H., T. Mitamura, and J. G. Carbonnell.
1994. Evaluation Metrics for Knowledge-Based
Machine Translation. In Proceedings of the 15th
International Conference on Computational
Linguistics, Kyoto, Japan (Coling 94). 95-99.
Platt, J., N. Cristianini, J. Shawe-Taylor. 2000. Large
margin DAGs for multiclass classification. In
Advances in Neural Information Processing
Systems 12, MIT Press. 547-553.
Richardson, S., B. Dolan, A. Menezes, and J.
Pinkham. 2001. Achieving commercial-quality
translation with example-based methods.
Submitted for review.
Ringger, E., M. Corston-Oliver, and R. Moore. 2001.
Using Word-Perplexity for Automatic Evaluation
of Machine Translation. Manuscript.
Su, K., M. Wu, and J. Chang. 1992. A new
quantitative quality measure for machine
translation systems. In Proceedings of COLING-
92, Nantes, France. 433-439.
Vapnik, V. 1998. Statistical Learning Theory, Wiley-
Interscience, New York.
Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the ACL, pages 249?256,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Correcting ESL Errors Using Phrasal SMT Techniques 
 
 Chris Brockett, William B. Dolan, and Michael Gamon 
Natural Language Processing Group 
Microsoft Research  
One Microsoft Way, Redmond, WA 98005, USA  
{chrisbkt,billdol,mgamon}@microsoft.com  
 
  
 
Abstract 
This paper presents a pilot study of the 
use of phrasal Statistical Machine Trans-
lation (SMT) techniques to identify and 
correct writing errors made by learners of 
English as a Second Language (ESL). 
Using examples of mass noun errors 
found in the Chinese Learner Error Cor-
pus (CLEC) to guide creation of an engi-
neered training set, we show that applica-
tion of the SMT paradigm can capture er-
rors not well addressed by widely-used 
proofing tools designed for native speak-
ers. Our system was able to correct 
61.81% of mistakes in a set of naturally-
occurring examples of mass noun errors 
found on the World Wide Web, suggest-
ing that efforts to collect alignable cor-
pora of pre- and post-editing ESL writing 
samples offer can enable the develop-
ment of SMT-based writing assistance 
tools capable of repairing many of the 
complex syntactic and lexical problems 
found in the writing of ESL learners. 
1 Introduction 
Every day, in schools, universities and busi-
nesses around the world, in email and on blogs 
and websites, people create texts in languages 
that are not their own, most notably English. Yet, 
for writers of English as a Second Language 
(ESL), useful editorial assistance geared to their 
needs is surprisingly hard to come by. Grammar 
checkers such as that provided in Microsoft 
Word have been designed primarily with native 
speakers in mind. Moreover, despite growing 
demand for ESL proofing tools, there has been 
remarkably little progress in this area over the 
last decade. Research into computer feedback for 
ESL writers remains largely focused on small-
scale pedagogical systems implemented within 
the framework of CALL (Computer Aided Lan-
guage Learning) (Reuer 2003; Vanderventer 
Faltin, 2003), while commercial ESL grammar 
checkers remain brittle and difficult to customize 
to meet the needs of ESL writers of different 
first-language (L1) backgrounds and skill levels.  
  Some researchers have begun to apply statis-
tical techniques to identify learner errors in the 
context of essay evaluation (Chodorow & Lea-
cock, 2000; Lonsdale & Strong-Krause, 2003), to 
detect non-native text (Tomokiyo & Jones, 2001), 
and to support lexical selection by ESL learners 
through first-language translation (Liu et al, 
2000). However, none of this work appears to 
directly address the more general problem of 
how to robustly provide feedback to ESL writ-
ers?and for that matter non-native writers in 
any second language?in a way that is easily tai-
lored to different L1 backgrounds and second-
language (L2) skill levels.  
In this paper, we show that a noisy channel 
model instantiated within the paradigm of Statis-
tical Machine Translation (SMT) (Brown et al, 
1993) can successfully provide editorial assis-
tance for non-native writers. In particular, the 
SMT approach provides a natural mechanism for 
suggesting a correction, rather than simply 
stranding the user with a flag indicating that the 
text contains an error. Section 2 further motivates 
the approach and briefly describes our SMT sys-
tem. Section 3 discusses the data used in our ex-
periment, which is aimed at repairing a common 
type of ESL error that is not well-handled by cur-
rent grammar checking technology: mass/count 
noun confusions. Section 4 presents experimental 
results, along with an analysis of errors produced 
by the system. Finally we present discussion and 
some future directions for investigation.  
249
2 Error Correction as SMT 
2.1 Beyond Grammar Checking 
A major difficulty for ESL proofing is that errors 
of grammar, lexical choice, idiomaticity, and 
style rarely occur in isolation. Instead, any given 
sentence produced by an ESL learner may in-
volve a complex combination of all these error 
types. It is difficult enough to design a proofing 
tool that can reliably correct individual errors; 
the simultaneous combination of multiple errors 
is beyond the capabilities of current proofing 
tools designed for native speakers. Consider the 
following example, written by a Korean speaker 
and found on the World Wide Web, which in-
volves the misapplication of countability to a 
mass noun:  
 
And I knew many informations 
about Christmas while I was 
preparing this article. 
 
The grammar and spelling checkers in Microsoft 
Word 2003 correctly suggest many ? much 
and informations ? information. 
Accepting these proposed changes, however, 
does not render the sentence entirely native-like. 
Substituting the word much for many leaves 
the sentence stilted in a way that is probably un-
detectable to an inexperienced non-native 
speaker, while the use of the word knew repre-
sents a lexical selection error that falls well out-
side the scope of conventional proofing tools. A 
better rewrite might be: 
 
And I learned a lot of in-
formation about Christmas 
while I was preparing this 
article. 
 
or, even more colloquially: 
 
And I learned a lot about 
Christmas while I was pre-
paring this article 
 
Repairing the error in the original sentence, 
then, is not a simple matter of fixing an agree-
ment marker or substituting one determiner for 
another. Instead, wholesale replacement of the 
phrase knew many informations with 
the phrase learned a lot is needed to pro-
duce idiomatic-sounding output. Seen in these 
terms, the process of mapping from a raw, ESL-
authored string to its colloquial equivalent looks 
remarkably like translation. Our goal is to show 
that providing editorial assistance for writers 
should be viewed as a special case of translation. 
Rather than learning how strings in one language 
map to strings in another, however, ?translation? 
now involves learning how systematic patterns of 
errors in ESL learners? English map to corre-
sponding patterns in native English    
2.2 A Noisy Channel Model of ESL Errors 
If ESL error correction is seen as a translation 
task, the task can be treated as an SMT problem 
using the noisy channel model of (Brown et al, 
1993): here the L2 sentence produced by the 
learner can be regarded as having been corrupted 
by noise in the form of interference from his or 
her L1 model and incomplete language models 
internalized during language learning. The task, 
then, is to reconstruct a corresponding valid sen-
tence of L2 (target). Accordingly, we can seek to 
probabilistically identify the optimal correct tar-
get sentence(s) T* of an ESL input sentence S by 
applying the familiar SMT formula: 
 
( ){ }
{ })P()|P(maxarg
|Pmaxarg*
TTS
STT
T
T
=
=
 
In the context of this model, editorial assis-
tance becomes a matter of identifying those seg-
ments of the optimal target sentence or sentences 
that differ from the writer?s original input and 
displaying them to the user. In practice, the pat-
terns of errors produced by ESL writers of spe-
cific L1 backgrounds can be captured in the 
channel model as an emergent property of train-
ing data consisting ESL sentences aligned with 
their corrected edited counterparts. The highest 
frequency errors and infelicities should emerge 
as targets for replacement, while lesser frequency 
or idiosyncratic problems will in general not sur-
face as false flags. 
2.3 Implementation 
In this paper, we explore the use of a large-scale 
production statistical machine translation system 
to correct a class of ESL errors. A detailed de-
scription of the system can be found in (Menezes 
& Quirk 2005) and (Quirk et al, 2005). In keep-
ing with current best practices in SMT, our sys-
tem is a phrasal machine translation system that 
attempts to learn mappings between ?phrases? 
(which may not correspond to linguistic units) 
rather than individual words. What distinguishes 
250
this system from other phrasal SMT systems is 
that rather than aligning simple sequences of 
words, it maps small phrasal ?treelets? generated 
by a dependency parse to corresponding strings 
in the target. This ?Tree-To-String? model holds 
promise in that it allows us to potentially benefit 
from being able to access a certain amount of 
structural information during translation, without 
necessarily being completely tied to the need for 
a fully-well-formed linguistic analysis of the in-
put?an important consideration when it is 
sought to handle ungrammatical or otherwise ill-
formed ESL input, but also simultaneously to 
capture relationships not involving contiguous 
strings, for example determiner-noun relations.  
In our pilot study, this system was em-
ployed without modification to the system archi-
tecture. The sole adjustment made was to have 
both Source (erroneous) and Target (correct) sen-
tences tokenized using an English language to-
kenizer. N-best results for phrasal alignment and 
ordering models in the decoder were optimized 
by lambda training via Maximum Bleu, along the 
lines described in (Och, 2003).  
3 Data Development 
3.1 Identifying Mass Nouns 
In this paper, we focus on countability errors as-
sociated with mass nouns. This class of errors 
(involving nouns that cannot be counted, such as 
information, pollution, and home-
work) is characteristically encountered in ESL 
writing by native speakers of several East Asian 
languages (Dalgish, 1983; Hua & Lee, 2004).1 
We began by identifying a list of English nouns 
that are frequently involved in mass/count errors 
in by writing by Chinese ESL learners, by taking 
the intersection of words which: 
? occurred in either the Longman Dictionary 
of Contemporary English or the American 
Heritage Dictionary with a mass sense 
? were involved in n ? 2 mass/count errors in 
the Chinese Learner English Corpus 
CLEC (Gui and Yang, 2003), either tagged 
as a mass noun error or else with an adja-
cent tag indicating an article error.2  
                                                 
1
 These constructions are also problematic for hand-
crafted MT systems (Bond et al, 1994). 
2
 CLEC tagging is not comprehensive; some common 
mass noun errors (e.g., make a good progress) 
are not tagged in this corpus. 
This procedure yielded a list of 14 words: 
knowledge, food, homework, fruit, 
news, color, nutrition, equipment, 
paper, advice, haste, information, 
lunch, and tea. 3   Countability errors in-
volving these words are scattered across 46 sen-
tences in the CLEC corpus.   
For a baseline representing the level of writing 
assistance currently available to the average ESL 
writer, we submitted these sentences to the 
proofing tools in Microsoft Word 2003. The 
spelling and grammar checkers correctly identi-
fied 21 of the 46 relevant errors, proposed one 
incorrect substitution (a few advice ? a few 
advices), and failed to flag the remaining 25 
errors. With one exception, the proofing tools 
successfully detected as spelling errors incorrect 
plurals on lexical items that permit only mass 
noun interpretations (e.g., informations), 
but ignored plural forms like fruits and pa-
pers even when contextually inappropriate. The 
proofing tools in Word 2003 also detected singu-
lar determiner mismatches with obligatory plural 
forms (e.g. a news).  
3.2 Training Data 
The errors identified in these sentences provided 
an informal template for engineering the data in 
our training set, which was created by manipulat-
ing well-formed, edited English sentences. Raw 
data came from a corpus of ~484.6 million words 
of Reuters Limited newswire articles, released 
between 1995 and 1998, combined with a 
~7,175,000-word collection of articles from mul-
tiple news sources from 2004-2005. The result-
ing dataset was large enough to ensure that all 
targeted forms occurred with some frequency. 
From this dataset we culled about 346,000 
sentences containing examples of the 14 targeted 
words. We then used hand-constructed regular 
expressions to convert these sentences into 
mostly-ungrammatical strings that exhibited 
characteristics of the CLEC data, for example:  
? much ? many: much advice ? 
many advice  
? some ? a/an: some advice ? 
an advice  
? conversions to plurals: much good 
advice ? many good advices  
                                                 
3
 Terms that also had a function word sense, such as 
will, were eliminated for this experiment.  
251
? deletion of counters: piece(s)/ 
item(s)/sheet(s) of)  
? insertion of determiners  
These were produced in multiple combinations 
for broad coverage, for example: 
 
I'm not trying to give you 
legal advice. ? 
? I'm not trying to give you a 
legal advice. 
? I'm not trying to give you 
the legal advice. 
? I'm not trying to give you 
the legal advices. 
A total of 24128 sentences from the news data 
were ?lesioned? in this manner to create a set of 
65826 sentence pairs. To create a balanced train-
ing set that would not introduce too many arti-
facts of the substitution (e.g., many should not 
always be recast as much just because that is the 
only mapping observed in the training data), we 
randomly created an equivalent number of iden-
tity-mapped pairs from the 346,000 examples, 
with each sentence mapping to itself. 
Training sets of various sizes up to 45,000 
pairs were then randomly extracted from the le-
sioned and non-lesioned pairs so that data from 
both sets occurred in roughly equal proportions.  
Thus the 45K data set contains approximately 
22,500 lesioned examples. An additional 1,000 
randomly selected lesioned sentences were set 
aside for lambda training the SMT system?s or-
dering and replacement models. 
4  Evaluation 
4.1 Test Data 
The amount of tagged data in CLEC is too small 
to yield both development and test sets from the 
same data. In order to create a test set, we had a 
third party collect 150 examples of the 14 words 
from English websites in China. After minor 
cleanup to eliminate sentences irrelevant to the 
task,4 we ended up with 123 example sentences 
to use as test set. The test examples vary widely 
in style, from the highly casual to more formal 
public announcements. Thirteen examples were 
determined to contain no errors relevant to our 
experiment, but were retained in the data.5  
4.2 Results 
Table 1 shows per-sentence results of translating 
the test set on systems built with training data 
sets of various sizes (given in thousands of sen-
tence pairs). Numbers for the proofing tools in 
Word 2003 are presented by way of comparison, 
with the caveat that these tools have been inten-
tionally implemented conservatively so as not to 
potentially irritate native users with false flags. 
For our purposes, a replacement string is viewed 
as correct if, in the view of a native speaker who 
might be helping an ESL writer, the replacement 
would appear more natural and hence potentially 
useful as a suggestion in the context of that sen-
tence taken in isolation. Number disagreement 
on subject and verb were ignored for the pur-
poses of this evaluation, since these errors were 
not modeled when we introduced lesions into the 
data. A correction counted as Whole if the sys-
tem produced a contextually plausible substitu-
tion meeting two criteria: 1) number and 2) de-
terminer/quantifier selection (e.g., many in-
formations ? much information). 
Transformations involving bare singular targets 
(e.g., the fruits ? fruit) also counted 
as Whole.  Partial corrections are those where 
only one of the two criteria was met and part of 
the desired correction was missing (e.g., an 
                                                 
4
 In addition to eliminating cases that only involved 
subject-verb number agreement, we excluded a small 
amount of spam-like word salad, several instances of 
the word homework being misused to mean ?work 
done out of the home?, and one misidentified quota-
tion from Scott?s Ivanhoe. 
5
 This test set may be downloaded at 
http://research.microsoft.com/research/downloads 
Data Size Whole Partial Correctly Left New Error Missed Word Order  Error 
45K 55.28  0.81  8.13  12.20  21.14  1.63  
30K 36.59  4.07  7.32  16.26  32.52  3.25  
15K 47.15  2.44  5.69  11.38  29.27  4.07  
cf. Word 29.27  0.81  10.57  1.63  57.72  N/A 
 
 
Table 1.  Replacement percentages (per sentence basis) using different training data sets  
 
252
equipments ? an equipment versus the 
targeted bare noun equipment). Incorrect sub-
stitutions and newly injected erroneous material 
anywhere in the sentence counted as New Errors, 
even if the proposed replacement were otherwise 
correct. However, changes in upper and lower 
case and punctuation were ignored.  
The 55.28% per-sentence score for Whole 
matches in the system trained on the 45K data set 
means that it correctly proposed full corrections 
in 61.8% of locations where corrections needed 
to be made. The percentage of Missed errors, i.e., 
targeted errors that were ignored by the system, 
is correspondingly low. On the 45K training data 
set, the system performs nearly on a par with 
Word in terms of not inducing corrections on 
forms that did not require replacement, as shown 
in the Correctly Left column.  The dip in accu-
racy in the 30K sentence pair training set is an 
artifact of our extraction methodology: the rela-
tively small lexical set that we are addressing 
here appears to be oversensitive to random varia-
tion in the engineered training data. This makes 
it difficult to set a meaningful lower bound on 
the amount of training data that might be needed 
for adequate coverage. Nonetheless, it is evident 
from the table, that given sufficient data, SMT 
techniques can successfully offer corrections for 
a significant percentage of cases of the phenom-
ena in question.  
Table 2 shows some sample inputs together 
with successful corrections made by the system. 
Table 3 illustrates a case where two valid correc-
tions are found in the 5-best ranked translations; 
intervening candidates were identical with the 
top-ranked candidate.   
4.3 Error Analysis 
Table 1 also indicates that errors associated with 
the SMT system itself are encouragingly few. A 
small number of errors in word order were found, 
one of which resulted in a severely garbled sen-
tence in the 45K data set. In general, the percent-
age of this type of error declines consistently 
with growth of the training data size. Linearity of 
the training data may play a role, since the sen-
tence pairs differ by only a few words. On the 
whole, however, we expect the system?s order 
model to benefit from more training data.  
The most frequent single class of newly intro-
duced error relates to sporadic substitution of the 
word their for determiners a/the. This is 
associated with three words, lunch, tea, and 
haste, and is the principal contributor to the 
lower percentages in the Correctly Left bin, as 
compared with Word. This overgeneralization 
error reflects our attempt to engineer the discon-
tinuous mapping the X of them ? their 
X, motivated by examples like the following, 
encountered in the CLEC dataset: 
Input Shanghai residents can buy the fruits for a cheaper price than before.  
Replacement Shanghai residents can buy fruit for a cheaper price than before . 
Input Thank u for giving me so many advice. 
Replacement thank u for giving me so much advice . 
Input Acquiring the knowledge of information warfare is key to 
winning wars 
Replacement acquiring knowledge of information warfare is key to win-
ning wars 
Input Many knowledge about Li Bai can be gain through it. 
Replacement much knowledge about Li Bai can be gain through it . 
Input I especially like drinking the tea. 
Replacement i especially like drinking tea . 
Input Icons printed on a paper have been brought from Europe, 
and were pasted on boards on Taiwan. 
Replacement icons printed on paper have been brought from Europe , and 
were pasted on boards on Taiwan . 
 
Table 2.  Sample corrections, using 45K engineered training data 
253
In this equal world, lots of 
people are still concerned 
on the colors of them ? 
 
The inability of our translation system to handle 
such discontinuities in a unitary manner reflects 
the limited ability of current SMT modeling 
techniques to capture long-distance effects. Simi-
lar alternations are rife in bilingual data, e.g., 
ne?pas in French (Fox, 2002) and separable 
prefixes in German (Collins et al 2005). As 
SMT models become more adept at modeling 
long-distance effects in a principled manner, 
monolingual proofing will benefit as well. 
The Missed category is heterogeneous. The 
SMT system has an inherent bias against deletion, 
with the result that unwanted determiners tended 
not to be deleted, especially in the smaller train-
ing sets.  
Other errors related to coverage in the devel-
opment data set. Several occurrences of green-
grocer?s apostrophes (tea?s, equipment?s) 
caused correction failures: these were not antici-
pated when engineering the training data. Like-
wise, the test data presented several malformed 
quantifiers and quantifier-like phrases (plenty 
tea ? plenty of tea, a lot infor-
mation ? a lot of information, 
few information ? too little in-
formation) that had been unattested in the 
development set. Examples such as these high-
light the difficulty in obtaining complete cover-
age when using handcrafted techniques, whether 
to engineer errors, as in our case, or to handcraft 
targeted correction solutions.    
The system performed poorly on words that 
commonly present both mass and count noun 
senses in ways that are apt to confuse L2 writers. 
One problematic case was paper. The follow-
ing sentences, for example, remained uncor-
rected:  
  
He published many paper in 
provincial and national pub-
lication. 
He has published thirty-two 
pieces of papers. 
 
Large amounts of additional training data 
would doubtless be helpful in providing contex-
tual resolutions to the problems. Improved 
alignment models may also play a role here in 
capturing complex structures of the kind repre-
sented by constructions involving counters.     
5 Discussion 
The artificially-engineered training data that we 
relied on for our experiments proved surprisingly 
useful in modeling real errors made by non-
native speakers. However, this is obviously a less 
than ideal data source, since the errors introduced 
by regular expressions are homogenously dis-
tributed in a way that naturally-occurring errors 
are not, creating artifacts that undoubtedly impair 
our SMT models.  
Artificial data of this sort may be useful as 
proof of concept, but hand engineering such data 
plainly does not present a viable path to develop-
ing real world applications. In order to be able to 
handle the rich panoply of errors and error inter-
actions encountered in the text of second lan-
guage learners large quantities of naturally-
occurring ?before? and ?after? texts will need to 
be collected. By way of illustration, Table 4 
shows the output of results of ?translating? our 
test data into more natural English by hand and 
dumping the pre- and post-editing pairs to the 
45K training set.6 Although we were unable to 
exactly recover the target sentences, inspection 
showed that 25 sentences had improved, some 
significantly, as Table 4 shows. Under the right 
conditions, the SMT system can capture contex-
tual morphological alternations (nutri-
tion/nutritious), together with complex 
mappings represented by the dependencies 
learn ? knowledge ? many (ESL) and 
                                                 
6
 Since a single example of each pair was insufficient 
to override the system?s inherent bias towards uni-
gram mappings, 5 copies of each pair were appended 
to the training data. 
Input: And we can learn many knowledge or new information from TV 
Candidate 1: And we can learn much knowledge or new information from TV 
Candidate 5: And we can learn a lot of knowledge or new information from TV 
 
Table 3.  Multiple replacement candidates generated by 45K training set 
 
254
gain ? knowledge ? a lot of (Eng-
lish). In a rule-based correction system, an im-
mense amount of hand-coding would be required 
to handle even a small subset of the potential 
range of such mismatches between learner and 
native-like English. This knowledge, we believe, 
is best acquired from data.  
5.1 The Need for Data Collection 
Given a sufficiently large corpus of aligned sen-
tences containing error patterns produced by ESL 
writers of the same L1 background and their cor-
rected counterparts we expect eventually to be 
able to capture the rich complexity of non-native 
error within a noisy-channel based SMT model.  
As a practical matter, however, parallel data of 
the kind needed is far from easy to come by. This 
does not mean, however, that such data does not 
exist. The void left by commercial grammar 
checkers is filled, largely unobserved, by a num-
ber of services that provide editorial assistance, 
ranging from foreign language teachers, to lan-
guage helpdesks in multinational corporations, to 
mentoring services for conferences. Translation 
bureaus frequently offer editing services for non-
native speakers. Yet, unlike translation, the ?be-
fore? and ?after? texts are rarely recycled in a 
form that can be used to build translation models. 
Although collecting this data will involve a large 
investment in time, effort, and infrastructure, a 
serious effort along these lines is likely to prove 
fruitful in terms of making it possible to apply 
the SMT paradigm to ESL error correction.  
5.2 Feedback to SMT 
One challenge faced by the SMT model is the 
extremely high quality that will need to be at-
tained before a system might be usable. Since it 
is highly undesirable that learners should be pre-
sented with inaccurate feedback that they may 
not have the experience or knowledge to assess, 
the quality bar imposed on error correction is far 
higher than is that tolerated in machine transla-
tion. Exploration of error correction and writing 
assistance using SMT models may thus prove an 
important venue for testing new SMT models. 
5.3 Advantages of the SMT Approach 
Statistical Machine Translation has provided a 
hugely successful research paradigm within the 
field of natural language processing over the last 
decade. One of the major advantages of using 
SMT in ESL writing assistance is that it can be 
expected to benefit automatically from any pro-
gress made in SMT itself. In fact, the approach 
presented here benefits from all the advantages 
of statistical machine translation. Since the archi-
tecture is not dependent on hard-to-maintain 
rules or regular expressions, little or no linguistic 
expertise will be required in developing and 
maintain applications. As with SMT, this exper-
tise is pushed into the data component, to be 
handled by instructors and editors, who do not 
need programming or scripting skills.  
We expect it to be possible, moreover, once 
parallel data becomes available, to quickly ramp 
up new systems to accommodate the needs of 
Input sentence And we can learn many knowledge or new information from 
TV. 
45K system output and we can learn much knowledge or new information from 
TV . 
45K + translation sys-
tem output 
we can gain a lot of knowledge or new information from 
TV . 
Input sentence The following is one of the homework for last week. 
45K system output the following is one of their homework for last week . 
45K + translation sys-
tem output 
the following is one of the homework assignments for 
last week . 
Input sentence i like mushroom,its very nutrition 
45K system output i like mushroom , its very nutrition 
45K + translation sys-
tem output i like mushroom , its very nutritious 
 
Table 4.  Contextual corrections before and after adding ?translations? to 45K training data 
255
learners with different first-language back-
grounds and different skill levels and to writing 
assistance for learners of L2s other than English. 
It is also likely that this architecture may have 
applications in pedagogical environments and as 
a tool to assist editors and instructors who deal 
regularly with ESL texts, much in the manner of 
either Human Assisted Machine Translation or 
Machine Assisted Human Translation. We also 
believe that this same architecture could be ex-
tended naturally to provide grammar and style 
tools for native writers.  
6 Conclusion and Future Directions 
In this pilot study we have shown that SMT tech-
niques have potential to provide error correction 
and stylistic writing assistance to L2 learners. 
The next step will be to obtain a large dataset of 
pre- and post-editing ESL text with which to 
train a model that does not rely on engineered 
data. A major purpose of the present study has 
been to determine whether our hypothesis is ro-
bust enough to warrant the cost and effort of a 
collection or data creation effort.  
Although we anticipate that it will take a sig-
nificant lead time to assemble the necessary 
aligned data, once a sufficiently large corpus is 
in hand, we expect to begin exploring ways to 
improve our SMT system by tailoring it more 
specifically to the demands of editorial assistance. 
In particular, we expect to be looking into alter-
native word alignment models and possibly en-
hancing our system?s decoder using some of the 
richer, more structured language models that are 
beginning to emerge. 
Acknowledgements 
The authors have benefited extensively from dis-
cussions with Casey Whitelaw when he interned 
at Microsoft Research during the summer of 
2005. We also thank the Butler Hill Group for 
collecting the examples in our test set.   
References 
Bond, Francis, Kentaro Ogura and Satoru Ikehara. 
1994. Countability and Number in Japanese-to-
English Machine Translation. COLING-94. 
Peter E Brown, Stephen A. Della Pietra, Robert L. 
Mercer, and Vincent J. Della Pietra. 1993. The 
Mathematics of Statistical Machine Translation. 
Computational Linguistics, Vol. 19(2): 263-311.  
Martin Chodorow and Claudia Leacock. 2000. An 
Unsupervised Method for Detecting Grammatical 
Errors. NAACL 2000.  
Michael Collins, Philipp Koehn and Ivona Ku?erov?. 
2005. Clause Restructuring for Statistical machine 
Translation. ACL 2005, 531-540.  
Gerard M. Dalgish. 1984. Computer-Assisted ESL 
Research. CALICO Journal.  2(2): 32-33 
Heidi J. Fox.  2002. Phrasal Cohesion and Statistical 
Machine Translation. EMNLP 2002. 
Shicun Gui and Huizhong Yang (eds). 2003 Zhong-
guo Xuexizhe Yingyu Yuliaohu. (Chinese Learner 
English Corpus). Shanghai: Shanghai Waiyu 
Jiaoyu Chubanshe. (In Chinese). 
Hua Dongfan and Thomas Hun-Tak Lee. 2004.  Chi-
nese ESL Learners' Understanding of the English 
Count-Mass Distinction. In Proceedings of the 7th 
Generative Approaches to Second Language Ac-
quisition Conference (GASLA 2004). 
Ting Liu, Ming Zhou, Jianfeng Gao, Endong Xun, 
and Changning Huang. 2000. PENS: A Machine-
aided English Writing System for Chinese Users. 
ACL 2000.  
Deryle Lonsdale and Diane Strong-Krause. 2003.  
Automated Rating of ESL Essays. In Proceedings 
of the HLT/NAACL Workshop: Building Educa-
tional Applications Using Natural Language Proc-
essing.   
Arul Menezes, and Chris Quirk. 2005. Microsoft Re-
search Treelet Translation System: IWSLT Evalua-
tion. Proceedings of the International Workshop on 
Spoken Language Translation.  
Franz Josef Och, 2003. Minimum error rate training 
in statistical machine translation. ACL 2003. 
Franz Josef Och and Hermann Ney. 2000. Improved 
Statistical Alignment Models.  ACL 2000. 
Chris Quirk, Arul Menezes, and Colin Cherry. 2005. 
Dependency Tree Translation: Syntactically In-
formed Phrasal SMT. ACL 2005. 
Veit Reuer. 2003. Error Recognition and Feedback 
with Lexical Functional Grammar. CALICO Jour-
nal, 20(3): 497-512. 
Laura Mayfield Tomokiyo and Rosie Jones. 2001. 
You?re not from round here, are you? Naive Bayes 
Detection of Non-Native Utterance Text. NAACL 
2001. 
Anne Vandeventer Faltin. 2003. Natural language 
processing tools for computer assisted language 
learning. Linguistik online 17, 5/03 (http:// 
www.linguistik-online.de/17_03/vandeventer.html) 
 
256
English-Japanese Example-Based Machine Translation Using Abstract 
Linguistic Representations 
 
Chris Brockett, Takako Aikawa, Anthony Aue, Arul Menezes, Chris Quirk  
and Hisami Suzuki 
Natural Language Processing Group, Microsoft Research 
One Microsoft Way 
Redmond, WA 98052, USA 
{chrisbkt,takakoa,anthaue,arulm,chrisq,hisamis}@microsoft.com 
 
 
Abstract 
This presentation describes an example- 
based English-Japanese machine trans- 
lation system in which an abstract 
linguistic representation layer is used to 
extract and store bilingual translation 
knowledge, transfer patterns between 
languages, and generate output strings. 
Abstraction permits structural neutral-
izations that facilitate learning of trans-
lation examples across languages with 
radically different surface structure charac-
teristics, and allows MT development to 
proceed within a largely language- 
independent NLP architecture. Com-
parative evaluation indicates that after 
training in a domain the English-Japanese 
system is statistically indistinguishable 
from a non-customized commercially 
available MT system in the same domain. 
Introduction 
In the wake of the pioneering work of Nagao 
(1984), Brown et al (1990) and Sato and 
Nagao (1990), Machine Translation (MT) 
research has increasingly focused on the issue 
of how to acquire translation knowledge from 
aligned parallel texts. While much of this 
research effort has focused on acquisition of 
correspondences between individual lexical 
items or between unstructured strings of words, 
closer attention has begun to be paid to the 
learning of structured phrasal units: Yamamoto 
and Matsumoto (2000), for example, describe a 
method for automatically extracting correspon-
dences between dependency relations in 
Japanese and English. Similarly, Imamura 
(2001a, 2001b) seeks to match corresponding 
Japanese and English phrases containing 
information about hierarchical structures, 
including partially completed parses. 
 
Yamamoto and Matsumoto (2000) explicitly 
assume that dependency relations between 
words will generally be preserved across 
languages. However, when languages are as 
different as Japanese and English with respect 
to their syntactic and informational structures, 
grammatical or dependency relations may not 
always be preserved: the English sentence ?the 
network failed? has quite a different 
grammatical structure from its Japanese 
translation equivalent ??????????
???? ?a defect arose in the network.? One 
issue for example-based MT, then, is to capture 
systematic divergences through generic 
learning applicable to multiple language pairs. 
 
In this presentation we describe the MSR-MT 
English-Japanese system, an example-based 
MT system that learns structured phrase-sized 
translation units. Unlike the systems discussed 
in Yamamoto and Matsumoto (2000) and 
Imamura (2001a, 2001b), MSR-MT places the 
locus of translation knowledge acquisition at a 
greater level of abstraction than surface 
relations, pushing it into a semantically- 
motivated layer called LOGICAL FORM (LF) 
(Heidorn 2000; Campbell & Suzuki 2002a, 
2002b). Abstraction has the effect of 
neutralizing (or at least minimizing) differences 
in word order and syntactic structure, so that 
mappings between structural relations 
associated with lexical items can readily be 
acquired within a general MT architecture. 
 
In Section 1 below, we present an overview of 
the characteristics of the system, with special 
reference to English-Japanese MT. Section 2 
discusses a class of structures learned through 
phrase alignment, Section 3 presents the results 
of comparative evaluation, and Section 4 some 
factors that contributed to the evaluation results. 
Section 5 addresses directions for future work. 
 
  
1 The MSR-MT System 
The MSR-MT English-Japanese system is a 
hybrid example-based machine translation 
system that employs handcrafted broad- 
coverage augmented phrase structure grammars 
for parsing, and statistical and heuristic 
techniques to capture translation knowlege and 
for transfer between languages. The parsers are 
general purpose: the English parser, for 
example, forms the core of the grammar 
checkers used in Microsoft Word (Heidorn 
2000). The Japanese grammar utilizes much of 
the same codebase, but contains language- 
specific grammar rules and additional features 
owing to the need for word-breaking in 
Japanese (Suzuki et al 2000; Kacmarcik et al 
2000). These parsers are robust in that if the 
analysis grammar fails to find an appropriate 
parse, it outputs a best-guess ?fitted? parse. 
 
System development is not confined to 
English-Japanese: MSR-MT is part of a 
broader natural language processing project 
involving three Asian languages (Japanese, 
Chinese, and Korean) and four European 
languages (English, French, German, and 
Spanish). Development of the MSR-MT 
systems proceeds more or less simultaneously 
across these languages and in multiple 
directions, including Japanese-English. The 
Spanish-English version of MSR-MT has been 
described in Richardson et al 2001a, Richardson 
et al2001b, and the reader is referred to these 
papers for more information concerning 
algorithms employed during phrase alignment. 
A description of the French-Spanish MT 
system is found in Pinkham & Smets. 2002. 
 
1.1 Training Data 
MSR-MT requires that a large corpus of 
aligned sentences be available as examples for 
training. For English-Japanese MT, the system 
currently trains on a corpus of approximately 
596,000 pre-aligned sentence pairs. About 
274,000 of these are sentence pairs extracted 
from Microsoft technical documentation that 
had been professionally translated from 
English into Japanese. The remaining 322,000 
are sentence examples or sentence fragments 
extracted from electronic versions of student 
dictionaries.1  
1.2  Logical Form 
MSR-MT employs a post-parsing layer of 
semantic representation called LOGICAL FORM 
(LF) to handle core components of the 
translation process, namely acquisition and 
storage of translation knowledge, transfer 
between languages, and generation of target 
output. LF can be viewed as a representation of 
the various roles played by the content words 
after neutralizing word order and local 
morphosyntactic variation (Heidorn 2000; 
Campbell & Suzuki 2002a; 2002b). These can 
be seen in the Tsub (Typical Subject) and Tobj 
(Typical Object) relations in Fig. 1 in the 
sentence ?Mary eats pizza? and its Japanese 
counterpart. The graphs are simplified for 
expository purposes. 
Although our hypothesis is that equivalent 
sentences in two languages will tend to 
resemble each other at LF more than they do in 
the surface parse, we do not adopt a na?ve 
reductionism that would attempt to make LFs 
completely identical. In Fig. 2, for example, the 
LFs of the quantified nouns differ in that the 
Japanese LF preserves the classifier, yet are 
similar enough that learning the mapping 
between the two structures is straightforward. 
It will be noted that since the LF for each 
language stores words or morphemes of that 
language, this level of representation is not in 
any sense an interlingua. 
 
                                                   
1 Kodansha?s Basic English-Japanese Dictionary, 
1999; Kenkyusha?s New College Japanese-English 
Dictionary, 4th Edition, 1995 ; and Kenkyusha?s 
New College English-Japanese Dictionary, 6th 
Edition, 1994. 
 
 
Fig. 1  Canonical English and Japanese 
Logical Forms 
 
 
1.3  Mapping Logical Forms 
In the training phase, MSR-MT learns transfer 
mappings from the sentence-aligned bilingual 
corpus. First, the system deploys the 
general-purpose parsers to analyze the English 
and Japanese sentence pairs and generate LFs 
for each sentence. In the next step, an LF 
alignment algorithm is used to match source 
language and target language LFs at the 
sub-sentence level. 
 
The LF alignment algorithm first establishes 
tentative lexical correspondences between 
nodes in the source and target LFs on the basis 
of lexical matching over dictionary information 
and approximately 31,000 ?word associations,? 
that is, lexical mappings extracted from the 
training corpora using statistical techniques 
based on mutual information (Moore 2001). 
From these possible lexical correspondences, 
the algorithm uses a small grammar of 
(language-pair-independent) rules to align LF 
nodes on lexical and structural principles. The 
aligned LF pairs are then partitioned into 
smaller aligned LF segments, with individual 
node mappings captured in a relationship we 
call ?sublinking.? Finally, the aligned LF 
segments are filtered on the basis of frequency, 
and compiled into a database known as a 
Mindnet. (See Menezes & Richardson 2001 for a 
detailed description of this process.) 
 
The Mindnet is a general-purpose database of 
semantic information (Richardson et al 1998) 
that has been repurposed as the primary 
repository of translation information for MT 
applications. The process of building the 
Mindnet is entirely automated; there is no 
human vetting of candidate entries. At the end 
of a typical training session, 1,816,520 transfer 
patterns identified in the training corpus may 
yield 98,248 final entries in the Mindnet. Only 
the output of successful parses is considered 
for inclusion, and each mapping of LF 
segments must have been encountered twice in 
the corpus before it is incorporated into the 
Mindnet. 
 
In the Mindnet, LF segments from the source 
language are represented as linked to the 
corresponding LF segment from the target 
languages. These can be seen in Figs. 3 and 4, 
discussed below in Section 2. 
1.4  Transfer and Generation 
At translation time, the broad-coverage source 
language parser processes the English input 
sentence, and creates a source-language LF. 
This LF is then checked against the Mindnet 
entries. 2  The best matching structures are 
extracted and stitched together determinist-
ically into a new target-language ?transferred 
LF? that is then submitted to the Japanese 
system for generation of the output string. 
 
The generation module is language-specific 
and used for both monolingual generation and 
MT. In the context of MT, generation takes as 
input the transferred LF and converts it into a 
basic syntactic tree. A small set of heuristic 
rules preprocesses the transferred LF to 
?nativize? some structural differences, such as 
pro-drop phenomena in Japanese. A series of 
core generation rules then applies to the LF tree, 
transforming it into a Japanese sentence string. 
Generation rules operate on a single tree only, 
are application-independent and are developed 
in a monolingual environment (see Aikawa et 
al. 2001a, 2001b for further details.) 
Generation of inflectional morphology is also 
handled in this component. The generation 
component has no explicit knowledge of the 
source language. 
 
2 Acquisition of Complex Structural 
Mappings 
The generalization provided by LF makes it 
possible for MSR-MT to handle complex 
structural relations in cases where English and 
Japanese are systematically divergent. This is 
                                                   
2 MSR-MT resorts to lexical lookup only when a 
term is not found in the Mindnet. The handcrafted 
dictionary is slated for replacement by purely 
statistically generated data.  
 
 
Fig. 2  Cross-Linguistic Variation in Logical 
Form 
 
illustrated by the sample training pair in the 
lefthand column of Table 1. In Japanese, 
inanimate nouns tend to be avoided as subjects 
of transitive verbs; the word ?URL?, which is 
subject in the English sentence, thus 
corresponds to an oblique relation in the 
Japanese. (The Japanese sentence, although a 
natural and idiomatic translation of the English,  
is literally equivalent to ?one can access public 
folders with this URL.?)   
 
Nonetheless, mappings turn out to be learnable 
even where the information is structured so 
radically differently. Fig. 3 shows the Mindnet 
entry for ?provide,? which is result of training 
on sentence pairs like those in the lefthand 
column of Table 1. The system learns not only 
the mapping between the phrase ?provide 
access? and the potential form of ???? 
?access?, but also the crucial sublinking of the 
Tsub node of the English sentence and the node 
headed by ?  (underspecified for semantic 
role) in the Japanese. At translation time the 
system is able to generalize on the basis of the 
functional roles stored in the Mindnet; it can 
substitute lexical items to achieve a relatively 
natural translation of similar sentences such as 
shown in the right-hand side of Table 1.  
Differences of the kind seen in Fig 3 are 
endemic in our Japanese and English corpora. 
Fig. 4 shows part of the example Mindnet entry 
for the English word ?fail? referred to in the 
Introduction, which exhibits another mismatch 
in grammatical roles somewhat similar to that 
in observed in Fig. 3. Here again, the lexical 
matching and generic alignment heuristics have 
allowed the match to be captured into the 
Mindnet. Although the techniques employed 
may have been informed by analysis of 
language-specific data, they are in principle of 
general application. 
 
 
3 Evaluation 
In May 2002, we compared output of the 
MSR-MT English-Japanese system with a 
commercially available desktop MT system.3 
                                                   
3 Toshiba?s The Honyaku Office v2.0 desktop MT 
system was selected for this purpose. The Honyaku 
is a trademark of the Toshiba Corporation. Another 
desktop system was also considered for evaluation; 
however, comparative evaluation with that system 
indicated that the Toshiba system performed 
marginally, though not significantly, better on our 
technical documentation.  
 
Training Data Translation Output  
This URL provides access to public folders. 
 
This computer provides access to the internet. 
 
?? URL ?????? ????? 
????????? 
????????????????? 
????????? 
 
Table 1.  Sample Input and Output 
Fig. 3.  Part of the Mindnet Entry for ?provide? 
 
 
Fig. 4.  Part of the Mindnet Entry for ?fail? 
 
A total of 238 English-Japanese sentence pairs 
were randomly extracted from held-out 
software manual data of the same kinds used 
for training the system. 4  The Japanese 
sentences, which had been translated by human 
translators, were taken as reference sentences 
(and were assumed to be correct translations). 
The English sentences were then translated by 
the two MT systems into Japanese for blind 
evaluation performed by seven outside vendors 
unfamiliar with either system?s characteristics. 
 
No attempt was made to constrain or modify 
the English input sentences on the basis of 
length or other characteristics. Both systems 
provided a translation for each sentence.5  
 
For each of the Japanese reference sentences, 
evaluators were asked to select which 
translation was closer to the reference sentence. 
A value of +1 was assigned if the evaluator 
considered MSR-MT output sentence better 
and ?1 if they considered the comparison 
system better. If two translated sentences were 
considered equally good or bad in comparison 
                                                   
4  250 sentences were originally selected for 
evaluation; 12 were later discarded when it was 
discovered by evaluators that the Japanese reference 
sentences (not the input sentences) were defective 
owing to the presence of junk characters (mojibake) 
and other deficiencies.  
5 In MSR-MT, Mindnet coverage is sufficiently 
complete with respect to the domain that an 
untranslated sentence normally represents a 
complete failure to parse the input, typically owing 
to excessive length. 
to the reference, a value of 0 was assigned. On 
this metric, MSR-MT scored slightly worse 
than the comparison system rating of ?0.015. 
At a two-way confidence measure of +/?0.16, 
the difference between the systems is 
statistically insignificant. By contrast, an 
earlier evaluation conducted in October 2001 
yielded a score of ?0.34 vis-?-vis the 
comparison system. 
 
In addition, the evaluators were asked to rate 
the translation quality on an absolute scale of 1 
through 4, according to the following criteria: 
 
1. Unacceptable: Absolutely not comprehen- 
sible and/or little or no information trans- 
ferred accurately. 
2. Possibly Acceptable: Possibly compre- 
hensible (given enough context and/or 
time to work it out); some information 
transferred accurately. 
3. Acceptable: Not perfect, but definitely 
comprehensible, and with accurate transfer 
of all important information. 
4. Ideal: Not necessarily a perfect translation, 
but grammatically correct, and with all 
information accurately transferred. 
 
On this absolute scale, neither system 
performed exceptionally well: MSR-MT scored 
an average 2.25 as opposed to 2.32 for the 
comparison system. Again, the difference 
between the two is statistically insignificant. It 
should be added that the comparison presented 
here is not ideal, since MSR-MT was trained 
principally on technical manual sentences, 
 Evaluation 
Date 
Transfers 
per Sentence 
Nodes  
Per Transfer
 
 Oct. 2001 5.8 1.6  
 May 2002 6.7 2.0  
Table 2. Number of Transfers and Nodes Transferred per Sentence 
 
 Evaluation Date Word Class Total From 
Mindnet 
From 
Dictionary
Untranslated  
 Prepositions 410 17.1% 77.1% 5.9%  
 
Oct. 2001  
(250 sentences) Content Lemmas 2124 88.4% 7.8% 3.9%  
 Prepositions 842 61.9% 37.5% 0.6%  
 
May 2002 
(520 sentences) Content Lemmas 4429 95.9% 1.5% 2.6%  
Table 3.  Sources of Different Word Classes at Transfer 
 
while the comparison system was not 
specifically tuned to this corpus. Accordingly 
the results of the evaluation need to be 
interpreted narrowly, as demonstrating that:  
l  A viable example-based English-Japanese 
MT system can be developed that applies 
general-purpose alignment rules to semantic 
representations; and  
l  Given general-purpose grammars, a 
representation of what the sentence means, 
and suitable learning techniques, it is 
possible to achieve in a domain, results 
analogous with those of a mature 
commercial product, and within a relatively 
short time frame. 
4 Discussion 
It is illustrative to consider some of the factors 
that contributed to these results. Table 2 shows 
the number of transfers per sentence and the 
number of LF nodes per transfer in versions of 
the system evaluated in October 2001 and May 
2002. Not only is the MSR-MT finding more 
LF segments in the Mindnet, crucially the 
number of nodes transferred has also grown. 
An average of two connected nodes are now 
transferred with each LF segment, indicating 
that the system is increasingly learning its 
translation knowledge in terms of complex 
structures rather than simple lexical 
correspondences. 
 
It has been our experience that the greater 
MSR-MT?s reliance on the Mindnet, the better 
the quality of its output. Table 2 shows the 
sources of selected word classes in the two 
systems. Over time, reliance on the Mindnet 
has increased overall, while reliance on 
dictionary lookup has now diminished to the 
point where, in the case of content words, it 
should be possible to discard the handcrafted 
dictionary altogether and draw exclusively on 
the contextualized resources of the Mindnet 
and statistically-generated lexical data. Also 
striking in Table 2 is the gain shown in 
preposition handling: a majority of English 
prepositions are now being transferred only in 
the context of LF structures found in the 
Mindnet. 
 
The important observation underlying the gains 
shown in these tables is that they have 
primarily been obtained either as the result of 
LF improvements in English or Japanese (i.e., 
from better sentence analysis or LF 
construction), or as a result of generic 
improvements to the algorithms that map 
between LF segments (notably better 
coindexation and improved learning of 
mappings involving lexical attributes). In the 
latter case, although certain modifications may 
have been driven by phenomena observed 
between Japanese and English, the heuristics 
apply across all seven languages on which our 
group is currently working. Adaptation to the 
case of Japanese-English MT usually takes the 
form of loosening rather than tightening of 
constraints.  
 
 
5 Future Work 
Ultimately it is probably desirable that the 
system?s mean absolute score should approach 
3 (Acceptable) within the training domain: this 
is a high quality bar that is not attained by 
off-the-shelf systems. Much of the work will be 
of a general nature: improving the parses and 
LF structures of source and target languages 
will bring automatic benefits to both alignment 
of structured phrases and runtime translation. 
For example, efforts are currently underway to 
redesign LF to better represent scopal 
properties of quantifiers and negation 
(Campbell & Suzuki 2002a, 2002b). 
 
Work to improve the quality of alignment and 
transfer is ongoing within our group. In 
addition to improvement of alignment itself, 
we are also exploring techniques to ensure that 
the transferred LF is consistent with known 
LFs in the target language, with the eventual 
goal of obviating the need for heuristic rules 
used in preprocessing generation. Again, these 
improvements are likely to be system-wide and 
generic, and not specific to the 
English-Japanese case. 
 
 
Conclusions 
Use of abstract semantically-motivated 
linguistic representations (Logical Form) 
permits MSR-MT to align, store, and translate 
sentence patterns reflecting widely varying 
syntactic and information structures in 
Japanese and English, and to do so within the 
framework of a general-purpose NLP 
architecture applicable to both European 
languages and Asian languages. 
 
Our experience with English-Japanese example 
based MT suggests that the problem of MT 
among Asian languages may be recast as a 
problem of implementing a general represen- 
tation of structured meaning across languages 
that neutralizes differences where possible, and 
where this is not possible, readily permits 
researchers to identify general-purpose 
techniques of bridging the disparities that are 
viable across multiple languages. 
 
Acknowledgements 
We would like to thank Bill Dolan and Rich 
Campbell for their comments on a draft of this 
paper. Our appreciation also goes to the 
members of the Butler Hill Group for their 
assistance with conducting evaluations. 
References  
Aikawa, T., M. Melero, L. Schwartz, and A. Wu. 
2001a. Multilingual sentence generation. In 
Proceedings of 8th European Workshop on 
Natural Language Generation, Toulouse, France.  
Aikawa, T., M. Melero, L. Schwartz, and A. Wu. 
2001b. Sentence generation for multilingual 
machine translation. In Proceedings of the MT 
Summit VIII, Santiago de Compostela, Spain.  
Brown, P. F.,  J. Cocke, S. A. D. Pietra, V. J. D. 
Pietra, F. Jelinek, J. D. Lafferty, R. L. Mercer, 
and P. S. Roossin. 1990. A statistical approach to 
machine translation. Computational Linguistics, 
16(2): 79-85. 
Campbell, R. and H. Suzuki. 2002a. Language- 
neutral representation of syntactic structure. In 
Proceedings of the First International Workshop 
on Scalable Natural Language Understanding 
(SCANALU 2002), Heidelberg, Germany. 
Campbell, R. and H. Suzuki. 2002b. Language- 
Neutral Syntax: An Overview. Microsoft Research 
Techreport: MSR-TR-2002-76. 
Heidorn, G. 2000. Intelligent writing assistance. In 
R. Dale, H. Moisl and H. Somers (eds.), A 
Handbook of Natural Language Processing: 
Techniques and Applications for the Processing 
of Language as Text. Marcel Dekker, New York. 
pp. 181-207.  
Imamura, K. 2001a. Application of translation 
knowledge acquired by ierarchical phrase 
alignment. In Proceedings of TMI.  
Imamura, K. 2001b. Hierarchical phrase alignment 
harmonized with parsing. In Proceedings of 
NLPRS, Tokyo, Japan, pp 377-384.  
Kacmarcik, G., C. Brockett, and H. Suzuki. 2000. 
Robust segmentation of Japanese text into a 
lattice for parsing. In Proceedings of COLING 
2000, Saarbrueken, Germany, pp. 390-396. 
Menezes, A. and S. D. Richardson. 2001. A 
best-first alignment algorithm for automatic 
extraction of transfer mappings from bilingual 
corpora. In Proceedings of the Workshop on 
Data-driven Machine Translation at 39th Annual 
Meeting of the Association for Computational 
Linguistics, Toulouse, France, pp. 39-46. 
Moore, R. C. 2001. Towards a simple and accurate 
statistical approach to learning translation 
relationships among words," in Proceedings, 
Workshop on Data-driven Machine Translation, 
39th Annual Meeting and 10th Conference of the 
European Chapter, Association for 
Computational Linguistics, Toulouse, France, pp. 
79-86. 
Nagao, M. 1984. A framework of a mechanical 
translation between Japanese and English by 
analogy principle. In A. Elithorn. and R. Bannerji 
(eds.) Artificial and Human Intelligence.  Nato 
Publications. pp. 181-207.   
Pinkham, J., M. Corston-Oliver, M. Smets and M. 
Pettenaro. 2001. Rapid Assembly of a Large-scale 
French-English MT system. In Proceedings of the 
MT Summit VIII, Santiago de Compostela, Spain. 
Pinkham, J., and M. Smets. 2002. Machine 
translation without a bilingual dictionary. In 
Proceedings of the 9th International Conference 
on Theoretical and Methodological Issues in 
Machine Translation. Kyoto, Japan, pp. 146-156. 
Richardson, S. D., W. B. Dolan, A. Menezes, and M. 
Corston-Oliver. 2001. Overcoming the 
customization bottleneck using example-based 
MT. In Proceedings, Workshop on Data-driven 
Machine Translation, 39th Annual Meeting and 
10th Conference of the European Chapter, 
Association for Computational Linguistics. 
Toulouse, France, pp. 9-16. 
Richardson, S. D., W. B. Dolan, A. Menezes, and J. 
Pinkham. 2001. Achieving commercial-quality 
translation with example-based methods. In 
Proceedings of MT Summit VIII, Santiago De 
Compostela, Spain, pp. 293-298.  
Richardson, S. D., W. B. Dolan, and L. 
Vanderwende. 1998 MindNet: Acquiring and 
structuring semantic information from text, 
ACL-98. pp. 1098-1102. 
Sato, S. and Nagao M. 1990. Toward 
memory-based translation. In Proceedings of 
COLING 1990, Helsinki, Finland, pp. 247-252. 
Suzuki, H., C. Brockett, and G. Kacmarcik. 2000. 
Using a broad-coverage parser for word-breaking 
in Japanese. In Proceedings of COLING 2000, 
Saarbrueken, Germany, pp. 822-827.  
Yamamoto K., and Y Matsumoto. 2000. 
Acquisition of phrase-level bilingual 
correspondence using dependency structure. In 
Proceedings of COLING 2000, Saarbrueken, 
Germany, pp. 933-939.  
Monolingual Machine Translation for Paraphrase Generation 
Chris QUIRK,  Chris BROCKETT  and  William DOLAN 
Natural Language Processing Group 
Microsoft Research 
One Microsoft Way 
Redmond, WA  90852  USA 
{chrisq,chrisbkt,billdol}@microsoft.com 
 
 
Abstract 
We apply statistical machine translation 
(SMT) tools to generate novel paraphrases 
of input sentences in the same language. 
The system is trained on large volumes of 
sentence pairs automatically extracted from 
clustered news articles available on the 
World Wide Web. Alignment Error Rate 
(AER) is measured to gauge the quality of 
the resulting corpus. A monotone phrasal 
decoder generates contextual replacements. 
Human evaluation shows that this system 
outperforms baseline paraphrase generation 
techniques and, in a departure from previ-
ous work, offers better coverage and scal-
ability than the current best-of-breed 
paraphrasing approaches. 
1 Introduction 
The ability to categorize distinct word sequences 
as ?meaning the same thing? is vital to applications 
as diverse as search, summarization, dialog, and 
question answering. Recent research has treated 
paraphrase acquisition and generation as a machine 
learning problem (Barzilay & McKeown, 2001; 
Lin & Pantel, 2002; Shinyama et al 2002, Barzilay 
& Lee, 2003, Pang et al, 2003). We approach this 
problem as one of statistical machine translation 
(SMT), within the noisy channel model of Brown 
et al (1993). That is, we seek to identify the opti-
mal paraphrase T* of a sentence S by finding: 
 
( ){ }
{ })P()|P(maxarg
|Pmaxarg*
TTS
STT
T
T
=
=
 
 
T and S being sentences in the same language.  
We describe and evaluate an SMT-based para-
phrase generation system that utilizes a monotone 
phrasal decoder to generate meaning-preserving 
paraphrases across multiple domains. By adopting 
at the outset a paradigm geared toward generating 
sentences, this approach overcomes many prob-
lems encountered by task-specific approaches. In 
particular, we show that SMT techniques can be 
extended to paraphrase given sufficient monolin-
gual parallel data.1 We show that a huge corpus of 
comparable and alignable sentence pairs can be 
culled from ready-made topical/temporal clusters 
of news articles gathered on a daily basis from 
thousands of sources on the World Wide Web, 
thereby permitting the system to operate outside 
the narrow domains typical of existing systems. 
2 Related work 
Until recently, efforts in paraphrase were not 
strongly focused on generation and relied primarily 
on narrow data sources. One data source has been 
multiple translations of classic literary works (Bar-
zilay & McKeown 2001; Ibrahim 2002; Ibrahim et 
al. 2003). Pang et al (2003) obtain parallel mono-
lingual texts from a set of 100 multiply-translated 
news articles. While translation-based approaches 
to obtaining data do address the problem of how to 
identify two strings as meaning the same thing, 
they are limited in scalability owing to the diffi-
culty (and expense) of obtaining large quantities of 
multiply-translated source documents.  
Other researchers have sought to identify pat-
terns in large unannotated monolingual corpora. 
Lin & Pantel (2002) derive inference rules by pars-
ing text fragments and extracting semantically 
similar paths. Shinyama et al (2002) identify de-
pendency paths in two collections of newspaper 
articles. In each case, however, the information 
extracted is limited to a small set of patterns.  
Barzilay & Lee (2003) exploit the meta-
information implicit in dual collections of news-
                                                          
1
 Barzilay & McKeown (2001), for example, reject the 
idea owing to the noisy, comparable nature of their data. 
wire articles, but focus on learning sentence-level 
patterns that provide a basis for generation. Multi-
sequence alignment (MSA) is used to identify sen-
tences that share formal (and presumably semantic) 
properties. This yields a set of clusters, each char-
acterized by a word lattice that captures n-gram-
based structural similarities between sentences. 
Lattices are in turn mapped to templates that can 
be used to produce novel transforms of input sen-
tences. Their methodology provides striking results 
within a limited domain characterized by a high 
frequency of stereotypical sentence types. How-
ever, as we show below, the approach may be of 
limited generality, even within the training domain.   
3 Data collection 
Our training corpus, like those of Shinyama et 
al. and Barzilay & Lee, consists of different news 
stories reporting the same event. While previous 
work with comparable news corpora has been lim-
ited to just two news sources, we set out to harness 
the ongoing explosion in internet news coverage. 
Thousands of news sources worldwide are compet-
ing to cover the same stories, in real time. Despite 
different authorship, these stories cover the same 
events and therefore have significant content over-
lap, especially in reports of the basic facts. In other 
cases, news agencies introduce minor edits into a 
single original AP or Reuters story. We believe 
that our work constitutes the first to attempt to ex-
ploit these massively multiple data sources for 
paraphrase learning and generation.  
3.1 Gathering aligned sentence pairs 
We began by identifying sets of pre-clustered 
URLs that point to news articles on the Web, gath-
ered from publicly available sites such as 
http://news.yahoo.com/, http://news.google.com 
and http://uk.newsbot.msn.com. Their clustering 
algorithms appear to consider the full text of each 
news article, in addition to temporal cues, to pro-
duce sets of topically/temporally related articles. 
Story content is captured by downloading the 
HTML and isolating the textual content. A super-
vised HMM was trained to distinguish story con-
tent from surrounding advertisements, etc.2 
Over the course of about 8 months, we collected 
11,162 clusters, comprising 177,095 articles and 
averaging 15.8 articles per cluster. The quality of 
                                                          
2
 We hand-tagged 1,150 articles to indicate which por-
tions of the text were story content and which were ad-
vertisements, image captions, or other unwanted 
material.  We evaluated several classifiers on a 70/30 
test train split and found that an HMM trained on a 
handful of features was most effective in identifying 
content lines (95% F-measure). 
these clusters is generally good. Impressionisti-
cally, discrete events like sudden disasters, busi-
ness announcements, and deaths tend to yield 
tightly focused clusters, while ongoing stories like 
the SARS crisis tend to produce very large and 
unfocused clusters. 
To extract likely paraphrase sentence pairs from 
these clusters, we used edit distance (Levenshtein 
1966) over words, comparing all sentences pair-
wise within a cluster to find the minimal number of 
word insertions and deletions transforming the first 
sentence into the second. Each sentence was nor-
malized to lower case, and the pairs were filtered 
to reject:  
 
? Sentence pairs where the sentences were 
identical or differed only in punctuation;  
? Duplicate sentence pairs;  
? Sentence pairs with significantly different 
lengths (the shorter is less than two-thirds 
the length of the longer);  
? Sentence pairs where the Levenshtein dis-
tance was greater than 12.0.3  
 
A total of 139K non-identical sentence pairs were 
obtained. Mean Levenshtein distance was 5.17; 
mean sentence length was 18.6 words. 
3.2 Word alignment 
To this corpus we applied the word alignment 
algorithms available in Giza++ (Och & Ney, 
2000), a freely available implementation of IBM 
Models 1-5 (Brown, 1993) and the HMM align-
ment (Vogel et al 1996), along with various im-
provements and modifications motivated by 
experimentation by Och & Ney (2000). In order to 
capture the many-to-many alignments that identify 
correspondences between idioms and other phrasal 
chunks, we align in the forward direction and again 
in the backward direction, heuristically recombin-
ing each unidirectional word alignment into a sin-
gle bidirectional alignment (Och & Ney 2000). 
Figure 1 shows an example of a monolingual 
alignment produced by Giza++. Each line repre-
sents a uni-directional link; directionality is indi-
cated by a tick mark on the target side of the link. 
We held out a set of news clusters from our 
training data and extracted a set of 250 sentence 
pairs for blind evaluation. Randomly extracted on 
the basis of an edit distance of 5 ? n ? 20 (to allow 
a range of reasonably divergent candidate pairs 
while eliminating the most trivial substitutions), 
the gold-standard sentence pairs were checked by 
an independent human evaluator to ensure that 
                                                          
3
 Chosen on the basis of ablation experiments and opti-
mal AER (discussed in 3.2).  
they contained paraphrases before they were hand 
word-aligned. 
To evaluate the alignments, we adhered to the 
standards established in Melamed (2001) and Och 
& Ney (2000, 2003). Following Och & Ney?s 
methodology, two annotators each created an ini-
tial annotation for each dataset, subcategorizing 
alignments as either SURE (necessary) or POSSIBLE 
(allowed, but not required). Differences were high-
lighted and the annotators were asked to review 
their choices on these differences. Finally we com-
bined the two annotations into a single gold stan-
dard: if both annotators agreed that an alignment 
should be SURE, then the alignment was marked as 
SURE in the gold-standard; otherwise the alignment 
was marked as POSSIBLE. 
To compute Precision, Recall, and Alignment 
Error Rate (AER) for the twin datasets, we used 
exactly the formulae listed in Och & Ney (2003). 
Let A be the set of alignments in the comparison, S 
be the set of SURE alignments in the gold standard, 
and P be the union of the SURE and POSSIBLE 
alignments in the gold standard. Then we have:  
 
||
||precision
A
PA ?
=  ||
||
  recall
S
SA ?
=
 
 
||
||AER
SA
SAPA
+
?+?
=  
 
Measured in terms of AER4, final interrater agree-
ment between the two annotators on the 250 sen-
tences was 93.1%. 
                                                          
4
 The formula for AER given here and in Och  & Ney 
(2003) is intended to compare an automatic alignment 
against a gold standard alignment. However, when com-
paring one human against another, both comparison and 
reference distinguish between SURE and POSSIBLE links. 
Because the AER is asymmetric (though each direction 
Table 1 shows the results of evaluating align-
ment after trainng the Giza++ model. Although the 
overall AER of 11.58% is higher than the best bi-
lingual MT systems (Och & Ney, 2003), the train-
ing data is inherently noisy, having more in 
common with analogous corpora than conventional 
MT parallel corpora in that the paraphrases are not 
constrained by the source text structure. The iden-
tical word AER of 10.57% is unsurprising given 
that the domain is unrestricted and the alignment 
algorithm does not employ direct string matching 
to leverage word identity.5 The non-identical word 
AER of 20.88% may appear problematic in a sys-
tem that aims to generate paraphrases; as we shall 
see, however, this turns out not to be the case. Ab-
lation experiments, not described here, indicate 
that additional data will improve AER.  
3.3 Identifying phrasal replacements 
Recent work in SMT has shown that simple 
phrase-based MT systems can outperform more 
sophisticated word-based systems (e.g. Koehn et 
al. 2003). Therefore, we adopt a phrasal decoder 
patterned closely after that of Vogel et al (2003). 
We view the source and target sentences S and T 
as word sequences s1..sm and t1..tn. A word align-
ment A of S and T can be expressed as a function 
from each of the source and target tokens to a 
unique cept (Brown et al 1993); isomorphically, a 
cept represents an aligned subset of the source and 
target tokens. Then, for a given sentence pair and 
word alignment, we define a phrase pair as a sub-
set of the cepts in which both the source and target 
tokens are contiguous. 6  We gathered all phrase 
                                                                                           
differs by less than 5%), we have presented the average 
of the directional AERs. 
5
 However, following SMT practice of augmenting data 
with a bilingual lexicon, we did append an identity lexi-
con to the training data. 
6
 While this does preclude the usage of ?gapped? phrase 
pairs such as or ? either ? or, we found such map-
Training Data Type: L12 
Precision   87.46% 
Recall      89.52% 
AER         11.58% 
Identical word precision   89.36% 
Identical word recall      89.50% 
Identical word AER         10.57% 
Non-identical word preci-
sion   76.99% 
Non-identical word recall      90.22% 
Non-identical word AER     20.88% 
 
Table 1. AER on the Lev12 corpus 
 
Figure 1. An example Giza++ alignment 
pairs (limited to those containing no more than five 
cepts, for reasons of computational efficiency) oc-
curring in at least one aligned sentence somewhere 
in our training corpus into a single replacement 
database. This database of lexicalized phrase pairs, 
termed phrasal replacements, serves as the back-
bone of our channel model. 
As in (Vogel et al 2003), we assigned probabili-
ties to these phrasal replacements via IBM Model 
1. In more detail, we first gathered lexical transla-
tion probabilities of the form P(s | t) by running 
five iterations of Model 1 on the training corpus. 
This allows for computing the probability of a se-
quence of source words S given a sequence of tar-
get words T as the sum over all possible 
alignments of the Model 1 probabilities: 
 
( ) ( )
( )??
?
? ?
=
=
Tt Ss
A
tsP
TASPTSP
|
|,|
 
 
(Brown et al (1993) provides a more detailed deri-
vation of this identity.) Although simple, this ap-
proach has proven effective in SMT for several 
reasons. First and foremost, phrasal scoring by 
Model 1 avoids the sparsity problems associated 
with estimating each phrasal replacement probabil-
ity with MLE (Vogel et al 2003). Secondly, it ap-
pears to boost translation quality in more 
sophisticated translation systems by inducing lexi-
cal triggering (Och et al 2004). Collocations and 
other non-compositional phrases receive a higher 
probability as a whole than they would as inde-
pendent single word replacements. 
One further simplification was made. Given that 
our domain is restricted to the generation of mono-
lingual paraphrase, interesting output can be pro-
duced without tackling the difficult problem of 
inter-phrase reordering.7 Therefore, along the lines 
of Tillmann et al (1997), we rely on only mono-
tone phrasal alignments, although we do allow in-
tra-phrasal reordering. While this means certain 
common structural alternations (e.g., ac-
tive/passive) cannot be generated, we are still able 
to express a broad range of phenomena: 
 
                                                                                           
pings to be both unwieldy in practice and very often 
indicative of poor a word alignment. 
7
 Even in the realm of MT, such an assumption can pro-
duce competitive results (Vogel et al 2003).  In addi-
tion, we were hesitant to incur the exponential increase 
in running time associated with those movement models 
in the tradition of Brown el al (1993), especially since 
these offset models fail to capture important linguistic 
generalizations (e.g., phrasal coherence, headedness). 
? Synonymy: injured ? wounded 
? Phrasal replacements: Bush administration 
? White House 
? Intra-phrasal reorderings: margin of error 
? error margin 
 
Our channel model, then, is determined solely 
by the phrasal replacements involved. We first as-
sume a monotone decomposition of the sentence 
pair into phrase pairs (considering all phrasal de-
compositions equally likely), and the probability 
P(S | T) is then defined as the product of the each 
phrasal replacement probability. 
The target language model was a trigram model 
using interpolated Kneser-Ney smoothing (Kneser 
& Ney 1995), trained over all 1.4 million sentences 
(24 million words) in our news corpus. 
3.4 Generating paraphrases 
To generate paraphrases of a given input, a stan-
dard SMT decoding approach was used; this is de-
scribed in more detail below. Prior to decoding, 
however, the input sentence underwent preprocess-
ing: text was lowercased, tokenized, and a few 
classes of named-entities were identified using 
regular expressions. 
To begin the decoding process, we first con-
structed a lattice of all possible paraphrases of the 
source sentence based on our phrasal translation 
database. Figure 2 presents an example. The lattice 
was realized as a set of |S| + 1 vertices v0..v|S| and a 
set of edges between those vertices; each edge was 
labeled with a sequence of words and a real num-
ber. Thus a edge connecting vertex vi to vj labeled 
with the sequence of words w1..wk and the real 
number p indicates that the source words si+1 to sj 
can be replaced by words w1..wk with probability p. 
Our replacement database was stored as a trie with 
words as edges, hence populating the lattice takes 
worst case O(n2) time. Finally, since source and 
target languages are identical, we added an identity 
mapping for each source word si: an edge from vi-1 
to vi with label si and a uniform probability u. This 
allows for handling unseen words. A high u value 
permits more conservative paraphrases. 
We found the optimal path through the lattice as 
scored by the product of the replacement model 
and the trigram language model. This algorithm 
reduces easily to the Viterbi algorithm; such a dy-
namic programming approach guarantees an effi-
cient optimal search (worst case O(kn), where n is 
the maximal target length and k is the maximal 
number of replacements for any word). In addition, 
fast algorithms exist for computing the n-best lists 
over a lattice (Soong & Huang 1991). 
Finally the resultant paraphrases were cleaned 
up in a post-processing phase to ensure output was 
not trivially distinguishable from other systems 
during human evaluation. All generic named entity 
tokens were re-instantiated with their source val-
ues, and case was restored using a model like that 
used in Vita et al (2003). 
3.5 Alternate approaches 
Barzilay &  Lee (2003) have released a common 
dataset that provides a basis for comparing differ-
ent paraphrase generation systems. It consists of 59 
sentences regarding acts of violence in the Middle 
East. These are accompanied by paraphrases gen-
erated by their Multi-Sequence Alignment (MSA) 
system and a baseline employing WordNet (Fell-
baum 1998), along with human judgments for each 
output by 2-3 raters. 
The MSA WordNet baseline was created by se-
lecting a subset of the words in each test sen-
tence?proportional to the number of words 
replaced by MSA in the same sentence?and re-
placing each with an arbitrary word from its most 
frequent WordNet synset. 
Since our SMT approach depends quite heavily 
on a target language model, we presented an alter-
nate WordNet baseline using a target language 
model.8 In combination with the language model 
described in section 3.4, we used a very simple 
replacement model: each appropriately inflected 
member of the most frequent synset was proposed 
as a possible replacement with uniform probability. 
This was intended to isolate the contribution of the 
language model from the replacement model. 
Given that our alignments, while aggregated into 
phrases, are fundamentally word-aligned, one 
question that arises is whether the information we 
learn is different in character than that learned 
                                                          
8
 In contrast, Barzilay and Lee (2003) avoided using a 
language model for essentially the same reason: their 
MSA approach did not take advantage of such a re-
source. 
from much simpler techniques. To explore this 
hypothesis, we introduced an additional baseline 
that used statistical clustering to produce an auto-
mated, unsupervised synonym list, again with a 
trigram language model. We used standard bigram 
clustering techniques (Goodman 2002) to produce 
4,096 clusters of our 65,225 vocabulary items. 
4 Evaluation 
We have experimented with several methods for 
extracting a parallel sentence-aligned corpus from 
news clusters using word alignment error rate, or 
AER, (Och & Ney 2003) as an evaluation metric. 
A brief summary of these experiments is provided 
in Table 1. To evaluate the quality of generation, 
we followed the lead of Barzilay & Lee (2003). 
We started with the 59 sentences and correspond-
ing paraphrases from MSA and WordNet (desig-
nated as WN below). Since the size of this data set 
made it difficult to obtain statistically significant 
results, we also included 141 randomly selected 
sentences from held-out clusters. We then pro-
duced paraphrases with each of the following sys-
tems and compared them with MSA and WN: 
 
? WN+LM: WordNet with a trigram LM 
? CL: Statistical clusters with a trigram LM 
? PR: The top 5 sentence rewrites produced by 
Phrasal Replacement. 
 
For the sake of consistency, we did not use the 
judgments provided by Barzilay and Lee; instead 
we had two raters judge whether the output from 
each system was a paraphrase of the input sen-
tence. The raters were presented with an input sen-
tence and an output paraphrase from each system 
in random order to prevent bias toward any par-
ticular judgment. Since, on our first pass, we found 
inter-rater agreement to be somewhat low (84%), 
we asked the raters to make a second pass of judg-
ments on those where they disagreed; this signifi-
cantly improved agreement (96.9%). The results of 
this final evaluation are summarized in Table 2. 
 
Figure 2. A simplified generation lattice: 44 top ranked edges from a total 4,140 
5 Analysis 
Table 2 shows that PR can produce rewordings 
that are evaluated as plausible paraphrases more 
frequently than those generated by either baseline 
techniques or MSA. The WordNet baseline per-
forms quite poorly, even in combination with a 
trigram language model: the language model does 
not contribute significantly to resolving lexical 
selection. The performance of CL is likewise 
abysmal?again a language model does nothing to 
help. The poor performance of these synonym-
based techniques indicates that they have little 
value except as a baseline. 
The PR model generates plausible paraphrases 
for the overwhelming majority of test sentences, 
indicating that even the relatively high AER for 
non-identical words is not an obstacle to successful 
generation. Moreover, PR was able to generate a 
paraphrase for all 200 sentences (including the 59 
MSA examples). The correlation between accept-
ability and PR sentence rank validates both the 
ranking algorithm and the evaluation methodology.  
In Table 2, the PR model scores significantly 
better than MSA in terms of the percentage of 
paraphrase candidates accepted by raters. More-
over, PR generates at least five (and often hun-
dreds more) distinct paraphrases for each test 
sentence. Such perfect coverage on this dataset is 
perhaps fortuitous, but is nonetheless indicative of 
scalability. By contrast Barzilay & Lee (2003) re-
port being able to generate paraphrases for only 59 
out of 484 sentences in their training (test?) set, a 
total of 12%.  
One potential concern is that PR paraphrases 
usually involve simple substitutions of words and 
short phrases (a mean edit distance of 2.9 on the 
top ranked sentences), whereas MSA outputs more 
complex paraphrases (reflected in a mean edit dis-
tance of 25.8). This is reflected in Table 3, which 
provides a breakdown of four dimensions of inter-
est, as provided by one of our independent evalua-
tors. Some 47% of MSA paraphrases involve 
significant reordering, such as an active-passive 
alternation, whereas the monotone PR decoder 
precludes anything other than minor transpositions 
within phrasal replacements. 
Should these facts be interpreted to mean that 
MSA, with its more dramatic rewrites, is ulti-
mately more ambitious than PR? We believe that 
the opposite is true. A close look at MSA suggests 
that it is similar in spirit to example-based machine 
translation techniques that rely on pairing entire 
sentences in source and target languages, with the 
translation step limited to local adjustments of the 
target sentence (e.g. Sumita 2001). When an input 
sentence closely matches a template, results can be 
stunning. However, MSA achieves its richness of 
substitution at the cost of generality. Inspection 
reveals that 15 of the 59 MSA paraphrases, or 
25.4%, are based on a single high-frequency, do-
main-specific template (essentially a running tally 
of deaths in the Israeli-Palestinian conflict). Unless 
one is prepared to assume that similar templates 
can be found for most sentence types, scalability 
and domain extensibility appear beyond the reach 
of MSA. 
In addition, since MSA templates pair entire 
sentences, the technique can produce semantically 
different output when there is a mismatch in in-
formation content among template training sen-
tences. Consider the third and fourth rows of Table 
3, which indicate the extent of embellishment and 
lossiness found in MSA paraphrases and the top-
ranked PR paraphrases. Particularly noteworthy is 
the lossiness of MSA seen in row 4. Figure 3 illus-
trates a case where the MSA paraphrase yields a 
significant reduction in information, while PR is 
more conservative in its replacements.  
While the substitutions obtained by the PR 
model remain for the present relatively modest, 
they are not trivial. Changing a single content word 
is a legitimate form of paraphrase, and the ability 
to paraphrase across an arbitrarily large sentence 
set and arbitrary domains is a desideratum of para-
phrase research. We have demonstrated that the 
SMT-motivated PR method is capable of generat-
ing acceptable paraphrases for the overwhelming 
majority of sentences in a broad domain.  
Method B&L59 B&L59 + 141 
PR #1 54 / 59 = 91.5% 177 / 200 = 89.5% 
PR #2 53 / 59 = 89.8% 168 / 200 = 84.0% 
PR #3 46 / 59 = 78.0% 164 / 200 = 82.0% 
PR #4 49 / 59 = 83.1% 163 / 200 = 81.5% 
MSA 46 / 59 = 78.0% 46 /   59 = 78.0% 
PR #5 44 / 59 = 74.6% 155 / 200 = 77.5% 
WN 23 / 59 = 39.0% 25 /   59 = 37.9% 
WN+LM 30 / 59 = 50.9% 53 / 200 = 27.5% 
CL  14 / 59 = 23.7% 26 / 200 = 13.0% 
 
Table 2. Human acceptability judgments 
 
MSA PR#1 
Rearrangement 28 / 59 = 47% 0 / 100 =   0% 
Phrasal alternation 11 / 59 = 19% 3 / 100 =   3% 
Info added 19 / 59 = 32% 6 / 100 =   6% 
Info lost 43 / 59 = 73% 31 / 100 = 31% 
 
Table 3. Qualitative analysis of paraphrases 
6 Future work 
Much work obviously remains to be done. Our 
results remain constrained by data sparsity, despite 
the large initial training sets. One major agenda 
item therefore will be acquisition of larger (and 
more diverse) data sets. In addition to obtaining 
greater absolute quantities of data in the form of 
clustered articles, we also seek to extract aligned 
sentence pairs that instantiate a richer set of phe-
nomena. Relying on edit distance to identify likely 
paraphrases has the unfortunate result of excluding 
interesting sentence pairs that are similar in mean-
ing though different in form. For example: 
 
The Cassini spacecraft, which is en route to Saturn, 
is about to make a close pass of the ringed 
planet's mysterious moon Phoebe 
 
On its way to an extended mission at Saturn, the 
Cassini probe on Friday makes its closest ren-
dezvous with Saturn's dark moon Phoebe. 
 
We are currently experimenting with data extracted 
from the first two sentences in each article, which 
by journalistic convention tend to summarize con-
tent (Dolan et al 2004). While noisier than the edit 
distance data, initial results suggest that these can 
be a rich source of information about larger phrasal 
substitutions and syntactic reordering.  
Although we have not attempted to address the 
issue of paraphrase identification here, we are cur-
rently exploring machine learning techniques, 
based in part on features of document structure and 
other linguistic features that should allow us to 
bootstrap initial alignments to develop more data. 
This will we hope, eventually allow us to address 
such issues as paraphrase identification for IR.  
To exploit richer data sets, we will also seek to 
address the monotone limitation of our decoder 
that further limits the complexity of our paraphrase 
output. We will be experimenting with more so-
phisticated decoder models designed to handle re-
ordering and mappings to discontinuous elements. 
We also plan to pursue better (automated) metrics 
for paraphrase evaluation.  
7 Conclusions 
We presented a novel approach to the problem 
of generating sentence-level paraphrases in a broad 
semantic domain. We accomplished this by using 
methods from the field of SMT, which is oriented 
toward learning and generating exactly the sorts of 
alternations encountered in monolingual para-
phrase. We showed that this approach can be used 
to generate paraphrases that are preferred by hu-
mans to sentence-level paraphrases produced by 
other techniques. While the alternations our system 
produces are currently limited in character, the 
field of SMT offers a host of possible enhance-
ments?including reordering models?affording a 
natural path for future improvements.  
A second important contribution of this work is 
a method for building and tracking the quality of 
large, alignable monolingual corpora from struc-
tured news data on the Web. In the past, the lack of 
such a data source has hampered paraphrase re-
search; our approach removes this obstacle. 
Acknowledgements 
We are grateful to Mo Corston-Oliver, Jeff Ste-
venson, Amy Muia, and Orin Hargraves of the 
Butler Hill Group for their work in annotating the 
data used in the experiments. This paper has also 
benefited from discussions with Ken Church, Mark 
Johnson, and Steve Richardson. We greatly appre-
ciate the careful comments of three anonymous 
reviewers. We remain, however, solely responsible 
for this content. 
References 
R. Barzilay and K. R. McKeown. 2001. Extracting Para-
phrases from a parallel corpus. In Proceedings of the 
ACL/EACL. 
R. Barzilay and L. Lee. 2003. Learning to Paraphrase; 
an unsupervised approach using multiple-sequence 
alignment. In Proceedings of HLT/NAACL. 
P. Brown, S. A. Della Pietra, V. J. Della Pietra and R. L. 
Mercer. 1993. The Mathematics of Statistical Ma-
chine Translation. Computational Linguistics 19(2): 
263-311. 
W. Dolan, C. Quirk and C. Brockett. 2004. Unsuper-
vised Construction of Large Paraphrase Corpora: Ex-
ploiting Massively Parallel News Sources.  To appear 
in Proceedings of COLING-2004. 
C. Fellbaum, ed. 1998. WordNet: An Electronic Lexical 
Database. MIT Press, Cambridge, MA. 
J. Goodman. 2002. JCLUSTER. Software available at 
http://research.microsoft.com/~joshuago/ 
A. Ibrahim. 2002. Extracting Paraphrases from Aligned 
Corpora. Master of Engineering Thesis, MIT. 
A. Ibrahim, B. Katz, and J. Lin. 2003. Extracting Struc-
tural Paraphrases from Aligned Monolingual Cor-
pora. In Proceedings of the Second International 
Workshop on Paraphrasing (IWP 2003). Sapporo, 
Japan. 
R. Kneser and H. Ney. 1995. Improved backing-off for 
N-gram language modeling. In Proc. Int. Conf. on 
Acoustics, Speech and Signal Processing: 181-184. 
Detroit, MI. 
P. Koehn, F. Och, and D. Marcu. 2003. Statistical 
Phrase-Based Translation. In Proceedings of 
HLT/NAACL. 
V. Levenshtein. 1966. Binary codes capable of correct-
ing deletions, insertions, and reversals. Soviet 
Physice-Doklady 10: 707-710. 
D. Lin and P. Pantel. 2001. DIRT - Discovery of Infer-
ence Rules from Text. In Proceedings of ACM 
SIGKDD Conference on Knowledge Discovery and 
Data Mining: 323-328. 
I. D. Melamed. 2001. Empirical Methods for Exploiting 
Parallel Texts. The MIT Press. 
R. Mihalcea and T. Pedersen. 2003. An Evaluation Ex-
ercise for Word Alignment. In HLT/NAACL Work-
shop: Building and Using Parallel Texts: 1-10. 
F. Och and H. Ney. 2000. Improved Statistical Align-
ment Models. In Proceedings of the ACL: 440-447. 
Hong Kong, China. 
F. Och and H. Ney. 2003. A Systematic Comparison of 
Various Statistical Alignment Models. Computa-
tional Linguistics 29(1): 19-52. 
B. Pang, K. Knight, and D. Marcu. 2003. Syntax-based 
Alignment of Multiple Translations: Extracting Para-
phrases and Generating New Sentences. Proceedings 
of HLT/NAACL.  
Y. Shinyama, S. Sekine and K. Sudo. 2002. Automatic 
Paraphrase Acquisition from News Articles. In Pro-
ceedings of NAACL-HLT. 
F. K. Soong and E. F. Huang. 1991. A tree-trellis based 
fast search for finding the n-best sentence hypotheses 
in continuous speech recognition. In Proceedings of 
the IEEE International Conference on Acoustics, 
Speech and Signal Processing 1: 705-708. Toronto, 
Canada. 
E. Sumita. 2001. Example-based machine translation 
using DP-matching between work sequences. In Pro-
ceedings of the ACL 2001 Workshop on Data-Driven 
Methods in Machine Translation: 1?8. 
C. Tillmann, S. Vogel, H. Ney, and A. Zubaiga. 1997. A 
DP Based Search Using Monotone Alignments in 
Statistical Translation. In Proceedings of the ACL. 
L. Vita, A. Ittycheriah, S. Roukos, and N. Kambhatla. 
2003. tRuEcasing. In Proceedings of the ACL: 152-
159. Sapporo, Japan. 
S. Vogel, H. Ney and C. Tillmann. 1996. HMM-Based 
Word Alignment in Statistical Translation. In Pro-
ceedings of the ACL: 836-841. Copenhagen Den-
mark. 
S. Vogel, Y. Zhang, F. Huang, A. Venugopal, B. Zhao, 
A. Tribble, M. Eck, and A. Waibel. 2003. The CMU 
Statistical Machine Translation System. In Proceed-
ings of MT Summit IX, New Orleans, Louisiana, 
USA. 
Proceedings of the NAACL HLT Workshop on Innovative Use of NLP for Building Educational Applications, pages 73?81,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
User Input and Interactions on Microsoft Research ESL Assistant 
 
 
Claudia Leacock Michael Gamon Chris Brockett 
Butler Hill Group Microsoft Research Microsoft Research 
P.O. Box 935 One Microsoft Way One Microsoft Way 
Ridgefield, CT, 06877, USA Redmond, WA, 98052, USA Redmond, WA, 98052, USA 
claudia.leacock@gmail.com mgamon@microsoft.com chrisbkt@microsoft.com 
 
 
 
 
 
 
Abstract 
ESL Assistant is a prototype web-based writ-
ing-assistance tool that is being developed for 
English Language Learners. The system fo-
cuses on types of errors that are typically 
made by non-native writers of American Eng-
lish. A freely-available prototype was dep-
loyed in June 2008. User data from this 
system are manually evaluated to identify 
writing domain and measure system accuracy. 
Combining the user log data with the eva-
luated rewrite suggestions enables us to de-
termine how effectively English language 
learners are using the system, across rule 
types and across writing domains. We find 
that repeat users typically make informed 
choices and can distinguish correct sugges-
tions from incorrect.  
1 Introduction 
Much current research in grammatical error detec-
tion and correction is focused on writing by Eng-
lish Language Learners (ELL). The Microsoft 
Research ESL Assistant is a web-based proofread-
ing tool designed primarily for ELLs who are na-
tive speakers of East-Asian languages. Initial 
system development was informed by pre-existing 
ELL error corpora, which were used both to identi-
fy common ELL mistakes and to evaluate system 
performance. These corpora, however, were 
created from data collected under arguably artifi-
cial classroom or examination conditions, leaving 
unresolved the more practical question as to 
whether the ESL Assistant can actually help a per-
son who produced the text to improve their English 
language writing skills in course of more realistic 
everyday writing tasks. 
In June of 2008, a prototype version of this sys-
tem was made freely available as a web service1
2 Related Work 
. 
Both the writing suggestions that visitors see and 
the actions that they then take are recorded. As 
these more realistic data begin to accumulate, we 
can now begin to answer the above question. 
Language learner error correction techniques  typi-
cally fall into either of two categories: rule-based 
or data-driven. Eeg-Olofsson and Knutsson (2003) 
report on a rule-based system that detects and cor-
rects preposition errors in non-native Swedish text. 
Rule-based approaches have also been used to pre-
dict definiteness and indefiniteness of Japanese 
noun phrases as a preprocessing step for Japanese 
to English machine translation (Murata and Nagao 
1993; Bond et al 1994; Heine, 1998), a task that is 
similar to the prediction of English articles. More 
recently, data-driven approaches have gained 
popularity and been applied to article prediction in 
English (Knight and Chander 1994; Minnen et al 
2000; Turner and Charniak 2007), to an array of 
Japanese learners? errors in English (Izumi et al 
2003), to verb errors (Lee and Seneff, 2008), and 
to article and preposition correction in texts written 
by non-native ELLs (Han et al 2004, 2006; Nagata 
et al 2005; Nagata et al 2006; De Felice and Pul-
man, 2007; Chodorow et al 2007; Gamon et al 
2008, 2009; Tetreault and Chodorow, 2008a). 
                                                          
1  http://www.eslassistant.com 
73
3 ESL Assistant 
ESL Assistant takes a hybrid approach that com-
bines statistical and rule-based techniques. Ma-
chine learning is used for those error types that are 
difficult to identify and resolve without taking into 
account complex contextual interactions, like ar-
ticle and preposition errors. Rule-based approaches 
handle those error types that are amenable to simp-
ler solutions. For example, a regular expression is 
sufficient for identifying when a modal is (incor-
rectly) followed by a tensed verb. 
The output of all modules, both machine-learned 
and rule-based, is filtered through a very large lan-
guage model. Only when the language model finds 
that the likelihood of the suggested rewrite is suffi-
ciently larger than the original text is a suggestion 
shown to the user. For a detailed description of 
ESL Assistant?s architecture, see Gamon et al
(2008, 2009). 
Although this and the systems cited in section 2 
are designed to be used by non-native writers, sys-
tem performance is typically reported in relation to 
native text ? the prediction of a preposition, for 
example, will ideally be consistent with usage in 
native, edited text. An error is counted each time 
the system predicts a token that differs from the 
observed usage and a correct prediction is counted 
each time the system predicts the usage that occurs 
in the text. Although somewhat artificial, this ap-
proach to evaluation offers the advantages of being 
fully automatable and having abundant quantities 
N
ou
n 
R
el
at
ed
 
(6
1%
) 
Articles (ML) 
We have just checked *the our stock. 
life is *journey/a journey, travel it well! 
I think it 's *a/the best way to resolve issues like this. 
Noun Number 
London is one of the most attractive *city/cities in the world. 
You have to write down all the details of each *things/thing to do. 
Conversion always takes a lot of *efforts/effort. 
Noun Of Noun 
Please send the *feedback of customer/customer feedback to me by 
mail. 
P
re
po
si
ti
on
 
R
el
at
ed
 
(2
7%
) Preposition (ML) 
I'm *on home today, call me if you have a problem.  
It seems ok and I did not pay much attention *on/to it. 
Below is my contact, looking forward *your/to your response, thanks! 
Verb and Preposition 
Ben is involved *this/in this transaction. 
I should *to ask/ask a rhetorical question ? 
But I?ll think *it/about it a second time. 
V
er
b 
R
el
at
ed
  
(1
0%
) 
Gerund / Infinitive 
(ML) 
He got me *roll/to roll up my sleeve and make a fist. 
On Saturday, I with my classmate went *eating/to eat. 
After *get/getting a visa, I want to study in New York. 
Auxiliary Verb (ML) 
To learn English we should *be speak/speak  it as much as possible . 
Hope you will *happy/be happy in Taiwan . 
what *is/do you want to say? 
Verb formation 
If yes, I will *attached/attach and resend to Geoff . 
The time and setting are *display/displayed at the same time. 
You had *order/ordered 3 items ? this time. 
I am really *hope/hoping to visit UCLA. 
Cognate/Verb Con-
fusion 
We cannot *image/imagine what the environment really is at the site 
of end user . 
Irregular Verbs I *teached/taught him all the things that I know ? 
A
dj
 
R
el
at
ed
 
(2
%
) Adjective Confu-
sions 
She is very *interesting/interested in the problem. 
So *Korea/Korean Government is intensively fostering trade . 
? and it is *much/much more reliable than your Courier Service. 
Adjective order Employing the *Chinese ancient/ancient Chinese proverb, that is  ? 
Table 1: ESL Assistant grammatical error modules.  ML modules are machine learned. 
 
74
of edited data readily available. With respect to 
prepositions and articles, the ESL Assistant's clas-
sifiers achieve state-of-the-art performance when 
compared to results reported in the literature (Ga-
mon et al 2008), inasmuch as comparison is possi-
ble when the systems are evaluated on different 
samples of native text. For articles, the system had 
86.76% accuracy as compared to 86.74% reported 
by Turner and Charniak (2007), who have the most 
recently reported results. For the harder problem of 
prepositions, ESL Assistant?s accuracy is compara-
ble to those reported by Tetreault and Chodorow 
(2008a) and De Felice and Pulman (2007).  
3.1 Error Types 
The ELL grammatical errors that  ESL Assistant 
tries to correct were distilled from analysis of the 
most frequent errors made in Chinese and Japanese 
English language learner corpora (Gui and Yang, 
2001; Izumi et al 2004). The error types are shown 
in Table 1: modules identified with ML are ma-
chine-learned, while the remaining modules are 
rule-based. ESL Assistant does not attempt to iden-
tify those errors currently found by Microsoft 
Word?, such as subject/verb agreement.  
ESL Assistant further contains a component to 
help address lexical selection issues. Since this 
module is currently undergoing major revision, we 
will not report on the results here. 
3.2 System Development  
Whereas evaluation on native writing is essential 
for system development and enables us to compare 
ESL Assistant performance with that of other re-
ported results, it tells us little about how the system 
would perform when being used by its true target 
audience ? non-native speakers of English engaged 
in real-life writing tasks. In this context, perfor-
mance measurement inevitably entails manual 
evaluation, a process that is notoriously time con-
suming, costly and potentially error-prone. Human 
inter-rater agreement is known to be problematic 
 
Figure 1: Screen shot of ESL Assistant 
 
75
on this task: it is likely to be high in the case of 
certain user error types, such as over-regularized 
verb inflection (where the system suggests replac-
ing ?writed? with ?wrote?), but other error types 
are difficult to evaluate, and much may hinge upon 
who is performing the evaluation: Tetreault and 
Chodorow (2008b) report that for the annotation of 
preposition errors ?using a single rater as a gold 
standard, there is the potential to over- or under-
estimate precision by as much as 10%.?  
With these caveats in mind, we employed a sin-
gle annotator to evaluate system performance on 
native data from the 1-million-word Chinese 
Learner?s of English corpus (Gui and Yang, 2001; 
2003). Half of the corpus was utilized to inform 
system development, while the remaining half was 
held back for "unseen" evaluation. While the abso-
lute numbers for some modules are more reliable 
than for others, the relative change in numbers 
across evaluations has proven a beneficial 
yardstick of improved or degraded performance in 
the course of development.  
3.3 The User Interface and Data Collection 
Figure 1 shows the ESL Assistant user interface. 
When a visitor to the site types or pastes text into 
the box provided and clicks the "Check" button, 
the text is sent to a server for analysis. Any loca-
tions in the text that trigger an error flag are then 
displayed as underscored with a wavy line (known 
as a "squiggle"). If the user hovers the mouse over 
a squiggle, one or more suggested rewrites are dis-
played in a dropdown list. Then, if the user hovers 
over one of these suggestions, the system launches 
parallel web searches for both original and rewrite 
phrases in order to allow the user to compare real-
word examples found on the World Wide Web. To 
accept a suggestion, the user clicks on the sug-
gested rewrite, and the text is emended. Each of 
these actions, by both system and user, are logged 
on the server. 
Since being launched in June, 2008, ESL Assis-
tant has been visited over 100,000 times. Current-
ly, the web page is being viewed between one to 
two thousand times every day. From these numbers 
alone it seems safe to conclude that there is much 
public interest in an ESL proofreading tool. 
Fifty-three percent of visitors to the ESL Assis-
tant web site are from countries in East Asia ? its 
primary target audience ? and an additional 15% 
are from the United States. Brazil, Canada, Ger-
many, and the United Kingdom each account for 
about 2% of the site?s visitors. Other countries 
represented in the database each account for 1% or 
less of all those who visit the site.  
3.4 Database of User Input 
User data are collected so that system performance 
can be evaluated on actual user input ? as opposed 
to running pre-existing learner corpora through the 
system. User data provide invaluable insight into 
which rewrite suggestions users spend time view-
ing, and what action they subsequently take on the 
basis of those suggestions.  
These data must be screened, since not all of the 
textual material entered by users in the web site is 
valid learner English language data. As with any 
publicly deployed web service, we find that nu-
merous users will play with the system, entering 
nonsense strings or copying text from elsewhere on 
the website and pasting it into the text box.  
To filter out the more obvious non-English data, 
we eliminate input that contains, for example, no 
alphabetic characters, no vowels/consonants in a 
sentence, or no white space. ?Sentences? consist-
ing of email subject lines are also removed, as are 
all the data entered by the ESL Assistant develop-
ers themselves. Since people often enter the same 
sentence many times within a session, we also re-
move repetitions of identical sentences within a 
single session.  
Approximately 90% of the people who have vi-
sited the web site visit it once and never return. 
This behavior is far from unusual on the web, 
where site visits may have no clear purpose beyond 
idle curiosity. In addition, some proportion of visi-
tors may in reality be automated "bots" that can be 
nearly indistinguishable from human visitors. 
Nevertheless, we observe a significant number 
of repeat visitors who return several times to use 
the system to proofread email or other writing, and 
these are the users that we are intrinsically interest-
ed in. To measure performance, we therefore de-
cided to evaluate on data collected from users who 
logged on and entered plausibly English-like text 
on at least four occasions. As of 2/10/2009, the 
frequent user database contained 39,944 session-
unique sentences from 578 frequent users in 5,305 
sessions.  
76
Data from these users were manually annotated 
to identify writing domains as shown in Table 2. 
Fifty-three percent of the data consists of people 
proofreading email.2
 
 The dominance of email data 
is presumably due to an Outlook plug-in that is 
available on the web site, and automates copying 
email content into the tool. The non-technical do-
main consists of student essays, material posted on 
a personal web site, or employees writing about 
their company ? for example, its history or 
processes. The technical writing is largely confe-
rence papers or dissertations in the fields of, for 
example, medicine and computer science. The 
?other? category includes lists and resumes (a writ-
ing style that deliberately omits articles and gram-
matical subjects), as well as text copied from 
online newspapers or other media and pasted in. 
Writing Domain Percent 
Email 53% 
Non-technical / essays 24% 
Technical / scientific 14% 
Other (lists, resumes, etc) 4% 
Unrelated sentences 5% 
Table 2: Writing domains of frequent users 
 
Sessions categorized as ?unrelated sentences? typi-
cally consist of a series of short, unrelated sen-
tences that each contain one or more errors. These 
users are testing the system to see what it does. 
While this is a legitimate use of any grammar 
checker, the user is unlikely to be proofreading his 
or her writing, so these data are excluded from 
evaluation.  
4 System Evaluation & User Interactions 
We are manually evaluating the rewrite sugges-
tions that ESL Assistant generated in order to de-
termine both system accuracy and whether user 
acceptances led to an improvement in their writing.  
These categories are shown in Table 3. Note that 
results reported for non-native text look very dif-
ferent from those reported for native text (dis-
cussed in Section 3) because of the neutral 
categories which do not appear in the evaluation of 
native text. Systems reporting 87% accuracy on 
native text cannot achieve anything near that on 
                                                          
2 These are anonymized to protect user privacy. 
non-native ELL text because almost one third of 
the flags fall into a neutral category. 
In 51% of the 39,944 frequent user sentences, 
the system generated at least one grammatical error 
flag, for a total of 17,832 flags. Thirteen percent of 
the time, the user ignored the flags. The remaining 
87% of the flags were inspected by the user, and of 
those, the user looked at the suggested rewrites 
without taking further action 31% of the time. For 
28% of the flags, the user hovered over a sugges-
tion to trigger a parallel web search but did not 
accept the proposed rewrite. Nevertheless, 41% of 
inspected rewrites were accepted, causing the orig-
inal string in the text to be revised. Overall, the 
users inspected about 15.5K suggested rewrites to 
accept about 6.4K. A significant number of users 
appear to be inspecting the suggested revisions and 
making deliberate choices to accept or not accept. 
The next question is: Are users making the right 
choices? To help answer this question, 34% of the 
user sessions have been manually evaluated for 
system accuracy ? a total of approximately 5.1K 
grammatical error flags. For each error category 
and for the three major writing domains, we: 
Evaluation Subcategory: Description 
Good 
Correct flag: The correction fixes a 
problem in the user input. 
Neutral 
Both Good: The suggestion is a legiti-
mate alternative to well-formed original 
input: I like working/to work. 
Misdiagnosis: the original input con-
tained an error but the suggested rewrite 
neither improves nor further degrades 
the input: If you have fail machine on 
hand. 
Both Wrong: An error type is correctly 
diagnosed but the suggested rewrite 
does not correct the problem: can you 
give me suggestion
Non-ascii: A non-ascii or text markup 
character is in the immediate context. 
. (suggests the in-
stead of a) 
Bad 
False Flag: The suggestion resulted in 
an error or would otherwise lead to a 
degradation over the original user input. 
Table 3: Evaluation categories 
 
77
1. Calculated system accuracy for all flags, 
regardless of user actions. 
2. Calculated system accuracy for only those 
rewrites that the user accepted 
3. Compared the ratio of good to bad flags. 
 
Results for the individual error categories are 
shown in Figure 2. Users consistently accept a 
greater proportion of good suggestions than they 
do bad ones across all error categories. This is 
most pronounced for the adjective-related modules, 
where the overall rate of good suggestions im-
proved 17.6% after the user made the decision to 
accept a  suggestion, while the system?s false posi-
tive rate dropped 14.1% after the decision. For the 
noun-related modules, the system?s most produc-
 
 Rewrite Suggestion Evaluation Accepted Suggestion 
Noun  
Related  
Modules 
 
3,017 suggestions 
   972 acceptances 
  
Preposition  
Related  
Modules 
 
1,465 suggestions 
   479 acceptances 
  
Verb  
Related  
Modules 
 
469 suggestions 
157 acceptances 
  
Adjective 
Related  
Modules 
 
125 suggestions 
  40 acceptances 
  
Figure 2: User interactions by module category 
good
56%
neut
28%
bad
16%
good
63%
neut
26%
bad
11%
good
37%
neut
39%
bad
24% good
45%
neut
42%
bad
13%
good
62%
neut
32%
bad
6%
good
72%
neut
25%
bad
3%
good
45%
neut
32%
bad
23%
good
63%
neut
28%
bad
9%
78
tive modules, the overall good flag rate increased 
by 7% while the false positive rate dropped 5%. 
All differences in false positive rates are statistical-
ly significant in Wilcoxon?s signed-ranks test.  
When all of the modules are evaluated across 
the three major writing domains, shown in figure 3, 
the same pattern of user discrimination between 
good and bad flags holds. This is most evident in 
the technical writing domain, where the overall 
rate of good suggestions improved 13.2% after 
accepting the suggestion and the false positive rate 
dropped 15.1% after the decision. It is least marked 
for the essay/nontechnical writing domain. Here 
the overall good flag rate increased by only .3% 
while the false positive rate dropped 1.6%. Again, 
all of the differences in false positive rates are sta-
tistically significant in Wilcoxon?s signed-ranks 
test. These findings are consistent with those for 
the machine learned articles and prepositions mod-
ules in the email domain (Chodorow et al under 
review).  
A probable explanation for the differences seen 
across the domains is that those users who are 
proofreading non-technical writing are, as a whole, 
less proficient in English than the users who are 
writing in the other domains. Users who are proof-
reading technical writing are typically writing a 
dissertation or paper in English and therefore tend 
to relatively fluent in the language. The email do-
main comprises people who are confident enough 
in their English language skills to communicate 
with colleagues and friends by email in English. 
With the essay/non-technical writers, it often is not 
clear who the intended audience is. If there is any 
indication of audience, it is often an instructor. Us-
ers in this domain appear to be the least English-
 Rewrite Suggestion Evaluation Accepted Suggestion 
Email  
Domain 
 
2,614 suggestions 
   772 acceptances 
  
Non-Technical 
Writing 
 Domain 
 
1,437 suggestions 
   684 acceptances 
  
Technical  
Writing 
Domain 
 
1,069 suggestions 
    205 acceptances 
  
Figure 3: User interactions by writing domain 
good
53%
neutral
32%
bad
15%
good
63%
neutral
28%
bad
9%
good
56%
neutral
32%
bad
12%
good
56%
neutral
34%
bad
10%
good
38%
neutral
28%
bad
34%
good
52%
neutral
29%
bad
19%
79
language proficient of the ESL Assistant users, so it 
is unsurprising that they are less effective in dis-
criminating between good and bad flags than their 
more proficient counterparts. Thus it appears that 
those users who are most in need of the system are 
being helped by it least ? an important direction for 
future work. 
Finally, we look at whether the neutral flags, 
which account for 29% of the total flags, have any 
effect. The two neutral categories highlighted in 
Table 3, flags that either misdiagnose the error or 
that diagnose it but do not correct it, account for 
74% of ESL Assistant?s neutral flags. Although 
these suggested rewrites do not improve the sen-
tence, they do highlight an environment that con-
tains an error. The question is: What is the effect of 
identifying an error when the rewrite doesn?t im-
prove the sentence?  
To estimate this, we searched for cases where 
ESL Assistant produced a neutral flag and, though 
the user did not accept the suggestion, a revised 
sentence that generated no flag was subsequently 
submitted for analysis. For example, one user en-
tered: ?This early morning  i got a from head office 
??. ESL Assistant suggested deleting from, which 
does not improve the sentence. Subsequently, in 
the same session, the user submitted, ?This early 
morning I heard from the head office ?? In this 
instance, the system correctly identified the loca-
tion of an error. Moreover, even though the sug-
gested rewrite was not a good solution, the 
information was sufficient to enable the user to fix 
the error on his or her own. 
Out of 1,349 sentences with neutral suggestions 
that were not accepted, we identified (using a 
fuzzy match) 215 cases where the user voluntarily 
modified the sentence so that it contained no flag, 
without accepting the suggestion. In 44% of these 
cases, the user had simply typed in the suggested 
correction instead of accepting it ? indicating that 
true acceptance rates might be higher than we orig-
inally estimated. Sixteen percent of the time, the 
sentence was revised but there remained an error 
that the system failed to detect. In the other 40% of 
cases, the voluntary revision improved the sen-
tence. It appears that merely pointing out the poss-
ible location of an error to the user is often 
sufficient to be helpful. 
5 Conclusion 
In conclusion, judging from the number of people 
who have visited the ESL Assistant web site, there 
is considerable interest in ESL proofreading tools 
and services. 
When using the tool to proofread text, users do 
not accept the proposed corrections blindly ? they 
are selective in their behavior. More importantly, 
they are making informed choices ? they can dis-
tinguish correct suggestions from incorrect ones. 
Sometimes identifying the location of an error, 
even when the solution offered is wrong, itself ap-
pears sufficient to cause the user to repair a prob-
lem on his or her own. Finally, the user 
interactions that we have recorded indicate that 
current state-of-the-art grammatical error correc-
tion technology has reached a point where it can be 
helpful to English language learners in real-world 
contexts. 
Acknowledgments 
We thank Bill Dolan, Lucy Vanderwende, Jianfeng 
Gao, Alexandre Klementiev and Dmitriy Belenko 
for their contributions to the ESL Assistant system. 
We are also grateful to the two reviewers of this 
paper who provided valuable feedback. 
References  
Francis Bond, Kentaro Ogura, and Satoru Ikehara. 1994. 
Countability and number in Japanese to English ma-
chine translation. In Proceedings of the 15th Confe-
rence on Computational Linguistics (pp. 32-38). 
Kyoto, Japan. 
Martin Chodorow, Michael Gamon, and Joel Tetreault. 
Under review. The utility of grammatical error detec-
tion systems for English language learners: Feedback 
and Assessment. 
Martin Chodorow, Joel Tetreault, and Na-Rae Han. 
2007. Detection of grammatical errors involving pre-
positions. In Proceedings of the Fourth ACL-
SIGSEM Workshop on Prepositions (pp. 25-30). Pra-
gue, Czech Republic. 
Rachele De Felice and Stephen G. Pulman. 2007. Au-
tomatically acquiring models of preposition use. In 
Proceedings of the Fourth ACL-SIGSEM Workshop 
on Prepositions (pp. 45-50). Prague, Czech Republic. 
Jens Eeg-Olofsson and Ola Knutsson.  2003. Automatic 
grammar checking for second language learners ? the 
use of prepositions.  Proceedings of NoDaLiDa 2003. 
Reykjavik, Iceland. 
80
Michael Gamon, Jianfeng Gao, Chris Brockett, Alexan-
dre Klementiev, William B. Dolan, Dmitriy Belenko, 
and Lucy Vanderwende. 2008. Using contextual 
speller techniques and language modeling for ESL 
error correction. In Proceedings of the Third Interna-
tional Joint Conference on Natural Language 
Processing (pp. 449-455). Hyderabad, India. 
Michael Gamon, Claudia Leacock, Chris Brockett, Wil-
liam B. Dolan,  Jianfeng Gao, Dmitriy Belenko, and 
Alexandre Klementiev. 2009. Using statistical tech-
niques and web search to correct ESL errors. To ap-
pear in CALICO Journal, Special Issue on Automatic 
Analysis of Learner Language. 
Shicun Gui and Huizhong Yan. 2001. Computer analy-
sis of Chinese learner English. Presentation at Hong 
Kong University of Science and Technolo-
gy.http://lc.ust.hk/~centre/conf2001/keynote/subsect4 
/yang.pdf. 
Shicun Gui and Huizhong Yang. (Eds.). 2003. Zhong-
guo Xuexizhe Yingyu Yuliaohu. (Chinese Learner 
English Corpus). Shanghai Waiyu Jiaoyu Chubanshe. 
(In Chinese). 
Na-Rae Han, Martin Chodorow, and Claudia Leacock 
2004). Detecting errors in English article usage with 
a maximum entropy classifier trained on a large, di-
verse corpus. In Proceedings of the 4th Interna-
tional Conference on Language Resources and 
Evaluation. Lisbon, Portugal. 
Na-Rae Han, Martin Chodorow, and Claudia Leacock 
2006. Detecting errors in English article usage by 
non-native speakers. Natural Language Engineer-
ing, 12(2), 115-129. 
Julia E. Heine. 1998. Definiteness predictions for Japa-
nese noun phrases. In Proceedings of the 36th Annual 
Meeting of the Association for Computational Lin-
guistics and 17th International Conference on Com-
putational Linguistics (pp. 519-525). Montreal, 
Canada. 
Emi Izumi, Kiyotaka Uchimoto, Toyomi Saiga, Thep-
chai Supnithi, and Hitoshi Isahara. 2003. Automatic 
error detection in the Japanese learners' English spo-
ken data. In Proceedings of the 41st Annual Meeting 
of the Association for Computational Linguistics (pp. 
145-148). Sapporo, Japan. 
Kevin Knight and Ishwar Chander,. 1994. Automatic 
postediting of documents. In Proceedings of the 12th 
National Conference on Artificial Intelligence (pp. 
779-784). Seattle: WA. 
John Lee. 2004. Automatic article restoration. In Pro-
ceedings of the Human Language Technology Confe-
rence of the North American Chapter of the 
Association for Computational Linguistics (pp. 31-
36). Boston, MA. 
John Lee and Stephanie Seneff. 2008. Correcting mi-
suse of verb forms. In Proceedings of ACl-08/HLT  
(pp. 174-182). Columbus, OH. 
Guido Minnen, Francis Bond, and Anne Copestake. 
2000. Memory-based learning for article generation. 
In Proceedings of the Fourth Conference on Compu-
tational Natural Language Learning and of the 
Second Learning Language in Logic Workshop (pp. 
43-48). Lisbon, Portugal. 
Masaki Murata and Makoto Nagao. 1993. Determina-
tion of referential property and number of nouns in 
Japanese sentences for machine translation into Eng-
lish. In Proceedings of the Fifth International Confe-
rence on Theoretical and Methodological Issues in 
Machine Translation (pp. 218-225). Kyoto, Japan. 
Ryo Nagata, Takahiro Wakana, Fumito Masui, Atsui 
Kawai, and Naoki Isu. 2005. Detecting article errors 
based on the mass count distinction. In R. Dale, W. 
Kam-Fie, J. Su and O.Y. Kwong (Eds.) Natural Lan-
guage Processing - IJCNLP 2005, Second Interna-
tional Joint Conference Proceedings (pp. 815-826). 
New York: Springer. 
Ryo Nagata, Atsuo Kawai, Koichiro Morihiro, and 
Naoki Isu. 2006. A feedback-augmented method for 
detecting errors in the writing of learners of English. 
In Proceedings of the 21st International Conference 
on Computational Linguistics and 44th Annual Meet-
ing of the Association for Computational Linguistics 
(pp. 241-248). Sydney, Australia. 
Joel Tetreault and Martin Chodorow. 2008a. The ups 
and downs of preposition error detection in ESL. 
COLING. Manchester, UK. 
Joel Tetreault and Martin Chodorow. 2008b. Native 
judgments of non-native usage: Experiments in pre-
position error detection. In Proceedings of the Work-
shop on Human Judgments in Computational 
Linguistics, 22nd International Conference on Com-
putational Linguistics (pp 43-48). Manchester, UK. 
Jenine Turner and Eugene Charniak. 2007. Language 
modeling for determiner selection. In Human Lan-
guage Technologies 2007: The Conference of the 
North American Chapter of the Association for Com-
putational Linguistics; Companion Volume, Short 
Papers (pp. 177-180). Rochester, NY. 
 
 
81
Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 145?153,
Los Angeles, California, June 2010. c?2010 Association for Computational Linguistics
Hitting the Right Paraphrases in Good Time
Stanley Kok
Department of Computer Science & Engineering
University of Washington
Seattle, WA 98195, USA
koks@cs.washington.edu
Chris Brockett
Microsoft Research
One Microsoft Way
Redmond, WA 98052, USA
chrisbkt@microsoft.com
Abstract
We present a random-walk-based approach to
learning paraphrases from bilingual parallel
corpora. The corpora are represented as a
graph in which a node corresponds to a phrase,
and an edge exists between two nodes if their
corresponding phrases are aligned in a phrase
table. We sample random walks to compute
the average number of steps it takes to reach
a ranking of paraphrases with better ones be-
ing ?closer? to a phrase of interest. This ap-
proach allows ?feature? nodes that represent
domain knowledge to be built into the graph,
and incorporates truncation techniques to pre-
vent the graph from growing too large for ef-
ficiency. Current approaches, by contrast, im-
plicitly presuppose the graph to be bipartite,
are limited to finding paraphrases that are of
length two away from a phrase, and do not
generally permit easy incorporation of domain
knowledge. Manual evaluation of generated
output shows that our approach outperforms
the state-of-the-art system of Callison-Burch
(2008).
1 Introduction
Automatically learning paraphrases, or alternative
ways of expressing the same meaning, is an ac-
tive area of NLP research because of its useful-
ness in a variety of applications, e.g., question an-
swering (Lin and Pantel, 2001; Ravichandran and
Hovy, 2002; Reizler et al, 2007), document sum-
marization (Barzilay et al, 1999; McKeown et al,
2002), natural language generation (Iordanskaja et
al., 1991; Lenke, 1994; Stede, 1999), machine trans-
lation (Kauchak and Barzilay, 2006; Callison-Burch
et al, 2006; Madnani et al, 2007).
Early work on paraphrase acquisition has focused
on using monolingual parallel corpora (Barzilay and
McKeown, 2001; Barzilay and Lee, 2003; Pang et
al., 2003; Quirk et al, 2004). While effective, such
methods are hampered by the scarcity of monolin-
gual parallel corpora, an obstacle that limits both
the quantity and quality of the paraphrases learned.
To address this limitation, Bannard and Callison-
Burch (2005) focused their attention on the abun-
dance of bilingual parallel corpora. The crux of
this system (referred to below as ?BCB?) is to align
phrases in a bilingual parallel corpus and hypothe-
size English phrases as potential paraphrases if they
are aligned to the same phrase in another language
(the ?pivot?). Callison-Burch (2008) further refines
BCB with a system that constrains paraphrases to
have the same syntactic structure (Syntactic Bilin-
gual Phrases: SBP).
We take a graphical view of the state-of-the-art
BCB and SBP approaches by representing the bilin-
gual parallel corpora as a graph. A node corresponds
to a phrase, and an edge exists between two nodes if
their corresponding phrases are aligned. This graph-
ical form makes the limitations of the BCB/SBP ap-
proaches more evident. The BCB/SBP graph is lim-
ited to be bipartite with English nodes on one side
and foreign language nodes on the other, and an
edge can only exist between nodes on different sides.
This neglects information between foreign language
nodes that may aid in learning paraphrases. Further,
by only considering English nodes that are linked
via a foreign language node as potential paraphrases,
145
these approaches will fail to find paraphrases sepa-
rated by distances greater than length two.
In this paper, we present HTP (Hitting Time
Paraphraser), a paraphrase learning approach that is
based on random walks (Lova?sz, 1996) and hitting
times (Aldous and Fill, 2001). Hitting time mea-
sures the average number of steps one needs to take
in a random traversal of a graph before reaching a
destination node from a source node. Intuitively, the
smaller the hitting time from a phrase E to E? (i.e.,
the closer E? is to E), the more likely it is that E? is
a good paraphrase of E. The advantages of HTP are
as follows:
? By traversing paths of lengths greater than two,
our approach is able to find more paraphrases
of a given phrase.
? We do not require the graph to be bipartite.
Edges can exist between nodes of different for-
eign languages if their corresponding phrases
are aligned. This allows information from for-
eign phrase alignments to be used in finding
English paraphrases.
? We permit domain knowledge to be easily in-
corporated as nodes in the graph. This allows
domain knowledge to favor good paraphrases
over bad ones, thereby improving performance.
In this paper, we focus on learning English para-
phrases. However, our system can be applied to
learning paraphrases in any language.
We begin by reviewing random walks and hitting
times in the next section. Then we describe our para-
phrase learning algorithm (Section 3), and report our
experiments (Section 4). We discuss related work in
Section 5. Finally, we conclude with future work
(Section 6).
2 Background
A directed graph consists of a set of nodes V , and a
set of edges E. A directed edge is a pair (i, j) where
i, j ? V . Associated with the graph is a |V | ? |V |
adjacency matrix W . Each entry Wij in the matrix
is the weight of edge (i, j), or zero if the edge does
not exist.
In a random walk (Lova?sz, 1996), we traverse
from node to node via the edges. Suppose at time
step t, we are at node i. In the next step, we move
to its neighbor j with probability proportional to
the weight of the edge (i, j), i.e., with probability
Wij/
?
jWij . This probability is known as the tran-
sition probability from i to j. Note that the transition
probabilities from a node to its neighbors sum to 1.
The hitting time hij (Aldous and Fill, 2001) from
node i to j is defined as the average number of steps
one takes in a random walk starting from i to visit j
for the first time. Hitting time has the property of be-
ing robust to noise. This is a desirable property for
our system which works on bilingual parallel cor-
pora containing numerous spurious alignments be-
tween phrases (i.e., edges between nodes). However,
as observed by Liben-Nowell and Kleinberg (2003),
hitting time has the drawback of being sensitive to
portions of the graph that are far from the start node
because it considers paths of length up to?.
To circumvent this problem, Sarkar and Moore
(2007) introduced the notion of truncated hitting
time where random walks are limited to have at most
T steps. The truncated hitting time hTij from node i
to j is defined as the average number of steps one
takes to reach j for the first time starting from i in a
random walk that is limited to at most T steps. hTij
is defined to be 0 if i = j or T = 0, and to be T if j
is not reach in T steps. As T ??, hTij ? hij .
In a recent work, Sarkar et al (2008) showed that
truncated hitting time can be approximated accu-
rately with high probability by sampling. They run
M independent length-T random walks from node
i. In m of these runs, node j is visited for the first
time at time steps t1j , . . . , t
m
j . The estimated trun-
cated hitting time is given by
h?Tij =
?m
k=1 t
k
j
M
+ (1?
m
M
)T (1)
They also showed that the number of samples of ran-
dom walks M has to be at least 122 log
2n
d in order
for the estimated truncated hitting time to be a good
estimate of the actual truncated hitting time with
high probability, i.e., for P (|h?Tij?h
T
ij |?T )?1? ?,
where n is the number of nodes in the graph,  and ?
are user-specified parameters, and 0 ? , ? ? 1.
3 Hitting Time Paraphraser (HTP)
HTP takes a query phrase as input, and outputs a list
of paraphrases, with better paraphrases at the top of
146
Figure 1: Graph created from English-French (E-F),
English-German (E-G), and French-German (F-G) bilin-
gual parallel corpora. Bold edges have large positive
weights (high transition probabilities).
the list. HTP also requires as input a set of bilin-
gual parallel corpora that have been processed into
phrase tables of the kind used in statistical machine
translation.
A bilingual parallel corpus is made up of sen-
tences in two languages. Two sentences that are
translations of one another are paired together, and
a phrase in one sentence is aligned with a phrase in
the other with the same meaning. From such align-
ments, we can count for a phrase E both the num-
ber of times it occurs (CountE), and the number of
times it is aligned with a phrase F in the other lan-
guage (CountE,F ). With these counts we can es-
timate the probability of F given E as P (F |E) =
CountE,F
CountE
.
HTP represents the aligned phrases as a graph. A
node corresponds to a phrase, and a directed edge
exists from node i to j if their corresponding phrases
are aligned. The weight of edge (i, j) is given by
P (j|i) which is computed as described in the previ-
ous paragraph.
Figure 1 gives an example of a graph created
from English-French, English-German, and French-
German parallel corpora. We use this figure to il-
lustrate the strengths of HTP. First, by using moder-
ately long random walks, HTP is able to find para-
phrases that are separated by long paths. For ex-
ample, there is a high probability path of length 4
(E1, F1, E2, F2, E3) from E1 to E3. Because of the
path?s high probability, it will appear in many of the
random walks starting from E1 that are sampled on
the graph, and thus E3 will be visited in many of
the samples. This causes the truncated hitting time
hTE1E3 to be small, allowing HTP to find E3 as a
plausible paraphrase of E1. Second, by allowing
edges between nodes of different foreign languages
Table 1: The HTP algorithm.
function HTP (E,C, d, n,m, T, ?, l)
input: E, query phrase
C, tables of aligned phrases
d, maximum distance of nodes from E
n, maximum number of nodes in graph
m, number of samples of random walks
T , maximum number of steps taken by a
random walk
?, probability that estimated truncated hitting
time deviates from actual value by a large
margin (see Equation 1)
l, number of top outgoing edges to select at
each node in a random walk
output:(E?1, . . . , E
?
k), paraphrases of E ranked in
order of increasing hitting times
calls: CreateGraph(E,C, d, n) creates graph G
from C containing at most n nodes that are
at most d steps from E
EstimateHitT imes(E,G,m, T, ?), estimates
the truncated hitting times of each node in G
by running m random walks
PruneNodes((E1, . . . , Ek), G), removes nodes
from G if their hitting times is equal to T .
AddFeatureNodes(G), adds nodes
representing domain knowledge to G
G? CreateGraph(E,C, d, n)
(E1, . . . , Ek)? EstimateHitT imes(E,G,m, T, ?)
G??PruneNodes((E1, . . . , Ek), G)
G???AddFeatureNodes(G?)
(E?1, . . . , E
?
k)? EstimateHitT imes(E,G
??,m, T, ?)
return (E?1, . . . , E
?
k)
(i.e., by not requiring the graph to be bipartite), HTP
allows strong correlation between foreign language
nodes to aid in finding paraphrases. In the figure,
even though E4 and E5 are not linked via a com-
mon foreign language node, there is a high proba-
bility path linking them (E4, F3, G1, E5). This al-
lows HTP to find E5 as a reasonable paraphrase of
E4. Third, HTP enables domain knowledge to be
incorporated as nodes in the graph. For example,
we could incorporate the domain knowledge that
phrases with lots of unigrams in common are likely
to be paraphrases. In Figure 1, the ?feature? node
represents such knowledge, linking E4 and E1 as
possible paraphrases even though they have no for-
eign language nodes in common. Note that such
147
domain knowledge nodes can be linked to arbitrary
nodes, not just English ones.
The HTP algorithm is shown in Table 1. It takes
as input a query phrase and a set of bilingual phrase
tables. The algorithm begins by creating a graph
from the phrase tables. Then it estimates the trun-
cated hitting times of each node from the query node
by sampling random walks of length T . Next it
prunes nodes (and their associated edges) if their
truncated hitting times are equal to T . To the result-
ing graph, it then adds nodes representing domain
knowledge and estimates the truncated hitting times
of the nodes by sampling random walks as before.
Finally, it returns the nodes in the same language as
the query phrase in order of increasing hitting times.
3.1 Graph Creation
An obvious approach to creating a graph from bilin-
gual parallel corpora is to create a node for every
phrase in the corpora, and two directed edges (i, j)
and (j, i) for every aligned phrase pair i and j. Let
H refer to the graph that is created in this manner.
Such an approach is only tractable for small bilin-
gual parallel corpora that would result in a small
H , but not for large corpora containing millions of
sentences, such as those described in Section 4.1.
Therefore we approximate H with a graph H ? that
only contains nodes ?near? to the node representing
the query phrase. Specifically, we perform breadth-
first search starting from the query node up to a
depth d, or until the number of nodes visited in the
search has reached a maximum of n nodes. Some
nodes at the periphery of H ? have edges to nodes
that are not in H ? but are in H . For a periph-
ery node j that has edges to nodes j1, . . . , jk out-
side H ?, we create a ?dummy? node a, and replace
edges (j, j1), . . . , (j, jk) with a single edge (j, a)
with weight
?k
x=1Wj,jx . We also add edges (a, j)
and (a, a) (each with a heuristic weight of 0.5). The
dummy nodes and their edges approximate the tran-
sition probabilities at H ??s periphery. Our empirical
results show that this approximation works well in
practice.
3.2 Graph Pruning
After H ? is created, we run M independent length-
T random walks on it starting from the query node
to estimate the truncated hitting times of all nodes.
Figure 2: Feature nodes representing domain knowledge.
Feature nodes are shaded. The bold node represents a
query phrase. (a) n-gram nodes (b) ?syntax? nodes (c)
?not-substring/superstring-of? nodes.
A node in H ? may have many outgoing edges, most
of which may be due to spurious phrase alignments.
For efficiency, and to reduce the noise due to spuri-
ous edges, we select among a node?s top l outgoing
edges with the highest transition probabilities, when
deciding which node to visit next at each step of a
random walk
For each random walk k, we record the first time
that a node j is visited tkj . Using Equation 1, we es-
timate the truncated hitting time of each node. Then
we remove nodes (and their associated edges) that
are far from the query node, i.e., with times equal
to T . Such nodes either are not visited in any of the
random walks, or are always visited for the first time
at step T .
3.3 Adding Domain Knowledge
Next we add nodes representing domain knowledge
to the pruned graph. In this version of HTP, we im-
plemented three types of feature nodes.
First, we have n-gram nodes. These nodes cap-
ture the domain knowledge that phrases containing
many words in common are likely to be paraphrases.
For each 1 to 4-gram that appears in English phrases,
we create an n-gram node a. We add directed edges
(a, j) and (j, a) if node j represents an English
phrase containing n-gram a. For example, in Fig-
ure 2(a), ?reach the objective? is connected to ?ob-
148
jective? because it contains that unigram. Note that
such nodes create short paths between nodes with
many n-grams in common, thereby reducing the hit-
ting times between them.
Second, we have ?syntax? nodes, which repre-
sent syntactic classes of the start and end words of
English phrases. We created classes such as inter-
rogatives (?whose?, ?what?, ?where?, etc.), articles
(?the?, ?a?, ?an?), etc. For each class c, we cre-
ate syntax nodes ac and a?c to respectively represent
the conditions that a phrase begins and ends with a
word in class c. Directed edges (ac, j) and (j, ac)
are added if node j starts with a word in class c (sim-
ilarly we add (a?c, j) and (j, a
?
c) if it ends with a word
in class c). For example, in Figure 2(b), ?the objec-
tive is? is linked to ?starts with article? because it
begins with ?the?. These syntax nodes allow HTP to
capture broad commonalities about structural distri-
bution, without requiring syntactic equivalence as in
Callison-Burch 2008 (or the use of a parser).
Third, we have ?not-substring/superstring-of?
nodes. We observed that many English phrases (e.g.,
?reach the objective? and ?reach the?) that are super-
strings or substrings of each other tend to be aligned
to several shared non-English phrases in the bilin-
gual parallel corpora used in our experiments. Most
such English phrase pairs are not paraphrases, but
they are linked by many short paths via their com-
mon aligned foreign phrase, and thus have small
hitting times. To counteract this, we create a ?not-
substring/superstring-of? node a. The query node i
is always connected to a via edges (i, a) and (a, i).
We add edges (a, j) and (j, a) if English phrase j
is not a substring or superstring of the query phrase
(see Figure 2(c)).
With the addition of the above, each node rep-
resenting an English phrase can have four kinds
of outgoing edges: edges to foreign phrase nodes,
and edges to the three kinds of feature nodes. Let
fphrase, fngram, fsyntax, fsubstring denote the distri-
bution of transition probabilities among the four
kinds of outgoing edges. Note that fphrase +
fngram + fsyntax + fsubstring = 1.0. These values
are user-specified or can be set with tuning data. An
outgoing edge from English phrase node i that orig-
inally had weight (transition probability) Wij will
now have weight Wij ? fphrase. All k edges from i
to n-gram nodes will have weight fngramk . Likewise
for edges to the other two kinds of feature nodes.
Each of the k outgoing edges from a feature node is
simply set to have a weight of 1k .
After adding the feature nodes, we again run M
independent length-T random walks to estimate the
truncated hitting times of the nodes, and return the
English phrase nodes in order of increasing hitting
times.
4 Experiments
We conducted experiments to investigate how HTP
compares with the state of the art, and to evaluate
the contributions of its components.
4.1 Dataset
We used the Europarl dataset (Koehn, 2005) for
our experiments. This dataset contains English
transcripts of the proceedings of the European
Parliament, and their translations into 10 other
European languages. In the dataset, there are
about a million sentences per language, and En-
glish sentences are aligned with sentences in the
other languages. Callison-Burch (2008) aligned
English phrases with phrases in each of the
other languages using Giza++ (Och and Ney,
2004). We used his English-foreign phrasal align-
ments which are publicly available on the web at
http://ironman.jhu.edu/emnlp08.tar. In addition, we
paired sentences of different non-English languages
that correspond to the same English sentence, and
aligned the phrases using 5 iterations of IBM model
1 in each direction, followed by 5 iterations of HMM
alignment with paired training using the algorithm
described in Liang et al (2006). We further used the
technique of Chen et al (2009) to remove a phrase
alignment F -G (where F and G are phrases in dif-
ferent foreign languages) if it was always aligned
to different phrases in a third ?bridge? foreign lan-
guage. As observed by Chen et al, this helped to
remove spurious alignments. We used Finnish as the
bridge language; when either F or G is Finnish, we
used Spanish as the bridge language; when F and
G were Finnish and Spanish, we used English as
the bridge language. In our experiments, we used
phrases of length 1 to 4 of the following six lan-
guages: English, Danish, German, Spanish, Finnish,
149
and Dutch. All the phrasal alignments between each
pair of languages (15 in total) were used as input to
HTP and its comparison systems. A small subset of
the remaining phrase alignments were used for tun-
ing parameters.
4.2 Systems
We compared HTP to the state-of-the-art SBP sys-
tem (Callison-Burch, 2008). We also investigated
the contribution of the feature nodes by running HTP
without them. In addition, we ran HTP on a bipartite
graph, i.e., one created from English-foreign phrase
alignments only without any phrase alignments be-
tween foreign languages.
We used Callison-Burch (2008)?s implemen-
tation of SBP that is publicly available at
http://ironman.jhu.edu/emnlp08.tar. SBP is based
on BCB (Bannard and Callison- Burch, 2005) which
computes the probability that English phrase E? is a
paraphrase of E using the following formula:
P (E?|E) ?
?
C?C
?
F?C
P (E?|F )P (F |E) (2)
where C is set of bilingual parallel corpora, and F is
a foreign language phrase. Representing phrases as
nodes, and viewing P (E?|F ) and P (F |E) as tran-
sition probabilities of edges (F,E?) and (E,F ), we
see that BCB is summing over the transition prob-
abilities of all length-two paths between E and E?.
All E? paraphrases of E can then be ranked in or-
der of decreasing probability as given by Equation 2.
The SBP system modifies Equation 2 to incorporate
syntactic information, thus:
P (E?|E) ?
1
|C|
?
C?C
?
F?C
P (E?|F, synE))P (F |E, synE) (3)
where synE is the syntax of phrase E, and
P (E?|F, synE)) = 0 ifE? is not of the same syntac-
tic category. From Equation 3, we can see that SBP
constrains E? to have the same syntactic structure
as E. To obtain the syntactic structure of each En-
glish phrase, each English sentence in every parallel
corpus has to be parsed to obtain its parse tree. An
English phrase can have several syntactic structures
because different parse trees can have the phrase as
their leaves, and in each of these, SBP associates the
Table 2: Scoring Standards.
0 Clearly wrong; grammatically incorrect, or
does not preserve meaning
1 Minor grammatical errors (e.g., subject-verb
disagreement or wrong tense), or meaning is
largely preserved but not completely
2 Totally correct; grammatically correct and
meaning is preserved
phrase with all subtrees that have the phrase as their
leaves. SBP thus offers several ways of choosing
which syntactic structure a phrase should be asso-
ciated with. In our experiments, we used the best
performing method of averaging Equation 3 over all
syntactic structures that E is associated with.
4.3 Methodology
To evaluate performance, we used 33,216 En-
glish translations from the Linguistic Data Con-
sortium?s Multiple Translation Chinese (MTC) cor-
pora (Huang et al, 2002). We randomly selected
100 1- to 4-grams that appeared in both Europarl
and MTC sentences (excluding stop words, num-
bers, and phrases containing periods and commas).
For each of those 100 phrases, we randomly se-
lected a MTC sentence containing that phrase. We
then replaced the phrase in the sentence with each
paraphrase output by the systems, and evaluated the
correctness of the paraphrase in the context of the
sentence. We had two volunteers manually score
the paraphrases on a 3-point scale (Table 2), using
a simplified version of the scoring system used by
Callison-Burch (2008). We deemed a paraphrase
to be correct if it was scored 1 or 2, and wrong
if it was scored 0. Evaluation was blind, and the
paraphrases were presented randomly to the volun-
teers. The Kappa measure of inter-annotator agree-
ment was 0.62, which indicates substantial agree-
ment between the evaluators. We took the average
score for each paraphrase.
The parameters used for HTP were as follows
(see Table 1 for parameter descriptions): d =
6, n = 50, 000,m = 1, 000, 000, T = 10, ? =
0.05, l= 20, fphrase = 0.1, fngram = 0.1, fsyntax =
0.4, fsubstring = 0.4. (? 0.03 with these values of
n,m, T, and ?.)
150
Table 3: HTP vs. SBP.
HTP SBP
Correct top-1 paraphrases 71% 53%
Correct top-k paraphrases 54% 39%
Count of correct paraphrases 420 145
Correct paraphrases 43% 39%
Table 4: HTP vs. HTP without feature nodes.
HTP HTP-
NoFeatNodes
Correct top-1 paraphrases 61% 41%
Correct top-k paraphrases 43% 29%
Count of correct paraphrases 420 283
Correct paraphrases 43% 29%
4.4 Results
HTP versus SBP. Comparison between HTP and
SBP is complicated by the fact that the two systems
did not output the same number of paraphrases for
the 100 query phrases. HTP output paraphrases for
all the query phrases, but SBP only did so for 49
query phrases. Of those 49 query phrases, HTP re-
turned at least as many paraphrases as SBP, and for
many it returned more.
To provide a fair comparison, we present results
both for these 49 query phrases, and for all para-
phrases returned by each of the systems. The up-
per half of Table 3 shows results for the 49 query
phrases. The first row of Table 3 reports the per-
centage of top-1 paraphrases from this set that are
correct, while the second row reports the percentage
of correct top-k paraphrases from this set, where k is
the number of queries returned by SBP, and is lim-
ited to at most 10. The value of k may differ for
each query: if SBP and HTP return 3 and 20 para-
phrases respectively, we only consider the top 3. On
the third and fourth rows, we present the number
of correct paraphrases and the percentage of correct
paraphrases among the top 10 paraphrases returned
by HTP for all 100 queries and the corresponding
figures for the 49 queries for SBP. (When a sys-
tem returned fewer than 10 paraphrases for a query,
we consider all the paraphrases for that query.) It
is evident from Table 3 that HTP consistently out-
performs SBP: not only does it return more cor-
rect paraphrases overall (420 versus 145), it also has
Table 5: HTP vs. HTP with bipartite graph.
HTP HTP-
Bipartite
Correct top-1 paraphrases 62% 58%
Correct top-k paraphrases 46% 41%
Count of correct paraphrases 420 361
Correct paraphrases 43% 41%
higher precision (43% versus 39%)
HTP and SBP respectively took 48 and 468 sec-
onds per query on a 3 GHz machine. The times are
not directly comparable because the systems are im-
plemented in different languages (HTP in C# and
SBP in Java), and use different data structures.
HTP without Feature Nodes. Both HTP and HTP
minus feature nodes output paraphrases for each of
the 100 query phrases. Table 4 compares perfor-
mance in the same manner as in Table 3, except that
the ?top-1? and ?top-k? results are over all 100 query
phrases. We see that feature nodes boost HTP?s per-
formance, allowing HTP to return more correct para-
phrases (420 versus 283), and at higher precision
(43% versus 29%).
HTP with Bipartite Graph. Lastly, we investi-
gate the contribution of alignments between foreign
phrases to HTP?s performance. HTP-Bipartite refers
to HTP that is given a set consisting only of English-
foreign phrase alignment as input. HTP-Bipartite
does not return paraphrases for 5 query phrases.
Thus, in Table 5, the ?top-1? and ?top-k? results are
for the 95 query phrases for which both systems re-
turn paraphrases. From the better performance of
HTP, we see that the foreign phrase alignments help
in finding English paraphrases.
5 Related Work
Random walks and hitting times have been suc-
cessfully applied to a variety of applications.
Brand (2005) has used hitting times for collabora-
tive filtering, in which product recommendations to
users are made based on purchase history. In com-
puter vision, hitting times have been used to de-
termine object shape from silhouettes (Gorelick et
al., 2004), and for image segmentation (Grady and
Schwartz, 2006). In social network analysis, Liben-
Nowell and Kleinberg (2003) have investigated the
151
use of hitting times for predicting relationships be-
tween entities. Recently, Mei et al (2008) have used
the hitting times of nodes in a bipartite graph cre-
ated from search engine query logs to find related
queries. They used an iterative algorithm to compute
the hitting time, which converges slowly on large
graphs. In HTP, we have sought to obviate this issue
by using truncated hitting time that can be computed
efficiently by sampling random walks.
Several approaches have been proposed to learn
paraphrases. Barzilay and Mckeown (2001) acquire
paraphrases from a monolingual parallel corpus us-
ing a co-training algorithm. Their co-trained classi-
fier determines whether two phrases are paraphrases
of one another using their surrounding contexts. Lin
and Pantel (2001) learn paraphrases using the dis-
tributional similarity of paths in dependency trees.
Ibrahim et al (2003) generalize syntactic paths in
aligned monolingual sentence pairs in order to gen-
erate paraphrases. Pang et al (2003) merge parse
trees of monolingual sentence pairs, and then com-
press the merged tree into a word lattice that can sub-
sequently be used to generate paraphrases. Recently,
Zhao et al (2008) used dependency parses to learn
paraphrase patterns that include part-of-speech slots.
In other recent work, Das and Smith (2009) use a
generative model for paraphrase detection. Rather
than outputing paraphrases of an input phrase, their
system detects whether two input sentences are para-
phrases of one another.
6 Conclusion and Future Work
We have introduced HTP, a novel approach based
on random walks and hitting times for the learning
of paraphrases from bilingual parallel corpora. HTP
works by converting aligned phrases into a graph,
and finding paraphrases that are ?close? to a phrase
of interest. Compared to previous approaches, HTP
is able to find more paraphrases by traversing paths
of lengths longer than 2; utilizes information in the
edges between foreign phrase nodes; and allows do-
main knowledge to be easily incorporated. Empir-
ical results show its effectiveness in learning new
paraphrases.
As future work, we plan to learn the distribution
of weights on edges to phrase nodes and feature
nodes automatically from data, rather than tuning
them manually, and to develop a probabilistic model
supporting HTP. We intend also to apply HTP to
learning paraphrases in languages other than English
and investigate the impact of the learned paraphrases
on resource-sparse machine translation systems.
Acknowledgments
This work was done while the first author was an
intern at Microsoft Research. We would like to
thank Xiaodong He, Jianfeng Gao, Chris Quirk,
Kristina Toutanova, Bob Moore, and other mem-
bers of the MSR NLP group, along with Dengyong
Zhou (TMSN) for their insightful comments and as-
sistance in the course of this project.
References
David Aldous and Jim Fill. 2001. Reversible
Markov Chains and Random Walks on Graphs.
http://www.stat.berkeley.edu/~aldous/RWG/book.html.
Colin Bannard and Chris Callison-Burch. 2005. Para-
phrasing with bilingual parallel corpora. In Proceed-
ings of the 43rd Annual Meeting of the ACL, pages
597?604.
Regina Barzilay and Lillian Lee. 2003. Learning to
paraphrase: an unsupervised approach using multiple-
sequence alignment. In Proceedings of HLT/NAACL,
pages 16?23.
Regina Barzilay and Kathleen McKeown. 2001. Extract-
ing paraphrases from a parallel corpus. In Proceedings
of the 39th Annual Meeting of the ACL, pages 50?57.
Regina Barzilay, Kathleen McKeown, and Michael El-
hadad. 1999. Information fusion in the context of
multi-document summarization. In Proceedings of the
37th Annual Meeting of the ACL, pages 550?557.
Matthew Brand. 2005. A random walks perspective on
maximizing satisfaction and profit. In Proceedings of
the 8th SIAM Conference on Optimization.
Chris Callison-Burch, Philipp Koehn, and Miles Os-
borne. 2006. Improved statistical machine translation
using paraphrases. In Proceedings of HLT/NAACL,
pages 17?24.
Chris Callison-Burch. 2008. Syntactic constraints on
paraphrases extracted from parallel corpora. In Pro-
ceedings of EMNLP, pages 196?205.
Yu Chen, Martin Kay, and Andreas Eisele. 2009. Inter-
secting multilingual data for faster and better statistical
translations. In Proceedings of HLT/NAACL.
Dipanjan Das and Noah A. Smith. 2009. Paraphrase
identification as probabilistic quasi-synchronous
recognition. In Proceedings of the Joint Conference
152
of the Annual Meeting of the Association for Com-
putational Linguistics and the International Joint
Conference on Natural Language Processing.
Lena Gorelick, Meirav Galun, Eitan Sharon, Ronen
Basri, and Achi Brandt. 2004. Shape representation
and classification using the Poisson equation. In Pro-
ceedings of the Conference on Computer Vision and
Pattern Recognition.
Leo Grady and Eric L. Schwartz. 2006. Isoperimet-
ric graph partitioning for image segmentation. IEEE
Transactions on Pattern Analysis and Machine Intelli-
gence, 28:469?475.
Shudong Huang, David Graff, and George Doddington.
2002. Multiple-translation Chinese corpus. Linguistic
Data Consortium, Philadelphia.
Ali Ibrahim, Boris Katz, and Jimmy Lin. 2003. Ex-
tracting structural paraphrases from aligned monolin-
gual corpora. In Proceedings of the 2nd International
Workshop on Paraphrasing, pages 57?64.
Lidija Iordanskaja, Richard Kittredge, and Alain
Polgue`re. 1991. Lexical selection and paraphrase in
a meaning-text generation model. In Ce?cile L. Paris,
William R. Swartout, and William C. Mann, editors,
Natural Language Generation in Artificial Intelligence
and Computational Linguistics. Kluwer Academic.
David Kauchak and Regina Barzilay. 2006. Para-
phrasing for automatic evaluation. In Proceedings of
HLT/NAACL, pages 455?462.
Philipp Koehn. 2005. Europarl: A parallel corpus for
statistical machine translation. In Proceedings of the
10th Machine Translation Summit.
Nils Lenke. 1994. Anticipating the reader?s problems
and the automatic generation of paraphrases. In Pro-
ceedings of the 15th Conference on Computational
Linguistics, pages 319?323.
Percy Liang, Ben Taskar, and Dan Klein. 2006. Align-
ment by agreement. In Proceedings of HLT/NAACL,
pages 104?111.
David Liben-Nowell and Jon Kleinberg. 2003. The link
prediction problem for social networks. In Proceed-
ings of the 12th International Conference on Informa-
tion and Knowledge, pages 556?559.
Dekang Lin and Patrick Pantel. 2001. Discovery of in-
ference rules for question answering. In Proceedings
of ACM SIGKDD Conference on Knowledge Discov-
ery and Data Mining, pages 323?328.
La?szlo? Lova?sz. 1996. Random walks on graphs: A sur-
vey. In D. Miklo?s, V. T. So?s, and T. Szo?nyi, editors,
Combinatorics, Paul Erdo?s is Eighty, Vol. 2, pages
353?398.
Nitin Madnani, Necip Fazil Ayan, Philip Resnik, and
Bonnie J. Dorr. 2007. Using paraphrases for param-
eter tuning in statistical machine translation. In Pro-
ceedings of the 2nd Workshop on Statistical Machine
Translation, pages 120?127.
Kathleen R. McKeown, Regina Barzilay, David Evans,
Vasileios Hatzivassiloglou, Judith L. Klavans, Ani
Nenkova, Carl Sable, Barry Schiffman, and Sergey
Sigelman. 2002. Tracking and summarizing news on
a daily basis with Columbia?s Newsblaster. In Pro-
ceedings of the 2nd International Conference on HLT
Research, pages 280?285.
Qiaozhu Mei, Dengyong Zhou, and Kenneth Church.
2008. Query suggestion using hitting time. In Pro-
ceeding of the 17th ACM Conference on Information
and Knowledge Management, pages 469?478.
Franz J. Och and Hermann Ney. 2004. The alignment
template approach to statistical machine translation.
Computational Linguistics, 30:417?449.
Bo Pang, Kevin Knight, and Daniel Marcu. 2003.
Syntax-based alignment of multiple translations: Ex-
tracting paraphrases and generating new sentences. In
Proceedings of HLT/NAACL, pages 102?109.
Chris Quirk, Chris Brockett, and William B. Dolan.
2004. Monolingual machine translation for paraphrase
generation. In Proceedings of EMNLP, pages 142?
149.
Deepak Ravichandran and Eduard Hovy. 2002. Learning
surface text patterns for a question answering system.
In Proceedings of the 40th Annual Meeting of the ACL,
pages 41?47.
Stefan Reizler, Alexander Vasserman, Ioannis Tsochan-
taridis, Vibhu Mittal, and Yi Liu. 2007. Statistical
machine translation for query expansion in answer re-
trieval. In Proceedings of the 45th Annual Meeting of
the ACL.
Purnamrita Sarkar and Andrew W. Moore. 2007.
A tractable approach to finding closest truncated-
commute-time neighbors in large graphs. In Proceed-
ings of the 23th Conference on Uncertainty in Artificial
Intelligence.
Purnamrita Sarkar, Andrew W. Moore, and Amit Prakash.
2008. Fast incremental proximity search in large
graphs. In Proceedings of the 25th International Con-
ference on Machine Learning.
Manfred Stede. 1999. Lexical Semantics and Knowl-
edge Representation in Multilingual Text Generation.
Kluwer Academic Publishers.
Shiqi Zhao, Haifeng Wang, Ting Liu, and Sheng Li.
2008. Pivot approach for extracting paraphrase pat-
terns from bilingual corpora. In Proceedings of ACL.
153
