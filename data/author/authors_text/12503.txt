Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 482?491,
MIT, Massachusetts, USA, 9-11 October 2010. c?2010 Association for Computational Linguistics
Multi-document summarization using A* search and discriminative training
Ahmet Aker Trevor Cohn
Department of Computer Science
University of Sheffield, Sheffield, S1 4DP, UK
{a.aker, t.cohn, r.gaizauskas}@dcs.shef.ac.uk
Robert Gaizauskas
Abstract
In this paper we address two key challenges
for extractive multi-document summarization:
the search problem of finding the best scoring
summary and the training problem of learn-
ing the best model parameters. We propose an
A* search algorithm to find the best extractive
summary up to a given length, which is both
optimal and efficient to run. Further, we pro-
pose a discriminative training algorithm which
directly maximises the quality of the best sum-
mary, rather than assuming a sentence-level
decomposition as in earlier work. Our ap-
proach leads to significantly better results than
earlier techniques across a number of evalua-
tion metrics.
1 Introduction
Multi-document summarization aims to present
multiple documents in form of a short summary.
This short summary can be used as a replacement
for the original documents to reduce, for instance,
the time a reader would spend if she were to read
the original documents. Following dominant trends
in summarization research (Mani, 2001), we focus
solely on extractive summarization which simplifies
the summarization task to the problem of identify-
ing a subset of units from the document collection
(here sentences) which are concatenated to form the
summary.
Most multi-document summarization systems de-
fine a model which assigns a score to a candidate
summary based on the features of the sentences in-
cluded in the summary. The research challenges are
then twofold: 1) the search problem of finding the
best scoring summary for a given document set, and
2) the training problem of learning the model pa-
rameters to best describe a training set consisting of
pairs of document sets with model or reference sum-
maries ? typically human authored extractive or ab-
stractive summaries.
Search is typically performed by a greedy al-
gorithm which selects each sentence in decreasing
order of model score until the desired summary
length is reached (see, e.g., Saggion (2005)) or us-
ing heuristic strategies based on position in docu-
ment or lexical clues (Edmundson, 1969; Brandow
et al, 1995; Hearst, 1997; Ouyang et al, 2010).1
We show in this paper that the search problem can
be solved optimally and efficiently using A* search
(Russell et al, 1995). Assuming the model only uses
features local to each sentence in the summary, our
algorithm finds the best scoring extractive summary
up to a given length in words.
Framing summarization as search suggests that
many of the popular training techniques are max-
imising the wrong objective. These approaches train
a classifier, regression or ranking model to distin-
guish between good and bad sentences under an
evaluation metric, e.g., ROUGE (Lin, 2004). The
model is then used during search to find a summary
composed of high scoring (?good?) sentences (see
for a review Ouyang et al (2010)). However, there
is a disconnect between the model used for training
and the model used for prediction. In this paper we
present a solution to this disconnect in the form of
a training algorithm that optimises the full predic-
tion model directly with the search algorithm intact.
The training algorithm learns parameters such that
1Genetic algorithms have also been devised for solving the
search problem (see, e.g., Riedhammer et al (2008)), however
these approaches do not guarantee optimality, nor are they effi-
cient enough to be practicable for large datasets.
482
the best scoring whole summary under the model
has a high score under the evaluation metric. We
demonstrate that this leads to significantly better test
performance than a competitive baseline, to the tune
of 3% absolute increase for ROUGE-1, -2 and -SU4.
The paper is structured as follows. Section 2
presents the summarization model. Next in sec-
tion 3 we present an A* search algorithm for finding
the best scoring (argmax) summary under the model
with a constraint on the maximum summary length.
We show that this algorithm performs search effi-
ciently, even for very large document sets composed
of many sentences. The second contribution of the
paper is a new training method which directly opti-
mises the summarization system, and is presented in
section 4. This uses the minimum error-rate training
(MERT) technique from machine translation (Och,
2003) to optimise the summariser?s output to an ar-
bitrary evaluation metric. Section 5 describes our
experimental setup and section 6 the results. Finally
we conclude in section 7.
2 Summarization Model
Extractive multi-document summarization aims to
find the most important sentences from a set of doc-
uments, which are then collated and presented to
the user in form of a short summary. Following
the predominant approach to data-driven summari-
sation, we define a linear model which scores sum-
maries as the weighted sum of their features,
s(y|x) = ?(x,y) ? ? , (1)
where x is the document set, composed of k sen-
tences, y ? {1 . . . k} are the set of selected sen-
tence indices, ?(?, ?) is a feature function which re-
turns a vector of features for the candidate summary
and ? are the model parameters. We further assume
that the features decompose with the sentences in
the summary, ?(x,y) =
?
i?y ?(xi), and there-
fore the scoring function also decomposes along the
same lines,
s(y|x) =
?
i?y
?(xi) ? ? . (2)
While this assumption greatly simplifies inference, it
does constrain the representative power of the model
by disallowing global features, e.g., those which
measure duplication in the summary.2 Under this
model, the search problem is to solve
y? = arg max
y
s(y|x) , (3)
for which we develop a best-first algorithm using A*
search, as described in section 3. The training chal-
lenge is to find the parameters, ?, to best model the
training set. This is achieved by finding ? such that
y? is similar to the gold standard summary accord-
ing to an automatic evaluation metric, as described
in section 4.
3 A* Search
The prediction problem is to find the best scoring
extractive summary (see Equation 3) up to a given
length, L. At first glance, this appears to be a sim-
ple problem that might be solved efficiently with a
greedy algorithm, say by taking the sentences in or-
der of decreasing score and stopping just before the
summary exceeds the length threshold. However,
the greedy algorithm cannot be guaranteed to find
the best summary; to do so requires arbitrary back-
tracking to revise previous incorrect decisions.
The problem of constructing the summary can be
considered a search problem in which we start with
an empty summary and incrementally enlarge the
summary by concatenating a sentence from our doc-
ument set. The search graph starts with an empty
summary (the starting state) and each outgoing edge
adds a sentence to produce a subsequent state, and
is assigned a score under the model. A goal state is
any state with no more words than the given thresh-
old. The summarisation problem is then equivalent
to finding the best scoring path (summed over the
edge scores) between the start state and a goal state.
The novel insight in our work is to use A* search
(Russell et al, 1995) to solve the prediction prob-
lem. A* is a best-first search algorithm which can
efficiently find the best scoring path or the n-best
paths (unlike the greedy algorithm which is not op-
timal, and the backtracking variant which is not ef-
ficient). The search procedure requires a scoring
function for each state, here s(y|x) from (2), and
2Our approach could be adapted to support global features,
which would require changes to the heuristic for A* search to
bound the score obtainable from the global features. This may
incur an additional computational cost over a purely local fea-
ture model and perhaps also necessitate using beam search.
483
a heuristic function which estimates the additional
score to get from a given state to a goal state. For
the search to be optimal ? guaranteed to find the best
scoring path as the first solution ? the heuristic must
be admissible, meaning that it bounds from above
the score for reaching a goal state. We present three
different admissible heuristics later in this section,
which bound the score with differing tightness and
consequently different search cost.
Algorithm 1 presents A* search for our extractive
summarisation model. Given a set of sentences to
summary, a scoring and a heuristic function, it finds
the best scoring summary. This is achieved by build-
ing the search graph incrementally, and storing each
frontier state in a priority queue (line 1) which is
sorted by the sum of the state?s score and its heuris-
tic. These states are popped off the queue (line 3)
and expanded by adding a sentence, which is then
added to the schedule (lines 8?14). We designate
special finishing states using a boolean variable (the
last entry in the tuple in lines 1, 7 and 12). Fin-
ishing states (with value T) denote ceasing to ex-
pand the summary, and consequently their scores
do not include the heuristic component. When-
ever one of these states is popped in line 2, we
know that it outscores all competing hypotheses and
therefore represents the optimal summary (because
the heuristic is guaranteed to never underestimate
the cost to a goal state from an unfinished state).3
Note that in algorithm 1 we create the summary
by building a list of sentence indices in sorted or-
der to avoid spurious ambiguity which would un-
necessarily expand the search space. The function
length(y,x) =
?
n?y length(xn) returns the length
of sentences specified.
We now return to the problem of defining the
heuristic function, h(y;x, l) which provides an up-
per bound on the additional score achievable in
reaching a goal state from state y. We present three
different variants of increasing fidelity, that is, that
bound the cost to a goal state more tightly. Algo-
rithm 2 is the simplest, which simply finds the max-
imum score per word from the set of unused sen-
3To improve the efficiency of Algorithm 1 we make a small
modification to avoid expanding every possible edge in step 8,
of which there are O(k) options. Instead we expand a small
number (here, 3) at a time and defer the remaining items until
later by inserting a special node into the schedule. These special
nodes are represented using a third ?to-be-continued? state into
the done flag.
Algorithm 1 A* search for extractive summarization.
Require: set of sentences, x = x1, . . . , xk
Require: scoring function s(?)
Require: heuristic function h(?)
Require: summary length limit L
1: schedule = [(0, ?, F)] {priority queue of triples}
{(A* score, sentence indices, done flag)}
2: while schedule 6= [] do
3: v,y, f ? pop(schedule)
4: if f = T then
5: return y {success}
6: else
7: push(schedule, (s(y|x),y,T))
8: for y ? [max(y) + 1, k] do
9: y? ? y ? y
10: if length(y?,x) ? L then
11: v? ? s(y?|x) + h(y?;x, l)
12: push(schedule, (v?,y?, F))
13: end if
14: end for
15: end if
16: end while
tences and then extrapolates this out over the re-
maining words available to the length threshold. In
the algorithm, we use the shorthand sn = ?(xn) ? ?
for sentence n?s score, ln = length(xn) for its length
and ly =
?
n?y ln for the total length of the current
state (unfinished summary).
Algorithm 2 Uniform heuristic, h1(y;x, L)
Require: x sorted in order of score/length
1: n? max(y) + 1
2: return (L? ly)max
(
sn
ln
, 0
)
The h1 heuristic is overly simple in that it assumes
we can ?reuse? a high scoring short sentence many
times despite this being disallowed by the model.
For this reason we develop an improved bound, h2,
in Algorithm 3. This incrementally adds each sen-
tence in order of its score-per-word until the length
limit is reached. If the limit is to be exceeded,
the heuristic scales down the final sentence?s score
based on the fraction of words than can be used to
reach the limit.
The fractional usage of the final sentence in h2
could be considered overly optimistic, especially
when the state has length just shy of the limit L. If
the next best ranked sentence is a long one, then it
will be used in the heuristic to over-estimate of the
state. This is complicated to correct, and doing so
exactly would require full backtracking which is in-
tractable and would obviate the entire point of using
A* search. Instead we use a subtle modification in
h3 (Alg. 4) which is equivalent to h2 except in the
484
Algorithm 3 Aggregated heuristic, h2(y;x, L)
Require: x sorted in order of score/length
1: v ? 0
2: l? ? ly
3: for n ? [max(y) + 1, k] do
4: if sn ? 0 then
5: return v
6: end if
7: if l? + ln ? L then
8: l? ? l? + ln
9: v ? v + sn
10: else
11: return v + lnL?l? sn
12: end if
13: end for
14: return v
instance where the next best score/word sentence is
too long, where it skips over these sentences until
it finds the best scoring sentence that does fit. This
helps to address the overestimate of h2 and should
therefore lead to a smaller search graph and faster
runtime due to its early elimination of dead-ends.
Algorithm 4 Agg.+final heuristic, h3(y;x, L)
Require: x sorted in order of score/length
1: n? max(y) + 1
2: if n ? k ? sn > 0 then
3: if ly + ln ? L then
4: return h2(y;x, L)
5: else
6: form ? [n+ 1, k] do
7: if ly + lm ? L then
8: return sm
L?ly
lm
9: end if
10: end for
11: end if
12: end if
13: return 0
The search process is illustrated in figure 1. When
a node is visited in the search, if it satisfied the
length constraint then the all its child nodes are
added to the schedule. These nodes are scored with
the score for the summary thus far plus a heuristic
term. For example, the value of 4+1.5=5.5 for the
{1} node arises from a score of 4 plus a heuristic of
(7? 5) ? 34 = 1.5, reflecting the additional score that
would arise if it were to use half of the next sentence
to finish the summary. Note that in finding the best
two summaries the search process did not need to
instantiate the full search graph.
To test the efficacy of A* search with each of the
different heuristic functions, we now present empir-
ical runtime results. We used the training data as
described in Section 5.2 and for each document set
start
(4+1.5,{1},F)
+1 (3+2,{2},F)+2 (2+2,{3},F)+3
(1+0,{4},F)+4
(0,{},T)
finish
(7+0,{1,2},F)+2 (6+0,{1,3},F)+3
(5+0,{1,4},F)+4
(5+0,{2,3},F)+3
(4+0,{2,4},F)+4
(5,{1,4},T)finish
(6+0,{2,3,4},F)+4
(5,{2,3},T)finish
Figure 1: Example of the A* search graph created to find
the two top scoring summaries of length ? 7 when sum-
marising four sentences with scores of 4, 3, 2 and 1 re-
spectively and lengths of 5, 4, 3 and 1 respectively. The
h1 heuristic was used and the score and heuristic scores
are shown separately for clarity. Bold nodes were visited
while dashed nodes were visited but found to exceed the
length constraint.
generated the 100-best summaries with word limit
L = 200. Figure 2 shows the number of nodes
and edges visited by A* search, reflecting the space
and time cost of the algorithm, as a function of the
number of sentences in the document set being sum-
marised. All three heuristics shown an empirical
increase in complexity that is roughly linear in the
document size, although there are some notable out-
liers, particularly for the uniform heuristic. Surpris-
ingly the aggregated heuristic, h2, is not consider-
ably more efficient than the uniform heuristic h1,
despite bounding the cost more precisely. However,
the aggregated+final heuristic, h3, consistently out-
performs the other two methods. For this reason we
have used h3 in all subsequent experimentation.
4 Training
We frame the training problem as one of finding
model parameters, ?, such that the predicted out-
put, y? closely matches the gold standard, r.4 The
quality of the match is measured using an automatic
evaluation metric. We adopt the standard machine
learning terminology of loss functions, which mea-
sure the degree of error in the prediction, ?(y?, r).
In our case the accuracy is measured by the ROUGE
4The gold standard is typically an abstractive summary, and
as such it is usually impossible for an extractive summarizer to
match it exactly.
485
ll
l
l
l
l
l
l l
l
l
l
l
l
l
ll
l
l
l
l
lll
l
l
l
l
l
l
l l
l
l
l
l
l
l
l
l
l
l
l
l
lll
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l ll
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l l
l
l
l l
l
l
l
l
l
l
l
l
l l l
l
ll l
l l
l
l
l
l
l
l
l
l
l l
ll
l l
l
ll
l
l
l
ll
l
l
ll
l
l
l
ll
ll
l l
l
l
l
l
l
l l
l
l
ll
l
l
l
l
l
l
l
l
ll
l
l
5 10 20 50 100 200 500 1000 2000
1e+
02
1e+
03
1e+
04
1e+
05
1e+
06
sentences in document set
tota
l edg
es a
nd n
odes
l uniformaggregatedaggregated+final
Figure 2: Efficiency of A* search search is roughly linear
in the number of sentences in the document set. The y
axis measures the search graph size in terms of the num-
ber of edges in the schedule and the number of nodes
visited. Measured with the final parameters after training
to optimise ROUGE-2 with the three different heuristics
and expanding five nodes in each step.
score, R, and the loss is simply 1 - R. The training
problem is to solve
?? = arg min
?
?(y?, r) , (4)
where with a slight abuse of notation, y? and r are
taken to range over the corpus of many document-
sets and summaries.
To optimise the weights we use the minimum er-
ror rate training (MERT) technique (Och, 2003), as
used for training statistical machine translation sys-
tems. This approach is a first order optimization
method using Powell search to find the parameters
which minimise the loss on the training data. MERT
requires n-best lists which it uses to approximate
the full space of possible outcomes. We use the
A* search algorithm to construct these n-best lists,5
and use MERT to optimise the ROUGE score on the
training set for the R-1, R-2 and R-SU4 variants of
the metric.
5We used n = 100 in our experiments.
5 Experimental settings
In this section we describe the features for which we
learn weights. We also describe the input data used
in training and testing.
5.1 Summarization system
The summarizer we use is an extractive, query-based
multi-document summarization system. It is given
two inputs: a query (place name) associated with an
image and a set of documents. The summarizer uses
the following features, as reported in previous work
(Edmundson, 1969; Brandow et al, 1995; Radev et
al., 2001; Conroy et al, 2005; Aker and Gaizauskas,
2009; Aker and Gaizauskas, 2010a):
? querySimilarity: Sentence similarity to the
query (cosine similarity over the vector repre-
sentation of the sentence and the query).
? centroidSimilarity: Sentence similarity to the
centroid. The centroid is composed of the 100
most frequently occurring non stop words in
the document collection (cosine similarity over
the vector representation of the sentence and
the centroid). For each word/term in the vec-
tor we store a value which is the product of
the term frequency in the document and the in-
verse document frequency, a measurement of
the term?s distribution over the set of docu-
ments (Salton and Buckley, 1988).
? sentencePosition: Position of the sentence
within its document. The first sentence in the
document gets the score 1 and the last one gets
1
n where n is the number of sentences in the
document.
? inFirst5: Binary feature indicating whether the
sentence occurs is one of the first 5 sentences
of the document.
? isStarter: A sentence gets a binary score if it
starts with the query term (e.g. Westminster
Abbey, The Westminster Abbey, The Westmin-
ster or The Abbey) or with the object type, e.g.
The church. We also allow gaps (up to four
words) between the and the query/object type
to capture cases such as The most magnificent
abbey, etc.
? LMProb: The probability of the sentence un-
der a unigram language model. We trained
a separate language model on Wikipedia arti-
cles about locations for each object type, e.g.,
486
church, bridge, etc. When we generate a sum-
mary about a location of type church, for in-
stance, then we apply the church language
model on the related input documents related
to the location.6
? sentenceCount: Each sentence gets assigned a
value of 1. This feature is used to learn whether
summaries with many sentences are better than
summaries with few sentences or vice versa.
? wordCount: Number of words in the summary,
to decide whether the model should favor long
summaries or short ones.
5.2 Data
For training and testing we use the freely avail-
able image description corpus described in Aker and
Gaizauskas (2010b). The corpus is based around
289 images of static located objects (e.g Eiffel
Tower, Mont Blanc) each with a manually assigned
place name and object type category (e.g. church,
mountain). For each place name there are up to
four model summaries that were created manually
after reading existing image descriptions taken from
the VirtualTourist travel community web-site. Each
summary contains a minimum of 190 and a maxi-
mum of 210 words. We divide this set of 289 place
names into training and testing sets. Both sets are
described in the following subsections.
Training We use 184 place names from the 289
set for training feature weights. For each train-
ing place name we gather all descriptions associ-
ated with it from VirtualTourist. We compute for
each sentence in each description a ROUGE score
by comparing the sentence to those included in the
model summaries for that particular place name and
retaining the highest score. Table 1 gives some de-
tails about this training data.
We use ROUGE as a metric to maximize be-
cause it is also used in DUC7 and TAC.8 How-
ever, it should be noted that any automatic metric
could be used instead of ROUGE. In particular we
use ROUGE 1 (R-1), ROUGE 2 (R-2) and ROUGE
SU4 (R-SU4). R-1 and R-2 compute the number
6For our training and testing sets we manually assigned each
location to its corresponding object type (Aker and Gaizauskas,
2009).
7http://duc.nist.gov/
8http://www.nist.gov/tac/
Max Min Avg
Sentences/place 1724 3 260
Words/sentence 37 3 17
Table 1: The training input data contains 184 place
names with 42333 sentences in total. The numbers in
the columns give detail about the number of sentences
for each place and the lengths of the sentences.
Max Min Avg
Documents/place 20 5 12
Sentences/place 1716 15 132
Sentences/document 275 1 10
Words/sentence 211 1 20
Table 2: In domain test data. The numbers in the columns
give detail about the number of documents (descriptions)
for each place, number of sentences for each place and
document (description) and the lengths of the sentences.
of uni-gram and bi-gram overlaps, respectively, be-
tween the automatic and model summaries. R-SU4
allows bi-grams to be composed of non-contiguous
words, with a maximum of four words between the
bi-grams.
Testing For testing purposes we use the rest of
the place names (105) from the 289 place name
set. For each place name we use a set of input
documents, generate a summary from these docu-
ments using our summarizer and compare the results
against model summaries of that place name using
ROUGE. We experimented with two different input
document types: out of domain and in domain.
The in domain documents are the VirtualTourist
original image descriptions from which the model
summaries were derived. As with the training set
we take all place name descriptions for a particular
place and use them as input documents to our sum-
marizer. Table 2 summarizes these input documents.
The out of domain documents are retrieved from
the web. Compared to the in domain documents
these documents should more challenging to sum-
marize because they will contain different kinds
of documents to those seen in training. For each
place name we retrieved the top ten related web-
documents using the Yahoo! search engine with the
place name as a query. The text from these docu-
ments is extracted using an HTML parser and passed
to the summarizer. Table 3 gives an overview of this
data.
487
Max Min Avg
Sentences/place 1773 55 328
Sentence/document 874 1 32
Words/sentence 236 1 21
Table 3: Out of domain test data. The numbers in the
columns give detail about the number of sentences for
each place and document and the lengths of the sentences.
6 Results
To evaluate our approach we used two different as-
sessment methods: ROUGE (Lin, 2004) and manual
readability. In the following we present the results
of each assessment.
6.1 Automatic Evaluation using ROUGE
We report results for training and testing. In
both training and testing we distinguish between
three different summaries: wordLimit, sentence-
Limit and regression. WordLimit and sentenceLimit
summaries are the ones generated using the model
trained by MERT. As described in section 4 we
trained the summariser using the A* search decoder
to maximise the ROUGE score of the best scoring
summaries. We used the heuristic function h3 in
A* search because it is the best performing heuris-
tic, and 100-best lists. To experiment with differ-
ent summary length conditions we differentiate be-
tween summaries with a word limit (wordLimit, set
to 200 words) and summaries containing N number
of sentences (sentenceLimit) as stop condition in A*
search. We set N so that in both wordLimit and sen-
tenceLimit summaries we obtain more or less the
same number of words (because our training data
contains on average 17 words for each word we set
N to 12, 12*17=194). However, this is only the case
in the training. In the testing for both wordLimit and
sentenceLimit we generate summaries with the same
word limit constraint which allows us to have a fair
comparison between the ROUGE recall scores.
The regression summaries are our baseline. In
these summaries the sentences are ranked based on
the weighted features produced by Support Vec-
tor Regression (SVR).9 Ouyang et al (2010) use
multi-document summarization and linear regres-
sion methods to rank sentences in the documents.
As regression model they used SVR and showed
9We use the term regression to refer to SVR.
Type metric R-1 R-2 R-SU4
wordLimit
R-1 0.5792 0.3176 0.3580
R-2 0.5656 0.3208 0.3510
R-SU4 0.5688 0.3197 0.3585
sentenceLimit
R-1 0.5915 0.3507 0.3881
R-2 0.5783 0.3601 0.3890
R-SU4 0.5870 0.3546 0.3929
regression
R-1 0.4993 0.1946 0.2448
R-2 0.4833 0.1949 0.2413
R-SU4 0.5009 0.2031 0.2562
Table 4: ROUGE scores obtained on the training data.
that it out-performed classification and Learning To
Rank methods on the DUC 2005 to 2007 data. For
comparison purpose we use SVR as a baseline sys-
tem for learning feature weights. It should be noted
that these weights are learned based on single sen-
tences. However, to have a fair comparison between
all our summary types we use these weights to gen-
erate summaries using the A* search with the word
limit as constraint. We do this for reporting both for
training and testing results.
The results for training are shown in Table 4. The
table shows ROUGE recall numbers obtained by
comparing model summaries against automatically
generated summaries on the training data. Because
in training we used three different metrics (R-1, R-2,
R-SU4) to train weights we report results for each of
these three different ROUGE metrics.
In Table 4 we can see that the scores for wordLimit
and sentenceLimit type summaries are always at
maximum on the metric they were trained on (this
can be observed by following the main diagonal of
the result matrix). This confirms that MERT is max-
imizing the metric for which it was trained. How-
ever, this is not the case for regression results. The
scores obtained with R-SU4 metric trained weights
achieve higher scores on R-1 and R-2 compared to
the scores obtained using weights trained on those
metrics. This is most likely due to SVR being
trained on sentences rather than over entire sum-
maries, and thereby not adequately optimising the
metric used for evaluation.
The results for testing are shown in Tables 5 and
6. As with the training setting we report ROUGE re-
call scores. We use the testing data described in sec-
tion 5.2 for this setting. However, because we have
two different input document sets we report sepa-
rate results for each of these (Table 5 shows result
for in domain data and Table 6 shows result for out
488
Type metric R-1 R-2 R-SU4
wordLimit
R-1 0.3733 0.0842 0.1399
R-2 0.3731 0.0842 0.1402
R-SU4 0.3627 0.0794 0.1340
sentenceLimit
R-1 0.3664 0.0774 0.1321
R-2 0.3559 0.0717 0.1251
R-SU4 0.3629 0.0778 0.1312
regression
R-1 0.3431 0.0669 0.1229
R-2 0.2934 0.0560 0.1043
R-SU4 0.3417 0.0668 0.1226
Table 5: ROUGE scores obtained on the testing data. The
automated summaries are generated using the in domain
input documents.
Type metric R-1 R-2 R-SU4
wordLimit
R-1 0.3758 0.0882 0.1421
R-2 0.3755 0.0895 0.1423
R-SU4 0.369 0.0812 0.137
sentenceLimit
R-1 0.3541 0.0693 0.1226
R-2 0.3426 0.0638 0.1157
R-SU4 0.3573 0.073 0.1251
regression
R-1 0.3392 0.0611 0.1179
R-2 0.3422 0.0606 0.1164
R-SU4 0.3413 0.0606 0.1176
Table 6: ROUGE scores obtained on the testing data. The
automated summaries are generated using the out of do-
main input documents.
of domain data). Again as with the training setting
we report results for the different metrics (R-1, R-2,
R-SU4) separately.
From Table 5 we can see that the wordLimit sum-
maries score highest compared to the other two types
of summaries. This is different from the train-
ing results where sentenceLimit summary type sum-
maries are the top scoring ones. As mentioned ear-
lier the sentenceLimit summaries contain exactly 12
sentences, where on average each sentence in the
training data has 17 words. We picked 12 sen-
tences to achieve roughly the same word limit con-
straint (12 ? 17 = 204) so they can be compared
to the wordLimit and regression type summaries.
However, these sentenceLimit summaries have an
average of 221 words, which explains the higher
ROUGE recall scores seen in training compared to
testing (where a 200 word limit was imposed).
The wordLimit summaries are significantly better
than the scores from the other summary types ir-
respective of the evaluation metric.10 It should be
10Significance is reported at level p < 0.001. We used
Wilcoxson signed ranked test to perform significance.
noted that these summaries are the only ones where
the training and testing had the same condition in
A* search concerning the summary word limit con-
straint. The scores in sentenceLimit type summaries
are significantly lower than wordLimit summaries,
despite using MERT to learn the weights. This
shows that training the true model is critical for
getting good accuracy. The regression type sum-
maries achieved the worst ROUGE metric scores.
The weights used to generate these summaries were
trained on single sentences using SVR. These results
indicate that if the goal is to generate high scoring
summaries under a length limit in testing, then the
same constraint should also be used in training.
From Table 5 and 6 we can see that the summaries
obtained from VirtualTourist captions (in domain
data) score roughly the same as the summaries gen-
erated using web-documents (out of domain data) as
input. A possible explanation is that in many cases
the VirtualTourist original captions contain text from
Wikipedia articles, which are also returned as results
from the web search. Therefore the web-document
sets included similar content to the VirtualTourist
captions.
6.2 Manual Evaluation
We also evaluated our summaries using a readabil-
ity assessment as in DUC and TAC. DUC and TAC
manually assess the quality of automatically gener-
ated summaries by asking human subjects to score
each summary using five criteria ? grammaticality,
redundancy, clarity, focus and coherence criteria.
Each criterion is scored on a five point scale with
high scores indicating a better result (Dang, 2005).
For this evaluation we used the best scoring sum-
maries from the wordLimit summary type (R-1, R-2
and R-SU4) generated using web-documents (out of
domain documents) as input. We also evaluate the
regression summary types generated using the same
input documents to investigate the correlation be-
tween high and low ROUGE metric scores to man-
ual evaluation ones. From the regression summary
type we only use summaries under the R2 and RSU4
trained models.
In total we evaluated five different summary types
(three from wordLimit and two from regression).
For each type we randomly selected 30 place names
and asked three people to assess the summaries for
these place names. Each person was shown all 150
489
Criterion wordLimit regression
R1 R2 RSU4 R2 RSU4
clarity 4.03 3.92 3.99 3.00 2.92
coherence 3.31 3.06 2.99 2.12 1.88
focus 3.79 3.56 3.54 2.44 2.29
grammaticality 4.21 4.13 4.13 3.93 3.87
redundancy 4.19 4.33 4.41 4.47 4.44
Table 7: Manual evaluation results for the wordLimit (R1,
R2, RSU4) and regression (R2, RSU4) summary types.
The numbers in the columns are the average scores.
summaries (30 from each summary type) in a ran-
dom way and was asked to assess them according to
the DUC and TAC manual assessment scheme. The
results are shown in Table 7.11
From Table 7 we can see that overall the
wordLimit type summaries perform better than the
regression ones. For each metric in regression sum-
mary types (R-2 and R-SU4) we compute the sig-
nificance of the difference with the same metrics
in wordLimit summary types.12 The results for the
clarity, coherence and focus criteria in wordLimit
summaries are significantly better than in regression
ones (p<0.001) irrespective of the training metric.
These results concur with the automatic evaluation
results as described in section 6.1. However, this
is not the case for the grammaticality and redun-
dancy criteria. Although in regression type sum-
maries the scores for the grammaticality criterion
are lower than those in wordLimit summaries the
difference is not significant. Furthermore, we can
see that the redundancy scores for regression sum-
maries are slightly higher than those for wordLimit
summaries.
One reason for these differences might be the
way we trained feature weights for wordLimit and
regression summaries. As mentioned above, fea-
ture weights for wordLimit summaries are trained
using summaries with a specific word limit con-
straint, whereas the weights for the regression sum-
maries are learned using single sentences. Maxi-
mizing the ROUGE metrics using ?final or output
11We computed the agreement between the users using intra
class correlation with Cronbach?s Alpha where the correlation
coefficient ranges between 0 and 1. Numbers close to 1 indicate
high correlation and numbers close to 0 indicate low correlation.
For the clarity criterion the assessors? correlation coefficient is
0.547, for coherence 0.687, for focus 0.688, for grammaticality
0.232 and for redundancy 0.453.
12We compute significance test for the manual evaluation re-
sults using ? square.
like summaries? will lead to a higher content agree-
ment between the training and the model summaries
whereas this is not guaranteed with single sentences.
With single sentences we have only a guarantee for
high content overlap between single training and
model sentences. However, when these sentences
are combined into summaries it is not guaranteed
that these summaries will also have high content
overlap with the entire model ones. Therefore we
believe if there is a high content agreement between
the training and model summaries this could lead to
more readable summaries. However, as we can see
from Table 7 this hypothesis does not hold for all
criteria. In case of the redundancy criterion we have
compared to wordLimit summary type high scores
in regression summaries although wordLimit sum-
maries are significantly better than regression ones
when it concerns the ROUGE scores. Thus it is
likely that by aggressively optimising the ROUGE
metric the model learns to game the metric, which
does not penalise redundancy in the summaries.
As such it may no longer possible to extrapolate
trends from earlier correlation studies against human
judgements (Lin, 2004).
To minimize redundancy in summaries it is nec-
essary to also take into consideration global features
addressing the linguistic aspects of the summaries.
Furthermore, instead of ROUGE recall scores which
do not take the repetition of information into consid-
eration, ROUGE precision scores could be used as a
metric in order to minimize the redundant content in
the summaries.
7 Conclusion
In this paper we have proposed an A* search ap-
proach for generating a summary from a ranked list
of sentences and learning feature weights for a fea-
ture based extractive multi-document summariza-
tion system. We developed an algorithm to learn
optimize an arbitrary metric and showed that our
approach significantly outperforms state of the art
techniques. Furthermore, we highlighted the impor-
tance of uniformity in training and testing and ar-
gued that if the goal is to generate high scoring sum-
maries under a length limit in testing, then the same
constraint should also be used in training.
In this paper we experimented with sentence-local
features. In the future we plan to expand this fea-
ture set with global features, especially ones mea-
490
suring lexical diversity in the summaries to reduce
the redundancy in them. We will investigate vari-
ous ways of incorporating these global features into
our A* search. However this will incur an additional
computational cost over a purely local feature model
and therefore may necessitate using an approximate
beam search. We also plan to investigate using other
metrics in training in order to reduce redundant in-
formation in the summaries. Finally, we have made
our summarizer publicly available as open-source
software.13
References
A. Aker and R. Gaizauskas. 2009. Summary Gener-
ation for Toponym-Referenced Images using Object
Type Language Models. International Conference
on Recent Advances in Natural Language Processing
(RANLP) September 14-16, 2009, Borovets, Bulgaria.
A. Aker and R. Gaizauskas. 2010a. Generating im-
age descriptions using dependency relational patterns.
Proc. of the ACL 2010, Upsala, Sweden.
A. Aker and R. Gaizauskas. 2010b. Model Summaries
for Location-related Images. In Proc. of the LREC-
2010 Conference.
R. Brandow, K. Mitze, and L.F. Rau. 1995. Automatic
condensation of electronic publications by sentence
selection* 1. Information Processing & Management,
31(5):675?685.
J.M. Conroy, J.D. Schlesinger, and J.G. Stewart. 2005.
CLASSY query-based multi-document summariza-
tion. Proc. of the 2005 Document Understanding
Workshop, Boston.
H.T. Dang. 2005. Overview of DUC 2005. DUC 05
Workshop at HLT/EMNLP.
H. Edmundson, P. 1969. New Methods in Automatic
Extracting. Journal of the Association for Computing
Machinery, 16:264?285.
M.A. Hearst. 1997. TextTiling: segmenting text into
multi-paragraph subtopic passages. Computational
linguistics, 23(1):33?64.
C-Y. Lin. 2004. Rouge: A package for automatic evalua-
tion of summaries. Text Summarization Branches Out:
Proc. of the ACL-04 Workshop, pages 74?81.
I. Mani. 2001. Automatic Summarization. John Ben-
jamins Publishing Company.
F.J. Och. 2003. Minimum error rate training in statistical
machine translation. Proc. of the 41st Annual Meeting
on Association for Computational Linguistics-Volume
1, page 167.
13Available from http://www.dcs.shef.ac.uk/
?tcohn/a-star
Y. Ouyang, W. Li, S. Li, and Q. Lu. 2010. Applying
regression models to query-focused multi-document
summarization. Information Processing & Manage-
ment.
D.R. Radev, S. Blair-Goldensohn, and Z. Zhang. 2001.
Experiments in single and multi-document summa-
rization using MEAD. Document Understanding Con-
ference.
K. Riedhammer, D. Gillick, B. Favre, and D. Hakkani-T
?ur. 2008. Packing the meeting summarization knap-
sack. Proc. Interspeech, Brisbane, Australia.
S.J. Russell, P. Norvig, J.F. Canny, J. Malik, and D.D.
Edwards. 1995. Artificial intelligence: a modern ap-
proach. Prentice hall Englewood Cliffs, NJ.
H. Saggion. 2005. Topic-based Summarization at
DUC 2005. Document Understanding Conference
(DUC05).
G. Salton and C. Buckley. 1988. Term-weighting ap-
proaches in automatic text retrieval. Information Pro-
cessing and Management: an International Journal,
24(5):513?523.
491
Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 1250?1258,
Uppsala, Sweden, 11-16 July 2010. c?2010 Association for Computational Linguistics
Generating image descriptions using dependency relational patterns
Ahmet Aker
University of Sheffield
a.aker@dcs.shef.ac.uk
Robert Gaizauskas
University of Sheffield
r.gaizauskas@dcs.shef.ac.uk
Abstract
This paper presents a novel approach
to automatic captioning of geo-tagged
images by summarizing multiple web-
documents that contain information re-
lated to an image?s location. The summa-
rizer is biased by dependency pattern mod-
els towards sentences which contain fea-
tures typically provided for different scene
types such as those of churches, bridges,
etc. Our results show that summaries bi-
ased by dependency pattern models lead
to significantly higher ROUGE scores than
both n-gram language models reported in
previous work and also Wikipedia base-
line summaries. Summaries generated us-
ing dependency patterns also lead to more
readable summaries than those generated
without dependency patterns.
1 Introduction
The number of images tagged with location infor-
mation on the web is growing rapidly, facilitated
by the availability of GPS (Global Position Sys-
tem) equipped cameras and phones, as well as by
the widespread use of online social sites. The ma-
jority of these images are indexed with GPS coor-
dinates (latitude and longitude) only and/or have
minimal captions. This typically small amount of
textual information associated with the image is of
limited usefulness for image indexing, organiza-
tion and search. Therefore methods which could
automatically supplement the information avail-
able for image indexing and lead to improved im-
age retrieval would be extremely useful.
Following the general approach proposed by
Aker and Gaizauskas (2009), in this paper we
describe a method for automatic image caption-
ing or caption enhancement starting with only a
scene or subject type and a set of place names per-
taining to an image ? for example ?church, {St.
Paul?s,London}?. Scene type and place names can
be obtained automatically given GPS coordinates
and compass information using techniques such as
those described in Xin et al (2010) ? that task is
not the focus of this paper.
Our method applies only to images of static fea-
tures of the built or natural landscape, i.e. objects
with persistent geo-coordinates, such as buildings
and mountains, and not to images of objects which
move about in such landscapes, e.g. people, cars,
clouds, etc. However, our technique is suitable not
only for image captioning but in any application
context that requires summary descriptions of in-
stances of object classes, where the instance is to
be characterized in terms of the features typically
mentioned in describing members of the class.
Aker and Gaizauskas (2009) have argued that
humans appear to have a conceptual model of
what is salient regarding a certain object type (e.g.
church, bridge, etc.) and that this model informs
their choice of what to say when describing an in-
stance of this type. They also experimented with
representing such conceptual models using n-gram
language models derived from corpora consisting
of collections of descriptions of instances of spe-
cific object types (e.g. a corpus of descriptions of
churches, a corpus of bridge descriptions, and so
on) and reported results showing that incorporat-
ing such n-gram language models as a feature in a
feature-based extractive summarizer improves the
quality of automatically generated summaries.
The main weakness of n-gram language mod-
els is that they only capture very local information
about short term sequences and cannot model long
distance dependencies between terms. For exam-
ple one common and important feature of object
descriptions is the simple specification of the ob-
ject type, e.g. the information that the object Lon-
don Bridge is a bridge or that the Rhine is a river.
If this information is expressed as in the first line
of Table 1, n-gram language models are likely to
1250
Table 1: Example of sentences which express the type of an object.
London Bridge is a bridge...
The Rhine (German: Rhein; Dutch: Rijn; French: Rhin; Romansh: Rain;
Italian: Reno; Latin: Rhenus West Frisian Ryn) is one of the longest and
most important rivers in Europe...
reflect it, since one would expect the tri-gram is a
bridge to occur with high frequency in a corpus of
bridge descriptions. However, if the type predica-
tion occurs with less commonly seen local context,
as is the case for the object Rhine in the second
row of Table 1 ? most important rivers ? n-gram
language models may well be unable to identify it.
Intuitively, what is important in both these cases
is that there is a predication whose subject is the
object instance of interest and the head of whose
complement is the object type: London Bridge ...
is ... bridge and Rhine ... is ... river. Sentences
matching such patterns are likely to be important
ones to include in a summary. This intuition sug-
gests that rather than representing object type con-
ceptual models via corpus-derived language mod-
els as do Aker and Gaizauskas (2009), we do so in-
stead using corpus-derived dependency patterns.
We pursue this idea in this paper, our hy-
pothesis being that information that is important
for describing objects of a given type will fre-
quently be realized linguistically via expressions
with the same dependency structure. We explore
this hypothesis by developing a method for deriv-
ing common dependency patterns from object type
corpora (Section 2) and then incorporating these
patterns into an extractive summarization system
(Section 3). In Section 4 we evaluate the approach
both by scoring against model summaries and via
a readability assessment. Since our work aims to
extend the work of Aker and Gaizauskas (2009)
we reproduce their experiments with n-gram lan-
guage models in the current setting so as to permit
accurate comparison.
Multi-document summarizers face the problem
of avoiding redundancy: often, important infor-
mation which must be included in the summary
is repeated several times across the document set,
but must be included in the summary only once.
We can use the dependency pattern approach to
address this problem in a novel way. The com-
mon approach to avoiding redundancy is to use a
text similarity measure to block the addition of a
further sentence to the summary if it is too simi-
lar to one already included. Instead, since specific
dependency patterns express specific types of in-
Table 2: Object types and the number of articles in each object type cor-
pus. Object types which are bold are covered by the evaluation image set.
village 39970, school 15794, city 14233, organization 9393, university
7101, area 6934, district 6565, airport 6493, island 6400, railway station
5905, river 5851, company 5734, mountain 5290, park 3754, college 3749,
stadium 3665, lake 3649, road 3421, country 3186, church 3005, way
2508, museum 2320, railway 2093, house 2018, arena 1829, field 1731,
club 1708, shopping centre 1509, highway 1464, bridge 1383, street 1352,
theatre 1330, bank 1310, property 1261, hill 1072, castle 1022, forest 995,
court 949, hospital 937, peak 906, bay 899, skyscraper 843, valley 763, ho-
tel 741, garden 739, building 722, market 712, monument 679, port 651,
sea 645, temple 625, beach 614, square 605, store 547, campus 525, palace
516, tower 496, cemetery 457, volcano 426, cathedral 402, glacier 392,
residence 371, dam 363, waterfall 355, gallery 349, prison 348, cave 341,
canal 332, restaurant 329, path 312, observatory 303, zoo 302, coast 298,
statue 283, venue 269, parliament 258, shrine 256, desert 248, synagogue
236, bar 229, ski resort 227, arch 223, landscape 220, avenue 202, casino
179, farm 179, seaside 173, waterway 167, tunnel 167, ruin 166, chapel 165,
observation wheel 158, basilica 157, woodland 154, wetland 151, cinema
144, gate 142, aquarium 136, entrance 136, opera house 134, spa 125,
shop 124, abbey 108, boulevard 108, pub 92, bookstore 76, mosque 56
formation we can group the patterns into groups
expressing the same type of information and then,
during sentence selection, ensure that sentences
matching patterns from different groups are se-
lected in order to guarantee broad, non-redundant
coverage of information relevant for inclusion in
the summary. We report work experimenting with
this idea too.
2 Representing conceptual models
2.1 Object type corpora
We derive n-gram language and dependency pat-
tern models using object type corpora made avail-
able to us by Aker and Gaizauskas. Aker and
Gaizauskas (2009) define an object type corpus as
a collection of texts about a specific static object
type such as church, bridge, etc. Objects can be
named locations such as Eiffel Tower. To refer to
such names they use the term toponym. To build
such object type corpora the authors categorized
Wikipedia articles places by object type. The ob-
ject type of each article was identified automati-
cally by running Is-A patterns over the first five
sentences of the article. The authors report 91%
accuracy for their categorization process. The
most populated of the categories identified (in to-
tal 107 containing articles about places around the
world) are shown in Table 2.
2.2 N-gram language models
Aker and Gaizauskas (2009) experimented with
uni-gram and bi-gram language models to capture
the features commonly used when describing an
object type and used these to bias the sentence se-
lection of the summarizer towards the sentences
that contain these features. As in Song and Croft
(1999) they used their language models in a gener-
1251
ative way, i.e. they calculate the probability that a
sentence is generated based on a n-gram language
model. They showed that summarizer biased with
bi-gram language models produced better results
than those biased with uni-gram models. We repli-
cate the experiments of Aker and Gaizauskas and
generate a bi-gram language model for each object
type corpus. In later sections we use LM to refer
to these models.
2.3 Dependency patterns
We use the same object type corpora to derive
dependency patterns. Our patterns are derived
from dependency trees which are obtained using
the Stanford parser1. Each article in each ob-
ject type corpus was pre-processed by sentence
splitting and named entity tagging2. Then each
sentence was parsed by the Stanford dependency
parser to obtain relational patterns. As with the
chain model introduced by Sudo et al (2001) our
relational patterns are concentrated on the verbs
in the sentences and contain n+1 words (the verb
and n words in direct or indirect relation with the
verb). The number n is experimentally set to two
words.
For illustration consider the sentence shown in
Table 3 that is taken from an article in the bridge
corpus. The first two rows of the table show the
original sentence and its form after named entity
tagging. The next step in processing is to replace
any occurrence of a string denoting the object type
by the term ?OBJECTTYPE? as shown in the third
row of Table 3. The final two rows of the table
show the output of the Stanford dependency parser
and the relational patterns identified for this ex-
ample. To obtain the relational patterns from the
parser output we first identified the verbs in the
output. For each such verb we extracted two fur-
ther words being in direct or indirect relation to the
current verb. Two words are directly related if they
occur in the same relational term. The verb built-4,
for instance, is directly related to DATE-6 because
they both are in the same relational term prep-
in(built-4, DATE-6). Two words are indirectly re-
lated if they occur in two different terms but are
linked by a word that occurs in those two terms.
The verb was-3 is, for instance, indirectly related
to OBJECTTYPE-2 because they are both in dif-
ferent terms but linked with built-4 that occurs in
1http://nlp.stanford.edu/software/lex-parser.shtml
2For performing shallow text analysis the OpenNLP tools
(http://opennlp.sourceforge.net/) were used.
Table 3: Example sentence for dependency pattern.
Original sentence: The bridge was built in 1876 by W. W.
After NE tagging: The bridge was built in DATE by W. W.
Input to the parser: The OBJECTTYPE was built in DATE by W. W.
Output of the parser: det(OBJECTTYPE-2, The-1), nsubjpass(built-
4, OBJECTTYPE-2), auxpass(built-4, was-3), prep-in(built-4, DATE-6),
nn(W-10, W-8), agent(built-4, W-10)
Patterns: The OBJECTTYPE built, OBJECTTYPE was built, OBJECT-
TYPE built DATE, OBJECTTYPE built W, was built DATE, was built W
both terms. E.g. for the term nsubjpass(built-4,
OBJECTTYPE-2) we use the verb built and ex-
tract patterns based on this. OBJECTTYPE is in
direct relation to built and The is in indirect rela-
tion to built through OBJECTTYPE. So a pattern
from these relations is The OBJECTTYPE built.
The next pattern extracted from this term is OB-
JECTTYPE was built. This pattern is based on di-
rect relations. The verb built is in direct relation
to OBJECTTYPE and also to was. We continue
this until we cover all direct relations with built re-
sulting in two more patterns (OBJECTTYPE built
DATE and OBJECTTYPE built W). It should be
noted that we consider all direct and indirect rela-
tions while generating the patterns.
Following these steps we extracted relational
patterns for each object type corpus along with the
frequency of occurrence of the pattern in the en-
tire corpus. The frequency values are used by the
summarizer to score the sentences. In the follow-
ing sections we will use the term DpM to refer to
these dependency pattern models.
2.3.1 Pattern categorization
In addition to using dependency patterns as mod-
els for biasing sentence selection, we can also use
them to control the kind of information to be in-
cluded in the final summary (see Section 3.2). We
may want to ensure that the summary contains
a sentence describing the object type of the ob-
ject, its location and some background informa-
tion. For example, for the object Eiffel Tower we
aim to say that it is a tower, located in Paris, de-
signed by Gustave Eiffel, etc. To be able to do
so, we categorize dependency patterns according
to the type of information they express.
We manually analyzed human written descrip-
tions about instances of different object types and
recorded for each sentence in the descriptions the
kind of information it contained about the object.
We analyzed descriptions of 310 different objects
where each object had up to four different human
written descriptions (Section 4.1). We categorized
the information contained in the descriptions into
1252
the following categories:
? type: sentences containing the ?type? information of
the object such as XXX is a bridge
? year: sentences containing information about when the
object was built or in case of mountains, for instance,
when it was first climbed
? location: sentences containing information about
where the object is located
? background: sentences containing some specific in-
formation about the object
? surrounding: sentences containing information about
what other objects are close to the main object
? visiting: sentences containing information about e.g.
visiting times, etc.
We also manually assigned each dependency
pattern in each corpus-derived model to one of the
above categories, provided it occurred five or more
times in the object type corpora. The patterns ex-
tracted for our example sentence shown in Table 3,
for instance, are all categorized by year category
because all of them contain information about the
foundation date of an object.
3 Summarizer
We adopted the same overall approach to sum-
marization used by Aker and Gaizauskas (2009)
to generate the image descriptions. The summa-
rizer is an extractive, query-based multi-document
summarization system. It is given two inputs: a
toponym associated with an image and a set of
documents to be summarized which have been re-
trieved from the web using the toponym as a query.
The summarizer creates image descriptions in a
three step process. First, it applies shallow text
analysis, including sentence detection, tokeniza-
tion, lemmatization and POS-tagging to the given
input documents. Then it extracts features from
the document sentences. Finally, it combines the
features using a linear weighting scheme to com-
pute the final score for each sentence and to cre-
ate the final summary. We modified the approach
to feature extraction and the way the summarizer
acquires the weights for feature combination. The
following subsections describe how feature extrac-
tion/combination is done in more detail.
3.1 Feature Extraction
The original summarizer reported in Aker and
Gaizauskas (2009) uses the following features to
score the sentences:
? querySimilarity: Sentence similarity to the query (to-
ponym) (cosine similarity over the vector representa-
tion of the sentence and the query).
? centroidSimilarity: Sentence similarity to the centroid.
The centroid is composed of the 100 most frequently
occurring non stop words in the document collection
(cosine similarity over the vector representation of the
sentence and the centroid).
? sentencePosition: Position of the sentence within its
document. The first sentence in the document gets the
score 1 and the last one gets 1n where n is the number
of sentences in the document.
? starterSimilarity: A sentence gets a binary score if it
starts with the query term (e.g. Westminster Abbey, The
Westminster Abbey, The Westminster or The Abbey) or
with the object type, e.g. The church. We also allow
gaps (up to four words) between the and the query to
capture cases such as The most magnificent Abbey, etc.
? LMSim3: The similarity of a sentence S to an n-gram
language model LM (the probability that the sentence
S is generated by LM).
In our experiments we extend this feature set by
two dependency pattern related features: DpMSim
and DepCat.
DpMSim is computed in a similar fashion to
LMSim feature. We assign each sentence a depen-
dency similarity score. To compute this score, we
first parse the sentence on the fly with the Stan-
ford parser and obtain the dependency patterns for
the sentence. We then associate each dependency
pattern of the sentence with the occurrence fre-
quency of that pattern in the dependency pattern
model (DpM). DpMSim is then computed as given
in Equation 1. It is a sum of all occurrence fre-
quencies of the dependency patterns detected in a
sentence S that are also contained in the DpM.
DpMSim(S,DpM) =
?
p?S
fDpM (p) (1)
The second feature, DepCat, uses dependency
patterns to categorize the sentences rather than
ranking them. It can be used independently from
other features to categorize each sentence by one
of the categories described in Section 2.3.1. To do
this, we obtain the relational patterns for the cur-
rent sentence, check whether for each such pattern
whether it is included in the DpM, and, if so, we
add to the sentence the category the pattern was
manually associated with. It should be noted that
a sentence can have more than one category. This
can occur, for instance, if the sentence contains in-
formation about when something was built and at
the same time where it is located. It is also impor-
tant to mention that assigning sentences categories
does not change the order in the ranked list.
We use DepCat to generate an automated sum-
mary by first including sentences containing the
category ?type?, then ?year? and so on until the
3In Aker and Gaizauskas (2009) this feature is called mod-
elSimilarity.
1253
summary length is violated. The sentences are se-
lected according to the order in which they occur
in the ranked list. From each of the first three cat-
egories (?type?, ?year? and ?location?) we take a
single sentence to avoid redundancy. The same is
applied to the final two categories (?surrounding?
and ?visiting?). Then, if length limit is not vio-
lated, we fill the summary with sentences from the
?background? category until the word limit of 200
words is reached. Here the number of added sen-
tences is not limited. Finally, we order the sen-
tences by first adding the sentences from the first
three categories to the summary, then the ?back-
ground? related sentences and finally the last two
sentences from the ?surrounding? and ?visiting?
categories. However, in cases where we have not
reached the summary word limit because of un-
covered categories, i.e. there were not, for in-
stance, sentences about ?location?, we add to the
end of the summary the next top sentence from the
ranked list that was not taken.
3.2 Sentence Selection
To compute the final score for each sentence Aker
and Gaizauskas (2009) use a linear function with
weighted features:
Sscore = (
n?
i=1
featurei ? weighti) (2)
We use the same approach, but whereas the fea-
ture weights they use are experimentally set rather
than learned, we learn the weights using linear re-
gression instead. We used 23 of the 310 images
from our image set (see Section 4.1) to train the
weights. The image descriptions from this data set
are used as model summaries.
Our training data contains for each image a
set of image descriptions taken from the Virtual-
Tourist travel community web-site 4. From this
web-site we took all existing image descriptions
about a particular image or object. Note that some
of these descriptions about a particular object were
used to derive the model summaries for that ob-
ject (see Section 4.1). Assuming that model sum-
maries contain the most relevant sentences about
an object we perform ROUGE comparisons be-
tween the sentences in all the image descriptions
and the model summaries, i.e. we pair each sen-
tence from all image descriptions about a particu-
lar place with every sentence from all the model
4www.virtualtourist.com
summaries for that particular object. Sentences
which are exactly the same or have common parts
will score higher in ROUGE than sentences which
do not have anything in common. In this way, we
have for each sentence from all existing image de-
scriptions about an object a ROUGE score5 indi-
cating its relevance. We also ran the summarizer
for each of these sentences to compute the values
for the different features. This gives information
about each feature?s value for each sentence. Then
the ROUGE scores and feature score values for ev-
ery sentence were input to the linear regression al-
gorithm to train the weights.
Given the weights, Equation 2 is used to com-
pute the final score for each sentence. The final
sentence scores are used to sort the sentences in
the descending order. This sorted list is then used
by the summarizer to generate the final summary
as described in Aker and Gaizauskas (2009).
4 Evaluation
To evaluate our approach we used two different as-
sessment methods: ROUGE (Lin, 2004) and man-
ual readability. In the following we first describe
the data sets used in each of these evaluations, and
then we present the results of each assessment.
4.1 Data sets
For evaluation we use the image collection de-
scribed in Aker and Gaizauskas (2010). The image
collection contains 310 different images with man-
ually assigned toponyms. The images cover 60
of the 107 object types identified from Wikipedia
(see Table 2). For each image there are up to
four short descriptions or model summaries. The
model summaries were created manually based on
image descriptions taken from VirtualTourist and
contain a minimum of 190 and a maximum of 210
words. An example model summary about the Eif-
fel Tower is shown in Table 4. 23 of this image
collection was used to train the weights and the
remaining 13 (105 images) for evaluation.
To generate automatic captions for the im-
ages we automatically retrieved the top 30 related
web-documents for each image using the Yahoo!
search engine and the toponym associated with the
image as a query. The text from these documents
was extracted using an HTML parser and passed
to the summarizer. The set of documents we used
to generate our summaries excluded any Virtual-
Tourist related sites, as these were used to generate
5We used ROUGE 1.
1254
Table 4: Model, Wikipedia baseline and starterSimilarity+LMSim+DepCat summary for Eiffel Tower.
Model Summary Wikipedia baseline summary starterSimilarity+LMSim+DepCat summary
The Eiffel Tower is the most famous place in Paris. It
is made of 15,000 pieces fitted together by 2,500,000
rivets. It?s of 324 m (1070 ft) high structure and
weighs about 7,000 tones. This world famous land-
mark was built in 1889 and was named after its de-
signer, engineer Gustave Alexandre Eiffel. It is now
one of the world?s biggest tourist places which is vis-
ited by around 6,5 million people yearly. There are
three levels to visit: Stages 1 and 2 which can be
reached by either taking the steps (680 stairs) or the
lift, which also has a restaurant ?Altitude 95? and a
Souvenir shop on the first floor. The second floor also
has a restaurant ?Jules Verne?. Stage 3, which is at
the top of the tower can only be reached by using the
lift. But there were times in the history when Tour Eif-
fel was not at all popular, when the Parisians thought
it looked ugly and wanted to pull it down. The Eif-
fel Tower can be reached by using the Mtro through
Trocadro, Ecole Militaire, or Bir-Hakeim stops. The
address is: Champ de Mars-Tour Eiffel.
The Eiffel Tower (French: Tour Eiffel, [tur efel])
is a 19th century iron lattice tower located on the
Champ de Mars in Paris that has become both a
global icon of France and one of the most recog-
nizable structures in the world. The Eiffel Tower,
which is the tallest building in Paris, is the single
most visited paid monument in the world; millions
of people ascend it every year. Named after its de-
signer, engineer Gustave Eiffel, the tower was built
as the entrance arch for the 1889 World?s Fair. The
tower stands at 324 m (1,063 ft) tall, about the
same height as an 81-story building. It was the
tallest structure in the world from its completion
until 1930, when it was eclipsed by the Chrysler
Building in New York City. Not including broad-
cast antennas, it is the second-tallest structure in
France, behind the Millau Viaduct, completed in
2004. The tower has three levels for visitors. Tick-
ets can be purchased to ascend either on stairs or
lifts to the first and second levels.
The Eiffel Tower, which is the tallest building in
Paris, is the single most visited paid monument in the
world; millions of people ascend it every year. The
tower is located on the Left Bank of the Seine River,
at the northwestern extreme of the Parc du Champ
de Mars, a park in front of the Ecole Militaire that
used to be a military parade ground. The tower was
met with much criticism from the public when it was
built, with many calling it an eyesore. Counting from
the ground, there are 347 steps to the first level, 674
steps to the second level, and 1,710 steps to the small
platform on the top of the tower. Although it was
the world?s tallest structure when completed in 1889,
the Eiffel Tower has since lost its standing both as
the tallest lattice tower and as the tallest structure in
France. The tower has two restaurants: Altitude 95,
on the first floor 311ft (95m) above sea level; and
the Jules Verne, an expensive gastronomical restau-
rant on the second floor, with a private lift.
Table 5: ROUGE scores for each single feature and Wikipedia baseline.
Recall centroidSimilarity sentencePosition querySimilarity starterSimilarity LMSim DpMSim*** Wiki
R2 .0734 .066 .0774 .0869 .0895 .093 .097
RSU4 .12 .11 .12 .137 .142 .145 .14
the model summaries.
4.2 ROUGE assessment
In the first assessment we compared the automat-
ically generated summaries against model sum-
maries written by humans using ROUGE (Lin,
2004). Following the Document Understanding
Conference (DUC) evaluation standards we used
ROUGE 2 (R2) and ROUGE SU4 (RSU4) as eval-
uation metrics (Dang, 2006) . ROUGE 2 gives re-
call scores for bi-gram overlap between the auto-
matically generated summaries and the reference
ones. ROUGE SU4 allows bi-grams to be com-
posed of non-contiguous words, with a maximum
of four words between the bi-grams.
As baselines for evaluation we used two dif-
ferent summary types. Firstly, we generated
summaries for each image using the top-ranked
non Wikipedia document retrieved in the Yahoo!
search results for the given toponyms. From this
document we create a baseline summary by select-
ing sentences from the beginning until the sum-
mary reaches a length of 200 words. As a second
baseline we use the Wikipedia article for a given
toponym from which we again select sentences
from the beginning until the summary length limit
is reached.
First, we compared the baseline summaries
against the VirtualTourist model summaries. The
comparison shows that the Wikipedia baseline
ROUGE scores (R2 .097***, RSU4 .14***) are
significantly higher than the first document ones
(R2 0.042, RSU4 .079) 6. Thus, we will focus
on the Wikipedia baseline summaries to draw con-
clusions about our automatic summaries. Table 4
shows the Wikipedia baseline summary about the
Eiffel Tower.
Secondly, we separately ran the summarizer
over the top ten documents for each single feature
and compared the automated summaries against
the model ones. The results of this comparison
are shown in Table 5.
Table 5 shows that the dependency model fea-
ture (DpMSim) contributes most to the summary
quality according to the ROUGE metrics. It is also
significantly better than all other feature scores
except the LMSim feature. Compared to LMSim
ROUGE scores the DpMSim feature offers only a
moderate improvement. The same moderate im-
provement we can see between the DpMSim RSU4
and the Wiki RSU4. The lowest ROUGE scores
are obtained if only sentence position (sentecePo-
sition) is used.
To see how the ROUGE scores change when
features are combined with each other we per-
formed different combinations of the features,
ran the summarizer for each combination and
compared the automated summaries against the
model ones. In the different combinations we
6To assess the statistical significance of ROUGE score
differences between multiple summarization results we per-
formed a pairwise Wilcoxon signed-rank test. We use the
following conventions for indicating significance level in the
tables: *** = p < .0001, ** = p < .001, * = p < .05 and no
star indicates non-significance.
1255
Table 6: ROUGE scores of feature combinations which score moderately
or significantly higher than dependency pattern model (DpMSim) feature and
Wikipedia baseline.
Recall starterSimilarity
+ LMSim
starterSimilarity
+ LMSim + Dep-
Cat***
DpmSim Wiki
R2 .095 .102 .093 .097
RSU4 .145 .155 .145 .14
also included the dependency pattern categoriza-
tion (DepCat) feature explained in Section 3.1.
Table 6 shows the results of feature combinations
which score moderately or significantly higher
than the dependency pattern model (DpMSim) fea-
ture score shown in Table 5.
The results showed that combining DpMSim
with other features did not lead to higher ROUGE
scores than those produced by that feature alone.
The summaries categorized by dependency pat-
terns (starterSimilarity+LMSim+DepCat) achieve
significantly higher ROUGE scores than the
Wikipedia baseline. For both ROUGE R2 and
ROUGE SU4 the significance is at level p <
.0001. Table 4 shows a summary about the
Eiffel Tower obtained using this starterSimilar-
ity+LMSim+DepCat feature. Table 5 also shows
the ROUGE scores of the feature combination
starterSimilarity and LMSim used without the de-
pendency categorization (DepCat) feature. It can
be seen that this combination without the depen-
dency patterns lead to lower ROUGE scores in
ROUGE 2 and only moderate improvement in
ROUGE SU4 if compared with Wikipedia base-
line ROUGE scores.
4.3 Readability assessment
We also evaluated our summaries using a read-
ability assessment as in DUC and TAC. DUC and
TAC manually assess the quality of automatically
generated summaries by asking human subjects to
score each summary using five criteria ? gram-
maticality, redundancy, clarity, focus and structure
criteria. Each criterion is scored on a five point
scale with high scores indicating a better result
(Dang, 2005).
For this evaluation we used the same 105 im-
ages as in the ROUGE evaluation. As the ROUGE
evaluation showed that the dependency pattern
categorization (DepCat) renders the best results
when used in feature combination starterSimilar-
ity + LMSim + DepCat, we further investigated
the contribution of dependency pattern categoriza-
tion by performing a readability assessment on
summaries generated using this feature combina-
tion. For comparison we also evaluated sum-
maries which were not structured by dependency
patterns (starterSimilarity + LMSim) and also the
Wikipedia baseline summaries.
We asked four people to assess the summaries.
Each person was shown all 315 summaries (105
from each summary type) in a random way and
was asked to assess them according to the DUC
and TAC manual assessment scheme. The results
are shown in Table 7.
We see from Table 7 that using dependency pat-
terns to categorize the sentences and produce a
structured summary helps to obtain better readable
summaries. Looking at the 5 and 4 scores the ta-
ble shows that the dependency pattern categorized
summaries (SLMD) have better clarity (85% of the
summaries), are more coherent (74% of the sum-
maries), contain less redundant information (83%
of the summaries) and have better grammar (92%
of the summaries) than the ones without depen-
dency categorization (80%, 70%, 60%, 84%).
The scores of our automated summaries were
better than the Wikipedia baseline summaries in
the grammar feature. However, in other features
the Wikipedia baseline summaries obtained better
scores than our automated summaries. This com-
parison show that there is a gap to fill in order to
obtain better readable summaries.
5 Related Work
Our approach has an advantage over related work
in automatic image captioning in that it requires
only GPS information associated with the image in
order to generate captions. Other attempts towards
automatic generation of image captions generate
captions based on the immediate textual context of
the image with or without consideration of image
related features such as colour, shape or texture
(Deschacht and Moens, 2007; Mori et al, 2000;
Barnard and Forsyth, 2001; Duygulu et al, 2002;
Barnard et al, 2003; Pan et al, 2004; Feng and La-
pata, 2008; Satoh et al, 1999; Berg et al, 2005).
However, Marsch & White (2003) argue that the
content of an image and its immediate text have
little semantic agreement and this can, according
to Purves et al (2008), be misleading to image
retrieval. Furthermore, these approaches assume
that the image has been obtained from a document.
In cases where there is no document associated
with the image, which is the scenario we are prin-
cipally concerned with, these techniques are not
applicable.
1256
Table 7: Readability evaluation results: Each cell shows the percentage of summaries scoring the ranking score heading the column for each criterion in the
row as produced by the summary method indicated by the subcolumn heading ? Wikipedia baseline (W), starterSimilarity + LMSim (SLM) and starterSimilarity +
LMSim + DepCat (SLMD). The numbers indicate the percentage values averaged over the four people.
5 4 3 2 1
Criterion W SLM SLMD W SLM SLMD W SLM SLMD W SLM SLMD W SLM SLMD
clarity 72.6 50.5 53.6 21.7 30.0 31.4 1.2 6.7 5.7 4.0 10.2 6.0 0.5 2.6 3.3
focus 72.1 49.3 51.2 20.5 26.0 25.2 3.8 10.0 10.7 3.3 10.0 10.5 0.2 4.8 2.4
coherence 67.1 39.0 48.3 23.6 31.4 26.9 4.8 12.4 11.9 3.3 10.2 9.8 1.2 6.9 3.1
redundancy 69.8 42.9 55.0 21.7 17.4 28.8 2.4 4.5 4.3 5.0 27.1 8.8 1.2 8.1 3.1
grammar 48.6 55.7 62.9 32.9 29.0 30.0 5.0 3.1 1.9 11.7 12.1 5.2 1.9 0 0
Dependency patterns have been exploited in
various language processing applications. In in-
formation extraction, for instance, dependency
patterns have been used to extract relevant in-
formation from text resources (Yangarber et al,
2000; Sudo et al, 2001; Culotta and Sorensen,
2004; Stevenson and Greenwood, 2005; Bunescu
and Mooney, 2005; Stevenson and Greenwood,
2009). However, dependency patterns have not
been used extensively in summarization tasks. We
are aware only of the work described in Nobata et
al. (2002) who used dependency patterns in com-
bination with other features to generate extracts in
a single document summarization task. The au-
thors found that when learning weights in a simple
feature weigthing scheme, the weight assigned to
dependency patterns was lower than that assigned
to other features. The small contribution of the de-
pendency patterns may have been due to the small
number of documents they used to derive their
dependency patterns ? they gathered dependency
patterns from only ten domain specific documents
which are unlikely to be sufficient to capture re-
peated features in a domain.
6 Discussion and Conclusion
We have proposed a method by which dependency
patterns extracted from corpora of descriptions of
instances of particular object types can be used in a
multi-document summarizer to automatically gen-
erate image descriptions. Our evaluations show
that such an approach yields summaries which
score more highly than an approach which uses a
simpler representation of an object type model in
the form of a n-gram language model.
When used as the sole feature for sentence rank-
ing, dependency pattern models (DpMSim) pro-
duced summaries with higher ROUGE scores than
those obtained using the features reported in Aker
and Gaizauskas (2009). These dependency pat-
tern models also achieved a modest improvement
over Wikipedia baseline ROUGE SU4. Further-
more, we showed that using dependency patterns
in combination with features reported in Aker and
Gaizauskas to produce a structured summary led
to significantly better results than Wikipedia base-
line summaries as assessed by ROUGE. However,
human assessed readability showed that there is
still scope for improvement.
These results indicate that dependency patterns
are worth investigating for object focused auto-
mated summarization tasks. Such investigations
should in particular concentrate on how depen-
dency patterns can be used to structure informa-
tion within the summary, as our best results were
achieved when dependency patterns were used for
this purpose.
There are a number of avenues to pursue in fu-
ture work. One is to explore how dependency pat-
terns could be used to produce generative sum-
maries and/or perform sentence trimming. An-
other is to investigate how dependency patterns
might be automatically clustered into groups ex-
pressing similar or related facts, rather than rely-
ing on manual categorization of dependency pat-
terns into categories such as ?type?, ?year?, etc.
as was done here. Evaluation should be extended
to investigate the utility of the automatically gen-
erated image descriptions for image retrieval. Fi-
nally, we also plan to analyze automated ways for
learning information structures (e.g. what is the
flow of facts to describe a location) from existing
image descriptions to produce better summaries.
7 Acknowlegment
The research reported was funded by the TRIPOD
project supported by the European Commission
under the contract No. 045335. We would like
to thank Emina Kurtic, Mesude Bicak, Edina Kur-
tic and Olga Nesic for participating in our manual
evaluation. We also would like to thank Trevor
Cohn and Mark Hepple for discussions and com-
ments.
References
A. Aker and R. Gaizauskas. 2009. Summary Gener-
ation for Toponym-Referenced Images using Object
1257
Type Language Models. International Conference
on Recent Advances in Natural Language Process-
ing (RANLP),2009.
A. Aker and R. Gaizauskas. 2010. Model Summaries
for Location-related Images. In Proc. of the LREC-
2010 Conference.
K. Barnard and D. Forsyth. 2001. Learning the seman-
tics of words and pictures. In International Confer-
ence on Computer Vision, volume 2, pages 408?415.
Vancouver: IEEE.
K. Barnard, P. Duygulu, D. Forsyth, N. de Freitas,
D.M. Blei, and M.I. Jordan. 2003. Matching words
and pictures. The Journal of Machine Learning Re-
search, 3:1107?1135.
T.L. Berg, A.C. Berg, J. Edwards, and DA Forsyth.
2005. Whos in the Picture? In Advances in Neural
Information Processing Systems 17: Proc. Of The
2004 Conference. MIT Press.
R.C. Bunescu and R.J. Mooney. 2005. A shortest
path dependency kernel for relation extraction. In
Proceedings of the conference on Human Language
Technology and Empirical Methods in Natural Lan-
guage Processing, pages 724?731. Association for
Computational Linguistics Morristown, NJ, USA.
A. Culotta and J. Sorensen. 2004. Dependency Tree
Kernels for Relation Extraction. In Proceedings of
the 42nd Meeting of the Association for Compu-
tational Linguistics (ACL?04), Main Volume, pages
423?429, Barcelona, Spain, July.
H.T. Dang. 2005. Overview of DUC 2005. DUC 05
Workshop at HLT/EMNLP.
H.T. Dang. 2006. Overview of DUC 2006. National
Institute of Standards and Technology.
K. Deschacht and M.F. Moens. 2007. Text Analy-
sis for Automatic Image Annotation. Proc. of the
45th Annual Meeting of the Association for Compu-
tational Linguistics. East Stroudsburg: ACL.
P. Duygulu, K. Barnard, JFG de Freitas, and D.A.
Forsyth. 2002. Object Recognition as Machine
Translation: Learning a Lexicon for a Fixed Im-
age Vocabulary. In Seventh European Conference
on Computer Vision (ECCV), 4:97?112.
X. Fan, A. Aker, M. Tomko, P. Smart, M Sanderson,
and R. Gaizauskas. 2010. Automatic Image Cap-
tioning From the Web For GPS Photographs. In
Proc. of the 11th ACM SIGMM International Con-
ference on Multimedia Information Retrieval, Na-
tional Constitution Center, Philadelphia, Pennsylva-
nia.
Y. Feng and M. Lapata. 2008. Automatic Image An-
notation Using Auxiliary Text Information. Proc.
of Association for Computational Linguistics (ACL)
2008, Columbus, Ohio, USA.
C.Y. Lin. 2004. ROUGE: A Package for Automatic
Evaluation of Summaries. Proc. of the Workshop
on Text Summarization Branches Out (WAS 2004),
pages 25?26.
E.E. Marsh and M.D. White. 2003. A taxonomy of
relationships between images and text. Journal of
Documentation, 59:647?672.
Y. Mori, H. Takahashi, and R. Oka. 2000. Automatic
word assignment to images based on image division
and vector quantization. In Proc. of RIAO 2000:
Content-Based Multimedia Information Access.
C. Nobata, S. Sekine, H. Isahara, and R. Grishman.
2002. Summarization system integrated with named
entity tagging and ie pattern discovery. In Proc. of
the LREC-2002 Conference, pages 1742?1745.
J.Y. Pan, H.J. Yang, P. Duygulu, and C. Faloutsos.
2004. Automatic image captioning. In Multime-
dia and Expo, 2004. ICME?04. IEEE International
Conference on, volume 3.
RS Purves, A. Edwardes, and M. Sanderson. 2008.
Describing the where?improving image annotation
and search through geography. 1st Intl. Workshop
on Metadata Mining for Image Understanding, Fun-
chal, Madeira-Portugal.
S. Satoh, Y. Nakamura, and T. Kanade. 1999. Name-It:
naming and detecting faces in news videos. Multi-
media, IEEE, 6(1):22?35.
F. Song and W.B. Croft. 1999. A general language
model for information retrieval. In Proc. of the
eighth international conference on Information and
knowledge management, pages 316?321. ACM New
York, NY, USA.
M. Stevenson and M.A. Greenwood. 2005. A seman-
tic approach to IE pattern induction. In Proc. of the
43rd Annual Meeting on Association for Computa-
tional Linguistics, pages 379?386. Association for
Computational Linguistics Morristown, NJ, USA.
M. Stevenson and M. Greenwood. 2009. Depen-
dency Pattern Models for Information Extraction.
Research on Language and Computation, 7(1):13?
39.
K. Sudo, S. Sekine, and R. Grishman. 2001. Auto-
matic pattern acquisition for Japanese information
extraction. In Proc. of the first international con-
ference on Human language technology research,
page 7. Association for Computational Linguistics.
R. Yangarber, R. Grishman, P. Tapanainen, and S. Hut-
tunen. 2000. Automatic acquisition of domain
knowledge for information extraction. In Proc. of
the 18th International Conference on Computational
Linguistics (COLING 2000), pages 940?946. Saar-
briicken, Germany, August.
1258
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 402?411,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
Extracting bilingual terminologies from comparable corpora
Ahmet Aker, Monica Paramita, Robert Gaizauskas
University of Sheffield
ahmet.aker, m.paramita, r.gaizauskas@sheffield.ac.uk
Abstract
In this paper we present a method for extracting
bilingual terminologies from comparable corpora.
In our approach we treat bilingual term extrac-
tion as a classification problem. For classification
we use an SVM binary classifier and training data
taken from the EUROVOC thesaurus. We test our
approach on a held-out test set from EUROVOC
and perform precision, recall and f-measure eval-
uations for 20 European language pairs. The per-
formance of our classifier reaches the 100% pre-
cision level for many language pairs. We also
perform manual evaluation on bilingual terms ex-
tracted from English-German term-tagged compa-
rable corpora. The results of this manual evalu-
ation showed 60-83% of the term pairs generated
are exact translations and over 90% exact or partial
translations.
1 Introduction
Bilingual terminologies are important for various
applications of human language technologies, in-
cluding cross-language information search and re-
trieval, statistical machine translation (SMT) in
narrow domains and computer-aided assistance
to human translators. Automatic construction of
bilingual terminology mappings has been investi-
gated in many earlier studies and various methods
have been applied to this task. These methods may
be distinguished by whether they work on parallel
or comparable corpora, by whether they assume
monolingual term recognition in source and target
languages (what Moore (2003) calls symmetrical
approaches) or only in the source (asymmetric ap-
proaches), and by the extent to which they rely on
linguistic knowledge as opposed to simply statis-
tical techniques.
We focus on techniques for bilingual term ex-
traction from comparable corpora ? collections of
source-target language document pairs that are not
direct translations but are topically related. We
choose to focus on comparable corpora because
for many less widely spoken languages and for
technical domains where new terminology is con-
stantly being introduced, parallel corpora are sim-
ply not available. Techniques that can exploit such
corpora to deliver bilingual terminologies are of
significant practical interest in these cases.
The rest of the paper is structured as follows.
In Section 2 we outline our method. In Section
3 we review related work on bilingual term ex-
traction. Section 4 describes feature extraction for
term pair classification. In Section 5 we present
the data used in our evaluations and discuss our
results. Section 6 concludes the paper.
2 Method
The method we present below for bilingual term
extraction is a symmetric approach, i.e. it assumes
a method exists for monolingual term extraction in
both source and target languages. We do not pre-
scribe what a term must be. In particular we do not
place any particular syntactic restrictions on what
constitutes an allowable term, beyond the require-
ment that terms must be contiguous sequences of
words in both source and target languages.
Our method works by first pairing each term ex-
tracted from a source language document S with
each term extracted from a target language doc-
ument T aligned with S in the comparable cor-
pus. We then treat term alignment as a binary
classification task, i.e. we extract features for each
source-target language potential term pair and de-
cide whether to classify the pair as a term equiv-
alent or not. For classification purposes we use
an SVM binary classifier. The training data for
the classifier is derived from EUROVOC (Stein-
berger et al, 2002), a term thesaurus covering
the activities of the EU and the European Parlia-
ment. We have run our approach on the 21 official
EU languages covered by EUROVOC, construct-
ing 20 language pairs with English as the source
402
language. Considering all these languages allows
us to directly compare our method?s performance
on resource-rich (e.g. German, French, Spanish)
and under-resourced languages (e.g. Latvian, Bul-
garian, Estonian). We perform two different tests.
First, we evaluate the performance of the classifier
on a held-out term-pair list from EUROVOC us-
ing the standard measures of recall, precision and
F-measure. We run this evaluation on all 20 lan-
guage pairs. Secondly, we test the system?s per-
formance on obtaining bilingual terms from com-
parable corpora. This second test simulates the
situation of using the term alignment system in a
real world scenario. For this evaluation we col-
lected English-German comparable corpora from
Wikipedia, performed monolingual term tagging
and ran our tool over the term tagged corpora to
extract bilingual terms.
3 Related Work
Previous studies have investigated the extraction
of bilingual terms from parallel and comparable
corpora. For instance, Kupiec (1993) uses statisti-
cal techniques and extracts bilingual noun phrases
from parallel corpora tagged with terms. Daille
et al (1994), Fan et al (2009) and Okita et
al. (2010) also apply statistical methods to extract
terms/phrases from parallel corpora. In addition
to statistical methods Daille et al use word trans-
lation information between two words within the
extracted terms as a further indicator of the correct
alignment. More recently, Bouamor et al (2012)
use vector space models to align terms. The en-
tries in the vectors are co-occurrence statistics be-
tween the terms computed over the entire corpus.
Bilingual term alignment methods that work on
comparable corpora use essentially three sorts of
information: (1) cognate information, typically es-
timated using some sort of transliteration similar-
ity measure (2) context congruence, a measure of
the extent to which the words that the source term
co-occurs with have the same sort of distribution
and co-occur with words with the same sort dis-
tribution as do those words that co-occur with the
candidate term and (3) translation of component
words in the term and/or in context words, where
some limited dictionary exists. For example, in
Rapp (1995), Fung and McKeown (1997), Morin
et. al. (2007), Cao and Li (2002) and Ismail
and Manandhar (2010) the context of text units
is used to identify term mappings. Transliteration
and cognate-based information is exploited in Al-
Onaizan and Knight (2002), Knight and Graehl
(1998), Udupa et. al. (2008) and Aswani and
Gaizauskas (2010).
Very few approaches have treated term align-
ment as a classification problem suitable for ma-
chine learning (ML) techniques. So far as we
are aware, only Cao and Li (2002), who treat
only base noun phrase (NP) mapping, consider the
problem this way. However, it naturally lends it-
self to being viewed as a classification task, as-
suming a symmetric approach, since the differ-
ent information sources mentioned above can be
treated as features and each source-target language
potential term pairing can be treated as an in-
stance to be fed to a binary classifier which decides
whether to align them or not. Our work differs
from that of Cao and Li (2002) in several ways.
First they consider only terms consisting of noun-
noun pairs. Secondly for a given source language
term ?N1, N2?, target language candidate terms
are proposed by composing all translations (given
by a bilingual dictionary) ofN1 into the target lan-
guage with all translations ofN2. We remove both
these restrictions. By considering all terms pro-
posed by monolingual term extractors we consider
terms that are syntactically much richer than noun-
noun pairs. In addition, the term pairs we align are
not constrained by an assumption that their com-
ponent words must be translations of each other as
found in a particular dictionary resource.
4 Feature extraction
To align or map source and target terms we use an
SVM binary classifier (Joachims, 2002) with a lin-
ear kernel and the trade-off between training error
and margin parameter c = 10. Within the classi-
fier we use language dependent and independent
features described in the following sections.
4.1 Dictionary based features
The dictionary based features are language depen-
dent and are computed using bilingual dictionar-
ies which are created with GIZA++ (Och and Ney,
2000; Och and Ney, 2003). The DGT-TM par-
allel data (Steinberger et al, 2012) was input to
GIZA++ to obtain the dictionaries. Dictionary en-
tries have the form ?s, ti, pi?, where s is a source
word, ti is the i-th translation of s in the dictio-
nary and pi is the probability that s is translated
by ti, the pi?s summing to 1 for each s in the dic-
tionary. From the dictionaries we removed all en-
tries with pi < 0.05. In addition we also removed
403
every entry from the dictionary where the source
word was less than four characters and the target
word more than five characters in length and vice
versa. This step is performed to try to eliminate
translation pairs where a stop word is translated
into a non-stop word. After performing these fil-
tering steps we use the dictionaries to extract the
following language dependent features:
? isFirstWordTranslated is a binary feature in-
dicating whether the first word in the source
term is a translation of the first word in the
target term. To address the issue of com-
pounding, e.g. for languages like German
where what is a multi-word term in En-
glish may be expressed as a single com-
pound word, we check whether the com-
pound source term has an initial prefix that
matches the translation of the first target
word, provided that translation is at least 5
character in length.
? isLastWordTranslated is a binary feature in-
dicating whether the last word in the source
term is a translation of the last word in the
target term. As with the previous feature in
case of compound terms we check whether
the source term ends with the translation of
the target last word.
? percentageOfTranslatedWords returns the
percentage of words in the source term which
have their translations in the target term. To
address compound terms we check for each
source word translation whether it appears
anywhere within the target term.
? percentageOfNotTranslatedWords returns
the percentage of words of the source term
which have no translations in the target term.
? longestTranslatedUnitInPercentage returns
the ratio of the number of words within the
longest contiguous sequence of source words
which has a translation in the target term to
the length of the source term, expressed as a
percentage. For compound terms we proceed
as with percentageOfTranslatedWords.
? longestNotTranslatedUnitInPercentage re-
turns the percentage of the number of words
within the longest sequence of source words
which have no translations in the target term.
These six features are direction-dependent and
are computed in both directions, reversing which
language is taken as the source and which as
the target. We also compute another feature av-
eragePercentageOfTranslatedWords which builds
the average between the feature values of percent-
ageOfTranslatedWords from source to target and
target to source. Thus in total we have 13 dic-
tionary based features. Note for non-compound
terms if we compare two words for equality we do
not perform string match but rather use the Lev-
enshtein Distance (see Section 4.2) between the
two words and treat them as equal if the Leven-
shtein Distance returns >= 0.95. This is per-
formed to capture words with morphological dif-
ferences. We set 0.95 experimentally.
4.2 Cognate based features
Dictionaries mostly fail to return translation en-
tries for named entities (NEs) or specialized termi-
nology. Because of this we also use cognate based
methods to perform the mapping between source
and target words or vice versa. Aker et al (2012)
have applied (1) Longest Common Subsequence
Ratio, (2) Longest Common Substring Ratio, (3)
Dice Similarity, (4) Needleman-Wunsch Distance
and (5) Levenshtein Distance in order to extract
parallel phrases from comparable corpora. We
adopt these measures within our classifier. Each
of them returns a score between 0 and 1.
? Longest Common Subsequence Ratio
(LCSR): The longest common subsequence
(LCS) measure measures the longest com-
mon non-consecutive sequence of characters
between two strings. For instance, the words
?dollars? and ?dolari? share a sequence of
5 non-consecutive characters in the same
ordering. We make use of dynamic program-
ming (Cormen et al, 2001) to implement
LCS, so that its computation is efficient and
can be applied to a large number of possible
term pairs quickly. We normalize relative to
the length of the longest term:
LCSR(X,Y ) = len[LCS(X,Y )]max[len(X), len(Y )]
where LCS is the longest common subse-
quence between two strings and characters
in this subsequence need not be contiguous.
The shorthand len stands for length.
? Longest Common Substring Ratio (LC-
STR): The longest common substring
(LCST) measure is similar to the LCS
measure, but measures the longest common
404
consecutive string of characters that two
strings have in common. I.e. given two terms
we need to find the longest character n-gram
the terms share. The formula we use for the
LCSTR measure is a ratio as in the previous
measure:
LCSTR(X,Y ) = len[LCST (X,Y )]max[len(X), len(Y )]
? Dice Similarity:
dice = 2 ? LCSTlen(X) + len(Y )
? Needlemann Wunsch Distance (NWD):
NWD = LCSTmin[len(X) + len(Y )]
? Levenshtein Distance (LD): This method
computes the minimum number of operations
necessary to transform one string into an-
other. The allowable operations are insertion,
deletion, and substitution. Compared to the
previous methods, which all return scores be-
tween 0 and 1, this method returns a score s
that lies between 0 and n. The number n rep-
resents the maximum number of operations
to convert an arbitrarily dissimilar string to a
given string. To have a uniform score across
all cognate methods we normalize s so that
it lies between 0 and 1, subtracting from 1 to
convert it from a distance measure to a simi-
larty measure:
LDnormalized = 1?
LD
max[len(X), len(Y )]
4.3 Cognate based features with term
matching
The cognate methods assume that the source and
target language strings being compared are drawn
from the same character set and fail to capture
the corresponding terms if this is not the case.
For instance, the cognate methods are not directly
applicable to the English-Bulgarian and English-
Greek language pairs, as both the Bulgarian and
Greek alphabets, which are Cyrillic-based, differ
from the English Latin-based alphabet. However,
the use of distinct alphabets is not the only prob-
lem when comparing source and target terms. Al-
though most EU languages use the Latin alpha-
bet, the occurrence of special characters and di-
acritics, as well spelling and phonetic variations,
are further challenges which are faced by term or
entity mapping methods, especially in determin-
ing the variants of the same mention of the entity
(Snae, 2007; Karimi et al, 2011).1 We address this
problem by mapping a source term to the target
language writing system or vice versa. For map-
ping we use simple character mappings between
the writing systems, such as ? ? a, ? ? ph,
etc., from Greek to English. The rules allow one
character on the lefthand side (source language) to
map onto one or more characters on the righthand
side (target language). We created our rules man-
ually based on sound similarity between source
and target language characters. We created map-
ping rules for 20 EU language pairs using primar-
ily Wikipedia as a resource for describing phonetic
mappings to English.
After mapping a term from source to target lan-
guage we apply the cognate metrics described in
4.2 to the resulting mapped term and the original
term in the other language. Since we perform both
target to source and source to target mapping, the
number of cognate feature scores on the mapped
terms is 10 ? 5 due to source to target mapping
and 5 due to target to source mapping.
4.4 Combined features
We also combined dictionary and cognate based
features. The combined features are as follows:
? isFirstWordCovered is a binary feature indi-
cating whether the first word in the source
term has a translation (i.e. has a translation
entry in the dictionary regardless of the score)
or transliteration (i.e. if one of the cognate
metric scores is above 0.72) in the target term.
The threshold 0.7 for transliteration similar-
ity is set experimentally using the training
data. To do this we iteratively ran feature
extraction, trained the classifier and recorded
precision on the training data using a thresh-
old value chosen from the interval [0, 1] in
steps of 0.1. We selected as final threshold
value, the lowest value for which the preci-
sion score was the same as when the thresh-
old value was set to 1.
? isLastWordCovered is similar to the previ-
ous feature one but indicates whether the last
word in the source term has a translation or
1Assuming the terms are correctly spelled, otherwise the
misspelling is another problem.
2Note that we use the cognate scores obtained on the char-
acter mapped terms.
405
transliteration in the target term. If this is the
case, 1 is returned otherwise 0.
? percentageOfCoverage returns the percent-
age of source term words which have a trans-
lation or transliteration in the target term.
? percentageOfNonCoverage returns the per-
centage of source term words which have nei-
ther a translation nor transliteration in the tar-
get term.
? difBetweenCoverageAndNonCoverage
returns the difference between the last two
features.
Like the dictionary based features, these five
features are direction-dependent and are computed
in both directions ? source to target and target to
source, resulting in 10 combined features.
In total we have 38 features ? 13 features based
on dictionary translation as described in Section
4.1, 5 cognate related features as outlined in Sec-
tion 4.2, 10 cognate related features derived from
character mappings over terms as described in
Section 4.3 and 10 combined features.
5 Experiments
5.1 Data Sources
In our experiments we use two different data re-
sources: EUROVOC terms and comparable cor-
pora collected from Wikipedia.
5.1.1 EUROVOC terms
EUROVOC is a term thesaurus covering the ac-
tivities of the EU and the European Parliament in
particular. It contains 6797 term entries in 24 dif-
ferent languages including 22 EU languages and
Croatian and Serbian (Steinberger et al, 2002).
5.1.2 Comparable Corpora
We also built comparable corpora in the infor-
mation technology (IT) and automotive domains
by gathering documents from Wikipedia for the
English-German language pair. First, we man-
ually chose one seed document in English as a
starting point for crawling in each domain3. We
then identified all articles to which the seed doc-
ument is linked and added them to the crawling
queue. This process is performed recursively for
each document in the queue. Since our aim is to
build a comparable corpus, we only added English
3http://en.wikipedia.org/wiki/Information technology for
IT and http://en.wikipedia.org/wiki/Automotive industry for
automotive domain.
documents which have an inter-language link in
Wikipedia to a German document. We set a max-
imum depth of 3 in the recursion to limit size of
the crawling set, i.e. documents are crawled only
if they are within 3 clicks of the seed documents.
A score is then calculated to represent the impor-
tance of each document di in this domain:
scoredi =
n?
j=1
freqdij
depthdj
where n is the total number of documents in the
queue, freqdij is 1 if di is linked to dj , or 0 other-
wise, and depthdj is the number of clicks between
dj and the seed document. After all documents in
the queue were assigned a score, we gathered the
top 1000 documents and used inter-language link
information to extract the corresponding article in
the target language.
We pre-processed each Wikipedia article by
performing monolingual term tagging using
TWSC (Pinnis et al, 2012). TWSC is a term ex-
traction tool which identifies terms ranging from
one to four tokens in length. First, it POS-tags
each document. For German POS-tagging we
use TreeTagger (Schmid, 1995). Next, it uses
term grammar rules, in the form of sequences of
POS tags or non-stop words, to identify candidate
terms. Finally, it filters the candidate terms us-
ing various statistical measures, such as pointwise
mutual information and TF*IDF.
5.2 Performance test of the classifier
To test the classifier?s performance we evaluated it
against a list of positive and negative examples of
bilingual term pairs using the measures of preci-
sion, recall and F -measure. We used 21 EU offi-
cial languages, including English, and paired each
non-English language with English, leading to 20
language pairs.4 In the evaluation we used 600
positive term pairs taken randomly from the EU-
ROVOC term list. We also created around 1.3M
negative term pairs by pairing a source term with
200 randomly chosen distinct target terms. We
select such a large number to simulate the real
application scenario where the classifier will be
confronted with a huge number of negative cases
4Note that we do not use the Maltese-English language
pair, as for this pair we found that 5861 out of 6797 term
pairs were identical, i.e. the English and the Maltese terms
were the same. Excluding Maltese, the average number of
identical terms between a non-English language and English
in the EUROVOC data is 37.7 (out of a possible 6797).
406
Table 1: Wikipedia term pairs processed and judged as pos-
itive by the classifier.
Processed Positive
DE IT 11597K 3249
DE Automotive 12307K 1772
and a relatively small number of positive pairs.
The 600 positive examples contain 200 single term
pairs (i.e. single word on both sides), 200 term
pairs with a single word on only one side (either
source or target) and 200 term pairs with more
than one word on each side. For training we took
the remaining 6200 positive term pairs from EU-
ROVOC and constructed another 6200 term pairs
as negative examples, leading to total of 12400
term pairs. To construct the 6200 negative exam-
ples we used the 6200 terms on the source side
and paired each source term with an incorrect tar-
get term. Note that we ensure that in both train-
ing and testing the set of negative and positive
examples do not overlap. Furthermore, we per-
formed data selection for each language pair sep-
arately. This means that the same pairs found
in, e.g., English-German are not necessarily the
same as in English-Italian. The reason for this is
that the translation lengths, in number of words,
vary between language pairs. For instance adult
education is translated into Erwachsenenbildung
in German and contains just a single word (al-
though compound). The same term is translated
into istruzione degli adulti in Italian and contains
three words. For this reason we carry out the data
preparation process separately for each language
pair in order to obtain the three term pair sets con-
sisting of term pairs with only a single word on
each side, term pairs with a single word on just
one side and term pairs with multiple words on
both sides.
5.3 Manual evaluation
For this evaluation we used the Wikipedia com-
parable corpora collected for the English-German
(EN-DE) language pair. For each pair of
Wikipedia articles we used the terms tagged by
TWSC and aligned each source term with every
target term. This means if both source and target
articles contain 100 terms then this leads to 10K
term pairs. We extracted features for each pair
of terms and ran the classifier to decide whether
the pair is positive or negative. Table 1 shows the
number of term pairs processed and the count of
pairs classified as positive. Table 2 shows five
positive term pairs extracted from the English-
German comparable corpora for each of the IT and
automotive domains. We manually assessed a sub-
set of the positive examples. We asked human as-
sessors to categorize each term pair into one of the
following categories:
1. Equivalence: The terms are exact transla-
tions/transliterations of each other.
2. Inclusion: Not an exact transla-
tion/transliteration, but an exact transla-
tion/transliteration of one term is entirely
contained within the term in the other lan-
guage, e.g: ?F1 car racing? vs ?Autorennen
(car racing)?.
3. Overlap: Not category 1 or 2, but the terms
share at least one translated/transliterated
word, e.g: ?hybrid electric vehicles? vs ?hy-
bride bauteile (hybrid components)?.
4. Unrelated: No word in either term is a trans-
lation/transliteration of a word in the other.
In the evaluation we randomly selected 300
pairs for each domain and showed them to two
German native speakers who were fluent in En-
glish. We asked the assessors to place each of the
term pair into one of the categories 1 to 4.
5.4 Results and Discussion
5.4.1 Performance test of the classifier
The results of the classifier evaluation are shown
in Table 3. The results show that the overall per-
formance of the classifier is very good. In many
cases the precision scores reach 100%. The low-
est precision score is obtained for Lithuanian (LT)
with 67%. For this language we performed an er-
ror analysis. In total there are 221 negative ex-
amples classified as positive. All these terms are
multi-term, i.e. each term pair contains at least
two words on each side. For the majority of the
misclassified terms ? 209 in total ? 50% or more
of the words on one side are either translations or
cognates of words on the other side. Of these, 187
contained 50% or more translation due to cognate
words ? examples of such cases are capital in-
crease ? kapitalo eksportas or Arab organisation
? Arabu lyga with the cognates capital ? kapitalo
and Arab ? Arabu respectively. For the remain-
der, 50% or more of the words on one side are
dictionary translations of words on the other side.
In order to understand the reason why the classi-
fier treats such cases as positive we examined the
407
Table 2: Example positive pairs for English-German.
IT Automotive
chromatographic technique ? chromatographie methode distribution infrastructure ? versorgungsinfrastruktur
electrolytic capacitor ? elektrolytkondensatoren ambient temperature ? au?enlufttemperatur
natural user interfaces ? natu?rliche benutzerschnittstellen higher cetane number ? erho?hter cetanzahl
anode voltage ? anodenspannung fuel tank ? kraftstoffpumpe
digital subscriber loop ? digitaler teilnehmeranschluss hydrogen powered vehicle ? wasserstoff fahrzeug
Table 3: Classifier performance results on EUROVOC data (P stands for precision, R for recall and F for F -measure). Each
language is paired with English. The test set contains 600 positive and 1359400 negative examples.
ET HU NL DA SV DE LV FI PT SL FR IT LT SK CS RO PL ES EL BG
P 1 1 .98 1 1 .98 1 1 .7 1 1 1 .67 .81 1 1 1 1 1 1
R .67 .72 .82 .69 .81 .77 .78 .65 .82 .66 .66 .7 .77 .84 .72 .78 .69 .8 .78 .79
F .80 .83 .89 .81 .89 .86 .87 .78 .75 .79 .79 .82 .71 .91 .83 .87 .81 .88 .87 .88
training data and found 467 positive pairs which
had the same characteristics as the negative exam-
ples in the testing set classified. We removed these
467 entries from the training set and re-trained the
classifier. The results with the new classifier are
99% precision, 68% recall and 80% F score.
In addition to Lithuanian, two further lan-
guages, Portuguese (PT) and Slovak (SK), also
had substantially lower precision scores. For these
languages we also removed positive entries falling
into the same problem categories as the LT ones
and trained new classifiers with the filtered train-
ing data. The precision results increased substan-
tially for both PT and SK ? 95% precision, 76%
recall, 84% F score for PT and 94% precision,
72% recall, 81% F score for SK. The recall scores
are lower than the precision scores, ranging from
65% to 84%. We have investigated the recall prob-
lem for FI, which has the lowest recall score at
65%. We observed that all the missing term pairs
were not cognates. Thus, the only way these terms
could be recognized as positive is if they are found
in the GIZA++ dictionaries. However, due to data
sparsity in these dictionaries this did not happen in
these cases. For these term pairs either the source
or target terms were not found in the dictionar-
ies. For instance, for the term pair offshoring ?
uudelleensijoittautuminen the GIZA++ dictionary
contains the entry offshoring but according to the
dictionary it is not translated into uudelleensijoit-
tautuminen, which is the matching term in EU-
ROVOC.
5.4.2 Manual evaluation
The results of the manual evaluation are shown in
Table 4. From the results we can see that both as-
sessors judge above 80% of the IT domain terms
as category 1 ? the category containing equivalent
Table 4: Results of the EN-DE manual evaluation by two
annotators. Numbers reported per category are percentages.
Domain Ann. 1 2 3 4
IT P1 81 6 6 7
P2 83 7 7 3
Automotive P1 66 12 16 6
P2 60 15 16 9
term pairs. Only a small proportion of the term
pairs are judged as belonging to category 4 (3?7%)
? the category containing unrelated term pairs. For
the automotive domain the proportion of equiva-
lent term pairs varies between 60 and 66%. For
unrelated term pairs this is below 10% for both as-
sessors.
We investigated the inter-annotator agreement.
Across the four classes the percentage agreement
was 83% for the automotive domain term pairs and
86% for the IT domain term pairs. The kappa
statistic, ?, was .69 for the automotive domain
pairs and .52 for the IT domain. We also consid-
ered two class agreement where we treated term
pairs within categories 2 and 3 as belonging to
category 4 (i.e. as ?incorrect? translations). In
this case, for the automotive domain the percent-
age agreement was 90% and ? = 0.72 and for the
IT domain percentage agreement was 89% with
? = 0.55. The agreement in the automotive do-
main is higher than in the IT one although both
judges were computer scientists. We analyzed
the differences and found that they differ in cases
where the German and the English term are both in
English. One of the annotators treated such cases
as correct translation, whereas the other did not.
We also checked to ensure our technique was
not simply rediscovering our dictionaries. Since
the GIZA++ dictionaries contain only single
word?single word mappings, we examined the
408
newly aligned term pairs that consisted of one
word on both source and target sides. Taking both
the IT and automotive domains together, our al-
gorithm proposed 5021 term pairs of which 2751
(55%) were word-word term pairs. 462 of these
(i.e. 17% of the word-word term pairs or 9% of
the overall set of aligned term pairs) were already
in either the EN-DE or DE-EN GIZA++ dictionar-
ies. Thus, of our newly extracted term pairs a rela-
tively small proportion are rediscovered dictionary
entries. We also checked our evaluation data to see
what proportion of the assessed term pairs were
already to be found in the GIZA++ dictionaries.
A total of 600 term pairs were put in front of the
judges of which 198 (33%) were word-word term
pairs. Of these 15 (less than 8% of the word-word
pairs and less then 3% of the overall assessed set of
assessed term pairs) were word-word pairs already
in the dictionaries. We conclude that our evalua-
tion results are not unduly affected by assessing
term pairs which were given to the algorithm.
Error analysis For both domains we performed
an error analysis for the unrelated, i.e. category
4 term pairs. We found that in both domains the
main source of errors is due to terms with different
meanings but similar spellings such as the follow-
ing example (1).
(1) accelerator ? decelerator
For this example the cognate methods, e.g. the
Levenshtein similarity measure, returns a score of
0.81. This problem could be addressed in different
ways. First, it could be resolved by applying a very
high threshold for the cognate methods. Any cog-
nate score below that threshold could be regarded
as zero ? as we did for the combined features (cf.
Section 4.4). However, setting a similarity thresh-
old higher than 0.9 ? to filter out cases as in (1)
? will cause real cognates with greater variation
in the spellings to be missed. This will, in par-
ticular, affect languages with a lot of inflection,
such as Latvian. Another approach to address this
problem would be to take the contextual or dis-
tributional properties of the terms into considera-
tion. To achieve this, training data consisting of
term pairs along with contextual information is re-
quired. However, such training data does not cur-
rently exist (i.e. resources like EUROVOC do not
contain contextual information) and it would need
to be collected as a first step towards applying this
approach to the problem.
Partial Translation The assessors assigned 6 ?
7% of the term pairs in the IT domain and 12 ?
16% in the automotive domain to categories 2 and
3. In both categories the term pairs share transla-
tions or cognates.
Clearly, if humans such as professional transla-
tors are the end users of these terms, then it could
be helpful for them to find some translation units
within the terms. In category 2 this will be the en-
tire translation of one term in the other such as the
following examples.5
(2) visible graphical interface ? grafische be-
nutzerschnittstelle
(3) modern turbocharger systems ? moderne
turbolader
In example (3) the a translation of the German
term is to be found entirely within in the English
term but the English term has the additional word
visible, a translation of which is not found in the
German term. In example (4), again the transla-
tion of the German term is entirely found in the
English term, but as in the previous example, one
of the English words ? systems ? in this case, has
no match within the German term. In category 3
there are only single word translation overlaps be-
tween the terms as shown in the following exam-
ples.
(4) national standard language ?
niederla?ndischen standardsprache
(5) thermoplastic material ? thermoplastische
elastomere
In example (5) standard language is translated
to standardsprache and in example (6) thermo-
plastic to thermoplastische. The other words
within the terms are not translations of each other.
Another application of the extracted term pairs
is to use them to enhance existing parallel corpora
to train SMT systems. In this case, including the
partially correct terms may introduce noise. This
is especially the case for the terms within category
3. However, the usefulness of terms in both these
scenarios requires further investigation, which we
aim to do in future work.
5In our data it is always the case that the target term is
entirely translated within the English one and the other way
round.
409
6 Conclusion
In this paper we presented an approach to align
terms identified by a monolingual term extractor in
bilingual comparable corpora using a binary clas-
sifier. We trained the classifier using data from
the EUROVOC thesaurus. Each candidate term
pair was pre-processed to extract various features
which are cognate-based or dictionary-based. We
measured the performance of our classifier using
Information Retrieval (IR) metrics and a manual
evaluation. In the IR evaluation we tested the per-
formance of the classifier on a held out test set
taken from EUROVOC. We used 20 EU language
pairs with English being always the source lan-
guage. The performance of our classifier in this
evaluation reached the 100% precision level for
many language pairs. In the manual evaluation
we had our algorithm extract pairs of terms from
Wikipedia articles ? articles forming comparable
corpora in the IT and automotive domains ? and
asked native speakers to categorize a selection of
the term pairs into categories reflecting the level
of translation of the terms. In the manual evalu-
ation we used the English-German language pair
and showed that over 80% of the extracted term
pairs were exact translations in the IT domain and
over 60% in the automotive domain. For both do-
mains over 90% of the extracted term pairs were
either exact or partial translations.
We also performed an error analysis and high-
lighted problem cases, which we plan to address
in future work. Exploring ways to add contextual
or distributional features to our term representa-
tions is also an avenue for future work, though it
clearly significantly complicates the approach, one
of whose advantages is its simplicitiy. Further-
more, we aim to extend the existing dictionaries
and possibly our training data with terms extracted
from comparable corpora. Finally, we plan to in-
vestigate the usefulness of the terms in different
application scenarios, including computer assisted
translation and machine translation.
Acknowledgements
The research reported was funded by the TaaS
project, European Union Seventh Framework Pro-
gramme, grant agreement no. 296312. The au-
thors would like to thank the manual annotators
for their helpful contributions. We would also like
to thank partners at Tilde SIA and at the University
of Zagreb for supplying the TWSC term extraction
tool, developed within the EU funded project AC-
CURAT.
References
A. Aker, Y. Feng, and R. Gaizauskas. 2012. Auto-
matic bilingual phrase extraction from comparable
corpora. In 24th International Conference on Com-
putational Linguistics (COLING 2012), IIT Bom-
bay, Mumbai, India, 2012. Association for Compu-
tational Linguistics.
Y. Al-Onaizan and K. Knight. 2002. Machine translit-
eration of names in arabic text. In Proceedings of
the ACL-02 workshop on Computational approaches
to semitic languages, pages 1?13. Association for
Computational Linguistics.
N. Aswani and R. Gaizauskas. 2010. English-hindi
transliteration using multiple similarity metrics. In
Proceedings of the Seventh International Confer-
ence on Language Resources and Evaluation (LREC
2010), Valetta, Malta.
D. Bouamor, N. Semmar, and P. Zweigenbaum. 2012.
Identifying bilingual multi-word expressions for sta-
tistical machine translation. In LREC 2012, Eigth
International Conference on Language Resources
and Evaluation, pages 674-679, Istanbul, Turkey,
2012. ELRA.
Y. Cao and H. Li. 2002. Base noun phrase translation
using web data and the em algorithm. In Proceed-
ings of the 19th international conference on Com-
putational linguistics-Volume 1, pages 1?7. Associ-
ation for Computational Linguistics.
T. H. Cormen, C. E. Leiserson, R. L. Rivest, and
C. Stein. 2001. Introduction to Algorithms. The
MIT Press, 2nd revised edition, September.
B. Daille, E?. Gaussier, and J.M. Lange?. 1994. Towards
automatic extraction of monolingual and bilingual
terminology. In Proceedings of the 15th conference
on Computational linguistics-Volume 1, pages 515?
521. Association for Computational Linguistics.
X. Fan, N. Shimizu, and H. Nakagawa. 2009. Auto-
matic extraction of bilingual terms from a chinese-
japanese parallel corpus. In Proceedings of the
3rd International Universal Communication Sympo-
sium, pages 41?45. ACM.
P. Fung and K. McKeown. 1997. Finding terminol-
ogy translations from non-parallel corpora. In Pro-
ceedings of the 5th Annual Workshop on Very Large
Corpora, pages 192?202.
A. Ismail and S. Manandhar. 2010. Bilingual lexi-
con extraction from comparable corpora using in-
domain terms. In Proceedings of the 23rd Inter-
national Conference on Computational Linguistics:
Posters, pages 481?489. Association for Computa-
tional Linguistics.
410
T. Joachims. 2002. Learning to classify text using sup-
port vector machines: Methods, theory and algo-
rithms, volume 186. Kluwer Academic Publishers
Norwell, MA, USA:.
S. Karimi, F. Scholer, and A. Turpin. 2011. Ma-
chine transliteration survey. ACM Computing Sur-
veys (CSUR), 43(3):17.
K. Knight and J. Graehl. 1998. Machine translitera-
tion. Computational Linguistics, 24(4):599?612.
J. Kupiec. 1993. An algorithm for finding noun phrase
correspondences in bilingual corpora. In Proceed-
ings of the 31st annual meeting on Association for
Computational Linguistics, pages 17?22. Associa-
tion for Computational Linguistics.
R. Moore. 2003. Learning translations of named-
entity phrases from parallel corpora. In In Proceed-
ings of the tenth conference on European chapter
of the Association for Computational Linguistics-
Volume 1, pages 259266. Association for Compu-
tational Linguistics.
E. Morin, B. Daille, K. Takeuchi, and K. Kageura.
2007. Bilingual terminology mining - using brain,
not brawn comparable corpora. In Proceedings
of the 45th Annual Meeting of the Association of
Computational Linguistics, pages 664?671, Prague,
Czech Republic, June. Association for Computa-
tional Linguistics.
F. J. Och and H. Ney. 2000. A comparison of align-
ment models for statistical machine translation. In
Proceedings of the 18th conference on Computa-
tional linguistics, pages 1086?1090, Morristown,
NJ, USA. Association for Computational Linguis-
tics.
F. J. Och Och and H. Ney. 2003. A systematic compar-
ison of various statistical alignment models. Com-
putational Linguistics, 29(1):19?51.
T. Okita, A. Maldonado Guerra, Y. Graham, and
A. Way. 2010. Multi-word expression-sensitive
word alignment. Association for Computational
Linguistics.
Ma?rcis Pinnis, Nikola Ljubes?ic?, Dan S?tefa?nescu, In-
guna Skadin?a, Marko Tadic?, and Tatiana Gornostay.
2012. Term extraction, tagging, and mapping tools
for under-resourced languages. In Proc. of the 10th
Conference on Terminology and Knowledge Engi-
neering (TKE 2012), June, pages 20?21.
R. Rapp. 1995. Identifying word translations in non-
parallel texts. In Proceedings of the 33rd annual
meeting on Association for Computational Linguis-
tics, pages 320?322. Association for Computational
Linguistics.
Helmut Schmid. 1995. Treetagger? a lan-
guage independent part-of-speech tagger. Insti-
tut fu?r Maschinelle Sprachverarbeitung, Universita?t
Stuttgart, page 43.
C. Snae. 2007. A comparison and analysis of
name matching algorithms. International Journal
of Applied Science. Engineering and Technology,
4(1):252?257.
R. Steinberger, B. Pouliquen, and J. Hagman. 2002.
Cross-lingual document similarity calculation using
the multilingual thesaurus eurovoc. Computational
Linguistics and Intelligent Text Processing, pages
101?121.
R. Steinberger, A. Eisele, S. Klocek, S. Pilos, and
P. Schlter. 2012. Dgt-tm: A freely available trans-
lation memory in 22 languages. In Proceedings of
LREC, pages 454?459.
R. Udupa, K. Saravanan, A. Kumaran, and J. Jagarla-
mudi. 2008. Mining named entity transliteration
equivalents from comparable corpora. In Proceed-
ing of the 17th ACM conference on Information and
knowledge management, pages 1423?1424. ACM.
411
Coling 2008: Proceedings of the workshop on Multi-source Multilingual Information Extraction and Summarization, pages 41?48
Manchester, August 2008
Evaluating automatically generated user-focused multi-document
summaries for geo-referenced images
Ahmet Aker
Department of Computer Science
University of Sheffield
Sheffield, S1 4DP, UK
A.Aker@dcs.shef.ac.uk
Robert Gaizauskas
Department of Computer Science
University of Sheffield
Sheffield, S1 4DP, UK
R.Gaizauskas@dcs.shef.ac.uk
Abstract
This paper reports an initial study that aims
to assess the viability of a state-of-the-art
multi-document summarizer for automatic
captioning of geo-referenced images. The
automatic captioning procedure requires
summarizing multiple web documents that
contain information related to images? lo-
cation. We use SUMMA (Saggion and
Gaizauskas, 2005) to generate generic and
query-based multi-document summaries
and evaluate them using ROUGE evalua-
tion metrics (Lin, 2004) relative to human
generated summaries. Results show that,
even though query-based summaries per-
form better than generic ones, they are still
not selecting the information that human
participants do. In particular, the areas
of interest that human summaries display
(history, travel information, etc.) are not
contained in the query-based summaries.
For our future work in automatic image
captioning this result suggests that devel-
oping the query-based summarizer further
and biasing it to account for user-specific
requirements will prove worthwhile.
1 Introduction
Retrieving textual information related to a loca-
tion shown in an image has many potential appli-
cations. It could help users gain quick access to
the information they seek about a place of inter-
est just by taking its picture. Such textual informa-
tion could also, for instance, be used by a journalist
c
? 2008. Licensed under the Creative Commons
Attribution-Noncommercial-Share Alike 3.0 Unported li-
cense (http://creativecommons.org/licenses/by-nc-sa/3.0/).
Some rights reserved.
who is planning to write an article about a building,
or by a tourist who seeks further interesting places
to visit nearby. In this paper we aim to generate
such textual information automatically by utilizing
multi-document summarization techniques, where
documents to be summarized are web documents
that contain information related to the image con-
tent. We focus on geo-referenced images, i.e. im-
ages tagged with coordinates (latitude and longi-
tude) and compass information, that show things
with fixed locations (e.g. buildings, mountains,
etc.).
Attempts towards automatic generation of
image-related textual information or captions have
been previously reported. Deschacht and Moens
(2007) and Mori et al (2000) generate image
captions automatically by analyzing image-related
text from the immediate context of the image,
i.e. existing image captions, surrounding text in
HTML documents, text contained in the image,
etc. The authors identify named entities and other
noun phrases in the image-related text and assign
these to the image as captions. Other approaches
create image captions by taking into considera-
tion image features as well as image-related text
(Westerveld, 2000; Barnard et al, 2003; Pan et
al., 2004). These approaches can address all kinds
of images, but focus mostly on images of people.
They analyze only the immediate textual context of
the image on the web and are concerned with de-
scribing what is in the image only. Consequently,
background information about the objects in the
image is not provided. Our aim, however, is to
have captions that inform users? specific interests
about a location, which clearly includes more than
just image content description. Multi-document
summarization techniques offer the possibility to
include image-related information from multiple
41
documents, however, the challenge lies in being
able to summarize unrestricted web documents.
Various multi-document summarization tools
have been developed: SUMMA (Saggion and
Gaizauskas, 2005), MEAD (Radev et al, 2004),
CLASSY (Conroy et al, 2005), CATS (Farzin-
der et al, 2005) and the system of Boros et al
(2001), to name just a few. These systems generate
either generic or query-based summaries or both.
Generic summaries address a broad readership
whereas query-based summaries are preferred by
specific groups of people aiming for quick knowl-
edge gain about specific topics (Mani, 2001).
SUMMA and MEAD generate both generic and
query-based multi-document summaries. Boros
et al (2001) create only generic summaries,
while CLASSY and CATS create only query-based
summaries from multiple documents. The perfor-
mance of these tools has been reported for DUC
tasks
1
. As Sekine and Nobata (2003) note, al-
though DUC tasks provide a common evaluation
standard, they are restricted in topic and are some-
what idealized. For our purposes the summarizer
needs to create summaries from unrestricted web
input, for which there are no previous performance
reports.
For this reason we evaluate the performance of
both a generic and a query-based summarizer and
use SUMMA which provides both summarization
modes. We hypothesize that a query-based sum-
marizer will better address the problem of creating
summaries tailored to users? needs. This is because
the query itself may contain important hints as to
what the user is interested in. A generic summa-
rizer generates summaries based on the topics it
observes from the documents and cannot take user
specific input into consideration. Using SUMMA,
we generate both generic and query-based multi-
document summaries of image-related documents
obtained from the web. In an online data collection
procedure we presented a set of images with re-
lated web documents to human subjects and asked
them to select from these documents the infor-
mation that best describes the image. Based on
this user information we created model summaries
against which we evaluated the automatically gen-
erated ones.
Section 2 in this paper describes how image-
related documents were collected from the web.
In section 3 SUMMA is described in detail. In
1
http://www-nlpir.nist.gov/projects/duc/index.html
section 4 we explain how the human image de-
scriptions were collected. Section 5 discusses the
results, and section 6 concludes the paper and out-
lines directions for future work and improvements.
2 Web Document Collection
For web document collection we used geo-
referenced images of locations in London such as
Westminster Abbey, London Eye, etc. The images
were taken with a digital SLR camera with a Geo-
tagger plugged-in to its flash slot. The Geotagger
helped us to identify the location by means of co-
ordinates of the position where the photographer
stands, as well as the direction the camera is point-
ing (compass information). Based on the coordi-
nates and compass information for each image, we
carried out the following steps to collect related
documents from the web:
? identify a set of toponyms (terms that denote
locations or associate names with locations,
e.g. Westminster Abbey) that can be passed to
a search engine as query terms for document
search;
? use a search engine to retrieve HTML docu-
ments to be summarized;
? extract the pure text out of the HTML docu-
ments.
2.1 Toponym Collection
In order to create the web queries a set of to-
ponyms were collected semi-automatically. We
implemented an application (cf. Figure 1) that
suggests a list of toponyms close to the photogra-
pher?s location. The application uses Microsoft?s
MapPoint
2
service which allows users to query
location-related information. For example, a user
can query for tourist attractions (interesting build-
ings, museums, art galleries etc.) close to a loca-
tion that is identified by its address or its coordi-
nates.
Based on the coordinates (latitude and longi-
tude), important toponyms for a particular image
can be queried from the MapPoint database. In
order to facilitate this, MapPoint returns a met-
ric that measures the importance of each toponym.
A value close to zero means that the returned to-
ponym is closer to the specified coordinates than
a toponym with a higher value. For instance for
2
http://www.microsoft.com/mappoint/
42
Figure 1: Image Toponym Collector: Westminster
Abbey, Lat: 51.50024 Lon: -0.128138333: Direc-
tion: 137.1
the image of Westminster Abbey shown in the Im-
age box of Figure 1 the following toponyms are
collected:
Queen Elizabeth II Conf. Centre: 0.059
Parliament Square: 0.067
Westminster Abbey: 0.067
The photographer?s location is shown with a black
dot on the first map in the Maps box of Figure 1.
The application suggests the toponyms shown in
the Suggested Terms list.
Knowing the direction the photographer was
facing helps us to select the correct toponyms from
the list of suggested toponyms. The current Map-
Point implementation does not allow an arrow to
be drawn on the map which would be the best in-
dication of the direction the photographer is facing.
To overcome this problem we create a second map
(cf. Maps box of Figure 1) that shows another dot
moved 50 meters in the compass direction. By fol-
lowing the dot from the first map to the second map
we can determine the direction the photographer is
facing. When the direction is known, it is certain
that the image shows Westminster Abbey and not
the Queen Elizabeth II Conf. Centre or Parliament
Square. The Queen Elizabeth II Conf. Centre is
behind the photographer and Parliament Square is
on the left hand side.
Consequently in this example the toponym
Westminster Abbey is selected manually for the
web search. In order to avoid ambiguities, the
city name and the country name (also generated
by MapPoint) are added manually to the selected
toponyms. Hence, for Westminster Abbey, Lon-
don and United Kingdom are added to the toponym
list. Finally the terms in the toponym list are sim-
ply separated by a boolean AND operator to form
the web query. Then, the query is passed to the
search engine as described in the next section.
2.2 Document Query and Text Extraction
The web queries were passed to the Google Search
engine and the 20 best search results were re-
trieved, from which only 11 were taken for the
summarization process. We ensure that these 20
search results are healthy hyperlinks, i.e. that the
content of the hyperlink is accessible. In addition
to this, multiple hyperlinks belonging to the same
domain are ignored as it is assumed that the con-
tent obtained from the same domain would be sim-
ilar. Each remaining search result is crawled to ob-
tain its content.
The web-crawler downloads only the content of
the document residing under the hyperlink, which
was previously found as a search result, and does
not follow any other hyperlinks within the docu-
ment. The content obtained by the web-crawler
encapsulates an HTML structured document. We
further process this using an HTML parser
3
to se-
lect the pure text, i.e. text consisting of sentences.
The HTML parser removes advertisements,
menu items, tables, java scripts etc. from the
HTML documents and keeps sentences which con-
tain at least 4 words. This number was chosen after
several experiments. The resulting data is passed
on to the multi-document summarizer which is de-
scribed in the next section.
3 SUMMA
SUMMA
4
is a set of language and processing re-
sources to create and evaluate summarization sys-
tems (single document, multi-document, multi-
lingual). The components can be used within
GATE
5
to produce ready summarization applica-
tions. SUMMA has been used in this work to
create an extractive multi-document summarizer:
both generic and query-based.
In the case of generic summarization SUMMA
uses a single cluster approach to summarize n re-
lated documents which are given as input. Using
GATE, SUMMA first applies sentence detection
and sentence tokenisation to the given documents.
Then each sentence in the documents is repre-
sented as a vector in a vector space model (Salton,
1988), where each vector position contains a term
3
http://htmlparser.sourceforge.net/
4
http://www.dcs.shef.ac.uk/ saggion/summa/default.htm
5
http://gate.ac.uk
43
(word) and a value which is a product of the term
frequency in the document and the inverse docu-
ment frequency (IDF), a measurement of the term?s
distribution over the set of documents (Salton and
Buckley, 1988). Furthermore, SUMMA enhances
the sentence vector representation with further fea-
tures such as the sentence position in its document
and the sentence similarity to the lead-part in its
document. In addition to computing the vector rep-
resentation for all sentences in the document col-
lection the centroid of this sentence representation
is also computed.
In the sentence selection process, each sentence
in the collection is ranked individually, and the top
sentences are chosen to build up the final summary.
The ranking of a sentence depends on its distance
to the centroid, its absolute position in its docu-
ment and its similarity to the lead-part of its doc-
ument. For calculating vector similarities, the co-
sine similarity measure is used (Salton and Lesk,
1968).
In the case of the query-based approach,
SUMMA adds an additional feature to the sentence
vector representation as computed for generic
summarization. For each sentence, cosine simi-
larity to the given query is computed and added
to the sentence vector representation. Finally, the
sentences are scored by summing all features in the
vector space model according to the following for-
mula:
Sentence
score
=
n
?
i=1
feature
i
? weight
i
After the scoring process, SUMMA starts selecting
sentences for summary generation. In both generic
and query-based summarization, the summary is
constructed by first selecting the sentence that has
the highest score, followed by the next sentence
with the second highest score until the compres-
sion rate is reached. However, before a sentence
is selected a similarity metric for redundancy de-
tection is applied to each sentence which decides
whether a sentence is distinct enough from already
selected sentences to be included in the summary
or not. SUMMA uses the following formula to
compute the similarity between two sentences:
NGramSim(S
1
, S
2
, n) =
n
?
j=1
w
j
?
grams(S
1
, j)
?
grams(S
2
, j)
grams(S
1
, j)
?
grams(S
2
, j)
where n specifies maximum size of the n-grams to
be considered, grams(S
X
, j) is the set of j-grams in
sentence X and w
j
is the weight associated with
j-gram similarity. Two sentences are similar if
NGramSim(S
1
, S
2
, n) > ?. In this work n is set
to 4 and ? to 0.1. For j-gram similarity weights
w
1
= 0.1, w
2
= 0.2, w
3
= 0.3 and w
4
= 0.4 are
selected. These values are coded in SUMMA as
defaults.
Using SUMMA, generic and query-based sum-
maries are generated for the image-related docu-
ments obtained from the web. Each summary con-
tains a maximum of 200 words. The queries used
in the query-based mode are toponyms collected as
described in section 2.1.
4 Creating Model Summaries
For evaluating automatically generated summaries
as image captions, information that people asso-
ciate with images is collected. For this purpose, an
online data collection procedure was set up. Par-
ticipants were provided with a set of 24 images.
Each image had a detailed map showing the loca-
tion where it was taken, along with URLs to 11
related documents which were used for the auto-
mated summarization. Figure 2 shows an example
of an image and Table 2 contains the correspond-
ing related information.
Each participant was asked to familiarize him-
or herself with the location of the image by an-
alyzing the map and going through all 11 URLs.
Then each participant decided on up to 5 different
pieces of information he/she would like to know if
he/she sees the image or information about some-
thing he/she relates with the image. The informa-
tion we collected in this way is similar to ?infor-
mation nuggets? (Voorhees, 2003). Information
nuggets are facts which help us assess automatic
summaries by checking whether the summary con-
tains the fact or not. In addition to this, each par-
ticipant was asked to collect the information only
from the given documents, ignoring any other links
in these documents.
Eleven students participated in this survey, sim-
ulating the scenario in which tourists look for in-
formation about an image of a popular sight. The
number of images annotated by each participant is
shown in Table 1.
The participants selected the information from
original HTML documents on the web and not
from the documents which were preprocessed for
the multi-document summarization task. We found
44
Table 1: Number of images annotated by each particant
User1 User2 User3 User4 User5 User6 User7 User8 User9 User10 User11
24 7 24 24 18 24 8 4 16 12 24
Figure 2: Example image
Table 2: Information related to Figure 2
1. Westminster Abbey is the place of the coronation, mar-
riage and burial of British monarchs, except Edward V and
Edward VIII since 1066
2. the parish church of the Royal Family
3. the centrepiece to the City of Westminster
4. first church on the site is believed to have been con-
structed around the year 700
5. The history and the monuments, crypts and memorials
are not to be missed.
out that in some cases the participants selected in-
formation that did not occur in the preprocessed
documents. To ensure that the information selected
by the participants also occurs in the preprocessed
documents, we retained only the information se-
lected by the participants that could also be found
in these documents, i.e. that was available to the
summarizer. Out of 807 nuggets selected by partic-
ipants 21 (2.6%) were not found in the documents
available to the summarizer and were removed.
Furthermore, as the example above shows (cf.
Table 2), not all the items of information se-
lected by the participants were in form of full sen-
tences. They vary from phrases to whole sen-
tences. The participants were free to select any
text unit from the documents that they related to
the image content. However, SUMMA works
extractively and its summaries contain only sen-
tences selected from the given input documents.
The user selected information was normalized to
sentences in order to have comparable summaries
for evaluation. This was achieved by selecting
the sentence(s) from the documents in which the
participant-selected information was found and re-
placing the participant-selected phrases or clauses
with the full sentence(s). In this way model sum-
maries were obtained.
5 Results
The model summaries were compared against
24 summaries generated automatically using
SUMMA by calculating ROUGE-1 to ROUGE-
4, ROUGE-L and ROUGE-W-1.2 recall metrics
(Lin, 2004). For all these metrics ROUGE com-
pares each automatically generated summary s
pairwise to every model summary m
i
from the set
of M model summaries and takes the maximum
ROUGE
Score
value among all pairwise compar-
isons as the best ROUGE
Score
score:
ROUGE
Score
= argmax
i
ROUGE
Score
(m
i
, s)
ROUGE repeats this comparisonM times. In each
iteration it applies the Jackknife method and takes
one model summary from theM model summaries
away and compares the automatically generated
summary s against the M ? 1 model summaries.
In each iteration one best ROUGE
Score
is calcu-
lated. The final ROUGE
Score
is then the average
of all best scores calculated in M iterations.
In this way each generic and query-based sum-
mary was compared with the corresponding model
summaries. The results are given in the first two
columns of Table 3. We also collected the com-
mon information all participants selected for a par-
ticular image and compared this to the correspond-
ing query-based summary. The common informa-
tion is the intersection set of the sets of information
each of the participants selected for a particular im-
age. The results for this comparison are shown in
column QueryToCPOfModel of Table 3.
The model summaries were also compared
against each other in order to assess the agreement
between the participants. To achieve this, the im-
age information selected by each participant was
compared against the rest. The corresponding re-
sults are shown in column UserToUser of Table
4. We applied the same pairwise comparison we
used for our model summaries to the model sum-
maries of task 5 in DUC 2004 in order to mea-
45
Table 3: Comparison: Automatically generated summaries against model summaries. The column GenericToModel for
example shows ROUGE results for generic summaries relative to model summaries. CP stands for common part, i.e. common
information selected by all participants.
Recall GenericToModel QueryToModel QueryToCPOfModel QueryToModelInDUC
R-1 0.38293 0.39655 0.22084 0.3341
R-2 0.14760 0.17266 0.09894 0.0723
R-3 0.09286 0.11196 0.06222 0.0279
R-4 0.07450 0.09219 0.04971 0.0131
R-L 0.34437 0.35837 0.20913 0.3320
R-W-1.2 0.11821 0.12606 0.06350 0.1130
Table 4: Comparison: Model summaries against each other
Recall UserToUser UserToUserInDUC
R-1 0.42765 0.45407
R-2 0.30091 0.13820
R-3 0.26338 0.05870
R-4 0.24964 0.02950
R-L 0.40403 0.41594
R-W-1.2 0.15846 0.13973
sure the agreements between the participants on
this standard task. This gives us a benchmark rel-
ative to which we can assess how well users agree
on what information should be related to images.
The results for this comparison are shown in col-
umn UserToUserInDUC of Table 4.
All ROUGE metrics except R-1 and R-L in-
dicate higher agreement in human image-related
summaries than in DUC document summaries.
The ROUGE metrics most indicative of agreement
between human summaries are those that best cap-
ture words occurring in longer sequences of words
immediately following each other (R-2, R-3, R-4
and R-W). If long word sequences are identical
in two summaries it is more likely that they be-
long to the same sentence than if only single words
are common, as captured by R-1, or sequences of
words that do not immediately follow each other,
as captured by R-L. In R-L gaps in word sequences
are ignored so that for instance A B C D G and
A E B F C K D have the common sequence A B
C D according to R-L. R-W considers the gaps in
words sequences so that this sequence would not
be recognized as common. Therefore the agree-
ment on our image-related human summaries is
substantially higher than agreement on DUC doc-
ument human summaries.
The results in Table 3 support our hypothesis
that query-based summaries will perform better
than generic ones on image-related summaries. All
ROUGE results of the query-based summaries are
greater than the generic summary scores. This
reinforces our decision to focus on query-based
summaries in order to create image-related sum-
maries which also satisfy the users? needs. How-
ever, even though the query-based summaries are
more appropriate for our purposes, they are not
completely satisfactory. The query-based sum-
maries cover only 39% of the unigrams (ROUGE
1) in the model summaries and only 17% of the
bigrams (ROUGE 2), while the model summaries
have 42% agreement in unigrams and 30% agree-
ment in bigrams (cf. column UserToUser in Table
4). The agreement between the query-based and
model summaries gets lower for ROUGE-3 and
ROUGE-4 indicating that the query-based sum-
maries contain very little information in common
with the participants? results. This indication is
supported by the ROUGE-L (35%) and the low
ROUGE-W (12%) agreement which are substan-
tially lower compared to the UserToUser ROUGE-
L (40%) and ROUGE-W (15%) and the low
ROUGE scores in column QueryToCPOfModel.
For comparison with automated summaries in a
different domain, we include ROUGE scores of
query based SUMMA used in DUC 2004 (Sag-
gion and Gaizauskas, 2005) as shown in the last
column of Table 3. All scores are lower than our
QueryToModel results which might be due to low
agreement between human generated summaries
for the DUC task (cf. UserToUserInDUC column
in Table 4) or maybe because image captioning is
an easier task. The possibility that our summariza-
tion task is easier than DUC due to the summa-
rizer having fewer documents to summarize or due
to the documents being shorter than those in the
DUC task can be excluded. In the DUC task the
multi-document clusters contain 10 documents on
average while our summarizer works with 11 doc-
uments. The mean length in documents in DUC
46
Table 5: Query-based summary for Westminster Abbey and information selected by participants
Query-based summary Information selected by participants
The City of London has St Pauls, but Westminster Abbey
is the centrepiece to the City of Westminster. Westmin-
ster Abbey should be at the top of any London traveler?s
list. Westminster Abbey, however, lacks the clear lines of
a Rayonnant church,... I loved Westminster Abbey on my
trip to London. Westminster Abbey was rebuilt after
1245 by Henry III?s order, and in 1258 the remodeling
of the east end of St. Paul?s Cathedral began. He was in-
terred in Westminster Abbey. From 1674 to 1678 he tuned
the organ at Westminster Abbey and was employed there
in 1675-76 to copy organ parts of anthems. The architec-
tural carving found at Westminster Abbey (mainly of the
1250s) has much of the daintiness of contemporary French
work, although the drapery is still more like that of the early
Chartres or Wells sculpture than that of the Joseph Master.
Nevertheless, Westminster Abbey is something to see if you
have not seen it before. I happened upon the Westminster
Abbey on an outing to Parliament and Big Ben.
1.(3) Westminster Abbey is the place of the coronation,
marriage and burial of British monarchs, except Edward
V and Edward VIII since 1066. 2.(1) What is unknown,
however is just how old it is. The first church on the
site is believed to have been constructed around the year
700. 3.(2) Standing as it does between Westminster Abbey
and the Houses of Parliament, and commonly called ?the
parish church of the House of Commons?, St Margaret?s has
witnessed many important events in the life of this coun-
try. 4.(1) In addition, the Abbey is the parish church of
the Royal Family, when in residence at Buckingham Palace.
5.(1) The history and the monuments, crypts and memorials
are not to be missed. 6.(1) For almost one thousand years,
Westminister Abbey has been the setting for much of Lon-
don?s ceremonies such as Royal Weddings, Coronations,
and Funeral Services. 7.(1) It is also where many visitors
pay pilgrimage to The Tomb of the Unknown Soldier. 8.(1)
The City of London has St Pauls, but Westminster Abbey is
the centrepiece to the City of Westminster.
is 23 sentences while our documents have 44 sen-
tences on average.
Table 5 shows an example query-based sum-
mary for the image of Westminster Abbey and the
information participants selected for this particu-
lar image. Jointly the participants have selected 8
different pieces of information as indicated by the
bold numbers in the table. The numbers in paren-
theses show the number of times that a particular
information unit was selected. By comparing the
two sides it can be seen that the query-based sum-
mary does not cover most of the information from
the list with the exception of item 2. The item 2 is
semantically related to the sentence in bold on the
summary side as it addresses the year the abbey
was built, but the information contained in the two
descriptions is different.
Our results have confirmed our hypothesis that
query-based summaries will better address the aim
of this research, which is to get summaries tai-
lored to users? needs. A generic summary does not
take the user query into consideration and gener-
ates summaries based on the topics it observes. For
a set of documents containing mainly historical
and little location-related information, a generic
summary will probably contain a higher number
of history-related than location-related sentences.
This might satisfy a group of people seeking his-
torical information, however, it might not be inter-
esting for a group who want to look for location-
related information. Therefore using a query-
based multi-document summarizer is more appro-
priate for image-related summaries than a generic
one. However, the results of the query-based sum-
maries show that even so they only cover a small
part of the information the users select. One reason
for this is that the query-based summarizer takes
relevant sentences according to the query given to
it and does not take into more general consider-
ation the information likely to be relevant to the
user. However, we can assume that users will have
shared interests in some of the information they
would like to get about a particular type of object
in an image (e.g. a bridge, church etc.). This as-
sumption is supported by the high agreement be-
tween participants? performances in our online sur-
vey (cf. column UserToUser of Table 4).
Therefore, one way to improve the performance
of the query-based summarizer is to give the sum-
marizer the information that users typically asso-
ciate with a particular object type as input and bias
the multi-document summarizer towards this in-
formation. To do this we plan to build models of
user preferences for different object types from the
large number of existing image captions from web
resources, which we believe will improve the qual-
ity of automatically generated captions.
6 Conclusion
In this work we showed that query-based summa-
rizers perform slightly better than generic sum-
marizers on an image captioning task. However,
their output is not completely satisfactory when
compared to what human participants indicated as
important in our data collection study. Our fu-
ture work will concentrate on extending the query-
47
based summarizer to improve its performance in
generating captions that match user expectations
regarding specific image types. This will include
collecting a large number of existing captions from
web sources and applying machine learning tech-
niques for building models of the kinds of informa-
tion that people use for captioning. Further work
also needs to be carried out on improving the read-
ability of the extractive caption summaries.
7 Acknowledgement
This work is supported by the EU-funded TRIPOD
project
6
. We would like to thank Horacio Saggion
for his support with SUMMA. We are also grateful
to Emina Kurtic, Mark Sanderson, Mesude Bicak
and Dilan Paranavithana for comments on the pre-
vious versions of this paper.
References
Barnard, Kobus and Duygulu, Pinar and Forsyth, David
and de Freitas, Nando and Blei M, David and Jor-
dan I, Michael. 2003. Matching words and pic-
tures. The Journal of Machine Learning Research,
MIT Press Cambridge, MA, USA, 3: 1107?1135.
Boros, Endre and Kantor B, Paul and Neu j, David.
2001. A Clustering Based Approach to Creating
Multi-Document Summaries. Proc. of the 24th An-
nual International ACM SIGIR Conference on Re-
search and Development in Information Retrieval.
Conroy M, John and Schlesinger D, Judith and Stew-
art G, Jade 2005. CLASSY query-based multi-
document summarization. Proc. of the 2005 Doc-
ument Understanding Workshop, Boston.
Deschacht, Koen andMoens F, Marie. 2007. Text Anal-
ysis for Automatic Image Annotation. Proc. of the
45th Annual Meeting of the Association for Compu-
tational Linguistics, Prague.
Farzindar, Atefeh and Rozon, Frederik and Lapalme,
Guy. 2005. CATS a topic-oriented multi-
document summarization system at DUC 2005.
Proc. of the 2005 Document Understanding Work-
shop (DUC2005).
Lin, Chin-Yew 2004. ROUGE: A Package for Auto-
matic Evaluation of Summaries. Proc. of the Work-
shop on Text Summarization Branches Out (WAS
2004).
Mani, Inderjeet. 2001. Automatic Summarization.
John Benjamins Publishing Company.
Mori, Yasuhide and Takahashi, Hironobu and Oka,
Ryuichi. 2000. Automatic word assignment to im-
ages based on image division and vector quantiza-
tion. Proc. of RIAO 2000: Content-Based Multime-
dia Information Access.
6
http://tripod.shef.ac.uk/
Pan, Jia-Yu. and Yang, Hyung-Jeong and Duygulu,
Pinar and Faloutsos, Christos. 2004. Automatic
image captioning. Multimedia and Expo, 2004.
ICME?04. 2004 IEEE International Conference on.
Radev R, Dragomir. and Jing, Hongyan and Sty?s, Mal-
gorzata and Tam, Daniel. 2004. Centroid-based
summarization of multiple documents. Information
Processing and Management,40(6): 919?938.
Saggion, Horacio and Gaizauskas, Robert 2004. Multi-
document summarization by cluster/profile relevance
and redundancy removal. Document Understanding
Conference (DUC04).
Salton, Gerhard 1988. Automatic text process-
ing. Addison-Wesley Longman Publishing Co., Inc.
Boston, MA, USA.
Salton, Gerhard and Buckley, Chris 1988. Term-
weighting approaches in automatic text retrieval.
Pergamon Press, Inc. Tarrytown, NY, USA.
Salton, Gerhard and Lesk E., Michael 1968. Computer
Evaluation of Indexing and Text Processing. Journal
of the ACM,15(1):8?36.
Sekine, Satoshi and Nobata, Chikashi. 2003. A Sur-
vey for Multi-Document Summarization. Associa-
tion for Computational Linguistics Morristown, NJ,
USA, Proc. of the HLT-NAACL 03 on Text summa-
rization workshop-Volume 5.
Voorhees M, Ellen. 2003. Overview of the TREC 2003
Question Answering Track. Proc. of the Twelfth Text
REtrieval Conference (TREC 2003).
Westerveld, Thijs. 2000. Image retrieval: Content ver-
sus context. Content-Based Multimedia Information
Access, RIAO 2000 Conference.
48
Proceedings of the 4th International Workshop on Computational Terminology, pages 11?21,
Dublin, Ireland, August 23 2014.
Assigning Terms to Domains by Document Classification
Robert Gaizauskas, Emma Barker, Monica Lestari Paramita and Ahmet Aker
Department of Computer Science, University of Sheffield, United Kingdom
{r.gaizauskas,e.barker,m.paramita,ahmet.aker}@sheffield.ac.uk
Abstract
In this paper we investigate a number of questions relating to the identification of the domain
of a term by domain classification of the document in which the term occurs. We propose and
evaluate a straightforward method for domain classification of documents in 24 languages that
exploits a multilingual thesaurus and Wikipedia. We investigate and provide quantitative results
about the extent to which humans agree about the domain classification of documents and terms
also the extent to which terms are likely to ?inherit? the domain of their parent document.
1 Introduction
In an increasingly interconnected world, characterised by high international mobility and globalised trade
patterns, communication across languages is ever more important. The demand for translation services
has never been higher and there is constant pressure for technological solutions, e.g., in the form of ma-
chine translation (MT) and computer-assisted translation (CAT), to increase translation throughput and
lower costs. One requirement of these technologies is bilingual lexical resources, i.e. dictionaries, partic-
ularly in specialist subject areas or domains, such as biomedicine, information techology, or aerospace.
While in theory statistical MT approaches need only parallel corpora to train their translation models,
there is never enough parallel material in technical areas or for minority languages to support high qual-
ity technical translation, so specialist bilingual terminological resources are very important. Similarly,
human translators using CAT systems need support in the form of bilingual terminological resources in
specialist areas about which they may know very little.
The EU FP-7 TaaS project has created a cloud-based terminological service, which makes available
bilingual terminological resources for all EU languages. These resources include both existing termi-
nological resources and resources derived automatically from parallel and comparable corpora available
on the web. Additionally, the service?s user community is able manually to supplement or correct these
resources. Like many other terminology resources (e.g. IATE1, Eurotermbank2), terms in TaaS have do-
mains associated with them. This is done for a number of reasons: (1) Computational Feasiblity: While
in theory a translator faced with a translation task could provide the set of documents to be translated to
a system that dynamically assembled a bespoke terminological resource specific to this task, this is not
computationally feasible, at least not in a time-frame a user is likely to accept. Much more feasible is
to collect bilingual terminology off-line and store it within a term repository with an associated domain
or domains. Then, an on-line user, having identified the domain of the document(s) to be translated,
searches for terms within that domain or may have terms from the domain into which his documents are
automatically classified made available to him. (2) Sense Disambiguation: Term expressions, or their
translations, may have multiple senses, but these are likely to be in different domains. By restricting
the domain when looking up terms, sense confusions are less likely to occur. (3) User Preference: Our
This work is licensed under a Creative Commons Attribution 4.0 International Licence. Page numbers and proceedings footer are
added by the organisers. Licence details: http://creativecommons.org/licenses/by/4.0/
1http://iate.europa.eu
2http://www.eurotermbank.com
Terminology questions in texts authored by patients
Noemie Elhadad
Department of Biomedical Informatics
Columbia University, USA
noemie@dbmi.columbia.edu
This work is licenced under a Creative Commons Attribution 4.0 International License. Page numbers and proceedings footer
are added by the organizers. License details: http://creativecommons.org/licenses/by/4.0/
Assigning Terms to Domains by Document Classification
Robert Gaizauskas, Emma Barker, Monica Lestari Paramita and Ahmet Aker
Department of Computer Science, University of Sheffield, United Kingdom
{r.gaizauskas,e.barker,m.paramita,ahmet.aker}@sheffield.ac.uk
Abstract
In this paper we investigate a number of questions relating to the identification of the domain
of a term by domain classification of the document in which the term occurs. We propose and
evaluate a straightforward method for domain classification of documents in 24 languages that
exploits a multilingual thesaurus and Wikipedia. We investigate and provide quantitative results
about the extent to which humans agree about the domain classification of documents and terms
also the extent to which terms are likely to ?inherit? the domain of their parent document.
1 Introduction
In an increasingly interconnected world, characterised by high international mobility and globalised trade
patterns, communication across languages is ever more important. The demand for translation services
has never been higher and there is constant pressure for technological solutions, e.g., in the form of ma-
chine translation (MT) and computer-assisted translation (CAT), to increase translation throughput and
lower costs. One requirement of these technologies is bilingual lexical resources, i.e. dictionaries, partic-
ularly in specialist subject areas or domains, such as biomedicine, information techology, or aerospace.
While in theory statistical MT approaches need only parallel corpora to train their translation models,
there is never enough parallel material in technical areas or for minority languages to support high qual-
ity technical translation, so specialist bilingual terminological resources are very important. Similarly,
human translators using CAT systems need support in the form of bilingual terminological resources in
specialist areas about which they may know very little.
The EU FP-7 TaaS project has created a cloud-based terminological service, which makes available
bilingual terminological resources for all EU languages. These resources include both existing termi-
nological resources and resources derived automatically from parallel and comparable corpora available
on the web. Additionally, the service?s user community is able manually to supplement or correct these
resources. Like many other terminology resources (e.g. IATE1, Eurotermbank2), terms in TaaS have do-
mains associated with them. This is done for a number of reasons: (1) Computational Feasiblity: While
in theory a translator faced with a translation task could provide the set of documents to be translated to
a system that dynamically assembled a bespoke terminological resource specific to this task, this is not
computationally feasible, at least not in a time-frame a user is likely to accept. Much more feasible is
to collect bilingual terminology off-line and store it within a term repository with an associated domain
or domains. Then, an on-line user, having identified the domain of the document(s) to be translated,
searches for terms within that domain or may have terms from the domain into which his documents are
automatically classified made available to him. (2) Sense Disambiguation: Term expressions, or their
translations, may have multiple senses, but these are likely to be in different domains. By restricting
the domain when looking up terms, sense confusions are less likely to occur. (3) User Preference: Our
This work is licensed under a Creative Commons Attribution 4.0 International Licen . Page numbers and proceedings footer are
dde by the organisers. Licen d tails: http://creativecommons.org/licenses/ y/4.0/
1http://i te.europa.eu
2 www.eurotermbank.com
11
discussions with technical translators show they are used to and comfortable with the notion of domains
and prefer terminological resources structured by domain.
Assuming, therefore, that term resources are to be structured into domains, the question arises as to
how this is to be done automatically for automatically acquired terms. While the notion of domain is
inherent in most definitions of ?term?3, most term extraction systems identify terms using grammatical
patterns and/or statistical occurrence information applied to and gathered from corpora deemed to be
either in-domain or general/multi-domain. I.e. such tools do not have any inherent notion of domain, but
instead rely on the external provision of documents pre-selected by domain to determine the domain of
the extracted terms. But how valid is this procedure?
In this paper we explore several questions related to the assignment of terms to domains. These ques-
tions were addressed within the evaluation of that component of the TaaS platform which automatically
creates bilingual term resources (the Bilingual Term Extraction System, aka BiTES). Specifically:
1. How well can a simple vector space classifier built from a multilingual thesaurus automatically clas-
sify documents into domains prior to assigning these domains to the terms within the documents?
2. To what extent do humans agree about the assignment of terms to domains?
3. How accurate is the assumption that terms can be assigned to the domains of the documents in
which they are found?
The rest of the paper is structured as follows. Section 2 gives a brief overview of the BiTES system
as a whole and the domain classification component in somewhat more detail. In section 3 we describe
the evaluation of those parts of BiTES relevant to the questions above, detailing the evaluation tasks,
participants and data used and as well as the results of the evaluation. Section 4 provides analysis and
discussion of results. Section 5 discusses related work. We conclude in Section 6.
2 System Components
2.1 BiTES overview
The Bilingual Term Extraction System (BiTES) uses different workflows, each comprising a set of tools
run in sequence, to collect bilingual term pairs. Each new bilingual term pair found by BiTES is fed into
a database for later retrieval. The workflows consist of four different types of tools:
1. tools for collecting Web resources, such as parallel and comparable corpora from which the bilingual
terms are extracted;
2. tools for performing document classification into pre-defined categories or domains;
3. tools for extracting terms from or tagging terms in monolingual documents collected from the Web;
4. tools for bilingual alignment of tagged terms in parallel or comparable document pairs collected
from the Web.
Each workflow can be run in an offline and periodic manner and starts with document collection from the
Web followed by document classification. The output of the document classifier is passed to the mono-
lingual term extractor. Term-tagged document pairs are fed to the bilingual term alignment processor
to extract bilingual terms. The main goal of BiTES within the TaaS platform is to automatically col-
lect large numbers of bilingual term pairs off-line that are then stored in a database for later retrieval by
users. This database of automatically collected terms is consulted when other pre-existing, and presumed
higher quality, manually gathered terminological resources, such as, EuroTermBank or IATE, which are
also available in the TaaS platform, do not contain translations for terms the user seeks.
3For example Besse? et al. (1997) define term as ?a lexical unit consisting of one or more than one word which represents a
concept inside a domain?; ISO 1087-1:2000 defines term as ?verbal designation of a general concept in a specific subject field?.
12
In this section we detail only the domain classification component of BiTES as it is the component that
has the most direct implications for the research questions addressed in the paper and as the underlying
methods and performance of the other tools used in BiTES have been reported elsewhere (Aker et al.,
2012; Pinnis et al., 2012; Su and Babych, 2012; Skadin?a et al., 2012; Aker et al., 2013; Aker et al.,
2014b; Aker et al., 2014a).
2.2 Domain Classification
2.2.1 Domain classification scheme
Despite the existence of various domain classification schemes, the TaaS project has created its own do-
main classification for several reasons. First, the TaaS platform requires a suitable classification system
which is easy to use, yet provides broad coverage of the topics that are of greatest interest to users work-
ing in terminology management and machine translation. The project conducted a user study to identify
the set of required domains. Various classification systems were considered, including the Dewey Dec-
imal Classification (DDC) and Universal Decimal Classification (UDC). These schemes, however, are
too complicated to be used by terminologists (the latter uses 10 level-1 domains and more than 60,000
level-2 domains) yet still did not sufficiently cover relevant subject fields identified by our users, such
as IT, medicine and mechanical engineering. The Internal Classification for Standards (ICS) scheme
was considered next, as it covers technical subject fields, but it was lacking with respect to legal and
humanities domains. Intially, therefore, the TaaS project decided to adopt the domain structuring used
in the EuroVoc thesaurus, which includes a broad range of domains. However, with 21 level-1 domains
and 127 level-2 domains, it too is quite complex and focuses more on European Union domains than the
industry-related domains identified in our user study. Therefore, various modifications to the EuroVoc
domain scheme were performed to merge and delete various domains so as to increase the scheme?s
suitability for the project and also improve its practicality and ease of use. This resulted in what we here
refer to as the TaaS domain classification scheme, which contains 11 level-1 domains and 66 level-2 do-
mains4. A mapping from EuroVoc level-1 and -2 domains to TaaS level-1 and -2 domains was manually
established.
2.2.2 Document classifier
Many approaches to document classification have been proposed in the literature ? see Agarwal et al.
(2014) for a survey. Our domain classifier uses the well-explored vector space approach. For each
language, each domain is represented by one vector and each document to be classified by another vector.
The cosine similarity measure (Salton and Lesk, 1968) is calculated between the vector representation
of the input document and the vector representation of a domain and serves as a measure of the extent
to which the document belongs to that domain. The highest scoring domain may be chosen if hard
classification is required, or a vector of scores, one per domain, may be returned, if soft classification
is needed. The advantage of this approach in our setting is that we can exploit an existing multilingual,
domain-structured thesaurus to build our domain vector to deliver domain classifiers for 11 domains in
24 languages, without the need for collecting training data.
To create a vector representation for an input document, the document is first pre-processed and stop
words and punctuation are removed from it. The TaaS project covers 23 of the 24 official EU languages5
as well as Russian. For each of these languages we took the entire dump of Wikipedia and weighted each
word in the articles using tf ? idf (Manning et al., 2008). Any word whose idf is below a predefined
threshold is used as a stop word. Using this method we collected stop word lists for all 24 languages.
To identify punctuation we used simple rules covering the major punctuation symbols. After filtering
out stop words and punctuation, the remaining words in the input document are stemmed. We adopted
Lucene stemmers for all languages for which these resources are available in and implemented new
stemmers for Latvian, Lithuanian and Estonian. Finally, term frequency counts for the stems in the input
document are gathered, idf scores are taken from the Wikipedia dump and tf ? idf weights are computed
and stored to create the vector representation of the input document.
4A full specification of the scheme is available at: https://demo.taas-project.eu/domains.
5The omitted language is Irish, for which insufficient data was available for training our tools.
13
To create domain vectors we did the following: (1) For each domain and language, we manually
downloaded the relevant EuroVoc term file from the EuroVoc website6. (2) We used the EuroVoc-to-
TaaS mapping described in Section 2.2.1 above to map all terms belonging to a specific EuroVoc domain
(level-1 or -2) to the corresponding TaaS domain (level-1 or -2). (3) For each TaaS domain (in each
language) we built a domain-specific vector from the set of newly derived TaaS terms in the domain.
Since our vector elements correspond to single words, we convert any multi-word term in the domain
into multiple single word representations. To do this we process each multi-word by splitting it on
whitespace, removing any words that are stop words and finally stemming the remaining words. For any
single word terms we simply take their stems. Finally, all the word stems so derived are stored in a vector.
We use simple term frequency, measured across the bag of stemmed words derived from all terms in the
domain, as a weight for each stem. In the experiment below we report results only for classification into
the 11 level-1 TaaS domains ? see Table1.
Level-1 Domain Level-2 Domain
Agriculture and foodstuff Agriculture, forestry, fisheries, foodstuff, beverages and tobacco, and food technol-
ogy.
Arts Plastic arts, music, literature, and dance.
Economics Business administration, national economics, finance and accounting, trade, mar-
keting and public relations, and insurance.
Energy Energy policy, coal and mining, oil and gas, nuclear energy, and wind, water and
solar energy.
Environment Climate, and environmental protection.
Industries and technology Information and communication technology, chemical industry, iron, steel and other
metal industries, mechanical engineering, electronics and electrical engineering,
building and public works, wood industry, leather and textile industries, transporta-
tion and aeronautics, and tourism.
Law Civil law, criminal law, commercial law, public law, and international law and hu-
man rights.
Medicine and pharmacy Anatomy, ophthalmology, dentistry, otolaryngology, paediatrics, surgery, alterna-
tive treatment methods, gynaecology, veterinary medicine, pharmacy, cosmetic, and
medical engineering.
Natural sciences Astronomy, biology, chemistry, geology, geography, mathematics and physics.
Politics and administration Administration, politics, international relations and defence, and European Union.
Social sciences Education, history, communication and media, social affairs, culture and religion,
linguistics, and sports.
Table 1: TaaS Domains
3 Evaluation
To evaluate the BiTES system we devised a set of four human assessment tasks focussed on different
aspects of the system. These tasks were designed to assess the domain classifier, the extent to which
terms found in a document judged to be in a given domain were in the domain of their document, the
accuracy of the boundaries of extracted terms in context and the accuracy of system proposed bilingual
term alignments. In this paper we focus on the first two of these tasks only. As noted above the TaaS
project addressed 24 languages in total. Evaluation of all these languages and language pairs was clearly
impossible. We chose to focus on six languages ? English (EN), German (DE), Spanish (ES), Czech
(CS), Lithuanian (LT) and Latvian (LV) ? and five language pairs EN-DE, EN-ES, EN-CS, EN-LT and
EN-LV. This gave us exemplars from the Germanic, Romance, Slavic and Baltic language groups.
3.1 Human assessment tasks
3.1.1 Domain classification assessment
In the domain classification assessment task we present participants with a document and the TaaS set of
domain classes (see Table 1), and ask them to select the TaaS level-1 domain that in their judgement best
represents the document. We provide a brief set of guidelines to help them carry out this task.
6http://eurovoc.europa.eu
14
We encourage participants to select a primary domain wherever possible ? i.e. a single domain that
best represents the document. But we allow them to select multiple domains from the list provided, if they
believe the text spans more than one domain and they are unable to decide upon a primary domain. If they
do opt to select multiple domains we ask them to keep the number of selected domains to a minimum.
For example, the Wikipedia article entitled ?Hydraulic Fracturing? 7 discusses a wide range of topics,
including the process of hydraulic fracturing and its impacts in the geological, environmental, economic
and political spheres. For this document, which we use in our guidelines for the task, we recommend
assessors choose ?Energy? as a primary domain and possibly also ?Industries and Technology?, since
these two domains best represent the overall document content, which is chiefly concerned with what is
described as a ?mechanical? process in the ?industrial sector of mining?, the products being natural gas
and oil. But we would limit our selection to these two.
The aim is for participants to select domains from the list we provide. However, in the event that they
are unable to do so, we provide an option ?none of the above?, which they may select and then provide
a domain of their own. In the guidelines we ask them to spend some time reviewing potential domain
candidates, and combinations of candidates, before opting to provide an as yet unspecified domain.
I.e. they should only select the option ?none of the above? if they have genuinely exhausted all the
possibilities using one or more domains from our list.
3.1.2 Term in domain assessment
Figure 1: Judging a Term Candidate in a Domain
This is the first of two tasks assessing the (monolingual) extraction of terms. It assesses whether an
automatically extracted term candidate is a term in a proposed, automatically determined, domain. As-
suming the candidate is a term, a subsequent task assesses whether the boundaries of the term candidate,
when taken in their original document context, are correct.
In this task (see Figure 1) we present assessors with a term candidate and a domain and then ask them
to judge if the candidate is a term in the given domain or if it is a term in a different domain. If they judge
the term to be in a different domain we ask them to specify the alternate domain(s). In this question the
candidate and the domain category are assessed together but we do not provide any specific context, such
as the source sentence or source document. As with the previous task we provide a brief set of guidelines
to help assessors carry out the task.
We ask assessors to base their judgement on the entire candidate string. If the string contains a term
but also contains, additional words that are not part of the term then they should answer ?no?. For
7Aka ?fracking?, see http://en.wikipedia.org/wiki/Hydraulic_fracturing
15
example, consider the candidate ?excessive fuel emissions? and the domain ?Industries and Technology?.
Although most people would agree that ?fuel emissions? is a term, Q1.1 and Q1.2 should be answered
?no? in this case since the candidate also contains noise, i.e. the word ?excessive?. Superfluous articles,
determiners and other closed class words are also considered ?noise? in this context.
We encourage assessors to search the Internet, as translators and terminologists might do, to help
determine whether the entire candidate is indeed a term in the given domain. Web searches can provide
examples of real world uses of a candidate in different domains. We also allow assessors to consult
existing terminological or dictionary resources, online or otherwise, during the evaluation task. However,
participants are encouraged not to assume that such resources are complete or entirely correct and advised
that such resources be used with some consideration and caution.
Finally, if assessors have answered ?yes? to one of Q1.1 or Q1.2, they will also be asked to indicate
the utility of the term candidate in Q1.3, however this aspect of the assessment is not of interest here and
will not be discussed further.
3.2 Participants
We recruited experienced translators to participate in the evaluation tasks. For English and for each
language pair, three assessors carried out each of the evaluation tasks. In total our study involved 17
assessors ? one assessor took part in DE only, EN-DE and EN only tasks. All assessors had an excellent
background in translation in a wide variety of domains, with an average of 8.5 years translation experi-
ence in the relevant language pairs. All assessors who evaluated the English, Lithuanian and Latvian data
were native speakers. For each of the remaining languages (Czech, German and Spanish), 2 were native
speakers whilst 1 was a fluent speaker with over 54 years, 15 years and 12 years experience (respectively)
in using these languages as a second language.
3.3 Data
3.3.1 Domain classification
For the domain classification task, we selected a set of documents to be evaluated using the following
approach. First, we gathered all articles from the August 2013 Wikipedia dump in each of the assesment
languages and extracted the main text paragraphs, i.e. tables, images, infoboxes and URLs were filtered
out. The number of articles ranged from 50,000 (for Latvian) to 4,000,000 (for English). We then ran
our domain classifier over each document in this dataset and assigned to each document the top domain
proposed by the classifier, i.e. the domain with the highest score according to our vector space approach
(Section 2.2.2). During processing we filtered out documents whose top domain scores were below
a previously set minimum threshold and those whose document length was below a minimum length.
Finally, for each domain D, we sorted the documents classified into D based on their scores, divided this
sequence into 10 equal-size bins and selected one document from each bin. Since we were classifying
documents into one of the 11 level-1 TaaS domains, this resulted in 110 documents for each language8.
3.3.2 Term extraction
For the term in domain assessment task, we narrowed the task to focus on two domains only ? ?Industries
and Technology? and ?Politics and Administration? ? since we could not hope to assess sufficient terms
in all domains in all languages. We extracted terms from all documents contained in the top bin of the
domain classifier, i.e. the 10% of documents in the domain with the highest similarity score to the domain
vector, using TWSC as the term extractor tool (Pinnis et al., 2012). Next, we selected 200 terms from
both domains, choosing terms of different word lengths: 50 of length 1, 70 of length 2, 50 of length 3
and 30 of length 4. This distribution was chosen in order to approximate roughly the distribution of term
lengths one might expect in the data9. This process was repeated for each of our six languages.
8The Latvian set contains a slightly smaller set (i.e. 106 documents) due to a fewer number of documents found in one of
the domains (i.e. 6 documents in the ?Energy? domains).
9This distribution was chosen after analysing term lengths in the EuroVoc thesaurus and in the term extractor results, which
indicated that terms length 2 are the most common, followed by terms length 1 and 3, and terms length 4 are found to be the
least common. We boosted slightly the numbers of length 4 terms in our test to try to eliminate very small number effects.
16
3.4 Results
3.4.1 Domain classification assessment
A total of 656 documents (in 6 languages) were assessed and on average 1.2 domains were selected for
each document. Regarding human-human agreement, at least 2 assessors fully agreed on their domain
selections (including cases where more than one domain was selected) on 78% of the cases. When
considering cases where at least 2 assessors agreed on at least one domain, agreement increases to 98%.
Regarding human-system agreement, since 3 assessors participated in each assessment, we produced
two types of human judgments: majority (i.e. any domains selected by at least two assessors) and union
(i.e. any domains selected by at least one assessor). We computed the agreements between the classifier
and both the majority and the union human judgments. Results averaged over all domains and languages
show the system?s proposed top domain agreed with the majority human judgment in 45% of cases and
with the union of human judgments in 58% of cases. Broken down by language, agreement with the
majority judgment ranged from a low of 35% (EN) to a high of over 53% (DE) while agreement with the
union of judgments ranged from a low of 48% (EN) to a high of over 64% (CS). By domain, agreement
with majority judgment ranged from just over 12% (Agriculture and foodstuff) to 88% (Medicine and
pharmacy) while agreement with the union of judgments ranged from 23% (Agriculture and foodstuff)
to over 91% (Social sciences).
Recall (Section 3.3.1) that our test data includes documents from different similarity score bins. This
enables us to analyse the agreement between the assessors and the classifier in more detail. In general
we see a monotonically increasing agreement with both the majority judgement and union of judgments
as we move from the lowest to highest scoring bin. The highest agreement is achieved in bin 10 which
represents the 10% of documents ?most confidently? classified to a given domain, i.e. those documents
with the highest similarity score to the domain vector. Just under 80% of these documents (77.27%) are
included in the union of assessors data and 63% are included in the majority. I.e. for approximately 77%
of the documents most confidently classified to a domain by our classifier, at least one in three humans
will agree with the domain classification and for about 63% the majority of humans will agree.
3.4.2 Term in domain assessment
Term length Total Term in the Term in agiven domain different domain
All length 457 88% 12%
1 144 88% 12%
2 182 87% 13%
3 84 92% 8%
4 47 91% 9%
Table 2: Terms with different term length
Languages Total Term in the Term in agiven domain different domain
All languages 457 88% 12%
CS 103 86% 14%
DE 79 82% 18%
EN 80 88% 13%
ES 54 80% 20%
LT 47 98% 2%
LV 94 97% 3%
Table 3: Terms of different languages
A total of 1,200 candidate terms in 6 languages were assessed by 3 assessors and the majority judg-
ments (i.e. cases where at least two assessors agree) show that 38% terms were assessed to be candidate
terms in the given domain, 5% terms were assessed to be candidate terms in a different domain, and the
rest (57%) were deemed not to be terms.
This indicates that out of all candidate terms which were identified to be correct terms (43% of the
data), 88% were assessed to be in the same domain as the documents they were extracted from. Further
analysis showed that the 57% of candidates judged not to be terms could be further broken down into
33% which contain an overlap with a term, i.e. term boundaries were incorrectly identified, and 24%
which neither are nor overlap with a term.
Of the 43% candidate terms that were judged to be terms, we examined the variation in extent to
which they were judged to be terms in the given domain across term lengths and across languages. These
figures are shown in Tables 2 and 3. We also examined variation in the extent to which these terms were
judged to be terms in the given domain across the two domains we were investigating: in ?Industries and
17
Technology? 92% of the terms were judged to be in the given domain and 8% in another domain, while
for ?Politics and Administration? these figures were 85% and 15% respectively.
For the 43% of the term candidates that were identified as correct terms (457 terms), all three assessors
agreed about the domain of the term, i.e. they either accepted the domain proposed by the system for the
term or they agreed on an alternative(s), in 45% of the cases. In 54% of the cases there was not universal
agreement but at least two assessors agreed on at least one domain they assigned to the term. Only in 1%
of the cases was there no overlap in judgment about term domain.
4 Analysis and Discussion
Let us now return to the research questions we raised in Section 1. Our first question was: How well
can a simple vector space classifier built from a multilingual thesaurus automatically classify documents
into domains prior to assigning these domains to the terms within the documents? First, we have to view
system performance in the context of human performance. Results in the last section show that 2 out
of 3 humans agree 78% of the time on exact assignment of (possibly multiple) domains to documents
and 98% of the time if only one of the domains they assign to a document need to match. Over all
languages and domains our classifier achieves only 45% agreement with the majority judgment and
58% with the union of judgments. However, if we restrict ourselves to the highest confidence domain
assigments, then the picture is much better: 63% agreement with the majority judgment and 77% with
the union of judgments. This restriction reduces the number of documents from which terms could be
mined from if accurate domain classification is important ? but so long as there are lots of documents
to mine terms from this may not be important. Furthermore note that our classifier could easily be
used to select multiple domains, perhaps, e.g., when the differences in scores between highest scoring
domains is small. This would make the comparison with the human figures fairer (now the system can
only propose one domain per document while the humans can propose several) and could only result
in higher system figures relative to human ones. We conclude that the vector space classifier utilizing
domain representations derived from a pre-existing multingual thesaurus has much to recommend: it is
simple, it needs no training data, it is straightforwardly applicable to multiple (24 in our case) different
languages and its performance is adequate if it is suitably constrained.
Our second question was: To what extent do humans agree about the assignment of terms to domains?
Our results show that in less than half the cases do all three human assessors agree with the assignment
of a term to a particular domain. However, in 99% of the cases at least two of three assessors concur on
at least one domain to which the term belongs. This suggests that using overlap with two of three human
assessors is a good approach to measuring automatic domain assignment to terms.
Our third question was: How accurate is the assumption that terms can be assigned to the domains of
the documents in which they are found? Tables 2 and 3 show that on average 88% of terms are judged to
be in the domain of the document in which they are found. Furthermore there is relatively little variation
in this figure ? it ranges from a low of 80% (ES) to a high of 98% (LT) and a low of 87% for terms of
length 2 to a high of 92% for terms of length 3. This suggests that assigning domains to terms based
on the domain of the document the term is found in is a relatively safe thing to do, but is by no means
perfect: just over 10% of terms will have their domains incorrectly assigned by making this assumption.
5 Related Work
There has been extensive work on the development of automated techniques to extract terminology from
document collections. Such term extraction approaches can be grouped into three categories based on
the information used to extract terms: approaches using purely linguistic information, approaches using
purely statistical information and those using combinations of both. An analysis of different approaches
is given by Pazienza et al. (2005). For the most part, however, such approaches make the assumption
that domain-specific, and perhaps also non-domain-specific, collections of texts are available. Justeson
and Katz (1995), for example, assume that term frequency of a limited sort of noun phrases in domain-
specific texts is sufficicent to indicate termhood. Others such as Chung (2003) and Drouin (2004) look
at statistical contrasts between domain-specific and general comparison or reference corpus. See also
18
(Kim et al., 2009; Marciniak and Mykowiecka, 2013; Kilgariff, 2014). By contrast our approach does
not presuppose the existence of documents pre-classified by domain (though we could benefit from this).
Rather our approach starts by classifying a document into a domain and then extracting terms from it and
assigning them the domain of the document.
Utsuro et al. (2006) and Kida et al. (2007) extract terms from web-documents. The domain spec-
ification of a term is determined in two stage approach. In the first stage for a term under inspection
web-documents which mention the term are collected. Then these documents are divided into two sets:
domain relevant and domain-irrelevant documents. A document whose content similarity to a domain
specific corpora is above a predefined threshold is regarded as relevant. Any other document is regarded
as irrelevant. In the second stage a ratio of times the term occurs in the relevant and the irrelevant set is
computed. This ratio is used to determine whether the extracted term belongs to the domain in hand or
not. Again, a domain-specific corpus is assumed for this approach to proceed.
Benedictis et al. (2013) use bootstrapping to collect domain specific terms. They start with some
manually selected domain specific seed terms, perform web-search to obtain documents, extract further
terms and re-start the process with the new terms. The documents returned by the search engine are
assumed to belong to the domain in hand and so are the extracted terms. By contrast our approach does
not require manually selected terms, but instead uses an existing domain structured multingual thesaurus.
6 Conclusion
In this paper we have investigated a number of questions relating to the identification of the domain of
a term by domain classification of the document in which the term occurs. We proposed and evaluated
a straightforward method for domain classification of documents in 24 languages which uses a multilin-
gual thesaurus to construct ?domain vectors?. We investigated the extent to which humans agree about
the domain classification of documents and terms. And, we investigated the extent to which terms are
likely to ?inherit? the domain of their parent document. Our results show that the domain classification
method has significant merit, that humans generally, but by no means universally, agree about domain
classification of documents and terms, and again that terms are generally, but certainly not universally,
likely to be of the same domain as the document in which they occur.
7 Acknowledgments
The authors would like to acknowledge funding from the European Union FP-7 programme for the TaaS
project, grant number: 296312. We would also like to thank the human assessors without whose careful
work the results reported here would not have been obtained. Finally we thank our project partners in the
TaaS project for user studies with translators and terminologists, contributions to the TaaS system, and
development of the TaaS domain classication scheme and the EuroVoc-to-TaaS mapping.
19
References
Basant Agarwal and Namita Mittal. 2014. Text classification using machine learning methods-a survey. In
Proceedings of the Second International Conference on Soft Computing for Problem Solving (SocProS 2012),
December 28-30, 2012, pages 701?709. Springer.
Ahmet Aker, Evangelos Kanoulas, and Robert J Gaizauskas. 2012. A light way to collect comparable corpora from
the web. In Proceedings of Eighth International Conference on Language Resources and Evalution (LREC),
pages 15?20.
Ahmet Aker, Monica Paramita, and Robert Gaizauskas. 2013. Extracting bilingual terminologies from comparable
corpora. In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (ACL
2013).
Ahmet Aker, Monica Paramita, Emma Barker, and Robert Gaizauskas. 2014a. Bootstraping Term Extractors for
Multiple Languages. In Proceedings of the International Conference on Language Resources and Evaluation
(LREC).
Ahmet Aker, Monica Paramita, Ma?rcis Pinnis, and Robert Gaizauskas. 2014b. Bilingual dictionaries for all EU
languages. In Proceedings of the International Conference on Language Resources and Evaluation (LREC).
Teresa Mihwa Chung. 2003. A corpus comparison approach for terminology extraction. Terminology, 9(2).
Flavio De Benedictis, Stefano Faralli, Roberto Navigli, et al. 2013. Glossboot: Bootstrapping multilingual domain
glossaries from the web. In Proceedings of the 51st Annual Meeting of the Association for Computational
Linguistics, pages 528?538.
Bruno de Besse?, Blaise Nkwenti-Azeh, and Juan C. Sager. 1997. Glossary of terms used in terminology. Termi-
nology. International Journal of Theoretical and Applied Issues in Specialized Communication, 4:117?156(39).
Patrick Drouin. 2004. Detection of domain specific terminology using corpora comparison. In Proceedings of the
Fourth International Conference on Language Resources and Evaluation (LREC2004).
John S. Justeson and Slava M. Katz. 1995. Technical terminology: Some linguistic properties and an algorithm
for identification in text. Natural Language Engineering, 1(1):9?27.
Mitsuhiro Kida, Masatsugu Tonoike, Takehito Utsuro, and Satoshi Sato. 2007. Domain classification of technical
terms using the web. Systems and Computers in Japan, 38(14):11?19.
Adam Kilgariff. 2014. Finding terms in corpora for many languages with the Sketch Engine. 14th Conference of
the European Chapter of the Association for Computational Linguistics.
Su Nam Kim, Timothy Baldwin, and Min-Yen Kan. 2009. An unsupervised approach to domain-specific term
extraction. In Australasian Language Technology Association Workshop 2009, page 94.
Christopher D Manning, Prabhakar Raghavan, and Hinrich Schu?tze. 2008. Introduction to information retrieval,
volume 1. Cambridge university press Cambridge.
Ma?gorzata Marciniak and Agnieszka Mykowiecka. 2013. Terminology extraction from domain texts in polish.
In Intelligent Tools for Building a Scientific Information Platform, pages 171?185. Springer.
Maria Teresa Pazienza, Marco Pennacchiotti, and Fabio Massimo Zanzotto. 2005. Terminology extraction: an
analysis of linguistic and statistical approaches. In Knowledge Mining, pages 255?279. Springer.
Ma?rcis Pinnis, Nikola Ljubes?ic?, Dan S?tefa?nescu, Inguna Skadin?a, Marko Tadic?, and Tatiana Gornostay. 2012.
Term extraction, tagging, and mapping tools for under-resourced languages. In Proceedings of the 10th Confer-
ence on Terminology and Knowledge Engineering (TKE 2012), June, pages 20?21.
Gerard Salton and Michael E Lesk. 1968. Computer evaluation of indexing and text processing. Journal of the
ACM (JACM), 15(1):8?36.
Inguna Skadin?a, Ahmet Aker, Nikos Mastropavlos, Fangzhong Su, Dan Tufis, Mateja Verlic, Andrejs Vasil?jevs,
Bogdan Babych, Monica Paramita, Paul Clough, Robert Gaizauskas, and Nikos Glaros. 2012. Collecting and
using comparable corpora for statistical machine translation. In Proceedings of the 8th International Conference
on Language Resources and Evaluation (LREC), Istanbul, Turkey.
20
Fangzhong Su and Bogdan Babych. 2012. Measuring comparability of documents in non-parallel corpora for
efficient extraction of (semi-) parallel translation equivalents. In Proceedings of the Joint Workshop on Exploit-
ing Synergies between Information Retrieval and Machine Translation (ESIRMT) and Hybrid Approaches to
Machine Translation (HyTra), pages 10?19. Association for Computational Linguistics.
Takehito Utsuro, Mitsuhiro Kida, Masatsugu Tonoike, and Satoshi Sato. 2006. Collecting novel technical terms
from the web by estimating domain specificity of a term. In Computer Processing of Oriental Languages.
Beyond the Orient: The Research Challenges Ahead, pages 173?180. Springer.
21
Proceedings of the 25th International Conference on Computational Linguistics, pages 38?45,
Dublin, Ireland, August 23-29 2014.
A Poodle or a Dog? Evaluating Automatic Image Annotation Using
Human Descriptions at Different Levels of Granularity
Josiah K. Wang
1
Fei Yan
2
Ahmet Aker
1
Robert Gaizauskas
1
1
Department of Computer Science, University of Sheffield, UK
2
Centre for Vision, Speech and Signal Processing, University of Surrey, UK
{j.k.wang, ahmet.aker, r.gaizauskas}@sheffield.ac.uk f.yan@surrey.ac.uk
Abstract
Different people may describe the same object in different ways, and at varied levels of granular-
ity (?poodle?, ?dog?, ?pet? or ?animal??) In this paper, we propose the idea of ?granularity-
aware? groupings where semantically related concepts are grouped across different levels of
granularity to capture the variation in how different people describe the same image content.
The idea is demonstrated in the task of automatic image annotation, where these semantic group-
ings are used to alter the results of image annotation in a manner that affords different insights
from its initial, category-independent rankings. The semantic groupings are also incorporated
during evaluation against image descriptions written by humans. Our experiments show that se-
mantic groupings result in image annotations that are more informative and flexible than without
groupings, although being too flexible may result in image annotations that are less informative.
1 Introduction
Describing the content of an image is essential for various tasks such as image indexing and retrieval, and
the organization and browsing of large image collections. Recent years have seen substantial progress
in the field of visual object recognition, allowing systems to automatically annotate an image with a list
of terms representing concepts depicted in the image. Fueled by advances in recognition algorithms and
the availability of large scale datasets such as ImageNet (Deng et al., 2009), current systems are able to
recognize thousands of object categories with reasonable accuracy, for example achieving an error rate
of 0.11 in classifying 1, 000 categories in the ImageNet Large Scale Visual Recognition Challenge 2013
(ILSVRC13) (Russakovsky et al., 2013).
However, the ILSVRC13 classification challenge assumes each image is annotated with only one
correct label, although systems are allowed up to five guesses per image to make the correct prediction
(or rather, to match the ground truth label). The problem with this is that it becomes difficult to guess
what the ?correct? label is, especially when many other categories can equally be considered correct.
For instance, should a system label an image containing an instance of a dog (and possibly some other
objects like a ball and a couch) as ?dog?, ?poodle?, ?puppy?, ?pet?, ?domestic dog?, ?canine? or even
?animal? (in addition to ?ball?, ?tennis ball?, ?toy?, ?couch?, ?sofa?, etc.)? The problem becomes even
harder when the number of possible ways to refer to the same object instance increases, but the number
of prediction slots to fill remains limited. With so many options from which to choose, how do we know
what the ?correct? annotation is supposed to be?
In this paper, we take a human-centric view of the problem, motivated by the observation that humans
are likely to be the end-users or consumers of such linguistic image annotations. In particular, we investi-
gate the effects of grouping semantically related concepts that may refer to the same object instance in an
image. Our work is related to the idea of basic-level categories (Biederman, 1995) in Linguistics, where
most people have a natural preference to classify certain object categories at a particular level of granu-
larity, e.g. ?bird? instead of ?sparrow? or ?animal?. However, we argue that what one person considers
This work is licensed under a Creative Commons Attribution 4.0 International Licence. Page numbers and proceedings footer are
added by the organisers. Licence details: http://creativecommons.org/licenses/by/4.0/
38
?basic-level? may not necessarily be ?basic-level? to another, depending on the person?s knowledge, ex-
pertise, interest, or the context of the task at hand. For example, Rorissa (2008) shows that users label
groups of images and describe individual images differently with regards to the level of abstraction. The
key idea behind our proposed ?granularity-aware? approach is to group semantically related categories
across different levels of granularity to account for how different people would describe content in an
image differently.
We demonstrate the benefits of the ?granularity-aware? approach by producing a re-ranking of visual
classifier outputs for groups of concept nodes, e.g. WordNet synsets. The concept nodes are grouped
across different levels of specificity within a semantic hierarchy (Section 3.1). This models better the
richness of the vocabulary and lexical semantic relations in natural language. In this sense these group-
ings are used to alter the results of image annotation in a manner that affords different insights from
its initial, category-independent rankings. For example, if the annotation mentions only ?dog? but not
?poodle?, a system ranking ?poodle? at 1 and ?dog? at 20 will have a lower overall score than a system
ranking ?dog? at 1, although both are equally correct. Grouping (?poodle? or ?dog?) however will allow a
fairer evaluation and comparison where both systems are now considered equally good. The ?granularity-
aware? groupings will also be used in evaluating these re-rankings using textual descriptions written by
humans, rather than a keyword-based gold-standard annotation. The hypothesis is that by modeling the
variation in granularity levels for different concepts, we can gain a more informative insight as to how
the output of image annotation systems can relate to how a person describes what he or she perceives in
an image, and consequently produce image annotation systems that are more human-centric.
Overview. The remainder of the paper is organized as follows: Section 2 discusses related work. Sec-
tion 3 describes our proposed ?granularity-aware? approach to group related concepts across different
levels of granularity. It also discusses how to apply the idea both in automatic image annotation, by
re-ranking noisy visual classifier outputs in a ?granularity-aware? manner, and in evaluation of classi-
fier outputs against human descriptions of images. The results of the proposed method are reported in
Section 4. Finally, Section 5 offers conclusions and proposes possible future work.
2 Related work
Work on automatic image annotation traditionally relies heavily on image datasets annotated with a fixed
set of labels as training data. For example, Duygulu et al. (2002) investigated learning from images
annotated with a set of keywords, posing the problem as a machine translation task between image
regions and textual labels. Gupta and Davis (2008) includes some semantic information by incorporating
prepositions and comparative adjectives, which also requires manual annotation as no such data is readily
available. Recent work has moved beyond learning image annotation from constrained text labels to
learning from real world texts, for example from news captions (Feng and Lapata, 2008) and sports
articles (Socher and Fei-Fei, 2010).
There is also recent interest in treating texts as richer sources of information than just simple bags
of keywords, for example with the use of semantic hierarchies for object recognition (Marsza?ek and
Schmid, 2008; Deng et al., 2012b) and the inclusion of attributes for a richer representation (Lampert
et al., 2009; Farhadi et al., 2009). Another line of recent work uses textual descriptions of images for
various vision tasks, for example for recognizing butterfly species from butterfly descriptions (Wang
et al., 2009) and discovering attributes from item descriptions on fashion shopping websites (Berg et
al., 2010). There has also been interest in recent years in producing systems that annotate images with
full sentences rather than just a list of terms (Kulkarni et al., 2011; Yang et al., 2011). We consider
our work to complement the work of generating full sentences, as it is important to filter and select the
most suitable object instances from noisy visual output. The shift from treating texts as mere labels to
utilizing them as human-centric, richer forms of annotations is important to gain a better understanding
of the processes underlying image and text understanding or interpretation.
Deng et al. (2012b) address the issue of granularity in a large number of object categories by allowing
classifiers to output decisions at the optimum level in terms of being accurate and being informative, for
example outputting ?mammal? rather than ?animal? while still being correct. Their work differs from
39
ours in that the semantic hierarchy is used from within the visual classifier to make a decision about
its output, rather than for evaluating existing outputs. More directly related to our work is recent work
by Ordonez et al. (2013), which incorporates the notion of basic-level categories by modeling word
?naturalness? from text corpora on the web. While their focus is on obtaining the most ?natural? basic-
level categories for different encyclopedic concepts as well as for image annotation, our emphasis is on
accommodating different levels of naturalness, not just a single basic level. We adapt their model directly
to our work, details of which will be discussed in Section 3.1.
3 Granularity-aware approach to image annotation
The proposed ?granularity-aware? approach to image annotation consists of several components. We first
define semantic groupings of concepts by considering hypernym/hyponym relations in WordNet (Fell-
baum, 1998) and also how people describe image content (Section 3.1). The groupings are then used to
re-rank the output of a set of category-specific visual classifiers (Section 3.2), and also used to produce
a grouped ?gold standard? from image captions (Section 3.3). The re-ranked output is then evaluated
against the ?gold standard?, and the initial rankings and ?granularity-aware? re-rankings are compared
to gain a different insight into the visual classifiers? performance as human-centric image annotation
systems.
3.1 Semantic grouping across different granularity levels
The goal of semantic grouping is to aggregate related concepts such that all members of the group refer
to the same instance of an object, even across different specificity levels. In particular, we exploit the
hypernym/hyponym hierarchy of WordNet (Fellbaum, 1998) for this task. WordNet is also the natural
choice as it pairs well with our visual classifiers which are trained on ImageNet (Deng et al., 2009)
categories, or synsets.
The WordNet hypernym hierarchy alone is insufficient for semantic grouping as we still need a way to
determine what constitutes a reasonable group, e.g. putting all categories into a single ?entity? group is
technically correct but uninformative. For this, we draw inspiration from previous work by Ordonez et al.
(2013), where a ?word naturalness? measure is proposed to reflect how people typically describe image
content. More specifically, we adapt for our purposes their proposed approach of mapping encyclopedic
concepts to basic-level concepts (mapping ?Grampus griseus? to the more ?natural? ?dolphin?). In this
approach, the task is defined as learning a translation function ?(v, ?) : V 7?W that best maps a node v
to a hypernym node w which optimizes a trade-off between the ?naturalness? of w (how likely a person
is to use w to describe something) and the distance between v and w (to constrain the translation from
being too general, e.g. ?entity?), with the parameter ? controlling this trade-off between naturalness and
specificity. Formally, ?(v, ?) is defined as:
?(v, ?) = arg max
w ? ?(v)
[??(w)? (1? ?)?(w, v) ] (1)
where ?(v) is the set of hypernyms for v (including v), ?(w) is naturalness measure for node w, and
?(w, v) is the number of edges separating nodes w and v in the hypernym structure of WordNet.
For our work, all synsets that map to a common hypernym w are clustered as a single semantic group
G
?
w
:
G
?
w
= {v : ?
v
?(v, ?) = w} (2)
In this sense, the parameter ? ? [0, 1] essentially also controls the average size of the groups: ? = 0
results in no groupings, while ? = 1 results in synsets being grouped with their most ?natural? hypernym,
giving the largest possible difference in the levels of granularity within each group.
Estimating the naturalness function using Flickr. Ordonez et al. (2013) use n-gram counts of the
Google IT corpus (Brants and Franz, 2006) as an estimate for term naturalness ?(w). Although large,
the corpus might not be optimal as it is a general corpus and may not necessarily mirror how people
40
describe image content. Thus, we explore a different corpus that (i) better reflects how humans describe
image content; (ii) is sufficiently large for a reasonable estimate of ?(w). The Yahoo! Webscope Yahoo
Flickr Creative Commons 100M (YFCC-100M) dataset (Yahoo! Webscope, 2014) fits these criteria with
100 million images containing image captions written by users. Hence, we compute term occurrence
statistics from the title, description, and user tags of images from this dataset. Following Ordonez et al.,
we measure ?(w) as the maximum log count of term occurrences for all terms appearing in synset w.
Internal nodes. Unlike Ordonez et al. (2013), we do not constrain v to be a leaf node, but instead
also allow for internal nodes to be translated to one of their hypernyms. We could choose to limit
visual recognition to leaf nodes and estimate the visual content of internal nodes by aggregating the
outputs from all its leaf nodes, as done by Ordonez et al. (2013). However, since the example images in
ImageNet are obtained for internal nodes pretty much in the same way as leaf nodes (by querying ?dog?
rather than by combining images from ?poodle?, ?terrier? and ?border collie?) (Deng et al., 2009), the
visual models learnt from images at internal nodes may capture different kinds of patterns than from
their hyponyms. For example, a model trained with ImageNet examples of ?dog? might capture some
higher-level information that may otherwise not be captured by merely accumulating the outputs of the
leaf nodes under it, and vice versa.
3.2 Re-ranking of visual classifier output
The visual classifier used in our experiments (Section 4.2) outputs a Platt-scaled (Platt, 2000) confidence
value for each synset estimating the probability of the synset being depicted in a given image. The
classifier outputs are then ranked in descending order of these probability values, and are treated as
image annotation labels.
As mentioned, these rankings do not take into consideration that some of these synsets are semantically
related. Thus, we aggregate classifier outputs within our semantic groupings (Section 3.1), and then re-
rank the scores of each grouped classifier. Formally, the new score of a classifier c, ?
c
(G
?
w
), for a
semantic group G
?
w
is defined as:
?
c
(G
?
w
) = max
v?G
?
w
p
c
(v) (3)
where v is a synset from the semantic groupG
?
w
, and p
c
(v) is the original probability estimate of classifier
c for synset v. I.e., the probability of the most probable synset in the group is taken as the probability of
the group.
To enable comparison of the rankings against a gold standard keyword annotation, a word label is
also generated for each semantic group. We assign as the semantic group?s label `(G
?
w
) the first term of
synset w, the common hypernym node to which members of the group best translates. Note that the term
merely acts a label for evaluation purposes and should not be treated as a word in a traditional sense. We
also merge semantic groups with the same label to account for polysemes/homonyms, again taking the
maximum of ?
c
among the semantic groups as the new score.
The semantic grouping of synsets is performed independently of visual classifier output. As such, we
only need to train each visual classifier once for each synset, without requiring re-training for different
groupings since we only aggregate the output of the visual classifiers. This allows for more flexibility
since the output for each semantic group is only aggregated at evaluation time.
3.3 Evaluation using human descriptions
The image dataset used in our experiments (Section 4.1) is annotated with five full-sentence captions
per image but not keyword labels. Although an option would be to obtain keyword annotations via
crowdsourcing, it is time consuming and expensive and also requires validating the annotation quality.
Instead, we exploit the existing full-sentence captions from the dataset to automatically generate a gold
standard keyword annotation for evaluating our ranked classifier outputs. The use of such captions is
also in line with our goal of making the evaluation of image annotation systems more human-centric.
For each caption, we extract nouns using the open source tool FreeLing (Padr?o and Stanilovsky, 2012).
41
?0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0
Semantic Grouping 0.3450 0.3450 0.3548 0.3735 0.4025 0.4417 0.4562 0.4702 0.4834 0.5059 0.5395
Random Grouping 0.3450 0.3450 0.3493 0.3529 0.3585 0.3689 0.3823 0.4067 0.4241 0.4359 0.4467
Number of groups 1294 1294 1237 1105 949 817 693 570 474 419 368
Table 1: Results of re-ranking with semantic groupings. The first two rows show the average NDCG
scores for the proposed groupings and the random baseline groupings, for different groupings formed by
varying ?. The bottom row shows the number of semantic groups formed for different values of ?.
For each image, each noun is assigned an individual relevance score, which is the number of captions
that mentions the noun. This upweights important objects while downweighting less important objects
(or errors from the annotator or the parser). The result is a list of nouns that humans use to describe
objects present in the image, each weighted by its relevance score. We assume nouns that appear in the
same WordNet synset (?bicycle? and ?bike?) are synonyms and that they refer to the same object instance
in the image. Hence, we group them as a single label-group, with the relevance score taken to be the
maximum relevance score among the nouns in the group.
Since there are only five captions per image, the proposed approach will result in a sparse set of
keywords. This mirrors the problem described in Section 1 where systems have to ?guess? the so-called
?correct? labels, thus allowing us to demonstrate the effectiveness of our ?granularity-aware? re-rankings.
In order to compare the annotations against the re-rankings, we will need to map the keywords to the
semantic groupings. This is done by matching the nouns to any of the terms in a semantic group, with a
corresponding label `(G
?
w
) for each group (Section 3.2). Nouns assigned the same label are merged, with
the new relevance score being the maximum relevance score among the nouns. If a noun matches more
than one semantic group (polyseme/homonym), we treat all groups as relevant and divide the relevance
score uniformly among the groups. Evaluation is then performed by matching the semantic group labels
against the image annotation output.
4 Experimental evaluation
Our proposed method is evaluated on the dataset and categories as will be described in Section 4.1, by re-
ranking the output of the visual classifiers in Section 4.2. The effects of semantic groupings are explored
using different settings of ? (see Section 3.1).
Baseline. To ensure any improvements in scores are not purely as a result of having a shorter list of
concepts to rank, we compare the results to a set of baseline groupings where synsets are grouped in a
random manner. For a fair comparison the baselines contain the same number of groups and cluster size
distributions as our semantic groupings.
4.1 Dataset and Object Categories
The Flickr8k dataset (Hodosh et al., 2013) is used in our image annotation experiments. The dataset
contains 8,091 images, each annotated with five textual descriptions. To demonstrate the notion of gran-
ularity in large-scale object hierarchies, we use as object categories synset nodes from WordNet (Fell-
baum, 1998). Ideally, we would like to be able to train visual classifiers for all synset categories in
ImageNet (Deng et al., 2009). However, we limit the categories to only synsets with terms occurring in
the textual descriptions of the Flickr8k dataset to reduce computational complexity, and regard the use
of more categories as future work. This results in a total of 1,372 synsets to be used in our experiments.
The synsets include both leaf nodes as well as internal nodes in the WordNet hierarchy.
4.2 Visual classifier
Deep learning (LeCun et al., 1989; Hinton and Salakhutdinov, 2006) based approaches have be-
come popular in visual recognition following the success of deep convolutional neural networks
42
(CNN) (Krizhevsky et al., 2012) in the ImageNet Large Scale Visual Recognition Challenge 2012
(ILSVRC12) (Deng et al., 2012a). Donahue et al. (2013) report that features extracted from the acti-
vation of a deep CNN trained in a fully supervised fashion can also be re-purposed to novel generic tasks
that differ significantly from the original task. Inspired by Donahue et al. (2013), we extract such acti-
vation as feature for ImageNet images that correspond to the 1,372 synsets, and train binary classifiers
to detect the presence of the synsets in the images of Flickr8k. More specifically, we use as our training
set the 1,571,576 ImageNet images in the 1,372 synsets, where a random sample of 5,000 images serves
as negative examples, and as our test set the 8,091 images in Flickr8k. For each image in both sets, we
extracted activation of a pre-trained CNN model as its feature. The model is a reference implementation
of the structure proposed in Krizhevsky et al. (2012) with minor modifications, and is made publicly
available through the Caffe project (Jia, 2013). It is shown in Donahue et al. (2013) that the activation
of layer 6 of the CNN performs the best for novel tasks. Our study on a toy example with 10 ImageNet
synsets however suggests that the activation of layer 7 has a small edge. Once the 4,096 dimensional
activation of layer 7 is extracted for both training and test sets, 1,372 binary classifiers are trained and
applied using LIBSVM (Chang and Lin, 2011), which give probability estimates for the test images. For
each image, the 1,372 classifiers are then ranked in order of their probability estimates.
4.3 Evaluation measure
The systems are evaluated using the Normalized Discounted Cumulative Gain (NDCG) (Wang et al.,
2013) measure. This measure is commonly used in Information Retrieval (IR) to evaluate ranked retrieval
results where each document is assigned a relevance score. This measure favours rankings where the
most relevant items are ranked ahead of less relevant items, and does not penalize irrelevant items.
The NDCG at position k, NDCG
k
, for a set of test images I is defined as:
NDCG
k
(I) =
1
|I|
|I|
?
i=1
1
IDCG
k
(i)
k
?
p=1
2
R
p
? 1
log
2
(1 + p)
(4)
where R
p
is the relevance score of the concept at position p, and IDCG
k
(i) is the ideal discounted
cumulative gain for a perfect ranking algorithm at position k, which normalizes the overall measure to
be between 0.0 to 1.0. This makes the scores comparable across rankings regardless of the number of
synset groups involved. For each grouping, we report the results of NDCG
k
for the largest possible k
(i.e. the number of synset groups), which gives the overall performance of the rankings.
4.4 Results
Table 1 shows the results of re-ranking the output of the visual classifiers (Section 4.2), with different
semantic groupings formed by varying ?. The effects of the proposed groupings is apparent when com-
pared to the random baseline groupings. As we increase the value of ? (allowing groups to have a larger
range of granularity), the NDCG scores also consistently increase. However, higher NDCG scores do
not necessarily equate to better groupings, as semantic groups with too much flexibility in granularity
levels may end up being less informative, for example by annotating a ?being? in an image. The in-
formativeness of the groupings is a subjective issue depending on the context, and makes an interesting
open question. To provide insight into the effects of our groupings, Figure 1 shows an example where at
low levels of ? (rigid flexibility), the various dog species are highly ranked but none of them is consid-
ered relevant by the evaluation system. However, at ? = 0.5 most dog species are grouped as a ?dog?
semantic group resulting in a highly relevant prediction, while at the same time allowing the ?sidewalk?
group to rise higher in the rankings. At higher levels of ?, however, the semantic groupings become less
informative when superordinate groups like ?being?, ?artifact? and ?equipment? are formed, suggesting
that higher flexibility with granularity levels may not always be more informative.
5 Conclusions and future work
We presented a ?granularity-aware? approach to grouping semantically related concepts across different
levels of granularity, taking into consideration that different people describe the same thing in different
43
dog (5)
road (2)
pavement (1)
street (1)
? = 0.0
score: 0.241
beagle
boston terrier
corgi
basset
hound
spaniel
border collie
terrier
dachshund
pup
st bernard
bulldog
springer spaniel
leash
kitten
pet
dog
sheepdog
penguin
sidewalk
? = 0.3
score: 0.480
beagle
boston terrier
dog
basset
hound
spaniel
border collie
terrier
dachshund
pup
bulldog
springer spaniel
leash
kitten
pet
penguin
sidewalk
doberman
collie
cat
? = 0.5
score: 0.941
dog
animal
leash
kitten
pet
penguin
sidewalk
cat
artifact
person
student
goat
livestock
rabbit
duck
baseball
chair
child
frisbee
spectator
? = 0.8
score: 0.943
dog
animal
leash
being
bird
sidewalk
cat
artifact
student
baseball
chair
child
frisbee
ball
slope
equipment
fabric
rug
seat
support
? = 1.0
score: 0.944
dog
animal
leash
being
bird
sidewalk
cat
artifact
sport
chair
child
device
ball
slope
equipment
fabric
rug
seat
support
stick
Figure 1: Example re-ranking of our visual classifier by semantic groupings, for selected values of
?. Words directly below the image indicate the ?gold standard? nouns extracted automatically from its
corresponding five captions. The number next to each noun indicate its relevance score. For each re-
ranking, we show the labels representing the semantic groupings. Italicized labels indicate a match with
the (grouped) ?gold standard? nouns (see Section 3.3).
ways, and at varied levels of specificity. To gain insight into the effects of our semantic groupings on
human-centric applications, the proposed idea was investigated in the context of re-ranking the output
of visual classifiers, and was also incorporated during evaluation against human descriptions. We found
that although the groupings help provide a more human-centric and flexible image annotation system,
too much flexibility may result in an uninformative image annotation system. Future work could include
(i) exploring different ways of grouping concepts; (ii) incorporating the output of visual classifiers to
improve both groupings and rankings; (iii) using information from more textual sources to improve
image annotation; (iv) taking the approach further to generate full sentence annotations. We believe that
these steps are important to bridge the semantic gap between computer vision and natural language.
Acknowledgements
The authors would like to acknowledge funding from the EU CHIST-ERA D2K programme, EPSRC
grant reference: EP/K01904X/1.
References
Tamara L. Berg, Alexander C. Berg, and Jonathan Shih. 2010. Automatic attribute discovery and characterization
from noisy web data. In Proceedings of ECCV, volume 1, pages 663?676.
Irving Biederman. 1995. Visual object recognition. In S. F. Kosslyn and D. N. Osherson, editors, An Invitation to
Cognitive Science, 2nd edition, Volume 2, Visual Cognition, pages 121?165. MIT Press.
Thorsten Brants and Alex Franz. 2006. Web 1T 5-gram Version 1. In Linguistic Data Consortium.
Chih-Chung Chang and Chih-Jen Lin. 2011. LIBSVM: A library for support vector machines. ACM Transactions
on Intelligent Systems and Technology, 2(3):1?27. http://www.csie.ntu.edu.tw/
?
cjlin/libsvm.
Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. 2009. ImageNet: A large-scale hierarchical
image database. In Proceedings of CVPR.
Jia Deng, Alexander C. Berg, Sanjeev Satheesh, Hao Su, Aditya Khosla, and Li Fei-Fei. 2012a. ImageNet large
scale visual recognition challenge (ILSVRC) 2012. http://image-net.org/challenges/LSVRC/2012/.
Jia Deng, Jonathan Krause, Alexander C. Berg, and Li Fei-Fei. 2012b. Hedging your bets: Optimizing accuracy-
specificity trade-offs in large scale visual recognition. In Proceedings of CVPR.
Jeff Donahue, Yangqing Jia, Oriol Vinyals, Judy Hoffman, Ning Zhang, Eric Tzeng, and Trevor Darrell. 2013.
DeCAF: A deep convolutional activation feature for generic visual recognition. arXiv:1310.1531 [cs.CV].
44
Pinar Duygulu, Kobus Barnard, Nando de Freitas, and David A. Forsyth. 2002. Object recognition as machine
translation: Learning a lexicon for a fixed image vocabulary. In Proceedings of ECCV, pages 97?112.
Ali Farhadi, Ian Endres, Derek Hoiem, and David A. Forsyth. 2009. Describing objects by their attributes. In
Proceedings of CVPR.
Christiane Fellbaum, editor. 1998. WordNet: An Electronic Lexical Database. MIT Press, Cambridge, MA.
Yansong Feng and Mirella Lapata. 2008. Automatic image annotation using auxiliary text information. In Pro-
ceedings of the 46th Annual Meeting of the Association for Computational Linguistics: Human Language Tech-
nologies, pages 272?280. Association for Computational Linguistics.
Abhinav Gupta and Larry S. Davis. 2008. Beyond nouns: Exploiting prepositions and comparative adjectives for
learning visual classifiers. In Proceedings of ECCV, pages 16?29.
Geoffrey E. Hinton and Ruslan R. Salakhutdinov. 2006. Reducing the dimensionality of data with neural networks.
Science, 313:504?507.
Micah Hodosh, Peter Young, and Julia Hockenmaier. 2013. Framing image description as a ranking task: Data,
models and evaluation metrics. Journal of Artificial Intelligence Research, 47:853?899.
Yangqing Jia. 2013. Caffe: An open source convolutional architecture for fast feature embedding. http://
caffe.berkeleyvision.org.
Alex Krizhevsky, Ilya Sutskever, and Geoffrey Hinton. 2012. ImageNet classification with deep convolutional
neural networks. In Advances in Neural Information Processing Systems.
Girish Kulkarni, Visruth Premraj, Sagnik Dhar, Siming Li, Yejin Choi, Alexander C. Berg, and Tamara L. Berg.
2011. Baby talk: Understanding and generating simple image descriptions. In Proceedings of CVPR.
Chris H. Lampert, Hannes Nickisch, and Stefan Harmeling. 2009. Learning to detect unseen object classes by
between-class attribute transfer. In Proceedings of CVPR.
Y. LeCun, B. Boser, J. Denker, D. Henerson, R. Howard, W. Hubbard, and L. Jackel. 1989. Backpropagation
applied to handwritten zip code recognition. Neural Computation, 1(4):541?551.
Marcin Marsza?ek and Cordelia Schmid. 2008. Constructing category hierarchies for visual recognition. In David
Forsyth, Philip Torr, and Andrew Zisserman, editors, Proceedings of ECCV, volume 5305 of Lecture Notes in
Computer Science, pages 479?491. Springer Berlin Heidelberg.
Vicente Ordonez, Jia Deng, Yejin Choi, Alexander C. Berg, and Tamara L. Berg. 2013. From large scale image
categorization to entry-level categories. In Proceedings of ICCV.
Llu??s Padr?o and Evgeny Stanilovsky. 2012. Freeling 3.0: Towards wider multilinguality. In Proceedings of the
Language Resources and Evaluation Conference, LREC ?12, Istanbul, Turkey, May. ELRA.
John C. Platt. 2000. Probabilities for SV machines. Advances in Large-Margin Classifiers, pages 61?74.
Abebe Rorissa. 2008. User-generated descriptions of individual images versus labels of groups of images: A
comparison using basic level theory. Information Processing and Management, 44(5):1741?1753.
Olga Russakovsky, Jia Deng, Jonathan Krause, Alexander C. Berg, and Li Fei-Fei. 2013. ImageNet large
scale visual recognition challenge (ILSVRC) 2013. http://image-net.org/challenges/LSVRC/2013/
results.php.
Richard Socher and Li Fei-Fei. 2010. Connecting modalities: Semi-supervised segmentation and annotation of
images using unaligned text corpora. In Proceedings of CVPR, pages 966?973.
Josiah Wang, Katja Markert, and Mark Everingham. 2009. Learning models for object recognition from natural
language descriptions. In Proceedings of BMVC.
Yining Wang, Liwei Wang, Yuanzhi Li, Di He, Wei Chen, and Tie-Yan Liu. 2013. A theoretical analysis of NDCG
ranking measures. In Proceedings of the 26th Annual Conference on Learning Theory (COLT 2013).
Yahoo! Webscope. 2014. Yahoo! Webscope dataset YFCC-100M. http://labs.yahoo.com/Academic_
Relations.
Yezhou Yang, Ching Lik Teo, Hal Daum?e, III, and Yiannis Aloimonos. 2011. Corpus-guided sentence generation
of natural images. In Proceedings of EMNLP, pages 444?454.
45
