Representation and Recognition Method 
for Multi-Word Translation Units 
in Korean-to-Japanese MT System 
Kyonghi Moon 
Dept. of Computer Science & Engineering 
Pohang Univ. of Science and Technology 
San 31 Hyoia-dong Nam-gu, Pohang 790-784 
Republic of Korea 
khmoon @ kle.po stech.ac.kr 
Jong-Hyeok Lee 
Dept. of Computer Science & Engineering 
Pohang Univ. of Science and Technology 
San 31 Hyoja-dong Nam-gu, Pohang 790-784 
Republic of Korea 
jhlee@postech.ac.kr 
Abstract 
Due to grammatical similarities, even a 
one-to-one mapping between Korean and 
Japanese words (or morphemes) can usually 
result in a high quality Korean-to-Japanese 
machine translation. However, multi-word 
translation units (MWTU) such as idioms, 
compound words, etc., need an n-to-m 
mapping, and their component words often 
do not appear adjacently, resulting in a 
discontinuous MWTU. During translation, 
the MWTU should be treated as one lexical 
item rather than a phrase. In this paper, we 
define the types of MWTUs and propose 
their representation a d recognition method 
depending on their characteristics in 
Korean-to-Japanese MT system. In an 
experimental evaluation, the proposed 
method turned out to be very effective in 
handling MWTUs, showing an average 
recognition accuracy of 98.4% and a fast 
recognition time. 
1 Introduction 
As a transfer problem in a machine 
translation (MT), lexical and structural 
differences exist between source and target 
languages, which requires l-n, m-n, or n-1 
mapping strategies for machine translation 
system. For such mapping strategies, we need to 
treat several (n, or m) words (or morphemes) as 
a single translation unit. Although some 
researches (D.Santos,1990; Linden E.,1990; 
Yoon Sung Hoe, 1992; Ha Gyu Lee, 1994; 
D.Arnold,1994) employ the term "idiom" for 
these units, we prefer MWTU (Multi-Word 
Translation Unit) because it is a more general 
and broader term for MT environment. 
Up to now, some reseamh as focused on 
recognition and transfer of MWTUs, although 
very little research has been undertaken for 
Korean-to-Japanese machine translation systems 
(Seen-He Kim,1997). In previous researches, 
some tended to simplify the problem by treating 
only special types of MWTUs, while others had 
some recognition errors and took too much 
recognition time because they did not restrict he 
recognition scope (D.Santos,1990; Yoon Sung 
Hee,1992; Ha Gyu Lee, 1994; Seen-He 
Kim, 1997). 
For a Korean-to-English MT, Lee and Kim 
(Ha Gyu Lee,1994) uses only weak restrictions 
like adjacent inforlnation for recognition scope. 
However, their method needs stronger 
restrictions to resolve recognition errors and to 
speed up the process. Although some differences 
exist depending on which kinds of source and 
target languages are dealt with, MWTUs in 
Korean-to-Japanese MT frequently have their 
component words close together, so that one can 
predict he location of their separated component 
words. For this reason, we can enhance the 
recognition accuracy and time effectively by 
restricting the recognition scope according to the 
characteristics of an MWTU rather than taking 
the whole sentence as the scope. 
Moreover, the method by Lee and Kim (Ha 
Gyu Lee,1994) deals with only surface-level 
consistency without considering word order 
because Korean has ahnost free word order. It is 
obvious that the method can deal with variable 
544 
word-order MWTUs, but some incorrect 
recognition results arc possible whcn meaning 
changes according to word order. Because 
MWTUs to be treated in Korean-to-Japanese 
MT have an almost fixed word order sequence, 
their meaning may vary if the word order is 
changed. In (1), both sentences have the same 
lexical words (or morphemes), but while the first 
sentence must be treated as an MWTU, the 
second, which has the different sequence from 
the first, does not have the meaning of an 
MWTU. In (1), the words surrounded with a box 
are an essential component morpheme for an 
MWTU. 
(big) (nose) (get hurt) 
/*(1) had a b i t~)er ience  */ 
(nose) (get hurt) (big) 
/* It is serious (that I) got hurt in my nose */ 
In this paper, to solve the word order 
problem and thus enhance a recognition 
accuracy and time for MWTUs, we fix the word 
order in an MWTU and define the recognition 
scope of component words according to their 
characteristics. Based on it, then we propose a 
representation and recognition method of 
MWTUs for a Korean-to-Japanese MT system. 
In the rest of this paper, details will be presented 
about lhese proposed ideas, logclher with some 
evalualion results. For representing Korean and 
Japanese expressions, the 1994-SK (ROK 
Ministry of Education) and the Kunrei 
Romanization systems are used respectively. 
2 Processing of MWTUs 
In developing MT systems, we frequently 
contact with some differences in word spacing, 
grammar, and so on, between sotuve and target 
languages. But the method and degree of 
difficulty of handling them highly depend upon 
the nature of the source and target hmguage in 
the MT system. In this paper, we treat the 
representation and recognition methods of 
MWTUs according to their characteristics for 
only a Korean-to-Japanese MT system. 
2.1 Types of MWTU 
There call be 1-1, l-m, n-l, and n-m 
mapping relations of morphemes between source 
and target language in machine translation. Due 
to the grammatical similarities of Korean and 
Japanese, Korean-to-Japanese machine 
translation systems have been developed under 
the direct MT strategy, which assumes a 1-1 
mapping relation. But a uniform application of 
this 1-1 mapping relation will easily result in an 
unnatural translation. 
It is not difficult to handle a 1-1 and l-m 
mapping relations in Korean-to-Japanese MT 
system although it uses only direct MT strategy, 
because it is easy to recognize only one 
morpheme in source language, Korean. It is also 
due to the fact that Japanese correspondences 
have characteristics of non-spacing and 
continuity, which allows several words to be 
treated as a single word. In this reason, we need 
to consider just types with n-I and n-m mapping 
relations. Table 1 shows the types of MWTUs to 
be handled in Korean-to-Japanese MT. 
The compound words in Table 1 are the 
units that must be translated into one Japanese 
morpheme though they are conlpound words ill 
Korean. For example, "wodett peuroseseo" is a 
Korean compound word which consists of two 
morphemes "wodeu" and "l)euroseseo", but its 
Japanese equivalent is only one morpheme, 
"walmro". The Korean word '),eojju -co be 
l-dal" is also a compound word, made by 2 
lexical morphemes "yeoiju" and "be" and 1 
functional morpheme "-eo", but it also 
corresponds to only one Japanese equivalent 
morpheme, "ukagal-u\]". in these cases, the 
Korean compound words shoukl be recognized 
as one unit to be transformed into one Japanese 
morpheme. 
We can classify verbal nouns into 2 types 
according to their Japanese quivalents. Table 2 
shows them. If we define a Korean verbal noun 
as X and its equivalent in Japanese as X', and 
another single word in Japanese as Y, we can 
describe the two types of relations between 
Korean and Japanese verbal nouns as below. 
Although the type 1 satisfies l:l mapping 
relation, the type 2 does not. So, for the type2, 
the verbal noun, X (e.g., "chuka") and "ha\[-da\]" 
need to be recognized as a single unit to be 
transformed into a Japanese quivalent, Y.
545 
5) Idiom :: :: 
~\] l-&,l 
(congratulation) (do) 
(noise) (play) 
(thing) (equal) 
(bi\[~) (nose) \[,,,~, I(~et hurt) 
(first) (see) 
(in favor of) 
/* ask */ 
iwal-ul 
/* congratulate */ 
sawa\[-gu\] 
/* disturb */ 
soul-da\] 
/,I: seen l  *'/ 
hide -i me -hi a \[-u\] 
/* have a bitter experience */ 
hazime -masi -te 
/* How do you do */ 
-110 t(l111(~ -I10 
/* lbr */ 
ITable 2\] Types of verbal nouns 
X + ha\[-dal 
X + hal-dal 
Japanese 
X'  .t- SHl"tl 
Y 
Collocation patterns are the units that 
frequently co-occurr in sentences and affect the 
semantics of each other. There are two kinds of 
collocation patterns. In one, each component 
morpheme is translated into different equivalents, 
such as "dambae \[-reul\] piu\[-&ll(smoke)" 
corresponding to "tabako -o su\[-u\]", and in the 
other, all component morphemes must be 
translated into one Japanese morpheme with an 
equivalent meaning, such as "soran \[-eul\] 
piu\[-da\]" corresponding to "sawa\[-gu\]". While 
the morphemes in the former case have a l-to-1 
mapping relation, the morphemes in the latter 
case have an n-to-1 mapping relation and 
therefore, must be treated as a single morpheme. 
While some modalitics consist of only one 
morpheme like "-eot" or "-da", there are also 
some modalities made up of several morphemes 
like "-neun geot gat". Accordingly, the latter 
must be handled as an MWTU. 
An Idiom is a general idiomatic unit 
defined in a dictionary. Generally, since an 
idiom does not reflect literal meaning itself, 
translating their component morphemes 
individually results in very different meaning, In 
this case, it must be treated as a single unit. 
A colloquial idiomatic phrase is also 
composed of several morphemes, but it is 
recognized like a single unit word. For instance, 
the Korean greeting "cheoeum bee 1) -get 
-seumnida" corresponds to "hazime -masi -le" 
in Japanese. In this case, a 1-to-I mapping 
transformation results in an unnatural translation. 
Therefore, it also should be recognized as 
MWTUs. 
Moreover, MWTUs can be used for groups 
of words that can give a more natural translation 
when they are treated as one unit. We will call 
these groups of words semi-words. 
2.2 The Characteristics of MWTUs 
To minimize the recognition time and 
recognition error rate of MWTUs, we need to 
represent MWTUs according to their 
characteristics. The following shows the 
characteristics of MWTUs. 
1) Fixed word order 
All of the 7 types of MWTUs in Table 1 
have a fixed word order sequence, even though 
Korean and Japanese are known as free word 
order languages. Expressions uch as "keu -n ko 
dachi" and "-neun geot gat" nmst be recognized 
546 
as MWTUs, but their meaning may be changed 
from thin of MWTUs if the word order sequence 
has been changed. This provides a good 
characteristic for simply representing MWTUs. 
2) Extension by insertion o1' other words 
For some kinds of MWTUs, it is possible to 
insert some grammatical morphemes or other 
words between their component n~orplaemes of 
an MWTU. "-do" in (2) , "-reul" and "-reul geu 
-ege" in (3) are those cases. 
(go) (means) (is) 
/* (l) can go */ 
/* (1) can go, too */ 
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  (3 )  
(a favor) (owe) 
/* be obliged to */ 
~ -reul ~ -da 
/* be obliged to */ 
, ~  -reul eg~ -egg ~ -da 
(he) 
/* be obliged to him */ 
According to this feature, the relations 
between immediately located two component 
morphemes of MWTUs can be classified as 
follows: 
A. tightly connected : the relation that no 
morpheme can be inserted between them 
B. loosely connected : the relation that some 
morphemes can be inserted between them. 
B-I. Only particles mad endings of a 
word are allowed to be inserted between 
them. 
B-2. Any kinds of morphemes can be 
inserted between them. 
\[Figure I\] Relations between two adjacent 
component morphemes of MWTUs 
3) Strong cohesion 
Although some MWTUs have 
characteristics of extension by insertion of other 
words, component morphemes in an MWTU 
have strong cohesion, not only logically but also 
physically. This means that tile recognition o1' an 
MWTU is possible by local comparison of its 
physical ocation. But it does not imply that the 
scope is limited in a simple sentence structure. 
4) The predictable recognition scope of 
MWTUs 
It is possible to predict tile recognition 
scope between two adjacent component 
morphemes of MWTUs, according to the above 
characteristics. The scope can be predicted as 
follows l'or each type of MWTUs shown in 
Table 1. 
Component morphemes of a compound 
word are corltiguous to the next Olle, so their 
scopes are predictable. 
Both verbal nouns and collocation patterns 
have the l'orm combined with "Noun" and 
"Verb", where other words can be inserted 
between them. But in the case (51' 
"Noun+Verb+Verb", which is the fern1 that 
another verb is inserted between the noun and 
verb, its meaning may be different in that of an 
MWTU. So ttae scope of the "Verb" can be 
limited up to the position of the first verb 
appearing after the "Noun", that is, the position 
where the POS(part-ofspeech) appears. 
Component morphemes of a modality have 
an especially strong cohesion. So at most, one 
particle is often inserted next to the bound noun. 
From this, we can predict the next component 
morpheme apart from pro component at most in 
distance 2. 
idioms, colloquial idiomatic phrases and 
senti-words consist of various colnponenl 
morphemes, which results in various scopes for 
MWTU recognition. The scopes of each 
conlpollellt ll\]Ol'phellles froul pl*e-colllponellt 
morphemes can be determined by distance 1, 
distance 2, or infinity. But inl'inite scope can 
also be limited by the position which the POS of 
the component morpheme appears. 
2.3 Representation of MWTU 
The representation f an MWTU must be 
considered in order to enhance recognition 
accuracy and speed up the process. Accordingly, 
in this paper, we propose representation method 
(51' MWTUs according to the characteristics 
mentioned in section 2.2. 
One basic rule for MWTU representation is 
that an MWTU is composed of only lexical 
morphemes if possible, that is, grammatical 
547 
morphemes uch as particles and the endings of 
a word will be extracted in the representation 
because of the above characteristics which are 
freely inserted and omitted. However, 
grammatical morphemes affecting the meanings 
of MWTUs must be described. 
Next, according to the characteristics 
described in section 2.2, we need to represent 
recognition scopes between adjacent component 
morphemes and POS of each component 
morpheme for the restriction of recognition 
scope. 
m,(POS,, d,2) m2(POS 2, d2~) ... m,(POS,, d,, m) ... 
m (POS,,, d,,.,,+,) 
m~: i-th COlnponent morpheme o1' an MWTU 
POS~ : POSofm~ 
d~.~+ x : maximum distance from m, tom~+~ 
\[Figure 2\] Representation of an MWTU 
d~,~+~ has 4 kinds of values according to 
Figure 1. For the case of A, d~,~+, is 1, for the case 
of B-l, it is 2, for the case of B-2, it is ~, mad 
then for the last component morpheme, it is 
always 0 because (n+l)-th component 
morpheme doesn't exist. 
The examples of MWTUs described by 
above representation are shown in Figure 3. 
? wodeu(N,1)proseseo(N,O) ~ wapuro 
(word) (processor) /* word processor */ 
? yeojju(V, 1) -eo(mC, 1 ) bo(V,0) ~ ukaga 
(ask) (see) /* ask */ 
? keu(ADJ, 1 ) -n(mT, l ) ko(N,2) dachi(V,O) 
(big) (nose) (get hurt) 
hidoinwnia /* have a bitter experience */
? -neunOnT,l) geot(ND,2) gat(ADJ,O) ~ sou 
(thing) (equal) /* seem */ 
? chuka(N,oo) ha(V,0) ~ iwa 
(congratulation) (do) /* congratulation*/ 
? -reul(j,1 ) wiha(V, 1) -n(mT,0) ~ notameno 
(in favor of) /* for */ 
? sesang(N, 2) muljeong(N, oo) moreu(V,O) 
(world) (condition) (don't know) 
seziniuto I* be ignorant of the world */ 
? jal(B,l) meok (V,l) -eot(e,l) -seumnidaOnT,O) 
(well) (eat) 
gotisousamadesita 
/* I have enjoyed my dinner very much */ 
\[Figure 3\] Examples of MWTUs 
Each MWTU is entered into the dictionary 
as an entry word such as the general morphemes 
as shown in Figure 4. Additionally, for 
recognition, we made the first component 
morpheme of the MWTU have an MWTU field, 
which is composed of MWTUs starting from the 
entry word. This means that only one access to 
the dictionary is needed after an MWTU is 
confirmed. Figure 4 shows the dictionary 
structure for an MWTU. 
4____  
;(mouth) (use) 
/* speak carelessly */ 
\[Connection i fo. for K~ 
\[Semantic info., Colloc~ 
\[Japanese quivalence, 
Janane, se ..... \] 
(ip) 
prcan\] 
tion pattern\] 
Connection 
\[Connection i fo. lot Kcrean, 
MWTU {ip(N ~ wlli(y;O), ip(N, 
bareu(V,O) ..... } \] 
\[Semantic info., Collocation pattern\] 
\[Japanese equivalence, Connection info. \['or 
.lanane~e ..... 1 
info. for 
~) 
\[Figure 4\] Dictionary for an MWTU 
2.4 Recogn i t ion  o f  MWTU 
Some rules are required in order to 
recognize MWTUs represented like those in 
section 2.3. 
First, the recognition scope of m~+~ after 
recognizing m~ is decided by POS~+, and d~.~+ c For 
restricting the recognition scope maximally 
while preventing other recognition errors, we 
formulated recognition scopes of each 
component morphemes of an MWTU as follows. 
RS(Recognition Scope) = min\[real_dist~<, d,+,\] 
real dist~+~ : the distance fi'om ln~ tothe i~oint 
- ' that the POS of In\[+ ~appears at 
first in an input sentence 
d~ ~+~ : maximum distance from m~ to in ,+, 
\[Figure 5\] Recognition scope 
In (4), for an MWTU "ip(N,oo) nolli(V,O), 
the recognition scope of "nolli" is 3 because dl, 2 
is oo and real_dist,, 2 is 3, which is fi'om 6-3. For 
an MWTU, "-ji(mC,2) an(V,0), the recognition 
scope of "an" is 1 because d3. 2 is 2 and real_dist,, 2 
is 1, which is from 12-11. Therefore, we can 
recognize MWTUs by a small comparison. 
548 
position 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 
Korean: , ,e -ga~-eu l  geureoke ~- ,nyeon bi,um-eul bat ~ ~-get-neut,ya ?...(4) 
(you) (mlmth) (in that manner) (u/e) (censure) (receive)\(nqt) 
speak carelessly in lhat manncr,'~u may be censured. */ 
Japanese: anatrt -ga sou nuka -se -ha hinan -o uke na -i ka "~ 
(you) (in that manner) (speak carelessly) (censure) (receive) (not) 
/* If you speak carelessly in that manner, you may be censured. */ 
position l 2 
Korean "~- reu l  
(a favor) 
I 
Japanese (a): !mlzi -nagmTa 
(deploring) 
Japanese (b): 
3 4 5 6 7 8 9 10 11 12 
hanta, -her -myeo ~ -neun hae -reul barabo -at -da ....... (5) 
(deploring) (?we or set) (sun) (look) 
/ 
~"~' /*Denlorine his circumstance. I looked at a settine sun.*/ 
osewanina -ru hi -o nagame -ta (X) 
(be obligated to) (sun) (look) 
/*Deploring, I looked at a sun which I am obligated to.*/ 
minoue -o tanzi -nagara irihi -o nagame -ta (O) 
(circumstance) (deploring) (a setting sun) (look) 
/*Deploring his circumstance, he looked at a setting sun.*/ 
\[Figure 6\] Recognition examples 
This Recognition rule can also prohibit 
some recognition errors generated from 
urlrlecessary comparisons. For instance, the 
recognition scope of "ji" in an MWTU 
"sinse(N,oo),ji(V,0)" was limited by 2, which is 
the minimum value between d~=(oo) and 
real_distj.2(3-1=2). So it prohibits errors, such as 
Japanese (a) in (5), occurring when an MWTU is 
recognized in whole sentence. 
The second rule states that morphemes 
inserted between the component morphemes of 
the recognized MWTU must be rearranged in 
the following manner: 
1) ff inserted morphemes are lexical 
morphemes, they are rearranged to the front of 
the MWTU. "geureoke(in that manner)" in (4) is 
such a case. 
2) If they are grammatical morphemes, they 
are ignored when they directly follow any 
component of the MWTU, and they are 
transl~rred to the front of the MWTU together 
with the inserted lexieal morphemes when thcy 
follow any inserted lexical morphemes. In (4), 
"-eul" is the former case. If any grammatical 
morpheme such as "-do" or "-ha" is attached 
after "geureoke", it will be the latter case. 
Third, if a morpheme is the common subset 
of the two MWTUs, we select the one such that 
its first component morpheme locates in the 
pre-position. This rule is used to reduce the 
recognition time by skipping morphemes which 
are subsets of the pre-confirmed MWTUs 
Fourth, we select he superset of MWTU in 
case that two or more MWTUs starting from a 
same morpheme are recognized and one is the 
superset of the others. For" example, let us 
consider two MWTUs: '~iamsi -man -yo (wait a 
moment)" and 'ijamsi -man(for a little while)", ff 
",jamsi -man-yo" is recognized, '~iamsi -man" 
can also be recognized and '~amsi -man -yo" is 
the supcrset of "jamsi -man". In this case, we 
select he supersct, '~antsi -man -yo". 
549 
3 Evaluat ion 
To demonstrate the efficiency of our 
proposed method, we applied it to a 
Korean-to-Japanese machine translation system 
(COBALT-K/J), and evaluated its recognition 
accuracy and recognition time. COBALT-K/J 
consists of about 150,000 general purpose words 
and 7,500 MWTUs. For the test corpus, we 
arbitrary extracted 2,808 sentences from a 10 
million word corpus, the KIBS (Korean 
Information Base System). MWTUs registered 
in the dictionary appeared 3,647 times in them. 
Table 3 shows the evaluation results 
classified by the types of MWTUs. 
\[Table 3\] Evaluation results on the recognition 
of MWTUs 
~i ,  Accur 
~u~o,~ 
33 32 97.0% 
:A)g:No! i
son  
918 907 
C0116~afio ..... 33 29 
1326 i 292 
5 5 
Coil0quial 
~3 83 
1249 ! 242 
> tola! 3,647 3,590 
98.8% 1.05 
87.9% 1.82 
97.4% 1.02 
100% 1.3 
100% 1.08 
99.4% t 1.01 
98.4% 1.03 
In Table 3, idioms, collocation patterns and 
compound words have a very low frequency 
while verbal nouns, modalities and semi-words 
have a relatively high frequency. Nevertheless, 
98.4% of the test samples were recognized 
correctly. In order to recognize an MWTU, it 
needed only 1.03 comparisons per each 
component morpheme of the MWTU on the 
average. This shows the effectiveness and the 
speed of our proposed method for treating 
MWTUs in Korean-to-Japanese MT. 
Conclusion 
In this paper, we classified the different 
kinds o1' MWTUs and proposed a representation 
and recognition method for them in a 
Korean-to-Japanese MT. 
MWTUs in Korean-to-Japanese MT have 
the characteristics of fixed word order, strong 
cohesion, predictable scope of its component 
morphemes, extension by other words, etc. 
Accordingly, we enhanced accuracy and 
recognition time by representing and 
recognizing MWTUs according to their 
characteristics. 
In our experiment, 98.4% of the test 
samples were recognized correctly, which shows 
the effectiveness of our proposed method. In 
future work, we will research in more strict 
recognition restrictions and plan to extract 
MWTUs from a corpus automatically. 
References 
D. Santos(1990), Lexical gaps and idioms in 
machine translation, 13" International 
Conference of Computational Linguistics. 
Coling 90, Finland, pp. 330-335. 
Linden E., Wessel K. (1990), Ambiguio~ 
resolution and the retriewE of idioms: two 
aM)roaches, 13'" International Conference of 
Computational Linguistics. Coling 90, Finland, 
pp. 245-248. 
Yoon Sung Hee (1992), Idiomatical and 
Collocational Approach to English-Korean 
Machine Translation., Proceedings of 
1CCPOL '92, pp.56-60. 
Ha Gyu Lee, Yung Taek Kim (1994), 
Representation arm Recognition of Korean 
Idioms for Machine Translation, Journal of the 
Korean Information Science Society, Vol. 21, 
No. 1, pp.139-149 (written in Korean). 
Seon-Ho Kim (1997), Lexicon-Based Approach 
to Recognition and Tran,sfer of Multi-Word 
Translation Units hz Korean-Japanese 
Machine 7)'anslation, MS Thesis, Pohang 
University of Science and Technology (written 
in Korean). 
D.Arnold, L.Balkan, R. Lee Hurnphreys, 
S.Meijer, L.sadler (1994), Machine 
Transhttion, Blackwell, USA. 
550 
Two-Phase Shift-Reduce Deterministic Dependency Parser of Chinese 
Meixun Jin, Mi-Young Kim and Jong-Hyeok Lee 
Div. of Electrical and Computer Engineering,  
Pohang University of Science and Technology (POSTECH) 
Advanced Information Technology Research Center (AITrc) 
{Meixunj, colorful, jhlee}@postech.ac.kr 
Abstract 
In the Chinese language, a verb may 
have its dependents on its left, right or on 
both sides. The ambiguity resolution of 
right-side dependencies is essential for de-
pendency parsing of sentences with two or 
more verbs. Previous works on shift-
reduce dependency parsers may not guar-
antee the connectivity of a dependency tree 
due to their weakness at resolving the 
right-side dependencies. This paper pro-
poses a two-phase shift-reduce dependency 
parser based on SVM learning. The left-
side dependents and right-side nominal de-
pendents are detected in Phase I, and right-
side verbal dependents are decided in 
Phase II. In experimental evaluation, our 
proposed method outperforms previous 
shift-reduce dependency parsers for the 
Chine language, showing improvement of 
dependency accuracy by 10.08%.  
1 Introduction 
Dependency parsing describes syntactic struc-
ture of a sentence in terms of links between in-
dividual words rather than constituency trees. 
The fundamental relation in dependency parsing 
is between head and dependent. Robinson[1] 
formulates four axioms to the well-formed de-
pendency structures, known as single headed, 
acyclic, connective and projective. 
In this paper, we present a dependency pars-
ing strategy that produces one dependency struc-
ture that satisfies all these constraints.  
This paper is organized as follows. Related 
works are introduced in section 2. In section 3, 
detailed analysis of the work of Nivre[2] and 
Yamada[3] are given. Then our parsing strategy 
is introduced. In section 4, experiments and re-
sults are delivered. Finally a conclusion will be 
given in section 5. 
2 Overview of Related Works 
Most nature language grammars tend to as-
sign many possible syntactic structures to the 
same input utterance. A parser should output a 
single analysis for each sentence. The task of 
selecting one single analysis for a given sen-
tence is known as disambiguation.  
Some of the parsing strategies first produce 
all possible trees for a sentence. The disam-
biguation work is done in the end by searching 
the most probable one through parsing tree for-
est. Statistical parsers employ probability as a 
disambiguation measure and output the tree with 
the highest probability[4,5]. However, in the 
work of Collins [6], 42% of the correct parse 
trees were not in the candidate pool of ~30-best 
parses. Disambiguation work by searching 
throughout the parsing tree forest has limitations. 
The alternative way is to disambiguate at each 
parsing step and output the parsing result deter-
ministically. Nivre[2] and Yamada[3] suggest a 
shift-reduce like dependency parsing strategy. In 
section 3.1 we give a detailed analysis of their 
approach.  
There are several approaches for dependency 
parsing on Chinese text. Ma[5] and Cheng[18] 
are examples of these approaches. The training 
and test set Ma[5] used, are not sufficient to 
prove the reliability of Ma?s[5] approach. On the 
frame of parsing Chinese with CFG, there are 
several approaches to apply the original English 
parsing strategies to Chinese [7,8,9]. The poten-
tial purposes of these works are to take advan-
tage of state-of-art English parsing strategy and 
to find a way to apply it to Chinese text. Due to 
the differences between Chinese and English, 
256
the performance of the system on Chinese is 
about 10% lower comparing the performance of 
the original system.  
3 Two-Phase Dependency Parsing 
3.1  Review of Previous Shift-Reduce Dependency 
Parsers 
Nivre[3] presented a shift-reduce dependency 
parsing algorithm which can parse in linear time. 
The Nivre?s parser was represented by a triples 
<S, I, A>, where S is a stack, I is a list of (re-
maining) input tokens, and A is the set of deter-
mined dependency relations. Nivre defined four 
transitions: Left-Arc, Right-Arc, Reduce, and 
Shift. If there is a dependency relation between 
the top word of the stack and the input word, 
according to the direction of the dependency arc, 
it can be either Left-Arc or Right-Arc. Otherwise, 
the transition can be either shift or reduce. If the 
head of the top word of the stack is already de-
termined, then the transition is reduce, otherwise 
shift. The action of each transition is shown in 
Fig.1. For details, please refer to Nivre[3,10]. 
Fig.2 gives an example1 of parsing a Chinese 
sentence using Nivre?s algorithm. 
Nivre?s[3,10] approach has several advan-
tages. First, the dependency structure produced 
by the algorithm is projective and acyclic[3]. 
Second, the algorithm performs very well for 
deciding short-distance dependences. Third, at 
each parsing step, all of the dependency rela-
tions on the left side of the input word are de-
termined. Also as the author emphasizes, the 
time complexity is linear.  
However, wrong decision of reduce transition, 
like early reduce, cause the word at the top of 
the stack loses the chance to be the head of oth-
ers. Some words lose the chance to be the head 
of other following words. As a result, the de-
pendents of this word will have a wrong head or 
may have no head. 
The parsing steps of a Chinese sentence using 
Nivre?s[3] algorithm are given in Fig.2. At step-
5 of Fig.2, after reduce, the top of the stack was 
popped. The algorithm doesn?t give a chance for 
the word ?? to be the head of other words. 
Therefore, word ???? cannot have word ??
?? as its head. In the final dependency tree of 
example-1 in Fig.2, the arc from ?? to ?? is 
wrong. Fig.3 gives the correct dependency tree. 
Here, ?? is the head of word ??. 
                                                          
1 All the example sentences are from CTB. 
 
If there is a dependency relation between top.stack and input 
If the dependency relation is Left_arc 
   Insert (input, top.stack) pair into set A 
   pop(stack);   
Else  
   Insert (top.stack, input) pair  into set A 
   push(input);   
Else 
If the head of top of the stack is determined 
   pop(stack); 
    Else 
      push(input); 
Fig. 1. Transitions defined  by Nivre[3] 
 
?? ?     ?? ??  ??         ??? 
This  province  plan extend  attract merchants  attract investments. 
The province plans to expand attracting merchants and investments. 
                           stack ,  input                      relation set A   
 
Step-0:     <nil,???????????,{}> 
Step-1:?S  <??,?????????,{}> 
Step-2:?LA <?,????????,{(?,??)}> 
Step-3:?LA <??,??????,{(?,??),(??,?)}> 
Step-4:?RA <?? ??,????,{(?,??),(??,?), 
(?????)}> 
Step-5:?R <??,????,{(?,??),(??,?), 
(??,??)}> 
Step-6:?S <?? ??,??,{(?,??),(??,?), 
(??,??)}> 
Step-7:?LA <??,??,{(?,??),(??,?), 
(??,??),(??,??)}> 
Step-8:?LA <??,nil,{(?,??),(??,?), 
(??,??),(??,??),(?????)}> 
The dependency structure of the output: 
 
 
?? ?     ?? ??  ??         ??? 
 
S:Shift LA:Left-arc RA:Right-arc R:reduce 
Fig. 2.  Example-1: Parsing using Nivre?s algorithm 
 
 
 
?? ?     ?? ??  ??         ??? 
Fig. 3. The correct parse tree of Example-1 
 
Fig.4. gives the parsing step of another example. 
As the final dependency tree in Fig.4 shows, 
there is no head for word ???After Step-5, 
the top of the stack is word ? and input word is 
? . There is no dependency relation between 
these two words. Since the head of the word ? 
is already determined in step-2?the next transi-
tion is R(educe). As a result, word ? loses the 
chance to be the head of word ??. So, there is 
no head assigned to word ?? in Fig.4. There-
fore, Nivre?s algorithm causes some errors for 
determining the right-side dependents. 
Yamada?s[4] approach is similar to Nivre?s[3]. 
Reduce
shift
Right_arc
Left_arc 
257
Yamada?s algorithm define three actions: left, 
right and shift, which were similar to those of 
Nivre?s. Yamada parsed a sentence by scanning 
the sentence word by word from left to right, 
during the meantime, left or right or shift actions 
were decided. For short dependents, Yamada?s 
algorithm can cope with it easily. For long de-
pendents, Yamada tried to solve by increasing 
the iteration of scanning the sentences. As Ya-
mada pointed out, ?shift? transition was executed 
for two kinds of structure. This may cause 
wrong decision while deciding the action of 
transition. Yamada tried to resolve it by looking 
ahead for more information on the right side of 
the target word.  
 
??    ? ??? ? ?        ???? ? ??? 
declare to teachers a  piece      exciting   of  news. 
Declare a piece of exciting news to teachers. 
? ? ? 
Step-2 :?S  <????????????????,{}> 
Step-3 :?RA <??????????????,{(??,?)}>
Step-4 :?RA <?????????????,{(??,?), 
(?,???)}> 
Step-5 :?R <???????????,{(??,?), 
(?,???)}> 
Step-6 :?R <????????????,{(??,?), 
(?,???)}> 
? ? ? 
Step-n:?RA <??,nil,{(??,?),(?,???),(?,?)? 
(?,????),(??,?),(??,?)}> 
 
The dependency structure of the output: 
 
 
??    ? ??? ? ?        ???? ? ??? 
Fig. 4. Example-2: Parsing with Nivre?s algorithm 
 
 
??  ? ??? ??        ??                   ??           ?   ??. 
report _  200       attract  foreign country     investment   of   plan. 
Report 200 plans in attracting foreign investment. 
? ? ? 
step-i : ?RA < ??, ?????????,{( ??,?)} >
Fig. 5. Example-3: Parsing with Nivre?s algorithm 
 
When applying to Chinese parsing, the deter-
mination of dependency relation between two 
verbs is not effective. In the example-3 of Fig.5, 
at step-i, the parser decides whether the depend-
ency relation between ?? and ?? is either 
Left-arc or Right-arc. The actual head of  the 
verb ?? is ?, which is distant. By looking 
only two or three right side words ahead, to de-
cide the dependency relation between these 
verbs at this moment is not reliable. Yamada?s 
algorithm is not a clear solution to determine the 
right side dependents either. 
3.2 Two-Phase Dependency Parsing 
For the head-final languages like Korean or 
Japanese, Nivre[3] and Yamada?s[4] approaches 
are efficient. However, being applied to Chinese 
text, the existing methods cannot correctly de-
tect various kinds of right-side dependents in-
volved in verbs. All wrong decisions of reduce 
transition mainly occur if the right dependent of 
a verb is also a verb, which may have right-side 
dependents.  
For the correct detection of the right-side de-
pendents, we divide the parsing procedure into 
two-phase. Phase I is to detect the left-side de-
pendents and right-side nominal dependents. 
Although some nominal dependents are right-
side, they don?t have dependents on the right 
side, and will not cause any ambiguities related 
to right-side dependents. In Phase II, the detec-
tion of right-side verbal dependents, are per-
formed.  
3.2.1 Phase I  
In Phase I, we determine the left-side depend-
ents and right-side nominal dependents. We de-
fine three transitions for Phase I: Shift, Left-Arc, 
Right-Arc. The actions of transition shift and 
Left-Arc are the same as Nivre[3] defines. How-
ever, in our method, the transition of Right-Arc 
does not push the input token to the stack. The 
original purpose for pushing input to stack after 
right-arc, is to give a chance for the input to be 
a potential head of the following words. In Chi-
nese, only verbs and prepositions have right-side 
dependents. For other POS categories, the action 
of pushing into stack is nonsense.  In case that 
the input word is a preposition, there is no am-
biguities we describe. Only the words belong to 
various verbal categories may cause problems. 
The method that we use is as follows. When the 
top word of the stack and the next input word 
are verbs, like VV, VE, VC or VA2 [11], the 
detection of the dependency relation between 
these two verbs is delayed by transition of shift. 
To differentiate this shift from original shift, we 
call this verbal-shift. The determination of the 
dependency relation between these two verbs 
will be postponed until phase II. The transitions 
are summarized as Fig.6. 
If there is no more input word, phase I termi-
nates. The output of the phase I is a stack, which 
                                                          
2 VV, VE, VC and VA are Penn Chinese Treebank POS 
categories related to verbs. For details, please refer to [11]. 
258
contains verbs in reverse order of the original 
appearance of the verbs in the sentence. Each 
verb in the stack may have their partial depend-
ents, which are determined in Phase I.  
    
If the action is Verbal-shift 
: push the input to the stack 
else if the action is Shift 
  push the input to the stack 
else if the action is Left-arc 
  set the dependency relation for two words; pop 
the top of the stack 
else if the action is Right-arc 
  set the dependency relation for two words 
Fig. 6.  Types of transitions in the phase I 
 
The type of transition is determined by the top 
word of the stack, input word and their context. 
Most of the previous parsing models[4,12,13] 
use lexical words as features. Compared to Penn 
English Treebank, the size of Penn Chinese 
Treebank (version 4.0, abbreviated as CTB) is 
rather small. Considering the data sparseness 
problem, we use POS tags instead of lexical 
words itself. As Fig.7. shows, the window for 
feature extraction is the top word of the stack, 
input word, previous word of the top of the 
stack, next word of the input. The left-side 
nearest dependent of these is also taken into 
consideration. Besides, we use two more fea-
tures, if_adjoin, and Punc. The feature vector for 
Phase I is shown in Fig.7.  
3.2.2 Phase II  
    After Phase I, only verbs remain in the stack. 
In Phase II, we determine the right-side verbal 
dependents.  We take the output stack of Phase I 
as input. Some words in the stack will have 
right-side dependents as shown in Fig.8. For 
Phase II, we also define three transitions: shift, 
left-arc, right-arc. The operations of these three 
transitions are the same as Phase I, but there are 
no verbal-shifts. Fig.9 shows the output of Phase 
I and parsing at Phase II of example given in 
Fig.8.  
The window for feature extraction is the same 
as that of Phase I. The right-side nearest de-
pendent is newly taken as features for Phase II. 
The feature vector for Phase II is shown in 
Fig.10. 
The two-phase parsing will output a projec-
tive, acyclic and connective dependency struc-
ture. Nivre[10] said that the time complexity of 
his parser is 2 times the size of the sentence. Our 
algorithm is 4 times the size of the sentence, so 
the time complexity of our parser is still linear to 
the size of the sentence. 
 
Windows for feature extraction : 
t.stack :  top word of the stack 
p.stack:  previous word of top of the stack 
input   :  input word 
n.input:  next word of the input word 
 
x.pos : POS tag of word x 
x.left.child : the left-side nearest dependent of word x 
 
punc : the surface form of punctuation between top word of the 
stack and input word, if there is any 
if_adjoin : a binary indicator to show if the top word of the 
stack and input word are adjoined  
 
The feature vector for Phase I is : 
<p.stack.pos t.stack.pos input.pos n.input.pos p.stack.left.child.pos 
t.stack.left.child.pos input.left.child.pos punc if_adjoin> 
Fig. 7. Feature vector for Phase I 
 
????????????????????????????
????????????????????? 
(The official said that Sichuan will pursue a more open door policy, 
continuously improve the investment environments and attract more 
capitals from overseas, advanced techniques and experiences of ad-
ministration.) 
 
The contents of stack after Phase I: <??????????>.  
(attract, improve, pursue, said ) 
 
The dependents  of each verb in the stack 
 
 
 
 
 
 
 
 
 
Fig. 8. Dependents of each verb after Phase I 
 
step-0      <nil, ?? ?? ?? ??{}> 
step-1?S   < ??, ?? ?? ??{}> 
step-2?RA  < ??, ?? ??{(??,??)}> 
step-3?RA  < ??, ??{(??,??),(??,??)}> 
step-4?LA  < nil, ??{(??,??),(??,??), 
(?,??)}> 
step-5 ?S   < ?, nil?{(??,??),(??,??), 
(?,??)}> 
Fig. 9. Example of parsing at Phase II 
 
The feature vector for Phase II is : 
<p.stack.pos t.stack.pos input.pos n.input.pos 
p.stack.left.child.pos t.stack.left.child.pos  input.left.child.pos 
p.stack.right.child.pos t.stack.right.child.pos in-
put.right.child.pos n.input.right.child.pos punc if_adjoin> 
Fig. 10. Feature vector for Phase II. 
4 Experiments and Evaluation  
Our parsing procedure is sequentially per-
formed from left to right. The feature vectors for 
?
?
?
? ?
?
?
? 
?
? 
?
right-side right-side right-sideleft-side left-side 
left-side 
left-side
259
Phase I and Phase II are used as the input for the 
parsing model. The model outputs a parsing ac-
tion, left-arc, right-arc or shift. We use SVM as 
the model to obtain a parsing action, and use 
CTB for training and test the model. 
4.1 Conversion of Penn Chinese Treebank to 
Dependency Trees 
Annotating a Treebank is a tedious task. To 
take the advantage of CTB, we made some heu-
ristic rules to convert CTB into dependency 
Treebank. This kind of conversion task has been 
done on English Treebank[14,10,4]. We use the 
dependency formalism as Zhou[15] defined. 
CTB contains 15,162 newswire sentences (in-
cluding titles, fragments and headlines). The 
contents of CTB are from Xinhua of mainland, 
information services department of HKSAR and 
Sinorama magazine of Taiwan. For experiments, 
12,142 sentences are extracted, excluding all the 
titles, headlines and fragments.  
For the conversion task, we made some heu-
ristic rules. CTB defines total 23 syntactic 
phrases and verb compounds[11]. A phrase is 
composed of several words accompanied to a 
head word. The head word of each phrase is 
used as an important resource for PCFG pars-
ing[12,13]. According to the position of the head 
word with respect to other words, a phrase3 can 
be categorized into head-final, head-initial or 
head-middle set. Table.1 shows the head-initial, 
head-final and head-middle groups.  
For VP, IP and CP, these phrases have a verb 
as its head word. So we find a main verb and 
regard the verb the head word of the phrase. If 
the head word for each phrase is determined, 
other words composing the phrase simply take 
the head word of the phrase as its head. In the 
case of BA/LB4, we take a different view from 
what is done in CTB. Zhou[15] regards BA/LB 
as the dependent of the following verb. We fol-
low Zhou?s[15] thought. For sentences contain-
ing BA/LB, we converted them into dependency 
trees manually. With above heuristics, we con-
verted the original CTB into dependency Tree-
bank.  
                                                          
3 We use the label of phrases as CTB has defined. We ex-
clude FRAG, LST, PRN. For each definition of the phrase 
please refer to [11]. 
4 BA, LB are two POS categories of CTB. For details, see 
[11]. 
4.2 Experiments 
SVM is one of the binary classifiers based on 
maximum margin strategy introduced by Vap-
nik[16]. SVM has been used for various NLP 
tasks, and gives reasonable outputs. For the ex-
periments reported in this paper, we used the 
software package SVMlight [17]. 
For evaluation matrix, we use Dependency 
Accuracy and Root Accuracy defined by Ya-
mada[4]. An additional evaluation measure, 
None Head is defined as following. 
 
None Head: the proportion of words whose 
head is not determined. 
 
GROUP PHRASES 
Head-initial PP; VRD; VPT; 
Head-final ADJP; ADVP; CLP; DNP; DVP; DP; 
LCP; NP; QP; VCD; VCP; UCP; VSB; 
VNV; 
Head-
middle 
CP; IP; VP; 
Table 1. Cluster of CTB syntactic phrases 
 
Table 2. Comparison of dependency accuracy with Nivre?s 
 
We construct two SVM binary classifiers, 
Dep vs. N_Dep and LA vs. RA, to output the 
transition action of Left-arc, Right-arc or Shift. 
Dep vs. N_Dep classifier determines if two 
words have a dependency relation. If two words 
have no dependency relation, the transition ac-
tion is simply Shift. If there is a dependency re-
lation, the second classifier will decide the 
direction of it, and the transition action is either 
Left-arc or Right-arc.  
We first train a model along the algorithm of 
Nivre[10]. The training and test sentences are 
randomly selected. Table.2 shows that 1.53% of 
the words cannot find their head after parsing. 
This result means that the original Nivre?s algo-
rithm cannot guarantee a connective dependency 
structure.  
With our two-phase parsing algorithm, there 
is no none head. Then, the dependency accuracy 
and root accuracy are increased by 10.08% and 
13.35% respectively.  
 Dependency 
accuracy 
Root ac-
curacy 
None 
head 
Nivre?s algorithm[10] 73.34% 69.98% 1.53% 
Ours  84.42% 83.33% ---- 
260
4.3 Comparison with Related Works 
Compared to the original works of Nivre[10] 
and Yamada[4], the performance of our system 
is lower. We think that is because the target lan-
guage is different.  
 
 Average 
sentence 
length 
Dependency 
accuracy 
Root 
accuracy
Ma[5] 9 80.25% 83.22%
Cheng[18] 5.27 94.44% -- 
Ours 34 84.42% 83.33%
Table 3 Comparison of the parsing performances 
between Ma[5], Cheng[18] and ours 
 
Table 3 gives the comparison of the perform-
ances between Ma[5], Chen[18] and ours. The 
training and test domain of Ma[5] is not clear. 
Cheng[18] used CKIP corpus in his experiments. 
The average length of sentence in our test set is 
34, which is much longer than that in Ma[5] and 
Cheng[18]. The performance of our system is 
still better than Ma[5] and less than Cheng[8]. 
5 Conclusion 
To resolve the right-side long distance de-
pendencies, we propose two-phase shift-reduce 
parsing strategy. The parsing strategy not only 
guarantees the connectivity of dependency tree, 
but also improves the parsing performance. As 
the length of sentences increases, the ambigui-
ties for parsing increase drastically. With our 
two-phase shift-reduce parsing strategy, the per-
formance of syntactic parsing of long sentences 
is also reasonable. 
The motivation of this paper is to design a 
well-formed dependency parser for Chinese. We 
believe that there?re rooms to improve the per-
formance. We plan to work further to explore 
the optimal features. We also plan to parse Eng-
lish text with our algorithm to see if it can com-
pete with the state-of-art dependency parsers on 
English. We believe that our parsing strategy 
can apply to other languages, in which head po-
sition is mixed, as Chinese language. We think 
that it is the main contribution of our approach. 
References 
1. Robinson, J.J.: Dependency structures and 
transformation rules. Language 46 (1970) 259-285 
2. Nivre, J.: An efficient algorithm for projective 
dependency parsing. In Proceedings of IWPT 
(2003) 149-160 
3. Yamada, H. and Matsumoto, Y.: Statistical de-
pendency analysis with support vector machines. 
In Proceedings of IWPT (2003) 195-206 
4. Eisner, J.M.:Three new probabilistic models for 
dependency parsing: An exploration. In Proceed-
ings of ACL.( 1996) 340-345 
5. Ma,J., Zhang,Y. and Li,S.: A statistical depend-
ency parser of Chinese under small training data. 
IJCNLP-04 Workshop : Beyond Shallow Analy-
ese-Formalisms and Statistical Modeling for Deep 
Analyses (2004) 
6. Collins,M.: Discriminative reranking for natural 
language parsing. In proceedings of ICML 
17.(2000) 175-182 
7. Fung,P., Ngai,G, Yang,Y.S and Chen,B.: A maxi-
mum-entropy Chinese parser augmented by trans-
formation-based learning. ACM transactions on 
Asian language information processing. Volume 
3. Number 2.(2004) 159-168 
8. Levy,R. and Manning,C.: Is it harder to parse Chi-
nese, or the Chinese Treebank? In Proceedings of 
ACL. (2003) 439-446  
9. Bikel, D.M. and.Chiang, D.: Two Statistical Pars-
ing models applied to the Chinese Treebank. In 
proceedings of  the second Chinese language 
processing workshop. (2000)  
10.Nivre,J, Hall,J and Nilsson,J.: Deterministic de-
pendency parsing of English text. In Proceedings 
of COLING. (2004) 23?27  
11.Xue,N and Xia,F.: The bracketing guidelines for 
the Penn Chinese Treebank(3.0). IRCS Report 00-
08, University of Pennsylvania (2000) 
12.Collins,M.: Three generative lexicalised models 
for statistical parsing. In Proceedings of the 35th  
Annual Meeting of the Association for Computa-
tional Linguistics, Madrid (1997) 16-23 
13.Charniak,E.: A maximum-entropy-inspired parser. 
In Proceedings of NAACL. Seattle (2000) 132?
139,  
14.Collins,M.: A new statistical parser based on bi-
gram lexical dependencies. In Proceedings of the 
Thirty-Fourth Annual Meeting of the Association 
for Computational Linguistics, philladelphia 
(1996) 184?191 
15.Zhou,M. and Huang,C.: Approach to the Chinese 
dependency formalism for the tagging of corpus. 
Journal of Chinese information processing.(in 
Chinese), Vol. 8(3) (1994) 35-52 
16.Joachims,T.: Making large-scale SVM learning 
practical. Advances in Kernel Methods-Support 
Vector Learning, B.Scholkopf and C.Burges and 
A.Smola(Eds.), MIT-Press (1999) 
17. Vapnik, V.N.: The nature of statistical learning 
theory. Springer, New York. (1995) 
18. Cheng, Y.C, Asahara,M and Matsumoto Y.: De-
terministic dependency structure analyzer for Chi-
nese. In proceedings of the first IJCNLP(2004) 
135-140 
261
Search Result Clustering Using Label Language Model
Yeha Lee Seung-Hoon Na Jong-Hyeok Lee
Div. of Electrical and Computer Engineering
Pohang University of Science and Technology (POSTECH)
Advanced Information Technology Research Center (AITrc)
San 31, Hyoja-Dong, Pohang, Republic of Korea, 790-784
{sion,nsh1979,jhlee}@postech.ac.kr
Abstract
Search results clustering helps users to
browse the search results and locate what
they are looking for. In the search result
clustering, the label selection which anno-
tates a meaningful phrase for each cluster
becomes the most fundamental issue. In this
paper, we present a new method of using
the language modeling approach over Dmoz
for label selection, namely label language
model. Experimental results show that our
method is helpful to obtain meaningful clus-
tering labels of search results.
1 Introduction
Most contemporary search engines generate a long
flat list in response to a user query. This result can
be ranked by using criteria such as PageRank (Brin
and Page, 1998) or relevancy to the query. However,
this long flat list is uncomfortable to users, since it
forces users to examine each page one by one, and
to spend significant time and effort for finding the
really relevant information. Most users only look
into top 10 web pages in the list (Kummamuru et al,
2004). Thus many other relevant information can be
missed out as a result. Clustering method is pro-
posed in order to remedy the problem. Instead of
the flat list, it groups the search results to clusters,
and annotates a label with a representative words or
phrases to each cluster. Then, these labeled clusters
of search results are presented to users. Users can
benefit from labeled clusters because the size of in-
formation presented is significantly reduced.
Search result clustering has several specific re-
quirements that may not be required by other cluster
algorithms. First, search result clustering should al-
low fast clustering and fast generation of a label on
the fly, since it is an online process. This require-
ment can be met by adopting ?snippets? 1 rather than
entire documents of a search result set. Second, la-
bels annotated for clusters should be meaningful to
users because they are presented to users as a general
view of results. For this reason, recent search result
clustering researches focus on selecting meaningful
labels. This differs from general clustering which
focuses on the similarity of documents. In Zamir
and Etzioni (Zamir and Etzioni, 1998), a few other
key requirements of search result clustering are pre-
sented.
In this paper, we present a language modeling
approach with Dmoz for search result clustering.
Dmoz 2 is an Open Directory Project, and contains
manually tagged categories for web-sites. Since
these categories are built by human, they provide a
good basis to build labels for clusters. We can view
the problem of label selection for clusters as a prob-
lem of label generation by Dmoz.
We define a language model for each Dmoz-
category and select labels for clusters according to
the probability that this language model would gen-
erate candidate labels.
Thus, our method can select more meaningful la-
bels for clusters because we use labels generated by
human-tagged categories of Dmoz. The selected la-
1The term ?snippet? is used here to denote fragment of a
Web page returned by certain search engines
2Open Directory Project, http://www.dmoz.com/
637
bels enable users to quickly identify the desired in-
formation.
The paper is organized as follows. The next sec-
tion introduces related works. In Section 3, we for-
mulate the problem and show the detail of our ap-
proach. The experiment results and evaluations are
presented in Section 4. Finally, we conclude the
paper and discuss future works in Section 5.
2 RELATED WORKS
Many approaches have been suggested for organiz-
ing search results to improve browsing effectiveness.
Previous researches such as scatter/Gather (Hearst
and Pedersen, 1996) and Leuski (Leuski and Allan,
2000), Leouski (Leouski and Croft, 1996), cluster
documents using document-similarity, and generate
representative terms or phrases as labels. However,
these labels are often not meaningful, which compli-
cates user relevance judgment. They are also slow
in generating clusters and labels because they use
entire document contents in the process. Thus it is
difficult to apply these approaches to search engine
applications.
Due to the problems mentioned above, research
in search result clustering has focused on choosing
meaningful labels which is not usually addressed
in general document clustering. Zeng et al pre-
sented salient phrase ranking problem for label se-
lection, which ranks labels scored by a combination
of some properties of labels and documents (Zeng
et al, 2004). Kummamuru regarded label se-
lection as a problem making a taxonomy of the
search result, and proposed a label selecting crite-
rion based on taxonomy likelihood (Kummamuru et
al., 2004). Zamir presented a Suffix Tree Cluster-
ing (STC) which identifies sets of documents that
share common phrases, and clusters according to
these phrases (Zamir and Etzioni, 1998). Maarek
et al and Osinski presented a singular value de-
composition of the term-document matrix for search
result clustering (Maarek et al, 2000), (Osinski
and Weiss, 2004). The problem of these methods
is that SVD is extremely time-consuming when ap-
plied to a large number of snippets. Ferragina pro-
posed a method for generating hierarchical labels by
which entire search results are hierarchically clus-
tered (Ferragina and Gulli, 2005). This method pro-
duces a hierarchy of labeled clusters by constructing
a sequence of labeled and weighted bipartite graphs
representing the individual snippets on one side and
a set of labeled clusters on the other side.
3 LABEL LANGUAGE MODEL
The main purpose of Label Language Model(LLM)
is to generate meaningful labels on-the-fly from
search results, specifically snippets, for web-users.
The generated labels provide a view of the search re-
sult to users, and allow the users to navigate through
them for their search needs.
Our algorithm is composed of the four phases:
1. Search result fetching
2. Candidate Labels Generation
3. Label Score Calculation
4. Post-processing
Search result fetching. LLM operates as a meta-
search engine on top of established search engines.
Our engine first retrieves results from dedicated
search engines in response to user queries. The
search results are parsed through HTML parser,
and snippets are obtained as a result. We assume
that these snippets contain enough information
to provide user-relevance judgment. Hence, we
can generate meaningful labels using only those
snippets rather than the entire document contents of
the search result set.
Candidate Labels Generation. Candidate labels
are generated using the snippets obtained by search
result fetching. Snippets are processed by Porter?s
algorithm for stemming and stopword removing,
then every n-grams becomes a candidate label. Each
candidate label is tagged with a score calculated
by the Label Language Model. Finally, top N
candidate labels with highest scores are displayed
to users as labels for clusters of search result.
Label Score Calculation. Our model utilizes
Dmoz to select meaningful labels. Dmoz is the
largest, most comprehensive human-edited directory
of the Web and classifies more than 3,500,000 sites
in more than 460,000 categories. It is used for rank-
ing and retrieval by many search engines, such as
638
Google (Rerragina and Gulli, 2005).
Language model ranks documents according to
the probability that the language model of each doc-
ument would generate the user query.
Dmoz is a human-edited directory, which contains
meaningful categories. We can use the probability
that categories of Dmoz would generate candidate
labels as criteria to rank labels.
In our approach, the user query and the document
correspond to the candidate label and the Dmoz?s
category, respectively. We can obtain the probability
that LLM of each category would generate a label by
language model. We assume that the probability of
certain candidate label being generated can be esti-
mated by the maximum value of the probability that
LLM of each category would generate the candidate
label.
Let labeli be ith label, wij be jth word of labeli,
and Ck be kth category of Dmoz, respectively. If we
assume that the labels are drawn independently from
the distribution, then we can express the probability
that Dmoz generates labels as follows:
p(labeli|Dmoz) = max
k
p(labeli|Ck) (1)
p(labeli|Ck) =
?
p(wij |Ck) (2)
We use two smoothing methods, Jelinek-Mercer
smoothing and Dirichlet Priors smoothing (Zhai and
Lafferty, 2001), in order to handle unseen words.
The score of labeli is calculated as follows:
Si = max
k
?
j
log
(
1 + ?p(wij |Ck)
(1 ? ?)p(wij |Call)
)
(3)
Si = max
k
?
j
log
#(wkij) + ?p(wij |Call)
#(Ck) + ?
(4)
To solve the equation, p(wij |Ck) and p(wij |Call)
should be estimated. Let #(Ck) and #(Call)3 be
the number of words in kth category and the number
of words in Dmoz. Further, let #(wkij) and #(wallij )
be the number of word, wij , in kth category and the
number of word, wij , in Dmoz. Then p(wij |Ck) is
estimated as #(w
k
ij)
#(Ck) , and p(wij |Call) as
#(wallij )
#(Call) .
3Call denotes all categories of Dmoz
In Candidate Labels Generation phase, all can-
didate labels are scored. After post-processing,
candidate labels are shown in a descending order.
Post-processing. In post processing phase, labels
are refined through several rules. First, labels com-
posed of only query words are removed because they
do not provide better clues for users. Second, la-
bels that are contained in another label are removed.
Since every possible n gram is eligible for candidate
labels, multiple labels that differ only at the either
ends, i.e., one label contained in another, can be as-
signed a high score. In such cases, longer labels
are more specific and meaningful than shorter ones,
therefore shorter ones are removed. Users can ben-
efit from a more specific and meaningful label that
clarifies what a cluster contains. Finally, Top N La-
bels with highest scores produced by post processing
are presented to users.
4 EXPERIMENTS
We conducted several experiments with varying
smoothing parameter values, ?, ?. We investigated
the influence of the smoothing parameter on the la-
bel selection procedure.
4.1 Experiment Setup
Despite heavy researches on search result cluster-
ing, a standard test-set or evaluation measurement
does not exist. This paper adopts the methodology
of (Zeng et al, 2004) in order to evaluate the ex-
pressiveness of selected label and LLM
4.1.1 Test Data Set
We obtained Google?s search results that corre-
spond to fifty queries. The fifty queries are com-
prised of top 25 queries to Google and 25 from
(Zeng et al, 2004). For each query of the fifty, 200
snippets from Google are obtained. Table 1 summa-
rizes the query used in our experiment.
Search results obtained from Google are parsed
to remove html-tag and stopword, and stemming is
applied to obtain the snippets. Every n-gram of the
snippets, where n ? 3, becomes candidate labels.
Labels that do not occur more than 3 times are re-
moved from candidate set in order to reduce noise.
639
Type Queries
2005 Google
Top query
Myspace, Ares, Baidu, orkut,
iTumes, Sky News, World of War-
craft, Green Day, Leonardo da
Vinci, Janet Jackson, Hurricane
Katrina, tsunami, xbox 360, Brad
Pitt, Michael Jackson, American
Idol, Britney Spears, Angelina
Jolie, Harry Potter, ipod, digi-
tal camera, psp, laptop, computer
desk
(Zeng et al,
2004) query
jaguar, apple, saturn, jobs, jordan,
tiger, trec, ups, quotes, matrix, su-
san dumais, clinton, iraq, dell, dis-
ney, world war 2, ford, health, yel-
low pages, maps, flower, music,
chat, games, radio, jokes, graphic
design, resume, time zones, travel
Table 1: Queries used in experiment
4.1.2 Answer Label Set for Evaluation
In order to evaluate LLM, we manually created
labels for each query which are desired as outputs of
our test, and we refer them as answer labels. There
might be a case where an answer label and label se-
lected by our model are semantically equivalent but
lexically different; for example, car and automobile.
To mitigate the problem, we used Wordnet to han-
dle two different words with the same semantic. We
explain the use of Wordnet further in section 4.1.3.
4.1.3 Evaluation Measure & Method
We used precision at top N labels to evaluate the
model. Precision at top N is defined as P@N =
M@N
N , where is M@N is the number of relevant la-
bels among the top N generated labels to the answer
set. As explained in section 4.1.2, the labels gen-
erated by our model might not be equal to answer
labels even when they have the same semantic mean-
ing. It might be very time consuming for a human
to manually compare the two label set where one set
can vary due to the varying smoothing parameter if
semantic meaning also has to be considered.
We used WordNet?s synonyms and hypernyms
relationships in order to mitigate the problem ad-
dressed above. We regard a test label to be equal
to an answer label when WordNet?s synonyms or
hypernyms relationship allows them. Only the first
listed sense in Wordnet is used to prevent over-
generation.
We evaluated the overall effectiveness of LLM
with P@N and the effect of smoothing parameter
on P@N .
4.2 Experimental Result
We used P@5, P@10 and P@20 to evaluate the ef-
fectiveness of our model because most users disre-
gard snippets beyond 20.
First, for each query, we obtained each label?s
MAP4 for two smoothing methods. Figures 1 and
2 depicts MAP of Jelinke-Mercer smoothing and
Dirichlet Priors smoothing.
0 0.2 0.4 0.6 0.8 10.4
0.45
0.5
0.55
0.6
0.65
0.7
0.75
0.8
Lambda
Prec
ision
 
 
P@5P@10P@20
Figure 1: Jelinek-Mercer Smoothing
0 1000 2000 3000 4000 50000.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
Mu
Prec
ision
 
 
P@5P@10P@20
Figure 2: Dirichlet priors Smoothing
In figures 1 and 2, X-axis denotes smoothing pa-
rameter, and Y -axis denotes MAP. The figures show
that the smaller the value of the smoothing is , the
4Mean Average Precision
640
higher the precision is. This indicates that a better
label is selected when the probability that a specific
category would generate the label is high. In our
test result, when using Dirichlet smoothing, the pre-
cision of top 5 and 10 labels are 82% and 80%, thus
users can benefit in browsing from our model using
5 or 10 labels. However, the precision rapidly drops
to 60% at P@20. The low precision at P@20 shows
the vulnerability of our model, indicating that our
model needs a refinement.
Figure 3 shows individual precisions of labels for
randomly selected five queries. The labels were gen-
erated by using Dirichlet priors smoothing.
Baidu tiger apple jaguar travel
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
Queries
Prec
ision
 
 P@20P@5P@10
Figure 3: Using Dirichlet priors Smoothing
As shown in figure 1 and 2, the general order of
result precisions is as follows: P@20 ? P@10 ?
P@5. However, figure 3 shows that the precision
for query ?travel? is the lowest at P@5. This re-
sult indicates that words that appear many times in a
specific category of Dmoz might have higher proba-
bility regardless of snippet?s contents.
Average Baidu apple jaguar tiger travel0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
Queries
Cov
erag
e
 
 
Top 5Top 10Top 20
Figure 4: Coverage
Figure 4 shows the average coverage of labels
generated by our model. The coverage of the labels
is about 0.32%, 0.51% and 0.66% at top 5, 10 and 20
labels respectively. This means that the labels allow
browsing over only 60% of the entire search results.
The lack of coverage is another pitfall of our model,
and further refining is needed.
Finally, in Table 2, we list top 10 labels for five
queries.
5 CONCLUSION & FUTURE WORKS
We proposed a LLM for label selection of search re-
sults, and analyzed the smoothing parameter?s effect
on the label selection. Experimental results showed
that LLM can pick up meaningful labels, and aid
users in browsing web search results. Experimental
results also validated our assumption that the high
probability that Dmoz categories generate a label in-
dicates meaningful labels. Further research direc-
tions remain as future works.
Our model is sensitive to Dmoz because we use
the language model based on Dmoz. Our model may
result in poor performance for labels that are not rep-
resented or over-represented in Dmoz. Therefore, it
is meaningful to study how sensitive to Dmoz the
performance of the LLM is, and how to mitigate sen-
sitivity. We used Google?s search results as an input
to our system. However, multiple engines offer a
better coverage of the web because of the low over-
lap of current search engines (Bharat and Broder,
1998). Further work can utilize multiple engines to
generate input to our system. In our test, snippet?s
title and content were assigned the same weight, and
titles and descriptions of Dmoz?s category were also
assigned the same weight. Future work might bene-
fit from varying the weights to them. We did not uti-
lize the information buried in the documents, such
as tf ? idf , but used only knowledge provided by the
external system, Dmoz. We believe that this also af-
fected LLM?s poor performance on over-represented
terms. Future work will benefit from incorporating
the information derivable from the documents.
6 ACKNOWLEDGMENTS
This work was supported by the Korea Science
and Engineering Foundation (KOSEF) through the
Advanced Information Technology Research Center
641
Queries Labels
Baidu language search set, Chinese
search engine, search engine com-
pany, Baidu.com, MP3 Search,
Baidu engine, Japanese Search
Engine, IPO, search market,
Mobile
apple Mac OS X, iPod, Apple Mac-
intosh, Apple products, language
character set, Music Store, Apple
develops, Apple Support, informa-
tion, San Francisco
jaguar Mac OS X, Jaguar Cars, Land
Rover, Jaguar XJ, Jaguar XK,
largest cat, Leopard, Photos
tagged jaguar, Jaguar dealer,
Jaguar Clubs
tiger Mac OS X, Tiger Woods, Tiger
Cats, Detroit Tigers, Security
tool, Parts PC Components, Paper
Tiger, Adventure Tour, National
Zoo, Tiger Beat
travel Car Rental, airline tickets, dis-
count hotels, Plan trip, Airfares,
package holidays, Visa, Travel
Cheap, Destination guides, Travel
news
Table 2: Queries used in experiment
(AITrc), also in part by the BK 21 Project and MIC
& IITA through IT Leading R&D Support Project in
2007.
References
P. Rerragina and A. Gulli. 2005. A personalized search
engine based on web-snippet hierarchical clustering.
In Special Interest Tracks and Poster Proceedings
of WWW-05, International Conference on the World
Wide Web, 801-810
H. Zeng, Q. He, Z. Chen, W. Ma and J. Ma 2004. Learn-
ing to cluster web search results. In Proceedings of the
27th ACM SIGIR Conference on Research and Devel-
opment of Information Retrieval
M. A. Hearst and J. O. Pedersen. 1996. Reexamining the
cluster hypothesis: Scatter/Gather on retrieval results.
In Proceedings of 19th ACM SIGIR Conference on Re-
search and Development in Information Retrieval, 76-
84
K. Kummamuru, R. Lotlikar, S. Roy, K. Signal and R.
Krishnapuram 2004. A hierarchical monothetic docu-
ment clustering algorithm for summarization browsing
search results. In Proceedings of 13th International
Conference on World Wide Web, 658-665
A. Leuski and J. Allan. 2000. Improving Interactive Re-
trieval by Combining Ranked List and Clustering. In
Proceedings of RIAOI, College de France, 665-681
A. V. Leouski and W. B. Croft. 1996. An Evaluation
of Techniques for Clustering Search Results. In Tech-
nical Report IR-76, Department of Computer Science,
University of Massachusetts, Amherst
O. Zamir and O. Etzioni. 1998. Web Document Clus-
tering: A Feasibility Demonstration. In Proceedings
of the 21th ACM SIGIR Conference on Research and
Development of Information Retrieval, 46-54
Y. Maarek, R. Fagin, I. Ben-Shaul and D. Pelleg. 2000.
Ephemeral document clustering for Web applications.
Technical Report RJ 10186, IBM, San Jose, US
S. Osinski and D. Weiss. 2004. Conceptual clustering
using Lingo algorithm: Evaluation on Open Directory
Project data In Proceedings of IIPWM-04, 5th Confer-
ence on Intelligent Information Processing and Web
Mining, 369-377
P. Ferragina and A. Gulli. 2005. A personalized search
engine based on Web-snippet hierarchical clustering.
In Special Interest Tracks and Poster Proceedings of
WWW-05, International conference on the World Wide
Web, 801-810
S. Brin and L. Page 1998. The anatomy of a large-scale
hypertextual(Web) Search Engine. In Proceedings of
the 7th International Conference on World Wide Web,
107-117
C. Zhai and J. Lafferty 2001. A study of smoothing
methods for language models applied to ad hoc infor-
mation retrieval. In Proceedings of the 24th ACM SI-
GIR Conference on Research and Development of In-
formation Retrieval, 334-342
K. Bharat and A. Broder. 1998. A technique for measur-
ing the relative size and overlap of public web search
engines. In Proceedings of the 7th International Con-
ference on World Wide Web
642
Automatic Extraction of English-Chinese Transliteration Pairs  
using Dynamic Window and Tokenizer 
Chengguo Jin 
Dept. of Graduate School for Information 
Technology, POSTECH, Korea 
chengguo@postech.ac.kr 
Dong-Il Kim 
Language Engineering Institute, YUST, 
China 
dongil@ybust.edu.cn 
Seung-Hoon Na 
Dept. of Computer Science & Engineering 
POSTECH, Korea 
nsh1979@postech.ac.kr 
Jong-Hyeok Lee 
Dept. of Computer Science & Engineering 
POSTECH, Korea  
jhlee@postech.ac.kr 
 
 
Abstract 
Recently, many studies have been focused 
on extracting transliteration pairs from bi-
lingual texts. Most of these studies are 
based on the statistical transliteration mod-
el. The paper discusses the limitations of 
previous approaches and proposes novel 
approaches called dynamic window and to-
kenizer to overcome these limitations. Ex-
perimental results show that the average 
rates of word and character precision are 
99.0% and 99.78%, respectively. 
1 Introduction 
Machine transliteration is a type of translation 
based on phonetic similarity between two lan-
guages. Chinese Named entities including foreign 
person names, location names and company names, 
etc are usually transliterated from foreign words. 
The main problem of transliteration resulted from 
complex relations between Chinese phonetic sym-
bols and characters. Usually, a foreign word can be 
transliterated into various Chinese words, and 
sometimes this will lead to transliteration complex-
ity.   In addition, dozens of Chinese characters cor-
respond to each pinyin which uses the Latin 
alphabet to represent sounds in Standard Mandarin. 
In order to solve these problems, Chinese 
government published the ?Names of the world's 
peoples?[12] containing 630,000 entries in 1993, 
which took about 40 years. However, some new 
foreign names still cannot be found in the diction-
ary. Constructing an unknown word dictionary is a 
difficult and time consuming job, so in this paper 
we propose a novel approach to automatically con-
struct the resource by efficiently extracting trans-
literation pairs from bilingual texts.  
Recently, much research has been conducted on 
machine transliteration. Machine transliteration is 
classified into two types. One is automatic genera-
tion of transliterated word from the source lan-
guage [6]; the other one is extracting transliteration 
pairs from bilingual texts [2]. Generally, the gen-
eration process performs worse than the extraction 
process. Especially in Chinese, people do not al-
ways transliterate foreign words only by sound but 
also consider the meanings. For example, the word 
?blog? is not transliterated into ???? ? (Bu-
LaoGe) which is phonetically equivalent to the 
source word, but transliterated into ????(BoKe) 
which means ?a lot of guests?. In this case, it is too 
difficult to automatically generate correct translit-
eration words.  Therefore, our approach is based on 
the method of extracting transliteration pairs from 
bilingual texts. 
The type of extraction of transliteration pairs can 
also be further divided into two types. One is ex-
tracting transliteration candidates from each lan-
guage respectively, and then comparing the pho-
netic similarities between those candidates of two 
languages [2, 8]. The other one is only extracting 
transliteration candidates from the source language, 
and using the candidates to extract corresponding 
transliteration words from the target language [1]. 
In Chinese, there is no space between two words 
and no special character set to represent foreign 
words such as Japanese; hence the candidate ex-
traction is difficult and usually results in a low pre-
cision. Therefore, the method presented in [2] 
which extracted transliteration candidates from 
9
Sixth SIGHAN Workshop on Chinese Language Processing
both English and Chinese result in a poor perform-
ance. Compared to other works, Lee[1] only ex-
tracts transliteration candidates from English, and 
finds equivalent Chinese transliteration words 
without extracting candidates from Chinese texts. 
The method works well, but the performance is 
required to be improved. In this paper we present a 
novel approaches to obtain a remarkable result in 
extracting transliteration word pairs from parallel 
texts.  
The remainder of the paper is organized as fol-
lows: Section 2 gives an overview of statistical 
machine transliteration and describes proposed 
approaches. Section 3 describes the experimental 
setup and a quantitative assessment of performance 
of our approaches. Conclusions and future work 
are presented in Section 4. 
 
2 Extraction of English-Chinese translit-
eration pairs 
In this paper, we first extract English named en-
tities from English-Chinese parallel texts, and se-
lect only those which are to be transliterated into 
Chinese. Next we extract Chinese transliteration 
words from corresponding Chinese texts. [Fig. 1] 
shows the entire process of extracting translitera-
tion word pairs from English-Chinese parallel texts. 
 
 
[Fig 1]. The process of extracting transliteration pairs from 
English-Chinese parallel corpus 
 
2.1 Statistical machine transliteration model 
  Generally, the Chinese Romanization system pin-
yin which is used to represent the pronunciation of 
each Chinese character is adopted in Chinese trans-
literation related studies. For example, the Chinese 
word ????? is first transformed to pinyin ?Ke 
Lin Dun?, and we compare the phonetic similarities 
between ?Clinton? and ?KeLinDun?. In this paper, 
we assume that E is written in English, while C is 
written in Chinese, and TU represents translitera-
tion units. So P(C|E), ??P( ? |Clinton) can be 
transformed to P(KeLinDun|Clinton). In this paper 
we define English TU as unigram, bigram, and tri-
gram; Chinese TU is pinyin initial, pinyin final and 
the entire pinyin. With these definitions we can 
further write the probability, ??P( ?|Clinton), as 
follows:  
(P ??? | Clinton ) ? )|( ClintonkelindunP   
 ?  )|()|()|()|()|( onunPdtPininPllPCkeP (1) 
 
 
[Fig 2]. TU alignment between English and Chinese pinyin 
 
[Fig 2] shows the possible alignment between Eng-
lish word ?Clinton? and Chinese word ??????s 
pinyin ?KeLinDun?.  
In [1], the authors add the match type informa-
tion in Eq. (1). The match type is defined with the 
lengths of TUs of two languages. For example, in 
the case of )|( CkeP the match type is 2-1, be-
cause the size of Chinese TU ke is 2 and the size 
of English TU C is 1. Match type is useful when 
estimating transliteration model?s parameters with-
out a pronunciation dictionary. In this paper, we 
use the EM algorithm to estimate transliteration 
model?s parameters without a pronunciation dic-
tionary, so we applied match type to our model. 
Add Match type(M) to Eq.(1) to formulate as fol-
lows: 
  
)|(),|(max)|( EMPEMCPECP
M
?  
    )(),|(max MPEMCP
M
?               (2) 
( )?
=
+?
N
i
iiiM
mPuvPECP
1
)(log)|((logmax)|(log  (3) 
 
10
Sixth SIGHAN Workshop on Chinese Language Processing
where u, v are English TU and Chinese TU, re-
spectively and m is the match type of u and v. 
 
[Fig 3]. The alignment of the English word and the Chinese 
sentence containing corresponding transliteration word 
 
[Fig 3] shows how to extract the correct Chinese 
transliteration ?? ??(KeLinDun) with the given 
English word ?Clinton? from a Chinese sentence.  
 
2.2 Proposed methods 
  When the statistical machine transliteration is 
used to extract transliteration pairs from a parallel 
text, the problems arise when there is more than 
one Chinese character sequence that is phonetically 
similar to the English word. In this paper we pro-
pose novel approaches called dynamic window and 
tokenizer to solve the problems effectively.  
 
2.2.1 Dynamic window method 
The dynamic window approach does not find the 
transliteration at once, but first sets the window 
size range according to the English word candi-
dates, and slides each window within the range to 
find the correct transliterations. 
 
[Fig 4]. Alignment result between English word ?Clinton? 
and correct Chinese transliteration, add a character into correct 
Chinese transliteration, and eliminate a character from correct 
Chinese transliteration. 
 
If we know the exact Chinese transliteration?s 
size, then we can efficiently extract Chinese trans-
literations by setting the window with the length of 
the actual Chinese transliteration word. For exam-
ple, in [Fig 4] we do alignment between the Eng-
lish word ?Clinton? and correct Chinese translit-
eration ?????(KeLinDun), add a character into 
correct Chinese transliteration ? ? ? ?
??(KeLinYiDun), and eliminate a character from 
correct Chinese transliteration ? ? ?(LinDun) 
respectively. The result shows that the highest 
score is the alignment with correct Chinese trans-
literation. This is because the alignment between 
the English word and the correct Chinese translit-
eration will lead to more alignments between Eng-
lish TUs and Chinese TUs, which will result in 
highest scores among alignment with other Chi-
nese sequences. This characteristic does not only 
exist between English and Chinese, but also exists 
between other language pairs. 
However, in most circumstances, we can hardly 
determine the correct Chinese transliteration?s 
length. Therefore, we analyze the distribution be-
tween English words and Chinese transliterations 
to predict the possible range of Chinese translitera-
tion?s length according to the English word. We 
11
Sixth SIGHAN Workshop on Chinese Language Processing
present the algorithm for the dynamic window ap-
proach as follows:  
Step 1: Set the range of Chinese transliteration?s 
length according to the extracted English word 
candidate.  
Step 2: Slide each window within the range to 
calculate the probability between an English word 
and a Chinese character sequence contained in the 
current window using Eq 3. 
Step 3: Select the Chinese character sequence 
with highest score and back-track the alignment 
result to extract the correct transliteration word. 
[Fig 5] shows the entire process of using the dy-
namic window approach to extract the correct 
transliteration word.  
 
English Word Ziegler 
Chinese Sentence ?? ? ??? ?? ?1963 ?? ??? 
English Sentence Ziegler and Italian Chemist Julio re-ceived the Nobel prize of 1963 together. 
Extracted translit-
eration without 
using dynamic 
window 
?? ? (JiaJuLiAo) 
Correct translitera-
tion ??  (QiGeLe) 
Steps 
1. Set Chinese transliteration?s range according to English 
word ?Ziegler? to [2, 7] (After analyzing the distribution be-
tween an English word and a Chinese transliteration word, we 
found that if the English word length is ?, then the Chinese 
transliteration word is between ?/3 and?.) 
2. Slide each window to find sequence with highest score. 
3 Select the Chinese character sequence with highest score and 
back-track the alignment result to extract a correct translitera-
tion word. 
Win-
dow 
size 
Chinese character sequence with high-
est score of each window (underline 
the back-tracking result) 
Score 
(normal-
ize with 
window 
size) 
2 ? (QiGe) -9.327 
3 ??  (QiGeLe) -6.290 
4 ?? ? (QiGeLeYu) -8.433 
5 ?? ?  (QiGeLeYuYi) -9.719 
6 ?? ??  (JiaJuLiAoGongTong) -10.458 
7 ?? ?  (QiGeLeYuYiDaLi) -10.721 
[Fig 5]. Extract the correct transliteration using the dynamic 
window method 
 
The dynamic window approach can effectively 
solve the problem shown in [Fig 5] which is the 
most common problem that arises from using sta-
tistical machine transliteration model to extract a 
transliteration from a Chinese sentence. However, 
it can not handle the case that a correct translitera-
tion with correct window size can not be extracted.   
Moreover, when the dynamic window approach is 
used, the processing time will increase severely. 
Hence, the following approach is presented to deal 
with the problem as well as to improve the per-
formance. 
 
2.2.2 Tokenizer method 
The tokenizer method is to divide a sentence 
with characters which have never been used in 
Chinese transliterations and applies the statistical 
transliteration model to each part to extract a cor-
rect transliteration.  
There are certain characters that are frequently 
used for transliterating foreign words, such as?
(shi)? (de)? (le)? (he) ??. On the other 
hand, there are other characters, such as ? (shi), 
(de)? (le)? (he),??, that have never been 
used for Chinese transliteration, while they are 
phonetically equivalent with the above characters. 
These characters are mainly particles, copulas and 
non-Chinese characters etc., and always come with 
named entities and sometimes also cause some 
problems. For example, when the English word 
?David? is transliterated into Chinese, the last pho-
neme is omitted and transliterated into ?
??(DaWei). In this case of a Chinese character 
such as ? ?(De) which is phonetically similar 
with the omitted syllable ?d?, the statistical translit-
eration model will incorrectly extract ? ?
?(DaWeiDe) as transliteration of ?David?. In [1], 
the authors deal with the problem through a post-
process using some linguistic rules. Lee and Chang 
[1] merely eliminate the characters which have 
never been used in Chinese transliteration such as 
? ?(De) from the results. Nevertheless, the ap-
proach cannot solve the problem shows in [Fig 6], 
because the copula ? ?(Shi) combines with the 
other character ? ?(zhe) to form the character 
sequence ? ?(ZheShi) which is phonetically 
similar with the English word ?Jacey?, and is in-
correctly recognized as a transliteration of ?Jacey?. 
Thus, in this case, although the copula ? ?(Shi) is 
12
Sixth SIGHAN Workshop on Chinese Language Processing
eliminated from the result through the post-process 
method presented in [1], the remaining part is not 
the correct transliteration. Compared with the 
method in [1], our tokenizer approach eliminates 
copula ? ?(Shi) at pre-processing time and then 
the phonetic similarity between ?Jacey? and the 
remaining part ? ?(Zhe) becomes very low; hence 
our approach overcomes the problem  prior to the 
entire process. In addition, the tokenizer approach 
also reduces the processing time dramatically due 
to separating a sentence into several parts. [Fig 6] 
shows the process of extracting a correct translit-
eration using the tokenizer method.  
 
English Word Jacey 
Chinese Sentence ? ? ?
? ? ? 
English Sentence The authors of this book are Peni-nah  Thomson and Jacey  Grahame. 
Incorrectly extracted 
transliteration (ZheShi) 
Correct transliteration ? (JieXi) 
Steps 
1. Separate the Chinese sentence with characters, ??, , , 
?? (including non-Chinese characters such as punctuation, 
number, English characters etc.), which have never been used 
in Chinese transliteration as follows: 
? ? ? ???? 
2. Apply statistical transliteration model to each part and se-
lect the part with highest score, and back-track the part to ex-
tract a correct transliteration.  
No. 
Chinese character sequence of 
each part (underline the back-
tracking result) 
Score 
(normalize with 
window size) 
1 ? (BenShu) -24.79 
2  (ZuoZhe) -15.83 
3 ?(PeiNiNaTangMuShen) -16.32 
4 ? ???? (JieXi) -10.29 
[Fig 6]. Extracting the correct transliteration using the to-
kenizer method. 
 
In conclusion, the two approaches complement 
each other; hence using them together will lead to 
a better performance. 
3  Experiments 
  In this section, we focus on the setup for the ex-
periments and a performance evaluation of the 
proposed approaches to extract transliteration word 
pairs from parallel corpora. 
3.1 Experimental setup 
We use 300 parallel English-Chinese sentences 
containing various person names, location names, 
company names etc. The corpus for training con-
sists of 860 pairs of English names and their Chi-
nese transliterations. The performance of translit-
eration pair extraction was evaluated based on pre-
cision and recall rates at the word and character 
levels. Since we consider exactly one proper name 
in the source language and one transliteration in 
the target language at a time, the word recall rates 
are the same as the word precision rates.  In order 
to demonstrate the effectiveness of our approaches, 
we perform the following experiments: firstly, only 
use STM(Statistical transliteration model) which is 
the baseline of our experiment; secondly, we apply 
the dynamic window and tokenizer method with 
STM respectively; thirdly, we apply these two 
methods together; at last, we perform experiment 
presented in [1] to compare with our methods. 
3.2 Evaluation of dynamic window and to-
kenizer methods 
 
  [table 1]. The experimental results of extracting 
transliteration pairs using proposed methods 
Methods Word  precision 
Character 
precision 
Character 
recall 
STM (baseline) 75.33% 86.65% 91.11% 
STM+DW 96.00% 98.51% 99.05% 
STM+TOK 78.66% 85.24% 86.94% 
STM+DW+TOK 99.00% 99.78% 99.72% 
STM+CW 98.00% 98.81% 98.69% 
STM+CW+TOK 99.00% 99.89% 99.61% 
 
As shown in table 1, the baseline STM achieves 
a word precision rate of 75%.  The STM works 
relatively well with short sentences, but as the 
length of sentences increases the performance sig-
nificantly decreases. The dynamic window ap-
proach overcomes the problem effectively. If the 
dynamic window method is applied with STM, the 
model will be tolerant with the length of sentences. 
The dynamic window approach improves the per-
formance of STM around 21%, and reaches the 
average word precision rate of 96% (STM+DW). 
In order to estimate the highest performance that 
the dynamic window approach can achieve, we 
apply the correct window size which can be ob-
tained from the evaluation data set with STM. The 
result (STM+CW) shows around 98% word preci-
13
Sixth SIGHAN Workshop on Chinese Language Processing
sion rate and about 23% improvement over the 
baseline. Therefore, dynamic window approach is 
remarkably efficient; it shows only 2% difference 
with theoretically highest performance.  However, 
the dynamic window approach increases the proc-
essing time too much.  
When using tokenizer method (STM+TOK), 
only about 3% is approved over the baseline. Al-
though the result is not considerably improved, it is 
extremely important that the problems that the dy-
namic window method cannot solve are managed 
to be solved. Thus, when using both dynamic win-
dow and tokenizer methods with STM (STM+ 
DW+TOK), it is found that around 3% improve-
ment is achieved over using only the dynamic win-
dow (STM+DW), as well as word precision rates 
of 99%.  
 
[table 2]. Processing time evaluation of proposed methods 
Methods Processing time 
STM (baseline) 5 sec (5751 milisec) 
STM+DW 2min 34sec (154893 milisec) 
STM+TOK 4sec (4574 milisec) 
STM+DW+TOK 32sec (32751 milisec) 
  
  Table 2 shows the evaluation of processing time 
of dynamic window and tokenizer methods. Using 
the dynamic window leads to 27 times more proc-
essing time than STM, while using the tokenizer 
method with the dynamic window method reduces 
the processing time around 5 times than the origi-
nal. Hence, we have achieved a higher precision as 
well as less processing time by combining these 
two methods.  
 
3.3 Comparing experiment 
  In order to compare with previous methods, we 
perform the experiment presented in [1]. Table 3 
shows using the post-processing method presented 
in [1] achieves around 87% of word precision rates, 
and about 12% improvement over the baseline. 
However, our methods are 11% superior to the 
method in [1].  
 
[Table 3] Comparing experiment with previous work 
4 Conclusions and future work 
  In this paper, we presented two novel approaches 
called dynamic window and tokenizer based on the 
statistical machine transliteration model. Our ap-
proaches achieved high precision without any post-
processing procedures. The dynamic window ap-
proach was based on a fundamental property, 
which more TUs aligned between correct translit-
eration pairs. Also, we reasonably estimated the 
range of correct transliteration?s length to extract 
transliteration pairs in high precision. The token-
izer method eliminated characters that have never 
been used in Chinese transliteration to separate a 
sentence into several parts. This resulted in a cer-
tain degree of improvement of precision and sig-
nificantly reduction of processing time.  These two 
methods are both based on common natures of all 
languages; thus our approaches can be readily port 
to other language pairs.  
In this paper, we only considered the English 
words that are to be transliterated into Chinese. 
Our work is ongoing, and in near future, we will 
extend our works to extract transliteration pairs 
from large scale comparable corpora. In compara-
ble corpora, there are many uncertainties, for ex-
ample, the extracted English word may be not 
transliterated into Chinese or there may be no cor-
rect transliteration in Chinese texts. However, with 
large comparable corpora, a word will appear sev-
eral times, and we can use the frequency or entropy 
information to extract correct transliteration pairs 
based on the proposed   perfect algorithm. 
 
Acknowledgement 
This work was supported by the Korea Science and Engineer-
ing Foundation (KOSEF) through the Advanced Information 
Technology Research Center (AITrc), also in part by the BK 
21 Project and MIC & IITA through IT Leading R&D Support 
Project in 2007. 
 
Reference 
 [1] C.-J. Lee, J.S. Chang, J.-S.R. Jang, Extraction of translit-
eration pairs from parallel corpora using a statistical translit-
eration model, in: Information Sciences 176, 67-90 (2006) 
[2] Richard Sproat, Tao Tao, ChengXiang Zhai, Named Entity 
Transliteration with Comparable Corpora, in: Proceedings of 
the 21st International Conference on Computational Linguis-
tics. (2006) 
[3] J.S. Lee and K.S. Choi, "English to Korean statistical 
transliteration for information retrieval," International Journal 
of Computer Processing of Oriental Languages, pp.17?37, 
(1998). 
Methods Word  Precision 
Character 
Precision 
Character 
Recall 
STM (baseline) 75.33% 86.65% 91.11% 
STM+DW+TOK 99.00% 99.78% 99.72% 
STM+[1]?s 
method 87.99% 90.17% 91.11% 
14
Sixth SIGHAN Workshop on Chinese Language Processing
[4] K. Knight, J. Graehl, Machine transliteration, Computa-
tional Linguistics 24 (4), 599?612, (1998). 
[5] W.-H. Lin, H.-H. Chen, Backward transliteration by learn-
ing phonetic similarity, in: CoNLL-2002, Sixth Conference on 
Natural Language Learning, Taipei, Taiwan, (2002). 
[6] J.-H. Oh, K.-S. Choi, An English?Korean transliteration 
model using pronunciation and contextual rules, in: Proceed-
ings of the 19th International Conference on Computational 
Linguistics (COLING), Taipei, Taiwan, pp. 758?764, (2002). 
[7] C.-J. Lee, J.S. Chang, J.-S.R. Jang, A statistical approach 
to Chinese-to-English Backtransliteration, in: Proceedings of 
the 17th Pacific Asia Conference on Language, Information, 
and Computation (PACLIC), Singapore, pp. 310?318, (2003). 
[8] Jong-Hoon Oh, Sun-Mee Bae, Key-Sun Choi, An Algo-
rithm for extracting English-Korean Transliteration pairs using 
Automatic E-K Transliteration In Proceedings of Korean In-
formation Science Socieity (Spring). (In Korean), (2004). 
[9] Jong-Hoon Oh, Jin-Xia Huang, Key-Sun Choi, An Align-
ment Model for Extracting English-Korean Translations of 
Term Constituents, Journal of Korean Information Science 
Society, SA, 32(4), (2005) 
[10] Chun-Jen Lee, Jason S. Chang, Jyh-Shing Roger Jang: 
Alignment of bilingual named entities in parallel corpora us-
ing statistical models and multiple knowledge sources. ACM 
Trans. Asian Lang. Inf. Process. 5(2): 121-145 (2006) 
[11] Lee, C. J. and Chang, J. S., Acquisition of English-
Chinese Transliterated Word Pairs from Parallel-Aligned 
Texts Using a Statistical Machine Transliteration Model, In. 
Proceedings of HLT-NAACL, Edmonton, Canada, pp. 96-103, 
(2003). 
[12] Xinhua Agency, Names of the world's peoples: a com-
prehensive dictionary of names in Roman-Chinese ( ?
? ? ), (1993) 
15
Sixth SIGHAN Workshop on Chinese Language Processing
Syllable-Pattern-Based Unknown-
Morpheme Segmentation and Estimation
for Hybrid Part-of-Speech Tagging of
Korean
Gary Geunbae Lee Jeongwon Chay
Pohang University of Science and
Technology
Pohang University of Science and
Technology
Jong-Hyeok Leez
Pohang University of Science and
Technology
Most errors in Korean morphological analysis and part-of-speech (POS) tagging are caused
by unknown morphemes. This paper presents a syllable-pattern-based generalized unknown-
morpheme-estimation method with POSTAG (POStech TAGger),1 which is a statistical and
rule-based hybrid POS tagging system. This method of guessing unknown morphemes is based
on a combination of a morpheme pattern dictionary that encodes general lexical patterns of Korean
morphemes with a posteriori syllable trigram estimation. The syllable trigrams help to calculate
lexical probabilities of the unknown morphemes and are utilized to search for the best tagging
result. This method can guess the POS tags of unknown morphemes regardless of their numbers
and/or positions in an eojeol (a Korean spacing unit similar to an English word), which is not
possible with other systems for tagging Korean. In a series of experiments using three different
domain corpora, the system achieved a 97% tagging accuracy even though 10% of the morphemes
in the test corpora were unknown. It also achieved very high coverage and accuracy of estimation
for all classes of unknown morphemes.
1. Introduction
Part-of-speech (POS) tagging involves many difficult problems, such as insufficient
amounts of training data, inherent POS ambiguities, and (most seriously) many types
of unknown words. Unknown words are ubiquitous in any application and cause
major tagging failures in many cases. Since Korean is an agglutinative language, it
presents more serious problems with unknown morphemes than with unknown words
because more than one morpheme can be unknown in a single word and morpheme
segmentation is usually very difficult.
 NLP Laboratory, Electrical and Computer Engineering Division, Pohang University of Science and
Technology (POSTECH), Pohang, 790-784, Korea. E-mail: gblee@postech.ac.kr.
y NLP Laboratory, Electrical and Computer Engineering Division, Pohang University of Science and
Technology (POSTECH), Pohang, 790-784, Korea. E-mail: himen@postech.ac.kr.
z NLP Laboratory, Electrical and Computer Engineering Division, Pohang University of Science and
Technology (POSTECH), Pohang, 790-784, Korea. E-mail: jhlee@postech.ac.kr.
1 The binary code of POSTAG is open to the public for research and evaluation purposes at
http://nlp.postech.ac.kr/. Follow the link OpenResources!DownLoad.
c? 2002 Association for Computational Linguistics
Computational Linguistics Volume 28, Number 1
Previous techniques for guessing unknown words mostly utilize the guessing rules
to analyze the word features by looking at leading and trailing characters. Most of them
employ the analysis of trailing characters and other features such as capitalization and
hyphenation (Kupiec 1992; Weischedel et al 1993). Some of them use more morpho-
logically oriented word features such as suffixes, prefixes, and character lengths (Brill
1995; Voutilainen 1995). The guessing rules are usually handcrafted using knowledge
of morphology but sometimes are acquired automatically using lexicons and corpora
(Brill 1995; Mikheev 1996; Oflazer and Tu?r 1996). Previously developed methods for
guessing unknown morphemes in Korean are not much different from the methods
used for English. Basically, they rely on the rules that reflect knowledge of Korean
morphology and word formation. The usual way of handling unknown morphemes is
to guess all the possible POS tags for an unknown morpheme by checking connectable
functional morphemes in the same eojeol (Kang 1993).2 However, in this way, it is only
possible to guess probable POS tags for a single unknown morpheme when it occurs
at the beginning of an eojeol. Unlike in English, in Korean, more than one unknown
morpheme can appear in a single eojeol because an eojeol can include complex compo-
nents such as Chinese characters, Japanese words, and other foreign words. If an eojeol
contains more than one unknown morpheme or if the unknown morphemes appear
in other than first position in the eojeol, all previous methods fail to efficiently estimate
them. This is the reason why we try to avoid conventional guessing rules using word
morphology features such as those proposed in Mikheev (1996) and Oflazer and Tu?r
(1996).3
In this paper, we propose a syllable-pattern-based generalized unknown-morph-
eme estimation method using a morpheme pattern dictionary that enables us to treat
unknown morphemes in the same way as registered known morphemes, and thereby
to guess them regardless of their numbers or positions in an eojeol. The method for
estimating unknown morphemes using the morpheme pattern dictionary in Korean
needs to be tightly integrated into morphological analysis and POS disambiguation
systems.
POS disambiguation has usually been performed by statistical approaches, mainly
using the hidden Markov model (HMM) in English research communities (Cutting
et al 1992; Kupiec 1992; Weischedel et al 1993). These approaches are also domi-
nant for Korean, with slight improvements to accommodate the agglutinative nature
of Korean. For Korean, early HMM tagging was based on eojeols. The eojeol-based
tagging model calculates lexical and transition probabilities with eojeols as a unit; it
suffers from severe data sparseness problems since a single eojeol consists of many
different morphemes (Lee, Choi, and Kim 1993). Later, morpheme-based HMM tag-
ging was tried; such models assign a single tag to a morpheme regardless of the
space in a sentence. Morpheme-based tagging can reduce data sparseness problems
but incurs multiple observation sequences in Viterbi decoding since an eojeol can be
segmented in many different ways. Researchers then tried many ways of reducing
computation due to multiple observation sequences, such as shared word sequences
and virtual words (Kim, Lim, and Seo 1995) and two-ply HMM for morpheme unit
computation but restricted within an eojeol (Kim, Im, and Im 1996). However, since
statistical approaches take neighboring tags into account only within a limited win-
2 An eojeol is a Korean spacing unit (similar to an English word), which usually consists of one or more
stem morphemes and a series of functional morphemes.
3 Even though Turkish and Finnish are in the same class of agglutinative languages and German also
has very complex morphological structures, in our view word formation is more diverse and complex
in Korean than in these Western languages because of its mix of Oriental and Western culture.
54
Lee, Cha, and Lee Syllable-Pattern-Based Unknown-Morpheme Estimation
dow (usually two or three), sometimes the decision fails to cover important linguistic
contexts necessary for POS disambiguation. Also, approaches using only statistical
methods are inappropriate for idiomatic expressions, for which lexical terms need
to be directly referenced. And especially, statistical approaches alone do not suffice
for agglutinative languages, which usually have complex morphological structures.
In agglutinative languages, a word usually consists of one or more stem morphemes
plus a series of functional morphemes; therefore, each morpheme should receive a
POS tag appropriate to its functional role to cope with the complex morphological
phenomena in such languages. Recently, rule-based approaches, which learn symbolic
tagging rules automatically from a corpus, have been reconsidered, to overcome the
limitations of statistical approaches (Brill 1995). Some systems even perform POS tag-
ging as part of a syntactic analysis process (Voutilainen 1995). Following the success
of transformation-based approaches, attempts have been made to use transformation
rules in systems for tagging Korean (Im, Kim, and Im 1996). However, in general,
rule-based approaches alone are not very robust and are not portable enough to be
adjusted to new tagsets or new languages. Also, they usually perform no better than
their statistical counterparts (Brill 1995). To gain portability and robustness and also
to overcome the limited coverage of statistical approaches, we need to somehow com-
bine the two approaches to gain the advantages of each. In this paper, we propose
a hybrid method that combines statistical and rule-based approaches to POS disam-
biguation and can be tightly coupled with generalized unknown-morpheme-guessing
techniques.
2. Linguistic Characteristics of Korean
Korean is classified as an agglutinative language. In Korean, an eojeol consists of sev-
eral morphemes that have clear-cut morpheme boundaries. For example, na-neun gam-
gi-e geol-lyeoss-dda ?I caught a cold? consists of 3 eojeols and 7 morphemes:4 na(?I?)/T
+ neun(?auxiliary particle?)/jS, gam-gi(?cold?)/MC + e(?adverb and conjunctive parti-
cle?)/jO, geol-li(?catch?)/DR + eoss(?past tense?)/eGS + dda(?final ending?)/eGE. Below
are the characteristics of Korean that must be considered for morphological-level nat-
ural language processing and POS tagging.
 POS tagging of Korean is usually performed on a morpheme basis rather
than on an eojeol basis. Accordingly, morphological analysis is essential
to POS tagging because morpheme segmentation is much more
important and difficult than POS assignment. Moreover, morphological
analysis should segment eojeols that contain unknown morphemes as
well as known morphemes. Hence, unknown-morpheme handling
should be integrated into the morphological analysis process. Because a
single eojeol can have many possible analyses (e.g., na-neun: na(?I?)/T +
neun(?topic marker?)/jS, na(?sprout?)/DR + neun(?adnominal?)/eCNMG,
nal(?fly?)/DI + neun(?adnominal?)/eCNMG, morpheme segmentation is
inherently ambiguous.
 Korean is a postpositional language with many kinds of noun endings
(particles), verb endings, and prefinal verb endings. It is these functional
morphemes, rather than the order of eojeols, that determine grammatical
4 Here, ?+? represents a morpheme boundary in an eojeol and ?/? introduces the POS tag symbols (see
Table 2).
55
Computational Linguistics Volume 28, Number 1
Table 1
Sample distribution of unknown morphemes in
Korean.
Tag # morphemes Tag # morphemes
MC 2,888 (29.7%) S 1,358 (14.0%)
MPN 650 (6.7%) B 603 (6.2%)
MPP 235 (2.4%) T 50 (0.5%)
MPC 56 (0.6%) Symbol 10 (0.1%)
MPO 728 (7.5%) Foreign word 3,140 (32.3%)
relations such as a noun?s syntactic function, a verb?s tense, aspect,
modals, and even modifying relations between eojeols. For example,
ga/jC is a case particle, so the eojeol uri(we)-ga has a subject role due to
the particle ga/jC. Korean has a clear syllable structure within the
morpheme; most nominal content morphemes keep their surface form
when they are combined with functional morphemes.
 Korean is basically an SOV language but has relatively free word order
compared with English. The weight , in Equation (1) (Section 4.1)
reflects the fact that transition probability is less important in Korean
than in English. However, Korean does have some word order
constraints: verbs must appear in sentence-final position, and modifiers
must be placed before the element they modify. So some order
constraints must be selectively utilized as contextual information in the
POS tagging process, which is taken well into account in the design of
error correction rules (Section 4.3).
 Complex spelling changes (irregular conjugations) frequently occur
between morphemes when two morphemes combine to form an eojeol.
These spelling changes make it difficult to segment the original
morphemes before the POS tag symbols are assigned.
 The unknown-morpheme problem in Korean differs in some ways from
the unknown-word problem in English. In English, it is easy to identify
unknown words because they occur between spaces. However, in
Korean, since unknown morphemes are hidden in an eojeol, we only
know that morphological analysis failed in that eojeol; pinpointing the
exact unknown morphemes is usually difficult. This is why, unlike in
English, it is not possible to fully guess an unknown morpheme using
only affixes. The distribution of POS tags for unknown morphemes
extracted from a 130,000-morpheme training corpus (9,718 unknown
morphemes) is shown in Table 1. The distribution from even a small
corpus shows that we need to estimate various parts of speech for
unknown morphemes rather than simply guess them as nouns.
Table 2 shows the tagset that was used in the experiments reported in Section 5.
The tagset was selected from hierarchically organized POS tags for Korean. We defined
about 100 different POS tags, which can be used in morphological analysis as well as
in POS tagging. We also designed over 300 morphotactic adjacency symbols to be
used in morpheme connectivity checks for correct morpheme segmentation (to be
explained in the next section). The POS tags are hierarchically organized symbols
56
Lee, Cha, and Lee Syllable-Pattern-Based Unknown-Morpheme Estimation
Table 2
A tagset with 41 tags.
Major category Tag Description
Nominal MC common noun
MPN person name
MPC country name
MPP place name
MPO other proper noun
MD bound noun
T pronoun
S numeral
Predicate DR regular verb
DI irregular verb
HR regular adjective
HI irregular adjective
I i-predicative particle
E existential predicate
b auxiliary verb
Modifier G adnoun
B adverb
Particle y predicative particle
jC case particle
jS auxiliary particle
jO adverb and conjunctive particle
Ending eGE final ending
eGS prefinal ending
eCNDI aux conj ending
eCNDC quote conj ending
eCNMM nominal ending
eCNMG adnominal ending
eCNB adverbial ending
eCC conjunctive ending
Affix + prefix
? suffix
Special symbol su unit symbol
s? left parenthesis
s? right parenthesis
s. sentence closer
s- sentence connection
s, sentence comma
sf foreign word
sh Chinese character
so other symbol
Interjection K interjection
57
Computational Linguistics Volume 28, Number 1
that were iteratively refined from the eight major grammatical categories of Korean:
nominal, predicate, modifier, particle, ending, affix, special symbol, and interjection.
For a given morpheme, the acronym of a path name in the symbol hierarchy up to a
certain level is assigned as a POS tag.5 The rest of the detailed hierarchies, which are
related only to morpheme connectivity, are independently assigned as morphotactic
adjacency symbols. Therefore, we can use either full or partial path names as POS tags
in order to adjust the total number of tags. The size of the tagset can thus be adapted
by refining grammatical categories that are more pertinent to a given application. For
example, for text-indexing applications, we refine nominals more than predicates since
index terms are usually nominals in these applications.
3. Unknown-Morpheme Segmentation during Morphological Analysis
The agglutinative nature of Korean inevitably requires doing morphological analysis
before POS tagging. Morphological analysis, which segments input texts into morpho-
tactically connectable morphemes and assigns all possible POS tags to each morpheme
by looking them up in a morpheme dictionary, is a basic step in natural language pro-
cessing.
Our morphological analysis follows three general steps (Sproat 1992): morpheme
segmentation, recovering original morphemes from spelling changes, and morpho-
tactic modeling. Input texts are scanned from left to right, character by character,6
to be matched with morphemes in a morpheme dictionary. The morpheme dictio-
nary has a trie structured index for fast matching. It also has an independent entry
for each variant surface form (called allomorph) of the original morpheme so the
original morphemes can easily be reconstructed from spelling changes (see Table 3).
For morphotactic modeling, we used the POS tags and the morphotactic adjacency
symbols in the dictionary. The POS tags provide information about morpheme class,
while the morphotactic adjacency symbols provide information about grammatical
connectivity between morphemes needed to form an eojeol. The full hierarchy of POS
tags and morphotactic adjacency symbols is encoded in the morpheme dictionary
for each morpheme. Besides the morpheme dictionary, to model morphemes? con-
nectability to one another the system uses an independent morpheme connectivity ta-
ble that encodes all the connectable pairs of morpheme groups using the morphemes?
tags and morphotactic adjacency symbol patterns. After an input eojeol is segmented
by trie indexed dictionary searches, the morphological analysis checks whether each
segmentation is grammatically connectable by looking in the morpheme connectivity
table.
For unknown-morpheme segmentation, we developed a generalized method for
estimating unknown morphemes regardless of their position and number. Using a
morpheme pattern dictionary, our system can look up unknown morphemes exactly
the same way it looks up known registered morphemes. The morpheme pattern dic-
tionary covers all the necessary syllable patterns for unknown morphemes, including
common nouns, proper nouns, adverbs, regular and irregular verbs, regular and ir-
regular adjectives, and special symbols for foreign words. The lexical patterns for
morphemes are collected from previous studies (Kang 1993) where the constraints on
Korean syllable patterns regarding morpheme connectivity are well described. Table 4
shows some sample entries in the morpheme pattern dictionary, where Z, V, ?*? are
5 For example, nominal(M):proper-noun(P):person-name(N) is a three-level path name.
6 The character sequence in na-neun is n, a, n, eu, n.
58
Lee, Cha, and Lee Syllable-Pattern-Based Unknown-Morpheme Estimation
Table 3
Examples of morpheme dictionary entries. MCC is a full POS tag that
identifies a common noun consisting of Chinese characters. MCK identifies a
common noun consisting only of Korean characters. DIgeo-la represents a
geo-la irregular verb, and HIl represents an l irregular adjective. Yu represents
that ga-gong has a final consonant (ng). D-ha, H-ha, and D-doe are morphotactic
adjacency symbols for predicate particles. Nominals that have a D-ha as a
morphotactic adjacency symbol can be connected with predicate particles, and
they play the role of a verb or adjective. In verb or adjective, gyu represents a
regular form of an irregular conjugation, bul represents an irregular form of
an irregular conjugation. Eo is a morphotactic adjacency symbol for vowel
harmony when connecting with endings. Chug-yag represents that a particular
verb (or adjective) contains the special contracted ending. ?>? is a special
symbol for adjacent direction (?>?= right connection; ?<?= left connection).
POS-tag<original form> (Allomorph) [Morphotactic adjacency symbols]
MCC<ga-gong> (ga-gong) [yu>D-ha>H-ha>D-doe>]
MCK<geo-leum> (geo-leum) [yu>D-ha>]
DIgeo-la<geon-neo-ga> (geon-neo-ga) [gyu>chug-yag>]
DId<al-a-deud> (al-a-deud) [gyu>]
DId<al-a-deud> (al-a-deul) [bul>eo>]
DIs<heu-li-jeos> (heu-li-jeo) [bul>eo>]
DIs<heu-li-jeos> (heu-li-jeos) [gyu>]
HIl<ga-neul> (ga-neu) [bul>]
HIl<ga-neul> (ga-neul) [gyu>eo>]
Table 4
Sample entries in the morpheme pattern dictionary. Symbol meanings are
explained in Table 3.
POS-tag<original form> (Allomorph) [Morphotactic adjacency symbols]
HIl<ZV*gal> (ZV*gal) [gyu>eo>]
HIl<ZV*gal> (ZV*ga) [bul>]
HIb<ZV*ZVb> (ZV*u) [bul>]
HIb<ZV*ZVb> (ZV*weo) [chug-yag>]
HIb<ZV*ZVb> (ZV*wa) [chug-yag>]
DIs<ZV*jeos> (ZV*jeos) [gyu>]
DIs<ZV*jeos> (ZV*jeo) [bul>eo>]
DId<ZV*deud> (ZV*deud) [gyu>]
DId<ZV*deud> (ZV*deul) [bul>eo>]
metacharacters that indicate a consonant, a vowel, and any number of Korean charac-
ters, respectively. For example, go-ma-weo ?thanks?, which is a morpheme and an eojeol
at the same time, is matched to (ZV*weo) (shown in Table 4, where it is b, irregular ad-
jective (HIb)) in the morpheme pattern dictionary, which allows the system to recover
its original morpheme form go-mab.
Once the unknown morphemes are identified and recovered using the pattern
dictionary, when checking the unknown morphemes to see if they are connectable,
the system can use the same information about adjacent morphemes in the unknown
morphemes? eojeol that it would use if they were known morphemes. This is the reason
why our method can be called ?generalized? and can identify unknown morphemes
regardless of their position and number in an eojeol. The actual POS estimation is inte-
grated into the POS tagging process that will be described in Section 4.2. The essential
59
Computational Linguistics Volume 28, Number 1
morphological
analyzer
statistical
POS tagger
post error-
corrector
morpheme
dictionary
morpheme
pattern
dictionary
morpheme
connectivity
table
Input
sentence
lexical /
transition
probabilities
syllable
trigram
morpheme
graph
morpheme
graph
tagged
sentence
error-
correcting
rules
Figure 1
Statistical and rule-based hybrid architecture for POS tagging of Korean.
idea of the morpheme pattern dictionary is to pre-collect all the possible general lexical
patterns of Korean morphemes and encode each lexical syllable pattern with all the
candidate POS tags. Therefore, the system can assign initial POS tags to each unknown
morpheme simply by matching the syllable patterns in the pattern dictionary. In this
way, unlike previous approaches, ours does not need to incorporate a special rule-
based unknown-morpheme-handling module into its morphological analyzer, and all
the possible POS tags can be assigned to unknown morphemes just as they are to
known morphemes.
4. A Statistical and Rule-Based Hybrid Tagging Model
Figure 1 shows a proposed hybrid architecture for POS tagging of Korean with syllable-
pattern-based generalized unknown-morpheme guessing. It has three major compo-
nents: the morphological analyzer with unknown-morpheme handler, the statistical
POS tagger, and the rule-based error corrector. The morphological analyzer segments
the morphemes from input eojeols and reconstructs the original morphemes from
spelling changes by recovering the irregular conjugations. It also assigns all possi-
ble POS tags to each morpheme by consulting a morpheme dictionary. The unknown-
morpheme handler, which is tightly integrated into the morphological analyzer, assigns
initial POS tags to morphemes that are not registered in the dictionary, as explained
in the previous section. The statistical POS tagger runs the Viterbi algorithm (Forney
1973) on the morpheme graph to search for the optimal tag sequence for POS dis-
ambiguation. To remedy the defects of a statistical POS tagger, we developed an a
posteriori error correction mechanism. The error corrector is a rule-based transformer
60
Lee, Cha, and Lee Syllable-Pattern-Based Unknown-Morpheme Estimation
(Brill 1995), and it corrects mistagged morphemes by consulting lexical patterns and
necessary contextual information.
4.1 The Statistical POS Tagger
The statistical POS tagging model takes the morpheme graph (output of the morpho-
logical analyzer) and selects the best morpheme and POS tag sequence7 for sentences
represented in the morpheme graph. The morpheme graph is a compact way of repre-
senting multiple morpheme sequences for a sentence. Each morpheme?s tag is a node
in the graph and its morpheme connectivity is a link. Our statistical tagging model is
modified from the standard bigrams (Cutting et al 1992) using Viterbi search plus on-
the-fly extra computing of lexical probabilities for unknown morphemes. The equation
used for the statistical tagging model is a modified bigram model with left-to-right
search,
T = argmax T
n
Y
i=1
Pr(ti j ti?1)

Pr(ti j mi)
Pr(ti)

(1)
where T is an optimal tag sequence that maximizes the forward Viterbi scores.
Pr(ti j ti?1) is a bigram tag transition probability and Pr(tijmi)Pr(ti) is a modified morpheme
lexical probability.  and  are weights and are set at 0.4 and 0.6, respectively, which
means that lexical probability is more important than transition probability given the
relatively free word order of Korean. This equation was finally selected after extensive
experiments using the following six equations:
T = argmax T
n
Y
i=1
Pr(ti j ti?1)Pr(mi j ti) (2)
T = argmax T
n
Y
i=1
Pr(ti j ti?1)Pr(mi j ti) (3)
T = argmax T
n
Y
i=1
Pr(ti j ti?1)Pr(ti j mi) (4)
T = argmax T
n
Y
i=1
Pr(ti j ti?1)Pr(ti j mi) (5)
T = argmax T
n
Y
i=1
Pr(ti j ti?1)
Pr(ti j mi)
Pr(ti)
(6)
T = argmax T
n
Y
i=1
Pr(ti j ti?1)

Pr(ti j mi)
Pr(ti)

(7)
In the experiments, we used the 10,204-morpheme training corpus from the Kemong
Encyclopedia.8 Table 5 shows the tagging performance of each equation.
Training of the statistical tagging model requires a parameter estimation process
for two different parameters, that is, morpheme lexical probabilities and bigram tag
transition probabilities. Several studies show that using as much tagged material as
7 Because a Korean eojeol can be segmented in many different ways, selecting the best morpheme
segmentation sequence is as important as selecting the best POS sequence in POS tagging.
8 Provided by the Electronics and Telecommunications Research Institute (ETRI).
61
Computational Linguistics Volume 28, Number 1
Table 5
Tagging performance (all in %) of each equation. The ?eojeol? row shows
eojeol-unit tagging performance, and the ?morpheme? row shows
morpheme-unit performance.
Eq. (2) Eq. (3) Eq. (4) Eq. (5) Eq. (6) Eq. (7) (Eq. (1))
Eojeol 86.80 90.48 89.40 89.62 91.73 92.48
Morpheme 91.32 94.93 94.40 94.48 95.77 96.12
possible for training gives much better performance than unsupervised training using
the Baum-Welch reestimation algorithm (Merialdo 1994). We therefore decided to use
supervised training using tagged corpora with relative frequency counts. The three
necessary probabilities can be estimated as in Equations (8)?(10),
Pr(ti j mi)  f (ti j mi) =
N(mi, ti)
N(mi)
(8)
Pr(ti)  f (ti) =
N(ti)
PNts
n=1 N(tn)
(9)
Pr(ti j ti?1)  f (ti j ti?1) =
N(ti?1, ti)
N(ti?1)
(10)
where N(mi, ti) indicates the total number of occurrences of the morpheme mi together
with the specific tag ti, while N(mi) indicates the total number of occurrences of the
morpheme mi in the tagged training corpus. Nts indicates the total number of POS tags
in the tagset. N(ti?1, ti) and N(ti?1) can be interpreted similarly for two consecutive
tags ti?1 and ti.
A beam search strategy is utilized for high-speed tagging. For each morpheme in
the sentence, the highest probability, Ph, of the tag is recorded. All other tags associated
with the same morpheme must have probabilities greater than Ph? for some constant
beam size ?; otherwise, they are discarded. The beam may introduce search errors,
but, in practice, search efficiency can be greatly improved with virtually no loss of
accuracy.
4.2 Lexical Probability Estimation for Unknown-Morpheme Guessing
The lexical probabilities for unknown morphemes cannot be precalculated using Equa-
tion (8) since we assume the unknown morphemes do not appear in the training cor-
pus, so a special on-the-fly estimation method must be applied. We suggest using
syllable trigrams since Korean syllables can play an important role in restricting units
for guessing the POS of a morpheme. The lexical probability Pr(tijmi)Pr(ti) for unknown mor-
phemes can be estimated using the frequency of syllable trigram products according
to the formula in (11)?(13) (Nagata 1994),
m = e1e2 : : : en (11)
Pr(t j m)
Pr(t)
 Pr t(e1 j #, #)Pr t(e2 j #, e1)

n
Y
i=3
Pr t(ei j ei?2, ei?1)
62
Lee, Cha, and Lee Syllable-Pattern-Based Unknown-Morpheme Estimation
 Pr(# j en?1, en) (12)
Pr t(ei j ei?2, ei?1)  ft(ei j ei?2, ei?1)
+ ft(ei j ei?1)
+ ft(ei) (13)
where m is a morpheme, e is a syllable, t is a POS tag, ?#? is a morpheme boundary
symbol, and ft(ei j ei?2, ei?1) is a frequency datum for tag t with co-occurrence syllables
ei?2, ei?1, and ei. Trigram probabilities are smoothed by Equation (13) to cope with the
data sparseness problem. For example, Park-jong-man is the name of a person, so it is
an unknown morpheme. The lexical probability that Park-jong-man should be assigned
the tag MPN (person name) is estimated using the following formula:
Pr(MPN j Park ? jong ?man)
Pr(MPN )
 PrMPN (Park j #, #)
 PrMPN (jong j #, Park)
 PrMPN (man j Park , jong)
 PrMPN (# j jong , man) (14)
In Park-jong-man, Park is usually a family name. If the first position of this mor-
pheme is a family name, the probability that MPN is the correct tag becomes higher
than the probability that the other tags are correct. Table 6 shows the distribution of
Pr(Park j #, #) for each possible tag. In Equation (14), PrMPN (Park j #, #) represents
the popularity of the tag MPN for the morpheme Park-jong-man.
All the trigrams for Korean syllables were precalculated and stored in the database
and are applied with the candidate tags during the unknown-morpheme POS guessing
and smoothing portion of the statistical tagging process.
4.3 A Posteriori Error Correction Rules
Statistical morpheme tagging is widely known to cover only a limited range of con-
textual information. Moreover, it cannot refer to lexical patterns as a context for POS
disambiguation. As mentioned earlier, because Korean eojeols have very complex mor-
phological structures, it is necessary to look at the functional morphemes selectively to
determine the grammatical relations between eojeols. For these reasons, we designed
error correction rules for eojeols to compensate for the estimation and modeling errors
Table 6
The distribution of Pr(Park j #, #) for each tag.
MC MPN MPC MPP MPO T
No. of ?##Park? 125 2081 0 0 8 0
No. of ?##? 115,841 25,915 589 1,209 50,671 4,255
Pr(Park j #, #) 0.001 0.080 0.000 0.000 0.000 0.000
B DR DI HR HI
No. of ?##Park? 5 17 2 0 9
No. of ?##? 9,169 21,119 13,555 2,243 5,217
Pr(Park j #, #) 0.000 0.000 0.000 0.000 0.001
63
Computational Linguistics Volume 28, Number 1
Table 7
Examples of rule schemata used to extract the error correction rules automatically from the
tagged corpus. The POSTAG system has about 24 rule schemata of this form.
Rule schema Acronym description
N1FT the tag of the first morpheme (FT) of the next eojeol (N1)
P1LT the tag of the last morpheme (LT) of the previous eojeol (P1)
N2FT the tag of the first morpheme (FT) of the eojeol after the next one (N2)
N3FT the tag of the first morpheme (FT) of the second eojeol after the next one (N3)
P1LM the lexical form of the last morpheme (LM) of the previous eojeol (P1)
P1FM the lexical form of the first morpheme (FM) of the previous eojeol (P1)
N1FM the lexical form of the first morpheme (FM) of the next eojeol (N1)
[current eojeol or morpheme] [rule schemata, referenced morpheme or tag]
! [corrected eojeol or morpheme]
Figure 2
Error correction rule format.
of the statistical morpheme tagger. However, designing the error correction rules with
knowledge engineering is tedious and error prone. Instead, we adopted Brill?s ap-
proach (Brill 1995) whereby the error correction rules are learned automatically from a
small amount of tagged corpus. Fortunately, Brill showed that one does not normally
need a large amount of tagged corpus to extract the symbolic tagging rules compared
with statistical tagging. Table 7 shows examples of carefully designed rule schemata
used to extract the error correction rules for Korean, where a rule schema designates
the context of rule applications (i.e., the morpheme position and the lexical/tag deci-
sion in a context eojeol).
The form of the rules that can be automatically learned using the schemata in
Table 7 is shown in Figure 2, where [current eojeol or morpheme] consists of the mor-
pheme (with current tag) sequence in an eojeol, and [corrected eojeol or morpheme]
consists of the morpheme (with corrected tag) sequence in the same eojeol. For exam-
ple, the rule [meog(?Chinese ink0)=MC + eun=jS ][N1FT , MC ] ! [meog(?to eat0)=DR +
eun=eCNMG ] says that the current eojeol was statistically tagged as a common noun
(MC) plus auxiliary particle (jS), but if the next eojeol?s (N1) first-position morpheme
tag (FT) is also MC, the eojeol should be tagged as a regular verb (DR) plus ad-
nominal ending (eCNMG). This statistical error is caused by the ambiguity of the
morpheme meog, which has two meanings: ?Chinese ink? (noun) and ?to eat? (verb).
Since morpheme segmentation is very difficult in Korean, many tagging errors also
arise from the morpheme segmentation errors. Our error correction rules can also cope
with these morpheme segmentation errors by correcting the errors in the whole eojeol
at once. For example, the following rule can correct morpheme segmentation errors:
[jul=MC + i ? go=jO ][P1LM , ] ! [jul ? i=DR + go=eCC ]. This rule says that the eojeol
jul-i-go is usually segmented as a common noun, jul ?string, rope?, plus the adverb
and conjunctive particle i-go, but when the morpheme eul appears before the eojeol,
it should be segmented as a regular verb, jul-i ?shrink?, plus the conjunctive ending
go. This kind of segmentation error correction can greatly enhance the tagging perfor-
mance. The rules are automatically learned by comparing the correctly tagged corpus
with the output of the statistical tagger. The training is leveraged so the error correc-
64
Lee, Cha, and Lee Syllable-Pattern-Based Unknown-Morpheme Estimation
Table 8
Performance of the statistical tagger (all in %) on
three document sets, using three progressively
degraded versions of the tagger.
Document set Version 1 Version 2 Version 3
Set 1 96.4 89.5 87.1
Set 2 96.0 92.8 89.0
Set 3 96.7 88.7 84.8
Total 96.4 90.3 87.0
tion rules are gradually learned as the statistically tagged texts are corrected by the
rules learned so far.
5. Experimental Results
5.1 Embedded Performance with Hybrid POS Tagging
For morphological analysis and POS tagging experiments, we used a 130,000-morph-
eme balanced training corpus for statistical parameter estimation and a 50,000-morph-
eme corpus for learning the a posteriori error correction rules. The training corpus was
collected from various sources such as Internet documents, encyclopedias, newspapers,
and school textbooks.
For test sets, we carefully selected three different document sets, aiming for broad
coverage. The first document set (Set 1: 25,299 morphemes, 1,338 sentences), which
was collected from the Kemong Encyclopedia,9 a hotel reservation dialogue corpus,10
and assorted Internet documents, contains about 10% unknown morphemes. The sec-
ond document set (Set 2: 15,250 morphemes, 574 sentences), which consists solely of
Internet documents from assorted domains, such as broadcasting scripts and news-
papers, contains about 8.5% unknown morphemes. The third document set (Set 3:
20,919 morphemes, 555 sentences), which comes from a standard Korean document
set called KTSET 2.011 including academic articles and electronic newspapers, con-
tains about 14% unknown morphemes (mainly technical jargon). Table 8 shows our
system?s statistical tagging performance for these three document sets, using three
progressively degraded versions of the tagging mechanism. Version 1 is a full version
using the statistical method. Version 2 is a somewhat degraded version that does not
use the system?s unknown-morpheme guessing capability but treats all the segmented
unknown morphemes as nouns (the typical method of estimation). Version 3 is an even
more degraded version that rejects all unknown morphemes as tagging failures; this
version does not even perform unknown-morpheme segmentation during morpho-
logical analysis. This experiment verifies the effectiveness of our unknown-morpheme
segmentation and guessing techniques, as shown by the sharp performance drops
between Versions 1, 2, and 3. As another experiment showed, the automatically ac-
quired a posteriori error correction rules also proved to be useful. In this experiment,
two versions of the hybrid tagger were tested on the three document sets. Version 1
was the full POSTAG system with unknown-morpheme segmentation, guessing, and
9 From the Electronics and Telecommunications Research Institute (ETRI).
10 From Sogang University, Seoul, Korea.
11 From KT (Korea Telecom).
65
Computational Linguistics Volume 28, Number 1
Table 9
Performance of the hybrid tagger (all
in %) on three document sets, using
two versions of the tagger.
Document set Version 1 Version 2
Set 1 97.2 96.4
Set 2 96.9 96.0
Set 3 97.4 96.7
Total 97.2 96.4
Table 10
Unknown-morpheme estimation performance
(all in %). Experiments were performed on
three different document sets as before. #UKM
designates the number of unknown morphemes
in each document set and their percentage.
Recall (Rec.) measures the coverage of the
estimation and precision (Pre.) demonstrates its
accuracy.
Document set #UKM Rec. Pre.
Set 1 2,531 (10.0%) 93.9 94.8
Set 2 1,303 (8.5%) 92.9 88.9
Set 3 2,889 (13.8%) 98.0 85.5
Total 6,723 (10.8%) 94.9 89.7
rule-based error correction. Version 2 did not employ a posteriori error correction rules
(the same system as Version 1 in the first experiment). Performance dropped between
Version 1 and Version 2 (see Table 9); however, the drop rates were mild due to the
performance saturation at Version 1, which means that our statistical tagger alone
already achieves state-of-the-art performance for tagging of Korean morphemes.
5.2 Unknown-Morpheme Segmentation and Guessing Performance
To see the independent performance of unknown-morpheme handling more precisely
(explained in Sections 3 and 4.2), we separated the unknown-morpheme performance
from hybrid tagging experiments. Using the same test corpus, we measured the cover-
age and correctness of our unknown-morpheme estimation techniques. Table 10 shows
the results, which were evaluated by the metrics defined as follows:
Recall =
#unknown morphemes detected
#unknown morphemes
(segmentation performance)
Precision =
#unknown morphemes correctly estimated
#unknown morphemes detected
(guessing performance)
When the morphological analyzer meets an unknown morpheme, it is important
to detect first whether it is unknown or not, because sometimes, due to incorrect
segmentation, an unknown morpheme can be incorrectly processed as a known one.
Our system reached an average recall level of 94.9%. Once the unknown morphemes
are detected, the correct POS needs to be estimated. Our system tries to guess the POS
66
Lee, Cha, and Lee Syllable-Pattern-Based Unknown-Morpheme Estimation
Table 11
Unknown-morpheme estimation performance (all in %) for each POS tag. N/A
means the morpheme with the corresponding tag does not appear in the
corpus. Recall (Rec.) measures the coverage of the estimation, and precision
(Pre.) demonstrates its accuracy.
Set 1 Set 2 Set 3 Total
POS tag Rec. Pre. Rec. Pre. Rec. Pre. Rec. Pre.
MC 96.9 95.4 94.5 91.7 93.9 72.5 95.1 86.5
MPN 80.0 86.7 87.4 95.0 100.0 100.0 89.1 93.9
MPC 54.3 73.7 72.7 37.5 N/A N/A 42.3 37.1
MPP 75.2 63.3 86.9 75.0 100.0 100.0 87.4 79.4
MPO 79.4 79.4 94.8 68.3 100.0 93.8 91.4 79.7
B 87.9 100.0 42.9 66.7 100.0 100.0 76.9 88.9
T N/A N/A 100.0 100.0 N/A N/A 100.0 100.0
S 99.8 100.0 99.0 100.0 100.0 100.0 99.6 100.0
Foreign word 100.0 100.0 100.0 100.0 100.0 100.0 100.0 100.0
Special symbol 100.0 100.0 N/A N/A 100.0 100.0 100.0 100.0
of every open class item including common nouns, proper nouns, pronouns, numbers,
adverbs, and others.12 The average precision of 89.7% reflects very accurate guessing
considering the range of POSs that need to be estimated. Table 11 shows the system?s
unknown-morpheme guessing performance for each POS tag.
To show the pattern dictionary?s utility, we conducted another experiment in which
we gradually reduced the morpheme dictionary size to see the smooth hybrid tagging
performance (same as in Table 9) drops. As the morpheme dictionary gets smaller,
POSTAG becomes more dependent on the morpheme pattern dictionary and also on
the unknown-morpheme estimation process. From the full dictionary (with 65,000
nouns), we randomly deleted 5,000 nouns step by step for this series of experiments.
(We deleted only nouns because noun estimation is the best arena for showing the sys-
tem?s unknown-morpheme estimation power.) Figure 3 shows the results. Even if the
POSTAG system relies heavily on unknown-morpheme estimation instead of on more
accurate dictionary lookups, the performance drop is very slow. This result explains
why POSTAG can be used on open domain materials such as Internet documents even
when only a small morpheme dictionary is available.
6. Conclusion
This paper presents a pattern-dictionary-based unknown-morpheme estimation method
for generalized and powerful unknown-morpheme segmentation and guessing for a
hybrid POS tagging system. Generalized unknown-morpheme handling is a new and
powerful idea that adopts a morpheme pattern dictionary and syllable-based lexical
probability estimation. The morpheme pattern dictionary enables the system to seg-
ment unknown morphemes in the same way as registered morphemes without any
separate rules for Korean, and thereby to handle them regardless of their numbers
or positions in an eojeol. The paper also presents an error-corrective statistical and
12 Pronouns, numbers, and adverbs may be considered as closed classes. However, in real-world corpora,
we frequently find unexpectedly coined terms in these classes since Korean word formation is affected
by very diverse sources such as foreign words, old Chinese words, archaic pure-Korean words, and so
on.
67
Computational Linguistics Volume 28, Number 1
90
92
94
96
98
100
0 2 4 6 8 10 12
?set1?
?set2?
?set3?
?total?
Figure 3
Hybrid tagging performance change (all in %), showing the utility of the pattern dictionary.
Experiments were performed on three different document sets as before. The x-axis designates
the number of deletion steps whereby the morpheme dictionary was decreased (by 5,000s)
from its full size of 65,000 nouns (Step 0) to 5,000 nouns (Step 12).
rule-based hybrid POS tagging method that exhibits many novel features such as an
experiment-based statistical model for Korean, rule-based error correction, and hier-
archically expandable tagsets. The POSTAG system was developed to test these novel
ideas, especially for agglutinative languages such as Korean. (Japanese, being similar
to Korean in linguistic characteristics, will be a good target for testing these ideas.)
Unlike previous systems, POSTAG is a hybrid tagging system; such a system has never
been tried before, but it turns out to be most suitable for agglutinative languages such
as Korean. POSTAG mainly applies a state-of-the-art HMM tagger for morphemes
but considers multiple observations in the Viterbi score calculation. Because of the
complexity of the morpheme sequence in a Korean eojeol, a morpheme-based HMM?s
tagging accuracy is relatively low for Korean, compared with its accuracy for English.
POSTAG compensates extremely well for the limitations of HMMs by rule-based error
correction. The error correction rules are automatically learned to selectively correct
HMM tagging errors. Similar hybrid methods have been tried for English, but they
integrate HMM tagging and rule-based tagging at the same level (Tapanainen and
Voutilainen 1994). POSTAG integrates morphological analysis with the generalized
68
Lee, Cha, and Lee Syllable-Pattern-Based Unknown-Morpheme Estimation
unknown-morpheme segmentation so that unknown morphemes can be processed in
the same manner as registered morphemes during tagging. POSTAG also employs
hierarchical tagsets that are flexible enough to expand/shrink according to the given
application. The hierarchical tagset is a novel idea. Most tagging systems for Korean
have applied flat, fixed tagsets and have suffered from using varying tagsets in various
applications. However, POSTAG?s tagsets, based on the over 100 finely differentiated
POS symbols for Korean are hierarchically organized and are flexibly reorganizable ac-
cording to the application at hand. The hierarchical tagsets can be mapped to any other
existing tagset as long as they are fairly well classified and therefore can encourage cor-
pus sharing in the Korean-tagging community. POSTAG is constantly being improved
through expansion of its morpheme dictionary, pattern dictionary, and tagged cor-
pus for statistical and rule-based learning. Since the generalized unknown-morpheme
handling is integrated into the system, POSTAG proves to be a good tagger for open
domain applications such as Internet indexing, filtering, and summarization, and we
are now developing a Web indexer using POSTAG technology.
Acknowledgments
This project was partly supported by
KOSEF (teukjeongkicho #970-1020-301-3,
1997.9-2000.8) and a Ministry of Education
BK21 program awarded to the Electrical
and Computer Engineering Division of
POSTECH. We would like to thank
JunHyeok Shim for coding the
unknown-morpheme estimation
experiments. An earlier version of this
paper was presented at the 6th Workshop
on Very Large Corpora in Montreal, 15?16
August 1998.
References
Brill, E. 1995. Transformation-based
error-driven learning and natural
language processing: A case study in
part-of-speech tagging. Computational
Linguistics, 21:543?565.
Cutting, D., J. Kupiec, J. Pedersen, and P.
Sibun. 1992. A practical part-of-speech
tagger. In Proceedings of the 3rd Conference
on Applied Natural Language Processing,
pages 133?140.
Forney, G. 1973. The Viterbi algorithm.
Proceedings of the IEEE, 61:268?278.
Im, H. S., J. D. Kim, and H. C. Im. 1996.
Transformation rule-based tagging
considering Korean characteristics. In
Proceedings of the Spring Conference of the AI
SIG Meeting of the Korean Information Science
Society, pages 3?10. (Written in Korean.)
Kang, S. S. 1993. Korean Morphological
Analysis Using Syllable Information and
Multiple-Word Units. Ph.D. thesis,
Department of Computer Engineering,
Seoul National University. (Written in
Korean.)
Kim, J. D., H. S. Im, and H. C. Im. 1996.
Morpheme-based Korean part-of-speech
tagging model considering eojeol-unit
contexts. In Proceedings of the Spring
Conference of the Korean Cognitive Science
Society, pages 97?106. (Written in Korean.)
Kim, J. H., C. S. Lim, and J. Seo. 1995. An
efficient Korean part-of-speech tagging
using a hidden Markov model. Journal of
the Korean Information Science Society,
22:136?146. (Written in Korean.)
Kupiec, J. 1992. Robust part-of-speech
tagging using a hidden Markov model.
Computer Speech and Language, 6:225?242.
Lee, U. J., K. S. Choi, and G. C. Kim. 1993.
Korean text-tagging system. In Proceedings
of the Spring Conference of the Korean
Information Science Society, pages 805?808.
(Written in Korean.)
Merialdo, B. 1994. Tagging English text with
a probabilistic model. Computational
Linguistics, 20:155?171.
Mikheev, A. 1996. Unsupervised learning of
word-category guessing rules. In
Proceedings of the 34th Annual Meeting of the
Association for the Computational Linguistics,
pages 327?334.
Nagata, M. 1994. A stochastic Japanese
morphological analyzer using a
forward-DP backward-A N-best search
algorithm. In Proceedings of the 15th
International Conference on Computational
Linguistics, pages 201?207.
Oflazer, K. and G. Tu?r. 1996. Combining
hand-crafted rules and unsupervised
learning in constraint-based
morphological disambiguation. In
Proceedings of the Conference on Empirical
Methods in Natural Language Processing,
pages 69?81.
69
Computational Linguistics Volume 28, Number 1
Sproat, R. 1992. Morphology and Computation.
MIT Press, Cambridge, MA.
Tapanainen, P. and A. Voutilainen. 1994.
Tagging accurately?don?t guess if you
know. In Proceedings of the Conference on
Applied Natural Language Processing,
pages 149?156.
Voutilainen, A. 1995. A syntax-based
part-of-speech analyzer. In Proceedings of
the 7th Conference of the European Chapter of
the Association for Computational Linguistics,
pages 157?164.
Weischedel, R., M. Meteer, R. Schwartz, L.
Rawshaw, and J. Ralmucci. 1993. Coping
with ambiguity and unknown words
through probabilistic models.
Computational Linguistics, 19:359?382.
70
Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 253?261,
Suntec, Singapore, 2-7 August 2009. c?2009 ACL and AFNLP
Discovering the Discriminative Views: Measuring Term Weights for
Sentiment Analysis
Jungi Kim, Jin-Ji Li and Jong-Hyeok Lee
Division of Electrical and Computer Engineering
Pohang University of Science and Technology, Pohang, Republic of Korea
{yangpa,ljj,jhlee}@postech.ac.kr
Abstract
This paper describes an approach to uti-
lizing term weights for sentiment analysis
tasks and shows how various term weight-
ing schemes improve the performance of
sentiment analysis systems. Previously,
sentiment analysis was mostly studied un-
der data-driven and lexicon-based frame-
works. Such work generally exploits tex-
tual features for fact-based analysis tasks
or lexical indicators from a sentiment lexi-
con. We propose to model term weighting
into a sentiment analysis system utilizing
collection statistics, contextual and topic-
related characteristics as well as opinion-
related properties. Experiments carried
out on various datasets show that our
approach effectively improves previous
methods.
1 Introduction
With the explosion in the amount of commentaries
on current issues and personal views expressed in
weblogs on the Internet, the field of studying how
to analyze such remarks and sentiments has been
increasing as well. The field of opinion mining
and sentiment analysis involves extracting opin-
ionated pieces of text, determining the polarities
and strengths, and extracting holders and targets
of the opinions.
Much research has focused on creating testbeds
for sentiment analysis tasks. Most notable
and widely used are Multi-Perspective Question
Answering (MPQA) and Movie-review datasets.
MPQA is a collection of newspaper articles anno-
tated with opinions and private states at the sub-
sentence level (Wiebe et al, 2003). Movie-review
dataset consists of positive and negative reviews
from the Internet Movie Database (IMDb) archive
(Pang et al, 2002).
Evaluation workshops such as TREC and NT-
CIR have recently joined in this new trend of re-
search and organized a number of successful meet-
ings. At the TREC Blog Track meetings, re-
searchers have dealt with the problem of retriev-
ing topically-relevant blog posts and identifying
documents with opinionated contents (Ounis et
al., 2008). NTCIR Multilingual Opinion Analy-
sis Task (MOAT) shared a similar mission, where
participants are provided with a number of topics
and a set of relevant newspaper articles for each
topic, and asked to extract opinion-related proper-
ties from enclosed sentences (Seki et al, 2008).
Previous studies for sentiment analysis belong
to either the data-driven approach where an anno-
tated corpus is used to train a machine learning
(ML) classifier, or to the lexicon-based approach
where a pre-compiled list of sentiment terms is uti-
lized to build a sentiment score function.
This paper introduces an approach to the senti-
ment analysis tasks with an emphasis on how to
represent and evaluate the weights of sentiment
terms. We propose a number of characteristics of
good sentiment terms from the perspectives of in-
formativeness, prominence, topic?relevance, and
semantic aspects using collection statistics, con-
textual information, semantic associations as well
as opinion?related properties of terms. These term
weighting features constitute the sentiment analy-
sis model in our opinion retrieval system. We test
our opinion retrieval system with TREC and NT-
CIR datasets to validate the effectiveness of our
term weighting features. We also verify the ef-
fectiveness of the statistical features used in data-
driven approaches by evaluating an ML classifier
with labeled corpora.
2 Related Work
Representing text with salient features is an im-
portant part of a text processing task, and there ex-
ists many works that explore various features for
253
text analysis systems (Sebastiani, 2002; Forman,
2003). Sentiment analysis task have also been us-
ing various lexical, syntactic, and statistical fea-
tures (Pang and Lee, 2008). Pang et al (2002)
employed n-gram and POS features for ML meth-
ods to classify movie-review data. Also, syntac-
tic features such as the dependency relationship of
words and subtrees have been shown to effectively
improve the performances of sentiment analysis
(Kudo and Matsumoto, 2004; Gamon, 2004; Mat-
sumoto et al, 2005; Ng et al, 2006).
While these features are usually employed by
data-driven approaches, there are unsupervised ap-
proaches for sentiment analysis that make use of a
set of terms that are semantically oriented toward
expressing subjective statements (Yu and Hatzi-
vassiloglou, 2003). Accordingly, much research
has focused on recognizing terms? semantic ori-
entations and strength, and compiling sentiment
lexicons (Hatzivassiloglou and Mckeown, 1997;
Turney and Littman, 2003; Kamps et al, 2004;
Whitelaw et al, 2005; Esuli and Sebastiani, 2006).
Interestingly, there are conflicting conclusions
about the usefulness of the statistical features in
sentiment analysis tasks (Pang and Lee, 2008).
Pang et al (2002) presents empirical results in-
dicating that using term presence over term fre-
quency is more effective in a data-driven sentiment
classification task. Such a finding suggests that
sentiment analysis may exploit different types of
characteristics from the topical tasks, that, unlike
fact-based text analysis tasks, repetition of terms
does not imply a significance on the overall senti-
ment. On the other hand, Wiebe et al (2004) have
noted that hapax legomena (terms that only appear
once in a collection of texts) are good signs for
detecting subjectivity. Other works have also ex-
ploited rarely occurring terms for sentiment anal-
ysis tasks (Dave et al, 2003; Yang et al, 2006).
The opinion retrieval task is a relatively recent
issue that draws both the attention of IR and NLP
communities. Its task is to find relevant documents
that also contain sentiments about a given topic.
Generally, the opinion retrieval task has been ap-
proached as a two?stage task: first, retrieving top-
ically relevant documents, then reranking the doc-
uments by the opinion scores (Ounis et al, 2006).
This approach is also appropriate for evaluation
systems such as NTCIR MOAT that assumes that
the set of topically relevant documents are already
known in advance. On the other hand, there are
also some interesting works on modeling the topic
and sentiment of documents in a unified way (Mei
et al, 2007; Zhang and Ye, 2008).
3 Term Weighting and Sentiment
Analysis
In this section, we describe the characteristics of
terms that are useful in sentiment analysis, and
present our sentiment analysis model as part of
an opinion retrieval system and an ML sentiment
classifier.
3.1 Characteristics of Good Sentiment Terms
This section examines the qualities of useful terms
for sentiment analysis tasks and corresponding
features. For the sake of organization, we cate-
gorize the sources of features into either global or
local knowledge, and either topic-independent or
topic-dependent knowledge.
Topic-independently speaking, a good senti-
ment term is discriminative and prominent, such
that the appearance of the term imposes greater
influence on the judgment of the analysis system.
The rare occurrence of terms in document collec-
tions has been regarded as a very important feature
in IR methods, and effective IR models of today,
either explicitly or implicitly, accommodate this
feature as an Inverse Document Frequency (IDF)
heuristic (Fang et al, 2004). Similarly, promi-
nence of a term is recognized by the frequency of
the term in its local context, formulated as Term
Frequency (TF) in IR.
If a topic of the text is known, terms that are rel-
evant and descriptive of the subject should be re-
garded to be more useful than topically-irrelevant
and extraneous terms. One way of measuring this
is using associations between the query and terms.
Statistical measures of associations between terms
include estimations by the co-occurrence in the
whole collection, such as Point-wise Mutual In-
formation (PMI) and Latent Semantic Analysis
(LSA). Another method is to use proximal infor-
mation of the query and the word, using syntactic
structure such as dependency relations of words
that provide the graphical representation of the
text (Mullen and Collier, 2004). The minimum
spans of words in such graph may represent their
associations in the text. Also, the distance between
words in the local context or in the thesaurus-
like dictionaries such as WordNet may be approx-
imated as such measure.
254
3.2 Opinion Retrieval Model
The goal of an opinion retrieval system is to find a
set of opinionated documents that are relevant to a
given topic. We decompose the opinion retrieval
system into two tasks: the topical retrieval task
and the sentiment analysis task. This two-stage
approach for opinion retrieval has been taken by
many systems and has been shown to perform well
(Ounis et al, 2006). The topic and the sentiment
aspects of the opinion retrieval task are modeled
separately, and linearly combined together to pro-
duce a list of topically-relevant and opinionated
documents as below.
ScoreOpRet(D,Q) = ??Scorerel(D,Q)+(1??)?Scoreop(D,Q)
The topic-relevance model Scorerel may be sub-
stituted by any IR system that retrieves relevant
documents for the query Q. For tasks such as
NTCIR MOAT, relevant documents are already
known in advance and it becomes unnecessary to
estimate the relevance degree of the documents.
We focus on modeling the sentiment aspect of
the opinion retrieval task, assuming that the topic-
relevance of documents is provided in some way.
To assign documents with sentiment degrees,
we estimate the probability of a document D to
generate a query Q and to possess opinions as in-
dicated by a random variable Op.1 Assuming uni-
form prior probabilities of documentsD, queryQ,
and Op, and conditional independence between Q
and Op, the opinion score function reduces to es-
timating the generative probability of Q and Op
given D.
Scoreop(D,Q) ? p(D | Op,Q) ? p(Op,Q | D)
If we regard that the document D is represented
as a bag of words and that the words are uniformly
distributed, then
p(Op,Q | D) =
X
w?D
p(Op,Q | w) ? p(w | D)
=
X
w?D
p(Op | w) ? p(Q | w) ? p(w | D) (1)
Equation 1 consists of three factors: the proba-
bility of a word to be opinionated (P (Op|w)), the
likelihood of a query given a word (P (Q|w)), and
the probability of a document generating a word
(P (w|D)). Intuitively speaking, the probability of
a document embodying topically related opinion is
estimated by accumulating the probabilities of all
1Throughout this paper, Op indicates Op = 1.
words from the document to have sentiment mean-
ings and associations with the given query.
In the following sections, we assess the three
factors of the sentiment models from the perspec-
tives of term weighting.
3.2.1 Word Sentiment Model
Modeling the sentiment of a word has been a pop-
ular approach in sentiment analysis. There are
many publicly available lexicon resources. The
size, format, specificity, and reliability differ in all
these lexicons. For example, lexicon sizes range
from a few hundred to several hundred thousand.
Some lexicons assign real number scores to in-
dicate sentiment orientations and strengths (i.e.
probabilities of having positive and negative sen-
timents) (Esuli and Sebastiani, 2006) while other
lexicons assign discrete classes (weak/strong, pos-
itive/negative) (Wilson et al, 2005). There are
manually compiled lexicons (Stone et al, 1966)
while some are created semi-automatically by ex-
panding a set of seed terms (Esuli and Sebastiani,
2006).
The goal of this paper is not to create or choose
an appropriate sentiment lexicon, but rather it is
to discover useful term features other than the
sentiment properties. For this reason, one sen-
timent lexicon, namely SentiWordNet, is utilized
throughout the whole experiment.
SentiWordNet is an automatically generated
sentiment lexicon using a semi-supervised method
(Esuli and Sebastiani, 2006). It consists of Word-
Net synsets, where each synset is assigned three
probability scores that add up to 1: positive, nega-
tive, and objective.
These scores are assigned at sense level (synsets
in WordNet), and we use the following equations
to assess the sentiment scores at the word level.
p(Pos | w) = max
s?synset(w)
SWNPos(s)
p(Neg | w) = max
s?synset(w)
SWNNeg(s)
p(Op | w) = max (p(Pos | w), p(Neg | w))
where synset(w) is the set of synsets of w and
SWNPos(s), SWNNeg(s) are positive and neg-
ative scores of a synset in SentiWordNet. We as-
sess the subjective score of a word as the maxi-
mum value of the positive and the negative scores,
because a word has either a positive or a negative
sentiment in a given context.
The word sentiment model can also make use
of other types of sentiment lexicons. The sub-
255
jectivity lexicon used in OpinionFinder2 is com-
piled from several manually and automatically
built resources. Each word in the lexicon is tagged
with the strength (strong/weak) and polarity (Pos-
itive/Negative/Neutral). The word sentiment can
be modeled as below.
P (Pos|w) =
8
><
>:
1.0 if w is Positive and Strong
0.5 if w is Positive and Weak
0.0 otherwise
P (Op | w) = max (p(Pos | w), p(Neg | w))
3.2.2 Topic Association Model
If a topic is given in the sentiment analysis, terms
that are closely associated with the topic should
be assigned heavy weighting. For example, sen-
timent words such as scary and funny are more
likely to be associated with topic words such as
book and movie than grocery or refrigerator.
In the topic association model, p(Q | w) is es-
timated from the associations between the word w
and a set of query terms Q.
p(Q | w) =
P
q?Q Asc-Score(q, w)
| Q |
?
X
q?Q
Asc-Score(q, w)
Asc-Score(q, w) is the association score between
q and w, and | Q | is the number of query words.
To measure associations between words, we
employ statistical approaches using document col-
lections such as LSA and PMI, and local proximity
features using the distance in dependency trees or
texts.
Latent Semantic Analysis (LSA) (Landauer and
Dumais, 1997) creates a semantic space from a
collection of documents to measure the semantic
relatedness of words. Point-wise Mutual Informa-
tion (PMI) is a measure of associations used in in-
formation theory, where the association between
two words is evaluated with the joint and individ-
ual distributions of the two words. PMI-IR (Tur-
ney, 2001) uses an IR system and its search op-
erators to estimate the probabilities of two terms
and their conditional probabilities. Equations for
association scores using LSA and PMI are given
below.
Asc-ScoreLSA(w1, w2) =
1 + LSA(w1, w2)
2
Asc-ScorePMI(w1, w2) =
1 + PMI-IR(w1, w2)
2
2http://www.cs.pitt.edu/mpqa/
For the experimental purpose, we used publicly
available online demonstrations for LSA and PMI.
For LSA, we used the online demonstration mode
from the Latent Semantic Analysis page from the
University of Colorado at Boulder.3 For PMI, we
used the online API provided by the CogWorks
Lab at the Rensselaer Polytechnic Institute.4
Word associations between two terms may also
be evaluated in the local context where the terms
appear together. One way of measuring the prox-
imity of terms is using the syntactic structures.
Given the dependency tree of the text, we model
the association between two terms as below.
Asc-ScoreDTP (w1, w2) =
(
1.0 min. span in dep. tree ? Dsyn
0.5 otherwise
where, Dsyn is arbitrarily set to 3.
Another way is to use co-occurrence statistics
as below.
Asc-ScoreWP (w1, w2) =
(
1.0 if distance betweenw1andw2 ? K
0.5 otherwise
where K is the maximum window size for the
co-occurrence and is arbitrarily set to 3 in our ex-
periments.
The statistical approaches may suffer from data
sparseness problems especially for named entity
terms used in the query, and the proximal clues
cannot sufficiently cover all term?query associa-
tions. To avoid assigning zero probabilities, our
topic association models assign 0.5 to word pairs
with no association and 1.0 to words with perfect
association.
Note that proximal features using co-occurrence
and dependency relationships were used in pre-
vious work. For opinion retrieval tasks, Yang et
al. (2006) and Zhang and Ye (2008) used the co-
occurrence of a query word and a sentiment word
within a certain window size. Mullen and Collier
(2004) manually annotated named entities in their
dataset (i.e. title of the record and name of the
artist for music record reviews), and utilized pres-
ence and position features in their ML approach.
3.2.3 Word Generation Model
Our word generation model p(w | d) evaluates the
prominence and the discriminativeness of a word
3http://lsa.colorado.edu/, default parameter settings for
the semantic space (TASA, 1st year college level) and num-
ber of factors (300).
4http://cwl-projects.cogsci.rpi.edu/msr/, PMI-IR with the
Google Search Engine.
256
w in a document d. These issues correspond to the
core issues of traditional IR tasks. IR models, such
as Vector Space (VS), probabilistic models such
as BM25, and Language Modeling (LM), albeit in
different forms of approach and measure, employ
heuristics and formal modeling approaches to ef-
fectively evaluate the relevance of a term to a doc-
ument (Fang et al, 2004). Therefore, we estimate
the word generation model with popular IR mod-
els? the relevance scores of a document d given w
as a query.5
p(w | d) ? IR-SCORE(w, d)
In our experiments, we use the Vector Space
model with Pivoted Normalization (VS), Proba-
bilistic model (BM25), and Language modeling
with Dirichlet Smoothing (LM).
V SPN(w, d) =
1 + ln(1 + ln(c(w, d)))
(1? s) + s ?
| d |
avgdl
? ln
N + 1
df(w)
BM25(w, d) = ln
N ? df(w) + 0.5
df(w) + 0.5
?
(k1 + 1) ? c(w, d)
k1
?
(1? b) + b |d|avgdl
?
+ c(w, d)
LMDI(w, d) = ln
 
1 +
c(w, d)
? ? c(w,C)
!
+ ln
?
| d | +?
c(w, d) is the frequency of w in d, | d | is the
number of unique terms in d, avgdl is the average
| d | of all documents, N is the number of doc-
uments in the collection, df(w) is the number of
documents with w, C is the entire collection, and
k1 and b are constants 2.0 and 0.75.
3.3 Data-driven Approach
To verify the effectiveness of our term weight-
ing schemes in experimental settings of the data-
driven approach, we carry out a set of simple ex-
periments with ML classifiers. Specifically, we
explore the statistical term weighting features of
the word generation model with Support Vector
machine (SVM), faithfully reproducing previous
work as closely as possible (Pang et al, 2002).
Each instance of train and test data is repre-
sented as a vector of features. We test various
combinations of the term weighting schemes listed
below.
? PRESENCE: binary indicator for the pres-
ence of a term
? TF: term frequency
5With proper assumptions and derivations, p(w | d) can
be derived to language modeling approaches. Refer to (Zhai
and Lafferty, 2004).
? VS.TF: normalized tf as in VS
? BM25.TF: normalized tf as in BM25
? IDF: inverse document frequency
? VS.IDF: normalized idf as in VS
? BM25.IDF: normalized idf as in BM25
4 Experiment
Our experiments consist of an opinion retrieval
task and a sentiment classification task. We use
MPQA and movie-review corpora in our experi-
ments with an ML classifier. For the opinion re-
trieval task, we use the two datasets used by TREC
blog track and NTCIR MOAT evaluation work-
shops.
The opinion retrieval task at TREC Blog Track
consists of three subtasks: topic retrieval, opinion
retrieval, and polarity retrieval. Opinion and polar-
ity retrieval subtasks use the relevant documents
retrieved at the topic retrieval stage. On the other
hand, the NTCIR MOAT task aims to find opin-
ionated sentences given a set of documents that are
already hand-assessed to be relevant to the topic.
4.1 Opinion Retieval Task ? TREC Blog
Track
4.1.1 Experimental Setting
TREC Blog Track uses the TREC Blog06 corpus
(Macdonald and Ounis, 2006). It is a collection
of RSS feeds (38.6 GB), permalink documents
(88.8GB), and homepages (28.8GB) crawled on
the Internet over an eleven week period from De-
cember 2005 to February 2006.
Non-relevant content of blog posts such as
HTML tags, advertisement, site description, and
menu are removed with an effective internal spam
removal algorithm (Nam et al, 2009). While our
sentiment analysis model uses the entire relevant
portion of the blog posts, further stopword re-
moval and stemming is done for the blog retrieval
system.
For the relevance retrieval model, we faithfully
reproduce the passage-based language model with
pseudo-relevance feedback (Lee et al, 2008).
We use in total 100 topics from TREC 2007 and
2008 blog opinion retrieval tasks (07:901-950 and
08:1001-1050). We use the topics from Blog 07
to optimize the parameter for linearly combining
the retrieval and opinion models, and use Blog 08
topics as our test data. Topics are extracted only
from the Title field, using the Porter stemmer and
a stopword list.
257
Table 1: Performance of opinion retrieval models
using Blog 08 topics. The linear combination pa-
rameter ? is optimized on Blog 07 topics. ? indi-
cates statistical significance at the 1% level over
the baseline.
Model MAP R-prec P@10
TOPIC REL. 0.4052 0.4366 0.6440
BASELINE 0.4141 0.4534 0.6440
VS 0.4196 0.4542 0.6600
BM25 0.4235? 0.4579 0.6600
LM 0.4158 0.4520 0.6560
PMI 0.4177 0.4538 0.6620
LSA 0.4155 0.4526 0.6480
WP 0.4165 0.4533 0.6640
BM25?PMI 0.4238? 0.4575 0.6600
BM25?LSA 0.4237? 0.4578 0.6600
BM25?WP 0.4237? 0.4579 0.6600
BM25?PMI?WP 0.4242? 0.4574 0.6620
BM25?LSA?WP 0.4238? 0.4576 0.6580
4.1.2 Experimental Result
Retrieval performances using different combina-
tions of term weighting features are presented in
Table 1. Using only the word sentiment model is
set as our baseline.
First, each feature of the word generation and
topic association models are tested; all features of
the models improve over the baseline. We observe
that the features of our word generation model is
more effective than those of the topic association
model. Among the features of the word generation
model, the most improvement was achieved with
BM25, improving the MAP by 2.27%.
Features of the topic association model show
only moderate improvements over the baseline.
We observe that these features generally improve
P@10 performance, indicating that they increase
the accuracy of the sentiment analysis system.
PMI out-performed LSA for all evaluation mea-
sures. Among the topic association models, PMI
performs the best in MAP and R-prec, while WP
achieved the biggest improvement in P@10.
Since BM25 performs the best among the word
generation models, its combination with other fea-
tures was investigated. Combinations of BM25
with the topic association models all improve the
performance of the baseline and BM25. This
demonstrates that the word generation model and
the topic association model are complementary to
each other.
The best MAP was achieved with BM25, PMI,
and WP (+2.44% over the baseline). We observe
that PMI and WP also complement each other.
4.2 Sentiment Analysis Task ? NTCIR
MOAT
4.2.1 Experimental Setting
Another set of experiments for our opinion analy-
sis model was carried out on the NTCIR-7 MOAT
English corpus. The English opinion corpus
for NTCIR MOAT consists of newspaper articles
from the Mainichi Daily News, Korea Times, Xin-
hua News, Hong Kong Standard, and the Straits
Times. It is a collection of documents manu-
ally assessed for relevance to a set of queries
from NTCIR-7 Advanced Cross-lingual Informa-
tion Access (ACLIA) task. The corpus consists of
167 documents, or 4,711 sentences for 14 test top-
ics. Each sentence is manually tagged with opin-
ionatedness, polarity, and relevance to the topic by
three annotators from a pool of six annotators.
For preprocessing, no removal or stemming is
performed on the data. Each sentence was pro-
cessed with the Stanford English parser6 to pro-
duce a dependency parse tree. Only the Title fields
of the topics were used.
For performance evaluations of opinion and po-
larity detection, we use precision, recall, and F-
measure, the same measure used to report the offi-
cial results at the NTCIR MOAT workshop. There
are lenient and strict evaluations depending on the
agreement of the annotators; if two out of three an-
notators agreed upon an opinion or polarity anno-
tation then it is used during the lenient evaluation,
similarly three out of three agreements are used
during the strict evaluation. We present the perfor-
mances using the lenient evaluation only, for the
two evaluations generally do not show much dif-
ference in relative performance changes.
Since MOAT is a classification task, we use a
threshold parameter to draw a boundary between
opinionated and non-opinionated sentences. We
report the performance of our system using the
NTCIR-7 dataset, where the threshold parameter
is optimized using the NTCIR-6 dataset.
4.2.2 Experimental Result
We present the performance of our sentiment anal-
ysis system in Table 2. As in the experiments with
6http://nlp.stanford.edu/software/lex-parser.shtml
258
Table 2: Performance of the Sentiment Analy-
sis System on NTCIR7 dataset. System parame-
ters are optimized for F-measure using NTCIR6
dataset with lenient evaluations.
Opinionated
Model Precision Recall F-Measure
BASELINE 0.305 0.866 0.451
VS 0.331 0.807 0.470
BM25 0.327 0.795 0.464
LM 0.325 0.794 0.461
LSA 0.315 0.806 0.453
PMI 0.342 0.603 0.436
DTP 0.322 0.778 0.455
VS?LSA 0.335 0.769 0.466
VS?PMI 0.311 0.833 0.453
VS?DTP 0.342 0.745 0.469
VS?LSA?DTP 0.349 0.719 0.470
VS?PMI?DTP 0.328 0.773 0.461
the TREC dataset, using only the word sentiment
model is used as our baseline.
Similarly to the TREC experiments, the features
of the word generation model perform exception-
ally better than that of the topic association model.
The best performing feature of the word genera-
tion model is VS, achieving a 4.21% improvement
over the baseline?s f-measure. Interestingly, this is
the tied top performing f-measure over all combi-
nations of our features.
While LSA and DTP show mild improvements,
PMI performed worse than baseline, with higher
precision but a drop in recall. DTP was the best
performing topic association model.
When combining the best performing feature
of the word generation model (VS) with the fea-
tures of the topic association model, LSA, PMI
and DTP all performed worse than or as well as
the VS in f-measure evaluation. LSA and DTP im-
proves precision slightly, but with a drop in recall.
PMI shows the opposite tendency.
The best performing system was achieved using
VS, LSA and DTP at both precision and f-measure
evaluations.
4.3 Classification task ? SVM
4.3.1 Experimental Setting
To test our SVM classifier, we perform the classi-
fication task. Movie Review polarity dataset7 was
7http://www.cs.cornell.edu/people/pabo/movie-review-
data/
Table 3: Average ten-fold cross-validation accura-
cies of polarity classification task with SVM.
Accuracy
Features Movie-review MPQA
PRESENCE 82.6 76.8
TF 71.1 76.5
VS.TF 81.3 76.7
BM25.TF 81.4 77.9
IDF 61.6 61.8
VS.IDF 83.6 77.9
BM25.IDF 83.6 77.8
VS.TF?VS.IDF 83.8 77.9
BM25.TF?BM25.IDF 84.1 77.7
BM25.TF?VS.IDF 85.1 77.7
first introduced by Pang et al (2002) to test various
ML-based methods for sentiment classification. It
is a balanced dataset of 700 positive and 700 neg-
ative reviews, collected from the Internet Movie
Database (IMDb) archive. MPQA Corpus8 con-
tains 535 newspaper articles manually annotated
at sentence and subsentence level for opinions and
other private states (Wiebe et al, 2005).
To closely reproduce the experiment with the
best performance carried out in (Pang et al, 2002)
using SVM, we use unigram with the presence
feature. We test various combinations of our fea-
tures applicable to the task. For evaluation, we use
ten-fold cross-validation accuracy.
4.3.2 Experimental Result
We present the sentiment classification perfor-
mances in Table 3.
As observed by Pang et al (2002), using the raw
tf drops the accuracy of the sentiment classifica-
tion (-13.92%) of movie-review data. Using the
raw idf feature worsens the accuracy even more
(-25.42%). Normalized tf-variants show improve-
ments over tf but are worse than presence. Nor-
malized idf features produce slightly better accu-
racy results than the baseline. Finally, combining
any normalized tf and idf features improved the
baseline (high 83% ? low 85%). The best combi-
nation was BM25.TF?VS.IDF.
MPQA corpus reveals similar but somewhat un-
certain tendency.
8http://www.cs.pitt.edu/mpqa/databaserelease/
259
4.4 Discussion
Overall, the opinion retrieval and the sentiment
analysis models achieve improvements using our
proposed features. Especially, the features of the
word generation model improve the overall per-
formances drastically. Its effectiveness is also ver-
ified with a data-driven approach; the accuracy of
a sentiment classifier trained on a polarity dataset
was improved by various combinations of normal-
ized tf and idf statistics.
Differences in effectiveness of VS, BM25, and
LM come from parameter tuning and corpus dif-
ferences. For the TREC dataset, BM25 performed
better than the other models, and for the NTCIR
dataset, VS performed better.
Our features of the topic association model
show mild improvement over the baseline perfor-
mance in general. PMI and LSA, both modeling
the semantic associations between words, show
different behaviors on the datasets. For the NT-
CIR dataset, LSA performed better, while PMI
is more effective for the TREC dataset. We be-
lieve that the explanation lies in the differences
between the topics for each dataset. In general,
the NTCIR topics are general descriptive words
such as ?regenerative medicine?, ?American econ-
omy after the 911 terrorist attacks?, and ?law-
suit brought against Microsoft for monopolistic
practices.? The TREC topics are more named-
entity-like terms such as ?Carmax?, ?Wikipedia
primary source?, ?Jiffy Lube?, ?Starbucks?, and
?Windows Vista.? We have experimentally shown
that LSA is more suited to finding associations
between general terms because its training docu-
ments are from a general domain.9 Our PMI mea-
sure utilizes a web search engine, which covers a
variety of named entity terms.
Though the features of our topic association
model, WP and DTP, were evaluated on different
datasets, we try our best to conjecture the differ-
ences. WP on TREC dataset shows a small im-
provement of MAP compared to other topic asso-
ciation features, while the precision is improved
the most when this feature is used alone. The DTP
feature displays similar behavior with precision. It
also achieves the best f-measure over other topic
association features. DTP achieves higher rela-
tive improvement (3.99% F-measure verse 2.32%
MAP), and is more effective for improving the per-
formance in combination with LSA and PMI.
9TASA Corpus, http://lsa.colorado.edu/spaces.html
5 Conclusion
In this paper, we proposed various term weighting
schemes and how such features are modeled in the
sentiment analysis task. Our proposed features in-
clude corpus statistics, association measures using
semantic and local-context proximities. We have
empirically shown the effectiveness of the features
with our proposed opinion retrieval and sentiment
analysis models.
There exists much room for improvement with
further experiments with various term weighting
methods and datasets. Such methods include,
but by no means limited to, semantic similarities
between word pairs using lexical resources such
as WordNet (Miller, 1995) and data-driven meth-
ods with various topic-dependent term weighting
schemes on labeled corpus with topics such as
MPQA.
Acknowledgments
This work was supported in part by MKE & IITA
through IT Leading R&D Support Project and in
part by the BK 21 Project in 2009.
References
Kushal Dave, Steve Lawrence, and David M. Pennock. 2003.
Mining the peanut gallery: Opinion extraction and seman-
tic classification of product reviews. In Proceedings of
WWW, pages 519?528.
Andrea Esuli and Fabrizio Sebastiani. 2006. Sentiword-
net: A publicly available lexical resource for opinion min-
ing. In Proceedings of the 5th Conference on Language
Resources and Evaluation (LREC?06), pages 417?422,
Geneva, IT.
Hui Fang, Tao Tao, and ChengXiang Zhai. 2004. A formal
study of information retrieval heuristics. In SIGIR ?04:
Proceedings of the 27th annual international ACM SIGIR
conference on Research and development in information
retrieval, pages 49?56, New York, NY, USA. ACM.
George Forman. 2003. An extensive empirical study of fea-
ture selection metrics for text classification. Journal of
Machine Learning Research, 3:1289?1305.
Michael Gamon. 2004. Sentiment classification on customer
feedback data: noisy data, large feature vectors, and the
role of linguistic analysis. In Proceedings of the Inter-
national Conference on Computational Linguistics (COL-
ING).
Vasileios Hatzivassiloglou and Kathleen R. Mckeown. 1997.
Predicting the semantic orientation of adjectives. In Pro-
ceedings of the 35th Annual Meeting of the Association
for Computational Linguistics (ACL?97), pages 174?181,
madrid, ES.
Jaap Kamps, Maarten Marx, Robert J. Mokken, and
Maarten De Rijke. 2004. Using wordnet to measure se-
mantic orientation of adjectives. In Proceedings of the
4th International Conference on Language Resources and
Evaluation (LREC?04), pages 1115?1118, Lisbon, PT.
260
Taku Kudo and Yuji Matsumoto. 2004. A boosting algorithm
for classification of semi-structured text. In Proceedings
of the Conference on Empirical Methods in Natural Lan-
guage Processing (EMNLP).
Thomas K. Landauer and Susan T. Dumais. 1997. A solution
to plato?s problem: The latent semantic analysis theory of
acquisition, induction, and representation of knowledge.
Psychological Review, 104(2):211?240, April.
Yeha Lee, Seung-Hoon Na, Jungi Kim, Sang-Hyob Nam,
Hun young Jung, and Jong-Hyeok Lee. 2008. Kle at trec
2008 blog track: Blog post and feed retrieval. In Proceed-
ings of TREC-08.
Craig Macdonald and Iadh Ounis. 2006. The TREC Blogs06
collection: creating and analysing a blog test collection.
Technical Report TR-2006-224, Department of Computer
Science, University of Glasgow.
Shotaro Matsumoto, Hiroya Takamura, and Manabu Oku-
mura. 2005. Sentiment classification using word sub-
sequences and dependency sub-trees. In Proceedings of
PAKDD?05, the 9th Pacific-Asia Conference on Advances
in Knowledge Discovery and Data Mining.
Qiaozhu Mei, Xu Ling, Matthew Wondra, Hang Su, and
ChengXiang Zhai. 2007. Topic sentiment mixture: Mod-
eling facets and opinions in weblogs. In Proceedings of
WWW, pages 171?180, New York, NY, USA. ACM Press.
George A. Miller. 1995. Wordnet: a lexical database for
english. Commun. ACM, 38(11):39?41.
Tony Mullen and Nigel Collier. 2004. Sentiment analysis
using support vector machines with diverse information
sources. In Proceedings of the Conference on Empiri-
cal Methods in Natural Language Processing (EMNLP),
pages 412?418, July. Poster paper.
Sang-Hyob Nam, Seung-Hoon Na, Yeha Lee, and Jong-
Hyeok Lee. 2009. Diffpost: Filtering non-relevant con-
tent based on content difference between two consecutive
blog posts. In ECIR.
Vincent Ng, Sajib Dasgupta, and S. M. Niaz Arifin. 2006.
Examining the role of linguistic knowledge sources in the
automatic identification and classification of reviews. In
Proceedings of the COLING/ACL Main Conference Poster
Sessions, pages 611?618, Sydney, Australia, July. Associ-
ation for Computational Linguistics.
I. Ounis, M. de Rijke, C. Macdonald, G. A. Mishne, and
I. Soboroff. 2006. Overview of the trec-2006 blog track.
In Proceedings of TREC-06, pages 15?27, November.
I. Ounis, C. Macdonald, and I. Soboroff. 2008. Overview
of the trec-2008 blog track. In Proceedings of TREC-08,
pages 15?27, November.
Bo Pang and Lillian Lee. 2008. Opinion mining and sen-
timent analysis. Foundations and Trends in Information
Retrieval, 2(1-2):1?135.
Bo Pang, Lillian Lee, and Shivakumar Vaithyanathan. 2002.
Thumbs up? Sentiment classification using machine
learning techniques. In Proceedings of the Conference
on Empirical Methods in Natural Language Processing
(EMNLP), pages 79?86.
Fabrizio Sebastiani. 2002. Machine learning in automated
text categorization. ACM Computing Surveys, 34(1):1?47.
Yohei Seki, David Kirk Evans, Lun-Wei Ku, Le Sun, Hsin-
Hsi Chen, and Noriko Kando. 2008. Overview of mul-
tilingual opinion analysis task at ntcir-7. In Proceedings
of The 7th NTCIR Workshop (2007/2008) - Evaluation of
Information Access Technologies: Information Retrieval,
Question Answering and Cross-Lingual Information Ac-
cess.
Philip J. Stone, Dexter C. Dunphy, Marshall S. Smith, and
Daniel M. Ogilvie. 1966. The General Inquirer: A Com-
puter Approach to Content Analysis. MIT Press, Cam-
bridge, USA.
Peter D. Turney and Michael L. Littman. 2003. Measur-
ing praise and criticism: Inference of semantic orientation
from association. ACM Transactions on Information Sys-
tems, 21(4):315?346.
Peter D. Turney. 2001. Mining the web for synonyms: Pmi-
ir versus lsa on toefl. In EMCL ?01: Proceedings of the
12th European Conference on Machine Learning, pages
491?502, London, UK. Springer-Verlag.
Casey Whitelaw, Navendu Garg, and Shlomo Argamon.
2005. Using appraisal groups for sentiment analysis. In
Proceedings of the 14th ACM international conference
on Information and knowledge management (CIKM?05),
pages 625?631, Bremen, DE.
Janyce Wiebe, E. Breck, Christopher Buckley, Claire Cardie,
P. Davis, B. Fraser, Diane Litman, D. Pierce, Ellen Riloff,
Theresa Wilson, D. Day, and Mark Maybury. 2003. Rec-
ognizing and organizing opinions expressed in the world
press. In Proceedings of the 2003 AAAI Spring Sympo-
sium on New Directions in Question Answering.
Janyce M. Wiebe, Theresa Wilson, Rebecca Bruce, Matthew
Bell, and Melanie Martin. 2004. Learning subjec-
tive language. Computational Linguistics, 30(3):277?308,
September.
Janyce Wiebe, Theresa Wilson, and Claire Cardie. 2005.
Annotating expressions of opinions and emotions in
language. Language Resources and Evaluation,
39(2/3):164?210.
Theresa Wilson, Janyce Wiebe, and Paul Hoffmann. 2005.
Recognizing contextual polarity in phrase-level sentiment
analysis. In Proceedings of the Conference on Human
Language Technology and Empirical Methods in Natural
Language Processing (HLT-EMNLP?05), pages 347?354,
Vancouver, CA.
Kiduk Yang, Ning Yu, Alejandro Valerio, and Hui Zhang.
2006. WIDIT in TREC-2006 Blog track. In Proceedings
of TREC.
Hong Yu and Vasileios Hatzivassiloglou. 2003. Towards an-
swering opinion questions: Separating facts from opinions
and identifying the polarity of opinion sentences. In Pro-
ceedings of 2003 Conference on the Empirical Methods in
Natural Language Processing (EMNLP?03), pages 129?
136, Sapporo, JP.
Chengxiang Zhai and John Lafferty. 2004. A study of
smoothing methods for language models applied to infor-
mation retrieval. ACM Trans. Inf. Syst., 22(2):179?214.
Min Zhang and Xingyao Ye. 2008. A generation model
to unify topic relevance and lexicon-based sentiment for
opinion retrieval. In SIGIR ?08: Proceedings of the 31st
annual international ACM SIGIR conference on Research
and development in information retrieval, pages 411?418,
New York, NY, USA. ACM.
261
Semi-Automatic Practical Ontology Construction by Using a 
Thesaurus, Computational Dictionaries, and Large Corpora 
Sin-Jae Kang and Jong-Hyeok Lee 
Div. of Electrical and Computer Engineering, Pohang University of Science and Technology
San 31 Hyoja-Dong, Nam-Gu, Pohang 790-784 
Republic of KOREA 
sjkang@postech.ac.kr, jhlee@postech.ac.kr 
 
Abstract 
This paper presents the semi-automatic 
construction method of a practical 
ontology by using various resources. In 
order to acquire a reasonably practical 
ontology in a limited time and with less 
manpower, we extend the Kadokawa 
thesaurus by inserting additional 
semantic relations into its hierarchy, 
which are classified as case relations 
and other semantic relations. The 
former can be obtained by converting 
valency information and case frames 
from previously-built computational 
dictionaries used in machine translation. 
The latter can be acquired from concept 
co-occurrence information, which is 
extracted automatically from large 
corpora. The ontology stores rich 
semantic constraints among 1,110 
concepts, and enables a natural 
language processing system to resolve 
semantic ambiguities by making 
inferences with the concept network of 
the ontology. In our practical machine 
translation system, our ontology-based 
word sense disambiguation method 
achieved an 8.7% improvement over 
methods which do not use an ontology 
for Korean translation. 
1 Introduction 
An ontology is a knowledge base with 
information about concepts existing in the world 
or domain, their properties, and how they relate 
to each other. The principal reasons to use an 
ontology in machine translation (MT) are to 
enable source language analyzers and target 
language generators to share knowledge, to store 
semantic constraints, and to resolve semantic 
ambiguities by making inferences using the 
concept network of the ontology (Mahesh, 1996; 
Nirenburg et al, 1992). An ontology is different 
from a thesaurus in that it contains only 
language independent information and many 
other semantic relations, as well as taxonomic 
relations. 
In general, to build a high-quality semantic 
knowledge base, manual processing is 
indispensable. Previous attempts were mostly 
performed manually, or were developed without 
considering the context of a practical situation 
(Mahesh, 1996; Lenat et al, 1990). Therefore, it 
is difficult to construct a practical ontology with 
limited time and manpower resources. To solve 
this problem, we propose a semi-automatic 
ontology construction method, which takes full 
advantage of already existing knowledge 
resources and practical usages in large corpora. 
First, we define our ontology representation 
language (ORL) by modifying the most suitable 
among previously developed ORLs, and then 
design a language-independent and practical 
(LIP) ontology structure based on the defined 
ORL. Afterwards, we construct a practical 
ontology by the semi-automatic construction 
method given below. 
We extend the existing Kadokawa thesaurus 
(Ohno & Hamanishi, 1981) by inserting 
additional semantic relations into the hierarchy 
of the thesaurus. Uramoto (1996) and Tokunaga 
(1997) propose thesaurus extension methods for 
positioning unknown words in an existing 
thesaurus. Our approach differs in that the 
objects inserted are not words but semantic 
relations. 
Additional semantic relations can be 
classified as case relations and other semantic 
relations. The former can be obtained by 
converting the established valency information 
in bilingual dictionaries of COBALT-J/K 
(Collocation-Based Language Translator from 
Japanese to Korean) and COBALT-K/J 
(Collocation-Based Language Translator from 
Korean to Japanese) (Moon & Lee, 2000)  MT 
systems, as well as from the case frame in the 
Sejong electronic dictionary1. The latter can be 
acquired from concept co-occurrence 
information, which is extracted automatically 
from a corpus (Li et al, 2000). 
The remainder of this paper is organized as 
follows. We describe the principles of ontology 
design and an ORL used to represent our LIP 
ontology in the next section. In Section 3, we 
describe the semi-automatic ontology 
construction methodology in detail. An 
ontology-based word sense disambiguation 
(WSD) algorithm is given in Section 4. 
Experimental results are presented and analyzed 
in Section 5. Finally, we make a conclusion and 
indicate the direction of our future work in 
Section 6. 
2 Ontology Design 
2.1    Basic Principles 
Although no formal principles exist to determine 
the structure or content of our ontology, we can 
suggest some principles underlying our 
methodology. Firstly, an ontology for natural 
language processing (NLP) must provide 
concepts for representing word meanings in the 
lexicon and store selectional constraints of 
concepts, which enable inferences using the 
network of an ontology (Onyshkevych, 1997). 
These inferences can assist in metaphor and 
metonymy processing, as well as word sense 
disambiguation. For these reasons, an ontology 
becomes an essential knowledge source for high 
quality NLP, although it is difficult and time-
consuming to construct. Secondly, an ontology 
can be effortlessly shared by any application and 
in any domain (Gruber, 1993; Karp et al, 1999; 
Kent, 1999). More than two different ontologies 
in a certain domain can produce a semantic 
mismatch problem between concepts. Further, if 
                                                          
1 The Sejong electronic dictionary has been developed by 
several Korean linguistic researchers, funded by Ministry 
of Culture and Tourism, Republic of Korea. 
(http://www.sejong.or.kr) 
you wish to apply an existing ontology to a new 
application, it will often be necessary to convert 
the structure of the ontology to a new one. 
Thirdly, an ontology must support language 
independent features, because constructing 
ontologies for each language is inefficient. 
Fourthly, an ontology must have capabilities for 
users to easily understand, search, and browse. 
Therefore, we define a suitable ORL to support 
these principles. 
2.2    Ontology Representation Language 
Many knowledge representation languages are 
built specifically to share knowledge among 
different knowledge representation systems. 
Five types of ORLs were reviewed, such as 
FRAMEKIT (Nirenburg et al, 1992), 
Ontolingua (Gruber, 1993), CycL (Lenat et al, 
1990), XOL (Karp et al, 1999), and Ontology 
Markup Language (OML) (Kent, 1999). 
According to their semantics, FRAMEKIT and 
XOL adopt frame representation, CycL and 
Ontolingua use an extended first order predicate 
calculus, and the OML is based on conceptual 
graphs (CGs). Excepting FRAMEKIT and CycL, 
the other ORLs have not yet been applied to 
build any large-scale ontology. 
Among this variety of ORLs, we chose the 
simplified OML as the ORL of our LIP ontology, 
which is based on Extensible Markup Language 
(XML) and CGs. Since XML has a well-
established syntax, it is reasonably simple to 
parse, and XML will be widely used, because it 
has many software tools for parsing and 
manipulating, and a human readable 
representation. We intend to leave room for 
improvement by adopting the semantics of CGs, 
because the present design of our LIP ontology 
is for the specific purpose of disambiguating 
word senses. In future, however, we must extend 
its structure and content to build an interlingual 
meaning representation during semantic analysis 
in machine translation. Sowa's CGs (1984) is a 
widely-used knowledge representation language, 
consisting of logic structures with a graph 
notation and several features integrated from 
semantic net and frame representation. Globally, 
many research teams are working on the 
extension and application of CGs in many 
domains. 
 
3 Ontology Construction 
Many ontologies are developed for purely 
theoretical purposes, or are constructed as 
language-dependent computational resources, 
such as WordNet and EDR. However, they are 
seldom constructed as a language-independent 
computational resource. 
To construct a language-independent and 
practical ontology, we developed two strategies. 
First, we introduced the same number and grain 
size of concepts of the Kadokawa thesaurus and 
its taxonomic hierarchy into the LIP ontology. 
The thesaurus has 1,110 Kadokawa semantic 
categories and a 4-level hierarchy as a 
taxonomic relation (see Figure 1). This approach 
is a moderate shortcut to construct a practical 
ontology which easily enables us to utilize its 
results, since some resources are readily 
available, such as bilingual dictionaries of 
COBALT-J/K and COBALT-K/J. In these 
bilingual dictionaries, nominal and verbal words 
are already annotated with concept codes from 
the Kadokawa thesaurus. By using the same 
sense inventories of these MT systems, we can 
easily apply and evaluate our LIP ontology 
without additional lexicographic works. In 
addition, the Kadokawa thesaurus has proven to 
be useful for providing a fundamental 
foundation to build lexical disambiguation 
knowledge in COBALT-J/K and COBALT-K/J 
MT systems (Li et al, 2000). 
The second strategy to construct a practical 
ontology is to extend the hierarchy of the 
Kadokawa thesaurus by inserting additional 
semantic relations into its hierarchy. The 
additional semantic relations can be classified as 
case relations and other semantic relations. Thus 
far, case relations have been used occasionally 
to disambiguate lexical ambiguities in the form 
of valency information and case frame, but other 
semantic relations have not, because of the 
problem of discriminating them from each other, 
making them difficult to recognize. We define a 
total of 30 semantic relation types for WSD by 
referring mainly to the Sejong electronic 
dictionary and the Mikrokosmos ontology 
(Mahesh, 1996), as shown in Table 1. These 
semantic relation types cannot express all 
possible semantic relations existing among 
concepts, but experimental results demonstrated 
their usefulness for WSD. 
Two approaches are used to obtain these 
additional semantic relations, which will be 
inserted into the LIP ontology. The first imports 
relevant semantic information from existing 
computational dictionaries. The second applies 
the semi-automatic corpus analysis method (Li 
et al, 2000). Both approaches are explained in 
Section 3.1 and 3.2, respectively. 
Figure 2 displays the overall constructing 
flow of the LIP ontology. First, we build an 
initial LIP ontology by importing the existing 
Kadokawa thesaurus. Each concept inserted into 
the initial ontology has a Kadokawa code, a 
Korean name, an English name, a timestamp, 
and a concept definition. Although concepts can 
be uniquely identified by the Kadokawa concept 
codes, their Korean and English names are 
Root concept
nature    character  change action     feeling     human  disposition  society institute things 
0           1   2   3   4      5     6     7       8       9
astro- calen- wea- geog- sight plant  ani- physi- subs- pheno-
nomy    dar    ther  raphy                   mal  ology tance  mena
00      01       02     03     04       05     06     07     08 09
goods drugs food clothes buil- furni- statio- mark tools mach-
ding   ture    nary                       ine
90      91       92     93     94       95     96     97     98 99
orga- ani- fish insect organ foot&  sin- intes- egg   sex
nism  mal                               tail   ews   tine    
060    061  062   063   064    065   066   067   068   069
supp- writing- count- binder  toy  doll  recreation- sports- music- bell
lies      tool      book                             thing    tool  instrument
960     961      962     963   964  965     966           967 968       969
?????
?????
 
Figure 1. Concept hierarchy of the Kadokawa 
thesaurus 
 
Table 1. Sematic relation types in the LIP 
ontology 
 
Types Relation Lists 
Taxonomic 
relation 
is-a 
Case relation agent, theme, experiencer, 
accompanier, instrument,  
location, source, destination, 
reason, appraisee, criterion, 
degree, recipient 
Other 
Semantic 
relation 
has-member, has-element, 
contains, material-of, headed-
by, operated-by,  
controls, owner-of, represents, 
symbol-of, name-of,  
producer-of, composer-of, 
inventor-of, make, measured-in
 
inserted for the readability and convenience of 
the ontology developer. 
3.1    Dictionary Resources Utilization 
Case relations between concepts can be 
primarily derived from semantic information in 
the Sejong electronic dictionary 2  and the 
bilingual dictionaries of MT systems, which are 
COBALT-J/K and COBALT-K/J.  
We obtained 7,526 case frames from verb and 
adjective sub-dictionaries, which contain 3,848 
entries. Automatically converting lexical words 
in the case frame into the Kadokawa concept 
codes by using COBALT-K/J (see Figure 33), 
we extracted a total of 6,224 case relation 
instances. 
The bilingual dictionaries, which contain 
20,580 verb and adjective entries, have 16,567 
instances of valency information. Semi-
automatically converting syntactic relations into 
semantic relations by using specific rules and 
human intuition (see Figure 4), we generated 
15,956 case relation instances. The specific rules, 
as shown in Figure 5, are inferred from training 
samples, which are explained in Section 4.1. 
These obtained instances may overlap each 
other, but all instances are inserted only once 
into the initial LIP ontology. 
                                                          
2 The Sejong electronic dictionary has sub-dictionaries, 
such as noun, verb, pronoun, adverb, and others. 
3 The Yale Romanization is used to represent Korean 
lexical words. 
3.2    Corpus Analysis 
For the automatic construction of a sense-tagged 
corpus, we used the COBALT-J/K, which is a 
high-quality practical MT system developed by 
POSTECH in 1996. The entire system has been 
used successfully at POSCO (Pohang Iron and 
Steel Company), Korea, to translate patent 
materials on iron and steel subjects. We 
performed a slight modification on COBALT-
J/K so that it can produce Korean translations 
from Japanese texts with all nominal and verbal 
words tagged with the specific concept codes of 
the Kadokawa thesaurus. As a result, a Korean 
sense-tagged corpus, which has two hundred and 
fifty thousand sentences, can be obtained from 
Japanese texts. Unlike English, the Korean 
language has almost no syntactic constraints on 
word order as long as a verb appears in the final 
position. So we defined 12 local syntactic 
patterns (LSPs) using syntactically-related 
words in a sentence. Frequently co-occurring 
words in a sentence have no syntactic relations 
to homographs but may control their meaning. 
Such words are retrieved as unordered co-
occurring words (UCWs). Case relations are 
obtained from LSPs, and other semantic 
Case frames
Cilmwunha-ta (question) agent Salam(person) 
Cungkasikh-ta (increase) theme Kakyek (price) 
Kyeyhoykha-ta (plan) theme Pepan (bill) 
Seywu-ta (construct) theme Saep (business)
346 agent 5
743 theme 171
344 theme 419
394 theme 369
Case relations
 
Figure 3. Example of conversion from case 
frames in the Sejong dictionary 
381 (exercise) ul/lul (object) 449 (right)
071 (live) wa/kwa (adverb) 06|05|5 (living thing)
217 (soar) lo/ulo (adverb) 002 (sky)
712 (join) i/ka (subject) 5 (person)
381 theme 449
071 accompanier 06|05|5
217 destination 002
712 agent 5
Valency information
Case relations
 
Figure 4. Example of conversion from 
valency information in the bilingual 
dictionaries 
Step 1: From Kadokawa
thesaurus
Step 2: From existing
computational dictionaries
LIP Ontology
Kadokawa
Thesaurus
?
? ?
Import 
concepts
& taxonomic
relations
Sejong 
Electronic Dic.
(Case frame)
K-J & J-K
Bilingual Dic.
(Valency Info.)
Import 
case
relations
Semi-Automatic
Relations or Words
Mapping
Step 3: From a corpus
Japanese Raw Corpus
COBALT J/K
Japanese-to-Korean Translation
Sense Tagged
Korean Corpus
Partial Parsing
& Statistical Processing
Generalized Concept
Co-occurrence Information
Import case 
& other semantic
relations
Semi-Automatic 
Relations Mapping
 
Figure 2. Ovreall constructing flow of the LIP 
ontology 
relations are acquired from UCWs. Concept co-
occurrence information (CCI), which is 
composed of LSPs and UCWs, can be extracted 
by partial parsing and scanning. To select the 
most probable concept types, Shannon's entropy 
model is adopted to define the noise of a concept 
type to discriminate the homograph. Although it 
processes for concept type discrimination, many 
co-occurring concept types, which must be 
further selected, remain in each LSP and UCW. 
To solve this problem, some statistical 
processing was automatically applied (Li et al, 
2000). Finally, manual processing was 
performed to generate the ontological relation 
instances from the generalized CCI, similar to 
the previous valency information. The results 
obtained include approximately about 3,701 
case relations and 1,650 other semantic relations 
from 9,245 CCI, along with their frequencies. 
The obtained instances are inserted into the 
initial LIP ontology. Table 2 shows the number 
of relation instances imported into the LIP 
ontology from the Kadokawa thesaurus, 
computational dictionaries, and a corpus. 
4 Ontology Application 
The LIP ontology is applicable to many NLP 
applications. In this paper, we propose to use the 
ontology to disambiguate word senses. All 
approaches to WSD make use of words in a 
sentence to mutually disambiguate each other. 
The distinctions between various approaches lie 
in the source and type of knowledge made by 
the lexical units in a sentence. 
Our WSD approach is a hybrid method, 
which combines the advantages of corpus-based 
and knowledge-based methods. We use the LIP 
ontology as an external knowledge source and 
secured dictionary information as context 
information. Figure 6 shows our overall WSD 
algorithm. First, we apply the previously-
secured dictionary information to select correct 
senses of some ambiguous words with high 
precision, and then use the LIP ontology to 
disambiguate the remaining ambiguous words. 
The following are detailed descriptions of the 
procedure for applying the LIP ontology to 
WSD work. 
4.1    Measure of Concept Association 
To measure concept association, we use an 
association ratio based on the information 
theoretic concept of mutual information (MI), 
which is a natural measure of the dependence 
071 (life)
072 (upbringing)
073 (disease)
YES
NO
agent / theme
295 (influence)
370 (giving & receiving)
YES
NO
recipient
49 (joy & sorrow)
62 (figure)
YES
NO
experiencer
201 (stable)
202 (vibration)
1 (natural condition)
07 (physiology)
YES
NO
theme
2 (change)
3 (action)
YES
NO
agent
Manual mapping by human intuition
 
Figure 5. Example of subject relation 
mapping rules with governer concept codes
 
Apply secured dictionary information with high precision
Verb?s valency information
Success?
Local syntactic patterns
YES
NO
Success? YES
NO
Unordered co-occurring words patterns
Success?
Infer with the LIP ontology
YES
NO
Success?
Answer
YES
NO
Set the answer to the most frequently appearing sense
 
Figure 6. The proposed WSD algorithm 
Table 2. Imported relation instances 
 
Types Number 
Taxonomic relations 1,100
Case relations 19,459
Other semantic relations 1,650
Total 22,209
between random variables (Church & Hanks, 
1989). Resnik (1995) suggested a measure of 
semantic similarity in an IS-A taxonomy, based 
on the notion of information content. However, 
his method differs from ours in that we consider 
all semantic relations in the ontology, not 
taxonomy relations only. To implement this idea, 
we bind source concepts (SC) and semantic 
relations (SR) into one entity, since SR is mainly 
influenced by SC, not the destination concepts 
(DC). Therefore, if two entities, < SC, SR>, and 
DC have probabilities P(<SC, SR>) and P(DC), 
then their mutual information I(<SC, SR>, DC) 
is defined as: 
 
???
?
???
? +><
><=>< 1
)(),(
),,(log),,( 2 DCPSRSCP
DCSRSCPDCSRSCI  
 
The MI between concepts in the LIP ontology 
must be calculated before using the ontology as 
knowledge for disambiguating word senses. 
Figure 7 shows the construction process for 
training data in the form of <SC (governer), SR, 
DC (dependent), frequency> and the calculation 
of MI between the LIP ontology concepts. We 
performed a slight modification on COBALT-
K/J and COBALT-J/K to enable them to 
produce sense-tagged valency information 
instances with the specific concept codes of the 
Kadokawa thesaurus. After producing the 
instances, we converted syntactic relations into 
semantic relations using the specific rules (see 
Figure 5) and human intuition. As a result, we 
extracted sufficient training data from the 
Korean raw corpus: KIBS (Korean Information 
Base System, '94-'97) is a large-scale corpus of 
70 million words, and the Japanese raw corpus, 
which has eight hundred and ten thousand 
sentences. During this process, more specific 
semantic relation instances are obtained when 
compared with previous instances obtained in 
Section 3. Since such specific instances reflect 
the context of a practical situation, they are also 
imported into the LIP ontology. Table 3 shows 
the final number of semantic relations inserted 
into the LIP ontology. 
 
Table 3. Final relation instances in the LIP 
ontology 
 
Types Number 
Taxonomic relations 1,100
Case relations 112,746
Other semantic relations 2,093
Total 115,939
 
4.2    Locate the Least Weighted Path from 
One Ontology Concept to Other Concept 
If we regard MI as a weight between ontology 
concepts, we can treat the LIP ontology as a 
graph with weighted edges. All edge weights are 
non-negative and weights are converted into 
penalties by the below formula Pe. c indicate a 
constant, maximum MI between concepts of the 
LIP ontology. 
 
),,(),,( DCSRSCIcDCSRSCPe ><?=><  
 
So we use the formula below to locate the 
least weighted path from one concept to the 
other concept. The score function S is defined 
as: 
 
( )
( )
??
??
?
??
??
?
?
????
+
????
><
=
=
??
.,
),(),(min
,
),,(min
,1
),(
}{
j
R
kji
jkkiCCC
j
R
iji
jpip
ji
ji
CCandCCif
CCSCCS
CCandCCif
CRCPe
CCif
CCS
p
jik
p
 
 
Here C and R indicate concepts and semantic 
relations, respectively. By applying this formula, 
we can verify how well selectional constraints 
between concepts are satisfied. In addition, if 
there is no direct semantic relation between 
concepts, this formula provides a relaxation 
procedure, which enables it to approximate their 
semantic relations. This characteristic enables us 
Apply valency information with high precision
Japanese 
Raw Corpus
COBALT J/K
Japanese-to-Korean
Translation
Applied ValencyPatterns
<SC, synRel, DC, frequency>
Semi-Automatic Relation Mapping
Calculating MI 
btw <SC, SR> & DC
Semantic Relation Instances
<SC, SR, DC, frequency>
Korean
Raw Corpus
COBALT K/J
Korean-to-Japanese
Translation
LIP Ontology
Importing semantic
relation instances
 
Figure 7. Construction flow of ontology 
training data 
to obtain hints toward resolving metaphor and 
metonymy expressions. For example, when 
there is no direct semantic relation between 
concepts such as ?school? and ?inform,? the 
inferring process is as follows. The concept 
?school? is a ?facility?, and the ?facility? has 
?social human? as its members. The concept 
?inform? has ?social human? as its agent. Figure 
8 presents an example of the best path between 
these concepts, which is shown with bold lines. 
To locate the best path, the search mechanism of 
our LIP ontology applies heuristics as follows. 
Firstly, a taxonomic relation must be treated as 
exceptional from other semantic relations, 
because they inherently lack frequencies 
between parent and child concepts. So we assign 
a fixed weight to those edges experimentally. 
Secondly, the weight given to an edge is 
sensitive to the context of prior edges in the path. 
Therefore, our mechanism restricts the number 
of times that a particular relation can be 
traversed in one path. Thirdly, this mechanism 
avoids an excessive change in the gradient. 
5 Experimental Evaluation 
For experimental evaluation, eight ambiguous 
Korean nouns were selected, along with a total 
of 404 test sentences in which one of the 
homographs appears. The test sentences were 
randomly selected from the KIBS. Out of 
several senses for each ambiguous word, we 
considered only two senses that are most 
frequently used in the corpus. We performed 
three experiments: The first experiment, BASE, 
is the case where the most frequently used 
senses are always taken as the senses of test 
words. The purpose of this experiment is to 
show the baseline for WSD work. The second, 
PTN, uses only secured dictionary information, 
such as the selectional restriction of verbs, local 
syntactic patterns, and unordered co-occurring 
word patterns in disambiguating word senses. 
This is a general method without an ontology. 
The third, LIP, shows the results of our WSD 
method using the LIP ontology. The 
experimental results are shown in Table 4. In 
these experiments, the LIP method achieved an 
8.7% improvement over the PTN method for 
Korean analysis. The main reason for these 
results is that, in the absence of secured 
dictionary information (see Figure 7) about an 
ambiguous word, the ontology provides a 
generalized case frame (i.e. semantic restriction) 
by the concept code of the word. In addition, 
when there is no direct semantic restriction 
between concepts, our search mechanism 
provides a relaxation procedure (see Figure 8). 
Therefore, the quality and usefulness of the LIP 
ontology were proved indirectly by these results. 
6 Conclusion 
In this paper we have proposed a semi-automatic 
construction method of the LIP ontology and an 
ontology-based WSD algorithm. The LIP 
Table 4. Experimental results of WSD (%) 
 
Homograph Sense BASE PTN LIP 
father & 
child Pwuca 
rich man 
76.9 69.2 86.0
liver Kancang 
soy sauce 
67.3 87.8 91.8
housework 
Kasa words of 
song 
48.1 88.5 96.1
shoe 
Kwutwu word of 
mouth 
79.6 85.7 95.9
eye Nwun 
snow 
82.0 96.0 92.0
courage Yongki 
container 
62.0 74.0 82.0
expenses Kyengpi 
defense 
74.5 78.4 90.2
times Kyeongki 
match 
52.9 80.4 95.6
Average Precision 67.9 82.5 91.2
Root
Concept
nature
0
character
1
change
2
action
3
feeling
4
human
5
disposition
6
society
7
institute
8
things
9
school
722
facility
72
is-a
is-a
is-a
social
human
507
person
50
is-a
is-a has
member
inform
750
report
75
is-a
is-a
agent
?
?
?
?
?
?
 
Figure 8. Example of the best path between 
concepts ?school? and ?inform? in the LIP 
ontology 
 
ontology includes substantial semantic relations 
between concepts, and differs from many of the 
resources in that there is no language-dependent 
knowledge in the resource, which is a network 
of concepts, not words. Semantic relations of the 
LIP ontology are generated by considering two 
different languages, Korean and Japanese. In 
addition, we can easily apply the ontology 
without additional lexicographic works, since 
large-scale bilingual dictionaries have words 
already annotated with concept codes of the LIP 
ontology. Therefore, our LIP ontology is a 
language independent and practical knowledge 
base. You can apply this ontology for other 
languages, if one merely inserts Kadokawa 
concept codes for each entry into the dictionary. 
Our ontology construction method requires 
manual processing, i.e., mapping from syntactic 
relations to semantic relations by specific rules 
and human intuition. However, this is necessary 
for building a high-quality semantic knowledge 
base. Our construction method is quite effective 
in comparison with other methods. 
We plan further research on how to 
effectively divide the grain size of ontology 
concepts to best express the whole world 
knowledge, and how to utilize the LIP ontology 
in a full semantic analysis process. 
Acknowledgements 
The authors would like to thank the Ministry of 
Education of Korea for its financial support 
toward the Electrical and Computer Engineering 
Division at POSTECH through its BK21 
program. 
References 
Church, K. and P. Hanks. 1989. Word association 
norms, mutual information, and lexicography. In 
Proceedings of the 27th Annual Meeting of the 
Association for Computational Linguistics, pages 
76-83, Vancouver, Canada. 
Gruber, Thomas R. 1993. A Translation Approach to 
Portable Ontology Specification. Knowledge 
Acquisition 5(2):199-220. 
Karp, P. D., V. K. Chaudhri, and J. F. Thomere. 1999. 
XOL: An XML-Based Ontology Exchange 
Language. Technical Note 559, AI Center, SRI 
International, July. 
Kent, Robert E. 1999. Conceptual Knowledge 
Markup Language: The Central Core. In the 
Electronic Proceedings of the Twelfth Workshop 
on Knowledge Acquisition, Modeling and 
Management(KAW`99). Banff, Alberta, Canada, 
October. 
Lenat, D. B. et al 1990. Cyc: toward programs with 
common sense. Communications of the ACM, 
33(8):30-49. 
Li, Hui-Feng et al 2000. Lexical Transfer Ambiguity 
Resolution Using Automatically-Extracted 
Concept Co-occurrence Information. International 
Journal of Computer Processing of Oriental 
Languages, World Scientific Pub., 13(1):53-68. 
Mahesh, Kavi. 1996. Ontology Development for 
Machine Translation: Ideology and Methodology. 
Technical Report MCCS 96-292, Computing 
Research Laboratory, New Mexico State 
University, Las Cruces, NM. 
Moon, Kyunghi and Jong-Hyeok Lee. 2000. 
Representation and Recognition Method for 
Multi-Word Translation Units in Korean-to-
Japanese MT System. COLING 2000, pages 544-
550, Germany. 
Nirenburg, Sergei, Jaime Carbonell, Masaru Tomita, 
and Kenneth Goodman. 1992. Machine 
Translation: A Knowledge-Based Approach, 
Morgan Kaufmann Pub., San Mateo, California. 
Ohno, S. and M. Hamanishi. 1981. New Synonyms 
Dictionary, Kadogawa Shoten, Tokyo. (Written in 
Japanese). 
Onyshkevych, Boyan A. 1997. An Ontological-
Semantic Framework for Text Analysis. Ph.D. 
dissertation, Program in Language and 
Information Technologies, School of Computer 
Science, Carnegie Mellon University, CMU-LTI-
97-148. 
Resnik, Philip. 1995. Using Information Content to 
Evaluate Semantic Similarity in a Taxonomy. In 
Proceedings of IJCAI-95, 1995, pages 448-453, 
Montreal, Canada. 
Sowa, John F. 1984. Conceptual Structures: 
Information Processing in Mind and Machine, 
Addison-Wesley Pub., MA. 
Takenobu, Tokunaga et al 1997. Extending a 
thesaurus by classifying words. In Proceedings of 
the ACL-EACL Workshop on Automatic 
Information Extraction and Building of Lexical 
Semantic Resources, pages 16-21, Madrid, Spain. 
Uramoto, Naohiko. 1996. Positioning Unknown 
Word in a Thesaurus by using Information 
Extracted from a Corpus. In Proceedings of 
COLING-96, pages 956-961, Copenhagen, 
Denmark. 
Word Sense Disambiguation in a Korean-to-Japanese 
MT System Using Neural Networks 
You-Jin Chung, Sin-Jae Kang, Kyong-Hi Moon, and Jong-Hyeok Lee 
Div. of Electrical and Computer Engineering, Pohang University of Science and Technology (POSTECH) 
and Advanced Information Technology Research Center(AlTrc) 
San 31, Hyoja-dong, Nam-gu, Pohang, R. of KOREA, 790-784 
{prizer,sjkang,khmoon,jhlee}@postech.ac.kr 
 
Abstract  
This paper presents a method to resolve 
word sense ambiguity in a 
Korean-to-Japanese machine translation 
system using neural networks. The 
execution of our neural network model is 
based on the concept codes of a thesaurus. 
Most previous word sense disambiguation 
approaches based on neural networks have 
limitations due to their huge feature set size. 
By contrast, we reduce the number of 
features of the network to a practical size by 
using concept codes as features rather than 
the lexical words themselves. 
Introduction 
Korean-to-Japanese machine translation (MT) 
employs a direct MT strategy, where a Korean 
homograph may be translated into a different 
Japanese equivalent depending on which sense 
is used in a given context. Thus, word sense 
disambiguation (WSD) is essential to the 
selection of an appropriate Japanese target word. 
Much research on word sense disambiguation 
has revealed that several different types of 
information can contribute to the resolution of 
lexical ambiguity. These include surrounding 
words (an unordered set of words surrounding a 
target word), local collocations (a short sequence 
of words near a target word, taking word order 
into account), syntactic relations (selectional 
restrictions), parts of speech, morphological 
forms, etc (McRoy, 1992, Ng and Zelle, 1997). 
Some researchers use neural networks in 
their word sense disambiguation systems 
Because of its strong capability in classification 
(Waltz et al, 1985, Gallant, 1991, Leacock et al, 
1993, and Mooney, 1996). Since, however, most 
such methods require a few thousands of 
features or large amounts of hand-written data 
for training, it is not clear that the same neural 
network models will be applicable to real world 
applications. 
We propose a word sense disambiguation 
method that combines both the neural net-based 
approach and the work of Li et al(2000), 
especially focusing on the practicality of the 
method for application to real world MT 
systems. To reduce the number of input features 
of neural networks to a practical size, we use 
concept codes of a thesaurus as features. 
In this paper, Yale Romanization is used to 
represent Korean expressions. 
1 System Architecture 
Our neural network method consists of two 
phases. The first phase is the construction of the 
feature set for the neural network; the second 
phase is the construction and training of the 
neural network. (see Figure 1.) 
For practical reasons, a reasonably small 
number of features is essential to the design of a 
neural network. To construct a feature set of a 
reasonable size, we adopt Li?s method (2000), 
based on concept co-occurrence information 
(CCI). CCI are concept codes of words which 
co-occur with the target word for a specific 
syntactic relation. 
In accordance with Li?s method, we 
automatically extract CCI from a corpus by 
constructing a Korean sense-tagged corpus. To 
accomplish this, we apply a Japanese-to-Korean 
MT system. Next, we extract CCI from the 
constructed corpus through partial parsing and 
scanning. To eliminate noise and to reduce the 
number of CCI, refinement proceesing is applied 
Japanese Corpus
COBALT-J/K
(Japanese-to-Korean
MT system)
Sense Tagged
Korean Corpus
Partial Parsing
& Pattern Scanning
Raw CCI
CCI Refinement
Processing
Refined CCI
Feature Set Construction Neural Net Construction
Feature Set
Network
Construction
Neural Network
Network
Learning
Stored in
MT Dictionary
Network
Parameters
Figure 1. System Architecture 
noun
nature  character          society institute  things 
0            1                     7          8          9
astro- calen- animal        pheno-
nomy    dar                          mena
00      01              06             09
goods drugs  food       stationary    machine
90      91       92             96             99
orga- ani- sin- intes- egg    sex
nism  mal              ews   tine    
060    061               066   067    068   069
supp- writing- count- bell
lies      tool      book
960     961      962               969
?????
?????
?????
????? ?????
?????
?????
?????
?????
?????
L1
L2
L3
L4
Figure 2. Concept hierarchy of the Kadokawa
thesaurus
to the extracted raw CCI. After completing 
refinement processing, we use the remaining 
CCI as features for the neural network. The 
trained network parameters are stored in a 
Korean-to-Japanese MT dictionary for WSD in 
translation. 
2 Construction of Refined Feature Set 
2.1 Automatic Construction of Sense-tagged 
Corpus 
For automatic construction of the sense-tagged 
corpus, we used a Japanese-to-Korean MT 
system called COBALT-J/K1. In the transfer 
dictionary of COBALT-J/K, nominal and verbal 
words are annotated with concept codes of the 
Kadokawa thesaurus (Ohno and Hamanishi, 
1
1
C
d
C
t
n
c
a
1
J
N
 
1
T
p
The quality of the constructed sense-tagged 
corpus is a critical issue. To evaluate the quality, 
we collected 1,658 sample sentences (29,420 
eojeols2) from the corpus and checked their 
precision. The total number of errors was 789, 
and included such errors as morphological 
analysis, sense ambiguity resolution and 
unknown words. It corresponds to the accuracy 
of 97.3% (28,631 / 29,420 eojeols). 
Because almost all Japanese common nouns 
represented by Chinese characters are 
monosemous little transfer ambiguity is 
exhibited in Japanese-to-Korean translation. In 
our test, the number of ambiguity resolution 
errors was 202 and it took only 0.69% of the 
overall corpus (202 / 29,420 eojeols). 
Considering the fact that the overall accuracy of 
the constructed corpus exceeds 97% and only a 
few sense ambiguity resolution errors were 
 
 
 
 
 
. 
, 
 
 
 
 981), which has a 4-level hierarchy of about 
,100 semantic classes, as shown in Figure 2. 
oncept nodes in level L1, L2 and L3 are further 
ivided into 10 subclasses. 
We made a slight modification of 
OBALT-J/K to enable it to produce Korean 
ranslations from a Japanese text, with all 
ominal words tagged with specific concept 
odes at level L4 of the Kadokawa thesaurus. As 
 result, a Korean sense-tagged corpus of 
,060,000 sentences can be obtained from the 
apanese corpus (Asahi Shinbun, Japanese 
ewspaper of Economics, etc.). 
                                                    
 COBALT-J/K (Collocation-Based Language 
ranslator from Japanese to Korean) is a high-quality 
ractical MT system developed by POSTECH.                                                      
found in the Japanese-to-Korean translation of
nouns, we regard the generated sense-tagged
corpus as highly reliable. 
2.2 Extraction of Raw CCI 
Unlike English, Korean has almost no syntactic
constraints on word order as long as the verb
appears in the final position. The variable word
order often results in discontinuous constituents
Instead of using local collocations by word order
Li et al (2000) defined 13 patterns of CCI for
homographs using syntactically related words in
a sentence. Because we are concerned only with
2 An Eojeol is a Korean syntactic unit consisting of a
content word and one or more function words. 
Table 2. Concept codes and frequencies in CFP 
({<Ci,fi>}, type2, nwun(eye)) 
Code Freq. Code Freq. Code Freq. Code Freq.
103 4 107 8 121 7 126 4 
143 8 160 5 179 7 277 4 
320 8 331 6 416 7 419 12
433 4 501 13 503 10 504 11
505 6 507 12 508 27 513 5 
530 6 538 16 552 4 557 7 
573 5 709 5 718 5 719 4 
733 5 819 4 834 4 966 4 
987 9 other* 210     
? ?other? in the table means the set of concept codes 
with the frequencies less than 4. 
Table 1. Structure of CCI Patterns 
CCI type Structure of pattern 
type0 unordered co-occurrence words 
type1 noun + noun  or  noun + noun 
type2 noun + uy + noun 
type3 noun + other particles + noun 
type4 noun + lo/ulo + verb 
type5 noun + ey + verb 
type6 noun + eygey + verb 
type7 noun + eyse + verb 
type8 noun + ul/lul + verb 
type9 noun + i/ka + verb 
type10 verb + relativizer + noun 
noun homographs, we adopt 11 patterns from 
them excluding verb patterns, as shown in Table 
1. The words in bold indicate the target 
homograph and the words in italic indicate 
Korean particles. 
For a homograph W, concept frequency 
patterns (CFPs), i.e., ({<C1,f1>,<C2,f2>, ... , 
<Ck,fk>}, typei, W(Si)), are extracted from the 
sense-tagged training corpus for each CCI type i 
by partial parsing and pattern scanning, where k 
is the number of concept codes in typei, fi is the 
frequency of concept code Ci appearing in the 
corpus, typei is an CCI type i, and W(Si) is a 
homograph W with a sense Si. All concepts in 
CFPs are three-digit concept codes at level L4 in 
the Kadokawa thesaurus. Table 2 demonstrates 
an example of CFP that can co-occur with the 
homograph ?nwun(eye)? in the form of the CCI 
type2 and their frequencies. 
2.3 CCI Refinement Processing 
The extracted CCI are too numerous and too 
noisy to be used in a practical system, and must 
to be further selected. To eliminate noise and to 
reduce the number of CCI to a practical size, we 
apply the refinement processing to the extracted 
CCI. CCI refinement processing is composed of 
2 processes: concept code discrimination and 
concept code generalization. 
2.3.1 Concept Code Discrimination 
In the extracted CCI, the same concept code may 
appear for determining the different meanings of 
a homograph. To select the most probable 
concept codes, which frequently co-occur with 
the target sense of a homograph, Li defined the 
discrimination value of a concept code using 
Shannon?s entropy (Shannon, 1951). A concept 
code with a small entropy has a large 
discrimination value. If the discrimination value 
of the concept code is larger than a threshold, 
the concept code is selected as useful 
information for deciding the word sense. 
Otherwise, the concept code is discarded. 
2.3.2 Concept Code Generalization 
After concept discrimination, co-occurring 
concept codes in each CCI type must be further 
selected and the code generalized. To perform 
code generalization, Li adopted to Smadja?s 
work (Smadja, 1993) and defined the code 
strength using a code frequency and a standard 
deviation in each level of the concept hierarchy. 
The generalization filter selects the concept 
codes with a strength larger than a threshold. We 
perform this generalizaion processing on the 
Kadokawa thesaurus level L4 and L3. 
After processing, the system stores the 
re
ty
re
fe
s
e
3
3
B
th
u
s
refined conceptual patterns ({C1, C2, C3, ...}, 
pei, W(Si)) as a knowledge source for WSD of 
al texts. These refined CCI are used as input 
atures for the neural network. The more 
pecific description of the CCI extraction is 
xplained in Li (2000). 
 Construction of Neural Network 
.1 Neural Network Architecture 
ecause of its strong capability for classification, 
e multilayer feedforward neural network is 
sed in our sense classification system. As 
hown in Figure 3, each node in the input layer 
presents a concept code in CCI of a target 
. .
CCI type i2
CCI type i1
input CCI type 0
input
CCI type 1
input
CCI type 8
input
CCI type 2
input
74
26
022
078
080
50
696
028
419
38
23
239
323
nwun1 (snow)
nwun2 (eye)
...
Figure 5. The Resulting Network for ?nwun? 
w
r
n
n
To determine a good topology for the network, 
we implemented a 2-layer (no hidden layer) and 
a 3-layer (with a single hidden layer of 5 nodes) 
network and compared their performance. The 
comparison result is given in Section 5. 
Each homograph has a network of its own. 
Figure 43 demonstrates a construction example 
of the input layer for the homograph ?nwun? 
with the sense ?snow? and ?eye?. The left side is 
the extracted CCI for each sense after refinement 
processing. We construct the input layer for 
?nwun? by merely integrating the concept codes 
in both senses. The resulting input layer is 
partitioned into several subgroups depending on 
their CCI types, i.e., type 0, type 1, type 2 and 
type 8. Figure 5 shows the overall network 
architecture for ?nwun?. 
 
3  
f  
c
3.2 Network Learning 
We selected 875 Korean homographs requring 
the WSD processing in a Korean-to-Japanese 
translation. Among the selected nouns, 736 
nouns (about 84%) had two senses and the other 
139 nouns had more than 3 senses. Using the 
extracted CCI, we constructed neural networks 
and trained network parameters for each 
homograph. The training patterns were also 
extracted from the previously constructed 
sense-tagged corpus. 
The average number of input features (i.e. 
input nodes) of the constructed networks was 
approximately 54.1 and the average number of 
senses (i.e. output nodes) was about 2.19. In the 
case of a 2-layer network, the total number of 
parameters (synaptic weights) needed to be 
trained is about 118 (54.1?2.19) for each 
homograph. This means that we merely need 
storage for 118 floating point numbers (for 
s
fe
re
? CCI type 0 : {26, 022}
? CCI type 1 : {080, 696}
nwun1 (snow)
CCI type 0
input
CCI type 1
74
26
022
078
080
Refined CCI
4
O
c
k
o
s
F
K                                                    
 The concept codes in Figure 4 are simplified ones
or the ease of illustration. In reality there are 87
? CCI type 8 : {38, 239}
Total 13 concept codes
integrate input
CCI type 8
input
CCI type 2
input
13 nodes
nwun2 (eye)
? CCI type 0 : {74, 078}
? CCI type 2 : {50, 028, 419}
? CCI type 8 : {23, 323}
50
696
028
419
38
23
239
323
Figure 4. Construction of Input layer for ?nwun?...
..
Output
(senses of the 
target word)
Inputs Outputs
..
Hidden
Layers
input
CCI type ik
input
...
Figure 3. Topology of Neural Network 
ord and each node in the output layer 
epresents the sense of a target word. The 
umber of hidden layers and the number of 
odes in a hidden layer are another crucial issue. oncept codes for ?nwun?. Cynaptic weights) and 54 integers (for input 
atures) for each homograph, which is a 
asonable size to be used in real applications. 
 Word Sense Disambiguation 
ur WSD approach is a hybrid method, which 
ombines the advantage of corpus-based and 
nowledge-based methods. Figure 6 shows our 
verall WSD algorithm. For a given homograph, 
ense disambiguation is performed as follows. 
irst, we search a collocation dictionary. The 
orean-to-Japanese translation system 
OBALT-K/J has an MWTU (Multi-Word 
{078}
CCI type 0 CCI type 0
input
CCI type 1
input
CCI type 8
input
CCI type 2
input
CCI type 1
nwunmwul-i   katuk-han   kunye-uy   nwun-ul   po-mye
input               : ??? ??? ??? ?? ?? ?
[078] [274]concept code  : [503] [331]targetword
CCI type        : (type 0) (type 0) (type 2) (type 8)
CCI type 2
CCI type 8
{none}
{503}
{331}
078
022
74
26
028
50
696
080
239
38
23
419
323
Input Layer
Similarity
Calculation
{274}
(0.000)
(0.285)
(0.250)
(1.000)
(0.000)
(0.857)
(0.000)
(0.000)
(0.000)
(0.285)
(0.000)
(0.000)
(0.250)
similarity values
Figure 7. Construction of Input Pattern by Using
Concept Similarity Calculation 
Neural Networks
Select the most frequent sense
Success
Success
Answer
NO
NO
NO
YES
YES
YES
Selectional Restrictions of the Verb
Collocation Dictionary
Success
 
Figure 6. The Proposed WSD Algorithm 
Translation Units) dictionary, which contains 
idioms, compound words, collocations, etc. If a 
collocation of the target word exists in the 
MWTU dictionary, we simply determine the 
sense of the target word to the sense found in the 
dictionary. This method is based on the idea of 
?one sense per collocation?. Next, we verify the 
selectional restriction of the verb described in 
the dictionary. If we cannot find any matched 
patterns for selectional restrictions, we apply the 
neural network approach. WSD in the neural 
network stage is performed in the following 3 
steps. 
Step 1. Extract CCI from the context of the 
target word. The window size of the context is a 
single sentence. Consider, for example, the 
sentence in Figure 7 which has the meaning of 
?Seeing her eyes filled with tears, ??. The 
target word is the homograph ?nwun?. We 
extract its CCI from the sentence by partial 
parsing and pattern scanning. In Figure 7, the 
words ?nwun? and ?kunye(her)? with the concept 
code 503 have the relation of <noun + uy + 
noun>, which corresponds to ?CCI type 2? in 
Table 1. There is no syntactic relation between 
the words ?nwun? and ?nwunmul(tears)? with the 
concept code 078, so we assign ?CCI type 0? to 
the concept code 078. 
Similarly, we can obtain all pairs of CCI types 
and their concept codes appearing in the context. 
All the extracted <CCI-type: concept codes> 
pairs are as follows: {<type 0: 078,274>, <type 
2: 503>, <type 8: 331>}. 
Step 2. Obtain the input pattern for the 
network by calculating concept similarities 
between the features of the input nodes and the 
concept code in the extracted <CCI-type: 
concept codes>. Concept similarity calculation 
is performed only between the concept codes 
with the same CCI-type. The calculated concept 
similarity score is assigned to each input node as 
the input value to the network. 
Csim(Ci, Pj) in Equation 1 is used to calculate 
the concept similarity between Ci and Pj, where 
MSCA(Ci, Pj) is the most specific common 
ancestor of concept codes Ci and Pj, and weight 
is a weighting factor reflecting that Ci as a 
descendant of Pj is preferable to other cases. 
That is, if Ci is a descendant of Pj, we set weight 
to 1. Otherwise we set weight to 0.5. 
weight
PlevelClevel
PCMSCAlevel
PCCsim
ji
ji
ji ?+
?=
)()(
)),((2
),(  (1)
The similarity values between the target 
(all 0.000)
(0.375)
(0.857)
(0.667)
(0.285)
(0.250) (0.250)
L1
L2
L3
L4
?
Ci
P1
P2
P3
P4
P5 P5
TOP
Figure 8. Concept Similarity on the Kadokawa
Thesaurus Hierarchy 
concept Ci and each Pj on the Kadokawa 
thesaurus hierarchy are shown in Figure 8. 
These similarity values are computed using 
Equation 1. For example, in ?CCI-type 0? part 
calculation, the relation between the concept 
codes 274 and 26 corresponds to the relation 
between Ci and P4 in Figure 8. So we assign the 
similarity 0.285 to the input node labeled by 26. 
As another example, the concept codes 503 and 
50 have a relation between Ci and P2 and we 
obtain the similarity 0.857. If more than two 
concept codes exist in one CCI-type, such as 
<CCI-type 0: 078, 274>, the maximum 
similarity value among them is assigned to the 
input node, as in Equation 2. 
In Equation 2, Ci is the concept code of the 
input node, and Pj is the concept codes in the 
<CCI-type: concept codes> pair which has the 
same CCI-type as Ci. 
By adopting this concept similarity calculation, 
we can achieve a broad coverage of the method. 
If we use the exact matching scheme instead of 
concept similarity, we may obtain only a few 
concept codes matched with the features. 
Consequently, sense disambiguation would fail 
because of the absence of clues. 
Step 3. Feed the obtained input pattern to the 
neural network and compute activation strengths 
for each output node. Next, select the sense of 
the node that has a larger activation value than 
all other output node. If the activation strength is 
lower than the threshold, it will be discarded and 
his 
5 Experimental Evaluation 
For an experimental evaluation, 10 ambiguous 
Korean nouns were selected, along with a total 
of 500 test sentences in which one homograph 
appears. In order to follow the ambiguity 
distribution described in Section 3.2, we set the 
number of test nouns with two senses to 8 (80%). 
The test sentences were randomly selected from 
the KIBS (Korean Information Base System) 
corpus. 
The experimental results are shown in Table 3, 
where result A is the case when the most 
frequent sense was taken as the answer. To 
compare it with our approach (result C), we also 
performed the experiment using Li?s method 
(result B). For sense disambiguation, Li?s 
method features which are similar to our method. 
However, unlike our method, which combines 
all features by using neural networks, Li 
considers only one clue at each decision step. As 
shown in the table, our approach exceeded Li?s 
)),((max)( jiPi PCCsimCInputVal i
=    (2)
Table 3. Comparison of WSD Results 
Precision (%) Word Sense No (A) (B) (C)
father & child 33 pwuca
rich man 17 
66 64 72
liver 37 kancang
soy source 13 
74 84 74
housework 39 kasa
words of song 11 
78 68 82
shoes 45 kwutwu
word of mouth 5 
90 70 92
eye 42 nwun
snow 8 
84 80 86
container 41 yongki 82 72 88
the network will not make any decisions. T
process is represented in Figure 9. courage 9 
doctor 27 uysa
intention 23 
54 80 84
district 27 cikwu
the earth 23 
54 84 92
whole body 39 
one?s past 6 censin
telegraph 5 
78 84 80
one?s best 27 
military strength 13 
electric power 7 
cenlyek
past record 3 
54 50 72
Average Precision 71.4 73.6 82.2
? (A) : Baseline   (B) : Li?s method 
(C) : Proposed method (using a 2-layer NN) 
nwun1 (snow)
nwun2 (eye)
...
threshold
(0.000)
(0.285)
(0.250)
(1.000)
(0.000)
(0.857)
(0.000)
(0.000)
(0.000)
(0.285)
(0.000)
(0.000)
(0.250)
Figure 9. Sense Disambiguation for ?nwun? 
in most of the results except ?kancang? and 
?censin?. This result shows that word sense  
disambiguation can be improved by combining 
several clues together (e.g. neural networks) 
rather than using them independently (e.g. Li?s 
method). 
The performance for each stage of the 
proposed method is shown in Table 4. Symbols 
COL, VSR, NN and MFS in the table indicate 4 
stages of our method in Figure 6, respectively. 
In the NN stage, the 3-layer model did not show 
a performance superior  to the 2-layer model 
because of the lack of training samples. Since 
the 2-layer model has fewer parameters to be 
trained, it is more efficient to generalize for 
limited training corpora than the 3-layer model. 
Conclusion 
To resolve sense ambiguities in 
Korean-to-Japanese MT, this paper has proposed 
a practical word sense disambiguation method 
using neural networks. Unlike most previous 
approaches based on neural networks, we reduce 
the number of features for the network to a 
practical size by using concept codes rather than 
lexical words. In an experimental evaluation, the 
proposed WSD model using a 2-layer network 
achieved an average precision of 82.2% with an 
improvement over Li?s method by 8.6%. This 
result is very promising for real world MT 
systems. 
We plan further research to improve precision 
and to expand our method for verb homograph 
disambiguation. 
Acknowledgements 
This work was supported by the Korea Science 
and Engineering Foundation (KOSEF)  through 
the Advanced Information Technology Research 
Center(AITrc). 
Table 4. Average Precision and Coverage 
for Each Stage of thePproposed Method 
 
<Case 1 : 2-layer NN> 
 COL VSR NN MFS 
Avg. Prec 100.0% 91.2% 86.3% 56.1%
Avg. Cov 3.6% 6.8% 73.2% 16.4%
 
<Case 2 : 3-layer NN> 
 COL VSR NN MFS 
Avg. Prec 100.0% 91.2% 87.1% 56.0%
Avg. Cov 3.6% 6.8% 72.5% 17.1%
 
References 
Gallant S. (1991) A Practical Approach for 
Representing Context and for Performing Word 
Sense Disambiguation Using Neural Networks. 
Neural Computation, 3/3, pp. 293-309 
Leacock C., Twell G. and Voorhees E. (1993) 
Corpus-based Statistical Sense Resolution. In 
Proceedings of the ARPA Human Language 
Technology Workshop, San Francisco, Morgan 
Kaufman, pp. 260-265 
Li H. F., Heo N. W., Moon K. H., Lee J. H. and Lee 
G. B. (2000) Lexical Transfer Ambiguity 
Resolution Using Automatically-Extracted Concept 
Co-occurrence Information. International Journal 
of Computer Processing of Oriental Languages, 
13/1, pp. 53-68  
McRoy S. (1992) Using Multiple Knowledge Sources 
for Word Sense Discrimination. Computational 
Linguistics, 18/1, pp. 1-30 
Mooney R. (1996) Comparative Experiments on 
Disambiguating Word Senses: An Illustration of 
the Role of Bias in Machine Learning. In 
Proceedings of the Conference on Empirical 
Methods in Natural Language Processing, 
Philadelphia, PA, pp. 82-91 
Ng, H. T. and Zelle J. (1997) Corpus-Based 
Approaches to Semantic Interpretation in Natural 
Language Processing. AI Magazine, 18/4, pp. 
45-64 
Ohno S. and Hamanishi M. (1981) New Synonym 
Dictionary. Kadokawa Shoten, Tokyo 
Smadja F. (1993) Retrieving Collocations from Text: 
Xtract. Computational Linguistics, 19/1, pp. 
143-177 
Waltz D. L. and Pollack J. (1985) Massively Parallel 
Parsing: A Strongly Interactive Model of Natural 
Language Interpretation. Cognitive Science, 9, pp. 
51-74 
 
A Knowledge Based Approach to Identification of Serial Verb Construction in 
Chinese-to-Korean Machine Translation System 
 
Dong-il Kim?, Zheng-Cui, Jinji-Li??, Jong-Hyeok Lee 
Department Computer Science and Engineering, Electrical and Computer Engineering Division, 
Pohang University of Science and Technology (POSTECH)   
and Advanced Information Technology Research Center (AlTrc) 
San 31 Hyoja Dong, Pohang, 790-784, Korea 
E-mail: {dongil, cuizheng, ljj,jhlee}@postech.ac.kr 
 
                                                     ? Also an assistant professor at Yanbian University of Science  
& Technology (YUST) Yanji, Jilin, China. 
?? Also a lecturer at YUST 
Abstraction 
In Chinese language processing, the 
recognition and analysis for serial verb 
constructions (SVCs) is a fascinating research 
topic. Chinese language researchers each may 
have a different definition and interpretation of 
SVC since the structure of SVC makes Chinese 
unique to other languages and contains complex 
semantic and pragmatic information. This paper 
proposes a formal definition of SVC and a 
knowledge based approach for the recognition of 
SVCs, which is adopted in TOTAL-CK, a 
transfer-based MT system from Chinese to 
Korean. The recognition process is carried out in 
two stages: the analysis stage classifies SVCs 
into general categories, and the transfer stage 
performs further classification for Korean 
transfer. Some evaluation result for each stage 
was also given with statistics of each category of 
SVCs 
Introduction 
Many Chinese language researchers have paid 
special attention to the so-called ?serial verb 
constructions (SVCs)?, where two or more 
semantically or pragmatically related verb 
phrases or clauses are juxtaposed together 
without any functional marker. Because of 
different definitions and interpretations of SVCs 
among researchers, their categorizations differ 
according to researchers? viewpoints. 
In a Chinese to Korean machine translation, 
the hidden relation of the serial verbs should be 
expressed with some function words from the 
target language viewpoint. Moreover, the 
conceptual scope of these function words is 
different from the scope of SVC categorizations 
that are classified based on the viewpoint of the 
Chinese language itself. 
 In this paper, we proposes a different 
categorization of SVCs defined by the 
contrastive analysis of the two languages, and 
also an SVC identification method that is 
adopted in a Chinese-to-Korean MT system, 
TOTAL-CK. The TOTAL (Translator Of Three 
Asian Languages: Chinese, Korean and 
Japanese) project has been conducted under a 
hybrid strategy with transfer-based and 
example-based engines.  
1. Language Characteristics between 
Chinese and Korean 
In this section, some contrastive analyses of 
the two languages are introduced for better 
understanding of an SVC sentence. Since 
Chinese is an isolating language, morphological 
or syntactic markers rarely appear in a sentence, 
while in an agglutinative language such as 
Korean, these functional markers are not an 
optional unit but an obligatory unit in a sentence.  
An example is given in (1). Notice that the 
Korean alphabets are written with Yale 
Romanization in this paper. 
 
(1) ? ??  ???(ta kai-men jin-qu) 
Ku-nun  mwun-ul  yel-ko   duleka-nta. 
He-NOM  door-ACC open-CON get in-PRENT-DEC. 
He opens the door and enters (the room). 
In the Korean sentence, ko is a connective 
particle, and also nun, ul, and nta denote a topic 
auxiliary particle, an object case particle and 
declarative terminative ending, respectively. All 
these functional markers should be decided in 
the Korean transfer stage. Specifically, we 
require a process to select one from the possible 
conjugational markers when a Chinese SVC 
sentence is transferred to its Korean counterpart. 
2. Related Works 
A SVC is studied among several researchers 
as different names. But the general syntactic 
form is (NP) V1 (NP) V2 (NP)1. The variance of 
definition for SVC comes from the different 
scope of interpretation for the sentence pattern. 
 We will introduce three typical researches to 
clearly outline our definition of SVC.  The 
narrowest view of scope is suggested in (L?, 
1953). In his interpretation, V1 and V2 have the 
same subject and should be not coordinative, but 
it is difficult to decide which one is main or 
additional.  Zhu (Zhu, 1981) includes all cases 
of L??s and the possibility of adding an adjective 
to substitute for the second verb position. He 
also includes the case where an additional verb 
and a main verb are used, such as  V+? 
expression in V1 position, which indicates that 
V1 is additional and V2 is main. The broadest 
scope is proposed in (Li & Thompson, 1981). 
According to his interpretation, an SVC includes 
not only all the patterns noted above but also a 
pivot construction, a subject/object clause, and a 
coordinate clause, but excludes the pattern with 
an adjective in the V2 position. In this paper, the 
scope of SVC is almost same as Li?s but the 
classification of SVCs differs slightly, detailing 
the categorization in chapter 4.  
 A few computational solutions to identifying 
SVCs have been proposed by some researchers. 
A formal description is shown in  (Chan, 1998) 
using time lapse notation and the related 
definition. However, her method makes it 
difficult to computationally detect SVCs without 
the resources containing the deep level of 
analysis of each lexical, which is not obtainable 
in the current stage of language processing. 
                                                     
1 V1 : first verb, V2 : second verb, NP : noun phrase.  
3. Overview of TOTAL-CK System 
Architecture 
As a typical transfer system, TOTAL-CK 
consists of three parts: Chinese analysis, 
dependency tree transfer, and Korean generation.  
The system architecture of TOTAL-CK is shown 
in figure 1. The design principles and the detail 
descriptions are given in (Kim et al, 2002). 
 
  
 
 
 
 
 
 
 
 
 
Figure 1: TOTAL-CK system architecture 
4. Classification of Serial Verb 
Construction 
In the previous chapter, we mentioned the 
syntactic format of SVCs which is NP V1 (NP) 
V2 (NP) and the different scope of definition of 
SVCs by the Chinese language researches. To 
outline the scope of SVCs, we define SVCs in 
terms of dependency relation such that V1 is the 
head of V2, or V2 is the head of V1. It is 
formally defined as follows: 
Definition 1 
Let N represent a set of nodes in a dependency 
tree, and W a set of words. Further Let V be a 
set of verbs, and P be a set of all parts of speech 
in Chinese. Then the functions: head, nw, and 
npos, are defined as below: 
head(n) =hn where n?N and hn is the head of n 
nw(n) = w where n ?N and  w ?  W 
npos(n) = np where n ?N and  np ?P  
A definition of SVC is: 
Given a node n such that npos(n) ?  V and 
Head(n) = hn, 
If and only if npos(hn) ?  V and hn is the head of 
a given sentence then the sentence is a SVC.  
The three sentences from the top of table 1 
satisfy the given condition. Also the head of 
the node is the sentence head, thus these must 
be SVCs. For the  last sentence, nw(n) is ?
?, and nw(Head(n)) is ?? whose the POS  
is not verb and also whose the node is not the 
sentence head. Thus it is not an SVC. 
Sentence nw nwh SH SVC
?????? ? ? Yes Yes
?????????? ? ? Yes Yes
???????? ? ? Yes Yes
?????????
???1000?? 
?? ?? No No
Table 1: Example of Testing SVC 
Where nw: nw(n); nwh: nw(head(n)); SH :testing if 
head(n) is the sentence head ; n is a given node. 
 
Our definition is employed to recognize a 
SVC in the Chinese analysis stage. First we 
describe the classification that is used in the 
Chinese analysis stage.  
4.1 Categories in Chinese Analysis Stage 
All dependency relations, which are detected 
by the above definition, are classified into five 
categories: separate events, object, subject, 
pivotal construction and descriptive clauses, 
based on the classification of Li (Li & 
Thompson, 1981). 
4.1.1 Separate Events  
The serial verb patterns classified by most 
researchers belong to this group where switching 
V1 to V2 provides us a different meaning. In 
addition, we add the case where transposing V1 
to V2 provides us the same meaning in this 
group. 
4.1.2 Object 
If V2 is the main verb in an object clause or a 
object phrase then it belongs to this group. 
4.1.3 Subject 
If V1 is the main verb in the subject clause or 
subject phrase, it is assigned to this group. 
4.1.4 Pivot 
If the noun phrase between V1 and V2 is the 
object of V1 and the subject of V2, then it is a 
pivot construction. 
4.1.5 Descriptive  
 If V2 describes the noun phrase between V1 
and V2, then it is a descriptive SVC. 
 All categories of SVCs are shown in Table 2 
The corresponding Chinese dependency 
relations to object, subject and pivot 
constructions also appear in the some research in 
Chinese language processing (Zhou & Huang, 
1994) but the other two are not shown due to 
their different viewpoints. 
The descriptive construction is directly able to 
be one-to-one mapped to the Korean counterpart. 
However the separate event SVCs should be 
further classified for Korean transfer since the 
separate event SVC is possibly mapped into 
sentences with several different Korean 
conjunctional particles.  Thus, it is touched in 
the transfer stage. 
Category Example 
Separate Event ?????; ????????? 
Object ?????????? 
Subject ???????? 
Pivot ???????? 
Descriptive ??????????? 
Table 2: Examples of SVC category 
4.2 Subcategories in Transfer Stage 
The separate event SVC for each sense of 
Korean conjunctional particles is classified into 
the following subcategories: restrictive, 
quasi-coordinative, simultaneous, transitional, 
and circumstantial by the Korean language 
viewpoint. 
4.2.1 Restrictive  
 The action of V2 is performed under the 
restriction given by V1.  There are different 
types of restriction, such as space, group-related, 
causal, and instrumental. The examples are 
presented in table 3. 
Sentence V1 V2 R 
type 
????????????
??2? 
?? ?? space 
????????????? ?? ?? group
?????????? ?? ?? causal
????????????
??????? 
?? ?? tool 
Table 3: Examples of Restrictive Separate Events 
 
                                                     
2  The sentence can also be interpreted as purposive 
separate events. But it is included into a restrictive separate 
event SVC because it is impossible to detect the differences 
between restrictive and purposive, as this requires 
pragmatic level information 
 
4.2.2 Quasi-Coordinative 
In quasi-coordinative, two different cases 
exist. First, transposing V1 to V2 never causes a 
meaning shift of the sentence, named alternative. 
The other is that V1 and V2 are only 
sequentially related, called consecutive. 
4.2.3. Simultaneous 
In a simultaneous case, V1 and V2 occur at the 
same time. 
4.2.4 Transitional 
 If the action of V1 is interrupted by the action 
V2, then it is transitional. 
4.2.5 Circumstantial 
When V2 occurs on the condition of the action 
of V1, then it is classified as a circumstantial 
case.  
The examples for rests of the separate event 
are given in Table 4. 
Type Example 
Q-Coordinative ?????????(alternative)  
??????(consecutive) 
Simultaneous  ?????? 
Transitional ?????????? 
Circumstantial ????? 
Table 4: Examples of Separate Events 
 
In restrictive, quasi-coordinate, simultaneous, 
transitional, and circumstantial separate event 
SVC Chinese sentences, all the above verbs are 
mapped into the corresponding Korean verb 
followed by the Korean conjunctional particle 
?se?, ?ko?, ?un-chay-lo?, ?taka? and ?myen?, 
respectively. 
5. Identification of SVCs 
 To recognize SVCs, we divide the identifying 
process into two stages. The general categories 
of SVCs are able to be found at the analysis 
stage and the subcategories of a separated event 
SVC are detected in the transfer stage.  
5.1 Analysis Stage 
To recognize the five general categories of 
SVCs, two resources are used: one is the 
Grammatical Knowledge Base of Contemporary 
Chinese (GKBCC) and the other is a verb list 
with valency information (VLVI) (Zhu et al, 
1995).  Checking a verb in GKBCC allows us 
to simply detect a pivot SVC. The remainders of 
the other types of SVCs should be carefully 
handled. There are two possible ambiguous 
structures of SVCs 
Case 1 : NP V1 V2 (NP2) 
Case 2 : NP V1 NP1 V2 (NP2) 
Where NP, NP1 and NP2 are noun phrases. 
The algorithm for each case is illustrated in 
figure 2 and figure 3. In Figure 3, the test ?V1 
takes NP & VP?  means that the verb ?? can 
have a noun phrase or an object clause as an 
object. The test, ?satisfy valency?  denotes that  
the second verb  ?? takes a human subject, 
and  ??? can be the subject of the verb ??, 
thus it is classified as an object case. For the 
other sentence, since ?? cannot be the subject 
of the verb ??, it is determined as a subject 
case. 
 
  SVC sentence 
 
 
 Y Object SVC 
Ex) ???????   an object exists?
 
N  
Separate Event SVC
Ex) ??????? 
 
 
 
Figure 2: Algorithm of Detecting SVC for Case 1 
 
 
 
 
Subject SVC 
Ex) ??????????
?????? 
 SVC sentence 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
   Figure 3: Algorithm of Detecting SVC for Case 2 
Object SVC 
Ex) ??????????
Y V1 takes 
NP &VP 
N 
N 
Y 
 V2 takes VPas Sub. ?
N 
Separate Events SVC 
Ex) ???????????
Y 
Descriptive SVC 
Ex) ????????? 
????? 
Subject SVC 
Ex) ??????? 
?????????????
 V2 takes VP  
as Sub. ? 
N
Satisfy 
Valency ? 
Y 
 
5.2 Transfer Stage 
The simultaneous separate events is easily 
recognized by the lexical (?) attached to the first 
verb. Also, we use a simple heuristic to detect 
the circumstantial separate events with the 
lexical pattern information.  
The resource used in this stage is a Chinese 
thesaurus called Tongyi-ci-cilin (Mei, 1983). 
With the thesaurus the remainders of separate 
event SVCs are processed with great care. If V2 
is related to the interrupt concept then the 
transitional separate events are assigned. The 
most difficult and frequently occurring cases are 
the restrictive separate events and 
quasi-coordinative separate event.  
The key idea of using the thesaurus is based 
on the observation that the verb V2, if restricted 
by V1 makes it possible that the concept of V2 
will also be restricted by the concept of V1. To 
complete the solution, we first define the 
relations: RSTV, RSTL and RSTM as follows:  
Definition 2 
We define the relations: RSTV, RSTL, and 
RSTM, as follows: 
RSTV= {(V1,V2)  where V1 and V2 are the 
first verb and second verb in a given SVC 
sentence and V2 is semantically restricted by 
V1 : (V1,V2)  (V2,V1)} ?
RSTL= {(CL1,CL2) where CL1 and CL2 are 
the low level concept of the first verb and the 
low level concept 3  of second verb in the 
Chinese thesaurus, respectively, and CL2 is 
semantically restricted by CL1 : (CL1,CL2) ?  
(CL2,CL1)} 
RSTM= {(CM1, CM2) where CM1 and CM2 
are the middle level concept of the first verb and 
the middle level concept of second verb in the 
Chinese thesaurus, respectively, and ML2 is 
semantically restricted by ML1 : (ML1,ML2) ?  
(ML2, ML1)} 
The relations RSTV, RSTL, and RSTM are 
not symmetric and not reflexive. Based on the 
definition we derive the following heuristics: 
 if (V1,V2)?  RSRV then (CL1,CL2) ?RSTL 
But if (V1,V2) ? RSTV then not always 
(CM1,CM2) ?  RSTM. 
                                                     
3 The thesaurus consists of three levels of hierarchy. For 
example, H, Hj, and Hj20 correspond to the one of highest 
concept, the next narrow term called middle-level concept 
and the narrowest term called low-level concept, 
respectively. 
All three examples from the top of table 5 
satisfy the condition that, if (V1, V2)  RSTV 
then (CL1,CL2) 
?
?RSTL and (CM1,CM2) ?  
RSTM. If the condition is always true, then we 
use the middle-level concept relation for 
detecting a restrictive separate event in order to 
increase the applicability of our rules. Also, the 
data structure of RSTM is easily represented 
with an adjacent matrix with the size of 21*21 4 
(Sahni, 1998) where the matrix M is a square 
matrix, whose column and row are the 
middle-level concept, and if M(i,j) = 1 then 
concept j is semantically restricted by concept i, 
otherwise (i,j) ?RSTM. 
RSTV RSTL RSTM Example 
V1 V2 CL1 CL2 CM1 CM2
????????
???????
??  
?
?
?
?
Hj20 Hi21 Hj Hi
???????
????? 
?
?
?
?
Hj20 Hj12 Hj Hi
???????
????????
?? 
?
?
?
?
Hj36 Hj14 Hj Hi
 
 
???????
?????? 
?
?
?
?
Hi17 Hj20 Hi Hj
Table 5: Example of RSTV, RSTL and RSTM 
 
However, the last example reveals that the 
condition is not always true since we have the 
result, both (Hi,Hj) and (Hj,Hi) ?  RSTM.    
Thus, it violates the definition of RSTM. Hence, 
we may not directly use the middle-level concept 
adjacent matrix and the size of the low-level 
concept matrix is too large to be used.5  
 We come up with a solution of a frame with 
multi level concepts. The frame consists of three 
parts: the middle-level concept adjacent matrix, 
the low-level concept adjacent lists and the 
collocation serial verb list for detecting a serial 
verb that always appears together. 
Our solution is that the exceptional cases are 
covered by either the collocation verb lists or the 
low-level concept adjacent list. The remaining 
frequently occurring cases are captured by the 
middle-level adjacent matrix. This leads to the 
sparse matrix of the low-level concept which 
                                                     
4 The number of verbs related middle-level concept in the 
Chinese thesaurus is 21. 
5 The number of verbs related low-level concepts in the 
Chinese thesaurus is about 500. 
causes the adaptation of adjacent lists rather than 
an adjacent matrix for the low-level concepts. 
The order of searching the frame is the 
collocation list, the low-level concept list and the 
middle-level concept matrix. In the collocation 
list, if V1 and V2 belongs to the collocation list 
of the restrictive separate events, such as ???
? or the one of quasi-coordinative, such as ??
?? then the sentence is assigned to a restrictive 
case or a quasi-coordinative case, respectively. 
In the low-level concept lists and the 
middle-level concept matrix, if matching 
succeeds, which means that V2 is semantically 
restricted by V1, then a restrictive case is 
assigned; otherwise, a quasi-coordinate case is 
detected6. The detailed process for identifying 
the subcategories of separate events is shown in 
figure 4. 
6. Evaluation 
We randomly selected 1000 SVC sentences 
from 1998 people?s daily newspapers. The 
number of verbs in the sentence is two since our 
dependency parser is still being improved to 
detect the sentences with multiple embedding 
clauses. In table 6, the distribution of each type 
of SVC and the precision are shown. 
Type Frequency Percentage 
Separate events 402 40.2 
Object 479 47.9 
Subject 31 3.1 
Pivot 39 3.9 
Descriptive 1 0.1 
Error 56 5.6 
(Presicion:94.4%)
Total 1000 100 
Table 6: Distribution of Categories of SVC 
 
The precision is 94.4% and some of the errors 
occur from the tagger, thus some sentences are 
not SVCs. The rest of the errors result from 
missing information in the knowledge bases:  
                                                     
6  For a sentence ???????????  where the 
relation (Hj20,Hj12) is not in the low-level adjacent list, but 
(Hj,Hi) is 1 in the middle-level matrix, it is assigned to the 
restrictive case, while for the sentence ?????????
??? where (Hi17,Hj20) is in the low-level adjacent list, 
thus searching is stopped, it is assigned as a restrictive case. 
A sententence ????????  do not satisfy all 
conditions, thus it is detected as Quasi-Coordinate. 
 
 Separate Event SVC 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
Figure 4: Detection Algorithm of Subcategory of 
Separate Event SVC. 
 
GKBCC and VLVI. We need the complete list 
of verbs, which has a clause as a subject. These 
verbs in the list will be gradually collected in 
future works. 
The evaluation table for the separate event 
SVCs is provided in Table 7. 
Type Frequency Percentage 
Restrictive 153 38.5 
Quasi-coordinative 184 45.7 
Simultaneous 33 8.2 
Transitional 3 0.7 
Circumstantial 12 2.9 
Error 19 4.7 
(Precision:95.3%)
Total 402 100 
Table 7: Result of Separate Event 
 
N 
N
 V2 is Interrupt 
Concept ? 
 V1 & V2 in middle 
-level concept? 
 
N 
Restrictive 
Separate Events
Quasi-Coordinate Separate
Events 
N 
 V1 & V2 in low-  
level concept? 
N 
N 
 
In 
Restrictive ?
Y 
Y 
Y 
Y V1 & V2 in 
Collocation list?  
In Quasi 
Coordinate?
Y 
Y Simultaneous 
Separate Events
 V1 with ? ?
N 
Circumstantial
Separate Events
Y  Match lexical
Pattern ? 
N 
Transitional 
Separate Events
Y 
The precision of identifying the category of 
separate event is 95.3%. The errors resulted from 
a circumstantial case since our heuristics is too 
restrictive to detect all cases, thus, it might be 
revised further, and since the low-level concept 
lists are not completed.  The low-level concept 
lists will be continuously updated for increasing 
coverage in the tuning stage of the machine 
translation system. 
Table 8 shows the distribution of the 
subcategory of restrictive separate events for 
Korean transfer. 
Type Frequency Percentage
Space 86 56.2 
Group-related 38 24.8 
Causal 17 11.1 
Instrumental 12 7.9 
Total 153 100 
Table 8: Category of Restricted Separated Event 
 
In table 9, the frequency for each type of 
accessed resource is listed. Notice that most 
restrictive separate event SVCs are recognized in 
the middle-level matrix. The two cases in 
collocation are all the case of quasi-coordinative, 
thus, the total number is greater than 153. 
Type of accessed 
resource 
Frequency Percentage
Middle-level matrix 121 78.0 
Low-level list 32 20.6 
Collocation list 2 1.29 
Total 155 100 
Table 9: Access Frequencies for Resource Type 
 
 
 
 
 
 
 
 
Figure 5: Demo system of TOTAL-CK 
 
In figure 5, a demo system of TOTAL-CK is 
illustrated. For a given Chinese SVC sentence 
displayed in the top position of the right-most 
window, the corresponding Korean sentence is 
followed in the next row.  The tagged results, 
the segment of chunking, and the Chinese 
dependency tree with indentation are shown in 
each window from left to right.  
Conclusion and Future work 
In this paper, we formally define serial verb 
constructions, and classified the SVC into 
several categories. These categories are related 
to the analysis stage and the transfer stage of 
TALK-CK. We provided a resolution algorithm 
detecting SVCs in each step. Finally, at each 
stage, a promising experimental result is shown.  
Further research must help to better resolve 
the conditional separate event SVC and 
purposive separate event SVC.  
Acknowledgements 
This work was supported by the Korea 
Science and Engineering Foundation 
(KOSEF)  through the Advanced Information 
Technology Research Center(AITrc). 
References  
Chan  Y. W. (1998) Formal Criteria for interpreting 
Chinese Serial Verb Constructions. 
Communications of COLIPS 8(1) ,pp.13-29. 
Kim D.I., Cui Z, Li J.J. and Lee J.H. (2002). 
Resolving Structural Transfer Ambiguity in 
Chinese-to Korean Machine Translation. 2002 
International Conference on Chinese Language 
Computing, Taichung, Taiwan. 
Li C  N. , Thompson S A. (1981), Mandarin 
Chinese: A functional reference grammar. 
University of California Press, USA. 
L? S. X.(1953) Yufa Xuexi (The Study of Chinese 
Grmmar). Bejing, Zhungguo Qingnian  Press. 
Mei J.J.(1983) Chinese Thesaurus 
(Tong-Yi-Ci-Ci-lin). Shanghai Cishu Press. 1983. 
Sahni, S.(1998) Data structures, algorithms, and 
applications in C++. Boston McGraw-Hill, USA.  
Zhou M and Huang C. (1994) Approach to the 
Chinese Dependency Formalism for the Tagging of 
Corpus. Journal of Chinese Information Processing, 
8/3, pp. 35-52, 1994 
Zhu D.X.(1981), Yufa Jiangyi (Lectures on Chinese 
Grammar),Bejing, Xiangwu Press. 
Zhu X. F, Yu S. W and Wang H. (1995) The 
Development of Contemporary Chinese 
Grammatical Knowledge Base and its Applications. 
Communications of COLIPS, 5/1-2 
Segmentation of Chinese Long Sentences Using Commas 
Mei xun Jin1, Mi-Young Kim2, Dongil Kim3 and Jong-Hyeok Lee4 
Graduate School for Informa-
tion Technology1, 
Div. of Electrical and 
Computer Engineering24,
Pohang University of Science and Technology 
Advanced Information Technology Research Center(Altrc)
{meixunj1,colorful2,jhlee4}@ 
postech.ac.kr 
Language Engineering Institute3
Div. of Computer, Electronics 
and Telecommunications 
Yanbian University of Science 
and Technology 
dongil@ybust.edu.cn 
 
 
 
 
Abstract 
The comma is the most common form of 
punctuation. As such, it may have the 
greatest effect on the syntactic analysis of a 
sentence. As an isolate language, Chinese 
sentences have fewer cues for parsing. The 
clues for segmentation of a long Chinese 
sentence are even fewer. However, the av-
erage frequency of comma usage in Chi-
nese is higher than other languages. The 
comma plays an important role in long 
Chinese sentence segmentation. This paper 
proposes a method for classifying commas 
in Chinese sentences by their context, then 
segments a long sentence according to the 
classification results. Experimental results 
show that accuracy for the comma classifi-
cation reaches 87.1 percent, and with our 
segmentation model, our parser?s depend-
ency parsing accuracy improves by 9.6 per-
cent.  
 
1 Introduction 
Chinese is a language with less morphology and 
no case marker. In Chinese, a subordinate clause or 
coordinate clause is sometimes connected without 
any conjunctions in a sentence. Because of these 
characteristics, Chinese has a rather different set of 
salient ambiguities from the perspective of statisti-
cal parsing (Levy and Manning, 2003). In addition, 
the work for clause segmentation is also rather dif-
ferent compared with other languages.  
    However, in written Chinese, the comma is used 
more frequently (Lin, 2000). In English, the average 
use of comma per sentence is 0.869 (Jones, 1996a)1 
~1.04(Hill, 1996), and in Chinese it is 1.792, which 
is one and a half to two more times as it is used in 
English. In Korean, the comma is used even less 
than it is in English (Lin, 2000). 
   Since Chinese has less morphology and no case 
marker, and the comma is frequently used, the 
comma becomes an important cue for long Chinese 
sentence parsing. Because more commas may ap-
pear in longer sentences, the necessity of analyzing 
the comma also increases.  
    Some handbooks about standard Chinese gram-
mars list ten to twenty uses of the comma, accord-
ing to the context. Among these uses, is occurrence 
at the end of a clause3 in a sentence (Lin, 2000). 
About 30% of commas are used to separate the 
clause from its main sentence or neighbor clause(s). 
If the comma appears at the end of a clause, the 
position can naturally be set as the clause segmen-
tation point.  
     This paper proposes Chinese long sentence 
segmentation by classifying the comma. In section 
2, related work in clause segmentation and punc-
tuation processing is presented. Comma classifica-
tion criteria are then introduced, and the 
classification model follows. Afterwards, some 
experimental results show how the proposed 
comma classification and long sentence segmenta-
tion are effective in Chinese parsing. Finally, a 
conclusion will be given.  
                                                          
1 The frequency of comma per sentence is calculated as = Total frequency of 
commas/(Total frequency of full stop + Total frequency of Question mark),  
based on the punctuation statistics of Jone?s Phd. thesis P56,57.  
2 The calculation is based on People?s Daily Corpus?98. 
3 Clause in this paper, is a predicate with its full complements, subject, object(s). 
According to the type of a predicate and the context, subject or object may or 
may not appear. Adjunct of the predicate may or may not be included in the 
clause.  
 
 
 
 
?????????????  
Figure1: example of a dependency relation  
 
 
 
??????????????????? 
 
 
 
Figure2: example of a dependency relation  
 
 
 
 
???????????????? 
 
 
Figure 3: example of a dependency relation 
Examples:  
(1) ????????????? 
They have class in the morning and do experiments in 
the afternoon. 
(2) ??????????????????? 
Several years ago,  BeiHai City was only an unknown 
small fishing village. 
(3) ???????????????? 
The students prefer young and beautiful teachers.  
(4) ?????????????? 
Xiao Ming is doing homework and his mom is 
knitting. 
(5) ?????????????? 
Though he studies very hard, his score is not satisfiable.   
(6) ??????????????????????
???? 
The change of domestic economic development in 
Russia has promoted the trade exchange between two 
countries. 
(7) ???????????????????? 
Bank of China invited a Japanese company as its con-
soler last October.  
(8) ????????????????????? 
He is a good leader in the company as well as a good 
daddy at home. 
(9) ?????????????????????
?? 
The quick transfer of the scientific research achieve-
ment to industry is the characteristic of this develop-
ment district. 
(10) ??????????????? 
The students happily come to the playground. 
(11) ??????????????????????
?????????? 
The investment from Korea to DaLian city has grown 
for three years, and all Korean investment companies 
in DaLian  receive preferential treatment. 
(12) ????????????????????   
The statistics show that the exportation from DaLian to 
Korea is reach to USD100,000,000. 
(13) ??????????????????????
?? 
In 1994, TongYong Company purchased goods worthy 
of more than  USD40,000,000.  
(14) ???????????????? 
She gets up early, and does physical exercise every 
morning. 
(15) ?????????????????????
?? 
The occupation of the first products is less than 3/10 
and the portion of the second ones is more than 7/10.   
2 Related Work  
2.1 Related Work for Clause Segmenta-
tion 
Syntactic ambiguity problems increase drasti-
cally as the input sentence becomes longer. Long 
sentence segmentation is a way to avoid the prob-
lem. Many studies have been made on clause seg-
mentation (Carreras and Marquez, 2002, Leffa, 1998, 
Sang and Dejean,2001). In addition, many studies 
also have been done on long sentences segmenta-
tion by certain patterns (Kim and Zhang, 2001, Li and 
Pei, 1990, Palmer and Hearst, 1997).  
However, some researchers merely ignore punc-
tuation, including the comma, and some research-
ers use a comma as one feature to detect the 
segmentation point, not fully using the information 
from the comma.  
2.2 Related Work for Punctuation Proc-
essing  
Several researchers have provided descriptive 
treatment of the role of punctuations: Jones (1996b) 
determined the syntactic function of the punctua-
tion mark. Bayraktar and Akman (1998) classified 
commas by means of the syntax-patterns in which 
they occur. However, theoretical forays into the 
syntactic roles of punctuation were limited. 
Many researchers have used the punctuation 
mark for syntactic analysis and insist that punctua-
tion indicates useful information. Jones (1994) suc-
cessfully shows that grammar with punctuation 
outperforms one without punctuation. Briscoe and 
Carroll 1995) also show the importance of punc-
tuation in reducing syntactic ambiguity. Collins 
(1999), in his statistical parser, treats a comma as an 
important feature. Shiuan and Ann (1996) separate 
complex sentences with respect to the link word, 
including the comma. As a result, their syntactic 
parser performs an error reduction of 21.2% in its 
accuracy.  
(
                                                          
Say (1997) provides a detailed introduction to us-
ing punctuation for a variety of other natural lan-
guage processing tasks.  
All of these approaches prove that punctuation 
analyses improve various natural language process-
ing performance, especially in complex sentence 
segmentation. 
3 Types of Commas  
The comma is the most common punctuation, 
and the one that might be expected to have the 
greatest effect on syntactic parsing. Also, it seems 
natural to break a sentence at the comma position 
in Chinese sentences. The procedure for syntactic 
analysis of a sentence, including the segmentation 
part, is as follows: 
1st step: segment the sentence at a comma 
2nd step: do the dependency analysis for each 
segment 
3rd step: set the dependency relation between  
segment pairs  
In Chinese dependency parsing, not all commas 
are proper as segmentation points.  
First, segmentation at comma in some sentences, 
will cause some of the words fail to find their heads. 
Figure 2 shows, in example (2), there are two 
words, ??  (BeiHai City)  and ?  (preposition) 
from the left segment have dependency relation 
with the word ?(is) of the right segment. So, the 
segmentation at comma , will cause two of  words 
?? (BeiHai City)  and ? (preposition) in the left 
segment, cannot find their head in the second step 
of syntactic parsing stage.  
Second, segmentation at commas can cause 
some words to find the wrong head. Example (3) of 
figure 3 shows two pairs of words with dependency 
relations. For each pair, one word is from the left 
segment, and one word is from the right segment :
?? (like) from the left segment and ??(teacher)  
from the right, ?? (young) from the left and ? 
(of) from the right. Segmentation at the comma 
will cause the word ??(young) to get the word ?
? (like) as its head, which is wrong. 
Example (2) and (3) demonstrate improper sen-
tence segmentation at commas. In figure 2 and fig-
ure 3, there are two dependency lines that cross 
over the commas for both sentences. We call these 
kinds of commas mul_dep_lines_cross comma 
(multiple lines cross comma). In figure 1, there is 
only one dependency line cross over the comma. 
We call these kinds of commas one_dep_line_cross 
comma.    
Segmentation at one_dep_line_cross comma is 
helpful for reducing parsing complexity and can 
contribute to accurate parsing results. However, we 
should avoid segmenting at the position of 
mul_dep_lines_cross comma. It is necessary to 
check each comma according to its context.  
3.1 Delimiter Comma and Separator 
Comma 
Nunberg (1990) classified commas in English 
into two categories, as a delimiter comma and a 
separator comma, by whether the comma is used to 
separate the elements of the same type 4  or not. 
While a delimiter comma is used to separate differ-
ent syntactic types, a separator comma is used to 
separate members of conjoined elements. The 
commas in Chinese can also be classified into these 
two categories. The commas in example (3) and (4) 
are separators, while those in (2) and (5), are 
delimiters. 
However, both delimiter comma and separator 
commas can be mul_dep_line_cross commas. In 
example (2), the comma is a delimiter comma as 
well as a mul_dep_line_cross comma. As a separa-
tor comma, the comma in example (3), is also a 
mul_dep_line_cross comma. Nunberg?s classifica-
tion cannot help to identify mul_dep_line_cross 
commas. 
We therefore need a different kind of classifica-
tion of comma. Both delimiter comma and separa-
tor comma can occur within a clause or at the end 
of a clause. Commas that appear at the end of a 
clause are clearly one_dep_line_cross commas. 
The segmentation at these kinds of comma is valid. 
4 Same type means that it has the same syntactic role in the sentence, it can be a 
coordinate phrase or coordinate clause. 
3.2 Inter-clause Comma and Intra-clause 
Comma 
Commas occurring within a clause are here 
called intra-clause commas. Similarly, commas at 
the end of a clause will be called inter-clause 
commas. Example  (2), (3) include intra-clause 
commas, and example (4), (5) include inter-clause 
commas. 
3.2.1 Constituents of the Two Segments 
Adjoining a Comma 
A segment is a group of words between two 
commas or a group of words from the beginning 
(or end) of a sentence to its nearest comma. 
To identify whether a comma is an inter-clause 
comma or an intra-clause comma, we assign values 
to each comma. These values reflect the nature of 
the two segments next to the comma. Either the left 
or right segment of a comma, can be deduced as a 
phrase5, or several non-overlapped phrases, or a 
clause.(see examples (6)~(15)). The value we as-
sign to a comma is a two-dimensional value 
(left_seg, right_seg). The value of left_seg and 
right_seg can be p(hrase) or c(lause), therefore the 
assigned value for each comma can be (p,p), (p,c), 
(c,p) or (c,c).  
 Commas with (p,p) as the assigned value, in-
clude the case when the left and right segment of 
the comma can be deduced as one phrase, as shown 
in example (6) or several non-overlapped phrases, 
as described in example (7).  
We can assign the value of (c,p) to commas in 
example (8), (9) and (10),  indicating the left ad-
joining segment is a clause and the right one is a 
phrase or several non-overlapped phrases. In a 
similar way, commas in example (11)~(13) are 
case of (p,c). 
 If a comma has (c,c) as the assigned value, both 
the left segment and the right segment can be de-
duced as a clause. The relation between the two 
clauses can be coordinate (example (14)) or subor-
dinate (example (15)).  
                                                          
5 Phrase is the group of words that can be deduced as the phrase in Chinese Penn 
Tree Bank 2.0. A phrase may contain an embedded clause as its adjunct or 
complement.  
(a), ??????????? 
(b) ????????? 
In example (a) ,the PP has the embedded clause as its complement. And in 
example (b), the embedded clause is the adjunct of the NP. 
3.2.2 Syntactic Relation between Two Ad-
joining Segments 
A word (some words) in the left segment and a 
word (some words) in the right segment of a 
comma may or may not have a dependency rela-
tion(s). For a comma, if at least one word from the 
left segment has a dependency relation with a word 
from the right segment, we say the left segment and 
the right segment have a syntactic relation. Other-
wise the two segments adjoining the comma have 
no syntactic relations. Rel() functions are defined 
in table-1. 
Table 1: functions Rel(), Dir() and Head() 
Rel() 
? To check if any words of the left segment has a 
dependency relation with the word of the right 
segment. 
? If there is, Rel()=1  
Otherwise Rel()=0. 
Dir() 
? To indicate how many direction(s) of the de-
pendency relations the left and right segment 
have. when Rel()=1. 
? For one_dep_line_cross comma, Dir()=1. 
? For mul_dep_line_cross comma, if the directions 
of the dependency relations are the same, 
Dir()=1, else Dir()=2. 
Head() 
? To indicate which side of segment contains the 
head of any words of the other side, when 
Rel()=1. 
? When Dir()=1, if the left segment contains any 
word as the head of a word of the right, Head() = 
left; Otherwise Head()=right. 
? When Dir()=2,   
1. According to the direction of dependency 
relation of these two segments, to find the 
word which has no head. 
2. If the word is on the left, Head()=left, other-
wise, Head()=right.  
 
For the one_dep_line_cross comma, the left and 
right segments have syntactic relation, and only 
one word from a segment has a dependency rela-
tion with a word from the other segment. For 
mul_dep_line_cross comma, at least two pairs of 
words from each segment have dependency rela-
tions. We then say that the left and right segments 
adjacent to the comma have multiple dependency 
relations. The directions of each relation may differ 
or not. We define a function Dir() as follows : if all 
the directions of the relations are the same, get 1 as 
its value, else 2 for its value. This is in table-1. We 
also define function Head() to indicate whether the 
left segment or the right segment contains the head 
word of the other when the two segments have syn-
tactic relation. This is also shown in table 1. 
In example (3) as figure 3 shows, Rel()=1, 
Dir()=2 and Head()=left. 
3.2.3 Inter-clause Comma and Intra-
clause Comma  
For commas assigned values  (p,p) or (c,c), the 
function Rel() is always 1. Commas with values (c, 
p) or (p,c) can be further divided into two sub-cases. 
Table 2 shows the sub-case of (c,p), and table 3 
shows the sub-cases of (p,c). 
 
Rel() =0 The 2nd comma of Example (8); 
Example (9); 
Head() =right = p 
(c,p)-
I 
Rel()=1 Example (10); 
Head() = left=c 
(c,p)-
II 
Table 2: sub-cases of commas with value of (c,p) 
 
Rel()=0 Example (11); 
Example (12); 
Head() =left = p 
(p,c)-
I 
Rel()=1 Example (13); 
Head() = right=c 
(p,c)-
II 
Table3: sub-cases of commas with value of (p,c) 
 
Commas with the value of (p,p), (c,p)-II and 
(p,c)-II are used to connect coordinate phrases or to 
separate two constituents of a clause. These com-
mas are intra-clause commas. 
Commas with (c,c), (c,p)-I and (p,c)-I are used 
as a clause boundaries. These are inter-clause 
commas. 
An inter-clause comma joins the clauses together 
to form a sentence. The commas that belong to an 
inter-clause category are safe as segmentation 
points (Kim, 2001).   
4 Feature Selection  
To identify the inter-clause or intra-clause role 
of a comma, we need to estimate the right and left 
segment conjuncts to the comma, using informa-
tion from both segments. Any information to iden-
tify a segment as a clause or a phrase or phrases is 
useful. Carreras and Marquez (2001) prove that 
using features containing relevant information 
about a clause leads to more efficient clause identi-
fication. Their system outperforms all other sys-
tems in CoNLL?01 clause identification shared task 
(Sang & Dejean, 2001). Given this consideration, we 
select two categories of features as follows. 
 (1) Direct relevant feature category: predicate 
and its complements. 
 (2) Indirect relevant feature category: auxiliary 
words or adverbials or prepositions or clausal 
conjunctions. 
Directly relevant features 
VC: if a copula ? appears 
VA: if an adjective appears 
VE: if ? as the main verb appears 
VV: if a verb appears 
CS: if a subordinate conjunction appears 
Table 4: feature types for classification 
Indirectly relevant features 
AD: if an adverb appears 
AS: if an aspect marker appears 
P: if a preposition appears 
DE: if ? appears 
DEV:if ? appears  
DER: if ? appears 
BA_BEI: if ? or ? appears 
LC: if a localizer appears 
FIR_PR : if the first word is a pronoun 
LAS_LO: if the last word is a localizer 
LAS_T : if the last word is a time 
LAS_DE_N : if the last word is a noun that follows ? 
No_word : if the length of a word is more than 5 
no_verb: if no verb(including VA)  
DEC: if there is relative clause 
ONE: if the segment has only one word 
 
To detect whether a segment is a clause or 
phrase, the verbs are important. However, Chinese 
has no morphological paradigms and a verb takes 
various syntactic roles besides the predicate, with-
out any change of its surface form. This means that  
information about the verb is not sufficient, in itself, 
to determine whether segment is a clause.  
When the verb takes other syntactic roles besides 
the predicate, it?s frequently accompanied by func-
tion words. For example, a verb can be used as the 
complement of the auxiliary word ? or ?(Xia, 
2000), to modify the following verb or noun. In 
these cases, the auxiliary words are helpful for de-
ciding the syntactic role of the verb. Other function 
words around the verb also help us to estimate the 
syntactic role of the verb. Under this consideration, 
we employ all the function words as features, 
where they are composed as the indirect relevant 
feature category.  
Table 4 gives the entire feature set. The label of 
each feature type is same as the tag set of Chinese 
Penn Treebank 2.0 (see Xia (2000) for more de-
tailed description). If the feature appears at the left 
segment, we label it as L_feature type, and if it is 
on the right, it?s labeled as R_ feature type, where 
feature type is the feature that is shown on table 4.  
The value for each feature is either 0 or 1. When 
extracting features of a sentence, if any feature in 
the table 4, appears in the sentence, we assign the 
value as 1 otherwise 0. The features of example (12) 
are extracted as table 5 describes. All of these val-
ues are composed as an input feature vector for 
comma classification. 
Table 5: the extracted features of example (12)  
5 Experiments 
For training and testing, we use the Chinese 
Penn Treebank 2.0 corpus based on 10-fold valida-
tion. First, using bracket information, we extract 
the type (inter-clause comma or intra-clause 
comma) for each comma, as we defined. The ex-
tracted information is used as the standard answer 
sheet for training and testing. 
 We extract the feature vector for each comma, 
and use support vector machines (SVM) to perform 
the classification work.  
Performances are evaluated by the following 
four types of measures: accuracy, recall, F?=1/2 for 
inter-clause and intra-clause comma respectively, 
and total accuracy. Each evaluation measure is cal-
culated as follows. 
Inter(or intra)-clause comma accuracy6 =  
 
identifiedofnumberthe
identifiedcorrectlyofnumberthe  
Inter(or intra)-clause comma recall  = 
 
classtheofnumbertotal
identifiedcorrectlyofnumberthe  
Inter(or intra)-clause comma F ?=1/2 = 
recall) comma clauseintra)inter(or              
precision comma clauseintra)(inter(or 
)recallcommaclauseintra)inter(or               
precisioncommaclauseintra)inter(or (2
?
+?
?
???
 
Total accuracy = 
 
commasofnumbertotal
identifiedcorrectlyofnumbertotal  
5.1 Classification Using SVM 
 Support vector machines (SVM) are one of the 
binary classifiers based on maximum margin strat-
egy introduced by Vapnik (Vapnik, 1995). For many 
classification works, SVM outputs a state of the art 
performance.  
L_VC =0 L_VA =0 L_VE =0  
R_VC = 0 R_VA =0 R_VE =0 
L_VV = 0 L_CS =0 L_AD =0 
R_VV =1 R_CS =0 R_AD =0 
L_AS =0 L_P =0  L_DE =0 
R_AS =1 R_P =0 R_DE =1 
L_DEV =0 L_DER = 0 L_BA_BEI =0 
R_DEV =0 R_DER =0 R_BA_BEI=0 
L_LC =0 L_DEC = 0 L_FIR_PR  =0 
R_LC =0 R_DEC =0 R_FIR_PR=0  
L_LAS_LO =0 L_LAS_T =1 L_LAS_DE_N=0 
R_LAS_LO=0 R_LAS_T=0 R_LAS_DE_N=1 
L_No_word=0 L_no_verb =1 L_ONE = 0 
R_No_word =1 R_no_verb =0 R_ONE =0 
There are two advantages in using SVM for clas-
sification: 
(1) High generalization performance in high di-
mensional feature spaces. 
(2) Learning with combination of multiple fea-
tures is possible via various kernel functions.  
Because of these characteristics, many research-
ers use SVM for natural language processing and 
obtain satisfactory experimental results (Yamada, 
2003). 
In our experiments, we use SVMlight (Joachims, 
1999) as a classification tool. 
5.2 Experimental Results 
First, we set the entire left segment and right 
segment as an input window. Table 6 gives the per-
formance with different kernel functions. The RBF 
kernel function with ? =1.5 outputs the best per-
formance. Therefore, in the following experiments, 
we use this kernel function only.  
Next, we perform several experiments on how 
the selection of word window affects performance. 
First, we select the adjoining 3 words of the right 
and left segment each, indicated as win-3 in table 7. 
                                                          
6 The inter-clause comma precision is abbreviated  as inter-P. Same way, Inter-R 
for inter-clause comma recall, ..etc. 
Second, we select the first 2 words and last 3 words 
of the left segment and the first 3 and last 2 of the 
right segment, indicated as win 2-3 in table 7. Fi-
nally, we use the part of speech sequence as input. 
As the experimental results show, the part of 
speech sequence is not a good feature. The features 
with clausal relevant information obtain a better 
output. We also find that the word window of first 
2-last 3 obtains the best total precision, better than 
using the entire left and right segments. From this, 
we conclude that the words at the beginning and 
end of the segment reveal segment clausal informa-
tion more effectively than other words in the seg-
ment. 
5.3 Comparison of Parsing Accuracy with 
and without Segmentation Model 
The next experiment tests how the segmentation 
model contributes to parsing performance. We use 
a Chinese dependency parser, which was imple-
mented with the architecture presented by Kim 
(2001) presents.  
After integrating the segmentation model, the 
parsing procedure is as follows: 
- Part of speech tagging. 
- Long sentence segmentation by comma. 
- Parsing based on segmentation. 
Table 9 gives a comparison of the results of the 
original parser with the integrated parser.  
5.4 Comparison with Related Work  
Shiuan and Ann?s (1996) system obtains the 
clues for segmenting a complex sentence in Eng-
lish by disambiguating the link words, including 
the comma. The approach to find the segmentation 
point by analyzing the specific role of the comma 
in the sentence seems similar with our approach. 
However, our system differs from theirs as follows: 
(1) Shiuan and Ann?s system sieves out just two 
roles for the comma, while ours gives an 
analysis for the complete usages of the 
comma. 
(2) Shiuan and Ann?s system also analyzes the 
clausal conjunction or subordinating preposi-
tion as the segmentation point. 
Although the language for analysis is different, 
and the training and testing data also differ, the 
motivation of the two systems is the same. In addi-
tion, both systems are evaluated by integrating the 
original parser. The average accuracy of comma 
disambiguation in Shiuan and Ann?s is 93.3% that 
is higher than ours by 6.2%. However, for parsing 
accuracy, Shiuan and Ann?s system improves by 
4%(error reduction of 21.2%), while ours improves 
by 9.6 percent. 
 
Kernel 
function Inter-P Inter-R Intra-P Intra-R Inter-F Intra-F Total-P
linear 
74.22
% 
77.87
% 
72.52
% 
70.61
% 
76.00
% 
71.56
% 
73.14
% 
Polynomial 
d=2 
79.84
% 
81.15
% 
84.51
% 
83.77
% 
80.49
% 
84.14
% 
82.86
% 
Polynomial 
d=3 
78.57
% 
81.15
% 
88.39
% 
86.84
% 
79.84
% 
87.61
% 
84.86
% 
RBF    ? = 0.5 78.46% 83.61% 88.64% 85.53% 80.95% 87.05% 84.86% 
RBF  ? = 1.5 78.69% 78.69% 89.04% 89.04% 78.69% 89.04% 85.43% 
RBF  ? = 2.5 80.62% 85.25% 88.24% 85.53% 82.87% 86.86% 85.43% 
RBF ? = 3.5 79.41% 88.52% 85.05% 79.82% 83.72% 82.35% 82.86% 
Table 6: experimental results with  
different kernel functions 
 
Word Win-
dow Inter-P Inter-R Intra-P Intra-R Inter-F Intra-F Total-P
Win3 
80.45
% 
87.70
% 
84.33
% 
80.26
% 
83.92
% 
82.25
% 
82.86
% 
Win2-3 
85.60
% 
87.70
% 
88.00
% 
86.84
% 
86.64
% 
87.42
% 
87.14
% 
Table 7: experimental results for  
word window size  
 
 Inter-P Inter-R Intra-P Intra-R Inter-F Intra-F Total-P
POS 
sequence 
75.42
% 
72.95
% 
80.60
% 
82.02
% 
74.17
% 
81.30
% 
78.86
% 
Table 8: experimental results for using  
part of speech sequence  
 
 Original 
parser 
Integrated 
parser 
Average dependency pars-
ing accuracy7 
73.8% 83.4% 
Average complete sentence 
accuracy 
23.8% 25.4% 
Table 9: comparison of parsing accuracy of the 
original parser with the integrated parser 
                                                          
7 The evaluation measures are used as it is defined in Kim (2001). 
6 Conclusion 
In this paper, we propose a method to segment a 
Chinese sentence by classification of the comma.  
We define the criteria for classification, and ac-
cording to the criteria, a model for classification of 
the comma is given. The segmentation at the 
comma position seems to be efficient for improv-
ing the accuracy of dependency parsing by 
9.6percent. Moreover, since commas more fre-
quently appear in Chinese language, we expect our 
approach including salient and refined analysis of 
comma usages provides feasible solutions for seg-
mentation. 
However, the accuracy for the segmentation is 
not yet satisfactory. Since erroneous segmentation 
may cause a parsing failure for the entire sentence, 
errors can be serious. Further research should be 
done to improve the performance and reduce side 
effects for parsing the entire sentence.  
Acknowledgments 
This work was supported by the KOSEF through 
the Advanced Information Technology Research 
Center (AITrc), and by the BK21 Project. 
References 
M. Bayparktar,  B. Say and V. Akman 1998, An analysis 
of English punctuation: the special case of comma, 
International Journal of Corpus Linguistics, 1998 
X. Carreras, L. Marquez, V. Punyakanok, and D. Roth 
2002, Learning and inference for clause identification, 
Proceeding of 13th European Conference on Machine 
Learning, Finland, 2002 
R.L. Hill 1996, A comma in parsing: A study into the 
influence of punctuation (commas) on contextually 
isolated "garden-path" sentences.  M.Phil disseration, 
Dundee University, 1996 
T.Joachims 1999, Making large-Scale SVM Learning 
Practical. Advances in Kernel Methods - Support 
Vector Learning, B. Sch?lkopf and C. Burges and A. 
Smola (ed.), MIT-Press, 1999 
B. Jones 1994, Exploring the role of punctuation in pars-
ing natural text, Proceedings of COLING-94, pages 
421-425 
B. Jones 1996a, What?s the point? A (computational)  
theory of punctuation, PhD Thesis, Centre for Cogni-
tive Science, University of Edinburgh, Edinburgh, 
UK, 1996 
B. Jones 1996b, Towards testing the syntax of punctua-
tion, Proceeding of 34th ACL, 1996 
M.Y.Kim, S.J. Kang, J.H. Lee 2001, Resolving ambigu-
ity in Inter-chunk dependency parsing, Proceedings 
of the sixth Natural Language Processing Pacific Rim 
Symposium, Tokyo, Japan, 2001 
S. Kim, B.Zhang and Y. Kim 2001, Learning-based 
intrasentence segmentation for efficient translation of 
long sentences, Machine Translation, Vol.16, no.3, 
2001 
Roger Levy and Christopher Manning. 2003. Is it harder 
to parse Chinese, or the Chinese Treebank? In 
Proceeding of ACL-2003. 
V.J. Leffa 1998, clause processing in complex sentences, 
Proceeding of 1st International Conference on Lan-
guage Resources and Evaluation, Spain,1998 
W.C. Li, T.Pei, B.H. Lee and Chiou, C.F. 1990, Parsing 
long English sentences with pattern rules, Proceeding 
of 13th International Conference on Computational 
Linguistics, Finland, 1990 
Shui-fang Lin 2000. study and application of punctua-
tion(??????????). People?s Publisher, 
P.R.China. (in Chinese) 
Geoffrey Nunberg 1990. the linguistics of punctua-
tion .CSLI lecture notes. 18, Stanford, California. 
D.D. Palmer and M.A. Hearst 1997, Adaptive multilin-
gual sentence boundary disambiguation, Computa-
tional Linguistics, Vol.27, 1997 
E.F.T.K. Sang and H.Dejean. 2001, Introduction to the 
CoNLL-2001 shared task: clause identification, Pro-
ceeding of CoNLL-2001 
B. Say and V. Akman 1997, current approaches to 
punctuation in computational linguistics, Computers 
and the Humanities, 1997 
P.L. Shiuan and C.T.H. Ann 1996, A divide-and-
conquer strategy for parsing, Proceedings of the 
ACL/SIGPARSE 5th international workshop on pars-
ing technologies, Santa Cruz, USA, pp57-66 
Fei Xia 2000, The bracketing Guidelines for the Penn 
Chinese Treebank(3.0) 
Vladimir N Vapnik 1995 The nature of statistical learn-
ing theory. New York, 1995 
Term Extraction from Korean Corpora via Japanese
Atsushi Fujii, Tetsuya Ishikawa
Graduate School of Library,
Information and Media Studies
University of Tsukuba
1-2 Kasuga, Tsukuba
305-8550, Japan
{fujii,ishikawa}@slis.tsukuba.ac.jp
Jong-Hyeok Lee
Division of Electrical and
Computer Engineering,
Pohang University of Science and Technology,
Advanced Information Technology Research Center
San 31 Hyoja-dong Nam-gu,
Pohang 790-784, Republic of Korea
jhlee@postech.ac.kr
Abstract
This paper proposes a method to extract foreign
words, such as technical terms and proper nouns,
from Korean corpora and produce a Japanese-
Korean bilingual dictionary. Specific words have
been imported into multiple countries simultane-
ously, if they are influential across cultures. The
pronunciation of a source word is similar in different
languages. Our method extracts words in Korean
corpora that are phonetically similar to Katakana
words, which can easily be identified in Japanese cor-
pora. We also show the effectiveness of our method
by means of experiments.
1 Introduction
Reflecting the rapid growth in science and tech-
nology, new words have progressively been created.
However, due to the limitation of manual compila-
tion, new words are often out-of-dictionary words
and decrease the quality of human language tech-
nology, such as natural language processing, infor-
mation retrieval, machine translation, and speech
recognition. To resolve this problem, a number
of automatic methods to extract monolingual and
bilingual lexicons from corpora have been proposed
for various languages.
In this paper, we focus on extracting foreign words
(or loanwords) in Korean. Technical terms and
proper nouns are often imported from foreign lan-
guages and are spelled out (or transliterated) by the
Korean alphabet system called Hangul . The similar
trend can be observable in Japanese and Chinese. In
Japanese, foreign words are spelled out by its special
phonetic alphabet (or phonogram) called Katakana.
Thus, foreign words can be extracted from Japanese
corpora with a high accuracy, because the Katakana
characters are seldom used to describe the conven-
tional Japanese words, excepting proper nouns.
However, extracting foreign words from Korean
corpora is more difficult, because in Korean both
the conventional and foreign words are written with
Hangul characters. This problem remains a chal-
lenging issue in computational linguistic research.
It is often the case that specific words have been
imported into multiple countries simultaneously, be-
cause the source words (or concepts) are usually in-
fluential across cultures. Thus, it is feasible that a
large number of foreign words in Korean can also be
foreign words in Japanese.
In addition, the foreign words in Korean and
Japanese corresponding to the same source word are
phonetically similar. For example, the English word
?system? has been imported into both Japanese and
Korean. The romanized words are /sisutemu/ and
/siseutem/ in both countries, respectively.
Motivated by these assumptions, we propose a
method to extract foreign words in Korean corpora
by means of Japanese. In brief, our method per-
forms as follows. First, foreign words in Japanese
are collected, for which Katakana words in corpora
and existing lexicons can be used. Second, from Ko-
rean corpora the words that are phonetically similar
to Katakana words are extracted. Finally, extracted
Korean words are compiled in a lexicon with the cor-
responding Japanese words.
In summary, our method can extract foreign words
in Korean and produce a Japanese-Korean bilingual
lexicon in a single framework.
2 Methodology
2.1 Overview
Figure 1 exemplifies our extraction method, which
produces a Japanese-Korean bilingual lexicon using
a Korean corpus and Japanese corpus and/or lexi-
con. The Japanese and Korean corpora do not have
to be parallel or comparable. However, it is desir-
able that both corpora are associated with the same
domain. For the Japanese resource, the corpus and
lexicon can alternatively be used or can be used to-
gether. Note that compiling Japanese monolingual
lexicon is less expensive than that for a bilingual lex-
icon. In addition, new Katakana words can easily be
extracted from a number of on-line resources, such
as the World Wide Web. Thus, the use of Japanese
lexicons does not decrease the utility of our method.
First, we collect Katakana words from Japanese
resources. This can systematically be performed by
means of a Japanese character code, such as EUC-
JP and SJIS.
Second, we represent the Korean corpus and
Japanese Katakana words by the Roman alphabet
(i.e., romanization), so that the phonetic similarity
can easily be computed. However, we use different
romanization methods for Japanese and Korean.
CompuTerm 2004 Poster Session  -  3rd International Workshop on Computational Terminology 71
Third, we extract candidates of foreign words
from the romanized Korean corpus. An alternative
method is to first perform morphological analysis
on the corpus, extract candidate words based on
morphemes and parts-of-speech, and romanize the
extracted words. Our general model does not con-
strain as to which method should be used in the
third step. However, because the accuracy of anal-
ysis often decreases for new words to be extracted,
we experimentally adopt the former method.
Finally, we compute the phonetic similarity be-
tween each combination of the romanized Hangul
and Katakana words, and select the combinations
whose score is above a predefined threshold. As a
result, we can obtain a Japanese-Korean bilingual
lexicon consisting of foreign words.
It may be argued that English lexicons or cor-
pora can be used as source information, instead of
Japanese resources. However, because not all En-
glish words have been imported into Korean, the
extraction accuracy will decrease due to extraneous
words.
Figure 1: Overview of our extraction method.
2.2 Romanizing Japanese
Because the number of phones consisting of Japanese
Katakana characters is limited, we manually pro-
duced the correspondence between each phone
and its Roman representation. The numbers of
Katakana characters and combined phones are 73
and 109, respectively. We also defined a symbol to
represent a long vowel. In Japanese, the Hepbern
and Kunrei systems are commonly used for roman-
ization purposes. We use the Hepburn system, be-
cause its representation is similar to that in Korean,
compared with the Kunrei system.
However, specific Japanese phones, such as /ti/,
do not exist in Korean. Thus, to adapt the Hepburn
system to Korean, /ti/ and /tu/ are converted to
/chi/ and /chu/, respectively.
2.3 Romanizing Korean
The number of Korean Hangul characters is much
greater than that of Japanese Katakana characters.
Each Hangul character is a combination of more
than one consonant. The pronunciation of each char-
acter is determined by its component consonants.
In Korean, there are types of consonant, i.e., the
first consonant, vowel, and last consonant. The
numbers of these consonants are 19, 21, and 27, re-
spectively. The last consonant is optional. Thus, the
number of combined characters is 11,172. However,
to transliterate imported words, the official guide-
line suggests that only seven consonants be used as
the last consonant. In EUC-KR, which is a stan-
dard coding system for Korean text, 2,350 common
characters are coded independent of the pronunci-
ation. Therefore, if we target corpora represented
by EUC-KR, each of the 2,350 characters has to be
corresponded to its Roman representation.
We use Unicode, in which Hangul characters are
sorted according to the pronunciation. Figure 2 de-
picts a fragment of the Unicode table for Korean,
in which each line corresponds to a combination
of the first consonant and vowel and each column
corresponds to the last consonant. The number of
columns is 28, i.e., the number of the last consonants
and the case in which the last consonant is not used.
From this figure, the following rules can be found:
 the first consonant changes every 21 lines, which
corresponds to the number of vowels,
 the vowel changes every line (i.e., 28 characters)
and repeats every 21 lines,
 the last consonant changes every column.
Based on these rules, each character and its pro-
nunciation can be identified by the three consonant
types. Thus, we manually corresponded only the 68
consonants to Roman alphabets.
Figure 2: A fragment of the Unicode table for Ko-
rean Hangul characters.
We use the official romanization system for Ko-
rean, but specific Korean phones are adapted to
Japanese. For example, /j/ and /l/ are converted
to /z/ and /r/, respectively.
It should be noted that the adaptation is not in-
vertible and thus is needed for both J-to-K and K-
to-J directions.
CompuTerm 2004 Poster Session  -  3rd International Workshop on Computational Terminology72
For example, the English word ?cheese?, which
has been imported to both Korean and Japanese as
a foreign word, is romanized as /chiseu/ in Korean
and /ti:zu/ in Japanese. Here, /:/ is the symbol
representing a Japanese long vowel. Using the adap-
tation, these expressions are converted to /chizu/
and /chi:zu/, respectively, which look more similar
to each other, compared with the original strings.
2.4 Extracting term candidates from
Korean corpora
To extract candidates of foreign words from a Ko-
rean corpus, we first extract phrases. This can be
performed systematically, because Korean sentences
are segmented on a phrase-by-phrase basis.
Second, because foreign words are usually nouns,
we use hand-crafted rules to remove post-position
suffixes (e.g., Josa) and extract nouns from phrases.
Third, we discard nouns including the last con-
sonants that are not recommended for translitera-
tion purposes in the official guideline. Although the
guideline suggests other rules for transliteration, ex-
isting foreign words in Korean are not necessarily
regulated by these rules.
Finally, we consult a dictionary to discard exist-
ing Korean words, because our purpose is to extract
new words. For this purpose, we experimentally
use the dictionary for SuperMorph-K morphologi-
cal analyzer1, which includes approximately 50,000
Korean words.
2.5 Computing Similarity
Given romanized Japanese and Korean words, we
compute the similarity between the two strings and
select the pairs associated with the score above a
threshold as translations. We use a DP (dynamic
programming) matching method to identify the
number of differences (i.e., insertion, deletion, and
substitution) between two strings, on a alphabet-
by-alphabet basis.
In principle, if two strings are associated with a
smaller number of differences, the similarity between
them becomes greater. For this purpose, a Dice-style
coefficient can be used.
However, while the use of consonants in translit-
eration is usually the same across languages, the
use of vowels can vary significantly depending on
the language. For example, the English word ?sys-
tem? is romanized as /sisutemu/ and /siseutem/
in Japanese and Korean, respectively. Thus, the dif-
ferences in consonants between two strings should
be penalized more than the differences in vowels.
In view of the above discussion, we compute the
similarity between two romanized words by Equa-
tion (1).
1 ?
2 ? (? ? dc + dv)
? ? c + v (1)
Here, dc and dv denote the numbers of differences
in consonants and vowels, respectively, and ? is a
1http://www.omronsoft.com/
parametric constant used to control the importance
of the consonants. We experimentally set ? = 2. In
addition, c and v denote the numbers of all conso-
nants and vowels in the two strings. The similarity
ranges from 0 to 1.
3 Experimentation
3.1 Evaluating Extraction Accuracy
We collected 111,166 Katakana words (word types)
from multiple Japanese lexicons, most of which were
technical term dictionaries.
We used the Korean document set in the NTCIR-3
Cross-lingual Information Retrieval test collection2.
This document set consists of 66,146 newspaper ar-
ticles of Korean Economic Daily published in 1994.
We randomly selected 50 newspaper articles and
used them for our experiment. We asked a grad-
uate student excluding the authors of this paper to
identify foreign words in the target text. As a result,
124 foreign word types (205 word tokens) were iden-
tified, which were less than we had expected. This
was partially due to the fact that newspaper articles
generally do not contain a large number of foreign
words, compared with technical publications.
We manually classified the extracted words and
used only the words that were imported to both
Japan and Korea from other languages. We dis-
carded foreign words in Korea imported from Japan,
because these words were often spelled out by non-
Katakana characters, such as Kanji (Chinese charac-
ter). A sample of these words includes ?Tokyo (the
capital of Japan)?, ?Heisei (the current Japanese
era name)?, and ?enko (personal connection)?. In
addition, we discarded the foreign proper nouns for
which the human subject was not able to identify
the source word. As a result, we obtained 67 target
word types. Examples of original English words for
these words are as follows:
digital, group, dollar, re-engineering, line,
polyester, Asia, service, class, card, com-
puter, brand, liter, hotel.
Thus, our method can potentially be applied to
roughly a half of the foreign words in Korean text.
We used the Japanese words to extract plausi-
ble foreign words from the target Korean corpus.
We first romanized the corpus and extracted nouns
by removing post-position suffixes. As a result, we
obtained 3,106 words including all the 67 target
words. By discarding the words in the dictionary
for SuperMorph-K, 958 words including 59 target
words were remained.
For each of the remaining 958 words, we computed
the similarity between each of the 111,166 Japanese
words. For evaluation purposes, we varied a thresh-
old for the similarity and investigated the relation
between precision and recall. Recall is the ratio
of the number of target foreign words extracted by
our method and the total number of target foreign
2http://research.nii.ac.jp/ntcir/index-en.html
CompuTerm 2004 Poster Session  -  3rd International Workshop on Computational Terminology 73
words. Precision is the ratio of the number of target
foreign words extracted by our method and the total
number of words obtained by our method.
Table 1 shows the precision and recall for differ-
ent methods. While we varied a threshold of a sim-
ilarity, we also varied the number of Korean words
corresponded to a single Katakana word (N). By
decreasing the value of the threshold and increasing
the number of words extracted, the recall can be im-
proved but the precision decreases. In Table 1, the
precision and recall are in an extreme trade-off rela-
tion. For example, when the recall was 69.5%, the
precision was only 1.2%.
We manually analyzed the words that were not ex-
tracted by our method. Out of the 59 target words,
12 compound words consisting of both conventional
and foreign words were not extracted. However,
our method extracted compound words consisting
of only foreign words. In addition, the three words
that did not have counterparts in the input Japanese
words were not extracted.
Table 1: Precision/Recall for term extraction.
Threshold for similarity
>0.9 >0.7 >0.5
N=1 50.0/8.5 12.7/40.7 4.1/47.5
N=10 50.0/8.5 7.4/47.5 1.2/69.5
3.2 Application-Oriented Evaluation
During the first experiment, we determined a specific
threshold value for the similarity between Katakana
and Hangul words and selected the pairs whose sim-
ilarity was above the threshold. As a result, we ob-
tained 667 Korean words, which were used to en-
hance the dictionary for the SuperMorph-K morpho-
logical analyzer.
We performed morphological analysis on the 50
articles used in the first experiment, which included
1,213 sentences and 9,557 word tokens. We also in-
vestigated the degree to which the analytical accu-
racy is improved by means of the additional dictio-
nary. Here, accuracy is the ratio of the number of
correct word segmentations and the total segmenta-
tions generated by SuperMorph-K. The same human
subject as in the first experiment identified the cor-
rect word segmentations for the input articles.
First, we focused on the accuracy of segmenting
foreign words. The accuracy was improved from
75.8% to 79.8% by means of the additional dictio-
nary. The accuracy for all words was changed from
94.6% to 94.8% by the additional dictionary.
In summary, the additional dictionary was effec-
tive for analyzing foreign words and was not asso-
ciated with side effect for the overall accuracy. At
the same time, we concede that we need larger-scale
experiments to draw firmer conclusions.
4 Related Work
A number of corpus-based methods to extract bilin-
gual lexicons have been proposed (Smadja et al,
1996). In general, these methods use statistics ob-
tained from a parallel or comparable bilingual corpus
and extract word or phrase pairs that are strongly
associated with each other. However, our method
uses a monolingual Korean corpus and a Japanese
lexicon independent of the corpus, which can easily
be obtained, compared with parallel or comparable
bilingual corpora.
Jeong et al (1999) and Oh and Choi (2001) in-
dependently explored a statistical approach to de-
tect foreign words in Korean text. Although the de-
tection accuracy is reasonably high, these methods
require a training corpus in which conventional and
foreign words are annotated. Our approach does not
require annotated corpora, but the detection accu-
racy is not high enough as shown in Section 3.1. A
combination of both approaches is expected to com-
pensate the drawbacks of each approach.
5 Conclusion
We proposed a method to extract foreign words,
such as technical terms and proper nouns, from Ko-
rean corpora and produce a Japanese-Korean bilin-
gual dictionary. Specific words, which have been
imported into multiple countries, are usually spelled
out by special phonetic alphabets, such as Katakana
in Japanese and Hangul in Korean.
Because extracting foreign words spelled out by
Katakana in Japanese lexicons and corpora can be
performed with a high accuracy, our method ex-
tracts words in Korean corpora that are phonetically
similar to Japanese Katakana words. Our method
does not require parallel or comparable bilingual cor-
pora and human annotation for these corpora.
We also performed experiments in which we ex-
tracted foreign words from Korean newspaper arti-
cles and used the resultant dictionary for morpho-
logical analysis. We found that our method did not
correctly extract compound Korean words consist-
ing of both conventional and foreign words. Future
work includes larger-scale experiments to further in-
vestigate the effectiveness of our method.
References
Kil Soon Jeong, Sung Hyon Myaeng, Jae Sung Lee,
and Key-Sun Choi. 1999. Automatic identification
and back-transliteration of foreign words for informa-
tion retrieval. Information Processing & Management,
35:523?540.
Jong-Hoon Oh and Key sun Choi. 2001. Automatic
extraction of transliterated foreign words using hid-
den markov model. In Proceedings of ICCPOL-2001,
pages 433?438.
Frank Smadja, Kathleen R. McKeown, and Vasileios
Hatzivassiloglou. 1996. Translating collocations for
bilingual lexicons: A statistical approach. Computa-
tional Linguistics, 22(1):1?38.
CompuTerm 2004 Poster Session  -  3rd International Workshop on Computational Terminology74
Proceedings of the Fourth Workshop on Statistical Machine Translation , pages 190?196,
Athens, Greece, 30 March ? 31 March 2009. c?2009 Association for Computational Linguistics
Chinese Syntactic Reordering for Adequate Generation of Korean 
Verbal Phrases in Chinese-to-Korean SMT 
Jin-Ji Li, Jungi Kim, Dong-Il Kim*, and Jong-Hyeok Lee 
Department of Computer Science and Engineering,  
Electrical and Computer Engineering Division, 
Pohang University of Science and Technology (POSTECH), 
San 31 Hyoja Dong, Pohang, 790-784, R. of Korea 
E-mail: {ljj, yangpa, jhlee}@postech.ac.kr 
 
*Language Engineering Institute,  
Department of Computer, Electron and Telecommunication Engineering, 
Yanbian University of Science and Technology (YUST), 
Yanji, Jilin, 133-000, P.R. of China 
E-mail: {dongil}@ybust.edu.cn 
 
Abstract 
Chinese and Korean belong to different lan-
guage families in terms of word-order and 
morphological typology. Chinese is an SVO 
and morphologically poor language while Ko-
rean is an SOV and morphologically rich one. 
In Chinese-to-Korean SMT systems, systemat-
ic differences between the verbal systems of 
the two languages make the generation of Ko-
rean verbal phrases difficult. To resolve the 
difficulties, we address two issues in this paper. 
The first issue is that the verb position is dif-
ferent from the viewpoint of word-order ty-
pology. The second is the difficulty of com-
plex morphology generation of Korean verbs 
from the viewpoint of morphological typology. 
We propose a Chinese syntactic reordering 
that is better at generating Korean verbal 
phrases in Chinese-to-Korean SMT. Specifi-
cally, we consider reordering rules targeting 
Chinese verb phrases (VPs), preposition 
phrases (PPs), and modality-bearing words 
that are closely related to Korean verbal phras-
es. We verify our system with two corpora of 
different domains. Our proposed approach 
significantly improves the performance of our 
system over a baseline phrased-based SMT 
system. The relative improvements in the two 
corpora are +9.32% and +5.43%, respectively. 
1 Introduction 
Recently, there has been a lot of research on en-
coding syntactic information into statistical ma-
chine translation (SMT) systems in various forms 
and in different stages of translation processes. 
During preprocessing source language sen-
tences undergo reordering and morpho-syntactic 
reconstruction phases to generate more target 
language-like sentences. Also, fixing erroneous 
words, generating complex morphology, and re-
ranking translation results in post-processing 
phases may utilize syntactic information of both 
source and target languages. A syntax-based 
SMT system encodes the syntactic information in 
its translation model of the decoding step. 
A number of researchers have proposed syn-
tactic reordering as a preprocessing step (Xia and 
McCord, 2004; Collins et al, 2005; Wang et al, 
2007). In these syntactic reordering approaches, 
source sentences are first parsed and a series of 
reordering rules are applied to the parsed trees to 
reorder the source sentences into target language-
like word orders. Such an approach is an effec-
tive method for a phrase-based SMT system that 
employs a relatively simple distortion model in 
the decoding phase. 
This paper concentrates upon reordering 
source sentences in the preprocessing step of a 
Chinese-to-Korean phrase-based SMT system 
using syntactic information. Chinese-to-Korean 
SMT has more difficulties than the language 
pairs studied in previous research (French-
English, German-English, and Chinese-English). 
From the viewpoint of language typology, these 
language pairs are all SVO languages and they 
have relatively simpler morphological inflections. 
On the other hand, Korean is an SOV and agglu-
tinative language with relatively free word order 
and with complex and rich inflections. 
For the Chinese-to-Korean SMT, these syste-
matic differences of the two languages make the 
generation of Korean verbal phrases very diffi-
cult. Firstly, the difference in the verb position of 
the two languages may not be reflected in the 
simple distortion model of a phrase-based SMT 
system. Secondly, Morphology generation of 
190
Korean verbs is difficult because of its complexi-
ty and the translation direction from a low-
inflection language to a high-inflection language. 
In the following sections, we describe the cha-
racteristics of Korean verbal phrases and their 
corresponding Chinese verbal phrases, and 
present a set of hand-written syntactic reordering 
rules including Chinese verb phrases (VPs), pre-
position phrases (PPs), and modality-bearing 
words. In the latter sections, we empirically veri-
fy that our reordering rules effectively reposition 
source words to target language-like order and 
improve the translation results. 
2 Contrastive analysis of Chinese and 
Korean with a focus on Korean verbal 
phrase generations 
In the Chinese-to-Korean SMT, the basic transla-
tion units are morphemes. For Chinese, sentences 
are segmented into words. As a typical isolating 
language, each segmented Chinese word is a 
morpheme. Korean is a highly agglutinative lan-
guage and an eojeol refers to a fully inflected 
lexical form separated by a space in a sentence. 
Each eojeol in Korean consists of one or more 
base forms (stem morphemes or content mor-
phemes) and their inflections (function mor-
phemes). Inflections usually include postposi-
tions and verb endings (verb affixes) of verbs 
and adjectives. These base forms and inflections 
are grammatical units in Korean, and they are 
defined as morphemes. As for the translation unit, 
eojeol cause data sparseness problems hence we 
consider a morpheme as a translation unit for 
Korean.  
As briefly mentioned in the previous section, 
Chinese and Korean belong to different word-
order typologies. The difference of verb position 
causes the difficulty in generating correct Korean 
verbal phrases. Also, the complexity of verb af-
fixes in Korean verbs is problematic in SMT sys-
tems targeting Korean, especially if the source 
language is isolated. 
In the Dong-A newspaper corpus on which we 
carry out our experiments in Section 4, Korean 
function morphemes occupy 41.3% of all Korean 
morphemes. Verb endings consist of 40.3% of all 
Korean function words, and the average number 
of function morphemes inflected by a verb or an 
adjective is 1.94 while that of other content mor-
phemes is only 0.7.  
These statistics indicate that the morphological 
form of Korean verbal phrases (Korean verbs) 1 
are significantly complex. A verbal phrase in 
Korean consists of a series of verb affixes along 
with a verb stem. A verb stem cannot be used by 
itself but should take at least one affix to form a 
verbal complex. Verb affixes in Korean are or-
dered in a relative sequence within a verbal com-
plex (Lee, 1991) and express different modality 
information2: tense, aspect, mood, negation, and 
voice (Figure 1). These five grammatical catego-
ries are the major constituents of modal expres-
sion in Korean. 
 
K1: ?(stem) +?_?(aspect prt.) +  ?(aspect prt.)
+ ?(tense prt.) + ?(mood prt.) 
E1: had been eating 
K2. ?(stem) + ?(passive prt.) + ?(aspect prt.) + 
?_ ?_?(modality prt.)  + ?(mood prt.) 
E2: might have been caught 
Figure 1. Verbal phrases in Korean. Bold-faced 
content morphemes followed by functional ones 
with ?+? symbols. Prt. is an acronym for particle. 
 
The modality of Korean is expressed inten-
sively by verb affixes. However, Chinese ex-
presses modality using discontinuous morphemes 
scattered throughout a sentence (Figure 2). Also, 
the prominence of grammatical categories ex-
pressing modality information is different from 
language to language, and correlations of such 
categories in a language are also different. The 
differences between the two languages lead to 
difficulties in alignment and cause linking obscu-
rities. 
 
C3: ??(thief)/??(might)/?(passive prt.)/??
(police)/?(catch)/?(aspect prt.)/? 
 
K3: ??(thief)+?   ??(police)+??   
?(catch)+?(passive prt.)+?(aspect prt.)+?_?_?
(modality prt.)+?(mood prt.)+./ 
E3: The thief might have been caught by the police.
Figure 2. Underlined morphemes are modality-
bearing morphemes in Chinese and Korean sen-
tences. Chinese words are separated by a ?/? 
symbol and Korean eojeols by a space.  
                                                 
1 ?Korean verbal phrase? or ?Korean verbs? in this paper 
refer to Korean predicates (verbs or adjectives) in a sentence.  
2  Modality system refers to five grammatical categories: 
tense, aspect, mood (modality & mood), negation, and voice. 
The definition of these categories is described in detail in 
(Li et al, 2005). 
191
We consider two issues for generating ade-
quate Korean verbal phrases. First is the correct 
position of verbal phrases, and the second is the 
generation of verb affixes which convey modali-
ty information. 
3 Chinese syntactic reordering rules 
In this section, we describe a set of manually 
constructed Chinese syntactic reordering rules. 
Chinese sentences are first parsed by Stanford 
PCFG parser which uses Penn Chinese Treebank 
as the training corpus (Levy and Manning, 2003). 
Penn Chinese Treebank adopts 23 tags for phras-
es (Appendix A). We identified three categories 
in Chinese that need to be reordered: verb phras-
es (VPs), preposition phrases (PPs), and modali-
ty-bearing words.  
3.1 Verb phrases 
Korean is a verb-final language, and verb phrase 
modifiers and complements occur in the pre-
verbal positions. However, in Chinese, verb 
phrase modifiers occur in the pre-verbal or post-
verbal positions, and complements mostly occur 
in post-verbal positions. 
We move the verb phrase modifiers and com-
plements located before the verbal heads to the 
post-verbal position as demonstrated in the fol-
lowing examples. A verbal head consists of a 
verb (including verb compound) and an aspect 
sequence (Xue and Xia, 2000). Therefore, aspect 
markers such as ?? (perfective prt.)?, ??
(durative prt.)?, ??(experiential prt.)? positioned 
immediately after a verb should remain in the 
relatively same position with the preceding verb. 
The third one in the example reordering rules 
shows this case. Mid-sentence punctuations are 
also considered when constructing the reordering 
rules. 
 
Examples of reordering rules of VPs3: 
 
VV0 NP1 ? NP1 VV0 
VV0 IP1 ? IP1 VV0 
VV0 AS1 NP2 ? NP2 VV0 AS1 
VV0 PU1 IP2 ? IP2 PU1 VV0 
 
Original parse tree: 
VP 
      PP (P ?) 
          NP (NN ??) 
      PP (P ?) 
                                                 
3 VV: common verb; AS: aspect marker; P: preposi-
tion; PU: punctuation; PN: pronoun; 
          NP (PN ??) 
      VP (VV ??) 
          NP (NN ??) 
 
Reordered parse tree: 
VP 
      PP (P ?) 
          NP (NN ??) 
      PP (P ?) 
          NP (PN ??) 
      NP (NN ??) 
VP (VV ??) 
 
3.2 Preposition phrases 
Chinese prepositions originate from verbs, and 
they preserve the characteristics of verbs. Chi-
nese prepositions are translated into Korean 
verbs, other content words, or particles. We only 
consider the Chinese prepositions that translate 
into verbs and other content words. We swap the 
prepositions with their objects as demonstrated in 
the following examples.  
 
Examples of reordering rules of PPs: 
 
Case 1: translate into Korean verbs 
P(?)0 NP1 ? NP1 P(?)0 
P(??)0 IP1 ? IP1 P(??)0 
P(??)0 LCP1 ? LCP1 P(??)0 
 
Case 2: translate into other content words 
P(??)0 IP1 ? IP1 P(??)0 
P(??)0 NP1 ? NP1 P(??)0 
 
Original parse tree: 
VP 
      PP (P ?) 
          NP (NN ??) 
      PP (P ?) 
          NP (PN ??) 
      VP (VV ??) 
          NP (NN ??) 
 
Reordered parse tree: 
VP 
  NP (NN ??) 
PP (P ?)  
      PP (P ?) 
          NP (PN ??) 
      VP (VV ??) 
          NP (NN ??) 
 
192
3.3 Modality-bearing words 
Verb affixes in Korean verbal phrases indicate 
modality information such as tense, aspect, mood, 
negation, and voice. The corresponding modality 
information is implicitly or explicitly expressed 
in Chinese. It is important to figure out what fea-
tures are used to represent modality information. 
Li et al (2008) describes in detail the features in 
Chinese that express modality information. 
However, since only lexical features can be reor-
dered, we consider explicit modality features 
only. 
Modality-bearing words are scattered over an 
entire sentence. We move them near their verbal 
heads because their correspondences in Korean 
sentences are always placed right after their 
verbs. 
When constructing reordering rules, we con-
sider temporal adverbs, auxiliary verbs, negation 
particles, and aspect particles only. The follow-
ing example sentences show the results of a few 
of our reordering rules for modality-bearing 
words. 
 
Examples of reordering rules of modality-
bearing words: 
 
Original parse tree: 
VP 
      ADVP (AD ?)                   ? Temporal adverb 
          PP (P ?) 
              LCP 
                  NP (NN ??) (NN ??) (NN ??) 
                  (LC?) 
          VP (VV ??) 
              NP (NN ??) 
 
Reordered parse tree: 
VP 
      PP (P ?) 
          LCP  
              NP (NN ??) (NN ??) (NN ??) 
              (LC ?) 
      ADVP (AD ?)  
VP (VV ??) 
          NP (NN ??) 
 
Original parse tree: 
VP (VV ?)                                ? Auxiliary verb 
       VP 
           PP (P ?) 
               LCP 
                   NP (NN ??) (NN ?) 
                   (LC ?) 
           VP (VV ??) 
Reordered parse tree: 
VP 
        PP (P ?) 
             LCP 
                  NP (NN ??) (NN ?) 
                  (LC ?) 
      VP (VV ?) 
VP (VV ??) 
 
Original parse tree: 
VP 
ADVP (AD ?)                ? Negation particle 
   VP (VV ??)                       ? Auxiliary verb 
   VP 
        PP (P ?) 
           NP (NN ???) (NN ??) 
             VP (VV ??) 
 
Reordered parse tree: 
VP 
     PP (P ?) 
         NP (NN ???) (NN ??) 
ADVP (AD ?) 
VP (VV ??) 
          VP (VV ??) 
 
Generally speaking, Chinese does not have 
grammatical forms for voice. Although, voice is 
also a grammatical category expressing modality 
information, we have left it out of the current 
phase of our experiment since voice detection is 
another research issue and reordering rules for 
voice are unavoidably complicated. 
4 Experiment 
Our baseline system is a popular phrase-based 
SMT system, Moses (Koehn et al, 2007), with 
5-gram SRILM language model (Stolcke, 2002), 
tuned with Minimum Error Training (Och, 2003). 
We adopt NIST (NIST, 2002) and BLEU (Papi-
neni et al, 2001) as our evaluation metrics.  
Chinese sentences in training and test corpora 
are first parsed and are applied a series of syntac-
tic reordering rules. To evaluate the contribution 
of the three categories of syntactic reordering 
rules, we perform the experiments applying each 
category independently. Experiments of various 
combinations are also carried out. 
4.1 Corpus profile 
We automatically collected and constructed a 
sentence-aligned parallel corpus from the online 
193
Dong-A newspaper 4 . Strictly speaking, it is a 
non-literally translated Korean-to-Chinese cor-
pus. The other corpus is provided by MSRA 
(Microsoft Research Asia). It is a Chinese-
Korean-English trilingual corpus of technical 
manuals and a literally translated corpus. 
Chinese sentences are segmented by Stanford 
Chinese word segmenter (Tseng et al, 2005), 
and parsed by Stanford Chinese parser (Levy and 
Manning, 2003). Korean sentences are seg-
mented into morphemes by an in-house morpho-
logical analyzer. 
The detailed corpus profiles are displayed in 
Table 1 and 2. The Dong-A newspaper corpus is 
much longer than the MSRA technical manual 
corpus. In Korean, we report the length of con-
tent and function words. 
 
 Training (99,226 sentences) 
Chinese Korean Content Function
# of words 2,692,474 1,859,105 1,277,756 
# of singletons 78,326 67,070 514
avg. sen. length 27.13 18.74 12.88
 Development (500 sentences) 
Chinese Korean Content Function
# of words 14,485 9,863 6,875
# of singletons 4,029 4,166 163
avg. sen. length 28.97 19.73 13.75
 Test (500 sentences) 
Chinese Korean Content Function
# of words 14,657 10,049 6,980
# of singletons 4,027 4,217 164
avg. sen. length 29.31 20.10 13.96
Table 1. Corpus profile of Dong-A newspaper. 
 
 Training (29,754 sentences) 
Chinese Korean Content Function
# of words 425,023  316,289 207,909
# of singletons 5,746 4,689 197
avg. sen. length 14.29 10.63 6.99
 Development (500 sentences) 
Chinese Korean Content Function
# of words 6,380 4,853 3,214
# of singletons 1,174 975 93
avg. sen. length 12.76 9.71 6.43
 Test (500 sentences) 
Chinese Korean Content Function
                                                 
4 http://www.donga.com/news/ (Korean) and 
http://chinese.donga.com/gb/index.html (Chinese) 
# of words 7,451 5,336 3,548
# of singletons 1,182 964 99
avg. sen. length 14.90 10.67 7.10
Table 2. Corpus profile of MSRA technical ma-
nual. 
4.2 Result and discussion 
The experimental results are displayed in Table 3 
and 4. Besides assessing the effectiveness of 
each reordering category, we test various combi-
nations of the three categories. 
 
Method NIST BLEU 
Baseline 5.7801 20.49 
Reorder.VP 5.8402  22.12 (+7.96%) 
Reorder.PP 5.7773  20.10 (-1.90%) 
Reorder.Modality 5.7682  20.93 (+2.15%) 
Reorder.VP+PP 5.8176  21.96 (+7.17%) 
Reorder.VP+Modality 5.9198  22.24 (+8.54%) 
Reorder.All 5.9361 22.40 (+9.32%) 
Table 3. Experimental results on the Dong-A 
newspaper corpus. 
 
Method NIST BLEU 
Baseline 7.2596 44.03 
Reorder.VP 7.2238  44.57 (+1.23%) 
Reorder.PP 7.2793  44.22 (+0.43%) 
Reorder.Modality 7.3110  44.25 (+0.50%) 
Reorder.VP+PP 7.3401  45.28 (+2.84%) 
Reorder.VP+Modality 7.4246  46.42 (+5.43%) 
Reorder.All 7.3849  46.33 (+5.22%) 
Table 4. Experimental results on the MSRA 
technical manual corpus. 
 
From the experimental result of the Dong-A 
newspaper corpus, we find that the most effec-
tive category is the reordering rules of VPs. 
When the VP reordering rules are combined with 
the modality ones, the performance is even better. 
The gain of BLEU is not significant, but the gain 
of NIST is significant from 5.8402 to 5.9198. 
The PP reordering rules do not contribute to the 
performance when they are singly applied. How-
ever, when combined with the other two catego-
ries, they contribute to the performance. The best 
performance is achieved when all three catego-
ries? reordering rules are applied and the relative 
improvement is +9.32% over the baseline system. 
In the MSRA corpus, the performance of vari-
ous combinations of the three categories is better 
than those of the individual categories. The PP 
category shows improvement when it is com-
bined with the VP category. The combination of 
VP and modality category improves the perfor-
mance by +5.43% over the baseline. 
194
These results agree with our expectations: re-
solving the word order and modality expression 
differences of verbal phrases between Chinese 
and Korean is an effective approach.  
4.3 Error Analysis 
We adopt an error analysis method proposed by 
Vilar et al (2006). They presented a framework 
for classifying error types of SMT systems. (Ap-
pendix B.)  
Since our approach focuses on verbal phrase 
differences between Chinese and Korean, we 
carry out the error analysis only on the verbal 
heads. Three types of errors are considered:  
word order, missing words, and incorrect words. 
We further classify the incorrect words category 
into two sub-categories: wrong lexical 
choice/extra word, and incorrect form of modali-
ty information. 50 sentences are selected from 
each test corpus on which to perform the error 
analysis. For each corpus, we choose the best 
system: Reorder.All for the Dong-A corpus and 
Reorder.VP+modality for the MSRA corpus. 
 The most frequent error type is wrong word 
order in both corpora. When a verb without any 
modality information appears in a wrong position, 
we only count it as a wrong word order but not 
as a wrong modality. Therefore, the number of 
wrong modalities is not as frequent as it should 
be. 
Table 5 and 6 indicate that our proposed me-
thod helps improve the SMT system to reduce 
the number of error types related to verbal phras-
es. 
 
Error type Frequency Baseline Reorder.All
wrong word order  34 7
missing content word  18 5
wrong lexical choice/ 
extra word  6 1
wrong modality  10 6
Table 5. Error analysis of the Dong-A newspaper 
corpus. 
 
Error type 
Frequency 
Baseline Reorder. VP+Modality
wrong word order 19 11
missing content word 4 2
wrong lexical choice/ 
extra word 8 3
wrong modality  11 6
Table 6. Error analysis of the MSRA technical 
manual corpus. 
 
5 Conclusion and future work 
In this paper, we proposed a Chinese syntactic 
reordering more suitable to adequately generate 
Korean verbal phrases in Chinese-to-Korean 
SMT. Specifically, we considered reordering 
rules targeting Chinese VPs, PPs, and modality-
bearing words that are closely related to Korean 
verbal phrases. 
Through a contrastive analysis between the 
two languages, we first showed the difficulty of 
generating Korean verbal phrases when translat-
ing from a morphologically poor language, Chi-
nese. Then, we proposed a set of syntactic reor-
dering rules to reorder Chinese sentences into a 
more Korean like word order. 
We conducted several experiments to assess 
the contributions of our method. The reordering 
of VPs is the most effective, and improves the 
performance even more when combined with the 
reordering rules of modality-bearing words. Ap-
plied to the Dong-A newspaper corpus and the 
MSRA technical manual corpus, our proposed 
approach improved the baseline systems by 
9.32% and 5.43%, respectively. We also per-
formed error analysis with a focus on verbal 
phrases. Our approach effectively decreased the 
size of all errors.  
There remain several issues as possible future 
work. We only considered the explicit modality 
features and relocated them near the verbal heads. 
In the future, we may improve our system by 
extracting implicit modality features. 
In addition to generating verbal phrases, there 
is the more general issue of generating complex 
morphology in SMT systems targeting Korean, 
such as generating Korean case markers. There 
are several previous studies on this topic (Min-
kov et al, 2007; Toutanova et al, 2008). This 
issue will also be the focus of our future work in 
both the phrase- and syntax-based SMT frame-
works. 
 
Acknowledgments 
This work was supported in part by MKE & II-
TA through the IT Leading R&D Support Project 
and also in part by the BK 21 Project in 2009. 
References  
Charles N. Li, and Sandra A. Thompson 1996. Man-
darin Chinese: A functional reference grammar, 
University of California Press, USA. 
David Vilar, Jia Xu, Luis Fernando D?Haro, and 
Hermann Ney. 2006. Error Analysis of Statistical 
195
Machine Translation Output. In Proceedings of 
LREC. 
Einat Minkov, Kristina Toutanova, and Hisami Suzu-
ki. 2007. Generating Complex Morphology for 
Machine Translation. In Proceedings of ACL.  
Fei Xia and Michael McCord. 2004. Improving a sta-
tistical MT system with automatically learned re-
write patterns. In Proceedings of COLING. 
Huihsin Tseng, Pichuan Chang, Galen Andrew, Da-
niel Jurafsky and Christopher Manning. 2005. A 
Conditional Random Field Word Segmenter. In 
Fourth SIGHAN Workshop on Chinese Language 
Processing.  
HyoSang Lee 1991. Tense, aspect, and modality: A 
discourse-pragmatic analysis of verbal affixes in 
Korean from a typological perspective, PhD thesis, 
Univ. of California, Los Angeles. 
Jin-Ji Li, Ji-Eun Roh, Dong-Il Kimand Jong-Hyeok 
Lee. 2005. Contrastive Analysis and Feature Selec-
tion for Korean Modal Expression in Chinese-
Korean Machine Translation System. International 
Journal of Computer Processing of Oriental Lan-
guages, 18(3), 227--242. 
 Jin-Ji Li, Dong-Il Kim and Jong-Hyeok Lee. 2008. 
Annotation Guidelines for Chinese-Korean Word 
Alignment. In Proceedings of LREC. 
Kristina Toutanova, Hisami Suzuki, and Achim 
Puopp. 2008. Applying Morphology Generation 
Models to Machine Translation. In Proceedings of 
ACL. 
Nianwen Xue, and Fei Xia. 2000. The bracketing 
guidelines for the Penn Chinese Treebank (3.0). 
IRCS technical report, University of Pennsylvania. 
Nianwen Xue, Fei Xia, Fu-Dong Chiou, and Martha 
Palmer. 2005. The Penn Chinese Treebank: Phrase 
structure annotation of a large corpus. Natural 
Language Engineering, 11(2):207?238. 
Michael Collins, Philipp Koehn, and Ivona 
Ku?cerov?a. 2005. Clause restructuring for statis-
tical machine translation. In Proceedings of ACL, 
pages 531?540. 
NIST. 2002. Automatic evaluation of machine trans-
lation quality using n-gram co-occurrence statis-
tics. 
Och, F. J. 2003. Minimum error rate training in sta-
tistical machine translation. In Proceedings of 
ACL. 
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris-
Callison-Burch, Marcello Federico, Nicola Bertoldi, 
Brooke Cowan, Wade Shen, Christine Moran, Ri-
chard Zens, Chris Dyer, Ondrej Bojar, Alexandra 
Constrantin, and Evan Herbst. 2007. Moses: Open 
source toolkit for statistical machine translation. In 
Proceedings of ACL, Demonstration Session.  
Roger Levy and Christopher D. Manning. 2003. Is it 
harder to parse Chinese, or the Chinese Treebank? 
In Proceedings of ACL. 
Stolcke, A. 2002. SRILM - an extensible language 
modeling toolkit. In Proceedings of ICSLP, 2:901-
904. 
Appendix A. Tag for phrases in Penn 
Chinese Treebank. 
ADJP adjective phrase 
ADVP adverbial phrase headed by AD (adverb)
CLP  classifier phrase 
CP   clause headed by C (complementizer) 
DNP  phrase formed by ?XP+DEG? 
DP   determiner phrase 
DVP  phrase formed by ?XP+DEV? 
FRAG fragment 
IP   simple clause headed by I (INFL) 
LCP  phrase formed by ?XP+LC? 
LST  list marker 
NP   noun phrase 
PP   preposition phrase 
PRN  parenthetical 
QP   quantifier phrase 
UCP  unidentical coordination phrase 
VP   verb phrase 
Appendix B. Classification of translation 
errors proposed by Vilar et al (2006). 
 
196
High Efficiency Realization for a
Wide-Coverage Unification Grammar
John Carroll1 and Stephan Oepen2
1 University of Sussex
2 University of Oslo and Stanford University
Abstract. We give a detailed account of an algorithm for efficient tactical gener-
ation from underspecified logical-form semantics, using a wide-coverage gram-
mar and a corpus of real-world target utterances. Some earlier claims about chart
realization are critically reviewed and corrected in the light of a series of practical
experiments. As well as a set of algorithmic refinements, we present two novel
techniques: the integration of subsumption-based local ambiguity factoring, and
a procedure to selectively unpack the generation forest according to a probability
distribution given by a conditional, discriminative model.
1 Introduction
A number of wide-coverage precise bi-directional NL grammars have been developed
over the past few years. One example is the LinGO English Resource Grammar (ERG)
[1], couched in the HPSG framework. Other grammars of similar size and coverage also
exist, notable examples using the LFG and the CCG formalisms [2,3]. These grammars
are used for generation from logical form input (also termed tactical generation or real-
ization) in circumscribed domains, as part of applications such as spoken dialog systems
[4] and machine translation [5].
Grammars like the ERG are lexicalist, in that the majority of information is encoded
in lexical entries (or lexical rules) as opposed to being represented in constructions (i.e.
rules operating on phrases). The semantic input to the generator for such grammars,
often, is a bag of lexical predicates with semantic relationships captured by appropriate
instantiation of variables associated with predicates and their semantic roles. For these
sorts of grammars and ?flat? semantic inputs, lexically-driven approaches to realization
? such as Shake-and-Bake [6], bag generation from logical form [7], chart generation
[8], and constraint-based generation [9] ? are highly suitable. Alternative approaches
based on semantic head-driven generation and more recent variants [10,11] would work
less well for lexicalist grammars since these approaches assume a hierarchically struc-
tured input logical form.
Similarly to parsing with large scale grammars, realization can be computation-
ally expensive. In his presentation of chart generation, Kay [8] describes one source
of potential inefficiency and proposes an approach for tackling it. However, Kay does
not report on a verification of his approach with an actual grammar. Carroll et al [12]
 Dan Flickinger and Ann Copestake contributed a lot to the work described in this paper. We
also thank Berthold Crysmann, Jan Tore L?nning and Bob Moore for useful discussions. Fund-
ing is from the projects COGENT (UK EPSRC) and LOGON (Norwegian Research Council).
R. Dale et al (Eds.): IJCNLP 2005, LNAI 3651, pp. 165?176, 2005.
c? Springer-Verlag Berlin Heidelberg 2005
166 J. Carroll and S. Oepen
? h1,
{ h1:proposition m(h2), h3: run v(e4, x5), h3:past(e4),
h6: the q(x5, h7, h8), h9: athlete n(x5), h9: young a(x5), h9: polish a(x5) },
{ h2 =q h3, h8 =q h9 } ?
Fig. 1. Simplified MRS for an utterance like the young Polish athlete ran (and variants). Elements
from the bag of EPs are linked through both scopal and ?standard? logical variables.
present a practical evaluation of chart generation efficiency with a large-scale HPSG
grammar, and describe a different approach to the problem which becomes necessary
when using a wide-coverage grammar. White [3] identifies further inefficiencies, and
describes and evaluates strategies for addressing them, albeit using what appears to be
a somewhat task-specific rather than genuine wide-coverage grammar. In this paper,
we revisit this previous work and present new, improved algorithms for efficient chart
generation; taken together these result in (i) practical performance that improves over
a previous implementation by two orders of magnitude, and (ii) throughput that is near
linear in the size of the input semantics.
In Section 2, we give an overview of the grammar and the semantic formalism
we use, recap the basic chart generation procedure, and discuss the various sources of
potential inefficiency in the basic approach. We then describe the algorithmic improve-
ments we have made to tackle these problems (Section 3), and conclude with the results
of evaluating these improvements (Section 4).
2 Background
2.1 Minimal Recursion Semantics and the LinGO ERG
Minimal Recursion Semantics (MRS) [13] is a popular member of a family of flat, un-
derspecified, event-based (neo-Davidsonian) frameworks for computational semantics
that have been in wide use since the mid-1990s. MRS allows both underspecification of
scope relations and generalization over classes of predicates (e.g. two-place temporal
relations corresponding to distinct lexical prepositions: English in May vs. on Monday,
say), which renders it an attractive input representation for tactical generation. While an
in-depth introduction to MRS is beyond the scope of this paper, Figure 1 shows an ex-
ample semantics that we will use in the following sections. The truth-conditional core is
captured as a flat multi-set (or ?bag?) of elementary predications (EPs), combined with
generalized quantifiers and designated handle variables to account for scopal relations.
The bag of EPs is complemented by the handle of the top-scoping EP (h1 in our exam-
ple) and a set of ?handle constraints? recording restrictions on scope relations in terms
of dominance relations.
The LinGO ERG [1] is a general-purpose, open-source HPSG implementation with
fairly comprehensive lexical and grammatical coverage over a variety of domains and
genres. The grammar has been deployed for diverse NLP tasks, including machine
translation of spoken and edited language, email auto response, consumer opinion track-
ing (from newsgroup data), and some question answering work.1 The ERG uses MRS
1 See http://www.delph-in.net/erg/ for background information on the ERG.
High Efficiency Realization for a Wide-Coverage Unification Grammar 167
as its meaning representation layer, and the grammar distribution includes treebanked
versions of several reference corpora ? providing disambiguated and hand-inspected
?gold? standard MRS formulae for each input utterance ? of which we chose one of the
more complex sets for our empirical investigations of realization performance using the
ERG (see Section 4 below).
2.2 The Basic Procedure
Briefly, the basic chart generation procedure works as follows. A preprocessing phase
indexes lexical entries, lexical rules and grammar rules by the semantics they contain.
In order to find the lexical entries with which to initialize the chart, the input semantics
is checked against the indexed lexicon. When a lexical entry is retrieved, the variable
positions in its relations are instantiated in one-to-one correspondence with the variables
in the input semantics (a process we term Skolemization, in loose analogy to the more
general technique in theorem proving; see Section 3.1 below). For instance, for the MRS
in Figure 1, the lookup process would retrieve one or more instantiated lexical entries
for run containing h3: run v(e4, x5). Lexical and morphological rules are applied to the
instantiated lexical entries. If the lexical rules introduce relations, their application is
only allowed if these relations correspond to parts of the input semantics (h3:past(e4),
say, in our example). We treat a number of special cases (lexical items containing more
than one relation, grammar rules which introduce relations, and semantically vacuous
lexical items) in the same way as Carroll et al [12].
After initializing the chart (with inactive edges), active edges are created from in-
active ones by instantiating the head daughter of a rule; the resulting edges are then
combined with other inactive edges. Chart generation is very similar to chart parsing,
but what an edge covers is defined in terms of semantics, rather than orthography. Each
edge is associated with the set of relations it covers. Before combining two edges a
check is made to ensure that edges do not overlap: i.e. that they do not cover the same
relation(s). The goal is to find all possible inactive edges covering the full input MRS.
2.3 Complexity
The worst-case time complexity of chart generation is exponential (even though chart
parsing is polynomial). The main reason for this is that in theory a grammar could allow
any pair of edges to combine (subject to the restriction described above that the edges
cover non-overlapping bags of EPs). For an input semantics containing n EPs, and
assuming each EP retrieves a single lexical item, there could in the worst case be O(2n)
edges, each covering a different subset of the input semantics. Although in the general
case we cannot improve the complexity, we can make the processing steps involved
cheaper, for instance efficiently checking whether two edges are candidates for being
combined (see Section 3.1 below). We can also minimize the number of edges covering
each subset of EPs by ?packing? locally equivalent edges (Section 3.2).
A particular, identifiable source of complexity is that, as Kay [8] notes, when a word
has more than one intersective modifier an indefinite number of its modifiers may be
applied. For instance, when generating from the MRS in Figure 1, edges corresponding
to the partial realizations athlete, young athlete, Polish athlete, and young Polish athlete
will all be constructed. Even if a grammar constrains modifiers so there is only one valid
168 J. Carroll and S. Oepen
ordering, or the generator is able to pack equivalent edges covering the same EPs, the
number of edges built will still be 2n, because all possible complete and incomplete
phrases will be built. Using the example MRS, ultimately useless edges such as the
young athlete ran (omitting Polish) will be created.
Kay proposes an approach to this problem in which edges are checked before they
are created to see if they would ?seal off? access to a semantic index (x5 in this case) for
which there is still an unincorporated modifier. Although individual sets of modifiers still
result in exponential numbers of edges, the exponentiality is prevented from propagating
further. However, Carroll et al [12] argue that this check works only in limited circum-
stances, since for example in (1) the grammar must allow the index for ran to be available
all the way up the tree to How, and simultaneously also make available the indexes for
newspapers, say, and athlete at appropriate points so these words could be modified2.
(1) How quickly did the newspapers say the athlete ran?
Carroll et al describe an alternative technique which adjoins intersective modifiers into
edges in a second phase, after all possible edges that do not involve intersective modi-
fication have been constructed by chart generation. This overcomes the multiple index
problem described above and reduces the worst-case complexity of intersective modi-
fication in the chart generation phase to polynomial, but unfortunately the subsequent
phase which attempts to adjoin sets of modifiers into partial realizations is still expo-
nential. We describe below (Section 3.3) a related technique which delays processing of
intersective modifiers by inserting them into the generation forest, taking advantage of
dynamic programming to reduce the complexity of the second phase. We also present
a different approach which filters out edges based on accessibility of sets of seman-
tic indices (Section 3.4), which covers a wider variety of cases than just intersective
modification, and in practice is even more efficient.
Exponential numbers of edges imply exponential numbers of realizations. For an
application task we would usually want only one (the most natural or fluent) realization,
or a fixed small number of good realizations that the application could then itself select
from. In Section 3.5 we present an efficient algorithm for selectively unpacking the
generation forest to produce the n-best realizations according to a statistical model.
3 Efficient Wide-Coverage Realization
3.1 Relating Chart Edges and Semantic Components
Once lexical lookup is complete and up until a final, post-generation comparison of
results to the input MRS, the core phases of our generator exclusively operate on typed
feature structures (which are associated to chart edges). For efficiency reasons, our algo-
rithm avoids any complex operations on the original logical-form input MRS. In order
to best guide the search from the input semantics, however, we employ two techniques
that relate components of the logical form to corresponding sub-structures in the feature
2 White [3] describes an approach to dealing with intersective modifiers which requires the
grammarian to write a collection of rules that ?chunk? the input semantics into separate modi-
fier groups which are processed separately; this involves extra manual work, and also appears
to suffer from the same multiple index problem.
High Efficiency Realization for a Wide-Coverage Unification Grammar 169
structure (FS) universe: (i) Skolemization of variables and (ii) indexing by EP cover-
age. Of these, only the latter we find commonly discussed in the literature, but we expect
some equivalent of making variables ground to be present in most implementations.
As part of the process of looking up lexical items and grammar rules introducing se-
mantics in order to initialize the generator chart, all FS correspondences to logical vari-
ables from the input MRS are made ?ground? by specializing the relevant sub-structure
with Skolem constants uniquely reflecting the underlying variable, for example adding
constraints like [SKOLEM ?x5?] for all occurrences of x5 from our example MRS.
Skolemization, thus, assumes that distinct variables from the input MRS, where supplied,
cannot become co-referential during generation. Enforcing variable identity at the FS
level makes sure that composition (by means of FS unification) during rule applications
is compatible to the input semantics. In addition, it enables efficient pre-unification fil-
tering (see ?quick-check? below), and is a prerequisite for our index accessibility test
described in Section 3.4 below.
In chart parsing, edges are stored into and retrieved from the chart data structure
on the basis of their string start and end positions. This ensures that the parser will
only retrieve pairs of chart edges that cover compatible segments of the input string (i.e.
that are adjacent with respect to string position). In chart generation, Kay [8] proposed
indexing the chart on the basis of logical variables, where each variable denotes an
individual entity in the input semantics, and making the edge coverage compatibility
check a filter. Edge coverage (with respect to the EPs in the input semantics) would be
encoded as a bit vector, and for a pair of edges to be combined their corresponding bit
vectors would have to be disjoint.
We implement Kay?s edge coverage approach, using it not only when combining
active and inactive edges, but also for two further tasks in our approach to realization:
? in the second phase of chart generation to determine which intersective modifier(s)
can be adjoined into a partially incomplete subtree; and
? as part of the test for whether one edge subsumes another, for local ambiguity
factoring (see Section 3.2 below)3.
In our testing with the LinGO ERG, many hundreds or thousands of edges may be
produced for non-trivial input semantics, but there are only a relatively small number
of logical variables. Indexing edges on these variables involves bookkeeping that turns
out not to be worthwhile in practice; logical bit vector operations on edge coverage
take negligible time, and these serve to filter out the majority of edge combinations
with incompatible indices. The remainder are filtered out efficiently before unification
is attempted by a check on which rules can dominate which others, and the quick-check,
as developed for unification-based parsing [14]. For the quick-check, it turns out that
the same set of feature paths that most frequently lead to unification failure in parsing
also work well in generation.
3 We therefore have four operations on bit vectors representing EP coverage (C) in chart edges:
? concatenation of edges e1 and e2 ? e3: C(e3) = OR(C(e1), C(e2));
? can edges e1 and e2 combine? AND(C(e1), C(e2)) = 0;
? do edges e1 and e2 cover the same EPs? C(e1) = C(e2);
? do edges e1, . . . , en cover all input EPs? NOT(OR(C(e1), . . . , C(en)) = 0.
170 J. Carroll and S. Oepen
3.2 Local Ambiguity Factoring
In chart parsing with context free grammars, the parse forest (a compact representation
of the full set of parses) can only be computed in polynomial time if sub-analyses dom-
inated by the same non-terminal and covering the same segment of the input string are
?packed?, or factored into a single unitary representation [15]. Similar benefits accrue
for unification grammars without a context free backbone such as the LinGO ERG,
if the category equality test is replaced by feature structure subsumption [16]4; also,
feature structures representing the derivation history need to be restricted out when ap-
plying a rule [17]. The technique can be applied to chart realization if the input span is
expressed as coverage of the input semantics. For example, with the input of Figure 1,
the two phrases in (2) below would have equivalent feature structures, and we pack the
one found second into the one found first, which then acts as the representative edge for
all subsequent processing.
(2) young Polish athlete | Polish young athlete
We have found that packing is crucial to efficiency: realization time is improved by more
than an order of magnitude for inputs with more than 500 realizations (see Section 4).
Changing packing to operate with respect just to feature structure equality rather than
subsumption degrades throughput significantly, resulting in worse overall performance
than with packing disabled completely: in other words, equivalence-only packing fails
to recoup the cost of the feature structure comparisons involved.
A further technique we use is to postpone the creation of feature structures for active
edges until they are actually required for a unification operation, since many end up as
dead ends. Oepen and Carroll [18] do a similar thing in their ?hyper-active? parsing
strategy, for the same reason.
3.3 Delayed Modifier Insertion
As discussed in Section 2.3, Carroll et al [12] adjoin intersective modifiers into each
partial tree extracted from the forest; their algorithm searches for partitions of modifier
phrases to adjoin, and tries all combinations. This process adds an exponential (in the
number of modifiers) factor to the complexity of extracting each partial realization.
This is obviously unsatisfactory, and in practice is slow for larger problems when
there are many possible modifiers. We have devised a better approach which delays
processing of intersective modifiers by inserting them into the generation forest at ap-
propriate locations before the forest is unpacked. By doing this, we take advantage of
the dynamic programming-based procedure for unpacking the forest to reduce the com-
plexity of the second phase. The procedure is even more efficient if realizations are
unpacked selectively (section 3.5).
3.4 Index Accessibility Filtering
Kay?s original proposal for dealing efficiently with modifiers founders because more
than one semantic index may need to be accessible at any one time (leading to the
4 Using subsumption-based packing means that the parse forest may represent some globally
inconsistent analyses, so these must be filtered out when the forest is unpacked.
High Efficiency Realization for a Wide-Coverage Unification Grammar 171
alternative solutions of modifier adjunction, and of chunking the input semantics ? see
Sections 2.3 and 3.3).
However, it turns out that Kay?s proposal can form the basis of a more generally
applicable approach to the problem. We assume that we have available an operation
collect-semantic-vars() that traverses a feature structure and returns the set of semantic
indices that it makes available5. We store in each chart edge two sets: one of semantic
variables in the feature structure that are accessible (that is, they are present in the
feature structure and could potentially be picked by another edge when it is combined
with this one), and a second set of inaccessible semantic variables (ones that were once
accessible but no longer are). Then,
? when an active edge is combined with an inactive edge, the accessible sets and
inaccessible sets in the resulting edge are the union of the corresponding sets in the
original edges;
? when an inactive edge is created, its accessible set is computed to be the semantic
indices available in its feature structure, and the variables that used to be accessible
but are no longer in the accessible set are added to its inaccessible set, i.e.
1 tmp ? edge.accessible;
2 edge.accessible ? collect-semantic-vars(edge.fs)
3 edge.inaccessible ? (tmp \ edge.accessible) ? edge.inaccessible
? immediately after creating an inactive edge, each EP in the input semantics that
the edge does not (yet) cover is inspected, and if the EP?s index is in the edge?s
inaccessible set then the edge is discarded (since there is no way in the future that
the EP could be integrated with any extension of the edge?s semantics).
A nice property of this new technique is that it applies more widely than to just
intersective modification: for instance, if the input semantics were to indicate that a
phrase should be negated, no edges would be created that extended that phrase without
the negation being present. Section 4 shows this technique results in dramatic improve-
ments in realization efficiency.
3.5 Selective Unpacking
The selective unpacking procedure outlined in this section allows us to extract a small
set of n-best realizations from the generation forest at minimal cost. The global rank
order is determined by a conditional Maximum Entropy (ME) model ? essentially an
adaptation of recent HPSG parse selection work to the realization ranking task [19]. We
use a similar set of features to Toutanova and Manning [20], but our procedure dif-
fers from theirs in that it applies the stochastic model before unpacking, in a guided
search through the generation forest. Thus, we avoid enumerating all candidate realiza-
tions. Unlike Malouf and van Noord [21], on the other hand, we avoid an approximative
beam search during forest creation and guarantee to produce exactly the n-best realiza-
tions (according to the ME model). Further looking at related parse selection work, our
procedure is probably most similar to those of Geman and Johnson [22] and Miyao and
5 Implementing collect-semantic-vars() can be efficient: searching for Skolem constants through-
out the full structure, it does a similar amount of computation as a single unification.
172 J. Carroll and S. Oepen
1 ?
?
2 3
? ?
4 3
?
2 ?
?
5 6
? ?
5 7
?
4 ?
?
8 6
? ?
8 7
? ?
9 6
? ?
9 7
?
6 ?
?
10
? ?
11
?
Fig. 2. Sample generator forest and sub-node decompositions: ovals in the forest (on the left)
indicate packing of edges under subsumption, i.e. edges 4 , 7 , 9 , and 11 are not in the gen-
erator chart proper. During unpacking, there will be multiple ways of instantiating a chart edge,
each obtained from cross-multiplying alternate daughter sequences locally. The elements of this
cross-product we call decomposition, and they are pivotal points both for stochastic scoring and
dynamic programming in selective unpacking. The table on the right shows all non-leaf decom-
positions for our example generator forest: given two ways of decomposing 6 , there will be three
candidate ways of instantiating 2 and six for 4 , respectively, for a total of nine full trees.
Tsujii [23], but neither provide a detailed discussion of the dependencies between local-
ity of ME features and the complexity of the read-out procedure from a packed forest.
Two key notions in our selective unpacking procedure are the concepts of (i) decom-
posing an edge locally into candidate ways of instantiating it and of (ii) nested contexts
of ?horizontal? search for ranked hypotheses (i.e. uninstantiated edges) about candidate
subtrees. See Figure 2 for examples of edge decomposition, but note that the ?depth?
of each local cross-product needs to correspond to the maximum required context size
of ME features; for ease of exposition, our examples assume a context size of no more
than depth one (but the algorithm straightforwardly generalizes to larger contexts). Given
one decomposition ? i.e. a vector of candidate daughters to a token construction ? there
can be multiple ways of instantiating each daughter: a parallel index vector ?i0 . . . in?
serves to keep track of ?vertical? search among daughter hypotheses, where each index ij
denotes the i-th instantiation (hypothesis) of the daughter at position j. Hypotheses are
associated with ME scores and ordered within each nested context by means of a local
agenda (stored in the original representative edge, for convenience). Given the additive
nature of ME scores on complete derivations, it can be guaranteed that larger derivations
including an edge e as a sub-constituent on the fringe of their local context of optimiza-
tion will use the best instantiation of e in their own best instantiation. The second-best
larger instantiation, in turn, will be obtained from moving to the second-best hypothesis
for one of the elements in the (right-hand side of the) decomposition. Therefore, nested
local optimizations result in a top-down, exact n-best search through the generation for-
est, and matching the ?depth? of local decompositions to the maximum required ME
feature context effectively prevents exhaustive cross-multiplication of packed nodes.
The main function hypothesize-edge() in Figure 3 controls both the ?horizontal? and
?vertical? search, initializing the set of decompositions and pushing initial hypothe-
ses onto the local agenda when called on an edge for the first time (lines 11 ? 17).
Furthermore, the procedure retrieves the current next-best hypothesis from the agenda
(line 18), generates new hypotheses by advancing daughter indices (while skipping over
High Efficiency Realization for a Wide-Coverage Unification Grammar 173
1 procedure selectively-unpack-edge(edge , n) ?
2 results ? ? ?; i ? 0;
3 do
4 hypothesis ? hypothesize-edge(edge , i); i ? i + 1;
5 if (new ? instantiate-hypothesis(hypothesis)) then
6 n ? n ? 1; results ? results ? ?new?;
7 while (hypothesis and n ? 1)
8 return results;
9 procedure hypothesize-edge(edge , i) ?
10 if (edge.hypotheses[i]) return edge.hypotheses[i];
11 if (i = 0) then
12 for each (decomposition in decompose-edge(edge)) do
13 daughters ? ? ?; indices ? ? ?
14 for each (edge in decomposition.rhs) do
15 daughters ? daughters ? ?hypothesize-edge(edge, 0)?;
16 indices ? indices ? ?0?;
17 new-hypothesis(edge, decomposition, daughters, indices);
18 if (hypothesis ? edge.agenda.pop()) then
19 for each (indices in advance-indices(hypothesis.indices)) do
20 if (indices ? edge.indices) then continue
21 daughters ? ? ?;
22 for each (edge in hypothesis.decomposition.rhs) each (i in indices) do
23 daughter ? hypothesize-edge(edge, i);
24 if (not daughter) then
25 daughters ? ? ?; break
26 daughters ? daughters ? ?daughter?;
27 if (daughters) then new-hypothesis(edge, decomposition, daughters, indices)
28 edge.hypotheses[i] ? hypothesis;
29 return hypothesis;
30 procedure new-hypothesis(edge , decomposition , daughters , indices) ?
31 hypothesis ? new hypothesis(decomposition, daughters, indices);
32 edge.agenda.insert(score-hypothesis(hypothesis), hypothesis);
33 edge.indices ? edge.indices ? {indices};
Fig. 3. Selective unpacking procedure, enumerating the n best realizations for a top-level result
edge from the generation forest. An auxiliary function decompose-edge() performs local cross-
multiplication as shown in the examples in Figure 2. Another utility function not shown in pseudo-
code is advance-indices(), another ?driver? routine searching for alternate instantiations of daughter
edges, e.g. advance-indices(?0 2 1?) ? {?1 2 1? ?0 3 1? ?0 2 2?}. Finally, instantiate-hypothesis() is
the function that actually builds result trees, replaying the unifications of constructions from the
grammar (as identified by chart edges) with the feature structures of daughter constituents.
configurations seen earlier) and calling itself recursively for each new index (lines 19 ?
27), and, finally, arranges for the resulting hypothesis to be cached for later invocations
on the same edge and i values (line 28). Note that we only invoke instantiate-hypothesis()
on complete, top-level hypotheses, as the ME features of Toutanova and Manning [20]
can actually be evaluated prior to building each full feature structure. However, the
procedure could be adapted to perform instantiation of sub-hypotheses within each lo-
cal search, should additional features require it. For better efficiency, our instantiate-
hypothesis() routine already uses dynamic programming for intermediate results.
4 Evaluation and Summary
Below we present an empirical evaluation of each of the refinements discussed in Sec-
tions 3.2 through 3.5. Using the LinGO ERG and its ?hike? treebank ? a 330-sentence
174 J. Carroll and S. Oepen
Table 1. Realization efficiency for various instantiations of our algorithm. The table is broken
down by average ambiguity rates, the first two columns showing the number of items per aggre-
gate and average string length. Subsequent columns show relative cpu time of one- and two-phase
realization with or without packing and filtering, shown as a relative multiplier of the baseline
performance in the 1p+f+ column. The rightmost column is for selective unpacking of up to 10
trees from the forest produced by the baseline configuration, again as a factor of the baseline. (The
quality of the selected trees depends on the statistical model and the degree of overgeneration in
the grammar, and is a completely separate issue which we do not address in this paper).
items length 1p?f? 2p?f? 1p?f+ 1p+f? 2p+f? 1p+f+ n=10Aggregate  ? ? ? ? ? ? s ?
500 < trees 9 23.9 31.76 20.95 11.98 9.49 3.69 31.49 0.33
100 < trees ? 500 22 17.4 53.95 36.80 3.80 8.70 4.66 5.61 0.42
50 < trees ? 100 21 18.1 51.53 13.12 1.79 8.09 2.81 3.74 0.62
10 < trees ? 50 80 14.6 35.50 18.55 1.82 6.38 3.67 1.77 0.89
0 ? trees ? 10 185 10.5 9.62 6.83 1.19 6.86 3.62 0.58 0.95
Overall 317 12.9 35.03 20.22 5.97 8.21 3.74 2.32 0.58
Coverage 95% 97% 99% 99% 100% 100% 100%
collection of instructional text taken from Norwegian tourism brochures ? we bench-
marked various generator configurations, starting from the ?gold? standard MRS formula
recorded for each utterance in the treebank. At 12.8 words, average sentence length in
the original ?hike? corpus is almost exactly what we see as the average length of all
paraphrases obtained from the generator (see Table 1); from the available reference
treebanks for the ERG, ?hike? appears to be among the more complex data sets.
Table 1 summarizes relative generator efficiency for various configurations, where
we use the best-performing exhaustive procedure 1p+f+ (one-phase generation with
packing and index accessibility filtering) as a baseline. The configuration 1p?f? (one-
phase, no packing or filtering) corresponds to the basic procedure suggested by Kay [8],
while 2p?f? (two-phase processing of modifiers without packing and filtering) imple-
ments the algorithm presented by Carroll et al [12]. Combining packing and filter-
ing clearly outperforms both these earlier configurations, i.e. giving an up to 50 times
speed-up for inputs with large numbers of realizations. Additional columns contrast the
various techniques in isolation, thus allowing an assessment of the individual strengths
of our proposals. On low- to medium-ambiguity items, for example, filtering gives rise
to a bigger improvement than packing, but packing appears to flatten the curve more.
Both with and without packing, filtering improves significantly over the Carroll et al
two-phase approach to intersective modifiers (i.e. comparing columns 2p?f? and 2p+f?
to 1p?f+ and 1p+f+, respectively), thus confirming the increased generality of our solu-
tion to the modification problem. Finally, the benefits of packing and filtering combine
more than merely multiplicatively: compared to 1p?f?, just filtering gives a speed-up of
5.9, and just packing a speed-up of 4.3. At 25, the product of these factors is well below
the overall reduction of 35 that we obtain from the combination of both techniques.
While the rightmost column in Table 1 already indicates that 10-best selective un-
packing further improves generator performance by close to a factor of two, Figure 4
breaks down generation time with respect to forest creation vs. unpacking time. When
plotted against increasing input complexity (in terms of the ?size? of the input MRS),
forest creation appears to be a low-order polynomial (or better), whereas exhaustive
High Efficiency Realization for a Wide-Coverage Unification Grammar 175
0 5 10 15 20 25 30 35
Input Complexity (Number of EPs in MRS)
0
2
4
6
8
10
12
14
s
(generated by [incr tsdb()] at 15-apr-2005 (00:55 h))
? packed forest creation
? selective unpacking
? exhaustive unpacking
Fig. 4. Break-down of generation times (in seconds) according to realization phases and input
complexity (approximated in the number of EPs in the original MRS used for generation). The
three curves are, from ?bottom? to ?top?, the average time for constructing the packed generation
forest, selective unpacking time (using n = 10), and exhaustive unpacking time. Note that both
unpacking times are shown as increments on top of the forest creation time.
unpacking (necessarily) results in an exponential explosion of generation time: with
more than 25 EPs, it clearly dominates total processing time. Selective unpacking, in
contrast, appears only mildly sensitive to input complexity and even on complex inputs
adds no more than a minor cost to total generation time. Thus, we obtain an over-
all observed run-time performance of our wide-coverage generator that is bounded (at
least) polynomially. Practical generation times using the LinGO ERG average below or
around one second for outputs of fifteen words in length, i.e. time comparable to human
production.
References
1. Flickinger, D.: On building a more efficient grammar by exploiting types. Natural Language
Engineering 6 (1) (2000) 15 ? 28
2. Butt, M., Dyvik, H., King, T.H., Masuichi, H., Rohrer, C.: The Parallel Grammar project.
In: Proceedings of the COLING Workshop on Grammar Engineering and Evaluation, Taipei,
Taiwan (2002) 1 ? 7
3. White, M.: Reining in CCG chart realization. In: Proceedings of the 3rd International Con-
ference on Natural Language Generation, Hampshire, UK (2004)
4. Moore, J., Foster, M.E., Lemon, O., White, M.: Generating tailored, comparative descriptions
in spoken dialogue. In: Proceedings of the 17th International FLAIRS Conference, Miami
Beach, FL (2004)
5. Oepen, S., Dyvik, H., L?nning, J.T., Velldal, E., Beermann, D., Carroll, J., Flickinger, D.,
Hellan, L., Johannessen, J.B., Meurer, P., Nordga?rd, T., Rose?n, V.: Som a? kapp-ete med
trollet? Towards MRS-based Norwegian ? English Machine Translation. In: Proceedings
of the 10th International Conference on Theoretical and Methodological Issues in Machine
Translation, Baltimore, MD (2004)
6. Whitelock, P.: Shake-and-bake translation. In: Proceedings of the 14th International Confer-
ence on Computational Linguistics, Nantes, France (1992) 610 ? 616
7. Phillips, J.: Generation of text from logical formulae. Machine Translation 8 (1993) 209 ?
235
176 J. Carroll and S. Oepen
8. Kay, M.: Chart generation. In: Proceedings of the 34th Meeting of the Association for
Computational Linguistics, Santa Cruz, CA (1996) 200 ? 204
9. Gardent, C., Thater, S.: Generating with a grammar based on tree descriptions. A constraint-
based approach. In: Proceedings of the 39th Meeting of the Association for Computational
Linguistics, Toulouse, France (2001)
10. Shieber, S., van Noord, G., Pereira, F., Moore, R.: Semantic head-driven generation. Com-
putational Linguistics 16 (1990) 30 ? 43
11. Moore, R.: A complete, efficient sentence-realization algorithm for unification grammar. In:
Proceedings of the 2nd International Natural Language Generation Conference, Harriman,
NY (2002) 41 ? 48
12. Carroll, J., Copestake, A., Flickinger, D., Poznanski, V.: An efficient chart generator for
(semi-)lexicalist grammars. In: Proceedings of the 7th European Workshop on Natural Lan-
guage Generation, Toulouse, France (1999) 86 ? 95
13. Copestake, A., Flickinger, D., Sag, I., Pollard, C.: Minimal Recursion Semantics. An intro-
duction. (1999)
14. Kiefer, B., Krieger, H.U., Carroll, J., Malouf, R.: A bag of useful techniques for efficient and
robust parsing. In: Proceedings of the 37th Meeting of the Association for Computational
Linguistics, College Park, MD (1999) 473 ? 480
15. Billot, S., Lang, B.: The structure of shared forests in ambiguous parsing. In: Proceedings of
the 27th Meeting of the Association for Computational Linguistics, Vancouver, BC (1989)
143 ? 151
16. Oepen, S., Carroll, J.: Ambiguity packing in constraint-based parsing. Practical results. In:
Proceedings of the 1st Conference of the North American Chapter of the ACL, Seattle, WA
(2000) 162 ? 169
17. Shieber, S.: Using restriction to extend parsing algorithms for complex feature-based for-
malisms. In: Proceedings of the 23rd Meeting of the Association for Computational Linguis-
tics, Chicago, IL (1985) 145 ? 152
18. Oepen, S., Carroll, J.: Performance profiling for parser engineering. Natural Language
Engineering 6 (1) (2000) 81 ? 97
19. Velldall, E., Oepen, S., Flickinger, D.: Paraphrasing treebanks for stochastic realization rank-
ing. In: Proceedings of the 3rd Workshop on Treebanks and Linguistic Theories, Tu?bingen,
Germany (2004)
20. Toutanova, K., Manning, C.: Feature selection for a rich HPSG grammar using decision
trees. In: Proceedings of the 6th Conference on Natural Language Learning, Taipei, Taiwan
(2002)
21. Malouf, R., van Noord, G.: Wide coverage parsing with stochastic attribute value grammars.
In: Proceedings of the IJCNLP workshop Beyond Shallow Analysis, Hainan, China (2004)
22. Geman, S., Johnson, M.: Dynamic programming for parsing and estimation of stochastic
unification-based grammars. In: Proceedings of the 40th Meeting of the Association for
Computational Linguistics, Philadelphia, PA (2002)
23. Miyao, Y., Tsujii, J.: Maximum entropy estimation for feature forests. In: Proceedings of the
Human Language Technology Conference, San Diego, CA (2002)
Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 595?603,
Uppsala, Sweden, 11-16 July 2010. c?2010 Association for Computational Linguistics
Evaluating Multilanguage-Comparability of Subjectivity Analysis
Systems
Jungi Kim, Jin-Ji Li and Jong-Hyeok Lee
Division of Electrical and Computer Engineering
Pohang University of Science and Technology, Pohang, Republic of Korea
{yangpa,ljj,jhlee}@postech.ac.kr
Abstract
Subjectivity analysis is a rapidly grow-
ing field of study. Along with its ap-
plications to various NLP tasks, much
work have put efforts into multilingual
subjectivity learning from existing re-
sources. Multilingual subjectivity analy-
sis requires language-independent crite-
ria for comparable outcomes across lan-
guages. This paper proposes to mea-
sure the multilanguage-comparability of
subjectivity analysis tools, and provides
meaningful comparisons of multilingual
subjectivity analysis from various points
of view.
1 Introduction
The field of NLP has seen a recent surge in the
amount of research on subjectivity analysis. Along
with its applications to various NLP tasks, there
have been efforts made to extend the resources
and tools created for the English language to other
languages. These endeavors have been success-
ful in constructing lexicons, annotated corpora,
and tools for subjectivity analysis in multiple lan-
guages.
There are multilingual subjectivity analysis sys-
tems available that have been built to monitor and
analyze various concerns and opinions on the In-
ternet; among the better known are OASYS from
the University of Maryland that analyzes opinions
on topics from news article searches in multiple
languages (Cesarano et al, 2007)1 and TextMap,
an entity search engine developed by Stony Brook
University for sentiment analysis along with other
functionalities (Bautin et al, 2008).2 Though these
systems currently rely on English analysis tools
and a machine translation (MT) technology to
1http://oasys.umiacs.umd.edu/oasysnew/
2http://www.textmap.com/
translate other languages into English, up-to-date
research provides various ways to analyze subjec-
tivity in multilingual environments.
Given sentiment analysis systems in differ-
ent languages, there are many situations when
the analysis outcomes need to be multilanguage-
comparable. For example, it has been common
these days for the Internet users across the world
to share their views and opinions on various top-
ics including music, books, movies, and global af-
fairs and incidents, and also multinational compa-
nies such as Apple and Samsung need to analyze
customer feedbacks for their products and services
from many countries in different languages. Gov-
ernments may also be interested in monitoring ter-
rorist web forums or its global reputation. Sur-
veying these opinions and sentiments in various
languages involves merging the analysis outcomes
into a single database, thereby objectively compar-
ing the result across languages.
If there exists an ideal subjectivity analy-
sis system for each language, evaluating the
multilanguage-comparability would be unneces-
sary because the analysis in each language would
correctly identify the exact meanings of all in-
put texts regardless of the language. However, this
requirement is not fulfilled with current technol-
ogy, thus the need for defining and measuring the
multilanguage-comparability of subjectivity anal-
ysis systems is evident.
This paper proposes to evaluate the
multilanguage-comparability of multilingual
subjectivity analysis systems. We build a number
of subjectivity classifiers that distinguishes sub-
jective texts from objective ones, and measure
the multilanguage-comparability according to our
proposed evaluation method. Since subjectivity
analysis tools in languages other than English are
not readily available, we focus our experiments on
comparing different methods to build multilingual
analysis systems from the resources and systems
595
created for English. These approaches enable us to
extend a monolingual system to many languages
with a number of freely available NLP resources
and tools.
2 Related Work
Much research have been put into developing
methods for multilingual subjectivity analysis re-
cently. With the high availability of subjectivity re-
sources and tools in English, an easy and straight-
forward approach would be to employ a machine
translation (MT) system to translate input texts
in target languages into English then carry out
the analyses using an existing subjectivity analy-
sis tool (Kim and Hovy, 2006; Bautin et al, 2008;
Banea et al, 2008). Mihalcea et al (2007) and
Banea et al (2008) proposed a number of ap-
proaches exploiting a bilingual dictionary, a paral-
lel corpus, and an MT system to port the resources
and systems available in English to languages with
limited resources.
For subjectivity lexicons translation, Mihalcea
et al (2007) and Wan (2008) used the first sense in
a bilingual dictionary, Kim and Hovy (2006) used
a parallel corpus and a word alignment tool to ex-
tract translation pairs, and Kim et al (2009) used
a dictionary to translate and a link analysis algo-
rithm to refine the matching intensity.
To overcome the shortcomings of available re-
sources and to take advantage of ensemble sys-
tems, Wan (2008) and Wan (2009) explored meth-
ods for developing a hybrid system for Chinese us-
ing English and Chinese sentiment analyzers. Ab-
basi et al (2008) and Boiy and Moens (2009) have
created manually annotated gold standards in tar-
get languages and studied various feature selec-
tion and learning techniques in machine learning
approaches to analyze sentiments in multilingual
web documents.
For learning multilingual subjectivity, the lit-
erature tentatively concludes that translating lex-
icon is less dependable in terms of preserving sub-
jectivity than corpus translation (Mihalcea et al,
2007; Wan, 2008), and though corpus translation
results in modest performance degradation, it pro-
vides a viable approach because no manual la-
bor is required (Banea et al, 2008; Brooke et al,
2009).
Based on the observation that the performances
of subjectivity analysis systems in comparable
experimental settings for two languages differ,
Texts with an identical negative sentiment:* The iPad could cannibalize the e-reader market. * ?????(iPad) ??? ???(e-reader market) 
???? ? ??(could cannibalize).
Texts with different strengths of positive sentiments:* Samsung cell phones have excellent battery life.* ??(Samsung) ????(cell phone) ????(battery) ????(somehow or other) ????(last long).
Figure 1: Examples of sentiments in multilingual
text
Banea et al (2008) have attributed the variations
in the difficulty level of subjectivity learning to
the differences in language construction. Bautin et
al. (2008)?s system analyzes the sentiment scores
of entities in multilingual news and blogs and ad-
justed the sentiment scores using entity sentiment
probabilities of languages.
3 Multilanguage-Comparability
3.1 Motivation
The quality of a subjectivity analysis tool is mea-
sured by its ability to distinguish subjectivity from
objectivity and/or positive sentiments from nega-
tive sentiments. Additionally, a multilingual sub-
jectivity analysis system is required to generate
unbiased analysis results across languages; the
system should base its outcome solely on the sub-
jective meanings of input texts irrespective of the
language, and the equalities and inequalities of
subjectivity labels and intensities must be useful
within and throughout the languages.
Let us consider two cases where the pairs of
multilingual inputs in English and Korean have
identical and different subjectivity meanings (Fig-
ure 1). The first pair of texts carry a negative sen-
timent about how the release of a new electronics
device might affect an emerging business market.
When a multilanguage-comparable system is in-
putted with such a pair, its output should appropri-
ately reflect the negative sentiment, and be identi-
cal for both texts. The second pair of texts share
a similar positive sentiment about a mobile de-
vice?s battery capacity but with different strengths.
A good multilingual system must be able to iden-
tify the positive sentiments and distinguish the dif-
ferences in their intensities.
However, these kinds of conditions cannot be
measured with performance evaluations indepen-
596
dently carried out on each language; A system
with a dissimilar ability to analyze subjective ex-
pressions from one language to another may de-
liver opposite labels or biased scores on texts with
an identical subjective meaning, and vice versa,
but still might produce similar performances on
the evaluation data.
Macro evaluations on individual languages can-
not provide any conclusions on the system?s
multilanguage-comparability capability. To mea-
sure how much of a system?s judgment principles
are preserved across languages, an evaluation from
a different perspective is necessary.
3.2 Evaluation Approach
An evaluation of multilanguage-comparability
may be done in two ways: measuring agreements
in the outcomes of a pair of multilingual texts with
an identical subjective meaning, or measuring the
consistencies in the label and/or accordance in the
order of intensity of a pair of texts with different
subjectivities.
There are advantages and disadvantages to each
approaches. The first approach requires multi-
lingual texts aligned at the level of specificity,
for instance, document, sentence and phrase, that
the subjectivity analysis system works. Text cor-
pora for MT evaluation such as newspapers,
books, technical manuals, and government offi-
cial records provide a wide variety of parallel
texts, typically at the sentence level. Annotating
these types of corpus can be efficient; as par-
allel texts must have identical semantic mean-
ings, subjectivity?related annotations for one lan-
guage can be projected into other languages with-
out much loss of accuracy.
The latter approach accepts any pair of multi-
lingual texts as long as they are annotated with la-
bels and/or intensity. In this case, evaluating the la-
bel consistency of a multilingual system is only as
difficult as evaluating that of a monolingual sys-
tem; we can produce all possible pairs of texts
from test corpora annotated with labels for each
language. Evaluating with intensity is not easy for
the latter approach; if test corpora already exist
with intensity annotations for both languages, nor-
malizing the intensity scores to a comparable scale
is necessary (yet is uncertain unless every pair is
checked manually), otherwise every pair of mul-
tilingual texts needs a manual annotation with its
relative order of intensity.
In this paper, we utilize the first approach be-
cause it provides a more rational means; we can
reasonably hypothesize that text translated into an-
other language by a skilled translator carries an
identical semantic meaning and thereby conveys
identical subjectivity. Therefore the required re-
source is more easily attained in relatively inex-
pensive ways.
For evaluation, we measure the consistency in
the subjectivity labels and the correlation of sub-
jectivity intensity scores of parallel texts. Section
5.1 describes the details of evaluation metrics.
4 Multilingual Subjectivity System
We create a number of multilingual systems con-
sisting of multiple subsystems each processing a
language, where one system analyzes English, and
the other systems analyze the Korean, Chinese,
and Japanese languages. We try to reproduce a set
of systems using diverse methods in order to com-
pare the systems and find out which methods are
more suitable for multilanguage-comparability.
4.1 Source Language System
We adopt the three systems described below as our
source language systems: a state-of-the-art sub-
jectivity classifier, a corpus-based, and a lexicon-
based systems. The resources needed for devel-
oping the systems or the system itself are readily
available for research purposes. In addition, these
systems cover the general spectrum of current ap-
proaches to subjectivity analysis.
State-of-the-art (S-SA): OpinionFinder is a
publicly-available NLP tool for subjectivity analy-
sis (Wiebe and Riloff, 2005; Wilson et al, 2005).3
The software and its resources have been widely
used in the field of subjectivity analysis, and it
has been the de facto standard system against
which new systems are validated. We use a high-
coverage classifier from the OpinionFinder?s two
sentence-level subjectivity classifiers. This Naive
Bayes classifier builds upon a corpus annotated by
a high-precision classifier with the bootstrapping
of the corpus and extraction patterns. The classi-
fier assesses a sentence?s subjectivity with a label
and a score for confidence in its judgment.
Corpus-based (S-CB): The MPQA opinion cor-
pus is a collection of 535 newspaper articles in En-
glish annotated with opinions and private states at
3http://www.cs.pitt.edu/mpqa/opinionfinderrelease/, ver-
sion 1.5
597
the sub-sentence level (Wiebe et al, 2003).4 We
retrieve the sentence level subjectivity labels for
11,111 sentences using the set of rules described
in (Wiebe and Riloff, 2005). The corpus provides
a relatively balanced corpus with 55% subjective
sentences. We train an ML-based classifier us-
ing the corpus. Previous studies have found that,
among several ML-based approaches, the SVM
classifier generally performs well in many subjec-
tivity analysis tasks (Pang et al, 2002; Banea et
al., 2008).
We use SVMLight with its default configura-
tions,5 inputted with a sentence represented as a
feature vector of word unigrams and their counts
in the sentence. An SVM score (a margin or the
distance from a learned decision boundary) with a
positive value predicts the input as being subjec-
tive, and negative value as objective.
Lexicon-based (S-LB): OpinionFinder contains a
list of English subjectivity clue words with in-
tensity labels (Wilson et al, 2005). The lexicon
is compiled from several manually and automati-
cally built resources and contains 6885 unique en-
tries.
Riloff and Wiebe (2003) constructed a high-
precision classifier for contiguous sentences us-
ing the number of strong and weak subjective
words in current and nearby sentences. Unlike pre-
vious work, we do not (or rather, cannot) main-
tain assumptions about the proximity of input text.
Using the lexicon, we build a simple and high-
coverage rule-based subjectivity classifier. Setting
the scores of strong and weak subjective words as
1.0 and 0.5, we evaluate the subjectivity of a given
sentence as the sum of subjectivity scores; above
a threshold, the input is subjective, and otherwise
objective. The threshold value is optimized for an
F-measure using the MPQA corpus, and is set to
1.0 throughout our experiments.
4.2 Target Language System
To construct a target language system leveraging
on available resources in the source language, we
consider three approaches from previous litera-
ture:
1. translating test sentences in target language
into source language and inputting them into
4http://www.cs.pitt.edu/mpqa/databaserelease/, version
1.2
5http://svmlight.joachims.org/, version 6.02
a source language system (Kim and Hovy,
2006; Bautin et al, 2008; Banea et al, 2008)
2. translating a source language training corpus
into target language and creating a corpus-
based system in target language (Banea et al,
2008)
3. translating a subjectivity lexicon from source
language to target language and creating a
lexicon-based system in target language (Mi-
halcea et al, 2007)
Each approach has its advantages and disadvan-
tages. The advantage of the first approach is its
simple architecture, clear separation of subjectiv-
ity and MT systems, and that it has only one sub-
jectivity system, and is thus easier to maintain.
Its disadvantage is that the time-consuming MT
has to be executed for each text input. In the sec-
ond and third approaches, a subjectivity system in
the target language is constructed sharing corpora,
rules, and/or features with the source language
system. Later on, it may also include its own set
of resources specifically engineered for the target
language as a performance improvement. How-
ever, keeping the systems up-to-date would require
as much effort as the number of languages. All
three approaches use MT, and would suffer sig-
nificantly if the translation results are poor.
Using the first approach, we can easily adopt all
three source language systems;
? Target input translated into source, analyzed
by source language system S-SA
? Target input translated into source, analyzed
by source language system S-CB
? Target input translated into source, analyzed
by source language system S-LB
The second and the third approaches are carried
out as follows:
Corpus-based (T-CB): We translate the MPQA
corpus into the target languages sentence by sen-
tence using a web-based service.6 Using the same
method for S-CB, we train an SVM model for
each language with the translated training corpora.
Lexicon-based (T-LB): This classifier is identi-
cal to S-LB, where the English lexicon is replaced
by one of the target languages. We automatically
translate the lexicon using free bilingual dictionar-
ies.7 First, the entries in the lexicon are looked
6Google Translate (http://translate.google.com/)
7quick english-korean, quick eng-zh CN, and JMDict
from StarDict (http://stardict.sourceforge.net/) licensed under
GPL and EDRDG.
598
Table 1: Agreement on subjectivity (S for subjec-
tive, O objective) of 859 sentence chunks in Ko-
rean between two annotators (An. 1 and An. 2).
An. 2
S O Total
A
n.
1 S 371 93 464
O 23 372 395
Total 394 465 859
up in the dictionary, if they are found, we se-
lect the first word in the first sense of the def-
inition. If the entry is not in the dictionary, we
lemmatize it,8 then repeat the search. Our sim-
ple approach produces moderate-sized lexicons
(3,808, 3,980, 3,027 for Korean, Chinese, and
Japanese) compared to Mihalcea et al (2007)?s
complicated translation approach (4,983 Roma-
nian words). The threshold values are optimized
using the MPQA corpus translated into each tar-
get language.9
5 Experiment
5.1 Experimental Setup
Test Corpus
Our evaluation corpus consists of 50 parallel
newspaper articles from the Donga Daily News
Website.10 The website provides news articles in
Korean and their human translations in English,
Japanese, and Chinese. We selected articles that
contain Editorial in its English title from a 30-
day period. Three human annotators who are flu-
ent in the two languages manually annotated N-
to-N sentence alignments for each language pairs
(KR-EN, KR-CH, KR-JP). By keeping only the
sentence chunks whose Korean chunk appears in
all language pairs, we were left with 859 sentence
chunk pairs.
The corpus was preprocessed with NLP tools
for each language,11 and the Korean, Chinese, and
Japanese texts were translated into English with
the same web-based service used to translate the
training corpus in Section 4.2.
Manual Annotation and Agreement Study
8JWI (http://projects.csail.mit.edu/jwi/)
9Korean 1.0, Chinese 1.0, and Japanese 0.5
10http://www.donga.com/
11Stanford POS Tagger 1.5.1 and Stanford Chinese Word
Segmenter 2008-05-21 (http://nlp.stanford.edu/software/),
Chasen 2.4.4 (http://chasen-legacy.sourceforge.jp/), Korean
Morphological Analyzer (KoMA) (http://kle.postech.ac.kr/)
Table 2: Agreement on projection of subjectivity
(S for subjective, O objective) from Korean (KR)
to English (EN) by one annotator.
EN
S O Total
K
R
S 458 6 464
O 12 383 395
Total 470 389 859
To assess the performance of our subjectiv-
ity analysis systems, the Korean sentence chunks
were manually annotated by two native speakers
of Korean with Subjective and Objective labels
(Table 1). A proportion agreement of 0.86 and a
kappa value of 0.73 indicate a substantial agree-
ment between the two annotators. We set aside
743 sentence chunks that both annotators agreed
on for the automatic evaluation of subjectivity
analysis systems, thereby removing the borderline
cases, which are difficult even for humans to as-
sess. The corresponding sentence chunks for other
languages were extracted and tagged with labels
equivalent to Korean chunks.
In addition, to verify how consistently the sub-
jectivity of the original texts is projected to the
translated, we carried out another manual annota-
tion and agreement study with Korean and English
sentence chunks (Table 2).
Note that our cross-lingual agreement study is
similar to the one carried out by Mihalcea et
al. (2007), where two annotators labeled the sen-
tence subjectivity of a parallel text in different lan-
guages. They reported that, similarly to monolin-
gual annotations, most cases of disagreements on
annotations are due to the differences in the anno-
tators? judgments on subjectivity, and the rest from
subjective meanings lost in the translation process
and figurative language such as irony.
To avoid the role played by annotators? pri-
vate views from disagreements, the subjectivity of
sentence chunks in English were manually anno-
tated by one of the annotators for the Korean text.
Judged by the same annotator, we speculate that
the disagreement in the annotation should account
only for the inconsistency in the subjectivity pro-
jection. By proportion, the agreement between the
annotation of Korean and English is 0.97, and the
kappa is 0.96, suggesting an almost perfect agree-
ment. Only a small number of sentence chunk
pairs have inconsistent labels; six chunks in Ko-
599
Texts swihanwseanwiadxcahh liwgcvm:giwc*nht*wsvnPoi???i???ubswgiwseazi?i???ulshx*csw-r:*xzi
????i??ubslansn:zkoi.vcha (iwgaiua vnves zilshx*csw-iu)awbaanifvmwgipvca*i*nliSvcwgipvca*zishibvchansn:ibswgiwseak
fanwseanwitvhwisniwc*nht*wsvnPoi???i??i??????uTnls*yhi#*w*i$vwvchzi%%&&????i???i???u%(%&&'lvtt*ci*mwvev)staiS*nvzi!"#uxcahanwalzi$%&i'(?ulcabi*wwanwsvnzkoiTnls*yhi#*w*i$vwvchig*hixcvlm aliwgai%(%&&'lvtt*cihm) vex* wiS*nvk
Figure 2: Excerpts from Donga Daily News with
differing sentiments between parallel texts
rean lost subjectivity in translation, and implied
subjective meanings in twelve chunks were ex-
pressed explicitly through interpretation. Excerpts
from our corpus show two such cases (Figure 2).
Evaluation Metrics
To evaluate the multilanguage-comparability of
subjectivity analysis systems, we measure 1) how
consistently the system assigns subjectivity labels
and 2) how closely numeric scores for systems?
confidences correlate with regard to parallel texts
in different languages.
In particular, we use Cohen?s kappa coefficient
for the first and Pearson?s correlation coefficient
for the latter. These widely used metrics provide
useful comparability measures for categorical and
quantitative data.
Both coefficients are scaled from ?1 to +1, in-
dicating negative to positive correlations. Kappa
measures are corrected for chance, thereby yield-
ing better measurements than agreement by pro-
portion. The characteristics of Pearson?s correla-
tion coefficient that it measures linear relation-
ships and is independent of change in origin, scale,
and unit comply with our experiments.
5.2 Subjectivity Classification
Our multilingual subjectivity analysis systems
were evaluated on the test corpora described in
Section 5.1 (Table 3).
Due to the difference in testbeds, the perfor-
mance of the state-of-the-art English system (S-
SA) on our corpus is lower by about 10% rela-
tively than the performance reported on the MPQA
corpus.12 However, it still performs sufficiently
12precision, recall, and F-measure of 79.4, 70.6, and 74.7.
well and provides the most balanced results among
the three source language systems; The corpus-
based system (S-CB) classifies with a high pre-
cision, and the lexicon-based (S-LB) with a high
recall. The source language systems (S-SA,-CB,-
LB) lose a small percentage in precision when in-
putted with translations, but the recalls are gener-
ally on a par or even higher in the target languages.
For the systems created from target language re-
sources, Corpus-based systems (T-CB) generally
perform better than the ones with source language
resource (S-CB), and lexicon-based systems (T-
LB) perform worse than (S-LB). Similarly to sys-
tems with source language resources, T-CB clas-
sifies with a high precision and T-LB with a high
recall, but the gap is less. Among the target lan-
guages, Korean tends to have a higher precision,
and Japanese a higher recall than other languages
in most systems.
Overall, S-SA provides easy accessibility when
analyzing both the source and the target languages,
with a balanced precision and recall performance.
Among the other approaches, only T-CB is bet-
ter in all measures than S-SA, and S-LB performs
best on F-measure evaluations.
5.3 Multilanguage-Comparability
The evaluation results on multilanguage-
comparability are presented in Table 4. The
subjectivity analysis systems are evaluated with
all language pairs with kappa and Pearson?s
correlation coefficients. Kappa and Pearson?s
correlation values are consistent with each other;
Pearson?s correlation between the two evaluation
measures is 0.91.
We observe a distinct contrast in performances
between corpus-based systems (S-CB and T-CB)
and lexicon-based systems (S-LB and T-LB); All
corpus-based systems show moderate agreements
while agreements on lexicon-based systems are
only fair.
Within corpus-based systems, S-CB performs
better with language pairs that include English,
and T-CB performs better with language pairs of
the target languages.
For lexicon-based systems, systems in the tar-
get languages (T-LB) performs the worst with
only slight to fair agreements between languages.
Lexicon-based systems and state-of-the-art sys-
tems in the source language (S-LB and S-SA) re-
sult in average performances.
600
Table 3: Performance of subjectivity analysis with precision (P), recall (R), and F-measure (F). S-SA,-
CB,-LB systems in Korean, Chinese, Japanese indicate English analysis systems inputted with transla-
tions of the target languages into English.
English Korean Chinese Japanese
P R F P R F P R F P R F
S-SA 71.1 63.5 67.1 70.7 61.1 65.6 67.3 68.8 68.0 69.1 67.5 68.3
S-CB 74.4 53.9 62.5 74.5 52.2 61.4 71.1 63.3 67.0 72.9 65.3 68.9
S-LB 62.5 87.7 73.0 62.9 87.7 73.3 59.9 91.5 72.4 61.8 94.1 74.6
T-CB 72.4 67.5 69.8 75.0 66.2 70.3 72.5 70.3 71.4
T-LB 59.4 71.0 64.7 58.4 82.3 68.2 56.9 92.4 70.4
Table 4: Performance of multilanguage-comparability: kappa coefficient (?) for measuring comparability
of classification labels and Pearson?s correlation coefficient (?) for classification scores for English (EN),
Korean (KR), Chinese (CH), and Japanese (JP). Evaluations of T-CB,-LB for language pairs including
English are carried out with results from S-CB,-LB for English and T-CB,-LB for target languages.
S-SA S-CB S-LB T-CB T-LB
? ? ? ? ? ? ? ? ? ?
EN & KR 0.41 0.55 0.45 0.60 0.37 0.59 0.42 0.60 0.25 0.41
EN & CH 0.39 0.54 0.41 0.62 0.33 0.52 0.39 0.57 0.22 0.38
EN & JP 0.39 0.53 0.43 0.65 0.30 0.59 0.40 0.59 0.15 0.33
KR & CH 0.36 0.54 0.39 0.59 0.28 0.57 0.46 0.64 0.23 0.37
KR & JP 0.37 0.60 0.44 0.69 0.50 0.69 0.63 0.76 0.18 0.38
CH & JP 0.37 0.53 0.49 0.66 0.29 0.57 0.46 0.63 0.22 0.46
Average 0.38 0.55 0.44 0.64 0.35 0.59 0.46 0.63 0.21 0.39
-100
-50
0
50
100
-100 -50 0 50 100
(a) S-SA
-4
-3
-2
-1
0
1
2
3
4
-4 -3 -2 -1 0 1 2 3 4
(b) S-CB
-10
-5
0
5
10
-10 -5 0 5 10
(c) S-LB
-4
-3
-2
-1
0
1
2
3
4
-4 -3 -2 -1 0 1 2 3 4
(d) T-CB
-10
-5
0
5
10
-10 -5 0 5 10
(e) T-LB
Figure 3: Scatter plots of English (x-axis) and Korean (y-axis) subjectivity scores from state-of-the-art
(S-SA), corpus-based (S-CB), and lexicon-based (S-LB) systems of the source language, and corpus-
based with translated corpora (T-CB), and lexicon-based with translated lexicon (T-LB) systems. Slanted
lines in figures are best-fit lines through the origins.
601
Figure 3 shows scatter plots of subjectivity
scores of our English and Korean test corpora eval-
uated on different systems; the data points on the
first and the third quadrants are occurrences of la-
bel agreements, and the second and the fourth are
disagreements. Linearly scattered data points are
more correlated regardless of the slope.
Figure 3a shows a moderate correlation for mul-
tilingual results from the state-of-the-art system
(S-SA). Agreements on objective instances are
clustered together while agreements on subjective
instances are diffused over a wide region.
Agreements between the source language
corpus-based system (S-CB) and the corpus-based
system trained with translated resources (T-CB)
are more distinctively correlated than the results
for other pairs of systems (Figures 3b and 3d). We
notice that S-CB seems to have a lower number of
outliers than T-CB, but slightly more diffusive.
Lexicon-based systems (S-LB, T-LB) gener-
ate noticeably uncorrelated scores (Figures 3c and
3e). We observe that the results from the English
system with translated inputs (S-LB) is more cor-
related than those from systems with translated
lexicons (T-LB), and that analysis results from
both systems are biased toward subjective scores.
6 Discussion
Which approach is most suitable for multilingual
subjectivity analysis?
In our experiments, the corpus-based sys-
tems trained on corpora translated from English
to the target languages (T-CB) perform well
for subjectivity classification and multilanguage-
comparability measures on the whole. However,
the methods we employed to expand the languages
were naively carried out without much considera-
tions for optimization. Further adjustments could
improve the other systems for both classification
and multilanguage-comparability performances.
Is there a correlation between classification per-
formance and multilanguage-comparability?
Lexicon-based systems in the source language
(S-LB) have good overall classification perfor-
mances, especially on recall and F-measures.
However, these systems performs worse on
multilanguage-comparability than other systems
with poorer classification performances. Intrigued
by the observation, we tried to measure which
criteria for classification performance influences
multilanguage-comparability. We again employed
Pearson?s correlation metrics to measure the corre-
lations of precision (P), recall (R), and F-measures
(F) to kappa (?) and Pearson?s correlation (?) val-
ues.
Specifically, we measure the correlations be-
tween the sums of P, the sums of R, and the
sums of F to ? and ? for all pairs of systems.13
The correlations of P with ? and ? are 0.78
and 0.68, R ?0.38 and ?0.28, and F ?0.20
and ?0.05. These numbers strongly suggest that
multilanguage-comparability correlates with the
precisions of classifiers.
However, we cannot always expect a high-
precision multilingual subjectivity classifier to be
multilanguage-comparable as well. For example,
the S-SA system has a much higher precision
than S-LB consistently over all languages, but
their multilanguage-comparability performances
differed only by small amounts.
7 Conclusion
Multilanguage-comparability is an analysis sys-
tem?s ability to retain its decision criteria across
different languages. We implemented a number of
previously proposed approaches to learning mul-
tilingual subjectivity, and evaluated the systems
on multilanguage-comparability as well as clas-
sification performance. Our experimental results
provide meaningful comparisons of the multilin-
gual subjectivity analysis systems across various
aspects.
Also, we developed a multilingual subjectivity
evaluation corpus from a parallel text, and studied
inter-annotator, inter-language agreements on sub-
jectivity, and observed persistent subjectivity pro-
jections from one language to another from a par-
allel text.
For future work, we aim extend this work to
constructing a multilingual sentiment analysis sys-
tem and evaluate it with multilingual datasets
such as product reviews collected from different
countries. We also plan to resolve the lexicon-
based classifiers? classification bias towards sub-
jective meanings with a list of objective words
(Esuli and Sebastiani, 2006) and their multilin-
gual expansion (Kim et al, 2009), and evaluate
the multilanguage-comparability of systems con-
structed with resources from different sources.
13Pairs of values such as 71.1 + 70.7 and 0.41 for preci-
sions and Kappa of S-SA for English and Korean.
602
Acknowledgement
We thank the anonymous reviewers for valuable
comments and helpful suggestions. This work is
supported in part by Basic Science Research Pro-
gram through the National Research Foundation
of Korea (NRF) funded by the Ministry of Edu-
cation, Science and Technology (MEST) (2009-
0075211), and in part by the BK 21 project in
2010.
References
Ahmed Abbasi, Hsinchun Chen, and Arab Salem.
2008. Sentiment analysis in multiple languages:
Feature selection for opinion classification in web
forums. ACM Transactions on Information Systems,
26(3):1?34.
Carmen Banea, Rada Mihalcea, Janyce Wiebe, and
Samer Hassan. 2008. Multilingual subjectivity
analysis using machine translation. In EMNLP ?08:
Proceedings of the Conference on Empirical Meth-
ods in Natural Language Processing, pages 127?
135, Morristown, NJ, USA.
Mikhail Bautin, Lohit Vijayarenu, and Steven Skiena.
2008. International sentiment analysis for news and
blogs. In Proceedings of the International Confer-
ence on Weblogs and Social Media (ICWSM).
Erik Boiy and Marie-Francine Moens. 2009. A
machine learning approach to sentiment analysis
in multlingual Web texts. Information Retrieval,
12:526?558.
Julian Brooke, Milan Tofiloski, and Maite Taboada.
2009. Cross-linguistic sentiment analysis: From en-
glish to spanish. In Proceedings of RANLP 2009,
Borovets, Bulgaria.
Carmine Cesarano, Antonio Picariello, Diego Refor-
giato, and V.S. Subrahmanian. 2007. The oasys 2.0
opinion analysis system: A demo. In Proceedings of
the International Conference on Weblogs and Social
Media (ICWSM).
Andrea Esuli and Fabrizio Sebastiani. 2006. Senti-
wordnet: A publicly available lexical resource for
opinion mining. In Proceedings of the 5th Con-
ference on Language Resources and Evaluation
(LREC?06), pages 417?422, Geneva, IT.
Soo-Min Kim and Eduard Hovy. 2006. Identifying
and analyzing judgment opinions. In Proceedings
of the Human Language Technology Conference of
the NAACL (HLT/NAACL?06), pages 200?207, New
York, USA.
Jungi Kim, Hun-Young Jung, Sang-Hyob Nam, Yeha
Lee, and Jong-Hyeok Lee. 2009. Found in trans-
lation: Conveying subjectivity of a lexicon of one
language into another using a bilingual dictionary
and a link analysis algorithm. In ICCPOL ?09: Pro-
ceedings of the 22nd International Conference on
Computer Processing of Oriental Languages. Lan-
guage Technology for the Knowledge-based Econ-
omy, pages 112?121, Berlin, Heidelberg.
Rada Mihalcea, Carmen Banea, and Janyce Wiebe.
2007. Learning multilingual subjective language via
cross-lingual projections. In Proceedings of the 45th
Annual Meeting of the Association of Computational
Linguistics (ACL?07), pages 976?983, Prague, CZ.
Bo Pang, Lillian Lee, and Shivakumar Vaithyanathan.
2002. Thumbs up? Sentiment classification using
machine learning techniques. In Proceedings of the
Conference on Empirical Methods in Natural Lan-
guage Processing (EMNLP), pages 79?86.
Ellen Riloff and Janyce Wiebe. 2003. Learning ex-
traction patterns for subjective expressions. In Pro-
ceedings of the Conference on Empirical Methods in
Natural Language Processing (EMNLP).
Xiaojun Wan. 2008. Using bilingual knowledge and
ensemble techniques for unsupervised Chinese sen-
timent analysis. In Proceedings of the 2008 Con-
ference on Empirical Methods in Natural Language
Processing, pages 553?561, Honolulu, Hawaii, Oc-
tober. Association for Computational Linguistics.
Xiaojun Wan. 2009. Co-training for cross-lingual sen-
timent classification. In Proceedings of the Joint
Conference of the 47th Annual Meeting of the ACL
and the 4th International Joint Conference on Nat-
ural Language Processing of the AFNLP, pages
235?243, Suntec, Singapore, August. Association
for Computational Linguistics.
Janyce Wiebe and Ellen Riloff. 2005. Creating subjec-
tive and objective sentence classifiers from unanno-
tated texts. In Proceedings of the 6th International
Conference on Intelligent Text Processing and Com-
putational Linguistics (CICLing-2005), pages 486?
497, Mexico City, Mexico.
Janyce Wiebe, E. Breck, Christopher Buckley, Claire
Cardie, P. Davis, B. Fraser, Diane Litman, D. Pierce,
Ellen Riloff, Theresa Wilson, D. Day, and Mark
Maybury. 2003. Recognizing and organizing opin-
ions expressed in the world press. In Proceedings
of the 2003 AAAI Spring Symposium on New Direc-
tions in Question Answering.
Theresa Wilson, Janyce Wiebe, and Paul Hoffmann.
2005. Recognizing contextual polarity in phrase-
level sentiment analysis. In Proceedings of the Con-
ference on Human Language Technology and Em-
pirical Methods in Natural Language Processing
(HLT-EMNLP?05), pages 347?354, Vancouver, CA.
603
Proceedings of SSST-5, Fifth Workshop on Syntax, Semantics and Structure in Statistical Translation, pages 41?51,
ACL HLT 2011, Portland, Oregon, USA, June 2011. c?2011 Association for Computational Linguistics
Multi-Word Unit Dependency Forest-based Translation Rule
Extraction
Hwidong Na Jong-Hyeok Lee
Department of Computer Science and Engineering
Pohang University of Science and Technology (POSTECH)
San 31 Hyoja Dong, Pohang, 790-784, Republic of Korea
{leona,jhlee}@postech.ac.kr
Abstract
Translation requires non-isomorphic
transformation from the source to the
target. However, non-isomorphism can
be reduced by learning multi-word units
(MWUs). We present a novel way of
representating sentence structure based
on MWUs, which are not necessarily
continuous word sequences. Our pro-
posed method builds a simpler structure
of MWUs than words using words
as vertices of a dependency structure.
Unlike previous studies, we collect
many alternative structures in a packed
forest. As an application of our proposed
method, we extract translation rules in
form of a source MWU-forest to the
target string, and verify the rule coverage
empirically. As a consequence, we
improve the rule coverage compare to a
previous work, while retaining the linear
asymptotic complexity.
1 Introduction
Syntax is the hierarchical structure of a natu-
ral language sentence. It is generally repre-
sented with tree structures using phrase struc-
ture grammar (PSG) or dependency grammar
Figure 1: A pair of sentences that require long
distance reordering (dashed line) and discontinuous
translation (thick line)
(DG). Although the state-of-the-art statistical
machine translation (SMT) paradigm is phrase-
based SMT (PBSMT), many researchers have
attempted to utilize syntax in SMT to over-
come the weaknesses of PBSMT. An emerging
paradigm alternative to PBSMT is syntax-based
SMT, which embeds the source and/or target
syntax in its translation model (TM). Utilizing
syntax in TM has two advantages over PBSMT.
The first advantage is that syntax eases global
reordering between the source and the target
language. Figure 1 shows that we need global
reordering in a complex real situation, where
a verbal phrase requires a long distance move-
ment. PBSMT often fails to handle global re-
ordering, for example, from subject-verb-object
(SVO) to SOV transformation where V should
be moved far away from the original position in
41
Table 1: Statistics of the corresponding target words
for the continuous word sequences in the source lan-
guage, or vice versa. C denotes consistent, O over-
lapped, D discontinuous, and N null.
Word Alignment C O D N
Manual 25 60 10 5
Automatic 20 55 15 5
the source language. This is because of the two
distance-based constraints in PBSMT: the dis-
tortion model cost and the distortion size limit.
For the distortion model cost, PBSMT sets zero
cost to the monotone translation and penalizes
the distorted translations as the distortion grows
larger. For the distortion size limit, a phrase can
only be moved from its original position within
a limit. Therefore, PBSMT fails to handle long
distance reordering. Syntax-based SMT man-
ages global reordering as structural transforma-
tion. Because reordering occurs at the sub-
structure level such as constituents or treelets
in syntax-based SMT, the transformation of the
sub-structure eventually yields the reordering of
the whole sentence.
The second advantage of using syntax in TM
is that syntax guides us to discontinuous trans-
lation patterns. Because PBSMT regards only
a continuous sequence of words as a transla-
tion pattern, it often fails to utilize many use-
ful discontinuous translation patterns. For ex-
ample, two discontinuous source words corre-
spond to a target word in Figure 1. In our in-
spection of the training corpus, a continuous
word sequence often corresponds to a set of
discontinuous words in the target language, or
vice versa (Table 1). Discontinuous translation
patterns frequently appear in many languages
(S?gaard and Kuhn, 2009). Syntax-based SMT
overcomes the limitations of PBSMT because it
finds discontinuous patterns along with the hier-
Figure 2: The maximum branching factor (BF) and
depth factor (DF) in a dependency tree in our corpus
archical structure. For example, the two discon-
tinuous source words have a head-dependent re-
lation (Figure 3). Especially with the depen-
dency tree, we can easily identify patterns that
have non-projectivity (Na et al, 2010). How-
ever, syntax-based patterns such as constituents
or treelets do not sufficiently cover various use-
ful patterns, even if we have the correct syn-
tactic analysis (Chiang, 2010). For this reason,
many researchers have proposed supplementary
patterns such as an intra/inter constituent or se-
quence of treelets (Galley et al, 2006; Shen et
al., 2008).
Unlike PSG, DG does not include non-
terminal symbols, which represent constituent
information. This makes DG simpler than PSG.
For instance, it directly associates syntatic role
with the structure, but introduces a difficulty in
syntax-based SMT. The branching factor of a
dependency tree becomes larger when a head
word dominates many dependents. We ob-
serve that the maximum branching factor of
an automatically parsed dependency tree ranges
widely, while most trees have depth under a cer-
tain degree (Figure 2). This indicates that we
have a horizontally flat dependency tree struc-
ture. The translation patterns extracted from the
42
flat dependency tree are also likely to be flat.
Unfortunately, the flat patterns are less appli-
cable at the decoding stage. When one of the
modifiers does not match, for instance, we fail
to apply the translation pattern. Therefore, we
need a more generally applicable representation
for syntax-based SMT using DG.
We propose a novel representation of DG that
regards a set of words as a unit of the depen-
dency relations, similar to (Ding, 2006; Wu et
al., 2009; Na et al, 2010). Unlike their work,
we consider many alternatives without prede-
fined units, and construct a packed forest of the
multi-word units (MWUs) from a dependency
tree. For brevity, we denote the forest based on
MWUs as an MWU-forest. Because all pos-
sible alternatives are exponentially many, we
give an efficient algorithm that enumerates the
k-best alternatives in section 3. As an appli-
cation, we extract translation patterns in form
of a source MWU-forest to the target string in
order to broaden the coverage of the extracted
patterns for syntax-based SMT in section 4. We
also report empirical results related to the use-
fulness of the extracted pattern in section 5. The
experimental results show that the MWU-forest
representation gives more applicable translation
patterns than the original word-based tree.
2 Related Work
Previous studies have proposed merging alter-
native analyses to deal with analysis errors for
two reasons: 1) the strongest alternative is not
necessarily the correct analysis, and 2) most
alternatives contain similar elements such as
common sub-trees. For segmentation alterna-
tives, Dyer et al (2008) proposed a word lattice
that represents exponentially large numbers of
segmentations of a source sentence, and inte-
grates reordering information into the lattice as
well. For parsing alternatives, Mi et al (2008)
suggested a packed forest that encodes alterna-
tive PSG derivations. Futher, Mi et al (2010)
combined the two approaches in order to bene-
fit from both.
The translation literature also shows that
translation requires non-isomorphic transfor-
mation from the source to the target. This yields
translation divergences such as head-switching
(Dorr, 1994). Ding and Palmer (2005) reported
that the percentage of the head-swapping cases
is 4.7%, and that of broken dependencies is
59.3% between Chinese and English. The large
amount of non-isomorphism, however, will be
reduced by learning MWUs such as elementary
trees (Eisner, 2003).
There are few studies that consider a depen-
dency structure based on MWUs. Ding (2006)
suggested a packed forest which consists of the
elementary trees, and described how to find
the best decomposition of the dependency tree.
However, Ding (2006) did not show how to de-
termine the MWUs and restrict them to form
a subgraph from a head. For opinion mining,
Wu et al (2009) also utilized a dependency
structure based on MWUs, although they re-
stricted MWUs with predefined relations. Na
et al (2010) proposed an MWU-based depen-
dency tree-to-string translation rule extraction,
but considered only one decomposition for ef-
ficiency. Our proposed method includes addi-
tional units over Ding?s method, such as a se-
quence of subgraphs within a packed forest. It
is also more general than Wu et al?s method
because it does not require any predefined re-
lations. We gain much better rule coverage
against Na et al?s method, while retaining linear
asymptotical computational time.
43
Figure 3: A dependency tree of the source sentence
in Figure 1
3 MWU-based Dependency Forest
There are two advantages when we use the
MWU-forest representaion with DG. First, we
express the discontinuous patterns in a vertex,
so that we can extract more useful translation
patterns beyond continuous ones for syntax-
based SMT. Second, an MWU-forest contains
many alternative structures which may be sim-
pler structures than the original tree in terms of
the branching factor and the maximum depth.
Wu et al (2009) utilized an MWU-tree to iden-
tify the product features in a sentence easily.
As in previous literature in syntax-based
SMT using DG, we only consider the well-
formed MWUs where an MWU is either a
treelet (a connected sub-graph), or a sequence
of treelets under a common head. In other
words, each vertex in an MWU-forest is either
?fixed on head? or ?floating with children?. The
formal definitions can be found in (Shen et al,
2008).
We propose encoding multiple dependency
structures based on MWUs into a hypergraph.
A hypergraph is a compact representation of
exponetially many variations in a polynomi-
nal space. Unlike PSG, DG does not have
Figure 4: An MWU-forest of Figure 3. The dashed
line indicates the alternative hyperedges.
non-terminals that represent the linguistically
motivated, intermediate structure such as noun
phrases and verb phrases. For this simplicity,
Tu et al (2010) proposed a dependency forest
as a hypergraph, regarding a word as a vertex
with a span that ranges for all its descendants.
The dependency forest offers tolerence of pars-
ing errors.
Our representation is different from the de-
pendency forest of Tu et al (2010) since a ver-
tex corresponds to multiple words as well as
words. Note that our representation is also
capable of incorporating multiple parse trees.
Therefore, MWU-forests will also be tolerant
of the parsing error if we provide multiple parse
trees. In this work, we concentrate on the ef-
fectiveness of MWUs, and hence utilize the
best dependency parse tree. Figure 4 shows an
MWU-forest of the dependency tree in Figure
3.
More formally, a hypergraph H = ?V,E?
consists of the vertices V and hyperedges
E. We assume that a length-J sentence has
a dependency graph which is single-headed,
44
acyclic, and rooted, i.e. hj is the index of the
head word of the j-th word, or 0 if the word is
the root. Each vertex v = {j|j ? [1, J ]} de-
notes a set of the indices of the words that satis-
fies the well-formed constraint. Each hyperedge
e = ?tails(e), head(e)? denotes a set of the de-
pendency relations between head(e) and ?v ?
tails(e). We include a special node v0 ? V
that denotes the dummy root of an MWU-forest.
Note that v0 does not appear in tails(e) for all
hyperedges. We denote |e| is the arity of hyper-
edge e, i.e. the number of tail nodes, and the
arity of a hypergraph is the maximum arity over
all hyperedges. Also, let ?(v) be the indices of
the words that the head lays out of the vertex,
i.e. ?(v) = {j|hj 6? v ? j ? v}, and ?(v) be
the indices of the direct dependent words of the
vertex, i.e. ?(v) = {j|hj ? v ? j 6? v}. Let
OUT (v) and IN(v) be the outgoing and in-
coming hyperedges of a vertex v, respectively.
It is challenging to weight the hyperedges
based on dependency grammar because a de-
pendency relation is a binary relation from a
head to a dependent. Tu et al (2010) assigned
a probability for each hyperedge based on the
score of the binary relation. We simply prefer
the hyperedges that have lower arity by scoring
as follows:
c(e) =
?
v?tails(e) |v|
|e|
p(e) = c(e)?
e??IN(head(e)) c(e?)
We convert a dependency tree into a hyper-
graph in two steps using the Inside-Outside al-
gorithm. Algorithm 1 shows the pseudo code
of our proposed method. At the first step, we
find the k-best incoming hyperedges for each
vertex (line 3-8), and compute the inside proba-
bility (line 9), in bottom-up order. At the sec-
ond step, we compute the outside probability
Algorithm 1 Build Forest
1: Initialize V
2: for v ? V in bottom-up order do
3: Create a chart C = |?(v)|2
4: for chart span [p, q] do
5: Initialize C[p, q] if ?v s.t. [p, q] = v or
?(v)
6: Combine C[p, i] and C[i + 1, q]
7: end for
8: Set IN(v) to the k-best in C[TOP ]
9: Set ?(v) as in Eq. 1
10: end for
11: for v ? V in top-down order do
12: Set ?(v) as in Eq. 2
13: end for
14: Prune out e if p(e) ? ?
15: return v0
(line 12) for each vertex in a top-down manner.
Finally we prune out less probable hyperedges
(line 14) similar to (Mi et al, 2008). The inside
and outside probabilities are defined as follows:
?(v) =
?
e?IN(v)
p(e)
?
d?tails(e)
?(d) (1)
where ?(v) = 1.0 if IN(v) = ?, and
?(v) =
?
h?OUT (v)
e?IN(head(h))
?(head(e))p(e)
|OUT (v)|
?
?
d?tails(e)\{v}
?(d) (2)
where ?(v) = 1.0 if OUT (v) = ?.
In practice, we restrict the number of words
in a vertex in the initialization (line 1). We ap-
proximate all possible alternative MWUs that
include each word as follows:
45
Figure 5: A sub-forest of Figure 4 with annotation
of aspan and cspan for each vertex. We omit the
span if it is not consistent.
? A horizontal vertex is a sequence of modi-
fiers for a common head word, and
? A vertical vertex is a path from a word to
one of the ancestors, and
? A combination of the horizontal vertices
and the vertical vertices, and
? A combination of the vertical vertices and
the vertical vertices.
The computational complexity of the initial-
izaion directly affects the complexity of the en-
tire procedure. For each word, generating the
horizontal vertices takes O(b2), and the vertical
vertices take O(bd?1), where b is the maximum
branching factor and d is the maximum depth
of a dependency tree. The two combinations
take O(bd+1) and O(b2(d?1)) time to initialize
the vertices. However, it takes O(mm+1) and
O(m2(m?1)) if we restrict the maximum num-
ber of the words in a vertex to a constant m.
Ding and Palmer (2005) insisted that the
Viterbi decoding of an MWU-forest takes lin-
ear time. In our case, we enumerate the k-best
incoming hyperedeges instead of the best one.
Because each enumeration takes O(k2|?(v)|3),
Table 2: The extracted rules in Figure 5. N denotes
the non-lexicalized rules with variables xi for each
v ? tails(e), and L denotes the lexicalized rule.
head(e) tails(e) rhs(?)
N
{3} {8} : x1 x1
{8} {4} : x1, {5} : x2 when x1 x2
{3, 8} {4, 5} : x1 when x1
{3, 8} {4} : x1, {5} : x2 when x1 x2
{4, 5} {6, 7} : x1 I?m in x1
{5} {6, 7} : x1 in x1
L
{6, 7}
N/A
the States
{4} I?m
{5} in
{4, 5} I?m in
{5, 6} in the State
{3, 8} When
the total time complexity also becomes linear
to the length of the sentence n similar to Ding
and Palmer (2005), i.e. O(|V |k2|?(v)|3), where
|V | = O(na2(a?1)) and a = min(m, b, d).
4 MWU-Forest-to-String Translation
Rule Extraction
As an application of our proposed MWU-forest,
we extract translation rules for syntax-based
SMT. Forest-based translation rule extraction
has been suggested by Mi and Huang (2008)
although their forest compacts the k-best PSG
trees. The extraction procedure is essentially
the same as Galley et al (2004), which iden-
tifies the cutting points (frontiers) and extracts
the sub-structures from a root to frontiers.
The situation changes in DG because DG
does not have intermediate representation. At
the dependency structure, a node corresponds
to two kinds of target spans. We borrow the
definitions of the aligned span (aspan), and the
covered span (cspan) from Na et al (2010), i.e.
46
? aspan(v) = [min(av),max(av)], and
? cspan(v) =
aspan(v)?d?tails(e)
e?IN(v)
cspan(d)
, where av = {i|j ? v ? (i, j) ? A}. Figure 5
shows aspans and cspans of a sub-forest of of
the MWU-forest in the previous example.
Each span type yields a different rule type:
aspan yields a lexicalized rule without any
variables, and cspan yields a non-lexicalized
rule with variables for the dependents of the
head word. For example, Table 2 shows the ex-
tracted rule in Figure 5.
In our MWU-forest, the rule extraction pro-
cedure is almost identical to a dependency
tree-to-string rule extraction except we regard
MWUs as vertices. Let fj and ei be the j-th
source and i-th target word, respectively. As an
MWU itself has a internal structure, a lexical
rule is a tree-to-string translation rule. There-
fore, a lexicalized rule is a pair of the source
words s and the target words t as follows:
s(v) = {fj |j ? v}
t(v) = {ei|i ? aspan(v)} (3)
In addition, we extract the non-lexicalized
rules from a hyperedge e to cspan of the
head(e). A non-lexicalized rule is a pair of the
source words in the vertices of a hyperedge and
the cspan of the target words with substitutions
of cspan(d) for each d ? tails(e). We abstract
d on the source with ?(d) for non-lexicalized
rules (row 2 in Table 2). We define the source
words s and the target words t as follows:
s(e) = {fj |j ? head(e) ? j ? ?(d)}
t(e) = {ei|i ? cspan(v) ? i 6? cspan(d)}
? {xi|d? xi} (4)
Algorithm 2 Extract Rules( H = ?V,E?)
1: ? = ?
2: for v ? V do
3: if aspan(v) is consistent then
4: ?? ? ? ? s(v) , t(v) ? as in Eq. 3
5: end if
6: if cspan(v) is consistent then
7: for e ? IN(v) do
8: if cspan(d)?d ? tails(e) then
9: ?? ?? ? s(e), t(e) ? as in Eq. 4
10: end if
11: end for
12: end if
13: end for
14: return ?
where d ? tails(e).
More formally, we extract a synchronous tree
substitution grammar (STSG) which regards the
MWUs as non-terminals.
Definition 1 A STSG using MWU (STSG-
MWU) is a 6-tuple G = ??S ,?T ,?,?, S, ??,
where:
? ?S and ?T are finite sets of terminals
(words, POSs, etc.) of the source and tar-
get languages, respectively.
? ? is a finite set of MWUs in the source
language, i.e. ? = {?S}+
? ? is a finite set of production rules
where a production rule ? : X ?
? lhs(?) , rhs(?), ? ?, which is a relation-
ship from ? to {x ? ?T } ?, where ? is the
bijective function from the source vertices
to the variables x in rhs(?). The asterisk
represents the Kleenstar operation, and
? S is the start symbol used to represent the
whole sentence, i.e. ?0 : S ? ? X , X ?.
47
For each type of span, we only extract the
rules if the target span has consistent word
alignments, i.e. span 6= ? ? ?i ? span,
{j|(i, j) ? A ? (i?, j) ? A s.t. i? 6? span} = ?.
Algorithm 2 shows the pseudo code of the
extraction. Because a vertex has aspan and
csapn, we extract a lexicalized rule (line 3-5)
and/or non-lexicalized rules (line 6-12) for each
vertex.
5 Experiment
We used the training corpus provided for the
DIALOG Task in IWSLT10 between Chinese
and English . The corpus is a collection of
30,033 sentence pairs and consists of dialogs in
travel situations (10,061) and parts of the BTEC
corpus (19,972). Details about the provided
corpus are described in (Paul, 2009). We used
the Stanford Parser 1 to obtain word-level de-
pendency structures of Chinese sentences, and
GIZA++ 2 to obtain word alignments of the
biligual corpus.
We extracted the SCFG-MWU from the
biligual corpus with word alignment. In or-
der to investigate the coverage of the extracted
rule, we counted the number of the recovered
sentences, i.e. counted if the extracted rule
for each sentence pair generates the target sen-
tence by combining the extracted rules. As we
collected many alternatives in an MWU-forest,
we wanted to determine the importance of each
source fragment. Mi and Huang (2008) penal-
ized a rule ? by the posterior probability of its
tree fragment lhs(?). This posterior probability
is also computed in the Inside-Outside fashion
that we used in Algorithm 1. Therefore, we re-
garded the fractional count of a rule ? as
1http://nlp.stanford.edu/software/lex-parser.shtml,
Version 1.6.4
2http://code.google.com/p/giza-pp/
Figure 6: The rule coverage according to the number
of the words in a vertex.
c(?) = ??(lhs(?))??(v0)
We prioritized the rule according to the frac-
tional count. The priority is used when we com-
bine the rules to restore the target sentence us-
ing the extracted rule for each sentence. We var-
ied the maximum size of a vertex m, and the
number of incoming hyperedges k. Figure 6
shows the emprical result.
6 Discussion
Figure 6 shows that we need MWU to broaden
the coverage of the extracted translation rules.
The rule coverage increases as the number of
words in an MWU increases, and almost con-
verges at m = 6. Our proposed method re-
cover around 75% of the sentences in the cor-
pus when we properly restrict m and k. This is
a great improvement over Na et al (2010), who
reported around 60% of the rule coverage with-
out the limitaion of the size of MWUs. They
only considered the best decomposition of the
dependency tree, while our proposed method
collects many alternative MWUs into an MWU-
forest. When we considered the best decom-
position (k = 1), the rule coverage dropped to
48
Figure 7: The frequency of the recovery according
to the length of the sentences in 1,000 sentences
around 65%. This can be viewed as an indirect
comparison between Na et al (2010) and our
proposed method in this corpus.
Figure 7 shows that the frequency of suc-
cess and failure in the recovery depends on the
length of the sentences. As the length of sen-
tences increase, the successful recovery occurs
less frequently. We investigated the reason of
failure in the longer sentences. As a result, the
two main sources of the failure are the word
alignment error and the dependency parsing er-
ror.
Our proposed method does not include all
translation rules in PBSMT because of the syn-
tactic constraint. Generally speaking, our pro-
posed method cannot deal with MWUs that do
not satisfy the well-formed constraint. How-
ever, ill-formed MWUs seems to be useful as
well. For example, our proposed method dose
not allow ill-formed vertices in an MWU-forest
as shown in Figure 8. This would be problem-
atic when we use an erroneuos parsing result.
Because dealing with parsing error has been
studied in literature, our proposed method has
the potential to improve thought future work.
Figure 8: An illustration of ill-formed MWUs
7 Conclusion
We have presented a way of representing sen-
tence structure using MWUs on DG. Because of
the absence of the intermdiate representation in
DG, we built a simpler structure of MWUs than
words using words as vertices of a dependency
structure. Unlike previous studies, we collected
many alternative structures using MWUs in a
packed forest, which is novel. We also ex-
tracted MWU-forest-to-string translation rules,
and verified the rule coverage empirically. As a
consequence, we improvemed the rule coverage
compared with a previous work, while retaining
the linear asymptotic complexity. We will ex-
pand our propose method to develop a syntax-
based SMT system in the future, and incoporate
the parsing error by considering multiple syn-
tactic analyses.
Acknowledgments
We appreciate the three anonymous reviewers.
This work was supported in part by the Korea
Science and Engineering Foundation (KOSEF)
grant funded by the Korean government (MEST
No. 2011-0003029), and in part by the BK 21
Project in 2011.
49
References
David Chiang. 2010. Learning to translate with
source and target syntax. In Proceedings of the
48th Annual Meeting of the Association for Com-
putational Linguistics, pages 1443?1452, Upp-
sala, Sweden, July. Association for Computa-
tional Linguistics.
Yuan Ding and Martha Palmer. 2005. Machine
translation using probabilistic synchronous de-
pendency insertion grammars. In ACL ?05: Pro-
ceedings of the 43rd Annual Meeting on Associ-
ation for Computational Linguistics, pages 541?
548, Morristown, NJ, USA. Association for Com-
putational Linguistics.
Yuan Ding. 2006. Machine Translation Using
Probabilistic Synchronous Dependency Insertion
Grammars. Ph.D. thesis, August.
Bonnie J. Dorr. 1994. Machine translation diver-
gences: a formal description and proposed solu-
tion. Comput. Linguist., 20:597?633, December.
Christopher Dyer, Smaranda Muresan, and Philip
Resnik. 2008. Generalizing word lattice trans-
lation. In Proceedings of ACL-08: HLT, pages
1012?1020, Columbus, Ohio, June. Association
for Computational Linguistics.
Jason Eisner. 2003. Learning non-isomorphic tree
mappings for machine translation. In ACL ?03:
Proceedings of the 41st Annual Meeting on As-
sociation for Computational Linguistics, pages
205?208, Morristown, NJ, USA. Association for
Computational Linguistics.
Michel Galley, Mark Hopkins, Kevin Knight, and
Daniel Marcu. 2004. What?s in a transla-
tion rule? In Daniel Marcu Susan Dumais
and Salim Roukos, editors, HLT-NAACL 2004:
Main Proceedings, pages 273?280, Boston, Mas-
sachusetts, USA, May 2 - May 7. Association for
Computational Linguistics.
Michel Galley, Jonathan Graehl, Kevin Knight,
Daniel Marcu, Steve DeNeefe, Wei Wang, and
Ignacio Thayer. 2006. Scalable inference and
training of context-rich syntactic translation mod-
els. In Proceedings of the 21st International Con-
ference on Computational Linguistics and 44th
Annual Meeting of the Association for Computa-
tional Linguistics, pages 961?968, Sydney, Aus-
tralia, July. Association for Computational Lin-
guistics.
Haitao Mi and Liang Huang. 2008. Forest-based
translation rule extraction. In Proceedings of the
2008 Conference on Empirical Methods in Nat-
ural Language Processing, pages 206?214, Hon-
olulu, Hawaii, October. Association for Compu-
tational Linguistics.
Haitao Mi, Liang Huang, and Qun Liu. 2008.
Forest-based translation. In Proceedings of ACL-
08: HLT, pages 192?199, Columbus, Ohio, June.
Association for Computational Linguistics.
Haitao Mi, Liang Huang, and Qun Liu. 2010. Ma-
chine translation with lattices and forests. In
Coling 2010: Posters, pages 837?845, Beijing,
China, August. Coling 2010 Organizing Commit-
tee.
Hwidong Na, Jin-Ji Li, Yeha Lee, and Jong-Hyeok
Lee. 2010. A synchronous context free grammar
using dependency sequence for syntax-base sta-
tistical machine translation. In The Ninth Confer-
ence of the Association for Machine Translation
in the Americas (AMTA 2010), Denver, Colorado,
October.
Michael Paul. 2009. Overview of the iwslt 2009
evaluation campaign.
Libin Shen, Jinxi Xu, and Ralph Weischedel. 2008.
A new string-to-dependency machine translation
algorithm with a target dependency language
model. In Proceedings of ACL-08: HLT, pages
577?585, Columbus, Ohio, June. Association for
Computational Linguistics.
Anders S?gaard and Jonas Kuhn. 2009. Empirical
lower bounds on aligment error rates in syntax-
based machine translation. In Proceedings of the
Third Workshop on Syntax and Structure in Statis-
tical Translation (SSST-3) at NAACL HLT 2009,
pages 19?27, Boulder, Colorado, June. Associa-
tion for Computational Linguistics.
Zhaopeng Tu, Yang Liu, Young-Sook Hwang, Qun
Liu, and Shouxun Lin. 2010. Dependency for-
est for statistical machine translation. In Pro-
ceedings of the 23rd International Conference on
Computational Linguistics (Coling 2010), pages
50
1092?1100, Beijing, China, August. Coling 2010
Organizing Committee.
Yuanbin Wu, Qi Zhang, Xuangjing Huang, and Lide
Wu. 2009. Phrase dependency parsing for opin-
ion mining. In Proceedings of the 2009 Con-
ference on Empirical Methods in Natural Lan-
guage Processing, pages 1533?1541, Singapore,
August. Association for Computational Linguis-
tics.
51
Proceedings of the Ninth Workshop on Statistical Machine Translation, pages 229?232,
Baltimore, Maryland USA, June 26?27, 2014. c?2014 Association for Computational Linguistics
Postech?s System Description for Medical Text Translation Task 
Jianri Li Se-Jong Kim Hwidong Na Jong-Hyeok Lee 
Department of Computer Science and Engineering 
Pohang University of Science and Technology, Pohang, Republic of Korea 
{skywalker, sejong, leona, jhlee}@postech.ac.kr 
 
 
 
Abstract 
This short paper presents a system description 
for intrinsic evaluation of the WMT 14?s med-
ical text translation task. Our systems consist 
of phrase-based statistical machine translation 
system and query translation system between 
German-English language pairs. Our work fo-
cuses on the query translation task and we 
achieved the highest BLEU score among the 
all submitted systems for the English-German 
intrinsic query translation evaluation.  
1 Overview 
The goal of WMT14?s medical text translation 
task is investigation of capability of machine 
translation (MT) technologies when it is applied 
to translating texts and query terms in medical 
domain. In our work, we focus on its application 
on cross-lingual information retrieval (CLIR) 
and evaluation of query translation task. 
CLIR techniques aim to increase the accessi-
bility of web documents written by foreign lan-
guage. One of the key techniques of cross-lingual 
IR is query translation, which aims to translate 
the input query into relevant terms in target lan-
guage.  
One way to translate queries is dictionary-
based query translation. However, an input query 
usually consists of multiple terms, which cause 
low coverage of bilingual dictionary. Alternative 
way is translating queries using statistical ma-
chine translation (SMT) system. However, trans-
lation model could contain some noise that is 
meaningless translation. The goal of our method 
is to overcome the shortcomings of these ap-
proaches by a heuristic hybrid approach.  
As a baseline, we use phrase-based statistical 
machine translation (PBSMT) (Koehn, Och, & 
Marcu, 2003) techniques to handle queries that 
consist of multiple terms. To identify multiple 
terms in a query, we analyze three cases of the 
formation of queries and generate query transla-
tion candidates using term-to-term dictionaries 
and PBSMT system, and then score these candi-
dates using co-occurrence word frequency meas-
ure to select the best candidate.  
We have done experiment on two language 
pairs 
? English-German 
? German-English 
The rest of parts in this paper are organized as 
following: section 2 describes the techniques and 
system settings used in our experiment, section 3 
presents used corpus and experiment result, and 
section 4 shows a brief conclusion of our work.  
2 Method 
2.1 Phrase-based machine translation sys-
tem 
The phrase-based statistical machine translation 
system is implemented using MOSE?S toolkits 
(Koehn et al., 2007). Bidirectional word align-
ments were built by MGIZA 1 , a multi-thread 
version of GIZA++ (Och & Ney, 2003), run on a 
24 threads machine. The alignment symmetriza-
tion method is grow-diag-final-and (Koehn et al., 
2003), and lexicalized-reordering method is msd-
bidirectional-fe (Koehn et al., 2007). 
For each monolingual corpus, we used a five-
gram language model, which was built by 
IRSTLM toolkit2 (Federico, Bertoldi, & Cettolo, 
2008) with improved Kneser Ney smoothing 
(Chen & Goodman, 1996; Kneser & Ney, 1995). 
The language model was integrated as a log-
linear feature to decoder.  
All the sentences in the training, development 
and test corpus were tokenized by inserting spac-
es between words and punctuations, and then 
converted to most probable cases by truecaseing. 
Both tokenization and truecasing were done by 
embedded tools in the MOSE?S toolkits. Finally, 
all the sentences in the train corpus were cleaned 
with maximum length 80.  
1 http://www.kyloo.net/software 
2 http://sourceforge.net/projects/irstlm 
                                                 
229
 Figure 1. Flow from queries to query translation candidates for each case. 
 
2.2 Query translation system 
In general, an input query is not a full sentence. 
Instead, most of queries contain one or more 
phrases that consist of several keywords. Fur-
thermore, in the medical domain, many key-
words are unfamiliar terminologies for general 
users. Therefore, term-to-term translation dic-
tionaries in medical domain could be useful re-
sources to translate the queries. In our experi-
ment, we used the parallel terms from Unified 
Medical Language System (UMLS) and titles of 
Wikipedia in medical domain, as the term-to-
term translation dictionary. 
First of all, if a given query is a combination 
of two or more phrases that concatenated by 
terms like comma, coordinate conjunction, then 
the given query is divided into several single 
phrases, and each of them is translated by our 
SMT system as a new single query. If the new 
query satisfies one of cases shown in Figure 1, 
then its query translation candidates are selected 
according to the corresponding case, and select 
the best one of them using proposed measures. 
Otherwise, if the new query does not satisfy any 
case, the top 1 result by our PBSMT system is 
selected as the best query translation candidate. 
Our method combines the translation results of 
single queries by following rules: 1) if the origi-
nal query consists of multiple phrases concate-
nated by functional words like coordinate con-
junctions, then the translation results are com-
bined by translated functional words, 2) if the 
original query is concatenated by punctuation, 
then the results are combined by the original 
punctuation. Finally, the final result is selected 
by comparing the result from QT system and 
PBSMT system using the co-occurrence word 
frequency measure (see Section 2.2.4). The fol-
lowing three subsections describe how we select 
translation candidate case by case. 
2.2.1 Case 1: Full matching 
If a single query exactly matches one instance in 
the dictionary, query translation candidates are 
the target-side entries in the translation diction-
ary (Case 1 in Figure 1). If a query translation 
candidate qt is a sequence of words (w1 to wn), it 
is ranked by the co-occurrence word frequency 
measure (CF) using the provided articles of Wik-
ipedia in the medical domain: 
 
,              (1) 
where freq(w1) is the frequency of a unigram w1 
in the articles; freq(wi, wi-1) is the frequency of a 
230
bigram ?wi wi-1? in the articles; and Nuni and Nbi is 
the sum of frequency of all unigram and bigram, 
respectively. 
2.2.2 Case 2: Full inclusion 
If a source-side entry of the term-to-term transla-
tion dictionary exactly includes a query, its query 
translation candidate is its SMT result whose all 
words appear in the target-side entry of the trans-
lation dictionary (Case 2 in Figure 1). Among the 
top 10 results by our PBSMT system, we select 
the results satisfying this case, and rank them 
using CF and our PBSMT result score 
(ScoreSMT): 
 
,  (2) 
where ? is the weight by the provided develop-
ment set; and QT is the set of query translation 
candidates for a query. 
2.2.3 Case 1: Full matching 
If the left phrase tleft or right phrase tright of a que-
ry exactly matches one instance in the dictionary, 
its query translation candidate is its SMT result 
that includes all words in the target-side entry of 
the translation dictionary (Case 3 in Figure 1). 
To rank our SMT results satisfying this case, if 
the total number of words in tleft and tright is same 
or larger than that in a query, ScoreQT is used, 
and the other case uses the weighted ScoreQT 
(WScoreQT): 
 
,  (3) 
where N(tleft) is the number of words in tleft; and q 
is a given query. 
2.2.4 Select final result 
If a query satisfies any case above, and the can-
didate with highest score is selected, then we 
compare the candidate with translation of origi-
nal query directly obtained from PBSMT system 
using equation (1). The final result would be the 
result with higher score between them.  
3 Experiment  
3.1 Corpus 
We only use constrained data provided by WMT 
2014 medical translation task.  
To train PBSMT system, we use parallel cor-
pora 
? EMEA 
? MuchMore 
? Wikipedia-titles 
? Patent-abstract, claim, title 
? UMLS 
We simply mixed up all available parallel cor-
pora to train a unique translation model.  
And for English-German language pair we use 
monolingual corpora 
? Wikipedia-articles 
? Patent-descriptions 
? UMLS descriptions 
And for German-English language pair we use 
monolingual corpora 
? Wikipedia-articles 
? Patent-descriptions 
? UMLS descriptions 
? AACT 
? GENIA 
? GREC 
? FMA 
? PIL 
We also use target side of parallel corpora as 
additional monolingual resource to train lan-
guage model. We separately train a 5-gram lan-
guage model for each monolingual corpus and 
integrate them as features to log-linear model in 
the PBSMT system.  
For the query translation (QT) system, we use 
parallel corpus Wikipedia-titles and UMLS dic-
tionary, and use monolingual corpus Wikipedia-
articles.  
3.2 Experiment Setting 
For the tuning of PBSMT system, we use devel-
opment set provided by WMT 14 medical task 
(khresmoi-summary-dev). And we use query 
translation development set (khresmoi-query-
dev) for the tuning of QT system.  
We test our systems on two test set provided 
by WMT 14 medical task. 
? khresmoi-summary-test (for PBSMT) 
? khresmoi-query-test (for QT) 
231
For comparison with result from QT system, 
we translate the test set of query translation task 
(khresmoi-query-test) using PBSMT system 
without any post-processing. 
In our experiment, the performance of transla-
tion system is measured by BLEU (%) and trans-
lation error rate - TER (%). All these results are 
evaluated from the evaluation website3.  
3.3 Experiment Result 
Table 1 shows the results for the task of transla-
tion of sentences from summaries of medical 
articles.  
Table 2 shows the results for the task of trans-
lation of queries entered by users of medical in-
formation search engines. The performance of 
QT system is relatively higher than PBSMT sys-
tem. Especially, the BLEU score of QT system 
on English-German language pair is the highest 
score among the all submitted systems.  
 
Language Pair BLEU TER 
English-German 15.8 0.746 
German-English 26.9 0.618 
 
Table 1: BLEU scores of result from PBSMT system 
for summary translation task. 
 
Language Pair BLEU TER 
PBSMT English-German 15.1 0.748 
 German-English 22.1 0.638 
QT English-German 15.3 0.746 
 German-English 24.5 0.586 
 
Table 2: BLEU scores of result for query translation 
task. 
4 Conclusion 
We describe the PBSMT system and QT system 
that are developed for summary translation and 
query translation of WMT 14 medical translation 
task. We focus on intrinsic query translation 
evaluation and propose a hybrid approach by 
combining dictionary-based approach and SMT 
based approach using heuristics. The result of 
query translation experiment shows that our 
method obtained higher translation accuracy than 
the baseline (PBSMT) system. 
 
Acknowledgments 
 
This work was supported in part by the National 
Korea Science and Engineering Foundation 
3 http://matrix.statmt.org 
(KOSEF) (NRF-2010-0012662), in part by the 
Brain Korea 21+ Project, and in part by the Ko-
rea Ministry of Knowledge Economy (MKE) 
under Grant No.10041807. 
References  
Chen, S. F., & Goodman, J. (1996). An empirical 
study of smoothing techniques for language 
modeling. Paper presented at the Proceedings of 
the 34th annual meeting on Association for 
Computational Linguistics. 
Federico, M., Bertoldi, N., & Cettolo, M. (2008). 
IRSTLM: an open source toolkit for handling large 
scale language models. Paper presented at the 
Interspeech. 
Kneser, R., & Ney, H. (1995). Improved backing-off 
for m-gram language modeling. Paper presented at 
the Acoustics, Speech, and Signal Processing, 1995. 
ICASSP-95., 1995 International Conference on. 
Koehn, P., Hoang, H., Birch, A., Callison-Burch, C., 
Federico, M., Bertoldi, N., . . . Herbst, E. (2007). 
Moses: open source toolkit for statistical machine 
translation. Paper presented at the Proceedings of 
the 45th Annual Meeting of the ACL on Interactive 
Poster and Demonstration Sessions, Prague, Czech 
Republic.  
Koehn, P., Och, F. J., & Marcu, D. (2003). Statistical 
phrase-based translation. Paper presented at the 
Proceedings of the 2003 Conference of the North 
American Chapter of the Association for 
Computational Linguistics on Human Language 
Technology-Volume 1. 
Och, F. J., & Ney, H. (2003). A systematic 
comparison of various statistical alignment models. 
Comput. Linguist., 29(1), 19-51. doi: 
10.1162/089120103321337421 
 
                                                 
232
