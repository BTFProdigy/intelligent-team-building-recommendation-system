Proceedings of the 13th Annual Meeting of the Special Interest Group on Discourse and Dialogue (SIGDIAL), pages 247?256,
Seoul, South Korea, 5-6 July 2012. c?2012 Association for Computational Linguistics
Combining Verbal and Nonverbal Features to Overcome the ?Information Gap? in Task-Oriented Dialogue 
 Eun Young Ha, Joseph F. Grafsgaard, Christopher M. Mitchell,  Kristy Elizabeth Boyer, and James C. Lester Department of Computer Science North Carolina State University Raleigh, NC, USA {eha, jfgrafsg, cmmitch2, keboyer, lester}@ncsu.edu    Abstract Dialogue act modeling in task-oriented dialogue poses significant challenges. It is particularly challenging for corpora consisting of two interleaved communication streams: a dialogue stream and a task stream. In such corpora, information can be conveyed implicitly by the task stream, yielding a dialogue stream with seemingly missing information. A promising approach leverages rich resources from both the dialog and the task streams, combining verbal and non-verbal features. This paper presents work on dialogue act modeling that leverages body posture, which may be indicative of particular dialogue acts. Combining three information sources (dialogue exchanges, task context, and users? posture), three types of machine learning frameworks were compared. The results indicate that some models better preserve the structure of task-oriented dialogue than others, and that automatically recognized postural features may help to disambiguate user dialogue moves.  1 Introduction Dialogue act classification is concerned with understanding users? communicative intentions as reflected in their utterances. It is an important first step toward building automated dialogue systems. To date, the majority of work on dialogue act 
modeling has addressed spoken dialogue (Samuel et al, 1998; Stolcke et al, 2000; Surendran and Levow, 2006; Bangalore et al, 2008; Sridhar et al, 2009; Di Eugenio et al, 2010). However, with the increasing popularity of computer-mediated means of conversation, such as instant messaging and social networking services, automated analysis of textual dialogue holds much appeal. Dialogue act modeling for textual conversations has many practical application areas, which include web-based intelligent tutoring systems (Boyer et al, 2010a), chat-based online customer service (Kim et al, 2010), and social media analysis (Joty et al, 2011). Human interaction involves not only verbal communication but also nonverbal communication. Research on nonverbal communication (Knapp and Hall, 2006; Mehrabian, 2007; Russell et al, 2003) has identified a range of nonverbal cues, such as posture, gestures, eye gaze, and facial and vocal expressions. However, the utility of these nonverbal cues has not been fully explored within the context of dialogue act classification research. Previous research has leveraged prosodic cues (Sridhar et al, 2009; Stolcke et al, 2000) and facial expressions (Boyer et al, 2011) for automatic dialogue act classification, but other types of nonverbal cues remain unexplored. As a first step toward a dialogue system that learns its behavior from a human corpus, this paper proposes a novel approach to dialogue act classification that leverages information about users? posture. Posture has been found to be a significant indicator of a broad range of emotions (D?Mello and Graesser, 2010; Kapoor et al, 2007; Woolf et al, 2009). Based on the premise that emotion plays an 
247
important role in dialogue, this work hypothesizes that adding posture features will improve the performance of automatic dialogue act models.   The domain considered in this paper is task-oriented textual dialogue collected in a human tutoring study. In contrast to conventional task-oriented dialogue corpora (e.g., Carletta et al, 1997; Jurafsky et al, 1998; Ivanovic, 2008) in which conversational exchanges are carried out within a single channel of dialogue between the dialogue participants, the corpus used in this work utilizes two separate and interleaved streams of communication. One stream is the textual conversation between a student and a tutor (dialogue stream). The other is the student?s problem-solving activity (task stream). As will be described in Section 3, the interface used in the corpus collection was designed to allow the tutor to monitor the student?s problem-solving activities. Thus, the student?s problem-solving activities and the tutor?s monitoring of those activities functioned as an implicit communication channel. This characteristic of the corpus poses significant challenges for dialogue act modeling. First, because the dialogue stream and the task stream are interleaved, the dialogue stream alone may not be coherent. Second, since information can be exchanged implicitly via the task stream, the dialogue likely contains substantial information gaps1. Addressing these challenges, the dialogue act models described in this paper combine three sources of information: the verbal information from the dialogue stream, the task-related context from the task stream, and information about users? posture. This paper makes several contributions to the dialogue research community. First, it is the first effort to explore posture as a nonverbal cue for dialogue act classification. Second, the proposed approach is fully automatic and ready for real-world application. Third, this paper explicitly defines the notion of information gap in task-oriented dialogue consisting of multiple communication channels, which has only begun to be explored in the context of dialogue act classification (Boyer et al, 2010a). Finally, this                                                 1 In this paper, information gap is defined as the information that is missing from the explicit verbal exchanges between the dialogue participants but conveyed by the implicit task stream. 
paper examines adaptability of previous dialogue act classification approaches in conventional task-oriented domains by comparing three classifiers previously applied to dialogue act modeling for task-oriented dialogue. 2 Related Work A rich body of research has addressed data-driven approaches for dialogue act modeling. Russell et al (2003) applied a transformation-based learning approach for dialogue act tagging for spoken dialogue, using speaker direction, punctuation, marks, and cue phrases. Stolcke et al (2000) modeled the structure of dialogue as an HMM, treating the dialogue acts as the observations emitted from the hidden states of the learned HMM. More recently, Bangalore et al (2008) proposed a unified approach to task-oriented dialogue, in which both the user dialogue act classification and the system dialogue act selection were informed by a shared maximum entropy dialogue act classifier. Sridhar et al (2009) also used a maximum entropy model, exploring the utility of different representations of prosodic features. Di Eugenio et al (2010) used a memory-based classifier, in combination with a modified latent semantic analysis (LSA) technique by augmenting the original word-document matrix in LSA with rich linguistic features. While most work on dialogue act modeling has focused on spoken dialogue, a recent line of investigation has explored the analysis of textual conversation, such as asynchronous online chat conversation (Wu et al, 2005; Forsyth, 2007; Reitter et al, 2010; Joty et al, 2011) and synchronous online chat conversation   (Ivanovic, 2008; Kim et al, 2010; Boyer et al, 2010a). Wu et al (2005) proposed a transformation-based learning approach for an asynchronous chat posting domain, utilizing regular expression-based selection rules. For a similar domain, Forsyth (2007) applied neural networks and Na?ve Bayes classification technique using lexical cues. Ritter et al (2010) and Joty et al (2011) applied unsupervised learning approaches to dialogue act modeling for Twitter conversations, in which dialogue acts were automatically discovered by clustering raw utterances. Work by Ivanovic (2008) and Kim et al (2010) analyzed one-to-one synchronous online chat dialogue in a task-oriented 
248
customer service domain. Ivanovic (2008) applied maximum entropy, na?ve Bayes, and support vector machines using word n-gram features. Kim et al (2010) compared the CRF, HMM-SVM, and Na?ve Bayes classifiers using word n-grams and features extracted from the dialogue structure, in which CRF achieved the highest performance. Boyer et al (2010a) investigated dialogue act modeling for task-oriented tutorial dialogue, applying a logistic regression approach using lexical, syntactic, dialogue structure, and task structure features. Some previous dialogue act modeling work (Boyer et al, 2011; Sridhar et al, 2009; Stolcke et al, 2000) leveraged nonverbal information such as prosodic cues (Sridhar et al, 2009; Stolcke et al, 2000) and facial expressions (Boyer et al, 2011). Stolcke et al (2000) combined various prosodic features such as pitch, duration, and energy. Sridhar et al (2009) represented the sequence of prosodic features as n-grams. Boyer et al (2011) leveraged confusion-related facial expressions for tutorial dialogue. Like Boyer et al (2010a), this work addresses dialogue act classification for task-oriented textual conversation in a web-based tutoring domain. In contrast to Boyer et al (2010a), whose approach directly leveraged manually annotated features, making it challenging to apply the proposed model to a real-world system, the present work is fully automatic and ready for real-world application.  A novel feature of this work is its utilization of nonverbal cues carried by users? posture. This is the first dialogue act classification work that leverages posture information. 3 Data The corpus used in this paper consists of textual exchanges between a student and a tutor in a web-based remote-tutoring interface for introductory programming in Java. The corpus was collected from a series of six tutoring lessons, covering progressive topics in computer science over the course of four weeks. The tutoring interface consisted of four windows: a task window displaying the current programming task; a code window in which the student writes Java code; an output window for displaying the result of compiling and running the code; and a chat window for instant exchange of textual dialogue 
between the student and tutor. With this tutoring interface, the student and the tutor were able to exchange textual dialogue and share a synchronized view of the task. Apart from sending dialogue messages, the only action the tutor could perform to affect the student?s interface was advancing to the next programming task.  3.1 Data Collection The data collection conducted in Fall 2011 paired 42 students with one of four tutors for six forty-minute tutoring sessions on introductory computer science topics.  The students were chosen from a first-year engineering course and were pre-screened to filter out those with significant programming experience. The tutors were graduate students with previous tutoring or teaching experience in Java programming. Students were compensated for their participation with partial course credit. The students worked with the same tutor for the entire study. Each lesson consisted of between four and thirteen distinct subtasks. During each tutoring session, the dialogue text exchanged between the student and the tutor was logged to a database. Additional runtime data including content of the student?s Java code, the result (e.g., success or failure) of compiling and running the student?s code, and the IDs of the subtask were logged. All logged data were time-stamped at a millisecond precision. Students? body posture was recorded at a rate of 8 frames per second with a Kinect depth camera, which emits infrared rays to measure distance for each pixel in a depth image frame. The camera was positioned above the student?s computer monitor, ensuring the student?s upper body is centered in the recorded image. Tutors were not recorded. 3.2 Dialogue Act Annotation For the work described in this paper, a subset of the collected data was manually annotated, which include the first of the six tutoring lessons from 21 students. This corpus contains 2564 utterances (1777 tutor, 787 student). The average number of utterances per tutoring session was 122 (min = 74; max = 201). The average number of tutor utterances per session was 84.6 (min = 51; max = 137) and the average number of student utterances per session was 37.4 (min = 22; max = 64). 
249
Extending a previous annotation scheme used for similar task-oriented tutorial dialogue (Boyer et al, 2010b), the scheme used in this work consists of 13 dialogue act tags (Appendix). The dialogue turns that contained more than one dialogue function were segmented into multiple utterances before being assigned a dialogue act tag. The annotation scheme did not constrain any of the dialogue act tags as applying either to students? or tutors? utterances only; however, the resulting distribution of the tags in the annotated corpus show certain dialogue act tags were more relevant to either students? or tutors? utterances. Figure 1 depicts an excerpt from the corpus with the manually applied dialogue act annotations.  
 Three human annotators were trained to apply the scheme. The training consisted of an iterative process involving collaborative and independent tagging, followed by refinements of the tagging protocol. At the initial phase of training, the annotators tagged the corpus collaboratively. In later phases annotators tagged independently. To compute agreement between different annotators, 24% (5 of the 21 sessions) of the corpus were doubly annotated by two annotators. All possible 
pairs of the annotators participated in double annotation. The aggregate agreement was .80 in Cohen?s Kappa (Cohen, 1960). 3.3 Posture Estimation Posture has been found to be a significant indicator of a broad range of emotions such as anxiety, boredom, confusion, engaged concentration (or flow), frustration, and joy (D?Mello and Graesser, 2010; Kapoor et al, 2007; Woolf et al, 2009). Early investigations into posture utilized pressure-sensitive chairs which provided indirect measures of upper-body posture (D?Mello and Graesser, 2010; Kapoor et al, 2007; Woolf et al, 2009). Newer, computer vision-based techniques provide more detailed postural data (Sanghvi et al, 2011). The present work uses a posture estimation algorithm developed to automatically detect the head, mid torso, and lower torso through depth image recordings of seated individuals (Grafsgaard et al, 2012). With this estimation algorithm, posture is represented as a triple of head depth (distance between camera and head), mid torso depth, and lower torso depth. A dataset of depth camera recordings from the first of the six tutoring lessons consists of 512,977 depth image frames collected across 18.5 hours of computer-mediated human-human tutoring among 33 participants.2 For each depth image frame, the posture algorithm scanned through the three middle regions that corresponded to head, mid-torso, and lower-torso of the recorded person, and selected a single representative depth pixel from each region. The boundaries for each region were heuristically determined relying on the placement of the students? chairs in the middle of the depth recording view at a common distance. Given these constraints, the model was manually verified by two independent human judges to have 95.1% accuracy across 1,109 depth image snapshots corresponding to one-minute intervals across the dataset. The algorithm output for each depth image was labeled as erroneous if either judge found that any of the posture tracking points did not coincide with its target region. Example output of the algorithm is shown in Figure 2.  
                                                2 The other 9 sessions were not successfully recorded because of technical errors. 
Tutor: hang on :) [S] Tutor: When we show you example code, it is not the code you need to write. [S] Tutor: Look at the task again. [H] Student writes programming code Tutor: YUP [PF] Tutor: Perfect [PF] Tutor: OK. Go ahead and test. [DIR] Student: And I don't need anything in the parentheses? [Q] Tutor: Line 9 is correct. You do NOT need anything inside the parentheses. [A] Student: Ok [ACK] Student compiles and runs code successfully Tutor: Good. [PF]  Tutor: Moving on. [S] Tutor advances to the next task. Student writes programming code Tutor: Syntactically correct. But there is a logic error [LF] Tutor: When will the output statement display your request to the player? [Q] Student: AFTER they put in their name [A] Tutor: Exactly [PF] Figure 1. Corpus Excerpt with Dialogue Act Annotation 
250
4 Features For web-based one-to-one dialogue systems, it is important to achieve efficient runtime performance. To maximize real-world feasibility of the learned dialogue act classifiers, this work only considers the features that can be automatically extracted at runtime. In addition, the use of linguistic analysis software, such as a part-of-speech tagger and a syntactic parser, is intentionally restrained. One might argue that rich linguistic analysis may provide additional information to dialogue act classifiers, potentially improving the performance of learned models. However, there is a trade-off between additional information obtained by rich linguistic analysis and processing time. In addition, previous work (Boyer et al, 2010a) found part-of-speech and syntax features did not provide obvious benefit for dialogue act classification in a domain similar to the one considered in this work. The dialogue act classifiers described in this paper integrate four classes of features automatically extracted from three sources of information: the textual dialogue utterances, task-related runtime information logged into the database, and the images of the students recorded by depth cameras. Each feature class is explained in the following subsections. 4.1 Lexical Features Based on previous dialogue act classification research (Bangalore et al, 2008; Boyer et al, 2010a; Kim et al, 2010), this work utilizes word n-grams as features for dialogue act classification. In the experiment reported in Section 5, unigrams and 
bigrams were used. Adding higher order n-grams did not improve model accuracies. In our corpus (Section 3), the nature of the student dialogues is informal and utterances contain many typos. To remove undesirable noise in the data such as typos and rare words, n-grams were filtered out according to their frequency in the training data (i.e., n-grams that appear less than a predefined cutoff threshold in the training data are not included as features). The value of the cutoff threshold was empirically determined by testing the values between 0 and 10 on a development data set that consisted of 20% of randomly selected dialogue sessions. The value of 3 was selected as it yielded the highest classification accuracy. 4.2 Dialogue Context Features While lexical features characterize the intrinsic nature of individual utterances, the context of the utterance within a larger dialogue structure provides additional information about a given utterance in relation with other utterances. This work considers the following dialogue context features: ? Utterance Position: Specifies the relative position of an utterance at a given turn. The value of this feature indicates whether the utterance is the first one in a given turn, the second or later one in a given turn, or the given turn consists of a single utterance. ? Length: Specifies the number of a given utterance in terms of individual word tokens. ? Previous Author: Indicates whether the author of the previous utterance was student or tutor. ? Previous Tutor Dialogue Act: Specifies dialogue act of the most recent tutor utterance. The value of this feature is directly extracted from the manual annotation in the corpus, because in the broader context of our work, tutor dialogue moves will be determined by an external dialogue management module.   4.3 Task Context Features In our data, students? problem-solving activities (e.g., reading the problem description, writing computer programming code, and compiling and running the code) functioned as an implicit communication channel between students and tutors (Section 1). Because of the existence of this 
Figure 2. Automatically detected posture points (H = headDepth, M = midTorsoDepth, L = lowerTorsoDepth)  
 H 
 M  L 
251
implicit communication channel, the dialogue exchanges between students and tutors likely contain substantial information gaps. To overcome such information gaps, it is important to identify effective task context features. The present work leverages the following task context features, which can be automatically extracted during runtime: ? Previous Task Action: Specifies the type of the most recent problem-solving action performed by the student. The value could be message (writing a textual message to the tutor) code (writing code in the code window), or compile_run (compiling or running the code). ? Task Begin Flag: A binary feature that indicates whether a given utterance is the first one since the current problem task was posted.  ? Task Activity Flag: Another binary feature indicating that a given utterance was preceded by a student?s task activity. ? Last Compile/Run Status: Specifies the status (e.g., begin, stop, success, error, input sent) of the most recent compile/run action performed by the students.  In addition to the listed task context features, the utility of time information was also explored, such as the amount of time taken for previous coding activity and the elapsed time since the beginning of the current task. However, these features did not positively impact the performance of the learned models and were thus excluded. 4.4 Posture Features After preprocessing recorded image frames with the estimation algorithm (Section 3.3), students? postures were represented as tuples of three different integer values, each respectively representing head depth, mid torso depth, and lower torso depth. To extract posture features, the time window of n seconds directly preceding a given utterance was compared with the previous time window of the same size in terms of min, max, median, average, and variance of each depth value. The indicators of whether each of these values has increased, decreased or remained the same were considered as potential posture features. To avoid introducing errors to the model by insignificant changes in posture, an error tolerance ?  was allowed (i.e., the two compared postures 
were considered the same unless the amount of the change in the posture was greater than ?). Optimal values for n and ?  were empirically determined, selecting the values that maximized classification accuracy on the development data set. For n, the values between 0 and 60 were compared at an interval of 10. The value of 50 was selected for head depth and 60 for both mid torso depth and lower torso depth.  Similarly, the value of ?  was determined by comparing the values between 0 and 200 with an increment of 10. The selected value was 100.  All the potential posture features were examined in an informal experiment, in which each of the potential posture features were added to the combination of the lexical, the dialogue context, and the task context features. The posture features that improved the classification accuracy after adding them were included in the present dialogue act models. The selected posture features are min of head depth and max, median, and average of lower torso depth. None of the mid torso depth features were selected. 5 Experiment The goal of this experiment is twofold: (1) to evaluate the effectiveness of the feature classes and (2) to compare the performance of three classifiers: maximum entropy (ME), na?ve Bayes (NB), and conditional random field (CRF). These classifiers are chosen because they have been shown effective for dialogue act modeling in traditional task-oriented textual dialogue, in which conversational exchanges were carried out by a single channel of dialogue (Ivanovic, 2008; Kim et al, 2010). Previous result by Kim et al (2010) suggests a structured model such as CRF yields more accurate dialogue act model compared to unstructured models (e.g., Na?ve Bayes), because of its ability to model the sequential patterns in target classification labels. This experiment examines whether a similar finding is observed for our domain, which exhibits substantial information gaps due to the existence of an implicit communication channel, the task stream. 5.1 Dialogue Act Modeling All classifiers were built using the MALLET package (McCallum, 2002). This experiment used the manually annotated portion of the data 
252
described in Section 3. The original dialogue scheme (Section 3.2) was slightly modified by introducing an additional dialogue act, GR, in order to distinguish conventional expressions, such as greetings and thanks, from other information-delivering utterances. For this modified scheme, annotator agreement was 0.81 in Cohen?s Kappa on the doubly annotated portion of the corpus. 6 among the 21 dialogue sessions in the annotated data do not have accompanying images due to technical problems with the depth camera, thus these sessions were excluded from this experiment. Table 1 shows the distribution of the student dialogue act tags in the resulting corpus of 15 dialogues used in this experiment. The most frequent tag was A (answer), followed by ACK (acknowledgement) and Q (question). The features were extracted by aligning three sources of information (the textual dialogue corpus, the task-related runtime log data, and the recorded images) by timestamp. Word boundaries in the dialogue corpus were recognized by the surrounding white spaces and punctuations. The dialogue context features (D) leveraged in this paper includes previous tutor dialogue act. This feature takes the manually annotated value in the corpus, because this work assumes the existence of an external dialogue manager. However, since the external dialogue manager is not likely to achieve 100% accuracy in predicting human tutor dialogue acts, it would be informative to estimate a reasonable range of the accuracies of the student dialogue act model, taking into account the errors introduced by the dialogue manager. For this reason, two versions of the dialogue context features were considered in this experiment: one that leverages the full set of dialogue context features (D) and the other that excludes previous 
tutor dialogue act (D-). These respectively provide the maximum and the minimum expected accuracy of the student dialogue act model, when used with a dialogue manager. The models were trained and tested using five-fold cross validation, in which the 15 dialogue sessions were partitioned into 5 non-overlapping sets of the same size (i.e., 3 sessions per partition). Each set was used for testing exactly once. 5.2 Results Table 2 reports the average classification accuracies from the five-fold cross validation. The majority baseline accuracy for our data is .347, when the classifier always chooses the most frequent dialog act (A). The first group of rows in Table 3 report the accuracies of individual feature classes. All of the individual features performed better than the baseline. The improvement from the baseline was significant except for D- with CRF. The most powerful feature class was dialogue context class when the full set was used. The second group in Table 3 shows the effects of incrementally combining the feature classes. Adding dialogue act features to the lexical features (L + D) brought significant improvement in the classification accuracy for ME and CRF. Adding posture features (L + D + T + P) also improved the accuracy of ME by a statistically significant margin. The last group shows similar results for ME when the previous tutor dialogue act was excluded from the dialogue context, except that the improvement achieved by adding the posture features (L + D- + T + P) was not significant.  
Student Dialogue Act Distribution A (answer) 192 (34.7%) ACK (acknowledgement) 124 (22.4%) Q (question)  92 (16.6%)  S (statement) 76 (13.7%) GR (greeting and thanks) 52 (9.4%) C (clarification) 6 (1.0%) RF (request for feedback) 5 (.9%) RC (request confirmation) 2 (.4%) O (other) 5 (.9%) Total 554 Table 1. Student dialogue acts in the experiment data 
Features ME NB CRF 
 Indiv
idual  Lexical (L)     .696
**     .703**     .599**  Dialogue (D)     .711**     .715**     .696**  Dialogue- (D-)     .477**     .473**     .405  Task (T)     .405**     .396*      .386*  Posture (P)     .382*     .385*     .399* 
 Max  L + D     .772
??     .724     .692??  L + D + T     .777     .729     .694  L + D + T + P     .789?     .714     .682 
 Min  L + D-     .724
??     .681     .606  L + D- + T     .733     .671     .627  L + D- + T + P     .750     .676     .644 Table 2. Classification accuracies (*p < .05, **p < .01 compared to baseline; ??p < .01 compared to L; and ?p < .05 compared to L + D + T, with paired-samples t-test)  
253
The highest accuracy was achieved by ME when using all four classes of the features, with maximum (L + D + T + P) .789 and minimum (L + D- + T + P) .750. For both the maximum and the minimum conditions, the differences among the classifiers were significant (p < .01, one-way repeated measure ANOVA), with post-hoc Tukey HSD tests revealing ME was significantly better than both NB (p < .05) and CRF (p < .01). There was no significant difference between NB and CRF. 6 Discussion The experiment described in Section 5 compared the utility of lexical, dialogue context, task context, and posture features for dialogue act classification. The results indicate the effectiveness of these features. Particularly, adding the dialogue context and the posture features improved the accuracy of the maximum entropy model. Although the margin of improvement achieved by adding posture features was relatively small, the improvement was statistically significant (p < .05) for the maximum condition (L + D + T + P), which suggests that the users? posture during computer-mediated textual dialogue conveys important communicative messages. The experiment also compared three classifiers: maximum entropy, na?ve Bayes, and CRF. Interestingly, CRF was the worst-performing model for our data, contradicting the previous finding by Kim et al (2010), in which CRF (a structured classifier) performed significantly better than Na?ve Bayes (a non-structured classifier). This contradictive result suggests that, in our domain, the presence of an implicit communication channel resulted in substantial information gaps in the dialogue and it poses new challenges that were not encountered by conventional task-oriented domains consisting of a single communication channel.  The maximum entropy classifier achieved the best overall performance, reaching accuracy of .789. This is an encouraging result compared to previous work in a similar domain. Boyer et al (2010a) reported an accuracy of .628 for dialogue act classification in a similar domain. However, a direct comparison is not applicable since different data were used in their work. 
7 Conclusions and Future Work Dialogue act modeling for a task-oriented domain in which the dialogue stream is interleaved with the task stream poses significant challenges. With the goal of effective dialogue act modeling, this work leverages information about users? posture as non-verbal features. An experiment found that posture is a significant indicator of dialogue acts, in addition to lexical features, dialogue context, and task context. The experiment also compared three statistical classifiers: maximum entropy, naive Bayes, and CRF. The best performing model was maximum entropy. Using all features, the maximum entropy achieved .789 in accuracy. Several directions for future work are promising. First, given the encouraging finding that nonverbal information plays a significant role as a communicative means for task-oriented dialogue, various types of non-verbal information can be investigated, such as gesture and facial expressions. Second, incorporating richer task features, such as in our case, deep analysis of student code, may contribute to more accurate dialogue act modeling. Third, it is important to generalize the findings to a larger data set, including across other task-oriented domains.  Finally, the community is embracing a move toward annotation-lean approaches such as unsupervised or semi-supervised learning, which hold great promise for dialogue modeling. Acknowledgments This research was supported by the National Science Foundation under Grant DRL-1007962. Any opinions, findings, and conclusions expressed in this material are those of the authors and do not necessarily reflect the views of the National Science Foundation. References Bangalore, S., Di Fabbrizio, G., & Stent, A. (2008). Learning the structure of task-driven human-human dialogs. IEEE Transactions on Audio, Speech, and Language Processing, 16(7), 1249-1259. Boyer, K. E., Grafsgaard, J. F., Ha, E. Y., Phillips, R., & Lester, J. C. (2011). An affect-enriched dialogue act classification model for task-oriented dialogue. Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human 
254
Language Technologies (pp. 1190-1199). Portland, OR. Boyer, K. E., Ha, E. Y., Phillips, R., Wallis, M. D., Vouk, M. A., & Lester, J. C. (2010a). Dialogue Act Modeling in a Complex Task-Oriented Domain. Proceedings of the 11th Annual SIGDIAL Meeting on Discourse and Dialogue (pp. 297-305). Tokyo, Japan. Boyer, K. E., Phillips, R., Ingram, A., Ha, E. Y., Wallis, M., Vouk, M., & Lester, J. (2010b). Characterizing the effectiveness of tutorial dialogue with hidden markov models. Proceedings of the 10th international conference on Intelligent Tutoring Systems (pp. 55-64). Pittsburgh, PA. Carletta, J., Isard, A., Isard, S., Kowtko, J., Doherty-Sneddon, G., & Anderson, A. (1997). The reliability of a dialogue structure coding scheme. Computational Linguistics, 23, 13?31. Cavicchio, F. (2009). The modulation of cooperation and emotion in dialogue: The REC corpus. Proceedings of the ACL-IJCNLP 2009 Student Research Workshop (pp. 81 - 87). Suntec, Singapore. Cohen, J. (1960). A coefficient of agreement for nominal scales. Educational and Psychological Measurement, 20(1), 37 - 46. Di Eugenio, B., Xie, Z., & Serafin, R. (2010). Dialogue act classification, instance-based learning, and higher order dialogue structure. Dialogue and Discourse, 1(2), 81 - 104. D?Mello, S., & Graesser, A. (2010). Mining Bodily Patterns of Affective Experience during Learning. Proceedings of the 3rd International Conference on Educational Data Mining (pp. 31-40). Pittsburgh, PA. Forsyth, E. N. (2007). Improving Automated Lexical and Discourse Analysis of Online Chat Dialog. Master's thesis. Naval Postgraduate School. Grafsgaard, J. F., Boyer, K. E., Wiebe, E. N., & Lester, J. C. (2012). Analyzing Posture and Affect in Task-Oriented Tutoring. Proceedings of the 25th Florida Artificial Intelligence Research Society Conference (pp. 438-443). Marco Island, FL. Ivanovic, E. (2008). Automatic instant messaging dialogue using statistical models and dialogue acts. Master's thesis. The University of Melbourne. Joty, S. R., Carenini, G., & Lin, C.-Y. (2011). Unsupervised Modeling of Dialog Acts in Asynchronous Conversations. Proceedings of the 22nd International Joint Conference on Artificial 
Intelligence (pp. 1807-1813). Barcelona, Catalonia, Spain. Jurafsky, D., Bates, R., Coccaro, N., Martin, R., Meteer, M., Ries, K., Shriberg, E., et al (1998). Switchboard discourse language modeling project report. Baltimore, MD. Kapoor, A., Burleson, W., & Picard, R. W. (2007). Automatic prediction of frustration. International Journal of Human-Computer Studies, 65(8), 724-736. Kim, S. N., Cavedon, L., & Baldwin, T. (2010). Classifying dialogue acts in one-on-one live chats. Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing (pp. 862-871). Cambridge, MA. Knapp, M. L., & Hall, J. A. (2006). Nonverbal Communication in Human Interaction (6th ed.). Belmont, CA: Wadsworth/Thomson Learning. McCallum, A. K. (2002). MALLET: A Machine Learning for Language Toolkit. Available from  http://mallet.cs.umass.edu Mehrabian, A. (2007). Nonverbal Communication. New Brunswick, NJ: Aldine Transaction. Ritter, A., Cherry, C., & Dolan, B. (2010). Unsupervised modeling of twitter conversations. Proceedings of Human Language Technologies: The 2010 Annual Conference of the North American Chapter (pp. 172 - 180). Los Angeles, CA. Russell, J. A., Bachorowski, J. A., & Fernandez-dols, J. M. (2003). Facial and vocal expressions of emotion. Annual Review of Psychology, 54, 329-349. Sanghvi, J., Castellano, G., Leite, I., Pereira, A., McOwan, P. W., & Paiva, A. (2011). Automatic analysis of affective postures and body motion to detect engagement with a game companion. Proceedings of the 6th international conference on Human-robot interaction (pp. 305-312). Lausanne, Switzerland. Sridhar, R., Bangalore, S., & Narayanan, S. (2009). Combining lexical, syntactic and prosodic cues for improved online dialog act tagging. Computer Speech and Language, 23(4), 407 - 422. Stolcke, A., Ries, K., Coccaro, N., Shriberg, E., Bates, R., Jurafsky, D., Taylor, P., et al (2000). Dialogue act modeling for automatic tagging and recognition of conversational speech. Computational Linguistics, 26(3), 339-373. Surendran, D., & Levow, G.-A. (2006). Dialog act tagging with support vector machines and hidden 
255
Markov models. Proceedings of Interspeech (pp. 1950 - 1953). Pittsburgh, PA. Woolf, B., Burleson, W., Arroyo, I., Dragon, T., Cooper, D., & Picard, R. W. (2009). Affect-aware tutors recognising and responding to student affect. International Journal of Learning Technology, 4(3/4), 129-164. Wu, T., Khan, F. M., Fisher, T. A., Shuler, L. A., & Pottenger, W. M. (2005). Posting Act Tagging Using Transformation-Based Learning. In T. Y. Lin, S. Ohsuga, C.-J. Liau, X. Hu, & S. Tsumoto (Eds.), Foundations of Data Mining and knowledge Discovery (pp. 319 - 331). Springer. 
  
Appendix. Dialogue Act Annotation Scheme and Inter-rater Agreement Tag Description Frequency Agreement (k) H  Hint:  The tutor gives advice to help the student proceed with the task Tutor:     Student:     133 0 .50 DIR   Directive:  The tutor explicitly tells the student the next step to take Tutor:     Student:     121 0 .63 ACK   Acknowledgement:  Either the tutor or the student acknowledges previous utterance; conversational grounding Tutor:       Student:  41 175 .73 RC   Request for Confirmation:  Either the tutor or the student requests confirmation from the other participant (e.g., ?Make sense??) Tutor:       Student:  11 2 Insufficient data RF   Request for Feedback:  The student requests an assessment of performance or work from the tutor Tutor:     Student:    0 5 1.0 PF  Positive Feedback:  The tutor provides a positive assessment of the student?s performance Tutor:     Student:     327 0 .90 LF Lukewarm Feedback:  The tutor provides an assessment that has both positive and negative elements Tutor:      Student:    13 0 .80 NF Negative Feedback:  The tutor provides a negative assessment of the student?s performance Tutor:        Student:     1 0 .40 Q Question:  A question regarding the task that is not a direct request for confirmation or feedback Tutor:     Student:  327 120   .95 A Answer:  An answer to an utterance marked Q Tutor:       Student:  96 295 .94 C Correction:  Correction of a typo in a previous utterance Tutor:       Student:  10 6 .54 S  Statement:  A statement regarding the task that does not fit into any of the above categories Tutor:     Student:  681 174 .71 O Other: Other utterances, usually containing only affective content Tutor:     Student:  6 10 .69 
256
Proceedings of the SIGDIAL 2013 Conference, pages 204?213,
Metz, France, 22-24 August 2013. c?2013 Association for Computational Linguistics
Learning Dialogue Management Models for Task-Oriented Dialogue with Parallel Dialogue and Task Streams 
 Eun Young Ha, Christopher M. Mitchell, Kristy Elizabeth Boyer,  and James C. Lester Department of Computer Science North Carolina State University Raleigh, NC 27695, USA {eha,cmmitch2,keboyer,lester}@ncsu.edu     Abstract 
Learning dialogue management models poses significant challenges. In a complex task-oriented domain in which information is ex-changed via parallel, interleaved dialogue and task streams, effective dialogue management models should be able to make dialogue moves based on both the dialogue and the task context. This paper presents a data-driven ap-proach to learning dialogue management mod-els that determine when to make dialogue moves to assist users? task completion activi-ties, as well as the type of dialogue move that should be selected for a given user interaction context. Combining features automatically ex-tracted from the dialogue and the task, we compare two alternate modeling approaches. The results of an evaluation indicate the learned models are effective in predicting both the timing and the type of system dialogue moves. 1 Introduction Automated dialogue systems allow users to in-teract with information systems in a natural and intuitive manner. With the growth of speech-enabled applications for mobile devices, the de-mands for practical dialogue systems have been increasing at an accelerating pace. The core tasks of automated dialogue systems include dialogue management, which is concerned with selecting system actions in response to a given user input. Traditionally, dialogue managers have been manually constructed. However, manually craft-ing dialogue managers is labor-intensive and yields systems that are brittle with respect to un-expected user behaviors. For rapid creation of robust and adaptive dialogue systems, data-driven approaches to dialogue management hold 
much appeal. Recent work on dialogue systems has explored machine learning techniques to au-tomatically learn dialogue managers from corpo-ra (Scheffler and Young, 2002; Hardy et al, 2006; Williams and Young, 2007; Bangalore et al, 2008; Sridhar et al, 2009). To support more natural human-computer dia-logue, earlier work on dialogue systems envi-sioned rich interaction environments that take into account observed user actions for selecting optimal dialogue strategies (Carberry, 1990; Rich and Sidner, 1998; Allen et al, 2001). However, recent data-driven approaches have primarily focused on application domains in which infor-mation between the user and the system are communicated solely by dialogue, such as tele-phone-based systems (Hardy et al, 2006; Bangalore et al, 2008) and online chat dialogues (Ivanovic, 2008; Kim et al, 2010). With increas-ing demands for natural human-computer inter-action beyond these restricted application do-mains, dialogue systems are required to support more complex types of interaction, in which us-ers perform tasks in parallel to exchanging dia-logue. For instance, dialogue interfaces for task-assistance systems, such as intelligent tutoring systems, should be able to monitor users? task completion activities and incorporate the ob-served activities into dialogue management deci-sions such that the systems can provide users with spontaneous assistance (e.g., providing hints) even without an explicit request from the user.  We have been exploring data-driven ap-proaches for a complex task-oriented application domain in which information is delivered both by exchanging dialogue with users and by ob-serving users? task completion activities. Our previous work has focused on the automatic in-terpretation of user dialogue input (Boyer et al, 
204
2010; Ha et al, 2012). Findings suggest that identifying an effective representation to com-bine information from dialogue and users? task completion activities is key to effective dialogue processing in a domain consisting of parallel dia-logue and task streams. As the next step in this line of investigation on complex task-oriented domains with parallel dia-logue and task streams, this work proposes an approach to automatically learning dialogue management models from a human dialogue cor-pus. The proposed approach combines infor-mation from a dialogue stream and a task stream in order to create spontaneous dialogue interven-tions for users based on monitoring users? activi-ties. Two subtasks of dialogue management are addressed: the first is to determine when to pro-vide dialogue feedback (timing), and the second is to determine what kind of dialogue feedback to provide (type). Dialogue managers in conven-tional domains have primarily focused on the selection of feedback type. However, determin-ing the appropriate timing of system moves is critical for dialogue systems that support parallel dialogue and task streams.  The work presented here makes three contri-butions. First, it endeavors to expand data-driven dialogue management by addressing more com-plex task-oriented domains consisting of parallel dialogue and task streams. Second, it proposes a timing intervention model that determines the correct time to make spontaneous system inter-ventions. Third, it presents a maximum entropy dialogue management model and compares al-ternate approaches. It also compares the predic-tive power of the dialogue and task streams on the targeted dialogue management tasks. 2 Related Work Data-driven approaches to dialogue management continue to be the subject of increasing attention within the dialogue community. Prominent among these are reinforcement learning ap-proaches for learning dialogue policies from cor-pora (Henderson et al, 2008; Levin et al, 2000; Lewis and Di Fabbrizio, 2006; Roy et al, 2000; Scheffler and Young, 2002; Singh et al, 2002; Williams and Young, 2007; Young, 2002). These approaches model dialogue as Markov decision processes, either fully observable (MDPs) or par-tially observable (POMDPs), in which the transi-tions of dialogue states are associated with sys-tem actions and rewards. The goal of reinforce-ment learning is to learn optimal policies that 
maximize aggregate expected rewards, such as user satisfaction (Walker et al, 1997). Learned policies that result from RL exploration do not, by design, necessarily reflect the patterns in the bootstrap dialogue corpus. Additionally, to cover all possible state spaces, reinforcement learning typically requires a very large set of training da-ta, which limits the complexity of the dialogue system in its representation of the dialogue states and the system actions (Young et al, 2013).  A second body of related work focuses on dia-logue act classification. Classification-based ap-proaches aim at learning the patterns of dialogue that are present in the corpus. A variety of ma-chine learning frameworks have been exploited, including hidden Markov models (Stolcke et al, 2000; Boyer et al,  2010), maximum entropy models (Bangalore et al, 2008; Sridhar et al, 2009; Ha et al, 2012), support vector machines (Ivanovic, 2008), conditional random fields (Kim et al, 2010),  and memory-based classifiers in combination with latent semantic analysis (Di Eugenio et al, 2010). Classification-based ap-proaches incorporate rich sets of features, includ-ing not only lexical information, syntactic fea-tures, and dialogue structure, but also prosodic features in the case of spoken dialogue (Stolcke et al, 2000; Sridhar et al, 2009) and non-verbal features such as facial expressions (Boyer et al, 2011) and shifts in posture (Ha et al, 2012). While most work on dialogue act classifica-tion has focused on either offline analysis of dia-logue (Stolcke et al, 2000; Ivanovic, 2008; Kim et al, 2010; Di Eugenio et al, 2010) or interpre-tation of user dialogue (Boyer et al, 2010; Ha et al, 2012), Bangalore et al (2008) utilized dia-logue act classification as a mechanism for de-termining system dialogue moves. They pro-posed a unified dialogue act classification ap-proach for both the interpretation of user utter-ances and selection of system dialogue moves. Our work is similar to Bangalore et al (2008) in that it takes a dialogue act classification ap-proach to the task of selecting system dialogue moves. However, it addresses the problems posed by complex task-oriented application do-mains in which information is communicated not only by dialogue exchanges but also by monitor-ing users? task performance. In such domains, a user?s task activities constitute a full communica-tive stream in its own right, separate from the dialogue stream. The challenges of parallel dia-logue and task streams are addressed by exploit-ing automatically obtained task features com-bined with dialogue features. In contrast to pre-
205
vious work (Bangalore et al 2008, Boyer et al, 2010), in which task information was derived from manual annotation, our work utilizes auto-matically computed task features. Our work also focuses on a growing applica-tion area of dialogue systems: intelligent tutor-ing. In support of student learning, recent work in this area utilized human tutorial dialogue cor-pora to learn effective tutorial strategies using MDPs (Chi et al, 2010; Mitchell et al, 2013), to develop tutorial dialogue models that adapt to students? affective states (Forbes-Riley and Litman, 2011), and to improve robustness of a symbolic tutorial dialogue system (Dzikovska et al, 2013).  3 Task-Oriented Dialogue Corpus To learn dialogue management models from nat-urally occurring human-to-human dialogue we utilize a human tutorial dialogue corpus we col-lected in the domain of introductory program-ming in Java. The corpus consists of textual dia-logue exchanges between students and tutors in a web-based remote-tutoring interface, aligned with task context logs (Appendix A). A subset of the corpus was annotated with dialogue acts, which was used to train and test the dialogue management models described in this paper. 3.1 Human tutoring study The data collection study involved forty-two un-dergraduate students who were paired with one of four tutors. The students were enrolled in a first-year engineering course and were pre-screened to filter out those with significant pro-gramming experience. The students were com-pensated for their participation with partial course credit. The tutors were graduate students with previous tutoring or teaching experience in Java programming, and the students worked with the same tutor for the entire study. Each lesson consisted of between four and thirteen distinct subtasks.  The students completed six forty-minute tutor-ing lessons, covering progressive topics in intro-ductory computer science over four weeks. Each lesson consisted of four to thirteen subtasks, in which later subtasks built upon earlier ones. Dur-ing each tutoring session, the paired student and tutor interacted remotely using a web-based tu-toring interface. With this tutoring interface, the student and the tutor were able to exchange tex-tual dialogue and share a synchronized view of the task.  
For each lesson, students completed a pre-test and a post-test before and after the main tutoring session. The pre- and post-test consisted of the same set of questions to assess students? knowledge related to the lesson?s objectives. Compared to students? pre-test results, signifi-cant learning gains were observed on the post-test, which indicates that the tutorial dialogue was effective for student learning (Mitchell et al, 2012).  3.2 Dialogue annotation A subset of the collected data was manually an-notated with dialogue acts using an annotation scheme consisting of 13 dialogue act tags for task-oriented tutorial dialogue (Table 1). The annotated corpus consists of the first of the six tutoring lessons from 21 students, which contains 2564 utterances (1777 tutor, 787 student). The average number of utterances per tutoring ses-sion was 122 (min = 74; max = 201). The aver-age number of tutor utterances per session was 84.6 (min = 51; max = 137), and the average number of student utterances per session was 37.4 (min = 22; max = 64). Three human annotators were trained to apply the scheme. The training consisted of an iterative process involving collaborative and independent tagging, followed by refinements of the tagging protocol. At the initial phase of training, the an-notators tagged the corpus collaboratively. In later phases annotators tagged independently. To compute agreement between different annotators, 24% (5 of the 21 sessions) of the corpus were doubly annotated by two annotators. All possible pairs of the annotators participated in double an-notation. The aggregate agreement was 0.80 in Cohen?s Kappa (Cohen, 1960). 4 Dialogue Management Models To support a task-oriented dialogue system capa-ble of not only responding to users? dialogue in-put but also providing spontaneous system inter-vention during users? task activities, a dialogue manager should provide two functionalities. The first is to determine the timing of a system dia-logue move (i.e., whether or not to provide a tu-torial dialogue move at a given context). The second is to determine the type of dialogue move (i.e., selecting from available system dialogue acts). In this work, the problem of determining the system?s next dialogue move is cast as a clas-sification task. In previous work we found a maximum entropy approach was effective for 
206
classifying user dialogue acts for task-oriented dialogue with parallel dialogue and task streams (Ha, 2012). Maximum entropy outperformed both Naive Bayes and conditional random fields. Building on these results, we employ a maximum entropy classifier to learn dialogue management models that predict both the timing and the type of the system dialogue move. The following sec-tions describe two alternate approaches to dia-logue management that can both determine the timing and determine the type of system dialogue interventions.  4.1 One-step dialogue management model In the first model, the two dialogue management tasks are framed as a single classification prob-lem by treating the decision of not to make a tu-torial dialogue move as a special dialogue act. Thus, a finite set of dialogue moves allowed for the system is defined as ? = ??,??,? ,?? , in which ? = ?? ? ? ?????? and ?? ={???, ???,? , ???}  is the set of dialogue acts available for the system. Given ?  and the ??? step in a given user interaction history ????? =  ????? , ??????,? , ??, the goal of the dia-logue management model is to predict system?s dialogue move ???? for the next step, which is determined by the following equation. ???? = ?????????? ? ?????              (1) 
The task-oriented dialogue considered in this work includes two parallel and interleaved data streams: an explicit dialogue stream, consisting of textual exchanges between a student and a tutor, and an implicit task stream, consisting of the student?s problem-solving activities. Thus, a given interaction history can be decomposed into a dialogue history and a task history, rewriting equation (1) as follows, ???? = ?????????? ? ????? , ?????     (2) in which ????? = ???? , ??????,? , ??  and ????? = ???? , ??????,? , ??  represent the history of dialogue utterances and the history of student task activities, respectively. In this work, the conditional probability distri-bution in Equation (2) is estimated using the maximum entropy framework (Berger et al, 1996). The maximum entropy framework selects a probability distribution that results in the high-est entropy among all possible solutions. Given a vector ? of feature set, the conditional probabil-ity distribution is estimated by the following equation, ? ? = ?? ? =  ? ??(?) ??????                     (3) in which ? represents weights and ? is a normal-izing factor. This work used MALLET 
Tag Description Agreement H Hint: The tutor gives advice to help the student proceed with the task .50 DIR  Directive: The tutor explicitly tells the student the next step to take .63 ACK  Acknowledgement: Either the tutor or the student acknowledges previous utterance; conversational grounding .73 RC  Request for Confirmation: Either the tutor or the student requests confirmation from the other participant (e.g., ?Make sense??) Insufficient data RF  Request for Feedback: The student requests an assessment of his performance or his work from the tutor 1.0 PF  Positive Feedback: The tutor provides a positive assessment of the student?s perfor-mance .90 LF Lukewarm Feedback: The tutor provides an assessment that has both positive and nega-tive elements .80 NF Negative Feedback: The tutor provides a negative assessment of the student?s perfor-mance .40 Q Question: A question which does not fit into any of the above categories .95 A Answer: An answer to an utterance marked Q .94 C Correction: Correction of a typo in a previous utterance .54 S  Statement: A statement which does not fit into any of the above categories .71 O Other: Other utterances, usually containing only affective content .69 Table 1.  Dialogue act annotation scheme and inter-rater agreement 
207
(McCallum, 2002) to estimate this conditional distribution.  4.2 Two-step dialogue management model A potential shortcoming of the one-step model is that the probability distribution over dialogue acts is prone to distortion depending on the por-tion of NoMove in the training data. To avoid this, the second model takes a two-step approach, treating each dialogue management task as an independent classification task. The two-step model first determines whether or not to make a dialogue move. If a decision is made to provide a dialogue move, the second classifier is called for a selection of the type of dialogue move.  In this model, system?s dialogue move ???? for the next interaction step is determined by a function ? ????? , such that ? ????? = ??????, ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? (4) when  ?? ?????? ????? > ? ???? ?????                               ? ????? = ??????? ? ? ? ?? ?????       (5) otherwise.  Similar to the one-step model, Equation (5) can be written as ? ????? = ??????? ? ? ? ?? ????? , ?????  (6) This conditional probability distribution is also estimated by the maximum entropy framework. 5 Features To learn high-performing dialogue management models for task-oriented dialogue with parallel dialogue and task streams, it is crucial to have an effective representation of user interaction state that captures information from all available data streams. The dialogue management models de-scribed in the previous section determine the sys-tem?s next dialogue move based on user interac-tion state specified by the features extracted from the dialogue and the task streams. In contrast to previous work on task-oriented dialogue, in which task information is incorporated into dia-logue utterances by manual tagging (Bangalore et al, 2008; Boyer et al, 2010), our work does not require manual effort to obtain the relevant task information. Instead, we rely on task context logs generated during students? interactions with the tutoring interface, as well as a notion of stu-dents? task progress automatically estimated by a task analysis algorithm. The same set of features 
is used for the prediction of both the timing and the type of system move. 5.1 Automatic task analysis In order to provide a measure of students? task progress through each of the tutoring sessions, an edit distance metric was implemented. This met-ric computes the minimum edit distance between a student?s program at a particular time t and a representative solution for a given programming task, in order to estimate how far away the stu-dent is from completing the task. Because our tutors were experienced in the subject matter and were familiar with the lesson structures, we can safely assume that they knew what this final state of the code would be and thus had an intuitive metric of student progress, which is analogous to our edit distance metric. As this value changes over a session, one can observe how the stu-dent?s progress is affected by tutor dialogue acts. Because a character-based edit distance would not capture the relative functional importance of each part of the student?s program, our edit dis-tance metric is based on tokenized versions of the program, as generated by the Java compiler, and the output is the number of tokens that differ from the solution for that task. In this way, comments, variable names, or string literals with many characters are all treated as single tokens and do not artificially inflate the edit distance. This tokenization also allows for abstraction of these comments, variable names, and string liter-als into generalized tokens so that they can be more easily compared between students.  5.2 Dialogue features Previous work on dialogue act classification has shown that lexical features extracted from dia-logue utterances are good predictors of dialogue acts (Bangalore et al, 2008; Boyer et al, 2010a; Kim et al, 2010). However, this finding does not apply when the goal of dialogue act classification is to learn dialogue management models because determining system moves precedes system ut-terance generation. Instead, this work exploits features that represent local interaction structure within dialogue streams, which includes current student dialogue act, current tutor dialogue act, previous tutor dialogue act, and tutor utterance count. ? Current student dialogue act represents the interpreted dialogue act for the previ-ous user dialogue input. Student dialogue act interpretation is not addressed in this 
208
paper, assuming the existence of an exter-nal module that carries out user dialogue interpretation (e.g., Ha et al, 2012). ? Current tutor dialogue act represents the type of system dialogue act at the current interaction step. In our tutorial dialogue corpus, we observed tutors often made several dialogue utterances in succession, such as a feedback (?Great Job.?) fol-lowed by a question (?Do you have any questions??). Thus, the value of the cur-rent tutor dialogue act impacts the proba-bility distribution over the tutor?s next dia-logue move. This feature captures such temporal patterns present in tutor dialogue moves as observed in the corpus. ? Previous tutor dialogue act represents the type of system dialogue act generated for the previous interaction step. This is simi-lar to the current tutor dialogue act fea-ture, but models longer temporal patterns by extending the size of interaction history. ?  Tutor utterance count represents the number of system dialogue acts generated in succession without interruption until the current interaction step. In our corpus, it was observed that the tutor dialogue turns often consist of multiple utterances. This feature is included to model system dia-logue turns consisting of multiple utteranc-es. 5.3 Task features To create a rich representation of task context, a number of features were automatically extracted from task streams. Three groups of task infor-mation were considered, including types of task activity taken by user, the amount of time taken between certain activities, and the user?s task progress estimated by the task analysis algorithm (Section 5.1). Alternate representations of these features were empirically compared, resulting in the following task features incorporated in cur-rent dialogue management models. ? Current log type represents the type of activity taken at the current interaction step either by the user or the system. A complete list of log types is shown in Appendix B.  ? Previous log type represents the type of activity taken at the previous interaction step. Analogous to previous tutor dia-logue act in dialogue stream, this feature 
models temporal patterns among task ac-tivities. ? Same log type is a binary feature indi-cating the type of activities at the current and previous interaction step is identical.  ? Previous and current log type is a fea-ture that combines the current and previ-ous log types (i.e., a bigram of log types). ? Elapsed time is the amount of time since the last logged activity, which rep-resents the duration of the user?s inac-tivity. This feature is included to enable the learned dialogue management model to make spontaneous dialogue interven-tions when a user has been detected to be inactive for an excessive period of time.  ? Elapsed coding time specifies the amount of time the user has taken since the beginning of current coding task.  6 Evaluation The dialogue act models were trained and tested using the manually annotated portion of the task-oriented tutorial dialogue corpus described in Section 3. The textual dialogue exchanges in the corpus were aligned with the logged task-completion activities based on the timestamp, resulting in 6919 total interaction logs. Table 2 shows the distribution of different types of ac-tivities in the resulting interaction logs. It was observed that tutors made a dialogue move in 26.5% of the total logged interactions (Table 3). 
Among the thirteen dialogue acts in the origi-nal annotation scheme (Section 3.2), four rarely occurring dialogue acts were combined into other categories, which include LF (lukewarm feed-back) merged with NF (negative feedback) and RC (request for confirmation), RF (request for feedback), and C (correction) merged to O (oth-er).  A new category, GREET (greetings) was 
Interaction Type Frequency (%) Programming 38.2 Compiling the Program 10.8 Running the Program 12.2 Progressing to Next Task 4.2 Exchanging Dialogue 34.6 Table 2. Distribution of interaction types 
Tutor Dialogue Move Frequency (%) Move 26.5 NoMove 73.5 Table 3. Distribution of system dialogue move 
209
added to distinguish conventional expressions for greetings and thanks from more general state-ments and questions. Table 4 shows the resulting distribution of tutor dialogue acts in the corpus. 
The performance of the dialogue act models were evaluated in a ten-fold cross validation. In the cross validation, the corpus was partitioned to ten non-overlapping sets and each set was used as testing data exactly once, while models were trained using the remaining nine sets. 6.1 Results The first study compared the accuracies of the dialogue management models on predicting the timing and the type of tutor dialogue moves. The accuracy of the timing prediction was calculated for all user interaction logs in the data, including both dialogue exchanges and task-completion activities. The accuracy of the type prediction was calculated for dialogue activities by tutors only. The results are shown in Table 5. 
Both the one-step (t(9) = 4.14, p = 0.0013) and the two-step (t(9) = 6.26, p < .0001) models per-formed significantly better than the majority baseline in predicting the timing of tutorial dia-logue moves. The two-step model achieved higher accuracy than the one-step model. The difference between the two models was statisti-cally significant (t(9) = 2.17, p = 0.0291).  The one-step (t(9) = 2.68, p = 0.0126) and the two-step (t(9) = 10.93, p < 0.0001) models 
achieved significantly higher accuracies over the baseline for the task of predicting the type of tu-torial dialogue moves, as well. Again, the two-step model performed significantly better than the one-step model (t(9) = 4.22, p = .0011).  6.2 Comparing dialogue and task streams The second study compared the predictive power of the dialogue stream and the task stream on the given two dialogue management tasks. In this study, the accuracy of the two-step model was compared in three conditions: using the dialogue features only (Dialogue), using the task features only (Task), and using all features (All). Table 6 reports the results. 
For determining when to intervene, the dia-logue and the task features exhibited similar pre-dictive power. No statistical significance was found for the difference between the dialogue and the task conditions. The highest accuracy was achieved by the All condition. Compared to the All condition, the Dialogue condition showed statistically significant decrease in accuracy (t(9) = 2.21, p = 0.0272), which implies the task stream provided important features for the dia-logue management model in determining the tim-ing of tutorial dialogue moves. A similar trend was observed for determining what type of dialogue move to make. The Dia-logue and the Task conditions achieved similar accuracies, with the highest accuracy achieved by the All condition. The drops in accuracy com-pared to the All condition were statistically sig-nificant for both the Dialogue (t(9) = 3.38, p = 0.0040) and the Task conditions. (t(9) = 4.36, p = 0.0009). The results imply that the prediction of the type of tutorial dialogue moves required in-formation from both the dialogue and the task streams.  7 Discussion The experiments presented in Section 6 com-pared two alternate approaches to learning dia-logue management models for two given sub-tasks: determining when to provide the user with a dialogue move, and determining which type of 
Dialogue Act Frequency (%) S (Statement) 35.4 PF (Positive Feedback) 19.8 Q (Question) 16.0 H (Hint)  8.0 DIR (Directive)  6.6 A (Answer)  5.7 GREET (Greetings)  3.1 ACK (Acknowledgement)  2.3 NF (Negative Feedback)  1.5 O (Other)  1.6 Table 4. Distribution of tutor dialogue acts 
Model Timing Type Baseline 73.5 35.4 One-step 79.2* 40.5* Two-step  80.3*?  49.7*? Table 5. Model accuracy (%) on dialogue man-agement tasks (*statistical significance over baseline, ?statistical significance over one-step model) 
Features Timing Type Dialogue 79.6 45.0 Task 80.1 44.9 All  80.3*  49.7*? Table 6. Comparison of features on dialogue management tasks (*statistical significance over Dialogue, ?statistical significance over Task) 
210
dialogue move to choose. The results suggest that the two-step approach, which models the two subtasks as separate classifiers, was more effective than the alternate one-step approach, which combined the two subtasks into a single classification problem. The two-step model achieved higher performance than the one-step model in both the timing and the type prediction. However, the difference in the performance of the two models was more apparent in the type prediction, with the two-step model achieving over 22% higher accuracy than the one-step model. One possible explanation for the superi-ority of the two step-model over the one-step model is that the corpus used to train the models was highly skewed. For more than 73% of the total interaction logs in the corpus, the tutors did not provide any dialogue feedback. Since the one-step model treated NoMove as a special dia-logue act, the skewed distribution over NoMove and Move impacted the learned distribution over dialogue acts.  Two previous investigations reported the accu-racies of dialogue act classification on system utterances. Bangalore et al (2008) reported a prediction accuracy of 55% for system dialogue acts when a flat task model was used in a cata-logue-ordering domain. When a hierarchical task structure was used in the same domain, the achieved prediction accuracy for system dialogue acts was 35.6% (Bangalore and Stent, 2009). Boyer (2010) achieved accuracy of 57% for sys-tem dialogue acts in a task-oriented tutorial dia-logue. While both of these lines of investigation employed task structure features that were manu-ally annotated, our best-performing two-step dia-logue management model resulted in comparable performance utilizing only automatic features, achieving an accuracy of 49.7%. A crucial distinction between user and system dialogue act classification is that lexical features for a given dialogue turn are not available for system dialogue act classification because a sys-tem utterance is generated after a system dia-logue act is selected. The absence of lexical fea-tures poses a significant challenge to system dia-logue act classification, given that lexical fea-tures have been among the most predictive fea-tures for this task. To address this challenge, fu-ture research should continue exploring larger spaces of features to improve prediction accura-cies of learned models. 
8 Conclusions and Future Work Automatically learning dialogue management models for complex task-oriented domains with separate dialogue and task streams poses signifi-cant challenges. Effective dialogue management models in such domains should be able to proac-tively intervene by making spontaneous dialogue moves based on the observed history of both the dialogue and the user?s task activities. With the overarching goal of creating a data-driven auto-mated dialogue system that incorporates parallel dialogue and task streams, this paper has pre-sented classification-based dialogue management models that integrate a rich set of features auto-matically extracted from parallel dialogue and task streams. Two subtasks of dialogue manage-ment were considered: when the system should provide user with a dialogue move and what type of system dialogue act the system should select for a given user interaction context. An evalua-tion found that a two-step approach that modeled the two subtasks as separate classifiers were ef-fective, achieving significantly higher perfor-mance than an alternate approach that modeled the two subtasks with a single classifier. The results suggest several promising direc-tions for future work. First, incorporating richer features may improve the accuracies of learned models, such as more global interaction histories and deeper dialogue structures. Second, develop-ing more sophisticated task analyses will inform the learned models with a representation of the user task context, guiding the models to make more context-appropriate decisions. Finally, it will be important to evaluate the learned models by incorporating them into a dialogue manage-ment system and validating their effectiveness in interactions with users in rich task-oriented dia-logue.  Acknowledgments This research was supported by the National Sci-ence Foundation under Grant DRL-1007962. Any opinions, findings, and conclusions ex-pressed in this material are those of the authors and do not necessarily reflect the views of the National Science Foundation. References  Allen, J., Ferguson, G., & Stent, A. (2001). An architecture for more realistic conversational systems. Proceedings of Intelligent User Interfaces (pp. 1?8). Santa Fe, NM. 
211
Bangalore, S., Di Fabbrizio, G., & Stent, A. (2008). Learning the structure of task-driven human-human dialogs. IEEE Transactions on Audio, Speech, and Language Processing, 16(7), 1249?1259. Bangalore, S., & Stent, A. J. (2009). Incremental parsing models for dialog task structure. Proceedings of the 12th Conference of the European Chapter of the Association for Computational Linguistics (pp. 94?102). Athens, Greece. Berger, A. L., Della Pietra, V. J., & Della Pietra, S. A. (1996). A maximum entropy approach to natural language processing. Computational Linguistics, 22(1), 39?71. Boyer, K. E. (2010). Structural and Dialogue Act Modeling in Task-Oriented Tutorial Dialogue. Ph.D. Dissertation. Department of Computer Science, North Carolina State University. Boyer, K. E., Grafsgaard, J. F., Ha, E. Y., Phillips, R., & Lester, J. C. (2011). An affect-enriched dialogue act classification model for task-oriented dialogue. Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies (pp. 1190?1199). Portland, OR. Boyer, K. E., Ha, E. Y., Phillips, R., Wallis, M. D., Vouk, M. A., & Lester, J. C. (2010). Dialogue Act Modeling in a Complex Task-Oriented Domain. Proceedings of the 11th Annual SIGDIAL Meeting on Discourse and Dialogue (pp. 297?305). Tokyo, Japan. Carberry, S. (1991). Plan Recognition in Natural Language Dialogue. MIT Press. Cavicchio, F. (2009). The modulation of cooperation and emotion in dialogue: The REC corpus. Proceedings of the ACL-IJCNLP 2009 Student Research Workshop (pp. 81?87). Suntec, Singapore. Chi, M., VanLehn, K., Litman, D., & Jordan, P. (2010). Inducing Effective Pedagogical Strategies Using Learning Context Features. Proceedings of the Eighteenth International Conference on User Modeling, Adaptation, and Personalization (pp 147-158). Big Island, HI. Cohen, J. (1960). A coefficient of agreement for nominal scales. Educational and Psychological Measurement, 20(1), 37 ? 46. Di Eugenio, B., Xie, Z., & Serafin, R. (2010). Dialogue act classification, instance-based learning, and higher order dialogue structure. Dialogue and Discourse, 1(2), 81 ? 104. Dzikovska, M.O., Farrow, E, & Moore, J.D. (2013). Combining deep parsing and classification for improved explanation processing in a tutorial dialogue system. Proceedings of the 16th International Conference on Artificial Intelligence in Education (pp. 279 - 288). Memphis, TN. Forbes-Riley, K. & Litman, D. (2011). Designing and evaluating a wizarded uncertainty-adaptive spoken 
dialogue tutoring system. Computer Speech and Language, 25(1), 105-126. Ha, E. Y., Grafsgaard, J. F., & Mitchell, C. M. (2012). Combining Verbal and Nonverbal Features to Overcome the ?Information Gap? in Task-Oriented Dialogue. Proceedings of the 13th Annual Meeting of the Special Interest Group on Discourse and Dialogue (pp. 247?256). Seoul, South Korea. Hardy, H., Biermann, A., Inouye, R. B., McKenzie, A., Strzalkowski, T., Ursu, C., Webb, N., et al (2006). The Amiti?s system: Data-driven techniques for automated dialogue. Speech Communication, 48(3-4), 354?373. Henderson, J., Lemon, O., & Georgila, K. (2008). Hybrid reinforcement/supervised learning of dialogue policies from fixed data sets. Computational Linguistics, 34(4), 487?511. Ivanovic, E. (2008). Automatic instant messaging dialogue using statistical models and dialogue acts. The University of Melbourne. Kim, S. N., Cavedon, L., & Baldwin, T. (2010). Classifying dialogue acts in one-on-one live chats. Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing (pp. 862?871). Cambridge, MA, USA: Association for Computational Linguistics. Levin, E., Pieraccini, R., & Eckert, W. (2000). A Stochastic Model of Human-Machine Interaction for Learning Dialog Strategies. IEEE Transactions on Speech and Audio Processing, 8(1), 11?23. Lewis, C., & Di Fabbrizio, G. (2006). Prompt selection with reinforcement learning in an AT&T call routing application. Proceedings of the Ninth International Conference on Spoken Language Processing (pp. 96?103). Mitchell, C.M., Boyer, K.E., & Lester, J.C. (2013). A Markov Decision Process Model of Tutorial Intervention in Task-Oriented Dialogue.  Proceedings of the International Conference on Artificial Intelligence in Education (pp. 828-831), Memphis, TN. Mitchell, C. M., Ha, E. Y., Boyer, K. E., & Lester, J. C. (2012). Recognizing effective and student-adaptive tutor moves in task-oriented tutorial dialogue. Proceedings of the Intelligent Tutoring Systems Track of the 25th International Conference of the Florida Artificial Intelligence Research Society (pp. 450?455). Rich, C., & Sidner, C. (1998). COLLAGEN: A col-laboration manager for software interface agents. User Modeling and User-Adapted Inter-action, 8(3-4), 315?350. Roy, N., Pineau, J., & Thrun, S. (2000). Spoken dialogue management using probabilistic reasoning. Proceedings of the 38th Annual Meeting on Association for Computational Linguistics (pp. 93?100). Wanchai, Hong Kong. Scheffler, K., & Young, S. (2002). Automatic learning of dialogue strategy using dialogue simulation and reinforcement learning. 
212
Proceedings of the second international conference on Human Language Technology Research (pp. 12?19). San Diego, CA. Singh, S., Litman, D. J., Kearns, M., & Walker, M. (2002). Optimizing Dialogue Management with Reinforcement Learning: Experiments with the NJFun System. Journal of Artificial Intelligence Research, 16, 105?133. Sridhar, R., Bangalore, S., & Narayanan, S. (2009). Combining lexical, syntactic and prosodic cues for improved online dialog act tagging. Computer Speech and Language, 23(4), 407 ? 422. Stolcke, A., Ries, K., Coccaro, N., Shriberg, E., Bates, R., Jurafsky, D., Taylor, P., et al (2000). Dialogue act modeling for automatic tagging and recognition of conversational speech. Computational Linguistics, 26(3), 339?373. Walker, M., Litman, D., Kamm, C., & Abella, A. (1997). Paradise: A framework for evaluating 
spoken dialogue agents. Proceedings of ACL (pp. 271?280). Madrid, Spain. Williams, J., & Young, S. (2007). Partially Observable Markov Decision Processes for Spoken Dialog Systems. Computer Speech and Language, 21(2), 393?422. Young, S. (2002). Talking to machines (statistically speaking). Proceedings of ICSLP (pp. 32?41). Denver, CO. Young, S., Gasic, M., Thomson, B., & Williams, J. (2013). POMDP-Based Statistical Spoken Dialog Systems: A Review. Proceedings of the IEEE, 101(5), 1160?1179.    
Appendix A. An Excerpt from the Task-Oriented Dialogue Corpus Lesson ID Task ID Role Type Text Timestamp 1 4 STUDENT CODING System.out.printIn("Hello World" 2011-09-21 08:17:17.737 1 4 STUDENT CODING System.out.printIn("Hello World") 2011-09-21 08:17:19.407 1 4 STUDENT CODING System.out.printIn("Hello World"); 2011-09-21 08:17:19.812 1 4 TUTOR MESSAGE good. 2011-09-21 08:17:24.913 1 4 TUTOR MESSAGE also you can try to compile at anytime. 2011-09-21 08:17:33.805 1 4 STUDENT COMPILE_ BEGIN studentCode\jt101\JavaTutor3.java 2011-09-21 08:17:38.080 1 4 STUDENT COMPILE_ ERROR line 1  : cannot find symbol symbol  : method printIn(java.lang.String) location: class java.io.PrintStream System.out.printIn("Hello World");           ^ 1 error 
2011-09-21 08:17:38.220 
1 4 TUTOR MESSAGE carefully compare your line with the example 2011-09-21 08:17:57.330 Appendix B.  Types of Activity Logs in Corpus Log Type Description Action Initiator MESSAGE Either student or tutor has sent a chat message. Student, Tutor SESSION_PROGRESS Tutor has allowed student to progress to next task. Tutor CODING Student has written programming code. Student COMPILE_BEGIN Student has begun compiling code. Student COMPILE_SUCCESS Recent code compilation has ended successfully. N/A COMPILE_ERROR Recent code compilation has failed with errors. N/A RUN_BEGIN Student has begun running code. Student INPUT_SENT Student has sent an input to a running code. Student RUN_SUCCESS Recent code running has ended successfully. N/A RUN_STOP Tutor has stopped running student?s code because of errors in the code. Tutor 
213
Proceedings of the SIGDIAL 2013 Conference, pages 339?343,
Metz, France, 22-24 August 2013. c?2013 Association for Computational Linguistics
Evaluating State Representations for Reinforcement  Learning of Turn-Taking Policies in Tutorial Dialogue 
 Christopher M. Mitchell Kristy Elizabeth Boyer James C. Lester Department of Computer Science North Carolina State University Raleigh, NC, USA {cmmitch2, keboyer, lester}@ncsu.edu     Abstract 
Learning and improving natural turn-taking behaviors for dialogue systems is a topic of growing importance. In task-oriented dia-logue where the user can engage in task ac-tions in parallel with dialogue, unrestricted turn taking may be particularly important for dialogue success. This paper presents a novel Markov Decision Process (MDP) representa-tion of dialogue with unrestricted turn taking and a parallel task stream in order to automat-ically learn effective turn-taking policies for a tutorial dialogue system from a corpus. It also presents and evaluates an approach to auto-matically selecting features for an MDP state representation of this dialogue. The results suggest that the MDP formulation and the feature selection framework hold promise for learning effective turn-taking policies in task-oriented dialogue systems. 1 Introduction Determining when to make a dialogue move is a topic of growing importance in dialogue systems. While systems historically relied on explicit turn-taking cues, more recent work has focused on learning and improving on natural turn-taking behaviors (Raux and Eskenazi 2012; Selfridge et al 2012). For tutorial dialogue in particular, ef-fectively timing system moves can substantially impact the success of the dialogue. For example, failing to provide helpful feedback to a student who is confused may lead to decreased learning (Shute 2008) or to disengagement (Forbes-Riley and Litman 2012), while providing tutorial feed-back or interventions at inappropriate times could also have a negative impact on the out-come of the dialogue (D?Mello et al 2010).  Reinforcement Learning (RL) is a widely used approach to constructing effective dialogue poli-
cies using either MDPs or POMDPs (Williams and Young 2007). To date, RL has been applied to learn the most effective dialogue move to make, but has not been applied to learning the timings of these moves, although the related con-cept of when to release a turn has been explored (English and Heeman 2005). The domain of tuto-rial dialogue poses an additional modeling chal-lenge: the dialogue is task-oriented, but unlike many task-oriented dialogues in which all infor-mation is communicated via dialogue, students solve problems within a separate task stream which conveys essential information for dialogue management decisions.  This paper addresses dialogue with both unre-stricted turn taking and a parallel task stream with a novel Markov Decision Process represen-tation. Because turn boundaries are not clearly defined or enforced, we apply RL to the problem of when to make a dialogue move, rather than what type of dialogue move to make. In order to determine which criteria are most relevant to making this decision, the approach utilizes a fea-ture selection approach based on a new Separa-tion Ratio metric and compares the selected fea-tures against an existing approach based on ex-pected cumulative reward (Chi et al 2011). Fi-nally, the resulting feature spaces are evaluated with simulated users acquired in a supervised fashion from held-out portions of the corpus. The results inform the development of turn-taking policies in task-oriented dialogue systems. 2 Corpus The corpus used for this work was collected dur-ing 2011 and 2012 as part of the JavaTutor tuto-rial dialogue project. It consists of 66 textual dia-logues between human tutors and students, with an average of 90 tutor dialogue moves and 36 student dialogue moves. Each pair interacted for through a computer-mediated interface to com-
339
plete introductory computer programming tasks. Students edited their computer programs within a parallel task stream also collected as part of the corpus (see Appendix A). Tutors viewed the task actions synchronously through the interface. The success of each dialogue was measured by learn-ing gain between pretest and posttest. Overall the dialogues were effective; the average learning gain was 42.3% (statistically > 0; p < .0001). The substantial variation in learning gains (min=-28.6%; max= 100%) will be leveraged within the MDP reward structure. 3 MDP Representation A Markov Decision Process (MDP) models a system in which a policy can be learned to max-imize reward (Sutton and Barto 1998). It consists of a set of states S, a set of actions A representing possible actions by an agent, a set of transition probabilities indicating how likely it is for the model to transition to each state s? ? S from each state s ? S when the agent performs each action a ? A in state s, and a reward function R that maps real values onto transitions and/or states, thus signifying their utility.  Previous applications of RL to dialogue sys-tems, using both MDPs and POMDPs, have dealt with the decision of what type of dialogue move to make (Chi et al 2011; Williams and Young 2007). These systems make this decision either at predetermined decision points (Tetreault and Litman 2008), following the trigger of a silence threshold (Raux and Eskenazi 2012), or when the system determines it has enough information to advance the dialogue (Selfridge et al 2012). For the JavaTutor corpus, however, the tutor could choose to make a move at any time. Rather than applying handcrafted rules to determine decision points, we apply RL to learn when to make a dia-logue move in order to maximize the success of the dialogue. For this MDP, the action set is de-fined as A = {TutorMove, NoMove}.  The states for the MDP consist of combina-tions of features representing the current state of the session. The possible features available for selection are described in Table 1, and are all automatically extracted from system logs. The Task Trajectory and Edit Distance features are based on computing a token-level edit distance from a student?s program with respect to that student?s final correct solution. This distance measures a student?s progress over the course of a dialogue while avoiding the need to manually annotate the task stream. In a deployed system, 
this edit distance can be estimated by comparing to previously acquired solutions from other stu-dents.   Feature Description Values Current Action The current action being taken by the student  ? TASK ? STUDENTDIAL ? NOACTION 
Task  Trajectory 
The effect of the last task action on the edit distance to the final task solution 
? CLOSER ? FARTHER ? NOCHANGE 
Last  Action Last turn taken by either interlocutor ? TUTORDIAL ? STUDENTDIAL ? TASK Number of Tutor Moves Number of tutor turns taken thus far in the dialogue ? LOW  (< 30) ? MID   (30-59) ? HIGH (> 60) Edit  Distance The edit distance to the final solution ? LOW  (< 20) ? MID   (20-49) ? HIGH (> 50) Elapsed Idle Time The number of se-conds since the last student action ? LOW  (< 7) ? MID   (7-15) ? HIGH (> 15)  Table 1. Features available to be selected Tutor moves are encoded as MDP actions, while student actions are encoded as transitions to a new state with a NoMove tutor action. To account for the possibility that both interlocutors could construct messages simultaneously or that dialogue and task actions could happen at the same time, the following protocol was applied: if a tutor was making a dialogue move (i.e., typing a message), the state transition accompanying a student action was made after the tutor move was complete, and the student move was associated with that TutorMove action.  Another important consideration for this rep-resentation was how to segment the task stream into discrete actions. Through empirical investi-gation the timeout threshold of 1.5 seconds was selected as a balance between large numbers of successive task events or very few, most of which overlapped with tutor turns.  There were three additional states in the MDP: the Initial state and two final states, FinalHigh and FinalLow, occurring only at the end of a dia-logue and providing rewards of +100 and ?100, respectively. A median split on student learning gains was used to assign each dialogue to either the FinalHigh state or FinalLow state. 4 Feature Selection While retaining all six features would allow for a rich state representation, it would also lead to 
340
issues with sparsity (Singh et al 2002). In fact, nearly 90% of states averaged less than one visit per dialogue when using all six features, leading to inadequate coverage of the state space on which to build reliable MDP policies. This sec-tion compares two methods used to select fea-tures from among the six available. The first approach is based on the Expected Cumulative Reward (ECR) in the initial state, a metric previously used to evaluate state represen-tations for a tutorial dialogue system using RL (Chi et al 2011; Tetreault and  Litman 2008). A higher initial-state ECR indicates a higher proba-bility of achieving a favorable outcome when following a reward-maximizing policy. Maxim-izing ECR has also been the focus of other fea-ture selection approaches for RL (Misu and Kashioka 2012,  Li et al 2009). While initial-state ECR provides a measure of the likelihood of a favorable outcome, it does not address how well a particular state representation captures key decision points. That is, it does not directly represent the extent to which each deci-sion along the path to a successful outcome con-tributed to that outcome, or whether the second-best decision in a particular state would have been equally useful. In order to measure this dif-ference, we introduce the Separation Ratio (SR), which represents how much better a particular policy is compared to its alternatives. SR for a state is calculated by taking the absolute differ-ence between the estimated values of two actions in that state and dividing by the mean of the two values. SR for a policy is the mean of the SRs across all states.  An SR near zero for a state indicates that the decision to take one action over another in that state is likely to have little effect on the final out-come of the dialogue. On the other hand, a high SR indicates a crucial decision point, where tak-ing an off-policy action leads to a much lower probability of a successful outcome. The intui-tion behind this metric is that a state representa-tion that supports policies with high SR high-lights features that are useful in executing an ef-fective turn-taking policy, while a state represen-tation that produces policies with low SR fails to capture this information. Using these two metrics, we evaluated the util-ity of each of the six features. Starting with two empty state representations, one for each metric, a greedy algorithm added one feature at a time to each. That is, at each step for each metric, the feature was added that led to the highest value on the metric when combined with the features al-
ready chosen. For each of the two metrics, we built a state representation and used it as the ba-sis for an MDP. This MDP was then trained with policy iteration (Sutton and Barto 1998), and the two state representations that led to the highest value on each metric were carried over to the next iteration. The goal here is to evaluate the relative utility of each feature, so we continued adding features until they were exhausted, lead-ing to a full ordering of features for each condi-tion (Table 2).   Iteration Initial-State ECR Feature Ordering Mean SR  Feature Ordering 1 Last Action Number of Tutor Moves 2 Task Trajectory Edit Distance 3 Current Action Last Action 4 Elapsed Idle Time Current Action 5 Number of Tutor Moves Elapsed Idle Time 6 Edit Distance Task Trajectory  Table 2. Feature selection using Expected Cumu-lative Reward (ECR) and Separation Ratio (SR) Given the orderings in Table 2, the next step in building a RL system is to decide which iteration of the feature spaces to use. That is, how does a system designer determine when to stop adding features? Previous work (Chi et al 2011; Tetreault and Litman 2008) viewed an absolute increase in the value of initial-state ECR as a signal for the quality of a newly added feature. So, one could say that feature addition should stop if initial-state ECR does not increase be-tween iterations. In the current analysis, howev-er, this would result in termination at the second iteration for the mean SR ordering and termina-tion at the first iteration for the initial-state ECR ordering. These undesirably early terminations most likely occur because the first features se-lected in both orderings represent tutor actions: a tutor can always choose to make a move, thus setting the Last Action feature to TUTORDIAL, and a tutor has direct control over the value of Number of Tutor Moves. This control of features leads to deterministic control of state if the con-text provided by student-driven features is ab-sent. This can allow a policy to remain in the state that maximizes the transition probability to the end state, thus increasing ECR for all states due to deterministic transitions. Therefore, a dif-ferent type of stopping criterion is required. 
341
A stopping criterion must balance two com-peting goals. On the one hand, the size of the state space must be limited to avoid issues with sparsity, as state-action pairs that are not well explored during training might not be assigned values proportional to their expected rewards in a deployed system. On the other hand, a feature space that is too small may not sufficiently repre-sent the possible states of the world, and might fail to capture the criteria most relevant to mak-ing decisions. These competing goals of com-pactness and descriptive power must both be considered when choosing an appropriate feature space for a RL model.  In an attempt to balance these goals, we pro-pose a stopping criterion based on the ratio of states that are sparse states. A sparse state is de-fined as any state that occurs less than once per dialogue on average. A sharp increase in sparse states was observed between the third and fourth iterations for both metrics (15% to 56% for ECR and 26% to 47% for SR), so feature addition stopped at the third iteration. This resulted in only one of the three selected features being shared among the two conditions: the Last Action made by either person (Table 2). In addition, both feature sets include a feature related to the task progress of the student: Task Trajectory for ECR and Edit Distance for SR. The next section reports on an experiment to evaluate these two feature spaces. 5 Evaluation A series of simulated dialogues was used to evaluate the two resulting feature spaces via the policies derived using them. These simulations were based on five-fold cross-validation, as in prior work (Henderson et al 2008), with policies trained on four of the five folds and simulated users learned from the remaining fold. As noted above, the rewards in the MDP were based on student learning gain, but learning gain (like user satisfaction in other dialogue domains) is not directly observable during the dialogues. However, we found that students in the high learning gain group had fewer non-zero task ac-tions (actions that changed the edit distance to the final task solution) than students in the low learning gain group (p < 0.05). Therefore, num-ber of non-zero task actions is used as a measure of dialogue success, with lower numbers being better. We derived the average change in edit distance on each state transition from the testing folds, and defined that a simulated dialogue 
would end when the edit distance reached zero (i.e., the student arrived at the correct solution).  Table 3 shows the results of running 5,000 simulations in each fold for both the learned pol-icy and for an anti-policy where each decision was reversed. The anti-policy is included to pro-vide a point of comparison for the policies learned in each feature space, and offers insight into the quality of the learned policies, similar to the inverse policies learned in prior work (Chi et al 2011). The table shows that the learned poli-cies in the ECR feature space had slightly better results overall (lower number of non-zero task actions), while the SR feature space had larger separation between the learned policies and anti-policies. These results suggest that feature selec-tion based on SR was able to identify important decision criteria with only a minor decrease in reward compared to ECR.    Feature Space Policy Average non-zero task action count ECR Learned policy 43.2 Anti-policy 49.6 SR Learned policy 47.3 Anti-policy 97.4  Table 3. Results of simulated dialogues (lower non-zero task action count is better) 6 Conclusion Modeling unrestricted turn taking within an RL framework, particularly for task-oriented dia-logue with both a dialogue and a parallel task stream, presents numerous challenges. This pa-per has presented a novel representation of such dialogue with a tutoring domain, and has pre-sented and evaluated a feature selection method based on a new Separation Ratio metric, which can inform the development of turn-taking poli-cies in dialogue systems. Future work includes a more fine-grained analysis of the timing of dia-logue moves as well as an evaluation of these results in a deployed system.  Acknowledgements This work is supported in part by the National Science Foundation through Grants DRL-1007962 and CNS-1042468. Any opinions, findings, conclusions, or recommendations expressed in this report are those of the participants, and do not necessarily represent the official views, opinions, or policy of the National Science Foundation.   
342
References Chi, M., VanLehn, K., Litman, D., and Jordan, P. (2011). An Evaluation of Pedagogical Tutorial Tactics for a Natural Language Tutoring System: a Reinforcement Learning Approach. International Journal of Artificial Intelligence in Educa-tion, 21(1), 83?113. D?Mello, S.K., Olney, A., and Person, N. (2010). Mining Collaborative Patterns in Tutorial Dia-logues. Journal of Educational Data Mining, 2(1), 1?37. English, M.S. and Heeman, P.A. (2005). Learning Mixed Initiative Dialog Strategies By Using Rein-forcement Learning On Both Conversants. In Pro-ceedings of the Conference on Human Language Technology and Empirical Methods in Natural Language Processing, 1011?1018. Forbes-Riley, K. and Litman, D.J. (2012). Adapting to Multiple Affective States in Spoken Dialogue. In Proceedings of the 13th Annual SIGDIAL Meeting on Discourse and Dialogue, 217?226. Henderson, J., Lemon, O., and Georgila, K. (2008). Hybrid Reinforcement/Supervised Learning of Dialogue Policies from Fixed Data Sets. Computa-tional Linguistics, 34(4), 487?511. Li, L., Williams, J. D., and Balakrishnan, S. (2009). Reinforcement Learning for Dialog Management Using Least-Squares Policy Iteration and Fast Fea-ture Selection. In Proceedings of the Conference of the International Speech Communication Associa-tion. 2475?2478. Misu, T., and Kashioka, H. (2012). Simultaneous Fea-ture Selection and Parameter Optimization for Training of Dialog Policy by Reinforcement Learn-ing. In Proceedings of the IEEE Workshop on Spo-ken Language Technology, 1?6. Raux, A. and Eskenazi, M. (2012). Optimizing the Turn-Taking Behavior of Task-Oriented Spoken Dialog Systems. Transactions on Speech and Lan-guage Processing, 9(1), 1?23. 
Selfridge, E.O., Arizmendi, I., Heeman, P.A., and Williams, J.D. (2012). Integrating Incremental Speech Recognition and POMDP-based Dialogue Systems. In Proceedings of the 13th Annual SIG-DIAL Meeting on Discourse and Dialogue, 275?279. Shute, V.J. (2008). Focus on Formative Feedback. Review of Educational Research, 78(1), 153?189. Singh, S., Litman, D., Kearns, M., and Walker, M. (2002). Optimizing Dialogue Management with Reinforcement Learning: Experiments with the NJFun System. Journal of Artificial Intelligence Research, 16, 105?133. Sutton, R. and Barto, A. (1998). Reinforcement Learning. MIT Press, Cambridge, MA, 1998. Tetreault, J.R. and Litman, D.J. (2008). A Reinforce-ment Learning Approach to Evaluating State Rep-resentations in Spoken Dialogue Systems. Speech Communication, 50(8), 683?696. Williams, J.D. and Young, S. (2007). Partially Ob-servable Markov Decision Processes for Spoken Dialog Systems. Computer Speech & Language, 21(2), 393?422.   Appendix A. Corpus excerpt 1. Student begins declaring a String variable. 2. Student starts typing a message. 3. Student message: Could I type in String The Adventure Quest; ? or would I need to put in quotes or something? 4. Student resumes working on task. 5. Tutor starts typing a message. 6. Tutor message: TheAdventureQuest is fine 7. Student declares variable called The Adven-ture Quest (Incorrect Java syntax) 8. Tutor starts typing a message. 9. Student catches mistake and renames variable to TheAdventureQuest 10. Tutor message: Can't have spaces :) 11. Tutor starts typing a message 12. Tutor message: Good job   
Appendix B. Dialogue interface 343
