Coling 2010: Poster Volume, pages 108?116,
Beijing, August 2010
Benchmarking of Statistical Dependency Parsers for French
Marie Candito!, Joakim Nivre!, Pascal Denis! and Enrique Henestroza Anguiano!
! Alpage (Universit? Paris 7/INRIA)
! Uppsala University, Department of Linguistics and Philology
marie.candito@linguist.jussieu.fr {pascal.denis, henestro}@inria.fr joakim.nivre@ling?l.uu.se
Abstract
We compare the performance of three
statistical parsing architectures on the
problem of deriving typed dependency
structures for French. The architectures
are based on PCFGs with latent vari-
ables, graph-based dependency parsing
and transition-based dependency parsing,
respectively. We also study the in?u-
ence of three types of lexical informa-
tion: lemmas, morphological features,
and word clusters. The results show that
all three systems achieve competitive per-
formance, with a best labeled attachment
score over 88%. All three parsers bene?t
from the use of automatically derived lem-
mas, while morphological features seem
to be less important. Word clusters have a
positive effect primarily on the latent vari-
able parser.
1 Introduction
In this paper, we compare three statistical parsers
that produce typed dependencies for French. A
syntactic analysis in terms of typed grammatical
relations, whether encoded as functional annota-
tions in syntagmatic trees or in labeled depen-
dency trees, appears to be useful for many NLP
tasks including question answering, information
extraction, and lexical acquisition tasks like collo-
cation extraction.
This usefulness holds particularly for French,
a language for which bare syntagmatic trees
are often syntactically underspeci?ed because
of a rather free order of post-verbal comple-
ments/adjuncts and the possibility of subject in-
version. Thus, the annotation scheme of the
French Treebank (Abeill? and Barrier, 2004)
makes use of ?at syntagmatic trees without VP
nodes, with no structural distinction between
complements, adjuncts or post-verbal subjects,
but with additional functional annotations on de-
pendents of verbs.
Parsing is commonly enhanced by using more
abstract lexical information, in the form of mor-
phological features (Tsarfaty, 2006), lemmas
(Seddah et al, 2010), or various forms of clusters
(see (Candito and Seddah, 2010) for references).
In this paper, we explore the integration of mor-
phological features, lemmas, and linear context
clusters.
Typed dependencies can be derived using many
different parsing architectures. As far as statistical
approaches are concerned, the dominant paradigm
for English has been to use constituency-based
parsers, the output of which can be converted
to typed dependencies using well-proven conver-
sion procedures, as in the Stanford parser (Klein
and Manning, 2003). In recent years, it has
also become popular to use statistical dependency
parsers, which are trained directly on labeled de-
pendency trees and output such trees directly, such
as MSTParser (McDonald, 2006) and MaltParser
(Nivre et al, 2006). Dependency parsing has been
applied to a fairly broad range of languages, espe-
cially in the CoNLL shared tasks in 2006 and 2007
(Buchholz and Marsi, 2006; Nivre et al, 2007).
We present a comparison of three statistical
parsing architectures that output typed dependen-
cies for French: one constituency-based architec-
ture featuring the Berkeley parser (Petrov et al,
2006), and two dependency-based systems using
radically different parsing methods, MSTParser
(McDonald et al, 2006) and MaltParser (Nivre et
al., 2006). These three systems are compared both
in terms of parsing accuracy and parsing times, in
realistic settings that only use predicted informa-
tion. By using freely available software packages
that implement language-independent approaches
108
and applying them to a language different from
English, we also hope to shed some light on the
capacity of different methods to cope with the
challenges posed by different languages.
Comparative evaluation of constituency-based
and dependency-based parsers with respect to la-
beled accuracy is rare, despite the fact that parser
evaluation on typed dependencies has been ad-
vocated for a long time (Lin, 1995; Carroll et
al., 1998). Early work on statistical dependency
parsing often compared constituency-based and
dependency-based methods with respect to their
unlabeled accuracy (Yamada and Matsumoto,
2003), but comparison of different approaches
with respect to labeled accuracy is more recent.
Cer et al (2010) present a thorough analysis of
the best trade-off between speed and accuracy in
deriving Stanford typed dependencies for English
(de Marneffe et al, 2006), comparing a number of
constituency-based and dependency-based parsers
on data from the Wall Street Journal. They con-
clude that the highest accuracy is obtained using
constituency-based parsers, although some of the
dependency-based parsers are more ef?cient.
For German, the 2008 ACL workshop on pars-
ing German (K?bler, 2008) featured a shared task
with two different tracks, one for constituency-
based parsing and one for dependency-based pars-
ing. Both tracks had their own evaluation metrics,
but the accuracy with which parsers identi?ed
subjects, direct objects and indirect objects was
compared across the two tracks, and the results
in this case showed an advantage for dependency-
based parsing.
In this paper, we contribute results for a
third language, French, by benchmarking both
constituency-based and dependency-based meth-
ods for deriving typed dependencies. In addi-
tion, we investigate the usefulness of morphologi-
cal features, lemmas and word clusters for each of
the different parsing architectures. The rest of the
paper is structured as follows. Section 2 describes
the French Treebank, and Section 3 describes the
three parsing systems. Section 4 presents the ex-
perimental evaluation, and Section 5 contains a
comparative error analysis of the three systems.
Section 6 concludes with suggestions for future
research.
2 Treebanks
For training and testing the statistical parsers, we
use treebanks that are automatically converted
from the French Treebank (Abeill? and Barrier,
2004) (hereafter FTB), a constituency-based tree-
bank made up of 12, 531 sentences from the Le
Monde newspaper. Each sentence is annotated
with a constituent structure and words bear the
following features: gender, number, mood, tense,
person, de?niteness, wh-feature, and clitic case.
Nodes representing dependents of a verb are la-
beled with one of 8 grammatical functions.1
We use two treebanks automatically obtained
from FTB, both described in Candito et al
(2010). FTB-UC is a modi?ed version of the
original constituency-based treebank, where the
rich morphological annotation has been mapped
to a simple tagset of 28 part-of-speech tags, and
where compounds with regular syntax are bro-
ken down into phrases containing several simple
words while remaining sequences annotated as
compounds in FTB are merged into a single token.
Function labels are appended to syntactic category
symbols and are either used or ignored, depending
on the task.
FTB-UC-DEP is a dependency treebank de-
rived from FTB-UC using the classic technique of
head propagation rules, ?rst proposed for English
by Magerman (1995). Function labels that are
present in the original treebank serve to label the
corresponding dependencies. The remaining un-
labeled dependencies are labeled using heuristics
(for dependents of non-verbal heads). With this
conversion technique, output dependency trees are
necessarily projective, and extracted dependen-
cies are necessarily local to a phrase, which means
that the automatically converted trees can be re-
garded as pseudo-projective approximations to the
correct dependency trees (Kahane et al, 1998).
Candito et al (2010) evaluated the converted trees
for 120 sentences, and report a 98% labeled at-
tachment score when comparing the automatically
converted dependency trees to the manually cor-
rected ones.
1These are SUJ (subject), OBJ (object), A-OBJ/DE-OBJ
(indirect object with preposition ? / de), P-OBJ (indirect
object with another preposition / locatives), MOD (modi?er),
ATS/ATO (subject/object predicative complement).
109
SNP-SUJ
DET
une
NC
lettre
VN
V
avait
VPP
?t?
VPP
envoy?e
NP-MOD
DET
la
NC
semaine
ADJ
derni?re
PP-A_OBJ
P+D
aux
NP
NC
salari?s
une lettre avait ?t? envoy?e la semaine derni?re aux salari?s
de
t
suj
au
x-t
ps
au
x-
pa
ss mod
de
t mod
a_obj
obj
Figure 1: An example of constituency tree of the FTB-UC (left), and the corresponding dependency tree
(right) for A letter had been sent the week before to the employees.
Figure 1 shows two parallel trees from FTB-UC
and FTB-UC-DEP. In all reported experiments in
this paper, we use the usual split of FTB-UC: ?rst
10% as test set, next 10% as dev set, and the re-
maining sentences as training set.
3 Parsers
Although all three parsers compared are statis-
tical, they are based on fairly different parsing
methodologies. The Berkeley parser (Petrov et
al., 2006) is a latent-variable PCFG parser, MST-
Parser (McDonald et al, 2006) is a graph-based
dependency parser, and MaltParser (Nivre et al,
2006) is a transition-based dependency parser.
The choice to include two different dependency
parsers but only one constituency-based parser is
motivated by the study of Seddah et al (2009),
where a number of constituency-based statisti-
cal parsers were evaluated on French, including
Dan Bikel?s implementation of the Collins parser
(Bikel, 2002) and the Charniak parser (Charniak,
2000). The evaluation showed that the Berke-
ley parser had signi?cantly better performance for
French than the other parsers, whether measured
using a parseval-style labeled bracketing F-score
or a CoNLL-style unlabeled attachment score.
Contrary to most of the other parsers in that study,
the Berkeley parser has the advantage of a strict
separation of parsing model and linguistic con-
straints: linguistic information is encoded in the
treebank only, except for a language-dependent
suf?x list used for handling unknown words.
In this study, we compare the Berkeley parser
to MSTParser and MaltParser, which have the
same separation of parsing model and linguistic
representation, but which are trained directly on
labeled dependency trees. The two dependency
parsers use radically different parsing approaches
but have achieved very similar performance for a
wide range of languages (McDonald and Nivre,
2007). We describe below the three architectures
in more detail.2
3.1 The Berkeley Parser
The Berkeley parser is a freely available imple-
mentation of the statistical training and parsing
algorithms described in (Petrov et al, 2006) and
(Petrov and Klein, 2007). It exploits the fact that
PCFG learning can be improved by splitting sym-
bols according to structural and/or lexical proper-
ties (Klein and Manning, 2003). Following Mat-
suzaki et al (2005), the Berkeley learning algo-
rithm uses EM to estimate probabilities on sym-
bols that are automatically augmented with la-
tent annotations, a process that can be viewed
as symbol splitting. Petrov et al (2006) pro-
posed to score the splits in order to retain only the
most bene?cial ones, and keep the grammar size
manageable: the splits that induce the smallest
losses in the likelihood of the treebank are merged
back. The algorithm starts with a very general
treebank-induced binarized PCFG, with order h
horizontal markovisation. created, where at each
level a symbol appears without track of its orig-
inal siblings. Then the Berkeley algorithm per-
forms split/merge/smooth cycles that iteratively
re?ne the binarized grammar: it adds two latent
annotations on each symbol, learns probabilities
for the re?ned grammar, merges back 50% of the
splits, and smoothes the ?nal probabilities to pre-
vent over?tting. All our experiments are run us-
ing BerkeleyParser 1.0,3 modi?ed for handling
2For replicability, models, preprocessing tools and ex-
perimental settings are available at http://alpage.
inria.fr/statgram/frdep.html.
3http://www.eecs.berkeley.edu/
\~petrov/berkeleyParser
110
French unknown words by Crabb? and Candito
(2008), with otherwise default settings (order 0
horizontal markovisation, order 1 vertical marko-
visation, 5 split/merge cycles).
The Berkeley parser could in principle be
trained on functionally annotated phrase-structure
trees (as shown in the left half of ?gure 1), but
Crabb? and Candito (2008) have shown that this
leads to very low performance, because the split-
ting of symbols according to grammatical func-
tions renders the data too sparse. Therefore, the
Berkeley parser was trained on FTB-UC without
functional annotation. Labeled dependency trees
were then derived from the phrase-structure trees
output by the parser in two steps: (1) function la-
bels are assigned to phrase structure nodes that
have functional annotation in the FTB scheme;
and (2) dependency trees are produced using the
same procedure used to produce the pseudo-gold
dependency treebank from the FTB (cf. Section 2).
The functional labeling relies on the Maximum
Entropy labeler described in Candito et al (2010),
which encodes the problem of functional label-
ing as a multiclass classi?cation problem. Specif-
ically, each class is of the eight grammatical func-
tions used in FTB, and each head-dependent pair
is treated as an independent event. The feature
set used in the labeler attempt to capture bilexi-
cal dependencies between the head and the depen-
dent (using stemmed word forms, parts of speech,
etc.) as well as more global sentence properties
like mood, voice and inversion.
3.2 MSTParser
MSTParser is a freely available implementation
of the parsing models described in McDonald
(2006). These models are often described as
graph-based because they reduce the problem
of parsing a sentence to the problem of ?nding
a directed maximum spanning tree in a dense
graph representation of the sentence. Graph-based
parsers typically use global training algorithms,
where the goal is to learn to score correct trees
higher than incorrect trees. At parsing time a
global search is run to ?nd the highest scoring
dependency tree. However, unrestricted global
inference for graph-based dependency parsing
is NP-hard, and graph-based parsers like MST-
Parser therefore limit the scope of their features
to a small number of adjacent arcs (usually two)
and/or resort to approximate inference (McDon-
ald and Pereira, 2006). For our experiments, we
use MSTParser 0.4.3b4 with 1-best projective de-
coding, using the algorithm of Eisner (1996), and
second order features. The labeling of dependen-
cies is performed as a separate sequence classi?-
cation step, following McDonald et al (2006).
To provide part-of-speech tags to MSTParser,
we use the MElt tagger (Denis and Sagot, 2009),
a Maximum Entropy Markov Model tagger en-
riched with information from a large-scale dictio-
nary.5 The tagger was trained on the training set
to provide POS tags for the dev and test sets, and
we used 10-way jackkni?ng to generate tags for
the training set.
3.3 MaltParser
MaltParser6 is a freely available implementation
of the parsing models described in (Nivre, 2006)
and (Nivre, 2008). These models are often char-
acterized as transition-based, because they reduce
the problem of parsing a sentence to the prob-
lem of ?nding an optimal path through an abstract
transition system, or state machine. This is some-
times equated with shift-reduce parsing, but in
fact includes a much broader range of transition
systems (Nivre, 2008). Transition-based parsers
learn models that predict the next state given the
current state of the system, including features over
the history of parsing decisions and the input sen-
tence. At parsing time, the parser starts in an ini-
tial state and greedily moves to subsequent states
? based on the predictions of the model ? until a
terminal state is reached. The greedy, determinis-
tic parsing strategy results in highly ef?cient pars-
ing, with run-times often linear in sentence length,
and also facilitates the use of arbitrary non-local
features, since the partially built dependency tree
is ?xed in any given state. However, greedy in-
ference can also lead to error propagation if early
predictions place the parser in incorrect states. For
the experiments in this paper, we use MaltParser
4http://mstparser.sourceforge.net
5Denis and Sagot (2009) report a tagging accuracy of
97.7% (90.1% on unknown words) on the FTB-UC test set.
6http://www.maltparser.org
111
1.3.1 with the arc-eager algorithm (Nivre, 2008)
and use linear classi?ers from the LIBLINEAR
package (Fan et al, 2008) to predict the next state
transitions. As for MST, we used the MElt tagger
to provide input part-of-speech tags to the parser.
4 Experiments
This section presents the parsing experiments that
were carried out in order to assess the state of the
art in labeled dependency parsing for French and
at the same time investigate the impact of different
types of lexical information on parsing accuracy.
We present the features given to the parsers, dis-
cuss how they were extracted/computed and inte-
grated within each parsing architecture, and then
summarize the performance scores for the differ-
ent parsers and feature con?gurations.
4.1 Experimental Space
Our experiments focus on three types of lexical
features that are used either in addition to or as
substitutes for word forms: morphological fea-
tures, lemmas, and word clusters. In the case
of MaltParser and MSTParser, these features are
used in conjunction with POS tags. Motivations
for these features are rooted in the fact that French
has a rather rich in?ectional morphology.
The intuition behind using morphological fea-
tures like tense, mood, gender, number, and per-
son is that some of these are likely to provide ad-
ditional cues for syntactic attachment or function
type. This is especially true given that the 29 tags
used by the MElt tagger are rather coarse-grained.
The use of lemmas and word clusters, on the
other hand, is motivated by data sparseness con-
siderations: these provide various degrees of gen-
eralization over word forms. As suggested by Koo
et al (2008), the use of word clusters may also re-
duce the need for annotated data.
All our features are automatically produced:
no features except word forms originate from the
treebank. Our aim was to assess the performance
currently available for French in a realistic setting.
Lemmas Lemmatized forms are extracted us-
ing Lefff (Sagot, 2010), a large-coverage morpho-
syntactic lexicon for French, and a set of heuristics
for unknown words. More speci?cally, Lefff is
queried for each (word, pos), where pos is the
tag predicted by the MElt tagger. If the pair is
found, we use the longest lemma associated with
it in Lefff. Otherwise, we rely on a set of simple
stemming heuristics using the form and the pre-
dicted tag to produce the lemma. We use the form
itself for all other remaining cases.7
Morphological Features Morphological fea-
tures were extracted in a way similar to lemmas,
again by querying Lefff and relying on heuristics
for out-of-dictionary words. Here are the main
morphological attributes that were extracted from
the lexicon: mood and tense for verbs; person
for verbs and pronouns; number and gender for
nouns, past participles, adjectives and pronouns;
whether an adverb is negative; whether an adjec-
tive, pronoun or determiner is cardinal, ordinal,
de?nite, possessive or relative. Our goal was to
predict all attributes found in FTB that are recov-
erable from the word form alone.
Word Form Clusters Koo et al (2008) have
proposed to use unsupervised word clusters as
features in MSTParser, for parsing English and
Czech. Candito and Crabb? (2009) showed that,
for parsing French with the Berkeley parser, us-
ing the same kind of clusters as substitutes for
word forms improves performance. We now ex-
tend their work by comparing the impact of such
clusters on two additional parsers.
We use the word clusters computed by Can-
dito and Crabb? (2009) using Percy Liang?s im-
plementation8 of the Brown unsupervised cluster-
ing algorithm (Brown et al, 1992). It is a bottom-
up hierarchical clustering algorithm that uses a bi-
gram language model over clusters. The result-
ing cluster ids are bit-strings, and various lev-
els of granularity can be obtained by retaining
only the ?rst x bits. Candito and Crabb? (2009)
used the L?Est R?publicain corpus, a 125 mil-
lion word journalistic corpus.9 To reduce lexi-
7Candito and Seddah (2010) report the following cover-
age for the Lefff : around 96% of the tokens, and 80.1% of
the token types are present in the Lefff (leaving out punctua-
tion and numeric tokens, and ignoring case differences).
8http://www.eecs.berkeley.edu/~pliang/
software
9http://www.cnrtl.fr/corpus/
estrepublicain
112
cal data sparseness caused by in?ection, they ran
a lexicon-based stemming process on the corpus
that removes in?ection marks without adding or
removing lexical ambiguity. The Brown algo-
rithm was then used to compute 1000 clusters of
stemmed forms, limited to forms that appeared at
least 20 times.
We tested the use of clusters with different val-
ues for two parameters: nbbits = the cluster pre-
?x length in bits, to test varying granularities, and
minocc = the minimum number of occurrences in
the L?Est R?publicain corpus for a form to be re-
placed by a cluster or for a cluster feature to be
used for that form.
4.2 Parser-Specific Configurations
Since the three parsers are based on different ma-
chine learning algorithms and parsing algorithms
(with different memory requirements and parsing
times), we cannot integrate the different features
described above in exactly the same way. For the
Berkeley parser we use the setup of Candito and
Seddah (2010), where additional information is
encoded within symbols that are used as substi-
tutes for word forms. For MaltParser and MST-
Parser, which are based on discriminative models
that permit the inclusion of interdependent fea-
tures, additional information may be used either
in addition to or as substitutes for word forms.
Below we summarize the con?gurations that have
been explored for each parser:
? Berkeley:
1. Morphological features: N/A.
2. Lemmas: Concatenated with POS tags
and substituted for word forms.
3. Clusters: Concatenated with morpho-
logical suf?xes and substituted for word
forms; grid search for optimal values of
nbbits and minocc.
? MaltParser and MSTParser:
1. Morphological features: Added as
features.
2. Lemmas: Substituted for word forms
or added as features.
3. Clusters: Substituted for word forms or
added as features; grid search for opti-
mal values of nbbits and minocc.
4.3 Results
Table 1 summarizes the experimental results. For
each parser we give results on the development
set for the baseline (no additional features), the
best con?guration for each individual feature type,
and the best con?guration for any allowed combi-
nation of the three features types. For the ?nal
test set, we only evaluate the baseline and the best
combination of features. Scores on the test set
were compared using a ?2-test to assess statisti-
cal signi?cance: unless speci?ed, all differences
therein were signi?cant at p ? 0.01.
The MSTParser system achieves the best la-
beled accuracy on both the development set and
the test set. When adding lemmas, the best con-
?guration is to use them as substitutes for word
forms, which slightly improves the UAS results.
For the clusters, their use as substitutes for word
forms tends to degrade results, whereas using
them as features alone has almost no impact. This
means that we could not replicate the positive ef-
fect10 reported by Koo et al (2008) for English
and Czech. However, the best combined con-
?guration is obtained using lemmas instead of
words, a reduced set of morphological features,11
and clusters as features, with minocc=50, 000 and
nbbits=10.
MaltParser has the second best labeled accu-
racy on both the development set and the test set,
although the difference with Berkeley is not sig-
ni?cant on the latter. MaltParser has the lowest
unlabeled accuracy of all three parsers on both
datasets. As opposed to MSTParser, all three fea-
ture types work best for MaltParser when used in
addition to word forms, although the improvement
is statistically signi?cant only for lemmas and
clusters. Again, the best model uses all three types
of features, with cluster features minocc=600 and
nbbits=7. MaltParser shows the smallest discrep-
ancy from unlabeled to labeled scores. This might
be because it is the only architecture where label-
ing is directly done as part of parsing.
10Note that the two experiments cannot be directly com-
pared. Koo et al (2008) use their own implementation of an
MST parser, which includes extra second-order features (e.g.
grand-parent features on top of sibling features).
11As MSTParser training is memory-intensive, we re-
moved the features containing information already encoded
part-of-speech tags.
113
Development Set Test Set
Baseline Morpho Lemma Cluster Best Baseline Best
Parser LAS UAS LAS UAS LAS UAS LAS UAS LAS UAS LAS UAS LAS UAS
Berkeley 85.1 89.3 ? ? 85.9 90.0 86.5 90.8 86.5 90.8 85.6 89.6 86.8 91.0
MSTParser 87.2 90.0 87.2 90.2 87.2 90.1 87.2 90.1 87.5 90.3 87.6 90.3 88.2 90.9
MaltParser 86.2 89.0 86.3 89.0 86.6 89.2 86.5 89.2 86.9 89.4 86.7 89.3 87.3 89.7
Table 1: Experimental results for the three parsing systems. LAS=labeled accuracy, UAS=unlabeled accuracy, for sentences
of any length, ignoring punctuation tokens. Morpho/Lemma/Cluster=best con?guration when using morphological features
only (resp. lemmas only, clusters only), Best=best con?guration using any combination of these.
For Berkeley, the lemmas improve the results
over the baseline, and its performance reaches that
of MSTParser for unlabeled accuracy (although
the difference between the two parsers is not sig-
ni?cant on the test set). The best setting is ob-
tained with clusters instead of word forms, using
the full bit strings. It also gives the best unlabeled
accuracy of all three systems on both the devel-
opment set and the test set. For the more impor-
tant labeled accuracy, the point-wise labeler used
is not effective enough.
Overall, MSTParser has the highest labeled ac-
curacy and Berkeley the highest unlabeled ac-
curacy. However, results for all three systems
on the test set are roughly within one percent-
age point for both labeled and unlabeled ac-
curacy, which means that we do not ?nd the
same discrepancy between constituency-based
and dependency-based parser that was reported
for English by Cer et al (2010).
Table 2 gives parsing times for the best con?g-
uration of each parsing architecture. MaltParser
runs approximately 9 times faster than the Berke-
ley system, and 10 times faster than MSTParser.
The difference in ef?ciency is mainly due to the
fact that MaltParser uses a linear-time parsing al-
gorithm, while the other two parsers have cubic
time complexity. Given the rather small differ-
ence in labeled accuracy, MaltParser seems to be
a good choice for processing very large corpora.
5 Error Analysis
We provide a brief analysis of the errors made by
the best performing models for Berkeley, MST-
Parser and MaltParser on the development set, fo-
cusing on labeled and unlabeled attachment for
nouns, prepositions and verbs. For nouns, Berke-
Bky Malt MST
Tagging _ 0:27 0:27
Parsing 12:19 0:58 (0:18) 14:12 (12:44)
Func. Lab. 0:23 _ _
Dep. Conv. 0:4 _ _
Total 12:46 1:25 14:39
Table 2: Parsing times (min:sec) for the dev set, for the
three architectures, on an imac 2.66GHz. The ?gures within
brackets show the pure parsing time without the model load-
ing time, when available.
ley has the best unlabeled attachment, followed by
MSTParser and then MaltParser, while for labeled
attachment Berkeley and MSTParser are on a par
with MaltParser a bit behind. For prepositions,
MSTParser is by far the best for both labeled and
unlabeled attachment, with Berkeley and Malt-
Parser performing equally well on unlabeled at-
tachment and MaltParser performing better than
Berkeley on labeled attachment.12 For verbs,
Berkeley has the best performance on both labeled
and unlabeled attachment, with MSTParser and
MaltParser performing about equally well. Al-
though Berkeley has the best unlabeled attach-
ment overall, it also has the worst labeled attach-
ment, and we found that this is largely due to the
functional role labeler having trouble assigning
the correct label when the dependent is a prepo-
sition or a clitic.
For errors in attachment as a function of word
distance, we ?nd that precision and recall on de-
pendencies of length > 2 tend to degrade faster
for MaltParser than for MSTParser and Berkeley,
12In the dev set, for MSTParser, 29% of the tokens that
do not receive the correct governor are prepositions (883 out
of 3051 errors), while these represent 34% for Berkeley (992
out of 2914), and 30% for MaltParser (1016 out of 3340).
114
with Berkeley being the most robust for depen-
dencies of length > 6. In addition, Berkeley is
best at ?nding the correct root of sentences, while
MaltParser often predicts more than one root for a
given sentence. The behavior of MSTParser and
MaltParser in this respect is consistent with the re-
sults of McDonald and Nivre (2007).
6 Conclusion
We have evaluated three statistical parsing ar-
chitectures for deriving typed dependencies for
French. The best result obtained is a labeled at-
tachment score of 88.2%, which is roughly on a
par with the best performance reported by Cer et
al. (2010) for parsing English to Stanford depen-
dencies. Note two important differences between
their results and ours: First, the Stanford depen-
dencies are in a way deeper than the surface de-
pendencies tested in our work. Secondly, we ?nd
that for French there is no consistent trend fa-
voring either constituency-based or dependency-
based methods, since they achieve comparable re-
sults both for labeled and unlabeled dependencies.
Indeed, the differences between parsing archi-
tectures are generally small. The best perfor-
mance is achieved using MSTParser, enhanced
with predicted part-of-speech tags, lemmas, mor-
phological features, and unsupervised clusters of
word forms. MaltParser achieves slightly lower
labeled accuracy, but is probably the best option
if speed is crucial. The Berkeley parser has high
accuracy for unlabeled dependencies, but the cur-
rent labeling method does not achieve a compara-
bly high labeled accuracy.
Examining the use of lexical features, we ?nd
that predicted lemmas are useful in all three ar-
chitectures, while morphological features have a
marginal effect on the two dependency parsers
(they are not used by the Berkeley parser). Unsu-
pervised word clusters, ?nally, give a signi?cant
improvement for the Berkeley parser, but have a
rather small effect for the dependency parsers.
Other results for statistical dependency pars-
ing of French include the pilot study of Candito
et al (2010), and the work ofSchluter and van
Genabith (2009), which resulted in an LFG sta-
tistical French parser. However, the latter?s re-
sults are obtained on a modi?ed subset of the FTB,
and are expressed in terms of F-score on LFG f-
structure features, which are not comparable to
our attachment scores. There also exist a num-
ber of grammar-based parsers, evaluated on gold
test sets annotated with chunks and dependen-
cies (Paroubek et al, 2005; de la Clergerie et al,
2008). Their annotation scheme is different from
that of the FTB, but we plan to evaluate the statis-
tical parsers on the same data in order to compare
the performance of grammar-based and statistical
approaches.
Acknowledgments
The ?rst, third and fourth authors? work was sup-
ported by ANR Sequoia (ANR-08-EMER-013).
We are grateful to our anonymous reviewers for
their comments.
References
Abeill?, A. and N. Barrier. 2004. Enriching a french
treebank. In LREC?04.
Bikel, D. M. 2002. Design of a multi-lingual, parallel-
processing statistical parsing engine. In HLT-02.
Brown, P., V. Della Pietra, P. Desouza, J. Lai, and
R. Mercer. 1992. Class-based n-gram models of
natural language. Computational linguistics, 18(4).
Buchholz, S. and E. Marsi. 2006. CoNLL-X shared
task on multilingual dependency parsing. In CoNLL
2006.
Candito, M. and B. Crabb?. 2009. Improving gener-
ative statistical parsing with semi-supervised word
clustering. In IWPT?09.
Candito, M. and D. Seddah. 2010. Parsing word clus-
ters. In NAACL/HLT Workshop SPMRL 2010.
Candito, M., B. Crabb?, and P. Denis. 2010. Statis-
tical french dependency parsing : treebank conver-
sion and ?rst results. In LREC 2010.
Carroll, J., E. Briscoe, and A. San?lippo. 1998. Parser
evaluation: A survey and a new proposal. In LREC
1998.
Cer, D., M.-C. de Marneffe, D. Jurafsky, and C. Man-
ning. 2010. Parsing to stanford dependencies:
Trade-offs between speed and accuracy. In LREC
2010.
Charniak, E. 2000. A maximum entropy inspired
parser. In NAACL 2000.
115
Crabb?, B. and M. Candito. 2008. Exp?riences
d?analyse syntaxique statistique du fran?ais. In
TALN 2008.
de la Clergerie, E. V., C. Ayache, G. de Chalendar,
G. Francopoulo, C. Gardent, and P. Paroubek. 2008.
Large scale production of syntactic annotations for
french. In First International Workshop on Auto-
mated Syntactic Annotations for Interoperable Lan-
guage Resources.
de Marneffe, M.-C., B. MacCartney, and C. D. Man-
ning. 2006. Generating typed dependency parses
from phrase structure parses. In LREC 2006.
Denis, P. and B. Sagot. 2009. Coupling an an-
notated corpus and a morphosyntactic lexicon for
state-of-the-art pos tagging with less human effort.
In PACLIC 2009.
Eisner, J. 1996. Three new probabilistic models for
dependency parsing: An exploration. In COLING
1996.
Fan, R.-E., K.-W. Chang, C.-J. Hsieh, X.-R. Wang, and
C.-J. Lin. 2008. LIBLINEAR: A library for large
linear classi?cation. Journal of Machine Learning
Research, 9.
Kahane, S., A. Nasr, and O. Rambow. 1998.
Pseudo-projectivity: A polynomially parsable non-
projective dependency grammar. In ACL/COLING
1998.
Klein, D. and C. D. Manning. 2003. Accurate unlexi-
calized parsing. In ACL 2003.
Koo, T., X. Carreras, and M. Collins. 2008. Sim-
ple semi-supervised dependency parsing. In ACL-
08:HLT.
K?bler, S. 2008. The PaGe 2008 shared task on pars-
ing german. In ACL-08 Workshop on Parsing Ger-
man.
Lin, D. 1995. A dependency-based method for evalu-
ating broad-coverage parsers. In IJCAI-95.
Magerman, D. M. 1995. Statistical decision-tree mod-
els for parsing. In ACL 1995.
Matsuzaki, T., Y. Miyao, and J. Tsujii. 2005. Proba-
bilistic cfg with latent annotations. In ACL 2005.
McDonald, R. and J. Nivre. 2007. Characterizing
the errors of data-driven dependency parsing mod-
els. In EMNLP-CoNLL 2007.
McDonald, R. and F. Pereira. 2006. Online learning
of approximate dependency parsing algorithms. In
EACL 2006.
McDonald, R., K. Lerman, and F. Pereira. 2006. Mul-
tilingual dependency analysis with a two-stage dis-
criminative parser. In CoNLL 2006.
McDonald, R. 2006. Discriminative Learning and
Spanning Tree Algorithms for Dependency Parsing.
Ph.D. thesis, University of Pennsylvania.
Nivre, J., Johan Hall, and Jens Nilsson. 2006. Malt-
parser: A data-driven parser-generator for depen-
dency parsing. In LREC 2006.
Nivre, J., J. Hall, S. K?bler, R. McDonald, J. Nils-
son, S. Riedel, and D. Yuret. 2007. The CoNLL
2007 shared task on dependency parsing. In CoNLL
Shared Task of EMNLP-CoNLL 2007.
Nivre, J. 2006. Inductive Dependency Parsing.
Springer.
Nivre, J. 2008. Algorithms for deterministic incre-
mental dependency parsing. Computational Lin-
guistics, 34.
Paroubek, P., L.-G. Pouillot, I. Robba, and A. Vilnat.
2005. Easy : Campagne d??valuation des analy-
seurs syntaxiques. In TALN 2005, EASy workshop :
campagne d??valuation des analyseurs syntaxiques.
Petrov, S. and D. Klein. 2007. Improved inference for
unlexicalized parsing. In NAACL-07: HLT.
Petrov, S., L. Barrett, R. Thibaux, and D. Klein. 2006.
Learning accurate, compact, and interpretable tree
annotation. In ACL 2006.
Sagot, B. 2010. The Lefff, a freely available and large-
coverage morphological and syntactic lexicon for
french. In LREC 2010.
Schluter, N. and J. van Genabith. 2009. Dependency
parsing resources for french: Converting acquired
lfg f-structure. In NODALIDA 2009.
Seddah, D., M. Candito, and B. Crabb?. 2009. Cross
parser evaluation and tagset variation: a french tree-
bank study. In IWPT 2009.
Seddah, D., G. Chrupa?a, O. Cetinoglu, J. van Gen-
abith, and M. Candito. 2010. Lemmatization and
statistical lexicalized parsing of morphologically-
rich languages. In NAACL/HLT Workshop SPMRL
2010.
Tsarfaty, R. 2006. Integrated morphological and syn-
tactic disambiguation for modern hebrew. In COL-
ING/ACL 2006 Student Research Workshop.
Yamada, H. and Y. Matsumoto. 2003. Statistical de-
pendency analysis with support vector machines. In
IWPT 2003.
116
Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 1222?1233,
Edinburgh, Scotland, UK, July 27?31, 2011. c?2011 Association for Computational Linguistics
Parse Correction with Specialized Models for Difficult Attachment Types
Enrique Henestroza Anguiano and Marie Candito
Alpage (Universite? Paris Diderot / INRIA)
Paris, France
henestro@inria.fr, marie.candito@linguist.jussieu.fr
Abstract
This paper develops a framework for syntac-
tic dependency parse correction. Dependen-
cies in an input parse tree are revised by se-
lecting, for a given dependent, the best gov-
ernor from within a small set of candidates.
We use a discriminative linear ranking model
to select the best governor from a group of
candidates for a dependent, and our model in-
cludes a rich feature set that encodes syntac-
tic structure in the input parse tree. The parse
correction framework is parser-agnostic, and
can correct attachments using either a generic
model or specialized models tailored to dif-
ficult attachment types like coordination and
pp-attachment. Our experiments show that
parse correction, combining a generic model
with specialized models for difficult attach-
ment types, can successfully improve the qual-
ity of predicted parse trees output by sev-
eral representative state-of-the-art dependency
parsers for French.
1 Introduction
In syntactic dependency parse correction, attach-
ments in an input parse tree are revised by selecting,
for a given dependent, the best governor from within
a small set of candidates. The motivation behind
parse correction is that attachment decisions, espe-
cially traditionally difficult ones like pp-attachment
and coordination, may require substantial contextual
information in order to be made accurately. Because
syntactic dependency parsers predict the parse tree
for an entire sentence, they may not be able to take
into account sufficient context when making attach-
ment decisions, due to computational complexity.
Assuming nonetheless that a predicted parse tree is
mostly accurate, parse correction can revise difficult
attachments by using the predicted tree?s syntactic
structure to restrict the set of candidate governors
and extract a rich set of features to help select among
them. Parse correction is also appealing because it
is parser-agnostic: it can be trained to correct the
output of any dependency parser.
In Section 2 we discuss work related to parse
correction, pp-attachment and coordination resolu-
tion. In Section 3 we discuss dependency struc-
ture and various statistical dependency parsing ap-
proaches. In Section 4 we introduce the parse cor-
rection framework, and Section 5 describes the fea-
tures and learning model used in our implementa-
tion. In Section 6 we present experiments in which
parse correction revises the predicted parse trees of
four state-of-the-art dependency parsers for French.
We provide concluding remarks in Section 7.
2 Related Work
Previous research directly concerning parse correc-
tion includes that of Attardi and Ciaramita (2007),
working on English and Swedish, who use an ap-
proach that considers a fixed set of revision rules:
each rule describes movements in the parse tree
leading from a dependent?s original governor to a
new governor, and a classifier is trained to select
the correct revision rule for a given dependent. One
drawback of this approach is that the classes lack
semantic coherence: a sequence of movements does
not necessarily have the same meaning across differ-
1222
ent syntactic trees. Hall and Nova?k (2005), working
on Czech, define a neighborhood of candidate gov-
ernors centered around the original governor of a de-
pendent, and a Maximum Entropy model determines
the probability of each candidate-dependent attach-
ment. We follow primarily from their work in our
use of neighborhoods to delimit the set of candidate
governors. Our main contributions are: specialized
corrective models for difficult attachment types (co-
ordination and pp-attachment) in addition to a gen-
eral corrective model; more sophisticated features,
feature combinations, and feature selection; and a
ranking model trained directly to select the true gov-
ernor from among a set of candidates.
There has also been other work on techniques
similar to parse correction. Attardi and Dell?Orletta
(2009) investigate reverse revision: a left-to-right
transition-based model is first used to parse a sen-
tence, then a right-to-left transition-based model is
run with additional features taken from the left-to-
right model?s predicted parse. This approach leads
to improved parsing results on a number of lan-
guages. While their approach is similar to parse cor-
rection in that it uses a predicted parse to inform a
subsequent processing step, this information is used
to improve a second parser rather than a model for
correcting errors. McDonald and Pereira (2006)
consider a method for recovering non-projective at-
tachments from a graph representation of a sentence,
in which an optimal projective parse tree has been
identified. The parse tree?s edges are allowed to be
rearranged in ways that introduce non-projectivity
in order to increase its overall score. This rearrange-
ment approach resembles parse correction because
it is a second step that can revise attachments made
in the first step, but it differs in a number of ways: it
is dependent on a graph-based parsing approach, it
does not model errors made by the parser, and it can
only output non-projective variants of the predicted
parse tree.
As a process that revises the output of a syntac-
tic parser, parse reranking is also similar to parse
correction. A well-studied subject (e.g. the work
of Charniak and Johnson (2005) and of Collins and
Koo (2005)), parse reranking is concerned with the
reordering of n-best ranked parse trees output by
a syntactic parser. Parse correction has a num-
ber of advantages compared to reranking: it can be
used with parsers that do not output n-best ranked
parses, it can be easily restricted to specific attach-
ment types, and its output space of parse trees is not
limited to those appearing in an n-best list. How-
ever, parse reranking has the advantage of selecting
the globally optimal parse for a sentence from an n-
best list, while parse correction makes only locally
optimal revisions in the predicted parse for a sen-
tence.
2.1 Difficult Attachment Types
Research on pp-attachment traditionally formulates
the problem in isolation, as in the work of Pantel and
Lin (2000) and of Olteanu and Moldovan (2005).
Examples consist of tuples of the form (v, n1, p, n2),
where either v or n1 is the true governor of the
pp comprising p and n2, and the task is to choose
between v and n1. Recently, Atterer and Schu?tze
(2007) have criticized this formulation as unrealistic
because it uses an oracle to select candidate gover-
nors, and they find that successful approaches for
the isolated problem perform no better than state-
of-the-art parsers on pp-attachment when evaluated
on full sentences. With parse correction, candi-
date governors are identified automatically with no
(v, n1, p, n2) restriction, and for several representa-
tive parsers we find that parse correction improves
pp-attachment performance.
Research on coordination resolution has also of-
ten formulated the problem in isolation. Resnik
(1999) uses semantic similarity to resolve noun-
phrase coordination of the form (n1, cc, n2, n3),
where the coordinating conjunction cc coordinates
either the heads n1 and n2 or the heads n1 and
n3. The same criticism as the one made by At-
terer and Schu?tze (2007) for pp-attachment might
be applied to this approach to coordination reso-
lution. In another formulation, the input consists
of a raw sentence, and coordination structure is
then detected and disambiguated using discrimina-
tive learning models (Shimbo and Hara, 2007) or
coordination-specific parsers (Hara et al, 2009). Fi-
nally, other work has focused on introducing spe-
cialized features for coordination into existing syn-
tactic parsing models (Hogan, 2007). Our approach
is novel with respect to previous work by directly
modeling the correction of coordination errors made
by general-purpose dependency parsers.
1223
ouvrit
Elle porte
la
avec
cle?
la
Figure 1: An unlabeled dependency tree for: Elle ouvrit
la porte avec la cle?. (She opened the door with the key).
3 Dependency Parsing
Dependency syntax involves the representation of
syntactic information for a sentence in the form a
directed graph, whose edges encode word-to-word
relationships. An edge from a governor to a de-
pendent indicates, roughly, that the presence of the
dependent is syntactically legitimated by the gover-
nor. An important property of dependency syntax is
that each word, except for the root of the sentence,
has exactly one governor; dependency syntax is thus
represented by trees. Figure 1 shows an example
of an unlabeled dependency tree.1 For languages
like English or French, most sentences can be rep-
resented with a projective dependency tree: for any
edge from word g to word d, g dominates any inter-
vening word between g and d.
Dependency trees are appealing syntactic repre-
sentations, closer than constituency trees to the se-
mantic representations useful for NLP applications.
This is true even with the projectivity requirement,
which occasionally creates syntax-semantics mis-
matches. Dependency trees have recently seen a
surge of interest, particularly with the introduction
of supervised models for dependency parsing us-
ing linear classifiers. Such parsers fall into two
main categories: transition-based parsing and graph-
based parsing. Additionally, an alternative method
for obtaining the dependency parse for a sentence
is to parse the sentence with a constituency-based
parser and then use an automatic process to convert
the output into dependency structure.
1Edges are generally labeled with the surface grammatical
function that the dependent bears with respect to its governor.
In this paper we focus on unlabeled dependency parsing, setting
aside labeling as a separate task.
3.1 Transition-Based Parsing
In transition-based dependency parsing, whose sem-
inal works are that of Yamada and Matsumoto
(2003) and Nivre (2003), the parsing process ap-
plies a sequence of incremental actions, which typ-
ically manipulate a buffer position in the sentence
and a stack for built sub-structures. Actions are of
the type ?read word from buffer?, ?build a depen-
dency from node on top of the stack to node that
begins the buffer?, etc. In a greedy version of this
process, the action to apply at each step is determin-
istically chosen to be the best-scoring action accord-
ing to a classifier, which is trained on a dependency
treebank converted into sequences of actions. The
strengths of this framework are O(n) time complex-
ity and a lack of restrictions on the locality of fea-
tures. A major drawback is its greedy behavior: it
can potentially make difficult attachment decisions
early in the processing of a sentence, without being
able to reconsider them when more information be-
comes available. Beamed versions of the algorithm
(Johansson and Nugues, 2006) partially address this
problem, but still do not provide a global optimiza-
tion for selecting the output parse tree.
3.2 Graph-Based Parsing
In graph-based dependency parsing, whose seminal
work is that of McDonald et al (2005), the parsing
process selects the globally optimal parse tree from
a graph containing attachments (directed edges) be-
tween each pair of words (nodes) in a sentence.
It finds the k-best scoring parse trees, both during
training and at parse time, where the score of a tree
is the sum of the scores of its factors (consisting of
one or more linked edges). While large factors are
desirable for capturing sophisticated linguistic con-
straints, they come at the cost of time complexity:
for the projective case, adaptations of Eisner?s algo-
rithm (Eisner, 1996) are O(n3) for 1-edge factors
(McDonald et al, 2005) or sibling 2-edge factors
(McDonald and Pereira, 2006), and O(n4) for gen-
eral 2-edge factors (Carreras, 2007) or 3-edge fac-
tors (Koo and Collins, 2010).
3.3 Constituency-Based Parsing
Beyond the two main approaches to dependency
parsing, there is also the approach of constituency-
1224
based parsing followed by a conversion step to de-
pendency structure. We use the three-step parsing
architecture previously tested for French by Candito
et al (2010a): (i) A constituency parse tree is out-
put by the BerkeleyParser, which has been trained to
learn a probabilistic context-free grammar with la-
tent annotations (Petrov et al, 2006) that has parsing
time complexity O(n3) (Matsuzaki et al, 2005); (ii)
A functional role labeler using a Maximum Entropy
model adds functional annotations to links between
a verb and its dependents; (iii) Constituency trees
are automatically converted into projective depen-
dency trees, with remaining unlabeled dependencies
assigned labels using a rule-based approach.
3.4 Baseline Parsers
In this paper, we use the following baseline parsers:
MaltParser (Nivre et al, 2007) for transition-based
parsing; MSTParser (McDonald et al, 2005) (with
sibling 2-edge factors) and BohnetParser (Bohnet,
2010) (with general 2-edge factors) for graph-based
parsing; and BerkeleyParser (Petrov et al, 2006) for
constituency-based parsing.
For MaltParser and MSTParser, we use the best
settings from a benchmarking of parsers for French
(Candito et al, 2010b), except that we remove un-
supervised word clusters as features. The parsing
models are thus trained using features including pre-
dicted part-of-speech tags, lemmas and morpholog-
ical features. For BohnetParser, we trained a new
model using these same predicted features. For
BerkelyParser, which was included in the bench-
marking experiments, we trained a model using the
so-called ?desinflection? process that addresses data
sparseness due to morphological variation: both
at training and parsing time, terminal symbols are
word forms in which redundant morphological suf-
fixes are removed, provided the original part-of-
speech ambiguities are kept (Candito et al, 2010b).
All models are trained on the French Treebank
(FTB) (Abeille? and Barrier, 2004), consisting of
12,351 sentences from the Le Monde newspaper, ei-
ther ?desinflected? for the BerkeleyParser, or con-
verted to projective dependency trees (Candito et al,
2010a) for the three dependency-native parsers.2 For
2The projectivity constraint is linguistically valid for most
French parses: the authors report < 2% non-projective edges in
a hand-corrected subset of the converted FTB.
INPUT: Predicted parse tree T
LOOP: For each chosen dependent d ? D
? Identify candidates Cd from T
? Predict c? = argmax
c ? Cd
S(c, d, T )
? Update T{gov(d) ? c?}
OUTPUT: Corrected version of parse tree T
Figure 2: The parse correction algorithm.
the dependency-native models, features include pre-
dicted part-of-speech (POS) tags from the MElt tag-
ger (Denis and Sagot, 2009), as well as predicted
lemmas and morphological features from the Lefff
lexicon (Sagot, 2010). These models constitute the
state-of-the-art for French dependency parsing: un-
labeled attachment scores (UAS) on the FTB test set
are 89.78% for MaltParser, 91.04% for MSTParser,
91.78% for BohnetParser, and 90.73% for Berkeley-
Parser.
4 Parse Correction
The parse correction algorithm is a post-processing
step to dependency parsing, where attachments from
the predicted parse tree of a sentence are corrected
by considering alternative candidate governors for
each dependent. This process can be useful for at-
tachments made too early in transition-based pars-
ing, or with features that are too local in MST-based
parsing.
The input is the predicted parse T of a sentence.
From T a set D of dependent nodes are chosen for
attachment correction. For each d ? D in left-to-
right sentence order, a set Cd of candidate governors
from T is identified, and then the highest scoring
c ? Cd, using a function S(c, d, T ), is assigned as
the new governor of d in T . Pseudo-code for parse
correction is shown in Figure 2.3
3Contrary to Hall and Nova?k (2005), our iterative algorithm
(along with the fact that Cd never includes nodes that are domi-
nated by d) ensures that corrected structures are trees, so it does
not require additional processing to eliminate cycles and pre-
serve connectivity.
1225
4.1 Choosing Dependents
Various criteria may be used to choose the set D
of dependents to correct. In the work of Hall and
Nova?k (2005) and of Attardi and Ciaramita (2007),
D contains all nodes in the input parse tree. How-
ever, one advantage of parse correction is its ability
to focus on specific attachment types, so an addi-
tional criterion for choosing dependents is to look
separately at those dependents that correspond to
difficult attachment types.
Analyzing errors made by the dependency parsers
introduced in Section 3 on the development set of
the FTB, we observe that two major sources of er-
ror across different parsers are coordination and pp-
attachment. Coordination accounts for around 10%
of incorrect attachments and has an error rate rang-
ing from 30 ? 40%, while pp-attachment accounts
for around 30% of incorrect attachments and has an
error rate of around 15%.
In this paper, we pay special attention to coordina-
tion and pp-attachment. Given the FTB annotation
scheme, coordination can be corrected by changing
the governor (first conjunct) of the coordinating con-
junction that governs the second conjunct, and pp-
attachment can be corrected by changing the gover-
nor of the preposition that heads the pp.4 We thus
train specialized corrective models for when the de-
pendents are coordinating conjunctions and preposi-
tions, in addition to a generic corrective model that
can be applied to any dependent.5
4.2 Identifying Candidate Governors
The set of candidate governors Cd for a dependent
d can be chosen in different ways. One method is
to let every other node in T be a candidate gover-
nor for d. However, parser error analysis has shown
that errors often occur in local contexts. Hall and
Nova?k (2005) define a neighborhood as a set of
nodes Nm(d) around the original predicted gover-
nor co of d, where Nm(d) includes all nodes in the
4The FTB handles pp-attachment in a typical fashion, but
coordination may be handled differently by other schemes (e.g.
the coordinating conjunction governs both conjuncts).
5In our experiments, we never revise punctuation and clitic
dependents. Since punctuation attachments mostly carry little
meaning, they are often annotated inconsistently and ignored
in parsing evaluations (including ours). Clitics are not revised
because they have a very low attachment error rate (2%).
parse tree T within graph distance m of d that pass
through co. They find that around 2/3 of the incor-
rect attachments in the output of Czech parses can be
corrected by selecting the best governor from within
N3(d). Similarly, in oracle experiments reported in
section 6, we find that around 1/2 of coordination
and pp-attachments in the output of French parses
can be corrected by selecting the best governor from
within N3(d). We thus use neighborhoods to delimit
the set of candidate governors.
While one can simply assign Cd ? Nm(d), we
add additional restrictions. First, in order to preserve
projectivity within T , we keep in Cd only those c
such that the update T{gov(d) ? c} would result
in a projective tree.6 Additionally, we discard candi-
dates with certain POS categories that are very un-
likely to be governors: clitics and punctuation are
always discarded, while determiners are discarded if
the dependent is a preposition.
4.3 Scoring Candidate Governors
A new governor c? for a dependent d is predicted by
selecting the highest scoring candidate c ? Cd ac-
cording to a function S(c, d, T ), which takes into
account features over c, d, and the parse tree T . We
use a linear model for our scoring function, which
allows for relatively fast training and prediction. Our
scoring function uses a weight vector ~w ? F, where
F is the feature space for dependents we wish to cor-
rect (either generic, or specialized for prepositions
or for coordinating conjunction), as well as the map-
ping ? : C?D?T ? F from combinations of candi-
date c ? C, dependent d ? D, and parse tree T ? T,
to vectors in the feature space F. The scoring func-
tion returns the inner product of ~w and ?(c, d, T ):
S(c, d, T ) = ~w ? ?(c, d, T ) (1)
4.4 Algorithm Complexity
The time complexity of our algorithm is O(n) in
the length n of the input sentence, which is consis-
tent with past work on parse correction by Hall and
Nova?k (2005) and by Attardi and Ciaramita (2007).
6We also keep candidates that would lead to a non-projective
tree, as long as it would be projective if we ignored punctuation.
This relaxation of the projectivity constraint leads to better or-
acle scores while retaining the key linguistic properties of pro-
jectivity.
1226
Attachments for up to n dependents in a sentence
are deterministically corrected in one pass. For each
such dependent d, the algorithm uses a linear model
to select a new governor after extracting features for
a local set of candidate governors Cd, whose size
does not dependent on n in the average case.7 Lo-
cality in candidate governor identification and fea-
ture extraction preserves linear time complexity in
the overall algorithm.
5 Model Learning
We now discuss our training setup, features, and
learning approach for obtaining the weight vector ~w.
5.1 Training Setup
The parse correction training set pairs gold parse
trees with corresponding predicted parse trees out-
put by a syntactic parser, and it is obtained us-
ing a jackknifing procedure to automatically parse
the gold-annotated training section of a dependency
treebank with a syntactic dependency parser.
We extract separate training sets for each type of
dependent we wish to correct (generic, prepositions,
coordinating conjunctions). Given p, then for each
token d we wish to correct in a sentence in the train-
ing section, we note its true governor gd in the gold
parse tree of the sentence, identify a set of candidate
governors Cd in the predicted parse T , and get fea-
ture vectors {?(c, d, T ) : c ? Cd}.
5.2 Feature Space
In order to learn an effective scoring function, we
use a rich feature space F that encodes syntactic con-
text surrounding a candidate-dependent pair (c, d)
within a parse tree T . Our primary features are indi-
cator functions for realizations of linguistic or tree-
based feature classes.8 From these primary features
we generate more complex feature combinations of
length up to P , which are then added to F. Each
combo represents a set of one or more primary fea-
tures, and is an indicator function that fires if and
only if all of its members do.
7Degenerate parse trees (e.g. flat trees) could lead to cases
where |Cd|=n, but for linguistically coherent parse trees |Cd| is
rather O(km), where k is the average -arity of syntactic parse
trees and m is the neighborhood distance used.
8For instance, there is a binary feature that is 1 if feature
class ?POS of c? takes on the value ?verb?, and 0 otherwise.
5.2.1 Primary Feature Classes
The primary feature classes we use are listed be-
low, grouped into categories corresponding to their
use in different corrective models (dobj is the object
of the dependent, cgov is the governor of the candi-
date, and cd?1 and cd+1 are the closest dependents
of c linearly to the left and right, respectively, of d).
Generic features (always included)
? POS, lemma, and number of dependents of c
? POS and dependency label of cd?1
? POS and dependency label of cd+1
? POS of cgov
? POS and lemma of d
? POS of dobj and whether dobj has a determiner
? Whether c is the predicted governor of d
? Binned linear distance between c and d
? Linear direction of c with respect to d
? POS sequence for nodes on path from c to d
? Graph distance between c and d
? Whether there is punctuation between c and d
Features exclusive to coordination
Whether d would coordinate two conjuncts that:
? Have the same POS
? Have the same word form
? Have number agreement
? Are both nouns with the same cardinality
? Are both proper nouns or both common nouns
? Are both prepositions with the same word form
? Are both prepositions with object of same POS
Features exclusive to pp-attachment
? Whether d immediately follows a punctuation
? Whether d heads a pp likely to be the agent of
a passive verb
? If c is a coordinating conjunction, then whether
c would coordinate two prepositions with the
same word form, and whether there is at least
one open-category word linearly between c and
d (in which case c is an unlikely governor)
1227
? If c is linearly after d, then whether there exists
a plausible rival candidate to the left of d (im-
plemented as whether there is a noun or adjec-
tive linearly before d, without any intervening
finite verb)
5.2.2 Feature Selection
Feature combos allow our models to effectively
sidestep linearity constraints, at the cost of an expo-
nential increase in the size of the feature space F. In
order to accommodate combos, we use feature se-
lection to help reduce the resulting space.
Our first feature selection technique is to apply a
frequency threshold: if a feature or a combo appears
less than K times among instances in our training
set, we remove it from F. In addition to making the
feature space more tractable, frequency thresholding
makes our scoring function less reliant on rare fea-
tures and combos.
Following frequency thresholding, we employ an
additional technique using conditional entropy (CE)
that we term CE-reduction. Let Y be a random vari-
able for whether or not an attachment is true, and
let A be a random variable for different combos that
can appear in an attachment. We calculate the CE of
a combo a with respect to Y as follows,
H(Y |A=a) = ?
?
y?Y
p(y|a) log p(y|a) (2)
where the probability p(y|a) is approximated from
the training set as freq(a, y)/freq(a), with exam-
ple balancing used here to account for more false at-
tachments (Y = 0) than true ones (Y = 1) in our train-
ing set. Having calculated the CE of each combo,
we remove from F those combos for which a subset
combo (or feature) exists with equal or lesser CE.
This eliminates any overly specific combo a when
the extra features encoded in a, compared to some
subset b, do not help a explain Y any better than b.
5.3 Ranking Model
The ranking setting for learning is used when a
model needs to discriminate between mutually ex-
clusive candidates that vary from instance to in-
stance. This is typically used in parse reranking
(Charniak and Johnson, 2005), where for each sen-
tence the model must select the correct parse from
within an n-best list. Denis and Baldridge (2007)
INPUT: Aggressiveness C, rounds R.
INITIALIZE: ~w0?(0, ..., 0), ~wavg?(0, ..., 0)
REPEAT: R times
LOOP: For t = 1, 2, . . . , |X|
? Get feature vectors {~xt,c : c ? Cdt}
? Get true governor gt ? Cdt
? Let ht = argmax
c?Cdt?{gt}
(~wt?1 ? ~xt,c)
? Let mt = (~wt?1 ? ~xt,gt)? (~wt?1 ? ~xt,ht)
IF: mt < 1
? Let ?t = min
{
C , 1?mt?~xt,gt?~xt,ht?2
}
? Set ~wt ? ~wt?1 + ?t(~xt,gt ? ~xt,ht)
ELSE:
? Set ~wt ? ~wt?1
? Set ~wavg ? ~wavg + ~wt
? Set ~w0 ? ~w|X|
OUTPUT: ~wavg/(R ? |X|)
Figure 3: Averaged PA-Ranking training algorithm.
also show that ranking outperforms a binary classifi-
cation approach to pronoun resolution (using a Max-
imum Entropy model), where for each pronominal
anaphor the model must select the correct antecedent
among candidates in a text.9
In our ranking approach to parse correction (PA-
Ranking), the weight vector is trained to select the
true governor from a set of candidates Cd for a de-
pendent d. The training set X is defined such that
the tth instance is a collection of feature vectors
{~xt,c = ?(c, dt, Tt) : c ? Cdt}, where Cdt is the
candidate set for the dependent dt within the pre-
dicted parse Tt, and the class is the true governor gt.
Instances in which gt 6? Cdt are discarded.
PA-Ranking training is carried out using a varia-
tion of the Passive-Aggressive algorithm (Crammer
et al, 2006), which has been adapted to the rank-
ing setting, implemented using the Polka library.10
For each training iteration t, the margin is defined as
9We considered a binary training approach to parse correc-
tion in which the model is trained to independently classify can-
didates as true or false governors, as used by Hall and Nova?k
(2005). However, we found that this approach performs no bet-
ter (and often worse) than the ranking approach, and is less ap-
propriate from a modeling standpoint.
10http://polka.gforge.inria.fr/
1228
mt = (~wt?1 ? ~xt,gt)? (~wt?1 ? ~xt,ht), where ht is the
highest scoring incorrect candidate. The algorithm
is passive because an update to the weight vector is
made if and only if mt < 1, either for incorrect pre-
dictions (mt < 0) or for correct predictions with in-
sufficient margin (0?mt<1). The new weight vec-
tor ~wt is as close as possible to ~wt?1, subject to the
aggressive constraint that the new margin be greater
than 1. We use weight averaging, so the final out-
put ~wavg is the average over the weight vectors after
each training step. Pseudo-code for the training al-
gorithm is shown in Figure 3. The rounds parameter
R determines the number of times to run through the
training set, and the aggressiveness parameter C sets
an upper limit on the update magnitude.
6 Experiments
We present experiments where we applied parse cor-
rection to the output of four state-of-the-art depen-
dency parsers for French. We conducted our eval-
uation on the FTB using the standard training, de-
velopment (dev), and test splits (containing 9,881,
1,235 and 1,235 sentences, respectively). To train
our parse correction models, we generated special-
ized training sets corresponding to each parser by
doing 10-fold jackknifing on the FTB training set
(cf. Section 5.1). Each parser was run on the FTB
dev and test sets, providing baseline unlabeled at-
tachment score (UAS) results and output parse trees
to be corrected.
6.1 Oracles and Neighborhood Size
To determine candidate neighborhood size, we con-
sidered an oracle scoring function that always se-
lects the true governor of a dependent if it appears
in the set of candidate governors, and otherwise se-
lects the predicted governor. Results for this oracle
on the dev set are shown in Table 1. The baseline
corresponds to m=1, where the oracle just selects
the predicted governor. Incrementing m to 2 and
to 3 resulted in substantial gains in oracle UAS, but
further incrementing m to 4 resulted in a relatively
small additional gain. We found that average can-
didate set size increases about linearly in m, so we
decided to use m=3 in order to have a high UAS up-
per bound without adding candidates that are very
unlikely to be true governors.
Neighborhood Size (m)
Base 2 3 4
Berkeley
Coords 67.2 76.5 82.8 84.8
Preps 82.9 88.5 92.2 93.2
Overall 90.1 94.0 96.0 96.5
Bohnet
Coords 70.1 80.6 85.6 87.7
Preps 85.4 89.4 93.4 94.5
Overall 91.2 94.4 96.1 96.6
Malt
Coords 60.9 72.2 78.2 80.5
Preps 82.6 88.1 92.6 93.7
Overall 89.3 93.2 95.1 95.8
MST
Coords 63.6 73.7 80.7 84.4
Preps 84.7 89.4 93.4 94.4
Overall 90.2 93.7 95.6 96.2
MST Overall Reranking top-100 parses: 95.4
Table 1: Parse correction oracle UAS (%) for differ-
ent neighborhood sizes, by dependent type (coordinating
conjunctions, prepositions, or all dependents). Also, a
reranking oracle for MSTParser using the top-100 parses.
We also compared the oracle for parse correc-
tion with an oracle for parse reranking, in which the
parse with the highest UAS for a sentence is selected
from the top-100 parses output by MSTParser. We
found that for MSTParser, the oracle for parse cor-
rection using neighborhood size m=3 (95.6% UAS)
is comparable to the oracle for parse reranking using
the top-100 parses (95.4% UAS). This is an encour-
aging result, showing that parse correction is capable
of the same improvement as parse reranking without
needing to process an n-best list of parses.
6.2 Feature Space Parameters
For the feature space F, we performed a grid search
to find good values for the parameters K (frequency
threshold), P (combo length), and CE-reduction.
We found that P=3 with CE-reduction allowed for
the most compactness without sacrificing correction
performance, for all of our corrective models. Ad-
ditionally, K=2 worked well for the coordinating
conjunction models, while K=10 worked well for
the preposition and generic models. CE-reduction
proved useful in greatly reducing the feature space
without lowering correction performance: it reduced
the size of the coordinating conjunction models from
400k to 65k features each, the preposition models
from 400k to 75k features each, and the generic
models from 800k to 200k features each.
1229
Corrective UAS (%)
Configuration Coords Preps Overall
Berkeley
Baseline 68.3 83.8 90.73
Generic 69.4 84.9* 91.13*
Specialized 71.5* 85.1* 91.23*
Bohnet
Baseline 70.5 86.1 91.78
Generic 71.2 86.4 91.88
Specialized 72.7* 86.2 91.88
Malt
Baseline 59.8 83.2 89.78
Generic 63.2* 84.5* 90.39*
Specialized 64.0* 85.0* 90.47*
MST
Baseline 60.5 85.9 91.04
Generic 64.2* 86.2 91.25*
Specialized 68.0* 86.2 91.36*
Table 2: Coordinating conjunction, preposition, and over-
all UAS (%) by corrective configuration on the test set.
Significant improvements over the baseline starred.
6.3 Corrective Configurations
For our evaluation of parse correction, we compared
two different configurations: generic (corrects all
dependents using the generic model) and specialized
(corrects coordinating conjunctions and prepositions
using their respective specialized models, and cor-
rects other dependents using the generic model).
The PA-Ranking aggressiveness parameter C was
set to 1 for our experiments, while the rounds pa-
rameter R was tuned separately for each corrective
model using the dev set. For our final tests, we ap-
plied each combination of parser + corrective con-
figuration by sequentially revising all dependents in
the output parse that had a relevant POS tag given
the corrective configuration. In the FTB test set,
this amounted to an evaluation over 5,706 prepo-
sition tokens, 801 coordinating conjunction tokens,
and 31,404 overall (non-punctuation) tokens.11
6.4 Results
Final results for the test set are shown in Table 2.
The overall UAS of each parser (except Bohnet-
Parser) was significantly improved under both cor-
rective configurations.12 The specialized configura-
11Since the MElt tagger and BerkeleyParser POS tagging ac-
curacies were around 97%, the sets of tokens considered for re-
vision differed slightly from the sets of tokens (with gold POS
tags) used to calculate UAS scores.
12We used McNemar?s Chi-squared test with p = 0.05 for all
significance tests.
tion performed as well as, and in most cases bet-
ter than, the generic configuration, indicating the
usefulness of specialized models and features for
difficult attachment types. Interestingly, the lower
the baseline parser?s UAS, the larger the overall
improvement from parse correction under the spe-
cialized configuration: MaltParser had the lowest
baseline and the highest error reduction (6.8%),
BerkeleyParser had the second-lowest baseline and
the second-highest error reduction (5.4%), MST-
Parser had the third-lowest baseline and the third-
highest error reduction (3.6%), and BohnetParser
had the highest baseline and the lowest error re-
duction (1.2%). It may be that the additional er-
rors made by a low-baseline parser, compared to a
high-baseline parser, involve relatively simpler at-
tachments that parse correction can better model.
Parse correction achieved significant improve-
ments for coordination resolution under the spe-
cialized configuration for each parser. MaltParser
and MSTParser had very low baseline coordinat-
ing conjunction UAS (around 60%), while Berke-
leyParser and BohnetParser had higher baselines
(around 70%). The highest error reduction was
achieved by MSTParser (19.0%), followed by Malt-
Parser (10.4%), BerkeleyParser (10.1%), and finally
BohnetParser (7.5%). The result for MSTParser was
surprising: although it had the second-highest base-
line overall UAS, it shared the lowest baseline coor-
dinating conjunction UAS and had the highest er-
ror reduction with parse correction. An explana-
tion for this result is that the annotation scheme for
coordination structure in the dependency FTB has
the first conjunct governing the coordinating con-
junction, which governs the second conjunct. Since
MSTParser is limited to sibling 2-edge factors (cf.
section 3), it is unable to jointly consider a full coor-
dination structure. BohnetParser, which uses general
2-edge factors, can consider full coordination struc-
tures and consequently has a much higher baseline
coordinating conjunction UAS than MSTParser.
Parse correction achieved significant but mod-
est improvements in pp-attachment performance un-
der the specialized configuration for MaltParser and
BerkeleyParser. However, parse correction did not
significantly improve pp-attachment performance
for MSTParser or BohnetParser, the two parsers that
had the highest baseline preposition UAS (around
1230
Modification Type
w?c c?w w?w Mods
Berkeley
Coords 40 14 33 10.9 %
Preps 118 39 41 3.5 %
Overall 228 67 104 1.3 %
Bohnet
Coords 32 15 33 10.0 %
Preps 52 46 32 2.3 %
Overall 150 121 130 1.1 %
Malt
Coords 55 21 56 16.5 %
Preps 149 50 76 4.8 %
Overall 390 172 293 2.4 %
MST
Coords 80 20 51 18.9 %
Preps 64 45 26 2.4 %
Overall 183 88 117 1.1 %
Table 3: Breakdown of modifications made under the
specialized configuration for each parser, by dependent
type. w?c is wrong-to-correct, c?w is correct-to-
wrong, w?w is wrong-to-wrong, and Mods is the per-
centage of tokens modified.
86%). These results are a bit disappointing, but they
suggest that there may be a performance ceiling for
pp-attachment beyond which rich lexical informa-
tion (syntactic and semantic) or full sentence con-
texts are needed. For English, the average human
performance on pp-attachment for the (v, n1, p, n2)
problem formulation is just 88.2% when given only
the four head-words, but increases to 93.2% when
given the full sentence (Ratnaparkhi et al, 1994).
If similar levels of human performance exist for
French, additional sources of information may be
needed to improve pp-attachment performance.
In addition to evaluating UAS improvements for
parse correction, we took a closer look at the best
corrective configuration (specialized) and analyzed
the types of attachment modifications made (Ta-
ble 3). In most cases there were around 2?3 times
as many error-correcting modifications (w?c) as
error-creating modifications (c?w), and the overall
% of tokens modified was very low overall (around
1-2%). Parse correction is thus conservative in the
number of modifications made, and rather accurate
when it does decide to modify an attachment.
Finally, we compared the running times of the
four parsers, as well as that of parse correction, on
the test set using a 2.66 GHz Intel Core 2 Duo ma-
chine. BerkeleyParser took 600s, BohnetParser took
450s using both cores (800s using a single core),
MaltParser took 45s, and MSTParser took 1000s. A
rough version of parse correction in the specialized
configuration took around 200s (for each parser). An
interesting result is that parse correction improves
MaltParser the most while retaining an overall time
complexity of O(n), compared to O(n3) or higher
for the other parsers. This suggests that linear-time
transition-based parsing and parse correction could
combine to form an attractive system that improves
parsing performance while retaining high speed.
7 Conclusion
We have developed a parse correction framework for
syntactic dependency parsing that uses specialized
models for difficult attachment types. Candidate
governors for a given dependent are identified in a
neighborhood around the predicted governor, and a
scoring function selects the best governor. We used
discriminative linear ranking models with features
encoding syntactic context, and we tested parse cor-
rection on coordination, pp-attachment, and generic
dependencies in the outputs of four representative
statistical dependency parsers for French. Parse cor-
rection achieved improvements in unlabeled attach-
ment score for three out of the four parsers, with
MaltParser seeing the greatest improvement. Since
both MaltParser and parse correction run in O(n)
time, a combined system could prove useful in situ-
ations where high parsing speed is required.
Future work on parse correction might focus on
developing specialized models for other difficult
attachment types, such as verb-phrase attachment
(verb dependents account for around 15% of incor-
rect attachments across all four parsers). Also, se-
lectional preferences and subcategorization frames
(from hand-built resources or extracted using distri-
butional methods) could make for useful features in
the pp-attachment corrective model; we suspect that
richer lexical information is needed in order to in-
crease the currently modest improvements achieved
by parse correction on pp-attachment.
Acknowledgments
We would like to thank Pascal Denis for his help us-
ing the Polka library, and Alexis Nasr for his advice
and comments. This work was partially funded by
the ANR project Sequoia ANR-08-EMER-013.
1231
References
A. Abeille? and N. Barrier. 2004. Enriching a French
treebank. In Proceedings of the Fourth International
Conference on Language Resources and Evaluation,
Lisbon, Portugal, May.
G. Attardi and M. Ciaramita. 2007. Tree revision learn-
ing for dependency parsing. In Proceedings of the
Conference of the North American Chapter of the As-
sociation for Computational Linguistics, pages 388?
395, Rochester, New York, April.
G. Attardi and F. Dell?Orletta. 2009. Reverse revision
and linear tree combination for dependency parsing.
In Proceedings of the 2009 Conference of the North
American Chapter of the Association for Computa-
tional Linguistics, pages 261?264, Boulder, Colorado,
June.
M. Atterer and H. Schu?tze. 2007. Prepositional phrase
attachment without oracles. Computational Linguis-
tics, 33(4):469?476.
B. Bohnet. 2010. Very high accuracy and fast depen-
dency parsing is not a contradiction. In Proceedings of
the 23rd International Conference on Computational
Linguistics, pages 89?97, Beijing, China, August.
M. Candito, B. Crabbe?, and P. Denis. 2010a. Statistical
French dependency parsing: Treebank conversion and
first results. In Proceedings of the Seventh Interna-
tional Conference on Language Resources and Evalu-
ation, Valetta, Malta, May.
M. Candito, J. Nivre, P. Denis, and E. Henestroza An-
guiano. 2010b. Benchmarking of statistical depen-
dency parsers for French. In Proceedings of the 23rd
International Conference on Computational Linguis-
tics, pages 108?116, Beijing, China, August.
X. Carreras. 2007. Experiments with a higher-order
projective dependency parser. In Proceedings of the
CoNLL Shared Task Session of EMNLP-CoNLL, pages
957?961, Prague, Czech Republic, June.
E. Charniak and M. Johnson. 2005. Coarse-to-fine n-
best parsing and MaxEnt discriminative reranking. In
Proceedings of the 43rd Annual Meeting of the Asso-
ciation for Computational Linguistics, pages 173?180,
Ann Arbor, Michigan, June.
M. Collins and T. Koo. 2005. Discriminative reranking
for natural language parsing. Computational Linguis-
tics, 31(1):25?70.
K. Crammer, O. Dekel, J. Keshet, S. Shalev-Shwartz,
and Y. Singer. 2006. Online passive-aggressive algo-
rithms. The Journal of Machine Learning Research,
7:551?585.
P. Denis and J. Baldridge. 2007. A ranking approach
to pronoun resolution. In Proceedings of the 20th In-
ternational Joint Conference on Artifical intelligence,
pages 1588?1593, Hyderabad, India, January.
P. Denis and B. Sagot. 2009. Coupling an annotated cor-
pus and a morphosyntactic lexicon for state-of-the-art
POS tagging with less human effort. In Proceedings
of the 23rd Pacific Asia Conference on Language, In-
formation and Computation, Hong Kong, China, De-
cember.
J.M. Eisner. 1996. Three new probabilistic models for
dependency parsing: An exploration. In Proceedings
of the 16th conference on Computational linguistics-
Volume 1, pages 340?345, Santa Cruz, California, Au-
gust.
K. Hall and V. Nova?k. 2005. Corrective modeling for
non-projective dependency parsing. In Proceedings
of the Ninth International Workshop on Parsing Tech-
nologies, pages 42?52, Vancouver, British Columbia,
October.
K. Hara, M. Shimbo, H. Okuma, and Y. Matsumoto.
2009. Coordinate structure analysis with global struc-
tural constraints and alignment-based local features.
In Proceedings of the Joint Conference of the 47th
Annual Meeting of the ACL and the 4th International
Joint Conference on Natural Language Processing of
the AFNLP, pages 967?975, Suntec, Singapore, Au-
gust.
D. Hogan. 2007. Coordinate noun phrase disambigua-
tion in a generative parsing model. In In Proceed-
ings of the 45th Annual Meeting of the Association
for Computational Linguistics, number 1, page 680,
Prague, Czech Republic, June.
R. Johansson and P. Nugues. 2006. Investigating
multilingual dependency parsing. In Proceedings of
the Tenth Conference on Computational Natural Lan-
guage Learning, pages 206?210, New York City, New
York, June.
T. Koo and M. Collins. 2010. Efficient third-order de-
pendency parsers. In Proceedings of the 48th Annual
Meeting of the Association for Computational Linguis-
tics, pages 1?11, Uppsala, Sweden, July.
T. Matsuzaki, Y. Miyao, and J. Tsujii. 2005. Probabilis-
tic CFG with latent annotations. In Proceedings of
the 43rd Annual Meeting on Association for Computa-
tional Linguistics, pages 75?82, Ann Arbor, Michigan,
June.
R. McDonald and F. Pereira. 2006. Online learning
of approximate dependency parsing algorithms. In
Proceedings of the 11th Conference of the European
Chapter of the Association for Computational Linguis-
tics, pages 81?88, Trento, Italy, April.
R. McDonald, K. Crammer, and F. Pereira. 2005. On-
line large-margin training of dependency parsers. In
Proceedings of the 43rd Annual Meeting on Associa-
tion for Computational Linguistics, pages 91?98, Ann
Arbor, Michigan, June.
1232
J. Nivre, J. Hall, J. Nilsson, A. Chanev, G. Eryigit,
S. Ku?bler, S. Marinov, and E. Marsi. 2007. Malt-
Parser: A language-independent system for data-
driven dependency parsing. Natural Language Engi-
neering, 13(02):95?135.
J. Nivre. 2003. An efficient algorithm for projective de-
pendency parsing. In Proceedings of the 8th Interna-
tional Workshop on Parsing Technologies, pages 149?
160, Nancy, France, April.
M. Olteanu and D. Moldovan. 2005. PP-attachment dis-
ambiguation using large context. In Proceedings of
the Conference on Human Language Technology and
Empirical Methods in Natural Language Processing,
pages 273?280, Vancouver, British Columbia, Octo-
ber.
P. Pantel and D. Lin. 2000. An unsupervised approach
to prepositional phrase attachment using contextually
similar words. In Proceedings of the 38th Annual
Meeting of the Association for Computational Linguis-
tics, volume 38, pages 101?108, Hong Kong, October.
S. Petrov, L. Barrett, R. Thibaux, and D. Klein. 2006.
Learning accurate, compact, and interpretable tree an-
notation. In Proceedings of the 21st International
Conference on Computational Linguistics and the 44th
annual meeting of the Association for Computational
Linguistics, pages 433?440, Sydney, Australia, July.
A. Ratnaparkhi, J. Reynar, and S. Roukos. 1994. A max-
imum entropy model for prepositional phrase attach-
ment. In Proceedings of the Workshop on Human Lan-
guage Technology, pages 250?255, Plainsboro, New
Jersey, March.
P. Resnik. 1999. Semantic similarity in a taxonomy: An
information-based measure and its application to prob-
lems of ambiguity in natural language. Journal of Ar-
tificial Intelligence Research, 11(95):130.
B. Sagot. 2010. The Lefff, a freely available, accurate
and large-coverage lexicon for French. In Proceedings
of the Seventh International Conference on Language
Resources and Evaluation, Valetta, Malta, May.
M. Shimbo and K. Hara. 2007. A discriminative learn-
ing model for coordinate conjunctions. In Proceedings
of the 2007 Joint Conference on Empirical Methods
in Natural Language Processing and Computational
Natural Language Learning, pages 610?619, Prague,
Czech Republic, June.
H. Yamada and Y. Matsumoto. 2003. Statistical depen-
dency analysis with support vector machines. In Pro-
ceedings of the 8th International Workshop on Parsing
Technologies, pages 195?206, Nancy, France, April.
1233
Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 1?11,
Jeju, Republic of Korea, 12 July 2012. c?2012 Association for Computational Linguistics
Probabilistic Lexical Generalization for French Dependency Parsing
Enrique Henestroza Anguiano and Marie Candito
Alpage (Universite? Paris Diderot / INRIA)
Paris, France
enrique.henestroza anguiano@inria.fr, marie.candito@linguist.jussieu.fr
Abstract
This paper investigates the impact on French
dependency parsing of lexical generalization
methods beyond lemmatization and morpho-
logical analysis. A distributional thesaurus
is created from a large text corpus and used
for distributional clustering and WordNet au-
tomatic sense ranking. The standard approach
for lexical generalization in parsing is to map
a word to a single generalized class, either re-
placing the word with the class or adding a
new feature for the class. We use a richer
framework that allows for probabilistic gener-
alization, with a word represented as a prob-
ability distribution over a space of general-
ized classes: lemmas, clusters, or synsets.
Probabilistic lexical information is introduced
into parser feature vectors by modifying the
weights of lexical features. We obtain im-
provements in parsing accuracy with some
lexical generalization configurations in exper-
iments run on the French Treebank and two
out-of-domain treebanks, with slightly better
performance for the probabilistic lexical gen-
eralization approach compared to the standard
single-mapping approach.
1 Introduction
In statistical, data-driven approaches to natural lan-
guage syntactic parsing, a central problem is that of
accurately modeling lexical relationships from po-
tentially sparse counts within a training corpus. Our
particular interests are centered on reducing lexical
data sparseness for linear classification approaches
for dependency parsing. In these approaches, linear
models operate over feature vectors that generally
represent syntactic structure within a sentence, and
feature templates are defined in part over the word
forms of one or more tokens in a sentence. Because
treebanks used for training are often small, lexical
features may appear relatively infrequently during
training, especially for languages with richer mor-
phology than English. This may, in turn, impede the
parsing model?s ability to generalize well outside of
its training set with respect to lexical features.
Past approaches for achieving lexical generaliza-
tion in dependency parsing have used WordNet se-
mantic senses in parsing experiments for English
(Agirre et al, 2011), and word clustering over large
corpora in parsing experiments for English (Koo
et al, 2008) as well as for French (Candito et al,
2010b). These approaches map each word to a sin-
gle corresponding generalized class (synset or clus-
ter), and integrate generalized classes into parsing
models in one of two ways: (i) the replacement
strategy, where each word form is simply replaced
with a corresponding generalized class; (ii) a strat-
egy where an additional feature is created for the
corresponding generalized class.
Our contribution in this paper is applying prob-
abilistic lexical generalization, a richer framework
for lexical generalization, to dependency parsing.
Each word form is represented as a categorical dis-
tribution over a lexical target space of generalized
classes, for which we consider the spaces of lemmas,
synsets, and clusters. The standard single-mapping
approach from previous work can be seen as a sub-
case: each categorical distribution assigns a proba-
bility of 1 to a single generalized class. The method
1
we use for introducing probabilistic information into
a feature vector is based on that used by Bunescu
(2008), who tested the use of probabilistic part-of-
speech (POS) tags through an NLP pipeline.
In this paper, we perform experiments for French
that use the replacement strategy for integrating
generalized classes into parsing models, comparing
the single-mapping approach for lexical generaliza-
tion with our probabilistic lexical generalization ap-
proach. In doing so, we provide first results on the
application to French parsing of WordNet automatic
sense ranking (ASR), using the method of McCarthy
et al (2004). For clustering we deviate from most
previous work, which has integrated Brown clusters
(Brown et al, 1992) into parsing models, and instead
use distributional lexical semantics to create both a
distributional thesaurus - for probabilistic general-
ization in the lemma space and ASR calculation -
and to perform hierarchical agglomerative clustering
(HAC). Though unlexicalized syntactic HAC clus-
tering has been used to improve English dependency
parsing (Sagae and Gordon, 2009), we provide first
results on using distributional lexical semantics for
French parsing. We also include an out-of-domain
evaluation on medical and parliamentary text in ad-
dition to an in-domain evaluation.
In Section 2 we describe the lexical target spaces
used in this paper, as well as the method of integrat-
ing probabilistic lexical information into a feature
vector for classification. In Section 3 we discuss de-
pendency structure and transition-based parsing. In
Section 4 we present the experimental setup, which
includes our parser implementation, the construction
of our probabilistic lexical resources, and evaluation
settings. We report parsing results both in-domain
and out-of-domain in Section 5, we provide a sum-
mary of related work in Section 6, and we conclude
in Section 7.
2 Probabilistic Lexical Target Spaces
Using terms from probability theory, we define a lex-
ical target space as a sample space ? over which
a categorical distribution is defined for each lexi-
cal item in a given source vocabulary. Because we
are working with French, a language with relatively
rich morphology, we use lemmas as the base lexi-
cal items in our source vocabulary. The outcomes
contained in a sample space represent generalized
classes in a target vocabulary. In this paper we con-
sider three possible target vocabularies, with cor-
responding sample spaces: ?l for lemmas, ?s for
synsets, and ?c for clusters.
2.1 ?l Lemma Space
In the case of the lemma space, the source and tar-
get vocabularies are the same. To define an ap-
propriate categorical distribution for each lemma,
one where the possible outcomes also correspond to
lemmas, we use a distributional thesaurus that pro-
vides similarity scores for pairs of lemmas. Such
a thesaurus can be viewed as a similarity function
D(x, y), where x, y ? V and V is the vocabulary
for both the source and target spaces.
The simplest way to define a categorical distribu-
tion over ?l, for a lemma x ? V , would be to use
the following probability mass function px:
px(y) =
D(x, y)
?
y??V
D(x, y?)
(1)
One complication is the identity similarity D(x, x):
although it can be set equal to 1 (or the similar-
ity given by the thesaurus, if one is provided), we
choose to assign a pre-specified probability mass m
to the identity lemma, with the remaining mass used
for generalization across other lemmas. Addition-
ally, in order to account for noise in the thesaurus,
we restrict each categorical distribution to a lemma?s
k-nearest neighbors. The probability mass function
px over the space ?l that we use in this paper is fi-
nally as follows:
px(y) =
?
?
?
?
?
?
?
?
?
?
?
m, if y = x
(1?m)D(x, y)
?
y??Nx(k)
D(x, y?)
, if y ? Nx(k)
0, otherwise
(2)
2.2 ?s Synset Space
In the case of the synset space, the target vacabulary
contains synsets from the Princeton WordNet sense
hierarchy (Fellbaum, 1998). To define an appro-
priate categorical distribution over synsets for each
2
lemma x in our source vocabulary, we first use the
WordNet resource to identify the set Sx of different
senses of x. We then use a distributional thesaurus to
perform ASR, which determines the prevalence with
respect to x of each sense s ? Sx, following the
approach of McCarthy et al (2004). Representing
the thesaurus as a similarity function D(x, y), let-
ting Nx(k) be the set of k-nearest neighbors for x,
and letting W (s1, s2) be a similarity function over
synsets in WordNet, we define a prevalence function
Rx(s) as follows:
Rx(s) =
?
y?Nx(k)
D(x, y)
max
s? ? Sy
W (s, s?)
?
t?Sx
max
s? ? Sy
W (t, s?)
(3)
This function essentially weights the semantic con-
tribution that each distributionally-similar neighbor
adds to a given sense for x. With the prevalence
scores of each sense for x having been calculated,
we use the following probability mass function px
over the space ?s, where Sx(k) is the set of k-most
prevalent senses for x:
px(s) =
?
?
?
?
?
?
?
Rx(s)
?
s??Sx(k)
Rx(s?)
, if s ? Sx(k)
0, otherwise
(4)
Note that the first-sense ASR approach to using
WordNet synsets for parsing, which has been previ-
ously explored in the literature (Agirre et al, 2011),
corresponds to setting k=1 in Equation 4.
2.3 ?c Cluster Space
In the case of the cluster space, any approach for
word clustering may be used to create a reduced tar-
get vocabulary of clusters. Defining a categorical
distribution over clusters would be interesting in the
case of soft clustering of lemmas, in which a lemma
can participate in more than one cluster, but we have
not yet explored this clustering approach.
In this paper we limit ourselves to the simpler
hard clustering HAC method, which uses a distri-
butional thesaurus and iteratively joins two clusters
together based on the similarities between lemmas
in each cluster. We end up with a simple probability
mass function px over the space ?c for a lemma x
with corresponding cluster cx:
px(c) =
{
1, if c = cx
0, otherwise (5)
2.4 Probabilistic Feature Generalization
In a typical classifier-based machine learning setting
in NLP, feature vectors are constructed using indi-
cator functions that encode categorical information,
such as POS tags, word forms or lemmas.
In this section we will use a running example
where a and b are token positions of interest to a
classifier, and for which feature vectors are created.
If we let t stand for POS tag and l stand for lemma,
a feature template for this pair of tokens might then
be [talb]. Feature templates are instantiated as ac-
tual features in a vector space depending on the cat-
egorical values they can take on. One possible in-
stantiation of the template [talb] would then be the
feature [ta=verb?lb=avocat], which indicates that a
is a verb and b is the lemma avocat (?avocado? or
?lawyer?), with the following indicator function:
f =
{
1, if ta=verb ? lb=avocat
0, otherwise (6)
To perform probabilistic feature generalization, we
replace the indicator function, which represents a
single original feature, with a collection of weighted
functions representing a set of derived features. Sup-
pose the French lemma avocat is in our source vo-
cabulary and has multiple senses in ?s (s1 for the
?avocado? sense, s2 for the ?lawyer? sense, etc.),
as well as a probability mass function pav. We
discard the old feature [ta=verb?lb=avocat] and
add, for each si, a derived feature of the form
[ta=verb?xb=si], where x represents a target space
generalized class, with the following weighted indi-
cator function:
f(i) =
{
pav(si), if ta=verb ? lb=avocat
0, otherwise (7)
This process extends easily to generalizing multiple
categorical variables. Consider the bilexical feature
[la=manger?lb=avocat], which indicates that a
is the lemma manger (?eat?) and b is the lemma
avocat. If both lemmas manger and avocat appear
3
ouvrit
Elle porte
la
avec
cle?
la
Figure 1: An unlabeled dependency tree for ?Elle ouvrit
la porte avec la cle?? (?She opened the door with the key?).
in our source vocabulary and have multiple senses
in ?s, with probability mass functions pma and pav,
then for each pair i, j we derive a feature of the
form [xa=si?xb=sj], with the following weighted
indicator function:
f(i,j)=
{
pma(si)pav(sj), if la=manger?lb=avocat
0, otherwise (8)
3 Dependency Parsing
Dependency syntax involves the representation of
syntactic information for a sentence in the form of
a directed graph, whose edges encode word-to-word
relationships. An edge from a governor to a de-
pendent indicates, roughly, that the presence of the
dependent is syntactically legitimated by the gover-
nor. An important property of dependency syntax is
that each word, except for the root of the sentence,
has exactly one governor; dependency syntax is thus
represented by trees. Figure 1 shows an example
of an unlabeled dependency tree.1 For languages
like English or French, most sentences can be rep-
resented with a projective dependency tree: for any
edge from word g to word d, g dominates any inter-
vening word between g and d.
Dependency trees are appealing syntactic repre-
sentations, closer than constituency trees to the se-
mantic representations useful for NLP applications.
This is true even with the projectivity requirement,
which occasionally creates syntax-semantics mis-
matches. Dependency trees have recently seen a
surge of interest, particularly with the introduction
of supervised models for dependency parsing using
linear classifiers.
1Our experiments involve labeled parsing, with edges addi-
tionally labeled with the surface grammatical function that the
dependent bears with respect to its governor.
3.1 Transition-Based Parsing
In this paper we focus on transition-based pars-
ing, whose seminal works are that of Yamada and
Matsumoto (2003) and Nivre (2003). The parsing
process applies a sequence of incremental actions,
which typically manipulate a buffer position in the
sentence and a stack for built sub-structures. In the
arc-eager approach introduced by Nivre et al (2006)
the possible actions are as follows, with s0 being the
token on top of the stack and n0 being the next token
in the buffer:
? SHIFT: Push n0 onto the stack.
? REDUCE: Pop s0 from the stack.
? RIGHT-ARC(r): Add an arc labeled r from s0
to n0; push n0 onto the stack.
? LEFT-ARC(r): Add an arc labeled r from n0
to s0; pop s0 from the stack.
The parser uses a greedy approach, where the ac-
tion selected at each step is the best-scoring action
according to a classifier, which is trained on a de-
pendency treebank converted into sequences of ac-
tions. The major strength of this framework is its
O(n) time complexity, which allows for very fast
parsing when compared to more complex global op-
timization approaches.
4 Experimental Setup
We now discuss the treebanks used for training and
evaluation, the parser implementation and baseline
settings, the construction of the probabilistic lexical
resources, and the parameter tuning and evaluation
settings.
4.1 Treebanks
The treebank we use for training and in-domain
evaluation is the French Treebank (FTB) (Abeille?
and Barrier, 2004), consisting of 12,351 sentences
from the Le Monde newspaper, converted to projec-
tive2 dependency trees (Candito et al, 2010a). For
our experiments we use the usual split of 9,881 train-
ing, 1,235 development, and 1,235 test sentences.
2The projectivity constraint is linguistically valid for most
French parses: the authors report < 2% non-projective edges in
a hand-corrected subset of the converted FTB.
4
Moving beyond the journalistic domain, we use
two additional treebank resources for out-of-domain
parsing evaluations. These treebanks are part of
the Sequoia corpus (Candito and Seddah, 2012),
and consist of text from two non-journalistic do-
mains annotated using the FTB annotation scheme:
a medical domain treebank containing 574 develop-
ment and 544 test sentences of public assessment
reports of medicine from the European Medicines
Agency (EMEA) originally collected in the OPUS
project (Tiedemann, 2009), and a parliamentary do-
main treebank containing 561 test sentences from
the Europarl3 corpus.
4.2 Parser and Baseline Settings
We use our own Python implementation of the arc-
eager algorithm for transition-based parsing, based
on the arc-eager setting of MaltParser (Nivre et al,
2007), and we train using the standard FTB training
set. Our baseline feature templates and general set-
tings correspond to those obtained in a benchmark-
ing of parsers for French (Candito et al, 2010b),
under the setting which combined lemmas and mor-
phological features.4 Automatic POS-tagging is per-
formed using MElt (Denis and Sagot, 2009), and
lemmatization and morphological analysis are per-
formed using the Lefff lexicon (Sagot, 2010). Ta-
ble 1 lists our baseline parser?s feature templates.
4.3 Lexical Resource Construction
We now describe the construction of our probabilis-
tic lexical target space resources, whose prerequi-
sites include the automatic parsing of a large corpus,
the construction of a distributional thesaurus, the use
of ASR on WordNet synsets, and the use of HAC
clustering.
4.3.1 Automatically-Parsed Corpus
The text corpus we use consists of 125 mil-
lion words from the L?Est Republicain newspa-
per5, 125 million words of dispatches from the
Agence France-Presse, and 225 million words from
a French Wikipedia backup dump6. The corpus is
3http://www.statmt.org/europarl/
4That work tested the use of Brown clusters, but obtained no
improvement compared to a setting without clusters. Thus, we
do not evaluate Brown clustering in this paper.
5http://www.cnrtl.fr/corpus/estrepublicain/
6http://dumps.wikimedia.org/
Feature Templates
Unigram tn0 ; ln0 ; cn0 ; wn0 ; ts0 ; ls0 ; cs0 ; ws0 ; ds0 ;
tn1 ; ln1 ; tn2 ; tn3 ; ts1 ; ts2 ; tn0l ; ln0l ; dn0l ;
ds0l ; ds0r ; ls0h ; {min0 : i ? |M |};
{mis0 : i ? |M |}
Bigram ts0tn0 ; ts0 ln0 ; ls0 ln0 ; ln0tn1 ; tn0 tn0l ;
tn0dn0l ; {mis0mjn0 : i; j ? |M |};
{tn0min0 : i ? |M |}; {ts0mis0 : i ? |M |}
Trigram ts2ts1 ts0 ; ts1ts0 tn0 ; ts0 tn0tn1 ; tn0 tn1tn2 ;
tn1tn2 tn3 ; ts0ds0lds0r
Table 1: Arc-eager parser feature templates. c = coarse
POS tag, t = fine POS tag, w = inflected word form, l =
lemma, d = dependency label, mi = morphological fea-
ture from set M . For tokens, ni = ith token in the buffer,
si = ith token on the stack. The token subscripts l, r, and
h denote partially-constructed syntactic left-most depen-
dent, right-most dependent, and head, respectively.
preprocessed using the Bonsai tool7, and parsed us-
ing our baseline parser.
4.3.2 Distributional Thesaurus
We build separate distributional thesauri for
nouns and for verbs,8 using straightforward meth-
ods in distributional lexical semantics based primar-
ily on work by Lin (1998) and Curran (2004). We
use the FreDist tool (Henestroza Anguiano and De-
nis, 2011) for thesaurus creation.
First, syntactic contexts for each lemma are ex-
tracted from the corpus. We use all syntactic de-
pendencies in which the secondary token has an
open-class POS tag, with labels included in the con-
texts and two-edge dependencies used in the case of
prepositional-phrase attachment and coordination.
Example contexts are shown in Figure 2. For verb
lemmas we limit contexts to dependencies in which
the verb is governor, and we add unlexicalized ver-
sions of contexts to account for subcategorization.
For noun lemmas, we use all dependencies in which
the noun participates, and all contexts are lexical-
ized. The vocabulary is limited to lemmas with at
least 1,000 context occurrences, resulting in 8,171
nouns and 2,865 verbs.
Each pair of lemma x and context c is sub-
sequently weighted by mutual informativeness us-
ing the point-wise mutual information metric, with
7http://alpage.inria.fr/statgram/frdep/fr_
stat_dep_parsing.html
8We additionally considered adjectives and adverbs, but our
initial tests yielded no parsing improvements.
5
? One-Edge Context: ?obj? N |avocat
? One-Edge Context: ?obj? N
(unlexicalized)
? Two-Edge Context: ?mod? P |avec ?obj? N |avocat
? Two-Edge Context: ?mod? P |avec ?obj? N
(unlexicalized)
Figure 2: Example dependency contexts for the verb
lemma manger. The one-edge contexts corresponds to
the phrase ?manger un avocat? (?eat an avocado?), and
the two-edge contexts corresponds to the phrase ?manger
avec un avocat? (?eat with a lawyer?).
probabilities estimated using frequency counts:
I(x, c) = log
(
p(x, c)
p(x)p(c)
)
(9)
Finally, we use the cosine metric to calculate the dis-
tributional similarity between pairs of lemmas x, y:
D(x, y) =
?
c
I(x, c)I(y, c)
?
(
?
c
I(x, c)2
)
?
(
?
c
I(y, c)2
)
(10)
4.3.3 WordNet ASR
For WordNet synset experiments we use the
French EuroWordNet9 (FREWN). A WordNet
synset mapping10 allows us to convert synsets in the
FREWN to Princeton WordNet version 3.0, and af-
ter discarding a small number of synsets that are
not covered by the mapping we retain entries for
9,833 nouns and 2,220 verbs. We use NLTK, the
Natural Language Toolkit (Bird et al, 2009), to cal-
culate similarity between synsets. As explained in
Section 2.2, ASR is performed using the method of
McCarthy et al (2004). We use k=8 for the distri-
butional nearest-neighbors to consider when ranking
the senses for a lemma, and we use the synset sim-
ilarity function of Jiang and Conrath (1997), with
default information content counts from NLTK cal-
culated over the British National Corpus11.
9http://www.illc.uva.nl/EuroWordNet/
10http://nlp.lsi.upc.edu/tools/download-map.
php
11http://www.natcorp.ox.ac.uk/
Source Evaluation Set
Vocabulary FTB Eval EMEA Eval Europarl
Nouns
FTB train 95.35 62.87 94.69
Thesaurus 96.25 79.00 97.83
FREWN 80.51 73.09 87.06
Verbs
FTB train 96.54 94.56 97.76
Thesaurus 98.33 97.82 99.54
FREWN 88.32 91.48 91.98
Table 2: Lexical occurrence coverage (%) of source
vocabularies over evaluation sets. FTB Eval contains
both the FTB development and test sets, while EMEA
Eval contains both the EMEA development and test sets.
Proper nouns are excluded from the analysis.
4.3.4 HAC Clustering
For the HAC clustering experiments in this paper,
we use the CLUTO package12. The distributional
thesauri described above are taken as input, and the
UPGMA setting is used for cluster agglomeration.
We test varying levels of clustering, with a parame-
ter z which determines the proportion of cluster vo-
cabulary size with respect to the original vocabulary
size (8,171 for nouns and 2,865 for verbs).
4.3.5 Resource Coverage
The coverage of our lexical resources over the
FTB and two out-of-domain evaluation sets, at the
level of token occurrences of verbs and common
nouns, is described in Table 2. We can see that
the FTB training set vocabulary provides better cov-
erage than the FREWN for both nouns and verbs,
while the coverage of the thesauri (and derived clus-
ters) is the highest overall.
4.4 Tuning and Evaluation
We evaluate four lexical target space configurations
against the baseline of lemmatization, tuning pa-
rameters using ten-fold cross-validation on the FTB
training set. The feature templates are the same as
those in Table 1, with the difference that features
involving lemmas are modified by the probabilistic
feature generalization technique described in Sec-
tion 2.4, using the appropriate categorical distribu-
tions. In all configurations, we exclude the French
auxiliary verbs e?tre and avoir from participation in
lexical generalization, and we replace proper nouns
12http://glaros.dtc.umn.edu/gkhome/cluto/
cluto/download
6
with a special lemma13. Below we describe the
tuned parameters for each configuration.
? RC: Replacement with cluster in ?c
For clusters and the parameter z (cf. Section
4.3.4), we settled on relative cluster vocabulary
size z=0.6 for nouns and z=0.7 for verbs. We
also generalized lemmas not appearing in the
distributional thesaurus into a single unknown
class.
? PKNL: Probabilistic k-nearest lemmas in ?l
For the parameters k and m (cf. Section 2.1),
we settled on k=4 and m=0.5 for both nouns
and verbs. We also use the unknown class for
low-frequency lemmas, as in the RC configura-
tion.
? RS: Replacement with first-sense (k=1) in ?s
Since the FREWN has a lower-coverage vo-
cabulary, we did not use an unknown class for
out-of-vocabulary lemmas; instead, we mapped
them to unique senses. In addition, we did not
perform lexical generalization for verbs, due to
low cross-validation performance.
? PKPS: Probabilistic k-prevalent senses in ?s
For this setting we decided to not place any
limit on k, due to the large variation in the
number of senses for different lemmas. As
in the RS configuration, we mapped out-of-
vocabulary lemmas to unique senses and did
not perform lexical generalization for verbs.
5 Results
Table 3 shows labeled attachment score (LAS) re-
sults for our baseline parser (Lemmas) and four lex-
ical generalization configurations. For comparison,
we also include results for a setting that only uses
word forms (Forms), which was the baseline for pre-
vious work on French dependency parsing (Candito
et al, 2010b). Punctuation tokens are not scored,
and significance is calculated using Dan Bikel?s ran-
domized parsing evaluation comparator14, at signif-
icance level p=0.05.
13Proper nouns tend to have sparse counts, but for computa-
tional reasons we did not include them in our distributional the-
saurus construction. We thus chose to simply generalize them
Parse Evaluation Set LAS
Configuration FTB Test EMEA Dev EMEA Test Europarl
Forms 86.85 84.08 85.41 86.01
Lemmas 87.30 84.34 85.41 86.26
RC 87.32 84.28 85.71* 86.28
PKNL 87.46 84.63* 85.82* 86.26
RS 87.34 84.48 85.54 86.34
PKPS 87.41 84.63* 85.68* 86.22
Table 3: Labeled attachment score (LAS) on in-domain
(FTB) and out-of-domain (EMEA, Europarl) evaluation
sets for the baseline (Lemmas) and four lexical general-
ization configurations (RC, PKNL, RS, PKPS). Signif-
icant improvements over the baseline are starred. For
comparison, we also include a simpler setting (Forms),
which does not use lemmas or morphological features.
5.1 In-Domain Results
Our in-domain evaluation yields slight improve-
ments in LAS for some lexical generalization con-
figurations, with PKNL performing the best. How-
ever, the improvements are not statistically signifi-
cant. A potential explanation for this disappointing
result is that the FTB training set vocabulary cov-
ers the FTB test set at high rates for both nouns
(95.25%) and verbs (96.54%), meaning that lexi-
cal data sparseness is perhaps not a big problem
for in-domain dependency parsing. While WordNet
synsets could be expected to provide the added ben-
efit of taking word sense into account, sense ambi-
guity is not really treated due to ASR not providing
word sense disambiguation in context.
5.2 Out-Of-Domain Results
Our evaluation on the medical domain yields statisti-
cally significant improvements in LAS, particularly
for the two probabilistic target space approaches.
PKNL and PKPS improve parsing for both the
EMEA dev and test sets, while RC improves pars-
ing for only the EMEA test set and RS does not sig-
nificantly improve parsing for either set. As in our
in-domain evaluation, PKNL performs the best over-
all, though not significantly better than other lexi-
cal generalization settings. One explanation for the
improvement in the medical domain is the substan-
tial increase in coverage of nouns in EMEA afforded
into a single class.
14http://www.cis.upenn.edu/
?
dbikel/software.
html
7
by the distributional thesaurus (+26%) and FREWN
(+16%) over the base coverage afforded by the FTB
training set.
Our evaluation on the parliamentary domain
yields no improvement in LAS across the different
lexical generalization configurations. Interestingly,
Candito and Seddah (2012) note that while Europarl
is rather different from FTB in its syntax, its vocabu-
lary is surprisingly similar. From Table 2 we can see
that the FTB training set vocabulary has about the
same high level of coverage over Europarl (94.69%
for nouns and 97.76% for verbs) as it does over the
FTB evaluation sets (95.35% for nouns and 96.54%
for verbs). Thus, we can use the same reasoning as
in our in-domain evaluation to explain the lack of
improvement for lexical generalization methods in
the parliamentary domain.
5.3 Lexical Feature Use During Parsing
Since lexical generalization modifies the lexical fea-
ture space in different ways, we also provide an anal-
ysis of the extent to which each parsing model?s lex-
ical features are used during in-domain and out-of-
domain parsing. Table 4 describes, for each config-
uration, the number of lexical features stored in the
parsing model along with the average lexical fea-
ture use (ALFU) of classification instances (each in-
stance represents a parse transition) during training
and parsing.15
Lexical feature use naturally decreases when
moving from the training set to the evaluation sets,
due to holes in lexical coverage outside of a parsing
model?s training set. The single-mapping configura-
tions (RC, RS) do not increase the number of lexical
features in a classification instance, which explains
the fact that their ALFU on the FTB training set (6.0)
is the same as that of the baseline. However, the de-
crease in ALFU when parsing the evaluation sets is
less severe for these configurations than for the base-
line: when parsing EMEA Dev with the RC configu-
ration, where we obtain a significant LAS improve-
ment over the baseline, the reduction in ALFU is
only 13% compared to 22% for the baseline parser.
For the probabilistic generalization configurations,
we also see decreases in ALFU when parsing the
15We define the lexical feature use of a classification instance
to be the number of lexical features in the parsing model that
receive non-zero values in the instance?s feature vector.
Parse Lexical Feats Average Lexical Feature Use
Configuration In Model FTB Train FTB Dev EMEA Dev
Lemmas 294k 6.0 5.5 4.7
RC 150k 6.0 5.8 5.2
PKNL 853k 15.7 14.8 12.0
RS 253k 6.0 5.6 4.9
PKPS 500k 9.2 8.6 7.0
Table 4: Parsing model lexical features (rounded to near-
est thousand) and average lexical feature use in classifi-
cation instances across different training and evaluation
sets, for the baseline (Lemmas) and four lexical general-
ization configurations (PKNL, RC, PKPS, and RS).
evaluation sets, though their higher absolute ALFU
may help explain the strong medical domain parsing
performance for these configurations.
5.4 Impact on Running Time
Another factor to note when evaluating lexical gen-
eralization is the effect that it has on running time.
Compared to the baseline, the single-mapping con-
figurations (RC, RS) speed up feature extraction and
prediction time, due to reduced dimensionality of
the feature space. On the other hand, the proba-
bilistic generalization configurations (PKNL, PKPS)
slow down feature extraction and prediction time,
due to an increased dimensionality of the feature
space and a higher ALFU. Running time is there-
fore a factor that favors the single-mapping approach
over our proposed probabilistic approach.
Taking a larger view on our findings, we hy-
pothesize that in order for lexical generalization
to improve parsing, an approach needs to achieve
two objectives: (i) generalize sufficiently to ensure
that lemmas not appearing in the training set are
nonetheless associated with lexical features in the
learned parsing model; (ii) substantially increase
lexical coverage over what the training set can pro-
vide. The first of these objectives seems to be ful-
filled through our lexical generalization methods, as
indicated in Table 4. The second objective, how-
ever, seems difficult to attain when parsing text in-
domain, or even out-of-domain if the domains have
a high lexical overlap (as is the case for Europarl).
Only for our parsing experiments in the medical do-
main do both objectives appear to be fulfilled, as
evidenced by our LAS improvements when parsing
EMEA with lexical generalization.
8
6 Related Work
We now discuss previous work concerning the use of
lexical generalization for parsing, both in the classic
in-domain setting and in the more recently popular
out-of-domain setting.
6.1 Results in Constituency-Based Parsing
The use of word classes for parsing dates back to the
first works on generative constituency-based pars-
ing, whether using semantic classes obtained from
hand-built resources or less-informed classes cre-
ated automatically. Bikel (2000) tried incorporat-
ing WordNet-based word sense disambiguation into
a parser, but failed to obtain an improvement. Xiong
et al (2005) generalized bilexical dependencies in
a generative parsing model using Chinese semantic
resources (CiLin and HowNet), obtaining improve-
ments for Chinese parsing. More recently, Agirre
et al (2008) show that replacing words with Word-
Net semantic classes improves English generative
parsing. Lin et al (2009) use the HowNet resource
within the split-merge PCFG framework (Petrov et
al., 2006) for Chinese parsing: they use the first-
sense heuristic to append the most general hyper-
nym to the POS of a token, obtaining a semantically-
informed symbol refinement, and then guide further
symbol splits using the HowNet hierarchy. Other
work has used less-informed classes, notably unsu-
pervised word clusters. Candito and Crabbe? (2009)
use Brown clusters to replace words in a generative
PCFG-LA framework, obtaining substantial parsing
improvements for French.
6.2 Results in Dependency Parsing
In dependency parsing, word classes are integrated
as features in underlying linear models. In a seminal
work, Koo et al (2008) use Brown clusters as fea-
tures in a graph-based parser, improving parsing for
both English and Czech. However, attempts to use
this technique for French have lead to no improve-
ment when compared to the use of lemmatization
and morphological analysis (Candito et al, 2010b).
Sagae and Gordon (2009) augment a transition-
based English parser with clusters using unlexical-
ized syntactic distributional similarity: each word is
represented as a vector of counts of emanating un-
lexicalized syntactic paths, with counts taken from
a corpus of auto-parsed phrase-structure trees, and
HAC clustering is performed using cosine similarity.
For semantic word classes, (Agirre et al, 2011) inte-
grate WordNet senses into a transition-based parser
for English, reporting small but significant improve-
ments in LAS (+0.26% with synsets and +0.36%
with semantic files) on the full Penn Treebank with
first-sense information from Semcor.
We build on previous work by attempting to
reproduce, for French, past improvements for in-
domain English dependency parsing with general-
ized lexical classes. Unfortunately, our results for
French do not replicate the improvements for En-
glish using semantic sense information (Agirre et al,
2011) or word clustering (Sagae and Gordon, 2009).
The primary difference between our paper and previ-
ous work, though, is our evaluation of a novel prob-
abilistic approach for lexical generalization.
6.3 Out-Of-Domain Parsing
Concerning techniques for improving out-of-
domain parsing, a related approach has been to use
self-training with auto-parsed out-of-domain data,
as McClosky and Charniak (2008) do for English
constituency parsing, though in that approach
lexical generalization is not explicitly performed.
Candito et al (2011) use word clustering for do-
main adaptation of a PCFG-LA parser for French,
deriving clusters from a corpus containing text
from both the source and target domains, and they
obtain parsing improvements in both domains.
We are not aware of previous work on the use of
lexical generalization for improving out-of-domain
dependency parsing.
7 Conclusion
We have investigated the use of probabilistic lexi-
cal target spaces for reducing lexical data sparse-
ness in a transition-based dependency parser for
French. We built a distributional thesaurus from an
automatically-parsed large text corpus, using it to
generate word clusters and perform WordNet ASR.
We tested a standard approach to lexical gener-
alization for parsing that has been previously ex-
plored, where a word is mapped to a single cluster
or synset. We also introduced a novel probabilis-
tic lexical generalization approach, where a lemma
9
is represented by a categorical distribution over the
space of lemmas, clusters, or synsets. Probabilities
for the lemma space were calculated using the dis-
tributional thesaurus, and probabilities for the Word-
Net synset space were calculated using ASR sense
prevalence scores, with probabilistic clusters left for
future work.
Our experiments with an arc-eager transition-
based dependency parser resulted in modest but sig-
nificant improvements in LAS over the baseline
when parsing out-of-domain medical text. However,
we did not see statistically significant improvements
over the baseline when parsing in-domain text or
out-of-domain parliamentary text. An explanation
for this result is that the French Treebank training set
vocabulary has a very high lexical coverage over the
evaluation sets in these domains, suggesting that lex-
ical generalization does not provide much additional
benefit. Comparing the standard single-mapping ap-
proach to the probabilistic generalization approach,
we found a slightly (though not significantly) better
performance for probabilistic generalization across
different parsing configurations and evaluation sets.
However, the probabilistic approach also has the
downside of a slower running time.
Based on the findings in this paper, our focus
for future work on lexical generalization for de-
pendency parsing is to continue improving parsing
performance on out-of-domain text, specifically for
those domains where lexical variation is high with
respect to the training set. One possibility is to
experiment with building a distributional thesaurus
that uses text from both the source and target do-
mains, similar to what Candito et al (2011) did
with Brown clustering, which may lead to a stronger
bridging effect across domains for probabilistic lex-
ical generalization methods.
Acknowledgments
This work was funded in part by the ANR project
Sequoia ANR-08-EMER-013.
References
A. Abeille? and N. Barrier. 2004. Enriching a French tree-
bank. In Proceedings of the 4th International Confer-
ence on Language Resources and Evaluation, Lisbon,
Portugal, May.
E. Agirre, T. Baldwin, and D. Martinez. 2008. Improv-
ing parsing and PP attachment performance with sense
information. In Proceedings of the 46th Annual Meet-
ing of the Association for Computational Linguistics,
pages 317?325, Columbus, Ohio, June.
E. Agirre, K. Bengoetxea, K. Gojenola, and J. Nivre.
2011. Improving dependency parsing with semantic
classes. In Proceedings of the 49th Annual Meeting of
the Association for Computational Linguistics, pages
699?703, Portland, Oregon, June.
D.M. Bikel. 2000. A statistical model for parsing and
word-sense disambiguation. In Proceedings of the
EMNLP/VLC-2000, pages 155?163, Hong Kong, Oc-
tober.
S. Bird, E. Loper, and E. Klein. 2009. Natural Language
Processing with Python. O?Reilly Media Inc.
P.F. Brown, P.V. Desouza, R.L. Mercer, V.J.D. Pietra, and
J.C. Lai. 1992. Class-based n-gram models of natural
language. Computational Linguistics, 18(4):467?479.
R.C. Bunescu. 2008. Learning with probabilistic fea-
tures for improved pipeline models. In Proceedings of
the Conference on Empirical Methods in Natural Lan-
guage Processing, pages 670?679, Honolulu, Hawaii,
October.
M. Candito and B. Crabbe?. 2009. Improving generative
statistical parsing with semi-supervised word cluster-
ing. In Proceedings of the 11th International Confer-
ence on Parsing Technologies, pages 138?141, Paris,
France, October.
M. Candito and D. Seddah. 2012. Le corpus Sequoia :
annotation syntaxique et exploitation pour l?adaptation
d?analyseur par pont lexical. In Actes de la 19e`me
confe?rence sur le traitement automatique des langues
naturelles, Grenoble, France, June. To Appear.
M. Candito, B. Crabbe?, and P. Denis. 2010a. Statistical
French dependency parsing: Treebank conversion and
first results. In Proceedings of the 7th International
Conference on Language Resources and Evaluation,
Valetta, Malta, May.
M. Candito, J. Nivre, P. Denis, and E. Henestroza An-
guiano. 2010b. Benchmarking of statistical depen-
dency parsers for French. In Proceedings of the 23rd
International Conference on Computational Linguis-
tics, pages 108?116, Beijing, China, August.
M. Candito, E. Henestroza Anguiano, D. Seddah, et al
2011. A Word Clustering Approach to Domain Adap-
tation: Effective Parsing of Biomedical Texts. In Pro-
ceedings of the 12th International Conference on Pars-
ing Technologies, Dublin, Ireland, October.
J.R. Curran. 2004. From distributional to semantic simi-
larity. Ph.D. thesis, University of Edinburgh.
P. Denis and B. Sagot. 2009. Coupling an annotated cor-
pus and a morphosyntactic lexicon for state-of-the-art
10
POS tagging with less human effort. In Proceedings
of the 23rd Pacific Asia Conference on Language, In-
formation and Computation, Hong Kong, China, De-
cember.
C. Fellbaum, editor. 1998. WordNet: An Electronic Lex-
ical Database. MIT Press, Cambridge, MA.
E. Henestroza Anguiano and P. Denis. 2011. FreDist:
Automatic construction of distributional thesauri for
French. In Actes de la 18e`me confe?rence sur le traite-
ment automatique des langues naturelles, pages 119?
124, Montpellier, France, June.
J.J. Jiang and D.W. Conrath. 1997. Semantic similarity
based on corpus statistics and lexical taxonomy. In In-
ternational Conference on Research in Computational
Linguistics, Taiwan.
T. Koo, X. Carreras, and M. Collins. 2008. Simple semi-
supervised dependency parsing. In Proceedings of the
46th Annual Meeting of the Association for Compu-
tational Linguistics, pages 595?603, Columbus, Ohio,
June.
X. Lin, Y. Fan, M. Zhang, X. Wu, and H. Chi. 2009. Re-
fining grammars for parsing with hierarchical semantic
knowledge. In Proceedings of the 2009 Conference on
Empirical Methods in Natural Language Processing,
pages 1298?1307, Singapore, August.
D. Lin. 1998. Automatic retrieval and clustering of simi-
lar words. In Proceedings of the 36th Annual Meeting
of the Association for Computational Linguistics and
17th International Conference on Computational Lin-
guistics, Volume 2, pages 768?774, Montreal, Quebec,
August.
D. McCarthy, R. Koeling, J. Weeds, and J. Carroll. 2004.
Finding predominant word senses in untagged text.
In Proceedings of the 42nd Meeting of the Associa-
tion for Computational Linguistics, pages 279?286,
Barcelona, Spain, July.
D. McClosky and E. Charniak. 2008. Self-training for
biomedical parsing. In Proceedings of the 46th Annual
Meeting of the Association for Computational Linguis-
tics, pages 101?104, Columbus, Ohio, June.
J. Nivre, J. Hall, J. Nilsson, G. Eryi it, and S. Marinov.
2006. Labeled pseudo-projective dependency pars-
ing with support vector machines. In Proceedings of
the Tenth Conference on Computational Natural Lan-
guage Learning, pages 221?225, New York City, NY,
June.
J. Nivre, J. Hall, J. Nilsson, A. Chanev, G. Eryigit,
S. Ku?bler, S. Marinov, and E. Marsi. 2007. Malt-
Parser: A language-independent system for data-
driven dependency parsing. Natural Language Engi-
neering, 13(02):95?135.
J. Nivre. 2003. An efficient algorithm for projective de-
pendency parsing. In Proceedings of the 8th Interna-
tional Workshop on Parsing Technologies, pages 149?
160, Nancy, France, April.
S. Petrov, L. Barrett, R. Thibaux, and D. Klein. 2006.
Learning accurate, compact, and interpretable tree an-
notation. In Proceedings of the 21st International
Conference on Computational Linguistics and the 44th
annual meeting of the Association for Computational
Linguistics, pages 433?440, Sydney, Australia, July.
K. Sagae and A. Gordon. 2009. Clustering words by
syntactic similarity improves dependency parsing of
predicate-argument structures. In Proceedings of the
11th International Conference on Parsing Technolo-
gies, pages 192?201, Paris, France, October.
B. Sagot. 2010. The Lefff, a freely available, accurate
and large-coverage lexicon for French. In Proceed-
ings of the 7th International Conference on Language
Resources and Evaluation, Valetta, Malta, May.
J. Tiedemann. 2009. News from OPUS - A collection of
multilingual parallel corpora with tools and interfaces.
In Recent Advances in Natural Language Processing,
volume 5, pages 237?248. John Benjamins, Amster-
dam.
D. Xiong, S. Li, Q. Liu, S. Lin, and Y. Qian. 2005. Pars-
ing the penn chinese treebank with semantic knowl-
edge. In Proceedings of the International Joint Con-
ference on Natural Language Processing, pages 70?
81, Jeju Island, Korea, October.
H. Yamada and Y. Matsumoto. 2003. Statistical depen-
dency analysis with support vector machines. In Pro-
ceedings of the 8th International Workshop on Parsing
Technologies, pages 195?206, Nancy, France, April.
11
