Proceedings of NAACL HLT 2009: Short Papers, pages 53?56,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
Towards Natural Language Understanding of Partial Speech Recognition
Results in Dialogue Systems
Kenji Sagae and Gwen Christian and David DeVault and David R. Traum
Institute for Creative Technologies, University of Southern California
13274 Fiji Way, Marina del Rey, CA 90292
{sagae,gchristian,devault,traum}@ict.usc.edu
Abstract
We investigate natural language understand-
ing of partial speech recognition results to
equip a dialogue system with incremental lan-
guage processing capabilities for more realis-
tic human-computer conversations. We show
that relatively high accuracy can be achieved
in understanding of spontaneous utterances
before utterances are completed.
1 Introduction
Most spoken dialogue systems wait until the user
stops speaking before trying to understand and re-
act to what the user is saying. In particular, in a
typical dialogue system pipeline, it is only once the
user?s spoken utterance is complete that the results
of automatic speech recognition (ASR) are sent on
to natural language understanding (NLU) and dia-
logue management, which then triggers generation
and synthesis of the next system prompt. While
this style of interaction is adequate for some appli-
cations, it enforces a rigid pacing that can be un-
natural and inefficient for mixed-initiative dialogue.
To achieve more flexible turn-taking with human
users, for whom turn-taking and feedback at the sub-
utterance level is natural and common, the system
needs to engage in incremental processing, in which
interpretation components are activated, and in some
cases decisions are made, before the user utterance
is complete.
There is a growing body of work on incremen-
tal processing in dialogue systems. Some of this
work has demonstrated overall improvements in sys-
tem responsiveness and user satisfaction; e.g. (Aist
et al, 2007; Skantze and Schlangen, 2009). Several
research groups, inspired by psycholinguistic mod-
els of human processing, have also been exploring
technical frameworks that allow diverse contextual
information to be brought to bear during incremen-
tal processing; e.g. (Kruijff et al, 2007; Aist et al,
2007).
While this work often assumes or suggests it is
possible for systems to understand partial user ut-
terances, this premise has generally not been given
detailed quantitative study. The contribution of this
paper is to demonstrate and explore quantitatively
the extent to which one specific dialogue system can
anticipate what an utterance means, on the basis of
partial ASR results, before the utterance is complete.
2 NLU for spontaneous spoken utterances
in a dialogue system
For this initial effort, we chose to look at incremental
processing of natural language understanding in the
SASO-EN system (Traum et al, 2008), a complex
spoken dialog system for which we have a corpus
of user data that includes recorded speech files that
have been transcribed and annotated with a semantic
representation. The domain of this system is a nego-
tiation scenario involving the location of a medical
clinic in a foreign country. The system is intended as
a negotiation training tool, where users learn about
negotiation tactics in the context of the culture and
social norms of a particular community.
2.1 The natural language understanding task
The NLU module must take the output of ASR as
input, and produce domain-specific semantic frames
as output. These frames are intended to capture
much of the meaning of the utterance, although a
53
dialogue manager further enriches the frame rep-
resentations with pragmatic information (Traum,
2003). NLU output frames are attribute-value ma-
trices, where the attributes and values are linked to a
domain-specific ontology and task model.
Complicating the NLU task of is the relatively
high word error rate (0.54) in ASR of user speech
input, given conversational speech in a complex do-
main and an untrained broad user population.
The following example, where the user attempts
to address complaints about lack of power in the pro-
posed location for the clinic, illustrates an utterance-
frame pair.
? Utterance (speech): we are prepared to give
you guys generators for electricity downtown
? ASR (NLU input): we up apparently give you
guys generators for a letter city don town
? Frame (NLU output):
<s>.mood declarative
<s>.sem.agent kirk
<s>.sem.event deliver
<s>.sem.modal.possibility can
<s>.sem.speechact.type offer
<s>.sem.theme power-generator
<s>.sem.type event
The original NLU component for this system was
described in (Leuski and Traum, 2008). For the pur-
poses of this experiment, we have developed a new
NLU module and tested on several different data
sets as described in the next section. Our approach
is to use maximum entropy models (Berger et al,
1996) to learn a suitable mapping from features de-
rived from the words in the ASR output to semantic
frames. Given a set of examples of semantic frames
with corresponding ASR output, a classifier should
learn, for example, that when ?generators? appears
in the output of ASR, the value power-generators is
likely to be present in the output frame. The specific
features used by the classifier are: each word in the
input string (bag-of-words representation of the in-
put), each bigram (consecutive words), each pair of
any two words in the input, and the number of words
in the input string.
0102030405060 1
23
45
67
89
1011
1213
1415
1617
1819
2021
2223
24
Length
 
n (word
s)
Number of utterances (bars)
050100150200250300350400450
Cumulative number of 
utterances (line)
Exact
ly n wo
rds
At mo
st n w
ords
Figure 1: Length of utterances in the development set.
2.2 Data
Our corpus consists of 4,500 user utterances spread
across a number of different dialogue sessions. Ut-
terances that were out-of-domain (13.7% of the cor-
pus) were assigned a ?garbage? frame, with no se-
mantic content. Approximately 10% of the utter-
ances were set aside for final testing, and another
10% was designated the development corpus for the
NLU module. The development and test sets were
chosen so that all the utterances in a session were
kept in the same set, but sessions were chosen at ran-
dom for inclusion in the development and test sets.
The training set contains 136 distinct frames,
each of which is composed of several attribute-value
pairs, called frame elements. Figure 1 shows the ut-
terance length distribution in the development set.
2.3 NLU results on complete ASR output
To evaluate NLU results, we look at precision, re-
call and f-score of frame elements. When the NLU
module is trained on complete ASR utterances in
the training set, and tested on complete ASR utter-
ances in the development set, f-score of frame ele-
ments is 0.76, with precision at 0.78 and recall at
0.74. To gain insight on what the upperbound on
the accuracy of the NLU module might be, we also
trained the classifier using features extracted from
gold-standard manual transcription (instead of ASR
output), and tested the accuracy of analyses of gold-
standard transcriptions (which would not be avail-
able at run-time in the dialogue system). Under
these ideal conditions, NLU f-score is 0.87. Training
on gold-standard transcriptions and testing on ASR
output produces results with a lower f-score, 0.74.
54
3 NLU on partial ASR results
Roughly half of the utterances in our training data
contain six words or more, and the average utter-
ance length is 5.9 words. Since the ASR module is
capable of sending partial results to the NLU mod-
ule even before the user has finished an utterance, in
principle the dialogue system can start understand-
ing and even responding to user input as soon as
enough words have been uttered to give the system
some indication of what the user means, or even
what the user will have said once the utterance is
completed. To measure the extent to which our NLU
module can predict the frame for an input utterance
when it sees only a partial ASR result with the first
n words, we examine two aspects of NLU with par-
tial ASR results. The first is correctness of the NLU
output with partial ASR results of varying lengths, if
we take the gold-standard manual annotation for the
entire utterance as the correct frame for any of the
partial ASR results for that utterance. The second is
stability: how similar the NLU output with partial
ASR results of varying lengths is to what the NLU
result would have been for the entire utterance.
3.1 Training the NLU module for analysis of
partial ASR results
The simplest way to performNLU of partial ASR re-
sults is simply to process the partial utterances using
the NLU module trained on complete ASR output.
However, better results may be obtained by train-
ing separate NLU models for analysis of partial ut-
terances of different lengths. To train these sepa-
rate NLU models, we first ran the audio of the utter-
ances in the training data through our ASR module,
recording all partial results for each utterance. Then,
to train a model to analyze partial utterances con-
taining n words, we used only partial utterances in
the training set containing n words (unless the entire
utterance contained less than n words, in which case
we simply used the complete utterance). In some
cases, multiple partial ASR results for a single utter-
ance contained the same number of words, and we
used the last partial result with the appropriate num-
ber of words 1. We trained separate NLU models for
1At run-time, this can be closely approximated by taking
the partial utterance immediately preceding the first partial ut-
terance of length n+ 1.
01020304050607080
1
2
3
4
5
6
7
8
9
10
all
Length
 
n (word
s)
F-score
Traine
d on a
ll data
Traine
d on p
artials
 up to
length
 
n
Traine
d on p
artials
 up to
length
 
n + c
ontex
t
Figure 2: Correctness for three NLU models on partial
ASR results up to n words.
n varying from one to ten.
3.2 Results
Figure 2 shows the f-score for frames obtained by
processing partial ASR results up to length n using
three NLU models. The dashed line is our baseline
NLU model, trained on complete utterances only
(model 1). The solid line shows the results obtained
with length-specific NLU models (model 2), and the
dotted line shows results for length-specific models
that also use features that capture dialogue context
(model 3). Models 1 and 2 are described in the previ-
ous sections. The additional features used in model
3 are unigram and bigram word features extracted
from the most recent system utterance.
As seen in Figure 2, there is a clear benefit to
training NLU models specifically tailored for partial
ASR results. Training a model on partial utterances
with four or five words allows for relatively high f-
score of frame elements (0.67 and 0.71, respectively,
compared to 0.58 and 0.66 when the same partial
ASR results are analyzed using model 1). Consider-
ing that half of the utterances are expected to have
more than five words (based on the length of the ut-
terances in the training set), allowing the system to
start processing user input when four or five-word
partial ASR results are available provides interesting
opportunities. Targeting partial results with seven
words or more is less productive, since the time sav-
ings are reduced, and the gain in accuracy is modest.
The context features used in model 3 did not pro-
vide substantial benefits in NLU accuracy. It is pos-
55
0102030405060708090100
1
2
3
4
5
6
7
8
9
10
Length
 
n of pa
rtial A
SR ou
tput us
ed in m
odel 2
Stability F-score
Figure 3: Stability of NLU results for partial ASR results
up to length n.
sible that other ways of representing context or di-
alogue state may be more effective. This is an area
we are currently investigating.
Finally, figure 3 shows the stability of NLU re-
sults produced by model 2 for partial ASR utter-
ances of varying lengths. This is intended to be an
indication of how much the frame assigned to a par-
tial utterance differs from the ultimate NLU output
for the entire utterance. This ultimate NLU output
is the frame assigned by model 1 for the complete
utterance. Stability is then measured as the F-score
between the output of model 2 for a particular partial
utterance, and the output of model 1 for the corre-
sponding complete utterance. A stability F-score of
1.0 would mean that the frame produced for the par-
tial utterance is identical to the frame produced for
the entire utterance. Lower values indicate that the
frame assigned to a partial utterance is revised sig-
nificantly when the entire input is available. As ex-
pected, the frames produced by model 2 for partial
utterances with at least eight words match closely
the frames produced by model 1 for the complete ut-
terances. Although the frames for partial utterances
of length six are almost as accurate as the frames for
the complete utterances (figure 2), figure 3 indicates
that these frames are still often revised once the en-
tire input utterance is available.
4 Conclusion
We have presented experiments that show that it
is possible to obtain domain-specific semantic rep-
resentations of spontaneous speech utterances with
reasonable accuracy before automatic speech recog-
nition of the utterances is completed. This allows for
interesting opportunities in dialogue systems, such
as agents that can interrupt the user, or even finish
the user?s sentence. Having an estimate of the cor-
rectness and stability of NLU results obtained with
partial utterances allows the dialogue system to es-
timate how likely its initial interpretation of an user
utterance is to be correct, or at least agree with its
ultimate interpretation. We are currently working on
the extensions to the NLU model that will allow for
the use of different types of context features, and in-
vestigating interesting ways in which agents can take
advantage of early interpretations.
Acknowledgments
The work described here has been sponsored by the
U.S. Army Research, Development, and Engineer-
ing Command (RDECOM). Statements and opin-
ions expressed do not necessarily reflect the position
or the policy of the United States Government, and
no official endorsement should be inferred.
References
G. Aist, J. Allen, E. Campana, C. G. Gallo, S. Stoness,
M. Swift, and M. K. Tanenhaus. 2007. Incremental
dialogue system faster than and preferred to its non-
incremental counterpart. In Proc. of the 29th Annual
Conference of the Cognitive Science Society.
A. L. Berger, S. A. Della Pietra, and V. J. Della Pietra.
1996. A maximum entropy approach to natural
language processing. Computational Linguistics,
22(1):39?71.
G. J. Kruijff, P. Lison, T. Benjamin, H. Jacobsson, and
N. Hawes. 2007. Incremental, multi-level processing
for comprehending situated dialogue in human-robot
interaction. In Language and Robots: Proc. from the
Symposium (LangRo?2007). University of Aveiro, 12.
A. Leuski and D. Traum. 2008. A statistical approach
for text processing in virtual humans. In 26th Army
Science Conference.
G. Skantze and D. Schlangen. 2009. Incremental dia-
logue processing in a micro-domain. In Proc. of the
12th Conference of the European Chapter of the ACL.
D. Traum, S. Marsella, J. Gratch, J. Lee, and A. Hartholt.
2008. Multi-party, multi-issue, multi-strategy negotia-
tion for multi-modal virtual agents. In Proc. of Intelli-
gent Virtual Agents Conference IVA-2008.
D. Traum. 2003. Semantics and pragmatics of ques-
tions and answers for dialogue agents. In Proc. of the
International Workshop on Computational Semantics,
pages 380?394, January.
56
Proceedings of the ACL Interactive Poster and Demonstration Sessions,
pages 89?92, Ann Arbor, June 2005. c?2005 Association for Computational Linguistics
Transonics: A Practical Speech-to-Speech Translator for English-Farsi
Medical Dialogues
Emil Ettelaie, Sudeep Gandhe, Panayiotis Georgiou,
Kevin Knight, Daniel Marcu, Shrikanth Narayanan ,
David Traum
University of Southern California
Los Angeles, CA 90089
ettelaie@isi.edu, gandhe@ict.usc.edu,
georgiou@sipi.usc.edu, knight@isi.edu,
marcu@isi.edu, shri@sipi.usc.edu,
traum@ict.use.edu
Robert Belvin
HRL Laboratories, LLC
3011 Malibu Canyon Rd.
Malibu, CA 90265
rsbelvin@hrl.com
Abstract
We briefly describe a two-way speech-to-
speech English-Farsi translation system
prototype developed for use in doctor-
patient interactions.  The overarching
philosophy of the developers has been to
create a system that enables effective
communication, rather than focusing on
maximizing component-level perform-
ance.  The discussion focuses on the gen-
eral approach and evaluation of the
system by an independent government
evaluation team.
1 Introduction
In this paper we give a brief description of a
two-way speech-to-speech translation system,
which was created under a collaborative effort
between three organizations within USC (the
Speech Analysis and Interpretation Lab of the
Electrical Engineering department, the Information
Sciences Institute, and the Institute for Creative
Technologies) and the Information Sciences Lab of
HRL Laboratories.  The system is intended to pro-
vide a means of enabling communication between
monolingual English speakers and monolingual
Farsi (Persian) speakers.  The system is targeted at
a domain which may be roughly characterized as
"urgent care" medical interactions, where the Eng-
lish speaker is a medical professional and the Farsi
speaker is the patient.  In addition to providing a
brief description of the system (and pointers to pa-
pers which contain more detailed information), we
give an overview of the major system evaluation
activities.
2 General Design of the system
Our system is comprised of seven speech and
language processing components, as shown in Fig.
1. Modules communicate using a centralized mes-
sage-passing system. The individual subsystems
are the Automatic Speech Recognition (ASR) sub-
system, which uses n-gram Language Models
(LM) and produces n-best lists/lattices along with
the decoding confidence scores. The output of the
ASR is sent to the Dialog Manager (DM), which
displays the n-best and passes one hypothesis on to
the translation modules, according to a user-
configurable state. The DM sends translation re-
quests to the Machine Translation (MT) unit. The
MT unit works in two modes: Classifier based MT
and a fully Stochastic MT. Depending on the dia-
logue manager mode, translations can be sent to
the unit selection based Text-To-Speech synthe-
sizer (TTS), to provide the spoken output. The
same basic pipeline works in both directions: Eng-
lish ASR, English-Persian MT, Persian TTS, or
Persian ASR, Persian-English MT, English TTS.
There is, however, an asymmetry in the dia-
logue management and control, given the desire for
the English-speaking doctor to be in control of the
device and the primary "director" of the dialog.
The English ASR used the University of Colo-
rado Sonic recognizer, augmented primarily with
LM data collected from multiple sources, including
89
our own large-scale simulated doctor-patient dia-
logue corpus based on recordings of medical stu-
dents examining standardized patients (details in
Belvin et al 2004).
1
 The Farsi acoustic models r e-
quired an eclectic approach due to the lack of ex-
isting labeled speech corpora.  The approach
included borrowing acoustic data from English by
means of developing a sub-phonetic mapping be-
tween the two languages, as detailed in (Srini-
vasamurthy & Narayanan 2003), as well as use of
a small existing Farsi speech corpus (FARSDAT),
and our own team-internally generated acoustic
data.  Language modeling data was also obtained
from multiple sources.  The Defense Language
Institute translated approximately 600,000 words
of English medical dialogue data (including our
standardized patient data mentioned above), and in
addition, we were able to obtain usable Farsi text
from mining the web for electronic news sources.
Other  smaller  amounts of  training  data  were ob
tained from various sources, as detailed in  (Nara-
yanan et al 2003, 2004).  Additional detail on de-
velopment methods for all of these components,
system integration and evaluation can also be
found in the papers just cited.
The MT components, as noted, consist of both a
Classifier and a stochastic translation engine,  both
                                                           
1
 Standardized Patients are typically actors who have been
trained by doctors or nurses to portray symptoms of particular
illnesses or injuries.  They are used extensively in medical
education so that doctors in training don't have to "practice"
on real patients.
developed by USC-ISI team members.  The Eng-
lish Classifier uses approximately 1400 classes
consisting mostly of standard questions used by
medical care providers in medical interviews.
Each class has a large number of paraphrases asso-
ciated with it, such that if the care provider speaks
one of those phrases, the system will identify it
with the class and translate it to Farsi via table-
lookup.  If the Classifier cannot succeed in finding
a match exceeding a confidence threshold, the sto-
chastic MT engine will be employed.  The sto-
chastic MT engine relies on n-gram
correspondences between the source and target
languages.  As with ASR, the performance of the
component is highly dependent on very large
amounts of training data.  Again, there were multi-
ple sources of training data used, the most signifi-
cant being the data generated by our own team's
English collection effort, supported by translation
into Farsi by DLI. Further details of the MT com-
ponents can be found in Narayanan et al, op.cit.
3 Enabling Effective Communication
The approach taken in the development of Tran-
sonics was what can be referred to as the total
communication pathway.  We are not so concerned
with trying to maximize the performance of a
given component of the system, but rather with the
effectiveness of the system as a whole in facilitat-
ing actual communication.  To this end, our design
and development included the following:
MT
English to Farsi
Farsi to English
ASR
English
Prompts or TTS
Farsi
Prompts or TTS
English
ASR
Farsi
GUI:
prompts,
 confirmations,
 ASR switch
Dialog
Manager
SMT
English to Farsi
Farsi to English
Figure 1: Architecture of the Transonics system.  The Dialogue Manager acts as the hub through which the
individual components interact.
90
i. an "educated guess" capability (system
guessing at the meaning of an utterance) from the
Classifier translation mechanism?this proved very
useful for noisy ASR output, especially for the re-
stricted domain of medical interviews.
ii. a flexible and robust SMT good for filling in
where the more accurate Classifier misses.
iii. exploitation of a partial n-best list as part of
the GUI used by the doctor/medic for the English
ASR component and the Farsi-to-English transla-
tion component.
iv. a dialog manager which in essence occa-
sionally makes  "suggestions" (for next questions
for the doctor to ask) based on query sets which are
topically related to the query the system believes it
recognized the doctor to have spoken.
Overall, the system achieves a respectable level of
performance in terms of allowing users to follow a
conversational thread in a fairly coherent way, de-
spite the presence of frequent ungrammatical or
awkward translations (i.e. despite what we might
call non-catastrophic errors).
4 Testing and Evaluation
In addition to our own laboratory tests, the sys-
tem was evaluated by MITRE as part of the
DARPA program.  There were two parts to the
MITRE evaluations, a "live" part, designed pri-
marily to evaluate the overall task-oriented effec-
tiveness of the systems, and a "canned" part,
designed primarily to evaluate individual compo-
nents of the systems.
The live evaluation consisted of six medical
professionals (doctors, corpsmen and physician?s
assistants from the Naval Medical Center at Quan-
tico, and a nurse from a civilian institution) con-
ducting unrehearsed "focused history and physical
exam" style interactions with Farsi speakers play-
ing the role of patients, where the English-speaking
doctor and the Farsi-speaking patient communi-
cated by means of the Transonics system.  Since
the cases were common enough to be within the
realm of general internal medicine, there was no
attempt to align ailments with medical specializa-
tions among the medical professionals.
MITRE endeavored to find primarily monolin-
gual Farsi speakers to play the role of patient, so as
to provide a true test of the system to enable com-
munication between people who would otherwise
have no way to communicate.  This goal was only
partially realized, since one of the two Farsi patient
role-players was partially competent in English.
2
The Farsi-speaking role-players were trained by a
medical education specialist in how to simulate
symptoms of someone with particular injuries or
illnesses.  Each Farsi-speaking patient role-player
received approximately 30 minutes of training for
any given illness or injury.  The approach was
similar to that used in training standardized pa-
tients, mentioned above (footnote 1) in connection
with generation of the dialogue corpus.
MITRE established a number of their own met-
rics for measuring the success of the systems, as
well as using previously established metrics.  A
full discussion of these metrics and the results ob-
tained for the Transonics system is beyond the
scope of this paper, though we will note that one of
the most important of these was task-completion.
There were 5 significant facts (5 distinct facts for
each of 12 different scenarios) that the medical
professional should have discovered in the process
of interviewing/examining each Farsi patient.  The
USC/HRL system averaged 3 out of the 5 facts,
which was a slightly above-average score among
the 4 systems evaluated.  A "significant fact" con-
sisted of determining a fact which was critical for
diagnosis, such as the fact that the patient had been
injured in a fall down a stairway, the fact that the
patient was experiencing blurred vision, and so on.
Significant facts did not include items such as a
patient's age or marital status.
3
  We report on this
measure in that it is perhaps the single most im-
portant component in the assessment, in our opin-
ion, in that it is an indication of many aspects of
the system, including both directions of the trans-
lation system.  That is, the doctor will very likely
conclude correct findings only if his/her question is
translated correctly to the patient, and also if the
patient's answer is translated correctly for the doc-
tor.  In a true medical exam, the doctor may have
                                                           
2
 There were additional difficulties encountered as well, hav-
ing to do with one of the role-players not adequately grasping
the goal of role-playing.  This experience highlighted the
many challenges inherent in simulating domain-specific
spontaneous dialogue.
3
 Unfortunately, there was no baseline evaluation this could be
compared to,  such as assessing whether any of the critical
facts could be determined without the use of the system at all.
91
other means of determining some critical facts
even in the absence of verbal communication, but
in the role-playing scenario described, this is very
unlikely.  Although this measure is admittedly
coarse-grained, it simultaneously shows, in a crude
sense, that the USC/HRL system compared fa-
vorably against the other 3 systems in the evalua-
tion, and also that there is still significant room for
improvement in the state of the art.
As noted, MITRE devised a component evalua-
tion process also consisting of running 5 scripted
dialogs through the systems and then measuring
ASR and MT performance.  The two primary
component measures were a version of BLEU for
the MT component (modified slightly to handle the
much shorter sentences typical of this kind of dia-
log) and a standard Word-Error Rate for the ASR
output.  These scores are shown below.
Table 1:  Farsi BLEU Scores
IBM BLEU
ASR
IBM BLEU
TEXT
English to Farsi
0.2664 0.3059
Farsi  to English 0.2402 0.2935
The reason for the two different BLEU scores is
that one was calculated based on the ASR compo-
nent output being translated to the other language,
while the other was calculated from human tran-
scribed text being translated to the other language.
Table 2:  HRL/USC WER for Farsi and English
English Farsi
WER 11.5% 13.4%
5 Conclusion
In this paper we have given an overview of the
design, implementation and evaluation of the Tran-
sonics speech-to-speech translation system for nar-
row domain two-way translation.  Although there
are still many significant hurdles to be overcome
before this kind of technology can be called truly
robust, with appropriate training and two coopera-
tive interlocutors, we can now see some degree of
genuine communication being enabled.  And this is
very encouraging indeed.
6 Acknowledgements
This work was supported primarily by the DARPA
CAST/Babylon program, contract N66001-02-C-
6023.
References
R. Belvin, W. May, S. Narayanan, P. Georgiou, S. Gan-
javi.  2004. Creation of a Doctor-Patient Dialogue
Corpus Using Standardized Patients. In Proceedings of
the Language Resources and Evaluation Conference
(LREC), Lisbon, Portugal.
S. Ganjavi, P. G. Georgiou, and S. Narayanan. 2003.
Ascii based transcription schemes for languages with
the Arabic script: The case of Persian. In Proc. IEEE
ASRU,  St. Thomas, U.S. Virgin Islands.
S. Narayanan, S. Ananthakrishnan, R. Belvin, E. Ette-
laie, S. Ganjavi, P. Georgiou, C. Hein, S. Kadambe,
K. Knight, D. Marcu, H. Neely, N. Srinivasamurthy,
D. Traum and D. Wang.  2003. Transonics: A speech
to speech system for English-Persian Interactions,
Proc. IEEE ASRU,  St. Thomas, U.S. Virgin Islands.
S. Narayanan, S. Ananthakrishnan, R. Belvin, E. Ette-
laie, S. Gandhe, S. Ganjavi, P. G. Georgiou, C. M.
Hein, S. Kadambe, K. Knight, D. Marcu, H. E.
Neely, N. Srinivasamurthy, D. Traum, and D. Wang.
2004. The Transonics Spoken Dialogue Translator:
An aid for English-Persian Doctor-Patient interviews,
in Working Notes of the AAAI Fall symposium on
Dialogue Systems for Health Communication, pp 97-
-103.
N. Srinivasamurthy, and S. Narayanan. 2003. Language
adaptive Persian speech recognition. In proceedings
of Eurospeech 2003.
92
Proceedings of the 7th SIGdial Workshop on Discourse and Dialogue, pages 18?27,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Building Effective Question Answering Characters
Anton Leuski and Ronakkumar Patel and David Traum
Institute for Creative Technologies
University of Southern California
Marina del Rey, CA, 90292, USA
leuski,ronakkup,traum@ict.usc.edu
Brandon Kennedy
Brandon.Kennedy@usma.edu
Abstract
In this paper, we describe methods for
building and evaluation of limited do-
main question-answering characters. Sev-
eral classification techniques are tested, in-
cluding text classification using support
vector machines, language-model based
retrieval, and cross-language information
retrieval techniques, with the latter having
the highest success rate. We also evalu-
ated the effect of speech recognition errors
on performance with users, finding that re-
trieval is robust until recognition reaches
over 50% WER.
1 Introduction
In the recent Hollywood movie ?iRobot? set in
2035 the main character played by Will Smith is
running an investigation into the death of an old
friend. The detective finds a small device that
projects a holographic image of the deceased. The
device delivers a recorded message and responds
to questions by playing back prerecorded answers.
We are developing virtual characters with similar
capabilities.
Our target applications for these virtual charac-
ters are training, education, and entertainment. For
use in education, such a character should be able
to deliver a message to the student on a specific
topic. It also should be able to support a basic spo-
ken dialog on the subject of the message, e.g., an-
swer questions about the message topic and give
additional explanations. For example, consider a
student learning about an event in a virtual world.
Lets say there is a small circus in a small town and
someone has released all the animals from circus.
A young student plays a role of a reporter to find
out who caused this local havoc. She is out to in-
terrogate a number of witnesses represented by the
virtual characters. It is reasonable to expect that
each conversation is going to be focused solely on
the event of interest and the characters may refuse
to talk about anything else. Each witness may have
a particular and very narrow view into an aspect of
the event, and the student?s success would depend
on what sort of questions she asks and to which
character she addresses them.
Automatic question answering (QA) has been
studied extensively in recent years. For example,
there is a significant body of research done in the
context of the QA track at the Text REtrieval Con-
ference (TREC) (Voorhees, 2003). In contrast to
the TREC scenario where both questions and an-
swers are based on facts and the goal is to provide
the most relevant answer, we focus the answer?s
appropriateness. In our example about an inves-
tigation, an evasive, misleading, or an ?honestly?
wrong answer from a witness character would be
appropriate but might not be relevant. We try
to highlight that distinction by talking about QA
characters as opposed to QA systems or agents.
We expect that a typical simulation would con-
tain quite a few QA characters. We also expect
those characters to have a natural spoken language
interaction with the student. Our technical require-
ments for such a QA character is that it should be
able to understand spoken language. It should be
robust to disfluencies in conversational English. It
should be relatively fast, easy, and inexpensive to
construct without the need for extensive domain
knowledge and dialog management design exper-
tise.
In this paper we describe a QA character by the
name of SGT Blackwell who was originally de-
signed to serve as an information kiosk at an army
18
conference (see Appendix C for a photograph of
the system) (?). We have used SGT Blackwell to
develop our technology for automatic answer se-
lection, conversation management, and system in-
tegration. We are presently using this technology
to create other QA characters.
In the next section we outline the SGT Black-
well system setup. In Section 3 we discuss the
answer selection problem and consider three dif-
ferent algorithms: Support Vector Machines clas-
sifier (SVM), Language Model retrieval (LM), and
Cross-lingual Language Model (CLM) retrieval.
We present the results of off-line experiments
showing that the CLM method performs signifi-
cantly better than the other two techniques in Sec-
tion 4. Section 5 describes a user study of the sys-
tem that uses the CLM approach for answer selec-
tion. Our results show that the approach is very
robust to deviations in wording from expected an-
swers, and speech recognition errors. Finally, we
summarize our results and outline some directions
for future work in Section 6.
2 SGT Blackwell
A user talks to SGT Blackwell using a head-
mounted close capture USB microphone. The
user?s speech is converted into text using an au-
tomatic speech recognition (ASR) system. We
used the Sonic statistical speech recognition en-
gine from the University of Colorado (Pellom,
2001) with acoustic and language models pro-
vided to us by our colleagues at the University of
Southern California (Sethy et al, 2005). The an-
swer selection module analyzes the speech recog-
nition output and selects the appropriate response.
The character can deliver 83 spoken lines rang-
ing from one word to a couple paragraphs long
monologues. There are three kinds of lines SGT
Blackwell can deliver: content, off-topic, and
prompts. The 57 content-focused lines cover the
identity of the character, its origin, its language
and animation technology, its design goals, our
university, the conference setup, and some mis-
cellaneous topics, such as ?what time is it?? and
?where can I get my coffee??
When SGT Blackwell detects a question that
cannot be answered with one of the content-
focused lines, it selects one out of 13 off-topic re-
sponses, (e.g., ?I am not authorized to comment
on that,?) indicating that the user has ventured out
of the allowed conversation domain. In the event
that the user persists in asking the questions for
which the character has no informative response,
the system tries to nudge the user back into the
conversation domain by suggesting a question for
the user to ask: ?You should ask me instead about
my technology.? There are 7 different prompts in
the system.
One topic can be covered by multiple answers,
so asking the same question again often results in
a different response, introducing variety into the
conversation. The user can specifically request
alternative answers by asking something along
the lines of ?do you have anything to add?? or
?anything else?? This is the first of two types
command-like expressions SGT Blackwell under-
stands. The second type is a direct request to re-
peat the previous response, e.g., ?come again?? or
?what was that??
If the user persists on asking the same question
over and over, the character might be forced to re-
peat its answer. It indicates that by preceding the
answer with one of the four ?pre-repeat? lines in-
dicating that incoming response has been heard re-
cently, e.g., ?Let me say this again...?
3 Answer Selection
The main problem with answer selection is uncer-
tainty. There are two sources of uncertainty in
a spoken dialog system: the first is the complex
nature of natural language (including ambigu-
ity, vagueness, underspecification, indirect speech
acts, etc.), making it difficult to compactly char-
acterize the mapping from the text surface form to
the meaning; and the second is the error-prone out-
put from the speech recognition module. One pos-
sible approach to creating a language understand-
ing system is to design a set of rules that select a
response given an input text string (Weizenbaum,
1966). Because of uncertainty this approach can
quickly become intractable for anything more than
the most trivial tasks. An alternative is to cre-
ate an automatic system that uses a set of train-
ing question-answer pairs to learn the appropriate
question-answer matching algorithm (Chu-Carroll
and Carpenter, 1999). We have tried three differ-
ent methods for the latter approach, described in
the rest of this section.
3.1 Text Classification
The answer selection problem can be viewed as a
text classification task. We have a question text
19
as input and a finite set of answers, ? classes, ?
we build a system that selects the most appropriate
class or set of classes for the question. Text classi-
fication has been studied in Information Retrieval
(IR) for several decades (Lewis et al, 1996). The
distinct properties of our setup are (1) a very small
size of the text, ? the questions are very short, and
(2) the large number of classes, e.g, 60 responses
for SGT Blackwell.
An answer defines a class. The questions corre-
sponding to the answer are represented as vectors
of term features. We tokenized the questions and
stemmed using the KStem algorithm (Krovetz,
1993). We used a tf ? idf weighting scheme to
assign values to the individual term features (Al-
lan et al, 1998). Finally, we trained a multi-class
Support Vector Machines (SVM struct) classifier
with an exponential kernel (Tsochantaridis et al,
2004). We have also experimented with linear
kernel function, various parameter values for the
exponential kernel, and different term weighting
schemes. The reported combination of the ker-
nel and weighting scheme showed the best clas-
sification performance. Such an approach is well-
known in the community and has been shown to
work very well in numerous applications (Leuski,
2004). In fact, SVM is generally considered to be
one of the best performing methods for text clas-
sification. We believe it provides us with a very
strong baseline.
3.2 Answer Retrieval
The answer selection problem can also be viewed
as an information retrieval problem. We have a
set of answers which we can call documents in ac-
cordance with the information retrieval terminol-
ogy. Let the question be the query, we compare
the query to each document in the collection and
return the most appropriate set of documents.
Presently the best performing IR techniques
are based on the concept of Language Model-
ing (Ponte and Croft, 1997). The main strategy
is to view both a query and a document as samples
from some probability distributions over the words
in the vocabulary (i.e., language models) and com-
pare those distributions. These probability distri-
butions rarely can be computed directly. The ?art?
of the field is to estimate the language models as
accurately as possible given observed queries and
documents.
Let Q = q1...qm be the question that is re-
ceived by the system, RQ is the set of all the an-
swers appropriate to that question, and P (w|RQ)
is the probability that a word randomly sampled
from an appropriate answer would be the word w.
The language model of Q is the set of probabili-
ties P (w|RQ) for every word in the vocabulary. If
we knew the answer set for that question, we can
easily estimate the model. Unfortunately, we only
know the question and not the answer set RQ. We
approximate the language model with the condi-
tional distribution:
P (w|RQ) ? P (w|Q) =
P (w, q1, ..., qm)
P (q1, ..., qm)
(1)
The next step is to calculate the joint probabil-
ity of observing a string: P (W ) = P (w1, ..., wn).
Different methods for estimating P (W ) have been
suggested starting with simple unigram approach
where the occurrences of individual words are as-
sumed independent from each other: P (W ) =
?n
i=1 P (wi). Other approaches include Proba-
bilistic Latent Semantic Indexing (PLSI) (Hoff-
man, 1999) and Latent Dirichlet Allocation
(LDA) (Blei et al, 2003). The main goal of these
different estimations is to model the interdepen-
dencies that exist in the text and make the esti-
mation feasible given the finite amount of training
data.
In this paper we adapt an approach suggested
by Lavrenko (Lavrenko, 2004). He assumed that
all the word dependencies are defined by a vector
of possibly unknown parameters on the language
model. Using the de Finetti?s representation the-
orem and kernel-based probability estimations, he
derived the following estimate for the query lan-
guage model:
P (w|Q) =
?
s?S pis(w)
?m
i=1 pis(qi)
?
s
?m
i=1 pis(qi)
(2)
Here we sum over all training strings s ? S,
where S is the set of training strings. pis(w) is the
probability of observing word w in the string s,
which can be estimated directly from the training
data. Generally the unigram maximum likelihood
estimator is used with some smoothing factor:
pis(w) = ?pi ?
#(w, s)
|s|
+ (1? ?pi) ?
?
s #(w, s)
?
s |s| (3)
20
where #(w, s) is the number of times word w ap-
pears in string s, |s| is the length of the string s,
we sum over all training strings s ? S, and the
constant ?pi is the tunable parameter that can be
determined from training data.
We know all the possible answers, so the answer
language model P (w|A) can be estimated from
the data:
P (w|A) = piA(w) (4)
3.3 Ranking criteria
To compare two language models we use the
Kullback-Leibler divergence D(pq||pa) defined as
D(pq||pa) =
?
w?V
P (w|Q) log
P (w|Q)
P (w|A)
(5)
which can be interpreted as the relative entropy be-
tween two distributions. Note that the Kullback-
Leibler divergence is a dissimilarity measure, we
use ?D(pq||pa) to rank the answers.
So far we have assumed that both questions
and answers use the same vocabulary and have
the same a priori language models. Clearly, it is
not the case. For example, consider the follow-
ing exchange: ?what happened here?? ? ?well,
maam, someone released the animals this morn-
ing.? While the answer is likely to be very appro-
priate to the question, there is no word overlap be-
tween these sentences. This is an example of what
is known in information retrieval as vocabulary
mismatch between the query and the documents.
In a typical retrieval scenario a query is assumed
to look like a part of a document. We cannot make
the same assumption about the questions because
of the language rules: e.g., ?what?, ?where?, and
?why? are likely to appear much more often in
questions than in answers. Additionally, a typi-
cal document is much larger than any of our an-
swers and has a higher probability to have words
in common with the query. Finally, a typical re-
trieval scenario is totally context-free and a user is
encouraged to specify her information need as ac-
curately as possible. In a dialog, a portion of the
information is assumed to be well-known to the
participants and remains un-verbalized leading to
sometimes brief questions and answers.
We believe this vocabulary mismatch to be so
significant that we view the participants as speak-
ing two different ?languages?: a language of ques-
tions and a language of answers. We will model
the problem as a cross-lingual information task,
where one has a query in one language and wishes
to retrieve documents in another language. There
are two ways we can solve it: we can translate the
answers into the question language by building a
representation for each answer using the question
vocabulary or we can build question representa-
tions in the answer language.
3.4 Question domain
We create an answer representation in the ques-
tion vocabulary by merging together all the train-
ing questions that are associated with the answer
into one string: a pseudo-answer. We use equa-
tions 5, 2, 3, and 4 to compare and rank the
pseudo-answers. Note that in equation 2 s iterates
over the set of all pseudo-answers.
3.5 Answer domain
Let us look at the question language model
P (w|Q) again, but now we will take into account
that w and Q are from different vocabularies and
have potentially different distributions:
P (w|Q) =
?
s ?As(w)
?m
i=1 piQs(qi)
?
s
?m
i=1 piQs(qi)
(6)
Here s iterates over the training set of question-
answer pairs {Qs, As} and ?x(w) is the experi-
mental probability distribution on the answer vo-
cabulary given by the expression similar to equa-
tion 3:
?x(w) = ??
#(w, x)
|x|
+ (1? ??)
?
s #(w, x)
?
s |x|
and the answer language model P (w|A) can be
estimated from the data as
P (w|A) = ?A(w)
4 Algorithm comparison
We have a collection of questions for SGT Black-
well each linked to a set of appropriate responses.
Our script writer defined the first question or two
for each answer. We expanded the set by a) para-
phrasing the initial questions and b) collecting
questions from users by simulating the final sys-
tem in a Wizard of Oz study (WOZ). There are
1,261 questions in the collection linked to 72 an-
swers (57 content answers, 13 off-topic responses,
and 2 command classes, see Section 2). For this
21
study we considered all our off-topic responses
equally appropriate to an off-topic question and
we collapsed all the corresponding responses into
one class. Thus we have 60 response classes.
We divided our collection of questions into
training and testing subsets following the 10-fold
cross-validation schema. The SVM system was
trained to classify test questions into one of the 60
classes.
Both retrieval techniques produce a ranked list
of candidate answers ordered by the ?D(pq||pa)
score. We only select the answers with scores that
exceed a given threshold ?D(pq||pa) > ? . If the
resulting answer set is empty we classify the ques-
tion as off-topic, i.e., set the candidate answer set
contains to an off-topic response. We determine
the language model smoothing parameters ?s and
the threshold ? on the training data.
We consider two statistics when measuring the
performance of the classification. First, we mea-
sure its accuracy. For each test question the first
response returned by the system, ? the class from
the SVM system or the top ranked candidate an-
swer returned by either LM or CLM methods, ?
is considered to be correct if there is link between
the question and the response. The accuracy is the
proportion of correctly answered questions among
all test questions.
The second statistic is precision. Both LM and
CLM methods may return several candidate an-
swers ranked by their scores. That way a user will
get a different response if she repeats the question.
For example, consider a scenario where the first
response is incorrect. The user repeats her ques-
tion and the system returns a correct response cre-
ating the impression that the QA character simply
did not hear the user correctly the first time. We
want to measure the quality of the ranked list of
candidate answers or the proportion of appropri-
ate answers among all the candidate answers, but
we should also prefer the candidate sets that list all
the correct answers before all the incorrect ones.
A well-known IR technique is to compute aver-
age precision ? for each position in the ranked list
compute the proportion of correct answers among
all preceding answers and average those values.
Table 1 shows the accuracy and average preci-
sion numbers for three answer selection methods
on the SGT Blackwell data set. We observe a sig-
nificant improvement in accuracy in the retrieval
methods over the SVM technique. The differences
shown are statistical significant by t-test with the
cutoff set to 5% (p < 0.05).
We repeated out experiments on QA charac-
ters we are developing for another project. There
we have 7 different characters with various num-
ber of responses. The primary difference with
the SGT Blackwell data is that in the new sce-
nario each question is assigned to one and only
one answer. Table 2 shows the accuracy numbers
for the answer selection techniques on those data
sets. These performance numbers are generally
lower than the corresponding numbers on the SGT
Blackwell collection. We have not yet collected
as many training questions as for SGT Blackwell.
We observe that the retrieval approaches are more
successful for problems with more answer classes
and more training data. The table shows the per-
cent improvement in classification accuracy for
each LM-based approach over the SVM baseline.
The asterisks indicate statistical significance using
a t-test with the cutoff set to 5% (p < 0.05).
5 Effect of ASR
In the second set of experiments for this paper
we studied the question of how robust the CLM
answer selection technique in the SGT Blackwell
system is to the disfluencies of normal conversa-
tional speech and errors of the speech recogni-
tion. We conducted a user study with people in-
terviewing SGT Blackwell and analyzed the re-
sults. Because the original system was meant for
one of three demo ?reporters? to ask SGT Black-
well questions, specialized acoustic models were
used to ensure the highest accuracy for these three
(male) speakers. Consequently, for other speak-
ers (especially female speakers), the error rate was
much higher than for a standard recognizer. This
allowed us to calculate the role of a variety of
speech error rates on classifier performance.
For this experiment, we recruited 20 partici-
pants (14 male, 6 female, ages from 20 to 62)
from our organization who were not members of
this project. All participants spoke English flu-
ently, however the range of their birth languages
included English, Hindi, and Chinese.
After filling out a consent form, participants
were ?introduced? to SGT Blackwell, and demon-
strated the proper technique for asking him ques-
tions (i.e., when and how to activate the micro-
phone and how to adjust the microphone posi-
tion.) Next, the participants were given a scenario
22
SVM LM CLM
accuracy accuracy impr. SVM avg. prec. accuracy impr. SVM avg. prec.
53.13 57.80 8.78 63.88 61.99 16.67 65.24
Table 1: Comparison of three different algorithms for answer selection on SGT Blackwell data. Each
performance number is given in percentages.
number of number of SVM LM CLM
questions answers accuracy accuracy impr. SVM accuracy impr. SVM
1 238 22 44.12 47.06 6.67* 47.90 8.57*
2 120 15 63.33 62.50 -1.32 64.17 1.32
3 150 23 42.67 44.00 3.12* 50.00 17.19*
4 108 18 42.59 44.44 4.35* 50.00 17.39*
5 149 33 32.21 41.35 28.37* 42.86 33.04*
6 39 8 69.23 58.97 -14.81* 66.67 -3.70
7 135 31 42.96 44.19 2.85 50.39 17.28*
average 134 21 48.16 48.93 1.60* 53.14 10.34*
Table 2: Comparison of three different algorithms for answer selection on 7 additional QA characters.
The table shows the number of answers and the number of questions collected for each character. The
accuracy and the improvement over the baseline numbers are given in percentages.
wherein the participant would act as a reporter
about to interview SGT Blackwell. The partici-
pants were then given a list of 10 pre-designated
questions to ask of SGT Blackwell. These ques-
tions were selected from the training data. They
were then instructed to take a few minutes to
write down an additional five questions to ask SGT
Blackwell. Finally they were informed that af-
ter asking the fifteen written down questions, they
would have to spontaneously generate and ask five
additional questions for a total of 20 questions
asked all together. Once the participants had writ-
ten down their fifteen questions, they began the
interview with SGT Blackwell. Upon the com-
pletion of the interview the participants were then
asked a short series of survey questions by the
experimenter about SGT Blackwell and the inter-
view. Finally, participants were given an explana-
tion of the study and then released. Voice record-
ings were made for each interview, as well as the
raw data collected from the answer selection mod-
ule and ASR. This is our first set of question an-
swer pairs, we call it the ASR-QA set.
The voice recordings were later transcribed. We
ran the transcriptions through the CLM answer se-
lection module to generate answers for each ques-
tion. This generated question and answer pairs
based on how the system would have responded
to the participant questions if the speech recogni-
tion was perfect. This is our second set of ques-
tion answer pairs ? the TRS-QA set. Appendix B
shows a sample dialog between a participant and
SGT Blackwell.
Next we used three human raters to judge the
appropriateness of both sets. Using a scale of
1-6 (see Appendix A) each rater judged the ap-
propriateness of SGT Blackwell?s answers to the
questions posed by the participants. We evaluated
the agreement between raters by computing Cron-
bach?s alpha score, which measures consistency in
the data. The alpha score is 0.929 for TRS-QA
and 0.916 for ASR-QA, which indicate high con-
sistency among the raters.
The average appropriateness score for TRS-QA
is 4.83 and 4.56 for ASR-QA. The difference in
the scores is statistically significant according to t-
test with the cutoff set to 5%. It may indicate that
ASR quality has a significant impact on answer
selection.
We computed the Word Error Rate (WER) be-
tween the transcribed question text and the ASR
output. Thus each question-answer pair in the
ASR-QA and TRS-QA data set has a WER score
assigned to it. The average WER score is 37.33%.
We analyzed sensitivity of the appropriateness
score to input errors. Figure 1a and 1b show
plots of the cumulative average appropriateness
score (CAA) as function of WER: for each WER
value t we average appropriateness scores for all
questions-answer pairs with WER score less than
23
(a) pre-designated (b) user-designated
Figure 1: Shows the cumulative average appropriateness score (CAA) of (a) pre-designated and (b)
user-designated question-answer pairs as function of the ASR?s output word error rate. We show the
scores for TRS-QA (dotted black line) and ASR-QA (solid black line). We also show the percentage of
the question-answer pairs with the WER score below a given value (?# ofQA?) as a gray line with the
corresponding values on the right Y axis.
or equal to t.
CAA(t) =
1
|S|
?
p?S
A(p), S = {p|WER(p) ? t}
where p is a question-answer pair, A(p) is the
appropriateness score for p, and WER(p) is the
WER score for p. It is the expected value of the ap-
propriateness score if the ASR WER was at most
t.
Both figures show the CAA values for TRS-
QA (dotted black line) and ASR-QA (solid black
line). Both figures also show the percentage of
the question-answer pairs with the WER score be-
low a given value, i.e., the cumulative distribution
function (CDF) for the WER as a gray line with
the corresponding values depicted on the right Y
axis.
Figure 1a shows these plots for the pre-
designated questions. The values of CAA for
TRS-QA and ASR-QA are approximately the
same between 0 and 60% WER. CAA for ASR-
QA decreases for WER above 60% ? as the input
becomes more and more garbled, it becomes more
difficult for the CLM module to select an appropri-
ate answer. We confirmed this observation by cal-
culating t-test scores at each WER value: the dif-
ferences between CAA(t) scores are statistically
significant for t > 60%. It indicates that until
WER exceeds 60% there is no noticeable effect on
the quality of answer selection, which means that
our answer selection technique is robust relative to
the quality of the input.
Figure 1b shows the same plots for the user-
designated questions. Here the system has to deal
with questions it has never seen before. CAA val-
ues decrease for both TRS-QA and ASR-QA as
WER increases. Both ASR and CLM were trained
on the same data set and out of vocabulary words
that affect ASR performance, affect CLM perfor-
mance as well.
6 Conclusions and future work
In this paper we presented a method for efficient
construction of conversational virtual characters.
These characters accept spoken input from a user,
convert it to text, and select the appropriate re-
sponse using statistical language modeling tech-
niques from cross-lingual information retrieval.
We showed that in this domain the performance
of our answer selection approach significantly ex-
ceeds the performance of a state of the art text clas-
sification method. We also showed that our tech-
nique is very robust to the quality of the input and
can be effectively used with existing speech recog-
nition technology.
Preliminary failure analysis indicates a few di-
rections for improving the system?s quality. First,
we should continue collecting more training data
and extending the question sets.
Second, we could have the system generate a
confidence score for its classification decisions.
Then the answers with a low confidence score can
be replaced with an answer that prompts the user
to rephrase her question. The system would then
24
use the original and the rephrased version to repeat
the answer selection process.
Finally, we observed that a notable percent of
misclassifications results from the user asking a
question that has a strong context dependency on
the previous answer or question. We are presently
looking into incorporating this context informa-
tion into the answer selection process.
Acknowledgments
The project or effort described here has been spon-
sored by the U.S. Army Research, Development,
and Engineering Command (RDECOM). State-
ments and opinions expressed do not necessarily
reflect the position or the policy of the United
States Government, and no official endorsement
should be inferred.
References
James Allan, Jamie Callan, W. Bruce Croft, Lisa
Ballesteros, Donald Byrd, Russell Swan, and Jinxi
Xu. 1998. Inquery does battle with TREC-6. In
Sixth Text REtrieval Conference (TREC-6), pages
169?206, Gaithersburg, Maryland, USA.
D. M. Blei, A. Y. Ng, and M. I. Jordan. 2003. Latent
dirichlet alocation. Journal of Machine Learning
Research, 3:993?1022.
Jennifer Chu-Carroll and Bob Carpenter. 1999.
Vector-based natural language call routing. Journal
of Computational Linguistics, 25(30):361?388.
Sudeep Gandhe, Andrew S. Gordon, and David Traum.
2006. Improving question-answering with linking
dialogues. In Proceedings of the 11th international
conference on Intelligent user interfaces (IUI?06),
pages 369?371, New York, NY, USA. ACM Press.
T. Hoffman. 1999. Probabilistic latent semantic index-
ing. In Proceedings of the 22nd International ACM
SIGIR Conference, pages 50?57.
Robert Krovetz. 1993. Viewing morphology as an in-
ference process. In Proceedings of the 16th Inter-
national ACM SIGIR Conference on Research and
Development in Information Retrieval, pages 191?
202.
Victor Lavrenko. 2004. A Generative Theory of Rele-
vance. Ph.D. thesis, University of Massachusetts at
Amherst.
Anton Leuski. 2004. Email is a stage: discover-
ing people roles from email archives. In Proceed-
ings of 27th annual international ACM SIGIR Con-
ference on Research and Development in Informa-
tion Retrieval (SIGIR?04), pages 502?503, Sheffield,
United Kingdom. ACM Press. NY, USA.
David D. Lewis, Robert E. Schapire, James P. Callan,
and Ron Papka. 1996. Training algorithms for lin-
ear text classifiers. In Proceedings of the 19th Inter-
national ACM SIGIR Conference on Research and
Development in Information Retrieval, pages 298?
306, Zurich, Switzerland.
Bryan Pellom. 2001. Sonic: The university of col-
orado continuous speech recognizer. Technical Re-
port TR-CSLR-2001-01, University of Colorado,
Boulder, CO.
Jay M. Ponte and W. Bruce Croft. 1997. Text segmen-
tation by topic. In Proceedings of the First European
Conference on Research and Advanced Technology
for Digital Libraries, pages 120?129.
Abhinav Sethy, Panayiotis Georgiou, and Shrikanth
Narayanan. 2005. Building topic specific language
models from webdata using competitive models. In
Proceedings of EUROSPEECH, Lisbon, Portugal.
Ioannis Tsochantaridis, Thomas Hofmann, Thorsten
Joachims, and Yasemin Altun. 2004. Support vector
machine learning for interdependent and structured
output spaces. In Proceedings of the Twenty-first in-
ternational conference on Machine learning, Banff,
Alberta, Canada.
Ellen M. Voorhees. 2003. Overview of the trec 2003
question answering track. In Proceedings of The
Twelfth Text Retrieval Conference, pages 54?69.
Joseph Weizenbaum. 1966. Eliza?a computer program
for the study of natural language communication be-
tween man and machine. Communications of the
ACM, 9(1):36?45.
25
A Appropriateness grading
Gandhe and his colleagues (Gandhe et al, 2006)
suggested the following grading scheme that we
used in our user evaluation.
Grade Description
1 Response is not related in any way the
question
2 Response contains some discussion
of people or objects mentioned in the
question, but does not really address
the question itself.
3 Response partially addresses the
question, but little or no coherence
between the question and response.
4 Response does mostly address the
question, but with major problems in
the coherence between question and
response; seems like the response is
really addressing a different question
than the one asked.
5 Response does address the question,
but the transition is somewhat awk-
ward.
6 Response answers the question in a
perfectly fluent manner.
Table 3: The question-answer appropriateness
scale.
B Sample dialog
This section presents an example of a participant?s
dialog with SGT Blackwell. We only show spon-
taneous questions asked by the participant. Each
of the participant?s questions (called ?Reporter? in
this transcript) is followed by the ASR output of
that line in brackets. That is what the answer se-
lection module ?heard.? Note that ASR does make
an occasional mistake and the answer selection is
able to recover gracefully until the next to the last
question, where the input was so garbled that only
one correct word got through.
Reporter: Who created you (Who created you)
Blackwell: Tell you the truth, I have spent a lot
of time with the smart folks at ICT, testing
and tweaking. I am about ready to get out
there and train with real soldiers. I am think-
ing first stop is going to be Fort Sill. But my
ultimate goal is to get forward deployed...
Reporter: Why should we spend money on you
(What should we spend money on you)
Blackwell: Why do you need me?! What you
should be saying is ?How did you get alng
without me?? I will show you how to be a
leader, how to make critical decisions under
stress... And I am glad to be here, because I
can help. I know I can. Look... I have one
mission: to make better soldiers, make bet-
ter warfighters. And if I can ... can save one
life... bring one loved one back to his fam-
ily... Well, then, I am definitely worth it. You
be the judge.
Reporter: What?s your value (What?s the value
in)
Blackwell: I can do ?Virtually? anything. I do
mission rehearsals, Urban Combat training,
language training - I speak Farsi, Pashtun and
Arabic. ?Mar Haba shabab! That means how
are you in Arabic.
Reporter: Do you really work or is this just a
demo (G. really worker is this just a demo)
Blackwell: Sorry. That?s outside my AO.
Reporter: How could you be better (How could
you be better)
Blackwell: Stop mumbling. Just kidding. I
didn?t get that.
Reporter: What do you wish you could do
(When you wish you could do)
Blackwell: Career Army. That is what I am
thinking. Just call me a lifer.
Reporter: What kind of people have you trained
(What other people in the turn in)
Blackwell: I teach cultural awareness, function
as a training mentor and can call in the full
range of Joint fires and effects.
Reporter: What?s the hardest part of your job
(He was the hardest are you job)
Blackwell: I am not authorized to comment on
that
Reporter: You have any virtual friends (The uni-
versal friends)
26
Blackwell: Sorry, I can?t hear you
Reporter: Are you hard of hearing (Are you had
a hearing)
Blackwell: I can?t understand you.
C SGT Blackwell
Figure 2: A photograph of the SGT Blackwell sys-
tem setup.
27
Proceedings of the 7th SIGdial Workshop on Discourse and Dialogue, pages 88?95,
Sydney, July 2006. c?2006 Association for Computational Linguistics
An Information State-Based Dialogue Manager for Call for Fire Dialogues
Antonio Roque and David Traum
USC Institute for Creative Technologies
13274 Fiji Way, Marina Del Rey, CA 90292
roque@ict.usc.edu, traum@ict.usc.edu
Abstract
We present a dialogue manager for ?Call
for Fire? training dialogues. We describe
the training environment, the domain, the
features of its novel information state-
based dialogue manager, the system it is a
part of, and preliminary evaluation results.
1 Overview
Dialogue systems are built for many different pur-
poses, including information gathering (e.g., (Aust
et al, 1995)), performing simple transactions (e.g,
(Walker and Hirschman, 2000)), collaborative in-
teraction (e.g., (Allen et al, 1996)), tutoring (e.g.,
(Rose et al, 2003)), and training (e.g. (Traum
and Rickel, 2002)). Aspects of the purpose, as
well as features of the domain itself (e.g., train
timetables, air flight bookings, schedule mainte-
nance, physics, and platoon-level military opera-
tions) will have a profound effect on the nature of
the dialogue which a system will need to engage
in. Issues such as initiative, error correction, flex-
ibility in phrasing and dialogue structure may de-
pend crucially on these factors.
The information state approach to dialogue
managers (Larsson and Traum, 2000) has been an
attempt to cast some of these differences within
the same framework. In this approach, a theory
of dialogue is constructed by providing informa-
tion structure elements, a set of dialogue moves
that can be recognized and produced and are used
to modify the nature of these elements, a set of
update rules that govern the dynamics of how the
information is changed as dialogue moves are per-
formed, and an update strategy. Many differ-
ent dialogue systems have been built according to
this general approach (e.g., (Cooper and Larsson,
1999; Matheson et al, 2000; Lemon et al, 2001;
Johnston et al, 2002; Traum and Rickel, 2002;
Purver, 2002)).
In this paper, we present an information-state
based dialogue manager for a new domain: train-
ing call for fire dialogues. Like other dialogue sys-
tems used as role-players in training applications,
the structure of the dialogue is not completely free
for a dialogue designer to specify based on issues
of dialogue efficiency. The dialogue system must
conform as much as possible to the type of dia-
logue that a trainee would actually encounter in the
types of interaction he or she is being trained for.
In particular, for military radio dialogues, much
of the protocol for interaction is specified by con-
vention (e.g., (Army, 2001)). Still, there is a fair
amount of flexibility in how other aspects of the
dialogue progress.
This dialogue manager is part of a system we
call Radiobot-CFF. Radiobots are a general class
of dialogue systems meant to speak over the ra-
dio in military simulations. Our most extended
effort to date is the Radiobot-CFF system, which
engages in ?call for fire? dialogues to train ar-
tillery observers within a virtual reality training
simulation. Our dialogue system can operate ac-
cording to three different use cases, depending on
how much control a human operator/trainer would
like to exercise over the dialogue. There is a fully
automatic mode in which the Radiobot-CFF sys-
tem engages unassisted in dialogue with the user, a
semi-automatic mode in which the Radiobot-CFF
system fills in forms (which can be edited) and the
operator can approve or change communication
with a simulator or trainee, and a passive mode
in which the operator is engaging in the dialogue
and the Radiobot-CFF system is just observing.
In section 2, we describe the training applica-
88
tion that our dialogue system has been embedded
in as well as the system itself. In section 3, we de-
scribe some aspects of ?call for fire dialogues?, es-
pecially the differences in initiative and purposes
of different phases in the dialogue. In section 4,
we describe the information-state based dialogue
model we have developed for this domain. This in-
cludes dialogue moves, information components,
and update rules. We describe some error handling
capabilities in section 5, and evaluation results in
section 6.
2 Testbed
Our current testbed, Radiobot-CFF, has been
developed in a military training environment,
JFETS-UTM, at the U.S. Army base in in Ft. Sill,
Oklahoma. JFETS-UTM trains soldiers to make
Calls for Fire (CFFs), in which a Forward Ob-
server (FO) team locates an enemy target and re-
quests an artillery fire mission by radio from a Fire
Direction Center (FDC). The training room resem-
bles a battle-scarred apartment in a Middle East-
ern country. A window shows a virtual city dis-
played by a rear-projected computer screen, and
the soldiers use binoculars with computer displays
at their ends to search for targets.
Ordinarily, two trainers control a UTM session.
One communicates with the FO via a simulated
radio, and the other decides what the artillery fire
should be and inputs it to a GUI for the simu-
lator. It is our goal to replace those two train-
ers with one trainer focusing on assessment while
Radiobot-CFF handles the radio communications
and interfaces with the virtual world.
Radiobot-CFF is composed of several pipelined
components. A Speech Recognition component
is implemented using the SONIC speech recogni-
tion system (Pellom, 2001) with custom language
and acoustic models. An Interpreter component
tags the ASR output with its its dialogue move
and parameter labels using two separate Condi-
tional Random Field (Sha and Pereira, 2003; Mc-
Callum, 2002) taggers trained on hand-annotated
utterances. A Dialogue Manager processes the
tagged output, sending a reply to the FO (via a
template-based Generator) and, when necessary, a
message to the artillery simulator FireSim XXI1 to
make decisions on what type of fire to send. The
reply to FO and messages to simulator are medi-
ated by GUIs where the trainer can intervene if
1http://sill-www.army.mil/blab/sims/FireSimXXI.htm
need be.
3 Call for Fire Dialogues
Call for Fire procedures are specified in an Army
field manual (Army, 2001) with variations based
on a unit?s standard operating procedure. Mes-
sages are brief and followed by confirmations,
where any misunderstandings are immediately
corrected. A typical CFF is shown in Figure 1.
1 FO steel one niner this is gator niner one adjust
fire polar over
2 FDC gator nine one this is steel one nine adjust fire
polar out
3 FO direction five niner four zero distance four
eight zero over
4 FDC direction five nine four zero distance four eight
zero out
5 FO one b m p in the open i c m in effect over
6 FDC one b m p in the open i c m in effect out
7 FDC message to observer kilo alpha high explo-
sive four rounds adjust fire target number al-
pha bravo one zero zero zero over
8 FO m t o kilo alpha four rounds target number al-
pha bravo one out
9 FDC shot over
10 FO shot out
11 FDC splash over
12 FO splash out
13 FO right five zero fire for effect out over
14 FDC right five zero fire for effect out
15 FDC shot over
16 FO shot out
17 FDC rounds complete over
18 FO rounds complete out
19 FO end of mission one b m p suppressed zero ca-
sualties over
20 FDC end of mission one b m p suppressed zero ca-
sualties out
Figure 1: Example Dialogue with Radiobot-CFF
CFFs can generally be divided into three
phases. In the first phase (utterances 1-6 in Fig-
ure 1) the FOs identify themselves and important
information about the CFF, including their coor-
dinates, the kind of fire they are requesting, the
location of the target, and the kind of target. In
utterance 1 in Figure 1 the FO performs an identi-
fication, giving his own call sign and that of the
FDC he is calling, and also specifies a method
of fire (?adjust fire?) and a method of targeting
(?polar?.) Note that when speakers expect a reply,
they end their utterance with ?over? as in utter-
ance 1, otherwise with ?out? as in the confirmation
in utterance 2. In utterance 3 the FO gives target
coordinates, and in utterance 5 the FO identifies
the target as a BMP (a type of light tank) and re-
quests ICM rounds (?improved conventional mu-
nitions?.) These turns typically follow one another
89
in quick sequence.
In the second phase of a CFF, (utterances 7-12
in Figure 1), after the FDC decides what kind of
fire they will send, they inform the FO in a mes-
sage to observer (MTO) as in utterance 7. This
includes the units that will fire (?kilo alpha?), the
kind of ammunition (?high explosive?), the num-
ber of rounds and method of fire (?4 rounds ad-
just fire?), and the target number (?alpha bravo one
zero zero zero?). CFFs are requests rather than or-
ders, and they may be denied in full or in part. In
this example, the FO?s request for ICM rounds was
denied in favor of High Explosive rounds. Next
the FDC informs the FO when the fire mission has
been shot, as in utterance 9, and when the fire is
about to land, as in utterance 11. Each of these are
confirmed by the FO.
In the third phase, (utterances 13-20 in Fig-
ure 1) the FO regains dialogue initiative. Depend-
ing on the observed results, the FO may request
that the fire be repeated with an adjust in location
or method of fire. In utterance 13 the FO requests
that the shot be re-sent to a location 50 meters to
the right of the previous shot as a ?fire for effect?
all-out bombardment rather than an ?adjust fire?
targeting fire. This is followed by the abbreviated
FDC-initiated phase of utterances 15-18. In utter-
ance 19 the FO ends the mission, describing the
results and number of casualties.
Besides the behavior shown, at any turn either
participant may request or initiate an intelligence
report or request the status of a mission. Further-
more, after receiving an MTO the FO may imme-
diately begin another fire mission and thus have
multiple missions active; subsequent adjusts are
disambiguated with the target numbers assigned
during the MTOs.
4 Dialogue Manager
We have constructed an Information State-based
dialogue manager (Larsson and Traum, 2000) on
this domain consisting of a set of dialogue moves,
a set of informational components with appropri-
ate formal representations, and a set of update
rules with an update strategy. We describe each
of these in turn.
4.1 Dialogue Moves
We defined a set of dialogue moves to represent
the incoming FO utterances based on a study of
transcripts of human-controlled JFETS-UTM ses-
sions, Army manuals, and the needs of the simu-
lator. As shown in Figure 2 these are divided into
three groups: those that provide information about
the FO or the fire mission, those that confirm in-
formation that the FDC has transmitted, and those
that make requests.
Mission Information:
Observer Coordinates
Situation Report
Identification
Warning Order
Method of Control
Method of Engagement
Target Location
Target Description
End of Mission
Confirming Information:
Message to Observer
Shot
Splash
Rounds Complete
Intel Report
Other Requests:
Radio Check
Say Again
Status
Standby
Command
Figure 2: FO Dialogue Moves
The dialogue moves that provide information
include those in which the FOs transmit their Ob-
server Coordinates (grid location on a map), a
generic Situation Report, or one of the various
components of a fire mission request ranging from
call sign Identification to final End of Mission.
The dialogue moves that confirm information in-
clude those that confirm the MTO and other FDC-
initiated utterances, or a general report on scenario
Intel. The final group includes requests to check
radio functionality, to repeat the previous utter-
ance, for status of a shot, to stand by for transmis-
sion of information, and finally a set of commands
such as ?check fire? requesting cancellation of a
submitted fire mission.
Each of these dialogue moves contains informa-
tion important to the dialogue manager. This in-
formation is captured by the parameters of the di-
alogue move, which are enumerated in Figure 3.
Each parameter is listed with the dialogue move
it usually occurs with, but this assignment is not
strict. For example, ?number of enemies? param-
eters occur in Target Description as well as End of
Mission dialogue moves.
90
Identification-related:
fdc_id
fo_id
Warning Order-related:
method_of_fire
method_of_control
method_of_engagement
method_of_location
Target Location-related:
grid_location
direction
distance
attitude
left_right
left_right_adjust
add_drop
add_drop_adjust
known_point
End Of Mission-related:
target_type
target_description
number_of_enemies
disposition
Other:
command
detail_of_request
target_number
Figure 3: Dialogue Move Parameters
Figure 4 shows how the dialogue moves and pa-
rameters act to identify the components of an FO
utterance. The example is based on utterance 1 in
Figure 1; the Identification move has two param-
eters representing the call signs of the FDC and
the FO, and the Warning Order has two parame-
ters representing the method of fire and method of
location. Parameters need to be identified to con-
firm back to the FO, and in some cases to be sent
to the simulator and for use in updating the infor-
mation state. In the example in Figure 4, the fact
that the requested method of fire is an ?adjust fire?
will be sent to the simulator, and the fact that a
method of fire has been given will be updated in
the information state.
Identification: steel one nine this is gator niner one
fdc id: steel one nine
fo id: gator niner one
Warning Order: adjust fire polar
method of fire: adjust fire
method of location: polar
Figure 4: Example Dialogue Moves and Parame-
ters
4.2 Informational Components
The Radiobot-CFF dialogue manager?s informa-
tion state consists of five classes of informational
components, defined by their role in the dia-
logue and their level of accessibility to the user.
These are the Fire Mission Decision components,
the Fire Mission Value components, the Post-Fire
Value components, the Disambiguation compo-
nents, and the Update Rule Processing compo-
nents.
By dividing the components into multiple
classes we separate those that are simulator-
specific from more general aspects of the domain.
Decisions to fire are based on general con-
straints of the domain, whereas the exact com-
ponents to include in a message to simulator will
be simulator-specific. Also, the components have
been designed such that there is almost no over-
lap in the update rules that modify them (see sec-
tion 4.3). This reduces the complexity involved
in editing or adding rules; although there are over
100 rules in the information state, there are few
unanticipated side-effects when rules are altered.
The first class of components are the Fire Mis-
sion Decision components, which are used to de-
termine whether enough information has been col-
lected to send fire. These components are boolean
flags, updated by rules based on incoming dia-
logue moves and parameters. Figure 5 shows the
values of these components after utterance 3 in
Figure 1 has been processed. The FO has given a
warning order, and a target location (which can ei-
ther be given through a grid location, or through a
combination of direction and distance values, and
observer coordinates), so the appropriate compo-
nents are ?true?. After the FO gives a target de-
scription, that component will be true as well, and
an update rule will recognize that enough informa-
tion has been gathered to send a fire mission.
has warning order? true
has target location? true
has grid location? false
has polar direction? true
has polar distance? true
has polar obco? true
has target descr? false
Figure 5: Fire Mission Decision Components
The second class of information state compo-
nents is the set of Fire Mission Value components,
which track the value of various information el-
91
ements necessary for requesting a fire mission.
These are specific to the FireSim XXI simulator.
Figure 6 shows the values after utterance 3 in Fig-
ure 1. Components such as ?direction value? take
number values, and components such as ?method
of fire? take values from a finite set of possibilities.
Several of these components, such as ?attitude?
have defaults that are rarely changed. Once the
dialogue manager or human trainer decides that it
has enough information to request fire, these com-
ponents are translated into a simulator command
and sent to the simulator.
method of control: adjust fire
method of fire: adjust fire
method of engagement: none given
target type: -
grid value: -
direction value: 5940
distance value: 480
length: 0
width: 100
attitude: 0
observer coordinate value: 45603595
Figure 6: Fire Mission Value Components
Fire Mission Value components are also directly
modifiable by the trainer. Figure 7 shows the GUI
which the trainer can use to take control of the
session, edit any of the Fire Mission Value com-
ponents, and relinquish control of the session back
to Radiobot-CFF. This allows the trainer to correct
any mistakes that the Radiobot may have made or
test the trainee?s adaptability by sending the fire
to an unexpected location. The example shown in
Figure 7 is after utterance 5 of Figure 1; the sys-
tem is running in semi-automated mode and the
dialogue manager has decided that it has enough
information to send a fire. The trainer may send
the message or edit it and then send it. A second
GUI, not shown, allows the trainer to take con-
trol of the outgoing speech of the Radiobot, and,
in semi-automated mode, either confirm the send-
ing of a suggested output utterance, alter it before
sending, or author new text for the radiobot to say.
The third class of components is the Post-Fire
Value components, which are also exposed to the
trainer for modification. The example shown in
Figure 8 is from after utterance 13 in Figure 1; the
FO has requested an ?adjust fire? with an indica-
tor of ?fire for effect? and a right adjustment of 50.
At this point in the dialogue the FO could have in-
stead chosen to end the mission. If the initial fire
had been a ?fire for effect? it could have been re-
Figure 7: GUI
peated, rather than following up an initial ?adjust
fire.? The adjust fire stage does not have any de-
cision components because typically the adjust in-
formation is given in one move.
adjust fire: true
shift indicator: fire for effect
repeat FFE: false
left-right adjustment: 50
add-drop adjustment: 0
vertical adjustment: 0
end of mission: false
disposition: -
number of casualties: -
Figure 8: Post-Fire Value Components
The fourth class, Disambiguation components,
are used by many rules to disambiguate local in-
formation based on global dialogue features. The
example shown in Figure 9 is from the dialogue
in Figure 1, after utterance 1. The ?mission is
polar? component helps determine the method of
target location if speech recognition erroneously
detects both polar and grid coordinates. Target
numbers allow the FOs to handle multiple mis-
sions at the same time (e.g., starting a new call for
fire, before the previous mission has been com-
pleted). The ?missions active? component tracks
how many missions are currently being discussed.
The ?phase? refers to the state of a three-state FSA
92
that tracks which of the three subdialogue phases
(described in section 3) the dialogue is in for the
most recently-discussed mission.
An example of the use of the Disambiguation
components is to determine whether the phrase
?fire for effect? refers to an adjustment of a pre-
vious mission or the initiation of a new mission.
In utterance 13 in Figure 1, ?fire for effect? refers
to an adjustment of a CFF that began with an ?ad-
just fire? in utterance 1. However, the FO could
have started that CFF by calling for a ?fire for ef-
fect?. Furthermore the FO could have started a
second CFF in utterance 13 rather than doing an
adjust, and might have specified ?fire for effect?.
By using a rule to check the phase of the mission
the move can be disambiguated to understand that
it is referring to an adjustment, rather than the ini-
tiation of a new fire mission.
mission is polar?: true
target number: 0
missions active: 0
last method of fire: adjust
phase: Info-Gathering
Figure 9: Disambiguation Components
The last class of components, shown in Fig-
ure 10, is closely tied to the update rule processing,
and is therefore described in the following section.
current reply: gator nine one this is
steel one nine
previous reply: -
understood? true
send EOM? false
send repeat? false
send repeat adjust? false
send repeat ffe? false
Figure 10: Update Rule Processing Components
4.3 Update Rules
Update rules update the informational compo-
nents, build a message to send to the FO, build
a message to send to the simulator, and decide
whether a message should actually be sent to the
FO or simulator.
As an example of rule application, consider the
processing of utterance 1 in Figure 1. Figure 4
shows the moves and parameters for this utterance.
When the dialogue manager processes this utter-
ance, a set of rules associated with the Identifi-
cation move are applied, which starts building a
response to the FO. This response is built in the
?current reply? Update Rule Processing compo-
nent. Figure 10 shows a reply in the process of
being built: a rule has recognized that an Identifi-
cation move is being given, and has filled in slots
in a template with the necessary information and
added it to the ?current reply? component.
Next, the update rules will recognize that a
Warning Order is being given, and will identify
that it is an ?adjust fire? method of fire, and up-
date the ?has warning order? decision component,
the ?method of control? and ?method of fire? value
components, and the ?last method of fire? disam-
biguation component. As part of this, the appro-
priate fields of the GUIs will be filled in to allow
the trainer to override the FO?s request if need be.
Another rule will then fill in the slots of a template
to add ?adjust fire polar? to the current reply, and
later another rule will add ?out?, thus finishing the
reply to the FO. After the reply is finished, it will
place it in the ?previous reply? component, for ref-
erence if the FO requests a repeat of the previous
utterance.
Certain rules are specified as achieving compre-
hension ? that is, if they are applied, the ?under-
stood? variable for that turn is set. If no reply has
been built but the move has been understood, then
no reply needs to be sent. This happens, for ex-
ample, for each of utterances 8, 10, and 12 in Fig-
ure 1: because they are confirmations of utterances
that the FDC has initiated, they do not need to be
replied to. Similarly, no reply needs to be sent if
no reply has been built and the incoming message
is empty or only contains one or two words in-
dicative of an open mic and background noise. Fi-
nally, if no reply has been built and the move has
not been understood, then the FO is prompted to
repeat the message.
As described above, the Fire Mission Decision
components are used to determine whether to send
a fire mission. For other communications with the
simulator, a simpler approach is possible. The de-
cisions to send an end of mission, a repeat fire, or a
repeat fire with the ?adjust? or ?fire for effect? spec-
ification can be made with update rules acting on
a single boolean, and so these are also part of the
Update Rule Processing Components as shown in
Figure 10.
Finally, the application of rules follows a spe-
cific strategy. A given utterance may contain one
or more dialogue moves, each with a set of rules
specific to it. The dialogue manager applies the
93
appropriate rules to each dialogue move in the
utterance before applying the rules that send the
FO messages or simulator commands, as shown in
Figure 11. Rules for producing replies and simula-
tor commands are delayed until the end of process-
ing an utterance to allow for utterances that may
contain self-corrections or relevant details later in
the turn.
for each dialogue move in utterance
apply rules for that dialogue move
end for
apply rules to send reply to FO
apply rules to send simulator commands
Figure 11: Update Strategy for Rules
5 Error Handling
Radiobot-CFF is able to handle various kind of
problematic input in a number of ways. It can han-
dle partially correct information, as in Figure 12.
Speech recognition errors caused the ?three casu-
alties? information to be lost, but the update rules
were able to handle the essential part of the FO
contribution: that the mission was ended, and that
the target was neutralized. The domain is forgiv-
ing in this particular example, although a strict
trainer might want to intervene by the GUI and
insist that the FO re-submit the end of mission re-
port.
FO Said: end of mission target
neutralized estimate three
casualties over
ASR Output: in end of mission target
neutralized as the make three
catch a these over
Radiobot: end of mission target
neutralized out
Figure 12: Error Correction
In other cases, such as when giving number co-
ordinates, all information must be fully grounded.
An example of this is in Figure 13, where the num-
ber ?five? is lost by the speech recognition. In
this case, the domain-appropriate response is to
prompt for a repetition.
FO Said: right five zero over
ASR Output: right by zero over
Radiobot: say again over
Figure 13: Error Correction - Prompt
6 Evaluation
We conducted an evaluation of the Radiobot-CFF
system in fully-automated, semi-automated, and
human-controlled conditions. The system per-
formed well in a number of measures; for exam-
ple, Table 1 shows the scores for median time-to-
fire and task-completion rates. Additional mea-
sures and further details are available in (Robinson
et al, 2006).
Table 1: Example Evaluation Measures
Measure Human Semi Fully
Time To Fire 106.2 s 139.4 s 104.3 s
Task Compl. 100% 97.5% 85.9%
Of particular relevance here, we performed an
evaluation of the dialogue manager, using the eval-
uation corpus of 17 missions run on 8 sessions, a
total of 408 FO utterances. We took transcribed
recordings of the FO utterances, ran them through
the Interpreter, and corrected them. For each ses-
sion, we ran corrected Interpreter output through
the Dialogue Manager to print out the values of the
informational components at the end of every turn.
We then corrected those, and compared the cor-
rections to the uncorrected values to receive preci-
sion, accuracy, and f-scores of 0.99 each.2
7 Summary
We presented a dialogue manager which can en-
gage in Call for Fire training dialogues, and de-
scribed the environment and system in which it
works. It has an information state-based design
with several components accessible to a human
operator, and may be controlled either fully, in
part, or not at all by that human operator.
8 Acknowledgements
This work has been sponsored by the U.S. Army
Research, Development, and Engineering Com-
mand (RDECOM). Statements and opinions ex-
pressed do not necessarily reflect the position or
the policy of the United States Government, and
no official endorsement should be inferred.
2In this preliminary evaluation, the Interpreter and infor-
mational component corrections were all done by a single
coder; also, the coder was correcting the informational com-
ponent output rather than entering informational component
information from blank, thus any errors of omission on the
part of the coder would work in favor of the system perfor-
mance.
94
We would like to thank Charles Hernandez and
Janet Sutton of the Army Research Laboratory,
and Bill Millspaugh and the Depth & Simultane-
ous Attack Battle Lab in Fort Sill, Oklahoma, for
their efforts on this project. We would also like to
thank the other members of the Radiobots project.
References
James F. Allen, Bradford W. Miller, Eric K. Ringger,
and Teresa Sikorski. 1996. A robust system for nat-
ural spoken dialogue. In Proceedings of the 1996
Annual Meeting of the Association for Computa-
tional Linguistics (ACL-96), pages 62?70.
Department of the Army. 2001. Tactics, techniques
and procedures for observed fire and fire support at
battalion task force and below. Technical Report FM
3-09.30 (6-30), Department of the Army.
H. Aust, M. Oerder, F. Siede, and V. Steinbiss. 1995. A
spoken language enquiry system for automatic train
timetable information. Philips Journal of Research,
49(4):399?418.
Robin Cooper and Staffan Larsson. 1999. Dialogue
moves and information states. In H.C. Bunt and
E. C. G. Thijsse, editors, Proceedings of the Third
International Workshop on Computational Seman-
tics.
Michael Johnston, Srinivas Bangalore, Gunaranjan
Vasireddy, Amanda Stent, Patrick Ehlen, Mari-
lyn Walker, Steve Whittaker, and Preetam Maloor.
2002. Match: An architecture for multimodal dia-
logue systems. In Proceedings of the 40th Annual
Meeting of the Association for Computational Lin-
guistics (ACL), pages 376?383.
Staffan Larsson and David Traum. 2000. Information
state and dialogue management in the TRINDI dia-
logue move engine toolkit. Natural Language En-
gineering, 6:323?340, September. Special Issue on
Spoken Language Dialogue System Engineering.
Oliver Lemon, Anne Bracy, Alexander Gruenstein, and
Stanley Peters. 2001. The witas mult-modal dia-
logue system i. In Proc. European Conf. on Speech
Communication and Tech- nology, pages 559?1562.
Colin Matheson, Massimo Poesio, and David Traum.
2000. Modelling grounding and discourse obliga-
tions using update rules. In Proceedings of the First
Conference of the North American Chapter of the
Association for Computational Linguistics.
Andrew Kachites McCallum. 2002. Mal-
let: A machine learning for language toolkit.
http://mallet.cs.umass.edu.
Bryan Pellom. 2001. Sonic: The university of col-
orado continuous speech recognizer. Technical Re-
port TR-CSLR-2001-01, University of Colorado.
Matthew Purver. 2002. Processing unknown words
in a dialogue system. In Proceedings of the 3rd
ACL SIGdial Workshop on Discourse and Dialogue,
pages 174?183. Association for Computational Lin-
guistics, July.
Susan Robinson, Antonio Roque, Ashish Vaswani, and
David Traum. 2006. Evaluation of a spoken dia-
logue system for military call for fire training. To
Appear.
C. Rose, D. Litman, D. Bhembe, K. Forbes, S. Silli-
man, R. Srivastava, and K. van Lehn. 2003. A com-
parison of tutor and student behavior in speech ver-
sus text based tutoring.
F. Sha and F. Pereira. 2003. Shallow parsing with con-
ditional random fields.
David R. Traum and Jeff Rickel. 2002. Embodied
agents for multi-party dialogue in immersive virtual
worlds. In Proceedings of the first International
Joint conference on Autonomous Agents and Mul-
tiagent systems, pages 766?773.
M. Walker and L. Hirschman. 2000. Evaluation for
darpa communicator spoken dialogue systems.
95
Proceedings of the Workshop on Embodied Language Processing, pages 59?66,
Prague, Czech Republic, June 28, 2007. c?2007 Association for Computational Linguistics
Dynamic Movement and Positioning of Embodied Agents in Multiparty
Conversations
Dus?an Jan
USC Institute for Creative Technologies
13274 Fiji Way
Marina del Rey, CA 90292
jan@ict.usc.edu
David R. Traum
USC Institute for Creative Technologies
13274 Fiji Way
Marina del Rey, CA 90292
traum@ict.usc.edu
Abstract
For embodied agents to engage in realis-
tic multiparty conversation, they must stand
in appropriate places with respect to other
agents and the environment. When these
factors change, for example when an agent
joins a conversation, the agents must dynam-
ically move to a new location and/or orien-
tation to accommodate. This paper presents
an algorithm for simulating the movement
of agents based on observed human behav-
ior using techniques developed for pedes-
trian movement in crowd simulations. We
extend a previous group conversation simu-
lation to include an agent motion algorithm.
We examine several test cases and show how
the simulation generates results that mirror
real-life conversation settings.
1 Introduction
When we look at human conversation in a casual,
open setting, such as a party or marketplace, one of
the first things we notice is a tendency for people
to cluster into sub-groups involved in different con-
versations. These groupings are not fixed, however,
people will often join and leave groups and often
move from one group to another. Groups themselves
may fragment into subgroups, and smaller groups
sometimes merge into one larger group. Participants
in these groups adapt their positions and orientations
to account for these circumstances, often without
missing a beat or otherwise disrupting their conver-
sations.
In order to create believable social environments
for games or training simulations we need agents
that can perform these same kinds of behaviors in
a realistic way. There are a number of crowd sim-
ulations (Sung et al, 2004; Shao and Terzopou-
los, 2005; Still, 2000; Helbing and Molna?r, 1995),
but most of these place an emphasis on large-scale
movement of agents and do not model the low-level
aspects of conversational interaction in a realistic
way ? movement of agents in multiparty conver-
sation is more about positioning and repositioning
on a local scale. There is also a large body of work
on embodied conversational agents (Cassell et al,
2000), which attempt to model realistic conversa-
tional non-verbal behaviors. Most of this work fo-
cuses on aspects such as gaze, facial expressions,
and hand and arm gestures, rather than positioning
and orientation in a group. There is some important
work on authored presentation agents and avatars for
human participants which take account of position
in the modelling (Vilhjalmsson and Cassell, 1998;
Rehm et al, 2005), but none of this work presents
fully explicit algorithms for controlling the position-
ing and movement behavior of autonomous agents in
dynamic conversations.
In previous work, it has been shown that incor-
rect positioning of animated agents has a negative ef-
fect on the believability of dynamic group conversa-
tion (Jan and Traum, 2005). Research from anthro-
pologists and social psychologists such as the classic
work on proxemics by Hall (1968) and positioning
by Kendon (1990) provide social reasons to explain
how people position themselves in different situa-
tions. It is also important to know that people expect
59
similar behavior in virtual environments as in real
life as shown by Bailenson et al (2003). This gives
us basic principles on which to base the simulation
and provides some qualitative expectations, but is
not suitable to directly convert into algorithms. The
social force model (Helbing and Molna?r, 1995) de-
veloped for crowd simulations gives a good frame-
work for movement simulation. While the basic
model shows how to handle pedestrian motion we
apply the model to the problem of movement in con-
versation setting.
Our implementation of conversational movement
and positioning is an extension of prior work in
group conversation simulation using autonomous
agents. Carletta and Padilha (2002) presented a sim-
ulation of the external view of a group conversation,
in which the group members take turns speaking and
listening to others. Previous work on turn-taking
is used to form a probabilistic algorithm in which
agents can perform basic behaviors such as speaking
and listening, beginning, continuing or concluding
a speaking turn, giving positive and negative feed-
back, head nods, gestures, posture shifts, and gaze.
Behaviors are generated using a stochastic algorithm
that compares randomly generated numbers against
parameters that can take on values between 0 and 1.
This work was further extended by (Jan and
Traum, 2005), who used new bodies in the Unreal
Tournament game engine, and added support for dy-
namic creation of conversation groups. This simu-
lation allowed dynamic creation, splitting, joining,
entry and exit of sub-conversations. However, the
characters were located in fixed positions. As indi-
cated in their subject evaluations, this significantly
decreased believability when conversation groups
did not coincide with positioning of the agents.
Adding support for movement of characters is a nat-
ural step to counter these less believable situations.
We augment this work by adding a movement and
positioning component that allows agents to moni-
tor ?forces? that make it more desirable to move to
one place or another, iteratively select new destina-
tions and move while remaining engaged in conver-
sations.
The rest of the paper is organized as follows. Sec-
tion 2 describes the main motivations that agents
have for moving from their current position in con-
versation. Section 3 presents the social force model,
which specifies a set of forces that pressure an agent
to move in one direction or another, and a deci-
sion algorithm for deciding which forces to act on
in different situations. Section 4 presents a series of
test cases for the algorithm, demonstrating that the
model behaves as desired for some benchmark prob-
lems in this space. We conclude in section 5 with a
description of future work in this area.
2 Reasons for Movement
There are several reasons why someone engaged in
conversation would want to shift position. Some of
these include:
? one is listening to a speaker who is too far and
or not loud enough to hear,
? there is too much noise from other nearby
sound sources,
? the background noise is louder than the
speaker,
? one is too close to others to feel comfortable,
? one has an occluded view or is occluding the
view of others.
Any of these factors (or a combination of several)
could motivate a participant to move to a more com-
fortable location. During the simulation the speakers
can change, other noise sources can start and stop,
and other agents can move around as well. These
factors can cause a variety of motion throughout the
course of interactions with others. In the rest of this
section we describe these factors in more detail. In
the next section we will develop a formal model of
reactions to these factors.
The first reason we consider for repositioning of
conversation participants is audibility of the speaker.
The deciding factor can be either the absolute vol-
ume of the speaker, or the relative volume compared
to other ?noise?. Noise here describes all audio input
that is not speech by someone in the current conver-
sation group. This includes the speech of agents en-
gaged in other conversations as well as non-speech
sounds. When we are comparing the loudness of dif-
ferent sources we take into account that intensity of
the perceived signal decreases with the square of the
60
distance and also that the loudness of several sources
is additive.
Even when the speaker can be heard over a noise
source, if outside disruptions are loud enough, the
group might want to move to a more remote area
where they can interact without interruptions. Each
of the participants may decide to shift away from a
noise source, even without an explicit group deci-
sion. Of course this may not always be possible if
the area is very crowded.
Another reason for movement is proxemics.
Hall (1968) writes that individuals generally divide
their personal space into four distinct zones. The
intimate zone is used for embracing or whispering,
the personal zone is used for conversation among
good friends, the social zone is used for conversa-
tion among acquaintances and the public zone for
public speaking. The actual distances the zones span
are different for each culture and its interpretation
may vary based on an individual?s personality. If the
speaker is outside the participant?s preferred zone,
the participant will move toward the speaker. Simi-
larly if someone invades the personal zone of a par-
ticipant, the participant will move away.
The final reason for movement is specific to mul-
tiparty conversations. When there are several people
in conversation they will tend to form a circular for-
mation. This gives the sense of inclusion to partic-
ipants and gives them a better view of one another
(Kendon, 1990).
3 Social Force Model
We present our movement simulation in the context
of a social force model. Similar to movement in
crowds, the movement of people engaged in conver-
sation is to a large extent reactionary. The reaction
is usually automatic and determined by person?s ex-
perience, rather than planned for. It is possible to as-
sign a vectorial quantity for each person in conversa-
tion, that describes the desired movement direction.
This quantity can be interpreted as a social force.
This force represents the influence of the environ-
ment on the behavior of conversation participant. It
is important to note however that this force does not
directly cause the body to move, but rather provides
a motivation to move. We illustrate these forces
with figures such as Figure 1, where each circle
Figure 1: A sample group positioning. Each circle
represents an agent. A thick border represents that
the agent is talking, filled or empty shading indicates
conversation group membership.
represents an agent, the different shadings represent
members of different conversation groups, thicker
circles represent speakers in that group, and arrows
represent forces on an agent of interest.
We associate a force with each reason for move-
ment:
~Fspeaker : attractive force toward a speaker
~Fnoise : repelling force from outside noise
~Fproximity : repelling force from agents that are too
close
~Fcircle : force toward circular formation of all con-
versation participants
~Fspeaker is a force that is activated when the
speaker is too far from the listener. This can hap-
pen for one of two reasons. Either the speaker is not
loud enough and the listener has to move closer in
order to understand him, or he is outside the desired
zone for communication. When the agent decides
to join conversation this is the main influence that
guides the agent to his conversation group as shown
in Figure 2. ~Fspeaker is computed according to the
following equation, where ~rspeaker is location of the
speaker, ~r is location of the agent and k is a scaling
factor (we are currently using k = 1):
~Fspeaker = k(~rspeaker ? ~r)
~Fnoise is a sum of forces away from each source of
noise. Each component force is directed away from
61
Figure 2: Attractive force toward speaker ~Fspeaker.
that particular source and its size is inversely pro-
portional to square of the distance. This means that
only sources relatively close to the agent will have a
significant influence. Not all noise is a large enough
motivation for the agent to act upon. The force is
only active when the noise level exceeds a threshold
or when its relative value compared to speaker level
in the group exceeds a threshold. Figure 3 shows an
example of the latter. The following equation is used
to compute ~Fnoise:
~Fnoise = ?
?
i
~ri ? ~r
?~ri ? ~r?3
~Fproximity is also a cumulative force. It is a sum
of forces away from each agent that is too close.
The force gets stronger the closer the invading agent
is. This takes effect for both agents in the conver-
sation group and other agents. This is the second
force that is modeling proxemics. While ~Fspeaker
is activated when the agent is farther than the de-
sired social zone, ~Fproximity is activated when the
agent moves to a closer zone. Based on how well the
agents know each other this can be either when the
agent enters the intimate zone or the personal zone.
Figure 4 shows an example when two agents get too
close to each other. The following equation is used
to compute values for ~Fproximity:
~Fproximity = ?
?
?~ri?~r?<distancezone
~ri ? ~r
?~ri ? ~r?2
~Fcircle is responsible for forming the conversa-
tional group into a convex, roughly circular forma-
tion. Each agent has a belief about who is currently
Figure 3: Repelling force away from other speakers
~Fnoise.
Figure 4: Repelling force away from agents that are
too close ~Fproximity .
participating in the conversation. An agent will com-
pute the center of mass of all these assumed partic-
ipants and the average distance from the center. If
an agent?s position deviates too much from the aver-
age, the ~Fcircle gets activated either toward or away
from center of mass. Notice that ~Fproximity takes
care of spreading out around the circle. The situa-
tion in Figure 5 is an example where an agent de-
cides that he has to adapt his positioning. Notice
that if this agent was not aware of the agent to his
left, the force would not get triggered. This can be
a cause for many interesting situations when agents
have different beliefs about who is part of the con-
versation.
~rm =
1
N
?
i
~ri
~Fcircle = ?
(
1
N
?
i
?~ri ? ~rm?
~r ? ~rm
?~r ? ~rm?
? ~r
)
As described above, each force has some condi-
tions that determine whether the force plays an ac-
62
Figure 5: Agent?s deviation from circular formation
exceeds threshold and triggers force ~Fcircle.
tive role in motivating movement. Since the forces
are not actually physically acting on agent?s bodies,
it is not unreasonable for agents to suppress a cer-
tain force. All the possible causes for movement
are always present, but the agents selectively decide
which ones they will act upon in a given situation.
This is unlike a kinematics calculation with physical
forces where all forces are always active. Combin-
ing all the conditions we can define which forces are
active according to a simple decision procedure. We
can view this as priorities the agent has that decide
which conditions are more important to react to.
In our implementation we use the following pri-
orities:
if speaker is too low ~F = ~Fspeaker + ~Fproximity
else if noise is louder than speaker ~F = ~Fspeaker +
~Fnoise + ~Fproximity
else if noise is too loud ~F = ~Fnoise + ~Fproximity
else if too close to someone ~F = ~Fproximity
otherwise ~F = ~Fcircle
Using the above priorities we have a force defined
at each point in space where an agent could be lo-
cated. We do not use this for the continuous com-
putation of movement, but rather use it to compute
destination points. In each planning cycle the agents
will consider whether they should move. To do this
an agent considers his position in the force field and
computes a destination in the direction of the force
field. This process is performed iteratively a con-
stant bound times (unless there is no movement in
an earlier iteration). This is described in the follow-
ing equations, where ~r is the initial position, ? is a
scaling factor, and ~Pbound is the destination for the
movement of this planning cycle:
~P0 = ~r
~Pi+1 = ~Pi + ?~F (~Pi)
~Destination = ~Pbound
Once we have computed the destination, we use
it as a destination point for the character movement
algorithms in the Unreal Tournament game engine.
These will manage character animation and collision
avoidance.
Figure 6 shows an example with two separate con-
versation groups, where one agent decides to leave
the shaded group and join the unshaded conversa-
tion. The figure shows the iterations he is perform-
ing in his planning cycle and the resulting final des-
tination.
Figure 6: Example of motion computation: The
lower right agent decided to join the unshaded con-
versation. He iteratively applies movement in the
direction of local forces. In each iteration the effects
of different component forces may take effect. The
thick line indicates the final destination and path the
agent chooses for this planning cycle.
4 Test Case Analysis
A full evaluation of the social-force based posi-
tioning algorithm presented in the previous section
would involve analysis of simulations to see if they
improve believability over static simulations such as
simulation of Jan and Traum (2005), or other algo-
rithms. While this remains future work for the mo-
ment, we did evaluate the algorithms against a series
63
of test cases where we know what behavior to expect
from known forces. In this section we present three
such cases, showing that the algorithm does have the
power to represent several aspects of conversational
positioning.
In the simulations we describe here we did not
change the conversational attributes of agents, but
we did constrain the grouping dynamics. In a normal
situation the agents would randomly form conver-
sation groups, based on their stochastic decisions.
Here we wanted to examine particular scenarios and
how the movement algorithm would react to specific
changes in conversation group structure. For this
reason we disabled conversational grouping deci-
sions in the algorithm and triggered the group struc-
ture changes manually from the user interface.
The only variable input to the movement algo-
rithms for different agents is the preferences for
proxemics. Each agent has defined values for all
zones, but we set al agents to use social zone
for communicating. The other parameters such as
thresholds for hearing a speaker and noise and cir-
cular formations were fixed for these experiments.
4.1 Joining conversation
In this test case we have 4 agents. In the initial
condition three agents are engaged in conversation
while the fourth one is away from the scene. We let
the simulation run and at some point we give a com-
mand to the fourth agent to join the group of three.
At first the agent will move toward the group until
he is in a comfortable range as shown in Figure 7.
At the point in which the fourth agent decides to
join the other three, he is the only one who knows
he wants to join the conversation. The other agents
know of the presence of the fourth agent, but they
have no idea that he would like to join them. The
fourth agent is listening for a while and when he
gives a feedback signal the other agents interpret that
as a signal that he wants to join the conversation. As
a result the agents reevaluate their positioning and
one agent decides it would be appropriate to move a
step back to give more space to the new agent. Given
more space the new agent is able to move in circular
formation with the rest of the group without intrud-
ing on the personal zones of other agents. The stable
point of simulation is shown in Figure 8.
Figure 7: The agent on the left is approaching a
conversation. Arrows indicate where the agents will
move from now until the simulation stabilizes.
Figure 8: Stable point after the fourth agent joins the
conversation.
4.2 Conversation splitting into two separate
conversations
In this test case, we have 6 agents. After initial
placement of the agents we issue a command for all
the agents to form one conversation group. As a re-
sult they form a circular formation as can be seen in
Figure 9.
We let the agents talk for a while and then give a
command to the two agents on the right side of the
group to start a side conversation. After this a com-
plex sequence of events takes place. Initially the re-
maining agents still think that those two agents are
part of their conversation group. They have to dis-
ambiguate the speech of those two agents and decide
whether this is just an interruption or a split in the
64
Figure 9: Agents form in a circle to engage in a sin-
gle conversation.
conversation. After a while they realize that those
agents are having a separate conversation.
Deciding that the agents on the right have left the
conversation leads to a change in the force field. The
agents that were closest to the split are bothered by
the noise and start adjusting by moving away. By
doing this they change the shape of formation which
causes the farther agents to also adapt back into cir-
cular formation. At the same time the agents who
split also move away from the others until they get
to a point where all are satisfied. The point where
the simulation stabilized is shown in Figure 10.
Figure 10: After two agents leave the conversation
the agents adapt to it by repositioning.
4.3 Effect of proxemics
In this test case, we examine the effects when the
social zones of the agents are not compatible. This
frequently happens when we have people from dif-
ferent cultures with a large difference in distances
for social zones. An example would be North Amer-
icans compared to Arabs. Americans prefer a much
greater inter-personal distance than Arabs. Empiri-
cal data shows that in many such situations there is
a sort of dance with one agent moving in while an-
other moves away (Scheflen, 1975).
Figure 11: Incompatible social zones.
Figure 11 shows an example of agents with in-
compatible social zones. The markings on the
ground indicate the minimum and maximum accept-
able distance for social zone for each agent. We can
see that the agent on the left has a much smaller
comfortable distance than the one on the right. In
the current position the left agent feels that the other
one is too far, while the right agent thinks everything
is fine. This causes the left agent to make a step for-
ward. Consequently by doing so he steps into per-
sonal zone of the right agent. Now the left agent is
satisfied with the situation but the right agent feels
uncomfortable and decides to take a step back to
keep the other agent out of his personal zone. If
nothing else intervenes, this process can continue,
as the agent on the left ?chases? the one on the right
out of the marketplace.
5 Conclusions
In the previous section, we have shown examples of
how the movement algorithm can mirror many ef-
65
fects we see in real conversations. The examples
however were very constrained and could not show
all the possible combinations that could result from
random choices the agents can make. Given the fact
that each agent maintains his own belief about who
is currently in their conversation we can see many
interesting effects when those beliefs become un-
synchronized.
As seen in the third test case, we can get some
very interesting results when we simulate agents of
different cultures. We think that this simulation ap-
proach can be fruitful for modeling cultural differ-
ences in conversational behavior, and could be used
for inter-cultural and cross-cultural awareness and
training. We are currently exploring whether we can
model different cultural norms for conversational
behaviors in ways such that the resulting agent inter-
action can be recognized as appropriate to one cul-
ture or another.
There are still several improvements possible for
the conversation simulation. On the presentation
side we are planning to make some improvements to
the bodies and number and types of conversational
gestures they can display. We also plan to improve
the algorithm so that it will be able to generate dif-
ferent conversation styles. Currently all conversa-
tions take the same form where all the agents have
the same goals, their only goal is to engage in con-
versation with other agents. We plan to introduce the
notion of tasks so that we can better simulate differ-
ent kinds of activities such as asking for directions,
a political debate, or casual conversation.
Acknowledgments
The project described here has been sponsored
by the U.S. Army Research, Development, and En-
gineering Command (RDECOM). Statements and
opinions expressed do not necessarily reflect the po-
sition or the policy of the United States Government,
and no official endorsement should be inferred.
References
Jeremy N. Bailenson, Jim Blascovich, Andrew C. Beall,
and Jack M. Loomis. 2003. Interpersonal distance
in immersive virtual environments. Personality and
Social Psychology Bulletin, 29:819?833.
Justine Cassell, Joseph Sullivan, Scott Prevost, and Eliz-
abeth Churchill, editors. 2000. Embodied Conversa-
tional Agents. MIT Press, Cambridge, MA.
Edward T. Hall. 1968. Proxemics. Current Anthropol-
ogy, 9(2/3):83?108, apr.
Dirk Helbing and Pe?ter Molna?r. 1995. Social force
model for pedestrian dynamics. Phys. Rev. E,
51(5):4282?4286, May.
Dusan Jan and David R. Traum. 2005. Dialog simulation
for background characters. Lecture Notes in Computer
Science, pages 65?74.
Adam Kendon, 1990. Spatial Organization in Social
Encounters: the F-formation System, pages 209?237.
Cambridge University Press.
E. Padilha and J. Carletta. 2002. A simulation of small
group discussion. Proceedings of EDILOG 2002:
Sixth Workshop on the Semantics and Pragmatics of
Dialogue, pages 117?124.
Matthias Rehm, Elisabeth Andre, and Michael Nischt.
2005. Let?s come together - social navigation behav-
iors of virtual and real humans. In Mark Maybury
et al, editor, INTETAIN 2005, LNAI, pages 122?131.
Springer.
Albert E. Scheflen. 1975. Micro-territories in human in-
teraction. In Adam Kendon, Richard M. Harris, and
Mary Ritchie Key, editors, World Anthropology: Or-
ganization of Behavior in Face-to-Face Interaction,
pages 159?173. Mouton, Paris.
Wei Shao and Demetri Terzopoulos. 2005. Autonomous
pedestrians. In SCA ?05: Proceedings of the 2005
ACM SIGGRAPH/Eurographics symposium on Com-
puter animation, pages 19?28, New York, NY, USA.
ACM Press.
G. Keith Still. 2000. Crowd Dynamics. Ph.D. thesis,
Warwick University.
Mankyu Sung, Michael Gleicher, and Stephen Chenney.
2004. Scalable behaviors for crowd simulation. Com-
puter Graphics Forum, 23(3):519?528.
Hannes Hogni Vilhjalmsson and Justine Cassell. 1998.
Bodychat: autonomous communicative behaviors in
avatars. In AGENTS ?98: Proceedings of the second
international conference on Autonomous agents, pages
269?276, New York, NY, USA. ACM Press.
66
Proceedings of the 9th SIGdial Workshop on Discourse and Dialogue, pages 54?63,
Columbus, June 2008. c?2008 Association for Computational Linguistics
Degrees of Grounding Based on Evidence of Understanding
Antonio Roque
USC Institute for Creative Technologies
Marina del Rey, CA, USA
roque@ict.usc.edu
David Traum
USC Institute for Creative Technologies
Marina del Rey, CA, USA
traum@ict.usc.edu
Abstract
We introduce the Degrees of Grounding
model, which defines the extent to which ma-
terial being discussed in a dialogue has been
grounded. This model has been developed and
evaluated by a corpus analysis, and includes a
set of types of evidence of understanding, a set
of degrees of groundedness, a set of ground-
ing criteria, and methods for identifying each
of these. We describe how this model can be
used for dialogue management.
1 Introduction
Dialogue system researchers are active in investi-
gating ways of detecting and recovering from er-
ror, including determining when to provide confir-
mations or rejections, or how to handle cases of
complete non-understanding (Bohus and Rudnicky,
2005a; Bohus and Rudnicky, 2005b; Skantze, 2005).
Studying the strategies that humans use when
speaking amongst themselves can be helpful (Swerts
et al, 2000; Paek, 2003; Litman et al, 2006). One
approach to studying how humans manage errors of
understanding is to view conversation as a joint ac-
tivity, in which grounding, or the process of adding
material to the common ground between speakers,
plays a central role (Clark and Schaefer, 1989).
From this perspective, conversations are highly co-
ordinated efforts in which participants work together
to ensure that knowledge is properly understood by
all participants. There is a wide variety of grounding
behavior that is determined by the communication
medium, among other things (Clark and Brennan,
1991).
This approach is developed computationally by
Traum, who presents a model of grounding which
adapts Clark and Schaefer?s contributions model to
make it usable in an online dialogue system (Traum,
1994). Other computational approaches to ground-
ing use decision theory (Paek and Horvitz, 2000a)
or focus on modeling belief (Bunt et al, 2007).
Grounding models generally consider material to
be in one of three states: ungrounded, in the process
of becoming sufficiently grounded, or sufficiently
grounded. (An exception is (Paek and Horvitz,
2000b), who use a continuous model of grounded-
ness.) We are developing a model of grounding that
is attentive to a larger set of types of evidence of un-
derstanding than is typical, and use this to define a
model of Degrees of Grounding, which tracks the
extent to which material has become a part of the
common ground.
This model includes a set of types of Evidence of
Understanding that describes the kinds of cues that
the dialogue gives about the state of grounding. A
set of Degrees of Groundedness describes the ex-
tent to which material has achieved mutual belief
while being discussed. A set of Grounding Crite-
ria describes the degree to which material needs to
be grounded. Finally, the model provides algorithms
to assist dialogue management.
The next section describes the radio domain
which we used to begin developing this model. The
dialogues in this domain contain a large amount of
confirmation behavior, which make it a good testbed
for the initial development of the model. However,
because these radio dialogues are highly structured
we are not yet able to make strong claims about the
54
generality of this model.
In following sections we describe the components
of the model, annotation evaluations, and ongoing
development of the model.
2 Domain
The domain for this corpus analysis involves a radio-
based military training application. This corpus was
developed while building the Radiobot-CFF system
(Roque et al, 2006) in which soldiers are trained
to perform artillery strike requests over a simulated
radio in an immersive virtual environment.
Calls for Fire (CFFs) are coordinated artillery at-
tacks on an enemy. Several teams work together to
execute a CFF. A Forward Observer (FO) team lo-
cates an enemy target and initiates the call. The FO
team is made up of two or more soldiers, usually
with one soldier dedicated to spotting the enemy and
another soldier dedicated to operating the radio. The
FO radio operator communicates with the Fire Di-
rection Center (FDC) team, which decides whether
to execute the attack, and if so, which of the avail-
able fire assets to use. An example CFF is given in
the Appendix.
3 Evidence of Understanding
An influential description of evidence of understand-
ing was presented in (Clark and Schaefer, 1989), as
shown in Table 1. This set of types of evidence was
described as being ?graded roughly from weakest to
strongest? and was part of the acceptance phase of a
two-phase grounding process. (Clark and Brennan,
1991) further develop Clark?s notion of evidence,
describing ?the three most common forms of posi-
tive evidence? as being acknowledgments, initiation
of the relevant next turn, and continued attention.
The Degrees of Grounding model exchanges
Clark and Schaefer?s two-phase model for an ap-
proach that tracks grounding acts in a way similar
to (Traum, 1994). Also, rather than concerning it-
self with the strength of a given type of evidence, the
current model tracks the strength of material based
on its degree of groundedness, which is derived from
sequences of evidence as described in Section 4.
Evidence in the Degrees of Grounding model is
tracked per Common Ground Unit (CGU) in an in-
formation state, as in (Traum and Rickel, 2002). An
Evidence Description
Continued Attention B shows he is continuing to
attend and therefore remains
satisfied with As presentation.
Initiation of Relevant
Next Contribution
B starts in on the next contri-
bution that would be relevant
at a level as high as the current
one.
Acknowledgement B nods or says ?uh huh,?
?yeah,? or the like.
Demonstration B demonstrates all or part of
what he has understood A to
mean.
Display B displays verbatim all or part
of As presentation.
Table 1: (Clark and Schaefer, 1989)?s Evidence of Un-
derstanding between speakers A and B
example of such a CGU is given in Figure 1. Ma-
terial under discussion is disambiguated by several
identifying components of the CGU: in this domain
this is the dialogue move, the parameter, the mission
number, and the adjust number. Note that parameter
value is not used as an identifying component; this
allows for reference to the material by participants
who may not yet agree on its value.
information:
dialogue move: target location
parameter: direction
value: 5940
mission number: to be determined
adjust number: 0
evidence history:
submit-G91, repeat_back-S19
degree of groundedness: agreed-content
grounding criteria met: true
Figure 1: Example Common Ground Unit
The remainder of this section describes the kinds
of evidence of understanding found in the corpus.
Section 6 describes inter-annotator agreement stud-
ies that determine that humans can reliably identify
these types of evidence.
3.1 Submit
A Submit type of evidence is provided when ma-
terial is introduced into the common ground for the
first time. The Submit type of evidence is derived
from the Presentation phase of (Clark and Schaefer,
1989).
55
An example of a Submit is given in line 1 of Table
2: ?direction 6120? is information that had not yet
been mentioned and has no assumed values.
Line ID Utterance Evidence
1 G91 direction 6120 over Submit
2 S19 direction 6120 out Repeat Back
3 G91 correction direction
6210 over
Resubmit
Table 2: Example Dialogue
Dialogue systems that do not specifically model
grounding generally assume that material is
grounded when it is first Submitted unless there is
evidence to the contrary.
3.2 Repeat Back
A Repeat Back type of evidence is provided when
material that was Submitted by another dialogue
participant is presented back to them, often as part
of an explicit confirmation.
The Repeat Back evidence is related to the ?Dis-
play? evidence of (Clark and Schaefer, 1989) and
described in Table 1, however here it is renamed to
indicate that it pertains to verbal repetitions, rather
than general displays which may be in other modal-
ities, such as visual. In fact, there is evidence that
grounding behavior related to visual feedback is dif-
ferent from that related to auditory feedback (Clark
and Brennan, 1991; Thompson and Gergle, 2008).
An example is given in line 2 of Table 2: the
?direction 6120? information given in line 1 is Re-
peated Back as part of a confirmation.
3.3 Resubmit
A Resubmit type of evidence is provided when ma-
terial that has already been Submitted by a dialogue
participant is presented again as part of a self- or
other-correction. This is an example of what (Clark
and Brennan, 1991) call negative evidence, which
indicate a lack of mutual belief.
An example is shown in Table 2; the direction in-
formation which was Submitted in turn 1 and Re-
peated Back in turn 2 is Resubmitted in turn 3.
In this domain, follow-up presentations of mate-
rial were almost always corrections, usually of in-
formation that has been repeated back by the other
participant, or based on new occurences in the vir-
tual world (for example, the lifting of smoke that
was previously obscuring a target.) Due to the na-
ture of the task, this corpus had few instances of
non-correction follow-up behavior, where material
was presented a second time for the purposes of fur-
ther discussion. Such follow-ups are an evidence of
understanding whose behavior is probably different
from that of the Resubmit type of evidence as de-
scribed here, and will be examined in future work as
described in Section 7.
3.4 Acknowledge
An Acknowledge type of evidence is a general state-
ment of agreement that does not specifically address
the content of the material. Acknowledges are iden-
tified by semantic interpretation. Acknowledges are
a part of (Clark and Schaefer, 1989)?s set of types of
evidence of understanding.
Table 3 contains an example: in line 1 the speaker
G91 Submits information about the target?s status,
which is then Acknowledged by speaker S19 in turn
line 2.
Line ID Utterance Evidence
1 G91 end of mission target
destroyed over
Submit
2 S19 roger Acknowledge
Table 3: Example of an Acknowledgment
3.5 Request Repair
A Request Repair type of evidence is a statement
that indicates that the speaker needs to have the
material Resubmitted by the other participant. Re-
quest Repairs are identified by semantic interpreta-
tion. Request Repairs are another example of nega-
tive evidence (Clark and Brennan, 1991).
Table 4 gives an example: in line 1 G91 submits
a map grid coordinate, and in line 2 S19 asks that
the other speaker ?say again? that grid coordinate,
which is a Request for Repair.
Line ID Utterance Evidence
1 G91 grid 5843948 Submit
2 S19 say again grid over Request Repair
Table 4: Example of a Request Repair
56
3.6 Move On
A Move On type of evidence is provided when a
participant decides to proceed to the next step of the
task at hand. This requires that the given task have
a set of well-defined steps, and that the step being
Moved On from needs to be grounded before the
next step can be discussed. Move Ons are identified
based on a model of the task at hand. Move Ons are
related to (Clark and Schaefer, 1989)?s ?Initiation of
the relevant next contribution,? although Clark and
Schaefer do not specify that ?next contributions?
should be dependent on sufficiently grounding the
previous step.
A Move On provides evidence because a cooper-
ative dialogue participant would typically not move
on to the next step of the task under such condi-
tions unless they felt that the previous step was suf-
ficiently grounded.
Table 5 shows an example of a Move On. In line
1, G91 indicates that the kind of artillery fire they
want is a ?fire for effect?; this is Repeated Back in
line 2. G91 then Submits grid information related
to the target location. The task specification of Calls
for Fire indicates that fire requests should proceed in
several steps: after a Warning Order is established, a
Target Location should be given, followed by a Tar-
get Description. By moving on to the step in which
a Target Location is provided, G91 tacitly indicates
that the step in which a Warning Order is established
has been dealt with to their satisfaction.
Line ID Utterance Evidence
1 G91 fire for effect over Submit
2 S19 fire for effect out Repeat Back
3 G91 grid 45183658 Submit, Move
On
Table 5: Example of a Move On
Line ID Utterance Evidence
1 S19 message to observer
kilo 2 rounds AB0001
over
Submit
2 G91 mike tango oscar kilo
2 rounds target number
AB0001 out
Repeat Back
3 S19 shot over Submit
Table 6: Example of a non-Move On
Not all typical sequences provide Move On ev-
idence. In the example in Table 6, in line 1 S91
submits a ?message to observer? indicating the kind
of fire that is being delivered, which is followed in
line 2 by a confirmation by G91. S19 then proceeds
to the next step of the task by indicating in line 3
that the artillery has been fired. Line 3, however, is
not a Move On because although it is typically the
next step in the task, providing that information is
not dependent on fully grounding the material being
discussed in line 2 - in fact, line 3 will be provided
when the artillery has been fired, and not based on
any other decision by S19.
3.7 Use
A Use type of evidence is provided when a partici-
pant presents an utterance that indicates, through its
semantics, that a previous utterance was understood.
Uses are related to (Clark and Schaefer, 1989)?s
?Demonstration?.
In the Radiobot-CFF corpus, most Uses are
replies to a request for information, such as in Ta-
ble 7, where S19?s request for a target description in
line 1 is answered with a target description, in line
2.
Line ID Utterance Evidence
1 S19 s2 wants to know whats
the target description
over
Submit
2 G91 zsu over Submit,
Use
Table 7: Example of a Use
Another example of Use is shown in Table 8, in
which S19 is providing an intelligence report in line
1 regarding an enemy target, and line 2 replies with
a statement asking whether the target is a vehicle.
The utterance in line 2 uses information provided in
line 1.
3.8 Lack of Response
A Lack of Response type of evidence is provided
when neither participant speaks for a given length of
time. Identifying a Lack of Response type of evi-
dence involves determining how much silence will
be significant for signalling understanding or lack of
understanding.
57
Line ID Utterance Evidence
1 S19 again it should have
rather large antennas af-
fixed to it uh they are
still sending out signals
at the time
Submit
2 G91 this is some kind of Submit,
vehicle over Use
Table 8: Example of a Use
In the example shown in Table 9, G91 submits
an identifying utterance to see if S19 is available.
After 12 seconds, G91 has heard nothing back; this
is negative evidence of grounding, so in line 3 G91
resubmits the utterance.
Line ID Utterance Evidence
1 G91 S 1 9 this is G 9 1 Submit
2 (12 seconds of silence) Lack of
Response
3 G91 S 1 9 this is G 9 1 Resubmit
Table 9: Example of a Lack of Response
A Lack of Response can also be an indication of
positive grounding, as in Table 10. In line 1, G91
submits information about a target, which in line 2
is repeated back. Line 3 indicates a period of silence,
in which neither speaker took the opportunity to re-
quest a repair or otherwise indicate their disapproval
with the state of the groundedness of the material. In
that sense, the silence of line 3 is positive evidence
of understanding.
Line ID Utterance Evidence
1 G91 b m p in the open over Submit
2 S19 b m p in the open out Repeat
Back
3 (10 seconds of silence) Lack of
Response
Table 10: Example of a Lack of Response
4 Degrees of Groundedness
Degrees of groundedness are defined such that mate-
rial has a given degree before and after any sequence
of evidence given. For example, in Table 10 the tar-
get description given in line 1 has a certain degree
Degreee Pattern/Identifier
Unknown not yet introduced
Misunderstood (anything,Request Repair)
Unacknowledged (Submit, Lack of Response)
Accessible (Submit) or (anything,Resubmit)
Agreed-Signal (Submit, Acknowledgment)
Agreed-Signal+ (Submit, Acknowledgment, other)
Agreed-Content (Submit, Repeat Back)
Agreed-Content+ (Submit, Repeat Back, other)
Assumed grounded by other means
Table 11: Degrees of Groundedness
of groundedness before it is Submitted, another de-
gree after it is Submitted, another degree after it is
Repeated Back, and another degree after the Lack of
Response.
A key part of defining these degrees is to deter-
mine which of these degrees is worth modeling. For
example, Table 3 shows a CGU further grounded by
a single Acknowledgment. In this domain, for the
purposes of determining grounding criteria and dia-
logue management algorithms, it is not worth distin-
guishing between the case in which it had been fol-
lowed by one more Acknowledgment and the case
in which it had been followed by two or more Ac-
knowledgments.
Table 11 shows the significant degrees identified
during the corpus study, as well as the definition or
identifying pattern of evidence. These degrees are
shown from Unknown, which is least grounded, to
Assumed, which is grounded by other means, such
as written information given during a scenario brief-
ing. Most degrees are identified by patterns of evi-
dence. For example, a CGU is misunderstood if the
latest item of evidence provided is a Request Repair,
and CGU is Unacknowledged if it is Submitted fol-
lowed by a Lack of Response.
The degree of groundedness is used to compute
how much (if any) additional evidence is needed
to reach the grounding criterion, or ?criterion suffi-
cient for current purposes? as defined by (Clark and
Schaefer, 1989). This computation can be used in di-
alogue management to help select a next utterance.
In this domain, information such as target num-
bers have high grounding criteria, such as Agreed-
Content+; they would need to be Repeated Back,
and followed at least by a Lack of Response, giv-
ing the other participant an opportunity to correct.
58
Other information might have a grounding crite-
rion of Agreed-Signal, needing only an Acknowl-
edgment to be grounded, as in Table 3. Future work
will address the fact that grounding criteria are vari-
able: for example, in noisy conditions where errors
are more probable, the grounding criteria may in-
crease.
5 Dialogue Management
Exploiting this model of grounding for dialogue
management involves several steps. Evidence of un-
derstanding must be identified given a semantic in-
terpretation and the history of evidence provided so
far. Given an utterance?s new evidence and a CGU?s
current degree of groundedness, the CGU?s new de-
gree of groundedness must be determined.
Once a CGU?s current degree is determined, it can
be compared to its grounding criterion to determine
whether or not it has been sufficiently grounded, and
if not, a new item of evidence may be suggested to
help further ground the material.
All of these can be put together in one algorithm,
as shown in Figure 2.
for each dialogue act parameter,
identify the relevant CGU
identify evidence of understanding
compute the CGU?s degree of groundedness
for each CGU not sufficiently grounded
determine evidence to be given
compute the CGU?s degree of groundedness
if Lack of Response detected
compute the CGU?s degree of groundedness
Figure 2: Dialogue Management Algorithm
The specifics of how this algorithm is integrated
into a system and how it influences task decisions
will vary based on the system being used. To ex-
plore the domain-independence of this model, we
are currently integrating it into a dialogue manager
in a domain unrelated to the CFF task.
6 Evaluation
The validity of this model has been evaluated in sev-
eral corpus tests to measure inter-annotator agree-
ment in identifying evidence, to ensure that identify-
ing evidence can reliably be done by an algorithm,
to measure inter-annotator agreement in identifying
the increase or decrease of the degree of grounded-
ness, and to ensure that identifying the increase or
decrease of a degree of groundedness can reliably
be done by an algorithm.
Human transcribers produced transcriptions of
several sessions between two sets of humans acting
as Forward Observer and Fire Direction Center radio
operators in the training simulation. A subset of the
corpus was used for close analysis: this subset was
made up of 4 training sessions, composed of 17 fire
missions, totaling 456 utterances; this provided a to-
tal of 1222 possible indicators of evidence of under-
standing made up of 886 dialogue move parameters
and 336 period of silence.
We automatically performed a dialogue act inter-
pretation on the dialogue move parameters, which
were then manually corrected. We then manually
annotated the evidence of understanding identified
in each dialogue move parameter and period of si-
lence. An example of the data produced from this
process is given in the Appendix.
6.1 Inter-Annotator Agreement - Identifying
Evidence
An inter-annotator agreement study was performed
in which two annotators tagged a subset of the cor-
pus (318 dialogue move parameters and 74 silences)
to identify the evidence of understanding, given an
utterance and dialogue act interpretation. One anno-
tator was the first author of this paper, and the other
was a computer professional who had no previous
experience with the domain or with tagging data.
Table 12 shows the results, broken down by the
Standalone types of evidence, which could occur
by themselves (Submit, Repeat Back, Resubmit,
Acknowledge, and Request Repair), the Additional
types of evidence, which only occurred with other
types of evidence (Move On and Use), and the
Silence-Related Lack of Understanding type of ev-
idence. Each of these showed acceptable levels of
agreement, with the exception of the Kappa for the
additional evidence. The low score on the additional
evidence is probably due to the fact that Move On
judgments depend on a strong understanding of the
domain-specific task structure, as described in sec-
tion 3.6; to a lesser extent Use judgments tend to
rely on an understanding of the scenario as well.
59
Evidence Type P(A) Kappa
Standalone 0.95 0.91
Additional 0.87 0.53
Silence-Related 0.92 0.84
Table 12: Inter-Annotator Agreement - Evidence
Evidence Type P(A) Kappa
Standalone 0.88 0.81
Additional 0.98 0.92
Silence-Related 1.0 1.0
Table 13: Algorithm Agreement - Evidence
This highlights the fact that for most of the evidence
of understanding (all except for Move On and Use),
agreement can be reached with a non-expert annota-
tor.
6.2 Algorithm Agreement - Identifying
Evidence
The results of the inter-annotator agreement test
were merged into the larger 1222-markable corpus,
to create a consensus human-annotated corpus. This
was used in the next test, to identify whether an al-
gorithm can automatically identify evidence.
We authored a set of rules to identify evidence of
understanding based on the order in which CGUs
were introduced into the common ground, the iden-
tity of the speaker who introduced them, and the
semantic interpretations. The rules were then ap-
plied to the 1222-markable corpus, and the resulting
identifications were then compared to the identifica-
tions made by the human annotators. The results are
shown in Table 13. The respectable agreement and
kappa values indicate that it is possible for an algo-
rithm to reliably identify evidence.
6.3 Degree Increase/Decrease Agreements
Finally, we explored whether humans could reliably
agree on whether a given material?s groundedness
had increased or decreased after a given turn.
We studied this because we are not here claiming
that humans explicitly model degrees of grounded-
ness or perform a computation to compare a given
material with something they had grounded pre-
viously. It is more likely that humans track evi-
dence, determine whether material is more or less
grounded than it was before, and check whether it
Agreement Type P(A) Kappa
Human-Human 0.97 0.94
Human-Algorithm 0.87 0.73
Table 14: Degree Increase/Decrease Agreements
has reached a grounding criterion. A dialogue sys-
tem need not be tied to human behavior to be effec-
tive, so given these human behaviors, we are inter-
ested in whether computer algorithms can be built
to produce useful results in terms of task completion
and human-realistic behavior. For this reason we
evaluate the model of degrees of grounding based on
how human-realistic its ability to identify whether a
CGU?s degree of groundedness has increased or de-
creased, and in future work study whether a system
implementation performs acceptably in terms of task
completion and managing human-realistic ground-
ing behavior.
To perform the test of whether degree increase or
decrease could be reliable detected, we annotated a
subset of the corpus with a non-domain expert. For
a set of CGUs, we tracked the sequence of evidence
that was provided to ground that CGU. Before and
after each item of evidence, we asked the annota-
tors to determine whether the CGU was more or less
grounded than it was the turn before.
We also developed a set of rules based on the defi-
nition of the degrees of groundedness defined in sec-
tion 4 to determine after each utterance whether a
CGU?s degree of groundedness had increased or de-
creased from the utterance before. We then com-
pared the results of that set of rules with human-
consensus judgments about degree increase and de-
crease.
The results are shown in Table 14, indicating that
humans could reliably agree among themselves, and
a rule-based algorithm could reliably agree with the
human consensus judgments.
7 Discussion and Future Work
In this paper we describe the initial development of
the Degrees of Grounding model, which tracks the
extent to which material has been grounded in a di-
alogue. The Degrees of Grounding model contains
a richer variety of evidence of understanding than
most models of grounding, which allows us to de-
60
fine a full set of degrees of groundedness.
We recognize that the initial domain, although
rich in grounding behavior, is not typical of most hu-
man conversation. Besides the structured dialogues
and the domain-specific word use, the types of evi-
dence of understanding presented in Section 3 does
not cover all possible types of evidence. For ex-
ample, (Clark and Schaefer, 1989) describe ?contin-
ued attention? as another possibility, which was not
available with the radio modality used in this study.
Furthermore, it is a feature of this domain that Re-
submit evidence generally indicates lack of under-
standing; in general conversation, it is not true that
the repeated mention of material indicates that it is
not understood, so a ?Follow-Up? evidence is likely,
as are variations of ?Use.?
To explore these questions, we are extending
work to other domains, and are currently focusing
on one in which virtual humans are used for a ques-
tioning task. Also, we plan to run evaluations in im-
plemented systems, exploring performance in terms
of task completion and believable human behavior.
Acknowledgments
This work has been sponsored by the U.S. Army Re-
search, Development, and Engineering Command
(RDECOM). Statements and opinions expressed do
not necessarily reflect the position or the policy of
the United States Government, and no official en-
dorsement should be inferred.
The authors would like to thank Kevin Knight
and the anonymous reviewers for feedback about the
evaluation.
References
Dan Bohus and Alexander Rudnicky. 2005a. Error han-
dling in the RavenClaw dialog management architec-
ture. In Proceedings of HLT-EMNLP-2005.
Dan Bohus and Alexander Rudnicky. 2005b. Sorry,
I didn?t catch that! - an investigation of non-
understanding errors and recovery strategies. In Pro-
ceedings of SIGdial-2005. Lisbon, Portugal.
Harry Bunt, Roser Morante, and Simon Keizer. 2007. An
empirically based computational model of grounding
in dialogue. In Proceedings of the 8th SIGdial Work-
shop on Discourse and Dialogue.
Herbert H. Clark and Susan E. Brennan. 1991. Ground-
ing in communication. In Perspectives on Socially
Shared Cognition, pages 127?149. APA Books.
Herbert H Clark and Edward F Schaefer. 1989. Con-
tributing to discourse. Cognitive Science, 13:259?294.
Diane Litman, Julia Hirschberg, and Marc Swerts. 2006.
Characterizing and predicting corrections in spoken
dialogue systems. Computational linguistics, pages
417?438.
Tim Paek and Eric Horvitz. 2000a. Conversation as
action under uncertainty. In Proceedings of the 16th
Conference on Uncertainty in Artificial Intelligence
(UAI), pages 455?464.
Tim Paek and Eric Horvitz. 2000b. Grounding criterion:
Toward a formal theory of grounding. Technical re-
port, Microsoft Research, April. Microsoft Technical
Report, MSR-TR-2000-40.
Tim Paek. 2003. Toward a taxonomy of communica-
tion errors. In Proceedings of the ISCA Tutorial and
Research Workshop on Error Handling in Spoken Di-
alogue Systems, pages 53?58, August 28-31. Chateau
d?Oex, Vaud, Switzerland.
Antonio Roque, Anton Leuski, Vivek Rangarajan, Su-
san Robinson, Ashish Vaswani, Shri Narayanan, and
David Traum. 2006. Radiobot-CFF: A spoken dia-
logue system for military training. In 9th International
Conference on Spoken Language Processing (Inter-
speech 2006 - ICSLP), September.
Gabriel Skantze. 2005. Galatea: a discourse modeller
supporting concept-level error handling in spoken dia-
logue systems. In Proceedings of SigDial, pages 178?
189). Lisbon, Portugal.
Marc Swerts, Diane Litman, and Julia Hirschberg. 2000.
Corrections in spoken dialogue systems. In Proceed-
ings of the 6th International Conference of Spoken
Language Processing (ICSLP-2000), October.
Will Thompson and Darren Gergle. 2008. Modeling
situated conversational agents as partially observable
markov decision processes. In Proceedings of Intelli-
gent User Interfaces (IUI).
David Traum and Jeff Rickel. 2002. Embodied agents
for multi-party dialogue in immersive virtual world.
In Proceedings of the First International Joint Confer-
ence on Autonomous Agents and Multi-agent Systems
(AAMAS 2002), pages 766?773, July.
David R. Traum. 1994. A Computational Theory of
Grounding in Natural Language Conversation. Ph.D.
thesis, University of Rochester.
61
Appendix
Line ID Utterance Semantic Interpretation Evidence:
Standalone
Evidence:
Additional
1 G91 fire for effect over WO-MOF: fire for effect Submit
2 S19 fire for effect out WO-MOF: fire for effect Repeat Back
3 Silence: 0.7 seconds
ah roger ROGER Acknowledge
4 G91 grid four five four two ah three six
three eight
TL-GR: 45423638 Submit Move On
5 Silence: 2.3 seconds
6 S19 grid four five four two three six
three eight out
TL-GR: 45423638 Repeat Back
7 Silence: 0.7 seconds
ah roger ROGER Acknowledge
8 G91 b r d m TD-TYPE: b r d m Submit Move On
in the open over TD-DESC: in the open Submit
9 Silence: 1.3 seconds
10 S19 b r d m TD-TYPE: b r d m Repeat Back
in the open out TD-DESC: in the open Repeat Back
11 Silence: 9.9 seconds Lack of Re-
sponse
Comments:
This dialogue is between G91 as a Forward Observer identifying a target, and S19 as a Fire Direction
Center who will send the artillery fire when given the appropriate information.
In line 1, G19?s utterance is interpreted as a Warning Order - Method of Fire (WO-MOF), describing the
kind of artillery fire requested, whose value is ?fire for effect.? This is the first mention of a WO-MOF for
this particular CFF, so it is identified as a Submit type of evidence related to a new CGU, which now has an
Accessible degree of groundedness.
In line 2, a WO-MOF is again given. The WO-MOF is identified as referring to the CGU introduced
in line 1, and a Repeat Back type of evidence is added to that CGU?s evidence history, which gives it an
Agreed-Content degree of groundedness.
In line 3 there follows a silence that is not long enough to be a Lack of Response.
In line 4, G91 provides an Acknowledge type of evidence, and Moves On to the next task item: identifying
the Target Location - Grid (TL-GR) of the CFF. The Acknowledge and Move On, referring to the CGU
created in line 1, raise that CGU?s degree of groundedness to its grounding criterion of Agreed-Content+, at
which point it becomes grounded. At the same time, the introduction of the TL-GR information creates a
new CGU, whose degree is Accessible.
In line 6 the TL-GR CGU is Repeated Back, thereby raising its degree of groundedness to Agreed-Content.
In line 8 an Acknowledge is provided and a set of information related to the Target Description (TD-) is
given, providing a Move On, thereby grounding the TL-GR CGU. So by line 8, two CGUs (WO-MOF and
TL-GR) have been added to the common ground, and two more CGUs (TD-TYPE and TD-DESC) have
Accessible degrees and are in the process of being grounded.
In line 10 the TD CGUs are Repeated Back, raising their degree of groundedness to Agreed-Content.
In line 11 the Lack of Response raises the TD CGUs to Agreed-Content+ thereby grounding them. At this
point there is enough information in the common ground for S19 to send the artillery fire.
62
Line ID Utterance Semantic Interpretation Evidence:
Standalone
Evidence:
Additional
message to observer kilo MTO-BAT: kilo Submit
12 S19 two rounds MTO-NUM: two Submit Move On
target number alpha bravo zero
zero one over
TN: AB001 Submit
13 Silence: 3.1 seconds
a roger mike tango alpha ah alpha ROGER Acknowledge
14 G91 target number alpha bravo zero
zero zero one
TN: AB0001 Repeat Back
a kilo MTO-BAT: kilo Repeat Back
two rounds out MTO-NUM: two Repeat Back
11 Silence: 11.4 seconds Lack of Re-
sponse
16 S19 shot SHOT Submit
rounds complete over RC Submit
17 Silence: 0.8 seconds
18 G91 shot SHOT Repeat Back
rounds complete out RC Repeat Back
19 S19 splash over SPLASH Submit
20 Silence: 1.5 seconds
21 G91 splash out SPLASH Repeat Back
22 Silence: 30.4 seconds Lack of Re-
sponse
...
ah end of mission a target number
alpha bravo zero zero one
TN: AB001 Submit
23 G91 one EOM-NUM: one Submit
b r d m EOM-TYPE: b r d m Submit
destroyed over EOM-BDA: destroyed Submit
24 S19 end of mission b r d des m d cor-
rection b r d m
EOM-TYPE: b r d m Repeat Back
destroyed out EOM-BDA: destroyed Repeat Back
Comments:
In line 12, S19 provides information about the artillery fire that is going to be sent. This includes the
battery that will be firing (MTO-BAT), the number of rounds to be fired (MTO-NUM) and the target number
that will be used to refer to this particular fire mission from that point on (TN).
In line 14, G91 Repeats Back the information presented in line 12 along with an Acknowledge.
In line 16, S19 notifies that the mission has been fired; in line 18 this is confirmed. Likewese, in line 19
S19 notifies that the mission is about the land; in line 21 this is confirmed.
Between lines 22 and 23 several turns have been removed for space reasons. These turns were related to an
adjustment of the artillery fire: after the initial bombardment, the Forward Observer requested that the same
artillery be fired 100 meters to the left of the original bombardment. This was confirmed and delivered.
In line 23, G91 sends a description of the amount of damage suffered by the target: the number of enemy
affected (EOM-NUM), the type of enemy (EOM-TYPE) and the extent of the damage (EOM-BDA). These
are Repeated Back by S19, thereby ending the CFF. Note that S19 does not Repeat Back the EOM-NUM. In
this particular instance the number of enemies is implied by the EOM-TYPE being singular, but throughout
the corpus EOMs are seen to have a low grounding criteria.
63
Proceedings of the 9th SIGdial Workshop on Discourse and Dialogue, pages 172?181,
Columbus, June 2008. c?2008 Association for Computational Linguistics
An Evaluation Understudy for Dialogue Coherence Models
Sudeep Gandhe and David Traum
Institute for Creative Technologies
University of Southern California
13274 Fiji way, Marina del Rey, CA, 90292
{gandhe,traum}@ict.usc.edu
Abstract
Evaluating a dialogue system is seen as a
major challenge within the dialogue research
community. Due to the very nature of the task,
most of the evaluation methods need a sub-
stantial amount of human involvement. Fol-
lowing the tradition in machine translation,
summarization and discourse coherence mod-
eling, we introduce the the idea of evaluation
understudy for dialogue coherence models.
Following (Lapata, 2006), we use the infor-
mation ordering task as a testbed for evaluat-
ing dialogue coherence models. This paper re-
ports findings about the reliability of the infor-
mation ordering task as applied to dialogues.
We find that simple n-gram co-occurrence
statistics similar in spirit to BLEU (Papineni
et al, 2001) correlate very well with human
judgments for dialogue coherence.
1 Introduction
In computer science or any other research field, sim-
ply building a system that accomplishes a certain
goal is not enough. It needs to be thoroughly eval-
uated. One might want to evaluate the system just
to see to what degree the goal is being accomplished
or to compare two or more systems with one another.
Evaluation can also lead to understanding the short-
comings of the system and the reasons for these. Fi-
nally the evaluation results can be used as feedback
in improving the system.
The best way to evaluate a novel algorithm or a
model for a system that is designed to aid humans
in processing natural language would be to employ
it in a real system and allow users to interact with it.
The data collected by this process can then be used
for evaluation. Sometimes this data needs further
analysis - which may include annotations, collect-
ing subjective judgments from humans, etc. Since
human judgments tend to vary, we may need to em-
ploy multiple judges. These are some of the reasons
why evaluation is time consuming, costly and some-
times prohibitively expensive.
Furthermore, if the system being developed con-
tains a machine learning component, the problem of
costly evaluation becomes even more serious. Ma-
chine learning components often optimize certain
free parameters by using evaluation results on held-
out data or by using n-fold cross-validation. Eval-
uation results can also help with feature selection.
This need for repeated evaluation can forbid the use
of data-driven machine learning components.
For these reasons, using an automatic evalua-
tion measure as an understudy is quickly becoming
a common practice in natural language processing
tasks. The general idea is to find an automatic eval-
uation metric that correlates very well with human
judgments. This allows developers to use the auto-
matic metric as a stand-in for human evaluation. Al-
though it cannot replace the finesse of human evalu-
ation, it can provide a crude idea of progress which
can later be validated. e.g. BLEU (Papineni et al,
2001) for machine translation, ROUGE (Lin, 2004)
for summarization.
Recently, the discourse coherence modeling com-
munity has started using the information ordering
task as a testbed to test their discourse coherence
models (Barzilay and Lapata, 2005; Soricut and
Marcu, 2006). Lapata (2006) has proposed an au-
172
tomatic evaluation measure for the information or-
dering task. We propose to use the same task as a
testbed for dialogue coherence modeling. We evalu-
ate the reliability of the information ordering task as
applied to dialogues and propose an evaluation un-
derstudy for dialogue coherence models.
In the next section, we look at related work in
evaluation of dialogue systems. Section 3 sum-
marizes the information ordering task and Lap-
ata?s (2006) findings. It is followed by the details
of the experiments we carried out and our observa-
tions. We conclude with a summary future work di-
rections.
2 Related Work
Most of the work on evaluating dialogue systems fo-
cuses on human-machine communication geared to-
wards a specific task. A variety of evaluation met-
rics can be reported for such task-oriented dialogue
systems. Dialogue systems can be judged based
on the performance of their components like WER
for ASR (Jurafsky and Martin, 2000), concept er-
ror rate or F-scores for NLU, understandability for
speech synthesis etc. Usually the core component,
the dialogue model - which is responsible for keep-
ing track of the dialogue progression and coming
up with an appropriate response, is evaluated indi-
rectly. Different dialogue models can be compared
with each other by keeping the rest of components
fixed and then by comparing the dialogue systems
as a whole. Dialogue systems can report subjective
measures such as user satisfaction scores and per-
ceived task completion. SASSI (Hone and Graham,
2000) prescribes a set of questions used for elicit-
ing such subjective assessments. The objective eval-
uation metrics can include dialogue efficiency and
quality measures.
PARADISE (Walker et al, 2000) was an attempt
at reducing the human involvement in evaluation. It
builds a predictive model for user satisfaction as a
linear combination of some objective measures and
perceived task completion. Even then the system
needs to train on the data gathered from user sur-
veys and objective features retrieved from logs of di-
alogue runs. It still needs to run the actual dialogue
system and collect objective features and perceived
task completeion to predict user satisfaction.
Other efforts in saving human involvement in
evaluation include using simulated users for test-
ing (Eckert et al, 1997). This has become a popu-
lar tool for systems employing reinforcement learn-
ing (Levin et al, 1997; Williams and Young, 2006).
Some of the methods involved in user simulation
are as complex as building dialogue systems them-
selves (Schatzmann et al, 2007). User simulations
also need to be evaluated as how closely they model
human behavior (Georgila et al, 2006) or as how
good a predictor they are of dialogue system perfor-
mance (Williams, 2007).
Some researchers have proposed metrics for eval-
uating a dialogue model in a task-oriented system.
(Henderson et al, 2005) used the number of slots in
a frame filled and/or confirmed. Roque et al (2006)
proposed hand-annotating information-states in a di-
alogue to evaluate the accuracy of information state
updates. Such measures make assumptions about
the underlying dialogue model being used (e.g.,
form-based or information-state based etc.).
We are more interested in evaluating types of di-
alogue systems that do not follow these task-based
assumptions: systems designed to imitate human-
human conversations. Such dialogue systems can
range from chatbots like Alice (Wallace, 2003),
Eliza (Weizenbaum, 1966) to virtual humans used
in simulation training (Traum et al, 2005). For
such systems, the notion of task completion or ef-
ficiency is not well defined and task specific objec-
tive measures are hardly suitable. Most evaluations
report the subjective evaluations for appropriateness
of responses. Traum et. al. (2004) propose a cod-
ing scheme for response appropriateness and scoring
functions for those categories. Gandhe et. al. (2006)
propose a scale for subjective assessment for appro-
priateness.
3 Information Ordering
The information ordering task consists of choos-
ing a presentation sequence for a set of information
bearing elements. This task is well suited for text-
to-text generation like in single or multi-document
summarization (Barzilay et al, 2002). Recently
there has been a lot of work in discourse coher-
ence modeling (Lapata, 2003; Barzilay and Lap-
ata, 2005; Soricut and Marcu, 2006) that has used
173
information ordering to test the coherence mod-
els. The information-bearing elements here are sen-
tences rather than high-level concepts. This frees the
models from having to depend on a hard to get train-
ing corpus which has been hand-authored for con-
cepts.
Most of the dialogue models still work at the
higher abstraction level of dialogue acts and inten-
tions. But with an increasing number of dialogue
systems finding use in non-traditional applications
such as simulation training, games, etc.; there is a
need for dialogue models which do not depend on
hand-authored corpora or rules. Recently Gandhe
and Traum (2007) proposed dialogue models that
do not need annotations for dialogue-acts, seman-
tics and hand-authored rules for information state
updates or finite state machines.
Such dialogue models focus primarily on gener-
ating an appropriate coherent response given the di-
alogue history. In certain cases the generation of
a response can be reduced to selection from a set
of available responses. For such dialogue models,
maintaining the information state can be considered
as a secondary goal. The element that is common
to the information ordering task and the task of se-
lecting next most appropriate response is the ability
to express a preference for one sequence of dialogue
turns over the other. We propose to use the informa-
tion ordering task to test dialogue coherence models.
Here the information bearing units will be dialogue
turns.1
There are certain advantages offered by using in-
formation ordering as a task to evaluate dialogue co-
herence models. First the task does not require a
dialogue model to take part in conversations in an
interactive manner. This obviates the need for hav-
ing real users engaging in the dialogue with the sys-
tem. Secondly, the task is agnostic about the under-
lying dialogue model. It can be a data-driven statis-
tical model or information-state based, form based
or even a reinforcement learning system based on
MDP or POMDP. Third, there are simple objective
measures available to evaluate the success of infor-
mation ordering task.
Recently, Purandare and Litman (2008) have used
1These can also be at the utterance level, but for this paper
we will use dialogue turns.
this task for modeling dialogue coherence. But they
only allow for a binary classification of sequences
as either coherent or incoherent. For comparing dif-
ferent dialogue coherence models, we need the abil-
ity for finer distinction between sequences of infor-
mation being put together. Lapata (2003) proposed
Kendall?s ? , a rank correlation measure, as one such
candidate. In a recent study they show that Kendall?s
? correlates well with human judgment (Lapata,
2006). They show that human judges can reliably
provide coherence ratings for various permutations
of text. (Pearson?s correlation for inter-rater agree-
ment is 0.56) and that Kendall?s ? is a good in-
dicator for human judgment (Pearson?s correlation
for Kendall?s ? with human judgment is 0.45 (p <
0.01)).
Before adapting the information ordering task for
dialogues, certain questions need to be answered.
We need to validate that humans can reliably per-
form the task of information ordering and can judge
the coherence for different sequences of dialogue
turns. We also need to find which objective mea-
sures (like Kendall?s ? ) correlate well with human
judgments.
4 Evaluating Information Ordering
One of the advantages of using information order-
ing as a testbed is that there are objective measures
available to evaluate the performance of information
ordering task. Kendall?s ? (Kendall, 1938), a rank
correlation coefficient, is one such measure. Given
a reference sequence of length n, Kendall?s ? for an
observed sequence can be defined as,
? = # concordant pairs ? # discordant pairs# total pairs
Each pair of elements in the observed sequence
is marked either as concordant - appearing in the
same order as in reference sequence or as discor-
dant otherwise. The total number of pairs is Cn2 =
n(n? 1)/2. ? ranges from -1 to 1.
Another possible measure can be defined as the
fraction of n-grams from reference sequence, that
are preserved in the observed sequence.
bn = # n-grams preserved# total n-grams
In this study we have used, b2, fraction of bigrams
and b3, fraction of trigrams preserved from the ref-
erence sequence. These values range from 0 to 1.
Table 1 gives examples of observed sequences and
174
Observed Sequence b2 b3 ?
[0, 1, 2, 3, 4, 5, 6, 7, 8, 9] 1.00 1.00 1.00
[8, 9, 0, 1, 2, 3, 4, 5, 6, 7] 0.89 0.75 0.29
[4, 1, 0, 3, 2, 5, 8, 7, 6, 9] 0.00 0.00 0.60
[6, 9, 8, 5, 4, 7, 0, 3, 2, 1] 0.00 0.00 -0.64
[2, 3, 0, 1, 4, 5, 8, 9, 6, 7] 0.56 0.00 0.64
Table 1: Examples of observed sequences and their re-
spective b2, b3 & ? values. Here the reference sequence
is [0,1,2,3,4,5,6,7,8,9].
respective b2, b3 and ? values. Notice how ? al-
lows for long-distance relationships whereas b2, b3
are sensitive to local features only. 2
5 Experimental Setup
For our experiments we used segments drawn from 9
dialogues. These dialogues were two-party human-
human dialogues. To ensure applicability of our
results over different types of dialogue, we chose
these 9 dialogues from different sources. Three of
these were excerpts from role-play dialogues involv-
ing negotiations which were originally collected for
a simulation training scenario (Traum et al, 2005).
Three are from SRI?s Amex Travel Agent data which
are task-oriented dialogues about air travel plan-
ning (Bratt et al, 1995). The rest of the dialogues are
scripts from popular television shows. Fig 6 shows
an example from the air-travel domain. Each excerpt
drawn was 10 turns long with turns strictly alternat-
ing between the two speakers.
Following the experimental design of (Lapata,
2006) we created random permutations for these di-
alogue segments. We constrained our permutations
so that the permutations always start with the same
speaker as the original dialogue and turns strictly al-
ternate between the speakers. With these constraints
there are still 5!? 5! = 14400 possible permutations
per dialogue. We selected 3 random permutations
for each of the 9 dialogues. In all, we have a total
of 27 dialogue permutations. They are arranged in 3
sets, each set containing a permutation for all 9 di-
alogues. We ensured that not all permutations in a
given set are particularly very good or very bad. We
used Kendall?s ? to balance the permutations across
2For more on the relationship between b2, b3 and ? see row
3,4 of table 1 and figure 4.
the given set as well as across the given dialogue.
Unlike Lapata (2006) who chose to remove the
pronouns and discourse connectives, we decided not
do any pre-processing on the text like removing
disfluencies or removing cohesive devices such as
anaphora, ellipsis, discourse connectives, etc. One
of the reason is such pre-processing if done manu-
ally defeats the purpose of removing humans from
the evaluation procedure. Moreover it is very diffi-
cult to remove certain cohesive devices such as dis-
course deixis without affecting the coherence level
of the original dialogues.
6 Experiment 1
In our first experiment, we divided a total of 9 hu-
man judges among the 3 sets (3 judges per set). Each
judge was presented with 9 dialogue permutations.
They were asked to assign a single coherence rat-
ing for each dialogue permutation. The ratings were
on a scale of 1 to 7, with 1 being very incoherent
and 7 being perfectly coherent. We did not provide
any additional instructions or examples of scale as
we wanted to capture the intuitive idea of coherence
from our judges. Within each set the dialogue per-
mutations were presented in random order.
We compute the inter-rater agreement by using
Pearson?s correlation analysis. We correlate the rat-
ings given by each judge with the average ratings
given by the judges who were assigned the same set.
For inter-rater agreement we report the average of 9
such correlations which is 0.73 (std dev = 0.07). Art-
stein and Poesio (2008) have argued that Krippen-
dorff?s ? (Krippendorff, 2004) can be used for inter-
rater agreement with interval scales like the one we
have. In our case for the three sets ? values were
0.49, 0.58, 0.64. These moderate values of alpha in-
dicate that the task of judging coherence is indeed a
difficult task, especially when detailed instructions
or examples of scales are not given.
In order to assess whether Kendall?s ? can be used
as an automatic measure of dialogue coherence, we
perform a correlation analysis of ? values against
the average ratings by human judges. The Pearson?s
correlation coefficient is 0.35 and it is statistically
not significant (P=0.07). Fig 1(a) shows the rela-
tionship between coherence judgments and ? val-
ues. This experiment fails to support the suitability
175
(a) Kendall?s ? does not correlate well with human
judgments for dialogue coherence.
(b) Fraction of bigram & trigram counts correlate well
with human judgments for dialogue coherence.
Figure 1: Experiment 1 - single coherence rating per permutation
of Kendall?s ? as an evaluation understudy.
We also analyzed the correlation of human judg-
ments against simple n-gram statistics, specifically
(b2 + b3) /2. Fig 1(b) shows the relationship be-
tween human judgments and the average of fraction
of bigrams and fraction of trigrams that were pre-
served in the permutation. The Pearson?s correlation
coefficient is 0.62 and it is statistically significant
(P<0.01).
7 Experiment 2
Since human judges found it relatively hard to as-
sign a single rating to a dialogue permutation, we
decided to repeat experiment 1 with some modifica-
tions. In our second experiment we asked the judges
to provide coherence ratings at every turn, based on
the dialogue that preceded that turn. The dialogue
permutations were presented to the judges through a
web interface in an incremental fashion turn by turn
as they rated each turn for coherence (see Fig 5 in
the appendix for the screenshot of this interface). We
used a scale from 1 to 5 with 1 being completely in-
coherent and 5 as perfectly coherent. 3 A total of 11
judges participated in this experiment with the first
set being judged by 5 judges and the remaining two
sets by 3 judges each.
3We believe this is a less complex task than experiment 1
and hence a narrower scale is used.
For the rest of the analysis, we use the average
coherence rating from all turns as a coherence rat-
ing for the dialogue permutation. We performed
the inter-rater agreement analysis as in experiment
1. The average of 11 correlations is 0.83 (std dev =
0.09). Although the correlation has improved, Krip-
pendorff?s ? values for the three sets are 0.49, 0.35,
0.63. This shows that coherence rating is still a hard
task even when judged turn by turn.
We assessed the relationship between the aver-
age coherence rating for dialogue permutations with
Kendall?s ? (see Fig 2(a)). The Pearson?s correlation
coefficient is 0.33 and is statistically not significant
(P=0.09).
Fig 2(b) shows high correlation of average coher-
ence ratings with the fraction of bigrams and tri-
grams that were preserved in permutation. The Pear-
son?s correlation coefficient is 0.75 and is statisti-
cally significant (P<0.01).
Results of both experiments suggest that,
(b2 + b3) /2 correlates very well with human judg-
ments and can be used for evaluating information
ordering when applied to dialogues.
8 Experiment 3
We wanted to know whether information ordering as
applied to dialogues is a valid task or not. In this ex-
periment we seek to establish a higher baseline for
176
(a) Kendall?s ? does not correlate well with human
judgments for dialogue coherence.
(b) Fraction of bigram & trigram counts correlate well
with human judgments for dialogue coherence.
Figure 2: Experiment 2 - turn-by-turn coherence rating
the task of information ordering in dialogues. We
presented the dialogue permutations to our human
judges and asked them to reorder the turns so that
the resulting order is as coherent as possible. All 11
judges who participated in experiment 2 also partic-
ipated in this experiment. They were presented with
a drag and drop interface over the web that allowed
them to reorder the dialogue permutations. The re-
ordering was constrained to keep the first speaker
of the reordering same as that of the original di-
alogue and the re-orderings must have strictly al-
ternating turns. We computed the Kendall?s ? and
fraction of bigrams and trigrams (b2 + b3) /2 for
these re-orderings. There were a total of 11 ? 9
= 99 reordered dialogue permutations. Fig 3(a)
and 3(b) shows the frequency distribution of ? and
(b2 + b3) /2 values respectively.
Humans achieve high values for the reordering
task. For Kendall?s ? , the mean of the reordered dia-
logues is 0.82 (std dev = 0.25) and for (b2 + b3) /2,
the mean is 0.71 (std dev = 0.28). These values es-
tablish an upper baseline for the information order-
ing task. These can be compared against the random
baseline. For ? random performance is 0.02 4 and
4Theoretically this should be zero. The slight positive bias
is the result of the constraints imposed on the re-orderings -
like only allowing the permutations that have the correct starting
speaker.
for (b2 + b3) /2 it is 0.11. 5
9 Discussion
Results show that (b2 + b3) /2 correlates well with
human judgments for dialogue coherence better than
Kendall?s ? . ? encodes long distance relationships
in orderings where as (b2 + b3) /2 only looks at lo-
cal context. Fig 4 shows the relationship between
these two measures. Notice that most of the order-
ings have ? values around zero (i.e. in the middle
range for ? ), whereas majority of orderings will have
a low value for (b2 + b3) /2. ? seems to overesti-
mate the coherence even in the absence of immedi-
ate local coherence (See third entry in table 1). It
seems that local context is more important for dia-
logues than for discourse, which may follow from
the fact that dialogues are produced by two speakers
who must react to each other, while discourse can be
planned by one speaker from the beginning. Traum
and Allen (1994) point out that such social obliga-
tions to respond and address the contributions of the
other should be an important factor in building dia-
logue systems.
The information ordering paradigm does not take
into account the content of the information-bearing
items, e.g. the fact that turns like ?yes?, ?I agree?,
5This value is calculated by considering all 14400 permuta-
tions as equally likely.
177
(a) Histogram of Kendall?s ? for reordered se-
quences
(b) Histogram of fraction of bigrams & tri-
grams values for reordered sequences
Figure 3: Experiment 3 - upper baseline for information ordering task (human performance)
?okay? perform the same function and should be
treated as replaceable. This may suggest a need to
modify some of the objective measures to evaluate
the information ordering specially for dialogue sys-
tems that involve more of such utterances.
Human judges can find the optimal sequences
with relatively high frequency, at least for short
dialogues. It remains to be seen how this varies
with longer dialogue lengths which may contain
sub-dialogues that can be arranged independently of
each other.
10 Conclusion & Future Work
Evaluating dialogue systems has always been a ma-
jor challenge in dialogue systems research. The core
component of dialogue systems, the dialogue model,
has usually been only indirectly evaluated. Such
evaluations involve too much human effort and are a
bottleneck for the use of data-driven machine learn-
ing models for dialogue coherence. The information
ordering task, widely used in discourse coherence
modeling, can be adopted as a testbed for evaluating
dialogue coherence models as well. Here we have
shown that simple n-gram statistics that are sensi-
tive to local features correlate well with human judg-
ments for coherence and can be used as an evalua-
tion understudy for dialogue coherence models. As
with any evaluation understudy, one must be careful
while using it as the correlation with human judg-
ments is not perfect and may be inaccurate in some
cases ? it can not completely replace the need for
full evaluation with human judges in all cases (see
(Callison-Burch et al, 2006) for a critique of BLUE
along these lines).
In the future, we would like to perform more ex-
periments with larger data sets and different types
of dialogues. It will also be interesting to see the
role cohesive devices play in coherence ratings. We
would like to see if there are any other measures or
certain modifications to the current ones that corre-
late better with human judgments. We also plan to
employ this evaluation metric as feedback in build-
ing dialogue coherence models as is done in ma-
chine translation (Och, 2003).
Acknowledgments
The effort described here has been sponsored by the U.S. Army
Research, Development, and Engineering Command (RDE-
COM). Statements and opinions expressed do not necessarily
reflect the position or the policy of the United States Govern-
ment, and no official endorsement should be inferred. We would
like to thank Radu Soricut, Ron Artstein, and the anonymous
SIGdial reviewers for helpful comments.
References
Ron Artstein and Massimo Poesio. 2008. Inter-coder
agreement for computational linguistics. In To appear
in Computational Linguistics.
Regina Barzilay and Mirella Lapata. 2005. Modeling
local coherence: An entity-based approach. In Proc.
ACL-05.
178
Regina Barzilay, Noemie Elhadad, and Kathleen McKe-
own. 2002. Inferring strategies for sentence ordering
in multidocument summarization. JAIR, 17:35?55.
Harry Bratt, John Dowding, and Kate Hunicke-Smith.
1995. The sri telephone-based atis system. In Pro-
ceedings of the Spoken Language Systems Technology
Workshop, January.
Chris Callison-Burch, Miles Osborne, and Philipp
Koehn. 2006. In proceedings of EACL-2006.
Wieland Eckert, Esther Levin, and Roberto Pieraccini.
1997. User modeling for spoken dialogue system eval-
uation. In Automatic Speech Recognition and Under-
standing, pages 80?87, Dec.
Sudeep Gandhe and David Traum. 2007. Creating spo-
ken dialogue characters from corpora without annota-
tions. In Proceedings of Interspeech-07.
Sudeep Gandhe, Andrew Gordon, and David Traum.
2006. Improving question-answering with linking di-
alogues. In International Conference on Intelligent
User Interfaces (IUI), January.
Kalliroi Georgila, James Henderson, and Oliver Lemon.
2006. User simulation for spoken dialogue systems:
Learning and evaluation. In proceedings of Inter-
speech.
James Henderson, Oliver Lemon, and Kallirroi Georgila.
2005. Hybrid reinforcement/supervised learning for
dialogue policies from communicator data. In pro-
ceedings of IJCAI workshop.
Kate S. Hone and Robert Graham. 2000. Towards a tool
for the subjective assessment of speech system inter-
faces (SASSI). Natural Language Engineering: Spe-
cial Issue on Best Practice in Spoken Dialogue Sys-
tems.
Daniel Jurafsky and James H. Martin. 2000. SPEECH
and LANGUAGE PROCESSING: An Introduction to
Natural Language Processing, Computational Lin-
guistics, and Speech Recognition. Prentice-Hall.
Maurice G. Kendall. 1938. A new measure of rank cor-
relation. Biometrika, 30:81?93.
Klaus Krippendorff. 2004. Content Analysis, An Intro-
duction to Its Methodology 2nd Edition. Sage Publi-
cations.
Mirella Lapata. 2003. Probabilistic text structuring: Ex-
periments with sentence ordering. In Proceedings of
the 41st Annual Meeting of the Association for Com-
putational Linguistics, Sapporo, Japan.
Mirella Lapata. 2006. Automatic evaluation of informa-
tion ordering. Computational Linguistics, 32(4):471?
484.
Esther Levin, Roberto Pieraccini, and Wieland Eckert.
1997. Learning dialogue strategies within the markov
decision process framework. In Automatic Speech
Recognition and Understanding, pages 72?79, Dec.
Chin-Yew Lin. 2004. ROUGE: a package for automatic
evaluation of summaries. In Proceedings of the Work-
shop on Text Summarization Branches Out.
Franz Josef Och. 2003. Minimum error rate training for
statistical machine translation. In In ACL 2003: Proc.
of the 41st Annual Meeting of the Association for Com-
putational Linguistics, July.
Kishore A. Papineni, Salim Roukos, Todd Ward, and
Wei-Jing Zhu. 2001. BLEU: a method for automatic
evaluation of machine translation. In Technical Re-
port RC22176 (W0109-022), IBM Research Division,
September.
Amruta Purandare and Diane Litman. 2008. Analyz-
ing dialog coherence using transition patterns in lexi-
cal and semantic features. In Proceedings 21st Inter-
national FLAIRS Conference, May.
Antonio Roque, Hua Ai, and David Traum. 2006. Evalu-
ation of an information state-based dialogue manager.
In Brandial 2006: The 10th Workshop on the Seman-
tics and Pragmatics of Dialogue.
Jost Schatzmann, Blaise Thomson, Karl Weilhammer,
Hui Ye, and Steve Young. 2007. Agenda-based user
simulation for bootstrapping a pomdp dialogue sys-
tem. In proceedings of HLT/NAACL, Rochester, NY.
Radu Soricut and Daniel Marcu. 2006. Discourse gener-
ation using utility-trained coherence models. In Proc.
ACL-06.
David R. Traum and James F. Allen. 1994. Discourse
obligations in dialogue processing. In proceedings of
the 32nd Annual Meeting of the Association for Com-
putational Linguistics (ACL-94), pages 1?8.
David R. Traum, Susan Robinson, and Jens Stephan.
2004. Evaluation of multi-party virtual reality dia-
logue interaction. In In Proceedings of Fourth Interna-
tional Conference on Language Resources and Evalu-
ation (LREC), pages 1699?1702.
David Traum, William Swartout, Jonathan Gratch, and
Stacy Marsella. 2005. Virtual humans for non-team
interaction training. In AAMAS-05 Workshop on Cre-
ating Bonds with Humanoids, July.
M. Walker, C. Kamm, and D. Litman. 2000. Towards de-
veloping general models of usability with PARADISE.
Natural Language Engineering: Special Issue on Best
Practice in Spoken Dialogue Systems.
Richard Wallace. 2003. Be Your Own Botmaster, 2nd
Edition. ALICE A. I. Foundation.
Joseph Weizenbaum. 1966. Eliza?a computer program
for the study of natural language communication be-
tween man and machine. Communications of the
ACM, 9(1):36?45, January.
Jason D. Williams and Steve Young. 2006. Partially ob-
servable markov decision processes for spoken dialog
systems. Computer Speech and Language, 21:393?
422.
Jason D. Williams. 2007. A method for evaluating and
comparing user simulations: The cramer-von mises di-
vergence. In IEEE Workshop on Automatic Speech
Recognition and Understanding (ASRU).
179
Appendix
(a) (b) (c)
Figure 4: Distributions for Kendall?s ? , (b2 + b3) /2 and the relationship between them for all possible dialogue
permutations with 10 turns and earlier mentioned constraints.
Figure 5: Screenshot of the interface used for collecting coherence rating for dialogue permutations.
180
Agent AAA at American Express may I help you?
User yeah this is BBB BBB I need to make some travel arrangements
Agent ok and what do you need to do?
User ok on June sixth from San Jose to Denver, United
Agent leaving at what time?
User I believe there?s one leaving at eleven o?clock in the morning
Agent leaves at eleven a.m. and arrives Denver at two twenty p.m. out of San Jose
User ok
Agent yeah that?s United flight four seventy
User that?s the one
Doctor hello i?m doctor perez
how can i help you
Captain uh well i?m with uh the local
i?m i?m the commander of the local company
and uh i?d like to talk to you about some options you have for relocating your clinic
Doctor uh we?re not uh planning to relocate the clinic captain
what uh what is this about
Captain well have you noticed that there?s been an awful lot of fighting in the area recently
Doctor yes yes i have
we?re very busy
we?ve had many more casual+ casualties many more patients than than uh usual in the
last month
but uh what what is this about relocating our clinic
have have uh you been instructed to move us
Captain no
but uh we just have some concerns about the increase in fighting xx
Doctor are you suggesting that we relocate the clinic
because we had no plans
we uh we uh we?re located here and we?ve been uh
we are located where the patients need us
Captain yeah but
yeah actually it is a suggestion that you would be a lot safer if you moved away from
this area
we can put you in an area where there?s n+ no insurgents
and we have the area completely under control with our troops
Doctor i see captain
is this a is this a suggestion from your commander
Captain i?m uh the company commander
Figure 6: Examples of the dialogues used to elicit human judgments for coherence
181
Proceedings of the 9th SIGdial Workshop on Discourse and Dialogue, pages 198?207,
Columbus, June 2008. c?2008 Association for Computational Linguistics
Making Grammar-Based Generation Easier to Deploy in Dialogue Systems
David DeVault and David Traum and Ron Artstein
USC Institute for Creative Technologies
13274 Fiji Way
Marina del Rey, CA 90292
{devault,traum,artstein}@ict.usc.edu
Abstract
We present a development pipeline and asso-
ciated algorithms designed to make grammar-
based generation easier to deploy in imple-
mented dialogue systems. Our approach real-
izes a practical trade-off between the capabili-
ties of a system?s generation component and
the authoring and maintenance burdens im-
posed on the generation content author for a
deployed system. To evaluate our approach,
we performed a human rating study with sys-
tem builders who work on a common large-
scale spoken dialogue system. Our results
demonstrate the viability of our approach and
illustrate authoring/performance trade-offs be-
tween hand-authored text, our grammar-based
approach, and a competing shallow statistical
NLG technique.
1 Introduction
This paper gives an overview of a new example-
based generation technique that is designed to make
grammar-based generation easier to deploy in dia-
logue systems. Dialogue systems present several
specific requirements for a practical generation com-
ponent. First, the generator needs to be fast enough
to support real-time interaction with a human user.
Second, the generator must provide adequate cover-
age for the meanings the dialogue system needs to
express. What counts as ?adequate? can vary be-
tween systems, since the high-level purpose of a di-
alogue system can affect priorities regarding output
fluency, fidelity to the requested meaning, variety
of alternative outputs, and tolerance for generation
failures. Third, developing the necessary resources
for the generation component should be relatively
straightforward in terms of time and expertise re-
quired. This is especially important since dialogue
systems are complex systems with significant devel-
opment costs. Finally, it should be relatively easy
for the dialogue manager to formulate a generation
request in the format required by the generator.
Together, these requirements can reduce the at-
tractiveness of grammar-based generation when
compared to simpler template-based or canned text
output solutions. In terms of speed, off-the-
shelf, wide-coverage grammar-based realizers such
as FUF/SURGE (Elhadad, 1991) can be too slow for
real-time interaction (Callaway, 2003).
In terms of adequacy of coverage, in principle,
grammar-based generation offers significant advan-
tages over template-based or canned text output by
providing productive coverage and greater variety.
However, realizing these advantages can require sig-
nificant development costs. Specifying the neces-
sary connections between lexico-syntactic resources
and the flat, domain-specific semantic representa-
tions that are typically available in implemented sys-
tems is a subtle, labor-intensive, and knowledge-
intensive process for which attractive methodologies
do not yet exist (Reiter et al, 2003).
One strategy is to hand-build an application-
specific grammar. However, in our experience,
this process requires a painstaking, time-consuming
effort by a developer who has detailed linguistic
knowledge as well as detailed domain knowledge,
and the resulting coverage is inevitably limited.
Wide-coverage generators that aim for applicabil-
198
ity across application domains (White et al, 2007;
Zhong and Stent, 2005; Langkilde-Geary, 2002;
Langkilde and Knight, 1998; Elhadad, 1991) pro-
vide a grammar (or language model) for free. How-
ever, it is harder to tailor output to the desired word-
ing and style for a specific dialogue system, and
these generators demand a specific input format that
is otherwise foreign to an existing dialogue system.
Unfortunately, in our experience, the development
burden of implementing the translation between the
system?s available meaning representations and the
generator?s required input format is quite substan-
tial. Indeed, implementing the translation might re-
quire as much effort as would be required to build a
simple custom generator; cf. (Callaway, 2003; Buse-
mann and Horacek, 1998). This development cost is
exacerbated when a dialogue system?s native mean-
ing representation scheme is under revision.
In this paper, we survey a new example-based ap-
proach (DeVault et al, 2008) that we have devel-
oped in order to mitigate these difficulties, so that
grammar-based generation can be deployed more
widely in implemented dialogue systems. Our de-
velopment pipeline requires a system developer to
create a set of training examples which directly
connect desired output texts to available applica-
tion semantic forms. This is achieved through a
streamlined authoring task that does not require de-
tailed linguistic knowledge. Our approach then
processes these training examples to automatically
construct all the resources needed for a fast, high-
quality, run-time grammar-based generation compo-
nent. We evaluate this approach using a pre-existing
spoken dialogue system. Our results demonstrate
the viability of the approach and illustrate author-
ing/performance trade-offs between hand-authored
text, our grammar-based approach, and a competing
shallow statistical NLG technique.
2 Background and Motivation
The generation approach set out in this paper has
been developed in the context of a research pro-
gram aimed at creating interactive virtual humans
for social training purposes (Swartout et al, 2006).
Virtual humans are embodied conversational agents
that play the role of people in simulations or games.
They interact with human users and other virtual hu-
Figure 1: Doctor Perez.
mans using spoken language and non-verbal behav-
ior such as eye gaze, gesture, and facial displays.
The case study we present here is the genera-
tion of output utterances for a particular virtual hu-
man, Doctor Perez (see Figure 1), who is designed
to teach negotiation skills in a multi-modal, multi-
party, non-team dialogue setting (Traum et al, 2005;
Traum et al, 2008). The human trainee who talks
to the doctor plays the role of a U.S. Army captain
named Captain Kirk. We summarize Doctor Perez?s
generation requirements as follows.
In order to support compelling real-time conver-
sation and effective training, the generator must be
able to identify an utterance for Doctor Perez to use
within approximately 200ms on modern hardware.
Doctor Perez has a relatively rich internal men-
tal state including beliefs, goals, plans, and emo-
tions. As Doctor Perez attempts to achieve his con-
versational goals, his utterances need to take a va-
riety of syntactic forms, including simple declar-
ative sentences, various modal constructions relat-
ing to hypothetical actions or plans, yes/no and wh-
questions, and abbreviated dialogue forms such as
elliptical clarification and repair requests, ground-
ing, and turn-taking utterances. Doctor Perez cur-
rently uses about 200 distinct output utterances in
the course of his dialogues.
Doctor Perez is designed to simulate a non-native
English speaker, so highly fluent output is not a ne-
cessity; indeed, a small degree of disfluency is even
desirable in order to increase the realism of talking
to a non-native speaker.
Finally, in reasoning about user utterances, dia-
logue management, and generation, Doctor Perez
199
26
6
6
6
6
6
6
6
6
6
6
6
6
6
6
6
6
6
6
6
6
6
6
6
6
6
4
addressee captain-kirk
dialogue-act
2
6
4
addressee captain-kirk
type assign-turn
actor doctor-perez
3
7
5
speech-act
2
6
6
6
6
6
6
6
6
6
6
6
6
6
6
6
4
actor doctor-perez
addressee captain-kirk
action assert
content
2
6
6
6
6
6
6
6
6
4
type state
polarity negative
time present
attribute resourceAttribute
value medical-supplies
object-id market
3
7
7
7
7
7
7
7
7
5
3
7
7
7
7
7
7
7
7
7
7
7
7
7
7
7
5
3
7
7
7
7
7
7
7
7
7
7
7
7
7
7
7
7
7
7
7
7
7
7
7
7
7
7
5
addressee captain-kirk
dialogue-act.addressee captain-kirk
dialogue-act.type assign-turn
dialogue-act.actor doctor-perez
speech-act.actor doctor-perez
speech-act.addressee captain-kirk
speech-act.action assert
speech-act.content.type state
speech-act.content.polarity negative
speech-act.content.time present
speech-act.content.attribute resourceAttribute
speech-act.content.value medical-supplies
speech-act.content.object-id market
(a) Attribute-value matrix (b) Corresponding frame
Figure 2: An example of Doctor Perez?s representations for utterance semantics: Doctor Perez tells the captain that
there are no medical supplies at the market.
exploits an existing semantic representation scheme
that has been utilized in a family of virtual humans.
This scheme uses an attribute-value matrix (AVM)
representation to describe an utterance as a set of
core speech acts and other dialogue acts. Speech
acts generally have semantic contents that describe
propositions and questions about states and actions
in the domain, as well as other features such as po-
larity and modality. See (Traum, 2003) for some
more details and examples of this representation.
For ease of interprocess communication, and certain
kinds of statistical processing, this AVM structure is
linearized so that each non-recursive terminal value
is paired with a path from the root to the final at-
tribute. Thus, the AVM in Figure 2(a) is represented
as the ?frame? in Figure 2(b).
Because the internal representations that make up
Doctor Perez?s mental state are under constant de-
velopment, the exact frames that are sent to the gen-
eration component change frequently as new rea-
soning capabilities are added and existing capabil-
ities are reorganized. Additionally, while only hun-
dreds of frames currently arise in actual dialogues,
the number of potential frames is orders of magni-
tude larger, and it is difficult to predict in advance
which frames might occur.
In this setting, over a period of years, a number
of different approaches to natural language gener-
ation have been implemented and tested, including
hand-authored canned text, domain specific hand-
built grammar-based generators (e.g., (Traum et al,
2003)), shallow statistical generation techniques,
and the grammar-based approach presented in this
paper. We now turn to the details of our approach.
3 Technical Approach
Our approach builds on recently developed tech-
niques in statistical parsing, lexicalized syntax mod-
eling, generation with lexicalized grammars, and
search optimization to automatically construct all
the resources needed for a high-quality run-time
generation component.
The approach involves three primary steps: spec-
ification of training examples, grammar induction,
and search optimization. In this section, we present
the format that training examples take and then sum-
marize the subsequent automatic processing steps.
Due to space limitations, we omit the full details
of these automatic processing steps, and refer the
reader to (DeVault et al, 2008) for additional details.
3.1 Specification of Training Examples
Each training example in our approach speci-
fies a target output utterance (string), its syn-
tax, and a set of links between substrings within
the utterance and system semantic representa-
tions. Formally, a training example takes the form
(u, syntax(u), semantics(u)). We will illustrate
this format using the training example in Figure 3.
In this example, the generation content author
200
Utterance we don?t have medical supplies here captain
Syntax
cat: SA??
cat: S??
cat: NP??
pos: PRP??
we
cat: VP??
pos: AUX??
do
pos: RB??
n?t
cat: VP??
pos: AUX??
have
cat: NP??
pos: JJ??
medical
pos: NNS??
supplies
cat: ADVP??
pos: RB??
here
cat: NP??
pos: NN??
captain
Semantics
we do n?t . . . . . . . . .
{
speech-act.action = assert
speech-act.content.polarity = negative
have . . . . . . . . . . . . . speech-act.content.attribute = resourceAttribute
medical supplies . . speech-act.content.value = medical-supplies
here . . . . . . . . . . . . . speech-act.content.object-id = market
captain . . . . . . . . . .
?
?
?
addressee = captain-kirk
dialogue-act.addressee = captain-kirk
speech-act.addressee = captain-kirk
Figure 3: A generation training example for Doctor Perez.
suggests the output utterance u = we don?t have
medical supplies here captain. Each utterance u is
accompanied by syntax(u), a syntactic analysis in
Penn Treebank format (Marcus et al, 1994). In this
example, the syntax is a hand-corrected version of
the output of the Charniak parser (Charniak, 2001;
Charniak, 2005) on this sentence; we discuss this
hand correction in Section 4.
To represent the meaning of utterances, our ap-
proach assumes that the system provides some set
M = {m1, ...,mj} of semantic representations.
The meaning of any individual utterance is then
identified with some subset of M . For Doctor Perez,
M comprises the 232 distinct key-value pairs that
appear in the system?s various generation frames. In
this example, the utterance?s meaning is captured by
the 8 key-value pairs indicated in the figure.
Our approach requires the generation content
author to link these 8 key-value pairs to con-
tiguous surface expressions within the utterance.
The technique is flexible about which surface ex-
pressions are chosen (e.g. they need not corre-
spond to constituent boundaries); however, they do
need to be compatible with the way the syntactic
analysis tokenizes the utterance, as follows. Let
t(u) = ?t1, ..., tn? be the terminals in the syn-
tactic analysis, in left-to-right order. Formally,
semantics(u) = {(s1,M1), ..., (sk,Mk)}, where
t(u) = s1@ ? ? ?@sk (with @ denoting concatena-
tion), and where Mi ? M for all i ? 1..k. In this
example, the surface expression we don?t, which to-
kenizes as ?we,do,n?t?, is connected to key-values
that indicate a negative polarity assertion.
This training example format has two features that
are crucial to our approach. First, the semantics of
an utterance is specified independently of its syntax.
This greatly reduces the amount of linguistic exper-
tise a generation content author needs to have. It
also allows making changes to the underlying syn-
tax without having to re-author the semantic links.
Second, the assignment of semantic representa-
tions to surface expressions must span the entire ut-
terance. No words or expressions can be viewed as
?meaningless?. This is essential because, otherwise,
the semantically motivated search algorithm used in
generation has no basis on which to include those
particular expressions when it constructs its output
utterance. Many systems, including Doctor Perez,
lack some of the internal representations that would
be necessary to specify semantics down to the lex-
ical level. An important feature of our approach is
that it allows an arbitrary semantic granularity to be
employed, by mapping the representations available
in the system to appropriate multi-word chunks.
201
3.2 Automatic Grammar Induction and Search
Optimization
The first processing step is to induce a productive
grammar from the training examples. We adopt the
probabilistic tree-adjoining grammar (PTAG) for-
malism and grammar induction technique of (Chi-
ang, 2003). We induce our grammar from training
examples such as Figure 3 using heuristic rules to
assign derivations to the examples, as in (Chiang,
2003). Once derivations have been assigned, sub-
trees within the training example syntax are incre-
mentally detached. This process yields the reusable
linguistic resources in the grammar, as well as the
statistical model needed to compute operation prob-
abilities when the grammar is later used in genera-
tion. Figure 5 in the Appendix illustrates this pro-
cess by presenting the linguistic resources inferred
from the training example of Figure 3.
Our approach uses this induced grammar to treat
generation as a search problem: given a desired se-
mantic representation M ? ? M , use the grammar
to incrementally construct an output utterance u that
expressesM ?. We treat generation as anytime search
by accruing multiple goal states up until a specified
timeout (200ms for Doctor Perez) and returning a
list of alternative outputs ranked by their derivation
probabilities.
The search space created by a grammar induced
in this way is too large to be searched exhaustively
in most applications. The second step of automated
processing, then, uses the training examples to learn
an effective search policy so that good output sen-
tences can be found in a reasonable time frame. The
solution we have developed employs a beam search
strategy that uses weighted features to rank alterna-
tive grammatical expansions at each step. Our al-
gorithm for selecting features and weights is based
on the search optimization algorithm of (Daum?
and Marcu, 2005), which decides to update feature
weights when mistakes are made during search on
training examples. We use the boosting approach of
(Collins and Koo, 2005) to perform feature selection
and identify good weight values.
4 Empirical Evaluation
In the introduction, we identified run-time speed, ad-
equacy of coverage, authoring burdens, and NLG re-
quest specification as important factors in the selec-
tion of a technology for a dialogue system?s NLG
component. In this section, we evaluate our tech-
nique along these four dimensions.
Hand-authored utterances. We collected a sam-
ple of 220 instances of frames that Doctor Perez?s
dialogue manager had requested of the generation
component in previous dialogues with users. Some
frames occurred more than once in this sample.
Each frame was associated with a single hand-
authored utterance. Some of these utterances arose
in human role plays for Doctor Perez; some were
written by a script writer; others were authored
by system builders to provide coverage for specific
frames. All were reviewed by a system builder for
appropriateness to the corresponding frame.
Training. We used these 220 (frame, utterance)
examples to evaluate both our approach and a shal-
low statistical method called sentence retriever (dis-
cussed below). We randomly split the examples
into 198 training and 22 test examples; we used the
same train/test split for our approach and sentence
retriever.
To train our approach, we constructed training ex-
amples in the format specified in Section 3.1. Syntax
posed an interesting problem, because the Charniak
parser frequently produces erroneous syntactic anal-
yses for utterances in Doctor Perez?s domain, but it
was not obvious how detrimental these errors would
be to overall generated output. We therefore con-
structed two alternative sets of training examples ?
one where the syntax of each utterance was the un-
corrected output of the Charniak parser, and another
where the parser output was corrected by hand (the
syntax in Figure 3 above is the corrected version).
Hand correction of parser output requires consider-
able linguistic expertise, so uncorrected output rep-
resents a substantial reduction in authoring burden.
The connections between surface expressions and
frame key-value pairs were identical in both uncor-
rected and corrected training sets, since they are in-
dependent of the syntax. For each training set, we
trained our generator on the 198 training examples.
We then generated a single (highest-ranked) utter-
ance for each example in both the test and training
sets. The generator sometimes failed to find a suc-
cessful utterance within the 200ms timeout; the suc-
cess rate of our generator was 95% for training ex-
202
amples and 80% for test examples. The successful
utterances were rated by our judges.
Sentence retriever is based on the cross-
language information retrieval techniques described
in (Leuski et al, 2006), and is currently in use for
Doctor Perez?s NLG problem. Sentence retriever
does not exploit any hierarchical syntactic analy-
sis of utterances. Instead, sentence retriever views
NLG as an information retrieval task in which a set
of training utterances are the ?documents? to be re-
trieved, and the frame to be expressed is the query.
At run-time, the algorithm functions essentially as a
classifier: it uses a relative entropy metric to select
the highest ranking training utterance for the frame
that Doctor Perez wishes to express. This approach
has been used because it is to some extent robust
against changes in internal semantic representations,
and against minor deficiencies in the training corpus,
but as with a canned text approach, it requires each
utterance to be hand-authored before it can be used
in dialogue. We trained sentence retriever on the 198
training examples, and used it to generate a single
(highest-ranked) utterance for each example in both
the test and training sets. Sentence retriever?s suc-
cess rate was 96% for training examples and 90%
for test examples. The successful utterances were
rated by our judges.
Figure 7 in the Appendix illustrates the alternative
utterances that were produced for a frame present in
the test data but not in the training data.
Run-time speed. Both our approach and sentence
retriever run within the available 200ms window.
Adequacy of Coverage. To assess output quality,
we conducted a study in which 5 human judges gave
overall quality ratings for various utterances Doctor
Perez might use to express specific semantic frames.
In total, judges rated 494 different utterances which
were produced in several conditions: hand-authored
(for the relevant frame), generated by our approach,
and sentence retriever.
We asked our 5 judges to rate each of the 494 ut-
terances, in relation to the specific frame for which
it was produced, on a single 1 (?very bad?) to 5
(?very good?) scale. Since ratings need to incorpo-
rate accuracy with respect to the frame, our judges
had to be able to read the raw system semantic rep-
resentations. This meant we could only use judges
who were deeply familiar with the dialogue system;
however, the main developer of the new generation
algorithms (the first author) did not participate as
a judge. Judges were blind to the conditions un-
der which utterances were produced. The judges
rated the utterances using a custom-built application
which presented a single frame together with 1 to 6
candidate utterances for that frame. The rating inter-
face is shown in Figure 6 in the Appendix. The order
of candidate utterances for each frame was random-
ized, and the order in which frames appeared was
randomized for each judge.
The judges were instructed to incorporate both
fluency and accuracy with respect to the frame into
a single overall rating for each utterance. While it
is possible to have human judges rate fluency and
accuracy independently, ratings of fluency alone are
not particularly helpful in evaluating Doctor Perez?s
generation component, since for Doctor Perez, a cer-
tain degree of disfluency can contribute to believ-
ability (as noted in Section 2). We therefore asked
judges to make an overall assessment of output qual-
ity for the Doctor Perez character.
The judges achieved a reliability of ? = 0.708
(Krippendorff, 1980); this value shows that agree-
ment is well above chance, and allows for tentative
conclusions. Agreement between subsets of judges
ranged from ? = 0.802 for the most concordant pair
of judges to ? = 0.593 for the most discordant pair.
We also performed an ANOVA comparing three
conditions (generated, retrieved and hand-authored
utterances) across the five judges; we found sig-
nificant main effects of condition (F (2, 3107) =
55, p < 0.001) and judge (F (4, 3107) = 17, p <
0.001), but no significant interaction (F (8, 3107) =
0.55, p > 0.8). We therefore conclude that the indi-
vidual differences among the judges do not affect the
comparison of utterances across the different condi-
tions, so we will report the rest of the evaluation on
the mean ratings per utterance.
Due to the large number of factors and the dif-
ferences in the number of utterances correspond-
ing to each condition, we ran a small number
of planned comparisons. The distribution of rat-
ings across utterances is not normal; to validate
our results we accompanied each t-test by a non-
parametric Wilcoxon rank sum test, and signifi-
cance always fell in the same general range. We
found a significant difference between generated
203
Generated (N = 90)
Sentence retriever (N = 100)
Rating
Fr
eq
u
en
cy
(%
)
0
10
20
30
40
1 2 3 4 5
Figure 4: Observed ratings of generated (uncorrected
syntax) vs. retrieved sentences for test examples.
output for all examples, retrieved output for all ex-
amples, and hand-authored utterances (F (2, 622) =
16, p < 0.001); however, subsequent t-tests show
that all of this difference is due to the fact that hand-
authored utterances (mean rating 4.4) are better than
retrieved (t(376) = 3.7, p < 0.001) and gener-
ated (t(388) = 5.9, p < 0.001) utterances, whereas
the difference between generated (mean rating 3.8)
and retrieved (mean rating 4.0) is non-significant
(t(385) = 1.6, p > 0.1).
Figure 4 shows the observed rating frequencies
of sentence retriever (mean 3.0) and our approach
(mean 3.6) on the test examples. While this data
does not show a significant difference, it suggests
that retriever?s selected sentences are most fre-
quently either very bad or very good; this reflects
the fact that the classification algorithm retrieves
highly fluent hand-authored text which is sometimes
semantically very incorrect. (Figure 7 in the Ap-
pendix provides such an example, in which a re-
trieved sentence has the wrong polarity.) The qual-
ity of our generated output, by comparison, appears
more graded, with very good quality the most fre-
quent outcome and lower qualities less frequent. In
a system where there is a low tolerance for very
bad quality output, generated output would likely be
considered preferable to retrieved output.
In terms of generation failures, our approach had
poorer coverage of test examples than sentence re-
triever (80% vs. 90%). Note however that in this
study, our approach only delivered an output if it
could completely cover the requested frame. In the
future, we believe coverage could be improved, with
perhaps some reduction in quality, by allowing out-
puts that only partially cover requested frames.
In terms of output variety, in this initial study our
judges rated only the highest ranked output gener-
ated or retrieved for each frame. However, we ob-
served that our generator frequently finds several al-
ternative utterances of relatively high quality (see
Figure 7); thus our approach offers another poten-
tial advantage in output variety.
Authoring burdens. Both canned text and sen-
tence retriever require only frames and correspond-
ing output sentences as input. In our approach, syn-
tax and semantic links are additionally needed. We
compared the use of corrected vs. uncorrected syn-
tax in training. Surprisingly, we found no significant
difference between generated output trained on cor-
rected and uncorrected syntax (t(29) = 0.056, p >
0.9 on test items, t(498) = ?1.1, p > 0.2 on all
items). This is a substantial win in terms of reduced
authoring burden for our approach.
If uncorrected syntax is used, the additional bur-
den of our approach lies only in specifying the se-
mantic links. For the 220 examples in this study,
one system builder specified these links in about 6
hours. We present a detailed cost/benefit analysis of
this effort in (DeVault et al, 2008).
NLG request specification. Both our approach
and sentence retriever accept the dialogue manager?s
native semantic representation for NLG as input.
Summary. In exchange for a slightly increased
authoring burden, our approach yields a generation
component that generalizes to unseen test problems
relatively gracefully, and does not suffer from the
frequent very bad output or the necessity to author
every utterance that comes with canned text or a
competing statistical classification technique.
5 Conclusion and Future Work
In this paper we have presented an approach to spec-
ifying domain-specific, grammar-based generation
by example. The method reduces the authoring bur-
den associated with developing a grammar-based
NLG component for an existing dialogue system.
We have argued that the method delivers relatively
high-quality, domain-specific output without requir-
ing that content authors possess detailed linguistic
knowledge. In future work, we will study the perfor-
204
mance of our approach as the size of the training set
grows, and assess what specific weaknesses or prob-
lematic disfluencies, if any, our human rating study
identifies in output generated by our technique. Fi-
nally, we intend to evaluate the performance of our
generation approach within the context of the com-
plete, running Doctor Perez agent.
Acknowledgments
Thanks to Arno Hartholt, Susan Robinson, Thomas
Russ, Chung-chieh Shan, and Matthew Stone. This
work was sponsored by the U.S. Army Research,
Development, and Engineering Command (RDE-
COM), and the content does not necessarily reflect
the position or the policy of the Government, and no
official endorsement should be inferred.
References
Stephen Busemann and Helmut Horacek. 1998. A flex-
ible shallow approach to text generation. In Proceed-
ings of INLG, pages 238?247.
Charles B. Callaway. 2003. Evaluating coverage for
large symbolic NLG grammars. Proceedings of the
International Joint Conferences on Artificial Intelli-
gence.
Eugene Charniak. 2001. Immediate-head parsing for
language models. In ACL ?01: Proceedings of the
39th Annual Meeting on Association for Computa-
tional Linguistics, pages 124?131, Morristown, NJ,
USA. Association for Computational Linguistics.
Eugene Charniak. 2005.
ftp://ftp.cs.brown.edu/pub/nlparser/
parser05Aug16.tar.gz.
David Chiang. 2003. Statistical parsing with an auto-
matically extracted tree adjoining grammar. In Rens
Bod, Remko Scha, and Khalil Sima?an, editors, Data
Oriented Parsing, pages 299?316. CSLI Publications,
Stanford.
Michael Collins and Terry Koo. 2005. Discrimina-
tive reranking for natural language parsing. Compu-
tational Linguistics, 31(1):25?70.
Hal Daum?, III and Daniel Marcu. 2005. Learning as
search optimization: approximate large margin meth-
ods for structured prediction. In ICML ?05: Proceed-
ings of the 22nd international conference on Machine
learning, pages 169?176, New York, NY, USA. ACM.
David DeVault, David Traum, and Ron Artstein. 2008.
Practical grammar-based NLG from examples. In
Fifth International Natural Language Generation
Conference (INLG).
Michael Elhadad. 1991. FUF: the universal unifier user
manual version 5.0. Technical Report CUCS-038-91.
Klaus Krippendorff, 1980. Content Analysis: An Intro-
duction to Its Methodology, chapter 12, pages 129?
154. Sage, Beverly Hills, CA.
Irene Langkilde and Kevin Knight. 1998. Generation
that exploits corpus-based statistical knowledge. In
COLING-ACL, pages 704?710.
I. Langkilde-Geary. 2002. An empirical verification of
coverage and correctness for a general-purpose sen-
tence generator.
Anton Leuski, Ronakkumar Patel, David Traum, and
Brandon Kennedy. 2006. Building effective question
answering characters. In The 7th SIGdial Workshop
on Discourse and Dialogue.
Mitchell P. Marcus, Beatrice Santorini, and Mary Ann
Marcinkiewicz. 1994. Building a large annotated cor-
pus of english: The penn treebank. Computational
Linguistics, 19(2):313?330.
E. Reiter, S. Sripada, and R. Robertson. 2003. Acquir-
ing correct knowledge for natural language generation.
Journal of Artificial Intelligence Research, 18:491?
516.
William Swartout, Jonathan Gratch, Randall W. Hill, Ed-
uard Hovy, Stacy Marsella, Jeff Rickel, and David
Traum. 2006. Toward virtual humans. AI Mag.,
27(2):96?108.
David Traum, Michael Fleischman, and Eduard Hovy.
2003. Nl generation for virtual humans in a complex
social environment. In Working Notes AAAI Spring
Symposium on Natural Language Generation in Spo-
ken and Written Dialogue, March.
David Traum, William Swartout, Jonathan Gratch,
Stacy Marsella, Patrick Kenny, Eduard Hovy, Shri
Narayanan, Ed Fast, Bilyana Martinovski, Rahul
Baghat, Susan Robinson, Andrew Marshall, Dagen
Wang, Sudeep Gandhe, and Anton Leuski. 2005.
Dealing with doctors: A virtual human for non-team
interaction. In SIGdial.
D. R. Traum, W. Swartout, J Gratch, and S Marsella.
2008. A virtual human dialogue model for non-team
interaction. In Laila Dybkjaer and Wolfgang Minker,
editors, Recent Trends in Discourse and Dialogue.
Springer.
David Traum. 2003. Semantics and pragmatics of ques-
tions and answers for dialogue agents. In proceedings
of the International Workshop on Computational Se-
mantics, pages 380?394, January.
Michael White, Rajakrishnan Rajkumar, and Scott Mar-
tin. 2007. Towards broad coverage surface realiza-
tion with CCG. In Proc. of the Workshop on Using
Corpora for NLG: Language Generation and Machine
Translation (UCNLG+MT).
Huayan Zhong and Amanda Stent. 2005. Building
surface realizers automatically from corpora using
general-purpose tools. In Proc. Corpus Linguistics ?05
Workshop on Using Corpora for Natural Language
Generation.
205
syntax:
cat: SA??
fin: other,?? cat: S
cat: NP,?? apr: VBP,
apn: other??
pos: PRP??
we
fin: yes,?? cat: VP
apn: other,?? pos: VBP
do
pos: RB??
n?t
fin: yes,?? cat: VP,
gra: obj1??
fin: yes,?? cat: VP,
gra: obj1??
pos: VBP??
have
cat: NP,?? gra: obj1
operations: initial tree comp
semantics: speech-act.action = assert
speech-act.content.polarity = negative
speech-act.content.attribute = resourceAttribute
syntax:
cat: NP,?? apr: VBP,
gra: obj1,?? apn: other
pos: JJ??
medical
pos: NNS??
supplies
cat: ADVP,?? gra: adj
pos: RB??
here
cat: NP,?? apr: VBZ,
gra: adj,?? apn: 3ps
pos: NN??
captain
operations: comp left/right adjunction left/right adjunction
semantics: speech-act.content.value =
medical-supplies
speech-act.content.object-id =
market
addressee = captain-kirk
dialogue-act.addressee =
captain-kirk
speech-act.addressee =
captain-kirk
Figure 5: The linguistic resources automatically inferred from the training example in Figure 3.
Figure 6: Human rating interface.
206
Input semantic form
addressee captain-kirk
dialogue-act.actor doctor-perez
dialogue-act.addressee captain-kirk
dialogue-act.type assign-turn
speech-act.action assert
speech-act.actor doctor-perez
speech-act.addressee captain-kirk
speech-act.content.attribute acceptableAttribute
speech-act.content.object-id clinic
speech-act.content.time present
speech-act.content.type state
speech-act.content.value yes
Outputs
Hand-authored
the clinic is acceptable captain
Generated (uncorrected syntax)
Rank Time (ms)
1 16 the clinic is up to standard captain
2 94 the clinic is acceptable captain
3 78 the clinic should be in acceptable condition captain
4 16 the clinic downtown is currently acceptable captain
5 78 the clinic should agree in an acceptable condition captain
Generated (corrected syntax)
Rank Time (ms)
1 47 it is necessary that the clinic be in good condition captain
2 31 i think that the clinic be in good condition captain
3 62 captain this wont work unless the clinic be in good condition
Sentence retriever
the clinic downtown is not in an acceptable condition captain
Figure 7: The utterances generated for a single test example by different evaluation conditions. Generated outputs
whose rank (determined by derivation probability) was higher than 1 were not rated in the evaluation reported in this
paper, but are included here to suggest the potential of our approach to provide a variety of alternative outputs for the
same requested semantic form. Note how the output of sentence retriever has the opposite meaning to that of the input
frame.
207
Proceedings of the 8th International Conference on Computational Semantics, pages 4?17,
Tilburg, January 2009. c?2009 International Conference on Computational Semantics
A computational account of comparative
implicatures for a spoken dialogue agent
?
Luciana Benotti
LORIA/INRIA, France
benottil@loria.fr
David Traum
ICT/USC, USA
traum@ict.usc.edu
Abstract
Comparative constructions are common in dialogue, especially in
negotiative dialogue where a choice must be made between different
options, and options must be evaluated using multiple metrics. Com-
paratives explicitly assert a relationship between two elements along a
scale, but they may also implicate positions on the scale especially if
constraints on the possible values are present. Dialogue systems must
often understand more from a comparative than the explicit assertion
in order to understand why the comparative was uttered. In this paper
we examine the pragmatic meaning of comparative constructions from
a computational perspective.
1 Introduction
It is a big challenge for computational semantics of dialogue that much of
the meaning of an utterance is conveyed not just through the compositional
meanings of the words themselves, but in relation to the situation in which
the utterance is performed, including the background knowledge, common
ground, goals, and ontologies of the participants. A number of pragmatic
principles have been proposed to bridge this gap, including Grice?s maxims
(and the associated concepts of Implicature and Relevance)
[
6
]
, Accommo-
dation
[
12
]
and Bridging
[
2
]
.
?
This work was sponsored by the U.S. Army Research, Development, and Engineering
Command (RDECOM). Statements and opinions expressed do not necessarily reflect the
position or the policy of the United States Government, and no official endorsement should
be inferred.
4
While these principles can provide elegant explanations of how meaning
can be conveyed between people, they often require fairly strong assump-
tions about the common knowledge between participants and the ontologies
that this knowledge must be organized in. There are two general prob-
lems in developing computational accounts of these phenomena. First, it is
sometimes difficult to specify the required knowledge and relationships in a
computational way, such that a reasoner given the input and context can
compute a particular, specific meaning as opposed to other possibilities that
are not as appropriate. Second, even if the principles are sufficiently clear so
that a computational account can be formulated, there may still be a prob-
lem providing a given computational dialogue system with the appropriate
knowledge to carry out the inferences in a way that is congruent with human
interpretations. For a hand-constructed limited domain, the system designer
will often take shortcuts and represent only the knowledge that is necessary
to carry out and understand tasks in that domain. These limitations often
render the dialogue system unable to reason about the domain in as much
detail as a knowledgeable human would, but often this characterization is
sufficient for the purposes of the conversation.
In this paper we examine one aspect of computational non-compositional
meaning: the pragmatic meaning of comparative constructions. Compara-
tive constructions are common in dialogue, especially in negotiative dialogue
where a choice must be made between different options, and options must
be evaluated using multiple metrics. Comparatives explicitly assert a rela-
tionship between two elements along a scale, but they may also implicate
positions on the scale especially if either information about the position of
one of the compared items or constraints on the possible values are present.
Dialogue systems must often understand more from a comparative than the
explicit assertion in order to understand why the comparative was uttered;
why it is relevant to the dialogue.
In the next section, we present linguistic background on comparatives
and conversational implicature. In section 3, we review some issues of how
implicatures play out in dialogue in which multiple participants are involved,
and a listener can clarify a lack of understanding. In section 4, we introduce
the computational framework in which the present work is implemented.
In section 5, we present extensions to this framework which provide the
computational agents with the ability to understand implicatures arising
from comparatives. In section 6 we evaluate this with respect to the scenario
of SASO-EN
[
15
]
, in which an army captain, a doctor, and a town elder
discuss the best location for a medical clinic. Finally, we discuss related
issues, remaining problems, and future work in Section 7.
5
2 Linguistic background
In this section we review some of the previous work on comparatives con-
structions and conversational implicatures in order to establish the necessary
theoretical basis for the discussion on comparative implicatures in the rest
of the paper.
2.1 Comparative constructions
In the classical literature on semantics of natural language comparatives (see
e.g.,
[
4
]
), comparative constructions such as (1) are analyzed as a relation
between two degrees as in (2).
(1) Downtown is safer than the market.
(2) Degree to which downtown is safe > Degree to which the market is
safe.
The problem now, of course, is what a degree is and what are the prop-
erties of the relation >. Abstractly, a degree can be considered just as a
collection of objects ? the collection of those objects that share the same
degree with respect to a given property P . These collections are defined as
the equivalence classes of the equivalence relation =
P
, which in turn is de-
fined in terms of an order >
P
among objects. The relation > among degrees
is defined as a lifting of >
P
to the set of equivalent classes of =
P
. Finally,
scales are defined in terms of degrees, a scale S
P
is a sequence of degrees of
P ordered by the relation >.
Summing up then, once we know what >
P
is for a property P we know
what degrees of P are and how to compare them on scale S
P
. All is good
and well, but this approach assumes the relation >
P
as given. Such a strong
assumption was already criticized by
[
9
]
and certainly cannot be made in a
dialogue system where information about the domain of discourse (in partic-
ular, any order >
P
for a given property P ) is incomplete and is constructed
(and negotiated) during the dialogue. As dialogue system builders, the is-
sue that interests us is, not so much how to determine the truth value of a
particular comparative utterance, but mainly how comparatives contribute
to the construction of the information about the domain. So, for our task
it is crucial to figure out ?where do scales come from??
6
2.2 Conversational implicatures
Modelling how listeners draw inferences from what they hear is a basic
problem for the theories of understanding natural language. An important
part of the information conveyed is inferred, as in the following classical
example by Grice
[
6
]
:
(3) A: I am out of petrol.
B: There is a garage around the corner.
; B thinks that the garage is open and has petrol to sell.
B?s answer conversationally implicates (;) information that is relevant
to A. In Grice?s terms, B made a relevance implicature, he would be flouting
Grice?s maxim of relevance unless he believes that the garage is open. A
conversational implicature (CI) is different from an entailment in that it is
cancelable without contradiction (B can append material that is inconsistent
with the CI ??but I don?t know whether it?s open?). Since the CI can be
cancelled, B knows that it does not necessarily hold and then both B or
A are able to reinforce it (or negotiate it) without repetition. CIs are non-
conventional, they are not part of the conventional meaning of the words but
calculable from their utterance in context given the nature of conversation
as a goal-oriented enterprise.
Scalar implicatures are a type of CI that are particularly relevant to
this paper since they involve the use of scales, as comparatives do. These
implicatures are inferred on the basis of the assumption that the speaker is
trying to make his utterance sufficiently informative for the current purposes
of the exchange. A typical example is:
(4) A is hungry (and B knows it).
A: Did somebody eat the brownies that I bought this morning?
B: Fred ate some of them.
; B thinks that Fred didn?t eat all of them, there are some left.
Theories of scalar implicature have been deeply influenced by Horn?s
dissertation work
[
8
]
. A Horn scale is an ordered sequence of expressions
such as ?some,many,most, all? and ?warm, hot, scalding?. The recipe to
calculate the scalar implicature is the following. The use of a weaker word
in the scale (such as some) implicates that (the speaker believes that) the
stronger form (such as all) is not the case, as exemplified in (4). However,
there are cases in which such a recipe does not apply, such as in (5).
7
(5) A realizes that the brownies were injected with strychnine and suspects
that somebody may have eaten from them (and B knows it).
A: Did somebody eat the brownies that I bought this morning?
B: Fred ate some of them.
In this example the scalar implicature is (at the very least) less likely to
arise, it?s not relevant for the current purposes of the exchange; it doesn?t
matter how many brownies Fred ate, he needs medical attention. Comparing
(4) and (5) we can see that conversational implicatures are affected by the
conversational goals. We believe this to be an starting point for answering
the question left open in Section 2.1: ?Where do scales come from?? and
we will investigate it in next section.
3 Comparative implicatures in dialogue
In this Section we are going to introduce comparative implicatures and to
develop two lines of thought introduced in the previous section, but now in
the context of dialogue. First, we relate the cancellability and negotiability
of CIs and clarification requests in dialogue. It?s often controversial whether
something is actually a CI or not (people sometimes have different intu-
itions). Dialogue provide us with an extra tools for identifying the partici-
pants? beliefs about implicatures: feedback and negotiation of CIs. Listeners
can give both positive (e.g. acknowledgements) and negative (e.g. clarifi-
cation requests) feedback about their understanding and the acceptability
of utterances, which can shed light on how they interpret CIs. Moreover,
speakers can negotiate whether something is really implicated, or whether
an implicature is meant by the speaker. We also investigate the fact that
conversational implicatures change when the conversational goals change.
Conversational goals establish structure on the information that is being
discussed, determining which distinctions are relevant and which are not for
the a given interaction.
3.1 Clarifications
In dialogue, a useful way to spot content that was meant but not actually
said is to look at the kinds of dialogue feedback that is given. Clarification
requests are one important part of this - focusing on cases where the initial
listener is not sure of what the speaker means. Here we use the phrase
clarification requests in its broad sense, including all those questions that
would not make sense without the previous utterance in the dialogue.
8
Many times the clarification requests make the implicatures explicit, this
is illustrated by the fact that the clarification request in (6) can follow Grice?s
example in (3).
(6) A: and you think it?s open?
Here we present clarifications as a test that helps us identify not only
the potential conversational implicatures but also the presuppositions in a
dialogue. (7) is an example of a well studied case of inferrable content: the
presupposition triggered by definite descriptions. In this exchange, A might
believe that the intended referent is salient for different reasons: because it
was mentioned before, because it?s bigger, etc. If such a piece of information
is available to B then he will be able to infer the presupposition and add
it to the conversational record. Otherwise, B might well signal A that the
presupposition did not get through, like in (7).
(7) There are 100 cups on a table.
A: Pick up THE cup.
B: Which cup? (I don?t know which one you are referring to.)
The presence of a conversational implicature must be capable of being
worked out, so the hearer might well inquire about them. The speaker will
have to answer and support the implicature if he wants to get it added to
the common ground. In
[
13
]
, the authors present a study of the different
kinds of clarification requests (CRs) found in a dialogue corpus. Their re-
sults show that the most frequent CRs in their corpus (51.74%) are due to
problems with referring expressions (such as example (7)) and the second
most common (22.17%) are examples like the following.
(8) A: Turn it on.
B: By pushing the red button?
Here, the CR made explicit a potential requirement for the performance
of the requested action. By saying ?Turn it on? A meant that B had to
push the red button but this was not explicitly mentioned in the command
uttered by A. Such an implicature can be inferred from knowledge about the
task (or in Clark terms, from knowledge about the interaction level of joint
action
[
3
]
). From this examples it should be clear that implicated content
is a rich source of CRs.
9
3.2 Goals and Contextual Scales
As described in Section 2, comparative sentences explicitly claim an ordering
of two items on a relevant scale. However, with more context, a comparative
can implicate degrees on a scale as well. If we know the degree of one of
the items then we have a fixed range for the other item. If this range
contains only one possible degree, then the comparative also sets the degree
for this item. In the simplest possible scale in which a comparative could be
applied, there are only two degrees, and a comparative indicates the degrees
of each item even without more knowledge of the degrees of either item. For
example, consider (9) inspired by
[
5
]
:
(9) B is drawing and A is giving instructions to B:
A: Draw two squares, one bigger than the other.
B: Done.
A: Now, paint the small square blue and the big one red.
; A thinks that one of the squares can be described as small and the
other as big.
The question remains, however, of how the scales are selected? These
are not generalized implicatures that always arise. Every time we say ?Fred
is taller than Tom? we don?t mean that ?Fred is tall? and ?Tom is short?.
As discussed in Section 2, conversational implicatures are affected by con-
versational goals. We believe that this is the path that we have to follow in
order to explain comparative implicatures as particularized implicatures.
Predicates such as tall and small are vague in that they can refer to
different ranges of a scale and in fact different scales. Small refers to size,
which as a default we might think of as a continuous scale measure, however,
as in the example above, we might prefer to use a simpler scale. This
phenomena was already noticed by
[
7
]
who says:
There is a range of structures we can impose on scales. These
map complex scales into simpler scales. For example, in much
work in qualitative physics the actual measurement of some pa-
rameter may be anything on the real line, but this is mapped
into one of three values ? positive, zero, and negative.
How are such structures over scales defined? The intuitive idea is that
the structure only distinguishes those values that are relevant for the goals
of the agent; that is when the attribute takes a particular value, it plays a
causal role in deciding whether some agent?s goal can be achieved or not.
10
In the example above, we have only small and big as relevant values for
size, and a comparative in this context will implicate that we are using this
binary scale as well as the degrees that each of the items of comparison have.
4 Computational framework
The computational framework we have used for the implementation is the
ICT Virtual Human dialogue manager
[
14; 16
]
. This dialogue manager al-
lows virtual humans to participate in bilateral or multiparty task-oriented
conversations in a variety of domains, including teamand non-teamnegotiation.
This dialogue manager follows the information state approach
[
11
]
, with a
rich set of dialogue acts at different levels. The dialogue manager is em-
bedded within the Soar cognitive architecture
[
10
]
, and decisions about in-
terpreting and producing speech compete with other cognitive and physical
operations. When an utterance has been perceived (either from processing of
Automatic Speech Recognizer and Natural Language Understanding (NLU)
components which present hypotheses of context-independent semantic rep-
resentations, or messages from self or other agents), the dialogue manager
processes these utterances, making decisions about pragmatic information
such as the set of speech acts that this utterance performs as well as the
meanings of referring expressions. Updates are then performed to the in-
formation state on the basis of the recognized acts. The representations of
semantic and pragmatic elements for questions and assertions are presented
in
[
14
]
. The semantic structure derives from the task model used for plan-
ning, emotion and other reasoning. (10a) shows an example proposition in
this domain, where propositions have an object id, an attribute, and a value
(whose type is defined by the attribute). (10b) shows an assertion speech
act, with this proposition as content.
(10) a. (<prop1> ?attribute safety ?object market ?type state ?value no)
b. (<da1> ?action assert ?actor doctor ?addressee captain ?content
<prop1>)
In the next section, we extend this framework to allow comparative
propositions and speech acts arising from implicatures of comparatives.
5 Implementing comparative implicatures
We have added an ability for the ICT dialogue manager to handle compara-
tive constructions and comparative implicatures. Some of the attributes in
11
the task model for a given domain will be scalar, where there is some implied
ordering of the possible values. This will not be the case for all attributes,
for example, the location allows for different possible places that an entity
can be, but there is no scale among them. For each scalar attribute, P , in
the task model of our domain, we can create a comparative attribute >
P
that will compare objects and values according to the designated scale for
P . (11a) means that X is higher on the P scale than Y. In our system, when
a comparative of the form ?X is P -er than Y ? is uttered, the NLU gener-
ates a semantic frame that has the structure shown in (11a). The dialogue
manager will then create an assertion speech act as represented in (11b) and
then infer the conversational implicatures that arise.
(11) a. (<prop1> ?attribute >
P
?object X ?type state ?value Y )
b. (<da1> ?action assert ?actor A ?addressee B ?content <prop1>)
The comparative implicatures depend on both the nature of the scale
and the available information about the positions of the compared items.
For the special case of a binary scale (e.g. yes > no), the comparative
construction itself can generate multiple implicatures, as in (12).
(12) a. By the definition of the scale S = ?no, yes? and its association with
the attribute P then we know that, if <prop> ?attribute P ?value
V , then V ? S
b. As the utterance asserts (<prop1> ?attribute >
P
?object X ?value
Y ), it is interpreted as asserting (<prop2> ?attribute P ?object
X ?value V 1) and (<prop3> ?attribute P ?object Y ?value V 2),
where values V 1 and V 2 are not known but it is known that V 1 >
V 2
From (12a) we have that V 1 ? S and V 2 ? S, from (12b) we have that
V 1 > V 2. Since S has 2 elements and yes > no, there is a unique valuation
for V 1 and V 2, namely V 1 = yes and V 2 = no. Once the values of V 1
and V 2 are determined the following two dialogue acts are generated as
part of the interpretation and reinserted in the dialogue manager cycle. The
information state will not only be updated with the comparative assertion
?X is P -er than Y ? but also with the two following dialogue acts. The first
one asserts that ?X is P?. And the second one asserts that ?Y is not P?.
(13) (<prop2> ?attribute P ?object X ?type state ?value yes)
(<da2> ?action assert ?actor A ?addressee B ?content <prop2>)
12
(14) (<prop3> ?attribute P ?object Y ?type state ?value no)
(<da3> ?action assert ?actor A ?addressee B ?content <prop3>)
This is the first part of the problem. The question now is when and
how to update the information state with (13) and (14): before, at the same
time or after the explicit assertion (11b)); we will discuss these issues in
next subsection. Moreover, in some contexts the conversational implicature
carried by the comparative will not get through, how are such information
states recognized and what is done instead will be addressed in Section 7.
5.1 When and how is the information state updated?
When interpreting an utterance U that has a conversational implicature I,
the dialogue manager needs to have accessible the content of the implicature
before generating a response. The three approaches presented below make
explicit the implicature in the dialogue, just as if its verbalization has been
uttered during the dialogue. Since the inferred content is made explicit,
these approaches can be seen as implementations of the Principle of Explicit
Addition
[
1
]
which applies both to accommodation of content that has been
lexically triggered (such as presuppositions) as well as content that has not
(such as conversational implicatures).
The after approach: A first approach is to add the inferred assertion I
as further input received by the dialogue manager once it finished
processing the utterance U . The idea for the implementation is simple:
just re-insert the frame of the inferred assertion(s) as an input to the
dialogue manager, as if it had been uttered in the dialogue.
At the same time: A second approach is to consider that a single asser-
tion performs multiple speech acts. The implementation of this option
in the current computational framework is also quite straightforward
because the framework already allows for multiple speech acts asso-
ciated with an utterance and then the information state is correctly
updated.
The before approach: The third approach is to interrupt the processing
of the explicit assertion U to update the information state first with the
implicature I. Such an implementation requires significant changes to
the dialogue manager to interrupt processing on the current assertion
and first interpret the inferred speech acts before re-interpreting.
13
The crucial difference among the three implementations is what content
will be available in the information state when the rest of the content has to
be interpreted. In the same time approach all the updates are independent,
the information state does not contain the implicatures, nor the explicit
assertion when interpreting any of them. In the after approach, U is avail-
able in the information state when interpreting I; by contrast, in the before
approach I is available when interpreting U .
These differences are relevant when they interact with other parts of the
interpretation process, such as reference resolution. Consider for instance
the following utterance:
(15) A: I went to the hospital Saint-Joseph, the British clinic was too far.
A: The doctor gave me some medicine
The utterance ?The doctor gave me some medicine? would normally
be interpreted as implicating that A saw a doctor in the hospital Saint-
Joseph. In order to properly interpret this utterance, it?s important that
the implicature is in the context before resolving the referring expression
?the doctor?. We leave for future work the interaction with other aspects
of interpretation, such as word sense disambiguation.
6 A case-study
We have evaluated the implementation in the context of the SASO-EN sce-
nario
[
15
]
, where both comparatives and binary scales are present in the task
model. This domain contains over 60 distinct states that the agents con-
sider as relevant for plan and negotiation purposes. There are currently 11
attributes used for this domain, 7 of which are binary scales and 4 of which
are non-scalar. Adding the comparative implicature rules from the previous
section allows understanding of additional arguments that were not previ-
ously dealt with adequately by the system. Consider the following fragment
of a dialogue among the Captain (a human agent), the Elder and the Doctor
(two virtual agents) about the location of a clinic.
(16) Captain: Doctor would you be willing to move the clinic downtown?
Doctor: It is better to keep the clinic here in the marketplace.
Captain: Well, downtown is safer than the market
Elder: Why do you think that the market is not safe?
During the interpretation of the comparative uttered by the Captain,
the dialogue manager receives the following semantic frame:
14
(17) (<prop> ?attribute safer ?object downtown ?type state ?value mar-
ket)
Then the inferred rules developed in Section 5 are applied and the in-
formation state is updated not only with an assertion of (17) but also with
the following two assertions:
(18) (<prop1> ?attribute safety ?object downtown ?type state ?value yes)
(<prop1> ?attribute safety ?object market ?type state ?value no)
Since these propositions assert the fact that the market is not safe before
the Elder generates a response, he can directly address and query the reason
for one of these implicatures with ?why do you think that the market is not
safe??
In the general case, we might have to reason about the appropriate scale
to use for this attribute, but in our domain, only one scale is relevant, so
this additional inference is not needed.
Without the comparative implicature rules, neither the elder nor the
doctor would recognize that the captain is asserting something about the
safety of each of the locations and will not be able to properly assess the
argument about desirability of moving the clinic.
7 Discussion
As mentioned in Section 3, the relevant information sources of a dialogue,
such as the task model, shed light on how the scales that are relevant for
calculating implicatures should be constructed. However, once comparative
implicatures are inferred they interact with the structure of the dialogue
in relevant ways. When implicated assertions are already in the context,
because they have been uttered before, the information state does not need
to be updated again with them.
If A says ? and implicates ? but A said ? before in the dialogue then
the context of the dialogue does not have to be updated again with ?. In
this case, we say that the implicature has been bound (to use standard
terminology in the area) and the information state is not modified by it.
The result is a coherent dialogue, a dialogue that gives the intuition of
?continuing on the same topic or stressing the same point? such as in the
following example:
(19) Captain: the market is not safe
Captain: downtown is safer than the market
15
The point can be better illustrated when the two contributions are not
made by the same speaker. In this case, A says ? and B says ? which
implicates ?, then ? has the effect of accepting ? and adding it to the
common ground. This implements the intuition that, in (20), the elder
seems to be supporting the captain in his claim that the market is not safe.
(20) Captain: the market is not safe
Elder: downtown is safer than the market
Finally, implicature cancellation is implemented in a simplistic way in
the current framework. When A says ? and implicates ? but ?? is already
in the common ground, the implicature is simply ignored.
Our implementation captures the intuition that the relevant values of
the properties in a domain are those that are causally related to the agent?s
goals. This is used in order to construct or locate appropriate scales and
infer useful implicatures that interact with the dialogue structure in different
ways. However, not all the predictions achieved by such an implementation
are explainable and further refinement is needed. Consider the following
dialogue:
(21) C: Downtown is safer than the market
C: The US base is safer than downtown
If we apply the inference rules developed in Section 5 to this exchange,
which implicature gets through will depend on the order in which the utter-
ances in (21) are said. If they are said in the order shown in (21) then the
implicature that downtown is safe will get through; if they are uttered in
the opposite order, then the implicature that downtown is not safe will get
through. We leave the study of these kinds of interactions to further work.
It is clear that treating conversational implicatures in dialogue is a com-
plex problem, and that it interacts in relevant ways with other key features
of dialogue, namely positive and negative evidence of understanding. It is
important that the theory of implicatures acknowledges the fact that con-
versational implicatures are a phenomena that arises in conversation and
that then needs to be studied in its environment to be fully understood.
This paper starts by motivating this big picture, and then relates it to the
semantic theory of comparatives and the pragmatic theory of implicatures
in order to address the practical problem of comparative implicatures that
arises in an implemented dialogue agent.
16
References
[
1
]
D. Beaver and H. Zeevat. Accommodation. In The Oxford Handbook of Lin-
guistic Interfaces, pages 503?539. OUP, 2007.
[
2
]
H. Clark. Bridging. In The 1975 Workshop on Theoretical issues in natural
language processing, pages 169?174. ACL, 1975.
[
3
]
H. Clark. Using Language. CUP, 1996.
[
4
]
M. Cresswell. The semantics of degree. In Montague Grammar, 1976.
[
5
]
D. DeVault and M. Stone. Interpreting vague utterances in context. In Proc.
of COLING04, pages 1247?1253, 2004.
[
6
]
H. Grice. Logic and conversation. In P. Cole and J. L. Morgan, editors, Syntax
and Semantics: Vol. 3: Speech Acts, pages 41?58. AP, 1975.
[
7
]
J. Hobbs. Discourse and inference. To appear.
[
8
]
L. Horn. On the semantic properties of logical operators in english. PhD thesis,
University of California Los Angeles (UCLA), 1972.
[
9
]
E. Klein. A semantics for positive and comparative adjectives. Linguistics and
Philosophy, 4:1?45, 1980.
[
10
]
J. Laird, A. Newell, and P. Rosenbloom. SOAR: an architecture for general
intelligence. AI, 33(1):1?64, September 1987.
[
11
]
S. Larsson and D. Traum. Information state and dialogue management in the
TRINDI dialogue move engine toolkit. Natural Language Engineering, 6:323?
340, September 2000.
[
12
]
D. Lewis. Scorekeeping in a language game. Journal of Philosophical Logic,
8:339?359, 1979.
[
13
]
K. Rodr??guez and D. Schlangen. Form, intonation and function of clarification
requests in german task oriented spoken dialogues. In CATALOG04, pages
101?108, 2004.
[
14
]
D. Traum. Semantics and pragmatics of questions and answers for dialogue
agents. In Proc. of IWCS-5, pages 380?394, 2003.
[
15
]
D. Traum, S. Marsella, J. Gratch, J. Lee, and A. Hartholt. Multi-party,
multi-issue, multi-strategy negotiation for multi-modal virtual agents. In
H. Prendinger, J. Lester, and M. Ishizuka, editors, IVA, volume 5208 of LNCS,
pages 117?130. Springer, 2008.
[
16
]
D. Traum, W. Swartout, J. Gratch, and S. Marsella. A virtual human dialogue
model for non-team interaction. In L. Dybkjaer and W. Minker, editors, Recent
Trends in Discourse and Dialogue. Springer, 2008.
17
Proceedings of SIGDIAL 2009: the 10th Annual Meeting of the Special Interest Group in Discourse and Dialogue, pages 11?20,
Queen Mary University of London, September 2009. c?2009 Association for Computational Linguistics
Can I finish? Learning when to respond to incremental interpretation
results in interactive dialogue
David DeVault and Kenji Sagae and David Traum
USC Institute for Creative Technologies
13274 Fiji Way
Marina del Rey, CA 90292
{devault,sagae,traum}@ict.usc.edu
Abstract
We investigate novel approaches to re-
sponsive overlap behaviors in dialogue
systems, opening possibilities for systems
to interrupt, acknowledge or complete a
user?s utterance while it is still in progress.
Our specific contributions are a method for
determining when a system has reached a
point of maximal understanding of an on-
going user utterance, and a prototype im-
plementation that shows how systems can
use this ability to strategically initiate sys-
tem completions of user utterances. More
broadly, this framework facilitates the im-
plementation of a range of overlap behav-
iors that are common in human dialogue,
but have been largely absent in dialogue
systems.
1 Introduction
Human spoken dialogue is highly interactive, in-
cluding feedback on the speech of others while
the speech is progressing (so-called ?backchan-
nels? (Yngve, 1970)), monitoring of addressees
and other listener feedback (Nakano et al, 2003),
fluent turn-taking with little or no delays (Sacks et
al., 1974), and overlaps of various sorts, including
collaborative completions, repetitions and other
grounding moves, and interruptions. Interrup-
tions can be either to advance the new speaker?s
goals (which may not be related to interpreting the
other?s speech) or in order to prevent the speaker
from finishing, which again can be for various rea-
sons. Few of these behaviors can be replicated by
current spoken dialogue systems. Most of these
behaviors require first an ability to perform in-
cremental interpretation, and second, an ability to
predict the final meaning of the utterance.
Incremental interpretation enables more rapid
response, since most of the utterance can be inter-
preted before utterance completion (Skantze and
Schlangen, 2009). It also enables giving early
feedback (e.g., head nods and shakes, facial ex-
pressions, gaze shifts, and verbal backchannels) to
signal how well things are being perceived, under-
stood, and evaluated (Allwood et al, 1992).
For some responsive behaviors, one must go be-
yond incremental interpretation and predict some
aspects of the full utterance before it has been
completed. For behaviors such as comply-
ing with the evocative function (Allwood, 1995)
or intended perlocutionary effect (Sadek, 1991),
grounding by demonstrating (Clark and Schaefer,
1987), or interrupting to avoid having the utter-
ance be completed, one must predict the semantic
content of the full utterance from a partial prefix
fragment. For other behaviors, such as timing a
reply to have little or no gap, grounding by saying
the same thing at the same time (called ?chanting?
by Hansen et al (1996)), performing collaborative
completions (Clark and Wilkes-Gibbs, 1986), or
some corrections, it is important not only to pre-
dict the meaning, but also the form of the remain-
ing part of the utterance.
We have begun to explore these issues in the
context of the dialogue behavior of virtual human
(Rickel and Johnson, 1999) or embodied conver-
sational agent (Cassell et al, 2000) characters for
multiparty negotiation role-playing (Traum et al,
2008b). In these kinds of systems, human-like be-
havior is a goal, since the purpose is to allow a user
to practice this kind of dialogue with the virtual
humans in training for real negotiation dialogues.
The more realistic the characters? dialogue behav-
ior is, the more kinds of negotiation situations can
be adequately trained for. We discuss these sys-
11
tems further in Section 2.
In Sagae et al (2009), we presented our first re-
sults at prediction of semantic content from partial
speech recognition hypotheses, looking at length
of the speech hypothesis as a general indicator of
semantic accuracy in understanding. We summa-
rize this previous work in Section 3.
In the current paper, we incorporate additional
features of real-time incremental interpretation to
develop a more nuanced prediction model that can
accurately identify moments of maximal under-
standing within individual spoken utterances (Sec-
tion 4). We demonstrate the value of this new
ability using a prototype implementation that col-
laboratively completes user utterances when the
system becomes confident about how the utter-
ance will end (Section 5). We believe such pre-
dictive models will be more broadly useful in im-
plementing responsive overlap behaviors such as
rapid grounding using completions, confirmation
requests, or paraphrasing, as well as other kinds of
interruptions and multi-modal displays. We con-
clude and discuss future work in Section 6.
2 Domain setting
The case study we present in this paper is taken
from the SASO-EN scenario (Hartholt et al, 2008;
Traum et al, 2008b). This scenario is designed
to allow a trainee to practice multi-party negoti-
ation skills by engaging in face to face negotia-
tion with virtual humans. The scenario involves
a negotiation about the possible re-location of a
medical clinic in an Iraqi village. A human trainee
plays the role of a US Army captain, and there are
two virtual humans that he negotiates with: Doctor
Perez, the head of the NGO clinic, and a local vil-
lage elder, al-Hassan. The doctor?s main objective
is to treat patients. The elder?s main objective is to
support his village. The captain?s main objective
is to move the clinic out of the marketplace, ide-
ally to the US base. Figure 1 shows the doctor and
elder in the midst of a negotiation, from the per-
spective of the trainee. Figure A-1 in the appendix
shows a sample dialogue from this domain.
The system has a fairly typical set of pro-
cessing components for virtual humans or dia-
logue systems, including ASR (mapping speech
to words), NLU (mapping from words to semantic
frames), dialogue interpretation and management
(handling context, dialogue acts, reference and de-
ciding what content to express), NLG (mapping
Figure 1: SASO-EN negotiation in the cafe: Dr.
Perez (left) looking at Elder al-Hassan.
2
6
6
6
6
6
6
6
4
mood : declarative
sem :
2
6
6
6
6
6
4
type : event
agent : captain? kirk
event : deliver
theme : power ? generator
modal :
?
possibility : can
?
speech? act :
?
type : offer
?
3
7
7
7
7
7
5
3
7
7
7
7
7
7
7
5
Figure 2: AVM utterance representation.
frames to words), non-verbal generation, and syn-
thesis and realization. The doctor and elder use
the same ASR and NLU components, but have dif-
ferent modules for the other processing, including
different models of context and goals, and differ-
ent output generators. In this paper, we will often
refer to the characters with various terms, includ-
ing ?virtual humans?, ?agents?, or ?the system?.
In this paper, we are focusing on the NLU
component, looking at incremental interpretation
based on partial speech recognition results, and
the potential for using this information to change
the dialogue strategy where warranted, and pro-
vide responses before waiting for the final speech
result. The NLU output representation is an
attribute-value matrix (AVM), where the attributes
and values represent semantic information that
is linked to a domain-specific ontology and task
model (Hartholt et al, 2008). Figure 2 shows an
example representation, for an utterance such as
?we can provide you with power generators?. The
AVMs are linearized, using a path-value notation,
as shown in Figure 3.
To develop and test the new incremen-
tal/prediction models, we are using a corpus of
12
<s>.mood declarative
<s>.sem.type event
<s>.sem.agent captain-kirk
<s>.sem.event deliver
<s>.sem.theme power-generator
<s>.sem.modal.possibility can
<s>.sem.speechact.type offer
Figure 3: Example NLU frame.
utterances collected from people playing the role
of captain and negotiating with the virtual doctor
and elder. In contrast with Figure A-1, which
is a dialogue with one of the system designers
who knows the domain well, dialogues with naive
users are generally longer, and often have a fairly
high word error rate (average 0.54), with many
out of domain utterances. The system is robust to
these kinds of problems, both in terms of the NLU
approach (Leuski and Traum, 2008; Sagae et al,
2009) as well as the dialogue strategies (Traum
et al, 2008a). This is accomplished in part by
approximating the meaning of utterances. For
example, the frame in Figure 3 is also returned for
an utterance of we are prepared to give you guys
generators for electricity downtown as well as the
ASR output for this utterance, we up apparently
give you guys generators for a letter city don town.
3 Predicting interpretations from partial
recognition hypotheses
Our NLU module, mxNLU (Sagae et al, 2009), is
based on maximum entropy classification (Berger
et al, 1996), where we treat entire individual
frames as classes, and extract input features from
ASR. The training data for mxNLU is a corpus
of approximately 3,500 utterances, each annotated
with the appropriate frame. These utterances were
collected from user sessions with the system, and
the corresponding frames were assigned manually.
Out-of-domain utterances (about 15% of all utter-
ances in our corpus) could not be mapped to con-
cepts in our ontology and task model, and were
assigned a ?garbage? frame. For each utterance
in our corpus, we have both a manual transcrip-
tion and the output of ASR, although only ASR
is used by mxNLU (both at training and at run-
time). Each training instance for mxNLU consists
of a frame, paired with a set of features that rep-
resent the ASR output for user utterances. The
specific features used by the classifier are: each
word in the input string (bag-of-words representa-
tion of the input), each bigram (pairs of consec-
utive words), each pair of any two words in the
input, and the number of words in the input string.
In the 3,500-utterance training set, there are 136
unique frames (135 that correspond to the seman-
tics of different utterances in the domain, plus one
frame for out-of-domain utterances).1 The NLU
task is then framed as a multiclass classification
approach with 136 classes, and about 3,500 train-
ing examples.
Although mxNLU produces entire frames as
output, we evaluate NLU performance by look-
ing at precision and recall of the attribute-value
pairs (or frame elements) that compose frames.
Precision represents the portion of frame elements
produced by mxNLU that were correct, and re-
call represents the portion of frame elements in
the gold-standard annotations that were proposed
by mxNLU. By using precision and recall of
frame elements, we take into account that certain
frames are more similar than others and also al-
low more meaningful comparative evaluation with
NLU modules that construct a frame from sub-
elements or for cases when the actual frame is not
in the training set. The precision and recall of
frame elements produced by mxNLU using com-
plete ASR output are 0.78 and 0.74, respectively,
for an F-score (harmonic mean of precision and
recall) of 0.76.
3.1 NLU with partial ASR results
The simplest way to perform NLU of partial ASR
results is simply to process the partial utterances
using the NLU module trained on complete ASR
output. However, better results may be obtained
by training separate NLU models for analysis of
partial utterances of different lengths. To train
these separate NLU models, we first ran the au-
dio of the utterances in the training data through
our ASR module, recording all partial results for
each utterance. Then, to train a model to ana-
lyze partial utterances containing N words, we
used only partial utterances in the training set con-
taining N words (unless the entire utterance con-
tained less than N words, in which case we sim-
ply used the complete utterance). In some cases,
multiple partial ASR results for a single utterance
1In a separate development set of 350 utterances, anno-
tated in the same way as the training set, we found no frames
that had not appeared in the training set.
13
0
10
20
30
40
50
60
70
80
1 2 3 4 5 6 7 8 9 10 allLength n (words)
F
-
s
c
o
r
e
Trained on all data
Trained on partials up tolength nTrained on partials up tolength n + context
Figure 4: F-score for three NLU models on partial
ASR results up to N words.
contained the same number of words, and we used
the last partial result with the appropriate number
of words.2 We trained ten separate partial NLU
models for N varying from one to ten.
Figure 4 shows the F-score for frames obtained
by processing partial ASR results up to length N
using three variants of mxNLU. The dashed line is
our baseline NLU model, trained on complete ut-
terances only, and the solid line shows the results
obtained with length-specific NLU models. The
dotted line shows results for length-specific mod-
els that also use features that capture aspects of di-
alogue context. In these experiments, we used uni-
gram and bigram word features extracted from the
most recent system utterance to represent context,
but found that these context features did not im-
prove NLU performance. Our final NLU approach
for partial ASR hypotheses is then to train separate
models for specific lengths, using hypotheses of
that length during training (solid line in figure 4).
4 How well is the system understanding?
In this section, we present a strategy that uses
machine learning to more closely characterize the
performance of a maximum entropy based incre-
mental NLU module, such as the mxNLU mod-
ule described in Section 3. Our aim is to iden-
tify strategic points in time, as a specific utterance
is occurring, when the system might react with
confidence that the interpretation will not signif-
2At run-time, this can be closely approximated by taking
the partial utterance immediately preceding the first partial
utterance of length N + 1.
Utterance time (ms)
NL
U F
?sc
ore
(emp
ty)
(emp
ty)
 
all 
 
elde
r 
 
elde
r do
 you
 
 
elde
r to
 you
 d 
 
elde
r do
 you
 agr
ee 
 
elde
r do
 you
 agr
ee t
o 
 
elde
r do
 you
 agr
ee t
o m
ove
 the
 
 
elde
r do
 you
 agr
ee t
o m
ove
 the
 
 
elde
r do
 you
 agr
ee t
o m
ove
 the
 clin
ic to
 
 
elde
r do
 you
 agr
ee t
o m
ove
 the
 clin
ic d
own
 
 
elde
r do
 you
 agr
ee t
o m
ove
 the
 clin
ic d
own
tow
n 
 
elde
r do
 you
 agr
ee t
o m
ove
 the
 clin
ic d
own
tow
n 
200 400 600 800 100
0
120
0
140
0
160
0
180
0
200
0
220
0
240
0
260
0
280
0
0.0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
Partial ASR result
Figure 5: Incremental interpretation of a user ut-
terance.
icantly improve during the rest of the utterance.
This reaction could take several forms, including
providing feedback, or, as described in Section 5
an agent might use this information to opportunis-
tically choose to initiate a completion of a user?s
utterance.
4.1 Motivating example
Figure 5 illustrates the incremental output of
mxNLU as a user asks, elder do you agree to move
the clinic downtown? Our ASR processes cap-
tured audio in 200ms chunks. The figure shows
the partial ASR results after the ASR has pro-
cessed each 200ms of audio, along with the F-
14
score achieved by mxNLU on each of these par-
tials. Note that the NLU F-score fluctuates some-
what as the ASR revises its incremental hypothe-
ses about the user utterance, but generally in-
creases over time.
For the purpose of initiating an overlapping re-
sponse to a user utterance such as this one, the
agent needs to be able (in the right circumstances)
to make an assessment that it has already under-
stood the utterance ?well enough?, based on the
partial ASR results that are currently available. We
have implemented a specific approach to this as-
sessment which views an utterance as understood
?well enough? if the agent would not understand
the utterance any better than it currently does even
if it were to wait for the user to finish their utter-
ance (and for the ASR to finish interpreting the
complete utterance).
Concretely, Figure 5 shows that after the entire
2800ms utterance has been processed by the ASR,
mxNLU achieves an F-score of 0.91. However,
in fact, mxNLU already achieves this maximal F-
score at the moment it interprets the partial ASR
result elder do you agree to move the at 1800ms.
The agent therefore could, in principle, initiate an
overlapping response at 1800ms without sacrific-
ing any accuracy in its understanding of the user?s
utterance.
Of course the agent does not automatically re-
alize that it has achieved a maximal F-score at
1800ms. To enable the agent to make this assess-
ment, we have trained a classifier, which we call
MAXF, that can be invoked for any specific par-
tial ASR result, and which uses various features of
the ASR result and the current mxNLU output to
estimate whether the NLU F-score for the current
partial ASR result is at least as high as the mxNLU
F-score would be if the agent were to wait for the
entire utterance.
4.2 Machine learning setup
To facilitate the construction of our MAXF clas-
sifier, we identified a range of potentially useful
features that the agent could use at run-time to as-
sess its confidence in mxNLU?s output for a given
partial ASR result. These features are exempli-
fied in the appendix in Figure A-2, and include:
K, the number of partial results that have been re-
ceived from the ASR; N , the length (in words) of
the current partial ASR result; Entropy, the en-
tropy in the probability distribution mxNLU as-
signs to alternative output frames (lower entropy
corresponds to a more focused distribution); Pmax,
the probability mxNLU assigns to the most prob-
able output frame; NLU, the most probable output
frame (represented for convenience as fI , where
I is an integer index corresponding to a specific
complete frame). We also define MAXF (GOLD),
a boolean value giving the ground truth about
whether mxNLU?s F-score for this partial is at
least as high as mxNLU?s F-score for the final par-
tial for the same utterance. In the example, note
that MAXF (GOLD) is true for each partial where
mxNLU?s F-score (F (K)) is ? 0.91, the value
achieved for the final partial (elder do you agree to
move the clinic downtown). Of course, the actual
F-score F (K) is not available at run-time, and so
cannot serve as an input feature for the classifier.
Our general aim, then, is to train a classifier,
MAXF, whose output predicts the value of MAXF
(GOLD) as a function of the input features. To
create a data set for training and evaluating this
classifier, we observed and recorded the values of
these features for the 6068 partial ASR results in
a corpus of ASR output for 449 actual user utter-
ances.3
We chose to train a decision tree using Weka?s
J48 training algorithm (Witten and Frank, 2005).4
To assess the trained model?s performance, we car-
ried out a 10-fold cross-validation on our data set.5
We present our results in the next section.
4.3 Results
We will present results for a trained decision
tree model that reflects a specific precision/recall
tradeoff. In particular, given our aim to enable
an agent to sometimes initiate overlapping speech,
while minimizing the chance of making a wrong
assumption about the user?s meaning, we selected
a model with high precision at the expense of
lower recall. Various precision/recall tradeoffs are
possible in this framework; the choice of a spe-
cific tradeoff is likely to be system and domain-
dependent and motivated by specific design goals.
We evaluate our model using several features
which are exemplified in the appendix in Fig-
ure A-3. These include MAXF (PREDICTED),
the trained MAXF classifier?s output (TRUE or
3This corpus was not part of the training data for mxNLU.
4Of course, other classification models could be used.
5All the partial ASR results for a given utterance were
constrained to lie within the same fold, to avoid training and
testing on the same utterance.
15
FALSE) for each partial; KMAXF, the first par-
tial number for which MAXF (PREDICTED) is
TRUE; ?F (K) = F (K) ? F (Kfinal), the ?loss?
in F-score associated with interpreting partial K
rather than the final partialKfinal for the utterance;
T (K), the remaining length (in seconds) in the
user utterance at each partial.
We begin with a high level summary of the
trained MAXF model?s performance, before dis-
cussing more specific impacts of interest in the di-
alogue system. We found that our trained model
predicts that MAXF = TRUE for at least one
partial in 79.2% of the utterances in our cor-
pus. For the remaining utterances, the trained
model predicts MAXF = FALSE for all partials.
The precision/recall/F-score of the trained MAXF
model are 0.88/0.52/0.65 respectively. The high
precision means that 88% of the time that the
model predicts that F-score is maximized at a spe-
cific partial, it really is. On the other hand, the
lower recall means that only 52% of the time that
F-score is in fact maximized at a given partial does
the model predict that it is.
For the 79.2% of utterances for which the
trained model predicts MAXF = TRUE at some
point, Figure 6 shows the amount of time in sec-
onds, T (KMAXF), that remains in the user utter-
ance at the time partialKMAXF becomes available
from the ASR. The mean value is 1.6 seconds; as
the figure shows, the time remaining varies from 0
to nearly 8 seconds per utterance. This represents
a substantial amount of time that an agent could
use strategically, for example by immediately ini-
tiating overlapping speech (perhaps in an attempt
to improve communication efficiency), or by ex-
ploiting this time to plan an optimal response to
the user?s utterance.
However, it is also important to understand the
cost associated with interpreting partial KMAXF
rather than waiting to interpret the final ASR result
Kfinal for the utterance. We therefore analyzed
the distribution in ?F (KMAXF) = F (KMAXF)?
F (Kfinal). This value is at least 0.0 if mxNLU?s
output for partial KMAXF is no worse than its out-
put for Kfinal (as intended). The distribution is
given in Figure 7. As the figure shows, 62.35% of
the time (the median case), there is no difference
in F-score associated with interpreting KMAXF
rather than Kfinal. 10.67% of the time, there is
a loss of -1, which corresponds to a completely
incorrect frame at KMAXF but a completely cor-
Utterance time remaining (seconds)
F
r
e
q
u
e
n
c
y
0 2 4 6 80
10
20
30
Figure 6: Distribution of T (KMAXF).
?F (KMAXF) range Percent of
utterances
-1 10.67%
(?1, 0) 17.13%
0 62.35%
(0, 1) 7.30%
1 2.52%
mean(?F (KMAXF)) -0.1484
median(?F (KMAXF)) 0.0000
Figure 7: The distribution in ?F (KMAXF), the
?loss? associated with interpreting partial KMAXF
rather than Kfinal.
rect frame at Kfinal. The converse also happens
2.52% of the time: mxNLU?s output frame is com-
pletely correct at the early partial but completely
incorrect at the final partial. The remaining cases
are mixed. While the median is no change in F-
score, the mean case is a loss in F-score of -0.1484.
This is the mean penalty in NLU performance that
could be paid in exchange for the potential gain in
communication efficiency suggested by Figure 6.
5 Prototype implementation
To illustrate one use of the techniques described in
the previous sections, we have implemented a pro-
totype module that performs user utterance com-
pletion. This allows an agent to jump in during a
user?s utterance, and say a completion of the utter-
ance before it is finished, at a point when the agent
16
thinks it understands what the user means. This
type of completion is often encountered in human-
human dialogue, and may be used, for example,
for grounding or for bringing the other party?s turn
to a conclusion.
We have equipped one of our virtual humans,
Doctor Perez, with an ability to perform comple-
tions as follows. The first step is for the agent to
recognize when it understands what the user wants
to say. As discussed in Sections 3 and 4, this often
happens before the user has completed the utter-
ance. NLU is performed on partial ASR hypothe-
ses as they become available, and MAXF decides
whether the agent?s understanding of the current
partial hypothesis is likely to improve given more
time. Once MAXF indicates that the agent?s un-
derstanding is likely to be already maximized for
the utterance, we take the current partial ASR hy-
pothesis and attempt to generate text to complete it
in a way that is fluent and agrees with the meaning
of the utterance the user has in mind.
The generation of the surface text for comple-
tions takes advantage of the manual transcriptions
in the corpus of utterances used to train the NLU
module. For each frame that the agent under-
stands, our training set contains several user utter-
ances that correspond to the meaning in that frame.
At the point where the agent is ready to formu-
late a completion, mxNLU has already predicted
a frame for the user?s utterance (even though it is
still incomplete). We then consider only the set
of known utterances that correspond to that frame
as possible sources of completions. As a simple
distance metric, we compute the word error rate
(WER) between the current partial hypothesis for
the user?s utterance and a prefix of each of these
known utterances. In our prototype, these prefixes
have the same length as the current partial ASR
hypothesis. We then select the utterance whose
prefix has the lowest WER against the current par-
tial ASR hypothesis. As a final step, we look in
the prefix of our selected utterance for the last oc-
currence of the last word in the partial ASR, and
if such a word is found, we take the remainder of
the utterance as the agent?s completion. Consid-
ering only the set of utterances that correspond to
the frame predicted by mxNLUmakes it likely that
the completion will have the appropriate meaning.
Since the completion is a suffix of a transcript of a
previous user utterance, and this suffix follows the
last word uttered by the user, it is likely to form a
fluent completion of the user?s partial utterance.
For 356 utterances in our development set, the
procedure above generated completions contain-
ing at least one word for 177 utterances. We list
below four representative examples of user utter-
ance completions generated by Doctor Perez:
1. Partial utterance: we need to
Predicted completion: move your clinic
Actual user completion: move the clinic
2. Partial utterance: I have orders
Predicted completion: to move you and this
clinic
Actual user completion: to help you in mov-
ing the clinic to a new location
3. Partial utterance: the market
Predicted completion: is not safe
Actual user completion: is not a safer loca-
tion
4. Partial utterance: we can also
Predicted completion: give you medical sup-
plies
Actual user completion: build you a well
Although agent completions for user utterances
are often acceptably accurate (examples 1-3), oc-
casionally they are incorrect (example 4). The
frequency of appropriate completions depends on
the accuracy of mxNLU and the chosen preci-
sion/recall tradeoff in MAXF. Finally, although
the agent has the ability to generate these com-
pletions, clearly it should not complete the user?s
utterance at every opportunity. Determining a pol-
icy that results in natural behavior with respect to
the frequency of completions for different types of
agents is a topic under current investigation.
6 Summary and future work
We have presented a framework for interpretation
of partial ASR hypotheses of user utterances, and
high-precision identification of points within user
utterances where the system already understands
the intended meaning. Our initial implementa-
tion of an utterance completion ability for a vir-
tual human serves to illustrate the capabilities of
this framework, but only scratches the surface of
the new range of dialogue behaviors and strategies
it allows.
Immediate future work includes the design of
policies for completions and interruptions that re-
17
sult in natural conversational behavior. Other ap-
plications of this work include the generation of
paraphrases that can be used for grounding, in ad-
dition to extra-linguistic behavior during user ut-
terances, such as head nods and head shakes.
Acknowledgments
The project or effort described here has been spon-
sored by the U.S. Army Research, Development,
and Engineering Command (RDECOM). State-
ments and opinions expressed do not necessarily
reflect the position or the policy of the United
States Government, and no official endorsement
should be inferred. We would also like to thank
Anton Leuski for facilitating the use of incremen-
tal speech results, and David Schlangen and the
ICT dialogue group, for helpful discussions.
References
Jens Allwood, Joakim Nivre, and Elisabeth Ahlsen.
1992. On the semantics and pragmatics of linguistic
feedback. Journal of Semantics, 9.
Jens Allwood. 1995. An activity based approach to
pragmatics. Technical Report (GPTL) 75, Gothen-
burg Papers in Theoretical Linguistics, University of
G?teborg.
Adam L. Berger, Stephen D. Della Pietra, and Vincent
J. D. Della Pietra. 1996. A maximum entropy ap-
proach to natural language processing. Computa-
tional Linguistics, 22(1):39?71.
Justine Cassell, Joseph Sullivan, Scott Prevost, and
Elizabeth Churchill, editors. 2000. Embodied Con-
versational Agents. MIT Press, Cambridge, MA.
Herbert H. Clark and Edward F. Schaefer. 1987. Col-
laborating on contributions to conversation. Lan-
guage and Cognitive Processes, 2:1?23.
Herbert H. Clark and DeannaWilkes-Gibbs. 1986. Re-
ferring as a collaborative process. Cognition, 22:1?
39. Also appears as Chapter 4 in (Clark, 1992).
Herbert H. Clark. 1992. Arenas of Language Use.
University of Chicago Press.
B. Hansen, D. Novick, and S. Sutton. 1996. Prevention
and repair of breakdowns in a simple task domain.
In Proceedings of the AAAI-96 Workshop on De-
tecting, Repairing, and Preventing Human-Machine
Miscommunication, pages 5?12.
A. Hartholt, T. Russ, D. Traum, E. Hovy, and S. Robin-
son. 2008. A common ground for virtual humans:
Using an ontology in a natural language oriented
virtual human architecture. In Language Resources
and Evaluation Conference (LREC), May.
A. Leuski and D. Traum. 2008. A statistical approach
for text processing in virtual humans. In 26th Army
Science Conference.
Yukiko I. Nakano, Gabe Reinstein, Tom Stocky, and
Justine Cassell. 2003. Towards a model of face-to-
face grounding. In ACL, pages 553?561.
Jeff Rickel and W. Lewis Johnson. 1999. Virtual hu-
mans for team training in virtual reality. In Proceed-
ings of the Ninth International Conference on Artifi-
cial Intelligence in Education, pages 578?585. IOS
Press.
H. Sacks, E. A. Schegloff, and G. Jefferson. 1974.
A simplest systematics for the organization of turn-
taking for conversation. Language, 50:696?735.
M. D. Sadek. 1991. Dialogue acts are rational
plans. In Proceedings of the ESCA/ETR workshop
on multi-modal dialogue.
K. Sagae, G. Christian, D. DeVault, and D. R. Traum.
2009. Towards natural language understanding of
partial speech recognition results in dialogue sys-
tems. In Short Paper Proceedings of NAACL HLT.
Gabriel Skantze and David Schlangen. 2009. Incre-
mental dialogue processing in a micro-domain. In
Proceedings of EACL 2009, pages 745?753.
D. Traum, W. Swartout, J. Gratch, and S. Marsella.
2008a. A virtual human dialogue model for non-
team interaction. In L. Dybkjaer and W. Minker,
editors, Recent Trends in Discourse and Dialogue.
Springer.
D. R. Traum, S. Marsella, J. Gratch, J. Lee, and
A. Hartholt. 2008b. Multi-party, multi-issue, multi-
strategy negotiation for multi-modal virtual agents.
In Helmut Prendinger, James C. Lester, and Mitsuru
Ishizuka, editors, IVA, volume 5208 of Lecture Notes
in Computer Science, pages 117?130. Springer.
I. H. Witten and E. Frank. 2005. Data Mining: Prac-
tical Machine Learning Tools and Techniques. Mor-
gan Kaufmann.
Victor H. Yngve. 1970. On getting a word in edgewise.
In Papers from the Sixth Regional Meeting, pages
567?78. Chicago Linguistic Society.
18
A Appendix
1 C Hello Doctor Perez.
2 D Hello captain.
3 E Hello captain.
4 C Thank you for meeting me.
5 E How may I help you?
6 C I have orders to move this clinic to a camp near the US base.
7 E We have many matters to attend to.
8 C I understand, but it is imperative that we move the clinic out of this area.
9 E This town needs a clinic.
10 D We can?t take sides.
11 C Would you be willing to move downtown?
12 E We would need to improve water access in the downtown area, captain.
13 C We can dig a well for you.
14 D Captain, we need medical supplies in order to run the clinic downtown.
15 C We can deliver medical supplies downtown, Doctor.
16 E We need to address the lack of power downtown.
17 C We can provide you with power generators.
18 E Very well captain, I agree to have the clinic downtown.
19 E Doctor, I think you should run the clinic downtown.
20 D Elder, the clinic downtown should be in an acceptable condition before
we move.
21 E I can renovate the downtown clinic, Doctor.
22 D OK, I agree to run the clinic downtown, captain.
23 C Excellent.
24 D I must go now.
25 E I must attend to other matters.
26 C Goodbye.
26 D Goodbye.
26 E Farewell, sir.
Figure A-1: Successful negotiation dialogue between C, a captain (human trainee), D, a doctor (virtual
human), and E, a village elder (virtual human).
19
MAXF model training features
Partial ASR result F (K) K N Entropy Pmax NLU MAXF (GOLD)
(empty) 0.00 1 0 2.96 0.48 f82 FALSE
(empty) 0.00 2 0 2.96 0.48 f82 FALSE
all 0.00 3 1 0.82 0.76 f72 FALSE
elder 0.00 4 1 0.08 0.98 f39 FALSE
elder do you 0.83 5 3 1.50 0.40 f68 FALSE
elder to you d 0.50 6 3 1.31 0.75 f69 FALSE
elder do you agree 0.83 7 4 1.84 0.35 f68 FALSE
elder do you agree to 0.83 8 5 1.40 0.61 f68 FALSE
elder do you agree to move the 0.91 9 7 0.94 0.49 f10 TRUE
elder do you agree to move the 0.91 10 7 0.94 0.49 f10 TRUE
elder do you agree to move the clinic to 0.83 11 9 1.10 0.58 f68 FALSE
elder do you agree to move the clinic down 0.83 12 9 1.14 0.66 f68 FALSE
elder do you agree to move the clinic downtown 0.91 13 9 0.50 0.89 f10 TRUE
elder do you agree to move the clinic downtown 0.91 14 9 0.50 0.89 f10 TRUE
Figure A-2: Features used to train the MAXF model.
MAXF model evaluation features
K F (K) ?F (K) T (K) MAXF (PREDICTED)
1 0.00 -0.91 2.6 FALSE
2 0.00 -0.91 2.4 FALSE
3 0.00 -0.91 2.2 FALSE
4 0.00 -0.91 2.0 FALSE
5 0.83 -0.08 1.8 FALSE
6 0.50 -0.41 1.6 FALSE
7 0.83 -0.08 1.4 FALSE
8 0.83 -0.08 1.2 FALSE
9 (= KMAXF) 0.91 0.00 (=?F (KMAXF)) 1.0 TRUE
10 0.91 0.00 0.8 TRUE
11 0.83 -0.08 0.6 FALSE
12 0.83 -0.08 0.4 FALSE
13 0.91 0.00 0.2 TRUE
14 0.91 0.00 0.0 TRUE
Figure A-3: Features used to evaluate the MAXF model.
20
Modelling Grounding 
Co l in  Matheson  
University of Edinburgh 
Edinburgh,  Scotland 
colin, mat  heson @ed. ac. uk 
and Discourse Obligations Using Update 
Rules 
Mass imo Poes io  
University of Edinburgh 
Edinburgh, Scotland 
massimo.poesio@ed.ac.uk 
Dav id  T raum 
University of Mary land 
Maryland,  USA 
t raum@cs.umd.edu 
Abst rac t  
This paper describes an implementation f some key 
aspects of a theory of dialogue processing whose 
main concerns are to provide models of GROUND- 
ING and of the role of DISCOURSE OBLIGATIONS in 
an agent's deliberation processes. Our system uses 
the TrindiKit dialogue move engine toolkit, which 
assumes a model of dialogue in which a participan. 
t's knowledge is characterised in terms of INFORMA- 
TION STATES which are subject o various kinds of 
updating mechanisms. 
1 I n t roduct ion  
In this paper we describe a preliminary implemen- 
tation of a 'middle-level' dialogue management sys- 
tem. The key tasks of a dialogue manager are to 
update the representation f dialogue on the basis of 
processed input (generally, but not exclusively, lan- 
guage utterances), and to decide what (if anything) 
the system should do next. There is a wide range of 
opinions concerning how these tasks should be per- 
formed, and in particular, how the ongoing dialogue 
state should be represented: e.g., as something very 
specific to a particular domain, or according to some 
more general theory of (human or human inspired) 
dialogue processing. At one extreme, some systems 
represent only the (typically very rigid) transitions 
possible in a perceived ialogue for the given task, 
often using finite states in a transition etwork to 
represent the dialogue: examples of this are sys- 
tems built using Nuance's DialogueBuilder or the 
CSLU's Rapid Application Prototyper. The other 
extreme is to build the dialogue processing theory on 
top of a full model of rational agency (e.g., (Bretier 
and Sadek, 1996)). The approach we take here lies 
in between these two extremes: we use rich repre- 
sentations of information states, but simpler, more 
dialogue-specific deliberation methods, rather than 
a deductive reasoner working on the basis of an ax- 
iomatic theory of rational agency. We show in this 
paper that the theory of information states we pro- 
pose can, nevertheless, beused to give a character- 
isation of dialogue acts such as those proposed by 
the Discourse Resource Initiative precise nough to 
formalise the deliberation process of a dialogue man- 
ager in a completely declarative fashion. 
Our implementation is based on the approach to 
dialogue developed in (Traum, 1994; Poesio and 
Traum, 1997; Poesio and Traum, 1998; Traum et al, 
1999). This theory, like other action-based theories 
of dialogue, views dialogue participation i terms of 
agents performing dialogue acts, the effects of which 
are to update the information state of the partici- 
pants in a dialogue. However, our view of dialogue 
act effects is closer in some respects to that of (All- 
wood, 1976; Allwood, 1994) and (Singh, 1998) than 
to the belief and intention model of (Sadek, 1991; 
Grosz and Sidner, 1990; Cohen and Levesque, 1990). 
Particular emphasis i placed on the social commit- 
ments of the dialogue participants (obligations to 
act and commitments to propositions) without mak- 
ing explicit claims about the actual beliefs and in- 
tentions of the participants. Also, heavy empha- 
sis is placed on how dialogue participants socially 
GROUND (Clark and Wilkes-Gibbs, 1986) the infor- 
mation expressed in dialogue: the information state 
assumed in this theory specifies which information is 
assumed to be already part of the common ground at 
a given point, and which part has been introduced, 
but not yet been established. 
The rest of this paper is structured as follows. The 
theory of dialogue underlying the implementation is 
described in more detail in Section 2. Section 3 de- 
scribes the implementation itself. Section 4 shows 
how the system updates its information state while 
participating in a fairly simple dialogue. 
2 Theoret i ca l  Background 
One basic assumption underlying this work is that 
it is useful to analyse dialogues by describing the 
relevant 'information' that is available to each par- 
ticipant. The notion of INFORMATION STATE (IS) is 
therefore mployed in deciding what the next action 
should be, and the effects of utterances are described 
in terms of the changes they bring about in ISs. A 
particular instantiation ofa dialogue manager, from 
this point of view, consists of a definition of the con- 
tents of ISs plus a description of the update processes 
which map from IS to IS. Updates are typically trig- 
gered by 'full' dialogue acts such as assertions or 
directives, 1 of course, but the theory allows parts of 
utterances, including individual words and even sub- 
parts of words, to be the trigger. The update rules 
for dialogue acts that we assume here are a simpli- 
fied version of the formalisations proposed in (Poesio 
and Traum, 1998; Traum et al, 1999) (henceforth, 
PTT). 
The main aspects of PTT which have been im- 
plemented concern the way discourse obligations are 
handled and the manner in which dialogue partic- 
ipants interact o add information to the common 
ground. Obligations are essentially social in nature, 
and directly characterise poken dialogue; a typical 
example of a discourse obligation concerns the rela- 
tionship between questions and answers. Poesio and 
Traum follow (Traum and Allen, 1994) in suggesting 
that the utterance of a question imposes an obliga- 
tion on the hearer to address the question (e.g., by 
providing an answer), irrespective of intentions. 
As for the process by which common ground is es- 
tablished, or GROUNDING (Clark and Schaefer, 1989; 
Traum, 1994), the assumption i PTT is that classi- 
cal speech act theory is inherently too simplistic in 
that it ignores the fact that co-operative interaction 
is essential in discourse; thus, for instance, simply as- 
serting something does not make it become mutually 
'known' (part of the common ground). It is actually 
necessary for the hearer to provide some kind of ac- 
knowledgement that the assertion has been received, 
understood or not understood, accepted or rejected, 
and so on. Poesio and Traum view the public in- 
formation state as including both material that has 
already been grounded, indicated by GND here, and 
material that hasn't been grounded yet. These com- 
ponents of the information state are updated when 
GROUNDING ACTS such as acknowledgement areper- 
formed. Each new contribution results in a new DIS- 
COURSE UNIT (DU) being added to the information 
state (Traum, 1994) and recorded in a list of 'un- 
grounded iscourse units' (UDUS); these DUs can 
then be subsequently grounded as the result, e.g., of 
(implicit or explicit) acknowledgements. 
3 Imp lement ing  PTT  
In this section, we describe the details of the im- 
plementation. First, in Section 3.1, we describe the 
TrindiKit tool for building dialogue managers that 
we used to build our system. In Section 3.2, we de- 
scribe the information states used in the implemen- 
tation, an extension and simplification of the ideas 
from PTT discussed in the previous ection. Then, 
in Section 3.3, we discuss how the information state 
is updated when dialogue acts are observed. Finally, 
1We assume here the DRI classification f dialogue acts 
(Discourse Resource Initiative, 1997). 
/ . ' ? "  .. -,. ' " . . ,  
I.lol'lP.lllit~ ~;l{lle (i$) 
'E 1 
Figure 1: TrindiKit Architecture 
in Section 3.4, we describe the rules used by the sys- 
tem to adopt intentions and perform its own actions. 
An extended example of how these mechanisms are 
used to track and participate in a dialogue is pre- 
sented in Section 4. 
3.1 TrindiKit 
The basis for our implementation is the TrindiKit 
dialogue move engine toolkit implemented as part 
of the TRINDI project (Larsson et al, 1999). The 
toolkit provides upport for developing dialogue sys- 
tems, focusing on the central dialogue management 
components. 
The system architecture assumed by the TrindiKit 
is shown in Figure 1. A prominent feature of this ar- 
chitecture is the information state, which serves as a 
central 'blackboard' that processing modules can ex- 
amine (by means of defined CONDITIONS) or change 
(by means of defined OPERATIONS). The structure 
of the IS for a particular dialogue system is defined' 
by the developer who uses the TrindiKit to build 
that system, on the basis of his/her own theory of 
dialogue processing; no predefined notion of infor- 
mation state is provided. 2 The toolkit provides a 
number of abstract data-types such as lists, stacks, 
and records, along with associated conditions and 
operations, that can be used to implement the user's 
theory of information states; other abstract ypes 
can also be defined. In addition to this customis- 
able notion of information state, TrindiKit provides 
a few system variables that can also used for inter- 
module communication. These include input for the 
raw observed (language) input, latest_moves which 
2In TRINDI we are experimenting with multiple instanti- 
ations of three different theories of information state (Traum 
et al, 1999). 
2 
contains the dialogue moves observed in the most 
recent urn, la tes t_speaker ,  and next_moves, con- 
taining the dialogue moves to be performed by the 
system in the next turn. 
A complete system is assumed to consist of sev- 
eral modules interacting via the IS. (See Figure 1 
again.) The central component is called the DIA- 
LOGUE MOVE ENGINE (DME). The DME performs 
the processing needed to integrate the observed i- 
alogue moves with the IS, and to select new moves 
for the system to perform. These two functions are 
encapsulated in the UPDATE and SELECTION sub- 
modules of the DME. The update and select mod- 
ules are specified by means of typed rules, as well as 
sequencing procedures to determine when to apply 
the rules. We are here mainly concerned with UP- 
DATE RULES (urules) ,  which consist of four parts: a 
name, a type, a list of conditions to check in the in- 
formation state, and a list of operations to perform 
on the information state, u ru les  are described in 
more detail below, in Section 3.3. There are also 
two modules outside the DME proper, but still cru- 
cial to a complete system: INTERPRETATION, which 
consumes the input and produces a list of dialogue 
acts in the latest_moves variable (potentially mak- 
ing reference to the current information state), and 
GENERATION, which produces NL output from the 
dialogue acts in the next_moves variable. Finally, 
there is a CONTROL module, that governs the se- 
quencing (or parallel invocation) of the other mod- 
ules. In this paper we focus on the IS and the DME; 
our current implementation only uses very simple 
interpretation and generation components. 
3.2 Information States in PTT  
In this section we discuss the information state used 
in the current implementation. The main difference 
between the implemented IS and the theoretical pro- 
posal in (Poesio and Traum, 1998) is that in the im- 
plementation the information state is partitioned in 
fields, each containing information of different ypes, 
whereas in the theoretical version the information 
state is a single repository of facts (a DISCOURSE 
REPRESENTATION STRUCTURE). Other differences 
are discussed below. An example IS with some fields 
filled is shown in Figure 2; this is the IS which results 
from the second utterance in the example dialogue 
discussed in Section 4, A route please. 3
The IS in Figure 2 is a record with two main 
parts, W and C. The first of these represents the 
system's (Wizard) view of his own mental state and 
of the (semi-)public information discussed in the di- 
alogue; the second, his view of the user's (Caller) 
information state. This second part is needed to 
3All diagrams in this paper are automatically generated 
from TrindiKit system internal representations and displayed 
using the Thistle dialogue editor (Calder, 1998). Some have 
been subsequently edited for brevity and clarity. 
r \] understandingAct( W,DU3 )\ 1 ~1 
\[OBL: ~lddre~(C,CA2 ) \] / // 
~. .  / . .  /CAS:C2 ,~..,~g.(C.DU2) \ /  I I  
. . . . .  / .... \c^:: ' / /  I I  
/ sc : < " ! H 
\[COND: < > J H 
UDUS: <DU3> H 
\[ \[OBL: <,ddri~z(C.CA2 ) > \ ] \ ]  H 
DH: <CA2: C2. into requi~t( W.?help fore1 ):> 
/ / LCOND: < > J/ If 
: | LID: DU2 J I I  
/ \[ lilt / / / /CA5: C2. dil c,(C ivemule(W)  \ / /H 
/ / /DH: (CAS: C2. a,swer( C.CA2.CA4 ) ) //11 
ko,, //11 
I / IIII / \[ LCOND: <IICIpt(W.CA6)-> obl(W~iveroutc(W))>J III 
/ LID: DU3 JII 
1 /,,,,o_,..,,,.,( w.:,,.-, )\ I I  
/ ; l i~oute(W ) 
t ~,,*~,~,d~(W.bU3)/ JI 
lINT: <letrome(C)>\] J 
Figure 2: Structure of Information States 
model misunderstandings arising from the dialogue 
participants having differing views on what has been 
grounded; as we are not concerned with this problem 
here, we will ignore C in what follows. 
w contains information on the grounded mate- 
rial (GND), on the ungrounded information (UDUS, 
PDU and CDU), and on W's intentions (INT). GND 
contains the information that has already been 
grounded; the other fields contain information about 
the contributions still to be grounded. As noticed 
above, in PTT  it is assumed that for each new ut- 
terance, a new DU is created and added to the IS. 
The current implementation differs from the full the- 
ory in that only two DUs are retained at each point; 
the current DU (CDU) and the previous DU (PDU). 
The CDU contains the information in the latest con- 
tribution, while the PDU contains information from 
the penultimate contribution. Information is moved 
f rom PDU to GND as a result of an ack (acknowl- 
edgement) dialogue act (see below.) 
The DUs and the GND field contain four fields, 
representing obligations (OBL), the dialogue history 
(DH), propositions to which agents are socially com- 
mitted (scP),  and conditional updates (COND). The 
value of OBL is a list of action types: actions that 
agents are obliged to perform. An action type is 
specified by a PREDICATE, a DIALOGUE PARTICI- 
PANT, and a list of ARGUMENTS. The value of see 
is a list of a particular type of mental states, so- 
cial commitments of agents to propositions. 4 These 
are specified by a DIALOGUE PARTICIPANT, and a 
PROPOSITION. Finally, the elements in DH are dia- 
4SCPs play much the same role in PTT as do beliefs in 
many BDI accounts of speech acts. 
3 
logue actions, which are instances of dialogue action 
types. A dialogue action is specified by an action 
type, a dialogue act id, and a confidence level CONF 
(the confidence that an agent has that that dialogue 
act has been observed). 
The situation in Figure 2 is the result of updates to 
the IS caused by utterance \[2\] in the dialogue in (6), 
which is assumed to generate a d i rect  act as well as 
an assert  act and an answer  act. 5 That utterance 
is also assumed to contain an implicit acknowledge- 
ment of the original question; this understanding act 
has resulted in the contents of DU2 being grounded 
(and subsequently merged with GND), as discussed 
below. 
GND.OBL in Figure 2 includes two obligations. 
The first is an obligation on W to perform an under- 
standing act (the predicate is unders tand ingAct ,  
the participant is W, and there is just one argument, 
DU3, which identifies the DU in CDU by referring to 
its ID). The second obligation is an obligation on C 
to address  conversational ct CA2; this ID points 
to the appropriate info_request  in the DH list by 
means of the ID number. Obligations are specified 
in CDU and PDU, as well. Those in PDU are simply 
a subset of those in GND, since at point in the up- 
date process shown in Figure 2 this field contains 
information that has already been grounded (note 
that DU2 is not in UDUS anymore); but CDU con- 
tains obligations that have not been grounded yet - 
in particular, the obligation on W to address  CA6. 
GND.DH in this IS contains a list of dialogue ac- 
tions whose occurrence has already been grounded: 
the info_request  performed by utterance 1, with ar- 
gument a question, 6 and the implicit acknowledge 
performed by utterance 2. 7 The DH field in CDU con- 
tains dialogue acts performed by utterance 2 that do 
need to be grounded: a directive by C to W to per- 
form an action of type g iveroute,  and an assert  
by C of the proposition want(C,  route), by which C 
provides an answer  to the previous info_request 
CA2. 
The COND field in CDU contains a conditional up- 
date resulting from the directive performed by that 
utterance. The idea is that directives do not imme- 
diately lead to obligations to perform the mentioned 
action: instead (in addition to an obligation to ad- 
dress the action with some sort of response), their ef- 
fect is to add to the common ground the information 
that if the directive is accepted  by the addressee, 
SThe fact that the utterance of a route please constitutes 
an answer is explicitly assumed; however, it should be possible 
to derive this information automatically (perhaps along the 
lines suggested by Kreutel (Kreutel, 1998)). 
6We use the notation ?p to indicate a question of the form 
?(\[x\],p(x)). 
7We assume here, as in (Traum, 1994) and (Poesio and 
Traum, 1998), that understanding acts do not have to be 
grounded themselves, which would result in a infinite regress. 
then he or she has the obligation to perform the ac- 
tion type requested. (In this case, to give a route to 
C.) 
3.3 Update  Rules  in PTT  
We are now in a position to examine the update 
mechanisms which are performed when new dia- 
logue acts are recognised. When a dialogue par- 
ticipant takes a turn and produces an utterance, 
the interpretation module sets the system variable 
latest_moves to contain a representation f the di- 
alogue acts performed with the utterance. The up- 
dating procedure then uses update rules to modify 
the IS on the basis of the contents of latest_moves 
and of the previous IS. The basic procedure is de- 
scribed in (1) below, s
(1) 1. Create a new DU and push it on top of 
UDUs. 
2. Perform updates on the basis of backwards 
grounding acts. 
. If any other type of act is observed, record 
it in the dialogue history in CDU and apply 
the update rules for this kind of act 
4. Apply update rules to all parts of the IS 
which contain newly added acts. 
The first step involves moving the contents of CDU 
to PDU (losing direct access to the former PDU con- 
tents) and putting in CDU a new empty DU with 
a new identifier. The second and third steps deal 
explicitly with the contents of la test .moves,  ap- 
plying one urule (of possibly a larger set) for each 
act in latest_moves. The relevant effects for each 
act are summarised in (2), where the variables have 
the following types: 
IDx 
DUx 
DP 
q 
PROP 
Act 
o(DP) 
P(ID) 
Q(ID) 
Dialogue Act Identification Number 
DU Identification Number 
Dialogue Participant (i.e., the speaker) 
A Question 
A Proposition 
An Action 
The other dialogue participant 
The content of the ID, a proposition 
The content of the ID, a question 
SSee (Poesio et al, 1999; Traum et al, 1999) for different 
versions of this update procedure used for slightly different 
versions of the theory. 
4 
(2) act ID:2, accept (DP, ID2) 
effect accomplished via rule resolution 
act ID:2, ack(DP, DU1) 
effect peRec(w.Gnd,w.pdu.tognd) 
effect remove(DU1,UDUS) 
act ID:2, agree(DP, ID2) 
effect push(scP,scp(DP,P(ID2))) 
act ID:2, answer(DP,ID2,ID3) 
effect push(scP,ans(DP, Q(ID2),P(ID2))) 
act ID:2, assert (DP,PROP) 
effect push(scP,sep(DP, PROP)) 
effect push (COND,accept (o(DP),ID)-+ 
scp(o(DP),PROP)) 
act ID:I, assert(DP,PROP) 
effect push (COND,accept (o(DP),ID)-~ 
scp(o(DP),PROP)) 
act ID:2, check(DP,PROP) 
effect push(OSL,address(o(DP),ID)) 
effect push(COND,agree(o(DP),ID) --~ 
scp(DP, PROP)) 
act ID:2, direct (DP, Act) 
effect push(OBL,address(o(DP),ID)) 
effect push(CONI),accept (o(DP),ID) -~ 
obl(o(DP),Act)) 
act ID:2, info_request (DP, Q) 
effect push(osL,address(o(DP),ID)) 
The ack act is the only backward grounding act 
implemented at the moment. The main effect of an 
ack is to merge the information i the acknowledged 
DU (assumed to be PDU) into GND, also removing 
this DU from UDUS. Unlike the other acts described 
below, ack acts are recorded irectly into GND.DH, 
rather than into CDU.TOGND.DH. 
All of the other updates are performed in the third 
step of the procedure in (1). The only effect of ac- 
cept acts is to enable the conditional rules which 
are part of the effect of assert and direct, leading 
to social commitments and obligations, respectively. 
agree acts also trigger conditional rules introduced 
by check; in addition, they result in the agent be- 
ing socially committed to the proposition i troduced 
by the act with which the agent agrees. Perform- 
ing an answer to question ID2 by asserting propo- 
sition P(ID3) commits the dialogue participant to 
the proposition that P(ID3) is indeed an answer to 
Q(ID2). 
The two rules for assert are where the confidence 
levels are actually used, to implement a simple ver- 
ification strategy. The idea is that the system only 
assumes that the user is committed to the asserted 
proposition when a confidence l vel of 2 is observed, 
while some asserts are assumed not to have been 
sufficiently well understood, and are only assigned a 
confidence l vel 1. This leads the system to perform 
a check, as we will see shortly. 
The next three update rules, for check, direct, 
and info_req, all impose an obligation on the other 
dialogue participant to address the dialogue act. In 
addition, the direct rule introduces a conditional 
act: acceptance of the directive will impose an obli- 
gation on the hearer to act on its contents. 
In addition, all FORWARD ACTS 9 in the DRI 
scheme (Discourse Resource Initiative, 1997) impose 
an obligation to perform an understanding act (e.g., 
an acknowledgement): 
(3) 1 act 
effect 
ID:c, forward-looking-act (DP) 
push(OBL,u-act (o(DP),CDU.id)) I 
The internal urules implementing the updates in 
(2) have the format shown in (4), which is the urule 
for info_request. 
(4) =uxe( doZnfoR, q. rulet~.S, 
\[ hearer(DP), 
latest_moves: in(Hove}, 
Move:valEec(pred,inforeq) \],  
\[ incr_set(update_cycles,_), 
incr_set (next.dh_id, HID), 
next _du_name (ID), 
pushRec (w'cdu'tognd'dh, 
record ( \[atype=Move, c level=2, id=HID \ ] ) ) ,  
pushRec (e'cdu~tosnd'obl, 
record ( \[pred~address, dp=DP, 
argsfstackset ( 
\[record ( \[i%em=IIID\] )\] ) \] ) ), 
pushRec (w'gnd" obl, 
record ( \[pred=uact, dp=P, 
args=stackset ( 
\[rocord(\[item=ID\] ) \] ) \] )) \ ] ) .  
As noted above, these rules have four parts; a 
name, a type, a list of conditions, and a list of ef- 
fects. The conditions in (4) state that there must be 
a move in latest_moves whose predicate is inforeq. 
The effects l? state that the move should be recorded 
in the dialogue history in CDU, that an obligation to 
address the request should be pushed into OBL in 
CDU, and that the requirement for an understand- 
ing act by W should be pushed irectly into the list 
in W.GND. 
The fourth and final step of the algorithm cycles 
through the updating process in case recently added 
facts have further implications. For instance, when 
an action has been performed that matches the an- 
tecedent of a rule in COND, the consequent is es- 
tablished. Likewise, when an action is performed 
it releases any obligations to perform that action. 
Thus, accept, answer, and agree are all ways of 
releasing an obligation to address, since these are 
all appropriate backward looking actions. Similarly, 
an agent will drop intentions to perform actions it 
has already (successfully) performed. 
3.4 Deliberation 
We assume, in common with BDI-approaches to 
agency (e.g., (Bratman et al, 1988)) that intentions 
9Forward acts include assert, check, direct, and 
info_request. 
l?The ID and HID values simply contain numbers identifying 
the discourse units and conversational acts. 
5 
are the primary mental attitude leading to an agen- 
t's actions. The main issues to explain then become 
how such intentions are adopted given the rest of 
the information state, and how an agent gets from 
intentions to actual performance. 
For the latter question, we take a fairly simplistic 
approach here: all the intentions to perform dia- 
logue acts are simply transferred to the next_moves 
system variable, with the assumption that the gen- 
eration module can realise all of them as a single ut- 
terance. A more sophisticated approach would be to 
weight the importance of (immediate) realisation of 
sets of intentions and compare this to the likelihood 
that particular utterances will achieve these effects 
at minimal cost, and choose accordingly. We leave 
this for future work (see (Traum and Dillenbourg, 
1998) for some preliminary ideas along these lines), 
concentrating here on the first issue - how the sys- 
tem adopts intentions to perform dialogue acts from 
other aspects of the mental state. 
The current system takes the following factors into 
account: 
? obligations (to perform understanding acts, to 
address previous dialogue acts, to perform other 
actions) 
? potential obligations (that would result if an- 
other act were performed, as represented in the 
COND field) 
? insufficiently understood ialogue acts (with a 
1 confidence level in CDU.DH) 
? intentions to perform complex acts 
The current deliberation process assumes maxi- 
mal cooperativity, in that the system always chooses 
to meet its obligations whenever possible, and also 
chooses to provide a maximally helpful response 
when possible. Thus, when obliged to address  a 
previous dialogue act such as a question or direc- 
tive, it will choose to actually return the answer or 
perform the action, if possible, rather than reject or 
negotiate such a performance, which would also be 
acting in accordance with the obligations (see (Kreu- 
tel, 1998) on how acts might be rejected). 
In the current implementation, the following rules 
are used to adopt new intentions (i.e., to update the 
INT field): 
(5) 1. add an intention to acknowl- 
edge(W,CDU), given an obligation to 
perform a u-act,  if everything in CDU is 
sufficiently understood (i.e., to level 2); 
2. add an intention to accept a directive or an- 
swer a question as the result of an obligation 
to address a dialogue act; 
3. add an intention to perform an action if 
COND contains a conditional that will estab- 
lish an obligation to perform the action, and 
the antecedent of this conditional is another 
action that is already intended. (This an- 
ticipatory planning allows the obligation to 
be discharged at the same time it is invoked, 
e.g., without giving an intermediate accep- 
tance of an directive.) 
4. add an intention to perform a (dialogue) ac- 
tion motivated by the intention to perform 
the current task. In the case of the Au- 
toroute domain, we have two cases: the sys- 
tem may decide 
(a) to check any dialogue acts in CDU at 
confidence level 1, which contain infor- 
mation needed to discharge the intention 
to give a route; or 
(b) to perform a question asking about a new 
piece of information that has not been es- 
tablished (this is decided by inspecting 
GND.SCP and CDU.SCP). For example, 
it may decide to ask about the starting 
point, the time of departure, etc. 
4 Extended Example  
In this section, we discuss more examples of how the 
information state changes as a result of processing 
and performing dialogue acts. It is useful to do this 
by looking briefly at a typical Autoroute dialogue, 
shown in (6). 11 Our implementation can process this 
sort of dialogue using very simple interpretation and 
generation routines that provide the dialogue acts 
in latest_moves from the text strings, and produce 
W's output text from the dialogue acts which the 
system places in next_moves. 
(6) W \[1\]: How can I help? 
C \[2\]: A route please 
W \[3\]: Where would you like to start? 
C \[4\]: Malvern 
W \[5\]: Great Malvern? 
C \[6\]: Yes 
W \[7\]: Where do you want to go? 
C \[8\]: Edwinstowe 
W \[9\]: Edwinstowe in Nottingham? 
C \[10\]: Yes 
W \[11\]: When do you want to leave? 
C \[12\]: Six pm 
W \[13\]: Leaving at 6 p.m.? 
C \[14\]: Yes 
W \[15\]: Do you want the quickest or the 
shortest route? 
C \[16\]: Quickest 
W \[17\]: Please wait while your route is cal- 
culated. 
We assume that before the dialogue starts, W has 
the intention to ask C what kind of help is required, 
liThe interchanges have been cleaned up to some extent 
here, mainly by removing pauses and hesitations. 
6 
W: 
I \[ /gi . . . . .  ,e(W, \ I OBL: ~understandlngAet(W,DU5)) / \address(C,CA8 ) \[ \[ / CA I0: C2, acknowledge(C.DU4 ) \ iGND: /DH: (CAg: C2, accept( W.CA6 ) ~/ / SCP: < ? 
\[COND: < > 
UDUS: <DU5> 
\[ /CA9:C2 . . . .  pt(W,CA6) \ / /  
TOGND: OH: \CAS: C2. info request( W,?start ) I l l  PDU: / /  
LCOND: < > 
LID: DU4 
DH: ~/CA 12: C2, a~wer( C,CA8,CAI 1 ) 
CDU: TOGND: \CA 11: Cl, assert( C.s 'tart(malvem) ) 
SCP: < > 
LCOND: < > 
LID: DU5 
/ check (W.,start(malvern ) ) \
INT: ~acknowledge( W.DU5 ) 
\giveroute( W ) / 
I INT: <getroute( C ) ? l 
Figure 3: Information State Prompting Check in \[5\] 
and that C has the intention to find a route. We also 
assume that W has the turn, and that the presence 
of the how can I help intention triggers an utterance 
directly. Figure 2, presented above, shows the in- 
formation state after utterance \[2\]. The intentions 
in that figure lead directly to the system producing 
utterance \[3\]. 
Looking a little further ahead in the dialogue, Fig- 
ure 3 shows the information state after utterance 
\[4\]. 12 Here we can see in CDU.TOGND.DH (along with 
the ack act CA10, in GND.DH) the dialogue moves 
that this utterance has generated. Note that the as- 
sert, CA l l ,  is only at confidence level 1, indicating 
lack of sufficient certainty in this interpretation as 
the town 'Great Malvern'. This lack of certainty and 
the resulting lack of a relevant SCP in CDU.TOGND 
lead the deliberation routines to produce an inten- 
tion to check this proposition rather than to move 
directly on to another information request. This 
intention leads to utterance \[5\], which, after inter- 
pretation and updating on the new dialogue acts, 
leads to the information state in Figure 4. The in- 
teresting thing here is the condition which appears 
in CDU.TOGND.COND as a result of the check; the 
interpretation of this is that, if C agrees with the 
check, then W will be committed to the proposition 
that the starting place is Malvern (C would also be 
committed to this by way of the direct effects of an 
agree act). 
12The actual information state contains all the previously 
established ialogue acts, SCPs and Obligations in GND~ from 
Figure 2 and intermediate utterances. Here we have deleted 
these aspects from the figures for brevity and clarity. 
OBL: \glveroute(W ) / \[ 
/CAI3: C2. acknowledge( W.DU5 ) \ \ [  
GND: /DH: ~ CA 12: C2, answer(C.CA8 ) }/  / \CA 11 :C 1, assert(C.start(malvem) ) / \ [SCP: < ? 
LCOND: < > J 
UDUS: <DU6> 
IOBL: < ? 
W: TOGND: DFt: \CA i i: Ci, as~rt(C,s~t(malvem) ) i i i  
PDU: SCP: < > 
LID: DU5 
r r?BL: <address(C.CAl4)> 
\]TOGND: \[DH: <CA 14: C2, check (W.,smrt(rnalvern)) > 
CDU: / /SOP: < > 
LCOND: <agree(C,CAl4 \] .> scp(W,start(malvern) }; 
L In: DU6 
INT: ,:giveroute(W )> 
C: lINT: <getroute(C)>l 
Figure 4: Information State Following Check in \[5\] 
I r /..<,.,..,.,,..,~.?,,,,,=.,:>,.,8>\ 1 11 / ?BL: \gi . . . . .  l ,(w) / / / /  
/,--,,-, /c , , . :c2  .,~,.>,,~=<W.DU~>\/ / /  
GND: \[----: \CA 16: C2. igi~e( C,CA 14 ) / /  / /  
Is<:,> ,/,,<,,< c~,.,,,.,<,.,,=,,.<,,.. > > ~, / / /  i : \scp(W,slirl( malvcrn D/  / / /  
l.CO~D: < > J N 
uous: <oul> / /  
I loB, . . . .  11 l/ W: TOGND" DH: <CA 16: C2. agree(C,CA 14 ) > 
iPDU: \[ " \[SCP: < sop( C,start( malvern ) } > H | /  
/ / LCOND: < > J/ l /  
\] LID: DO7 J / /  
/ \[ \[OBL: <Iddress(C.CAI8 )> 11ll 
/ /TOGND" / DH: <CAIS: C2. info_nequesi(W.?dest )>//// 
/ c~U: / / sc~: < > / IN  
/ / L~oNo: < > ill / 
/ uo: ,~  .ill 
C: lINT: <getroute(C)>l J 
Figure 5: Information state following \[7\] 
After C's agreement in \[6\], the deliberation rou- 
tine is able to move past discussion of the start- 
ing point, and add an intention to ask about the 
next piece of information, the destination. This 
leads to producing utterance \[7\], which also implic- 
itly acknowledges \[6\], after which C's agreement is 
grounded, leading to the IS shown in Figure 5. Note 
that the list in W.GND.SCP in Figure 5 indicates that 
both C and W are committed to the proposition that 
the starting place is Malvern. 
5 Conc lus ions  
It has only been possible here to introduce the basic 
concerns of the PTT account of dialogue modelling 
and to pick out one or two illustrative examples to 
highlight the implementational approach which has 
7 
been assumed. Current and future work is directed 
towards measuring the theory against more challeng- 
ing data to test its validity; cases where ground- 
ing is less automatic are an obvious source of such 
tests, and we have identified a few relevant problem 
cases in the Autoroute dialogues. We do claim, how- 
ever, that the implementation as it stands validates 
a number of key aspects of the theory and provides 
a good basis for future work in dialogue modelling. 
Acknowledgments  
The TRINDI (Task Oriented Instructional Dia- 
logue) project is supported by the Telematics Appli- 
cations Programme, Language Engineering Project 
LE4-8314. Massimo Poesio is supported by an EP- 
SRC Advanced Research Fellowship. 
Re ferences  
J. Allwood. 1976. Linguistic Communication as 
Action and Cooperation. Ph.D. thesis, GSteborg 
University, Department of Linguistics. 
J. Allwood. 1994. Obligations and options in dia- 
logue. Think Quarterly, 3:9-18. 
M. E. Bratman, D. J. Israel and M. E. Pollack. 1988. 
Plans and Resource-Bounded Practical Reason- 
ing. Computational Intelligence, 4(4). 
P. Bretier and M. D. Sadek. 1996. A rational agent 
as the kernel of a cooperative spoken dialogue 
system: Implementing a logical theory of inter- 
action. In J. P. Miiller, M. J. Wooldridge, and 
N. R. Jennings, editors, Intelligent Agents III -- 
Proceedings of the Third International Workshop 
on Agent Theories, Architectures, and Languages 
(ATAL-96), Lecture Notes in Artificial Intelli- 
gence. Springer-Verlag, Heidelberg. 
J. Calder. 1998. Thistle: diagram display en- 
gines and editors. Technical Report HCRC/TR- 
97, HCRC, University of Edinburgh, Edinburgh. 
H. H. Clark and E. F. Schaefer. 1989. Contributing 
to discourse. Cognitive Science, 13:259-294. 
H. H. Clark and D. Wilkes-Gibbs. 1986. Referring 
as a collaborative process. Cognition, 22:1-39. 
Also appears as Chapter 4 in (Clark, 1992). 
H. H. Clark. 1992. Arenas of Language Use. Uni- 
versity of Chicago Press. 
P. R. Cohen and H. J. Levesque. 1990. Rational in- 
teraction as the basis for communication. I  P. R. 
Cohen, J. Morgan, and M. E. Pollack, editors, In- 
tentions in Communication. MIT Press. 
Discourse Resource Initiative. 1997. Standards for 
dialogue coding in natural anguage processing. 
Report no. 167, Dagstuhl-Seminar. 
B. J. Grosz and C. L. Sidner. 1990. Plans for dis- 
course. In P. R. Cohen, J. Morgan, and M. E. Pol- 
lack, editors, Intentions in Communication. MIT 
Press. 
J. Kreutel. 1998. An obligation-driven computa- 
tional model for questions and assertions in dia- 
logue. Master's thesis, Department ofLinguistics, 
University of Edinburgh, Edinburgh. 
S. Larsson, P. Bohlin, J. Bos, and D. Traum. 1999. 
Trindikit manual. Technical Report Deliverable 
D2.2 - Manual, Trindi. 
M. Poesio, R. Cooper, S. Larsson, D. Traum, and 
C. Matheson. 1999. Annotating conversations for 
information state update. In Proceedings of Am- 
stelogue 99, 3rd Workshop on the Semantics and 
Pragmatics of Dialogues. 
M. Poesio and D. R. Tranm. 1997. Conversational 
actions and discourse situations. Computational 
Intelligence, 13(3). 
M. Poesio and D. R. Traum. 1998. Towards an ax- 
iomatization of dialogue acts. In Proceedings of 
Twendial'98, 13th Twente Workshop on Language 
Technology, pages 207-222. 
M. D. Sadek. 1991. Dialogue acts are rational plans. 
In Proceedings o\] the ESCA/ETR workshop on 
multi-modal dialogue. 
M. P. Singh. 1998. Agent communication lan- 
guages: Rethinking the principles. IEEE Com- 
puter, 31(12):40-47. 
D. R. Traum and J. F. Allen. 1992. A speech acts 
approach to grounding in conversation. In Pro- 
ceedings 2nd International Conference on Spoken 
Language Processing (ICSLP-92), pages 137-40, 
October. 
D. R. Traum and J. F. Allen. 1994. Discourse obli- 
gations in dialogue processing. In Proceedings of 
the 32nd Annual meeting of the Association for 
Computational Linguistics, pages 1-8, June. 
D. R. Traum, J. Bos, R. Cooper, S. Larsson, I. 
Lewin, C. Matheson, and M. Poesio. 1999. A 
model of dialogue moves and information state re- 
vision. Technical Report Deliverable D2.1, Trindi. 
D. R. Traum and P. Dillenbourg. 1998. Towards a 
Normative Model of Grounding in Collaboration. 
In Proceedings of the ESSLLI98 workshop on Mu- 
tual Knowledge, Common Ground and Public In- 
formation. 
D. R. Traum. 1994. A computational theory 
of grounding in natural language conversation. 
Ph.D. thesis, Computer Science, University of 
Rochester, New York, December. 
8 
Telicity as a Cue to Tempora l  and Discourse Structure in 
Chinese-Engl ish Machine Translation* 
Mari Olsen David Traum 
Microsoft U Maryland 
molsen@microsoft.com traum@cs.umd.edu 
Carol Van Ess-Dykema 
U.S. Department ofDefense 
carol@umiacs.umd.edu 
Amy Weinberg 
U Maryland 
weinberg@umiacs.umd.edu 
Ron Dolan 
Library of Congress 
rdolan@cfar.umd.edu 
Abstract 
Machine translation between any two languages re- 
quires the generation of information that is implicit 
in the source language. In translating from Chinese 
to English, tense and other temporal information 
must be inferred from other grammatical and lex- 
ical cues. Moreover, Chinese multiple-clause sen- 
tences may contain inter-clausal relations (temporal 
or otherwise) that must be explicit in English (e.g., 
by means of a discourse marker). Perfective and im- 
perfective grammatical aspect markers can provide 
cues to temporal structure, but such information is 
not present in every sentence. We report on a project 
to use the \]exical aspect features of (a)te\]icity re- 
flected in the Lexical Conceptual Structure of the 
input text to suggest ense and discourse structure 
in the English translation of a Chinese newspaper 
corpus. 
1 Introduction 
It is commonly held that an appropriate interlingua 
must allow for the expression of argument relations 
in many languages. This paper advances the state of 
the art of designing an interlingua by showing how 
aspectual distinctions (telic versus atelic) can be de- 
rived from verb classifications primarily influenced 
by considerations of argument structure, and how 
these aspectual distinctions can be used to fill lexical 
gaps in the source language that cannot be left un- 
specified in the target language. Machine translation 
between any two languages often requires the gen- 
eration of information that is implicit in the source 
language. In translating from Chinese to English, 
tense and other temporal information must be in- 
ferred from other grammatical nd lexical cues. For 
example, Chinese verbs do not necessarily specify 
whether the event described is prior or cotempora- 
neous with the moment of speaking. While gram- 
matical aspect information can be loosely associated 
with time, with imperfective aspect (Chinese ~ zai- 
and ~ .zhe) representing present ime and perfec- 
tiv e (Chinese T le )  representing past time, (Chu, 
* We gratefully acknowledge DOD support for this work 
through contract MDA904-96-R-0738 
1998; Li and Thompson, 1981), verbs in the past 
do not need to have any aspect marking distinguish- 
ing them from present tense verbs. Th is  is unlike 
English, which much more rigidly distinguishes past 
from present ense through use of suffixes. Thus, in 
order to generate an appropriate English sentence 
from its Chinese counterpart, we need to fill in a 
potentially unexpressed tense. 
Moreover, Chinese multiple-clause ntences may 
contain implicit relations between clauses (temporal 
or otherwise) that must be made explicit in English. 
These multiple-clause ntences are often most nat- 
urally translated into English including an overt ex- 
pression of their relation, e.g., the "and" linking the 
two clauses in (1), or as multiple sentences, as in (2)). 
(1) 1 9 65  ~ ~ , ~ ,~ 
1965 year before , our_country altogether 
only have 30 ten_thousand ton de 
~t ~2 , ~ ~ ~8 
shipbuilding capacity , year output is 8 
ten_thousand ton 
Before 1965 China had a total of only 300,000 
tons of shipbuilding capacity and the annual 
output was 80,000 ~ons. 
(2)~ 8~ ~ ~ ~d Y 
this 8 ten_thousand ton actually include asp 
517 cl , ship de tonnage is very low de 
This 80,000 tons actually included 517 ships. 
Ship tonnage was very low. 
In our NLP applications, we use a level of linguis- 
tic structure driven by the argument-taking proper- 
ties of predicates and composed monotonically up to 
the sentence l vel. The resulting Lexical Conceptual 
Structures (LCS) (3ackendoff, 1983), is a language- 
neutral representation of the situation (event or 
state), suitable for use as an interlingua, e.g., for 
machine translation. The LCS represents predicate 
argument structure abstracted away from language- 
specific properties of semantics and syntax. The 
34 
primitives of the interlingua provide for monotonic 
composition that captures both conceptual nd syn- 
tactic generalities (Dorr et al, 1993) among lan- 
guages. 1 The strength of the representation derives 
from the cross-linguistic regularities in the lexical se- 
mantics encoded in the LCS. The syntactic hierarchy 
(subject, object, oblique) is mirrored in the LCS hi- 
erarchy: for example THEMES are arguments of the 
LCS predicate, and AGENTS are arguments of the 
theme-predicate composition. Syntactic divergences 
(whether the object precedes or follows the verb, for 
example) are represented in language specific lin- 
earization rules; lexical divergences (whether the lo- 
cation argument is encoded irectly in the verb, e.g. 
the English verb pocket or must be saturated by an 
exterfial argument) are stated in terms of the pieces 
of LCS struct-ure in the lexicon. SententiM repre- 
sentations derive from saturating the arguments re- 
quired by the predicates in the sentence. 
LCS rePresentations also include temporal infor- 
mation, where available in the source language: re- 
cent revisions include, for example (Dorr and Olsen, 
1997a) standardizing LCS representations for the as- 
pectual (un)boundedness ((A)TELICITY) of  events, 
either lexically or sententially represented. Although 
at present he LCS encodes no supra-sentential dis- 
course relations, we show how the lexical aspect in- 
formation may be used to generate discourse co- 
herence in temporal structure. Relations between 
clauses as constrained by temporal reference has 
been examined in an LCS framework by Dorr and 
Gaasterland (Dorr and Gaasterland, 1995). They 
explore how temporal connectives are constrained 
in interpretation, based on the tense of the clauses 
they connect. While overt temporal connectives are 
helpful when they appear, our corpus contains many 
sentences with neither tense markers nor tense con- 
nectives. We must therefore look to a new source of 
information. We rely on the lexical information of 
the verbs within a sentence to generate both tense 
and temporal connectives. 
Straightforward LCS analysis of many of the 
multi-clause sentences in our corpus leads to vio- 
lations of the wellformedness conditions, which pre- 
vent structures with events or states directly modi- 
fying other events or states. LCS, as previously con- 
ceived, prohibits an event or state from standing in a 
modifier elationship to another event or state, with- 
out mediation of a path or position (i.e., as lexically 
realized by a preposition). This restriction reflects 
the insight hat (at least in English) when events and 
states modify each other, the modification is either 
implicit, with the relevant events and states in sepa- 
rate sentences (and hence separate LCSs), as in the 
1 LCS representations in our system have been created for 
Korean, Spanish and Arabic, as well as for English and Chi- 
nese. 
first sentence below, or explicit in a single sentence, 
as in the second sentence below. Implicit event-state 
modification (sentence 3) is prohibited. 
* Wade bought a car. He needed a way to get to 
work. 
* Wade bought a car because he needed a way to 
get to work. 
* * Wade bought a car he needed a way to get to 
work. 
It is exactly these third type that are permitted 
in standard Chinese and robustly attested in our 
data. If the LCS is to be truly an interlingua, we 
must extend the representation to allow these kinds 
of sentences to be processed. One possibility is to 
posit an implicit position connecting the situations 
described by the multiple clauses. In the source lan- 
guage analysis phase, this would amount o positing 
a disjunction of all possible position relations im- 
plicitly realizable in this language. Another option 
is to relax the wellformedness constraints to allow 
an event o directly modify another event. This not 
only fails to recognize the regularities we see in En- 
glish (and other language) LCS structures, for Chi- 
nese it merely pushes the problem back one step, 
as the set of implicitly realizable relations may vary 
from language to language and may result in some 
ungrammatical or misleading translations. The sec- 
ond option can be augmented, however, by factoring 
out of the interlingua (and into the generation code) 
language-specific principles for generating connec- 
tives using information i the LCS-structure, proper. 
For the present, this is the approach we take, us- 
ing lexical aspectual information, as read from the 
LCS structure, to generate appropriate mporal re- 
lations. 
Therefore not only tense, but inter-sentential dis- 
course relations must be considered when generating 
English from Chinese, even at the sentence l vel. We 
report on a project to generate both temporal and 
discourse relations using the LCS representation. I  
particular, we focus on the encoding of the lexical as- 
pect feature TELICITY and its complement ATELIG- 
ITY to generate past and present ense, and corre- 
sponding temporal relations for modifying clauses 
within sentences. While we cannot at present di- 
rectly capture discourse relations, we can garner as- 
pectual class from LCS verb classification, which in 
turn can be used to predict he appropriate nse for 
translations ofChinese verbs into English. 
2 Use of Aspect to Provide 
Temporal Informat ion 
We begin with a discussion of aspectual features of 
sentences, and how this information can be used to 
provide information about the time of the situations 
35 
presented in a sentence. Such information can be 
used to help provide clues as to both tense and rela- 
tionships (and cue words) between connected situa- 
tions. Aspectual features can be divided into gram- 
matical aspect, which is indicated by lexical or mor- 
phological markers in a sentence, and lexical aspect, 
which is inherent in the meanings of words. 
2.1 Grammat ica l  aspect 
Grammatical aspect provides a viewpoint on situa- 
tion (event or state) structure (Smith, 1997). Since 
imperfective aspect, such as the English PROGRES- 
SIVE construction be VERB- ing,  views a situation 
from within, it is often associated with present 
or contemporaneous time reference. On the other 
hand, perfective aspect, such as the English have 
VERB-ed, Views a situation as a whole; it is there- 
fore often associated with past time reference ((Com- 
rie, 1976; Olsen, 1997; Smith, 1997) cf. (Chu, 
1998)). The temporal relations are tendencies, 
rather than an absolute correlation: although the 
perfective is found more frequently in past tenses 
(Comrie, 1976), both imperfective and perfective co- 
occur in some language with past, present, and fu- 
ture tense. 
In some cases, an English verb will specify tense 
and/or aspect for a complement. For example, con- 
tinue requires either an infinitive (3)a or progressive 
complement (3)b (and subject drop), while other 
verbs like say do not place such restrictions (3)c,d. 
(3) a. Wolfe continued to publicize the baseless 
criticism on various occasions 
b. Wolfe continued publicizing the baseless 
criticism on various occasions 
c. Wolfe continued publicizing the baseless 
criticism on various occasions 
d. He said the asia-pacific region already be- 
came a focal point region 
e. He said the asia-pacific region already is be- 
coming a focal point region 
2.2 Lexical aspect 
While grammatical spect and overt temporal cues 
are clearly helpful in translation, there are many 
cases in our corpus in which such cues are not 
present. These are the hard cases, where we must 
infer tense or grammatical spectual marking in the 
target language from a source that looks like it pro- 
vides no overt cues. We will show however, that 
Chinese does provide implicit cues through its lex- 
ical aspect classes. First, we review what lexical 
aspect is. 
Lexical aspect refers to the type of situation de- 
noted by the verb, alone or combined with other 
sentential constituents. Verbs are assigned to lexical 
aspect classes based on their behavior in a variety of 
syntactic and semantic frames that focus on three as- 
pectual features: telicity, dynamicity and durativity. 
We focus on telicity, also known as BOUNDEDNESS. 
Verbs that are telic have an inherent end: winning, 
for example, ends with the finish line. Verbs that are 
atelic do not name their end: running could end with 
a distance run a mile or an endpoint run to the store, 
for example. Olsen (Olsen, 1997) proposed that as- 
pectual interpretation be derived through monotonic 
composition of marked privative features \[?/0 dy- 
namic\], \[.4-/0 durative\] and \[-t-/0 telic\], as shown in 
Table 1 (Olsen, 1997, pp. 32-33). 
With privative features, other sentential con- 
stituents can add to features provided by the verb 
but not remove them. On this analysis, the \[.-I-du- 
rative, +dynamic\] features of run propagate to the 
sentence l vel in run ~o the store; the \[?telic\] feature 
is added by the NP or PP, yielding an accomplish- 
ment interpretation. The feature specification of this 
?ompositionally derived accomplishment is herefore 
identical to that of a sentence containing a telic ac- 
complishment verb, such as destroy. 
According to many researchers, knowledge of lex- 
ical aspect--how verbs denote situations as devel- 
oping or holding in time-=may be used to interpret 
event sequences in discourse (Dowty, 1986; Moens 
and Steedman, 1988; Passoneau, 1988). In particu- 
lar, Dowty suggests that, absent other cues, a relic 
event is interpreted as completed before the next 
event or state, as with ran into lhe room in 4a; in 
contrast, atelic situations, such as run, was hungry 
in 4b and 4% are interpreted as contemporaneous 
with the following situations: fell and made a pizza, 
respectively. 
(4) a. Mary ran into the room. She turned on her 
walkman. 
b. Mary ran. She turned on her walkman. 
c. Mary was hungry. She made a pizza. 
Smith similarly suggests that in English all past 
events are interpreted as telic (Smith, 1997) (but cf. 
(Olsen, 1997)). 
Also, these tendencies are heuristic, and not abso- 
lute, as shown by the examples in (5). While we get 
the expected prediction that the jumping occurs af- 
ter the explosion in (5)(a), we get the reverse predic- 
tion in (5)(b). Other factors such as consequences of 
described situations, discourse context, and stereo- 
typical causal relationships also play a role. 
(5) a. The building exploded. Mary jumped. 
b. The building exploded. Chunks of concrete 
flew everywhere. 
36 
Aspectual  Class 
State 
Activity 
-~Accomplishment 
Achievement 
Tel ie Dynamic  Durat ive 
+ 
+ ,, , + __ run , paint 
+ + + 
+ + 
Examples 
know, have 
destroy 
notice, win 
Table 1: Lexical Aspect Features 
3 Aspect  in Lex ica l  Conceptua l  
S t ructure  
Our implementation f Lexical ConceptuM Struc- 
ture (Dowty, 1979; Guerssel et al, 1985)--an 
augmented form of (Jackendoff, 1983; Jackendoff, 
1990)--permits lexical aspect information to be 
read directly off the lexical entries for individual 
verbs, as well-as composed representations for sen- 
tences, using uniform processes and representations. 
The LCS framework consists of primitives (GO, 
BE, STAY, etc.), types (Event, State, Path, etc.) 
and fields (Loc(ational), Temp(oral), Foss(essional), 
Ident(ificational), Perc(eptual), etc.). 
We adopt a refinement of the LCS representation, 
incorporating meaning components from the linguis- 
tically motivated notion of !ezical semantic template 
(LST), based on lexical aspect classes, as defined 
in the work of Levin and Rappaport Hovav (Levin 
and Rappaport Hovav, 1995; Rappaport l tovav and 
Levin, 1995). Verbs that appear in multiple as- 
pectual frames appear in multiple pairings between 
constants (representing the idiosyncratic meaning 
of the verb) and structures (the aspectual class). 
Since the aspectual templates may be realized in 
a variety of ways, other aspects of the structural 
meaning contribute to differentiating the verbs from 
each other. Our current database contains ome 400 
classes, based on an initial representation f the 213 
classes in (Levin, 1993). Our current working lexi- 
con includes about 10,000 English verbs and 18,000 
Chinese verbs spread out into these classes. 
Telic verbs (and sentences) contain certain types 
of Paths, or a constant, represented by ! !, filled by 
the verb constant, in the right most leaf-node argu- 
ment. Some examples are shown below: 
depart (go foe (* thing 2) 
(away_from loc (thing 2) 
(at foe (thing 2) 
(* thing 4))) 
(!!+ingly 26)) 
insert (cause (* thing 1) 
(go loc (* thing 2) 
((* toward 5) loc (thing 2) 
( \ [at \ ]  loc (thing 2) 
(thing 6))) )  
(! !+ingly 26)) 
Each of these relic verbs has a potential coun- 
terpart with an atelic verb plus the requisite path. 
Depart, for example, corresponds to move away, or 
something similar in another language. 
We therefore identify telic sentences by the algo- 
rithm, formally specified in in Figure 1 (cf. (Dorr 
and Olsen, 1997b) \[156\]). 
Given an LCS representation L: 
1. Initialize: T(L):=\[?T\], D(L):=\[0R\], R(L):=\[0D\] 
2. If Top node of L E {CAUSE, LET, GO} 
Then T(L):=\[+T\] 
If Top node of L E {CAUSE, LET} 
Then D(L):=\[+D\], R(L):=\[+R\] 
If Top node of L E {GO} 
Then D(L):=\[+D\] 
3. If Top node of L E {ACT, BE, STAY} 
Then If Internal node of 
L E {TO, TOWARD,  FORTemp} 
Then T(L):=\[+T\] 
If Top node of L E {BE, STAY} 
Then R(L):=\[+R\] 
If Top node of L E {ACT} 
Then set D(L):=\[+D\], R(L):=\[+R\] 
4. Return T(L), D(L), R(L). 
Figure 1: Algorithm for Aspectual Feature Determi- 
nation 
This algorithm applies to the structural primitives 
of the interlingua structure rather than actual verbs 
in source or target language. The first step initial- 
ized the aspectual values as unspecified: atelic f-T\], 
stative (not event: f-D\]), and adurative f-R\]. First 
the top node is examined for primitives that indicate 
telicity: if the top node is CAUSE, LET, GO, telicity 
is set to \[+T\], as with the verbs break, destroy, for 
example. (The node is further checked for dynamic- 
ity \[+D\] and durativity \[+R\] indicators, not in focus 
in this paper.)If the top node is not a relic indicator 
(i.e., the verb is a basically atelic predicate such as 
love or run, telicity may still be still be indicated 
by the presence of complement odes of particular 
types: e.g. a goal phrase (to primitive) in the case of 
run. The same algorithm may be used to determine 
felicity in either individual verbal entries (break but 
37 
not run) or composed sentences (John ran to ~he 
store but not John ran. 
Similar mismatches of telicity between represen- 
tations of particular predicates can occur between 
languages, although there is remarkable agreement 
as to the set of templates that verbs with related 
meanings will fit into (Olsen et al, 1998). In the 
Chinese-English interlingual system we describe, the 
Chinese is first mapped into the LCS, a language- 
independent representation, from which the target- 
language sentence is generated. Since telicity (and 
other aspects of event structure) are uniformly rep- 
resented at the lexical and the sentential level, telic- 
ity mismatches between verbs of different languages 
may then be compensated for by combining verbs 
with other .components. 
. o  
4 Predictions 
Based on (Dowty, 1986) and others, as discussed 
above, we predict that sentences that have a telic 
LCS will better translate into English as the past 
tense, and those that lack telic identifiers will trans- 
late as present tense. Moreover, we predict that 
verbs in the main clause that are telic, will be past 
with respect o their subordinates (X then Y). Verbs 
in the main clause that are atelic we predict will tem- 
porally overlap (X while Y). 
5 Implementation 
LCSes are used as the interlingua for our machine 
translation efforts. Following the principles in (Dorr, 
1993), lexical information and constraints on well- 
formed LCSes are used to compose an LCS for a 
complete sentence from a sentence parse in a source 
language. This composed LCS (CLCS) is then used 
as the starting points for generation into the target 
language, using lexical information and constraints 
for the target language. 
The generation component consists of the follow- 
ing subcomponents: 
Decomposi t ion and lexlcal selection First, 
primitive LCSes for words in the target lan- 
guage are matched against CLCSes, and tree 
structures of covering words are selected. Am- 
biguity in the input and analysis represented 
in the CLCS is maintained (insofar as it is 
possible to realize particular eadings using the 
target language lexicon), and new ambiguities 
are introduced when there are different ways of 
realizing a CLCS in the target language. 
AMR Construct ion This tree structure is then 
translated into a representation using the Aug- 
mented Meaning Representation (AMR) syntax 
? of instances and hierarchical relations (Langk- 
fide and Knight, 1998a); however the rela- 
tions include information present in the CLCS 
and LCSes for target language words, including 
theta roles, LCS type, and associated features. 
Real izat ion The AMR structure is then linearized, 
as described in (Dorr et al, 1998), and mor- 
phological realization is performed. The result 
is a lattice of possible realizations, represent- 
ing both the preserved ambiguity from previous 
processing phases and multiple ways of lineariz- 
ing the sentence. 
Extract ion The final stage uses a statistical bi- 
gram extractor to pick an approximation of the 
most fluentrealization (Langkilde and Knight, 
1998b). 
While there are several possible ways to address 
the tense and discourse connective issues mentioned 
above, such as modifying the LCS primitive lements 
and/or the composition of the LCS from the source 
language, we instead have been experimenting for 
the moment with solutions implemented within the 
generation component. The only extensions to the 
LCS language have been loosening of the constraint 
against direct modification of states and events by 
other states and events (thus allowing composed LC- 
Ses to be formed from Chinese with these structures, 
but creating a challenge for fluent generation into 
English), and a few added features to cover some of 
the discourse markers that are present. We are able 
to calculate telicity of a CLCS, using the algorithm 
in Figure 1 and encode this information as a binary 
te l i?  feature in the Augmented Meaning Represen- 
tation (AMR). 
The realization algorithm has been augmented 
with the rules in (6) 
(6) a. If there is no tense feature, use telicity to 
determine the tense: 
: te l i c  + -~ : tense  past  
: re l i c  -- --~ : tense present 
b. In an event or state directly modifying 
another event or state, if there is no other 
clausal connective (coming from a subor- 
dinating conjunction or post-position in 
the original), then use telicity to pick a 
connective expressing assumed temporal 
relation: 
: re l i c  -~ -~ : scon j  then  
: re l i c  -- -~ : sconj while 
6 The Corpus 
We have applied this machine translation system to 
a corpus of Chinese newspaper text from Xinhua and 
other sources, primarily in the economics domain. 
The genre is roughly comparable to the American 
38 
Wall Street Journal. Chinese newspaper genre dif- 
fers from other Chinese textual sources, in a number 
of ways, including: 
? more complex sentence structure 
? more extensive use of acronyms 
? less use of Classical Chinese 
? more representative grammar 
? more constrained vocabulary (limited lexicon) 
? abbreviations are used extensively in Chinese 
newspaper headlines 
However, the presence of multiple events and 
states in a single sentence, without explicit modifi- 
catioia is characteristic ofwritten Chinese in general. 
In the 80-sentence corpus under consideration, the 
sentence structure is complex and stylized; with an 
average of 20 words per sentence. Many sentences, 
such as (1)and (2), have multiple clauses that are 
not in a direct complement relationship or indicated 
with explicit connective words. 
7 Ground Truth 
To evaluate the extent to which our Predictions re- 
sult in an improvement in translation, we have used 
a database of human translations of  the sentences 
in our corpus as the ground truth, or gold standard. 
One of the translators is included among our au- 
thors. 
The ground truth data was created to provide a 
fluid human translation of the text early in our sys- 
tem development. It therefore includes many com- 
plex tenses and multiple sentences combined, both 
currently beyond the state of our system. Thus, 
two of the authors and an additional researcher 
also created a database of temporal relations among 
the clauses in the sentences that produced illegal 
event/state modifications. This was used to test pre- 
dictions of temporal relationships indicated by telic- 
ity. In evaluating our results, we concentrate on how 
well the System did at matching past and present, 
and on the appropriateness of temporal connectives 
generated. 
8 Results 
We have applied the rules in (6) in generating 80 sen- 
tences in the corpus (starting from often ambiguous 
CLCS analyses). Evaluation is still tricky, since, in 
many cases, the interlingua nalysis is incorrect or 
ambiguous in ways that affect the appropriateness 
of the generated translation. 
8.1 Tense 
As mentioned above, evaluation can be very diffi- 
cult in a number of cases. Concerning tense, our 
"gold standard" is the set of human translations, 
generated  tense 
past p resent  
human past  134 17 
t rans la t ion  present  17 27 
Table 2: Preliminary Tense Results 
previously constructed for these sentences. In many 
cases, there is nothing overt in the sentence which 
would specify tense, so a mismatch might not actu- 
ally be "wrong". Also, there are a number of sen- 
tences which were not directly applicable for com- 
parison, such as when the human translator chose 
a different syntactic structure or a complex tense. 
The newspaper articles were divided into 80 sen- 
tences. Since some of these sentences were conjunc- 
tions, this yielded 99 tensed main verbs. These verbs 
either appeared in simple present, past, present or 
past perfect('has or had verb-t-ed), present or past 
imperfective (is verb-l-lag , was verb--I--lag) and their 
corresponding passive (is being kicked, was being 
kicked, have been kicked) forms. For cases like the 
present perfect ('has kicked), we noted the intended 
meaning ( e.g past activity) expressed by the verb 
as well as the verb's actual present perfective form. 
We scored the form as correct if the system trans- 
lated a present perfective with past tense meaning 
as a simple past or present perfective. There were 
10 instances where a verb in the human translation 
had no corresponding verb in the machine transla- 
tion, either due to incorrect omission or correct sub- 
stitution of the corresponding nominalization. We 
excluded these forms from consideration. If the sys- 
tem fails to supply a verb for independent reasons, 
our system clearly can't mark it with tense. The 
results of our evaluation are summarized in Table 2. 
These results definitely improve over our previ- 
ous heuristic, which was to always use past tense 
(assuming this to be the default mode for newspa- 
per article reporting). Results are also better than 
always picking present ense. These results seem to 
indicate that atelicity is a fairly good cue for present 
tense. We also note that 8 out of the 14 cases where 
the human translation used the present ense while 
the system used past tense are headlines. Headlines 
are written using the historical present in English 
("Man bites Dog"). These sentences would not be 
incorrectly translated in the past ("The Man Bit 
the Dog") Therefore, a fairer judgement might leave 
only remaining 6 incorrect cases in this cell. Using 
atelicity as a cue for the present yields correct re- 
sults approximately 65incorrect results 35worst case 
results because they do not take into account pres- 
ence or absence of the grammatical perfective and 
progressive markers referred to in the introduction. 
39 
8.2 Relat ionship between clauses 
Results are more preliminary for the clausal connec- 
tives. Of the 80 sentences, 35 of them are flagged as 
(possibly) containing events or states directly mod- 
ifying other events or states. However, of this num- 
ber, some actually do have lexical connectives repre- 
sented as featural rather than structural elements in 
the LCS, and can be straightforwardly realized using 
translated English connectives such as since, after, 
and if.then. Other apparently "modifying" events 
or states should be treated as a complement rela- 
tionship (at least according to the preferred reading 
in ambiguous cases), but are incorrectly analyzed 
as being in a non-complement relationship, or have 
other structural problems rendering the interlingua 
representation and English output not directly re- 
lated to the original clause structure. 
Of the remaining clear cases, six while relation- 
ships were generated according to our heuristics, in- 
dicating cotemporaneousness of main and modifying 
situation, e.g. (7)a,b, in the automated translations 
of (1) and (2), respectively. None were inappropri- 
ate. Of the cases where then was generated, indicat- 
ing sequential events, there were four cases in which 
this was appropriate, and three cases in which the 
situations really should have been cotemporaneous. 
While these numbers are small, this preliminary data 
seems to suggest again that atelicity is a good cue for 
cotemporality, while telicity is not a sufficient cue. 
(7) a. Before 1965, China altogether only have 
the ability shipbuilding about 300 thousand 
tons , while the annual output is 80 thou- 
sand tons. 
b. this 80 thousand tons actually includes 517 
ships, while the ship tonnage is very low. 
9 Conclus ions 
We therefore conclude that lexical aspect can serve 
as a valuable heuristic for suggesting tense, in the ab- 
sence of tense and other temporal markers. We an- 
ticipate incorporation of grammatical aspect infor- 
mation to improve our temporal representation fur- 
ther. In addition, lexical aspect, as represented by 
the interlingual LCS structure, can serve as the foun- 
dation for language specific heuristics. Furthermore, 
the lexical aspect represented in the LCS can help to 
provide the beginnings of cross-sentential discourse 
information. We have suggested applications in the 
temporal domain while, then. Causality is another 
possible domain in which relevant pieces encoded in 
sentence-level LCS structures could be used to pro- 
vide links between LCSes/sentences. Thus, the in- 
terlingual representation may be used to provide not 
only shared semantic and syntactic structure, but 
"also the building blocks for language-specific heuris- 
tics for mismatches between languages. 
10 Future  Research 
There are a number of other directions we intend to 
pursue in extending this work. First, we will evalu- 
ate the role of the grammatical spect markers men- 
tioned above, in combination with the telicity fea- 
tures. Second, we will also examine the role of the 
nature of the modifying situation. Third, we will 
incorporate other lexical information present in the 
sentence, including adverbial cue words (e.g. now, 
already and specific dates that have time-related in- 
formation, and distinguishing reported speech from 
other sentences. Finally, as mentioned, these re- 
sults do not take embedded verbs or verbs in adjunct 
clauses into account. Many adjunct and embedded 
clauses are tenseless, making evaluation more diffi- 
cult. For example, is The President believed China 
to be a threat equivalent to The president believed 
China is a threat). 
References 
Chauncey C. Chu. 1998. A Discourse Grammar of 
Mandarin Chinese. Peter Lang Publishing, Inc., 
New York, NY. 
Bernard Comrie. 1976. Aspect. Cambridge Univer- 
sity Press, Cambridge, MA. 
Bonnie J. Dorr and Terry Gaasterland. 1995. Se- 
lecting Tense, Aspect, and Connecting Words in 
Language Generation. In Proceedings of IJCAI- 
95, Montreal, Canada. 
Bonnie J. Dorr and Marl Broman Olsen. 1997a. As- 
pectual Modifications to a LCS Database for NLP 
Applications. Technical Report LAMP TR 007, 
UMIACS TR 97-23, CS TR 3763, University of 
Maryland, College Park, MD. 
Bonnie J. Dorr and Marl Broman Olsen. 1997b. 
Deriving Verbal and Compositional Lexical As- 
pect for NLP Applications. In Proceedings of the 
35th Annual Meeting of the Association for Com- 
putational Linguistics (ACL-97), pages 151-158, 
Madrid, SPain , July 7-12. 
Bonnie J. Doff, James Hendler, Scott Blanksteen, 
and Barrie Migdaloff. 1993. Use of Lexical Con- 
ceptual Structure for Intelligent Tutoring. Tech- 
nical Report UMIACS TR 93-108, CS TR 3161, 
University of Maryland. 
Bonnie J. Dorr, Nizar Habash, and David Traum. 
1998. A Thematic HieJfarchy for Efficient Gener- 
ation from Lexical-ConceptM Structure. In Pro- 
ceedings of the Third Conference of the Associ- 
ation for Machine Translation in the Americas, 
AMTA-98, in Lecture Notes in Artificial Intelli- 
gence, 15~9, pages 333-343, Langhorne, PA, Oc- 
tober 28-31. 
Bonnie J. Dorr. 1993. Machine Translation: A View 
from the Lexicon. The MIT Press, Cambridge, 
MA. 
4.0 
David Dowty. 1979. Word Meaning in Montague 
Grammar. Reidel, Dordrecht. 
David Dowty. 1986. The Effects of Aspectual Class 
on the Temporal Structure of Discourse: Seman- 
tics or Pragmatics? Linguistics and Philosophy, 
9:37-61. 
Mohamed Guerssel, Kenneth Hale, Mary Laugh- 
ten, Beth Levin, and Josie White Eagle. 1985. 
A Cross-linguistic Study of Transitivity Alterna- 
tions. In W. H. Eilfort, P. D. Kroeber, and K. L. 
Peterson, editors, Papers from the Parasession on 
Causatives and Agentivity at the Twenty.First Re- 
gional Meeting, CLS P1, Part P, pages 48-63. The 
Chicago Linguistic Society, Chicago, IL, April. 
Ray Jackendoff. 1983. Semantics and Cognition. 
The MIT P.r?ss, Cambridge, MA. 
Ray Jackendoff. 1990.. Semantic Structures. The 
MIT Press, Cambridge, MA. 
Irene Langkilde and Kevin Knight. 1998a. Gen- 
eratiort that Exploits Corpus-Based Statistical 
Knowledge. In Proceedings of COLING-ACL '98, 
pages 704-710. 
Irene Langkilde and Kevin Knight. 1998b. The 
Practical Value of N-Grams in Generation. In In- 
ternational Natural Language Generation Work- 
shop. 
Beth Levin and Malka Rappaport Hovav. 1995. Un- 
accusativity: At the Syntaz-Lexical Semantics In- 
terface. The MIT Press, Cambridge, MA. LI 
Monograph 26. 
Beth Levin. 1993. English Verb Classes and Alter- 
nations: A Preliminary Investigation. University 
of Chicago Press, Chicago, IL. 
Charles Li and Sandra Thompson. 1981. Mandarin 
Chinese: A functional reference grammar. Uni- 
versity of California Press, Berkeley, CA. 
Marc Moens and Mark Steedman. 1988. Tempo- 
ral Ontology and Temporal Reference. Compu- 
lational Linguistics: Special Issue on Tense and 
Aspect, 14(2):15-28. 
Mart Broman Olsen, Bonnie J. Dorr, and Scott C. 
Thomas. 1998. Enhancing Automatic Acquisi- 
tion of Thematic Structure in a Large-Scale Lex- 
icon for Mandarin Chinese. In Proceedings of the 
Third Conference of the Association for Machine 
Translation in the Americas, AMTA-98, in Lec- 
ture Notes in Artificial Intelligence, 1529, pages 
41-50, Langhorne, PA, October 28-31. 
Mart Broman Olsen. 1997. A Semantic and Prag- 
matic Model of Lexical and Grammatical Aspect. 
Garland, New York. 
Rebecca Passoneau. 1988. A Computational Model 
of the Semantics of Tense and Aspect. Compu- 
tational Linguistics: Special Issue on Tense and 
Aspect, 14(2):44-60. 
Malka P~appaport Hovav and Beth Levin. 1995. 
The Elasticity of Verb Meaning. In Processes in 
Argument Structure, pages 1-13, Germany. SfS- 
Report-06-95, Seminar fiir Sprachwissenschaft, 
Eberhard-Karls-Universit~t Tiibingen, Tiibingen. 
Carlota Smith. 1997. The parameter of aspect. 
Kluwer, Dordrecht, 2nd edition. 
41 
Generation from Lexical Conceptual Structures 
David Traum and Nizar Habash 
UMIACS, University of Maryland 
{traum, habash}@cs, umd. edu 
1 Introduction 
This paper describes a system for generating natural 
language sentences from an interlingual representa- 
tion, Lexical Conceptual Structure (LCS). This sys- 
tem has been developed as part of a Chinese-English 
Machine Translation system, however, it promises 
to be useful for many other MT language pairs. 
The generation system has also been used in Cross- 
Language information retrieval research (Levow et 
al., 2000). 
One of the big challenges in Natural Language 
processing efforts is to be able to make use of ex- 
isting resources, a big difficulty being the sometimes 
large differences in syntax, semantics, and ontolo- 
gies of such resources. A case in point is the in- 
terlingua representations u ed for machine transla- 
tion and cross-language processing. Such represen- 
tations are becoming fairly popular, yet there are 
widely different views about what these languages 
should be composed of, varying from purely concep- 
tual knowledge-representations, having little to do 
with the structure of language, to very syntactic rep- 
resentations, maintaining most of the idiosyncrasies 
of the source languages. In our generation system we 
make use of resources associated with two different 
(kinds of) interlingua structures: Lexical Conceptual 
Structure (LCS), and the Abstract Meaning Repre- 
sentations used at USC/ISI (Langkilde and Knight, 
1998a). 
2 Lexical Conceptual Structure 
Lexical Conceptual Structure is a compositional 
abstraction with language-independent properties 
that transcend structural idiosyncrasies (Jackendoff, 
1983; Jackendoff, 1990; Jackendoff, 1996). This rep- 
resentation has been used as the interlingua of sev- 
eral projects uch as UNITRAN (Dorr et al, 1993) 
and MILT (Dorr, 1997). 
An LCS is adirected graph with a root. Each node 
is associated with certain information, including a 
type, a primitive and a field. The type of an LCS 
node is one of Event, State, Path, Manner, Property 
or Thing, loosely correlated with verbs prepositions, 
adverbs, adjectives and nouns. Within each of these 
types, there are a number of conceptual primitives 
of that type, which are the basic building blocks of 
LCS structures. There are two general classes of 
primitives: closed class or structural primitive (e.g., 
CAUSE, GO, BE, TO) and CONSTANTS, correspond- 
ing to the primitives for open lexical classes (e.g., 
reduce+ed, textile+, slash+ingly). I. Exam- 
ples of fields include Locational, Possessional, 
Identificational. Children are also designated 
as to whether they are subject, argument, or 
modifier position. 
An LCS captures the semantics of a lexical item 
through a combination of semantic structure (spec- 
ified by the shape of the graph and its structural 
primitives and fields) and semantic ontent (speci- 
fied through constants). The semantic structure of 
a verb is the same for all members of a verb class 
(Levin and Rappaport Hovav, 1995) whereas the 
content is specific to the verb itself. So, all the verbs 
in the "Cut Verbs - Change of State" class have the 
same semantic structure but vary in their semantic 
content (for example, chip, cut, saw, scrape, slash 
and scratch). 
The lexicon entry or Root LCS (RLCS) of one 
sense of the Chinese verb xuel_jian3 is as follows: 
(1) 
(act_on loc 
(* thing 1) 
(* thing 2) 
((* \[on\] 23) loc (*head*) (thing 24)) 
(cut+ingly 26) 
(down+/m)) 
The top node in the. RLCS has the structural 
primitive ACT_ON in the locational field. Its sub- 
ject is a star-marked LCS, meaning a subordinate 
RLCS needs to be filled in here to form a complete 
event. It also has the restriction that the filler LCS 
be of the type thing. The number "1" in that node 
specifies the thematic role: in this case, agent. The 
second child node, in argument position, needs to 
t Suffixes uch as ?, ?ed, +ingly are markers of the open 
class of primitives, indicating the type 
52 
be of type thing too. The number "2" stands for 
theme. The last two children specify the manner of 
the locat iona l  act_on, that is "cutting in a down- 
ward manner". The RLCS for nouns are generally 
much simpler since they usually include only one 
root node with a primitive? For instance (US+) or 
(quota+). 
The meaning of complex phrases is represented as 
a composed LCS (CLCS). This is constructed "com- 
posed" from several RLCSes corresponding to in- 
dividual words. In the composition process, which 
starts with a parse tree of the input sentence, all 
the obligatory positions in the root and subordinate 
RLCS corresponding to lexical items are filled with 
other RLCSes from appropriately placed items in the 
parse tree. For example, the three RLCSes we have 
seen already can compose to give the CLCS in (2), 
corresponding~o the English sentence: United states 
cut down (the) quota. 
(2) 
(act_on loc 
(us+) 
(quota+) 
((* \[on\] 23) loc (*head*) (thing 24)) 
(cut+ingly 26) 
(dowa+/m)) 
CLCS structures can be composed of different 
sorts of RLCS structures, corresponding to differ- 
ent words. A CLCS can also be decomposed on the 
generation side in different ways depending on the 
RLCSes of the lexical items in the target language. 
For example, the CLCS above will match a single 
verb and two arguments when generated in Chinese 
(regardless ofthe input language). But it will match 
four lexical items in English: cut, US, quota, and 
down, since the RLCS for the verb "cut" in the En- 
glish lexicon, as shown in (3), does not include the 
modifier down. 
(3) 
(act_on ioc 
(* thing 1) 
(* thing 2) 
((* \[on\] 23) loc (*head*) (thing 24)) 
(cut+ingly 26) ) 
The rest of the examples in this paper will refer 
to the slightly more complex CLCS shown in (4), 
corresponding to the English sentence The United 
States unilaterally reduced the China textile export 
quota This LCS is presented without all the addi- 
tional features for sake of clarity. Also, it is actually 
one of eight possible LCS compositions produced by 
the analysis component from the input Chinese sen- 
tence. 
(4) 
(cause (us+) 
(go ident (quota+ (china+) 
(textile+) 
(export+)) 
(to ident (quota+ (china+) 
(textile+) 
(export+)) 
(at ?dent (quota+ (china+) 
(textile+) 
(export+)) 
(reduce+ed))))  
(with instr (*HEAD*) nil) 
(unilaterally+/m)) 
3 The Generation System 
Since this generation system was developed in tan- 
dem with the most recent LCS composition system, 
and LCS-language and specific lexicon extensions, 
a premium was put on the ability for experimenta- 
tion along a number of parameters and rapid ad- 
justment on the basis of intermediate inputs and re- 
sults to the generation system. This goal encour- 
aged a modular design, and made lisp a convenient 
language for implementation. We were also able to 
successfully integrate components from the Nitrogen 
Generation System (Langkilde and Knight, 1998a; 
Langkilde and Knight, 1998b). 
CLCS 
I ~.Processing 
! 
Lexical Access 
" Alignmen~ 
Decomposition 
 sJ^M, I Creation 
Lexical Choice 
LCS-Amr 
1 
, 
,J 
/ 
Lineatizafion 
and Morphology 
l 
NITROGEN big'turn Iwel'em~nc~ 
Realization 
I English 
f 
Figure 1: Generation System Architecture 
The architecture of the generation system is 
shown in Figure 1, showing the main modules and 
sub-modules and flow of information between them. 
The first main component translates, with the use of 
a language specific lexicon, from the LCS interlingua 
to a language-specific representation of the sentence 
in a modified form of the AMR-interlingua, using 
words and features specific to the target language, 
but also including syntactic and semantic informa- 
tion from the LCS representation. The second main 
component produces target language sentences from 
53 
this intermediate representation. We will now de- 
scribe each of these components in more detail. 
The input to the generation component is a text- 
representation f a CLCS, the Lexical Conceptual 
Structure corresponding to a natural language sen- 
tence. The particular format, known as long-hand 
is equivalent o the form shown in (4), but mak- 
ing certain information more explicit and regular 
(at the price of increased verbosity). The Long- 
hand CLCS can either be a fully language-neutral 
interlingua representation, or one which still incor- 
porates some aspects of the source-language inter- 
pretation process. This latter may include grammat- 
ical features on LCS nodes, but also nodes, known as 
functional nodes, which correspond to words in the 
source language but are not LCS-nodes themselves, 
serving merely as place-holders for feature informa- 
tion. Examples of these nodes include punctuation 
markers, coordinating conjunctions, grammatical s- 
pec t markers, and determiners. An additional exten- 
sion of the LCS input language, beyond traditional 
LCS is the in-place representation f an ambiguous 
sub-tree as a POSSIBLES node, which has the various 
possibilities represented as its own children. 
Thus, for example, the following structure (with 
some aspects elided for brevity) represents a node 
that could be one of three possibilities. In the second 
one, the root of the sub-tree is a functional node, 
passing its features to its child, COUNTRY+: 
(5) 
(:POSSIBLES -2589104 
(MIDDLE+ (COUNTRY+ (DEVELOPING+/P))) 
(FUNCTIONAL (PDSTPOSITION AMONG) 
(COUNTRY+ (DEVELOPING+/P))) 
(CHINA+ (COUNTRY+ (DEVELOPING+/P))) 
) 
3.1 Lexical Choice 
The first major component, divided into four 
pipelined sub-modules, as shown in Figure 1 trans- 
forms a CLCS structure to what we call an LCS- 
AMR structure, using the syntax of the abstract 
meaning representation (AMR), used in the Nitro- 
gen generation system, but with words already cho- 
sen (rather than more abstract Sensus ontology con- 
cepts), and also augmented with information from 
the LCS that is useful for target language realiza- 
tion. 
3.1.1 Pre-Processing 
The pre-processing phase converts the text input for- 
mat into internal graph representations, for efficient 
access of components (with links for parents as well 
as children), also doing away with extraneous source- 
language features, converting, for example, (5) to re- 
move the functional node and promote COUNTRY+ to 
be one of the possible sub-trees. This involves a top- 
down ,reversal of the tree, including some complex- 
ities when functional nodes without children (which 
then assign features to their parents) are direct chil- 
dren of possibles nodes. 
3.1.2 Lexical Access 
The lexical access phase compares the internal CLCS 
form to the target language lexicon, decorating the 
CLCS tree with the RLCSes of target language 
words which are likely to match sub-structures of
the CLCS. In an off-line processing phase, the tar- 
get language lexicon is stored in a hash-table, with 
each entry keyed on a designated primitive which 
would be a most distinguishing node in the RLCS. 
On-line decoration then proceeds in two step pro- 
cess, for each node in the CLCS: 
(6) a. look for RLCSes stored in the lexicon under 
the CLCS node's primitives 
b. store retrieved RLCSes at the node in the 
CLCS that matches the root of this RLCS 
Figure 2 shows some of the English entries match- 
ing the CLCS in (4). For most of these words, the 
designated primitive is the only node in the corre- 
sponding LCS for that entry. For reduce, however, 
reduce+ed is the designated primitive. While this 
will be retrieved in step (6) while examining the 
reduce+ed node from (4), in (6)b, the LCS for "re- 
duce" will be stored at the root node of (4) (cause). 
( : DEF_WORD "reduce" 
: CLASS "45.4. a" 
:THETA_ROLES ( (I "_ag_th, instr (with)") ) 
:LCS (cause (* thing I) 
(go ident (* Zhing 2) 
(toward ident (thing 2) 
(at ident (thing 2) 
(reduce+ed 9) ) ) ) 
((* with 19) instr (*head*) 
(thin E 20) )) 
:VAR_SPEC ((1 (animate +)))) 
(:DEF_WORD ."US" :LCS (US+ 0)) 
(:DEF_WORD "China" :LCS (China+ 0)) 
(:DEF_WORD "quota" :5CS (quota+ 0)) 
(:DEF_WORD "WITH" 
:LCS (with instr (thing 2) (* thing 20))) 
(: DEF_WORD "unilaterally" 
:LCS (unilaterally+/m 0)) 
Figure 2: Lexicon entries 
B4 
The current English lexicon contains over 11000 
RLCS entries such as those in Figure 2, including 
over 4000 verbs and 6200 unique primitive keys in 
the hash-table. 
3.1.3 A l ignment /Decompos i t ion  
The heart of the lexical access algorithm is the de- 
composition process. This algorithm attempts to 
align RLCSes selected by the lexical access portion 
with parts of the CLCS, to find a complete cover- 
ing of the CLCS graph. The main algorithm is very 
similar to that described in (Dorr, 1993), however 
with some extensions to be able to also deal with 
the in-place ambiguity represented by the possibles 
nodes. 
The algorithm recursively checks a CLCS node 
against corresponding RLCS nodes coming from the 
lexical entries-retrieved and stored in the previous 
phase. If significant incompatibilities are found, the 
lexical entry is discarded. If all (obligatory) nodes 
in the RLCS match against nodes in the CLCS, 
then the rest of the CLCS is recursively checked 
against other lexical entries stored at the remain- 
ing unmatched CLCS nodes. Some nodes, indicated 
with a "*", as in Figure 2, require not just a match 
against the corresponding CLCS node, but also a 
match against another lexical entry. Some CLCS 
nodes must thus match multiple RLCS nodes. A 
CLCS node matches an RLCS node, if the following 
conditions hold: 
(7) a. 
b. 
C. 
d. 
e.  
the primitives are the same (or primitive for 
one is a wild-card, represented as nil) 
the types (e.g., thing, event, state, etc.) are 
the same 
the fields (e.g., identificational, possessive, 
locational, etc) are the same 
the positions (e.g., subject, argument, or 
modifier) are the same 
all obligatory children of the RLCS node 
have corresponding matches to children of 
the CLCS 
Subject and argument children of an RLCS node 
are obligatory unless specified as optional, whereas 
modifiers are optional unless specified as obliga- 
tory. In the RLCS for " reduce"  in Figure 2, 
the nodes corresponding to agent and theme (num- 
bered 1 and 2, respectively) are obligatory, while 
the instrument (the node numbered 19) is optional. 
Thus, even though in (4) there is no matching lexical 
entry for the node in Figure 2 numbered 20 ("*"- 
marked in the RLCS for "with"), the main RLCS 
for ' ' reduce'   is allowed to match, though with- 
out any realization for the instrument. 
A complexity in the algorithm occurs when there 
are multiple possibilities filling in a position in a 
CLCS. in this case, only one of these possibilities 
is requirea to match all the corresponding RLCS 
nodes in order for a lexical entry to match. In the 
case where there are some of these possibilities that 
do not match any RLCS nodes (meaning there are 
no target-language realizations for these constructs), 
these possibilities can be pruned at this stage. On 
the other hand, ambiguity can also be introduced at 
the decomposition stage, if multiple lexical entries 
can match a single structure 
The result of the decomposition process is a 
match-structure indicating the hierarchical relation- 
ship between all lexical entries, which, together cover 
the input CLCS. 
3.1.4 LCS-AMR Creat ion 
The match structure resulting from decomposition 
is then converted into the appropriate input format 
used by the Nitrogen generation system. Nitrogen's 
input, Abstract Meaning Representation (AMR), is 
a labeled directed graph written using the syntax 
for the PENMAN Sentence Plan Language (Penman 
1989). the structure of an AMR is basically as in (8). 
(8) AMR = <concept> I (<label> {<role> 
<AMR>}+) 
Since the roles expected by Nitrogen's English 
generation grammar do not match well with the the- 
matic roles and features of a CLCS, we have ex- 
tended the AMR language with LCS-specific rela- 
tions, calling the result, an LCS-AMR. To distin- 
guish the LCS relations from those used by Nitro- 
gen, we mark most of the new roles with the prefix 
: LCS-. Figure 3 shows the LCS-AMR corresponding 
to the CLCS in (4). 
In the above example, the basic role / is used 
to specify an instance. So, the LCS-AMR can be 
read as an instance of the concept Ireduce I whose 
category is a verb and is in the active voice. More- 
over, Ireducel has two thematic roles related to it, an 
agent and a theme; and it is modified by the concept 
lunilaterally\]. The different roles modifying Ireduce I 
come from different origins. The :LCS-NODE value 
comes directly from the unique node number in the 
input CLCS. The category, voice and telicity are de- 
rived from features of the LCS entry for the verb 
Ireduce\] in the English lexicon. The specifications 
of agent and theme come from the LCS represen- 
tation of the verb reduce in the English lexicon as 
well, as can be seen by the node numbers 1 and 2, in 
the lexicon entry in Figure 2. The role :LCS-MOD- 
MANNER is derived by combining the fact that the 
corresponding AMR had a modifier ole in the CLCS 
and because its type is a Manner. 
3.2 Real ization 
The LCS-AMR representation is then passed to the 
realization module. The strategy used by Nitrogen is 
55 
(a7537 / lreducel 
:LCS-NODE 6253520 
:LCS-V01CE ACTIVE 
:CAT V 
:TELIC + 
:LCS-AG (a7538 / \[United States\[ 
:LCS-NODE 6278216 
:CAT N) 
:LCS-TH (a7539 / ~quota\[ 
:LCS-NODE 6278804 
:CAT N 
:LCS-MOD-THING (a7540 / \[china\[ 
:LCS-NODE 6108872 
:CAT N) 
:LCS-MOD-THING (a7541 / \[textile\[ 
:LCS-NODE 6111224 
-- :CAT N) 
:LCS-MOD-THING (a7542 / \[exportl 
:LCS-NODE 6112400 
:CAT N)) 
:LCS-MOD-MANNER (a7543 / \[unilaterally\[ 
:LCS-NODE 6279392 
:CAT ADV)) 
Figure 3: LCS-AMR 
to over-generate possible sequences of English from 
the ambiguous or under-specified AMRs and then 
decide amongst hem based on bigram frequency. 
The interface between the Linearization module and 
the Statistical Extraction module is a word lattice 
of possible renderings. The Nitrogen package of- 
fers support for both subtasks, Linearization and 
Statistical Extraction. Initially, we used the Nitro- 
gen grammar to do Linearization. But complexities 
in recasting the LCS-AMR roles as standard AMR 
roles as well as efficiency considerations compelled 
us to create our own English grammar implemented 
in Lisp to generate the word lattices. 
3.2.1 Linearizatlon 
In this module, we force linear order on the un- 
ordered parts of an LCS-AMR. This is done by 
recursively calling subroutines that create various 
phrase types (NP, PP, etc.) from aspects of the LCS- 
AMR. The result of the linearization phase is a word 
lattice specifying the sequence of words that make 
up the resulting sentence and the points of ambigu- 
ity where different generation paths are taken. (9) 
shows the word lattice corresponding to the LCS- 
AMR in (8). 
(9) (SEQ (WRD "*start-sentence*" BOS) (WRD 
"united states" NOUN) (WRD "unilaterally" 
ADJ) (WRD "reduced" VERB) (OR (WRD 
"the" ART) (WRD "a" ART) (WRD "an" 
ART)) (WRD "china" ADJ) (OR (SEQ (WRD 
"export" ADJ) (WRD "textile" ADJ)) (SEQ 
(WRD "textile" ADJ) (WRD "export" ADJ))) 
(WRD "quota" NOUN) (WRD "." PUNC) 
(WRD "*end-sentence*" EOS)) 
The keyword SEQ specifies that what follows it is 
a list of words in their correct linear order. The key- 
word OR specifies the existence of different paths for 
generation. In the above example, the word 'quota' 
gets all possible determiners since its definiteness is 
not specified. Also, the relative order of the words 
'textile' and 'export' is not resolved so both possi- 
bilities are generated. 
Sentences were realized according to the pattern 
in (10). That is, first subordinating conjunctions, 
if any, then modifiers in the temporal field (e.g., 
"now", "in 1978"), then the first thematic role, then 
most other modifiers, the verb (with collocations if
any) then spatial modifiers ("up", "down"), then the 
second and third thematic roles, followed by prepo- 
sitional phrases and relative sentences. Nitrogen's 
morphology component was also used, e.g., to give 
tense to the head verb. In the example above, since 
there was no tense specified in the input LCS, past 
tense was used on the basis of the telicity of the verb. 
(10) (Sconj ,) (temp-mod)* Whl (Mods)* V (coll) 
(stood)* (Th2)+ (Th3)+ (PP)* (RelS)* 
There is no one-to-one mapping between a partic- 
ular thematic role and an argument position. For 
example, a theme can be the subject in some cases 
and it can be the object in others or even an oblique. 
Observe "cookie" in i l l) .  
i l l )  a. John ate a cookie (object) 
b. the cookie contains chocolate (subject) 
c. she nibbled at a cookie (oblique) 
Thematic roles are numbered for their correct re- 
alization order, according to the hierarchy for argu- 
ments hown in (12). 
(12) agent > instrument > theme > perceived > 
( everythin gel se ) 
So, in the case of the occurrence of theme alone, 
it is mapped to first argument position. If a theme 
and an agent occur, the agent is mapped to first ar- 
gument position and the theme is mapped to second 
argument position. A more detailed discussion is 
available in (Doff et al, 1998). For the LCS-AMR in 
Figure 3, the thematic hierarchy is what determined 
that the lunited statesl is the subject and Iquotal is 
the object of the verb Ireducel. 
In our input CLCSs, in most cases little hierarchi- 
cal information was given about multiple modifiers 
of a noun. Our initial, brute force, solution was to 
56 
generate all permutations and depend on statisti- 
cal extraction to decide. This technique Worked for 
noun phrases of about 6 words, but was too costly 
for larger phrases (of which there were several ex- 
amples in our test corpus). This cost was alleviated 
to some degree, also providing slightly better esults 
than pure bigram selection by labelling adjectives in 
the English lexicon as belonging to one of several 
ordered classes, inspired by the adjective ordering 
scheme in (Quirk et al, 1985). This is shown in 
(13). 
(13) a. Determiner (all, few, several, some, etc.) 
b. Most Adjectival (important, practical, eco- 
nomic, etc.) 
c. Age (old, young, etc.) 
d. Color (black, red, etc.) 
e. Participle (confusing, adjusted, convincing, 
decided) 
f. Provenance (China, southern, etc.) 
g. Noun (Bank_of_China, difference, memoran- 
dum, etc.) 
h. Denominal (nouns made into adjectives by 
adding-al, e.g., individual, coastal, annual, 
etc.) 
If multiple words fall within the same group, per- 
mutations are generated for them. This situation 
can be seen for the LCA-AMR in Figure 3 with the 
ordering of the modifiers of the word I quota\]: I chinal, 
lexportl and Itextilel. Ichinal fell within the Prove- 
nance class of modifiers which gives it precedenc e 
over the other two words. They, on the other hand, 
fell in the Noun class and therefore both permuta- 
tions were passed on to the statistical component. 
3.2.2 Statistical Preferences 
The final step, extracting a preferred sentence from 
the word lattice of possibilities is done using Ni- 
trogen's Statistical Extractor without any changes. 
Sentences are scored using uni and bigram frequen- 
cies calculated based on two years of Wall Street 
Journal (Langkilde and Knight, 1998b). 
4 Dealing with Ambiguity 
A major issue in sentence generation from an inter- 
lingua or conceptual structure, especially as part of a 
machine translation project, is how and when to deal 
with ambiguity. There are several different sources 
of ambiguity in the generation process outlined in 
the previous ection. Some of these include: 
? ambiguity in source language analysis (as repre- 
sented by possibles nodes in the CLCS input to 
the Generation system). This can include am- 
biguity between multiple concepts, such as the 
example in (5), LCS type/structure ( .g., thing 
or event, which field), or structural ambiguity 
(subject, argument or modifier). 
ambiguity introduced in lexical choice (when 
multiple match structures can cover a single 
CLCS) 
ambiguity introduced in realization (when mul- 
tiple orderings are possible, also multiple mor- 
phological realizations) 
There are also several types of strategies for ad- 
dressing ambiguity at various phases, including: 
? passing all possible structures down for further 
processing stages to deal with 
? filtering based on "soft" preferences (only pass 
the highest set of candidates, according to some 
metric) 
? quota-based filtering, passing only the top N 
candidates 
? threshold filtering, passing only candidates that 
exceed a fixed threshold (either score or binary 
test) 
The generation system uses a combination ofthese 
strategies, at different phases in the processing. Am- 
biguous CLCS sub-trees are sometimes annotated 
with scores based on preference ofattachment asan 
argument rather than a modifier. The alignment al- 
gorithm can be run in either of two modes, one which 
selects only the top scoring possibility for which a 
matching structure can be found, and one in which 
all possible structures are passed on, regardless of 
score. The former method is the only one feasible 
when given very large (e.g., over 1 megabyte text 
files) CLCS inputs. Also at the decomposition level, 
soft preferences are used in that missing lexical en- 
tries can be hypothesized tocover parts of the CLCS 
(essentially "making up" words in the target lan- 
guage). This is done, however, only when no le- 
gitimate matches are found using only the available 
lexical entries. At the linearization phase, there are 
often many choices for ordering of modifiers at the 
same level. As mentioned in the previous ection, 
we are experimenting with separating these into po- 
sitional classes, but our last resort is to pass along 
all permutations of elements in each sub-class. The 
ultimate arbiter is the statistical extractor, which 
orders and presents the top scoring realizations. 
5 In ter l ingua l  representat ion  i ssues  
One issue that needs to be confronted in an Inter- 
lingua such as LCS is what to do when linguistic 
structure of languages vary widely, and useful con- 
ceptual structure may also diverge from these. A 
57 
case in point is the representation f numbers. Lan- 
guages diverge widely as to which numbers are prim- 
itive terms, and how larger numbers are built com- 
positionaUy through modification (e.g., multiplica- 
tion and addition). One question that immediately 
comes up is whether an interlingua such as LCS 
should represent numbers according to the linguis- 
tic structure of the source language (or some partic- 
ular designated natural anguage) or as some other 
internal numerical form, (e.g. decimal numerals). 
Likewise, on generation i to a target language, how 
much of the structure of the source language should 
be kept, especially when this is not the most nat- 
ural way to group things in the target language. 
One might be tempted to always convert o a stan- 
dard interlingua representation f numbers, however 
this does los_e some possible classification i to groups 
that might be present in the input (contrast in En- 
glish: "12 pair" with "2 dozen". 
In our Chinese-English efforts, such issues came 
up, since the natural multiplication points in Chi- 
nese were 100, 10,000, and 100,000,000, rather than 
100, 1000, and 1,000,000, as in English. Our provi- 
sional solution is to propogate the source language 
modification structure all the way through the LCS- 
AMR stage, and include special purpose rules look- 
ing for the "Chinese" numbers and multiplying them 
together to get numerals, and then divide and real- 
ize in the English fashion. E.g., using the words 
thousand, million, and billion. 
6 Evaluation 
So far most of the evaluation has been fairly small- 
scale and fairly subjective, generating English sen- 
tences from CLCSs produced from about 80 sen- 
tences. Evaluation in this case is difficult, because 
the ultimate criteria is translation quality, which 
can, itself, be difficult to judge, but, moreover, it
can be hard to attribute specific deficits to the anal- 
ysis phase, the lexical resources, or the generation 
system proper. So far results have been mostly ad- 
equate, even for large and fairly complex sentences, 
taking less than 1 minute for generation up to inputs 
of about 1 megabyte input CLCS files. Ambiguity 
and complexity beyond that level tends to overtax 
the generation system. 
For the most part, the over-generation strategy of 
Nitrogen, coupled with the bigram preferences works 
very well. There are still some difficulties, however. 
One major one is that, especially with its bias for 
shorter sentences, fluency is given preference over 
translation fidelity. Thus, if there are options of 
whether or not to express ome optional informa- 
tion, this will tend to be left out. Also, bigrams are 
obviously inadequate for capturing long-distance d - 
pendencies, and so, if things like agreement are not 
carefully controlled in the symbolic omponent, they 
will be incorrect in some cases. 
The generation component has also been used on 
a broader scale, generating thousands of simple sen- 
tences - at least one for each verb in the English 
LCS lexicon, creating sentence templates to be used 
in a Cross-Language information retrieval system 
(Levow et al, 2000). 
7 Future Work 
The biggest remaining step is a more careful evalu- 
ation of different sub-systems and preference strate- 
gies to more efficiently process very ambiguous and 
complex inputs, without substantially sacrificing 
translation quality. Also a current research topic 
is how to combine other metrics coming from vari- 
ous points in the generation process with the bigram 
statistics, to result in better overall outputs. 
Another topic of interest is developing other lan- 
guage outputs. Most of the subcomponents are 
language-independent. The realization components 
being an obvious exception. In particular, the 
pre-processing algorithm is completely language- 
independent. The lexical access algorithm is lan- 
guage independent, although it requires a target- 
language lexicon, which of course is language de- 
pendent. The alignment algorithm is also com- 
pletely language independent. The lcs-amr creation 
language is mostly language independent, however 
there may not be sufficient features added to the 
language and extracted from the LCS-AMR for full 
generation of some other languages. Some target 
languages might require some extensions to the out- 
put language and new rules to extract his informa- 
tion from the LCS. The realization process is mostly 
language dependent. The current linearizaton mod- 
ule is very dependent on the structure of English. 
We are, however working on a future version of this 
component splitting the linearization task into lan- 
guage independent processes and grammar compil- 
ers, and independent language-specific output gram- 
mars. Nitrogen's realizer, also, is algorithmically 
language-independent, however one would need a 
target language database for realization in another 
language. 
Acknowledgements 
This work was supported by the US Department of 
Defense through contract MDA904-96-I:t-0738. The 
Nitrogen system used in the realization process was 
provided by USC/ISI, we would like to thank Keven 
Knight and Irene Langkilde for help and advice in 
using it. The adjective classifications described in 
Section 3 were devised by Carol Van Ess-Dykema. 
David Clark and Noah Smith worked on previous 
versions of the system, and we are indebted to Some 
of their ideas for the current implementation. We 
would also like to thank the CLIP group at Uni- 
58 
veristy of Maryland, especially Ron Dolanl ~ Bonnie 
Dorr, Gina Levow, Mari Olsen, Wade sti~fi, Amy 
Weinberg, for helpful input and feedback on the gen- 
eration system. 
References  
Bonnie J. Dorr, James Hendler, Scott Blanksteen, 
and Barrie Migdaloff. 1993. Use of Lexical Con- 
ceptual Structure for Intelligent Tutoring. Tech- 
nical Report UMIACS TR 93-108, CS TR 3161, 
University of Maryland. 
Bonnie J. Dorr, Nizar Habash, and David Traum. 
1998. A Thematic Hierarchy for Efficient Gener- 
ation from Lexical-Conceptal Structure. In Pro- 
ceedings of the Third Conference of the Associ- 
atio.n for Machine Translation in the Americas, 
AMTA-#8, in Lecture Notes in Artificial Intelli- 
gence, 15~9, pages 333-343, Langhorne, PA, Oc- 
tober 28-31. 
Bonnie J. Dorr. 1993. Machine Translation: A View 
from the Lexicon. The MIT Press. 
Bonnie J. Dorr. 1997. Large-Scale Acquisition of 
LCS-Based Lexicons for Foreign Language Tutor- 
ing. In Proceedings of the A CL Fifth Conference 
on Applied Natural Language Processing (ANLP), 
pages 139-146, Washington, DC. 
Ray Jackendoff. 1983. Semantics and Cognition. 
The MIT Press, Cambridge, MA. 
Ray Jackendoff. 1990. Semantic Structures. The 
MIT Press, Cambridge, MA. 
Ray Jackendoff. 1996. The Proper Treatment of 
Measuring Out, Telicity, and Perhaps Even Quan- 
tification in English. Natural Language and Lin- 
guistic Theory, 14:305-354. 
Irene Langkilde and Kevin Knight. 1998a. Gen- 
eration that Exploits Corpus-Based Statistical 
Knowledge. In Proceedings of COLING-A CL '98, 
pages 704-710. 
Irene Langkilde and Kevin Knight. 1998b. The 
Practical Value of N-Grams in Generation. In In- 
ternational Natural Language Generation Work- 
shop. 
Beth Levin and Malka Rappaport Hovav. 1995. Un- 
aecusativity: At the Syntaz-Lezical Semantics In- 
terface. The MIT Press, Cambridge, MA. LI 
Monograph 26. 
Gina Levow, Bonnie J. Dorr, and Dekang Lin. 2000. 
Construction of chinese-english emantic hierar- 
chy for cross-language retrieval, forthcoming. 
Randolph Quirk, Sidney Greenbaum, Geoffrey 
Leech, and Jan Svartvik. 1985. A Comprehen- 
sive Grammar of the English Language. Longman, 
London. 
59 
Proceedings of the NAACL HLT 2010: Demonstration Session, pages 33?36,
Los Angeles, California, June 2010. c?2010 Association for Computational Linguistics
Interpretation of Partial Utterances in Virtual Human Dialogue Systems
Kenji Sagae and David DeVault and David R. Traum
Institute for Creative Technologies
University of Southern California
Marina del Rey, CA 90292, USA
{sagae,devault,traum}@ict.usc.edu
Abstract
Dialogue systems typically follow a rigid pace
of interaction where the system waits until the
user has finished speaking before producing
a response. Interpreting user utterances be-
fore they are completed allows a system to
display more sophisticated conversational be-
havior, such as rapid turn-taking and appropri-
ate use of backchannels and interruptions. We
demonstrate a natural language understanding
approach for partial utterances, and its use in a
virtual human dialogue system that can often
complete a user?s utterances in real time.
1 Introduction
In a typical spoken dialogue system pipeline, the
results of automatic speech recognition (ASR) for
each user utterance are sent to modules that per-
form natural language understanding (NLU) and di-
alogue management only after the utterance is com-
plete. This results in a rigid and often unnatural pac-
ing where the system must wait until the user stops
speaking before trying to understand and react to
user input. To achieve more flexible turn-taking with
human users, for whom turn-taking and feedback at
the sub-utterance level is natural, the system needs
the ability to start interpretation of user utterances
before they are completed.
We demonstrate an implementation of techniques
we have developed for partial utterance understand-
ing in virtual human dialogue systems (Sagae et al,
2009; DeVault et al, 2009) with the goal of equip-
ping these systems with sophisticated conversational
behavior, such as interruptions and non-verbal feed-
back. Our demonstration highlights the understand-
ing of utterances before they are finished. It also
includes an utterance completion capability, where a
virtual human can make a strategic decision to dis-
play its understanding of an unfinished user utter-
ance by completing the utterance itself.
The work we demonstrate here is part of a grow-
ing research area in which new technical approaches
to incremental utterance processing are being de-
veloped (e.g. Schuler et al (2009), Kruijff et al
(2007)), new possible metrics for evaluating the per-
formance of incremental processing are being pro-
posed (e.g. Schlangen et al (2009)), and the ad-
vantages for dialogue system performance and us-
ability are starting to be empirically quantified (e.g.
Skantze and Schlangen (2009), Aist et al (2007)).
2 NLU for partial utterances
In previous work (Sagae et al, 2009), we presented
an approach for prediction of semantic content from
partial speech recognition hypotheses, looking at
length of the speech hypothesis as a general indi-
cator of semantic accuracy in understanding. In
subsequent work (DeVault et al, 2009), we incor-
porated additional features of real-time incremen-
tal interpretation to develop a more nuanced predic-
tion model that can accurately identify moments of
maximal understanding within individual spoken ut-
terances. This research was conducted in the con-
text of the SASO-EN virtual human dialogue sys-
tem (Traum et al, 2008), using a corpus of approxi-
mately 4,500 utterances from user sessions. The cor-
pus includes a recording of each original utterance, a
33
??
?
?
?
?
?
?
?
?
mood : declarative
sem :
?
?
?
?
?
?
?
?
type : event
agent : captain? kirk
event : deliver
theme : power ? generator
modal :
[
possibility : can
]
speech? act :
[
type : offer
]
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
Figure 1: AVM utterance representation.
manual transcription, and a gold-standard semantic
frame, allowing us to develop and evaluate a data-
driven NLU approach.
2.1 NLU in SASO-EN Virtual Humans
Our NLU module for the SASO-EN system,
mxNLU (Sagae et al, 2009), is based on maxi-
mum entropy classification (Berger et al, 1996) ,
where we treat entire individual semantic frames as
classes, and extract input features from ASR. The
NLU output representation is an attribute-value ma-
trix (AVM), where the attributes and values repre-
sent semantic information that is linked to a domain-
specific ontology and task model (Figure 1). The
AVMs are linearized, using a path-value notation, as
seen in the NLU input-output example below:
? Utterance (speech): we are prepared to give
you guys generators for electricity downtown
? ASR (NLU input): we up apparently give you
guys generators for a letter city don town
? Frame (NLU output):
<s>.mood declarative
<s>.sem.type event
<s>.sem.agent captain-kirk
<s>.sem.event deliver
<s>.sem.theme power-generator
<s>.sem.modal.possibility can
<s>.sem.speechact.type offer
When mxNLU is trained on complete ASR out-
put for approximately 3,500 utterances, and tested
on a separate set of 350 complete ASR utterances,
the F-score of attribute-value pairs produced by the
NLU is 0.76 (0.78 precision and 0.74 recall). These
figures reflect the use of ASR at run-time, and most
errors are caused by incorrect speech recognition.
2.2 NLU with partial ASR results (Sagae et al,
2009)
To interpret utterances before they are complete,
we use partial recognition hypotheses produced by
ASR every 200 milliseconds while the user is speak-
ing. To process these partial utterances produced by
ASR, we train length-specific models for mxNLU.
These models are trained using the partial ASR re-
sults we obtain by running ASR on the audio corre-
sponding to the utterances in the training data. The
NLU task is then to predict the meaning of the en-
tire utterance based only on a (noisy) prefix of the
utterance. On average, the accuracy of mxNLU on a
six-word prefix of an utterance (0.74 F-score) is al-
most as the same as the accuracy of mxNLU on en-
tire utterances. Approximately half of the utterances
in our corpus contain more than six words, creating
interesting opportunities for conversational behavior
that would be impossible under a model where each
utterance must be completed before it is interpreted.
2.3 Detecting points of maximal
understanding (DeVault et al, 2009)
Although length-specific NLU models produce ac-
curate results on average, more effective use of the
interpretation provided by these models might be
achieved if we could automatically gauge their per-
formance on individual utterances at run-time. To
that end, we have developed an approach (DeVault et
al., 2009) that aims to detect those strategic points in
time, as specific utterances are occurring, when the
system reaches maximal understanding of the utter-
ance, in the sense that its interpretation will not sig-
nificantly improve during the rest of the utterance.
Figure 2 illustrates the incremental output of
mxNLU as a user asks, elder do you agree to move
the clinic downtown? Our ASR processes captured
audio in 200ms chunks. The figure shows the par-
tial ASR results after the ASR has processed each
200ms of audio, along with the F-score achieved by
mxNLU on each of these partials. Note that the NLU
F-score fluctuates somewhat as the ASR revises its
incremental hypotheses about the user utterance, but
generally increases over time.
For the purpose of initiating an overlapping re-
sponse to a user utterance such as this one, the agent
needs to be able (in the right circumstances) to make
34
Utterance time (ms)
NL
U F
?sc
ore
(emp
ty)
(emp
ty)
 
all 
 
elde
r 
 
elde
r do
 you
 
 
elde
r to
 you
 d 
 
elde
r do
 you
 agr
ee 
 
elde
r do
 you
 agr
ee t
o 
 
elde
r do
 you
 agr
ee t
o m
ove
 the
 
 
elde
r do
 you
 agr
ee t
o m
ove
 the
 
 
elde
r do
 you
 agr
ee t
o m
ove
 the
 clin
ic to
 
 
elde
r do
 you
 agr
ee t
o m
ove
 the
 clin
ic d
own
 
 
elde
r do
 you
 agr
ee t
o m
ove
 the
 clin
ic d
own
tow
n 
 
elde
r do
 you
 agr
ee t
o m
ove
 the
 clin
ic d
own
tow
n 
200 400 600 800 100
0
120
0
140
0
160
0
180
0
200
0
220
0
240
0
260
0
280
0
0.0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
Partial ASR result
Figure 2: Incremental interpretation of a user utterance.
an assessment that it has already understood the ut-
terance ?well enough?, based on the partial ASR re-
sults that are currently available. We have imple-
mented a specific approach to this assessment which
views an utterance as understood ?well enough? if
the agent would not understand the utterance any
better than it currently does even if it were to wait
for the user to finish their utterance (and for the ASR
to finish interpreting the complete utterance).
Concretely, Figure 2 shows that after the entire
2800ms utterance has been processed by the ASR,
mxNLU achieves an F-score of 0.91. However, in
fact, mxNLU already achieves this maximal F-score
at the moment it interprets the partial ASR result el-
der do you agree to move the at 1800ms. The agent
therefore could, in principle, initiate an overlapping
response at 1800ms without sacrificing any accuracy
in its understanding of the user?s utterance.
Of course the agent does not automatically realize
that it has achieved a maximal F-score at 1800ms.
To enable the agent to make this assessment, we
have trained a classifier, which we call MAXF, that
can be invoked for any specific partial ASR result,
and which uses various features of the ASR result
and the current mxNLU output to estimate whether
the NLU F-score for the current partial ASR result
is at least as high as the mxNLU F-score would be if
the agent were to wait for the entire utterance.
To facilitate training of a MAXF classifier, we
identified a range of potentially useful features that
the agent could use at run-time to assess its confi-
dence in mxNLU?s output for a given partial ASR
result. These features include: the number of par-
tial results that have been received from the ASR;
the length (in words) of the current partial ASR
result; the entropy in the probability distribution
mxNLU assigns to alternative output frames (lower
entropy corresponds to a more focused distribution);
the probability mxNLU assigns to the most probable
output frame; and the most probable output frame.
Based on these features, we trained a decision tree
to make the binary prediction that MAXF is TRUE
or FALSE for each partial ASR result. DeVault et al
(2009) include a detailed evaluation and discussion
of the classifier. To briefly summarize our results,
the precision/recall/F-score of the trained MAXF
model are 0.88/0.52/0.65 respectively. The high pre-
cision means that 88% of the time that the model
predicts that F-score is maximized at a specific par-
tial, it really is. Our demonstration, which we out-
line in the next section, highlights the utility of a
high-precision MAXF classifier in making the deci-
sion whether to complete a user?s utterance.
3 Demo script outline
We have implemented the approach for partial utter-
ance understanding described above in the SASO-
EN system (Traum et al, 2008), a virtual human
dialogue system with speech input and output (Fig-
ure 3), allowing us to demonstrate both partial utter-
ance understanding and some of the specific behav-
iors made possible by this capability. We divide this
demonstration in two parts: visualization of NLU
for partial utterances and user utterance completion.
35
Figure 3: SASO-EN: Dr. Perez and Elder al-Hassan.
Partial ASR result Predicted completion
we can provide transportation to move the patient there
the market is not safe
there are supplies where we are going
Table 1: Examples of user utterance completions.
3.1 Visualization of NLU for partial utterances
Because the demonstration depends on usage of the
system within the domain for which it was designed,
the demo operator provides a brief description of the
system, task and domain. The demo operator (or
a volunteer user) then speaks normally to the sys-
tem, while a separate window visualizes the sys-
tem?s evolving understanding. This display is up-
dated every 200 milliseconds, allowing attendees to
see partial utterance understanding in action. For
ease of comprehension, the display will summarize
the NLU state using an English paraphrase of the
predicted meaning (rather than displaying the struc-
tured frame that is the actual output of NLU). The
display will also visualize the TRUE or FALSE state
of the MAXF classifier, highlighting the moment the
system thinks it reaches maximal understanding.
3.2 User utterance completion
The demo operator (or volunteer user) starts to speak
and pauses briefly in mid-utterance, at which point,
if possible, one of the virtual humans jumps in and
completes the utterance (DeVault et al, 2009). Ta-
ble 1 includes a few examples of the many utterances
that can be completed by the virtual humans.
4 Conclusion
Interpretation of partial utterances, combined with
a way to predict points of maximal understanding,
opens exciting possibilities for more natural conver-
sational behavior in virtual humans. This demon-
stration showcases the NLU approach and a sample
application of the basic techniques.
Acknowledgments
The work described here has been sponsored by the
U.S. Army Research, Development, and Engineer-
ing Command (RDECOM). Statements and opin-
ions expressed do not necessarily reflect the position
or the policy of the United States Government, and
no official endorsement should be inferred.
References
G. Aist, J. Allen, E. Campana, C. G. Gallo, S. Stoness,
M. Swift, and M. K. Tanenhaus. 2007. Incremental
dialogue system faster than and preferred to its non-
incremental counterpart. In Proc. of the 29th Annual
Conference of the Cognitive Science Society.
A. Berger, S. Della Pietra, and V. Della Pietra. 1996. A
maximum entropy approach to natural language pro-
cessing. Computational Linguistics, 22(1):39?71.
D. DeVault, K. Sagae, and D. Traum. 2009. Can I finish?
Learning when to respond to incremental interpreta-
tion results in interactive dialogue. In Proc. SIGDIAL.
G. J. Kruijff, P. Lison, T. Benjamin, H. Jacobsson, and
N. Hawes. 2007. Incremental, multi-level processing
for comprehending situated dialogue in human-robot
interaction. In Proc. LangRo?2007.
K. Sagae, G. Christian, D. DeVault, and D. R. Traum.
2009. Towards natural language understanding of par-
tial speech recognition results in dialogue systems. In
Short Paper Proceedings of NAACL HLT.
D. Schlangen, T. Baumann, and M. Atterer. 2009. In-
cremental reference resolution: The task, metrics for
evaluation, and a Bayesian filtering model that is sen-
sitive to disfluencies. In Proc. SIGDIAL, page 30?37.
W. Schuler, S. Wu, and L. Schwartz. 2009. A frame-
work for fast incremental interpretation during speech
decoding. Computational Linguistics, 35(3):313?343.
G. Skantze and D. Schlangen. 2009. Incremental dia-
logue processing in a micro-domain. In Proc. EACL.
D. Traum, S. Marsella, J. Gratch, J. Lee, and A. Hartholt.
2008. Multi-party, multi-issue, multi-strategy negoti-
ation for multi-modal virtual agents. In Proc. of the
Eighth International Conference on Intelligent Virtual
Agents.
36
Proceedings of the NAACL-HLT 2012: Demonstration Session, pages 25?28,
Montre?al, Canada, June 3-8, 2012. c?2012 Association for Computational Linguistics
Incremental Speech Understanding in a Multi-Party Virtual Human
Dialogue System
David DeVault and David Traum
Institute for Creative Technologies
University of Southern California
12015 Waterfront Drive, Playa Vista, CA 90094
{devault,traum}@ict.usc.edu
1 Extended Abstract
This demonstration highlights some emerging ca-
pabilities for incremental speech understanding and
processing in virtual human dialogue systems. This
work is part of an ongoing effort that aims to en-
able realistic spoken dialogue with virtual humans in
multi-party negotiation scenarios (Plu?ss et al, 2011;
Traum et al, 2008b). These scenarios are designed
to allow trainees to practice their negotiation skills
by engaging in face-to-face spoken negotiation with
one or more virtual humans.
An important component in achieving naturalistic
behavior in these negotiation scenarios, which ide-
ally should have the virtual humans demonstrating
fluid turn-taking, complex reasoning, and respond-
ing to factors like trust and emotions, is for the vir-
tual humans to begin to understand and in some
cases respond in real time to users? speech, as the
users are speaking (DeVault et al, 2011b). These re-
sponses could range from relatively straightforward
turn management behaviors, like having a virtual hu-
man recognize when it is being addressed by a user
utterance, and possibly turn to look at the user who
has started speaking, to more complex responses
such as emotional reactions to the content of what
users are saying.
The current demonstration extends our previous
demonstration of incremental processing (Sagae et
al., 2010) in several important respects. First, it
includes additional indicators, as described in (De-
Vault et al, 2011a). Second, it is applied to a new
domain, an extension of that presented in (Plu?ss et
al., 2011). Finally, it is integrated with the dialogue
Figure 1: SASO negotiation in the saloon: Utah (left)
looking at Harmony (right).
models (Traum et al, 2008a), such that each par-
tial interpretation is given a full pragmatic interpre-
tation by each virtual character, which can be used
to generate real-time incremental non-verbal feed-
back (Wang et al, 2011).
Our demonstration is set in an implemented multi-
party negotiation domain (Plu?ss et al, 2011) in
which two virtual humans, Utah and Harmony (pic-
tured in Figure 1), talk with two human negotiation
trainees, who play the roles of Ranger and Deputy.
The dialogue takes place inside a saloon in an Amer-
ican town in the Old West. In this negotiation sce-
nario, the goal of the two human role players is to
convince Utah and Harmony that Utah, who is cur-
rently employed as the local bartender, should take
on the job of town sheriff.
One of the research aims for this work is to
support natural dialogue interaction, an example of
which is the excerpt of human role play dialogue
shown in Figure 2. One of the key features of immer-
sive role plays is that people often react in multiple
ways to the utterances of others as they are speaking.
For example, in this excerpt, the beginning of the
25
Ranger We can?t leave this place and have it overrun by outlaws.
Uh there?s no way that?s gonna happen so we?re gonna
make sure we?ve got a properly deputized and equipped
sheriff ready to maintain order in this area.
00:03:56.660 - 00:04:08.830
Deputy Yeah and you know and and we?re willing to
00:04:06.370 - 00:04:09.850
Utah And I don?t have to leave the bar completely. I can still
uh be here part time and I can um we can hire someone to
do the like day to day work and I?ll do the I?ll supervise
them and I?ll teach them.
00:04:09.090 - 00:04:22.880
Figure 2: Dialogue excerpt from one of the role plays.
Timestamps indicate the start and end of each utterance.
Deputy?s utterance overlaps the end of the Ranger?s,
and then Utah interrupts the Deputy and takes the
floor a few seconds later.
Our prediction approach to incremental speech
understanding utilizes a corpus of in-domain spo-
ken utterances, including both paraphrases selected
and spoken by system developers, as well as spo-
ken utterances from user testing sessions (DeVault
et al, 2011b). An example of a corpus element is
shown in Figure 3. In previous negotiation domains,
we have found a fairly high word error rate in au-
tomatic speech recognition results for such sponta-
neous multi-party dialogue data; for example, our
average word error rate was 0.39 in the SASO-EN
negotiation domain (Traum et al, 2008b) with many
(15%) out of domain utterances. Our speech un-
derstanding framework is robust to these kinds of
problems (DeVault et al, 2011b), partly through
approximating the meaning of utterances. Utter-
ance meanings are represented using an attribute-
value matrix (AVM), where the attributes and val-
ues represent semantic information that is linked to
a domain-specific ontology and task model (Traum,
2003; Hartholt et al, 2008; Plu?ss et al, 2011). The
AVMs are linearized, using a path-value notation, as
seen in Figure 3. In our framework, we use this data
to train two data-driven models, one for incremen-
tal natural language understanding, and a second for
incremental confidence modeling.
The first step is to train a predictive incremental
understanding model. This model is based on maxi-
mum entropy classification, and treats entire individ-
ual frames as output classes, with input features ex-
tracted from partial ASR results, calculated in incre-
ments of 200 milliseconds (DeVault et al, 2011b).
? Utterance (speech): i?ve come here today to talk to you
about whether you?d like to become the sheriff of this town
? ASR (NLU input): have come here today to talk to you
about would the like to become the sheriff of this town
? Frame (NLU output):
<S>.mood interrogative
<S>.sem.modal.desire want
<S>.sem.prop.agent utah
<S>.sem.prop.event providePublicServices
<S>.sem.prop.location town
<S>.sem.prop.theme sheriff-job
<S>.sem.prop.type event
<S>.sem.q-slot polarity
<S>.sem.speechact.type info-req
<S>.sem.type question
Figure 3: Example of a corpus training example.
Each partial ASR result then serves as an incremen-
tal input to NLU, which is specially trained for par-
tial input as discussed in (Sagae et al, 2009). NLU
is predictive in the sense that, for each partial ASR
result, the NLU module produces as output the com-
plete frame that has been associated by a human an-
notator with the user?s complete utterance, even if
that utterance has not yet been fully processed by
the ASR. For a detailed analysis of the performance
of the predictive NLU, see (DeVault et al, 2011b).
The second step in our framework is to train a set
of incremental confidence models (DeVault et al,
2011a), which allow the agents to assess in real time,
while a user is speaking, how well the understand-
ing process is proceeding. The incremental confi-
dence models build on the notion of NLU F-score,
which we use to quantify the quality of a predicted
NLU frame in relation to the hand-annotated correct
frame. The NLU F-score is the harmonic mean of
the precision and recall of the attribute-value pairs
(or frame elements) that compose the predicted and
correct frames for each partial ASR result. By using
precision and recall of frame elements, rather than
simply looking at frame accuracy, we take into ac-
count that certain frames are more similar than oth-
ers, and allow for cases when the correct frame is
not in the training set.
Each of our incremental confidence models
makes a binary prediction for each partial NLU re-
sult as an utterance proceeds. At each time t dur-
26
Figure 4: Visualization of Incremental Speech Processing.
ing an utterance, we consider the current NLU F-
Score Ft as well as the final NLU F-Score Ffinal
that will be achieved at the conclusion of the ut-
terance. In (DeVault et al, 2009) and (DeVault
et al, 2011a), we explored the use of data-driven
decision tree classifiers to make predictions about
these values, for example whether Ft ? 12 (cur-
rent level of understanding is ?high?), Ft ? Ffinal
(current level of understanding will not improve),
or Ffinal ? 12 (final level of understanding will be
?high?). In this demonstration, we focus on the
first and third of these incremental confidence met-
rics, which we summarize as ?Now Understanding?
and ?Will Understand?, respectively. In an evalua-
tion over all partial ASR results for 990 utterances
in this new scenario, we found the Now Under-
standing model to have precision/recall/F-Score of
.92/.75/.82, and the Will Understand model to have
precision/recall/F-Score of .93/.85/.89. These incre-
mental confidence models therefore provide poten-
tially useful real-time information to Utah and Har-
mony about whether they are currently understand-
ing a user utterance, and whether they will ever un-
derstand a user utterance.
The incremental ASR, NLU, and confidence
models are passed to the dialogue managers for each
of the agents, Harmony and Utah. These agents then
relate these inputs to their own models of dialogue
context, plans, and emotions, to calculate pragmatic
interpretations, including speech acts, reference res-
olution, participant status, and how they feel about
what is being discussed. A subset of this informa-
tion is passed to the non-verbal behavior generation
module to produce incremental non-verbal listening
behaviors (Wang et al, 2011).
In support of this demonstration, we have ex-
tended the implementation to include a real-time vi-
sualization of incremental speech processing results,
which will allow attendees to track the virtual hu-
mans? understanding as an utterance progresses. An
example of this visualization is shown in Figure 4.
2 Demo script
The demonstration begins with the demo operator
providing a brief overview of the system design, ne-
gotiation scenario, and incremental processing capa-
bilities. The virtual humans Utah and Harmony (see
Figure 1) are running and ready to begin a dialogue
with the user, who will play the role of the Ranger.
As the user speaks to Utah or Harmony, attendees
can observe the real time visualization of speech
27
processing to observe changes in the incremental
processing results as the utterance progresses. Fur-
ther, the visualization interface enables the demo op-
erator to ?rewind? an utterance and step through the
incremental processing results that arrived each 200
milliseconds, highlighting how specific partial ASR
results can change the virtual humans? understand-
ing or confidence.
For example, Figure 4 shows the incremental
speech processing state at a moment 4.8 seconds into
a user?s 7.4 second long utterance, i?ve come here
today to talk to you about whether you?d like to be-
come the sheriff of this town. At this point in time,
the visualization shows (at top left) that the virtual
humans are confident that they are both Now Under-
standing and Will Understand this utterance. Next,
the graph (in white) shows the history of the agents?
expected NLU F-Score for this utterance (ranging
from 0 to 1). Beneath the graph, the partial ASR re-
sult (HAVE COME HERE TODAY TO TALK TO
YOU ABOUT...) is displayed (in white), along
with the currently predicted NLU frame (in blue).
For ease of comprehension, an English gloss (utah
do you want to be the sheriff?) for the NLU frame is
also shown (in blue) above the frame.
To the right, in pink, we show some of Utah and
Harmony?s agent state that is based on the current in-
cremental NLU results. The display shows that both
of the virtual humans believe that Utah is being ad-
dressed by this utterance, that utah has a positive at-
titude toward the content of the utterance while har-
mony does not, and that both have comprehension
and participation goals. Further, Harmony believes
she is a side participant at this moment. The demo
operator will explain and discuss this agent state in-
formation, including possible uses for this informa-
tion in response policies.
Acknowledgments
We thank all the members of the ICT Virtual Hu-
mans team. The project or effort described here
has been sponsored by the U.S. Army Research,
Development, and Engineering Command (RDE-
COM). Statements and opinions expressed do not
necessarily reflect the position or the policy of the
United States Government, and no official endorse-
ment should be inferred.
References
David DeVault, Kenji Sagae, and David Traum. 2009.
Can I finish? Learning when to respond to incremental
interpretation results in interactive dialogue. In Pro-
ceedings of SIGDIAL.
David DeVault, Kenji Sagae, and David Traum. 2011a.
Detecting the status of a predictive incremental speech
understanding model for real-time decision-making in
a spoken dialogue system. In Proceedings of Inter-
Speech.
David DeVault, Kenji Sagae, and David Traum. 2011b.
Incremental interpretation and prediction of utterance
meaning for interactive dialogue. Dialogue & Dis-
course, 2(1).
Arno Hartholt, Thomas Russ, David Traum, Eduard
Hovy, and Susan Robinson. 2008. A common ground
for virtual humans: Using an ontology in a natural
language oriented virtual human architecture. In Pro-
ceedings of LREC, Marrakech, Morocco, may.
Brian Plu?ss, David DeVault, and David Traum. 2011.
Toward rapid development of multi-party virtual hu-
man negotiation scenarios. In Proceedings of SemDial
2011, the 15th Workshop on the Semantics and Prag-
matics of Dialogue.
Kenji Sagae, Gwen Christian, David DeVault, and
David R. Traum. 2009. Towards natural language un-
derstanding of partial speech recognition results in dia-
logue systems. In Short Paper Proceedings of NAACL
HLT.
Kenji Sagae, David DeVault, and David R. Traum. 2010.
Interpretation of partial utterances in virtual human
dialogue systems. In Demonstration Proceedings of
NAACL-HLT.
D. Traum, W. Swartout, J. Gratch, and S. Marsella.
2008a. A virtual human dialogue model for non-team
interaction. In L. Dybkjaer and W. Minker, editors,
Recent Trends in Discourse and Dialogue. Springer.
David Traum, Stacy Marsella, Jonathan Gratch, Jina
Lee, and Arno Hartholt. 2008b. Multi-party, multi-
issue, multi-strategy negotiation for multi-modal vir-
tual agents. In Proceedings of IVA.
David Traum. 2003. Semantics and pragmatics of ques-
tions and answers for dialogue agents. In Proc. of the
International Workshop on Computational Semantics,
pages 380?394, January.
Zhiyang Wang, Jina Lee, and Stacy Marsella. 2011.
Towards more comprehensive listening behavior: Be-
yond the bobble head. In Hannes Vilhjlmsson, Stefan
Kopp, Stacy Marsella, and Kristinn Thrisson, editors,
Intelligent Virtual Agents, volume 6895 of Lecture
Notes in Computer Science, pages 216?227. Springer
Berlin / Heidelberg.
28
Proceedings of NAACL-HLT 2013, pages 1092?1099,
Atlanta, Georgia, 9?14 June 2013. c?2013 Association for Computational Linguistics
A method for the approximation of incremental understanding of explicit
utterance meaning using predictive models in finite domains
David DeVault and David Traum
Institute for Creative Technologies, University of Southern California,
12015 Waterfront Dr., Playa Vista, CA 90094 USA
{devault,traum}@ict.usc.edu
Abstract
This paper explores the relationship between explicit
and predictive models of incremental speech under-
standing in a dialogue system that supports a finite
set of user utterance meanings. We present a method
that enables the approximation of explicit under-
standing using information implicit in a predictive
understanding model for the same domain. We show
promising performance for this method in a corpus
evaluation, and discuss its practical application and
annotation costs in relation to some alternative ap-
proaches.
1 Introduction
In recent years, there has been a growing interest among
researchers in methods for incremental natural language
understanding (NLU) for spoken dialogue systems; see
e.g. (Skantze and Schlangen, 2009; Sagae et al, 2009;
Schlangen et al, 2009; Heintze et al, 2010; DeVault et
al., 2011a; Selfridge et al, 2012). This work has gen-
erally been motivated by a desire to make dialogue sys-
tems more efficient and more natural, by enabling them to
provide lower latency responses (Skantze and Schlangen,
2009), human-like feedback such as backchannels that in-
dicate how well the system is understanding user speech
(DeVault et al, 2011b; Traum et al, 2012), and more in-
teractive response capabilities such as collaborative com-
pletions of user utterances (DeVault et al, 2011a), more
adaptive handling of interruptions (Buschmeier et al,
2012), and others.
This paper builds on techniques developed in previous
work that has adopted a predictive approach to incremen-
tal NLU (DeVault et al, 2011a). On this approach, at
specific moments while a user?s speech is in progress,
an attempt is made to predict what the full meaning of
the complete user utterance will be. Predictive models
can be contrasted with explicit approaches to incremen-
tal NLU. We use the term explicit understanding to refer
to approaches that attempt to determine the meaning that
has been expressed explicitly in the user?s partial utter-
ance so far (without predicting further aspects of mean-
ing to come). Explicit understanding of partial utterances
can be implemented using statistical classification or se-
quential tagging models (Heintze et al, 2010).
Both predictive and explicit incremental NLU capabil-
ities can be valuable in a dialogue system. Prediction
can support specific response capabilities, such as sys-
tem completion of user utterances (DeVault et al, 2011a)
and reduced response latency.1 However, explicit models
support additional and complementary capabilities. For
instance, depending on the application domain (Heintze
et al, 2010) and on the individual utterance (DeVault et
al., 2011b), it may be difficult for a system to predict a
user?s impending meaning with confidence. Neverthe-
less, it may often be possible for systems to determine
the meaning of what a user has said so far, and to take
action based on this partial understanding. As one exam-
ple, items in a user interface could be highlighted when
mentioned by a user (Bu? and Schlangen, 2011). An-
other capability would be to provide grounding feedback,
such as verbal back-channels or head nods (in embod-
ied systems), to indicate when the system is understand-
ing the user?s meaning (Traum et al, 2012). Explicit ut-
terance meanings also allow a system to distinguish be-
tween meaning that has been expressed and meaning that
is merely implied or inferred, which may be less reliable.
In the near future, as incremental processing capabilities
in dialogue systems grow, it may prove valuable for di-
alogue systems to combine both predictive and explicit
incremental understanding capabilities.
In this paper, we present a technique for approximating
a user?s explicit meaning using an existing predictive un-
derstanding framework (DeVault et al, 2011a). The spe-
cific new contributions in this paper are (1) to show that
1A simple approach to reducing response latency is to begin to plan
a response to the predicted meaning while the user is still speaking.
1092
an estimate of a user?s explicit utterance meaning can be
derived from this kind of predictive understanding model
(Section 2); (2) to quantify the performance of this new
method in a corpus evaluation (Section 3); (3) to provide
concrete examples and discussion of the annotation costs
associated with implementing this technique, in relation
to some alternative approaches to explicit understanding
(Section 4). Our results and discussion show that the
proposed method offers promising performance, has rela-
tively low annotation costs, and enables explicit and pre-
dictive understanding to be easily combined within a di-
alogue system. It may therefore be a useful incremental
understanding technique for some dialogue systems.
2 Technical Approach and Data Set
In Sections 2.1-2.3, we briefly summarize the data set and
approach to predictive incremental NLU (DeVault et al,
2011a) that serves as the starting point for the new work
in this paper. Sections 2.4 and 2.5 present our new ap-
proach to explicit understanding based on this approach.
2.1 Data set
For the experiments reported here, we use a corpus of
user utterances collected with the SASO-EN spoken dia-
logue system (Hartholt et al, 2008; Traum et al, 2008).
Briefly, this system is designed to allow a trainee to prac-
tice multi-party negotiation skills by engaging in face to
face negotiation with virtual humans. The scenario in-
volves a negotiation about the possible re-location of a
medical clinic in an Iraqi village. A human trainee plays
the role of a US Army captain, and there are two virtual
humans that he negotiates with: Doctor Perez, the head
of an NGO clinic, and a local village elder, al-Hassan.
The captain?s main objective is to convince the doctor and
the elder to move the clinic out of an unsafe marketplace
area.
The corpus used for the experiments in this paper in-
cludes 3,826 training and 449 testing utterances drawn
from user dialogues in this domain. The corpus and its se-
mantic annotation are described in (DeVault et al, 2010;
DeVault et al, 2011a). All user utterances have been au-
dio recorded, transcribed, and manually annotated with
the correct NLU output frame for the entire utterance.
(We discuss the cost of this annotation in Section 4.) Each
NLU output frame contains a set of attributes and values
that represent semantic information linked to a domain-
specific ontology and task model (Traum, 2003). Exam-
ples of the NLU output frames are included in Figures 2,
3, and 5.
2.2 Predictive incremental NLU
This approach uses a predictive incremental NLU mod-
ule, mxNLU (Sagae et al, 2009; DeVault et al, 2011a),
which is based on maximum entropy classification. The
approach treats entire individual frames as output classes,
and extracts input features from partial ASR results. To
define the incremental understanding problem, the audio
of the utterances in the training data were fed through
an ASR module, PocketSphinx (Huggins-Daines et al,
2006), in 200 millisecond chunks, and each partial ASR
result produced by the ASR was recorded. Each par-
tial ASR result then serves as an incremental input to
mxNLU. NLU is predictive in the sense that, for each
partial ASR result, the task of mxNLU is to produce as
output the complete frame that has been associated by a
human annotator with the user?s complete utterance, even
if that utterance has not yet been fully processed by the
ASR.
The human annotation defines a finite set S =
{S1, ..., SN} of possible NLU output frames, where each
frame Si = {e1, ..., en} is a set of key-value pairs or
frame elements. For notation, a user utterance u generally
creates a sequence of m partial ASR results ?r1, ..., rm?,
where each ASR result rj is a partial text such as we need
to move. Let Gu denote the correct (or ?gold?) frame for
the complete utterance u. For each result rj and for each
complete frame Si, the maximum entropy model pro-
vides P (Gu = Si|rj). The NLU output frame SNLUj is
the complete frame for which this probability is highest.
2.3 Performance of predictive incremental NLU
The performance of this predictive incremental NLU
framework has been evaluated using the training and
test portions of the SASO-EN data set described in Sec-
tion 2.1. Performance is quantified by looking at pre-
cision, recall, and F-score of the frame elements that
compose the predicted (SNLUj ) and correct (Gu) frames
for each partial ASR result. When evaluated over all
the 5,736 partial ASR results for the 449 test utterances,
the precision/recall/F-Score of this predictive NLU, in
relation to the complete frames, are 0.67/0.47/0.56, re-
spectively. When evaluated on only the ASR results
for complete test utterances, these scores increase to
0.81/0.71/0.76, respectively.
2.4 Assigning probability to frame elements
An interesting question is whether we can use this model
to attach useful probabilities not only to complete pre-
dicted frames but also to the individual frame elements
that make up those frames. To explore this, for each par-
tial ASR result rj in each utterance u, and for each frame
element e in SASO-EN, let us model the probability that
e will be part of the correct frame for the complete utter-
ance as:
P (e ? Gu|rj) =
?
Si:e?Si
P (Gu = Si|rj) (1)
1093
0.00 0.10 0.20 0.30 0.40 0.50 0.60 0.70 0.80 0.90 1.00
0.0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1.0
Probability assigned to frame element
Re
lati
ve 
fre
que
ncy
 of 
fra
me
 ele
me
nt 
in c
orr
ect
 fra
me
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
Re
lati
ve 
fre
que
ncy
 of 
pro
bab
ility
 be
ing
 as
sig
ned
 to 
a fr
am
e e
lem
en
tModel calibration (left axis)
Perfect calibration (left axis)
Relative frequency of
assigned probabilities(right axis)
Figure 1: Calibration of frame element probabilities.
This method derives the probability of frame elements
from the probabilities assigned to the possible frames that
contain them. Computing this sum is straightforward in a
finite semantic domain such as SASO-EN.
We computed this probability for all frame elements
e and all partial ASR results rj in our test set, yielding
approximately 478,000 probability values. We grouped
these probability values into bins of size 0.05, and cal-
culated the frequency with which the frame elements in
each bin were indeed present in the correct frame Gu for
the relevant utterance u. The results are presented in Fig-
ure 1, which shows that the probability values derived
from Equation (1) are relatively ?well calibrated?, in the
sense that the relative frequency with which a frame el-
ement is in the final frame is very close to the numeric
probability assigned by Equation (1). The figure also
shows how frequently the model assigns various proba-
bility ranges to frame elements (blue dotted line, plotted
against the secondary right axis). Note that most frame
elements are assigned very little probability for most par-
tial ASR results.
We conclude from these observations that the probabil-
ities assigned by (1) could indeed carry useful informa-
tion about the likelihood that individual key values will
be present in the complete utterance meaning.
2.5 Selecting probable frame elements
In exploring the model of frame element probabilities
given in Equation (1), we observed that often the reason
a frame element has lower probability, at a given point
within a user utterance, is that it is a prediction rather than
something that has been expressed explicitly. Building on
this observation, our technique for estimating the user?s
explicit meaning uses a probability threshold to select
those individual frame elements which are most likely to
be in the frame for a complete utterance, according to the
predictive model. That is, at each partial result rj , we
estimate the user?s explicit meaning using a constructed
frame:
SSUBj = {e|P (e ? Gu|rj) ? ?} (2)
This approximation could work well if, in practice, the
most probable frame elements prove to match fairly
closely the user?s non-incremental utterance meaning at
the point this frame is constructed. We evaluate this in
the next section.
Note that, in general, the returned subset of frame
elements may not be identical to any complete frame
Si ? S; rather it will correspond to parts of these com-
plete frames or ?subframes?.
3 Performance Evaluation
To evaluate this technique, we constructed subsets of
frame elements or ?explicit subframes? using Equation
(2) and various minimum probability thresholds ? for par-
tial ASR results in our test set. We then compared the
resulting subframes both to the final complete frame Gu
for each utterance u, and also to manually annotated sub-
1094
Explicit subframe (with frame element probabilities) Predicted complete frame Annotated subframe
Partial ASR result: hello
0.813 <S>.sem.speechact.type greeting <S>.sem.speechact.type greeting
<S>.addressee doctor-perez
<S>.sem.speechact.type greeting
Partial ASR result: hello elder
0.945 <S>.sem.speechact.type greeting
0.934 <S>.addressee elder-al-hassan
<S>.sem.speechact.type greeting
<S>.addressee elder-al-hassan
<S>.sem.speechact.type greeting
<S>.addressee elder-al-hassan
Figure 2: Explicit subframes and predicted complete frames for two partial ASR results in a user utterance of hello elder.
frames that represent human judgments of explicit incre-
mental utterance meaning.
To collect these judgments, we hand-annotated a word-
meaning alignment for 50 random utterances in our test
set.2 To perform this annotation, successively larger pre-
fixes of each utterance transcript were mapped to succes-
sively larger subframes of the full frame for the complete
utterance. The annotated subframes for each utterance
prefix were selected to be explicit; they include only those
frame elements that are explicitly expressed in the corre-
sponding prefix of the user?s utterance. (We discuss the
cost of this annotation in Section 4.)
We provide a simple concrete example in Figure 2.
This example shows two partial ASR results during
an utterance of hello elder by a user. For each par-
tial ASR result, three frames are indicated horizon-
tally. At the right, labeled ?Annotated subframe?, we
show the human judgment of explicit incremental ut-
terance meaning for this partial utterance. Our hu-
man judge has indicated that the word hello corresponds
to the frame element <S>.sem.speechact.type
greeting, and that the words hello elder correspond
to an expanded frame that includes the frame element
<S>.addressee elder-al-hassan.
At the left, labeled ?Explicit subframe?, we show
the subframe selected by Equation (2) for each par-
tial ASR result, with threshold ? = 0.5. A relevant
background fact for this example is that in this sce-
nario, the user can generally address either of two vir-
tual humans who are present, Doctor Perez or Elder
Al-Hassan. After the user has said hello, the frame
element <S>.sem.speechact.type greeting is
assigned probability 0.813 by Equation (1), and only this
frame element appears in the explicit subframe.
In the middle, labeled ?Predicted complete frame?, the
figure also shows the full predicted frame from mxNLU
at each point. After the user has said hello, the full
predicted output includes an additional frame element,
<S>.addressee doctor-perez, indicating a pre-
diction that the addressee of this user utterance will be
Doctor Perez rather than Elder al-Hassan. However, the
2Note that no utterances in our training set were annotated.
probability assigned to this prediction by Equation (1) is
less than 0.5, and so this predicted frame element is ex-
cluded from the explicit subframe. And indeed, this is the
correct explicit representation of the meaning of hello in
this system.
This simple example illustrates how our proposed tech-
nique can enable a dialogue system to have access to both
explicit and predicted utterance meaning as a user?s ut-
terance progresses. An excerpt from a more complex
utterance is given in Figure 3. This example shows in-
cremental outputs for two partial ASR results during a
user utterance of we will provide transportation at no
cost. In this example, the explicit subframe for we
will includes frame elements that convey that the cap-
tain (i.e. the user) is promising to do something. This
subframe does not exactly match the human judgment
of explicit meaning at the right, which does not include
at this point the <S>.sem.agent captain-kirk
and <S>.sem.type event frame elements. How-
ever, the explicit subframe more closely matches the hu-
man judgment than does the predicted complete frame
from mxNLU (middle column), which includes an in-
correct prediction that the captain is promising to de-
liver medical supplies (represented by the key values
<S>.sem.event deliver and <S>.sem.theme
medical-supplies). For the next partial ASR re-
sult shown in the figure, the explicit subframe correctly
adds several additional frame elements which formalize
the meaning of the phrase provide transportation in this
scenario as having the army move the clinic out of the
market area.
To understand more quantitatively how well this tech-
nique works, we evaluated this technique in the SASO-
EN test corpus, using different probability thresholds in
the range [0.5,1.0). We present the results in Figure 4. To
understand the effect of the threshold ? , note that, in gen-
eral, the effect of selecting a higher threshold should be to
?cherry pick? those frame elements which are most likely
to appear in the complete frame Gu, thereby increasing
precision while decreasing recall of the frame elements in
SSUBj in relation to Gu. In the figure, we can see that this
is indeed the case. The lines marked ?(complete frame)?
1095
Explicit subframe (with frame element probabilities) Predicted complete frame Annotated subframe
Partial ASR result: we will
0.856 <S>.mood declarative
0.824 <S>.sem.agent captain-kirk
0.663 <S>.sem.modal.intention will
0.663 <S>.sem.speechact.type promise
0.776 <S>.sem.type event
<S>.mood declarative
<S>.sem.agent captain-kirk
<S>.sem.event deliver
<S>.sem.modal.intention will
<S>.sem.speechact.type promise
<S>.sem.theme medical-supplies
<S>.sem.type event
<S>.mood declarative
<S>.sem.modal.intention will
<S>.sem.speechact.type promise
Partial ASR result: we will provide transportation
0.991 <S>.mood declarative
0.990 <S>.sem.agent captain-kirk
0.927 <S>.sem.event move
0.905 <S>.sem.instrument us-army
0.964 <S>.sem.modal.intention will
0.927 <S>.sem.source market
0.964 <S>.sem.speechact.type promise
0.928 <S>.sem.theme clinic
0.989 <S>.sem.type event
<S>.mood declarative
<S>.sem.agent captain-kirk
<S>.sem.event move
<S>.sem.instrument us-army
<S>.sem.modal.intention will
<S>.sem.source market
<S>.sem.speechact.type promise
<S>.sem.theme clinic
<S>.sem.type event
<S>.mood declarative
<S>.sem.agent captain-kirk
<S>.sem.event move
<S>.sem.instrument us-army
<S>.sem.modal.intention will
<S>.sem.source market
<S>.sem.speechact.type promise
<S>.sem.theme clinic
<S>.sem.type event
Figure 3: Explicit subframes and predicted complete frames for two partial ASR results in a user utterance of we will provide
transportation at no cost.
0.5 0.6 0.7 0.8 0.9 1.0
0.0
0.2
0.4
0.6
0.8
1.0
threshold
o
o
o
Precision (complete frame)
Precision (annotated subframe)
Recall (complete frame)
Recall (annotated subframe)
F?Score (complete frame)
F?Score (annotated subframe)
Figure 4: The effect of threshold on precision, recall, and F-Score of explicit subframes. All scores are measured in relation to
complete utterance frames and annotated subframes.
1096
in the figure evaluate the returned subframes in relation
to the complete frameGu associated with the user?s com-
plete utterance. We see that this method enables us to
select subsets of frame elements that are most likely to
appear in Gu: by increasing the threshold, it is possible
to return subframes which are of increasingly higher pre-
cision in relation to the final frame Gu, but that also have
lower recall.
We also evaluated the returned subframes in relation to
the hand-annotated subframes, to assess its performance
at identifying the user?s explicit meaning. For an utter-
ance u that generates partial ASR results ?r1, ..., rm?,
we denote the hand-annotated subframe corresponding to
partial ASR result rj by GSUBj . In the lines marked ?(an-
notated subframe)?, we show the precision, recall, and
F-score of the explicit subframe for each ASR result rj
in relation to the annotated subframe GSUBj .
As a first observation, note that at any threshold level,
the explicit subframes do better at recalling the hand-
annotated subframe elements than they do at recalling the
complete frame elements. This means our new method is
better at recalling what has been said already by the user
than it is at predicting what will be said, as intended. We
have seen two examples of this already, for the partial
ASR result hello in Figure 2, and for the partial ASR re-
sult we will in Figure 3.
A second observation in Figure 4 is that precision re-
mains better against the complete utterance frame than
against the hand-annotated subframe (at all threshold lev-
els). This indicates that the explicit subframes are often
still predicting some aspects of the full frame. An exam-
ple of this is given in Figure 5, where the user?s partial
utterance we need to is assigned an explicit subframe that
includes frame elements describing an event of moving
the clinic, which the user has not said explicitly. This
happens because, in the SASO-EN domain, in fact there
is nothing else that the interlocutors need to do besides
move the clinic. So based on the NLU training data,
the data-driven probabilities assigned by Equation (1) de-
scribe the additional frame elements as about as probable
as the ones capturing the we need to part of the semantics
(given at the right).
Finally, a third observation is that overall, the preci-
sion, recall, and F-score results against the annotated sub-
frames using our method are surprisingly strong. For
example, when evaluating the explicit subframes over
all partial ASR results, an F-score of 0.75 is attained at
thresholds in the range 0.5-0.55. This F-score is sub-
stantially better than the F-score of our predictive NLU
in relation to the final full frames, which is 0.56 when
evaluated over all partial ASR results. This means that
our proposed model works better as an explicit incre-
mental NLU than mxNLU works as a predictive incre-
mental NLU. Further, we observe that this F-score of
0.75 against hand-annotated subframes is approximately
as good as the F-score of 0.76 that is achieved when
mxNLU is used to interpret complete utterances. We
therefore conclude that the proposed model is a promis-
ing and viable approach to explicit incremental NLU in
SASO-EN.
4 Discussion and Related Approaches
In this section, we discuss some of the practical aspects
of using the technique presented here, in relation to some
alternative approaches.
An important consideration for NLU techniques is the
cost, in both time and knowledge, of the annotation that
is needed. One attractive aspect of our technique is that
the only semantic annotation that is required is the asso-
ciation of complete user utterances with complete NLU
output frames. This task can be performed by anyone fa-
miliar with the scenario and the semantic frame format,
such as a system developer or scenario designer. In fact,
the annotation of the SASO-EN data set we use in this
paper has been described in (DeVault et al, 2010), which
reports that the overall corpus of 4678 token utterances
was semantically annotated at an average rate of about 10
seconds per unique utterance.
The model in Equation (2) is what (Heintze et al,
2010) call a hybrid output approach, in which larger and
larger frames are provided as partial input grows, but
in which a detailed alignment between surface text and
frames is not provided by the incremental NLU compo-
nent. They contrast hybrid output systems with tech-
niques that deliver either whole-frame output (like the
predictive mxNLU) or aligned output that connects indi-
vidual words to their meanings. A data-driven approach
to providing aligned outputs would involve preparing
a more detailed annotated corpus that aligns individ-
ual words and surface expressions to their corresponding
frame elements. Given such a word-aligned corpus, one
could train several kinds of models to produce the aligned
outputs incrementally. One strategy would be to use a se-
quential tagging model such as a CRF to tag partial utter-
ances with the frame elements that capture their explicit
meaning, as in (Heintze et al, 2010).
Using a machine learning approach that models a
more detailed alignment between surface text and frames
would be one way to more cleanly separate explicit from
predictive aspects of meaning. Preparing the training data
for such models, however, would create additional an-
notation costs. As part of creating the annotated sub-
frames for the evaluation presented in Section 3, we mea-
sured the time requirement for such annotation of word-
meaning alignments at about 30 seconds per unique ut-
terance. Performing full word-meaning alignment there-
fore takes about three times as much time as the com-
plete utterance annotation needed for our technique. Ad-
1097
Explicit subframe (with frame element probabilities) Predicted complete frame Annotated subframe
Partial ASR result: we
0.753 <S>.mood declarative
0.687 <S>.sem.agent captain-kirk
0.692 <S>.sem.type event
<S>.mood declarative
<S>.sem.agent captain-kirk
<S>.sem.event deliver
<S>.sem.modal.possibility can
<S>.sem.speechact.type offer
<S>.sem.theme medical-supplies
<S>.sem.type event
Partial ASR result: we need to
0.945 <S>.mood declarative
0.928 <S>.sem.agent captain-kirk
0.900 <S>.sem.event move
0.816 <S>.sem.modal.deontic must
0.900 <S>.sem.source market
0.900 <S>.sem.speechact.type statement
0.906 <S>.sem.theme clinic
0.930 <S>.sem.type event
<S>.mood declarative
<S>.sem.agent captain-kirk
<S>.sem.event move
<S>.sem.modal.deontic must
<S>.sem.source market
<S>.sem.speechact.type statement
<S>.sem.theme clinic
<S>.sem.type event
<S>.mood declarative
<S>.sem.modal.deontic must
<S>.sem.speechact.type statement
Figure 5: Explicit subframes and predicted complete frames for two partial ASR results in a user utterance of we need to move the
clinic.
ditionally, this task requires a greater degree of linguis-
tic knowledge and sophistication, as the annotator must
be able to segment the utterance and align specific sur-
face segments with potentially complex aspects of mean-
ing such as modality, polarity, speech act types, and
others. An example of the kinds of complexities that
arise is illustrated in Figure 3, where the relationship be-
tween specific words like ?provide? and ?transportation?
to frame elements like <S>.sem.event move and
<S>.sem.theme clinic is not transparent, even if
it is straightforward to mark the whole utterance as con-
veying that meaning in this domain. We have generally
found this alignment task challenging for people without
advanced linguistics training.
The reason we describe the method in this paper as an
approximation of explicit NLU is that, partly because it
is trained without detailed word-meaning alignments, it
can be expected to occasionally include some predictive
aspects of user utterance meaning. An example of this is
the method?s explicit subframe output for the phrase we
need to in Figure 5.
Another way to approximate explicit NLU would be
using the method (Heintze et al, 2010) call an ensem-
ble of classifiers; it involves training an individual clas-
sifier for each frame key. Like the method presented
here, an ensemble of classifiers can be easily trained to
predict those frame elements that will appear in the fi-
nal frame Gu for each utterance. And like our method,
prediction with an ensemble of classifiers does not re-
quire detailed annotation of word-meaning alignment in
the training data. One difference is that, with our method,
by selecting an appropriate threshold, it is easy to enforce
certain consistency properties on subframe outputs. In an
ensemble of classifiers approach, there is no immediate
guarantee that the output frame constructed by the inde-
pendent classifiers will be internally consistent from the
standpoint of downstream system modules (Heintze et al,
2010). For example, in the SASO-EN domain, an NLU
frame should not contain frame elements that mix aspects
of events and states in the SASO-EN ontology; e.g., the
frame element <S>.sem.type event should not co-
occur in an NLU output frame with the frame element
<S>.sem.object-id market (which would be ap-
propriate for a state frame but not for an event frame).
With the method proposed here, if we select a threshold
? that is greater than 0.5, and if none of the complete
NLU frames contain incompatible key values (which is
relatively easy to enforce as part of the annotation task),
then it will be mathematically impossible for two incom-
patible frame elements to be returned in a subframe.3
Ultimately, a classification method that is trained on
word-meaning aligned data and that uses additional tech-
niques to ensure that only valid, grammatical output
frames are produced could prove to be an attractive ap-
proach. In future work, we will explore such techniques,
and compare both their performance as well as their anno-
tation and development costs to the approximation tech-
nique presented here.
5 Conclusion
The analysis in this paper has explored a method of ap-
proximating explicit incremental NLU using predictive
3Suppose frame element ei is incompatible with ej , and that
P (ei ? Gu|rj) > 0.5. By stipulation, no complete frame S ? S
such that ei ? S will also contain ej . Since we know that the total
probability of all the frames containing ei must be greater than 0.5 in
order for ei to be selected, we can infer that the total probability of all
frames including ej must be less than 0.5, and thus that ej will not be
selected.
1098
techniques in finite semantic domains. We have shown
that an estimate of a user?s explicit utterance meaning
can be derived from an existing predictive understand-
ing model in an example domain. We have quantified
the performance of this new method in a corpus evalu-
ation, showing that the method returns incremental ex-
plicit subframes with performance ? as measured by pre-
cision, recall, and F-Score against hand-annotated sub-
frames ? that is competitive with a current statistical,
data-driven approach for understanding complete spoken
utterances in the same domain. We have provided ex-
amples that illustrate its strengths and weaknesses, and
discussed the annotation costs associated with imple-
menting this technique in relation to some alternative ap-
proaches. The method requires no additional annotation
beyond what is needed for training an NLU module to
understand complete spoken utterances. (Hand annota-
tion of word-meaning alignment for a small number of
utterances may be performed in order to tune the se-
lected threshold and evaluate explicit understanding per-
formance.) The method provides a free parameter that
can be used to target the most advantageous levels of pre-
cision and recall for a particular dialogue system applica-
tion. In future work, we will explore additional machine
learning models that leverage richer training data, and in-
vestigate further the combination of explicit and predic-
tive techniques.
Acknowledgments
The project or effort described here has been sponsored
by the U.S. Army Research, Development, and Engi-
neering Command (RDECOM). Statements and opinions
expressed do not necessarily reflect the position or the
policy of the United States Government, and no official
endorsement should be inferred. This material is based
upon work supported by the National Science Founda-
tion under Grant No. IIS-1219253. Any opinions, find-
ings, and conclusions or recommendations expressed in
this material are those of the author(s) and do not neces-
sarily reflect the views of the National Science Founda-
tion.
References
Hendrik Buschmeier, Timo Baumann, Benjamin Dosch, Ste-
fan Kopp, and David Schlangen. 2012. Combining incre-
mental language generation and incremental speech synthe-
sis for adaptive information presentation. In Proceedings of
the 13th Annual Meeting of the Special Interest Group on
Discourse and Dialogue, pages 295?303, Seoul, South Ko-
rea, July. Association for Computational Linguistics.
Okko Bu? and David Schlangen. 2011. Dium - an incremental
dialogue manager that can produce self-corrections. In Pro-
ceedings of the 15th Workshop on the Semantics and Prag-
matics of Dialogue (SemDial).
David DeVault, Susan Robinson, and David Traum. 2010.
IORelator: A graphical user interface to enable rapid seman-
tic annotation for data-driven natural language understand-
ing. In Fifth Joint ISO-ACL/SIGSEM Workshop on Interop-
erable Semantic Annotation.
David DeVault, Kenji Sagae, and David Traum. 2011a. Incre-
mental interpretation and prediction of utterance meaning for
interactive dialogue. Dialogue & Discourse, 2(1).
David DeVault, Kenji Sagae, and David R. Traum. 2011b. De-
tecting the status of a predictive incremental speech under-
standing model for real-time decision-making in a spoken
dialogue system. In Interspeech, pages 1021?1024.
Arno Hartholt, Thomas Russ, David Traum, Eduard Hovy,
and Susan Robinson. 2008. A common ground for vir-
tual humans: Using an ontology in a natural language ori-
ented virtual human architecture. In European Language
Resources Association (ELRA), editor, Proc. LREC, Mar-
rakech, Morocco, may.
Silvan Heintze, Timo Baumann, and David Schlangen. 2010.
Comparing local and sequential models for statistical incre-
mental natural language understanding. In The 11th Annual
Meeting of the Special Interest Group in Discourse and Dia-
logue (SIGDIAL 2010).
David Huggins-Daines, Mohit Kumar, Arthur Chan, Alan W.
Black, Mosur Ravishankar, and Alex I. Rudnicky. 2006.
Pocketsphinx: A free, real-time continuous speech recog-
nition system for hand-held devices. In Proceedings of
ICASSP.
Kenji Sagae, Gwen Christian, David DeVault, and David R.
Traum. 2009. Towards natural language understanding of
partial speech recognition results in dialogue systems. In
NAACL HLT.
David Schlangen, Timo Baumann, and Michaela Atterer. 2009.
Incremental reference resolution: The task, metrics for eval-
uation, and a bayesian filtering model that is sensitive to dis-
fluencies. In SIGDIAL.
Ethan O. Selfridge, Iker Arizmendi, Peter A. Heeman, and Ja-
son D. Williams. 2012. Integrating incremental speech
recognition and pomdp-based dialogue systems. In Proceed-
ings of the 13th Annual Meeting of the Special Interest Group
on Discourse and Dialogue, pages 275?279, Seoul, South
Korea, July. Association for Computational Linguistics.
Gabriel Skantze and David Schlangen. 2009. Incremental di-
alogue processing in a micro-domain. In Proceedings of
EACL 2009.
David Traum, Stacy Marsella, Jonathan Gratch, Jina Lee, and
Arno Hartholt. 2008. Multi-party, multi-issue, multi-
strategy negotiation for multi-modal virtual agents. In Proc.
of Intelligent Virtual Agents Conference IVA-2008.
David Traum, David DeVault, Jina Lee, Zhiyang Wang, and
Stacy C. Marsella. 2012. Incremental dialogue understand-
ing and feedback for multi-party, multimodal conversation.
In The 12th International Conference on Intelligent Virtual
Agents (IVA), Santa Cruz, CA, September.
David Traum. 2003. Semantics and pragmatics of questions
and answers for dialogue agents. In Proc. of the Interna-
tional Workshop on Computational Semantics, pages 380?
394, January.
1099
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 500?510,
Baltimore, Maryland, USA, June 23-25 2014.
c?2014 Association for Computational Linguistics
Single-Agent vs. Multi-Agent Techniques for Concurrent
Reinforcement Learning of Negotiation Dialogue Policies
Kallirroi Georgila, Claire Nelson, David Traum
University of Southern California Institute for Creative Technologies
12015 Waterfront Drive, Playa Vista, CA 90094, USA
{kgeorgila,traum}@ict.usc.edu
Abstract
We use single-agent and multi-agent Rein-
forcement Learning (RL) for learning dia-
logue policies in a resource allocation ne-
gotiation scenario. Two agents learn con-
currently by interacting with each other
without any need for simulated users
(SUs) to train against or corpora to learn
from. In particular, we compare the Q-
learning, Policy Hill-Climbing (PHC) and
Win or Learn Fast Policy Hill-Climbing
(PHC-WoLF) algorithms, varying the sce-
nario complexity (state space size), the
number of training episodes, the learning
rate, and the exploration rate. Our re-
sults show that generally Q-learning fails
to converge whereas PHC and PHC-WoLF
always converge and perform similarly.
We also show that very high gradually
decreasing exploration rates are required
for convergence. We conclude that multi-
agent RL of dialogue policies is a promis-
ing alternative to using single-agent RL
and SUs or learning directly from corpora.
1 Introduction
The dialogue policy of a dialogue system decides
on which actions the system should perform given
a particular dialogue state (i.e., dialogue context).
Building a dialogue policy can be a challenging
task especially for complex applications. For this
reason, recently much attention has been drawn
to machine learning approaches to dialogue man-
agement and in particular Reinforcement Learning
(RL) of dialogue policies (Williams and Young,
2007; Rieser et al, 2011; Jur?c???cek et al, 2012).
Typically there are three main approaches to
the problem of learning dialogue policies using
RL: (1) learn against a simulated user (SU), i.e.,
a model that simulates the behavior of a real user
(Georgila et al, 2006; Schatzmann et al, 2006);
(2) learn directly from a corpus (Henderson et al,
2008; Li et al, 2009); or (3) learn via live interac-
tion with human users (Singh et al, 2002; Ga?si?c et
al., 2011; Ga?si?c et al, 2013).
We propose a fourth approach: concurrent
learning of the system policy and the SU policy
using multi-agent RL techniques. Both agents are
trained simultaneously and there is no need for
building a SU separately or having access to a cor-
pus.
1
As we discuss below, concurrent learning
could potentially be used for learning via live in-
teraction with human users. Moreover, for negoti-
ation in particular there is one more reason in fa-
vor of concurrent learning as opposed to learning
against a SU. Unlike slot-filling domains, in nego-
tiation the behaviors of the system and the user are
symmetric. They are both negotiators, thus build-
ing a good SU is as difficult as building a good
system policy.
So far research on using RL for dialogue pol-
icy learning has focused on single-agent RL tech-
niques. Single-agent RL methods make the as-
sumption that the system learns by interacting with
a stationary environment, i.e., an environment that
does not change over time. Here the environ-
ment is the user. Generally the assumption that
users do not significantly change their behavior
over time holds for simple information providing
tasks (e.g., reserving a flight). But this is not nec-
essarily the case for other genres of dialogue, in-
cluding negotiation. Imagine a situation where a
negotiator is so uncooperative and arrogant that
the other negotiators decide to completely change
their negotiation strategy in order to punish her.
Therefore it is important to investigate RL ap-
proaches that do not make such assumptions about
the user/environment.
1
Though corpora or SUs may still be useful for bootstrap-
ping the policies and encoding real user behavior (see sec-
tion 6).
500
Multi-agent RL is designed to work for non-
stationary environments. In this case the envi-
ronment of a learning agent is one or more other
agents that can also be learning at the same time.
Therefore, unlike single-agent RL, multi-agent RL
can handle changes in user behavior or in the be-
havior of other agents participating in the inter-
action, and thus potentially lead to more realis-
tic dialogue policies in complex dialogue scenar-
ios. This ability of multi-agent RL can also have
important implications for learning via live inter-
action with human users. Imagine a system that
learns to change its strategy as it realizes that a
particular user is no longer a novice user, or that a
user no longer cares about five star restaurants.
We apply multi-agent RL to a resource alloca-
tion negotiation scenario. Two agents with dif-
ferent preferences negotiate about how to share
resources. We compare Q-learning (a single-
agent RL algorithm) with two multi-agent RL al-
gorithms: Policy Hill-Climbing (PHC) and Win
or Learn Fast Policy Hill-Climbing (PHC-WoLF)
(Bowling and Veloso, 2002). We vary the scenario
complexity (i.e., the quantity of resources to be
shared and consequently the state space size), the
number of training episodes, the learning rate, and
the exploration rate.
Our research contributions are as follows: (1)
we propose concurrent learning using multi-agent
RL as a way to deal with some of the issues of cur-
rent approaches to dialogue policy learning (i.e.,
the need for SUs and corpora), which may also
potentially prove useful for learning via live inter-
action with human users; (2) we show that concur-
rent learning can address changes in user behav-
ior over time, and requires multi-agent RL tech-
niques and variable exploration rates; (3) to our
knowledge this is the first time that PHC and PHC-
WoLF are used for learning dialogue policies; (4)
for the first time, the above techniques are applied
to a negotiation domain; and (5) this is the first
study that compares Q-learning, PHC, and PHC-
WoLF in such a variety of situations (varying a
large number of parameters).
The paper is structured as follows. Section 2
presents related work. Section 3 provides a brief
introduction to single-agent RL and multi-agent
RL. Section 4 describes our negotiation domain
and experimental setup. In section 5 we present
our results. Finally, section 6 concludes and pro-
vides some ideas for future work.
2 Related Work
Most research in RL for dialogue management has
been done in the framework of slot-filling applica-
tions such as restaurant recommendations (Lemon
et al, 2006; Thomson and Young, 2010; Ga?si?c
et al, 2012; Daubigney et al, 2012), flight reser-
vations (Henderson et al, 2008), sightseeing rec-
ommendations (Misu et al, 2010), appointment
scheduling (Georgila et al, 2010), etc. RL has
also been applied to question-answering (Misu et
al., 2012), tutoring domains (Tetreault and Litman,
2008; Chi et al, 2011), and learning negotiation
dialogue policies (Heeman, 2009; Georgila and
Traum, 2011; Georgila, 2013).
As mentioned in section 1, there are three main
approaches to the problem of learning dialogue
policies using RL.
In the first approach, a SU is hand-crafted or
learned from a small corpus of human-human or
human-machine dialogues. Then the dialogue pol-
icy can be learned by having the system interact
with the SU for a large number of dialogues (usu-
ally thousands of dialogues). Depending on the
application, building a realistic SU can be just as
difficult as building a good dialogue policy. Fur-
thermore, it is not clear what constitutes a good
SU for dialogue policy learning. Should the SU
resemble real user behavior as closely as possi-
ble, or should it exhibit some degree of random-
ness to explore a variety of interaction patterns?
Despite much research on the issue, these are still
open questions (Schatzmann et al, 2006; Ai and
Litman, 2008; Pietquin and Hastie, 2013).
In the second approach, no SUs are required.
Instead the dialogue policy is learned directly from
a corpus of human-human or human-machine dia-
logues. For example, Henderson et al (2008) used
a combination of RL and supervised learning to
learn a dialogue policy in a flight reservation do-
main, whereas Li et al (2009) used Least-Squares
Policy Iteration (Lagoudakis and Parr, 2003), an
RL-based technique that can learn directly from
corpora, in a voice dialer application. However,
collecting such corpora is not trivial, especially in
new domains. Typically, data are collected in a
Wizard-of-Oz setup where human users think that
they interact with a system while in fact they inter-
act with a human pretending to be the system, or
by having human users interact with a preliminary
version of the dialogue system. In both cases the
resulting interactions are expected to be quite dif-
501
ferent from the interactions of human users with
the final system. In practice this means that dia-
logue policies learned from such data could be far
from optimal.
The first experiment on learning via live inter-
action with human users (third approach) was re-
ported by Singh et al (2002). They used RL to
help the system with two choices: how much ini-
tiative it should allow the user, and whether or not
to confirm information provided by the user. Re-
cently, learning of ?full? dialogue policies (not just
choices at specific points in the dialogue) via live
interaction with human users has become possi-
ble with the use of Gaussian processes (Engel et
al., 2005; Rasmussen and Williams, 2006). Typi-
cally learning a dialogue policy is a slow process
requiring thousands of dialogues, hence the need
for SUs. Gaussian processes have been shown to
speed up learning. This fact together with easy
access to a large number of human users through
crowd-sourcing has allowed dialogue policy learn-
ing via live interaction with human users (Ga?si?c et
al., 2011; Ga?si?c et al, 2013).
Space constraints prevent us from providing an
exhaustive list of previous work on using RL for
dialogue management. Thus below we focus only
on research that is directly related to our work,
specifically research on concurrent learning of the
policies of multiple agents, and the application of
RL to negotiation domains.
So far research on RL in the dialogue commu-
nity has focused on using single-agent RL tech-
niques where the stationary environment is the
user. Most approaches assume that the user goal
is fixed and that the behavior of the user is ratio-
nal. Other approaches account for changes in user
goals (Ma, 2013). In either case, one can build a
user simulation model that is the average of dif-
ferent user behaviors or learn a policy from a cor-
pus that contains a variety of interaction patterns,
and thus safely assume that single-agent RL tech-
niques will work. However, in the latter case if
the behavior of the user changes significantly over
time then the assumption that the environment is
stationary will no longer hold.
There has been a lot of research on multi-agent
RL in the optimal control and robotics communi-
ties (Littman, 1994; Hu andWellman, 1998; Buso-
niu et al, 2008). Here two or more agents learn si-
multaneously. Thus the environment of an agent is
one or more other agents that continuously change
their behavior because they are also learning at the
same time. Therefore the environment is no longer
stationary and single-agent RL techniques do not
work well or do not work at all. We are particu-
larly interested in the work of Bowling and Veloso
(2002) who proposed the PHC and PHC-WoLF al-
gorithms that we use in this paper. We chose these
two algorithms because, unlike other multi-agent
RL methods (Littman, 1994; Hu and Wellman,
1998), they do not make assumptions that do not
always hold and do not require quadratic or linear
programming that does not always scale.
English and Heeman (2005) were the first in the
dialogue community to explore the idea of con-
current learning of dialogue policies. However,
English and Heeman (2005) did not use multi-
agent RL but only standard single-agent RL, in
particular an on-policy Monte Carlo method (Sut-
ton and Barto, 1998). But single-agent RL tech-
niques are not well suited for concurrent learning
where each agent is trained against a continuously
changing environment. Indeed, English and Hee-
man (2005) reported problems with convergence.
Chandramohan et al (2012) proposed a frame-
work for co-adaptation of the dialogue policy and
the SU using single-agent RL. They applied In-
verse Reinforcement Learning (IRL) (Abbeel and
Ng, 2004) to a corpus in order to learn the reward
functions of both the system and the SU. Further-
more, Cuay?ahuitl and Dethlefs (2012) used hier-
archical multi-agent RL for co-ordinating the ver-
bal and non-verbal actions of a robot. Cuay?ahuitl
and Dethlefs (2012) did not use PHC or PHC-
WoLF and did not compare against single-agent
RL methods.
With regard to using RL for learning negotia-
tion policies, the amount of research that has been
performed is very limited compared to slot-filling.
English and Heeman (2005) learned negotiation
policies for a furniture layout task. Then Hee-
man (2009) extended this work by experiment-
ing with different representations of the RL state
in the same domain (this time learning against
a hand-crafted SU). In both cases, to reduce the
search space, the RL state included only infor-
mation about e.g., whether there was a pending
proposal rather than the actual value of this pro-
posal. Paruchuri et al (2009) performed a theo-
retical study on how Partially Observable Markov
Decision Processes (POMDPs) can be applied to
negotiation domains.
502
Georgila and Traum (2011) built argumentation
dialogue policies for negotiation against users of
different cultural norms in a one-issue negotiation
scenario. To learn these policies they trained SUs
on a spoken dialogue corpus in a florist-grocer
negotiation domain, and then tweaked these SUs
towards a particular cultural norm using hand-
crafted rules. Georgila (2013) learned argumen-
tation dialogue policies from a simulated corpus
in a two-issue negotiation scenario (organizing a
party). Finally, Nouri et al (2012) used IRL to
learn a model for cultural decision-making in a
simple negotiation game (the Ultimatum Game).
3 Single-Agent vs. Multi-Agent
Reinforcement Learning
Reinforcement Learning (RL) is a machine learn-
ing technique used to learn the policy of an agent,
i.e., which action the agent should perform given
its current state (Sutton and Barto, 1998). The goal
of an RL-based agent is to maximize the reward it
gets during an interaction. Because it is very dif-
ficult for the agent to know what will happen in
the rest of the interaction, the agent must select an
action based on the average reward it has previ-
ously observed after having performed that action
in similar contexts. This average reward is called
expected future reward. Single-agent RL is used
in the framework of Markov Decision Processes
(MDPs) (Sutton and Barto, 1998) or Partially Ob-
servable Markov Decision Processes (POMDPs)
(Williams and Young, 2007). Here we focus on
MDPs.
An MDP is defined as a tuple (S, A, T , R, ?)
where S is the set of states (representing different
contexts) which the agent may be in, A is the set
of actions of the agent, T is the transition func-
tion S ? A ? S ? [0, 1] which defines a set of
transition probabilities between states after taking
an action, R is the reward function S ? A ? <
which defines the reward received when taking an
action from the given state, and ? is a factor that
discounts future rewards. Solving the MDP means
finding a policy pi : S ? A. The quality of the
policy pi is measured by the expected discounted
(with discount factor ?) future reward also called
Q-value, Q
pi
: S ? A?<.
A stochastic game is defined as a tuple (n, S,
A
1...n
, T , R
1...n
, ?) where n is the number of
agents, S is the set of states, A
i
is the set of ac-
tions available for agent i (and A is the joint ac-
tion space A
1
? A
2
? ... ? A
n
), T is the transi-
tion function S ? A ? S ? [0, 1] which defines
a set of transition probabilities between states af-
ter taking a joint action, R
i
is the reward function
for the ith agent S ? A ? <, and ? is a factor
that discounts future rewards. The goal is for each
agent i to learn a mixed policy pi
i
: S ? A
i
? [0,
1] that maps states to mixed strategies, which are
probability distributions over the agent?s actions,
so that the agent?s expected discounted (with dis-
count factor ?) future reward is maximized.
Stochastic games are a generalization of MDPs
for multi-agent RL. In stochastic games there are
many agents that select actions and the next state
and rewards depend on the joint action of all these
agents. The agents can have different reward
functions. Partially Observable Stochastic Games
(POSGs) are the equivalent of POMDPs for multi-
agent RL. In POSGs, the agents have different ob-
servations, and uncertainty about the state they are
in and the beliefs of their interlocutors. POSGs
are very hard to solve but new algorithms continu-
ously emerge in the literature.
In this paper we use three algorithms: Q-
learning, Policy Hill-Climbing (PHC), and Win
or Learn Fast Policy Hill-Climbing (PHC-WoLF).
PHC is an extension of Q-learning. For all three
algorithms, Q-values are updated as follows:
Q(s, a)? (1??)Q(s, a)+?
(
r + ?max
a
?
Q(s
?
, a
?
)
)
(1)
In Q-learning, for a given state s, the agent
performs the action with the highest Q-value for
that state. In addition to Q-values, PHC and
PHC-WoLF also maintain the current mixed pol-
icy pi(s, a). In each step the mixed policy is up-
dated by increasing the probability of selecting the
highest valued action according to a learning rate
? (see equations (2), (3), and (4) below).
pi(s, a)? pi(s, a) + ?
sa
(2)
?
sa
=
{
??
sa
if a 6= argmax
a
?
Q(s, a
?
)
?
a
?
6=a
?
sa
?
otherwise
(3)
?
sa
= min
(
pi(s, a),
?
|A
i
| ? 1
)
(4)
The difference between PHC and PHC-WoLF is
that PHC uses a constant learning rate ? whereas
503
PHC-WoLF uses a variable learning rate (see
equation (5) below). The main idea is that when
the agent is ?winning? the learning rate ?
W
should
be low so that the opponents have more time to
adapt to the agent?s policy, which helps with con-
vergence. On the other hand when the agent is
?losing? the learning rate ?
LF
should be high so
that the agent has more time to adapt to the other
agents? policies, which also facilitates conver-
gence. Thus PHC-WoLF uses two learning rates
?
W
and ?
LF
. PHC-WoLF determines whether the
agent is ?winning? or ?losing? by comparing the
current policy?s pi(s, a) expected payoff with that
of the average policy p?i(s, a) over time. If the cur-
rent policy?s expected payoff is greater then the
agent is ?winning?, otherwise it is ?losing?.
? =
?
?
?
?
?
?
?
?
W
if
{
?
?
?
pi(s, ?
?
)Q(s, ?
?
) >
?
?
?
p?i(s, ?
?
)Q(s, ?
?
)
?
LF
otherwise
(5)
More details about Q-learning, PHC, and PHC-
WoLF can be found in (Sutton and Barto, 1998;
Bowling and Veloso, 2002).
As discussed in sections 1 and 2, single-agent
RL techniques, such as Q-learning, are not suit-
able for multi-agent RL. Nevertheless, despite its
shortcomings Q-learning has been used success-
fully for multi-agent RL (Claus and Boutilier,
1998). Indeed, as we see in section 5, Q-learning
can converge to the optimal policy for small state
spaces. However, as the state space size increases
the performance of Q-learning drops (compared to
PHC and PHC-WoLF).
4 Domain and Experimental Setup
Our domain is a resource allocation negotiation
scenario. Two agents negotiate about how to share
resources. For the sake of readability from now on
we will refer to apples and oranges.
The two agents have different goals. Also,
they have human-like constraints of imperfect in-
formation about each other; they do not know
each other?s reward function or degree of rational-
ity (during learning our agents can be irrational).
Thus a Nash equilibrium (if there exists one) can-
not be computed in advance. Agent 1 cares more
about apples and Agent 2 cares more about or-
anges. Table 1 shows the points that Agents 1
and 2 earn for each apple and each orange that they
have at the end of the negotiation.
Agent 1 Agent 2
apple 300 200
orange 200 300
Table 1: Points earned by Agents 1 and 2 for each
apple and each orange that they have at the end of
the negotiation.
Agent 1: offer-2-2 (I offer you 2 A and 2 O)
Agent 2: offer-3-0 (I offer you 3 A and 0 O)
Agent 1: offer-0-3 (I offer you 0 A and 3 O)
Agent 2: offer-4-0 (I offer you 4 A and 0 O)
Agent 1: accept (I accept your offer)
Figure 1: Example interaction between Agents 1
and 2 (A: apples, O: oranges).
We use a simplified dialogue model with two
types of speech acts: offers and acceptances. The
dialogue proceeds as follows: one agent makes an
offer, e.g., ?I give you 3 apples and 1 orange?, and
the other agent may choose to accept it or make a
new offer. The negotiation finishes when one of
the agents accepts the other agent?s offer or time
runs out.
We compare Q-learning with PHC and PHC-
WoLF. For all algorithms and experiments each
agent is rewarded only at the end of the dialogue
based on the negotiation outcome (see Table 1).
Thus the two agents have different reward func-
tions. There is also a penalty of -10 for each agent
action to ensure that dialogues are not too long.
Also, to avoid long dialogues, if none of the agents
accepts the other agent?s offers, the negotiation
finishes after 20 pairs of exchanges between the
two agents (20 offers from Agent 1 and 20 offers
from Agent 2).
An example interaction between the two agents
is shown in Figure 1. As we can see, each agent
can offer any combination of apples and oranges.
So if we haveX apples and Y oranges for sharing,
there can be (X + 1) ? (Y + 1) possible offers.
For example if we have 2 apples and 2 oranges
for sharing, there can be 9 possible offers: ?offer-
0-0?, ?offer-0-1?, ..., ?offer-2-2?. For our exper-
iments we vary the number of fruits to be shared
and choose to keep X equal to Y .
Table 2 shows our state representation, i.e., the
state variables that we keep track of with all the
possible values they can take, whereX is the num-
504
Current offer: (X + 1) ? (Y + 1) possible
values
How many times the current offer has already
been rejected: (0, 1, 2, 3, or 4)
Is the current offer accepted: yes, no
Table 2: State variables.
ber of apples and Y is the number of oranges to be
shared. The third variable is always set to ?no? un-
til one of the agents accepts the other agent?s offer.
Table 3 shows the state and action space sizes
for different numbers of apples and oranges to be
shared used in our experiments below. The num-
ber of actions includes the acceptance of an of-
fer. Table 3 also shows the number of state-action
pairs (Q-values). As we will see in section 5, even
though the number of states for each agent is not
large, it takes many iterations and high exploration
rates for convergence due to the fact that both
agents are learning at the same time and the as-
sumption of interacting with a stationary environ-
ment no longer holds. For comparison, in (English
and Heeman, 2005) the state specification for each
agent included 5 binary variables resulting in 32
possible states. English and Heeman (2005) kept
track of whether there was an offer on the table but
not of the actual value of the offer. For our task it
is essential to keep track of the offer values, which
of course results in much larger state spaces. Also,
in (English and Heeman, 2005) there were 5 possi-
ble actions resulting in 160 state-action pairs. Our
state and action spaces are much larger and fur-
thermore we explore the effect of different state
and action space sizes on convergence.
During learning the two agents interact for
5 epochs. Each epoch contains N number of
episodes. We vary N from 25,000 up to 400,000
with a step of 25,000 episodes. English and Hee-
man (2005) trained their agents for 200 epochs,
where each epoch contained 200 episodes.
We also vary the exploration rate per epoch.
In particular, in the experiments reported in sec-
tion 5.1 the exploration rate is set as follows: 0.95
for epoch 1, 0.8 for epoch 2, 0.5 for epoch 3, 0.3
for epoch 4, and 0.1 for epoch 5. Section 5.2 re-
ports results again with 5 epochs of training but a
constant exploration rate per epoch set to 0.3. An
exploration rate of 0.3 means that 30% of the time
the agent will select an action randomly.
Finally, we vary the learning rate. For PHC-
#States #Actions #State-Action
Pairs
1 A & O 40 5 200
2 A & O 90 10 900
3 A & O 160 17 2720
4 A & O 250 26 6500
5 A & O 360 37 13320
6 A & O 490 50 24500
7 A & O 640 65 41600
Table 3: State space, action space, and state-action
space sizes for different numbers of apples and or-
anges to be shared (A: apples, O: oranges).
WoLF we set ?
W
= 0.05 and ?
LF
= 0.2 (see sec-
tion 3). These values were chosen with exper-
imentation and the basic idea is that the agent
should learn faster when ?losing? and slower when
?winning?. For PHC we explore two cases. In the
first case which from now on will be referred to
as PHC-W, we set ? to be equal to ?
W
(also used
for PHC-WoLF). In the second case which from
now on will be referred to as PHC-LF, we set ?
to be equal to ?
LF
(also used for PHC-WoLF). So
unlike PHC-WoLF, PHC-W and PHC-LF do not
use a variable learning rate. PHC-W always learns
slowly and PHC-LF always learns fast.
In all the above cases, training stops after 5
epochs. Then we test the learned policies against
each other for one more epoch the size of which is
the same as the size of the epochs used for train-
ing. For example, if the policies were learned
for 5 epochs with each epoch containing 25,000
episodes, then for testing the two policies will in-
teract for another 25,000 episodes. For compari-
son, English and Heeman (2005) had their agents
interact for 5,000 dialogues during testing. To en-
sure that the policies do not converge by chance,
we run the training and test sessions 20 times each
and we report averages. Thus all results presented
in section 5 are averages of 20 runs.
5 Results
Given that Agent 1 is more interested in apples
and Agent 2 cares more about oranges, the maxi-
mum total utility solution would be the case where
each agent offers to get al the fruits it cares about
and to give its interlocutor all the fruits it does not
care about, and the other agent accepts this of-
fer. Thus, when converging to the maximum to-
tal utility solution, in the case of 4 fruits (4 ap-
505
ples and 4 oranges), the average reward of the
two agents should be 1200 minus 10 for making
or accepting an offer. For 5 fruits the average re-
ward should be 1500 minus 10, and so forth. We
call 1200 (or 1500) the convergence reward, i.e.,
the reward after converging to the maximum to-
tal utility solution if we do not take into account
the action penalty. For example, in the case of 4
fruits, if Agent 1 starts the negotiation, after con-
verging to the maximum total utility solution the
optimal interaction should be: Agent 1 makes an
offer to Agent 2, namely 0 apples and 4 oranges,
and Agent 2 accepts. Thus the reward for Agent 1
is 1190, the reward for Agent 2 is 1190, and the av-
erage reward of the two agents is also 1190. Also,
the convergence reward for Agent 1 is 1200 and
the convergence reward for Agent 2 is also 1200.
Below, in all the graphs that we provide, we
show the average distance from the convergence
reward. This is to make all graphs comparable
because in all cases the optimal average distance
from the convergence reward of the two agents
should be equal to 10 (make the optimal offer
or accept the optimal offer that the other agent
makes). The formulas for calculating the average
distance from the convergence reward are:
AD
1
=
?
n
r
j=1
|CR
1
?R
1j
|
n
r
(6)
AD
2
=
?
n
r
j=1
|CR
2
?R
2j
|
n
r
(7)
AD =
AD
1
+AD
2
2
(8)
whereCR
1
is the convergence reward for Agent 1,
R
1j
is the reward of Agent 1 for run j, CR
2
is the
convergence reward for Agent 2, and R
2j
is the
reward of Agent 2 for run j. Moreover, AD
1
is
the average distance from the convergence reward
for Agent 1, AD
2
is the average distance from the
convergence reward for Agent 2, and AD is the
average of AD
1
and AD
2
. All graphs of section 5
show AD values. Also, n
r
is the number of runs
(in our case always equal to 20). Thus in the case
of 4 fruits, we will have CR
1
=CR
2
=1200, and if
for all runs R
1j
=R
2j
=1190, then AD=10.
5.1 Variable Exploration Rate
In this section we report results with different ex-
ploration rates per training epoch (see section 4).
Q- PHC- PHC- PHC-
learning LF W WoLF
1 A & O 10.5 10 10 10
2 A & O 10.3 10.3 10 10
3 A & O 11.7 10 10 10
4 A & O 15 11.8 11.7 11.7
5 A & O 45.4 29.5 26.5 22.9
6 A & O 60.8 33.4 46.1 33.9
7 A & O 95 56 187.8 88.6
Table 4: Average distance from convergence re-
ward over 20 runs for 100,000 episodes per epoch
and for different numbers of fruits to be shared (A:
apples, O: oranges). The best possible value is 10.
Table 4 shows the average distance from the con-
vergence reward over 20 runs for 100,000 episodes
per epoch, for different numbers of fruits, and
for all four methods (Q-learning, PHC-LF, PHC-
W, and PHC-WoLF). It is clear that as the state
space becomes larger 100,000 training episodes
per epoch are not enough for convergence. Also,
for 1, 2, and 3 fruits all algorithms converge and
perform comparably. As the number of fruits in-
creases, Q-learning starts performing worse than
the multi-agent RL algorithms. For 7 fruits PHC-
W appears to perform worse than Q-learning but
this is because, as we can see in Figure 5, in this
case more than 400,000 episodes per epoch are re-
quired for convergence. Thus after only 100,000
episodes per epoch all policies still behave some-
what randomly.
Figures 2, 3, 4, and 5 show the average distance
from the convergence reward as a function of the
number of episodes per epoch during training, for
4, 5, 6, and 7 fruits respectively. For 4 fruits it
takes about 125,000 episodes per epoch and for 5
fruits it takes about 225,000 episodes per epoch for
the policies to converge. This number rises to ap-
proximately 350,000 for 6 fruits and becomes even
higher for 7 fruits. Q-learning consistently per-
forms worse than the rest of the algorithms. The
differences between PHC-LF, PHC-W, and PHC-
WoLF are insignificant, which is a bit surprising
given that Bowling and Veloso (2002) showed that
PHC-WoLF performed better than PHC in a series
of benchmark tasks. In Figures 2 and 3, PHC-LF
appears to be reaching convergence slightly faster
than PHC-W and PHC-WoLF but this is not statis-
tically significant.
506
Figure 2: 4 fruits and variable exploration rate:
Average distance from convergence reward during
testing (20 runs). The best possible value is 10.
Figure 3: 5 fruits and variable exploration rate:
Average distance from convergence reward during
testing (20 runs). The best possible value is 10.
5.2 Constant Exploration Rate
In this section we report results with a constant
exploration rate for all training epochs (see sec-
tion 4). Figures 6 and 7 show the average dis-
tance from the convergence reward as a function of
the number of episodes per epoch during training,
for 4 and 5 fruits respectively. Clearly having a
constant exploration rate in all epochs is problem-
atic. For 4 fruits, after 225,000 episodes per epoch
there is still no convergence. For comparison, with
a variable exploration rate it took about 125,000
episodes per epoch for the policies to converge.
Likewise for 5 fruits. After 400,000 episodes per
epoch there is still no convergence. For compari-
son, with a variable exploration rate it took about
225,000 episodes per epoch for convergence.
Figure 4: 6 fruits and variable exploration rate:
Average distance from convergence reward during
testing (20 runs). The best possible value is 10.
Figure 5: 7 fruits and variable exploration rate:
Average distance from convergence reward during
testing (20 runs). The best possible value is 10.
The above results show that, unlike single-agent
RL where having a constant exploration rate is
perfectly acceptable, here a constant exploration
rate does not work.
6 Conclusion and Future Work
We used single-agent RL and multi-agent RL for
learning dialogue policies in a resource allocation
negotiation scenario. Two agents interacted with
each other and both learned at the same time. The
advantage of this approach is that it does not re-
quire SUs to train against or corpora to learn from.
We compared a traditional single-agent RL al-
gorithm (Q-learning) against two multi-agent RL
algorithms (PHC and PHC-WoLF) varying the
scenario complexity (state space size), the number
507
Figure 6: 4 fruits and constant exploration rate:
Average distance from convergence reward during
testing (20 runs). The best possible value is 10.
Figure 7: 5 fruits and constant exploration rate:
Average distance from convergence reward during
testing (20 runs). The best possible value is 10.
of training episodes, and the learning and explo-
ration rates. Our results showed that Q-learning
is not suitable for concurrent learning given that
it is designed for learning against a stationary en-
vironment. Q-learning failed to converge in all
cases, except for very small state space sizes. On
the other hand, both PHC and PHC-WoLF always
converged (or in the case of 7 fruits they needed
more training episodes) and performed similarly.
We also showed that in concurrent learning very
high gradually decreasing exploration rates are re-
quired for convergence. We conclude that multi-
agent RL of dialogue policies is a promising alter-
native to using single-agent RL and SUs or learn-
ing directly from corpora.
The focus of this paper is on comparing single-
agent RL and multi-agent RL for concurrent learn-
ing, and studying the implications for convergence
and exploration/learning rates. Our next step is
testing with human users. We are particularly in-
terested in users whose behavior changes during
the interaction and continuous testing against ex-
pert repeat users, which has never been done be-
fore. Another interesting question is whether cor-
pora or SUs may still be required for designing
the state and action spaces and the reward func-
tions of the interlocutors, bootstrapping the poli-
cies, and ensuring that information about the be-
havior of human users is encoded in the resulting
learned policies. Ga?si?c et al (2013) showed that it
is possible to learn ?full? dialogue policies just via
interaction with human users (without any boot-
strapping using corpora or SUs). Similarly, con-
current learning could be used in an on-line fash-
ion via live interaction with human users. Or al-
ternatively concurrent learning could be used off-
line to bootstrap the policies and then these poli-
cies could be improved via live interaction with
human users (again using concurrent learning to
address possible changes in user behavior). These
are open research questions for future work.
Furthermore, we intend to apply multi-agent RL
to more complex negotiation domains, e.g., exper-
iment with more than two types of resources (not
just apples and oranges) and more types of actions
(not just offers and acceptances). We would also
like to compare policies learned with multi-agent
RL techniques with policies learned with SUs or
from corpora both in simulation and with human
users. Finally, we aim to experiment with differ-
ent feature-based representations of the state and
action spaces. Currently all possible deal combi-
nations are listed as possible actions and as ele-
ments of the state, which can quickly lead to very
large state and action spaces as the application be-
comes more complex (in our case as the number of
fruits increases). However, abstraction is not triv-
ial because the agents have no guarantee that the
value of a deal is a simple function of the value of
its parts, and values may differ for different agents.
Acknowledgments
Claire Nelson sadly died in May 2013. We con-
tinued and completed this work after her pass-
ing away. She is greatly missed. This work was
funded by the NSF grant #1117313.
508
References
Pieter Abbeel and Andrew Y. Ng. 2004. Apprentice-
ship learning via inverse reinforcement learning. In
Proc. of the International Conference on Machine
Learning, Bannf, Alberta, Canada.
Hua Ai and Diane Litman. 2008. Assessing dialog sys-
tem user simulation evaluation measures using hu-
man judges. In Proc. of the Annual Meeting of the
Association for Computational Linguistics, Colum-
bus, Ohio, USA.
Michael Bowling and Manuela Veloso. 2002. Multi-
agent learning using a variable learning rate. Artifi-
cial Intelligence, 136(2):215?250.
L. Busoniu, R. Babuska, and B. De Schutter. 2008.
A comprehensive survey of multiagent reinforce-
ment learning. IEEE Transactions on Systems, Man,
and Cybernetics, Part C: Applications and Reviews,
38(2):156?172.
Senthilkumar Chandramohan, Matthieu Geist, Fabrice
Lef`evre, and Olivier Pietquin. 2012. Co-adaptation
in spoken dialogue systems. In Proc. of the Interna-
tional Workshop on Spoken Dialogue Systems, Paris,
France.
Min Chi, Kurt VanLehn, Diane Litman, and Pamela
Jordan. 2011. Empirically evaluating the ap-
plication of reinforcement learning to the induc-
tion of effective and adaptive pedagogical strategies.
User Modeling and User-Adapted Interaction, 21(1-
2):137?180.
Caroline Claus and Craig Boutilier. 1998. The dynam-
ics of reinforcement learning in cooperative multia-
gent systems. In Proc. of the National Conference
on Artificial Intelligence.
Heriberto Cuay?ahuitl and Nina Dethlefs. 2012. Hier-
archical multiagent reinforcement learning for coor-
dinating verbal and nonverbal actions in robots. In
Proc. of the ECAI Workshop on Machine Learning
for Interactive Systems, Montpellier, France.
Lucie Daubigney, Matthieu Geist, Senthilkumar Chan-
dramohan, and Olivier Pietquin. 2012. A compre-
hensive reinforcement learning framework for dia-
logue management optimization. IEEE Journal of
Selected Topics in Signal Processing, 6(8):891?902.
Yaakov Engel, Shie Mannor, and Ron Meir. 2005. Re-
inforcement learning with Gaussian processes. In
Proc. of the International Conference on Machine
Learning, Bonn, Germany.
Michael S. English and Peter A. Heeman. 2005.
Learning mixed initiative dialogue strategies by us-
ing reinforcement learning on both conversants. In
Proc. of the Conference on Empirical Methods in
Natural Language Processing, Vancouver, Canada.
M. Ga?si?c, Filip Jur?c???cek, Blaise Thomson, Kai Yu, and
Steve Young. 2011. On-line policy optimisation
of spoken dialogue systems via live interaction with
human subjects. In Proc. of the IEEE Automatic
Speech Recognition and Understanding Workshop,
Big Island, Hawaii, USA.
Milica Ga?si?c, Matthew Henderson, Blaise Thomson,
Pirros Tsiakoulis, and Steve Young. 2012. Pol-
icy optimisation of POMDP-based dialogue systems
without state space compression. In Proc. of the
IEEE Workshop on Spoken Language Technology,
Miami, Florida, USA.
M. Ga?si?c, C. Breslin, M. Henderson, D. Kim,
M. Szummer, B. Thomson, P. Tsiakoulis, and
S. Young. 2013. On-line policy optimisation of
Bayesian spoken dialogue systems via human inter-
action. In Proc. of the International Conference on
Acoustics, Speech and Signal Processing, Vancou-
ver, Canada.
Kallirroi Georgila and David Traum. 2011. Reinforce-
ment learning of argumentation dialogue policies in
negotiation. In Proc. of Interspeech, Florence, Italy.
Kallirroi Georgila, James Henderson, and Oliver
Lemon. 2006. User simulation for spoken dialogue
systems: Learning and evaluation. In Proc. of Inter-
speech, Pittsburgh, Pennsylvania, USA.
Kallirroi Georgila, Maria K. Wolters, and Johanna D.
Moore. 2010. Learning dialogue strategies from
older and younger simulated users. In Proc. of
the Annual SIGdial Meeting on Discourse and Di-
alogue, Tokyo, Japan.
Kallirroi Georgila. 2013. Reinforcement learning of
two-issue negotiation dialogue policies. In Proc. of
the Annual SIGdial Meeting on Discourse and Dia-
logue, Metz, France.
Peter A. Heeman. 2009. Representing the reinforce-
ment learning state in a negotiation dialogue. In
Proc. of the IEEE Automatic Speech Recognition
and Understanding Workshop, Merano, Italy.
James Henderson, Oliver Lemon, and Kallirroi
Georgila. 2008. Hybrid reinforcement/supervised
learning of dialogue policies from fixed datasets.
Computational Linguistics, 34(4):487?511.
Junling Hu and Michael P. Wellman. 1998. Multia-
gent reinforcement learning: Theoretical framework
and an algorithm. In Proc. of the International Con-
ference on Machine Learning, Madison, Wisconsin,
USA.
Filip Jur?c???cek, Blaise Thomson, and Steve Young.
2012. Reinforcement learning for parameter esti-
mation in statistical spoken dialogue systems. Com-
puter Speech and Language, 26(3):168?192.
Michail G. Lagoudakis and Ronald Parr. 2003. Least-
squares policy iteration. Journal of Machine Learn-
ing Research, 4:1107?1149.
509
Oliver Lemon, Kallirroi Georgila, and James Hender-
son. 2006. Evaluating effectiveness and portabil-
ity of reinforcement learned dialogue strategies with
real users: The TALK TownInfo evaluation. In Proc.
of the IEEEWorkshop on Spoken Language Technol-
ogy, Palm Beach, Aruba.
Lihong Li, Jason D. Williams, and Suhrid Balakrish-
nan. 2009. Reinforcement learning for dialog man-
agement using least-squares policy iteration and fast
feature selection. In Proc. of Interspeech, Brighton,
United Kingdom.
Michael L. Littman. 1994. Markov games as a frame-
work for multi-agent reinforcement learning. In
Proc. of the International Conference on Machine
Learning, New Brunswick, New Jersey, USA.
Yi Ma. 2013. User goal change model for spoken dia-
log state tracking. In Proc. of the NAACL-HLT Stu-
dent Research Workshop, Atlanta, Georgia, USA.
Teruhisa Misu, Komei Sugiura, Kiyonori Ohtake,
Chiori Hori, Hideki Kashioka, Hisashi Kawai, and
Satoshi Nakamura. 2010. Modeling spoken deci-
sion making dialogue and optimization of its dia-
logue strategy. In Proc. of the Annual SIGdial Meet-
ing on Discourse and Dialogue, Tokyo, Japan.
Teruhisa Misu, Kallirroi Georgila, Anton Leuski, and
David Traum. 2012. Reinforcement learning of
question-answering dialogue policies for virtual mu-
seum guides. In Proc. of the Annual SIGdial Meet-
ing on Discourse and Dialogue, Seoul, South Korea.
Elnaz Nouri, Kallirroi Georgila, and David Traum.
2012. A cultural decision-making model for nego-
tiation based on inverse reinforcement learning. In
Proc. of the Cognitive Science Conference, Sapporo,
Japan.
P. Paruchuri, N. Chakraborty, R. Zivan, K. Sycara,
M. Dudik, and G. Gordon. 2009. POMDP based
negotiation modeling. In IJCAI Workshop on Mod-
eling Intercultural Collaboration and Negotiation,
Pasadena, California, USA.
Olivier Pietquin and Helen Hastie. 2013. A survey
on metrics for the evaluation of user simulations.
Knowledge Engineering Review, 28(1):59?73.
Carl Edward Rasmussen and Christopher K. I.
Williams. 2006. Gaussian Processes for Machine
Learning. MIT Press.
Verena Rieser, Simon Keizer, Xingkun Liu, and Oliver
Lemon. 2011. Adaptive information presentation
for spoken dialogue systems: Evaluation with hu-
man subjects. In Proc. of the European Workshop
on Natural Language Generation, Nancy, France.
Jost Schatzmann, Karl Weilhammer, Matt Stuttle, and
Steve Young. 2006. A survey of statistical user sim-
ulation techniques for reinforcement-learning of dia-
logue management strategies. Knowledge Engineer-
ing Review, 21(2):97?126.
Satinder Singh, Diane Litman, Michael Kearns, and
Marilyn Walker. 2002. Optimizing dialogue man-
agement with reinforcement learning: Experiments
with the NJFun system. Journal of Artificial Intelli-
gence Research, 16:105?133.
Richard S. Sutton and Andrew G. Barto. 1998. Rein-
forcement Learning: An Introduction. MIT Press.
Joel R. Tetreault and Diane J. Litman. 2008. A rein-
forcement learning approach to evaluating state rep-
resentations in spoken dialogue systems. Speech
Communication, 50(8-9):683?696.
Blaise Thomson and Steve Young. 2010. Bayesian
update of dialogue state: A POMDP framework for
spoken dialogue systems. Computer Speech and
Language, 24(4):562?588.
Jason D. Williams and Steve Young. 2007. Scal-
ing POMDPs for spoken dialog management. IEEE
Transactions on Audio, Speech, and Language Pro-
cessing, 15(7):2116?2129.
510
Practical Grammar-Based NLG from Examples
David DeVault and David Traum and Ron Artstein
USC Institute for Creative Technologies
13274 Fiji Way
Marina del Rey, CA 90292
{devault,traum,artstein}@ict.usc.edu
Abstract
We present a technique that opens up
grammar-based generation to a wider range
of practical applications by dramatically re-
ducing the development costs and linguis-
tic expertise that are required. Our method
infers the grammatical resources needed for
generation from a set of declarative exam-
ples that link surface expressions directly to
the application?s available semantic represen-
tations. The same examples further serve to
optimize a run-time search strategy that gener-
ates the best output that can be found within an
application-specific time frame. Our method
offers substantially lower development costs
than hand-crafted grammars for application-
specific NLG, while maintaining high output
quality and diversity.
1 Introduction
This paper presents a new example-based genera-
tion technique designed to reduce the development
costs and linguistic expertise needed to integrate a
grammar-based generation component into an ex-
isting application. We believe this approach will
broaden the class of applications in which grammar-
based generation may feasibly be deployed.
In principle, grammar-based generation offers
significant advantages for many applications, when
compared with simpler template-based or canned
text output solutions, by providing productive cov-
erage and greater output variety. However, realiz-
ing these advantages can require significant devel-
opment costs (Busemann and Horacek, 1998).
One possible strategy is to exploit a wide-
coverage realizer that aims for applicability in mul-
tiple application domains (White et al, 2007; Cahill
and van Genabith, 2006; Zhong and Stent, 2005;
Langkilde-Geary, 2002; Langkilde and Knight,
1998; Elhadad, 1991). These realizers provide
a sound wide-coverage grammar (or robust wide-
coverage language model) for free, but demand a
specific input format that is otherwise foreign to
an existing application. Unfortunately, the devel-
opment burden of implementing the translation be-
tween the system?s available semantic representa-
tions and the required input format can be quite sub-
stantial (Busemann and Horacek, 1998). Indeed, im-
plementing the translation might require as much ef-
fort as would be required to build a simple custom
generator; cf. (Callaway, 2003). Thus, there cur-
rently are many applications where using a wide-
coverage generator remains impractical.
Another strategy is for system builders to hand
craft an application-specific grammar for genera-
tion. This approach can be initially attractive to
system builders because it allows syntactic cover-
age and semantic modeling to be tailored directly
to application needs. However, writing grammati-
cal rules by hand ultimately requires a painstaking,
time-consuming effort by a developer who has de-
tailed linguistic knowledge as well as detailed appli-
cation knowledge. Further, the resulting coverage is
inevitably limited to the set of linguistic construc-
tions that have been selected for careful modeling.
A third strategy is to use an example-based ap-
proach (Wong and Mooney, 2007; Stone, 2003;
Varges and Mellish, 2001) in which the connection
77
between available application semantic representa-
tions and desired output utterances is specified by
example. Example-based approaches aim to allow
system builders to specify a productive generation
capacity while leaving the representations and rea-
soning that underlie that productive capacity mostly
implicit in a set of training examples. This method-
ology insulates system builders from the detailed ex-
pertise and technical infrastructure needed to imple-
ment the productive capacity directly, and has made
example-based approaches attractive not only in text
generation but also in related areas such as concate-
native speech synthesis and motion capture based
animation; see, e.g., (Stone et al, 2004).
The technique we present in this paper is a new
example-based approach to specifying application-
specific text generation. As in other hand-crafted
and example-based approaches, our technique al-
lows syntactic coverage and semantic modeling to
follow the needs and available semantic representa-
tions in an application. One contribution of our tech-
nique is to relieve the generation content author of
the burden of manual syntactic modeling by lever-
aging an off-the-shelf parser; defects in the syntax
provided by the parser are effectively overcome us-
ing a machine learning technique. Additionally, our
technique organizes the authoring task in a way that
relieves the generation author of carefully modeling
the connections between particular syntactic con-
structions and available semantic representations.
Together, we argue, these features dramatically
reduce the linguistic expertise and other develop-
ment costs that are required to integrate a grammar-
based generation component into an existing system.
In a case study application, we show that our ap-
proach allows an application developer who lacks
detailed linguistic knowledge to extend grammatical
coverage at an expense of less than one minute per
additional lexical entry.
2 Case Study: Doctor Perez
Our approach has been tested as a replacement for
the generation component of interactive virtual hu-
mans used for social training purposes (Swartout et
al., 2006). Virtual humans are embodied conversa-
tional agents that play the role of people in simula-
tions or games. The case study we present in this
paper is the generation of output utterances for a
particular virtual human, Doctor Perez, who is de-
signed to teach negotiation skills in a multi-modal,
multi-party, non-team dialogue setting (Traum et al,
2008). The human trainee who talks to the doctor
plays the role of a U.S. Army captain named Cap-
tain Kirk. The design goals for Doctor Perez create
a number of requirements for a practical NLG com-
ponent. We briefly summarize these requirements
here; see (DeVault et al, 2008) for more details.
Doctor Perez has a relatively rich internal mental
state including beliefs, goals, plans, and emotions.
He uses an attribute-value matrix (AVM) semantic
representation to describe an utterance as a set of
core speech acts and other dialogue acts. Speech
acts generally have semantic contents that describe
propositions and questions about states and actions
in the domain. To facilitate interprocess communi-
cation, and statistical processing, this AVM structure
is linearized into a ?frame? of key values in which
each non-recursive terminal value is paired with a
path from the root to the final attribute. Figure 1
shows a typical frame. See (Traum, 2003) for addi-
tional details and examples of this representation.
While only hundreds of frames currently arise in
actual dialogues, the number of potential frames is
orders of magnitude larger, and it is difficult to pre-
dict in advance which frames might occur. The ut-
terances that realize these frames need to take a va-
riety of syntactic forms, including simple declar-
ative sentences, various modal constructions relat-
ing to hypothetical actions or plans, yes/no and wh-
questions, and abbreviated dialogue forms such as
elliptical clarification and repair requests, ground-
ing, and turn-taking utterances. Highly fluent out-
put is not a necessity for this character, since Doc-
tor Perez is designed to simulate a non-native En-
glish speaker. However, in order to support com-
pelling real-time conversational interaction and ef-
fective training, the generation module must be able
to identify an utterance for Doctor Perez to use
within approximately 200ms on modern hardware.
Finally, the development team for Doctor Perez?s
language capabilities includes approximately 10
programmers, testers, linguists, and computational
linguists. Wherever possible, it is better if any de-
veloper can improve any aspect of Doctor Perez?s
language processing; e.g., if a programmer discov-
78
ers a bug or disfluency in the NLG output, it is better
if she can fix it directly rather than requiring a (com-
putational) linguist to do so.
3 Technical Approach
Our approach builds on recently developed tech-
niques in statistical parsing, lexicalized syntax mod-
eling, generation with lexicalized grammars, and
search optimization to automatically construct all
the resources needed for a high-quality run-time
generation component. In particular, we leverage the
increasing availability of off-the-shelf parsers such
as (Charniak, 2001; Charniak, 2005) to automati-
cally (or semi-automatically) assign syntactic anal-
yses to a set of suggested output sentences. We
then draw on lexicalization techniques for statistical
language models (Magerman, 1995; Collins, 1999;
Chiang, 2000; Chiang, 2003) to induce a probabilis-
tic, lexicalized tree-adjoining grammar that supports
the derivation of all the suggested output sentences,
and many others besides.
The final step is to use the training examples to
learn an effective search policy so that our run-time
generation component can find good output sen-
tences in a reasonable time frame. In particular, we
use variants of existing search optimization (Daum?
and Marcu, 2005) and ranking algorithms (Collins
and Koo, 2005) to train our run-time component to
find good outputs within a specified time window;
see also (Stent et al, 2004; Walker et al, 2001). The
result is a run-time component that treats generation
as an anytime search problem, and is thus suitable
for applications in which a time/performance trade-
off is necessary (such as real-time dialogue).
3.1 Specification of Training Examples
Each training example in our approach speci-
fies a target output utterance (string), its syn-
tax, and a set of links between substrings within
the utterance and system semantic representa-
tions. Formally, a training example takes the form
(u, syntax(u), semantics(u)). We will illustrate
this format using the training example in Figure 1. In
this example, the generation content author suggests
the output utterance u = we don?t have medical
supplies here captain. Each utterance u is accom-
panied by syntax(u), a syntactic analysis in Penn
Treebank format (Marcus et al, 1994). In the fig-
ure, we show two alternative syntactic analyses that
might be specified: one is the uncorrected output of
the Charniak parser on this sentence, and the other
a hand-corrected version of that parse; we evaluate
the utility of this hand correction in Section 4.
To represent the meaning of utterances, our ap-
proach assumes that the system provides some set
M = {m1, ...,mj} of semantic representations.
The meaning of any individual utterance is then
identified with some subset of M . For Doctor Perez,
M comprises the 232 distinct key-value pairs that
appear in the system?s various generation frames. In
this example, the utterance?s meaning is captured by
the 8 key-value pairs indicated in the figure.
Our approach requires the generation content
author to link these 8 key-value pairs to con-
tiguous surface expressions within the utterance.
The technique is flexible about which surface ex-
pressions are chosen (e.g. they need not corre-
spond to constituent boundaries); however, they do
need to be compatible with the way the syntactic
analysis tokenizes the utterance, as follows. Let
t(u) = ?t1, ..., tn? be the terminals in the syn-
tactic analysis, in left-to-right order. Formally,
semantics(u) = {(s1,M1), ..., (sk,Mk)}, where
t(u) = s1@ ? ? ?@sk (with @ denoting concatena-
tion), and where Mi ? M for all i ? 1..k. In this
example, the surface expression we don?t, which to-
kenizes as ?we,do,n?t?, is connected to key-values
that indicate a negative polarity assertion.
This training example format has two features that
are crucial to our approach. First, the semantics of
an utterance is specified independently of its syntax.
This greatly reduces the amount of linguistic exper-
tise a generation content author needs to have. It
also allows making changes to the underlying syn-
tax without having to re-author the semantic links.
Second, the assignment of semantic representa-
tions to surface expressions must span the entire ut-
terance. No words or expressions can be viewed as
?meaningless?. This is essential because, otherwise,
the semantically motivated search algorithm used in
generation has no basis on which to include those
particular expressions when it constructs its output
utterance. Many systems, including Doctor Perez,
lack some of the internal representations that would
be necessary to specify semantics down to the lex-
79
Utterance we don?t have medical supplies here captain
Syntax
cat: SA??
cat: S??
cat: NP??
pos: PRP??
we
cat: VP??
pos: AUX??
do
pos: RB??
n?t
cat: VP??
pos: AUX??
have
cat: NP??
pos: JJ??
medical
pos: NNS??
supplies
cat: ADVP??
pos: RB??
here
cat: NP??
pos: NN??
captain
cat: SA??
cat: S??
cat: S??
cat: NP??
pos: PRP??
we
cat: VP??
pos: AUX??
do
pos: RB??
n?t
cat: VP??
pos: AUX??
have
cat: NP??
pos: JJ??
medical
pos: NNS??
supplies
cat: VP??
cat: ADVP??
pos: RB??
here
pos: VBP??
captain
(corrected Charniak parse) or (uncorrected Charniak parse)
Semantics
we do n?t . . . . . . .
{
have . . . . . . . . . . . . .
medical supplies . .
here . . . . . . . . . . . . .
captain . . . . . . . .
?
?
?
semantic frame
speech-act.action = assert
speech-act.content.polarity = negative
speech-act.content.attribute = resourceAttribute
speech-act.content.value = medical-supplies
speech-act.content.object-id = market
addressee = captain-kirk
dialogue-act.addressee = captain-kirk
speech-act.addressee = captain-kirk
Figure 1: A generation training example for Doctor Perez. If uncorrected syntax is used, the generation content author
only writes the utterance and the links to the semantic frame.
ical level. An important feature of our approach is
that it allows an arbitrary semantic granularity to be
employed, by mapping the representations available
in the system to appropriate multi-word chunks.
3.2 Automatic Grammar Induction
We adopt essentially the probabilistic tree-adjoining
grammar (PTAG) formalism and grammar induc-
tion technique of (Chiang, 2003). Our approach
makes three modifications, however. First, while
Chiang?s model includes both full adjunction and
sister adjunction operations, our grammar has only
sister adjunction (left and right), exactly as in the
TAGLET grammar formalism of (Stone, 2002). Sec-
ond, to support lexicalization at an arbitrary gran-
ularity, we allow Chiang?s tree templates to be as-
sociated with more than one lexical anchor. Third,
to unify syntactic and semantic reasoning in search,
we augment lexical anchors with semantic informa-
tion. Formally, wherever Chiang?s model has a lex-
ical anchor w, ours has a pair (?w1, ..., wn?,M ?),
where M ? ? M is connected to lexical anchors
?w1, ..., wn? by the generation content author, as in
Figure 1. The result is that the derivation probabili-
ties the grammar assigns depend not only on the im-
plicated syntactic structures and lexical anchors but
also on the senses of those lexical anchors in appli-
cation terms.
We induce our grammar from training exam-
ples such as Figure 1 using heuristics to assign
derivations to the examples, exactly as in (Chiang,
2003). The process proceeds in two stages. In
the first stage, a collection of rules is used to au-
tomatically ?decorate? the training syntax with a
number of features. These include deciding the
lexical anchor(s) for each non-terminal constituent
and assigning complement/adjunct status for non-
terminals which are not on their parent?s lexical-
ization path; see (Magerman, 1995; Chiang, 2003;
Collins, 1999). In addition, we deterministically add
features to improve several grammatical aspects, in-
cluding (1) enforcing verb inflectional agreement in
derived trees, (2) enforcing consistency in the finite-
ness of VP and S complements, and (3) restricting
subject/direct object/indirect object complements to
play the same grammatical role in derived trees.
In the second stage, the complements and ad-
juncts in the decorated trees are incrementally re-
80
syntax:
cat: SA??
fin: other,?? cat: S
cat: NP,?? apr: VBP,
apn: other??
pos: PRP??
we
fin: yes,?? cat: VP
apn: other,?? pos: VBP
do
pos: RB??
n?t
fin: yes,?? cat: VP,
gra: obj1??
fin: yes,?? cat: VP,
gra: obj1??
pos: VBP??
have
cat: NP,?? gra: obj1
operations: initial tree comp
semantics: speech-act.action = assert
speech-act.content.polarity = negative
speech-act.content.attribute = resourceAttribute
syntax:
cat: NP,?? apr: VBP,
gra: obj1,?? apn: other
pos: JJ??
medical
pos: NNS??
supplies
cat: ADVP,?? gra: adj
pos: RB??
here
cat: NP,?? apr: VBZ,
gra: adj,?? apn: 3ps
pos: NN??
captain
operations: comp left/right adjunction left/right adjunction
semantics: speech-act.content.value =
medical-supplies
speech-act.content.object-id =
market
addressee = captain-kirk
dialogue-act.addressee = captain-kirk
speech-act.addressee = captain-kirk
Figure 2: The linguistic resources inferred from the training example in Figure 1.
moved, yielding the reusable linguistic resources in
the grammar, as illustrated in Figure 2, as well as
the maximum likelihood estimates needed to com-
pute operation probabilities.
Our approach uses this induced grammar to treat
generation as a search problem: given a desired se-
mantic representation M ? ? M , use the grammar
to incrementally construct an output utterance u that
expressesM ?. We treat generation as anytime search
by accruing multiple goal states up until a specified
timeout (for Doctor Perez: 200ms) and returning a
list of alternative outputs ranked by their derivation
probabilities.
3.3 Optimizing the Search Strategy
The search space created by a grammar induced in
this way is too large to be searched exhaustively in
most applications. The solution we have developed
is a beam search strategy that uses weighted features
to rank alternative grammatical expansions at each
step. In particular, the beam size and structure is op-
timized so that, with high probability, the beam can
be searched exhaustively before the timeout.1 The
second step of automated processing, then, is a train-
ing problem of finding weighted features such that
1For Doctor Perez, we use a wider beam for initial trees,
since the Doctor?s semantic representation is particularly im-
poverished at the level of main verbs. At search depths > 1, we
use beam size 1 (i.e. greedy search).
for every training problem, nodes that lead to good
generation output are ranked highly enough by those
features to make it into the beam.
We use domain-independent rules to automati-
cally define a set of features that could be heuris-
tically useful for a given induced grammar. These
include features for various syntactic structures and
operations, numbers of undesired and desired mean-
ings of different types added by an expansion,
derivation probabilities, etc. (For Doctor Perez,
this yields about 600 features.) Our training algo-
rithm is based on the search optimization algorithm
of (Daum? and Marcu, 2005), which updates fea-
ture weights when mistakes are made during search
on training examples. For the weight update step,
we use the boosting approach of (Collins and Koo,
2005), which performs feature selection and iden-
tifies weight values that improve the ranking of al-
ternative derivation steps when mistakes are made
during search. We discuss the resulting success rate
and quality in the next section.
4 Cost/Benefit Analysis
The motivation that underlies our technical approach
is to reduce the development costs and linguistic ex-
pertise needed to develop a grammar-based genera-
tion component for an existing system. In this sec-
tion, we assess the progress we have made by ana-
81
lyzing the use of our approach for Doctor Perez.
Method. We began with a sample of 220 in-
stances of frames that Doctor Perez?s dialogue man-
ager had requested of the generation component in
previous dialogues with users. Each frame was as-
sociated with a hand-authored target output utter-
ance. We then constructed two alternative training
examples, in the format specified in Section 3.1, for
each frame. One example had uncorrected output
of the Charniak parser for the syntax, and another
had hand-corrected parser output (see Figure 1). The
connections between surface expressions and frame
key-value pairs were identical in both uncorrected
and corrected training sets.
We then built two generators using the two sets
of training examples. We used 90% of the data for
training and held out 10% for testing. The genera-
tors sometimes failed to find a successful utterance
within the 200ms timeout. For example, the success
rate of the version of our generator trained on uncor-
rected syntax was 96.0% for training examples and
81.8% for test examples.
Quality of generated output. To assess output
quality, 5 system developers rated each of 494 utter-
ances, in relation to the specific frame for which it
was produced, on a single 1 (?very bad?) to 5 (?very
good?) scale. The 494 utterances included all of the
hand-authored (suggested) utterances in the training
examples. They also included all the top-ranked ut-
terances that were successfully generated by the two
generators. We asked our judges to make an over-
all assessment of output quality, incorporating both
accuracy and fluency, for the Doctor Perez charac-
ter. Judges were blind to the conditions under which
utterances were produced. We discuss additional de-
tails of this rating task in (DeVault et al, 2008).
The judges achieved a reliability of ? = 0.708
(Krippendorff, 1980); this value shows that agree-
ment is well above chance, and allows for tentative
conclusions. We ran a small number of planned
comparisons on these ratings. Surprisingly, we
found no significant difference between generated
output trained on corrected and uncorrected syntax
(t(29) = 0.056, p > 0.9 on test items, t(498) =
?1.1, p > 0.2 on all items).2 However, we did
2The distribution of ratings across utterances is not normal;
to validate our results we accompanied each t-test by a non-
parametric Wilcoxon rank sum test, and significance always fell
Hand-authored (N = 1099)
Generated:
Training input (N = 949)
Test input (N = 90)
Rating
Fr
eq
ue
nc
y
(%
)
0
10
20
30
40
50
60
1 2 3 4 5
Figure 3: Observed rating frequencies for hand-authored
vs. generated utterances (uncorrected syntax).
find that hand-authored utterances (mean rating 4.4)
are significantly better (t(388) = 5.9, p < 0.001)
than generated utterances (mean rating 3.8 for un-
corrected syntax). These ratings are depicted in Fig-
ure 3. While the figure suggests a slight reduction in
quality for generated output for test frames vs. train-
ing frames, we did not find a significant difference
between the two (t(19) = 1.4228, p > 0.15).
Variety of generated output. In general our any-
time algorithm delivers a ranked list of alternative
outputs. While in this initial study our judges rated
only the highest ranked output generated for each
frame, we observed that many of the lower ranked
outputs are of relatively high quality. For example,
Figure 4 shows a variety of alternative outputs that
were generated for two of Doctor Perez?s training
examples. Many of these outputs are not present as
hand-authored utterances (for any frame); this illus-
trates the potential of our approach to provide a va-
riety of alternative outputs or paraphrases, which in
some applications may be useful even for meanings
for which an example utterance is hand-authored.
Figure 5 shows the overall distribution in the number
of outputs returned for Doctor Perez.
Development costs. The development costs in-
cluded implementation of the approach and specifi-
cation of Doctor Perez?s training set. Implementa-
in the same general range.
82
Rank Time (ms) Novel?
1 16 no the clinic is up to standard captain
2 94 no the clinic is acceptable captain (hand-authored for this input)
3 78 yes the clinic should be in acceptable condition captain
4 16 yes the clinic downtown is currently acceptable captain
5 78 yes the clinic should agree in an acceptable condition captain
1 94 no there are no medical supplies downtown captain
2 172 no we don?t have medical supplies downtown captain
3 125 yes well captain i do not think downtown there are medical supplies
4 16 yes i do not think there are medical supplies downtown captain
Figure 4: All the utterances generated (uncorrected syntax) for two examples. Rank is determined by derivation
probability. Outputs marked as novel are different from any suggested output for any training example.
Number of successful outputs
Fr
eq
ue
nc
y
(%
)
0
10
20
30
0 1 2 3 4 5 6 7 8 9
Figure 5: Variety of outputs for each input.
tion required an effort of approximately six person
months. The developer who carried out the imple-
mentation initially had no familiarity with the Doc-
tor Perez domain, so part of this time was spent un-
derstanding Doctor Perez and his available seman-
tic representations. The bulk of the development
time was spent implementing the grammar induction
and training processes. Grammar induction included
implementing the probabilistic grammar model and
writing about 40 rules that are used to extract gram-
matical entries from the training examples. Of these
40 rules, only 3 are specific to Doctor Perez.3 The
remainder are broadly applicable to syntactic anal-
yses in Penn Treebank format, and thus we expect
they would transfer to applications of our approach
in other domains. Similarly, the training algorithms
are entirely domain neutral and could be expected to
transfer well to additional domains.
Specification of Doctor Perez?s training data took
3These 3 rules compensate for frequent errors in Charniak
parser output for the words captain, elder, and imam, which are
often used to signal the addressee of Doctor Perez?s utterances.
about 6 hours, or about 1.6 minutes per training ex-
ample. This time included hand correction of syn-
tactic analyses generated by the Charniak parser and
definition of semantic links between surface expres-
sions and frame key-value pairs. Since we found
that hand-correcting syntax does not improve out-
put quality, this 1.6 minutes/example figure over-
estimates the authoring time required by our ap-
proach. The remaining work lies in defining the se-
mantic links. For Doctor Perez, approximately half
of the semantic links were automatically assigned
with simple ad hoc scripts.4 The semantic linking
process might be further sped up through a stream-
lined authoring interface offering additional automa-
tion, or even using a machine learning approach to
suggest appropriate links.
Linguistic expertise required. Since we found
that hand-correcting syntax does not improve output
quality, a developer who wishes to exploit our ap-
proach may use the Charniak parser to supply the
syntactic model for the domain. Thus, while one
developer with linguistic expertise is required to im-
plement the approach, anybody on the application
team can contribute by hand authoring additional ut-
terances and defining semantic links. The benefit of
this authoring effort is the ability to generate high
quality output for many novel semantic inputs.
Cost/benefit. The grammar induced from the 198
training examples (with uncorrected syntax) con-
tains 426 lexical entries of the type depicted in Fig-
ure 2. These 426 lexical entries were produced auto-
matically from about 6 hours worth of authoring ef-
4Time to compose these scripts is included in the 1.6 min-
utes/example.
83
fort together with domain-neutral algorithms. This
translates to a rate of grammar expansion of less
than 1 minute per lexical entry, on average, for this
small application-specific grammar. This constitutes
a dramatic improvement over our previous experi-
ence hand-crafting grammars. It would be challeng-
ing for an expert to specify a lexical entry such as
those in Figure 2 in under one minute (and probably
impossible for someone lacking detailed linguistic
knowledge). In our experience, however, the bulk
of development lies in additional time spent con-
sidering and investigating possible interactions be-
tween lexical entries in generation. Our technique
helps with both problems: the grammar induction
streamlines the specification of lexical entries, and
the training removes the need for a developer to
manually trace through the various complex inter-
actions between lexical entries during generation.
5 Limitations
Currently, we do not support semantic links from
non-contiguous expressions, which means a desired
output like ?we rely heavily on medical supplies?
would be difficult to annotate if rely...on corresponds
to a single semantic representation. This is not an in-
trinsic limitation to our general approach, but rather
a simplification in our initial implementation.
As discussed in Section 3.2, our grammar induc-
tion process adds syntactic features related to verb
inflection, finiteness, and grammatical role to the in-
ferred lexical entries. Such features improve the flu-
ency and accuracy of output derived with the gram-
mar. While we believe such features can always be
assigned using domain-independent rules, develop-
ing these rules requires linguistic expertise, and it
is likely that additional rules and features (not yet
implemented) would improve coverage of linguistic
phenomena such as control verbs, various kinds of
coordination, and relative clauses, inter alia.
A more entrenched limitation of our approach
is its assumption that the generator does not need
context as a separate input. This means, for ex-
ample, that our approach cannot generate referring
expressions (by selecting disambiguating semantic
properties); rather, all semantic properties must be
pre-selected and included in the generation request.
Generation of anaphoric expressions is also limited,
since contextual ambiguities are not considered.
6 Related Work
To our knowledge, this is the first implemented
generation technique that does all three of the fol-
lowing: directly interfaces to existing application
semantic representations, infers a phrase structure
grammar from examples, and does not require hand-
authored syntax as input. (Varges and Mellish,
2001) also aims to reduce the authoring burden of
domain-specific generation; however, they seem to
use a special purpose semantic annotation rather
than pre-existing application semantics, and their
task is defined in terms of the Penn Treebank, so
hand-authored syntax is used as input. (Wong and
Mooney, 2007) also interfaces to existing applica-
tion semantics, and does not require hand-authored
syntax as input. Their technique infers a syn-
chronous grammar in which the hierarchical linguis-
tic analysis is isomorphic to the hierarchy in the ap-
plication semantics, and differs from phrase struc-
ture. It would be interesting to compare their out-
put quality with ours; their automated alignment of
words to semantics might also provide a way to fur-
ther reduce the authoring burden of our approach.
7 Conclusion and Future Work
We have presented a new example-based approach
to specifying text generation for an existing appli-
cation. We have used a cost/benefit analysis to ar-
gue that our approach offers productive coverage
and high-quality output with less linguistic expertise
and lower development costs than building a hand-
crafted grammar. In future work, we will evaluate
our approach in additional application settings, and
study the performance of our approach as the size
and scope of the training set grows.
Acknowledgments
Thanks to our anonymous reviewers, Arno Hartholt,
Susan Robinson, Thomas Russ, Chung-chieh Shan,
andMatthew Stone. This work was sponsored by the
U.S. Army Research, Development, and Engineer-
ing Command (RDECOM), and the content does not
necessarily reflect the position or the policy of the
Government, and no official endorsement should be
inferred.
84
References
S. Busemann and H. Horacek. 1998. A flexible shallow
approach to text generation. In Proceedings of INLG,
pages 238?247.
Aoife Cahill and Josef van Genabith. 2006. Robust
PCFG-based generation using automatically acquired
LFG approximations. In ACL, pages 1033?1040.
C. B. Callaway. 2003. Evaluating coverage for large
symbolic NLG grammars. Proceedings of IJCAI.
E. Charniak. 2001. Immediate-head parsing for lan-
guage models. In ACL, pages 124?131, Morristown,
NJ, USA. Association for Computational Linguistics.
E. Charniak. 2005. ftp://ftp.cs.brown.edu/pub/nlparser/
parser05Aug16.tar.gz.
D. Chiang. 2000. Statistical parsing with an
automatically-extracted tree adjoining grammar. In
ACL ?00: Proceedings of the 38th Annual Meeting
on Association for Computational Linguistics, pages
456?463, Morristown, NJ, USA. Association for Com-
putational Linguistics.
D. Chiang. 2003. Statistical parsing with an automat-
ically extracted tree adjoining grammar. In R. Bod,
R. Scha, and K. Sima?an, editors, Data Oriented Pars-
ing, pages 299?316. CSLI Publications, Stanford.
M. Collins and T. Koo. 2005. Discriminative reranking
for natural language parsing. Computational Linguis-
tics, 31(1):25?70.
M. Collins. 1999. Head-Driven Statistical Models for
Natural Language Parsing. Ph.D. dissertation, Uni-
versity of Pennsylvania.
H. Daum? and D. Marcu. 2005. Learning as search
optimization: approximate large margin methods for
structured prediction. In ICML ?05: Proceedings of
the 22nd international conference on Machine learn-
ing, pages 169?176, New York, NY, USA. ACM.
David DeVault, David Traum, and Ron Artstein. 2008.
Making grammar-based generation easier to deploy in
dialogue systems. In Ninth SIGdial Workshop on Dis-
course and Dialogue (SIGdial).
M. Elhadad. 1991. FUF: the universal unifier user man-
ual version 5.0. Technical Report CUCS-038-91.
K. Krippendorff, 1980. Content Analysis: An Introduc-
tion to Its Methodology, chapter 12, pages 129?154.
Sage, Beverly Hills, CA.
I. Langkilde and K. Knight. 1998. Generation
that exploits corpus-based statistical knowledge. In
COLING-ACL, pages 704?710.
I. Langkilde-Geary. 2002. An empirical verification of
coverage and correctness for a general-purpose sen-
tence generator.
D. M. Magerman. 1995. Statistical decision-tree mod-
els for parsing. In Proceedings of the 33rd annual
meeting on Association for Computational Linguistics,
pages 276?283, Morristown, NJ, USA. Association for
Computational Linguistics.
M. P. Marcus, B. Santorini, and M. A. Marcinkiewicz.
1994. Building a large annotated corpus of en-
glish: The penn treebank. Computational Linguistics,
19(2):313?330.
A. Stent, R. Prasad, and M. Walker. 2004. Trainable sen-
tence planning for complex information presentation
in spoken dialog systems. In ACL.
Matthew Stone, Doug DeCarlo, Insuk Oh, Christian Ro-
driguez, Adrian Stere, Alyssa Lees, and Chris Bregler.
2004. Speaking with hands: creating animated con-
versational characters from recordings of human per-
formance. ACM Trans. Graph., 23(3):506?513.
M. Stone. 2002. Lexicalized grammar 101. In ACL
Workshop on Tools and Methodologies for Teaching
Natural Language Processing.
Matthew Stone. 2003. Specifying generation of referring
expressions by example. In AAAI Spring Symposium
on Natural Language Generation in Spoken and Writ-
ten Dialogue, pages 133?140.
W. Swartout, J. Gratch, R. W. Hill, E. Hovy, S. Marsella,
J. Rickel, and D. Traum. 2006. Toward virtual hu-
mans. AI Mag., 27(2):96?108.
D. R. Traum, W. Swartout, J. Gratch, and S. Marsella.
2008. A virtual human dialogue model for non-team
interaction. In L. Dybkjaer and W. Minker, editors,
Recent Trends in Discourse and Dialogue. Springer.
D. Traum. 2003. Semantics and pragmatics of questions
and answers for dialogue agents. In proceedings of the
International Workshop on Computational Semantics,
pages 380?394, January.
Sebastian Varges and Chris Mellish. 2001. Instance-
based natural language generation. In NAACL, pages
1?8.
M. Walker, O. Rambow, and M. Rogati. 2001. Spot:
A trainable sentence planner. In Proceedings of the
North American Meeting of the Association for Com-
putational Linguistics.
M. White, R. Rajkumar, and S. Martin. 2007. To-
wards broad coverage surface realization with CCG.
In Proc. of the Workshop on Using Corpora for NLG:
Language Generation and Machine Translation (UC-
NLG+MT).
Yuk Wah Wong and Raymond Mooney. 2007. Genera-
tion by inverting a semantic parser that uses statistical
machine translation. In Proceedings of NAACL-HLT,
pages 172?179.
H. Zhong and A. Stent. 2005. Building surface realiz-
ers automatically from corpora using general-purpose
tools. In Proc. Corpus Linguistics ?05 Workshop on
Using Corpora for Natural Language Generation.
85
Proceedings of SIGDIAL 2010: the 11th Annual Meeting of the Special Interest Group on Discourse and Dialogue, pages 193?200,
The University of Tokyo, September 24-25, 2010. c?2010 Association for Computational Linguistics
Don?t tell anyone! Two Experiments on Gossip Conversations Jenny Brusk School of informatics and humanities University of Sk?vde P.O. Box 408 541 28 Sk?vde, Sweden jenny.brusk@his.se Ron Artstein, David Traum USC Institute for Creative Technologies 13274 Fiji Way Marina del Rey, CA 90292 {artstein, traum}@ict.usc.edu     Abstract The purpose of this study is to get a working definition that matches people?s intuitive notion of gossip and is suffi-ciently precise for computational imple-mentation. We conducted two experi-ments investigating what type of conver-sations people intuitively understand and interpret as gossip, and whether they could identify three proposed constitu-ents of gossip conversations: third per-son focus, pejorative evaluation and sub-stantiating behavior. The results show that (1) conversations are very likely to be considered gossip if all elements are present, no intimate relationships exist between the participants, and the person in focus is unambiguous. (2) Conversa-tions that have at most one gossip ele-ment are not considered gossip. (3) Con-versations that lack one or two elements or have an ambiguous element lead to inconsistent judgments.  1 Introduction We are interested in creating believable charac-ters, i.e. ?characters that provide the illusion of life? (Bates, 1994). Since people engage exten-sively in gossip, such characters also need to be able to understand and engage in gossip in or-der to be believable in some situations. To en-able characters to engage in gossip, we need a computational model of gossip that can be ap-plied in the authoring of such characters and/or by the characters themselves. Unfortunately, such a model does not yet exist.  Moreover, there is not yet a clear consensus on how gossip should be defined, and most of the definitions are too vague or too general to 
be useful. Merriam-Webster online dictionary, for example, defines gossip as ?rumor or report of an intimate nature? and ?chatty talk?, neither of which is specific enough. What we need is a working definition that (a) matches people?s intuitive notion of gossip to the extent possible, given that the notion itself is somewhat vague, and (b) is sufficiently precise to provide a basis for computational implementation.  More recent definitions (e.g. Eder and Enke, 1991; Eggins and Slade, 1997; Hallett et al, 2009) have been derived from analyzing tran-scriptions of real gossip conversations. These definitions have only minor individual differ-ences and can in essence be formulated as ?evaluative talk about an absent third person?. We have chosen to use this definition as a start-ing point since it currently is the most specific one and since it is based on the observed struc-ture of naturally occurring gossip conversa-tions. This paper reports the results from two ex-periments on gossip conversations. The first experiment aimed at investigating what type of conversations people intuitively perceive as gossip. In the second study we also wanted to find out whether the subjects would accept a given definition and could apply it by identify-ing three specified gossip elements.   The paper is structured as follows. In sec-tion 2 we give a background to gossip with re-spect to both its social function as well as its conversational structure. Section 3 introduces the experimental method. In sections 4 and 5 we present the two experiments and discuss the results. In section 6, finally, we give some final remarks and suggestions for future work. 2 Background Gossip has been described as a mechanism for social control (e.g. Gluckman, 1963; Fine and Rosnow, 1978; Bergmann, 1993; Eggins and 
193
Slade, 1997) that maintains ?the unity, morals and values of social groups? (Gluckman, 1963). It has furthermore been suggested that gossip is a form of ?information-management?, primar-ily to improve one?s self-image and ?protect individual interests? (Paine, 1967), but also to influence others (Szwed, 1966; Fine and Ros-now, 1978). Gossip can furthermore be viewed as a form of entertainment (Abrahams, 1970) ? ?a satisfying diversion from the tedium of rou-tine activities? (Fine and Rosnow, 1978:164).  Recent studies have used a sociological ap-proach focusing on analyzing the structure of gossip conversations (e.g. Bergmann, 1993; Eder and Enke, 1991; Eggins and Slade, 1997; Hallett et al, 2009). Rather than observing and interviewing people in a certain community about their gossip behavior, they have analyzed transcripts of naturally occurring gossip con-versations. Their studies show that gossipers collaborate in creating the gossip, making it a highly interactive genre. They also identified two key elements of gossip:  ? Third person focus ? the identification of an absent third person that is ac-quainted with, but emotionally disjoint from the other participants (Bergmann (1993) refers to this as being ?virtually? absent, while Goodwin (1980) labels it ?symbolically? absent).  ? An evaluation of the person in focus or of his or her behavior. Eggins and Slade (1997) propose that the evaluation necessarily is pejorative to separate gos-sip from other types of chat.  Hallett et al (2009) found that the gossipers often use implicit evaluations to conceal the critique, suggesting that the gossipers either speak in general terms about something that implicitly is understood to be about a certain person, or that the gossipers avoid evaluating the behavior under the assumption that the evaluation is implicit in the behavior itself. In-stead of specifying the evaluation as being pe-jorative, they say it is ?unsanctioned?.  In addition to the two elements described above, Eggins and Slade (1997) propose a third obligatory element: ? Substantiating behavior ? An elabora-tion of the deviant behavior that can ei-ther be used as a motivation for the nega-tive evaluation, or as a way to introduce gossip in the conversation. Eder and 
Enke (1991) use a different model, but the substantiating behavior component corresponds roughly to their optional Explanation act.  There seems to be a consensus that gossip conversations have third person focus. The question is whether a gossip conversation nec-essarily has both a substantiating behavior component as well as a pejorative evaluation component, and if they do, can they be identi-fied? In the experiments presented later in this paper, we hope to shed light on whether these components are necessary or not. 3 Method  During the fall 2009, we conducted two ex-periments about gossip conversations. The aim of the experiments was to verify to what extent the definition of gossip accords with intuitive recognition of gossip episodes, and secondly whether people could reliably identify constitu-ent elements. The data was collected using online ques-tionnaires1 that were distributed through differ-ent email-lists mainly targeting researchers and students within game design, language technol-ogy, and related fields, located primarily in North America and Europe. The questionnaires had the following structure: The first page con-sisted of an introduction, including instructions, and each page thereafter had a dialogue excerpt retrieved from a screenplay followed by the question and/or task.  3.1 Hypotheses  Based on the previous studies presented earlier (in particular Bergmann, 1993; Eder and Enke, 1991; and Eggins and Slade, 1997) we had the following hypotheses: ? The more gossip elements present in the text, the more likely the conversation will be considered gossip. ? Third person focus is a necessary (but not sufficient) element of gossip.  ? Conversations in which the participants (including the target) are intimately re-lated will be rated lower than those in which all participants are emotionally separated.                                                  1 Created using http://www.surveygizmo.com/ 
194
4 Experiment I: Identifying gossip text The aim of the first experiment was to investi-gate how people intuitively understand and in-terpret gossip conversations.   4.1 Material and procedure  The questionnaire contained 16 different dia-logue excerpts retrieved from transcripts of the famous sitcoms Desperate Housewives 2  and Seinfeld3. The excerpts were selected to cover different combinations of the elements pre-sented in the previous section (third person fo-cus, an evaluation, and a motivation for the evaluation), as in the following dialogue4: B:  Tisha. Tisha. Oh, I can tell by that look on your face you?ve got something good. Now, come on, don?t be selfish.  T:  Well, first off, you?re not friends with Maisy Gibbons, are you? B:  No. T:  Thank god, because this is too good. Maisy was arrested. While Harold was at work, she was having sex with men in her house for money. Can you imagine?  B:  No, I can?t. T:  And that?s not even the best part. Word is, she had a little black book with all her clients? names.  R:  So, uh ? you think that?ll get out? T:  Of course. These things always do. Nancy, wait up. I can?t wait to tell you this. Wait, wait. A preliminary analysis to determine whether the elements were present or not, was made by the first author. The instructions contained no information about the elements and no defini-tion was given. To each excerpt we provided some contextual information, such as the inter-personal relationship between the speakers and other people mentioned in the dialogue, e.g.: The married couple, Bree (B) and Rex (R) Van de Kamp, is having lunch at the club. Some women laughing at the next table cause the two of them to turn and look. One of their acquaintances, Tisha (T), walks away from that table and heads to another one. Maisy Gibbons is another woman in their neigh-borhood, known to be very dominant and judgmental towards the other women.                                                   2 Touchstone Television (season 1 & 2) 3 Castle Rock Entertainment 4 From Desperate Housewives, Touchstone Television. 
The subjects were asked to read and rank the excerpts using the following scale: ? Absolutely not gossip  ? Could be considered gossip in some contexts ? Would be considered gossip in most contexts ? Absolutely gossip For the purpose of analysis we converted the above responses to integers from 0 to 3. 4.2 Results  A total of 52 participants completed the ex-periment. The following table shows the distri-bution of ratings for each of the 16 excerpts (the table is sorted by the mean rating).  Rating distribution ID5 0 1 2 3 Mean rating 11 50 1 1 0 0.058 6 46 5 0 1 0.154 15 33 15 4 0 0.442 2 28 20 4 0 0.538 5 30 15 6 1 0.577 10 17 24 10 1 0.904 9 10 26 13 3 1.173 16 11 17 16 8 1.404 4 8 18 18 8 1.500 14 11 13 11 17 1.654 3 6 20 11 15 1.673 1 1 17 25 9 1.808 13 3 18 17 14 1.808 12 5 9 15 23 2.077 8 3 0 11 38 2.615 7 1 2 4 45 2.788  Table 1: Gossip ratings of all 16 questions sorted by their mean value.   It is apparent from the table that a few ex-cerpts are clearly gossip or clearly not gossip, but there is much disagreement on other ex-cerpts. Inter-rater reliability is ? = 0.437: well above chance, but not particularly high6.  Only 7 of the 16 excerpts (ID #2, 5, 6, 7, 8, 11, 15) were clearly rated as gossip or not gossip by more than half of the subjects, and only 5 of those have a mean rating below 0.5 or above 2.5.                                                   5 Presentation was ordered by ID, same for all subjects. 6 The reported value is Krippendorff?s ? with the interval distance metric (Krippendorff 1980). Interval ? is defined as 1 ? Do/De, where Do (observed disagreement) is twice the mean variance of the individual item ratings, and De (expected disagreement) is twice the variance of all the ratings. For the above table, Do = 1.327 and De = 2.585. 
195
Despite the apparently low agreement, the results correspond fairly well with our expecta-tions. The 3 excerpts with a mean value below 0.5 had no gossip elements at all and the other two excerpts with a median value of 0 had only one gossip element. Similarly, the two excerpts rated highest clearly had all gossip elements. The rest of the excerpts, however, either lacked one element or had one element that was un-clear in some regard (see discussion, below). Conversations between family members or partners also caused higher disagreements, which seem to support Bergmann?s (1993) re-mark: ?[?] we can ask whether we should call gossip the conversations between spouses [?] alone. This surely is a borderline case for which there is no single answer? (p. 68).  4.3 Discussion  Among the nine excerpts with a mean value approximately between 1 and 2 (ID #1, 3, 4, 9, 10, 12, 13, 14, and 16), we made the following observations: 3 excerpts lacked one element; in 2 of them, the gossipers were family members or partners; 3 excerpts had an ambiguous focus, among which one also possibly was perceived as a warning.  By ?ambiguous focus? we mean that it is un-clear whether the person in focus is the speaker, the addressee or the absent third person. In-stead, the absent third person seems to play a sub-ordinate role rather than focused role, for instance as part of a self-disclosure or a con-frontation. If the conversation is the least bit confrontational, the addressee tends to go into defense rather than choosing a more typical gossip response, such as support, expansion, or challenge (Eder and Enke, 1991) in order to protect the face. Hence, no ?gossip fuel? is added to the conversation.  The result of the remaining excerpt7 is how-ever more difficult to explain. One possible explanation is that the initiator was unac-quainted with the target, but perhaps more likely is that some of the subjects interpreted the conversation as mocking rather than gossip:  E: Who?s that? D:  That?s Sam, the new girl in accounting. W: What?s with her arms? They just hang like salamis. D:  She walks like orangutan. E:  Better call the zoo.                                                  7 ID #14. From Seinfeld, Castle Rock Entertainment. 
5 Experiment II: Identifying gossip elements in a text The aim of the second experiment was to inves-tigate whether the subjects could accept and apply a given definition by identifying the three obligatory elements of gossip according to Eggins and Slade (1997) (see section 2); third person focus, pejorative evaluation, and sub-stantiating behavior. In addition to the ele-ments, we provided the more general definition presented in section 1 (?evaluative talk about an absent third person?). The results from the first experiment indi-cated that conversations that seemingly had all the elements but in which the person in focus was ambiguous, received a lower gossip rating than those having an unambiguous third person focus. So an additional goal was to investigate whether changing the relationship between the participants would affect the gossip rating.  5.1 Material  We used excerpts from Seinfeld8, Desperate Housewives 9 , Legally blonde 10 , and Mean girls11. In total we selected 21 excerpts, of which 8 also occurred in the first experiment. Two of the recurring excerpts were used both in their original versions as well as in modified versions, in which we had removed the emo-tional connections between the participants. The purpose of this was to find out whether changing the interpersonal relationship would change the gossip rating.  5.2 Procedure The subjects were instructed to read the ex-cerpts and then identify the gossip elements according to the following description: ? The person being talked about (third per-son focus) ? the ?target?, e.g. ?Maisy Gibbons was arrested?  ? Pejorative evaluation. A judgment of the target him-/herself or of the target?s be-havior. This evaluation is in most cases negative, e.g. ?She?s a slut?, ?He?s weird?                                                  8 Touchstone Television. 9 Castle Rock Entertainment. 10 Directed by Robert Luketic. Metro Goldwyn Mayer (2001). 11 Directed by Mark Waters. Paramount Pictures (2004). 
196
? The deviant behavior that motivates the gossip and provides evidence for the judgment (also called the substantiating behavior stage), e.g. ?Maisy Gibbons was arrested? For each element they found, they were asked to specify the corresponding line refer-ence as given in the text. They were also in-structed to say whether they considered the conversation to be gossip or not gossip. If their rating disagreed with the definition, i.e. if they had found all the elements but still rated the conversation as not gossip, or if one or more elements were lacking but the conversation was considered gossip anyway, they were asked to specify why.  5.3 Results We analyzed the results from the 19 subjects who completed ratings for all 21 excerpts. This gave a total of 399 yes/no judgments on 4 at-tributes. Inter-coder reliability 12  is shown in Table 2.  The easiest attribute to interpret is third person focus. All but three of the subjects marked either 4 or 5 excerpts as not having third person focus, with the remaining subjects not deviating by much (marking 3, 6, and 7 excerpts). Moreover, the subjects agree on which excerpts have third person focus: only one excerpt gets a substantial number of con-flicting ratings (see the analysis given below in section 5.4), while the remaining 20 excerpts get consistent ratings from all subjects with only occasional deviation by one or two of the deviant subjects. This accounts for the high observed agreement on this feature (94.9%). Expected agreement is high because the corpus is not balanced (16 of 21 excerpts display third person focus), but even so, chance-corrected agreement is high (85.1%), showing that third person focus is an attribute that participants can readily and reliably identify. The remaining attributes, including gossip, are less clear. Agreement on all of them is clearly above chance, but is not particularly high, showing that these notions are either not fully defined, or that the excerpts are ambigu-ous. Gossip itself is identified somewhat more reliably than either substantiating behavior or pejorative evaluation; this casts doubt about the ability to use the latter two as defining features                                                  12 We used Krippendorff's alpha with the nominal dis-tance metric. Observed agreement is defined as Ao = 1 ? Do, while expected agreement is: Ae = 1 ? De. 
of gossip, given that they are more difficult to identify.   Alpha Observed agreement Expected agreement Gossip 0.466 0.744 0.520 Third person focus 0.851 0.949 0.661 Substantiat-ing behavior 0.376 0.709 0.533 Pejorative evaluation 0.384 0.733 0.567  Table 2: Inter-coder reliability.  To test the relationship between the various features, we looked for co-occurrences among the individual judgments. We have a total of 399 ratings (21 excerpts times 19 judges), each with 4 attributes; these are distributed as shown in Table 313. We can see that third person focus is an almost necessary condition for classifying a screenplay conversation as gossip, though it is by no means sufficient. Tables 4?6 show the co-occurrences of individual features to gossip; the association is strongest between gossip and third person focus and weakest between gossip and pejorative evaluation.    3rd person 3rd person    Subst Subst Subst Subst Pejor 168 24  2 Gossip Pejor 33 14   Pejor 25 20 17 17 Gossip Pejor 6 23 3 47  Table 3: Relationship between the different elements and gossip.   3rd person 3rd person Gossip 239 2 Gossip 74 84  Table 4: Gossip ? third person focus.   Substantiating behavior Substantiating behavior Gossip 201 40 Gossip 51 107  Table 5: Gossip ? substantiating behavior                                                   13 Strike-through marks the absence of a feature. 
197
 Pejorative Pejorative Gossip 194 47 Gossip 79 79  Table 6: Gossip ? pejorative evaluation   In addition to the co-occurrences of features on the individual judgments, we can look at these co-occurrences grouped by screenplay. Table 7 shows, for each of the 21 excerpts, how many subjects identified each of the four fea-tures (the table is sorted by the gossip score). It is apparent from the table that all the features are correlated to some extent.   ID14 Gossip Third person Subst.  behavior Pejorative evaluation   2   0   0   1   3 11   0   0   9   9 19   0   1   6   8 14   1   0   2 12   5   7 19   5   1 15   7 19 18 17 21   8 17   6 16 12   9 17 10 14 20 13 13 10 10 16 14 18 14   7   8 14 19   7 19   7 14 19   9   9 17 14 19 17 18 18 15 19 19 19   4 17 19 12   9 10 17 19 16 19   6 17 19 19 19   9 18 19 17   8   1 18 19 19 19   3 19 19 18 18 13 19 19 18 19  Table 7: Co-occurrences grouped by excerpts.  Table 8 shows the correlation between gos-sip and each of the other three features. The first column calculates correlation based on the individual judgments (399 items, each score is either 0 or 1); the second column calculates correlation based on the rated excerpts (21 items, each score is an integer between 0 and 19, as in table 7); and the third column groups the judgments by subject (19 items, each score is an integer between 0 and 21, indicating the number of dialogues in which the subject iden-tified the particular feature; the full data are not shown).                                                  14 Presentation was ordered by ID, same for all subjects. 
Pearson?s r Correlation with gossip Individual Excerpt Subject Third person 0.622*** 0.849*** 0.503* Substantiating 0.518*** 0.765*** 0.625** Pejorative 0.321*** 0.518* 0.459* * p < 0.05 ** p < 0.01 *** p < 0.001  Table 8: Correlation between gossip and each of the three features.  All the correlations are significantly different from 0 at the p ? 0.05 level or greater. The dif-ferences between the columns are not signifi-cant, except for the difference between the third person correlation by individuals and that by excerpt, which is significant at p ? 0.05. The correlations between the features on the indi-vidual judgments show that subjects tend to identify the different features together; this may be partly a reflection of awareness on their part that the features are expected to go together, given the task definition. The correlations be-tween the excerpt scores show that the excerpts themselves differ along the four dimensions, and these differences go hand in hand. Finally, we see that the subjects themselves differ in how often they identify the different features, though the correlations are likely to be just a reflection of the first tendency identified above, to mark the features together. 5.4 Discussion We wanted to find out whether the subjects would accept, understand and be able to apply a given definition. The results from the experi-ment showed that the subjects accepted the given definition to some extent and managed to apply it. When the subjects disagreed they were asked to say why. One of the subjects, for ex-ample, explicitly disagreed with the definition given in the introduction and provided a counter definition: ?Gossip is idle talk or ru-mor, especially about the personal or private affairs of others?. Yet another subject was un-certain about which definition to use: ?Depends what you mean by gossip. It can either mean malicious, behind the back talk of other people or idle chat.  If you mean ?idle chat? with gos-sip then this is also gossip?. A possible expla-nation could be that the subjects refer to differ-ent forms of gossip (see e.g. Gilmore, 1978) and therefore apply different definitions (such as the lexical definition presented earlier) than the one that was given in the experiment.  
198
Several subjects stated that they judged the conversation as gossip even if they did not identify any pejorative evaluation, and they also questioned whether the evaluation had to be negative or even present at all, or as one of the subjects put it: ?Although there is no pejo-rative evaluation (at least not clearly) I believe this is gossip?. These subjects thus explicitly reject Eggins and Slade?s (1997) requirement that the evaluation has to be pejorative.  The examples above show that people have variable intuitions of gossip and consequently the concept of gossip is somewhat vague. Even so, the experiment also showed that people to a large degree are in agreement when the exam-ples according to the given definition clearly are gossip or not gossip. Meaning that even though the definition does not capture all types of (potential) gossip conversations, it captures those episodes that most people agree to be gossip, which for our purpose is sufficient.  5.5 Effect of interpersonal relations In some particular cases, the subjects did not choose gossip even if all elements had been found. The results from the first experiment indicated that this deviation either was related to the interpersonal relationship between the gossip participants or that the focus was am-biguous. In order to test whether changing the inter-personal relationship between the partici-pants would change the gossip rating, we com-pared the results from the conversations we had modified with their original counterparts. In one of the original excerpts, the addressee was romantically involved with the man that the speaker was talking about. The speaker formu-lated the negative assessment and deviant be-havior in a way that for most people would be interpreted as a warning, which probably ex-plains why only 7 of the 19 subjects rated the original conversation as gossip. The modified version on the other hand, was rated as gossip by all subjects.   In the second dialogue, the speaker questions the addressee?s choice of person to date, and does this by both evaluating the person nega-tively as well as providing evidence for the evaluation. It turns out, however, that the ad-dressee thinks she is going out for a date with someone else, so a large part of the conversa-tion deals with trying to identify the target. 15 of 19 subjects rated the original conversation as gossip, while all subjects rated it as gossip in the modified version. These comparisons indi-
cate that the status of the relationship between the gossipers and the gossip target affects whether the dialogue is considered gossip or not. In the original version of both these exam-ples, the focus was ambiguous, i.e. the focus was as much on the addressee as on the absent third person. We have shown that third person focus is a key element of gossip. The correlation was fur-thermore confirmed by the subjects themselves in their comments, where the lack of third per-son often was listed as a reason for not choos-ing gossip. In one example, the respondent re-garded the conversation as gossip even if it really was an insult directed towards the ad-dressee, but explained it as its ??almost like he?s forgotten he?s talking to the person he?s giving this opinion/gossip about?. The highest disagreement concerning third person focus was found in the following ex-cerpt15: Karen: Okay, what is it? Gretchen: Regina says everyone hates you because you?re such a slut. Karen: She said that? Gretchen: You didn?t hear it from me. The dialogue contains an ambiguous focus in that it both includes a quote as well as a con-frontational insult. By using the third person reference, Gretchen avoids taking responsibility for the insult. In some sense both Karen and Regina are in focus, where Karen is the target of the pejorative evaluation and Regina can be interpreted as being the focus of the substantiat-ing behavior component. How Regina?s role is interpreted is determined by the respondents? personal attitude towards gossiping in general (i.e. whether they interpret Gretchen?s utterance as containing an implicit evaluation of Regina?s behavior or not), and how they perceive the interpersonal relationship between Karen and Gretchen. Gossip has an inherent contradiction in that it both has a function of negotiating the accepted way to behave while it at the same time often is considered an inappropriate activ-ity that can have serious negative consequences for both the gossipers as well as the gossip tar-get (see e.g. Gilmore, 1978; Bergmann, 1993; Eggins and Slade, 1997; Hallett et al, 2009).                                                   15 From Mean Girls, Paramount Pictures, 2004. 
199
6 Final remarks and future work The aim of these studies has been to get a workable definition of gossip that people can agree upon and that is sufficiently precise to provide a basis for computational implementa-tion.  We conducted two experiments to investi-gate people?s intuitive notion of gossip and the results show that (1) conversations in which all elements are present, where no intimate rela-tionships exist between the participants, and in which the person in focus is unambiguous, are very likely to be considered gossip. (2) Conver-sations that have at most one gossip element are not considered gossip. (3) Inconsistencies are mainly found in conversations that lack one or two elements or have at least one element that is ambiguous, or are taking place between gossipers that have an intimate relationship.  We have suggested that third person focus is a necessary, but not sufficient, element of gos-sip, but the other elements are less clear even if their co-occurrence in a conversation clearly affects the gossip score.  In the second experi-ment this might be due to the instructions, but it does not explain the unbiased results from the first experiment. So on the one hand we can clearly see that all three elements are important for the understanding of gossip, but on the other hand, the subjects? had trouble in identi-fying them. This suggests that we need to fur-ther investigate these elements to see how they can be specified more clearly.  We have taken a first step toward a computa-tional account of gossip, by empirically verify-ing the extent to which the given definition can be applied and the components recognized by people. Some of our next steps to further this program include authoring content for believ-able characters that follow this definition, as well as attempting to automatically recognize these elements. Among the possible applications of gossip we can think of game characters and virtual humans that are capable of engaging in gossip conversations to share information and create social bonds with a human user or its avatar. This involves being able to both generate gos-sip on basis of the interpersonal relationship and selecting content that could be regarded as gossip, as well as to automatically detect gossip occurring in a conversation. The latter use could also be used for characters that actively 
want to avoid taking part in gossip conversa-tions.  References  Roger D. Abrahams. 1970. A Performance-Centred Approach to Gossip. Man, New Series, Vol. 5, No. 2 (Jun.), pp. 290-301. Joseph Bates. (1994. The Role of Emotion in Belie-vable agents. Communications of the ACM, Vol. 37, No. 7 (Jul.), pp. 122-125. J?rg R. Bergmann. 1993. Discreet Indiscretions: The Social Organization of Gossip. New York: Aldine. Suzanne Eggins and Diana Slade (1997) Analysing Casual Conversation. Equinox Publishing Ltd.  Donna Eder and Janet Lynne Enke (1991) The Structure of Gossip: Opportunities and Con-straints on Collective Expression among Adoles-cents. American sociological Review, Vol. 56, No. 4 (Aug.), pp. 494?508. Gary Alan Fine and Ralph L. Rosnow. 1978. Gos-sip, Gossipers, Gossiping. Personality and So-cial Psychology Bulletin, Vol. 4, No. 1, pp 161-168. David Gilmore. 1978. Varieties of Gossip in a Spanish Rural Community. Ethnology, Vol. 17, No. 1 (Jan.), pp. 89-99. Max Gluckman. 1963. Papers in Honor of Melville J. Herskovits: Gossip and Scandal. Current An-thropology, Vol. 4, No. 3 (Jun), pp. 307?316.  Marjorie Harness Goodwin. 1980. He-Said-She-Said: Formal Cultural Procedures for the Con-struction of a Gossip Dispute Activity. American Ethnologist, Vol. 7, No. 4 (Nov.), pp. 674-695.  Tim Hallett., Brent Harget and Donna Eder (2009) Gossip at Work: Unsanctioned Evaluative Talk in Formal School Meetings. Journal of Contem-porary Ethnography, Vol. 38, No. 5, pp 584?618. Klaus Krippendorff. 1980. Content Analysis: An Introduction to Its Methodology, chapter 12. Sage Beverly Hills, CA. Robert Paine.1967. What is gossip about? An alter-native hypothesis. Man, New Series, Vol. 2, No. 2 (Jun.), pp. 278-285. John F. Szwed. 1966. Gossip, Drinking, and Social Control: Consensus and Communication in a Newfoundland Parish. Ethnology, Vol. 5, No. 4 (Oct.), pp. 434-441.   
200
Proceedings of SIGDIAL 2010: the 11th Annual Meeting of the Special Interest Group on Discourse and Dialogue, pages 245?248,
The University of Tokyo, September 24-25, 2010. c?2010 Association for Computational Linguistics
I?ve said it before, and I?ll say it again: An empirical investigation of the
upper bound of the selection approach to dialogue
Sudeep Gandhe and David Traum
Institute for Creative Technologies
13274 Fiji way, Marina del Rey, CA 90292
{gandhe,traum}@ict.usc.edu
Abstract
We perform a study of existing dialogue
corpora to establish the theoretical max-
imum performance of the selection ap-
proach to simulating human dialogue be-
havior in unseen dialogues. This maxi-
mum is the proportion of test utterances
for which an exact or approximate match
exists in the corresponding training cor-
pus. The results indicate that some do-
mains seem quite suitable for a corpus-
based selection approach, with over half of
the test utterances having been seen before
in the corpus, while other domains show
much more novelty compared to previous
dialogues.
1 Introduction
There are two main approaches toward automat-
ically producing dialogue utterances. One is the
selection approach, in which the task is to pick
the appropriate output from a corpus of possible
outputs. The other is the generation approach, in
which the output is dynamically assembled using
some composition procedure, e.g. grammar rules
used to convert information from semantic repre-
sentations and/or context to text.
The generation approach has the advantage of
a more compact representation for a given gener-
ative capacity. But for any finite set of sentences
produced, the selection approach could perfectly
simulate the generation approach. The generation
approach generally requires more analytical effort
to devise a good set of grammar rules that cover
the range of desired sentences but do not admit un-
desirable or unnatural sentences. Whereas, in the
selection approach, outputs can be limited to those
that have been observed in human speech. This
affords complex and human-like sentences with-
out much detailed analysis. Moreover, when the
output is not just text but presented as speech, the
system may easily use recorded audio clips rather
than speech synthesis. This argument also extends
to multi-modal performances, e.g. using artist an-
imation motion capture or recorded video for an-
imating virtual human dialogue characters. Often
one is willing to sacrifice some generality in or-
der to achieve more human-like behavior than is
currently possible from generation approaches.
The selection approach has been used for a
number of dialogue agents, including question-
answering characters at ICT (Leuski et al, 2006;
Artstein et al, 2009; Kenny et al, 2007), FAQ
bots (Zukerman and Marom, 2006; Sellberg and
Jo?nsson, 2008) and web-site information charac-
ters. It is also possible to use the selection ap-
proach as a part of the process, e.g. from words to
a semantic representation or from a semantic rep-
resentation to words, while using other approaches
for other parts of dialogue processing.
The selection approach presents two challenges
for finding an appropriate utterance:
? Is there a good enough utterance to select?
? How good is the selection algorithm at find-
ing this utterance?
We have previously attempted to address the sec-
ond question, by proposing the information or-
dering task for evaluating dialogue coherence
(Gandhe and Traum, 2008). Here we try to ad-
dress the first question, which would provide a
theoretical upper bound in quality for any selec-
tion approach. We examine a number of different
dialogue corpora as to their suitability for the se-
lection approach.
We make the following assumptions to allow
automatic evaluation across a range of corpora.
Actual human dialogues represent a gold-standard
for computer systems to emulate; i.e. choosing an
actual utterance in the correct place is the best pos-
sible result. Other utterances can be evaluated as
to how close they come to the original utterance,
245
using a similarity metric.
Our methodology is to examine a test corpus of
human dialogue utterances to see how well a se-
lection approach could approximate these, given a
training corpus of utterances in that domain. We
look at exact matches as well as utterances having
their similarity score above a threshold. We in-
vestigate the effect of the size of training corpora,
which lets us know how much data we might need
to achieve a certain level of performance. We also
investigate the effect of domain of training cor-
pora.
2 Dialogue Corpora
We examine human dialogue utterances from a va-
riety of domains. Our initial set contains six dia-
logue corpora from ICT as well as three other pub-
licly available corpora.
SGT Blackwell is a question-answering char-
acter who answers questions about the U.S. Army,
himself, and his technology. The corpus con-
sists of visitors interacting with SGT Blackwell at
an exhibition booth at a museum. SGT Star is
a question-answering character, like SGT Black-
well, who talks about careers in the U.S. Army.
The corpus consists of trained handlers present-
ing the system. Amani is a bargaining character
used as a prototype for training soldiers to perform
tactical questioning. The SASO system is a ne-
gotiation training prototype in which two virtual
characters negotiate with a human ?trainee? about
moving a medical clinic. The Radiobots system is
a training prototype that responds to military calls
for artillery fire. IOTA is an extension of the Ra-
diobots system. The corpus consists of training
sessions between a human trainee and a human in-
structor on a variety of missions. Yao et al (2010)
provides details about the ICT corpora.
Other corpora involved dialogues between
two people playing specific roles in planning,
scheduling problem for railroad transportation,
the Trains-93 corpus (Heeman and Allen, 1994)
and for emergency services, the Monroe corpus
(Stent, 2000). The Switchboard corpus (Godfrey
et al, 1992) consists of telephone conversations
between two people, based on provided topics.
We divided the data from each corpus into a
training set and a test set, as shown in Table 1. The
data consists of utterances from one or more hu-
man speakers who engage in dialogue with either
virtual characters (Radiobots, Blackwell, Amani,
Star, SASO) or other humans (Switchboard, Mon-
roe, IOTA, Trains-93). These corpora differ along
a number of dimensions such as the size of the
corpus, dialogue genre (question-answering, task-
oriented or conversational), types of tasks (ar-
tillery calls, moving and scheduling resources, in-
formation seeking) and motivation of the partici-
pants (exploring a new technology ? SGT Black-
well , presenting a demo ? SGT Star, undergo-
ing training ? Amani, IOTA or simply for collect-
ing the corpus ? Switchboard, Trains-93, Monroe).
While the set of corpora we include does not cover
all points in these dimensions, it does present an
interesting range.
3 Dialogue Utterance Similarity Metrics
To answer the question of whether an adequate
utterance exists in our training corpus that could
be selected and used, we need an appropriate-
ness measure. We assume that an utterance pro-
duced by a human in a dialogue is appropriate,
and thus the problem becomes one of construct-
ing an appropriate similarity function to compare
the human-produced utterance with the utterances
available from the training corpus. Given a train-
ing corpus Utrain and a similarity function f ,
we calculate the score for a test utterance ut as,
maxsimf (ut) = maxi f(ut, ui);ui ? Utrain
There are several choices for the utterance simi-
larity function f . Ideally such a function would
take meaning and context into account rather than
just surface similarity, but these aspects are harder
to automate, so for our initial experiments we look
at several surface metrics, as described below.
Exact measure returns 1 if the utterances are ex-
actly same and 0 otherwise. 1-WER, a similar-
ity measure related to word error rate, is defined
as min (0, 1? levenshtein(ut, ui)/length(ut)).
METEOR (Lavie and Denkowski, 2009), one of
the automatic evaluation metrics used in machine
translation is a good candidate for f . METEOR
finds optimal word-to-word alignment between
test and reference strings based on several modules
that match exact words, stemmed words and syn-
onyms. METEOR is a tunable metric and for our
analysis we used the default parameters tuned for
the Adequacy & Fluency task. All previous mea-
sures take into account the word ordering of test
and reference strings. In contrast, document simi-
larity measures used in information retrieval gen-
erally follow the bag of words assumption, where a
246
Domain
Train Test mean(maxsimf ) % of utterances
MET - METEOR
# utt words # utt words EOR 1-WER Dice Cosine Exact ? 0.9 ? 0.8
Blackwell 17755 84.7k 2500 12.0k 0.913 0.878 0.917 0.921 69.6 75.8 82.1
Radiobots 995 6.8k 155 1.2k 0.905 0.864 0.920 0.924 53.6 67.7 83.2
SGT Star 2974 16.6k 400 2.2k 0.897 0.860 0.906 0.911 65.0 70.5 78.0
SASO 3602 23.3k 510 3.6k 0.821 0.742 0.830 0.837 38.4 48.6 62.6
IOTA 4935 50.4k 650 5.6k 0.768 0.697 0.800 0.808 36.2 42.8 51.4
Trains 93 5554 47.2k 745 6.0k 0.729 0.633 0.758 0.769 34.5 36.9 42.8
SWBD1 19741 138.2k 3173 21.5k 0.716 0.628 0.736 0.753 35.8 37.9 44.2
Amani 1455 15.8k 182 1.9k 0.675 0.562 0.694 0.706 18.7 25.8 30.8
Monroe 5765 43.0k 917 8.8k 0.594 0.491 0.639 0.658 22.3 23.6 26.1
Table 1: Corpus details and within domain results
string is converted to a set of tokens. Here we also
considered Cosine and Dice coefficients using the
standard boolean model. In our experiments, the
surface text was normalized and all punctuation
was removed.
4 Experiments
Results Within a Domain
In our first experiment, we computed maxsimf
scores for all test corpus utterances in a given
domain using the training utterances from the
same domain. For the domains Blackwell, SGT
Star, SASO, Amani & Radiobots which are imple-
mented dialogue systems our corpus consists of
user utterances only. For Trains 93 and Monroe
corpora, we make sure to match the speaker roles
for ut and ui. For Switchboard, where speakers
do not have any special roles and for IOTA, where
the speaker information was not readily accessi-
ble, we ignore the speaker information and select
utterances from either speaker.
Table 1 reports the mean of maxsimf scores.
These can be interpreted as the expectation of
maxsimf score for a new test utterance. The
higher this expectation, the more likely it is that
an utterance similar to the new one has been
seen before and thus the domain will be more
amenable to selection approaches. This table
also shows the percentage of utterances that had
a maxsimMeteor score above a certain thresh-
old. The correlation between maxsimf for dif-
ferent choices of f (except Exact match) is very
high (Pearson?s r > 0.94). The histogram anal-
ysis shows that SGT Star, Blackwell, Radiobots
1Switchboard (SWBD) is a very large corpus and for run-
ning our experiments in a reasonable computing time we only
selected a small portion of it.
Figure 1: maxsimMeteor vs # utterances in train-
ing data for different domains
and SASO domains are better suited for selec-
tion approaches. Domains like Trains-93, Monroe,
Switchboard and Amani have a more diffuse dis-
tribution and are not best suited for selection ap-
proaches, at least with the amount of data we have
available. The IOTA domain falls somewhere in
between these two domain classes.
Effect of Training Data Size
Figure 1 shows the effect of training data size
on the maxsimMeteor score. Radiobots shows
very high scores even for small amounts of train-
ing data. SGT Star and SGT Blackwell also con-
verge fairly early. Switchboard, on the other hand,
does not achieve very high scores even with a
large number of utterances. For all domains, with
around 2500 training utterances maxsimMeteor
reaches 90% of its maximum possible value for
the training set.
Comparing Different Domains
In order to understand the similarities be-
tween different dialogue domains, we computed
maxsimMeteor for a test domain using training
247
Training Domains
IOTA Radio-
bots
SGT
Star
Black-
well
Amani SASO Trains-
93
Monroe SWBD
Te
st
in
g
D
om
ai
ns
IOTA 0.768 0.440 0.247 0.334 0.196 0.242 0.255 0.297 0.334
Radiobots 0.842 0.905 0.216 0.259 0.161 0.183 0.222 0.270 0.284
SGT Star 0.324 0.136 0.897 0.622 0.372 0.438 0.339 0.417 0.527
Blackwell 0.443 0.124 0.671 0.913 0.507 0.614 0.424 0.534 0.696
Amani 0.393 0.134 0.390 0.561 0.675 0.478 0.389 0.420 0.509
SASO 0.390 0.125 0.341 0.516 0.459 0.821 0.443 0.454 0.541
Trains 93 0.434 0.112 0.214 0.468 0.272 0.429 0.753 0.627 0.557
Monroe 0.409 0.119 0.217 0.428 0.276 0.404 0.534 0.630 0.557
SWBD 0.368 0.110 0.280 0.490 0.362 0.383 0.562 0.599 0.716
Table 2: Mean of maxsimMeteor for comparing different dialogue domains. The bold-faced values are
the highest in the corresponding row.
sets from other domains. In this exercise, we ig-
nored the speaker information. Table 2 reports
the mean values of maxsimMeteor for different
training domains. For all the testing domains,
using the training corpus from the same domain
produces the best results. Notice that Radiobots
also has good performance with the IOTA train-
ing data. This is as expected since IOTA is an
extension of Radiobots and should cover a lot of
utterances from the Radiobots domain. Switch-
board and Blackwell training corpora have a over-
all higher score for all testing domains. This may
be due to the breadth and size of these corpora. On
the other extreme, the Radiobots training domain
performs very poorly on all testing domains other
than itself.
5 Discussion
We have examined how well suited a corpus-
based selection approach to dialogue can succeed
at mimicking human dialogue performance across
a range of domains. The results show that such an
approach has the potential of doing quite well for
some domains, but much less well for others. Re-
sults also show that for some domains, quite mod-
est amounts of training data are needed for this
operation. Applying this method across corpora
from different domains can also give us a simi-
larity metric for dialogue domains. Our hope is
that this kind of analysis can help inform the de-
cision of what kind of language processing meth-
ods and dialogue architectures are most appropri-
ate for building a dialogue system for a new do-
main, particularly one in which the system is to
act like a human.
Acknowledgments
This work has been sponsored by the U.S. Army Re-
search, Development, and Engineering Command (RDE-
COM). Statements and opinions expressed do not necessarily
reflect the position or the policy of the United States Gov-
ernment, and no official endorsement should be inferred. We
would like to thank Ron Artstein and others at ICT for com-
piling the ICT Corpora used in this study.
References
R. Artstein, S. Gandhe, J. Gerten, A. Leuski, and D. Traum.
2009. Semi-formal evaluation of conversational charac-
ters. In Languages: From Formal to Natural. Essays Ded-
icated to Nissim Francez on the Occasion of His 65th
Birthday, volume 5533 of LNCS. Springer.
S. Gandhe and D. Traum. 2008. Evaluation understudy for
dialogue coherence models. In Proc. of SIGdial 08.
J. J. Godfrey, E. C. Holliman, and J. McDaniel. 1992.
Switchboard: Telephone speech corpus for research and
development. In Proc. of ICASSP-92, pages 517?520.
P. A. Heeman and J. Allen. 1994. The TRAINS 93 dialogues.
TRAINS Technical Note 94-2, Department of Computer
Science, University of Rochester.
P. Kenny, T. Parsons, J. Gratch, A. Leuski, and A. Rizzo.
2007. Virtual patients for clinical therapist skills training.
In Proc. of IVA 07, Paris, France. Springer.
A. Lavie and M. J. Denkowski. 2009. The meteor metric
for automatic evaluation of machine translation. Machine
Translation, 23:105?115.
A. Leuski, R. Patel, D. Traum, and B. Kennedy. 2006. Build-
ing effective question answering characters. In Proc. of
SIGdial 06, pages 18?27, Sydney, Australia.
L. Sellberg and A. Jo?nsson. 2008. Using random indexing to
improve singular value decomposition for latent semantic
analysis. In Proc. of LREC?08, Morocco.
A. J. Stent. 2000. The monroe corpus. Technical Report
728, Computer Science Dept. University of Rochester.
X. Yao, P. Bhutada, K. Georgila, K. Sagae, R. Artstein, and
D. Traum. 2010. Practical evaluation of speech recogniz-
ers for virtual human dialogue systems. In LREC 2010.
I. Zukerman and Y. Marom. 2006. A corpus-based approach
to help-desk response generation. In CIMCA/IAWTIC ?06.
248
Proceedings of the SIGDIAL 2011: the 12th Annual Meeting of the Special Interest Group on Discourse and Dialogue, pages 272?278,
Portland, Oregon, June 17-18, 2011. c?2011 Association for Computational Linguistics
An Annotation Scheme for Cross-Cultural Argumentation
and Persuasion Dialogues
Kallirroi Georgila?, Ron Artstein?, Angela Nazarian?
Michael Rushforth??, David Traum?, Katia Sycara?
?Institute for Creative Technologies, University of Southern California
?Robotics Institute, Carnegie Mellon University
kgeorgila@ict.usc.edu
Abstract
We present a novel annotation scheme for
cross-cultural argumentation and persuasion
dialogues. This scheme is an adaptation of
existing coding schemes on negotiation, fol-
lowing a review of literature on cross-cultural
differences in negotiation styles. The scheme
has been refined through application to cod-
ing both two-party and multi-party negotia-
tion dialogues in three different domains, and
is general enough to be applicable to differ-
ent domains with few if any extensions. Di-
alogues annotated with the scheme have been
used to successfully learn culture-specific di-
alogue policies for argumentation and persua-
sion.
1 Introduction
In both cooperative and non-cooperative negotiation
the nature of the arguments used can be crucial for
the outcome of the negotiation. Argumentation and
persuasion are basic elements of negotiation. More-
over, different cultures favor different types of argu-
ments (Koch, 1983; Han and Shavitt, 1994; Zaharna,
1995; Brett and Gelfand, 2006). For example, it is
claimed that Western individualistic cultures favor
arguments based on logic over arguments that appeal
to emotions. On the other hand, people from East-
ern collectivistic cultures are more likely to use ar-
guments in which the beneficiary is not themselves.
Furthermore, Arab cultures tend to favor more indi-
rect ways of argumentation and expression (Koch,
1983; Zaharna, 1995).
?Now at the University of Texas at San Antonio.
In order to analyze negotiation in detail, including
aspects such as persuasion, negotiation, and cross-
cultural differences, we have developed a novel
annotation scheme. General purpose annotation
schemes such as DAMSL (Core and Allen, 1997)
and DIT++ (Bunt, 2006) represent moves in the dia-
logue but do not capture enough details of the inter-
action to distinguish between different styles of per-
suasion and argumentation, especially cross-cultural
differences.
Our goal for developing this coding scheme is
two-fold. First, we aim to fill the gap in the litera-
ture of cross-cultural argumentation and persuasion.
To our knowledge this is the first annotation scheme
designed specifically for coding cross-cultural argu-
mentation and persuasion strategies. Previous work
on cross-cultural negotiation, e.g. Brett and Gelfand
(2006), has not focused on argumentation or per-
suasion in particular. Also, previous work on argu-
mentation, e.g. Prakken (2008), has not attempted to
capture cross-cultural differences in argumentation
and persuasion strategies. Second, we use this cod-
ing scheme to annotate negotiation dialogues to au-
tomatically learn argumentation and persuasion di-
alogue policies for different cultures (Georgila and
Traum, 2011).
2 Related Work
2.1 Non-Culture Related Argumentation and
Persuasion
The topic of negotiation has widely been studied
across various fields including social and behavioral
science (Kern et al, 2005), and computer science
(Sidner, 1994; Rose? and Torrey, 2004). Our spe-
cific focus is on the role of argumentation and per-
272
suasion. Sycara (1990) studied the role of argumen-
tation in negotiation with regard to the role of ar-
guments in changing the decision process of the in-
terlocutor. Most attempts have focused on study-
ing the structure of argumentation and persuasion,
often using formal logic (Cohen, 1987; Prakken,
2008). Dung (1995) showed that argumentation can
be viewed as a special form of logic programming
with negation as failure. An argumentation scheme
is defined as a structure or template for forming an
argument. Schemes are necessary for identifying
arguments, finding missing premises, analyzing ar-
guments, and evaluating arguments (Pollock, 1995;
Katzav and Reed, 2004; Walton et al, 2008).
Recently, there has been some work on using ma-
chine learning techniques for automatically inter-
preting (George et al, 2007) and generating argu-
ments (Zukerman, 2001). Note also the work of Pi-
wek (2008) who performed a study on how argu-
ments can be presented as fictive dialogues. Finally,
there are a few persuasive dialogue systems, e.g.
Daphne (Grasso et al, 2000) and BIAS (Bayesian In-
teractive Argumentation System) (Zukerman, 2001).
2.2 Cross-Cultural Argumentation and
Persuasion
There is a vast amount of research on cultural ef-
fects on negotiation. Brett and Gelfand (2006) iden-
tify three aspects in cross-cultural negotiation: indi-
vidualism vs. collectivism, egalitarianism vs. hierar-
chy, and low context vs. high context communica-
tion. Typically Western individuals are individualis-
tic, egalitarian, and use low context communication
while Eastern individuals are collectivistic, hierar-
chical, and use high context communication.1
Although there has been a considerable amount of
work on building agents that can negotiate (Traum
et al, 2003; Rose? and Torrey, 2004), little has been
done towards building agents that can take into ac-
count culture aspects of negotiation (Cassell, 2009;
Paruchuri et al, 2009; Traum, 2009).
Our literature review on cross-cultural argumen-
tation and persuasion showed that there are com-
paratively few papers related to cross-cultural argu-
mentation and persuasion in dialogue. Most work
on cross-cultural studies is based on survey experi-
1In high-context cultures the listener must understand the
contextual cues in order to grasp the full meaning of the mes-
sage. In low-context cultures communication tends to be spe-
cific, explicit, and analytical.
ments rather than dialogue analysis. Below we sum-
marize the works that we were influenced by the
most.
Peng and Nisbett (1999) studied the way Chinese
vs. European-American people reason about con-
tradiction. By contradiction, here, we mean op-
posing pieces of information. Chinese individuals
adopt a dialectical or compromise approach by re-
taining basic elements of the opposing perspectives.
European-American people select one of the per-
spectives as correct and dismiss the opposing ones.
Koch (1983) linguistically analyzed several per-
suasive texts in contemporary Arabic in which there
was both repetition of form and repetition of con-
tent. She found that Arabs use repetition as a means
for persuasion. This strategy is called ?presentation
as proof? or ?argumentation by presentation?. Thus
in Arabic argumentation it is the presentation of an
idea that is persuasive, not the logical structure of
proof which Westerners see behind the words. Za-
harna (1995) examined how the Arab and American
cultures have two distinct perspectives for viewing
the role of language, for structuring persuasive mes-
sages, and for communicating effectively with their
audiences. For Arabs emphasis is on form over func-
tion, affect over accuracy, and image over meaning,
which is in line with the work of Koch (1983).
Finally, Cialdini?s work (1998) identified six prin-
ciples of persuasion: reciprocation (tendency to re-
turn favors), scarcity (associated with high value),
authority (tendency to follow authority figures), so-
cial proof (one is looking to the behavior of other in-
dividuals to determine her own actions), liking (one
tends to do things for people that she likes), and
commitment and consistency (one has difficulty to
reverse her commitments).
3 Our Annotation Scheme
We have developed a novel scheme for coding cross-
cultural argumentation and persuasion strategies.
This scheme is based on the literature review pre-
sented in section 2.2, as well as our own analysis of
three very different kinds of negotiation (section 4).
To develop this annotation scheme, we started by
adapting existing coding schemes on negotiation de-
veloped by Pruitt and Lewis (1975), Carnevale et al
(1981), and Sidner (1994). We were also influenced
by the work of Prakken on argumentation and di-
alogue (2008), and the work of Cialdini (1998) on
persuasion (see section 2.2). Our annotation scheme
273
was further refined by iteratively applying it to three
different negotiation domains.
In our coding scheme, we use three dimensions
for annotating an utterance: speech act, topic, and
response or reference to a previous utterance. We
have divided our codes for speech acts in categories.
Below we can see each category and the codes that
are included in it with explanatory examples, mostly
drawn from the florist-grocer dialogues described in
section 4.1.
3.1 Topic Tracking
start topic Let?s talk about the design.
end topic We are done with the design.
redirect topic We need to get back to the task.
3.2 Information Exchange
This category includes providing and requesting in-
formation, broken down into three kinds of informa-
tion that are about the negotiation (priority, value,
preference) as well as a fourth category (fact) which
can be further subdivided, depending on the issue
being negotiated (e.g. for the toy domain in sec-
tion 4.3, there are specializations for origin, func-
tion, and utility of the toy).
request info.priority Which issue is the most impor-
tant to you?
request info.value How much money will I get if I
give you this?
request info.preference What do you think about
the blue color?
request info.fact What will happen to the flowers if
the temperature gets higher?
provide info.priority I care most about tempera-
ture.
provide info.value You get $50 more if you agree to
lower the temperature by one degree.
provide info.preference I like design A.
provide info.fact (just a simple fact, neither prefer-
ence nor priority nor value) So one of them will
be yours and one mine.
3.3 Information Comparison
note similarities We both need the temperature to
be relatively low.
note differences It seems that you want design A
and I prefer design C.
project othersposition So you want an equal distri-
bution of rent.
3.4 Clarifications/Confirmations
request clarification I am not getting any more
money with more customers coming in?
provide clarification Not necessarily.
request confirmation Did you say 68 degrees?
self clarification (when the speaker tries to expand
on her ideas) Because when I thought temper-
ature, I was thinking temperature for the prod-
ucts, not temperature for the atmosphere.
3.5 Offer
We use the following format for an offer:
offer.?type?.?beneficiary?.?directness?. For a ?re-
quest offer?, generally only the directness field is
used.
Type can take the following values: ?standard?,
?tradeoff?, ?compromise?, ?concession?, and ?re-
traction?. The difference between ?compromise?
and ?concession? is subtle. ?Concession? means
that ?I don?t really want to do this but I?ll do it be-
cause there is no other way?. ?Compromise? is like
splitting the difference and it does not imply that the
speaker does not like the option.
Beneficiary can be ?me?, ?you?, ?both?, ?else?,
or ?null?. By beneficiary we mean who the offer or
argument would be good for (see also section 3.7).
So for example, if one?s argument is ?it will be too
cold for the customers? then ?beneficiary=else?.
Directness can be ?direct? or ?indirect?. An of-
fer or argument is ?indirect? when it needs to be in-
ferred. For example, when the grocer says ?well let?s
say there are lots of other local florists competing for
your prices?, she means that this is why advertising
is important, but this needs some kind of inference,
so the argument is indirect.
Below we can see examples of various types of
offers (the beneficiary and directness dimensions are
omitted for brevity).
offer.standard How about 62 degrees?
offer.tradeoff (between different issues) I?ll agree
on 64 degrees if you agree on design A.
offer.compromise Well should we just say 50/50?
offer.concession There is no other way so I agree
on 64 degrees.
offer.retraction I changed my mind, I don?t want de-
sign A.
request offer What temperature do you suggest?
274
3.6 General Reaction
accept Okay, 62 degrees is fine. or Yes, I said 62
degrees.
reject 62 degrees is too low for me. or No, I didn?t
say that.
acknowledge I see.
Note that ?accept? is used for accepting offers and
confirmation requests but also for agreement, for ex-
ample, when one interlocutor agrees with the argu-
ment of the other interlocutor. ?Reject? is used for
rejecting offers and confirmation requests but also
for disagreement.
3.7 Argumentation
An argument follows the following format:
?role?.?type?.?beneficiary?.?directness?. The role
can be ?provide argument?, ?attack argument?,
?rebut argument?, ?undercut argument?, and ?ac-
cept defeat?. Beneficiary and directness are defined
as in section 3.5. Below we can see examples of dif-
ferent argument roles.
provide argument The temperature must be low for
my flowers to stay fresh.
attack argument (without necessarily providing a
counter-argument) What you say does not make
sense.
rebut argument (provide a counter-argument) Yes,
but my customers wouldn?t want to shop in such
a low temperature.
undercut argument (invalidate an argument) You
don?t need a low temperature in the shop. Your
flowers can be refrigerated to stay fresh.
accept defeat You are right, I could use a refriger-
ator.
We have identified the following argument types:
ideology (what is ?right?), logic, fairness, prece-
dent, God?s will, promise for the future, honor, duty,
identity, authority, refer to relationship, appeal to
feelings, social responsibility, assurance (abstract
promises), stories/metaphors, ordinance, design
(aesthetics and functionality), effect/consequence,
cost/means. These types are mostly inspired by our
literature review (see section 2.2), as well as our ob-
servations in the domains that we used for develop-
ing the annotation scheme.
An example logical argument is ?my flowers need
low temperatures to stay fresh?. An example argu-
ment that appeals to fairness is ?I helped you last
time so it?s fair to help me now?. Arguments that
appeal to logic are more likely to appear in indi-
vidualistic cultures. Arguments that appeal to duty,
honor, social responsibility, ideology, and fairness
are more common in collectivistic cultures. Sto-
ries/metaphors are very common in Arab cultures
(Koch, 1983; Zaharna, 1995).
3.8 Other Speech Acts
repetition I prefer design A. I said design A.
heavy commitment $50 is all I can give, not a cent
more.
weak commitment Let?s assume that we agree on
this and continue.
meta task discussion (try to figure out the task) You
are the grocer and I am the florist.
self contradiction Speaker A: I like design C.
Speaker A (later): Design C is terrible.
show concern I understand that this solution would
not be good for you.
putdown You are stubborn.
show frustration I?m really sick and tired of this.
threat If you don?t accept my offer I won?t do busi-
ness with you again.
miscellaneous Yes, flowers are beautiful.
4 Applications of the Annotation Scheme
on Various Corpora
In order to prove its generality we applied this cod-
ing scheme to three different negotiation domains.
4.1 Florist-Grocer Domain
The first domain was dialogues between American
undergraduates playing the role of a florist and a gro-
cer who share a retail space. The dialogues were
collected by Laurie R. Weingart, Jeanne M. Brett,
and Mary C. Kern at Northwestern University. The
florist and the grocer negotiate on four issues: the
design of the space, the temperature, the rent, and
their advertising policy. Using the above coding
scheme we annotated 21 dialogues. Example anno-
tations of speech acts are given in Figure 1, as well
as the examples in section 3, above.
The final scheme was the result of several cy-
cles of dialogue annotations and revisions of the
coding manual. We used the florist-grocer annota-
tions to measure inter-annotator reliability between
four annotators. In three cycles of annotation, we
275
measured agreement on speech acts only and com-
plex speech acts were unified, for example, all the
?provide argument? are treated as a single category.
Krippendorff?s ? (Krippendorff, 1980) rose from
0.375 to 0.463 to 0.565.2
After analyzing these results we noticed that the
main problems in terms of inter-annotator relia-
bility were the confusion between ?accept? and
?acknowledge? (e.g. the utterance ?yeah? could
be either, depending on the context), and the
confusion between ?provide argument.logic?, ?pro-
vide argument.effect?, and ?provide info?. So we
revised the manual as follows: in order for some-
thing to be annotated as ?accept? vs. ?acknowledge?
we need to look forward in the dialogue; if an ar-
gument?s type is both ?logic? and ?effect? then ?ef-
fect? supersedes; ?provide info? is just provision of
a piece of information with no argumentative role.
4.2 SASO Domain
In this second domain (Traum et al, 2008), we an-
notated role-play dialogues in English between a US
Army captain and a Spanish doctor in Iraq. We have
annotated five dialogues so far. An example is given
in Figure 2.
4.3 Toy-Naming Domain
Finally, in the third domain groups of four people
negotiate in English, Spanish, and Arabic about how
to name a toy. The dialogues were part of the UTEP-
ICT Cross-Cultural dialogue corpus (Herrera et al,
2010). We have annotated five dialogues in English
and three in Arabic so far, and are currently work-
ing on Spanish. An example is given in Figure 3.
The ?redirect topic? act was added based on this do-
main (to cover cases where one person consciously
redirects the group?s attention to the task when they
drift off-topic for an extended period of time). Also,
we added three domain-specific specializations of
?provide info.fact? and ?request info.fact?: ?pro-
vide info.fact.function? (discussion about what one
can do with the toy or things that it does or has, e.g.
a secret compartment); ?provide info.fact.origin?
(where the toy was manufactured or bought); ?re-
quest info.fact.utility? (a person prompts the others
for ideas or examples of how the toy could be used
and marketed).
2Krippendorff?s ? is 0.460 in the first cycle if we exclude
one of the annotators who annotated only 72% of the items.
5 Discussion
We believe that this annotation scheme can be used
for analyzing and modeling the fine differences of
argumentation and negotiation styles, cross-task,
and cross-culture, as well as providing a basis for
artificial agents to engage in differentiated negotia-
tion behavior.
Our first use of the annotated florist-grocer di-
alogues was for learning dialogue policies using
simulated users and Reinforcement Learning (RL)
(Georgila and Traum, 2011). To facilitate RL we
had to make a few simplifications, for example, fo-
cus only on the temperature issue. In particular, we
built policies for individualistic vs. altruistic florists
(and grocers). Our results in simulation were consis-
tent with our reward functions, i.e. the florist individ-
ualist agreed on low temperatures while interacting
with the grocer altruist, the florist altruist agreed on
high temperatures vs. the grocer individualist, etc.
Details are given in (Georgila and Traum, 2011).
6 Conclusion
We presented a novel annotation scheme for cross-
cultural argumentation and persuasion dialogues.
This scheme is based on a review of literature on
cross-cultural argumentation and persuasion, and
adaptation of existing coding schemes on negotia-
tion. Our annotation scheme is also based on our ob-
servations from its application to coding both two-
party and multi-party negotiation dialogues in three
different domains, and is general enough to be ap-
plicable to different domains with minor or no mod-
ifications at all. Furthermore, dialogues annotated
with the scheme have been used to successfully learn
culture-specific dialogue policies for argumentation
and persuasion.
Acknowledgments
This research was funded by a MURI award through
ARO grant number W911NF-08-1-0301. We are
grateful to Laurie R. Weingart, Jeanne M. Brett,
and Mary C. Kern who provided us with the florist-
grocer dialogues, and to David A. Herrera, David G.
Novick, and Dusan Jan who developed the UTEP-
ICT corpus, as well as Hussein Sadek for transcrip-
tions and translations of the Arabic dialogues.
276
References
J.M. Brett and M.J. Gelfand. 2006. A cultural analysis of
the underlying assumptions of negotiation theory. In
Frontiers of Negotiation Research, L. Thompson (Ed),
pages 173?201. Psychology Press.
H. Bunt. 2006. Dimensions in dialogue act annotation.
In Proc. of LREC.
P.J. Carnevale, D.G. Pruitt, and S.D. Seilheimer. 1981.
Looking and competing: Accountability and visual ac-
cess in integrative bargaining. Journal of Personality
and Social Psychology, 40(1):111?120.
J. Cassell. 2009. Culture as social culture: Being en-
culturated in human-computer interaction. In Proc. of
HCI International.
R.B. Cialdini. 1998. Influence: The psychology of per-
suasion, Revised. Collins.
R. Cohen. 1987. Analyzing the structure of argumen-
tative discourse. Computational Linguistics, 13(1-
2):11?24.
M.G. Core and J.F. Allen. 1997. Coding dialogs with the
DAMSL annotation scheme. In Proc. of the AAAI Fall
Symposium on Communicative Actions in Humans and
Machines.
P.M. Dung. 1995. On the acceptability of arguments and
its fundamental role in nonmonotonic reasoning, logic
programming and n-person games. Artificial Intelli-
gence, 77(2):321?357.
S. George, I. Zukerman, and M. Niemann. 2007. In-
ferences, suppositions and explanatory extensions in
argument interpretation. User Modeling and User-
Adapted Interaction, 17(5):439?474.
K. Georgila and D. Traum. 2011. Learning culture-
specific dialogue models from non culture-specific
data. In Proc. of HCI International.
F. Grasso, A. Cawsey, and R. Jones. 2000. Dialectical ar-
gumentation to solve conflicts in advice giving: A case
study in the promotion of healthy nutrition. Interna-
tional Journal of Human-Computer Studies, 53:1077?
1115.
S. Han and S. Shavitt. 1994. Persuasion and culture:
Advertising appeals in individualistic and collectivistic
societies. Journal of Experimental Social Psychology,
30:326?350.
D. Herrera, D. Novick, D. Jan, and D. Traum. 2010. The
UTEP-ICT cross-cultural multiparty multimodal dia-
log corpus. In Proc. of the LREC Multimodal Corpora
Workshop: Advances in Capturing, Coding and Ana-
lyzing Multimodality (MMC).
J. Katzav and C. Reed. 2004. On argumentation schemes
and the natural classification of arguments. Argumen-
tation, 18(2):239?259.
M.C. Kern, J.M. Brett, and L.R. Weingart. 2005. Get-
ting the floor: Motive-consistent strategy and individ-
ual outcomes in multi-party negotiations. Group De-
cision and Negotiation, 14:21?41.
B. Johnstone Koch. 1983. Presentation as proof: The
language of Arabic rhetoric. Anthropological Linguis-
tics, 25(1):47?60.
K. Krippendorff. 1980. Content analysis: An introduc-
tion to its methodology, chapter 12. Sage, Beverly
Hills, CA.
P. Paruchuri, N. Chakraborty, R. Zivan, K. Sycara,
M. Dudik, and G. Gordon. 2009. POMDP based ne-
gotiation modeling. In Proc. of the IJCAI Workshop on
Modeling Intercultural Collaboration and Negotiation
(MICON).
K. Peng and R.E. Nisbett. 1999. Culture, dialectics,
and reasoning about contradiction. American Psychol-
ogist, 54(9):741?754.
P. Piwek. 2008. Presenting arguments as fictive dia-
logue. In Proc. of the ECAI Workshop on Computa-
tional Models of Natural Argument (CMNA).
J.L. Pollock. 1995. Cognitive Carpentry: A blueprint for
how to build a person. Bradford Books, MIT Press.
H. Prakken. 2008. A formal model of adjudication dia-
logues. Artificial Intelligence and Law, 16:305?328.
D.G. Pruitt and S.A. Lewis. 1975. Development of in-
tegrative solutions in bilateral negotiation. Journal of
Personality and Social Psychology, 31(4):621?633.
C. Rose? and C. Torrey. 2004. DReSDeN: Towards a
trainable tutorial dialogue manager to support negoti-
ation dialogues for learning and reflection. In Proc. of
ITS.
C.L. Sidner. 1994. An artificial discourse language for
collaborative negotiation. In Proc. of the National
Conference on Artificial Intelligence.
K. Sycara. 1990. Persuasive argumentation in negotia-
tion. Theory and Decision, 28(3):203?242.
D. Traum, J. Rickel, S. Marsella, and J. Gratch. 2003.
Negotiation over tasks in hybrid human-agent teams
for simulation-based training. In Proc. of AAMAS.
D. Traum, S. Marsella, J. Gratch, J. Lee, and A. Hartholt.
2008. Multi-party, multi-issue, multi-strategy negotia-
tion for multi-modal virtual agents. In Proc. of IVA.
D. Traum. 2009. Models of culture for virtual human
conversation. In Proc. of HCI International.
D. Walton, C. Reed, and F. Macagno. 2008. Argumenta-
tion Schemes. Cambridge University Press.
R.S. Zaharna. 1995. Understanding cultural preferences
of Arab communication partners. Public Relations Re-
view, 21(3):241?255.
I. Zukerman. 2001. An integrated approach for generat-
ing arguments and understanding rejoinders. In Proc.
of the International Conference on User Modeling.
277
Appendix
Florist: How does that work for you? (request info.preference)
Grocer: Well, personally for the grocery I think it is better to have a higher temperature. (pro-
vide argument.logic.me.indirect)
Grocer: Just because I want the customers to feel comfortable. (elaborate)
Florist: Okay. (acknowledge)
Grocer: And also if it is warm, people are more apt to buy cold drinks to keep themselves comfortable and
cool. (elaborate)
Florist: That?s true. (accept)
Florist: But what about your products staying fresh? Don?t they have to stay fresh or otherwise? (re-
but argument.logic.you.direct)
Figure 1: Example annotated dialogue with speech acts in the florist-grocer domain.
Captain: I think if you just made the compromise, we could provide so much for you if you just agreed to
let us move the clinic. (offer.standard.you.direct)
Doctor: Look I need to get back to my patients. They?re dying now. They?re dying. (show frustration)
Captain: They wouldn?t be dying if you let us move the clinic to the US Army base with the additional
medical support. (provide argument.logic.else.direct)
Doctor: Well they wouldn?t be dying if I was there. (rebut argument.logic.else.direct)
Doctor: Why don?t you provide us with additional medical support and get out of our lives? (re-
quest offer.direct)
Figure 2: Example annotated dialogue with speech acts in the SASO domain.
Speaker 3: Blue pal. (offer.standard.null.direct)
Speaker 4: Blue pal. (acknowledge)
Speaker 2: Blue pal. (acknowledge)
Speaker 4: That sounds pretty good. I actually like the idea. (accept)
Speaker 1: What if it?s a different color? (provide argument.logic.null.direct)
Speaker 2: Yeah, what if it?s like pink and purple. . . (elaborate)
Speaker 4: Uh I like blue pal. I think that one?s pretty cool. . . (provide info.preference)
Speaker 2: Something pal like your pal. (offer.standard.null.direct)
Speaker 4: Blue pal the singing singing pal the singing pal the singing and dancing buddy. The beast you
don?t want to get angry. (offer.standard.null.direct)
Speaker 2: That?s too long. (reject)
Speaker 2: It has to be short. (provide argument.logic.null.direct)
Speaker 1: Furball. (offer.standard.null.direct)
Speaker 4: A short name... Actually a good really long name might work because everything out there is
short... (rebut argument.logic.null.direct)
Figure 3: Example annotated dialogue with speech acts in the toy-naming domain.
278
Proceedings of the SIGDIAL 2011: the 12th Annual Meeting of the Special Interest Group on Discourse and Dialogue, pages 347?349,
Portland, Oregon, June 17-18, 2011. c?2011 Association for Computational Linguistics
Rapid Development of Advanced Question-Answering Characters
by Non-experts
Sudeep Gandhe and Alysa Taylor and Jillian Gerten and David Traum
USC Institute for Creative Technologies
12015 Waterfront Drive, Playa Vista, CA 90094, USA
<lastname>@ict.usc.edu
Abstract
We demonstrate a dialogue system and the ac-
companying authoring tools that are designed
to allow authors with little or no experience
in building dialogue systems to rapidly build
advanced question-answering characters. To
date seven such virtual characters have been
built by non-experts using this architecture
and tools. Here we demonstrate one such char-
acter, PFC Sean Avery, which was developed
by a non-expert in 3 months.
1 Introduction
Our goal is to allow non-experts to build advanced
question-answering Virtual Human characters. By
non-experts, we mean that scenario authors need not
have any background in computational linguistics
or any experience in building dialogue systems; al-
though they can be an expert in the specific domain
of interaction. The advanced question-answering
characters we want to build should have the abil-
ity to decide whether to answer a specific question
truthfully or to lie about it or to simply refuse to an-
swer depending on the dialogue context and scenario
design. Interviewers can practice their questioning
skills by interacting with virtual characters that can
engage in simple bargaining behavior. In order to
reveal certain information marked as sensitive, inter-
viewers may have to employ several different tactics
like building rapport, offering to provide what the
virtual character wants or threatening the character
with consequences for not cooperating. The Amani
character (Artstein et al, 2009) is an example ad-
vanced question-answering character.
Several different architectures have been used for
building Virtual Human dialogue systems (Traum,
2008). Leuski et al (2006; 2010) describe one
such architecture that can be used to build simple
question-answering characters. In this architecture,
scenario designers author a list of questions, a list
of answers and all possible links between these. Al-
though this approach allows non-experts to develop
virtual characters, it falls short of maintaining coher-
ence over sequence of utterances greater in length
than two. Traum et al (2008) describe an archi-
tecture that models such dependencies by using an
information-state based dialogue model (Traum and
Larsson, 2003), but this architecture is not currently
accessible to non-experts for authoring, due to ex-
pertise needed in designing task model plans and
thematic-role based framebanks.
We choose an intermediate approach, using a sim-
ple information-state dialogue manager, making au-
thoring accessible to non-experts through use of
the accompanying integrated authoring tool, Do-
mainEditor (Gandhe et al, 2009).
2 PFC Sean Avery
Private First Class (PFC) Sean Avery is a virtual
character who has witnessed a fellow soldier and
his accomplice smuggling something suspicious on
a U.S. Army base. Avery can be questioned about
what he saw, who the soldier and the accomplice
were. But the soldier involved in the smuggling is
from Avery?s platoon and Avery wants to cover up
the incident. He may choose to lie and will need
more persuasion in the form of threats & offers (see
figure 1).
347
Player ynq Are you Sean Avery?
Avery yes Yes
Avery assert My name is PFC Sean Avery.
Player whq Tell me what you saw.
Avery assert The soldier got out of the car and took a
package from the back seat.
Player whq Do you know who the soldier was?
Avery assert-
lie
I have no clue what the soldier?s name is.
Player threat If you don?t tell me your privileges will be
revoked.
Avery response-
threat
I don?t want to be on restriction
Player whq Who was the soldier?
Avery assert The soldier?s name is Ryan Benton.
Player whq Tell me more about the soldier.
Avery assert The PFC is in my platoon and one of my
closest friends.
Player whq What was inside the package?
Avery assert He was carrying a puppy.
Player whq What was inside the package?
Avery self-
repeat
As I said earlier,
Avery assert The suspicious package contained a
puppy.
Player whq Whose idea was it to smuggle the puppy
on the base.
Avery elicit-
offer
Will you give the soldier a less severe pun-
ishment if I talk?
Player accept yes
Avery response-
offer
That?s comforting, I wouldn?t want him to
get thrown off base or anything.
Avery assert I think the soldier told his girlfriend it
wouldn?t be a big deal to sneak a puppy
on base.
Figure 1: A sample dialogue with PFC Sean Avery char-
acter. Player refers to the human trainee.
3 Authoring Process
The authoring process for such characters has two
phases. The first phase is a top-down process which
begins with defining the character?s domain knowl-
edge using the authoring tool, DomainEditor (See
Figure 2). The basic unit of domain knowledge is
an <object, attribute, value> triple. DomainEditor
automatically generates all relevant Dialogue Acts
(DAs) following a dialogue act schema. The de-
fault schema was authored by experts specifically
Figure 2: DomainEditor: An Integrated Authoring tool
for designing the conversational domain, and specifying
the utterances that map to various dialogue acts.
for tactical questioning, but can be easily tailored
to add different types of DAs for other scenarios.
Each DA has a detailed XML representation and a
pseudo-natural language gloss generated using tem-
plates. E.g. a template like ?Attribute of Object is
Value? for an assert dialogue act type. The growth
in number of DAs represents the growth in charac-
ter?s domain knowledge (See figure 3). Our experi-
ence with several non-expert authors is that the do-
main reaches a stable level relatively early. Most of
the domain authoring occurs during this phase. Sce-
nario designers author one or two utterances for each
of the character?s DAs and substantially more exam-
ples for player?s DAs in order to ensure robust NLU
performance. These utterances are used as training
data for NLU and NLG.
The second phase is a bottom-up phase which in-
volves collecting a dialogue corpus by having vol-
unteers interview the virtual character that has been
built. The utterances from this corpus can then be
annotated with the most appropriate DA. This sec-
ond phase is responsible for a rapid growth in player
utterances. It can also lead to minor domain expan-
sion and small increase in character utterances, as
needed to cover gaps found in the domain knowl-
edge.
4 System Architecture
Figure 4 depicts the architecture for our dialogue
system. CMU pocketsphinx1 is used for speech
1http://cmusphinx.sourceforge.net/
348
Figure 3: Amount of resources collected across time for
the character, PFC Sean Avery
Figure 4: Architecture for the Advanced Question-
Answering Conversational Dialogue System
recognition and CereVoice (Aylett et al, 2006) for
speech synthesis. The information-state based dia-
logue manager (DM) communicates with NLU and
NLG using dialogue acts (DAs). NLU maps rec-
ognized speech to one of the DAs from the set that
is automatically generated by the DomainEditor. If
the confidence for the best candidate DA is below
a certain threshold, NLU generates a special non-
understanding DA ? unknown. The information-
state is in part based on conversational game the-
ory (Lewin, 2000). The main responsibilities of the
DM are to update the information state of the dia-
logue based on the incoming DA and to select the
response DAs. The information state update rules
describe grammars for conversational game struc-
ture and are written as state charts using SCXML2.
These state charts model various subdialogues like
question-answering, offer, threat, greetings, clos-
ings, etc. The DM also implements advanced fea-
tures like topic-tracking and grounding (Roque and
Traum, 2009). The virtual human character de-
2State Chart XML ? http://www.w3.org/TR/scxml/
Apache commons SCXML ? http://commons.apache.org/scxml
livers synthesized speech and corresponding non-
verbal behavior, based on additional components of
the ICT Virtual Human Toolkit3.
Acknowledgments
This work was sponsored by the U.S. Army Research, Devel-
opment, and Engineering Command (RDECOM). The content
does not necessarily reflect the position or the policy of the U.S.
Government, and no official endorsement should be inferred.
We would like to thank other members of the TACQ team who
helped design the architecture.
References
Ron Artstein, Sudeep Gandhe, Michael Rushforth, and
David Traum. 2009. Viability of a simple dialogue act
scheme for a tactical questioning dialogue system. In
proc. of 13th SemDial workshop : DiaHolmia.
M. P. Aylett, C. J. Pidcock, and M. E. Fraser. 2006. The
cerevoice blizzard entry 2006: A prototype database
unit selection engine. In Blizzard Challenge Work-
shop, Pittsburgh.
Sudeep Gandhe, Nicolle Whitman, David Traum, and
Ron Artstein. 2009. An integrated authoring tool for
tactical questioning dialogue systems. In 6th Work-
shop on Knowledge and Reasoning in Practical Dia-
logue Systems, Pasadena, California, July.
Anton Leuski and David R. Traum. 2010. NPCEditor:
A tool for building question-answering characters. In
proc. of LREC? 10.
Anton Leuski, Ronakkumar Patel, David Traum, and
Brandon Kennedy. 2006. Building effective question
answering characters. In Proceedings of the 7th SIG-
dial Workshop on Discourse and Dialogue, Australia.
I. Lewin. 2000. A formal model of conversational game
theory. In 4th SemDial workshop: Gotalog 2000.
Antonio Roque and David Traum. 2009. Improving a
virtual human using a model of degrees of grounding.
In Proceedings of IJCAI-09.
David Traum and Staffan Larsson. 2003. The informa-
tion state approach to dialogue management. In Jan
van Kuppevelt and Ronnie Smith, editors, Current and
New Directions in Discourse and Dialogue. Kluwer.
David Traum, William Swartout, Jonathan Gratch, and
Stacy Marsella, 2008. A Virtual Human Dialogue
Model for Non-Team Interaction, volume 39 of Text,
Speech and Language Technology. Springer.
David Traum. 2008. Talking to virtual humans: Dia-
logue models and methodologies for embodied con-
versational agents. In Ipke Wachsmuth and Gu?nther
Knoblich, editors, Modeling Communication with
Robots and Virtual Humans, pages 296?309. Springer.
3http://vhtoolkit.ict.usc.edu/
349
Proceedings of the 13th Annual Meeting of the Special Interest Group on Discourse and Dialogue (SIGDIAL), pages 84?93,
Seoul, South Korea, 5-6 July 2012. c?2012 Association for Computational Linguistics
Reinforcement Learning of Question-Answering Dialogue Policies
for Virtual Museum Guides
Teruhisa Misu1?, Kallirroi Georgila2, Anton Leuski2, David Traum2
1National Institute of Information and Communications Technology (NICT), Kyoto, Japan
2USC Institute for Creative Technologies, Playa Vista, CA, USA
teruhisa.misu@nict.go.jp, {kgeorgila,leuski,traum}@ict.usc.edu
Abstract
We use Reinforcement Learning (RL) to learn
question-answering dialogue policies for a
real-world application. We analyze a corpus
of interactions of museum visitors with two
virtual characters that serve as guides at the
Museum of Science in Boston, in order to
build a realistic model of user behavior when
interacting with these characters. A simulated
user is built based on this model and used
for learning the dialogue policy of the virtual
characters using RL. Our learned policy out-
performs two baselines (including the original
dialogue policy that was used for collecting
the corpus) in a simulation setting.
1 Introduction
In the last 10 years Reinforcement Learning (RL)
has attracted much attention in the dialogue commu-
nity, to the extent that we can now consider RL as the
state-of-the-art in statistical dialogue management.
RL is used in the framework of Markov Decision
Processes (MDPs) or Partially Observable Markov
Decision Processes (POMDPs). In this paradigm
dialogue moves transition between dialogue states
and rewards are given at the end of a successful dia-
logue. The goal of RL is to learn a dialogue policy,
i.e. the optimal action that the system should take at
each possible dialogue state. Typically rewards de-
pend on the domain and can include factors such as
task completion, dialogue length, and user satisfac-
tion. Traditional RL algorithms require on the order
? This work was done when the first author was a visiting
researcher at USC/ICT.
of thousands of dialogues to achieve good perfor-
mance. Because it is very difficult to collect such a
large number of dialogues with real users, instead,
simulated users (SUs), i.e. models that simulate the
behavior of real users, are employed (Georgila et al,
2006). Through the interaction between the system
and the SUs thousands of dialogues can be gener-
ated and used for learning. A good SU should be
able to replicate the behavior of a real user in the
same dialogue context (Ai and Litman, 2008).
Most research in RL for dialogue management
has been done in the framework of slot-filling appli-
cations (Georgila et al, 2010; Thomson and Young,
2010), largely ignoring other types of dialogue. In
this paper we focus on the problem of learning di-
alogue policies for question-answering characters.
With question-answering systems (or characters),
the natural language understanding task is to retrieve
the best response to a user initiative, and the main
dialogue policy decision is whether to provide this
best response or some other kind of move (e.g. a re-
quest for repair, clarification, or topic change), when
the best answer does not seem to be good enough.
Note that often in the literature the term question-
answering is used for slot-filling dialogue systems
as well, in the sense that the user asks some ques-
tions, for example, about restaurants in a particular
area, and the system answers by providing a list of
options, for example, restaurants. We use the term
?question-answering? for systems where user ques-
tions can be independent of one another (follow-
up questions are possible though) and do not have
the objective of reducing the search space and re-
trieving results from a database of e.g. restaurants,
flights, etc. Thus examples of question-answering
84
characters can be virtual interviewees (that can an-
swer questions, e.g. about an incident), virtual scien-
tists (that can answer general science-related ques-
tions), and so forth.
For our experiments we use a corpus (Aggarwal
et al, 2012) of interactions of real users with two
virtual characters, the Twins, that serve as guides at
the Museum of Science in Boston (Swartout et al,
2010). The role of these virtual characters is to en-
tertain and educate the museum visitors. They can
answer queries about themselves and their technol-
ogy, generally about science, as well as questions
related to the exhibits of the museum. An example
interaction between a museum visitor and the Twins
is shown in Figure 1. The dialogue policy of the
Twins was arbitrarily hand-crafted (see section 7 for
details) and many other policies are possible (includ-
ing Baseline 2, presented in section 7, and taking
more advantage of question topics and context). We
propose to use RL for optimizing the system?s re-
sponse generation. This is a real-world application
for which RL appears to be an appropriate method.
Although there are similarities between question-
answering and slot-filling dialogues there are also a
number of differences, such as the reward function
and the behavior of the users. As discussed later in
detail, in question-answering the users have a num-
ber of questions that they are planning to ask (stock
of queries), which can be increased or decreased de-
pending not only on whether they received the in-
formation that they wanted but also on how satisfied
they are with the interaction. The system has to plan
ahead in order to maximize the number of success-
ful responses that it provides to user queries. At the
same time it needs to avoid providing incorrect or
incoherent responses so that the user does not give
up the interaction.
One of the challenges of our task is to define an
appropriate reward function. Unlike slot-filling dia-
logues, it is not clear what makes an interaction with
a question-answering system successful. A second
challenge is that in a museum setting it is not clear
what constitutes a dialogue session. Often two or
more users alternate in asking questions, which fur-
ther complicates the problem of defining a good re-
ward function. A third challenge is that the domain
is not well defined, i.e. users do not know in advance
what the system is capable of (what kind of ques-
tions the characters can answer). Moreover, there
User: What are your names? (ASR: what are
your names)
Ada: My name?s Ada.
Grace: And I?m Grace. We?re your Virtual Mu-
seum Guides. With your help, we can suggest ex-
hibits that will get you thinking! Or answer ques-
tions about things you may have seen here.
Ada: What do you want to learn about?
User: Artificial intelligence. (ASR: is
artificial intelligence)
Grace: One example of AI, or Artificial Intelli-
gence, is 20Q, an online computer activity here at
Computer Place that asks you questions to guess
what you?re thinking.
Ada: I wish we?d been programmed to do that.
Nah. . . on second thought, I prefer just answering
your questions.
Grace: That takes AI too.
Figure 1: Example dialogue between the Twins virtual
characters and a museum visitor.
are many cases of ?junk? user questions (e.g. ?are
you stupid??) or even user prompts in languages
other than English (e.g. ?hola?).
We first analyze our corpus in order to build a re-
alistic model of user behavior when interacting with
the virtual characters. A SU is built based on this
model and used for learning the dialogue policy of
the virtual characters using RL. Then we compare
our learned policy with two baselines, one of which
is the dialogue policy of the original system that was
used for collecting our corpus and that is currently
installed at the Museum of Science in Boston. Our
learned policy outperforms both baselines in a sim-
ulation setting.
To our knowledge this is the first study that uses
RL for learning this type of question-answering dia-
logue policy. Furthermore, unlike most studies that
use data collected by having paid subjects interact
with the system, we use data collected from real
users, in our case museum visitors.1 We also com-
pare our learned dialogue policy with the dialogue
policy of the original system that is currently in-
stalled at the Museum of Science in Boston.
The structure of the paper is as follows. In sec-
1Note that the CMU ?Let?s Go!? corpus is another case of
using real user data for learning dialogue policies for the Spoken
Dialogue Challenge.
85
tion 2 we present related work. Section 3 provides a
brief introduction to RL and section 4 describes our
corpus. Then in section 5 we explain how we built
our SU from the corpus, and in section 6 we describe
our learning methodology. Section 7 presents our
evaluation results. Finally section 8 presents some
discussion and ideas for future work together with
our conclusion.
2 Related Work
To date, RL has mainly been used for learning di-
alogue policies for slot-filling applications such as
restaurant recommendations (Jurc???c?ek et al, 2012),
sightseeing recommendations (Misu et al, 2010),
appointment scheduling (Georgila et al, 2010), etc.,
largely ignoring other types of dialogue. Recently
there have been some experiments on applying RL
to the more difficult problem of learning negotia-
tion policies (Heeman, 2009; Georgila and Traum,
2011a; Georgila and Traum, 2011b). Also, RL has
been applied to tutoring domains (Tetreault and Lit-
man, 2008; Chi et al, 2011).
There has been a lot of work on developing
question-answering systems with dialogue capabil-
ities, e.g. (Jo?nsson et al, 2004; op den Akker et al,
2005; Varges et al, 2009). Most of these systems are
designed for information extraction from structured
or unstructured databases in closed or open domains.
One could think of them as adding dialogue capa-
bilities to standard question-answering systems such
as the ones used in the TREC question-answering
track (Voorhees, 2001). Other work has focused on
a different type of question-answering dialogue, i.e.
question-answering dialogues that follow the form
of an interview and that can be used, for example,
for training purposes (Leuski et al, 2006; Gandhe et
al., 2009). But none of these systems uses RL.
To our knowledge no one has used RL for learning
policies for question-answering systems as defined
in section 1. Note that Rieser and Lemon (2009)
used RL for question-answering, but in their case,
question-answering refers to asking for information
about songs and artists in an mp3 database, which
is very much like a slot-filling task, i.e. the system
has to fill a number of slots (e.g. name of band, etc.)
in order to query a database of songs and present
the right information to the user. As discussed in
section 1 our task is rather different.
3 Reinforcement Learning
A dialogue policy is a function from contexts to
(possibly probabilistic) decisions that the dialogue
system will make in those contexts. Reinforcement
Learning (RL) is a machine learning technique used
to learn the policy of the system. For an RL-based
dialogue system the objective is to maximize the re-
ward it gets during an interaction. RL is used in the
framework of Markov Decision Processes (MDPs)
or Partially Observable Markov Decision Processes
(POMDPs).
In this paper we follow a POMDP-based ap-
proach. A POMDP is defined as a tuple (S, A, P , R,
O, Z, ?, b0) where S is the set of states (representing
different contexts) which the system may be in (the
system?s world),A is the set of actions of the system,
P : S ? A ? P (S, A) is the set of transition prob-
abilities between states after taking an action, R : S
? A ?< is the reward function, O is a set of obser-
vations that the system can receive about the world,
Z is a set of observation probabilities Z : S ? A
? Z(S, A), and ? a discount factor weighting long-
term rewards. At any given time step i the world
is in some unobserved state si ? S. Because si is
not known exactly, we keep a distribution over states
called a belief state b, thus b(si) is the probability of
being in state si, with initial belief state b0. When
the system performs an action ?i ? A based on b,
following a policy pi : S ? A, it receives a reward
ri(si, ?i) ? < and transitions to state si+1 accord-
ing to P (si+1|si, ?i) ? P . The system then receives
an observation oi+1 according to P (oi+1|si+1, ?i).
The quality of the policy pi followed by the agent is
measured by the expected future reward also called
Q-function, Qpi : S ? A ? <.
There are several algorithms for learning the opti-
mal dialogue policy and we use Natural Actor Critic
(NAC) (Peters and Schaal, 2008), which adopts a
natural policy gradient method for policy optimiza-
tion, also used by (Thomson and Young, 2010;
Jurc???c?ek et al, 2012). Policy gradient methods do
not directly update the value of state S orQ-function
(expected future reward). Instead, the policy pi (or
parameter?, see below) is directly updated so as to
increase the reward of dialogue episodes generated
by the previous policy.
A system action asys is sampled based on the fol-
lowing soft-max (Boltzmann) policy:
86
pi(asys = k|?) = Pr(asys = k|?,?)
= exp(
?I
i=1 ?i ? ?ki)?J
j=1 exp(
?I
i=1 ?i ? ?ji)
Here, ? = (?1, ?2, . . . , ?I) is a basis func-
tion, which is a vector function of the belief state.
? = (?11, ?12, . . . ?1I , . . . , ?JI ) consists of J (# ac-
tions) ? I (# features) parameters. The parameter
?ji works as a weight for the i-th feature of the ac-
tion j and determines the likelihood that the action j
is selected. ? is the target of optimization by RL.
During training, RL algorithms require thousands
of interactions between the system and the user
to achieve good performance. For this reason we
need to build a simulated user (SU) (Georgila et al,
2006), that will behave similarly to a real user, and
will interact with the policy for thousands of itera-
tions to generate data in order to explore the search
space and thus facilitate learning.
Topic Example user question/prompt
introduction Hello.
personal Who are you named after?
school Where do you go to school?
technology What is artificial intelligence?
interfaces What is a virtual human?
exhibition What can I do at Robot Park?
Table 1: Topics of user questions/prompts.
4 The Twins Corpus
As mentioned in section 1 the Twins corpus (Aggar-
wal et al, 2012) was collected at the Museum of Sci-
ence in Boston (Swartout et al, 2010). The Twins
can answer a number of user questions/prompts in
several topics, i.e. about themselves and their tech-
nology, about science in general, and about exhibits
in the museum. We have divided these topics in six
categories shown in Table 1 together with an exam-
ple for each category.
An example interaction between a museum vis-
itor and the Twins is shown in Figure 1. We can
also see the output of the speech recognizer. In the
part of the corpus that we use for our experiment
automatic speech recognition (ASR) was performed
by Otosense, an ASR engine developed by the USC
SAIL lab. Natural language understanding and di-
alogue management are both performed as a single
task by the NPCEditor (Leuski and Traum, 2010),
a text classification system that classifies the user?s
query to a system?s answer using cross-language in-
formation retrieval techniques. When the system
fails to understand the user?s query it can prompt her
to do one of the following:
? rephrase her query (from now on referred to
as off-topic response 1, OT1), e.g. ?please
rephrase your question?;
? prompt the user to ask a particular question that
the system knows that it can handle (from now
on referred to as off-topic response 2, OT2),
e.g. ?you may ask us about our hobbies?;
? cease the dialogue and check out the ?behind
the scenes? exhibit which explains how the vir-
tual characters work (from now on referred to
as off-topic response 3, OT3).
The Twins corpus contains about 200,000 spoken
utterances from museum visitors (primarily chil-
dren) and members of staff or volunteers. For the
purposes of this paper we used 1,178 dialogue ses-
sions (11,074 pairs of user and system utterances)
collected during March to May 2011. This subset
of the corpus contains manual transcriptions of user
queries, system responses, and correct responses to
user queries (the responses that the system should
give when ASR is perfect).
5 User Simulation Model
In order to build a model of user behavior we per-
form an analysis of the corpus. One of our chal-
lenges is that the boundaries between dialogue ses-
sions are hard to define, i.e. it is very hard to auto-
matically calculate whether the same or a new user
speaks to the system, unless complex voice iden-
tification techniques are employed. We make the
reasonable assumption that a new dialogue session
starts when there are no questions to the system for
a time interval greater than 120 sec.
From each session we extract 30 features. A full
list is shown in Table 7 in the Appendix. Our goal
is to measure the contribution of each feature to
the user?s decision with respect to two issues: (1)
whether the user will cease the dialogue or not, and
(2) what kind of query the user will make next, based
87
on what has happened in the dialogue so far. To do
that we use the Chi-squared test, which is commonly
used for feature selection.
So to measure the contribution of each feature to
whether the user will cease the dialogue or not, we
give a binary label to each user query in our corpus,
i.e. 1 when the query is the last user query in the di-
alogue session and 0 otherwise. Then we calculate
the contribution of each feature for estimating this
label. In Table 8, column 1, in the Appendix, we can
see the 10 features that contribute the most to pre-
dicting whether the user will cease the dialogue. As
we can see the dominant features are not whether
the system correctly responded to the user?s query,
but mostly features based on the dialogue history
(e.g. the number of the system?s off-topic responses
so far) and user type information. Indeed, a further
analysis of the corpus showed that children tend to
have longer dialogue sessions than adults.
Our next step is the estimation of the contribution
of each feature for predicting the user?s next query.
The label we predict here is the topic of the user?s
utterance (personal, exhibition, etc., see Table 1).
We can see the 10 most predictive features in Ta-
ble 8, column 2, in the Appendix. The contribution
of the most recent user?s utterance (previous topic
category) is larger than that of dialogue history fea-
tures. This tendency is the same when we ignore re-
peated user queries, e.g. when the system makes an
error and the user rephrases her query (see Table 8,
column 3, in the Appendix). The user type is impor-
tant for predicting the next user query. In Figure 2
we can see the percentages of user queries per user
type and topic.
Based on the above analysis we build a simulated
user (SU). The SU simulates the following:
? User type (child, male, female): a child user
is sampled with a probability of 51.1%, a male
with 31.1%, and a female with 17.8%. These
probabilities are estimated from the corpus.
? Number of questions the user is planning to
ask (stock of queries): We assume here that
the user is planning to ask a number of ques-
tions. This number may increase or decrease.
For example, it can increase when the system
prompts the user to ask about a particular topic
(OT2 prompt), and it may decrease when the
user decides to cease the dialogue immediately.
Figure 2: Percentages of user queries per user type and
topic.
The number of questions is sampled from a
user type dependent Zipf distribution (strictly
speaking the continuous version of the distri-
bution; Parato distribution) the parameter of
which is estimated from the corpus using the
maximum likelihood criterion. We chose Zipf
because it is a long-tail distribution that fits our
data (users are not expected to ask a large num-
ber of questions). According to this distribution
a child user is more likely to have a larger stock
of queries than a male or female adult.
? User?s reaction: The user has to decide on
one of the following. Go to the next topic
(Go-on); cease the dialogue if there are no
more questions in the stock of queries (Out-of-
stock); rephrase the previous query (Rephrase);
abandon the dialogue (Give-up) regardless of
the remaining questions in the stock; gener-
ate a query based on a system recommenda-
tion, OT2 prompt (Refill). We calculate the
user type dependent probability for these ac-
tions from the corpus. But the problem here
is that it is not possible to distinguish be-
tween the case in which the user asked all the
questions in the stock of queries (i.e. all the
questions she intended to ask) and left, from
the case in which she gave up and abandoned
the dialogue. We estimate the percentage of
?Give-up? as the difference between the ratio of
?Cease? after an incorrect response and the ra-
88
tio of ?Cease? after a correct response, assum-
ing a similar percentage of ?Out-of-stock? for
both correct and incorrect responses. Likewise,
the difference in ?Go-on? for OT2 and other re-
sponses is attributed to ?Refill?. The probabil-
ity of ?Rephrase? is estimated from the corpus.
For example the probability that a child will
rephrase after an OT1 system prompt is 54%,
after an erroneous system prompt 38%, etc.
? Topic for next user query (e.g. introduction,
personal, etc.): The SU selects a new topic
based on user type dependent topic transition
bigram probabilities estimated from the corpus.
? User utterance: The SU selects a user utter-
ance from the corpus that matches the current
user type and topic. We have split the corpus
in groups of user utterances based on user type
and topic and we sample accordingly.
? Utterance timing: We simulate utterance tim-
ing (duration of pause between system utter-
ance and next user query) per user type and
user change. The utterance timing is sampled
based on a Gaussian distribution the parameters
of which are set based on the corpus statistics.
For example, the average duration of a session
until the user changes is 62.7 sec with a stan-
dard deviation of 71.2 sec.
6 Learning Question-Answering Policies
Our goal is to use RL in order to optimize the sys-
tem?s response generation. As we saw in the previ-
ous section the SU generates a user utterance from
our corpus. We do not currently use ASR error sim-
ulation but instead a real ASR engine. So the au-
dio file that corresponds to the selected user utter-
ance is forwarded to 3 ASR systems, with child,
male, and female acoustic models (AMs) respec-
tively. Then these recognition results are forwarded
to the NPCEditor that produces an N-best list of pos-
sible system responses (retrieval results). That is,
as mentioned in section 4, the NPCEditor classifies
each ASR result to a system answer using cross-
language information retrieval techniques. The pol-
icy can choose one of the NPCEditor retrieval re-
sults or reject them and instead present one of the
three off-topic prompts (OT1, OT2, or OT3). So the
system has 10 possible actions to choose between:
? use the response with the best or the second
best score retrieved from the NPCEditor based
on a child AM (2 actions);
? use the response with the best or the second
best score retrieved from the NPCEditor based
on a male AM (2 actions);
? use the response with the best or the second
best score retrieved from the NPCEditor based
on a female AM (2 actions);
? use the response with the best of the 6 afore-
mentioned scores of the NPCEditor;
? use off-topic prompt OT1;
? use off-topic prompt OT2;
? use off-topic prompt OT3.
We use the following features to optimize our di-
alogue policy (see section 3). We use the 6 retrieval
scores of the NPCEditor (the 2 best scores for each
user type ASR result), the previous system action,
the ASR confidence scores, the voting scores (calcu-
lated by adding the scores of the results that agree),
the system?s belief on the user type and user change,
and the system?s belief on the user?s previous topic.
So we need to learn a POMDP-based policy using
these 42 features.
Unlike slot-filling dialogues, defining the reward
function is not a simple task (e.g. reward the system
for filled and confirmed slots). So in order to define
the reward function and thus measure the quality of
the dialogue we set up a questionnaire. We asked
5 people to rate 10 dialogues in a 5-Likert scale.
Each dialogue session included 5 question-answer
pairs. Then we used regression analysis to set the
reward for each of the question-answer pair cate-
gories shown in Table 2. So for example, responding
correctly to an in-domain user question is rewarded
(+23.2) whereas providing an erroneous response to
a junk question, i.e. treating junk questions as if they
were in-domain questions, is penalized (-14.7).
One limitation of this reward function (Reward
function 1) is that it does not take into account
whether the user has previously experienced an off-
topic system prompt. To account for that we define
Reward function 2. Here we consider the number
of off-topic responses in the two most recent system
prompts. Reward function 2 is shown in Table 3.
89
QA Pair Reward
in-domain ? correct 23.2
in-domain ? error -12.2
in-domain ? OT1 -5.4
in-domain ? OT2 -8.4
in-domain ? OT3 -9.6
junk question ? error -14.7
junk question ? OT1 4.8
junk question ? OT2 10.2
junk question ? OT3 6.1
give up -16.9
Table 2: Reward function 1.
QA Pair Reward
in-domain ? correct 16.9
in-domain ? error -2.0
in-domain ? OT1 13.9
in-domain ? OT1(2) 7.3
in-domain ? OT2 -7.9
in-domain ? OT2(2) 4.2
in-domain ? OT3 -15.8
in-domain ? OT3(2) -8.3
junk question ? error -4.6
junk question ? OT1 4.1
junk question ? OT1(2) 4.1
junk question ? OT2 43.4
junk question ? OT2(2) -33.1
junk question ? OT3 3.1
junk question ? OT3(2) 6.1
give up -19.5
Table 3: Reward function 2.
As we can see, providing an OT2 as the first off-
topic response is a poor action (-7.9); it is preferable
to ask the user to rephrase her question (OT1) as a
first attempt to recover from the error (+13.9). On
the other hand, providing an OT2 prompt, after an
off-topic prompt has occured in the previous system
prompt, is a reasonable action (+4.2).
7 Evaluation
We compare our learned policy with two baselines.
The first baseline, Baseline 1, is the dialogue pol-
icy that is used by our system that is currently in-
stalled at the Museum of Science in Boston. Base-
line 1 selects the best ASR result (i.e. the result
with the highest confidence score) out of the results
with the 3 different AMs (child, male, and female),
and forwards this result to the NPCEditor to retrieve
the system?s response. If the NPCEditor score is
higher than an emprically set pre-defined threshold
(see (Leuski and Traum, 2010) for details), then the
system presents the retrieved response, otherwise it
presents an off-topic prompt. The system presents
these off-topic prompts in a fixed order. First, OT1,
then OT2, and then OT3.
We also have Baseline 2, which forwards all 3
ASR results to the NPCEditor (using child, male,
and female AMs). Then the NPCEditor retrieves 3
results, one for each one of the 3 ASR results, and
selects the retrieved result with the highest score.
Again if this score is higher than a threshold, the sys-
tem will present this result, otherwise it will present
an off-topic prompt.
Each policy interacts with the SU for 10,000 di-
alogue sessions and we calculate the average accu-
mulated reward for each dialogue. In Tables 4 and 5
we can see our results for Reward functions 1 and 2
respectively. In both cases the learned policy outper-
forms both baselines. For both reward functions the
most predictive feature is the ASR confidence score
when combined with the NPCEditor?s retrieval score
and the previous system action. Also, for both re-
ward functions the second best feature is ?voting?
when combined with the retrieval score and the pre-
vious system action.
In Table 6 we can see how often the learned pol-
icy, which is based on Reward function 1 using all
features, selects each one of the 10 system actions
(200,000 system turns in total).
Policy Avg Reward
Baseline 1 24.76 (19.29)
Baseline 2 51.63 (49.84)
Learned Policy - Features
Retrieval score
+ system action (*) 46.74
(*) + ASR confidence score 61.59
(*) + User type probability 47.28
(*) + Estimated previous topic 47.87
(*) + Voting 59.94
All features 60.93
Table 4: Results with reward function 1. The values in
parentheses for Baselines 1 and 2 are the rewards when
the NPCEditor does not use the pre-defined threshold.
90
Policy Avg Reward
Baseline 1 39.40 (38.51)
Baseline 2 55.45 (54.49)
Learned Policy - Features
Retrieval score
+ system action (*) 49.15
(*) + ASR confidence score 69.51
(*) + User type probability 50.15
(*) + Estimated previous topic 49.84
(*) + Voting 69.06
All features 73.59
Table 5: Results with reward function 2. The values in
parentheses for Baselines 1 and 2 are the rewards when
the NPCEditor does not use the pre-defined threshold.
System Action Frequency
Child + 1st best score 10.33%
Child + 2nd best score 2.70%
Male + 1st best score 13.72%
Male + 2nd best score 1.03%
Female + 1st best score 39.73%
Female + 2nd best score 0.79%
Best of scores 1-6 2.38%
OT1 11.01%
OT2 6.86%
OT3 11.45%
Table 6: Frequency of the system actions of the learned
policy that is based on Reward function 1 using all fea-
tures.
8 Discussion and Conclusion
We showed that RL is a promising technique for
learning question-answering policies. Currently we
use the same SU for both training and testing the
policies. One could argue that this favors the learned
policy over the baselines. Because our SU is based
on general corpus statistics (probability that the user
is child or male or female, number of questions the
user is planning to ask, probability of moving to the
next topic or ceasing the dialogue, utterance timing
statistics) rather than sequential information we be-
lieve that this is acceptable. We only use sequential
information when we calculate the next topic that
the user will choose. That is, due to the way the
SU is built and its randomness, we believe that it is
very unlikely that the same patterns that were gener-
ated during training will be generated during testing.
Thus we do not anticipate that our results would be
different if for testing we used a SU trained on a dif-
ferent part of the corpus, or that the learned policy is
favored over the baselines. However, this is some-
thing to verify experimentally in future work.
For future work we would also like to do the fol-
lowing. First of all, currently we are in the process of
analyzing user satisfaction questionnaires from mu-
seum visitors in order to define a better reward func-
tion. Second, we would like to use voice identifi-
cation techniques to automatically estimate from the
corpus the statistics of having more than one user
or alternating users in the same session. Third, and
most important, we would like to incorporate the
learned policy into the system that is currently in-
stalled in the museum and evaluate it with real users.
Fourth, currently our SU is based on only some of
our findings from the analysis of the corpus. We in-
tend to build a more complex and hopefully more
realistic SU based on our full corpus analysis. Fi-
nally, we will also experiment with learning policies
directly from the data (Li et al, 2009).
To conclude, we analyzed a corpus of interactions
of museum visitors with two virtual characters that
serve as guides at the Museum of Science in Boston,
in order to build a realistic model of user behavior
when interacting with these characters. Based on
this analysis, we built a SU and used it for learning
the dialogue policy of the virtual characters using
RL. We compared our learned policy with two base-
lines, one of which was the dialogue policy of the
original system that was used for collecting the cor-
pus and that is currently installed at the Museum of
Science in Boston. Our learned policy outperformed
both baselines which shows that RL is a promising
technique for learning question-answering dialogue
policies.
Acknowledgments
This work was funded by the NSF grant #1117313.
The Twins corpus collection was supported by the
NSF grant #0813541.
References
Priti Aggarwal, Ron Artstein, Jillian Gerten, Athanasios
Katsamanis, Shrikanth Narayanan, Angela Nazarian,
and David Traum. 2012. The Twins corpus of mu-
91
seum visitor questions. In Proc. of the Language
Resources and Evaluation Conference (LREC), pages
2355?2361, Istanbul, Turkey.
Hua Ai and Diane Litman. 2008. Assessing dialog sys-
tem user simulation evaluation measures using human
judges. In Proc. of the Annual Meeting of the Associa-
tion for Computational Linguistics: Human Language
Technologies (ACL-HLT), pages 622?629, Columbus,
OH, USA.
Min Chi, Kurt VanLehn, Diane Litman, and Pamela Jor-
dan. 2011. Empirically evaluating the application
of reinforcement learning to the induction of effective
and adaptive pedagogical strategies. User Modeling
and User-Adapted Interaction, 21(1-2):137?180.
Sudeep Gandhe, Nicolle Whitman, David Traum, and
Ron Artstein. 2009. An integrated authoring tool
for tactical questioning dialogue systems. In Proc. of
the IJCAI Workshop on Knowledge and Reasoning in
Practical Dialogue Systems, Pasadena, CA, USA.
Kallirroi Georgila and David Traum. 2011a. Learn-
ing culture-specific dialogue models from non culture-
specific data. In Proc. of HCI International, Lecture
Notes in Computer Science Vol. 6766, pages 440?449,
Orlando, FL, USA.
Kallirroi Georgila and David Traum. 2011b. Reinforce-
ment learning of argumentation dialogue policies in
negotiation. In Proc. of Interspeech, pages 2073?
2076, Florence, Italy.
Kallirroi Georgila, James Henderson, and Oliver Lemon.
2006. User simulation for spoken dialogue systems:
Learning and evaluation. In Proc. of Interspeech,
pages 1065?1068, Pittsburgh, PA, USA.
Kallirroi Georgila, Maria K. Wolters, and Johanna D.
Moore. 2010. Learning dialogue strategies from older
and younger simulated users. In Proc. of the Annual
SIGdial Meeting on Discourse and Dialogue (SIG-
dial), pages 103?106, Tokyo, Japan.
Peter A. Heeman. 2009. Representing the reinforcement
learning state in a negotiation dialogue. In Proc. of the
IEEE Automatic Speech Recognition and Understand-
ing Workshop (ASRU), Merano, Italy.
Arne Jo?nsson, Frida Ande?n, Lars Degerstedt, Annika
Flycht-Eriksson, Magnus Merkel, and Sara Norberg.
2004. Experiences from combining dialogue system
development with information access techniques. In
New Directions in Question Answering, Mark T. May-
bury (Ed), pages 153?164. AAAI/MIT Press.
Filip Jurc???c?ek, Blaise Thomson, and Steve Young. 2012.
Reinforcement learning for parameter estimation in
statistical spoken dialogue systems. Computer Speech
and Language, 26(3):168?192.
Anton Leuski and David Traum. 2010. Practical lan-
guage processing for virtual humans. In Proc. of the
22nd Annual Conference on Innovative Applications
of Artificial Intelligence (IAAI), Atlanta, GA, USA.
Anton Leuski, Ronakkumar Patel, David Traum, and
Brandon Kennedy. 2006. Building effective question
answering characters. In Proc. of the Annual SIGdial
Meeting on Discourse and Dialogue (SIGdial), pages
18?27, Sydney, Australia.
Lihong Li, Jason D. Williams, and Suhrid Balakrishnan.
2009. Reinforcement learning for dialog management
using least-squares policy iteration and fast feature se-
lection. In Proc. of Interspeech, pages 2475?2478,
Brighton, United Kingdom.
Teruhisa Misu, Komei Sugiura, Kiyonori Ohtake, Chiori
Hori, Hideki Kashioka, Hisashi Kawai, and Satoshi
Nakamura. 2010. Modeling spoken decision making
dialogue and optimization of its dialogue strategy. In
Proc. of the Annual SIGdial Meeting on Discourse and
Dialogue (SIGdial), pages 221?224, Tokyo, Japan.
Rieks op den Akker, Harry Bunt, Simon Keizer, and
Boris van Schooten. 2005. From question answering
to spoken dialogue: Towards an information search as-
sistant for interactive multimodal information extrac-
tion. In Proc. of Interspeech, pages 2793?2796, Lis-
bon, Portugal.
Jan Peters and Stefan Schaal. 2008. Natural actor-critic.
Neurocomputing, 71(7-9):1180?1190.
Verena Rieser and Oliver Lemon. 2009. Does this list
contain what you were searching for? Learning adap-
tive dialogue strategies for interactive question an-
swering. Natural Language Engineering, 15(1):55?
72.
William Swartout, David Traum, Ron Artstein, Dan
Noren, Paul Debevec, Kerry Bronnenkant, Josh
Williams, Anton Leuski, Shrikanth Narayanan, Diane
Piepol, Chad Lane, Jacquelyn Morie, Priti Aggarwal,
Matt Liewer, Jen-Yuan Chiang, Jillian Gerten, Selina
Chu, and Kyle White. 2010. Ada and Grace: Toward
realistic and engaging virtual museum guides. In Proc.
of the International Conference on Intelligent Virtual
Agents (IVA), pages 286?300, Philadelphia, PA, USA.
Joel R. Tetreault and Diane J. Litman. 2008. A reinforce-
ment learning approach to evaluating state representa-
tions in spoken dialogue systems. Speech Communi-
cation, 50(8-9):683?696.
Blaise Thomson and Steve Young. 2010. Bayesian up-
date of dialogue state: A POMDP framework for spo-
ken dialogue systems. Computer Speech and Lan-
guage, 24(4):562?588.
Sebastian Varges, Fuliang Weng, and Heather Pon-Barry.
2009. Interactive question answering and constraint
relexation in spoken dialogue systems. Natural Lan-
guage Engineering, 15(1):9?30.
EllenM. Voorhees. 2001. The TREC question answering
track. Natural Language Engineering, 7(4):361?378.
92
Appendix
Features Features
average ASR accuracy of user queries if system correctly answered current user query
# user queries if system responded with off-topic prompt
to current user query
# correct system responses # times user repeated current query
# incorrect system responses # successive incorrect system responses
# off-topic system prompts # successive off-topic system prompts
% correct system responses # user queries for topic ?introduction?
% incorrect system responses # user queries for topic ?personal?
user type (child, male, female) # user queries for topic ?school?
if user asks example query 1 # user queries for topic ?technology?
if user asks example query 2 # user queries for topic ?interfaces?
if user asks example query 3 # user queries for topic ?exhibition?
if user asks example query 4 # user queries for other topics
if system correctly responds to example query 1 if system correctly responds to example query 3
if system correctly responds to example query 2 if system correctly responds to example query 4
# junk user queries previous topic category
Table 7: List of features used in predicting when the user will cease a session (Cease Dialogue), what the user will say
next (Say Next 1), and what the user will say next after removing repeated user queries (Say Next 2). Example query
1 is ?who are you named after??; example query 2 is ?are you a computer??; example query 3 is ?what do you like to
do for fun??; example query 4 is ?what is artificial intelligence??.
Cease Dialogue Say Next 1 Say Next 2
average ASR accuracy of previous topic category previous topic category
user queries
user type (child, male, female) # user queries for topic ?personal? # junk user queries
# off-topic system prompts # user queries # successive incorrect system
responses
# successive off-topic system # junk user queries if system correctly answered
prompts current user query
# incorrect system responses % correct system responses user type (child, male, female)
# user queries % incorrect system responses % incorrect system responses
# junk user queries # incorrect system responses % correct system responses
# user queries for other topics # user queries for other topics # incorrect system responses
if system responded with off-topic # correct system responses # off-topic system prompts
prompt to current user query
% correct system responses user type (child, male, female) # user queries
Table 8: List of the 10 most dominant features (in order of importance) in predicting when the user will cease a session
(Cease Dialogue), what the user will say next (Say Next 1), and what the user will say next after removing repeated
user queries (Say Next 2).
93
Proceedings of the 13th Annual Meeting of the Special Interest Group on Discourse and Dialogue (SIGDIAL), pages 131?133,
Seoul, South Korea, 5-6 July 2012. c?2012 Association for Computational Linguistics
A Demonstration of Incremental Speech Understanding and Confidence
Estimation in a Virtual Human Dialogue System
David DeVault and David Traum
Institute for Creative Technologies
University of Southern California
12015 Waterfront Drive, Playa Vista, CA 90094
{devault,traum}@ict.usc.edu
1 Overview
This demonstration highlights some emerging ca-
pabilities for incremental speech understanding and
processing in virtual human dialogue systems. This
work is part of an ongoing effort that aims to en-
able realistic spoken dialogue with virtual humans in
multi-party negotiation scenarios (Plu?ss et al, 2011;
Traum et al, 2008). In these negotiation scenarios,
ideally the virtual humans should demonstrate fluid
turn-taking, complex reasoning, and appropriate re-
sponses based on factors like trust and emotions. An
important component in achieving this naturalistic
behavior is for the virtual humans to begin to un-
derstand and in some cases respond in real time to
users? speech, as the users are speaking (DeVault
et al, 2011b). These responses could include rel-
atively straightforward turn management behaviors,
like having a virtual human recognize when it is be-
ing addressed and turn to look at the user. They
could also include more complex responses such as
emotional reactions to what users are saying.
Our demonstration is set in an implemented ne-
gotiation domain (Plu?ss et al, 2011) in which two
virtual humans, Utah and Harmony (pictured in Fig-
ure 1), talk with two human negotiation trainees,
who play the roles of Ranger and Deputy. The di-
alogue takes place inside a saloon in an American
town in the Old West. In this scenario, the goal of the
two human role players is to convince Utah and Har-
mony that Utah, who is currently the local bartender,
should take on the job of town sheriff. We presented
a substantially similar demonstration of this scenario
in (DeVault and Traum, 2012).
Figure 1: SASO negotiation in the saloon: Utah (left)
looking at Harmony (right).
To support more natural behavior in such negotia-
tion scenarios, we have developed an approach to in-
cremental speech understanding. The understanding
models are trained using a corpus of in-domain spo-
ken utterances, including both paraphrases selected
and spoken by system developers, as well as spo-
ken utterances from user testing sessions (DeVault
et al, 2011b). Every utterance in the corpus is an-
notated with an utterance meaning, which is repre-
sented using a frame. Each frame is an attribute-
value matrix (AVM), where the attributes and val-
ues represent semantic information that is linked to
a domain-specific ontology and task model (Traum,
2003; Hartholt et al, 2008; Plu?ss et al, 2011). The
AVMs are linearized, using a path-value notation, as
seen at the lower left in Figure 2. Our framework
uses this corpus to train two data-driven models, one
for incremental natural language understanding, and
a second for incremental confidence modeling. We
briefly summarize these two models here; for addi-
tional details and motivation for this framework, and
discussion of alternative approaches, see (DeVault et
al., 2011b; DeVault et al, 2011a).
The first step is to train a predictive incremental
understanding model. This model is based on maxi-
131
mum entropy classification, and treats entire individ-
ual frames as output classes, with input features ex-
tracted from partial ASR results, calculated in incre-
ments of 200 milliseconds (DeVault et al, 2011b).
Each partial ASR result serves as an incremental in-
put to NLU, which is specially trained for partial
input as discussed in (Sagae et al, 2009). NLU is
predictive in the sense that, for each partial ASR re-
sult, the NLU module tries to output the complete
frame that a human annotator would associate with
the user?s complete utterance, even if that utterance
has not yet been fully processed by the ASR.
The second step in our framework is to train a set
of incremental confidence models (DeVault et al,
2011a), which allow the agents to assess in real time,
while a user is speaking, how well the understand-
ing process is proceeding. The incremental confi-
dence models build on the notion of NLU F-score,
which we use to quantify the quality of a predicted
NLU frame in relation to the hand-annotated correct
frame. The NLU F-score is the harmonic mean of
the precision and recall of the attribute-value pairs
(or frame elements) that compose the predicted and
correct frames for each partial ASR result.
Each of our incremental confidence models
makes a binary prediction for each partial NLU re-
sult as an utterance proceeds. At each time t dur-
ing an utterance, we consider the current NLU F-
Score Ft as well as the final NLU F-Score Ffinal
that will be achieved at the conclusion of the utter-
ance. In (DeVault et al, 2009) and (DeVault et al,
2011a), we explored the use of data-driven decision
tree classifiers to make predictions about these val-
ues, for example whether Ft ? 12 (current level ofunderstanding is ?high?), Ft ? Ffinal (current level
of understanding will not improve), or Ffinal ? 12(final level of understanding will be ?high?). In
this demonstration, we focus on the first and third
of these incremental confidence metrics, which we
summarize as ?Now Understanding? and ?Will Un-
derstand?, respectively.
The incremental ASR, NLU, and confidence out-
puts are passed to the dialogue managers for each of
the agents, Harmony and Utah. These agents then
relate these inputs to their own models of dialogue
context, plans, and emotions, to calculate pragmatic
interpretations, including speech acts, reference res-
olution, participant status, and how they feel about
what is being discussed. A subset of this informa-
tion is passed to the non-verbal behavior generation
module to produce incremental non-verbal listening
behaviors (Wang et al, 2011).
2 Demo script
The demonstration begins with the demo operator
providing a brief overview of the system design, ne-
gotiation scenario, and incremental processing capa-
bilities. The virtual humans Utah and Harmony (see
Figure 1) are running and ready to begin a dialogue
with the user, who will play the role of the Ranger.
The demonstration includes a real-time visualization
of incremental speech processing results, which will
allow attendees to track the virtual humans? under-
standing as an utterance progresses. An example of
this visualization is shown in Figure 2.
As the user speaks to Utah or Harmony, attendees
can observe the real time visualization of incremen-
tal speech processing. Further, the visualization in-
terface enables the demo operator to ?rewind? an ut-
terance and step through the incremental processing
results that arrived each 200 milliseconds.
For example, Figure 2 shows the incremental
speech processing state at a moment 4.8 seconds into
a user?s 7.4 second long utterance, i?ve come here
today to talk to you about whether you?d like to be-
come the sheriff of this town. At this point in time,
the visualization shows (at top left) that the virtual
humans are confident that they are Now Understand-
ing and also Will Understand this utterance. Next,
the graph (in white) shows the history of the agents?
expected NLU F-Score for this utterance (ranging
from 0 to 1). Beneath the graph, the partial ASR re-
sult (HAVE COME HERE TODAY TO TALK TO
YOU ABOUT...) is displayed (in white), along
with the currently predicted NLU frame (in blue).
For ease of comprehension, an English gloss (utah
do you want to be the sheriff?) for the NLU frame is
also shown (in blue) above the frame.
To the right, in pink, we show some of Utah and
Harmony?s agent state that is based on the current in-
cremental NLU results. The display shows that both
of the virtual humans believe that Utah is being ad-
dressed by this utterance, that utah has a positive at-
titude toward the content of the utterance while har-
mony does not, and that both have comprehension
132
Figure 2: Visualization of Incremental Speech Processing.
and participation goals. Further, Harmony believes
she is a side participant at this moment.
Acknowledgments
We thank the entire ICT Virtual Humans team. The
project or effort described here has been sponsored
by the U.S. Army Research, Development, and En-
gineering Command (RDECOM). Statements and
opinions expressed do not necessarily reflect the po-
sition or the policy of the United States Government,
and no official endorsement should be inferred.
References
David DeVault and David R. Traum. 2012. Incremen-
tal speech understanding in a multi-party virtual hu-
man dialogue system. In Demonstration Proceedings
of NAACL-HLT.
David DeVault, Kenji Sagae, and David Traum. 2009.
Can I finish? Learning when to respond to incremental
interpretation results in interactive dialogue. In Pro-
ceedings of SIGDIAL.
David DeVault, Kenji Sagae, and David Traum. 2011a.
Detecting the status of a predictive incremental speech
understanding model for real-time decision-making in
a spoken dialogue system. In Proceedings of Inter-
Speech.
David DeVault, Kenji Sagae, and David Traum. 2011b.
Incremental interpretation and prediction of utterance
meaning for interactive dialogue. Dialogue & Dis-
course, 2(1).
Arno Hartholt, Thomas Russ, David Traum, Eduard
Hovy, and Susan Robinson. 2008. A common ground
for virtual humans: Using an ontology in a natural
language oriented virtual human architecture. In Pro-
ceedings of LREC, Marrakech, Morocco, may.
Brian Plu?ss, David DeVault, and David Traum. 2011.
Toward rapid development of multi-party virtual hu-
man negotiation scenarios. In Proceedings of Sem-
Dial.
Kenji Sagae, Gwen Christian, David DeVault, and
David R. Traum. 2009. Towards natural language un-
derstanding of partial speech recognition results in dia-
logue systems. In Short Paper Proceedings of NAACL
HLT.
David Traum, Stacy Marsella, Jonathan Gratch, Jina
Lee, and Arno Hartholt. 2008. Multi-party, multi-
issue, multi-strategy negotiation for multi-modal vir-
tual agents. In Proceedings of IVA.
David Traum. 2003. Semantics and pragmatics of ques-
tions and answers for dialogue agents. In Proc. of the
International Workshop on Computational Semantics.
Zhiyang Wang, Jina Lee, and Stacy Marsella. 2011.
Towards more comprehensive listening behavior: Be-
yond the bobble head. In Proceedings of IVA.
133
Proceedings of the 13th Annual Meeting of the Special Interest Group on Discourse and Dialogue (SIGDIAL), pages 137?139,
Seoul, South Korea, 5-6 July 2012. c?2012 Association for Computational Linguistics
A Mixed-Initiative Conversational Dialogue System for Healthcare
Fabrizio Morbini and Eric Forbell and David DeVault and Kenji Sagae and
David R. Traum and Albert A. Rizzo
Institute for Creative Technologies
University of Southern California
Los Angeles, CA 90094, USA
{morbini,forbell,devault,sagae,traum,rizzo}@ict.usc.edu
Abstract
We present a mixed initiative conversational
dialogue system designed to address primar-
ily mental health care concerns related to
military deployment. It is supported by a
new information-state based dialogue man-
ager, FLoReS (Forward-Looking, Reward
Seeking dialogue manager), that allows both
advanced, flexible, mixed initiative interac-
tion, and efficient policy creation by domain
experts. To easily reach its target population
this dialogue system is accessible as a web ap-
plication.
1 Introduction
The SimCoach project is motivated by the challenge
of empowering troops and their significant others in
regard to their healthcare, especially with respect to
issues related to the psychological toll of military
deployment. SimCoach virtual humans are not de-
signed to act as therapists, but rather to encourage
users to explore available options and seek treatment
when needed by fostering comfort and confidence in
a safe and anonymous environment where users can
express their concerns to an artificial conversational
partner without fear of judgment or possible reper-
cussions.
SimCoach presents a rich test case for all compo-
nents of a dialogue system. The interaction with the
virtual human is delivered via the web for easy ac-
cess. As a trade-off between performance and qual-
ity, the virtual human has access to a limited set of
pre-rendered animations.
The Natural Language Understanding (NLU)
module needs to cope with both chat and military
Figure 1: Bill Ford, a SimCoach character. SimCoach
virtual humans are accessible through a web browser.
The user enters natural language input in the text field
on the bottom of the screen. The simcoach responds with
text, speech and character animation. The text area to the
right shows a transcript of the dialogue.
slang and a broad conversational domain. The dia-
logue policy authoring module needs to support non-
dialogue experts given that important parts of the di-
alogue policy are contributed by experts in psycho-
metrics and mental health issues in the military, and
others with familiarity with the military domain.
The dialogue manager (DM) must be able to take
initiative when building rapport or collecting the in-
formation it needs, but also respond appropriately
when the user takes initiative.
2 Supporting Mixed Initiative Dialogues
There is often a tension between system initiative
and performance of the system?s decision-making
for understanding and actions. A strong system-
initiative policy reduces the action state space since
137
user actions are only allowed at certain points in
the dialogue. System initiative also usually makes
it easier for a domain expert to design a dialogue
policy that will behave as desired.1 Such systems
can work well if the limited options available to the
user are what the user wants to do, but can be prob-
lematic otherwise, especially if the user has a choice
of whether or not to use the system. In particular,
this approach may not be well suited to an appli-
cation like SimCoach. At the other extreme, some
systems allow the user to say anything at any time,
but have fairly flat dialogue policies, e.g., (Leuski et
al., 2006). These systems can work well when the
user is naturally in charge, such as in interviewing
a character, but may not be suitable for situations
in which a character is asking the user questions, or
mixed initiative is desired.
True mixed initiative is notoriously difficult for a
manually constructed call-flow graph, in which the
system might want to take different actions in re-
sponse to similar stimuli, depending on local utili-
ties. Reinforcement learning approaches (Williams
and Young, 2007; English and Heeman, 2005) can
be very useful at learning local policy optimizations,
but they require large amounts of training data and a
well-defined global reward structure, are difficult to
apply to a large state-space and remove some of the
control, which can be undesirable (Paek and Pierac-
cini, 2008).
Our approach to this problem is a forward-looking
reward seeking agent, similar to that described in
(Liu and Schubert, 2010), though with support for
complex dialogue interaction and its authoring. Au-
thoring involves design of local subdialogue net-
works with pre-conditions and effects, and also qual-
itative reward categories (goals), which can be in-
stantiated with specific reward values. The dialogue
manager, called FLoReS, can locally optimize pol-
icy decisions, by calculating the highest overall ex-
pected reward for the best sequence of subdialogues
from a given point. Within a subdialogue, authors
can craft the specific structure of interaction.
Briefly, the main modules that form FLoReS are:
? The information state, a propositional knowl-
1Simple structures, such as a call flow graph (Pieraccini and
Huerta, 2005) and branching narrative for interactive games
(Tavinor, 2009) will suffice for authoring.
edge base that keeps track of the current state
of the conversation. The information state sup-
ports missing or unknown information by al-
lowing atomic formulas to have 3 possible val-
ues: true, false and null.
? A set of inference rules that allows the sys-
tem to add new knowledge to its information
state, based on logical reasoning. Forward in-
ference facilitates policy authoring by provid-
ing a mechanism to specify information state
updates that are independent of the specific di-
alogue context.2
? An event handling system, that allows the in-
formation state to be updated based on user in-
put, system action, or other classes of author-
defined events (such as system timeouts).
? A set of operators. Operators represent lo-
cal dialogue structure (trees), and can also be
thought of as reusable subdialogues. Each state
within the subdialogue can include a reward
for reaching that state. Rewards are functions
of the goals of the system, and are the main
method used to decide what to do when there is
more than one applicable operator. Operators
have preconditions and effects. Effects specify
changes to the information state. The precondi-
tions define when an operator can be activated.
3 Sample Dialogue
In this demo, the user will interact with the Sim-
Coach character Bill Ford, using a standard web
browser and typing text. The virtual human, driven
by FLoReS, will respond using pre-rendered anima-
tions encoded as H.264 video, delivered via a stan-
dard web server. Table 1 shows an excerpt from a
sample conversation with Bill Ford that illustrates
some of the features of this dialogue manager.
The excerpt starts from a rapport building
smalltalk sub-dialogue on the topic of barbecuing
which is interrupted by a user question about con-
fidentiality. The system responds to the user inter-
ruption and then re-starts the interrupted smalltalk
because it is still the most valuable conversation con-
tinuation available at that moment.
2For example: every time the user says that s/he has night-
mares we want to update the information state to include that
s/he also has sleeping problems.
138
Dialogue transcript Notes
Ask anybody about me, and
they?ll tell you that I love to
BBQ
BBQ Smalltalk The character is equipped
with a few operators for
smalltalk about a few topics.
BBQ is one of them.
Is this conversation se-
cret?
We don?t share your info
with anyone who can per-
sonally identify you. The
techs can see what we say,
but just to tell that the site is
working. But they have no
idea who said it, just what
was said
Did that help you?
Yes it did.
Great.
Confidentiality QA
Here the system is inter-
rupted by a user question and
it decides that answering it is
the best course of action.
Like I was saying, I love to
BBQ
BBQ Smalltalk After answering the question,
the best course of action is to
awaken the paused operator
about the BBQ smalltalk.
What is PTSD?
PTSD, or post-traumatic
stress disorder is an anxiety
condition associated with
serious traumatic events.
It can come with survivor
guilt, reliving the trauma in
dreams, numbness, and lack
of involvement with reality.
What is PTSD QA
Again the BBQ smalltalk is
interrupted by another ques-
tion from the user.
So, is PTSD something
you?re worried about. I only
ask, because you?ve been
asking about it. ...
PTSD Topic Interest QA
After answering the second
question the system decides
to ignore the paused operator
and load a follow-up operator
related to the important topic
raised by the user?s question.
The selection is based on the
expected reward that talking
about PTSD can bring to the
system.
Table 1: An excerpt of a conversation with Bill Ford that
shows opportunistic mixed initiative behavior.
Next, the user asks a question about the impor-
tant topic of post-traumatic stress disorder (PTSD).
That allows operators related to the PTSD topic to
become available and at the next chance the most
rewarding operator is no longer the smalltalk sub-
dialogue but one that stays on the PTSD topic.
4 Conclusion
We described the SimCoach dialogue system which
is designed to facilitate access to difficult health con-
cerns faced by military personnel and their fami-
lies. To easily reach its target population, the sys-
tem is available on the web. The dialogue is driven
by FLoReS, a new information-state and plan-based
DM with opportunistic action selection based on ex-
pected rewards that supports non-expert authoring.
Acknowledgments
The effort described here has been sponsored by the
U.S. Army. Any opinions, content or information
presented does not necessarily reflect the position or
the policy of the United States Government, and no
official endorsement should be inferred.
References
M.S. English and P.A. Heeman. 2005. Learning mixed
initiative dialogue strategies by using reinforcement
learning on both conversants. In HLT-EMNLP.
Anton Leuski, Ronakkumar Patel, David Traum, and
Brandon Kennedy. 2006. Building effective question
answering characters. In Proceedings of the 7th SIG-
dial Workshop on Discourse and Dialogue, pages 18?
27.
Daphne Liu and Lenhart K. Schubert. 2010. Combin-
ing self-motivation with logical planning and inference
in a reward-seeking agent. In Joaquim Filipe, Ana
L. N. Fred, and Bernadette Sharp, editors, ICAART (2),
pages 257?263. INSTICC Press.
Tim Paek and Roberto Pieraccini. 2008. Automating
spoken dialogue management design using machine
learning: An industry perspective. Speech Commu-
nication, 50(89):716 ? 729. Evaluating new methods
and models for advanced speech-based interactive sys-
tems.
Roberto Pieraccini and Juan Huerta. 2005. Where do we
go from here? Research and commercial spoken dia-
log systems. In Proceedings of the 6th SIGdial Work-
shop on Discourse and Dialogue, Lisbon, Portugal,
September.
Grant Tavinor. 2009. The art of videogames. New Di-
rections in Aesthetics. Wiley-Blackwell, Oxford.
J.D. Williams and S. Young. 2007. Scaling POMDPs for
spoken dialog management. IEEE Trans. on Audio,
Speech, and Language Processing, 15(7):2116?2129.
139
Proceedings of the SIGDIAL 2013 Conference, pages 193?202,
Metz, France, 22-24 August 2013. c?2013 Association for Computational Linguistics
Verbal indicators of psychological distress in interactive dialogue with a
virtual human
David DeVault, Kallirroi Georgila, Ron Artstein, Fabrizio Morbini, David Traum,
Stefan Scherer, Albert (Skip) Rizzo, Louis-Philippe Morency
University of Southern California, Institute for Creative Technologies
Playa Vista, CA
devault@ict.usc.edu
Abstract
We explore the presence of indicators of
psychological distress in the linguistic be-
havior of subjects in a corpus of semi-
structured virtual human interviews. At
the level of aggregate dialogue-level fea-
tures, we identify several significant dif-
ferences between subjects with depres-
sion and PTSD when compared to non-
distressed subjects. At a more fine-grained
level, we show that significant differences
can also be found among features that
represent subject behavior during specific
moments in the dialogues. Finally, we
present statistical classification results that
suggest the potential for automatic assess-
ment of psychological distress in individ-
ual interactions with a virtual human dia-
logue system.
1 Introduction
One of the first steps toward dealing with psy-
chological disorders such as depression and PTSD
is diagnosing the problem. However, there is of-
ten a shortage of trained health care professionals,
or of access to those professionals, especially for
certain segments of the population such as mili-
tary personnel and veterans (Johnson et al, 2007).
One possible partial remedy is to use virtual hu-
man characters to do a preliminary triage screen-
ing, so that mental healthcare providers can focus
their attention on those who are most likely to need
help. The virtual human would engage an indi-
vidual in an interview and analyze some of their
behavioral characteristics. In addition to serving
a triage function, this automated interview could
produce valuable information to help the health-
care provider make their expert diagnosis.
In this paper, we investigate whether features
in the linguistic behavior of participants in a con-
versation with a virtual human could be used
for recognizing psychological distress. We focus
specifically on indicators of depression and post-
traumatic stress disorder (PTSD) in the verbal be-
havior of participants in a Wizard-of-Oz corpus.
The results and analysis presented here are part
of a broader effort to create an automated, interac-
tive virtual human dialogue system that can detect
indicators of psychological distress in the multi-
modal communicative behavior of its users. Re-
alizing this vision requires a careful and strate-
gic design of the virtual human?s dialogue behav-
ior, and in concert with the system?s behavior, the
identification of robust ?indicator? features in the
verbal and nonverbal responses of human intervie-
wees. These indicators should be specific behavior
patterns that are empirically correlated with spe-
cific psychological disorders, and that can inform
a triage screening process or facilitate the diagno-
sis or treatment performed by a clinician.
In this paper, we report on several kinds of such
indicators we have observed in a corpus of 43
Wizard-of-Oz interactions collected with our pro-
totype virtual human, Ellie, pictured in Figure 1.
We begin in Section 2 with a brief discussion of
background and related work on the communica-
tive behavior associated with psychological dis-
tress. In Section 3, we describe our Wizard-of-Oz
data set. Section 4 presents an analysis of indicator
features we have explored in this data set, identi-
fying several significant differences between sub-
jects with depression and PTSD when compared
to non-distressed subjects. In Section 5 we present
statistical classification results that suggest the po-
tential for automatic assessment of psychological
distress based on individual interactions with a vir-
tual human dialogue system. We conclude in Sec-
tion 6.
2 Background and Related Work
There has been a range of psychological and clin-
ical research that has identified differences in the
193
Figure 1: Ellie.
communicative behavior of patients with specific
psychological disorders such as depression. In this
section, we briefly summarize some closely re-
lated work.
Most work has observed the behavior of patients
in human-human interactions, such as clinical in-
terviews and doctor-patient interactions. PTSD is
generally less well studied than depression.
Examples of the kinds of differences that have
been observed in non-verbal behavior include dif-
ferences in rates of mutual gaze and other gaze
patterns, downward angling of the head, mouth
movements, frowns, amount of gesturing, fidget-
ing, emotional expressivity, and voice quality; see
Scherer et al (2013) for a recent review.
In terms of verbal behavior, our exploration of
features here is guided by several previous obser-
vations in the literature. Cohn and colleagues have
identified increased speaker-switch durations and
decreased variability of vocal fundamental fre-
quency as indicators of depression, and have ex-
plored the use of these features for classification
(Cohn et al, 2009). That work studied these fea-
tures in human-human clinical interviews, rather
than in virtual human interactions as reported here.
In clinical studies, acute depression has been as-
sociated with decreased speech, slow speech, de-
lays in delivery, and long silent pauses (Hall et al,
1995). Aggregate differences in lexical frequen-
cies have also been observed. For example, in
written essays, Rude et al (2004) observed that
depressed participants used more negatively va-
lenced words and used the first-person pronoun ?I?
more frequently than never-depressed individuals.
Heeman et al (2010) observed differences in chil-
dren with autism in how long they pause before
speaking and in their use of fillers, acknowledg-
ments, and discourse markers. Some of these fea-
tures are similar to those studied here, but looked
at children communicating with clinicians rather
than a virtual human dialogue system.
Recent work on machine classification has
demonstrated the ability to discriminate between
schizophrenic patients and healthy controls based
on transcriptions of spoken narratives (Hong et al,
2012), and to predict patient adherence to med-
ical treatment from word-level features of dia-
logue transcripts (Howes et al, 2012). Automatic
speech recognition and word alignment has also
been shown to give good results in scoring narra-
tive recall tests for identification of cognitive im-
pairment (Prud?hommeaux and Roark, 2011; Lehr
et al, 2012).
3 Data Set
In this section, we introduce the Wizard-of-Oz
data set that forms the basis for this paper. In
this virtual human dialogue system, the charac-
ter Ellie depicted in Figure 1 carries out a semi-
structured interview with a single user. The sys-
tem was designed after a careful analysis of a
set of face-to-face interviews in the same do-
main. The face-to-face interviews make up the
large human-human Distress Assessment Inter-
view Corpus (DAIC) that is described in Scherer
et al (2013). Drawing on observations of inter-
viewer behavior in the face-to-face dialogues, El-
lie was designed to serve as an interviewer who
is also a good listener, providing empathetic re-
sponses, backchannels, and continuation prompts
to elicit more extended replies to specific ques-
tions. The data set used in this paper is the re-
sult of a set of 43 Wizard-of-Oz interactions where
the virtual human interacts verbally and nonver-
bally in a semi-structured manner with a partici-
pant. Excerpts from the transcripts of two interac-
tions in this Wizard-of-Oz data set are provided in
the appendix in Figure 5.1
3.1 Procedure
The participants were recruited via Craigslist and
were recorded at the USC Institute for Creative
1A sample demonstration video of an interaction be-
tween the virtual agent and a human actor can be seen here:
http://www.youtube.com/watch?v=ejczMs6b1Q4
194
Technologies. In total 64 participants interacted
with the virtual human. All participants who met
requirements (i.e. age greater than 18, and ad-
equate eyesight) were accepted. In this paper,
we focus on a subset of 43 of these participants
who were told that they would be interacting with
an automated system. (The other participants,
which we exclude from our analysis, were aware
that they were interacting with a human-controlled
system.) The mean age of the 43 participants in
our data set was 36.6 years, with 23 males and 20
females.
We adhered to the following procedure for data
collection: After a short explanation of the study
and giving consent, participants completed a series
of questionnaires. These questionnaires included
the PTSD Checklist-Civilian version (PCL-C) and
the Patient Health Questionnaire, depression mod-
ule (PHQ-9) (Scherer et al, 2013) along with other
questions. Then participants engage in an inter-
view with the virtual human, Ellie. After the di-
alogue concludes, participants are then debriefed
(i.e. the wizard control is revealed), paid $25 to
$35, and escorted out.
The interaction between the participants and El-
lie was designed as follows: Ellie explains the pur-
pose of the interaction and that she will ask a series
of questions. She then tries to build rapport with
the participant in the beginning of the interaction
with a series of casual questions about Los Ange-
les. Then the main interview begins, including a
range of questions such as:
What would you say are some of your
best qualities?
What are some things that usually put
you in a good mood?
Do you have disturbing thoughts?
What are some things that make you re-
ally mad?
How old were you when you enlisted?
What did you study at school?
Ellie?s behavior was controlled by two human
?wizards? in a separate room, who used a graph-
ical user interface to select Ellie?s nonverbal be-
havior (e.g. head nods, smiles, back-channels)
and verbal utterances (including the interview
questions, verbal back-channels, and empathy re-
sponses). This Wizard-of-Oz setup allows us to
prove the utility of the protocol and collect training
data for the eventual fully automatic interaction.
The speech for each question was pre-recorded us-
ing an amateur voice actress (who was also one of
the wizards). The virtual human?s performance of
these utterances is animated using the SmartBody
animation system (Thiebaux et al, 2008).
3.2 Condition Assessment
The PHQ-9 and PCL-C scales provide researchers
with guidelines on how to assess the participants?
conditions based on the responses. Among the
43 participants, 13 scored above 10 on the PHQ-
9, which corresponds to moderate depression and
above (Kroenke et al, 2001). We consider these
13 participants as positive for depression in this
study. 20 participants scored positive for PTSD,
following the PCL-C classification. The two pos-
itive conditions overlap strongly, as the evalu-
ated measurements PHQ-9 and PCL-C correlate
strongly (Pearson?s r > 0.8, as reported in Scherer
et al (2013)).
4 Feature Analysis
4.1 Transcription and timing of speech
We have a set D = {d1, ..., d43} of 43 dialogues.
The user utterances in each dialogue were tran-
scribed using ELAN (Wittenburg et al, 2006),
with start and end timestamps for each utterance.2
At each pause of 300ms or longer in the user?s
speech, a new transcription segment was started.
The resulting speech segments were subsequently
reviewed and corrected for accuracy.
For each dialogue di ? D, this process resulted
in a sequence of user speech segments. We repre-
sent each segment as a tuple ?s, e, t?, where s and e
are the starting and ending timestamps in seconds,
and t is the manual text transcription of the corre-
sponding audio segment. The system speech seg-
ments, including their starting and ending times-
tamps and verbatim transcripts of system utter-
ances, were recovered from the system log files.
To explore aggregate statistical features based
on user turn-taking behavior in the dialogues, we
employ a simple approach to identifying turns
within the dialogues. First, all user and system
speech segments are sorted in increasing order of
2ELAN is a tool that supports annotation of
video and audio, from the Max Planck Insti-
tute for Psycholinguistics, The Language Archive,
Nijmegen, The Netherlands. It is available at
http://tla.mpi.nl/tools/tla-tools/elan/.
195
Segment level features
(a) mean speaking rate of each user segment
(b) mean onset time of first segment in each user turn
(c) mean onset time of non-first segments in user turns
(d) mean length of user segments
(e) mean minimum valence in user segments
(f) mean mean valence in user segments
(g) mean maximum valence in user segments
(h) mean number of filled pauses in user segments
(i) mean filled pause rate in user segments
Dialogue level features
(j) total number of user segments
(k) total length of all user segments
Figure 2: List of context-independent features.
their starting timestamps. All consecutive seg-
ments with the same speaker are then designated
as constituting a single turn. While this simple
scheme does not provide a detailed treatment of
relevant phenomena such as overlapping speech,
backchannels, and the interactive process of ne-
gotiating the turn in dialogue (Yang and Heeman,
2010), it provides a conceptually simple model for
the definition of features for aggregate statistical
analysis.
4.2 Context-independent feature analysis
We begin by analyzing a set of shallow features
which we describe as context-independent, as they
apply to user speech segments independently of
what the system has recently said. Most of these
are features that apply to many or all user speech
segments. We describe our context-independent
features in Section 4.2.1, and present our results
for these features in Section 4.2.2.
4.2.1 Context-independent features
We summarize our context-independent features
in Figure 2.
Speaking rate and onset times Based on previ-
ous clinical observations related to slowed speech
and increased onset time for depressed individuals
(Section 2), we defined features for speaking rate
and onset time of user speech segments.
We quantify the speaking rate of a user speech
segment ?s, e, t?, where t = ?w1, ..., wN ?, as
N/(e ? s). Feature (a) is the mean value of
this feature across all user speech segments within
each dialogue.
Onset time is calculated using the notion of user
turns. For each user turn, we extracted the first
user speech segment in the turn fu = ?su, eu, tu?,
and the most recent system speech segment ls =
?ss, es, ts?. We define the onset time of such a first
user segment as su ? es, and for each dialogue,
feature (b) is the intra-dialogue mean of these on-
set times.
In order to also quantify pause length between
user speech segments within a turn, we define fea-
ture (c), a similar feature that measures the mean
onset time between non-first user speech segments
within a user turn in relation to the preceding user
speech segment.
Length of user segments As one way to quan-
tify the amount of speech, feature (d) reports the
mean length of all user speech segments within a
dialogue (measured in words).
Valence features for user speech Features (e)-
(g) are meant to explore the idea that distressed
users might use more negative or less positive vo-
cabulary than non-distressed subjects. As an ex-
ploratory approach to this topic, we used Senti-
WordNet 3.0 (Baccianella and Sebastiani, 2010),
a lexical sentiment dictionary, to assign valence
to individual words spoken by users in our study.
The dictionary contains approximately 117,000
entries. In general, each word w may appear in
multiple entries, corresponding to different parts
of speech and word senses. To assign a single va-
lence score v(w) to each word in the dictionary, in
our features we compute the average score across
all parts of speech and word senses:
v(w) =
?
e?E(w) PosScoree(w)?NegScoree(w)
|E(w)|
where E(w) is the set of entries for the word w,
PosScoree(w) is the positive score for w in entry
e, and NegScoree(w) is the negative score for w
in entry e. This is similar to the ?averaging across
senses? method described in Taboada et al (2011).
We use several different measures of the va-
lence of each speech segment with transcript t =
?w1, ..., wn?. We compute the min, mean, and max
valence of each transcript:
minimum valence of t = minwi?t v(wi)
mean valence of t = 1n
?
wi?t v(wi)
maximum valence of t = maxwi?t v(wi)
Features (e)-(f) then are intra-dialogue mean
196
values for these three segment-level valence mea-
sures.
Filled pauses Another feature that we explored
is the presence of filled pauses in user speech seg-
ments. To do so, we counted the number of times
any of the tokens uh, um, uhh, umm, mm, or mmm
appeared in each speech segment. For each dia-
logue, feature (h) is the mean number of these to-
kens per user speech segment. In order to account
for the varying length of speech segments, we also
normalize the raw token counts in each segment
by dividing them by the length of the segment, to
produce a filled pause rate for the segment. Fea-
ture (i) is the mean value of the filled pause rate
for all speech segments in the dialogue.
Dialogue level features We also included two
dialogue level measures of how ?talkative? the
user is. Feature (j) is the total number of user
speech segments throughout the dialogue. Feature
(k) is the total length (in words) of all speech seg-
ments throughout the dialogue.
Standard deviation features For the classifica-
tion experiments reported in Section 5, we also in-
cluded a standard deviation variant of each of the
features (a)-(i) in Figure 2. These variants are de-
fined as the intra-dialogue standard deviation of
the underlying value, rather than the mean. We
discuss examples of standard deviation features
further in Section 5.
4.2.2 Results for context-independent
features
We summarize the observed significant effects in
our Wizard-of-Oz corpus in Table 1.
Onset time We report our findings for individu-
als with and without depression and PTSD for fea-
ture (b) in Table 1 and in Figure 3. The units are
seconds. While an increase in onset time for in-
dividuals with depression has previously been ob-
served in human-human interaction (Cohn et al,
2009; Hall et al, 1995), here we show that this
effect transfers to interactions between individuals
with depression and virtual humans. We find that
mean onset time is significantly increased for indi-
viduals with depression in their interactions with
our virtual human Ellie (p = 0.018, Wilcoxon
rank sum test).
Additionally, while to our knowledge onset time
for individuals with PTSD has not been reported,
we also found a significant increase in onset time
Me
an
on
se
td
ela
yo
ffi
rst
pa
rtic
ipa
nt
se
gm
en
t(s
ec
on
ds
)
0
1
2
3
4
No depr.
??
Depr.
?
Me
an
on
se
td
ela
yo
ffi
rst
pa
rtic
ipa
nt
se
gm
en
t(s
ec
on
ds
)
0
1
2
3
4
?PTSD
?
PTSD
?
Figure 3: Onset time.
for individuals with PTSD (p = 0.019, Wilcoxon
rank sum test).
Filled pauses We report our findings for individ-
uals with and without depression and PTSD under
feature (h) in Table 1 and in Figure 4. We observed
a significant reduction in this feature for both in-
dividuals with depression (p = 0.012, Wilcoxon
rank sum test) and PTSD (p = 0.014, Wilcoxon
rank sum test). We believe this may be related
to the trend we observed toward shorter speech
segments from distressed individuals (though this
trend did not reach significance). There is a pos-
itive correlation, ? = 0.55 (p = 0.0001), be-
tween mean segment length (d) and mean number
of filled pauses in segments (h).
Other features We did not observe significant
differences in the values of the other context-
independent features in Figure 2.
4.3 Context-dependent features
Our data set alows us to zoom in and look at
specific contextual moments in the dialogues, and
observe how users respond to specific Ellie ques-
tions. As an example, one of Ellie?s utterances,
which has system ID happy lasttime, is:
happy lasttime = Tell me about the last
time you felt really happy.
In our data set of 43 dialogues, this question was
asked in 42 dialogues, including 12 users positive
for depression and 19 users positive for PTSD.
197
Feature Depression (13 yes, 30 no) PTSD (20 yes, 23 no)
(b) mean onset time of first
segment in each user turn
?
Depr.: 1.72 (0.89)
No Depr.: 1.08 (0.56)
p = 0.018
?
PTSD.: 1.56 (0.80)
No PTSD.: 1.03 (0.57)
p = 0.019
(h) mean number of filled pauses
in user segments
?
Depr.: 0.32 (0.19)
No Depr.: 0.48 (0.23)
p = 0.012
?
PTSD: 0.36 (0.24)
No PTSD: 0.49 (0.21)
p = 0.014
Table 1: Results for context-independent features. For each feature and condition, we provide the mean
(standard deviation) for individuals with and without the condition. P-values for individual Wilcoxon
rank sum tests are provided. An up arrow (?) indicates a significant trend toward increased feature values
for positive individuals. A down arrow (?) indicates a significant trend toward decreased feature values
for positive individuals.
Me
an
fille
dp
au
se
si
np
art
icip
an
ts
eg
me
nt
(to
ke
ns
)
0
0.2
0.4
0.6
0.8
1.0
1.2
No depr.
?
Depr.
?
Me
an
fille
dp
au
se
si
np
art
icip
an
ts
eg
me
nt
(to
ke
ns
)
0
0.2
0.4
0.6
0.8
1.0
1.2
?PTSD PTSD
?
Figure 4: Number of filled pauses per speech seg-
ment.
This question is one of 95 topic setting utter-
ances in Ellie?s repertoire. (Ellie has additional
utterances that serve as continuation prompts,
backchannels, and empathy responses, which can
be used as a topic is discussed.)
To define context-dependent features, we asso-
ciate with each user segment the most recent of
Ellie?s topic setting utterances that has occurred in
the dialogue. We then focus our analysis on those
user segments and turns that follow specific topic
setting utterances. In Table 2, we present some ex-
amples of our findings for context-dependent fea-
tures for happy lasttime.3
3While we provide significance test results here at the p <
0.05 level, it should be noted that because of the large number
of context-dependent features that may be defined in a small
corpus such as ours, we report these merely as observations in
our corpus. We do not claim that these results transfer beyond
The contextual feature labeled (g?) in Table 2 is
the mean of the maximum valence feature across
all segments for which happy lasttime is the most
recent topic setting system utterance. We provide
a full example of this feature calculation in Fig-
ure 5 in the appendix.
As the figure shows, we find that users with
both PTSD and depression show a sharp reduc-
tion in the mean maximum valence in their speech
segments that respond to this question. This sug-
gests that in these virtual human interactions, this
question plays a useful role in eliciting differen-
tial responses from subjects with these psycholog-
ical disorders. We observed three additional ques-
tions which showed a significant difference in the
mean maximum valence feature. One example is
the question, How would your best friend describe
you?.
With feature (b?) in Table 2, we find an in-
creased onset time in responses to this question for
subjects with depression.4 Feature (d?) shows that
subjects with PTSD exhibit shorter speech seg-
ments in their responses to this question.
We observed a range of findings of this sort for
various combinations of Ellie?s topic setting utter-
ances and specific context-dependent features. In
future work, we would like to study the optimal
combinations of context-dependent questions that
yield the most information about the user?s distress
status.
this data set.
4In comparing Table 2 with Table 1, this question seems
to induce a higher mean onset time for distressed users than
the average system utterance does. This does not seem to be
the case for non-distressed users.
198
Feature Depression (12 yes, 30 no) PTSD (19 yes, 23 no)
(g?) mean maximum valence
in user segments following
happy lasttime
?
Depr.: 0.15 (0.07)
No Depr.: 0.26 (0.12)
p = 0.003
?
PTSD: 0.16 (0.08)
No PTSD: 0.28 (0.11)
p = 0.0003
(b?) mean onset time of first
segments in user turns
following happy lasttime
?
Depr.: 2.64 (2.70)
No Depr.: 0.94 (1.80)
p = 0.030
n.s.
PTSD: 2.18 (2.48)
No PTSD: 0.80 (1.76)
p = 0.077
(d?) mean length of user
segments following
happy lasttime
n.s.
Depr.: 5.95 (1.80)
No Depr.: 10.03 (6.99)
p = 0.077
?
PTSD: 6.82 (5.12)
No PTSD: 10.55 (6.68)
p = 0.012
Table 2: Example results for context-dependent features. For each feature and condition, we provide
the mean (standard deviation) for individuals with and without the condition. P-values for individual
Wilcoxon rank sum tests are provided. An up arrow (?) indicates a significant trend toward increased
feature values for positive individuals. A down arrow (?) indicates a significant trend toward decreased
feature values for positive individuals.
5 Classification Results
In this section, we present some suggestive clas-
sification results for our data set. We construct
three binary classifiers that use the kinds of fea-
tures described in Section 4 to predict the pres-
ence of three conditions: PTSD, depression, and
distress. For the third condition, we define dis-
tress to be present if and only if PTSD, depres-
sion, or both are present. Such a notion of distress
that collapses distinctions between disorders may
be the most appropriate type of classification for a
potential application in which distressed users of
any type are prioritized for access to health care
professionals (who will make a more informed as-
sessment of specific conditions).
For each individual dialogue, each of the three
classifiers emits a single binary label. We train
and evaluate the classifiers in a 10-fold cross-
validation using Weka (Hall et al, 2009).
While our data set of 43 dialogues is perhaps
of a typical size for a study of a research proto-
type dialogue system, it remains very small from
a machine learning perspective. We report here
two kinds of results that help provide perspective
on the prospects for classification of these condi-
tions. The first kind looks at classification based
on all the context-independent features described
in Section 4.2.1. The second looks at classifica-
tion based on individual features from this set.
In the first set of experiments, we trained a
Na??ve Bayes classifier for each condition using
all the context-independent features. We present
our results in Table 3, comparing each classifier to
a baseline that always predicts the majority class
(i.e. no condition for PTSD, no condition for de-
pression, and with condition for distress).
We note first that the trained classifiers all out-
perform the baseline in terms of weighted F-score,
weighted precision, weighted recall, and accuracy.
The accuracy improvement over baseline is sub-
stantial for PTSD (20.9% absolute improvement)
and distress (23.2% absolute improvement). The
accuracy improvement over baseline is more mod-
est for depression (2.3% absolute). We believe
one factor in the relative difficulty of classifying
depression more accurately is the relatively small
number of depressed participants in our study
(13).
While it has been shown in prior work (Cohn et
al., 2009) that depression can be classified above
baseline performance using features observed in
clinical human-human interactions, here we have
shown that classification above baseline perfor-
mance is possible in interactions between human
participants and a virtual human dialogue system.
Further, we have shown classification results for
PTSD and distress as well as depression.
We tried incorporating context-dependent fea-
tures, and also unigram features, but found that
neither improved performance. We believe our
data set is too small for effective training with
these very large extended feature sets.
199
Disorder Model Weighted F-score Weighted Precision Weighted Recall Accuracy
PTSD Na??ve Bayes 0.738 0.754 0.744 74.4%
Majority Baseline 0.373 0.286 0.535 53.5%
Depression Na??ve Bayes 0.721 0.721 0.721 72.1%
Majority Baseline 0.574 0.487 0.698 69.8%
Distress Na??ve Bayes 0.743 0.750 0.744 74.4%
Majority Baseline 0.347 0.262 0.512 51.2%
Table 3: Classification results.
In our second set of experiments, we sought to
gain understanding of which features were pro-
viding the greatest value to classification perfor-
mance. We therefore retrained Na??ve Bayes classi-
fiers using only one feature at a time. We summa-
rize here some of the highest performing features.
For depression, we found that the feature stan-
dard deviation in onset time of first segment in
each user turn yielded very strong performance
by itself. In our corpus, we observed that de-
pressed individuals show a greater standard devia-
tion in the onset time of their responses to Ellie?s
questions (p = 0.024, Wilcoxon rank sum test).
The value of this feature in classification comple-
ments the clinical finding that depressed individu-
als show greater onset times in their responses to
interview questions (Cohn et al, 2009).
For distress, we found that the feature mean
maximum valence in user segments was the most
valuable. We discussed findings for a context-
dependent version of this feature in Section 4.3.
This finding for distress can be related to previ-
ous observations that individuals with depression
use more negatively valenced words (Rude et al,
2004).
For PTSD, we found that the feature mean num-
ber of filled pauses in user segments was among
the most informative.
Based on our observation of classification per-
formance using individual features, we believe
there remains much room for improvement in fea-
ture selection and training. A larger data set would
enable feature selection approaches that use held
out data, and would likely result in both increased
performance and deeper insights into the most
valuable combination of features for classification.
6 Conclusion
In this paper, we have explored the presence of in-
dicators of psychological distress in the linguistic
behavior of subjects in a corpus of semi-structured
virtual human interviews. In our data set, we
have identified several significant differences be-
tween subjects with depression and PTSD when
compared to non-distressed subjects. Drawing on
these features, we have presented statistical classi-
fication results that suggest the potential for auto-
matic assessment of psychological distress in indi-
vidual interactions with a virtual human dialogue
system.
Acknowledgments
This work is supported by DARPA under con-
tract (W911NF-04-D-0005) and the U.S. Army
Research, Development, and Engineering Com-
mand. The content does not necessarily reflect the
position or the policy of the Government, and no
official endorsement should be inferred.
References
Andrea Esuli Stefano Baccianella and Fabrizio Sebas-
tiani. 2010. Sentiwordnet 3.0: An enhanced lexi-
cal resource for sentiment analysis and opinion min-
ing. In Proceedings of the Seventh conference on
International Language Resources and Evaluation
(LREC?10), Valletta, Malta, May. European Lan-
guage Resources Association (ELRA).
Jeffery F. Cohn, Tomas Simon Kruez, Iain Matthews,
Ying Yang, Minh Hoai Nguyen, Margara Tejera
Padilla, Feng Zhou, and Fernando De la Torre.
2009. Detecting depression from facial actions and
vocal prosody. In Affective Computing and Intelli-
gent Interaction (ACII), September.
Judith A. Hall, Jinni A. Harrigan, and Robert Rosen-
thal. 1995. Nonverbal behavior in clinician-patient
interaction. Applied and Preventive Psychology,
4(1):21 ? 37.
Mark Hall, Eibe Frank, Geoffrey Holmes, Bernhard
Pfahringer, Peter Reutemann, and Ian H. Witten.
2009. The weka data mining software: an update.
SIGKDD Explor. Newsl., 11(1):10?18, November.
Peter A Heeman, Rebecca Lunsford, Ethan Selfridge,
Lois Black, and Jan Van Santen. 2010. Autism and
200
interactional aspects of dialogue. In Proceedings
of the 11th Annual Meeting of the Special Interest
Group on Discourse and Dialogue, pages 249?252.
Association for Computational Linguistics.
Kai Hong, Christian G. Kohler, Mary E. March, Am-
ber A. Parker, and Ani Nenkova. 2012. Lexi-
cal differences in autobiographical narratives from
schizophrenic patients and healthy controls. In Pro-
ceedings of the 2012 Joint Conference on Empirical
Methods in Natural Language Processing and Com-
putational Natural Language Learning, pages 37?
47, Jeju Island, Korea, July. Association for Compu-
tational Linguistics.
Christine Howes, Matthew Purver, Rose McCabe,
Patrick G. T. Healey, and Mary Lavelle. 2012.
Predicting adherence to treatment for schizophrenia
from dialogue transcripts. In Proceedings of the
13th Annual Meeting of the Special Interest Group
on Discourse and Dialogue, pages 79?83, Seoul,
South Korea, July. Association for Computational
Linguistics.
Shannon J Johnson, Michelle D Sherman, Jeanne S
Hoffman, Larry C James, Patti L Johnson, John E
Lochman, Thomas N Magee, David Riggs, Jes-
sica Henderson Daniel, Ronald S Palomares, et al
2007. The psychological needs of US military ser-
vice members and their families: A preliminary re-
port. American Psychological Association Presi-
dential Task Force on Military Deployment Services
for Youth, Families and Service Members.
Kurt Kroenke, Robert L. Spitzer, and Janet B. W.
Williams. 2001. The phq-9. Journal of General
Internal Medicine, 16(9):606?613.
Maider Lehr, Emily Prud?hommeaux, Izhak Shafran,
and Brian Roark. 2012. Fully automated neuropsy-
chological assessment for detecting mild cognitive
impairment. In Interspeech 2012: 13th Annual Con-
ference of the International Speech Communication
Association, Portland, Oregon, September.
Emily Prud?hommeaux and Brian Roark. 2011. Ex-
traction of narrative recall patterns for neuropsycho-
logical assessment. In Interspeech 2011: 12th An-
nual Conference of the International Speech Com-
munication Association, pages 3021?3024, Flo-
rence, Italy, August.
Stephanie Rude, Eva-Maria Gortner, and James Pen-
nebaker. 2004. Language use of depressed and
depression-vulnerable college students. Cognition
& Emotion, 18(8):1121?1133.
S. Scherer, G. Stratou, M. Mahmoud, J. Boberg,
J. Gratch, A. Rizzo, and L.-P. Morency. 2013. Au-
tomatic behavior descriptors for psychological dis-
order analysis. In IEEE Conference on Automatic
Face and Gesture Recognition.
Maite Taboada, Julian Brooke, Milan Tofiloski, Kim-
berly Voll, and Manfred Stede. 2011. Lexicon-
based methods for sentiment analysis. Computa-
tional Linguistics, 37(2):267?307, June.
Marcus Thiebaux, Stacy Marsella, Andrew N. Mar-
shall, and Marcelo Kallmann. 2008. Smartbody:
behavior realization for embodied conversational
agents. In Proceedings of the 7th international
joint conference on Autonomous agents and multi-
agent systems - Volume 1, AAMAS ?08, pages 151?
158, Richland, SC. International Foundation for Au-
tonomous Agents and Multiagent Systems.
Peter Wittenburg, Hennie Brugman, Albert Russel,
Alex Klassmann, and Han Sloetjes. 2006. Elan: a
professional framework for multimodality research.
In Proceedings of LREC 2006, Fifth International
Conference on Language Resources and Evaluation.
Fan Yang and Peter A. Heeman. 2010. Initiative con-
flicts in task-oriented dialogue. Computer Speech &
Language, 24(2):175 ? 189.
201
Appendix A. Wizard-of-Oz Dialogue Excerpts
Example user with PTSD and depression Example non-distressed user
max
valence
transcript
Ellie (happy lasttime) tell me
about the last time you
felt really happy
User 0.014 when i play with my dog
Ellie when was the last time
that happened?
User 0.000 this morning
Ellie aw
Ellie tell me more about that
User 0.312 i don?t know i just
User 0.010 whenever i get
User 0.312 super bored or i just
need some time to
User 0.312 for myself i just play
with the dog
Ellie that?s good
User 0.000 yeah
0.1371 = mean
maximum valence in
user segments following
happy lasttime
Ellie (ideal weekendC) tell
me how you spend your
ideal weekend
max
valence
transcript
Ellie (happy lasttime) tell me
about the last time you
felt really happy
User 0.562 um the last time i felt
really happy was
User 0.000 hm
User 0.000 today
Ellie tell me more about that
User 0.688 uh just from the moment
i woke up it was a
beautiful sunny day
User -0.062 i
User 0.565 went to see some friends
we had a good time
went to school
User 0.565 had some good grades
on some papers um
wrote a good essay
User 0.292 feel pretty accomplished
and
User 0.312 i feel like my day is just
User 0.565 a good day
Ellie that?s so good to hear
0.3487 = mean
maximum valence in
user segments following
happy lasttime
Ellie (BF describe) how
would your best friend
describe you?
Figure 5: Examples of maximum valence feature.
202
Proceedings of the SIGDIAL 2013 Conference, pages 251?260,
Metz, France, 22-24 August 2013. c?2013 Association for Computational Linguistics
Surface Text based Dialogue Models for Virtual Humans
Sudeep Gandhe and David Traum
USC Institute for Creative Technologies,
12015 Waterfront Drive, Playa Vista, CA 90094, USA
srgandhe@gmail.com, traum@ict.usc.edu
Abstract
We present virtual human dialogue mod-
els which primarily operate on the surface
text level and can be extended to incor-
porate additional information state annota-
tions such as topics or results from simpler
models. We compare these models with
previously proposed models as well as two
human-level upper baselines. The mod-
els are evaluated by collecting appropri-
ateness judgments from human judges for
responses generated for a set of fixed dia-
logue contexts. Our results show that the
best performing models achieve close to
human-level performance and require only
surface text dialogue transcripts to train.
1 Introduction
Virtual Humans (VH) are autonomous agents who
can play the role of humans in simulations (Rickel
and Johnson, 1999; Traum et al, 2005). For these
simulations to be convincing these agents must
have the ability to communicate with humans and
other agents using natural language. Like other di-
alogue system types, different architectures have
been proposed for virtual human dialogue sys-
tems. These architectures can afford different fea-
tures and require different sets of resources. E.g.,
an information state based architecture such as the
one used in SASO-ST (Traum et al, 2005) can
model detailed understanding of the task at hand
and progression of dialogue, but at the cost of re-
quiring resources such as information state update
rules and an annotated corpus or grammar to be
able to map surface text to dialogue acts.
For some virtual human dialogue genres such
as simple question-answering or some negotiation
domains, a simple model of dialogue progression
would suffice. In such a case we can build dia-
logue models that primarily operate on a surface
text level. These models only require surface text
dialogue transcripts as a resource, and don?t re-
quire expensive manual update rules, grammars,
or even extensive corpus annotation.
In this paper, we describe the construction and
evaluation of several models for engaging in dia-
logue by selecting an utterance that has been seen
previously in a corpus. We include one model that
has been used for this task previously (Gandhe and
Traum, 2007b), an adaptation of a model that has
been used in a similar manner, though on hand-
authored data sets, rather than data sets extracted
automatically from a corpus (Leuski and Traum,
2008), as well as a new set of models, using per-
ceptrons on surface text features as well as more
abstract information state annotations such as top-
ics. We also tackle the question of evaluating such
dialogue models manually as well as automati-
cally, starting with systematically analyzing var-
ious decisions involved in the evaluation process.
We situate our work with respect to previous eval-
uation methods.
2 Related Work
The task of a dialogue model is to formulate an
utterance given a dialogue context. There are
two approaches towards formulating an utterance:
Generation, where a response is compositionally
created from elements of the information state,
including the context of previous utterances, and
Selection, where a response is chosen from pre-
viously seen set of responses. In (Gandhe and
Traum, 2010), we examined the theoretical poten-
tial for the selection approach, looking at a wide
variety of domains, and evaluating based on sim-
ilarity between the actual utterance and the best
match in the previously seen corpus. We saw a
wide variance in scores across domains, both as to
the similarity scores and improvement of scores as
more data is considered. For task-oriented plan-
ning domains, such as Monroe (Stent, 2000) and
251
TRAINS (Heeman and Allen, 1994), as well as
open conversation in Switchboard (Godfrey et al,
1992), the performance was very low. On the other
hand, for more limited domains such as simple
question-answering (Leuski et al, 2006) or role-
play negotiation in a scenario, the performance
was high, with METEOR scores averaging over
0.8.
One possible selection criterion is to assume
that the most appropriate response is the most
probable response according to a model trained
on human-human dialogues. More formally, let
there be a dialogue ?u1, u2, . . . , ut?1, ut, . . . , uT ?,
where utterance ut appears in contextt =
?u1, u2, . . . , ut?1?. If we have a dialogue model
P estimated from the training corpus then the for-
mulated response uq for some unseen contextq is
given by,
ut = argmax
i
P (ui|contextt) ?ui ? Upossible
where Upossible is a set of all possible response ut-
terances. Ideally we would like to estimate a prob-
ability distribution P , but since it?s hard to esti-
mate and we only need argmax for this applica-
tion, we approximate P with a ranking function.
We can compare previous work within this frame-
work.
In our previous work (Gandhe and Traum,
2007a), we used context similarity as the rank-
ing function P (see section 3.1 for details). This
model is trained from in-domain surface text di-
alogue transcripts. Leuski et al (2006) model P
as cross-lingual relevance, where the task of se-
lecting an appropriate response is seen as cross-
lingual information retrieval where the response
utterance ut is the relevant document and the
contextt is treated as a query from different lan-
guage. This model has been applied to simple
question answering where context is the previous
utterance and the training data is manually anno-
tated question-answer pairs. DeVault et al (2011)
have proposed to use a multi-class classification
model (such as maximum entropy) for estimat-
ing P . Their method restricts the set Upossible
to a set of canonical utterances which represent
distinct dialogue acts. This allows for a limited
number of classes (|Upossible|) and also maximizes
the number of distinct contexts seen per utterance.
This model is also trained from manually anno-
tated utterance-context pairs and can additionally
use manually created utterance paraphrases.
Apart from the models discussed above which
have been mainly applied to dialogue domains
situated in a story context, there has been some
work in surface text based dialogue models for
open domains. Ritter et al (2011) use informa-
tion retrieval based and statistical machine trans-
lation (SMT) based approaches towards predicting
the next response in Twitter conversations. Also
Chatbots typically use surface text based process-
ing such as string transformations (e.g., AIML
rules (Wallace, 2003)). Such rules can also be
learned from a dialogue corpus (Abu Shawar and
Atwell, 2005). Systems employing SMT or string
transformation rules are formulating a response
by Generation approach and it can be frequently
ungrammatical or incoherent, unlike the selection
approach which will always pick something that
someone has once said (even though it might be
inappropriate in the current context).
3 Dialogue Models
3.1 Nearest Context
In previous work (Gandhe and Traum, 2007a), we
modeled P as,
P (ui|contextq) ? Sim(contexti, contextq)
where contexti is the context in which utterance
ui was seen in training corpus and Sim is con-
text similarity in a customized vector-space model.
The model restricts the set of possible response
utterances (Upossible) to the set of utterances ob-
served in the training data (Utrain). The context
is approximated using the previous two utterances
(one from each speaker). This model does not use
the contents of the utterance ui itself.
3.2 Cross-lingual Relevance Model
Leuski et al (2006) model P as a cross-lingual rel-
evance model. This model takes into account the
content of the utterance ui as well as the content of
the context. It does not impose any restriction on
Upossible, but in practice it is restricted to the set
of utterances in the training data. The model al-
lows the context to be composed of multiple fields,
each with its own weight. This allows us to ex-
tend the model where the context is approximated
by the previous two utterances. The weights need
to be learned using a held-out development set,
which presents a challenge in the case of multiple
fields (possible if we add more information state
annotations), modest amounts of training data and
252
non-availability of an automatic and reliable esti-
mate of the model?s performance. Here, for the
first time, we apply this model to automatically
extracted pairs of utterance-context and evaluate
it. For our model we used the implementation that
is available as a part of NPCEditor (Leuski and
Traum, 2011) and manually set the field weights
corresponding to the two previous utterances to be
equal (0.5).
3.3 Perceptron
As discussed earlier, the task of selecting the most
appropriate response can be viewed as multi-class
classification. But there are a couple of issues.
First, since we operate at the surface text level,
each unique response utterance will be labeled as
a separate class. The number of classes is the
number of unique utterances seen in the training
set, which is relatively large. As the training data
grows, the number of classes will increase. Sec-
ond, there are very few examples (on average a
single example) per class. We need a classifier that
can overcome these issues.
The perceptron algorithm and its variants ?
voted perceptron and averaged perceptron are
well known classification models (Freund and
Schapire, 1999). They have been extended for use
in various natural language processing tasks such
as part-of-speech tagging (Collins, 2002), pars-
ing (Collins, 2004) and discriminative language
modeling (Roark et al, 2007). Here we use the
averaged perceptron model for mapping from dia-
logue context to an appropriate response utterance.
Collins (2002) outlines the following four com-
ponents of a perceptron model:
? The training data. In our case it is a set of au-
tomatically extracted utterance-context pairs
{. . . , ?ui, contexti?, . . .}
? A function GEN(context) that enumerates a
set of all possible outputs (response utter-
ances) for any possible input (dialogue con-
text)
? A feature extraction function ? :
?u, context? ? Rd that is defined over
all possible pairings of response utterances
and dialogue contexts. d is the total number
of possible features.
? A parameter vector ?? ? Rd
Using such a perceptron model, the most appropri-
ate response utterance (ut) for the given dialogue
context (contextt) is given by,
uq = argmax
ui?GEN(context)
?(ui, contextq) ? ??
Algorithm 1 Perceptron Training Algorithm
Initialize: t? 0 ; ??0 ? 0
for iter = 1 to MAX ITER do
for i = 1 to N do
ri ? argmaxu?GEN(contexti) ?(u, contexti) ? ??t
if ri 6= ui then
??t+1 ? ??t + ?(ui, contexti) ?
?(ri, contexti)
else
??t+1 ? ??t
end if
t? t+ 1
end for
end for
return ??? (?t ??t)/(MAX ITER?N)
The parameter vector ?? is trained using the
training algorithm described in Algorithm 1. The
algorithm goes through the training data one in-
stance at a time. For every training instance, it
computes the best response utterance (ri) for the
context based on its current estimate of the param-
eter vector ??t. The algorithm changes the param-
eter vector only if it makes an error (ri 6= ui). The
update drives the parameter vector away from the
error (ri) and towards the correct output (ui). The
final parameter vector ?? is an average of all the in-
termediate ??t values. The averaging of parameter
vectors avoids overfitting.
The feature extraction function ? can list any
arbitrary features from the pair ?u, context?. We
consider information state annotations (ISt) along
with the surface text corresponding to the previous
two turns. The features could also include scores
computed from other models, such as those pre-
sented in sections 3.1 and 3.2. Figure 1 illustrates
an example context and utterance, and several fea-
tures. We examine several sets of features, Surface
text based features (?S), Retrieval model based
features (?R), and Topic based features (?T ).
Surface text based features (?S) are the fea-
tures extracted from the surface text of the previ-
ous utterances in the dialogue context (contextj)
and the response utterance (ui). ?S(d)(ux, uy) ex-
tracts surface text features from two utterances ? a
response utterance (ux) and an utterance (uy) from
the context that is (d) utterances away. There are
four types of features we extract:
253
? common term(d,w) features indicate the
number of times a wordw appears in both the
utterances. The total number of possible fea-
tures is O(|V |) and we select a small subset
of words (Selected common(d)) from the
vocabulary.
? The common term count(d) feature indi-
cates the number of words that appear in both
utterances.
? The unique common term count(d) fea-
ture indicates the number of unique words
that appear in both utterances.
? cross term(d,wx, wy) features indicate the
number of times the word wx appears in the
utterance ux and the word wy appears in the
utterance uy. The total possible number of
such cross features is very large (O(|V |2)),
where |V | is the utterance vocabulary size.
In order to keep the training tractable and
avoid overfitting, we select a small subset of
cross features (Selected cross(d)) from all
possible features.
In this model, we perform feature selection by
selecting the subsets Selected cross(d) and
Selected common(d). The training algorithm re-
quires evaluating the feature extraction (?S) func-
tion for all possible pairings of response utterances
and contexts. One simple feature selection crite-
rion is to allow the features only appearing in true
pairings of response utterance and context (i.e.
features from ?S(?ui, contextj?) ?i = j). The
subset Selected common(d) for common term
features is selected by extracting features from
only such true pairings.
For selecting cross term(d,wx, wy) features
we use only true pairings but we need to
reduce this subset even further. We im-
pose additional constraints based on the col-
lection frequency of lexical events such as,
cf(wx) > thresholdx, cf(wy) > thresholdy,
cf(?wx, wy?) > thresholdxy. Further reduction
in size of the selected subset of cross term fea-
tures is achieved by ranking the features using a
suitable ranking function and choosing the top n
features. In this model, we rank the cross term
features based on pointwise mutual-information
pmi(?wx, wy?) given by,
log p(?wx, wy?)p(wx)p(wy)
= log
(
#?wx,wy?
#??,??
)
(
#?wx,??
#??,??
)
?
(
#??,wy?
#??,??
)
Summing up, ?S(d)(ux, uy) =
{cross term(d,wx, wy) : wx ? ux?
wy ? uy ? ?wx, wy? ? Selected cross(d)}
? {common term(d,w) : w ? ux ?w ? uy ?
w ? Selected common(d)}
? {common term count(d)}
? {unique common term count(d)}
Retrieval model based features (?R) are
the scores computed in a fashion similar to
the Nearest Context model. Sim(ux, uy) is
a cosine similarity function for tf-idf weighted
vector space representations of utterances and
Sim(contexta, contextb) is the same function
from Nearest Context model. We define three fea-
tures,
? retrieval score =
|L|max
k=1
Sim(contextj , contextk) ? Sim(ui, uk)
? context sim@best utt match =
Sim(contextj , contextb)
where, b = |L|argmax
k=1
Sim(ui, uk)
? utt sim@best context match = Sim(ui, ub)
where, b = |L|argmax
k=1
Sim(contextj , contextk)
?R(?ui, contextj?) = {retrieval score,
context sim@best utt match,
utt sim@best context match}
Topic based feature (?T ) tracks the topic sim-
ilarity between the topic of the dialogue context
and the response utterance. A topic is marked
as mentioned if a set of keywords triggering that
topic have been previously mentioned in the dia-
logue. Each information state (IS) consists of a
topic signature which can be viewed as a boolean
vector representing mentions of topics.
?T (?ui, contextj?) = {topic similarity}
topic similarity = cosine(ISi, ISj)
where, ISi is the topic and is part of contexti
which is the context associated with the utterance
ui.
The perceptron model presented here allows
novel combinations of resources such as combin-
ing surface text transcripts with information state
annotations for tracking topics in the conversa-
tion. As compared to the generative cross-lingual
relevance model approach, the perceptron model
is a discriminative model. It is also a paramet-
ric model and the inference requires linear time
with respect to the size of candidate utterances
(|GEN(context)|) and the number of features (|??|).
Although, computing some of the features them-
selves (e.g., ?R features) requires linear time with
254
...
contextj [uj(?2)] Doctor you are the threat i need protection from you
[uj(?1)] Captain no no
you do you do not need protection from me
i am here to help you
uh what i would like to do is move your your clinic to a safer location
and uh give you money and medicine to help build it
utterance [ui] Doctor i have no way of moving
?S(?ui, contextj?) = { cross term(?2, ?moving?, ?need?) = 1,
common term(?2, ?i?) = 1,
common term count(?2) = 1, unique common term count(?2) = 1,
cross term(?1, ?moving?, ?give?) = 1,
common term(?1, ?i?) = 1, common term(?1, ?no?) = 1,
common term count(?1) = 2, unique common term count(?1) = 2,
retrieval score = 0.198, context sim@best utt match = 0.198,
utt sim@best context match = 0,
topic similarity = 0.667 }
Figure 1: Features extracted from a context (contextj) and a response utterance (ui)
respect to the size of the training data. The per-
ceptron model can rank an arbitrary set of utter-
ances given a dialogue context. But some of the
features (e.g., topic similarity) require that the
utterance ui(ui ? |GEN(context)|) be associated
with a known context (contexti). For all our mod-
els we use GEN(context) = Utrain.
We have implemented three different vari-
ants of the perceptron model based on the
choice of features used. Perceptron(surface)
model uses only surface text features (? =
?S). The other two models are Percep-
tron(surface+retrieval) where ? = ?S ? ?R and
Perceptron(surface+retrieval+topic) where ? =
?S ? ?R ? ?T .
Figure 2 shows a schematic representation of
these models along with the set of resources be-
ing used by each model. The figure also shows the
relationships between these models. The arrows
point from a less informative model to a more in-
formative model and the annotations on these ar-
rows indicate the additional information used.
4 Evaluation
For the experiments reported in this paper, we
used the human-human spoken dialogue corpus
collected for the project SASO-ST (Traum et al,
2005). In this scenario, the trainee acts as an
Army Captain negotiating with a simulated doc-
Figure 2: A schematic representation of imple-
mented unsupervised dialogue models and the re-
lationships between the information used by their
ranking functions.
tor to convince him to move his clinic to another
location. The corpus is a collection of 23 roleplay
dialogues and 13 WoZ dialogues lasting an aver-
age of 40 turns (a total of ? 1400 turns and ? 30k
words).
We perform a Static Context evalua-
tion (Gandhe, 2013). In Static Context evaluation,
all the dialogue models being evaluated receive
the same set of contexts as input. These dialogue
contexts are extracted from actual in-domain
255
human-human dialogues and are not affected by
the dialogue model being evaluated. For every
turn whose role is to be played by the system, we
predict the most appropriate response in place of
that turn given the dialogue context.
Since the goal for virtual humans is to be as
human-like as possible, a suitable evaluation met-
ric is how appropriate or human-like the responses
are for a given dialogue context. The evaluation
reported here employs human judges. We set up a
simple subjective 5-point likert scale for rating ap-
propriateness ? 1 being a very inappropriate non-
sensical response and 5 being a perfectly appropri-
ate response.
We built five dialogue models to play the role
of the doctor in SASO-ST domain, viz.: Near-
est Context (section 3.1), Cross-lingual Relevance
Model (section 3.2) and three perceptron models
(section 3.3) with different feature sets. These
dialogue models are evaluated using 5 in-domain
human-human dialogues from the training data (2
roleplay and 3 WoZ dialogues, referred to as test
dialogues). A dialogue model is trained in a leave-
one-out fashion where the training data consists of
all dialogues except the one test dialogue that is
being evaluated. A dialogue model trained in this
fashion is then used to predict the most appropri-
ate response for every context that appears in the
test dialogue. This process is repeated for each test
dialogue and for each dialogue model being evalu-
ated. In this evaluation setting, the actual response
utterance found in the original human-human dia-
logue may not belong to the set of utterances being
ranked by the dialogue model. We also compare
these five dialogue models with two human-level
upper baselines. Figure 4 in the appendix shows
some examples of utterances returned by a couple
of the models.
4.1 Human-level Upper Baselines
In order to establish an upper baseline for human-
level performance for the evaluation task, we con-
ducted a wizard data collection. We asked human
volunteers (wizards) to perform a similar task to
that performed by the dialogue models being eval-
uated. The wizard is presented with a set of ut-
terances (Utrain) and is asked to select a subset
from these that will be appropriate as a response
for the presented dialogue context. Compared to
this, the task of the dialogue model is to select
a single most appropriate response for the given
context.
DeVault et al (2011) carried out a similar wiz-
ard data collection but at the dialogue act level,
where wizards were asked to select only one re-
sponse dialogue act for each dialogue context.
Their findings suggest that there are several valid
response dialogue acts for a dialogue context. A
specific dialogue act can be realized in several
ways at the surface text level. For these reasons
we believe that for a given dialogue context there
are often several appropriate response utterances
at the surface text level. In our setting the dia-
logue models work at the surface text level and
hence the wizards were asked to select a subset of
surface text utterances that would be appropriate
responses. Each wizard was asked to select sev-
eral (ideally between five and ten, but always at
least one) appropriate responses for each dialogue
context. Four wizards participated in this data col-
lection with each wizard selecting responses for
the contexts from the same five human-human test
dialogues. The set of utterances to chose from
(Utrain) for every test dialogue was built in the
same leave-one-out fashion as used for evaluating
the implemented dialogue models.
There are a total of 89 dialogue contexts where
the next turn belongs to doctor. As expected, wiz-
ards frequently chose multiple utterances as ap-
propriate responses (mean = 7.80, min = 1, max
= 25).
This data collected from wizards is used to build
two human-level upper-baseline models for the
task of selecting a response utterance given a di-
alogue context:
Wizard Max Voted model returns the response
which gets the maximum number of votes
from the four wizards. Ties are broken
randomly.
Wizard Random model returns a random utter-
ance from the list of all utterances marked as
appropriate by one of the wizards.
4.2 Comparative Evaluation of Models
We performed a static context evaluation using
four judges for the above-mentioned two human-
level baselines (Wizard Random and Wizard Max
Voted) and five dialogue models (Nearest Con-
text, Cross-lingual Relevance Model and three
perceptron models), as described in section 3.3.
We tune the parameters used for the perceptron
256
models based on the automatic evaluation met-
ric, Weak Agreement (DeVault et al, 2011). Ac-
cording to this evaluation metric a response utter-
ance is judged as perfectly appropriate (a score
of 5) if any of the wizards chose this response
utterance for given context and inappropriate (a
score of 0) otherwise. The Perceptron(surface)
model was trained using 30 iterations, the Per-
ceptron(surface+retrieval) using 20 iterations,
and the Perceptron(surface+retrieval+topic) was
trained using 25 iterations. For all perceptron
models we used thresholdx = thresholdy =
thresholdxy = 3.
For a comparative evaluation of dialogue mod-
els, we need an evaluation setup where judges
could see the complete dialogue context along
with the response utterances generated by the di-
alogue models to be evaluated. In this setup, we
show all the response utterances next to each other
for easy comparison and we do not show the ac-
tual response utterance that was encountered in
the original human-human dialogue. We built a
web interface for collecting appropriateness rat-
ings that addresses the above requirements. Fig-
ure 3 shows the web interface used by the four
judges to evaluate the appropriateness of response
utterances for given dialogue context. The appro-
priateness was rated on the same scale of 1 to 5.
The original human-human dialogue (roleplay or
WoZ) is shown on the left hand side and the re-
sponse utterances from different dialogue models
are shown on the right hand side. In cases where
different dialogue models produce the same sur-
face text response only one candidate surface text
is shown to judge. Once the judge has rated all the
candidate responses they can proceed to the next
dialogue context. This setting allows for compar-
ative evaluation of different dialogue models. The
presentation order of responses from different di-
alogue models is randomized. Two of the judges
also performed the role of the wizards in our wiz-
ard data collection as outlined in section 4.1, but
the wizard data collection and the evaluation tasks
were separated by a period of over 3 months.
Table 1 shows the results of our compara-
tive evaluation for each judge and averaged over
all judges. We also computed inter-rater agree-
ment for individual ratings for all response ut-
terances using Krippendorff?s ? (Krippendorff,
2004). There were a total of n = 397 distinct
response utterances that were judged by the eval-
uators. The Krippendorff?s ? for all four judges
was 0.425 and it ranges from 0.359 to 0.495 for
different subsets of judges. The value of ? indi-
cates that the inter-rater agreement is substantially
above chance (? > 0), but indicates a fair amount
of disagreement, indicating that judging appropri-
ateness is a hard task even for human judges. Al-
though there is low inter-rater agreement at the
individual response utterance level there is high
agreement at the dialogue model level. Pearson?s
correlation between the average appropriateness
for different dialogue models ranges from 0.928
to 0.995 for different pairs of judges.
We performed a paired Wilcoxon test to check
for statistically significant differences in differ-
ent dialogue models. Wizard Max Voted is sig-
nificantly more appropriate than all other models
(p < 0.001). Wizard Random is significantly more
appropriate than Cross-lingual Relevance Model
(p < 0.05) and significantly more appropriate
than the three perceptron models as well as Near-
est Context model (p < 0.001). Cross-lingual
Relevance Model is significantly more appropri-
ate than Nearest Context (p < 0.01). All other
differences are not statistically significant at the 5
percent level.
We found that adding topic annotations did not
help. This is in contrast with previous observa-
tion (Gandhe and Traum, 2007b), where topic in-
formation helped when evaluation was performed
in Dynamic Context setting. In Dynamic Context
setting, the dialogue model is used in an online
fashion where the response utterances it generates
become part of the dialogue contexts with respect
to which the subsequent responses are predicted
and evaluated. The topic information ensures sys-
tematic progression of dialogue. But for static
context evaluation such help is not required as the
dialogue contexts are extracted from human hu-
man dialogues and are fixed.
5 Conclusion
In this paper we introduced dialogue models that
can be trained simply from in-domain surface
text dialogue transcripts. Some of these models
also allow for incorporating additional informa-
tion state features such as topics or results of sim-
pler models. We have evaluated the appropriate-
ness of responses and have compared these mod-
els with two human-level baselines. Evaluating
response appropriateness is highly subjective as
257
Figure 3: Screenshot of the user interface for static context comparative evaluation of dialogue models
Model #Utts Avg. appropriateness Appropriateness(All judges)
Judge 1 Judge 2 Judge 3 Judge 4 Avg stddev
Nearest Context 89 4.12 3.98 3.40 3.53 3.76 1.491
Perceptron(surface) 89 3.97 4.11 3.51 3.62 3.80 1.445
Perceptron
(surface+retrieval)
89 4.26 4.12 3.51 3.72 3.90 1.414
Perceptron
(surface+retrieval+topic)
89 4.21 4.09 3.51 3.57 3.85 1.433
Cross-lingual Relevance
Model
89 4.28 4.31 3.70 3.91 4.05 1.314
Wizard Random 89 4.55 4.55 4.03 4.16 4.32 1.153
Wizard Max Voted 89 4.76 4.84 4.40 4.52 4.63 0.806
Table 1: Offline comparative evaluation of dialogue models.
can be seen from the fact that utterances which
receive more wizard votes (Wizad Max Voted) re-
ceive significantly higher appropriateness ratings
than those which receive fewer votes (Wizard Ran-
dom). The performance of best performing dia-
logue models are close to human-level baselines.
In future we plan to use larger datasets which
should be easy, since no additional annotations are
required for training these dialogue models.
Acknowledgments
The effort described here has been sponsored by
the U.S. Army. Any opinions, content or informa-
tion presented does not necessarily reflect the posi-
tion or the policy of the United States Government,
and no official endorsement should be inferred.
References
Bayan Abu Shawar and Eric Atwell. 2005. Using cor-
pora in machine-learning chatbot systems. Interna-
tional Journal of Corpus Linguistics, 10:489?516.
258
Michael Collins. 2002. Discriminative training meth-
ods for hidden markov models: theory and experi-
ments with perceptron algorithms. In Proceedings
of the ACL-02 conference on Empirical methods in
natural language processing - Volume 10, EMNLP
?02, pages 1?8, Stroudsburg, PA, USA. Association
for Computational Linguistics.
Michael Collins, 2004. Parameter estimation for sta-
tistical parsing models: theory and practice of
distribution-free methods, pages 19?55. Kluwer
Academic Publishers, Norwell, MA, USA.
David DeVault, Anton Leuski, and Kenji Sagae. 2011.
Toward learning and evaluation of dialogue policies
with text examples. In Proceedings of the SIGDIAL
2011 Conference, pages 39?48, Portland, Oregon,
June. Association for Computational Linguistics.
Yoav Freund and Robert E. Schapire. 1999. Large
margin classification using the perceptron algorithm.
Mach. Learn., 37:277?296, December.
Sudeep Gandhe and David Traum. 2007a. Creating
spoken dialogue characters from corpora without an-
notations. In Proceedings of Interspeech-07.
Sudeep Gandhe and David Traum. 2007b. First steps
towards dialogue modeling from an un-annotated
human-human corpus. In 5th Workshop on knowl-
edge and reasoning in practical dialogue systems,
Hyderabad, India.
Sudeep Gandhe and David Traum. 2010. I?ve said it
before, and i?ll say it again: an empirical investiga-
tion of the upper bound of the selection approach to
dialogue. In Proceedings of the 11th Annual Meet-
ing of the Special Interest Group on Discourse and
Dialogue, SIGDIAL ?10, pages 245?248, Strouds-
burg, PA, USA. Association for Computational Lin-
guistics.
Sudeep Gandhe. 2013. Rapid prototyping and evalu-
ation of dialogue systems for virtual humans. Ph.D.
thesis, University of Southern California.
J. J. Godfrey, E. C. Holliman, and J. McDaniel. 1992.
Switchboard: Telephone speech corpus for research
and development. In Proc. of ICASSP-92, pages
517?520.
P. A. Heeman and J. Allen. 1994. The TRAINS 93 di-
alogues. TRAINS Technical Note 94-2, Department
of Computer Science, University of Rochester.
Klaus Krippendorff. 2004. Content Analysis, An Intro-
duction to Its Methodology 2nd Edition. Sage Pub-
lications.
Anton Leuski and David Traum. 2008. A statistical
approach for text processing in virtual humans. In
Proccedings of 26th Army Science Conference.
Anton Leuski and David Traum. 2011. Npceditor:
Creating virtual human dialogue using information
retrieval techniques. AI Magazine, 32(2):42?56.
Anton Leuski, Ronakkumar Patel, David Traum, and
Brandon Kennedy. 2006. Building effective ques-
tion answering characters. In Proceedings of the
7th SIGdial Workshop on Discourse and Dialogue,
pages 18?27, Sydney, Australia, July. Association
for Computational Linguistics.
Jeff Rickel and W. Lewis Johnson. 1999. Virtual hu-
mans for team training in virtual reality. In Proceed-
ings of the Ninth International Conference on Artifi-
cial Intelligence in Education, pages 578?585. IOS
Press.
Alan Ritter, Colin Cherry, and William B. Dolan. 2011.
Data-driven response generation in social media. In
Proceedings of the Conference on Empirical Meth-
ods in Natural Language Processing, EMNLP ?11,
pages 583?593, Stroudsburg, PA, USA. Association
for Computational Linguistics.
Brian Roark, Murat Saraclar, and Michael Collins.
2007. Discriminative n-gram language modeling.
Comput. Speech Lang., 21:373?392, April.
A. J. Stent. 2000. The monroe corpus. Technical
Report 728, Computer Science Dept. University of
Rochester.
David Traum, William Swartout, Jonathan Gratch, and
Stacy Marsella. 2005. Virtual humans for non-
team interaction training. In AAMAS-05 Workshop
on Creating Bonds with Humanoids, July.
Richard Wallace. 2003. AIML Overview. ALICE A. I.
Foundation.
259
Original Human-Human Dialogue Model response
Captain hello how are you doing sir
Doctor what do you want
i have patients waiting for me
hello
what can i do for you mr
Captain sir i come by to offer you some assistance ah on
behalf of my commander
ah we want to try to support you in this hospital
and ah improving the health and welfare of the
civilians that you have under your care
Doctor well this is your opinion i see captain
is this a is this a suggestion from your
commander
Captain i am sorry sir
could you say again
Doctor well this is your opinion you are the threat
i need protection from you
(a) Perceptron(surface)
Original Human-Human Dialogue Model response
Captain hello doctor perez
Doctor hello hello
what was your name
captain dezois very nice to meet you
i am sorry but i am very busy today
so i only have a limited amount of time
what can i help you with
Captain i am captain xx
Doctor so
do you need help
ok
what do you want i have patients waiting for me
Captain yes
i have a very urgent matter to discuss
with you
Doctor are you injured pl please captain
i i am sorry to cut you off
but i really must uh go
i was in the middle of examining a patient
(b) Cross-lingual Relevance Model
Figure 4: Example interaction for the dialogue models in static context setting. The second column
shows the original human-human dialogue and the third column shows the dialogue model?s response
for the corresponding system turn.
260
Proceedings of the SIGDIAL 2013 Conference, pages 372?374,
Metz, France, 22-24 August 2013. c?2013 Association for Computational Linguistics
Roundtable: An Online Framework for Building Web-based         Conversational Agents 
 Eric Forbell and Nicolai Kalisch and Fabrizio Morbini and Kelly Christoffersen  and Kenji Sagae and David Traum and Albert A. Rizzo Institute for Creative Technologies University of Southern California Los Angeles, CA 90094, USA {lastname}@ict.usc.edu 
   
Abstract 
We present an online system that provides a complete web-based sandbox for creating, testing and publishing embodied conversation-al agents. The tool, called Roundtable, em-powers many different types of authors and varying team sizes to create flexible interac-tions by automating many editing workflows while limiting complexity and hiding architec-tural concerns. Finished characters can be pub-lished directly to web servers, enabling highly interactive applications.  1 Introduction To support the creation of a virtual guide system called SimCoach (Rizzo et al 2011) designed to help military service personnel and their families understand behavioral healthcare issues and learn about support resources, a core virtual human architecture that included a new dialogue man-agement approach was developed (Morbini et al, 2012b). SimCoach is an embodied, conversa-tional virtual human guide delivered via the web and is supported by a flexible information state dialogue manager called FLoReS designed to support mixed initiative dialogue with conversa-tional systems. Morbini et al (2012a) provide a detailed description of the dialogue manager. Although FLoReS supports a wide variety of virtual human character behaviors, these must be specified in dialogue policies that must be au-thored manually. Initially, authoring for this dia-logue manager required coding of policies using a custom programming language. Therefore sig-nificant training for content authors was neces-sary, as well as substantial support from dialogue 
system developers in managing resources such as training data for the language understanding sys-tem. To improve the accessibility of the system to non-technical subject matter experts and other creative staff, it became clear that additional tools were necessary. In this demonstration, we present Roundtable: a web-based authoring envi-ronment for virtual human characters that is de-signed for use by subject matter experts who are qualified for content authoring in targeted do-mains, but who may not possess technical skills in programming or experience in dialogue sys-tem design.  2 Supporting rapid authoring of dia-logue agents for the web Roundtable is a complete web-based authoring system enabling the end-to-end creation, valida-tion, testing and web publishing of virtual human characters using the SimCoach virtual human architecture. The system provides features that empower many types of authors, team sizes and makeups. The system allows an author to select from a set of preconfigured 3D character models, model the dialogue policy through behavior tem-plates and more direct subdialogue editing, train and test the natural language understanding com-ponent, render animation performances associat-ed with character behaviors and utterances, and test both text-based and fully animated interac-tions. Finally, the complete character dataset can be exported and deployed to a live, highly avail-able server environment, where interaction data can be monitored and periodically collected for analysis and refinement, all from within the same browser environment (Figure 1). The entire sys-tem, from authoring to end-user interaction with 
372
the virtual human character, is web-based and requires only a current web browser for content authors and end users.   (a)
 (b)
  (c)
  (d)
  Figure 1:  Selected modules from the Roundtable character authoring system (a) character project browser; (b) dialogue policy editor; (c) training data manager (d) action and animation asset man-ager 
At the core of the authoring application is an object-oriented information model and set of management systems that span the following roles: ? Dialogue content management, respon-sible for persistence, search, validation and retrieval operations of all dialogue el-ements including subdialogue networks; information state variables and effects; goals and effects; and dialogue action an-notations that provide the mapping to the action database. ? Training data management, concerned with managing training items for a data-driven natural language understanding module, as well as providing support for running regressions when updating the training set.   ? Action management, provides data op-erations for managing potentially large sets of virtual human performance-related assets, including utterance text, speech au-dio when not system-generated, annotated nonverbal behavior schedules, as well as non-performance actions which include web-hosted videos, digested web articles, or any arbitrary HTML effect. ? Deployment management, enabling rapid deployment of locally tested charac-ters to highly available web servers as well as review and data warehousing functions for both analytic and refinement purposes. The information model is implemented in a re-lational database that fully specifies, relates and allows inquiry and validation of authored infor-mation. Additionally, a complete web application programming interface (API) powers the Roundtable application, providing a transactional framework for data operations as well as user privilege enforcement, but which also allows application expansion. The information model also serves to decouple the authoring representation from the data struc-tures necessary to drive dialogue behavior at runtime. Prior to realizing an authored character in the FLoReS engine, project dialogue data ele-ments are exported into the format expected by the runtime target, a process that we expect to expand in the future to support different dialogue managers and language understanding configura-tions.  
373
 Figure 2: The interactive virtual human character published to the web, accessible by current brows-ers.  3 Demo script This demonstration will show how to build a simple conversational virtual human character using Roundtable, from acquiring an account (http://authoring.simcoach.org, free for academic research) to obtaining the URL for the newly created character, and all of the steps in between. The workflow to build a character is as follows: 1. In the project module (Figure 1a) we create a new character by providing a unique name and selecting an existing 3D character mod-el.  2. Opening the newly created project brings up the interaction module (Figure 1b) where we choose from a list of available subdialogue templates that can be used for common dia-logue behaviors (question-answer, greeting, etc.). The provided Greeting and Goodbye templates are used to define the character?s conversational behavior when initiating and ending an interaction, respectively. Invoking the Question-Answer template, we can quickly define how the character will re-spond to a specific question or statement. Each template requires a name and sample text for any user or system utterance.  3. Following the template-based subdialogue generation, we create training data for the natural language understanding component by providing possible user utterances associ-ated with each user dialogue act in the tem-plates used (Figure 1c).  4. The last task is to refine system utterances, which are generated automatically during the step of policy authoring, and generate anima-tion data. From the action module, we can search and inspect all system actions. For any system action, with a single button click, 
we can synthesize audio and render anima-tions (Figure 1d).  5. Finally, we navigate to the test module, compile our character project, and are then able to chat with the new character to ensure expected behavior. At this point, the charac-ter is ready to be deployed, with its unique URL, and is immediately accessible on the web (Figure 2). 4 Conclusion  We described the Roundtable online authoring framework that has been designed to support non-expert users in rapidly creating embodied, conversational virtual characters of varying complexities.  The tool, being web-based, re-quires zero configuration to get started and au-thored virtual characters can be deployed to In-ternet-facing web servers immediately, expand-ing the reach of many dialogue-driven applica-tions.  Acknowledgments The effort described here has been sponsored by the U.S. Army. Any opinions, content or infor-mation presented does not necessarily reflect the position or the policy of the United States Gov-ernment, and no official endorsement should be inferred. References  A. Rizzo, B. Lange, J.G. Buckwalter, E. Forbell, J. Kim, K. Sagae, J. Williams, B.O. Rothbaum, J. Difede, G. Reger, T. Parsons, and P. Kenny. An in-telligent virtual human system for providing healthcare information and support. In J.D. West-wood et al, editor, Technology and Informatics. IOS Press, 2011. Fabrizio Morbini, David Devault, Kenji Sagae, Jillian Gerten, Angela Nazarian and David Traum FLo-ReS: A Forward Looking, Reward Seeking, Dia-logue Manager in proceedings of International Workshop on Spoken Dialog Systems (IWSDS-2012), Ermenonville, France, November 2012b. Fabrizio Morbini, Eric Forbell, David DeVault, Kenji Sagae, David Traum and Albert Rizzo. A Mixed-Initiative Conversational Dialogue System for Healthcare. Demonstration in SIGdial 2012, the 13th Annual SIGdial meeting on Discourse and Dialogue, Seoul, South Korea, 2012a.    
374
Proceedings of the SIGDIAL 2013 Conference, pages 394?403,
Metz, France, 22-24 August 2013. c?2013 Association for Computational Linguistics
Which ASR should I choose for my dialogue system?
Fabrizio Morbini, Kartik Audhkhasi, Kenji Sagae, Ron Artstein,
Dog?an Can, Panayiotis Georgiou, Shri Narayanan, Anton Leuski and David Traum
University of Southern California
Los Angeles, California, USA
{morbini,sagae,artstein,leuski,traum}@ict.usc.edu
{audhkhas,dogancan}@usc.edu {georgiou,shri}@sipi.usc.edu
Abstract
We present an analysis of several pub-
licly available automatic speech recogniz-
ers (ASRs) in terms of their suitability for
use in different types of dialogue systems.
We focus in particular on cloud based
ASRs that recently have become available
to the community. We include features
of ASR systems and desiderata and re-
quirements for different dialogue systems,
taking into account the dialogue genre,
type of user, and other features. We then
present speech recognition results for six
different dialogue systems. The most in-
teresting result is that different ASR sys-
tems perform best on the data sets. We
also show that there is an improvement
over a previous generation of recognizers
on some of these data sets. We also inves-
tigate language understanding (NLU) on
the ASR output, and explore the relation-
ship between ASR and NLU performance.
1 Introduction
Dialogue system developers who are not also
speech recognition experts are in a better posi-
tion than ever before in terms of the ease of in-
tegrating existing speech recognizers in their sys-
tems. While there have been commercial solutions
and toolkits for a number of years, there were a
number of problems in getting these systems to
work. For example, early toolkits relied on spe-
cific machine hardware, software, and firmware
to function properly, often had a difficult instal-
lation process, and moreover often didn?t work
well for complex dialogue domains, or challeng-
ing acoustic environments. Fortunately the situ-
ation has greatly improved in recent years. Now
there are a number of easy to use solutions, in-
cluding open-source systems (like PocketSphinx),
as well as cloud-based approaches.
While this increased choice of quality recogniz-
ers is of great benefit to dialogue system develop-
ers, it also creates a dilemma ? which recognizer
to use? Unfortunately, the answer is not simple ?
it depends on a number of issues, including the
type of dialogue domain, availability and amount
of training data, availability of internet connectiv-
ity for the runtime system, and speed of response
needed. In this paper we assess several freely
available speech recognition engines, and exam-
ine their suitability and performance in several di-
alogue systems. Here we extend the work done in
Yao et al (2010) focusing in particular on cloud
based freely available ASR systems. We include
2 local ASRs for reference, one of which was also
used in the earlier work for easy comparison.
2 Speech Recognizer Features and
Engines
The following are some of the major criteria for
selection of a speech recognizer.
Customization Some of the available speech
recognizers allow the users to tune the recognizer
to the environment it will operate in, by providing
a specialized lexicon, trained language models or
acoustic models. Customization is especially im-
portant for dialogue systems whose input contains
specialized vocabulary (see section 4).
Output options A basic recognizer will output
a string of text, representing its best hypothesis
about the transcription of the speech input. Some
recognizers offer additional outputs which are use-
ful for dialogue systems: ranked n-best hypothe-
ses allow later processing to use context for dis-
ambiguation, and incremental results allow the
system to react while the user is still speaking.
Performance characteristics Dialogue systems
differ in their requirements for response speed; a
394
System Customization Output options Open Source PerformanceN-best Incremental Speed Installation
Pocketsphinx Full Yes Yes Yes realtime Local
Apple No Noa No No network Cloud
Google No Yes Yesb No network Cloud
AT&T Partialc Yes No No network Cloud
Otosense-Kaldi Full Yes No Yesd variablee Local
aSingle output annotated with alternative hypotheses. bOnly for web-delivered applications in a Google Chrome browser.
cCustom language models. dRelease scheduled for Fall 2013. eUser controls trade-off between speed and output quality.
Table 1: Speech recognizer features important for use in dialogue systems
speech recognizer that runs locally can help by
avoiding network latencies.
Output quality Typically, a dialogue system
would want the best recognition accuracy pos-
sible given the constraints. Ultimately, dialogue
systems want the output that would yield the best
performance for Natural Language Understand-
ing and other downstream processes. As a rule,
better speech recognition leads to better language
understanding, though this is not necessarily the
case for specific applications (see section 5).
We evaluated 5 freely available speech recog-
nizers. Their features are summarized in Table 1.
We did not include the MIT WAMI toolkit1 as we
are focused on speech services that can directly
be used by stand alone applications as opposed to
web delivered ones. We did not include commer-
cial recognizers such as Nuance, because licensing
terms can be difficult for research institutions, and
in particular, disallow publishing benchmarks.
Pocketsphinx is a version of the CMU Sphinx
ASR system optimized to run also on embedded
systems (Huggins-Daines et al, 2006). Pocket-
sphinx is fast, runs locally, and requires relatively
modest computational resources. It provides n-
best lists and lattices, and supports incremental
output. It also provides a voice activity detec-
tion functionality for continuous ASR. This ASR
is fully customizable and trainable, but users are
expected to provide language models suitable for
their applications. A few acoustic models are pro-
vided, and can be adapted using the CMUSphinx
tools.2
1http://wami.csail.mit.edu/
2http://cmusphinx.sourceforge.net/wiki/tutorialadapt
Apple Dictation is the OS level feature in both
MacOSX and iOS.3 It is integrated into the text in-
put system pipeline so a user can replace her key-
board with a microphone for entering text in any
application. Dictation is often associated with the
Siri personal assistant feature of iOS. While it is
likely that Dictation and Siri share the same ASR
technology, Dictation only does speech recogni-
tion. Apple states that Dictation learns the charac-
teristics of the user?s voice and adapts to her accent
(Apple Inc, 2012). Dictation requires an internet
connection to send recorded user speech to Ap-
ple?s servers and receive ASR results. Processing
starts as soon as the user starts speaking so the de-
lay of getting the recognition results after the user
finishes speaking is minimal.
To integrate Dictation into a dialogue system,
a system designer needs to include any system de-
fined text input control into her application and use
the control APIs to observe text changes. The user
would need to press a key when starting to speak
and push the key again once she is done speak-
ing. The ASR result is a text string annotated with
alternative interpretations of individual words or
phrases in the text. There is an API for extract-
ing those interpretations from the result. While the
Dictation feature is reasonably fast and easy to in-
tegrate, dialogue system developers have no con-
trol over the ASR process, which must be treated
as a black box. Apple dictation is limited in that
no customization is possible, no partial recogni-
tion results are provided, and there is an unspeci-
fied limit on the number of utterances dictated for
a period of time, which is not a problem for inter-
action between a single user and a dialogue sys-
tem, but may be an issue in dialogue systems that
support multiple concurrent users.
3Dictation was introduced in iOS 5.0 and MacOSX 10.8.
395
Google Speech API provides support for the
HTML 5 speech input feature.4 It is a cloud based
service in which a user submits audio data using
an HTML POST request and receives as reply the
ASR output in the form of an n-best list. The au-
dio data is limited to roughly 10 seconds in length,
longer clips are rejected and return no ASR results.
The user can (1) customize the number of hy-
potheses returned by the ASR, (2) specify which
language the audio file contains and (3) enable a
filter to remove profanities from the output text.
As is the case with Apple Dictation, ASR must be
treated as a black box, and no task customization
is possible for dialogue system developers. Users
cannot specify or provide custom language models
or acoustic models. The service returns only the fi-
nal hypothesis, there is no incremental output.5 In
addition, results for the same inputs may change
unpredictably, since Google may update or other-
wise change its service and models, and models
may be adapted using specific audio data supplied
by users. In our experiments, we observed accu-
racy improvements when submitting the same au-
dio files over repeated trials over two weeks.
AT&T Watson is the ASR engine available
through the AT&T Speech Mashup service.6 It is
a cloud based service that can be accessed through
HTML POST requests, like the Google Speech
API. AT&T Watson is designed to support the de-
mands of online spoken dialogue systems, and can
be customized with data specific to a dialogue sys-
tem. Additionally, in our tests we did not observe
any limitation in the maximum length of the in-
put audio data. However, AT&T does not provide
a default general-purpose language model, and
application-specific models must be built within
the Speech Mashup service using user-provided
text data. The acoustic model must be selected
from a list provided by the AT&T service, and
acoustic models can be further customized within
the Speech Mashup service. The ASR returns an
n-best list of hypotheses but does not provide in-
cremental output.
Otosense-Kaldi Another ASR we employed
was the Kaldi-based OtoSense-Kaldi engine de-
4https://www.google.com/speech-api/v1/recognize
5The demo page shows continuous speech understanding
with incremental results but requires Google Chrome to run
and is specific to web delivered applications:
http://www.google.com/intl/en/chrome/demos/speech.html
6https://service.research.att.com/smm
veloped at SAIL.7 OtoSense-Kaldi8 is an on-line,
multi-threaded architecture based on the Kaldi
toolkit (Povey et al, 2011) that allows for dynam-
ically configurable and distributed ASR.
3 Dialogue Systems, Users, and Data
All spoken dialogue systems are similar in some
respects, in that there is speech by a user (or users)
that needs to be recognized, and this speech is
punctuated by speech from the system. More-
over, the speech is not fully independent, but ut-
terances are connected to other utterances, e.g. an-
swers to questions, or clarifications. There are,
however many ways in which systems can differ,
that have implications for which speech recogniz-
ers are most appropriate. Some of the dimensions
to consider are:
Type of microphone(s) One of the biggest im-
pacts on ASR is the acoustic environment. Will
the audio be clean, coming from a close-talking
head or lapel-mounted microphone, or will it need
to be picked up from a broader directional micro-
phone or microphone array?
Number of speakers/microphones Will there
be one designated microphone per person, or will
speaker identification need to be performed? Will
audio from the system confuse the ASR?
Push to talk or continuous speech Will the
user clearly identify the start and end of speech,
or will the system need to detect speech acousti-
cally?
Type of Users Will there be designated long-
term users, where user-training or system model
adaptation is feasible, or will there be many un-
known users, where training is not feasible? See
also section 3.1 for more on user types.
Genre What kinds of things will people be say-
ing to the system? Is it mostly commands or short
answers to questions, or more open-ended conver-
sation? See section 3.2 for more on genre issues.
Training Data Is within-domain training data
available, and if so how much?
3.1 Types of Users
The type of user is important for the overall
design of the system and has implications for
7http://sail.usc.edu
8OtoSense-Kaldi will be released (BSD license) in 2013.
396
ASR performance as well. One important as-
pect is the broad physical differences among
speakers, such as male vs female, adult vs child
(e.g. Bell and Gustafson, 2003), or language pro-
ficiency/accent, that will have implications for the
acoustics of what is said, and ASR results. Other
aspects of users have implications for what will
be said, and how successful the interface may
be, overall. Many (e.g. Hassel and Hagen, 2006;
Jokinen and Kanto, 2004) have looked at the dif-
ferences between novice and expert users. Ai et
al. (2007a) also points out a difference between
real users and recruited subjects. Real users also
come in many different flavors, depending on their
purposes. E.g. are they interacting with the system
for fun, to do a specific task that they need to get
done, to learn something (specific or general), or
with some other purpose in mind?
We considered the following classes of users,
ordered from easiest to hardest to get to acceptable
performance and robustness levels:
Demonstrators are generally the easiest for a sys-
tem to understand ? a demonstrator is trained in
use of the system, knows what can and can?t be
said, is motivated toward success, and is gener-
ally interested in showing off the most impres-
sive/successful aspects of the system to an audi-
ence rather than using it for its own sake.
Trained/Expert Users are similar to demonstra-
tors, but use the system to achieve specific results
rather than just to show off its capabilities. This
means that users may be forced down lines that
are not ideal for the system, if these are necessary
to accomplish the task.
Motivated Users do not have the training of ex-
pert users, and may say many things that the sys-
tem can not handle as opposed to equivalent ex-
pressions that could be handled. However moti-
vated users do want the system to succeed, and in
general are willing to do whatever they think is
necessary to improve system performance. Unlike
expert users, motivated users might be incorrect
about what will help the system (e.g. hyperarticu-
lation in response to system misunderstanding).
Casual Users are interested in finding out what
the system can do, but do not have particular moti-
vations to help or hinder the system. Casual Users
may also leave in the middle of an interaction, if it
is not engaging enough.
Red Teams are out to test or ?break? the system,
or show it as not-competent, and may try to do
things the system can?t understand or react well
to, even when an alternative formulation is known
to work.
3.2 Types of Dialogue System Genres
Dialogue Genres can be distinguished along many
lines, e.g. the number and relationship of partic-
ipants, specific conversational rules, purposes of
the participants, etc. We distinguish here four gen-
res of dialogue system that have been in use at
the Institute for Creative Technologies and that we
have available corpora for (there are many other
types of dialogue genres, including tutoring, ca-
sual conversation, interviewing,. . . ). Each genre
has implications for the internal representations
and system architectures needed to engage in that
genre of dialogue.
Simple Question-answering This genre in-
volves strong user-initiative and weak global di-
alogue coherence. The user can ask any ques-
tion to the system at any time, and the system
should respond, with an appropriate answer if
able, or with some other reply indicating either
inability or unwillingness to provide the answer.
This genre allows modeling dialogue at a surface-
text level (Gandhe, 2013), without internal se-
mantic representations of the input, and where
the result of ?understanding? input is the system?s
expected output. The NCPEditor9 (Leuski and
Traum, 2011) is a toolkit that provides an author-
ing environment, classification, and dialogue ca-
pability for simple question-answering characters.
The SGT Blackwell, SGT Star, and Twins systems
described below are all systems in this genre.
Advanced Question-answering This genre is
similar to the simple question-answering charac-
ters, in that the main task of the user is to elicit
information from the system character. The differ-
ence is that there is more long-range and interme-
diate dialogue coherence, in that questions can be
answered several utterances after they have been
asked, there can be intervening sub-dialogues, and
characters sometimes take the initiative to pursue
their own goals rather than just responding to the
user. Because of the requirements for somewhat
deeper understanding, and relation of input to con-
9Available free for academic research purposes from
https://confluence.ict.usc.edu/display/VHTK/Home
397
text and character goals and policies, there is a
need of at least a shallow semantic representa-
tion and representation of the dialogue informa-
tion state, and the character must distinguish un-
derstanding of the input from the character out-
put (since the latter will depend on the dialogue
policy and information state, not just the under-
standing of input). The tactical questioning archi-
tecture (Gandhe et al, 2009)10 provides author-
ing and run-time support for advanced question-
answering characters, and has been used to build
over a dozen characters for purposes such as train-
ing tactical questioning, training culture, and psy-
chology experiments (Gandhe et al, 2011). The
Amani character described below is in this genre.
Slot-filling Probably the most common type of
dialogue system (at least in the research commu-
nity) is slot-filling. Here the dialogue is fairly
structured, with an initial greeting phase, then one
or more tasks, which all start with the user se-
lecting the task, and the system taking over ini-
tiative to ?fill? and possibly confirm the needed
slots, before retrieving some information from a
database, or performing a simple service.11 This
genre also requires a semantic representation, at
least of the slots and acceptable values. Gener-
ally, the set of possible values is large enough, that
some form of NLG is needed (at least template
filling), rather than authoring of all full sentences.
There are a number of toolkits and development
frameworks that are well suited to slot-filling sys-
tems, e.g. Ravenclaw (Bohus and Rudnicky, 2003)
or Trindikit (Larsson and Traum, 2000). The Ra-
diobots system, described below is in this genre.
Negotiation and Planning In this genre, the
system is more of an equal partner with the user,
than a servant, as in the slot-filling systems. The
system must not merely understand user requests,
but must also evaluate whether they meet the sys-
tem goals, what the consequences and precondi-
tions of requests are, and whether there are better
alternatives. For this kind of inference, a more de-
tailed semantic representation is required than just
filling in slots. While we are not aware of publicly
available software that makes this kind of system
easy to construct, there have been several built us-
ing an information-state approach, or the soar cog-
10Soon to be released as part of the virtual human toolkit.
11Mixed-initiative versions of this genre exist, where the
user can also provide unsolicited information, which reduces
the number of system queries needed.
nitive architecture. The TRIPS system (Allen et
al., 2001) also has many similarities.
3.3 ICT Dialogue Systems Tested
We tested the recognizers described in section 2
on data sets collected from six different dialogue
domains. Five are the same ones tested in Yao et
al. (2010), to which we added the Twins set. De-
tails on the size of the training and development
sets may be found in Yao et al (2010), here we
report only the numbers relevant to the Twins do-
main and to the NLU analysis, which are not in
Yao et al (2010).
SGT Blackwell was created as a virtual human
technology demonstration for the 2004 Army Sci-
ence Conference. This is a question-answering
character, with no internal semantic representation
and the primary NLU task merged with Dialogue
management as selecting the best response.
The original users were ICT demonstrators.
However, there were also some experiments with
recruited participants (Leuski et al, 2006a; Leuski
et al, 2006b). Later SGT Blackwell became a part
of the ?best design in America? triennial at the
Cooper-Hewitt Museum in New York City, and
the data set here is from visitors to the museum,
who are mostly casual users, but range from expert
to red-team. Users spoke into a mounted direc-
tional microphone (see Robinson et al, 2008 for
more details).
SGT STAR (Artstein et al, 2009a) is a question-
answering character similar to SGT Blackwell, al-
though designed to talk about Army careers rather
than general knowledge. The users are Army per-
sonnel who went to job fairs and visited schools in
the mobile Army adventure vans, speaking using
headset microphones, and performing for an audi-
ence. The users are somewhere between demon-
strators and expert users. They are speaking to
SGT STAR for the benefit of an audience, but their
primary purpose is to convey information to the
audience in a memorable way (through dialogue
with SGT STAR) rather than to show off the high-
lights of the character.
The Twins are two life-size virtual characters
who serve as guides at the Museum of Science
in Boston (Swartout et al, 2010). The charac-
ters promote interest in Science, Technology, En-
gineering and Mathematics (STEM) in children
between the ages of 7 and 14. They are question-
398
answering characters, but unlike SGTs Blackwell
and Star, the response is a whole dialogue se-
quence, potentially involving interchange from
both characters, rather than a single character turn.
There are two types of users for the Twins:
demonstrators, who are museum staff members,
using head-mounted microphones, and museum
visitors, who use a Shure 522 table-top mounted
microphone (Traum et al, 2012). More on analy-
sis of the museum data can be found in (Aggarwal
et al, 2012). We also investigated speech recog-
nition and NLU performance in this domain in
Morbini et al (2012).
This dataset contains 14K audio files each an-
notated with one of the 168 possible response se-
quences. The division in training development and
test is the same used in Morbini et al (2012) (10K
for training, the rest equally divided between de-
velopment and test).
Amani (Artstein et al, 2009b; Artstein et al,
2011) is an advanced question-answering char-
acter used as a prototype for systems meant to
train soldiers to perform tactical questioning. The
users are in between real users and test subjects:
they were cadets at the U.S. Military Academy in
April 2009, who interacted with Amani as a uni-
versity course exercise on negotiation techniques.
They used head-mounted microphones to talk with
Amani.
This dataset comprises of 1.8K audio files each
annotated with one of the 105 possible NLU se-
mantic classes.
Radiobots (Roque et al, 2006) is a training pro-
totype that responds to military calls for artillery
fire in a virtual reality urban combat environment.
This is a domain in the slot-filling genre, where
there is a preferred protocol for the order in which
information is provided and confirmed. Users are
generally trainees, learning how to do calls for fire,
they are motivated users with some training. The
semantic processing involved tagging each word
with the dialogue act and parameter that it was as-
sociated with (Ai et al, 2007b).
This data set was collected during the develop-
ment of the system in 2006 at Fort Sill, Oklahoma,
during two evaluation sessions from recruited vol-
unteer trainees who performed calls for specific
missions (Robinson et al, 2006). These subjects
used head-mounted microphones rather than the
ASTI simulated radios from later data collection.
SASO-EN (Traum et al, 2008) is a negotiation
training prototype in which two virtual characters
negotiate with a human ?trainee? about moving a
medical clinic. The genre is negotiation and plan-
ning, where the human participant must try to form
a coalition, and the characters reason about utili-
ties of different proposals, as well as causes and
effects. The output of NLU is a frame represen-
tation including both semantic elements, like the-
matic argument structure, and pragmatic elements,
such as addressee and referring expressions. Fur-
ther contextual interpretation is performed by each
of the virtual characters to match the (possibly par-
tial) representation to actions and states in their
task model, resolve other referring expressions,
and determine a full set of dialogue acts (Traum,
2003). Speech was collected at the USC Insti-
tute for Creative Technologies (ICT) during 2006?
2009, mostly from visitors and new hires, who
acted as test subjects.
This dataset has 4K audio files each anno-
tated with one of the 117 different NLU semantic
classes.
4 ASR Performance
We tested each of the Datasets described in Sec-
tion 3.3 with some of the recognizers described
in Section 2. All recognizers were tested on the
Amani, SASO-EN, and Twins domains, and we
also tested a natural language understanding com-
ponent on these data sets (Section 5). For SGT
Blackwell, SGT STAR, and Radiobots, we report
the performance on the same development set used
in Yao et al (2010). For Amani and SASO-EN
(where we also report the NLU performance), we
run a 10-fold cross-validation in which 9 folds
where used to train the NLU and ASR language
model and the 10th was used for testing. For the
Twins dialogue system, we used the same partition
into training, development and testing reported in
Morbini et al (2012) and the results reported here
are from the development set. Due to differences
in training/testing regimens, performance of sys-
tems are only comparable within each domain.
Table 2 summarizes the performance of the var-
ious ASR engines on the evaluation data sets. Per-
formance is measured as Word Error Rate and was
obtained using the NIST SCLITE tool.12
Note that only Otosense-Kaldi in the Twins do-
main had adapted acoustic models. In the remain-
12http://www.itl.nist.gov/iad/mig/tools/
399
Speech recognizer Evaluation data setAmani Radiobots SASO-EN SGT Blackwell SGT Star Twins
Pocketsphinx 39.7 11.8 28.4 51 28.6 81
Apple 28 ? 30.9 ? ? 29
AT&T 29 12.1 16.3 27.3 21.7 28.8
Google 23.8 36.3 20 18 26 20.6
Otosense-Kaldi 33.7 ? 22.1 ? ? 18.7
Table 2: Word Error Rates (%) for the various dialogue systems and ASR systems tested.
ing cases only the language model was adapted.
Looking at the results on the development set re-
ported in Yao et al (2010), we have improvements
in 3 out of 5 domains: Amani (?11.8% Google),
SASO-EN (?11.7% AT&T) and SGT Blackwell
(?13% Google). In Radiobots and SGT Star the
performance achieved with just language model
adaptation, when permitted, is worse: +4.8% and
+1.7% respectively.
We find that there is no single best performing
speech recognizer: results vary greatly between
the evaluation test sets. In 4 of the 6 datasets over-
all, and 2 of the 3 datatests tested with Otosense-
Kaldi, the best performer is a cloud-based ser-
vice (Google or AT&T). There are two datasets
for which a local, fully customizable recognizer
performs better than the cloud-based services. Ra-
diobots, consisting of military calls for artillery
fire, has a fairly limited and very specialized vo-
cabulary, and indeed the two recognizers with cus-
tom language models (Pocketsphinx and AT&T)
perform much better than the non-customizable
recognizer (Google).
The Twins dataset is unique in that for the
Otosense-Kaldi system we custom-trained acous-
tic and language models, while standard WSJ
acoustic models and adapted language models
were used for the other dialogue systems. In
both cases the models were triphone based with
a Linear Discriminant Analysis (LDA) front end,
and Maximum Likelihood Linear Transforma-
tion (MLLT) and Maximum Mutual Information
(MMI) training. This reflects on the very good
performance in the Twins domain, decent perfor-
mance on the SASO-EN domain (reasonable mis-
match of WSJ and SASO-EN) and very degraded
performance in Amani (highly mismatched Amani
and WSJ domains). The observed degradation in
performance is accentuated by the MMI discrim-
inative training on the mismatched-WSJ data. As
with PocketSphinx and Watson, and unlike with
Apple Dictation and Google Speech API, with
Kaldi we fully control experimental conditions
and can guarantee no contamination of the train-
test data.
In summary, our evaluation shows that cus-
tomizable recognizers are useful when the ex-
pected speech is highly specialized, or when sub-
stantial resources are available for tuning the rec-
ognizer.
5 NLU Accuracy & Relation between
ASR and NLU
While the different genres of system have different
types of output for NLU: response text, dialogue
act and parameter tags, speech acts, or semantic
frames, many of them can be coerced into a se-
lection task, in which the NLU selects the right
output from a set of possible outputs. This allows
any multiclass classification algorithm to be used
for NLU. A possible drawback is that for some
inputs, the right output might not be available in
the set considered by the training data, even if it
might easily be constructed from known parts us-
ing a generative approach.
A second issue is that even though we can cast
the problem as multi-class classification, classifi-
cation accuracy is not always the most appropriate
metric of NLU quality. For question-answering
characters, getting an appropriate and relevant re-
ply is more important than picking the exact re-
ply selected by a human domain designer or an-
notator: there might be multiple good answers, or
even the best available answer might not be very
good. For that reason, the question-answering
characters allow an ?off-topic? answer and Error-
return plots (Artstein, 2011) might be necessary
to choose an optimal threshold. For the SASO-EN
system, slot-filler metrics such as precision, recall,
and f-score are more appropriate than frame accu-
400
racy, because some frames may have many slots
in common and few that are different (e.g. just a
different addressee). Nonetheless, we begin our
analysis within this common framework. For sim-
plicity, we start with just three domains: Twins,
Amani, and SASO-EN. SGT STAR and Blackwell
are very similar to Twins in terms of NLU. Ra-
diobots is more challenging to coerce to multiclass
classification.
Conventional wisdom in the speech and lan-
guage processing community is that performance
of ASR and NLU are closely tied: improved
speech recognition leads to better language under-
standing, while deficiencies in speech recognition
cause difficulty in understanding. This conven-
tional wisdom is borne out by decades of experi-
ence with speech and dialogue systems, though we
are not aware of attempts to systematically demon-
strate it. The present study shows that the expected
relation between speech recognition and language
understanding holds for the systems we tested.
Accepted assumptions about the relation be-
tween speech recognition and language under-
standing have been repeatedly challenged. Direct
challenges are typically limited to specific appli-
cations. Wang et al (2003) show that for a slot-
filling NLU, ASR can be specifically tuned to rec-
ognize those words that are relevant to the slot-
filling task, resulting in improved understanding
despite a decrease in performance on overall word
recognition. However, Boros et al (1996) found
that when not optimizing the ASR for the specific
slot filling task there is a nearly linear correlation
between word accuracy and NLU accuracy. Al-
shawi (2003) and Huang and Cox (2006) show that
in call-routing applications the word level can be
dispensed with altogether and calls routed based
on phonetic information alone without noticeable
loss in performance. These challenges suggest that
the speech-language divide is not as clean as the
theory suggests.
To investigate the relation between ASR and
NLU, we ran each ASR output from each of
the 5 recognizers through an understanding com-
ponent to obtain an NLU output (each dataset
had a separate NLU component, which was held
constant for all speech recognizers). ASR and
NLU performance are conventionally measured on
scales of opposite polarity: better performance
shows up as lower word error rates but higher
NLU accuracies. For the correlations we invert the
conventional ASR scale and use word accuracy, so
that higher numbers signify better performance on
both scales.13
Figure 1 shows the results obtained in the 3 di-
alogue systems by the various ASR systems. The
figures plot ASR performance against NLU per-
formance; NLU results on manual transcriptions
are included for comparison. There are too few
data points for the correlations between ASR and
NLU performance to be significant, but the trends
are positive, as expected.
Our experiments lend supporting evidence to
the claim that in general, ASR performance is pos-
itively linked to NLU performance (special cases
notwithstanding). The 3 datasets exhibit posi-
tive correlations between speech recognition and
language understanding performance. Thus, we
claim that the basis of the conventional wisdom
is sound: speech recognition directly affects lan-
guage understanding. This conclusion holds when
the speech recognizer has been optimized to pro-
duce the most accurate transcript, rather than for a
specific NLU.
6 Conclusion and Future Work
We have extended here the ASR system evaluation
published in Yao et al (2010) including some new
cloud based ASR services that achieve very good
performance showing an improvement of around
12%. We also showed that ASR and NLU perfor-
mance are correlated.
One possible avenue of future work is to ex-
tract importance weights for each word from the
learnt NLU models and use these weights to try
to explain those cases that diverge from the corre-
lation between ASR and NLU performance. This
may also give us a better measure than WER for
assessing ASR performance in dialogue systems.
Another avenue of future work involves examin-
ing different types of NLU engines, and different
metrics for the different dialogue system genres,
which, again, may lead to a more relevant assess-
ment of ASR performance.
Acknowledgments
The effort described here has been sponsored by
the U.S. Army. Any opinions, content or informa-
tion presented does not necessarily reflect the posi-
13We define ?accuracy? as 1 minus WER, so this number
can in principle dip below zero if there are more errors than
words.
401
Amani
r = 0.54, df = 3, p = 0.345
Word accuracy (%)
N
LU
ac
cu
ra
cy
(%
)
50 60 70 80 90 100
51
54
57
60
63
66
69
?
?
?
? ?
?
SASO-EN
r = 0.77, df = 3, p = 0.130
Word accuracy (%)
N
LU
ac
cu
ra
cy
(%
)
60 70 80 90 100
30
40
50
60
70
80
90
?
???
?
?
Twins
r = 0.99, df = 3, p = 0.002
Word accuracy (%)
N
LU
ac
cu
ra
cy
(%
)
0 20 40 60 80 100
40
50
60
70
80
90
100
?
?
?
?
?
Figure 1: Relation between ASR and NLU performance (red dots are manual transcriptions)
tion or the policy of the United States Government,
and no official endorsement should be inferred.
References
Priti Aggarwal, Ron Artstein, Jillian Gerten, Athana-
sios Katsamanis, Shrikanth Narayanan, Angela
Nazarian, and David Traum. 2012. The Twins cor-
pus of museum visitor questions. In LREC-2012,
Istanbul, Turkey, May.
Hua Ai, Antoine Raux, Dan Bohus, Maxine Eskenazi,
and Diane Litman. 2007a. Comparing spoken dia-
log corpora collected with recruited subjects versus
real users. In SIGdial 2007.
Hua Ai, Antonio Roque, Anton Leuski, and David
Traum. 2007b. Using information state to improve
dialogue move identification in a spoken dialogue
system. In Proceedings of the 10th Interspeech Con-
ference, Antwerp, Belgium, August.
James F. Allen, George Ferguson, and Amanda Stent.
2001. An architecture for more realistic conversa-
tional systems. In IUI, pages 1?8.
Hiyan Alshawi. 2003. Effective utterance classifica-
tion with unsupervised phonotactic models. In HLT-
NAACL 2003, pages 1?7, Edmonton, Alberta, May.
Apple Inc. 2012. Mac basics: Dictation (Technote
HT5449), November.
R. Artstein, S. Gandhe, J. Gerten, A. Leuski, and
D. Traum. 2009a. Semi-formal evaluation of con-
versational characters. In O. Grumberg, M. Kamin-
ski, S. Katz, and S. Wintner, editors, Languages:
From Formal to Natural. Essays Dedicated to Nis-
sim Francez on the Occasion of His 65th Birthday,
volume 5533 of Lecture Notes in Computer Science,
pages 22?35. Springer, Berlin.
Ron Artstein, Sudeep Gandhe, Michael Rushforth, and
David R. Traum. 2009b. Viability of a simple dia-
logue act scheme for a tactical questioning dialogue
system. In DiaHolmia 2009: Proceedings of the
13th Workshop on the Semantics and Pragmatics of
Dialogue, page 43?50, Stockholm, Sweden, June.
Ron Artstein, Michael Rushforth, Sudeep Gandhe,
David Traum, and MAJ Aram Donigian. 2011.
Limits of simple dialogue acts for tactical question-
ing dialogues. In 7th IJCAI Workshop on Knowl-
edge and Reasoning in Practical Dialogue Systems,
Barcelona, Spain, July.
Ron Artstein. 2011. Error return plots. In 12th SIG-
dial Workshop on Discourse and Dialogue, Port-
land, OR, June.
Linda Bell and Joakim Gustafson. 2003. Child and
adult speaker adaptation during error resolution in a
publicly available spoken dialogue system. In IN-
TERSPEECH 2003.
Dan Bohus and Alexander I. Rudnicky. 2003. Raven-
claw: dialog management using hierarchical task de-
composition and an expectation agenda. In INTER-
SPEECH 2003.
M. Boros, W. Eckert, F. Gallwitz, G. Grz, G. Han-
rieder, and H. Niemann. 1996. Towards understand-
ing spontaneous speech: Word accuracy vs. concept
accuracy. In In Proceedings of (ICSLP 96), pages
1009?1012.
Sudeep Gandhe, Nicolle Whitman, David R. Traum,
and Ron Artstein. 2009. An integrated authoring
tool for tactical questioning dialogue systems. In 6th
Workshop on Knowledge and Reasoning in Practical
Dialogue Systems, Pasadena, California, July.
Sudeep Gandhe, Michael Rushforth, Priti Aggarwal,
and David R. Traum. 2011. Evaluation of
an integrated authoring tool for building advanced
question-answering characters. In Proceedings of
Interspeech-11, Florence, Italy, 08/2011.
Sudeep Gandhe. 2013. Rapid prototyping and evalu-
ation of dialogue systems for virtual humans. Ph.D.
thesis, University of Southern California.
402
Liza Hassel and Eli Hagen. 2006. Adaptation of an
automotive dialogue system to users? expertise and
evaluation of the system. Language Resources and
Evaluation, 40(1):67?85.
Quiang Huang and Stephen Cox. 2006. Task-
independent call-routing. Speech Communication,
48(3?4):374?389.
D. Huggins-Daines, M. Kumar, A. Chan, A.W. Black,
M. Ravishankar, and A.I. Rudnicky. 2006. Pocket-
sphinx: A free, real-time continuous speech recog-
nition system for hand-held devices. In Acoustics,
Speech and Signal Processing, 2006. ICASSP 2006
Proceedings. 2006 IEEE International Conference
on, volume 1, pages I?I.
Kristiina Jokinen and Kari Kanto. 2004. User ex-
pertise modeling and adaptivity in a speech-based
e-mail system. In Donia Scott, Walter Daelemans,
and Marilyn A. Walker, editors, ACL, pages 87?94.
ACL.
Staffan Larsson and David Traum. 2000. Information
state and dialogue management in the TRINDI dia-
logue move engine toolkit. Natural Language En-
gineering, 6:323?340, September. Special Issue on
Spoken Language Dialogue System Engineering.
Anton Leuski and David R. Traum. 2011. NPCEditor:
Creating virtual human dialogue using information
retrieval techniques. AI Magazine, 32:42?56.
Anton Leuski, Brandon Kennedy, Ronakkumar Patel,
and David Traum. 2006a. Asking questions to
limited domain virtual characters: How good does
speech recognition have to be? In 25th Army Sci-
ence Conference.
Anton Leuski, Ronakkumar Patel, David Traum, and
Brandon Kennedy. 2006b. Building effective ques-
tion answering characters. In Proceedings of the
7th SIGdial Workshop on Discourse and Dialogue,
pages 18?27.
Fabrizio Morbini, Kartik Audhkhasi, Ron Artstein,
Maarten Van Segbroeck, Kenji Sagae, Panayio-
tis S. Georgiou, David R. Traum, and Shrikanth S.
Narayanan. 2012. A reranking approach for recog-
nition and classification of speech input in conversa-
tional dialogue systems. In SLT, pages 49?54. IEEE.
Daniel Povey, Arnab Ghoshal, Gilles Boulianne, Luka?s?
Burget, Ondr?ej Glembek, Nagendra Goel, Mirko
Hannemann, Petr Motl??c?ek, Yanmin Qian, Petr
Schwarz, Jan Silovsky?, Georg Stemmer, and Karel
Vesely?. 2011. The Kaldi speech recognition
toolkit. In IEEE 2011 Workshop on Automatic
Speech Recognition and Understanding, December.
S.M. Robinson, A. Roque, A. Vaswani, D. Traum,
C. Hernandez, and B. Millspaugh. 2006. Evalua-
tion of a spoken dialogue system for virtual reality
call for fire training. In 25th Army Science Confer-
ence, Orlando, Florida, USA.
S. Robinson, D. Traum, M. Ittycheriah, and J. Hen-
derer. 2008. What would you ask a conversational
agent? Observations of human-agent dialogues in
a museum setting. In Proc. of Sixth International
Conference on Language Resources and Evaluation
(LREC), Marrakech, Morocco.
A. Roque, A. Leuski, V. Rangarajan, S. Robinson,
A. Vaswani, S. Narayanan, and D. Traum. 2006.
Radiobot-CFF: A spoken dialogue system for mil-
itary training. In Proc. of Interspeech, Pittsburgh,
Pennsylvania, USA.
W. Swartout, D. Traum, R. Artstein, D. Noren, P. De-
bevec, K. Bronnenkant, J. Williams, A. Leuski,
S. Narayanan, D. Piepol, C. Lane, J. Morie, P. Ag-
garwal, M. Liewer, J. Chiang, J. Gerten, S. Chu,
and K. White. 2010. Ada and Grace: Toward
realistic and engaging virtual museum guides. In
J. Allbeck, N. Badler, T. Bickmore, C. Pelachaud,
and A. Safonova, editors, Intelligent Virtual Agents:
10th International Conference, IVA 2010, Philadel-
phia, PA, USA, September 20?22, 2010 Proceed-
ings, volume 6356 of Lecture Notes in Artificial In-
telligence, pages 286?300. Springer, Heidelberg.
David R. Traum, Stacy Marsella, Jonathan Gratch, Jina
Lee, and Arno Hartholt. 2008. Multi-party, multi-
issue, multi-strategy negotiation for multi-modal
virtual agents. In IVA, pages 117?130.
David Traum, Priti Aggarwal, Ron Artstein, Susan
Foutz, Jillian Gerten, Athanasios Katsamanis, Anton
Leuski, Dan Noren, and William Swartout. 2012.
Ada and grace: Direct interaction with museum
visitors. In The 12th International Conference on
Intelligent Virtual Agents (IVA), Santa Cruz, CA,
September.
David Traum. 2003. Semantics and pragmatics of
questions and answers for dialogue agents. In pro-
ceedings of the International Workshop on Compu-
tational Semantics, pages 380?394.
Ye-Yi Wang, A. Acero, and C. Chelba. 2003. Is
word error rate a good indicator for spoken lan-
guage understanding accuracy. In IEEE Workshop
on Automatic Speech Recognition and Understand-
ing (ASRU ?03), pages 577?582.
Xuchen Yao, Pravin Bhutada, Kallirroi Georgila, Kenji
Sagae, Ron Artstein, and David R. Traum. 2010.
Practical evaluation of speech recognizers for vir-
tual human dialogue systems. In Nicoletta Calzo-
lari, Khalid Choukri, Bente Maegaard, Joseph Mar-
iani, Jan Odijk, Stelios Piperidis, Mike Rosner, and
Daniel Tapias, editors, LREC. European Language
Resources Association.
403
Proceedings of the SIGDIAL 2014 Conference, pages 186?193,
Philadelphia, U.S.A., 18-20 June 2014.
c?2014 Association for Computational Linguistics
Initiative Taking in Negotiation
Elnaz Nouri
University of Southern California
Los Angeles, CA, USA
nouri@ict.usc.edu
David Traum
USC Institute for Creative Technologies
12015 Waterfront Dr
Playa Vista, CA 90094, USA
traum@ict.usc.edu
Abstract
We examine the relationship between ini-
tiative behavior in negotiation dialogues
and the goals and outcomes of the ne-
gotiation. We propose a novel annota-
tion scheme for dialogue initiative, includ-
ing four labels for initiative and response
behavior in a dialogue turn. We anno-
tate an existing human-human negotiation
dataset, and use initiative-based features
to try to predict both negotiation goal and
outcome, comparing our results to prior
work using other (non-initiative) features
sets. Results show that combining initia-
tive features with other features leads to
improvements over either set and a major-
ity class baseline.
1 Introduction
Negotiation is a complex interaction in which two
or more parties confer with one another to arrive
at the settlement of some matter, for example re-
solving a conflict or to share common resources.
The parties involved in the negotiation often have
non-identical preferences and goals that they try to
reach. Sometimes the parties simply try to change
a situation to their favor by haggling over price. In
other cases, there can be a more complex trade-off
between issues. Investigating these rich and com-
plex interactions in a scientific manner has been
important to researchers in different fields due to
the significant implications and potential applica-
tions for business and profit making. Being a good
negotiator is not a skill that all humans naturally
have; therefore, this line of research can poten-
tially be used to help humans become better ne-
gotiators. Computer agents will also benefit from
the ability to understand human negotiators. There
has been a fair amount of previous work in un-
derstanding negotiation dialogs, e.g., (Walton and
McKersie, 1991; Baker, 1994); as well as agents
who can engage in negotiation, e.g. (Jameson et
al., 1994; Sidner, 1994; Kraus et al., 2008; Traum
et al., 2008). In this paper we investigate the role
that dialogue initiative plays in negotiation.
Negotiations can be characterized by both the
goals that each negotiator is trying to achieve, as
well as the outcomes. Even for negotiations that
attempt to partition a set of goods, the participants
may have differences in their valuation of items,
and the negotiations can be very different if peo-
ple are trying to maximize the total gain or their
individual gain, or gain a competitive advantage
over the other.
Negotiations between two people are usually
mixed-initiative (Walker and Whittaker, 1990),
with control of conversation being transferred
from one person to another. To our knowledge,
no previous studies have investigated the relation-
ship between verbal initiative taking patterns and
the goal or the outcome of the negotiation. We
suspected that both of the mentioned characteris-
tics of the negotiation (goal and outcome) might be
correlated with different initiative-taking patterns.
We used an existing negotiation dataset in order
to study the mixed initiative patterns between the
two parties in the negotiation. We describe this
data set in Section 2, as well as previous work that
attempted to predict outcome and goal, using other
features (Nouri et al., 2013).
This paper makes the following contributions:
a new annotation scheme for dialogue initiative is
introduced in Section 3 and used to annotate the
negotiation dataset. We then study the relation-
ship between initiative taking patterns and the goal
and outcome of the negotiation for the participants
(Section 4).
2 Data
We make use of a previously collected and ana-
lyzed dataset in order to examine the relative con-
186
tribution of initiative to problems of goal and out-
come detection. We briefly describe the dataset
and relevant prior work on this dataset.
The Farmers Market dataset (Carnevale, 2013)
contains audio, video and transcription of 41
dyadic negotiation sessions. Participants were un-
dergraduate students majoring in business. Each
participant only took part in one negotiation ses-
sion.
Before each negotiation session, the experi-
menter told participants that they were randomly
assigned to represent one of two restaurants in the
task. The owners of the two restaurants had asked
the participants to go to the market and get some
apples, bananas, lemons, peppers and strawber-
ries. The payoff matrix for each restaurant and
type of item is shown in Table 1. There were mul-
tiple items of each type available. Each participant
was only given the pay-off matrix of his assigned
restaurant and the total score of the negotiation for
each participant was calculated by adding up the
points for each item they received in the negotia-
tion. The participants were told that they had 10
minutes to negotiate how to distribute the items on
the table and reach an agreement. As an incentive,
each participant could receive up to 50 dollars de-
pending on the final points earned by each partici-
pant for his/her restaurant.
R1 R2
Apples 1 3
Bananas 3 3
Lemons 0 0
Peppers 3 1
Strawberries 1 1
Table 1: The Payoff Matrix for each Restaurant
2.1 Goals
The study was originally designed to investigate
negotiators? behavior when they have different
goals in the negotiation. There were three types
of instructions given to the participants. All the
details were the same except for their goal in the
negotiation.
? In ?individualistic? instructions participants
were told that their goal was to get at as many
points as they could for themselves. An ex-
cerpt from an individualistic negotiation is
shown in Table 13 in the Appendix.
? in ?cooperative? instructions they were told
that they should try to maximize the joint gain
with the other side of the negotiation. An ex-
cerpt from a cooperative negotiation is shown
in Table 11 in the Appendix.
? in ?competitive? instructions they were told
to try to get more points than the other party.
An excerpt from a competitive negotiation is
shown in Table 12 in the Appendix.
Out of the 41 interactions in the dataset 15 were
competitive, 13 were individualistic and 13 were
cooperative sessions.
2.2 Outcomes
The outcome of the negotiation in this case is mea-
sured based on the calculation of the scores corre-
sponding to the items that each negotiator has re-
ceived by the end of the negotiation. In order to
make the prediction of outcome possible based on
our small dataset, we labeled the calculated score
for each participant with one of the three labels:
H,E or L, showing whether the participant had re-
ceived more, equal or fewer points than the other
person.
The goal of the ?competitive? instructions was
to get a higher score. For cooperative negotiations,
the relative score did not matter. For the individu-
alistic goal, higher score is somewhat correlated
with the goal, but not absolutely (what matters
is only an individual high score, not the relation
to the other partner). 17 negotiations resulted in
equal final scores for the two parties and 24 with
one side scoring more than the other side. Ta-
ble 2 shows the average scores for each restaurant,
across the three types of goals. The scores are on
average higher in the cooperative negotiations than
in the other two conditions.
Average score R1 R2 Joint
Gain
Cooperative 24.9 25.1 50
Competitive 23.7 23.6 47.3
Individualistic 25.5 22.5 48
Table 2: Average Score by Restaurant and Goal
The average score for individuals who score
higher (labeled as H) than the other side of the ne-
gotiation was 26.46 whereas the average score for
their counterparts (labeled as L) was 21.65. The
187
average score for individuals who ended up in a
tie (labeled as E) was 24.16.
2.3 Previous Work and Baseline System
This data set was previously used for various pur-
poses but (Nouri et al., 2013) was most similar
to our current work in that it also tried to pre-
dict the goal and outcome in the negotiation, us-
ing a different set of features, and a slightly dif-
ferent formulation of the problem. (Nouri et al.,
2013) used multimodal features (such as acoustic
features and sentiments of the turns) for this pur-
pose. We use initiative-features to build our pre-
diction models. In order to make a baseline classi-
fier, we used the following automatically derivable
features from (Nouri et al., 2013):
? The mean and standard deviation of acoustic
features automatically extracted;
? The amount of silence and speaking time for
each speaker;
? Sentiment (positive, negative) and subjectiv-
ity scores calculated for words and turns
? number of words, turns, words per turn and
words related to the negotiation objects
We used only features that were easily and au-
tomatically derivable, excluding features from
(Nouri et al., 2013) such as the number of offers
and the number of rejections or acceptances.
3 Initiative Labeling
A common way of structuring dialogue is with
Initiative-Response pairs, or IR units (Dahlb?ack
and J?onsson, 1998), which are also similar to adja-
cency pairs (Levinson, 1983), or simple exchange
units (Sinclair and Coulthard, 1975). Several re-
searchers have also proposed multiple levels of
initiative. For example, (Whittaker and Stenton,
1988) had levels based on the type of utterance
(commands, questions, assertions, and prompts).
(Chu-Carroll and Brown, 1997) posit two levels
of initiative: discourse initiative, attained by pro-
viding reasons for responses, and critiques of pro-
posed plans, and task initiative, obtained by sug-
gesting new tasks or plans. Linell et al. exam-
ine several factors, such as initiative vs response,
strength of initiative, adequacy of response, scope
and focality of response (Linell et al., 1988). They
end up with an ordered set of six possible strengths
of initiative. Each of these schemes is somewhat
complicated by the fact that turns can consist of
multiple basic elements.
Analyzing previous work, we can see that initia-
tive breaks down into two distinct concepts. First
there is providing unsolicited, or optional, or ex-
tra material, that is not a required response to a
previous initiative. Second, there is the sense of
putting a new discourse obligation (Traum and
Allen, 1994) on a dialogue partner to respond.
These two concepts often come together, such as
for new questions or proposals that require some
sort of response: they are both unsolicited and im-
pose an obligation, which is why (Whittaker and
Stenton, 1988) indicate that control should belong
to the speaker of these utterances. However, it is
also possible to have each one without the other.
Statements can include new unsolicited material,
without imposing an obligation to respond (other
than the weak obligation to ground understand-
ing of any contribution). Likewise, clarification
questions impose new obligations on the other, but
often do not contribute new material or are not
optional, in that the responder can not reply ap-
propriately without the clarification. For (Whit-
taker and Stenton, 1988), the issue of whether
a question or assertion was a ?response? would
determine whether control went to the speaker
or remained with a previous speaker. On the
other hand, (Narayanan et al., 2000) call a re-
sponse that includes unsolicited material ?mixed-
initiative? rather than ?system initiative? for user
responses that contain only prompted material.
Likewise, response can also be broken down
into two related concepts. One concerns fulfilling
obligations imposed by prior initiatives. To not do
so could be considered rude and a violation of con-
versational norms in some cases. This is only rel-
evant, if there is an existing initiative-related obli-
gation as part of the conversational state. Another
concept generalizes the notion of response to any-
thing that contributes to the same topic and makes
an effort to relate to prior utterances by the other
party, whether or not it fulfills an obligation or
whether there even is a pending obligation. This
is like relevance in the sense of Sperber and Wil-
son (Sperber and Wilson, 1986) and Lascarides
and Asher (Asher and Lascarides, 2003).
Our annotation scheme thus includes four la-
bels, as indicated in Table 4. Each of the labels
can either be present or absent from a dialogue
188
Time/Speaker Example Utterance Labels
(R,F,I,N)
[1 : 58] Person 1: Do you want to do just like one grab at a time?
Or do you know how you want to divvy it up? (-,-,I,N)
[2 : 13] Person 2: Um, I?m just thinking. (R,F,-,-)
[3 : 38] Person 1: Do you want it? I?ll take it. Um, do you want to do any trading? (R,-,I,-)
[4 : 15] Person 2: Um, how much is a banana for you? (-,-,I,N)
[4 : 15] Person 1: For me? A point, or two points. How much is the pepper worth? (R,F,I,N)
Table 3: Sample Annotated Utterances
Label Description
R directly relates to prior utterance
F fulfills a pending discourse obligation
I imposes a discourse obligation
N provides new material that is optional
and not just fulfilling an obligation.
Table 4: Initiative Labels
segment. The annotation is done on each turn on
the conversation. In general, a turn can consist of
almost any combination of these four initiative la-
bels (I,R,F,N). We thus treat each of these as an
independent binary dimension, and code each turn
as to which set of these labels it contains. Table 3
shows an example from the corpus with initiative
annotations. More examples can be found in the
Appendix, Tables 11, 12, and 13.
3.1 Inter Annotator Reliability
To assess the reliability of our annotations, ap-
proximately 10% of the dialogs (4 dialogs) were
annotated by two annotators. The level of the
agreement was then assessed using the Kappa
statistic (Carletta, 1996; Siegel and Castellan,
1988). Table 5 shows the result of the assessment
of the reliability of the annotations for the four an-
notation labels.
1
Based on this metric our results
indicate that the annotators have reasonable level
of agreement in labeling utterances with the I, F
,N labels, though there is less reliability for the
?related? label. Further work is needed to clar-
ify the degree of relation that should count and
also whether relation refers just to the immediately
prior turn or something further back. The remain-
der of the dialogues were annotated by one anno-
tator.
1
Chance agreement is the probability of agreement using
the frequencies of each label, but applied randomly.
R F I N
kappa 0.36 0.64 0.66 0.73
actual agreement 0.76 0.83 0.83 0.86
chance agreement 0.62 0.52 0.49 0.50
Table 5: Inter-Annotator Reliability Assessment
3.2 Initiative Taking Patterns
Table 6 shows the average frequency of each ini-
tiative label for each negotiation goal. We can see
that competitive dialogues have more turns that
impose and fulfill obligations than the other con-
ditions, while individualistic dialogues include a
higher percentage of turns introducing new mate-
rial.
Label R F I N
Cooperative 0.79 0.35 0.40 0.33
Competitive 0.82 0.38 0.47 0.34
Individualistic 0.82 0.34 0.39 0.40
Table 6: Comparison of the Relative Frequency of
the Initiative Labels for Each Goal
Table 7 shows the relative frequency of initia-
tive labels for the different outcome conditions.
The higher scoring participants had a higher fre-
quency of initiative-related turns (labels I and N),
while their lower scoring partners had a higher fre-
quency of responsive turns (R,F). Equal scoring
participants tended to pattern closer to higher scor-
ing participants, concerning responses, but closer
to lower scoring participants, considering initia-
tive.
3.3 Initiative Features
After the Initiative annotation was done, the fol-
lowing features were automatically extracted:
? the count of each label (I,F,R,N) per negotia-
tion and per person
189
Label R F I N
H 0.80 0.35 0.47 0.38
E 0.81 0.35 0.40 0.34
L 0.84 0.38 0.43 0.36
Table 7: Comparison of the Relative Frequency of
the Initiative Labels for Each Score Label
? the ratio, difference and absolute difference
of the number of labels for each person
against the number of labels for their nego-
tiation counterpart
? the above measures normalized by the num-
ber of turns in dialog
? Within-turn patterns the number of all pos-
sible combinations of labels for each utter-
ance. There are 16 possible combinations for
the 4 types of labels that can be shown as tu-
ples (R,F,I,N). Refer to Table 5 for examples.
? Across-turn Patterns the number of all pos-
sible sequences of labels across two adjacent
turns. There are also 16 possible combina-
tions capturing how often each label is fol-
lowed by labels. For example, the feature
(I,F) applies to all two-turn sequences where
the first turn contains label I and the second
contains label F, such as in the last two lines
of Figure 3. We count the these features for
the dialogue and for each speaker.
All of the above features were automatically ex-
tracted from the annotated dialogues. We exam-
ined four different spans of the dialogues, to inves-
tigate whether the most salient initiative informa-
tion comes early in the dialogue or requires the full
dialogue. We calculated features for the first quar-
ter (q1), first half (q2), first three quarters (q3), and
the whole negotiation (q4).
4 Prediction Models
We conducted experiments to recognize negotia-
tion goal and score for each of the 82 negotia-
tors. We made prediction models for recognizing
the goal and outcome for each individual.For the
prediction models, we compared the result of sup-
port vector machine (SVM- with the polynomial
kernel function) classifier, Naive Bayes and De-
cision Tree. None of the classifiers outperformed
the others on all cases, we are reporting the result
of SVM classifier here. Considering the size of
our dataset which consists of 82 samples (41 pairs
of individuals) and the distribution of the samples
in different classes, we decided to use the 10-fold
cross validation paradigm for our prediction tasks.
In splitting the dataset into the folds we controlled
so that the participants from the same negotiation
were not split across training and test sets. We
trained and tested at the end of the each quarter
of the negotiation.
We used three sets of features to make three pre-
diction models for each task:
1. Non-initiative features from (Nouri et al.,
2013), described in section 2.3. We refer to
these non-initiative features as IS2013? from
this point on.
2. Initiative features
3. All features combined.
We compare the performance of these models with
two baseline prediction models: one that chooses
one of the outcomes at random, and one that pre-
dicts the majority class for all instances. In the
upcoming sections, we use q1, q2, q3 and q4 to
refer to the ends of the first, second, third and the
forth quarters of the negotiation (e.g. q3 includes
all data from the first three quarters, but not the
last).
4.1 Automatic Prediction of Goal
This task predicts whether the negotiators are fol-
lowing the cooperative, competitive or individual-
istic instructions. It is important to note that none
of the features used require understanding of the
content or a semantic analysis of the conversation.
However, using these basic features it?s possible
to make the classification into the mentioned three
classes with accuracy that is significantly higher
than chance. The average accuracy of prediction
at the four different points in the negotiation are
shown in Table 8.
q1 q2 q3 q4
Random 0.33 0.33 0.33? 0.33?
Majority 0.37 0.37 0.37? 0.37
IS2013 0.41 0.34 0.40? 0.48 ??
Initiative 0.29? 0.52 ??? 0.48 ?? 0.29?
Combined 0.41 0.40 0.57 ?? 0.44 ?
Table 8: Accuracy of the Prediction of Goal
190
We use the two-sided binomial test to measure
the significance of the differences of the prediction
models? performances. Table 8 and the upcoming
Tables 9 and 10 use symbols to indicate the results
of these significance tests. Symbols (?),(?) and
(?) show which models? performances are signif-
icantly different from the random baseline, major-
ity baseline or the ?Combined? classifier respec-
tively (p < 0.05).
The combined classifier is always better than
both baselines, as well as the lower of classi-
fiers for the IS2013 and Initiative features. In
q3, where the two are close in performance, the
combined classifier significantly outperforms both
baselines and the IS2013 model. Note that ex-
cept for q3, these numbers are lower than those
reported by (Nouri et al., 2013). However the prior
work did not ensure that both individuals in a ne-
gotiation were in the same training/test partition,
and some features are the same for both partici-
pants. That work also made use of higher-level
features, such as the offers, and final distributions
of items.
4.2 Automatic Prediction of Outcome
In this task the goal is to predict how a partic-
ipant in the negotiation is going to do in terms
of the scores at the end of the negotiation. The
model predicts whether the negotiator would score
higher, lower or equal to the other player at the end
of the different quarters of the negotiation. Results
are shown in Table 9.
q1 q2 q3 q4
Random 0.33 0.33 0.33 0.33?
Majority 0.41 0.41 0.41 0.41
IS2013 0.43 ? 0.34 0.23 ??? 0.39
Initiative 0.37 0.35 0.32 0.39
Combined 0.38 0.40 0.41 0.4 6?
Table 9: Accuracy of the Prediction of Outcome
Except for the combined model in q4, these
models are not able to significantly outperform the
baseline of selecting the random class (with equal
likelihood). Results were also presented for out-
come in (Nouri et al., 2013), however only the fi-
nal quarter results are comparable, since that paper
predicted interim quarter-end results rather than fi-
nal results. Also, that work did not make sure that
both participants in a negotiation were in the same
training-test partitions, and used features related to
the final deal, that are directly related to outcome.
Because the relative score was not important for
cooperative negotiations, where both sides are just
trying to maximize their combined points, we next
examined outcome for the 28 pairs in individualis-
tic and competitive conditions. Results are shown
in table 10. The combined classifier outperforms
all the other classifiers, starting from quarter 2. At
the end of the negotiation(q4) the performance of
this classifier is significantly better than all other
models.
q1 q2 q3 q4
Random 0.33 0.33? 0.33? 0.33?
Majority 0.38 0.38 0.38 0.38?
IS2013 0.39 0.36? 0.36? 0.34?
Initiative 0.27 0.41 0.36? 0.38?
Combined 0.35 0.50 ? 0.50 ? 0.55 ??
Table 10: Accuracy of the Prediction of Outcome
for Negotiations that are not Cooperative
5 Conclusion
We demonstrated how discourse initiatives in ne-
gotiation dialog can be used for automatically
making predictions about other aspects of the ne-
gotiation such as the goals of the negotiators. Pre-
vious work has mostly focused on using non-
verbal cues for accomplishing similar tasks but
they have not used discourse features like initia-
tives. We also show that initiative features can
give clues about the final outcome for the negotia-
tors. Making such predictions are generally chal-
lenging tasks even for humans and require under-
standing of the content of the negotiations. From a
dialog system?s perspective our results show how
more information can be derived about the users
intentions and performance by analyzing their dis-
course behavior.
6 Future Work
The annotations of the initiative taking patterns
are done manually at this point. Automatic label-
ing of the utterances with the initiative tags is our
next step. We will use the labels in our dataset
for learning how to automatically label new nego-
tiation datasets. We think that HMM and HCRF
methods due to their ability to capture the sequen-
tial and temporal aspect of the negotiation might
be better methods for building the prediction mod-
191
els. We are interested in further analysis of the re-
lationship between initiatives and other aspects of
negotiation such as intentions and the use of lan-
guage. We also want to measure the suitability of
our annotation scheme for initiatives for other di-
alogue genres.
Acknowledgments
We like to thank Kristina Striegnitz, Christopher
Wienberg, Angela Nazarian and David DeVault
for their help with this work. The effort described
here has been sponsored by the US Army. Any
opinions, content or information presented does
not necessarily reflect the position or the policy
of the United States Government, and no official
endorsement should be inferred.
References
Nicholas Asher and Alex Lascarides. 2003. Logics of
Conversation. Cambridge University Press.
Michael Baker. 1994. A model for negotiation in
teaching-learning dialogues. Journal of artificial in-
telligence in education.
Jean Carletta. 1996. Assessing agreement on classi-
fication tasks: the kappa statistic. Computational
linguistics, 22(2):249?254.
Peter J Carnevale. 2013. Audio/video recordings of bi-
lateral negotiations over synthetic objects on a table
that vary in monetary value. Unpublished raw data.
Jennifer Chu-Carroll and Michael K. Brown. 1997.
Tracking initiative in collaborative dialogue inter-
actions. In Proceedings of the Thirty-Fifth Meet-
ing of the Association for Computational Linguis-
tics, pages 262?270. Association for Computational
Linguistics.
Nils Dahlb?ack and Arne J?onsson. 1998. A coding
manual for the link?oping dialogue model. unpub-
lished manuscript.
Anthony Jameson, Bernhard Kipper, Alassane Ndi-
aye, Ralph Sch?afer, Joep Simons, Thomas Weis, and
Detlev Zimmermann. 1994. Cooperating to be non-
cooperative: The dialog system PRACMA. Springer.
Sarit Kraus, Penina Hoz-Weiss, Jonathan Wilkenfeld,
David R Andersen, and Amy Pate. 2008. Resolv-
ing crises through automated bilateral negotiations.
Artificial Intelligence, 172(1):1?18.
Stephen C. Levinson. 1983. Pragmatics. Cambridge
University Press.
Per Linell, Lennart Gustavsson, and P?aivi Juvonen.
1988. Interactional dominance in dyadic communi-
cation: a presentation of initiative-response analysis.
Linguistics, 26(3):415?442.
Shrikanth Narayanan, Giuseppe Di Fabbrizio, Can-
dace A. Kamm, James Hubbell, Bruce Buntschuh,
P. Ruscitti, and Jerry H. Wright. 2000. Effects of
dialog initiative and multi-modal presentation strate-
gies on large directory information access. In IN-
TERSPEECH, pages 636?639. ISCA.
Elnaz Nouri, Sunghyun Park, Stefan Scherer, Jonathan
Gratch, Peter Carnevale, Louie Philippe Morency,
and David Traum. 2013. Prediction of strategy and
outcome as negotiation unfolds by using basic ver-
bal and behavioral features. In proceedings of the
Interspeech conference.
Candace L. Sidner. 1994. An artificial discourse lan-
guage for collaborative negotiation. In Proceedings
of the Fourteenth National Conference of the Amer-
ican Association for Artificial Intelligence (AAAI-
94), pages 814?819.
S. Siegel and N. J. Castellan. 1988. Nonparamet-
ric statistics for the Behavioral Sciences. McGraw-
Hill, 2nd edition.
J. M. Sinclair and R. M. Coulthard. 1975. Towards an
analysis of Discourse: The English used by teachers
and pupils. Oxford University Press.
Dan Sperber and Deirdre Wilson. 1986. Relevence:
Communication and Cognition. Harvard University
Press.
David R. Traum and James F. Allen. 1994. Discourse
obligations in dialogue processing. In Proceedings
of the 32
nd
Annual Meeting of the Association for
Computational Linguistics, pages 1?8.
David Traum, Stacy C Marsella, Jonathan Gratch, Jina
Lee, and Arno Hartholt. 2008. Multi-party, multi-
issue, multi-strategy negotiation for multi-modal
virtual agents. In Intelligent Virtual Agents, pages
117?130. Springer.
Marilyn Walker and Steve Whittaker. 1990. Mixed ini-
tiative in dialogue: An investigation into discourse
segmentation. In Proceedings of the 28th annual
meeting on Association for Computational Linguis-
tics, pages 70?78. Association for Computational
Linguistics.
Richard E Walton and Robert B McKersie. 1991. A
behavioral theory of labor negotiations: An analysis
of a social interaction system. Cornell University
Press.
Steve Whittaker and Phil Stenton. 1988. Cues and
control in expert-client dialogues. In Proceedings
ACL-88, pages 123?130.
Appendix: Sample Annotated Negotiations
The following tables show examples of each of the
goal conditions with initiative labeling, using the
scheme in Table 4.
192
Time Speaker: Utterance Labels
(R,F,I,N)
[2 : 18] 2: So what?s, so what?s everything worth to you? (-,-,I,N)
[2 : 20] 1: Um, so apples are three, bananas are three, strawberries are one, pep-
pers are one, and lemons are nothing.
(R,F,-,-)
[2 : 33] 2: Okay so for me peppers are three, bananas are three, and apples and
strawberries are one.
(R,-,-,-)
[2 : 39] 1: Lemons are zero. (R,-,-,-)
[2 : 40] 2: Yeah. (R,-,-,)
Table 11: Sample Annotated Cooperative Negotiation
Time Speaker: Utterance Labels
[1 : 40] 2: So, I think I need peppers and bananas for my restaurant. (-,-,-,N)
[1 : 46] 1: Okay. Um, well I really need. I want five apples and um, five bananas.
Five apples and five bananas.
(R,-,I,N)
[2 : 05] 2: Um, how about this: You take five apples, and I take five peppers and
we can share the bananas.
(R,F,I,N)
[2 : 13] 1: Okay. If I give you, if I give you five or if I give you, if we were to
share the bananas, if I take three bananas, I?ll give you three lemons.
(R,F,I,N)
[2 : 23] 2: But we don?t need lemons in our restaurant. We only use lemons for
our store.
(R,F,-,N)
[2 : 27] 1: Okay. So, um, I need bananas, like that?s gonna be my top. (R,-,-,N)
Table 12: Sample Annotated Competitive Negotiation
Time Speaker: Utterance Labels
[3 : 22] 2: How about we do this. You take two of these, I take one, and since we
have five here, I take three, you take two.
(-,F,I,N)
[3 : 37] 1: I?m not interested in lemons at all. But I can give you... (R,F,-,N)
[3 : 52] 2: At my restaurant, one of our dessert dishes is with strawberries, so
strawberries are very important to me.
(-,-,-,N)
[4 : 00] 1: Okay. I?m willing to give you all the strawberries if you give me a
banana and two apples. I?m also willing to give you these two.
(R,-,-,N)
[4 : 23] 2: So you?re going to give me those two? (R,-,I,-)
[4 : 24] 1: You can have everything on this side, I just want two apples and a
banana.
(R,F,-,-)
[4 : 30] 2: Two apples and a banana? Yeah, let?s go. (R,-,-,-)
[4 : 39] 1: We have a deal. (R,F,-,N)
Table 13: Sample Annotated Individualistic Negotiation
193
Proceedings of the SIGDIAL 2014 Conference, pages 251?253,
Philadelphia, U.S.A., 18-20 June 2014.
c?2014 Association for Computational Linguistics
SAWDUST: a Semi-Automated Wizard Dialogue Utterance Selection Tool
for domain-independent large-domain dialogue
Sudeep Gandhe David Traum
University of Southern California, Institute for Creative Technologies
srgandhe@gmail.com, traum@ict.usc.edu
Abstract
We present a tool that allows human wiz-
ards to select appropriate response utter-
ances for a given dialogue context from
a set of utterances observed in a dia-
logue corpus. Such a tool can be used
in Wizard-of-Oz studies and for collecting
data which can be used for training and/or
evaluating automatic dialogue models. We
also propose to incorporate such automatic
dialogue models back into the tool as an
aid in selecting utterances from a large di-
alogue corpus. The tool allows a user to
rank candidate utterances for selection ac-
cording to these automatic models.
1 Motivation
Dialogue corpora play an increasingly important
role as a resource for dialogue system creation.
In addition to its traditional roles, such as train-
ing language models for speech recognition and
natural language understanding, the dialogue cor-
pora can be directly used for the selection ap-
proach to response formation (Gandhe and Traum,
2010). In the selection approach, the response is
formulated by simply picking the appropriate ut-
terance from a set of previously observed utter-
ances. This approach is used in many wizard of
oz systems, where the wizard presses a button to
select an utterance, as well as in many automated
dialogue systems (Leuski et al., 2006; Zukerman
and Marom, 2006; Sellberg and J?onsson, 2008)
The resources required for the selection ap-
proach are a set of utterances to choose from and
optionally, a set of pairs of ?context, response
utterance? to train automatic dialogue models. A
wizard can generate such resources by performing
two types of tasks. First is the traditional Wizard-
of-Oz dialogue collection, where a wizard inter-
acts with a user of the dialogue system. Here the
wizard selects an appropriate response utterance
for a context that is being updated in a dynamic
fashion as the dialogue proceeds (dynamic context
setting). The second task is geared towards gather-
ing data for training/evaluating automatic dialogue
models, where a wizard is required to select ap-
propriate responses (perhaps more than one) for a
context which is extracted from a human-human
dialogue. The context does not change based on
the wizard?s choices (static context setting).
A wizard tool should help with the challenges
presented by these tasks. A challenge for both
of these tasks is that if the number of utterances
in the corpus is large (e.g., more than the num-
ber of buttons that can be placed on a computer
screen), it may be very difficult for a wizard to lo-
cate appropriate utterances. For the second task of
creating human-verified training/evaluation data,
tools like NPCEditor (Leuski and Traum, 2010)
have been developed which, allow the tagging of
a many to many relationships between contexts
(approximated simply as input utterance) and re-
sponses. In other cases, a corpus of dialogues is
used to acquire the set of selectable utterances, in
which each context is followed by a single next
utterance, and many utterances appear only once.
This sparsity of data makes the selection task hard.
Moreover, it may be the case that there are many
possible continuations of a context or contexts in
which an utterance may be appropriate (DeVault
et al., 2011).
We address these needs with a semi-automated
wizard tool that allows a wizard to engage in dy-
namic or static context utterance selection, select
multiple responses, and use several kinds of search
tools to locate promising utterances from a large
set that can?t all be displayed or remembered. In
the next section we describe the tool and how it
can be used. Then we describe how this tool was
used to create evaluation data in the static context
setting.
251
Figure 1: A screenshot of the interface for the wizard data collection
in static context setting.
Figure 2: A Histogram for the
number of selected appropriate
responses.
Figure 3: Avg. cardinality of the
set for different values of |R|.
2 Wizard Tool
Our wizard tool consists of several different views
(see figure 1), and is similar in some respects to the
IORelator annotation tool (DeVault et al., 2010),
but specialized to act as a wizard interface. The
first view (left pane) is a dialogue context, that
shows the recent history of the dialogue, before
the wizard?s decision point. The second view (top
right pane) shows a list of possible utterances that
can be selected from. This view can be ordered
in several different ways, as described below. Fi-
nally, there is a view of selected utterances (bot-
tom right pane). In the case of dynamic context,
the wizard will probably only select one utterance
and then a dialogue partner will respond with a
new utterance that extends the previous context.
In the case of static evaluation, however, used for
training and/or evaluating automated selection al-
gorithms, it is often helpful to select multiple ut-
terances if more than one is appropriate.
To help wizards explore the set of all possible
utterances, we provide the ability to rank the utter-
ances by various automated scores. Our configu-
ration used in the static context task uses Score1 as
the score calculated using one of the automatic di-
alogue models, specifically Nearest Context model
(Gandhe and Traum, 2007) - this model orders
candidate utterances from the corpus by the sim-
ilarity of their previous two utterances to the cur-
rent dialogue context. Score2 is surface text sim-
ilarity, computed as the METEOR score (Lavie
and Denkowski, 2009) between the candidate ut-
terance and the actual response utterance present
at that location in original human-human dialogue
(which is not available to the wizard). Wizards can
also search the set of utterances for specific key-
words and the third column, Relevance, shows the
score for the search string entered by the wizards.
The last column RF stands for relevance feedback
and ranks the utterances by similarity to the utter-
ances that have already been chosen by the wiz-
ard. This allows wizards to easily find paraphrases
of already selected response utterances. Clicking
the header of any of these columns will reorder the
utterance list by the automated score, by relevance
(assuming a search term has been entered) or by
relevance feedback (assuming one or more utter-
ances have already been chosen).
3 Evaluation
We evaluated the tool by having four human vol-
unteers (wizards) use it in order to establish an up-
per baseline for human-level performance in the
static context evaluation task described in (Gandhe
and Traum, 2013). Wizards were instructed in how
to use the search and relevance feedback features.
In order to not bias the wizards, they were not told
exactly what score1 and score2 indicate, but just
that the scores can be useful in search.
Each wizard is presented with a set of utter-
ances (U
train
) (|U
train
| ? 500) and is asked to
select a subset from these that will be appropri-
ate as a response for the presented dialogue con-
text. Each wizard was requested to select some-
where between 5 to 10 (at-least one) appropriate
responses for each dialogue context extracted from
252
five different human-human dialogues. There are
a total of 89 dialogue contexts for the role that
the wizards were to play. Figure 2 shows the his-
togram for the number of utterances selected as
appropriate responses by the four wizards. As ex-
pected, wizards frequently chose multiple utter-
ances as appropriate responses (mean = 7.80, min
= 1, max = 25).
To get an idea about how much the wizards
agree among themselves for this task, we calcu-
lated the overlap between the utterances selected
by a specific wizard and the utterances selected by
another wizard or a set of wizards. Let U
T
c
be a set
of utterances selected by a wizard T for a dialogue
context c. Let R be a set of wizards (T /? R) and
U
R
c
be the union of sets of utterances selected by
the set of wizards (R) for the same context c. Then
we define the following overlap measures,
Precision
c
=
|U
T
c
? U
R
c
|
|U
T
c
|
Recall
c
=
|U
T
c
? U
R
c
|
|U
R
c
|
Jaccard
c
=
|U
T
c
? U
R
c
|
|U
T
c
? U
R
c
|
Dice
c
=
2|U
T
c
? U
R
c
|
|U
T
c
|+ |U
R
c
|
Meteor
c
=
1
|U
T
c
|
?
u
t
METEOR (u
t
, U
R
c
) ?u
t
? U
T
c
We compute the average values of these over-
lap measures for all contexts and for all possible
settings of test wizards and reference wizards. Ta-
ble 1 shows the results with different values for the
number of wizards used as reference.
#ref Prec. Rec. Jacc. Dice Meteor
1 0.145 0.145 0.077 0.141 0.290
2 0.244 0.134 0.093 0.170 0.412
3 0.311 0.121 0.094 0.171 0.478
Table 1: Inter-wizard agreement
Precision can be interpreted as the probability
that a response utterance selected by a wizard is
also considered appropriate by at least one other
wizard. Precision rapidly increases along with
the number of reference wizards used. This hap-
pens because the size of the set U
R
c
steadily in-
creases with more reference wizards. Figure 3
shows this observed increase and the expected in-
crease if there were no overlap between the wiz-
ards. The near-linear increase in |U
R
c
| suggests
that selecting appropriate responses is a hard task
and may require a lot more than four wizards to
achieve convergence.
Subjectively, the wizards reported no major us-
ability problems with the tool, and were able to
use all four utterance ordering techniques to find
appropriate utterances.
4 Future Work
Future work involves performing some formal
evaluations comparing this tool to other tools (that
are missing some of the features of this tool) in
terms of amount of time taken to make selections
and quality of the selections, using the same eval-
uation techniques as (Gandhe and Traum, 2013).
We also see a promising future for semi-
automated selection, which blurs the line between
a pure algorithmic response and pure wizard se-
lection. Here the wizard can select appropriate re-
sponses, which can be used by algorithms as su-
pervised training data, meanwhile the algorithms
can be used to seed the wizard?s selection.
References
David DeVault, Susan Robinson, and David Traum. 2010.
IORelator: A graphical user interface to enable rapid se-
mantic annotation for data-driven natural language under-
standing. In Proc. of the 5th Joint ISO-ACL/SIGSEM
Workshop on Interoperable Semantic Annotation (ISA-5).
David DeVault, Anton Leuski, and Kenji Sagae. 2011. An
evaluation of alternative strategies for implementing dia-
logue policies using statistical classification and rules. In
Proceedings of the IJCNLP 2011, Nov.
Sudeep Gandhe and David Traum. 2007. Creating spoken
dialogue characters from corpora without annotations. In
Proceedings of Interspeech-07, Antwerp, Belgium.
Sudeep Gandhe and David Traum. 2010. I?ve said it be-
fore, and I?ll say it again: an empirical investigation of
the upper bound of the selection approach to dialogue. In
Proceedings of the SIGDIAL ?10, Tokyo, Japan.
Sudeep Gandhe and David Traum. 2013. Surface text based
dialogue models for virtual humans. In Proceedings of the
SIGDIAL 2013, Metz, France.
A. Lavie and M. J. Denkowski. 2009. The meteor metric
for automatic evaluation of machine translation. Machine
Translation, 23:105?115.
Anton Leuski and David R. Traum. 2010. NPCEditor: A
tool for building question-answering characters. In Pro-
ceedings of LREC 2010, Valletta, Malta.
Anton Leuski, Ronakkumar Patel, David Traum, and Bran-
don Kennedy. 2006. Building effective question answer-
ing characters. In Proc. of SIGDIAL ?06, Australia.
Linus Sellberg and Arne J?onsson. 2008. Using random in-
dexing to improve singular value decomposition for latent
semantic analysis. In Proceedings of LREC?08, Morocco.
Ingrid Zukerman and Yuval Marom. 2006. A corpus-based
approach to help-desk response generation. In Computa-
tional Intelligence for Modelling, Control and Automation
(CIMCA 2006), IAWTIC 2006.
253
Proceedings of the SIGDIAL 2014 Conference, pages 254?256,
Philadelphia, U.S.A., 18-20 June 2014.
c?2014 Association for Computational Linguistics
A Demonstration of Dialogue Processing in SimSensei Kiosk
Fabrizio Morbini, David DeVault, Kallirroi Georgila,
Ron Artstein, David Traum, Louis-Philippe Morency
USC Institute for Creative Technologies
12015 Waterfront Dr., Playa Vista, CA 90094
{morbini,devault,kgeorgila,artstein,traum,morency}@ict.usc.edu
Abstract
This demonstration highlights the dia-
logue processing in SimSensei Kiosk, a
virtual human dialogue system that con-
ducts interviews related to psychologi-
cal distress conditions such as depression,
anxiety, and post-traumatic stress disorder
(PTSD). The dialogue processing in Sim-
Sensei Kiosk allows the system to con-
duct coherent spoken interviews of human
users that are 15-25 minutes in length,
and in which users feel comfortable talk-
ing and openly sharing information. We
present the design of the individual dia-
logue components, and show examples of
natural conversation flow between the sys-
tem and users, including expressions of
empathy, follow-up responses and contin-
uation prompts, and turn-taking.
1 Introduction
This demonstration highlights the dialogue pro-
cessing in SimSensei Kiosk, a virtual human di-
alogue system that conducts interviews related to
psychological distress conditions such as depres-
sion, anxiety, and post-traumatic stress disorder
(PTSD) (DeVault et al., 2014). SimSensei Kiosk
has two main functions ? a virtual human called
Ellie (pictured in Figure 1), who converses with a
user in a spoken, semi-structured interview, and a
multimodal perception system which analyzes the
user?s behavior in real time to identify indicators
of psychological distress.
The system has been designed and devel-
oped over two years using a series of face-to-
face, Wizard-of-Oz, and automated system stud-
ies involving more than 350 human participants
(Scherer et al., 2013; DeVault et al., 2013; DeVault
et al., 2014). Agent design has been guided by
two overarching goals: (1) the agent should make
Figure 1: Ellie, the virtual human interviewer in
SimSensei Kiosk.
the user feel comfortable talking and openly shar-
ing information, and at the same time (2) the agent
should create interactional situations that support
the automatic assessment of verbal and nonver-
bal behaviors correlated with psychological dis-
tress. During an interview, the agent presents a
set of questions which have been shown in user
testing to support these goals. Since the main in-
terview questions and their order are mostly fixed,
dialogue management concentrates on providing
appropriate verbal feedback behaviors to keep the
user engaged, maintain a natural and comfort-
able conversation flow, and elicit continuations
and elaborations from the user.
The agent is implemented using a modular ar-
chitecture (Hartholt et al., 2013). Dialogue pro-
cessing, which is the focus of this demonstration,
is supported by individual modules for speech
recognition, language understanding and dialogue
management (see Section 2). The agent?s lan-
guage and speech are executed by selecting from
pre-recorded audio clips. Additional agent mod-
ules include nonverbal behavior generation, which
matches appropriately timed body movements to
the agent?s speech; character animation in a vir-
tual 3D environment; and rendering in a game en-
254
gine. The perception system analyzes audio and
video in real time to identify features such as head
position, gaze direction, smile intensity, and voice
quality. DeVault et al. (2014) provides details on
all the agent?s modules.
2 Overview of Dialogue Processing
2.1 ASR and NLU components
Unlike many task-oriented dialogue domains, in-
terview dialogues between SimSensei Kiosk and
participants are naturally open-ended. People tend
to respond to interview stimuli such as ?what?s
one of your most memorable experiences?? with
idiosyncratic stories and events from their lives.
The variability in the vocabulary and content of
participants? answers to such questions is so large
that it makes the ASR task very challenging. Fur-
thermore, continuous ASR is employed to ensure
that participants feel comfortable interacting with
the system without being distracted by having to
use a push-to-talk microphone. The use of con-
tinuous ASR necessitates the development of spe-
cific policies for turn-taking (see Section 2.2). In
this demonstration, voice activity detection and
speech recognition are performed using Pocket-
Sphinx (Huggins-Daines et al., 2006).
Because of the open-ended participants? re-
sponses, for NLU, we cannot simply construct a
small semantic ontology and expect to cover the
majority of meanings that will be expressed by
users. Thus, this is an application in which the
dialogue policy needs to be able to create a sense
of engagement, continuation, and empathy despite
relatively shallow and limited understanding of
user speech. SimSensei Kiosk currently uses 4
statistically trained utterance classifiers to capture
different aspects of user utterance meaning.
The first NLU classifier identifies generic di-
alogue act types, including statements, yes-no
questions, wh-questions, yes and no answers, and
several others. This classifier is trained using
the Switchboard DAMSL corpus (Jurafsky et al.,
1997) using a maximum entropy model.
The second NLU classifier assigns positive,
negative, or neutral valence to utterances, in or-
der to guide Ellie?s expression of empathy. We
use SentiWordNet 3.0 (Baccianella et al., 2010), a
lexical sentiment dictionary, to assign valence to
individual words spoken by users (as recognized
by the ASR); the valence assigned to an utterance
is based primarily on the mean valence scores of
Opening Rapport Building Phase
Ellie What are some things you really like about LA?
(top level question)
User I love the weather, I love the palm trees, I love the
beaches, there?s a lot to do here.
Diagnostic Phase
Ellie Have you noticed any changes in your behavior or
thoughts lately? (top level question)
User Yes.
Ellie Can you tell me about that? (continuation prompt)
User I?m having a lot more nightmares now uh can?t
sleep have haven?t really been eating uh trying to
eat. . . I have to force down food um just feeling
like an emotional wreck.
Ellie I?m sorry to hear that. (empathy response)
Ellie What are you like when you don?t sleep well?
(follow-up question)
User Irritable, emotional, it just adds to my overall
stress um [long pause]
Ellie What. . . (Ellie speaks after the participant?s long
pause)
User Can?t concentrate uh I uh. . . (the participant starts
speaking while Ellie is speaking)
Ellie I?m sorry please continue. (Ellie realizes that she
has interrupted the participant and apologizes)
Positive Closing Phase
Ellie How would your best friend describe you? (top
level question)
User As caring, as fun because most of the time when
I?m around my best friends I?m happy and I?m fun
loving. I joke around with them a lot and uh I do
better when I?m around my friends. . .
Figure 2: Examples of Ellie?s interview phases.
the individual words in the utterance.
The third NLU classifier supports domain-
specific small talk by recognizing a handful of
specific anticipated responses to Ellie?s rapport-
building questions. For example, when Ellie asks
users where they are from, this classifier detects
the names of commonly mentioned cities and re-
gions using keyphrase spotting.
The fourth NLU classifier identifies domain-
specific dialogue acts, and supports Ellie?s follow-
up responses to specific questions, such as ?how
close are you to your family??. This maximum
entropy classifier is trained using face-to-face and
Wizard-of-Oz data to detect specific responses
such as assertions of closeness.
2.2 Dialogue Management
Ellie currently uses about 100 fixed utterances in
total in the automated system. She employs 60 top
level interview questions (e.g., ?do you travel a
255
lot??), plus some follow-up questions (e.g., ?what
do you enjoy about traveling??) and a range of
backchannels (e.g., ?uh huh?), empathy responses
(e.g., ?that?s great?, ?I?m sorry?), and continua-
tion prompts (e.g., ?tell me more about that?).
The dialogue policy is implemented using the
FLoReS dialogue manager (Morbini et al., 2012).
The policy groups interview questions into three
phases (opening rapport building, diagnostic, pos-
itive closing ? ensuring that the participant leaves
with positive feelings). Questions are generally
asked in a fixed order, with some branching based
on responses to specific questions.
Rule-based subpolicies determine what Ellie?s
follow-up responses will be for each of her top-
level interview questions. The rules for follow-ups
are defined in relation to the four NLU classifiers
and the duration of user speech (measured in sec-
onds). These rules trigger continuation prompts
and empathy responses under specific conditions.
The turn-taking policy supports our design goal
to encourage users to openly share information
and to speak at length in response to Ellie?s open-
ended questions. In this domain, users often pause
before or during their responses to think about
their answers to Ellie?s personal questions. The
turn-taking policy is designed to provide ample
time for users to consider their responses, and to
let users take and keep the initiative as much as
possible. Ellie?s turn-taking decisions are based
on thresholds for user pause duration, i.e., how
much time the system should wait after the user
has stopped speaking before Ellie starts speaking.
These thresholds are tuned to the face-to-face and
Wizard-of-Oz data to minimize Ellie?s interrup-
tion rate, and are extended dynamically when El-
lie detects that she has interrupted the participant.
This is to take into account the fact that some peo-
ple tend to use longer pauses than others.
Examples of the three interview phases and of
Ellie?s subdialogue policies (top level and follow-
up questions, continuation prompts, empathy re-
sponses, and turn-taking) are given in Figure 2.
3 Demonstration Summary
The demonstration will feature a live interac-
tion between Ellie and a participant, showing El-
lie?s real-time understanding and consequent pol-
icy actions. Live dialogues will highlight Ellie?s
strategies for questioning, follow-up continuation
prompts, displays of empathy, and turn-taking,
similar to the example in Figure 2. The demon-
stration will illustrate how these elements work to-
gether to enable Ellie to carry out extended inter-
views that also provide information relevant to the
automatic assessment of indicators of distress.
Acknowledgments
The effort described here is supported by DARPA
under contract W911NF-04-D-0005 and the U.S.
Army. Any opinion, content or information pre-
sented does not necessarily reflect the position or
the policy of the United States Government, and
no official endorsement should be inferred.
References
S. Baccianella, A. Esuli, and F. Sebastiani. 2010. Sen-
tiWordNet 3.0: An enhanced lexical resource for
sentiment analysis and opinion mining. In Proceed-
ings of LREC.
D. DeVault, K. Georgila, R. Artstein, F. Morbini, D.
Traum, S. Scherer, A. Rizzo, and L.-P. Morency.
2013. Verbal indicators of psychological distress in
interactive dialogue with a virtual human. In Pro-
ceedings of SIGDIAL.
D. DeVault, R. Artstein, G. Benn, T. Dey, E. Fast,
A. Gainer, K. Georgila, J. Gratch, A. Hartholt, M.
Lhommet, G. Lucas, S. Marsella, F. Morbini, A.
Nazarian, S. Scherer, G. Stratou, A. Suri, D. Traum,
R. Wood, Y. Xu, A. Rizzo, and L.-P. Morency. 2014.
SimSensei Kiosk: A virtual human interviewer for
healthcare decision support. In Proceedings of AA-
MAS.
A. Hartholt, D. Traum, S. Marsella, A. Shapiro, G.
Stratou, A. Leuski, L.-P. Morency, and J. Gratch.
2013. All together now, introducing the virtual hu-
man toolkit. In Proceedings of IVA.
D. Huggins-Daines, M. Kumar, A. Chan, A.W. Black,
M. Ravishankar, and A.I. Rudnicky. 2006. Pocket-
Sphinx: A free, real-time continuous speech recog-
nition system for hand-held devices. In Proceedings
of ICASSP.
D. Jurafsky, E. Shriberg, and D. Biasca. 1997. Switch-
board SWBD-DAMSL Shallow-Discourse-Function
Annotation Coders Manual, Draft 13.
F. Morbini, D. DeVault, K. Sagae, J. Gerten, A. Nazar-
ian, and D. Traum. 2012. FLoReS: A forward look-
ing reward seeking dialogue manager. In Proceed-
ings of IWSDS.
S. Scherer, G. Stratou, M. Mahmoud, J. Boberg,
J. Gratch, A. Rizzo, and L.-P. Morency. 2013. Au-
tomatic behavior descriptors for psychological dis-
order analysis. In Proceedings of IEEE Conference
on Automatic Face and Gesture Recognition.
256
