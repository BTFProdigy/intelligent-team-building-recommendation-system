Antonymy and Conceptual Vectors
Didier Schwab, Mathieu Lafourcade and Violaine Prince
LIRMM
Laboratoire d?informatique, de Robotique
et de Microe?lectronique de Montpellier
MONTPELLIER - FRANCE.
{schwab,lafourca,prince}@lirmm.fr
http://www.lirmm.fr/ ?{schwab, lafourca, prince}
Abstract
For meaning representations in NLP, we focus
our attention on thematic aspects and concep-
tual vectors. The learning strategy of concep-
tual vectors relies on a morphosyntaxic analy-
sis of human usage dictionary definitions linked
to vector propagation. This analysis currently
doesn?t take into account negation phenomena.
This work aims at studying the antonymy as-
pects of negation, in the larger goal of its inte-
gration into the thematic analysis. We present a
model based on the idea of symmetry compat-
ible with conceptual vectors. Then, we define
antonymy functions which allows the construc-
tion of an antonymous vector and the enumer-
ation of its potentially antinomic lexical items.
Finally, we introduce a measure which evaluates
how a given word is an acceptable antonym for
a term.
1 Introduction
Research in meaning representation in NLP is
an important problem still addressed through
several approaches. The NLP team at LIRMM
currently works on thematic and lexical disam-
biguation text analysis (Laf01). Therefore we
built a system, with automated learning capa-
bilities, based on conceptual vectors for mean-
ing representation. Vectors are supposed to en-
code ?ideas? associated to words or to expres-
sions. The conceptual vectors learning system
automatically defines or revises its vectors ac-
cording to the following procedure. It takes, as
an input, definitions in natural language con-
tained in electronic dictionaries for human us-
age. These definitions are then fed to a morpho-
syntactic parser that provides tagging and anal-
ysis trees. Trees are then used as an input
to a procedure that computes vectors using
tree geometry and syntactic functions. Thus,
a kernel of manually indexed terms is necessary
for bootstrapping the analysis. The transver-
sal relationships1, such as synonymy (LP01),
antonymy and hyperonymy, that are more or
less explicitly mentioned in definitions can be
used as a way to globally increase the coher-
ence of vectors. In this paper, we describe a
vectorial function of antonymy. This can help
to improve the learning system by dealing with
negation and antonym tags, as they are often
present in definition texts. The antonymy func-
tion can also help to find an opposite thema to
be used in all generative text applications: op-
posite ideas research, paraphrase (by negation
of the antonym), summary, etc.
2 Conceptual Vectors
We represent thematic aspects of textual seg-
ments (documents, paragraph, syntagms, etc)
by conceptual vectors. Vectors have been used
in information retrieval for long (SM83) and
for meaning representation by the LSI model
(DDL+90) from latent semantic analysis (LSA)
studies in psycholinguistics. In computational
linguistics, (Cha90) proposes a formalism for
the projection of the linguistic notion of se-
mantic field in a vectorial space, from which
our model is inspired. From a set of elemen-
tary concepts, it is possible to build vectors
(conceptual vectors) and to associate them to
lexical items2. The hypothesis3 that considers
a set of concepts as a generator to language
has been long described in (Rog52). Polysemic
words combine different vectors corresponding
1well known as lexical functions (MCP95)
2Lexical items are words or expressions which consti-
tute lexical entries. For instance, ?car ? or ?white ant ? are
lexical items. In the following we will (some what) use
sometimes word or term to speak about a lexical item.
3that we call thesaurus hypothesis.
to different meanings. This vector approach
is based on known mathematical properties, it
is thus possible to undertake well founded for-
mal manipulations attached to reasonable lin-
guistic interpretations. Concepts are defined
from a thesaurus (in our prototype applied to
French, we have chosen (Lar92) where 873 con-
cepts are identified). To be consistent with the
thesaurus hypothesis, we consider that this set
constitutes a generator family for the words and
their meanings. This familly is probably not
free (no proper vectorial base) and as such, any
word would project its meaning on it according
to the following principle. Let be C a finite set
of n concepts, a conceptual vector V is a linear
combinaison of elements ci of C. For a meaning
A, a vector V (A) is the description (in exten-
sion) of activations of all concepts of C. For ex-
ample, the different meanings of ?door ? could be
projected on the following concepts (the CON-
CEPT [intensity] are ordered by decreasing val-
ues): V(?door ?) = (OPENING[0.8], BARRIER[0.7],
LIMIT [0.65], PROXIMITY [0.6], EXTERIOR[0.4], IN-
TERIOR[0.39], . . .
In practice, the larger C is, the finer the mean-
ing descriptions are. In return, the computing
is less easy: for dense vectors4, the enumera-
tion of activated concepts is long and difficult
to evaluate. We prefer to select the themati-
cally closest terms, i.e., the neighbourhood. For
instance, the closest terms ordered by increas-
ing distance to ?door ? are: V(?door ?)=?portal ?,
?portiere?, ?opening?, ?gate?, ?barrier ?,. . .
2.1 Angular Distance
Let us define Sim(A,B) as one of the similar-
ity measures between two vectors A et B, of-
ten used in information retrieval (Mor99). We
can express this function as: Sim(A,B) =
cos(A?, B) = A?B?A???B? with ??? as the scalar
product. We suppose here that vector com-
ponents are positive or null. Then, we define
an angular distance DA between two vectors A
and B as DA(A,B) = arccos(Sim(A,B)). In-
tuitively, this function constitutes an evaluation
of the thematic proximity and measures the an-
gle between the two vectors. We would gener-
ally consider that, for a distance DA(A,B) ? pi4
4Dense vectors are those which have very few null
coordinates. In practice, by construction, all vectors are
dense.
(45 degrees) A and B are thematically close and
share many concepts. For DA(A,B) ? pi4 , the
thematic proximity between A and B would be
considered as loose. Around pi2 , they have no
relation. DA is a real distance function. It ver-
ifies the properties of reflexivity, symmetry and
triangular inequality. We have, for example,
the following angles(values are in radian and de-
grees).
DA(V(?tit ?), V(?tit ?))=0 (0)
DA(V(?tit ?), V(?bird ?))=0.55 (31)
DA(V(?tit ?), V(?sparrow ?))=0.35 (20)
DA(V(?tit ?), V(?train ?))=1.28 (73)
DA(V(?tit ?), V(?insect ?))=0.57 (32)
The first one has a straightforward interpreta-
tion, as a ?tit ? cannot be closer to anything else
than itself. The second and the third are not
very surprising since a ?tit ? is a kind of ?sparrow ?
which is a kind of ?bird ?. A ?tit ? has not much
in common with a ?train?, which explains a large
angle between them. One can wonder why there
is 32 degrees angle between ?tit ? and ?insect ?,
which makes them rather close. If we scruti-
nise the definition of ?tit ? from which its vector
is computed (Insectivourous passerine bird with
colorful feather.) perhaps the interpretation of
these values seems clearer. In effect, the the-
matic is by no way an ontological distance.
2.2 Conceptual Vectors Construction.
The conceptual vector construction is based on
definitions from different sources (dictionaries,
synonym lists, manual indexations, etc). Defini-
tions are parsed and the corresponding concep-
tual vector is computed. This analysis method
shapes, from existing conceptual vectors and
definitions, new vectors. It requires a bootstrap
with a kernel composed of pre-computed vec-
tors. This reduced set of initial vectors is man-
ually indexed for the most frequent or difficult
terms. It constitutes a relevant lexical items
basis on which the learning can start and rely.
One way to build an coherent learning system
is to take care of the semantic relations between
items. Then, after some fine and cyclic compu-
tation, we obtain a relevant conceptual vector
basis. At the moment of writing this article,
our system counts more than 71000 items for
French and more than 288000 vectors, in which
2000 items are concerned by antonymy. These
items are either defined through negative sen-
tences, or because antonyms are directly in the
dictionnary. Example of a negative definition:
?non-existence?: property of what does not exist.
Example of a definition stating antonym: ?love?:
antonyms: ?disgust ?, ?aversion?.
3 Definition and Characterisation of
Antonymy
We propose a definition of antonymy compat-
ible with the vectorial model used. Two lexi-
cal items are in antonymy relation if there is
a symmetry between their semantic components
relatively to an axis. For us, antonym construc-
tion depends on the type of the medium that
supports symmetry. For a term, either we can
have several kinds of antonyms if several possi-
bilities for symmetry exist, or we cannot have
an obvious one if a medium for symmetry is not
to be found. We can distinguish different sorts
of media: (i) a property that shows scalar val-
ues (hot and cold which are symmetrical values
of temperature), (ii) the true-false relevance or
application of a property (e.g. existence/non-
existence) (iii) cultural symmetry or opposition
(e.g. sun/moon).From the point of view of lex-
ical functions, if we compare synonymy and
antonymy, we can say that synonymy is the
research of resemblance with the test of sub-
stitution (x is synonym of y if x may replace
y), antonymy is the research of the symmetry,
that comes down to investigating the existence
and nature of the symmetry medium. We have
identified three types of symmetry by relying
on (Lyo77), (Pal76) and (Mue97). Each sym-
metry type characterises one particular type of
antonymy. In this paper, for the sake of clarity
and precision, we expose only the complemen-
tary antonymy. The same method is used for
the other types of antonymy, only the list of
antonymous concepts are different.
3.1 Complementary Antonymy
The complementary antonyms are couples like
event/unevent, presence/absence.
he is present ? he is not absent
he is absent ? he is not present
he is not absent ? he is present
he is not present ? he is absent
In logical terms, we would have:
?x P (x)? ?Q(x) ?x ?P (x)? Q(x)
?x Q(x)? ?P (x) ?x ?Q(x)? P (x)
This corresponds to the exclusive disjunction
relation. In this frame, the assertion of one
of the terms implies the negation of the other.
Complementary antonymy presents two kinds
of symmetry, (i) a value symmetry in a boolean
system, as in the examples above and (ii) a sym-
metry about the application of a property (black
is the absence of color, so it is ?opposed? to all
other colors or color combinaisons).
4 Antonymy Functions
4.1 Principles and Definitions.
The aim of our work is to create a function
that would improve the learning system by sim-
ulating antonymy. In the following, we will be
mainly interested in antonym generation, which
gives a good satisfaction clue for these functions.
We present a function which, for a given lex-
ical item, gives the n closest antonyms as the
neighbourhood function V provides the n clos-
est items of a vector. In order to know which
particular meaning of the word we want to op-
pose, we have to assess by what context mean-
ing has to be constrained. However, context is
not always sufficient to give a symmetry axis
for antonymy. Let us consider the item ?father ?.
In the ?family? context, it can be opposite to
?mother ? or to ?children? being therefore ambigu-
ous because ?mother ? and ?children? are by no way
similar items. It should be useful, when context
cannot be used as a symmetry axis, to refine
the context with a conceptual vector which is
considered as the referent. In our example, we
should take as referent ?filiation?, and thus the
antonym would be ?children? or the specialised
similar terms (e.g. ?sons? , ?daughters?) ?marriage?
or ?masculine? and thus the antonym would be
?mother ?.
The function AntiLexS returns the n closest
antonyms of the word A in the context defined
by C and in reference to the word R.
AntiLexS(A,C,R, n)
AntiLexR(A,C, n) = AntiLexS(A,C,C, n)
AntiLexB(A,R, n) = AntiLexS(A,R,R, n)
AntiLexA(A, n) = AntiLexS(A,A,A, n)
The partial function AntiLexR has been de-
fined to take care of the fact that in most cases,
context is enough to determine a symmetry axis.
AntiLexB is defined to determine a symmetry
axis rather than a context. In practice, we have
AntiLexB = AntiLexR. The last function is
the absolute antonymy function. For polysemic
words, its usage is delicate because only one
word defines at the same time three things: the
word we oppose, the context and the referent.
This increases the probability to get unsatis-
factory results. However, given usage habits,
we should admit that, practically, this function
will be the most used. It?s sequence process is
presented in picture 1. We note Anti(A,C) the
ITEMS
ANTONYMOUS
CONCEPTUAL VECTOR
CALCULATION
IDENTIFICATION
OF THE CLOSEST 
ITEMS
neighbourhood
CONCEPTUAL VECTORS
strong contextualisation
CALCULATION
X, C, R
X1, X2, ..., Xn
ITEMS
VAnti
VECTOR
ANTONYMOUS
OF THEanti
Vcx, Vcr
VECTORS
CORRESPONDING
OF THE
Figure 1: run of the functions AntiLex
antonymy function at the vector level. Here,
A is the vector we want to oppose and C the
context vector.
Items without antonyms: it is the case
of material objects like car, bottle, boat, etc.
The question that raises is about the continu-
ity the antonymy functions in the vector space.
When symmetry is at stake, then fixed points
or plans are always present. We consider the
case of these objects, and in general, non op-
posable terms, as belonging to the fixed space
of the symmetry. This allows to redirect the
question of antonymy to the opposable proper-
ties of the concerned object. For instance, if we
want to compute the antonym of a ?motorcycle?,
which is a ROAD TRANSPORT, its opposable prop-
erties being NOISY and FAST, we consider its cat-
egory (i.e. ROAD TRANSPORT) as a fixed point,
and we will look for a road transport (SILEN-
CIOUS and SLOW ), something like a ?bicycle? or
an ?electric car ?. With this method, thanks to
the fixed points of symmetry, opposed ?ideas?
or antonyms, not obvious to the reader, could
be discovered.
4.2 Antonym vectors of concept lists
Anti functions are context-dependent and can-
not be free of concepts organisation. They
need to identify for every concept and for ev-
ery kind of antonymy, a vector considered as
the opposite. We had to build a list of triples
?concept, context, vector?. This list is called
antonym vectors of concept list (AVC).
4.2.1 AVC construction.
The Antonym Vectors of Concepts list is manu-
ally built only for the conceptual vectors of the
generating set. For any concept we can have the
antonym vectors such as:
AntiC(EXISTENCE, V ) = V (NON-EXISTENCE)
AntiC(NON-EXISTENCE, V ) = V (EXISTENCE)
AntiC(AGITATION, V ) = V (INERTIA)? V (REST)
AntiC(PLAY, V ) = V (PLAY)
?V
AntiC(ORDER, V (order) ? V (disorder)) =
V (DISORDER)
AntiC(ORDER, V (classification) ? V (order)) =
V (CLASSIFICATION)
As items, concepts can have, according to
the context, a different opposite vector even
if they are not polysemic. For instance, DE-
STRUCTION can have for antonyms PRESERVA-
TION, CONSTRUCTION, REPARATION or PROTEC-
TION. So, we have defined for each concept, one
conceptual vector which allows the selection of
the best antonym according to the situation.
For example, the concept EXISTENCE has the
vector NON-EXISTENCE for antonym for any con-
text. The concept DISORDER has the vector of
ORDER for antonym in a context constituted by
the vectors of ORDER ?DISORDER5 and has CLAS-
SIFICATION in a context constituted by CLASSI-
FICATION and ORDER.
The function AntiC(Ci, Vcontext) returns for
a given concept Ci and the context defined by
Vcontext , the complementary antonym vector in
the list.
4.3 Construction of the antonym
vector: the Anti Function
4.3.1 Definitions
We define the relative antonymy function
AntiR(A,C) which returns the opposite vec-
tor of A in the context C and the absolute
antonymy function AntiA(A) = AntiR(A,A).
The usage of AntiA is delicate because the lexi-
cal item is considered as being its own context.
We will see in 4.4.1 that this may cause real
problems because of sense selection. We should
stress now on the construction of the antonym
vector from two conceptual vectors: Vitem, for
5? is the normalised sum V = A?B | vi = xi+yi?V ?
the item we want to oppose and the other, Vc,
for the context (referent).
4.3.2 Construction of the Antonym
Vector
The method is to focus on the salient notions in
Vitem and Vc. If these notions can be opposed
then the antonym should have the inverse ideas
in the same proportion. That leads us to define
this function as follows:
AntiR(Vitem, Vc) =
?N
i=1 Pi ?AntiC(Ci, Vc)
with Pi = V 1+CV (Vitem)itemi ?max(Vitemi , Vci)
We crafted the definition of the weight P after
several experiments. We noticed that the func-
tion couldn?t be symmetric (we cannot reason-
ably have AntiR(V(?hot ?),V(?temperature?)) =
AntiR(V(?temperature?),V(?hot ?))). That is why
we introduce this power, to stress more on the
ideas present in the vector we want to oppose.
We note also that the more conceptual6 the vec-
tor is, the more important this power should be.
That is why the power is the variation coeffi-
cient7 which is a good clue for ?conceptuality?.
To finish, we introduce this function max be-
cause an idea presents in the item, even if this
idea is not present in the referent, has to be op-
posed in the antonym. For example, if we want
the antonym of ?cold ? in the ?temperature? con-
text, the weight of ?cold ? has to be important
even if it is not present in ?temperature?.
4.4 Lexical Items and Vectors:
Problem and Solutions
The goal of the functions AntiLex is to return
antonym of a lexical item. They are defined
with the Anti function. So, we have to use tools
which allow the passage between lexical items
and vectors. This transition is difficult because
of polysemy, i.e. how to choose the right relation
between an item and a vector. In other words,
how to choose the good meaning of the word.
4.4.1 Transition lexical items ?
Conceptual Vectors
As said before, antonymy is relative to a con-
text. In some cases, this context cannot be suf-
ficient to select a symmetry axis for antonymy.
6In this paragraph, conceptual means: closeness of a
vector to a concept
7The variation coefficient is SD(V )?(V ) with SD as the
standart deviation and ? as the arithmetic mean.
To catch the searched meaning of the item and,
if it is different from the context, to catch the
selection of the meaning of the referent, we use
the strong contextualisation method. It com-
putes, for a given item, a vector. In this vector,
some meanings are favoured against others ac-
cording to the context. Like this, the context
vector is also contextualised.
This contextualisation shows the problem
caused by the absolute antonymy function
Anti?R . In this case, the method will compute
the vector of the word item in the context item.
This is not a problem if item has only one defini-
tion because, in this case, the strong contextu-
alisation has no effect. Otherwise, the returned
conceptual vector will stress on the main idea it
contains which one is not necessary the appro-
priate one.
4.4.2 Transition Conceptual Vectors ?
Lexical Items
This transition is easier. We just have to com-
pute the neighbourhood of the antonym vector
Vant to obtain the items which are in thematic
antonymy with Vitem. With this method, we
have, for instance:
V(AnticR(death, ?death ? & ?life?))=(LIFE 0.4)
(?killer ? 0.449) (?murderer ? 0.467) (?blood sucker ?
0.471) (?strige? 0.471) (?to die? 0.484) (?to live? 0.486)
V(AnticR(life, ?death ? & ?life?))=(?death ? 0.336)
(DEATH 0.357) (?murdered ? 0.367) (?killer ? 0.377)
(C3:AGE OF LIFE 0.481) (?tyrannicide? 0.516) (?to kill ?
0.579) (?dead ? 0.582)
V(AntiCcA(LIFE))=(DEATH 0.034) (?death ? 0.427)
(C3:AGE OF LIFE 0.551) (?killer ? 0.568) (?mudered ?
0.588) (?tyrannicide? 0.699) (C2:HUMAN 0.737) (?to
kill ? 0.748) (?dead ? 0.77)
It is not important to contextualise the con-
cept LIFE because we can consider that, for ev-
ery context, the opposite vector is the same.
In complementary antonymy, the closest item
is DEATH. This result looks satisfactory. We can
see that the distance between the antonymy vec-
tor and DEATH is not null. It is because our
method is not and cannot be an exact method.
The goal of our function is to build the best
(closest) antonymy vector it is possible to have.
The construction of the generative vectors is the
second explanation. Generative vectors are in-
terdependent. Their construction is based on an
ontology. To take care of this fact, we don?t have
boolean vectors, with which, we would have ex-
actly the same vector. The more polysemic the
term is, the farthest the closest item is, as we
can see it in the first two examples.
We cannot consider, even if the potential of
antonymy measure is correct, the closest lexical
item from Vanti as the antonym. We have to
consider morphological features. Simply speak-
ing, if the antonym of a verb is wanted, the re-
sult would be better if a verb is caught.
4.5 Antonymy Evaluation Measure
Besides computing an antonym vector, it seems
relevant to assess wether two lexical items can
be antonyms. To give an answer to this ques-
tion, we have created a measure of antonymy
evaluation. Let A and B be two vectors.
The question is precisely to know if they can
reasonably be antonyms in the context of C.
The antonymy measure MantiEval is the an-
gle between the sum of A and B and the sum
of AnticR(A,C) and AnticR(B,C). Thus, we
have:
MantiEval = DA(A?B,AntiR(A,C)?AntiR(B,C))
A+B
A
B
Anti(A,C)
Anti(B,C)
Anti(A,C)+Anti(B,C)
Figure 2: 2D geometric representation of the antonymy
evaluation measure MantiEval
The antonymy measure is a pseudo-distance.
It verifies the properties of reflexivity, symme-
try and triangular inequality only for the subset
of items which doesn?t accept antonyms. In this
case, notwithstanding the noise level, the mea-
sure is equal to the angular distance. In the
general case, it doesn?t verify reflexivity. The
conceptual vector components are positive and
we have the property: Distanti ? [0, pi2 ]. The
smaller the measure, the more ?antonyms? the
two lexical items are. However, it would be a
mistake to consider that two synonyms would be
at a distance of about pi2 . Two lexical items atpi
2 have not much in common8. We would rather
see here the illustration that two antonyms
share some ideas, specifically those which are
not opposable or those which are opposable with
a strong activation. Only specific activated con-
cepts would participate in the opposition. A
distance of pi2 between two items should rather
be interpreted as these two items do not share
much idea, a kind of anti-synonymy. This re-
sult confirms the fact that antonymy is not the
exact inverse of synonymy but looks more like a
?negative synonymy? where items remains quite
related. To sum up, the antonym of w is not
a word that doesn?t share ideas with w, but a
word that opposes some features of w.
4.5.1 Examples
In the following examples, the context has been
ommited for clarity sake. In these cases, the
context is the sum of the vectors of the two
items.
MantiEval(EXISTENCE,NON-EXISTENCE) = 0.03
MantiEvalC(?existence?, ?non-existence?) = 0.44
MantiEvalC(EXISTENCE, CAR) = 1.45
MantiEvalC(?existence?, ?car ?) = 1.06
MantiEvalC(CAR, CAR) = 0.006
MantiEvalC(?car ?, ?car ?) = 0.407
The above examples confirm what pre-
sented. Concepts EXISTENCE and NON-
EXISTENCE are very strong antonyms in comple-
mentary antonymy. The effects of the polysemy
may explain that the lexical items ?existence? and
?non-existence? are less antonyms than their re-
lated concepts. In complementary antonymy,
CAR is its own antonym. The antonymy mea-
sure between CAR and EXISTENCE is an exam-
ple of our previous remark about vectors shar-
ing few ideas and that around pi/2 this mea-
sure is close to the angular distance (we have
DA(existence, car) = 1.464.). We could con-
sider of using this function to look in a concep-
tual lexicon for the best antonyms. However,
the computation cost (around a minute on a P4
at 1.3 GHz) would be prohibitive.
8This case is mostly theorical, as there is no language
where two lexical items are without any possible relation.
5 Action on learning and method
evaluation
The function is now used in the learning process.
We can use the evaluation measure to show the
increase of coherence between terms:
MantiEvalC new old
?existence?, ?non-existence? 0.33 0.44
?existence?, ?car ? 1.1 1.06
?car ?, ?car ? 0.3 0, 407
There is no change in concepts because they are
not learned. In the opposite, the antonymy eval-
uation measure is better on items. The exemple
shows that ?existence? and ?non-existence? have
been largely modified. Now, the two items are
stronger antonyms than before and the vector
basis is more coherent. Of course, we can test
these results on the 71000 lexical items which
have been modified more or less directly by the
antonymy function. We have run the test on
about 10% of the concerned items and found an
improvement of the angular distance through
MantiEvalC ranking to 0.1 radian.
6 Conclusion
This paper has presented a model of antonymy
using the formalism of conceptual vectors. Our
aim was to be able: (1) to spot antonymy if
it was not given in definition and thus provide
an antonym as a result, (2) to use antonyms
(discovered or given) to control or to ensure the
coherence of an item vector, build by learning,
which could be corrupted. In NLP, antonymy is
a pivotal aspect, its major applications are the-
matic analysis of texts, construction of large lex-
ical databases and word sense disambiguation.
We grounded our research on a computable lin-
guisitic theory being tractable with vectors for
computational sake. This preliminary work on
antonymy has also been conducted under the
spotlight of symmetry, and allowed us to express
antonymy in terms of conceptual vectors. These
functions allow, from a vector and some contex-
tual information, to compute an antonym vec-
tor. Some extensions have also been proposed so
that these functions may be defined and usable
from lexical items. A measure has been identi-
fied to assess the level of antonymy between two
items. The antonym vector construction is nec-
essary for the selection of opposed lexical items
in text generation. It also determines opposite
ideas in some negation cases in analysis.
Many improvements are still possible, the
first of them being revision of the VAC lists.
These lists have been manually constructed by
a reduced group of persons and should widely be
validated and expanded especially by linguists.
We are currently working on possible improve-
ments of results through learning on a corpora.
References
Jacques Chauche?. De?termination se?mantique
en analyse structurelle : une expe?rience base?e
sur une de?finition de distance. TAL Informa-
tion, 1990.
Scott C. Deerwester, Susan T. Dumais,
Thomas K. Landauer, George W. Furnas, and
Richard A. Harshman. Indexing by latent se-
mantic analysis. Journal of the American So-
ciety of Information Science, 41(6):391?407,
1990.
Mathieu Lafourcade. Lexical sorting and lexical
transfer by conceptual vectors. In Proceeding
of the First International Workshop on Mul-
tiMedia Annotation, Tokyo, January 2001.
Larousse. The?saurus Larousse - des ide?es aux
mots, des mots aux ide?es. Larousse, 1992.
Mathieu Lafourcade and Violaine Prince. Syn-
onymies et vecteurs conceptuels. In actes de
TALN?2001, Tours, France, July 2001.
John Lyons. Semantics. Cambridge University
Press, 1977.
Igor Mel?c?uk, Andre? Clas, and Alain Polgue`re.
Introduction a` la lexicologie explicative et
combinatoire. Duculot, 1995.
Emmanuel Morin. Extraction de liens
se?mantiques entre termes a` partir de
corpus techniques. PhD thesis, Universite? de
Nantes, 1999.
Victoria Lynn Muehleisen. Antonymy and se-
mantic range in english. PhD thesis, North-
western university, 1997.
F.R. Palmer. Semantics : a new introduction.
Cambridge University Press, 1976.
P. Roget. Roget?s Thesaurus of English Words
and Phrases. Longman, London, 1852.
Gerard Salton and Michael McGill. Introduc-
tion to Modern Information Retrieval. Mc-
GrawHill, 1983.
Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers,
pages 365?375, Dublin, Ireland, August 23-29 2014.
Inferring Knowledge with Word Refinements
in a Crowdsourced Lexical-Semantic Network
Manel Zarrouk
UM2-LIRMM
161 rue Ada
34095 Montpellier, FRANCE
manel.zarrouk@lirmm.fr
Mathieu Lafourcade
UM2-LIRMM
161 rue Ada
34095 Montpellier, FRANCE
mathieu.lafourcade@lirmm.fr
Abstract
Automatically inferring new relations from already existing ones is a way to improve the qual-
ity and coverage of a lexical network and to perform error detection. In this paper, we devise
such an approach for the crowdsourced JeuxDeMots lexical network and we focus especially
on word refinements. We first present deduction (generic to specific) and induction (specific
to generic) which are two inference schemes ontologically founded and then propose a trans-
fer schema devoted to infer relations with and for word refinements.
1 Introduction
Efficiently building useful resources for Computational Linguistics (CL) is of a crucial interest. Most
of existing lexical-semantic networks have been built by hand (like for instance WordNet (Miller et
al., 1990)) and, despite that assisting tools are generally designed for consistency checking, the task
remains time consuming and costly. Fully automated approaches are generally limited to term co-
occurrences as extracting precise semantic relations between terms from corpora remains at best
difficult. Crowdsourcing approaches are flowering in CL especially with the advent of Amazon Me-
chanical Turk or in a broader scope Wikipedia, to cite the most well known examples. WordNet is such
a lexical network, constructed at great cost, based on synsets which can be roughly considered as con-
cepts (Fellbaum, 1988). EuroWordnet (Vossen., 1998) a multilingual version of WordNet and WOLF
(Sagot., 2008) a French version of WordNet, were built by automated crossing of the original Princeton
WordNet and other lexical resources along with some more or less manual checking. Navigli (2010)
constructed automatically BabelNet a large multilingual lexical network from term co-occurrences in
Wikipedia. Although being very large and multilingually connected (which is tremendously usefull
for machine translation, for instance) it contains few various lexical-semantic relations.
An ideal lexical-semantic network contains interconnected lemmas, word forms and multi-word
expressions as entry points (nodes) along with word meanings and concepts. The idea itself of word
senses as forwarded in the lexicographic tradition may be debatable in the context of resources for se-
mantic analysis, and we generally prefer to consider the psycholinguistic idea of word usages. A given
polysemous word, as identified by locutors, has several usages that might differ substantially from
word senses as classically defined. A given usage can also in turn have several deeper refinements
and the whole set of usages can take the form of a decision tree. For a very classical example, bank
can be related to money or river : bank m ?bank>money? and bank m ?bank>river?. A ?bank>money?
can be distinguished as the financial institution or the actual building.
In the context of a collaborative construction, such a lexical resource should be considered as being
constantly evolving and a general pragmatic rule of thumb is to have no definite certitude about the
state of an entry. For a polysemous term, some refinements might be just missing at a given time
This work is licensed under a Creative Commons Attribution 4.0 International Licence. Page numbers and proceedings
footer are added by the organisers. Licence details: http://creativecommons.org/licenses/by/4.0/
365
notwithstanding the evolution of language which might be very fast, especially in technical domains.
There is no way (unless by inspection) to know if a given entry refinements are fully completed, and
even if this question is really relevant.
Creating collaboratively a lexical-semantic network (or, in all generality, any similar resource) can
be devised according to two broad strategies. Firstly, it can be designed as a contributive system like
Wikipedia where people willingly add and complete entries (like for Wiktionary). Secondly, contribu-
tion can be undertaken indirectly thanks to games (also known as GWAP (vonAhn, 2008)). In this case,
players do not need to be aware that while playing they are helping building a lexical and semantic
resource. In any case, the built network is not free of errors which are (or should be) corrected along
their discovery. Thus, a large number of obvious relations may be missing in the lexical network but
are indeed necessary for a high quality resources usable in various NLP applications, or even crucial
notably for textual semantic analysis.
For example, contributors seldomly indicate that a particular bird type can fly, as it is considered
as an obvious generality. Only notable facts which are not easily deductible are naturally contributed.
Conversly, well known exceptions are also generally contributed and take the form of a negative
weight and anotated as such (for example, fly
ag ent :?100
???????? ostrich [exception: bird]). In order to con-
solidate the lexical network, we adopt a strategy based on a simple inference mechanism to propose
new relations from those already existing. The approach is strictly endogenous (i.e. self-contained)
as it doesn?t rely on any other external resources. Inferred relations are submitted either to contrib-
utors for voting or to experts for direct validation/invalidation. A large percentage of the inferred
relations has been found to be correct However, a non negligible part of them are found to be wrong
and understanding why is both interesting and useful. The explanation process can be viewed as
a reconciliation between the inference engine and contributors who are guided through a dialog to
explain why they found the considered relation incorrect. The possible causes for a wrong inferred
relation may come from three possible origins: false premises that were used by the inference engine,
exception or confusion due to some polysemy.
In (Sajous et al., 2013) an endogenous enrichment of Wiktionary is done thanks to a crowdsourcing
tool. A quite similar approach of using crowdsourcing has been considered by (Zeichner, 2012) for
evaluating inference rules that are discovered from texts. In (Krachina, 2006), some specific inference
methods are conducted on text with the help of an ontology. Similarly, (Besnard, 2008) capture expla-
nation with ontology-based inference. OntoLearn (Velardi, 2006) is a system that automatically build
ontologies of specific domains from texts and also makes use of inferences. There have been also
researchs on taxonomy induction based on WordNet (see (Snow, 2006)). Although extensive work on
inference from texts or handcrafted resources has been done, almost none endogenously on lexical
network built by the crowds. In this article, we first present the principles behind the lexical network
construction with crowdsourcing and games with a purpose (also known as human-based computa-
tion games) and illustrated them with the JeuxDeMots (JDM) project. Then, we present the outline of
an elicitation engine based on an inference engine using deduction, induction and especially relation
transfer schemes. The reconciliation engine which presents the second part of the elicitation engine
is detailed on previous papers (Zarrouk, LREC2014) (Zarrouk, TALN2013). An experimentation with a
discussion is then detailed.
2 Crowdsourced lexical networks
For validating our approach, we used the JDM lexical network, which has been made freely available
by its authors, and constructed thanks to a set of associatory games (Lafourcade, 2007). There is an
increasing trend of using online GWAPs (game with a purpose (Thaler et al., 2011)) method for feeding
such resources. Beside manual or automated strategies, contributive approaches are flowering and
becoming more and more popular as they are both cheap to set up and efficient in quality.
The network is composed of terms (as vertices) and typed relations (as links between vertices) with
weights. It contains terms and possible refinements. There are more than 50 types for relations, that
range from ontological (hypernym, hyponym), to lexical-semantic (synonym, antonym) and to se-
366
mantic role (agent, patient, instrument). The weight of a relation is interpreted as a strength, but
not directly as a probability of being valid. The JDM network is not an ontology with some pris-
tine, factorized and well-thought hierarchy of concepts or terms. A given term can have a substan-
tial set of hypernyms that covers a large part of the ontological chain to upper concepts. For exam-
ple, hypernym(cat) = {feline,mammal, living being,pet,vertebrate, ...}. Heavier weights associated to
terms are those felt by users as being the most relevant. On the 1st of January 2014, there are more
than 6 800 000 relations and roughly 310 000 lexical items in the JDM lexical network (according to the
figures given by the game site: http://jeuxdemots.org). To our knowledge, there is no other, in French
at least, existing freely available crowdsourced lexical-network, especially with weighted relations,
thus enabling strongly heuristics or psycho-linguistically motivated methods.
3 Inferring Semantic Relations...
Adding new relations to the JDM lexical network may rely on two components: (a) an inference en-
gine and (b) a reconciliator. The inference engine proposes relations as if it was a contributor, to be
validated by other human contributors or experts. In case of invalidation of an inferred relation, the
reconciliator is invoked to try to assess why the inferred relation was found wrong. Elicitation here
should be understood as the process to transform some implicit knowledge of the user into explicit
relations in the lexical network. The core ideas about inferences in our engine are the following:
? inferring is to derive new premises (taking the form of relations between terms) from previously
known premises, which are existing relations;
? candidate inferences may be logically blocked on the basis of the presence or the absence of
some other relations;
? candidate inferences can be filtered out on the basis of a strength evaluation. The strong as-
sumption here is to consider strengh as a confidence level, which is in fact only partially ex-
act. More precisely, high strengh values clearly correlate to confidence, but we cannot say much
about low strength values.
Figure 1: On the left, triangular deductive inference scheme where logical blocking based on the pol-
ysemy of the central term B which has two distinct meanings B ? and B ?? is applied. Arrows labelledm
are word meaning/refinements. The relation R? is the conclusion that may be blocked. On the right,
(A is-a B) and (A R C) are the premises, and (B R C) is the induction proposed for validation. Term A
may be polysemous with refinements holding premises, thus inducing a probably wrong relation.
3.1 ... by Deduction and by Induction...
Inferring by deduction (Zarrouk, RANLP2013) is a top-down scheme based on the transitivity of the
ontological relation is-a (hypernym). If a term A is a kind of B and B holds some relation R with C,
then we can expect that A holds the same relation type with C. The scheme can be formally written as
follows:
? A i s?a???? B ? ? B R???? C ? A R???? C
For example, shark i s?a???? fish and fish
has?par t
???????? fin, thus we can expect that shark
has?par t
???????? fin.
The inference engine is applied on terms having at least one hypernym (the scheme could not be
applied otherwise). Of course, this scheme is far too naive, especially considering the resource we are
dealing with and may produce wrong relations. Indeed, the central term B is possibly polysemous
and ways to avoid probably wrong inferences can be done through a logical blocking: if there are two
distinct meanings for B that hold respectively the first and the second relation, then most probably
367
the inferred relation is wrong (see figure 1) and hence should be blocked. Moreover, if one of the
premises is tagged by contributors as true but irrelevant, then the inference is blocked.
It is possible to evaluate a confidence level (on an open scale) for each produced inference, in a
way that dubious inferences can be eliminated out through statistical filtering. The weight w of an
inferred relation is the geometric mean of the weights of the premises (relations (A is-a B) and (B R C)
in figure 1). If the second premise has a negative value, the weight is not a number and the proposal is
discarded. As the geometric mean is less tolerant to small values than the arithmetic mean, inferences
which are not based on two rather strong relations (premises) are unlikely to pass.
w(A R???? C) = ( w(A i s?a???? B) ? w(B R???? C) )1/2 ? w3 = (w1 ? w2)1/2
Although making a transitive closure over a knowledge base is not new, doing so considering word
usages (refinements) over a crowdsourced lexical network is an original approach. As for the deduc-
tive inference, induction (Zarrouk, RANLP2013) exploits the transitivity of the relation is-a. If a term
A is a kind of B and A holds a relation R with C , then we might expect that B could hold the same type
of relation with C . More formally we can write: ? A i s?a???? B ? ? A R???? C ? B R???? C
For example, shark i s?a???? fish and shark
has?par t
?????? jaw, thus we might expect that fish
has?par t
?????? jaw. This
scheme is a generalization inference. The principle is similar to the one applied to the deduction
scheme and similarly some logical and statistical filtering may be undertaken. The central term here
A, is possibly polysemous (as shown in figure 1). In that case, we have the same polysemy issues with
the deduction, and the inference may be blocked. The estimated weight for the induced relation is:
w(B R?? C) = (w(A R?? C))2 / w(A i s?a???? B) ? w2 = (w3)2/w1
3.2 ... and Performing Reconciliation
Inferred relations are presented to the validator to decide of their status. In case of invalidation, a
reconciliation procedure is launched in order to diagnose the reasons: error in one of the premises
(previously existing relations are false), exception or confusion due to polysemy (the inference has
been made on a polysemous central term). A dialog is initiated with the user. To know in which order
to proceed, the reconciliator checks if the weights of the premises are rather strong or weak.
Errors in the premises. We suppose that the relation (A is-a B) (in figures 1) has a relatively low
weight. The reconciliation process asks the validator if that relation is true. It sets a negative weight
to this relation if it is false so that the inference engine blocks further inferences. Else, if the relation
(A is-a B) is true, we ask about the second relation (B R C or A R C) and proceed as above if the answer
is negative. Otherwise, we check the other cases (exception, polysemy).
Errors due to exceptions. For the deduction, in case we have two trusted relations, the reconcilia-
tion process asks the validators if the inferred relation is a kind of exception relatively to the term B .
If it is the case, the relation is stored in the lexical network with a negative weight and annotated as
exception. Relations that are exceptions do not participate further as premises for deducing. For the
induction, in case we have two trusted relations, the reconciliator asks the validators if the relation (A
R???? C) (which served as premise) is an exception relatively to the term B . If it is the case, in addi-
tion to storing the false inferred relation (B R???? C) in the lexical network with a negative weight, the
relation (A R???? C) is annotated as exception. In the induction case, the exception is a true premise
which leads to a false induced relation. In both cases of induction and deduction, the exception tag
concerns always the relation (A R???? C). Once this relation is annotated as an exception, it will not
participate as a premise in inferring generalized relations (bottom-up model) but can still be used in
inducing specified relations (top-down model).
Errors due to Polysemy. If the central term (B for deduction and A for induction) presenting a pol-
ysemy is mentioned as polysemous in the network, the refinement terms ter m1, ter m2, . . . ter mn
are presented to the validator so he can choose the appropriate one. The validator can propose new
terms as refinements if he is not satisfied with the listed ones (inducing the creation of new appro-
priate refinements). If there is no meta information indicating that the term is polysemous, we ask
368
Figure 2: Refinement (noted m) tree of the term frigate.
The first level discriminates between frigate>bird
and frigate>boat which itself is refined between
(frigate>boat)>ancient and (frigate>boat)>modern.
This tree is a part of the lexical network which makes
use of a specific refinement relation. Each refinement is
connected to other terms of the network.
first the validator if it is indeed the case. After this procedure, new relations will be included in the
network with positive values and the inference engine will use them later on as premises.
3.3 Transferring Relations with Refinements
A given polysemous word, as identified by locutors, has several usages that might differ substantially
from word senses as classically defined. A given usage can also in turn have several deeper refine-
ments and the whole set of usages can take the form of a decision tree. For example, frigate can
be a bird or a ship. A frigate>boat can be distinguished as a modern ship with missiles and radar
(frigate>boat>modern) or an ancient vessel with sails (frigate>boat>ancient). Having proper rela-
tions between refinements and other terms or refinements is crucial for word sense disambiguation.
The purpose of this scheme is to enrich refinements and terms that are ontologically connected. As
its name indicates, this scheme requires the term A to have at least a refinement A? and at least one
support relation that is ontological. The Relation Inference Scheme with Refinements (RI SR ) scheme,
for each synonym, hypernym or hyponym (the support) B of the start term A, tries to share the outgo-
ing relations between A? and B . The relations exchanged are the inferred relations to be validated or
rejected latterly. To increase the relevance of the proposed relations, we make sure that some relation
exists between the refinement term A? and the term B . For example, suppose we have A: r ose which
has two refinements at least A?: rose>flower and rose>color and a hypernym B : pl ant . In this exam-
ple, the terms A?: rose>flower and B : pl ant are related (some relation exists between them) unlike
the terms A?: rose>color and B : pl ant . This strategy avoid proposing for example rose>color
has?par t
??????
leaf (an outgoing relation coming from B).
Figure 3: Relation Inference Scheme with Refinements (RI SR ). Above A (resp. B) has a refinement A?
(resp. B ?). Outgoing relations of A? are copied as outgoing relations of B ? and vice-versa, according to
the support relation (syn, hyper, hypo). On the right, we are in a minimal situation where B has no
refinement.
Another strategy is not to propose outgoing relations from an hypernym to its hyponyms. The
direction of the transfer is always from the hyponym to the hypernym because generally, outgoing
relations of an hypernym are not all valid for its hyponyms. For example, for the term A: animal
having a refinement A?: animal>zoology which can have as parts fin, scale, fang... Those relations x
has?par t
?????? (fin, scale, fang) are not valid for the hyponym cow, for example.
This scheme has a behavior subtly different according to the nature of the term B (synonym, hy-
pernym or hyponym) relatively to A. In figure 3, we use the following notations:
? A# B: propose all the outgoing relations of A as outgoing relations for the term B (other notation
as C to copy relations and D to displace them are available but not used here);
? A ???? B: a relation between A and B in any direction exists.
369
4 Experimentations and Discussion
Our experiments consisted in applying and assessing the schemes presented above on the entire lex-
ical network. This has been once during one run. At the time of writing of this article, the JeuxDeMots
consists in more than 6 800 000 relations betweeen 310 000 terms. Specifically, it contains over 150 000
hypernym is-a relations, 170 000 syn relations and 27 000 hyponym relations.
Relation type Proposed %
is-a (x is a type of y) 6.2
has-parts (x is composed of y) 25
holonyms (y specific of x) 7.2
typical place (of x) 7.2
charac (x as characteristic y) 13.7
agent-1 (x can do y) 13.3
instr-1 (x instrument of y) 1.7
patient-1 (x can be y) 1
place-1 (x located in the place y) 9.8
place > action (y can be done in place x) 3.4
object > mater (x is made of y) 0.3
Table 1: Percentages of relation proposed per relation type globally for deduction and induction.
4.1 Assessing Deduction and Induction
We applied the inference engine on around 32 000 randomly selected terms having at least one hyper-
nym or one hyponym and thus produced by deduction more than 2 700 000 inferences and produced
by induction over 430 000 relation candidates. The threshold for filtering was set to a weight of 25.
This value is relevant as when a human contributor proposed relation is validated by experts, it is
introduced with a default weight of 25 (the choice of this particular value is arbitrary and could have
been different). The transitive is-a (Table1) is not very productive which might seem surprising at
first glance. In fact, the is-a relation is already quite populated in the network, and as such, fewer
new relations can be inferred. The figures are inverted for some other relations that are not so well
populated in the lexical network but still are potentially valid. The has-parts relation and the agent
semantic role (the agent-1 relation) are by far the most productive types.
Table 2: On the left, number of propositions produced by deduction and ratio of relations found as
true or false. On the right, Number of propositions produced by induction and ratio of relations found
as true or false.
Table 2 presents some evaluations of the status of the inferences proposed by the inference en-
gine through deduction and induction respectively. Inferences are valid for an overall of 80-90% with
around 10% valid but not relevant (like for instance dog
has?par t s
???????? proton). We observe that error
number in premises is quite low, and errors can be easily corrected. Of course, not all possible errors
are detected through this process. More interestingly, the reconciliation allows in 5% of the cases to
370
RI SR # existed # proposed productivity
syn 38 792 105 288 271.41%
hyper 139 490 101 908 73.05%
hypo 38 756 101 336 261.47%
Table 3: The number of relations existing before ap-
plication of the scheme and those proposed by the
scheme. The statistics were made on the terms on
which the scheme has proposed inferences
identify polysemous terms and refinements. Globally false negatives (inferences voted false while be-
ing true) and false positives (inferences voted true while being false) are evaluated to less than 0,5%.
For the induction process, the relation is-a is not obvious (a lexical network is not reductible to an
ontology and multiple inheritance is possible). Result seems about 5% better than for the deduction
process: inferences are valid for an overall of 80-95%. The error number is quite low. The main dif-
ference with the deduction process is on errors due to polysemy which is lower with the induction
process. To try to assess a baseline for those results, we compute the full closure of the lexical net-
work, i.e. we produce iteratively all possible candidate relations until no more could be found, each
candidate being considered as correct and participating to the process. We got more than 6 million
relations out of which 45% were wrong (evaluated on around 1 000 candidates randomly chosen).
4.2 Assessing Relation Transfer
We applied the scheme of refinements relation transfer with three different support relations:
? RI SR (synonym): the scheme applied with syn as support (in case of existence of B ? the terms A?
and B ? share relations.)
? RI SR (hyponym): the scheme applied with hypo (relations are shared from B or B ? to A?)
? RI SR (hypernym): the scheme applied with R=hyper (relations are shared from A? to B or B ?).
RI SR stands for Relation Inference Schema with Refinements.
Table 4: On the left, relations proposed by type of the support relation and relation type of the con-
clusion. On the right, percentage of valid relations by type of the support relation and relation type of
the conclusion.
Relation Transfer Productivity - Since the schema has a condition to be applied, the propositions
(inferred relations) are made for only 6 349 terms fullfilling the constraints. The whole process pro-
duced 308 532 inferences presenting totally new relations not existing before in the network which
make about 49 new relations per entry. The RI SR (syn) produced 2.7 times the existing relations
which make it the most productive version, followed by the RI SR (hypo) producing 2.6 times and
the RI SR (hyper) with a productivity of 0.73 (table 3). The inferred relations are detailed by relation
type in the left table 4. The different relation types are variously productive, and this is mainly due to
the number of existing relations and the distribution of their type. The "associated" type is the most
proposed from both three schemes and this is explained by the large semantic spectre of this relation
type since it refers to every term associated to the target term. In the network, the most possessed
relations of a term are typed with the associated relations. The amount of the relations proposed is
related to the one existing in the network. If a relation type is quite populated in the network, fewer
new relations can be inferred. The figures are inverted for some other relations that are not so well
populated in the lexical network but still are potentially valid.
371
Relation Transfer Accuracy - The validation process was applied manually on a sample of around
1 000 propositions randomly choosen for each scheme. The synonym version has the highest ac-
curacy with 90.76 % valid relations, hypernym version with 72.69 % and 66.24 % for the hyponym
version (table 4). The synonym version of the scheme has systematically the best accuracy for all the
relation types. Some accuracy percentages are lower than others for some reasons. In certain cases,
some outgoing relations of an hyponym do not suit for the hypernym. For example:
?A: animal ?A?: animal>animalia ?B(hy po): cat
? The inference scheme will propose the outgoing relation of cat (cat i s?a?????? pet) to
ani mal>ani mali a (animal>animalia i s?a?????? pet) which is wrong and this explain the weak per-
centage of accuracy for example of the relation is-a (56.4% by the RI SR (hypo) and 46% by the
RI SR (hyper)) and has?par t (46.9% by the RI SR (hypo)).
Another reason is that in the network, some terms are not refined (or not completely refined) which
can lead to some wrong relations, as for example: ?A: cheese ?A?: cheese>dairy product ?B(hy po):
goat 1
? The inference scheme will propose the relation (cheese>dairy product
has?par t
?????? teats) which is
wrong and thus because the term g oat is not yet refined into goat>dairy product and goat>animal.
From the figures, we can make the following observations. First, global results show that produced
inferences are strongly valid with synonyms. The results are poorer with hypernyms and hyponyms
(table 4) which is obvious regarding that with synonym, the terms exchanging relations are roughly
at the same level of the taxonomic hierarchy which is not the case when they are related with an
hyponym or hypernym relation.
5 Conclusion
We have presented some issues in inferring new relations from existing ones to consolidate a lexical-
semantic network built with games and user contributions. To be able to enhance the network qual-
ity and coverage, we proposed an elicitation engine based on inferences (induction, deduction and
relation transfer with refinements) and reconciliation. If an inferred relation is proven wrong, a rec-
onciliation process is conducted in order to identify the underlying cause and solve the problem.
We focused our work on the transfer of relations related to word usage (refinements) with help of
a support relation being either synonym, hypernym or hyponym. Unlike deduction and induction,
the transfer scheme does not rely directly on the relation (is-a), but merely on terms that may be
ontologicaly connected to the target. Experiments showed that relation transfer for refinements is
quite productive (compared to deduction and induction), and is satisfying in correctness especially
with synonym as support relation. The most obvisous reason is that in general a (quasi-)synonym is
almost at the same level with the target term, and at least much more often than a hypernym or hy-
ponym. User evaluation showed that wrong inferred relations (between around 20-15% of all inferred
relations) are still logically sound and could not have been dismissed a priori. Relation transfer with
refinements can conclusively be considered as a usefull and efficient tool for relation inference, and
it may be really crucial as support for building information to be used in word sense disambiguation.
In particular, it can help proposing hypernyms for the target term when they are missing, making
possible further deductions or inductions. Hence, a virtuous circle may be initiated.
Still, the main difficulty of such approach relies in setting the various parameters in order to achieve
an appropriate and fragil tradeoff between an over-restrictive filter (many false negatives, resulting in
information losses) and a too lenient engine (many false postive, resulting in more human effort).
The elicitation engine we presented through schemes based on deduction, induction and more pre-
cisely on relation transfer is an efficient error detector and a polysemy identifier. The actions taken
during the reconciliation forbid an inference proven wrong or exceptional to be inferred again. Each
inference scheme may be supported by the two others in particular for refinements, and if a given
inference has been produced by more than one of these three schemes, it is almost surely correct.
1In french, some dairy products are called sometimes by the name of the producer animal, like chevr e(g oat ) for the
cheese made from the goat?s milk
372
An additional inference scheme, abduction, reinforced our inference engine and guided it through
producing accurate new relations with an interesting accuracy. This scheme can be viewed as an ex-
ample based strategy. Hence abduction relies on similarity between terms, which may be formalized
in our context as sharing some outgoing relations between terms. The abductive inferring layout sup-
poses that relations held by a term can be proposed to similar terms. Abduction first selects a set of
similar terms to the target term A which are considered as proper examples. The outgoing relations
from the examples which are not common with those of A are proposed as potential relations for A
and then presented for validation/invalidation to users. Unlike induction and deduction, abduction
can be applied on terms with missing or irrelevant ontological relations, and can generate ontologi-
cal relations to be used afterward by the inference loop. This scheme was detailed in our paper (M.
Zarrouk, EACL2014).
Researches are undertaken on (semi)automating the inference schemes or inference rules (scheme
with just one or two unknown terms) discovery by our elicitation system. Enhancements are also con-
sidered on our previous schemes as for exemple defining the inference?s scope especially in deduction
and induction (example: what to do to avoid transferring invalid inferences from the term animal as
has-part wings to its hyponyms like cat or fish).
We are also modelling a declarative query language that allows users to manipulate the lexical-
semantic network and to apply our elicitation engine according to their needs while remaining fo-
cused on their request and without drifting in database access or linguistic domain.
373
References
von Ahn, L. and Dabbish, L. 2008. Designing games with a purpose. in Communications of the ACM, number 8,
volume 51.p58-67.
Besnard, P. Cordier, M.O., and Moinard, Y. 2008. Ontology-based inference for causal explanation.. Integrated
Computer-Aided Engineering , IOS Press, Amsterdam , Vol. 15 , No. 4 , 351-367 , 2008.
Fellbaum, C. and Miller, G. 1988. (eds) WordNet.. The MIT Press.
Krachina, O., Raskin, V. 2006. Ontology-Based Inference Methods. CERIAS TR 2006-76, 6p.
Lafourcade, M. 2007. Making people play for Lexical Acquisition.. In Proc. SNLP 2007, 7th Symposium on
Natural Language Processing. Pattaya, Thailande, 13-15 December. 8 p.
Lafourcade, M., Joubert, A. 2008. JeuxDeMots : un prototype ludique pour l??l?mergence de relations entre
termes.. In proc of JADT?2008, Ecole normale sup?l?rieure Lettres et sciences humaines , Lyon, France, 12-14
mars 2008 .
Lafourcade, M., Joubert, A. 2012. Long Tail in Weighted Lexical Networks.. In proc of Cognitive Aspects of the
Lexicon (CogAlex-III), COLING, Mumbai, India, December 2012.
Lieberman, H, Smith, D. A and Teeters, A 2007. Common consensus: a web-based game for collecting common-
sense goals.. In Proc. of IUI, Hawaii,2007.12p .
Marchetti, A and Tesconi, M and Ronzano, F and Mosella, M and Minutoli, S. 2007. SemKey: A Semantic
Collaborative Tagging System.. in Procs of WWW2007, Banff, Canada. 9 p.
Mihalcea, R and Chklovski, T. 2003. Open MindWord Expert: Creating large annotated data collections with
web users help.. In Proceedings of the EACL 2003, Workshop on Linguistically Annotated Corpora (LINC). 10
p.
Miller, G.A. and Beckwith, R. and Fellbaum, C. and Gross, D. and Miller, K.J. 1990. Introduction to WordNet: an
on-line lexical database.. International Journal of Lexicography. Volume 3, p 235-244.
Navigli, R and Ponzetto, S. 2010. BabelNet: Building a very large multilingual semantic network.. in Proceed-
ings of the 48th Annual Meeting of the Association for Computational Linguistics, Uppsala, Sweden, 11-16
July 2010.p 216-225.
Sagot, B. and Fier, D. 2010. Construction d?un wordnet libre du fran???ais ??? partir de ressources multilingues..
in Proceedings of TALN 2008, Avignon, France, 2008.12 p.
Thaler, S and Siorpaes, K and Simperl, E. and Hofer, C. 2011. A Survey on Games for Knowledge Acquisition..
STI Technical Report, May 2011.19 p.
Sajous, F., Navarro, E., Gaume, B,. Pr???vot, L. and Chudy, Y. 2013. Semi-Automatic Enrichment of Crowdsourced
Synonymy Networks: The WISIGOTH system applied to Wiktionary.. Language Resources & Evaluation, 47(1),
pp. 63-96.
Siorpaes, K. and Hepp, M. 2008. Games with a Purpose for the Semantic Web.. in IEEE Intelligent Systems,
number 3, volume 23.p 50-60.
Snow, R. Jurafsky, D., Y. Ng., A. 2006. Semantic taxonomy induction from heterogenous evidence. in Proceedings
of COLING/ACL 2006, 8 p.
Velardi, P. Navigli, R. Cucchiarelli, A. Neri, F. 2006. Evaluation of OntoLearn, a methodology for Auto-
matic Learning of Ontologies. in Ontology Learning and Population, Paul Buitelaar Philipp Cimmiano and
Bernardo Magnini Editors, IOS press 2006).
Vossen, P. 2011. EuroWordNet: a multilingual database with lexical semantic networks.. Kluwer Academic
Publishers.Norwell, MA, USA.200 p.
Zarrouk, M., Lafourcade, M. and Joubert, A. 2013. Inference and reconciliation in a lexical-semantic network.
14th International Conference on Intelligent Text Processing and Computational Linguistic (CICLING-2013),
13 p.
Zarrouk, M., Lafourcade, M. and Joubert, A. 2013. Inductive and deductive inferences in a Crowdsourced
Lexical-Semantic Network. 9th International Conference on Recent Advances in Natural Language Process-
ing (RANLP 2013), 6 p.
374
Zarrouk, M., Lafourcade, M. and Joubert, A. 2014. About Inferences in a Crowdsourced Lexical-Semantic Net-
work. In proc of 14th Conference of the European Chapter of the Association for Computational Linguistics
(EACL 2014), 8 p.
Zarrouk, M., Lafourcade, M. and Joubert, A. 2013. Inf?l?rences d?l?ductives et r?l?conciliation dans un r?l?seau
lexico-s?l?mantique. 20?l?me conf?l?rence du Traitement Automatique du Langage Naturel 2013 (TALN
2013), 14 p.
Zarrouk, M.and Lafourcade, M. 2014. Relation Inference in Lexical Networks ... with Refinements. The 9th
edition of the Language Resources and Evaluation Conference, 26-31 May, Reykjavik, Iceland, 6 p.
Zeichner, N., Berant J., and Dagan I. 2012. Crowdsourcing Inference-Rule Evaluation. in proc of ACL 2012
(short papers).
375
Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 174?182,
Gothenburg, Sweden, April 26-30 2014. c?2014 Association for Computational Linguistics
About Inferences
in a Crowdsourced Lexical-Semantic Network
Manel Zarrouk
UM2-LIRMM
161 rue Ada
34095 Montpellier, FRANCE
manel.zarrouk@lirmm.fr
Mathieu Lafourcade
UM2-LIRMM
161 rue Ada
34095 Montpellier, FRANCE
mathieu.lafourcade@lirmm.fr
Alain Joubert
UM2-LIRMM
161 rue Ada
34095 Montpellier, FRANCE
alain.joubert@lirmm.fr
Abstract
Automatically inferring new relations
from already existing ones is a way to
improve the quality of a lexical network
by relation densification and error de-
tection. In this paper, we devise such
an approach for the JeuxDeMots lexi-
cal network, which is a freely avalaible
lexical network for French. We first
present deduction (generic to specific)
and induction (specific to generic) which
are two inference schemes ontologically
founded. We then propose abduction
as a third form of inference scheme,
which exploits examples similar to a tar-
get term.
1 Introduction
Building resources for Computational Linguis-
tics (CL) is of crucial interest. Most of exist-
ing lexical-semantic networks have been built
by hand (like for instance WordNet (Miller et
al., 1990)) and, despite that tools are generally
designed for consistency checking, the task re-
mains time consuming and costly. Fully auto-
mated approaches are generally limited to term
co-occurrences as extracting precise semantic
relations between terms from corpora remains
really difficult. Meanwhile, crowdsourcing ap-
proaches are flowering in CL especially with
the advent of Amazon Mechanical Turk or in a
broader scope Wikipedia and Wiktionary, to cite
the most well-known examples. WordNet is such
a lexical network, constructed by hand at great
cost, based on synsets which can be roughly
considered as concepts (Fellbaum, 1988). Eu-
roWordnet (Vossen., 1998) a multilingual ver-
sion of WordNet and WOLF (Sagot., 2008) a
French version of WordNet, were built by auto-
mated crossing of WordNet and other lexical re-
sources along with some manual checking. Nav-
igli (2010) constructed automatically BabelNet a
large multilingual lexical network from term co-
occurrences in Wikipedia.
A lexical-semantic network can contain lem-
mas, word forms and multi-word expressions as
entry points (nodes) along with word meanings
and concepts. The idea itself of word senses in
the lexicographic tradition may be debatable in
the context of resources for semantic analysis,
and we generally prefer to consider word us-
ages. A given polysemous word, as identified
by locutors, has several usages that might dif-
fer substantially from word senses as classically
defined. A given usage can also in turn have
several deeper refinements and the whole set
of usages can take the form of a decision tree.
For example, frigate can be a bird or a ship. A
frigate>boat can be distinguished as a modern
ship with missiles and radar or an ancient vessel
with sails. In the context of a collaborative con-
struction, such a lexical resource should be con-
sidered as being constantly evolving and a gen-
eral rule of thumb is to have no definite certi-
tude about the state of an entry. For a polysemic
term, some refinements might be just missing at
a given time notwithstanding evolution of lan-
guage which might be very fast, especially in
technical domains. There is no way (unless by
inspection) to know if a given entry refinements
are fully completed, and even if this question is
really relevant.
The building of a collaborative lexical network
(or, in all generality, any similar resource) can
be devised according to two broad strategies.
First, it can be designed as a contributive system
174
like Wikipedia where people willingly add and
complete entries (like for Wiktionary). Second,
contributions can be made indirectly thanks to
games (better known as GWAP (vonAhn, 2008))
and in this case players do not need to be aware
that while playing they are helping building a
lexical resource. In any case, the built lexical
network is not free of errors which are corrected
along their discovery. Thus, a large number of
obvious relations are not contained in the lexi-
cal network but are indeed necessary for a high
quality resources usable in various NLP applica-
tions and notably semantic analysis. For exam-
ple, contributors seldom indicate that a particu-
lar bird type can fly, as it is considered as an obvi-
ous generality. Only notable facts which are not
easily deductible are naturally contributed. Well
known exceptions are also generally contributed
and take the form of a negative weight and anno-
tated as such (for example, fly
ag ent :?100
???????? ostrich
[exception: bird]).
In order to consolidate the lexical network,
we adopt a strategy based on a simple in-
ference mechanism to propose new relations
from those already existing. The approach is
strictly endogenous (i.e. self-contained) as it
doesn?t rely on any other external resources. In-
ferred relations are submitted either to contrib-
utors for voting or to experts for direct valida-
tion/invalidation. A large percentage of the in-
ferred relations has been found to be correct
however, a non-negligible part of them are found
to be wrong and understanding why is both in-
teresting and useful. The explanation process
can be viewed as a reconciliation between the in-
ference engine and contributors who are guided
through a dialog to explain why they found
the considered relation incorrect. The possible
causes for a wrong inferred relation may come
from three possible origins: false premises that
were used by the inference engine, exception or
confusion due to some polysemy.
In (Sajous et al., 2013) an endogenous enrich-
ment of Wiktionary is done thanks to a crowd-
sourcing tool. A quite similar approach of us-
ing crowdsourcing has been considered by (Ze-
ichner, 2012) for evaluating inference rules that
are discovered from texts. In (Krachina, 2006),
some specific inference methods are conducted
on text with the help of an ontology. Simi-
larly, (Besnard, 2008) capture explanation with
ontology-based inference. OntoLearn (Velardi,
2006) is a system that automatically build on-
tologies of specific domains from texts and also
makes use of inferences. There have been
also researchs on taxonomy induction based on
WordNet (Snow, 2006). Although extensive work
on inference from texts or handcrafted resources
has been done, almost none endogenously on
lexical network built by the crowds. Most prob-
ably the main reason of that situation is the lack
of such specific resources.
In this article, we first present the principles
behind the lexical network construction with
crowdsourcing and games with a purpose (also
know as human-based computation games) and
illustrated them with the JeuxDeMots (JDM)
project. Then, we present the outline of an elici-
tation engine based on an inference engine using
deduction, induction and especially abduction
schemes. An experimentation is then presented.
2 Crowdsourced Lexical Networks
For validating our approach, we used the JDM
lexical network, which is constructed thanks to
a set of associatory games (Lafourcade, 2007)
and has been made freely available by its au-
thors. There is an increasing trend of using on-
line GWAPs (game with a purpose (Thaler et
al., 2011)) method for feeding such resources.
Beside manual or automated strategies, con-
tributive approaches are flowering and becom-
ing more and more popular as they are both
cheap to set up and efficient in quality.
The network is composed of terms (as ver-
tices) and typed relations (as links between
vertices) with weight. It contains terms and
possible refinements. There are more than 50
types of relations, that range from ontological
(hypernym, hyponym), to lexical-semantic
(synonym, antonym) and to semantic role
(agent, patient, instrument). The weight of a
relation is interpreted as a strength, but not
directly as a probability of being valid. The JDM
network is not an ontology with some clean
hierarchy of concepts or terms. A given term
can have a substantial set of hypernyms that
covers a large part of the ontological chain to
upper concepts. For example, hypernym(cat) =
{feline,mammal, living being,pet,vertebrate, ...}.
Heavier weights associated to relations are those
felt by users as being the most relevant. The
175
1st January 2014, there are more than 6 700 000
relations and roughly 310 000 lexical items in the
JDM lexical network (according to the figures
given by the game site: http://jeuxdemots.org).
To our knowledge, there is no other existing
freely available crowdsourced lexical-network,
especially with weighted relations, thus enabling
strongly heuristic methods.
3 Inferring with Deduction & Induction
Adding new relations to the JDM lexical network
may rely on two components: (a) an inference
engine and (b) a reconciliator. The inference en-
gine proposes relations as a contributor to be
validated by other human contributors or ex-
perts. In case of invalidation of an inferred re-
lation, the reconciliator is invoked to try to as-
sess why the inferred relation was found wrong.
Elicitation here should be understood as the pro-
cess to transform some implicit knowledge of the
user into explicit relations in the lexical network.
The core ideas about inferences in our engine are
the following:
? inferring is to derive new premises (as
relations between terms) from previously
known premises, which are existing rela-
tions;
? candidate inferences may be logically
blocked on the basis of the presence or the
absence of some other relations;
? candidate inferences can be filtered out on
the basis of a strength evaluation.
3.1 Deduction Scheme
Inferring by deduction is a top-down scheme
based on the transitivity of the relation is-a (hy-
pernym). If a term A is a kind of B and B holds
some relation R with C, then we can expect that A
holds the same relation type with C. The scheme
can be formally written as follows: ? A i s?a???? B
? ? B R?? C ? A R?? C.
For example, shark i s?a???? fish and fish
has?par t
???????? fin, thus we can expect that shark
has?par t
???????? fin. The inference engine is applied
on terms having at least one hypernym (the
scheme could not be applied otherwise). Of
course, this scheme is far too naive, especially
considering the resource we are dealing with
and may produce wrong relations (noise). In
effect, the central term B is possibly polysemous
and ways to avoid probably wrong inferences
can be done through a logical blocking: if
there are two distinct meanings for B that hold
respectively the first and the second relation,
then most probably the inferred relation R(3)
is wrong (see figure 1) and hence should be
blocked. Moreover, if one of the premises is
tagged by contributors as true but irrelevant,
then the inference is blocked.
B
Bi
Bj
A
C
(
1
)
i
s
-
a
:
w 1
(3) R? : w3
(
4
)
i
s
-
a
(
2
)
R
: w
2
(
5
)
R
Figure 1: Triangular inference scheme where the
logical blocking based on the polysemy of the
central term B which has two distinct meanings
Bi and B j is applied. The two arrows without la-
bel are those of word meanings.
It is possible to evaluate a confidence level (on
an open scale) for each produced inference, in a
way that dubious inferences can be eliminated
out through statistical filtering. The weight w
of an inferred relation is the geometric mean of
the weight of the premises (relations (1) and (2)
in Figure 1). If the second premise has a nega-
tive value, the weight is not a number and the
proposal is discarded. As the geometric mean is
less tolerant to small values than the arithmetic
mean, inferences which are not based on two
rather strong relations (premises) are unlikely to
pass.
w(A R?? C) = ( w(A i s?a???? B) ? w(B R?? C) )1/2
? w3 = (w1 ? w2)1/2
Inducing a transitive closure over a knowledge
base is not new, but doing so considering word
meanings over a crowdsourced lexical network is
an original approach.
3.2 Induction Scheme
As for the deductive inference, induction ex-
ploits the transitivity of the relation is-a. If a term
A is a kind of B and A holds a relation R with C ,
then we might expect that B could hold the same
type of relation with C . More formally we can
write: ? A i s?a???? B ? ? A R?? C ? B R?? C.
For example, shark i s?a???? fish and shark
has?par t
??????
jaw, thus we might expect that fish
has?par t
?????? jaw.
This scheme is a generalization inference. The
principle is similar to the one applied to the de-
176
duction scheme and similarly some logical and
statistical filtering may be undertaken.
B
C
A
Ai
Aj
(
1
)
i
s
-
a
:
w 1
(
2
)
R
: w
3
(
5
)
i
s
-
a
(
4
)
R
(
3
)
R
?
:
w
2
Figure 2: (1) and (2) are the premises, and (3)
is the induction proposed for validation. Term
A may be polysemous with meanings holding
premises, thus inducing a probably wrong rela-
tion.
The central term here A, is possibly polyse-
mous (as shown in Figure 2). In that case, we
have the same polysemy issues than with the de-
duction, and the inference may be blocked. The
estimated weight for the induced relation is:
w(B R?? C) = (w(A R?? C))2 / w(A i s?a???? B)
? w2 = (w3)2/w1
3.3 Performing Reconciliation
Inferred relations are presented to the validator
to decide of their status. In case of invalida-
tion, a reconciliation procedure is launched in
order to diagnose the reasons: error in one of the
premises (previously existing relations are false),
exception or confusion due to polysemy (the in-
ference has been made on a polysemous central
term). A dialog is initiated with the user (Cohen?s
kappa of 0.79). To know in which order to pro-
ceed, the reconciliator checks if the weights of
the premises are rather strong or weak.
Errors in the premises. We suppose that rela-
tion (1) (in Figure 1 and 2) has a relatively low
weight. The reconciliation process asks the val-
idator if the relation (1) is true. It sets a negative
weight to this relation if not so that the engine
blocks further inferences. Else, if relation (1) is
true, we ask about relation (2) and proceed as
above if the answer is negative. Otherwise, we
check the other cases (exception, polysemy).
Errors due toExceptions. For the deduction, in
case we have two trusted relations, the reconcil-
iation process asks the validators if the inferred
relation is a kind of exception relatively to the
term B . If it is the case, the relation is stored in
the lexical network with a negative weight and
annotated as exception. Relations that are ex-
ceptions do not participate further as premises
for deducing. For the induction, in case we have
two trusted relations, the reconciliator asks the
validators if the relation (A R?? C) (which served
as premise) is an exception relatively to the term
B . If it is the case, in addition to storing the false
inferred relation (B R?? C) in the lexical network
with a negative weight, the relation (A R?? C) is
annotated as exception. In the induction case,
the exception is a true premise which leads to a
false induced relation. In both cases of induc-
tion and deduction, the exception tag concerns
always the relation (A R?? C). Once this relation
is annotated as an exception, it will not partic-
ipate as a premise in inferring generalized rela-
tions (bottom-up model) but can still be used in
inducing specified relations (top-down model).
Errors due to Polysemy. If the central term
(B for deduction and A for induction) present-
ing a polysemy is mentioned as polysemous
in the network, the refinement terms ter m1,
ter m2, . . . ter mn are presented to the validator
so she/he can choose the appropriate one. The
validator can propose new terms as refinements
if she/he is not satisfied with the listed ones (in-
ducing the creation of new appropriate refine-
ments). If there is no meta information indicat-
ing that the term is polysemous, we ask first the
validator if it is indeed the case. After this proce-
dure, new relations will be included in the net-
work with positive values and the inference en-
gine will use them later on as premises.
4 Abductive Inference
The last inferring scheme is built upon abduc-
tion and can be viewed as an example based
strategy. Hence abduction relies on similarity
between terms, which may be formalized in our
context as sharing some outgoing relations be-
tween terms. The abductive inferring layout
supposes that relations held by a term can be
proposed to similar terms. Here, abduction first
selects a set of similar terms to the target term A
which are considered as proper examples. The
outgoing relations from the examples which are
not common with those of A are proposed as
potential relations for A and then presented for
validation/invalidation to users. Unlike induc-
tion and deduction, abduction can be applied on
177
terms with missing or irrelevant ontological rela-
tions, and can generate ontological relations to
be used afterward by the inference loop.
4.1 Abduction Scheme
We note an outgoing relation as a 3-uple of a type
t , a weight w and a target node n:Ri = ? ti , wi , ni
?. For example, consider the term A having n
outgoing relations. Amongst these relations, we
have for example:
? beak
has?par t
???? A & ? nest locati on???? A.
We found 3 examples sharing those two rela-
tions with the term A:
? beak
has?par t
???? {ex1,ex2,ex3}
? nest l ocati on???? {ex1,ex2,ex3}
We consider these terms as a set of exam-
ples to follow and similar to A. These examples
have also other outgoing relations which are pro-
posed as potential relations for A. For example :
? {ex1,ex2}
ag ent?1
???? fly ? {ex2}
car ac???? colorful
? {ex1,ex2,ex3}
has?par t
???? feather
? {ex3}
ag ent?1
???? sing
We infer that A can hold these relations and we
propose them for validation.
? A
ag ent?1
???? fly ? ? A
has?par t
???? feather ?
? A car ac???? colorful ? ? A
ag ent?1
???? sing ?
4.2 Abduction Filtering
Applying the abduction procedure crudely on
the terms generates a lot of waste as a consid-
erable amount of erroneous inferred relations.
Hence, we elaborated a filtering strategy to avoid
having a lot of dubious proposed candidates. For
this purpose, we define two different threshold
pairs. The first threshold pair (?1, ?1) is used to
select proper examples x1,x2...xn and is defined
as follows:
?1 =max(3,nbogr(A)?0.1) (1)
where nbogr(A) is the number of outgoing rela-
tions from the term A.
?1 =max(25,mwogr(A)?0.5) (2)
where mwogr(A) is the mean of weights of outgo-
ing relations from A. The second threshold pair
(?2, ?2) is used to select proper candidate re-
lations from outgoing relations of the examples
R ?1,R
?
2...R
?
q .
?2 =max(3,{xi }?0.1) (3)
where {xi } is the cardinal of the set {xi }.
?2 =max(25,mwogr({xi })?0.5) (4)
where mwogr({xi }) is the mean of weights of out-
going relations from the set of examples xi .
If a term A is sharing at least ?1 relations, hav-
ing a weight over ?1, of the total of the rela-
tions R1, R2, . . . Rp toward terms T1, T2, . . . Tp
with a group of examples x1, x2, . . . xn , we admit
that this term has a degree of similarity strong
enough with these examples. After building up
a set of examples on which we can apply our ab-
duction engine we proceed with the second part
of the strategy. If we have at least ?2 examples xi
holding a specific relation R ?k weighting over ?2
with a term Bk , more formally R ?k = ? t , w ? ?2,
Bk ?, we can suppose that the term A may hold
this same relation R ?k with the same target term
Bk (figure 3).
x1
x2
x3
xn
T1
T2
Tp
A
B1
B2
Bq
R1
R2
Rp
R ?1
R ?q
R ?2
R ?1?
R ?q ?
Figure 3: Abduction scheme with examples xi
sharing relations with A and proposing new ab-
ducted relations.
On figure 3, we simplified thresholds to 2
for illustrative purpose. So, to be selected, the
examples x1 ,x2, x3, . . . xn must have at least 2
common relations with A. A relation R ?1?q must
be hold by at least 2 examples to be proposed as
a potential relation for A. More clearly:
? x1
R ?1???? B1 and x2
R ?1???? B1 ? R ?1 : 2
=? propose A
R ?1????? B1
? xn
R ?2???? B2 ? R ?2 : 1
=? do not propose this relation.
? x1
R ?q???? Bq , x3
R ?q???? Bq and xn
R ?q???? Bq
? R ?q : 3
=? propose A
R ?q ????? Bq
For statistical filtering, we can act on the
178
threshold (?2, ?2) as the minimum number of
examples xi being R ? related with a target term
Bk . It is also possible to evaluate the weight of
the abducted relation as following:
w(A
R ?k?? Bk )=
1
nbR ?cd
n,p,q?
i=1, j=1,k=1
3pw1w2w3 (5)
where nbR ?cd is the number of the relations R
?
candidate to be proposed and w1=A
R j???? T j &
w2=xi
R j???? T j & w3=xi
R ?k???? Bk .
This filtering parameters are adjustable ac-
cording to the user?s requirements, so it can fulfil
various expectations. Constant values in thresh-
old formulas have been determined empirically.
5 Experimentation
We made an experiment with a unique run of
the deduction, induction and abduction engines
over the lexical network. Contributors have ei-
ther accepted or rejected a subset of those can-
didates during the normal course of their activ-
ity. This experiment is for an evaluation pur-
pose only, as actually the system is running iter-
atively along with contributors and games. The
experiment has been done with the parameters
given previously, which are determined empri-
cally as those maximizing recall and precision
(over a very small subset of the JDM lexical net-
work, around 1?).
5.1 Appliying Deductions and Inductions
We applied the inference engine on around
25 000 randomly selected terms having at least
one hypernym or one hyponym and thus pro-
duced by deduction more than 1 500 000 infer-
ences and produced by induction over 360 000
relation candidates. The threshold for filtering
was set to a weight of 25. This value is relevant
as when a human contributor proposed relation
is validated by experts, it is introduced with a de-
fault weight of 25.
The transitive is-a (Table1) is not very produc-
tive which might seems surprising at first glance.
In fact, the is-a relation is already quite popu-
lated in the network, and as such, fewer new re-
lations can be inferred. The figures are inverted
for some other relations that are not so well pop-
ulated in the lexical network but still are poten-
tially valid. The has-parts relation and the agent
semantic role (the agent-1 relation) are by far the
most productive types.
Relation type Proposed%
is-a (x is a type of y) 6.1
has-parts (x is composed of y) 25.1
holonym (y specific of x) 7.2
typical place (of x) 7.2
charac (x as characteristic y) 13.7
agent-1 (x can do y) 13.3
instr-1 (x instrument of y) 1.7
patient-1 (x can be y) 1
place-1 (x located in the place y) 9.8
place > action (y can be done in place x) 3.4
object > mater (x is made of y) 0.3
Table 1: Global percentages of relations pro-
posed per type for deduction and induction.
Deduction % valid % error
Relation type rlvt ? rlvnt prem excep pol
is-a 76% 13% 2% 0% 9%
has-parts 65% 8% 4% 13% 10%
holonym 57% 16% 2% 20% 5%
typical place 78% 12% 1% 4% 5%
charac 82% 4% 2% 8% 4%
agent-1 81% 11% 1% 4% 3%
instr-1 62% 21% 1% 10% 6%
patient-1 47% 32% 3% 7% 11%
place-1 72% 12% 2% 10% 6%
place > action 67% 25% 1% 4% 3%
object > mater 60% 3% 7% 18% 12%
Table 2: Number of propositions produced by
deduction and ratio of relations found as true or
false.
In tables 2 and 3 are presented some evalu-
ations of the status of the inferences proposed
by the inference engine through deduction and
induction respectively. Inferences are valid for
an overall of 80-90% with around 10% valid but
not relevant (like for instance dog
has?par t s
???????? pro-
ton). We observe that error number in premises
is quite low, and nevertheless errors can be eas-
ily corrected. Of course, not all possible errors
are detected through this process. More inter-
estingly, the reconciliation allows in 5% of the
cases to identify polysemous terms and refine-
ments. Globally false negatives (inferences voted
false while being true) and false positives (infer-
ences voted true while being false) are evaluated
to less than 0.5%.
For the induction process, the relation is-a is
not obvious (a lexical network is not reductible
to an ontology and multiple inheritance is possi-
ble). Result seems about 5% better than for the
deduction process: inferences are valid for an
overall of 80-95%. The error number is very low.
The main difference with the deduction process
is on errors due to polysemy which is lower with
the induction process.
179
To try to assess a baseline for those results,
we compute the full closure of the lexical net-
work, i.e. we produce iteratively all possible can-
didate relations until no more could be found,
each candidate being considered as correct and
participating to the process. We got more than
6 000 000 relations out of which 45% were wrong
(evaluation on around 1 000 candidates ran-
domly chosen).
5.2 Unleashing the Abductive Engine
We applied systematically the abduction engine
on the lexical items contained in the network,
and produce 629 987 abducted relations out of
which 137 416 were not already existing in the
network. Those 137 416 are candidate relations
concerning 10 889 distinct lexical entries, hence
producing a mean of around 12 new relations
per entry. The distribution of the proposed re-
lations follows a power law, which is not totally
surprising as the relation distribution in the lex-
ical network is by itself governed by such a dis-
tribution. Those figures indicate that abduction
seems to be still quite productive in terms of raw
candidates, even not relying on ontological ex-
isting relations.
The table 4 presents the number of relations
proposed by the inference engine through ab-
duction. The different relation types are var-
iously productive, and this is mainly due to
the number of existing relations and the dis-
tribution of their type. The most productive
relation is has-part and the least one is holo
(holonym/whole). Correct relations represent
around 80% of the relations that have been eval-
uated (around 5.6% of the total number of pro-
duced relations).
One suprising fact, is that the 80% seem to
be quite constant notwithstanding the relation
type, the lowest value being 77% (for instr-1
which is the relation specifying what can be done
with x as an instrument) and the highest being
85% (for action-place which is the relation asso-
ciating for an action the typical locations where
it can occur). The abduction process is not onto-
logically based, and hence does not rely on the
generic (is-a) or specific (hyponym) relations,
but on the contrary on any set of examples that
seems to be alike the target term. The apparent
stability of 80% correct abducted relations may
be a positive consequence of relying on a set of
examples, with a potentially irreductible of 20%
wrong abducted relations.
Figure 4 presents two types of data: (1) the
percentage of correct abducted relations accord-
ing to the number of examples required to pro-
duce the inference, and (2) the proportion be-
tween the produced relations and the total of
107 416 relations according to the minimal num-
ber of examples allowed. What can clearly be
seen is that when the number of required ex-
amples is increased, the ratio of correct abduc-
tions increases accordingly, but the number of
proposed relations dramaticaly falls. The num-
ber of abductions is an inverse power law of the
number of examples required.
Figure 4: Production of abducted relations and
percentage of correctness according to examples
number.
At 3 examples, only 40% of the proposed re-
lations are correct, and with a minimum of 6
examples, more than 3/4 of the proposals are
deemed correct. The balanced F-score is opti-
mal at the intersection of both curves, that is to
say for at least 4 examples.
In figure 5, is showed the mean number of
new relations during an iteration of the infer-
ence engine on abduction. Between two runs,
users and validators are invited to accept or re-
ject abducted relations. This process is done
at their discretion and users may leave some
propostions unvoted. Experiments showed that
users are willing to validate strongly true rela-
tions and invalidate clearly false relations. Rela-
tions whose status may be difficult are more of-
ten left aside than other easiest proposals. The
third run is the most productive with a mean of
almost 20 new abducted relations. After 3 runs,
the abductive process begins to be less produc-
tive by attrition of new possible candidates. No-
tice that the abduction process may, on subse-
quent runs, remove some previsouly done pro-
posals and as such is not monotonous.
180
Figure 5: Mean number of new relations rela-
tively to runs in iterated abduction.
5.3 Figures on Reconciliation
Reconciliation in abduction is simpler than in
deduction or induction, as the potential adverse
effect of polysemy is counterbalanced by the
statistical approach implemented by the large
number of examples (when available). The rec-
onciliation in the case of abduction is to deter-
mine if the wrong proposal has been produced
logically considering the support examples. In
97% of the cases, the wrong abducted relation
has been qualified as wrong but logical by vot-
ers or validators. For examples: ? Boeing
747
has?par t
?????? propeller* ? whale
pl ace
???? lake *
? pelican
ag ent?1
?????? sing *. All those wrong ab-
ducted relations given as examples above might
have been correct. Considering the examples ex-
ploited to produce the candidates, in those cases
there is no possible way to guess those relations
are wrong. This is even reinforced by the fact that
abduction does not rely on ontological relations,
which in some cases could have avoided wrong
abduction. However, abduction compared to in-
duction and deduction, can be used on terms
that do not hold ontological relations, either they
are missing or they are not relevant (for verbs, in-
stances...).
6 Conclusion
We presented some issues in inferring new rela-
tions from existing ones to consolidate a lexical-
semantic network built with games and user
contributions. New inferred relations are stored
to avoid having to infer them again and again dy-
namically. To be able to enhance the network
quality and coverage, we proposed an elicitation
engine based on inferences (induction, deduc-
tion and abduction) and reconciliation. If an in-
ferred relation is proven wrong, a reconciliation
process is conducted in order to identify the un-
derlying cause and solve the problem. The ab-
duction scheme does not rely on the ontologi-
cal relation (is-a) but merely on examples that
are similarly close to the target term. Experi-
ments showed that abduction is quite produc-
tive (compared to deduction and induction), and
is stable in correctness. User evaluation showed
that wrong abducted relations (around 20% of
all abducted relations) are still logically sound
and could not have been dismissed a priori. Ab-
duction can conclusively be considered as a use-
full and efficient tool for relation inference. The
main difficulty relies in setting the various pa-
rameter in order to achieve a fragile tradeoff be-
tween an overrestrictive filter (many false nega-
tives, resulting in information losses) and the op-
posite (many false postive, more human effort).
The elicitation engine we presented through
schemas based on deduction, induction and ab-
duction is an efficient error detector, a polysemy
identifier but also a classifier by abduction. The
actions taken during the reconciliation forbid
an inference proven wrong or exceptional to be
inferred again. Each inference scheme is sup-
ported by the two others, and if a given inference
has been produced by more than one of these
three schemas, it is almost surely correct.
Induction % valid % error
Relation types rlvt ?rlvnt prem excep pol
is-a - - - - -
has-parts 78% 10% 3% 2% 7%
holonyme 68% 17% 2% 8% 5%
typical place 81% 13% 1% 2% 3%
charac 87% 6% 2% 2% 3%
agent-1 84% 12% 1% 2% 1%
instr-1 68% 24% 1% 4% 3%
patient-1 57% 36% 3% 2% 2%
place-1 75% 16% 2% 5% 2%
place > action 67% 28% 1% 3% 1%
object > mater 75% 10% 7% 5% 3%
Table 3: Number of propositions produced by in-
duction and ratio of relations found as true or
false.
Abduction #prop #eval (%) True (%) False (%)
is-a 7141 421 (5.9) 343 (81.5) 78 (18.5)
has-parts 26517 720 (2.7) 578 (80.3) 142 (19.7)
holo 1592 153 (9.6) 124 (81) 29 (18.9)
agent 7739 298 (3.9) 236 (79.2) 62 (20.8)
place 17148 304 (1.8) 253 (83.2) 51 (16.8)
instr 10790 431 (4) 356 (82.6) 75 (17.4)
charac 7443 319 (4.3) 251 (78.7) 68 (21.3)
agent-1 18147 955 (5.3) 780 (81.7) 175 (18.3)
instr-1 11867 886 (7.5) 682 (77) 204 (23)
place-1 14787 1106 (7.5) 896 (81) 210 (19)
place>act 8268 270 (3.3) 214 (79.3) 56 (20.7)
act>place 5976 170 (2.8) 145 (85.3) 25 (14.7)
Total 137416 6033 (4.3) 4858 (81) 1175 (19)
Table 4: Number of propositions produced by
abduction and ratio of relations found as true or
false.
181
References
von Ahn, L. and Dabbish, L. 2008. Designing games
with a purpose. in Communications of the ACM,
number 8, volume 51. p 58-67.
Besnard, P. Cordier, M.-O., and Moinard, Y. 2008.
Ontology-based inference for causal explanation.
Integrated Computer-Aided Engineering , IOS
Press, Amsterdam, Vol. 15 , No. 4, 351-367, 2008.
Fellbaum, C. and Miller, G. 1988. (eds) WordNet. The
MIT Press.
Krachina, O., Raskin, V. 2006. Ontology-Based Infer-
ence Methods. CERIAS TR 2006-76, 6 p.
Lafourcade, M. 2007. Making people play for Lex-
ical Acquisition. In Proc. SNLP 2007, 7th Sym-
posium on Natural Language Processing. Pattaya,
Thailande, 13-15 December. 8 p.
Lafourcade, M., Joubert, A. 2012. Long Tail in
Weighted Lexical Networks. In proc of Cogni-
tive Aspects of the Lexicon (CogAlex-III), COLING,
Mumbai, India, December 2012.
Lieberman, H, Smith, D. A and Teeters, A 2007.
Common consensus: a web-based game for col-
lecting commonsense goals. In Proc. of IUI,
Hawaii,2007.12 p .
Marchetti, A and Tesconi, M and Ronzano, F and
Mosella, M and Minutoli, S. 2007. SemKey: A Se-
mantic Collaborative Tagging System. in Procs of
WWW2007, Banff, Canada. 9 p.
Mihalcea, R and Chklovski, T. 2003. Open MindWord
Expert: Creating large annotated data collections
with web users help.. In Proceedings of the EACL
2003, Workshop on Linguistically Annotated Cor-
pora (LINC). 10 p.
Miller, G.A. and Beckwith, R. and Fellbaum, C. and
Gross, D. and Miller, K.J. 1990. Introduction to
WordNet: an on-line lexical database. Interna-
tional Journal of Lexicography. Volume 3, p 235-
244.
Navigli, R and Ponzetto, S. 2010. BabelNet: Build-
ing a very large multilingual semantic network. in
Proceedings of the 48th Annual Meeting of the As-
sociation for Computational Linguistics, Uppsala,
Sweden, 11-16 July 2010.p 216-225.
Sagot, B. and Fier, D. 2010. Construction d?un word-
net libre du fran?ais ? partir de ressources multi-
lingues. in Proceedings of TALN 2008, Avignon,
France, 2008.12 p.
Sajous, F., Navarro, E., Gaume, B,. Pr?vot, L. and
Chudy, Y. 2013. Semi-Automatic Enrichment of
Crowdsourced Synonymy Networks: The WISIG-
OTH system applied to Wiktionary. Language Re-
sources & Evaluation, 47(1), pp. 63-96.
Siorpaes, K. and Hepp, M. 2008. Games with a Pur-
pose for the Semantic Web. in IEEE Intelligent Sys-
tems, number 3, volume 23.p 50-60.
Snow, R. Jurafsky, D., Y. Ng., A. 2006. Semantic tax-
onomy induction from heterogenous evidence. in
Proceedings of COLING/ACL 2006, 8 p.
Thaler, S and Siorpaes, K and Simperl, E. and Hofer,
C. 2011. A Survey on Games for Knowledge Acqui-
sition. STI Technical Report, May 2011.19 p.
Velardi, P. Navigli, R. Cucchiarelli, A. Neri, F. 2006.
Evaluation of OntoLearn, a methodology for Auto-
matic Learning of Ontologies. in Ontology Learn-
ing and Population, Paul Buitelaar Philipp Cim-
miano and Bernardo Magnini Editors, IOS press
2006).
Vossen, P. 2011. EuroWordNet: a multilingual
database with lexical semantic networks. Kluwer
Academic Publishers.Norwell, MA, USA.200 p.
Zeichner, N., Berant J., and Dagan I. 2012. Crowd-
sourcing Inference-Rule Evaluation. in proc of ACL
2012 (short papers).
182
