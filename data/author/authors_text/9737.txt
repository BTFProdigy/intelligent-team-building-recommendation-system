Proceedings of the IJCNLP-08 Workshop on NER for South and South East Asian Languages, pages 33?40,
Hyderabad, India, January 2008. c?2008 Asian Federation of Natural Language Processing
 
Language Independent Named Entity Recognition in  Indian Languages  
Asif Ekbal, Rejwanul Haque, Amitava Das, Venkateswarlu Poka 
and Sivaji Bandyopadhyay 
 Department of Computer Science and Engineering 
Jadavpur University 
Kolkata-700032, India 
asif.ekbal@gmail.com, rejwanul@gmail.com, 
amit_santu_kuntal@yahoo.com, venkat.ju@gmail.com and 
sivaji_cse_ju@yahoo.com 
            
Abstract 
This paper reports about the development 
of a Named Entity Recognition (NER) sys-
tem for South and South East Asian lan-
guages, particularly for Bengali, Hindi, Te-
lugu, Oriya and Urdu as part of the 
IJCNLP-08 NER Shared Task
1
. We have 
used the statistical Conditional Random 
Fields (CRFs). The system makes use of 
the different contextual information of the 
words along with the variety of features 
that are helpful in predicting the various 
named entity (NE) classes. The system uses 
both the language independent as well as 
language dependent features. The language 
independent features are applicable for all 
the languages. The language dependent 
features have been used for Bengali and 
Hindi only. One of the difficult tasks of 
IJCNLP-08 NER Shared task was to iden-
tify the nested named entities (NEs) though 
only the type of the maximal NEs were 
given. To identify nested NEs, we have 
used rules that are applicable for all the 
five languages. In addition to these rules, 
gazetteer lists have been used for Bengali 
and Hindi. The system has been trained 
with Bengali (122,467 tokens), Hindi 
(502,974 tokens), Telugu (64,026 tokens), 
Oriya (93,173 tokens) and Urdu (35,447 
tokens) data. The system has been tested 
with the 30,505 tokens of Bengali, 38,708 
tokens of Hindi, 6,356 tokens of Telugu, 
                                                
1
http://ltrc.iiit.ac.in/ner-ssea-08  
24,640 tokens of Oriya and 3,782 tokens of 
Urdu. Evaluation results have demonstrated 
the highest maximal F-measure of 53.36%, 
nested F-measure of 53.46% and lexical F-
measure of 59.39% for Bengali.   
1 Introduction 
Named Entity Recognition (NER) is an impor-
tant tool in almost all Natural Language Proc-
essing (NLP) application areas. Proper identifi-
cation and classification of named entities are 
very crucial and pose a very big challenge to 
the NLP researchers. The level of ambiguity in 
named entity recognition (NER) makes it diffi-
cult to attain human performance.     
NER has drawn more and more attention 
from the named entity (NE) tasks (Chinchor 
95; Chinchor 98) in Message Understanding 
Conferences (MUCs) [MUC6; MUC7]. The 
problem of correct identification of named enti-
ties is specifically addressed and benchmarked 
by the developers of Information Extraction 
System, such as the GATE system (Cunning-
ham, 2001). NER also finds application in 
question-answering systems (Maldovan et al, 
2002) and machine translation (Babych and 
Hartley, 2003).  
The current trend in NER is to use the ma-
chine-learning approach, which is more attrac-
tive in that it is trainable and adoptable and the 
maintenance of a machine-learning system is 
much cheaper than that of a rule-based one. 
The representative machine-learning ap-
proaches used in NER are HMM (BBN?s Iden-
tiFinder in (Bikel, 1999)), Maximum Entropy 
33
(New York University?s MENE in (Borthwick, 
1999)), Decision Tree (New York University?s 
system in (Sekine 1998), SRA?s system in 
(Bennet, 1997) and Conditional Random Fields 
(CRFs) (Lafferty et al, 2001; McCallum and 
Li, 2003).       
There is no concept of capitalization in Indian 
languages (ILs) like English and this fact makes 
the NER task more difficult and challenging in 
ILs. There has been very little work in the area of 
NER in Indian languages. In Indian languages par-
ticularly in Bengali, the work in NER can be found 
in (Ekbal and Bandyopadhyay, 2007a) and  (Ekbal 
and Bandyopadhyay, 2007b). These two systems 
are based on the pattern directed shallow parsing 
approach. An HMM-based NER in Bengali can be 
found in (Ekbal et al, 2007c). Other than Bengali, 
the work on NER can be found in (Li and 
McCallum, 2004) for Hindi. This system is based 
on CRF.  
In this paper, we have reported a named entity 
recognition system for the south and south east 
Asian languages, particularly for Bengali, Hindi, 
Telugu, Oriya and Urdu. Bengali is the seventh 
popular language in the world, second in India and 
the national language of Bangladesh. Hindi is the 
third popular language in the world and the na-
tional language of India. Telugu is one of the popu-
lar languages and predominantly spoken in the 
southern part of India. Oriya and Urdu are the 
other two popular languages of India and widely 
used in the eastern and the northern part, respec-
tively. The statistical Conditional Random Field 
(CRF) model has been used to develop the system, 
as it is more efficient than HMM to deal with the 
non-independent and diverse overlapping features 
of the highly inflective Indian languages. We have 
used a fine-grained named entity tagset
2
, defined as 
part of the IJCNLP-08 NER Shared Task for 
SSEA. The system makes use of the different con-
textual information of the words along with the 
variety of orthographic word level features that are 
helpful in predicting the various named entity 
classes. In this work, we have considered language 
independent features as well as the language de-
pendent features. Language independent features 
include the contextual words, prefix and suffix in-
formation of all the words in the training corpus, 
several digit features depending upon the presence 
                                                
 
2
http://ltrc.iiit.ac.in/ner-ssea-08/index.cgi?topic=3  
and/or the number of digits in a token and the fre-
quency features of the words. The system consid-
ers linguistic features particularly for Bengali and 
Hindi. Linguistic features of Bengali include the 
set of known suffixes that may appear with named 
entities, clue words that help in predicating the lo-
cation and organization names, words that help to 
recognize measurement expressions, designation 
words that help in identifying person names, the 
various gazetteer lists like the first names, middle 
names, last names, location names and organiza-
tion names. As part of linguistic features for Hindi, 
the system uses only the lists of first names, middle 
names and last names along with the list of words 
that helps to recognize measurements. No linguis-
tic features have been considered for Telugu, Oriya 
and Urdu. It has been observed from the evaluation 
results that the use of linguistic features improves 
the performance of the system. A number of ex-
periments have been carried out to find out the 
best-suited set of features for named entity recog-
nition in Bengali, Hindi, Telugu, Oriya and Urdu.  
2 Conditional Random Fields 
Conditional Random Fields (CRFs) (Lafferty et al, 
2001) are undirected graphical models, a special 
case of which corresponds to conditionally trained 
probabilistic finite state automata. Being 
conditionally trained, these CRFs can easily 
incorporate a large number of arbitrary, non-
independent features while still having efficient 
procedures for non-greedy finite-state inference 
and training. CRFs have shown success in various 
sequence modeling tasks including noun phrase 
segmentation (Sha and Pereira, 2003) and table 
extraction (Pinto et al, 2003).     
CRFs are used to calculate the conditional 
probability of values on designated output nodes 
given values on other designated input nodes. The 
conditional probability of a state sequence 
1, 2, , TS s s s given an observation 
sequence 1 2,, ....., )TO o o o  is calculated as: 
1 ,
1
1
( | ) exp( ( , , )),
T
k k t t
o
t k
P s o f s s o t
Z
where
1 ,( , , )k t tf s s o t is a feature function whose weight 
k is to be learned via training. The values of the 
feature functions may range between ..... , 
but typically they are binary. To make all 
34
conditional probabilities sum up to 1, we must 
calculate the normalization 
factor, 0 1 ,
1
exp( ( , , ))
T
s k k t t
t k
Z f s s o t , 
which, as in HMMs, can be obtained efficiently by 
dynamic programming. 
To train a CRF, the objective function to be 
maximized is the penalized log-likelihood of the 
state sequences given observation sequences: 
2
( ) ( )
2
1
log( ( | ))
2
N
i i
k
i k
L P s o , 
where, {
( ) ( )
,
i i
o s } is the labeled training 
data. The second sum corresponds to a zero-mean,  
2
-variance Gaussaian prior over parameters, 
which facilitates optimization by making the like-
lihood surface strictly convex. Here, we set pa-
rameters 
 
to maximize the penalized log-
likelihood using Limited-memory BFGS (Sha and 
Pereira, 2003), a quasi-Newton method that is sig-
nificantly more efficient, and which results in only 
minor changes in accuracy due to changes in .  
When applying CRFs to the named entity 
recognition problem, an obsevation sequence is a 
token of a sentence or document of text and the 
state sequence is its corresponding label sequence. 
While CRFs generally can use real-valued 
functions, in our experiments maximum of the 
features are binary. A feature function 
1 ,( , , )k t tf s s o t has a value of 0 for most cases and 
is only set to be 1, when 1,t ts s are certain states 
and the observation has certain properties. We 
have used the C
++ 
based OpenNLP CRF++ pack-
age
3
, a simple, customizable, and open source im-
plementation of Conditional Random Fields 
(CRFs) for segmenting /labeling sequential data. 
3 Named Entity Recognition in Indian 
Languages 
Named Entity Recognition in Indian languages 
(ILs) is difficult and challenging as capitalization 
is not a clue in ILs. The training data were pro-
vided for five different Indian languages, namely 
Bengali, Hindi, Telugu, Oriya and Urdu in Shakti 
Standard Format
4
. The training data in all the lan-
                                                
3
http://crfpp.sourceforge.net  
4
http://shiva.iiit.ac.in/SPSAL 2007/ssf.html  
guages were annotated with the twelve NE tags, as 
defined for the IJCNLP-08 NER shared task taget
5
. 
Only the maximal named entities and not the inter-
nal structures of the entities were annotated in the 
training data. For example, mahatma gandhi road 
was annotated as location and assigned the tag 
?NEL? even if mahatma and gandhi  are named 
entity title person (NETP) and person name (NEP) 
respectively, according to the IJCNLP-08 shared 
task tagset. These internal structures of the entities 
were to be identified during testing. So, mahatma 
gandhi road will be tagged as mahatma /NETP 
gandhi/NEP road/NEL. The structure of the tagged 
element using the SSF form will be as follows:  
1 (( NP <ne=NEL>  
1.1 (( NP <ne=NEP> 
1.1.1 (( NP  <ne=NETP> 
1.1.1.1 mahatma 
)) 
1.1.2 gandhi 
)) 
1.2 road 
)) 
3.1 Training Data Preparation for CRF  
Training data for all the languages required some 
preprocessing in order to use in the Conditional 
Random Field framework. The training data is 
searched for the multiword NEs. Each component 
of the multiword NE is searched in the training set 
to find whether it occurs as a single-word NE. The 
constituent components are then replaced by their 
NE tags (NE type of the single-word NE). For ex-
ample, mahatma gandhi road/NEL will be tagged 
as mahatma/NETP gandhi/NEP road/NEL if the 
internal components are found to appear with these 
NE tags in the training set. Each component of a 
multiword NE is also checked whether the compo-
nent is made up of digits only. If a component is 
made up digits only, then it is assigned the tag 
?NEN?. Various gazetteers for Bengali and Hindi 
have been also used in order to identify the internal 
structure of the NEs properly.  The list of gazet-
teers, which have been used in preparing the train-
ing data, is shown in Table 1. 
The individual components (not occurring as a 
single-word NE in the training data) of a multi-
word NE are searched in the gazetteer lists and 
                                                
5
http://ltrc.iiit.ac.in/ner-ssea-08/index.cgi?topic=3  
35
assigned the appropriate NE tags. Other than NEs 
are marked with the NNE tags. The procedure is 
given below:  
Gazetteer list Number of entries 
First person name in Ben-
gali 
27,842 
Last person name in Ben-
gali 
5,288 
Middle name in Bengali 1,491 
Person name designation 
in Bengali 
947 
Location name in Bengali 7,870 
First person name in Hindi 1,62,881 
Last person name in Hindi 3,573 
Middle name in Hindi 450 
Cardinals in Bengali, 
Hindi and Telugu 
100 
Ordinals in Bengali, Hindi 
and Telugu 
65 
Month names in Bengali, 
Hindi and Telugu 
24 
Weekdays in Bengali, 
Hindi and Telugu 
14 
Words that denote meas-
urement in Bengali, Hindi 
and Telugu 
52 
Table 1. Gazetteer lists used during training data 
preparation  
 Step 1: Search the multiword NE in the training 
data 
Step 2: Extract each component from the mult-
word NE. 
Step 3: Check whether the constituent individual 
component (except the last one) appears in the 
training data as a single-word NE. 
Step 4: If the constituent NE appears in the training 
data as a single-word NE then 
Step 4.1: Assign the NE tag, extracted from the 
single-word NE, to the component of the multi-
word NE. 
else 
Step 4.2: Search the component in the gazetteer 
lists and assign the appropriate NE tag. 
Step 4.2.1: If the component is not found to appear 
in the gazetteer list then assign the NE tag of the 
maximal NE to the individual component.  
For example, if mahatma gandhi road is tagged 
as NEL, i.e., mahatma gandhi road/NEL then each 
component except the last one (road ) of this mult-
word NE is searched in the training set to look for 
it?s appearance (Step 3). Gazetteer lists are 
searched in case the component is not found in the 
training set (Step 4.2). If the components are found 
either in the training set or in the gazetteer list, 
then mahatma gandhi road/NEL will be tagged as: 
mahatma/NETP gandhi/NEP road/NEL. 
3.2 Named Entity Features 
Feature selection plays a crucial role in CRF 
framework. Experiments were carried out to find 
out most suitable features for NE tagging task. The 
main features for the NER task have been identi-
fied based on the different possible combination of 
available word and tag context. The features also 
include prefix and suffix for all words. The term 
prefix/suffix is a sequence of first/last few charac-
ters of a word, which may not be a linguistically 
meaningful prefix/suffix. The use of prefix/suffix 
information works well for highly inflected lan-
guages like the Indian languages. In addition, vari-
ous gazetteer lists have been developed to use in 
the NER task particularly for Bengali and Hindi. 
We have considered different combination from 
the following set for inspecting the best feature set 
for the NER task: 
 F={
1 1
,..., , , ,...,
i m i i i i n
w w w w w , |prefix| n, 
|suffix| n, previous NE tag, POS tags, First word, 
Digit information, Gazetteer lists} 
     Following is the details of the set of features 
that were applied to the NER task: 
 
Context word feature: Previous and next words 
of a particular word might be used as a feature. We 
have considered the word window of size five, i.e., 
previous and next two words from the current word 
for all the languages.  
Word suffix: Word suffix information is helpful 
to identify NEs. A fixed length word suffix of the 
current and surrounding words might be treated as 
feature. In this work, suffixes of length up to three 
the current word have been considered for all the 
languages. More helpful approach is to modify the 
feature as binary feature. Variable length suffixes 
of a word can be matched with predefined lists of 
useful suffixes for different classes of NEs. For 
Bengali, we have considered the different suffixes 
that may be particularly helpful in detecting person 
(e.g., -babu, -da, -di etc.). 
36
Word prefix: Prefix information of a word is also 
helpful. A fixed length prefix of the current and the 
surrounding words might be treated as features. 
Here, the prefixes of length up to three have been 
considered for all the language. 
Rare word: The lists of most frequently occurring 
words in the training sets have been calculated for 
all the five languages. The words that occur more 
than 10 times are considered as the frequently oc-
curring words in Bengali and Hindi. For Telugu, 
Oriya and Urdu, the cutoff frequency was chosen 
to be 5. Now, a binary feature ?RareWord? is de-
fined as: If current word is found to appear in the 
frequent word list then it is set to 1; otherwise, set 
to 0.   
First word: If the current token is the first word of 
a sentence, then this feature is set to 1. Otherwise, 
it is set to 0. 
Contains digit: For a token, if it contains digit(s) 
then the feature ?ContainsDigit? is set to 1. This 
feature is helpful for identifying the numbers.  
Made up of four digits: For a token if all the char-
acters are digits and having 4 digits then the fea-
ture ?FourDigit? is set to 1. This is helpful in iden-
tifying the time (e.g., 2007sal) and numerical (e.g., 
2007) expressions. 
Made up of two digits: For a token if all the char-
acters are digits and having 2 digits then the fea-
ture ?TwoDigit? is set to 1. This is helpful for iden-
tifying the time expressions (e.g., 12 ta, 8 am, 9 pm) 
in general. 
Contains digits and comma: For a token, if it con-
tains digits and commas then the feature ?Con-
tainsDigitsAndComma? is set to 1. This feature is 
helpful in identifying named entity measurement 
expressions (e.g., 120,45,330 taka) and numerical 
numbers (e.g., 120,45,330) 
Contains digits and slash: If the token contains 
digits and slash then the feature ?ContainsDigi-
tAndslash? is set to 1. This helps in identifying 
time expressions (e.g., 15/8/2007). 
Contains digits and hyphen: If the token contains 
digits and hyphen then the feature ?ContainsDigit-
sAndHyphen? is set to 1. This is helpful for the 
identification of time expressions (e.g., 15-8-2007). 
Contains digits and period: If the token contains 
digits and periods then the feature ?ContainsDigit-
sAndPeriod? is set to 1. This helps to recognize 
numerical quantities (e.g., 120453.35) and meas-
urements (e.g., 120453.35 taka). 
Contains digits and percentage: If the token con-
tains digits and percentage symbol then the feature 
?ContainsDigitsAndPercentage? is set to 1. This 
helps to recognize measurements (e.g., 120%). 
Named Entity Information: The NE tag of the 
previous word is also considered as the feature, i.e., 
the combination of the current and the previous 
output token has been considered. This is the only 
dynamic feature in the experiment. 
Gazetteer Lists: Various gazetteer lists have been 
created from a tagged Bengali news corpus (Ekbal 
and Bandyopadhyay, 2007d) for Bengali. The first, 
last and middle names of person for Hindi have 
been created from the election commission data
6
. 
The person name collections had to be processed in 
order to use it in the CRF framework. The simplest 
approach of using these gazetteers is to compare 
the current word with the lists and make decisions. 
But this approach is not good, as it can?t resolve 
ambiguity. So, it is better to use these lists as the 
features of the CRF. If the current token is in a par-
ticular list, then the corresponding feature is set to 
1 for the current/previous/next token; otherwise, 
set to 0. The list of gazetteers is shown in Table 2. 
3.3 Nested Named Entity Identification 
One of the important tasks of the IJCNLP-NER 
shared task was to identify the internal named enti-
ties within the maximal NEs. In the training data, 
only the type of the maximal NEs were given. In 
order to identify the internal NEs during testing, 
we have defined some rules. After testing the un-
annotated test data with the CRF based NER sys-
tem, it is searched to find the sequence of NE tags. 
The last NE tag in the sequence is assigned as the 
NE tag of the maximal NE. The NE tags of the 
constituent NEs may either be changed or may not 
be changed. The NE tags are changed with the help 
of rules and various gazetteer lists. We identified 
NEM (Named entity measurement), NETI (Named 
entity time expressions), NEO (Named entity or-
ganization names), NEP (Named entity person 
names) and NEL (Named entity locations) to be 
the potential NE tags, where nesting could occur. 
A NEM expression may contain NEN, an NETI 
may contain NEN, an NEO may contain NEP/ 
NEL, an NEL may contain NEP/NETP/NED and 
an NEP may contain NEL expressions. The nested 
                                                
 
6 
http://www.eci.gov.in/DevForum/Fullname.asp 
37
NEN tags could be identified by simply checking 
whether it contains digits only and checking the 
lists of cardinal and ordinal numbers.  
  Gazetteer  Number 
of entries
 
 Feature Descrip-
tions 
Designation 
words in Bengali 
947 ?Designation? set to 
1, otherwise 0  
Organization 
names in Bengali 
2, 225 ?Organization? set 
to 1, otherwise 0. 
Organization 
suffixes in Ben-
gali 
94 ?OrgSuffix? set to 
1, otherwise 0 
Person prefix for 
Bengali 
245 ?PersonPrefix? set 
to 1, otherwise set 
to 0 
First person 
names in Bengali
27,842 ?FirstName? set to 
1, otherwise 0 
Middle names in 
Bengali 
1,491 ?MiddleName? set 
to 1, otherwise 0 
Surnames in 
Bengali 
5,288 ?SurName? set to 1, 
otherwise 0 
Common loca-
tion word in 
Bengali 
75 ?CommonLocation? 
set 1, otherwise 0 
Action verb in 
Bengali 
215 ?ActionVerb? set to 
1, otherwise 0 
First person 
names in Hindi 
1,62,881 ?FirstName? set to 
1, otherwise 0 
Middle person 
names in Hindi 
450 ?MiddleName? set 
to 1, otherwise 0 
Last person 
names in Hindi 
3,573 ?SurName? set to 1, 
otherwise 0 
Location names 
in Bengali 
7,870 ?LocationName? 
set to 1, otherwise 
0 
Week days in 
Bengali, Hindi 
and Telugu 
14 ?WeekDay? set to 
1, otherwise 0 
Month names in 
Bengali, Hindi 
and Telugu 
24 ?MonthName? set 
to 1, otherwise 0 
Measurements in 
Bengali, Hindi 
and Telugu 
52 ?Measurement? set 
to 1, otherwise 0. 
 Table 2. Named entity gazetteer list   
The procedure for identifying the nested NEs are 
shown below: 
Step1: Test the unannotated test set. 
Step 2: Look for the sequence of NE tags. 
Step 3: All the words in the sequence will belong     
to a maximal NE. 
Step 4: Assign the last NE tag in the sequence to 
the maximal NE. 
Step 5: The test set is searched to look whether 
each component word appears with a NE tag.  
Step 6: Assign the particular NE tag to the compo-
nent if it appears in the test set with that NE tag. 
Otherwise, search the gazetteer lists as shown in 
Tables 1-2 to assign the tag. 
4 Evaluation 
The evaluation measures used for all the five lan-
guages are precision, recall and F-measure. These 
measures are calculated in three different ways: 
(i). Maximal matches: The largest possibles 
named entities are matched with the reference data. 
(ii). Nested matches: The largest possible as 
well as nested named entities are matched. 
(iii). Maximal lexical item matches: The lexical 
items inside the largest possible named entities are 
matched. 
(iv). Nested lexical item matches: The lexical 
items inside the largest possible as well as nested 
named entities are matched.  
5 Experimental Results 
The CRF based NER system has been trained and 
tested with five different Indian languages namely, 
Bengali, Hindi, Telugu, Oriya and Urdu data.  The 
training and test sets statistics are presented in Ta-
ble 3. Results of evaluation as explained in the 
previous section are shown in Table 4. The F-
measures for the nested lexical match are also 
shown individually for each named entity tag sepa-
rately in Table 5. 
Experimental results of Table 4 show that the 
CRF based NER system performs best for Bengali 
with maximal F-measure of 55.36%, nested F-
measure of 61.46% and lexical F-measure 59.39%. 
The system has demonstrated the F-measures of 
35.37%, 36.75% and 33.12%, respectively for 
maximal, nested and lexical matches. The system 
has shown promising precision values for Hindi. 
But due to the low recall values, the F-measures 
get reduced. The large difference between the re-
call and precision values in the evaluation results 
of Hindi indicates that the system is not able to 
retrieve a significant number of NEs from the test 
38
data. In comparison to Hindi, the precision values 
are low and the recall values are high for Bengali. 
It can be decided from the evaluation results that 
system retrieves more NEs in Bengali than Hindi 
but involves more errors. The lack of features in 
Oriya, Telugu and Urdu might be the reason be-
hind their poor performance.   
Language
 
Number of 
tokens in the 
training set 
Number of to-
kens in the test 
set 
Bengali 122,467 30,505 
Hindi 502,974 38,708 
Telugu 64,026 6,356 
Oriya 93,173 24,640 
Urdu 35,447 3,782 
Table 3: Training and Test Sets Statistics  
Tag Bengali Hindi Oriya Telugu Urdu 
NEP 85.68 21.43 43.76 1.9 7.69 
NED 35.9 38.70 NF NF NF 
NEO 52.53 NF 5.60 NF 22.02 
NEA 26.92 30.77 NF NF NF 
NEB NF NF NF NF NF 
NETP 61.44 NF 12.55 NF NF 
NETO 45.98 NF NF NF NF 
NEL 80.00 22.70 31.49 0.73 50.14 
NETI 53.43 49.60 27.08 7.64 49.28 
NEN 30.12 85.40 9.19 9.16 NF 
NEM 79.08 36.64 7.56 NF 79.27 
NETE 18.06 1.64 NF 5.74 NF 
Table 4. Evaluation for Specific NE Tags (F-
Measures for nested lexical match) [NF: Nothing 
found]  
Experimental results of Table 5 show the F-
measures for the nested lexical item matches for 
individual NE tags. For Bengali, the system has 
shown reasonably high F-measures for NEP, NEL 
and NEM tags and medium F-measures for NETP, 
NETI, NEO and NETO tags. The overall F-
measures in Bengali might have reduced due to 
relatively poor F-measures for NETE, NEN, NEA 
and NED tags. For Hindi, the highest F-measure 
obtained is 85.4% for NEN tag followed by NETI, 
NED, NEM, NEA, NEL and NEP tags. In some 
cases, the system has shown better F-measures for 
Hindi than Bengali also. The system has performed 
better for NEN, NED and NEA tags in Hindi than 
all other languages. 
6 Conclusion  
We have developed a named entity recognition 
system using Conditional Random Fields for the 
five different Indian languages, namely Bengali, 
Hindi, Telugu, Oriya and Urdu. We have consid-
ered the contextual window of size five, prefix and 
suffix of length upto three of the current word, NE 
information of the previous word, different digit 
features and the frequently occurring word lists. 
The system also uses linguistic features extracted 
from the various gazetteer lists for Bengali and 
Hindi. Evaluation results show that the system per-
forms best for Bengali. The performance of the 
system for Bengali can further be improved by in-
cluding the part of speech (POS) information of the 
current and/or the surrounding word(s). The per-
formance of the system for other languages can be 
improved with the use of different linguistic fea-
tures as like Bengali.  
The system did not perform as expected due to 
the problems faced during evaluation regarding the 
tokenization. We have tested the system for Ben-
gali with 10-fold cross validation and obtained im-
pressive results.  
References 
Babych, Bogdan, A. Hartley. Improving machine trans-
lation quality with automatic named entity recogni-
tion. In Proceedings of EAMT/EACL 2003 Workshop 
on MT and other language technology tools, 1-8, 
Hungary. 
Bennet, Scott W.; C. Aone; C. Lovell. 1997. Learning to 
Tag Multilingual Texts Through Observation. In 
Proceedings of EMNLP, 109-116,  Rhode Island.  
Bikel, Daniel M., R. Schwartz, Ralph M. Weischedel. 
1999. An Algorithm that Learns What?s in Name. 
Machine Learning (Special Issue on NLP), 1-20. 
Bothwick, Andrew. 1999. A Maximum Entropy Ap-
proach to Named Entity Recognition. Ph.D. Thesis, 
New York University. 
Chinchor, Nancy. 1995. MUC-6 Named Entity Task 
Definition (Version 2.1). MUC-6, Columbia, Mary-
land.   
39
     
Table 5. Evaluation of the Five Languages  
Chinchor, Nancy. 1998. MUC-7 Named Entity Task 
Definition (Version 3.5). MUC-7. Fairfax, Vir-
ginia. 
Cunningham, H. 2001. GATE: A general architecture 
for text engineering. Comput.  Humanit. (36), 223-
254. 
Ekbal, Asif, and S. Bandyopadhyay. 2007a. Pattern 
Based Bootstrapping Method for Named Entity 
Recognition. In Proceedings of 6
th 
International 
Conference on Advances in Pattern Recognition, 
Kolkata,    India, 349-355. 
Ekbal, Asif, and S. Bandyopadhyay. 2007b. Lexical 
Pattern Learning from Corpus Data for Named En-
tity Recognition. In Proceedings of the 5
th 
Interna-
tional Conference on Natural Language Process-
ing, Hyderabad, India, 123-128. 
Ekbal, Asif, Naskar, Sudip and S. Bandyopadhyay.   
2007c. Named Entity Recognition and Translitera-
tion in Bengali. Named Entities: Recognition, 
Classification and Use, Special Issue of Lingvisti-
cae Investigationes Journal, 30:1 (2007), 95-114. 
Ekbal, Asif, and S. Bandyopadhyay. 2007d. A Web-
based Bengali News Corpus for Named Entity 
Recognition. Language Resources and Evaluation 
Journal (Accepted) 
Lafferty, J., McCallum, A., and Pereira, F. 2001. 
Conditional random fields: Probabilistic models 
for segmenting and labeling sequence data. In 
Proc. of 18
th
 International Conference on Machine 
learning. 
Li, Wei and Andrew McCallum. 2003. Rapid Devel-
opment of Hindi Named Entity Recognition Using 
Conditional Random Fields and Feature Induc-
tions, ACM TALIP, 2(3), (2003), 290-294. 
McCallum, A.; W. Li. 2003. Early Results for Named 
Entity Recognition with Conditional Random 
Fields, Feature Induction and Web-Enhanced 
Lexicons. In Proceedings CoNLL-03, Edmanton, 
Canada. 
Moldovan, Dan I., Sanda M. Harabagiu, Roxana 
Girju, P. Morarescu, V. F. Lacatusu, A. Novischi, 
A. Badulescu, O. Bolohan. 2002. LCC Tools for 
Question Answering. In Proceedings of the TREC, 
Maryland, 1-10. 
Pinto, D., McCallum, A., Wei, X., and Croft, W. B. 
2003. Table extraction using conditional random 
fields. In Proceedings of SIGIR 03 Conference, 
Toronto, Canada. 
Sekine, Satoshi. 1998. Description of the Japanese 
NE System Used for MET-2, MUC-7, Fairfax, 
Virginia. 
Sha, F. and Pereira, F. 2003. Shallow parsing with 
conditional random fields. In Proceedings of Hu-
man Language Technology, NAACL. 
Measure
Precision Recall F-measure 
Language P
m 
P
n 
P
l 
R
m 
R
n 
R
l 
F
m 
F
n 
F
l 
Bengali 51.63 47.74 52.90 59.60 61.46 67.71 55.36 61.46 59.39
Hindi 71.05 76.08 80.59 23.54 24.23 20.84 35.37 36.75 33.12
Oriya 27.12 27.18 50.40 12.88 10.53 20.07 17.47 15.18 28.71
Telugu 1.70 2.70 8.10 0.538 0.539 3.34 0.827 0.902 4.749
Urdu 49.16 48.93 54.45 21.95 20.15 26.36 30.35 28.55 35.52
M: Maximal,  n: Nested, l: Lexical 
40
Proceedings of the 2009 Named Entities Workshop, ACL-IJCNLP 2009, pages 80?83,
Suntec, Singapore, 7 August 2009. c?2009 ACL and AFNLP
English to Hindi Machine Transliteration System at NEWS 2009 
 
Amitava Das, Asif Ekbal, Tapabrata Mandal and Sivaji Bandyopadhyay 
Computer Science and Engineering Department 
Jadavpur University, Kolkata-700032, India 
amitava.research@gmail.com, asif.ekbal@gmail.com, ta-
pabratamondal@gmail.com, sivaji_cse_ju@yahoo.com 
 
 
Abstract 
 
This paper reports about our work in the 
NEWS 2009 Machine Transliteration Shared 
Task held as part of ACL-IJCNLP 2009. We 
submitted one standard run and two non-
standard runs for English to Hindi translitera-
tion. The modified joint source-channel model 
has been used along with a number of alterna-
tives. The system has been trained on the 
NEWS 2009 Machine Transliteration Shared 
Task datasets. For standard run, the system 
demonstrated an accuracy of 0.471 and the 
mean F-Score of 0.861. The non-standard runs 
yielded the accuracy and mean F-scores of 
0.389 and 0.831 respectively in the first one 
and 0.384 and 0.828 respectively in the second 
one. The non-standard runs resulted in sub-
stantially worse performance than the standard 
run. The reasons for this are the ranking algo-
rithm used for the output and the types of to-
kens present in the test set. 
1 Introduction 
Technical terms and named entities (NEs) consti-
tute the bulk of the Out Of Vocabulary (OOV) 
words. Named entities are usually not found in 
bilingual dictionaries and are very generative in 
nature. Proper identification, classification and 
translation of Named entities (NEs) are very im-
portant in many Natural Language Processing 
(NLP) applications. Translation of NEs involves 
both translation and transliteration. Translitera-
tion is the method of translating into another lan-
guage by expressing the original foreign word 
using characters of the target language preserv-
ing the pronunciation in their source language. 
Thus, the central problem in transliteration is 
predicting the pronunciation of the original word. 
Transliteration between two languages that use 
the same set of alphabets is trivial: the word is 
left as it is. However, for languages those use 
different alphabet sets the names must be transli-
terated or rendered in the target language alpha-
bets. Transliteration of NEs is necessary in many 
applications, such as machine translation, corpus 
alignment, cross-language Information Retrieval, 
information extraction and automatic lexicon 
acquisition. In the literature, a number of transli-
teration algorithms are available involving Eng-
lish (Li et al, 2004; Vigra and Khudanpur, 2003; 
Goto et al, 2003), European languages (Marino 
et al, 2005) and some of the Asian languages, 
namely Chinese (Li et al, 2004; Vigra and Khu-
danpur, 2003), Japanese (Goto et al, 2003; 
Knight and Graehl, 1998), Korean (Jung et al, 
2000) and Arabic (Al-Onaizan and Knight, 
2002a; Al-Onaizan and Knight, 2002c). Recent-
ly, some works have been initiated involving 
Indian languages (Ekbal et al, 2006; Ekbal et al, 
2007; Surana and Singh, 2008). 
 
2 Machine Transliteration Systems  
Three transliteration models have been used that 
can generate the Hindi transliteration from an 
English named entity (NE). An English NE is 
divided into Transliteration Units (TUs) with 
patterns C*V*, where C represents a consonant 
and V represents a vowel. The Hindi NE is di-
vided into TUs with patterns C+M?, where C 
represents a consonant or a vowel or a conjunct 
and M represents the vowel modifier or matra. 
The TUs are the lexical units for machine transli-
teration. The system considers the English and 
Hindi contextual information in the form of col-
located TUs simultaneously to calculate the plau-
sibility of transliteration from each English TU 
to various Hindi candidate TUs and chooses the 
one with maximum probability. This is equiva-
lent to choosing the most appropriate sense of a 
word in the source language to identify its repre-
sentation in the target language. The system 
learns the mappings automatically from the bi-
lingual NEWS training set being guided by lin-
80
guistic features/knowledge. The system consid-
ers the linguistic knowledge in the form of con-
juncts and/or diphthongs in English and their 
possible transliteration in Hindi. The output of 
the mapping process is a decision-list classifier 
with collocated TUs in the source language and 
their equivalent TUs in collocation in the target 
language along with the probability of each deci-
sion obtained from the training set. Linguistic 
knowledge is used in order to make the number 
of TUs in both the source and target sides equal. 
A Direct example base has been maintained that 
contains the bilingual training examples that do 
not result in the equal number of TUs in both the 
source and target sides during alignment. The 
Direct example base is checked first during ma-
chine transliteration of the input English word. If 
no match is obtained, the system uses direct or-
thographic mapping by identifying the equivalent 
Hindi TU for each English TU in the input and 
then placing the Hindi TUs in order. The transli-
teration models are described below in which S 
and T denotes the source and the target words 
respectively: 
 
? Model A 
This is essentially the joint source-channel model 
(Hazhiou et al, 2004) where the previous TUs 
with reference to the current TUs in both the 
source (s) and the target sides (t) are considered 
as the context.  
1
1
( | ) ( , | , )k k
k
K
P S T P s t s t
?
=
= < > < >?  
( ) arg max { ( ) ( | )}S T S P T P S TT? = ?  
? Model B 
This is basically the trigram model where the 
previous and the next source TUs are considered 
as the context.  
 1, 1
1
( | ) ( , | )k k k
k
K
P S T P s t s s
? +
=
= < >?  
  ( ) arg max { ( ) ( | )}S T S P T P S TT? = ?  
? Model C 
In this model, the previous and the next TUs in 
the source and the previous target TU are 
considered as the context. This is the  improved 
modified joint source-channel model. 
1, 1
1
( | ) ( , | , )k k k
k
K
P S T P s t s t s
? +
=
= < > < >?   
 ( ) arg max { ( ) ( | )}S T S P T P S TT? = ?               
For NE transliteration, P(T), i.e., the 
probability of transliteration in the target 
language, is calculated from a English-Hindi 
bilingual database of approximately 961,890 
English person names, collected from the web1.  
If, T is not found in the dictionary, then a very 
small value is assigned to P(T). These models 
have been desribed in details in Ekbal et al 
(2007). 
 
? Post-Processing 
Depending upon the nature of errors involved in 
the results, we have devised a set of translitera-
tion rules. A few rules have been devised to pro-
duce more spelling variations. Some examples 
are given below. 
Spelling variation rules 
Badlapur ??????? | ??????? 
Shree | Shri ? 
 
3 Experimental Results   
We have trained our transliteration models using 
the English-Hindi datasets obtained from the 
NEWS 2009 Machine Transliteration Shared 
Task (Li et al, 2009). A brief statistics of the 
datasets are presented in Table 1. Out of 9975 
English-Hindi parallel examples in the training 
set, 4009 are multi-words. During training, we 
have split these multi-words into collections of 
single word transliterations. It was observed that 
the number of tokens in the source and target 
sides mismatched in 22 multi-words and these 
cases were not considered further. Following are 
some examples:  
Paris Charles de Gaulle ????  
???? ??? ? ?????  
South Arlington Church of 
Christ ???? ???? 
In the training set, some multi-words were partly 
translated and not transliterated. Such examples 
were dropped from the training set. Finally, the 
training set consists of 15905 single word Eng-
lish-Hindi parallel examples.  
                                                 
1http://www.eci.gov.in/DevForum/Fullname.asp  
81
      
Set Number of examples 
Training 9975 
Development 974 
Test 1000 
Table 1. Statistics of Dataset 
 
The output of the modified joint source-
channel model is given more priority during out-
put ranking followed by the trigram and the joint 
source-channel model. During testing, the Direct 
example base is searched first to find the transli-
teration. Experimental results on the develop-
ment set yielded the accuracy of 0.442 and mean 
F-score of 0.829. Depending upon the nature of 
errors involved in the results, we have devised a 
set of transliteration rules. The use of these trans-
literation rules increased the accuracy and mean 
F-score values up to 0.489 and 0.881 respective-
ly.  
The system has been evaluated for the test set 
and the detailed reports are available in Li et al 
(2009). There are 88.88% unknown examples in 
the test set. We submitted one standard run in 
which the outputs are provided for the modified 
joint source-channel model (Model C), trigram 
model (Model B) and joint source-channel model 
(Model A). The same ranking procedure (i.e., 
Model C, Model B and Model A) has been fol-
lowed as that of the development set. The output 
of each transliteration model has been post-
processed with the set of transliteration rules. For 
each word, three different outputs are provided in 
a ranked order. If the outputs of any two models 
are same for any word then only two outputs are 
provided for that particular word. Post-
processing rules generate more number of possi-
ble transliteration output. Evaluation results of 
the standard run are shown in Table 2.  
 
Parameters Accuracy 
Accuracy in top-1 0.471 
Mean F-score 0.861 
Mean Reciprocal Rank 
(MRR) 
0.519 
Mean Average Preci-
sion (MAP)ref 
0.463 
MAP10 0.162 
MAPsys 0.383 
Table 2. Results of the standard run  
 
The results of the two non-standard runs are 
presented in Table 3 and Table 4 respectively.  
Parameters Accuracy 
Accuracy in top-1 0.389 
Mean F-score 0.831 
Mean Reciprocal Rank 
(MRR) 
0.487 
Mean Average Preci-
sion (MAP)ref 
0.385 
MAP10 0.16 
MAPsys 0.328 
  
Table 3. Results of the non-standard run 1 
 
Parameters Accuracy 
Accuracy in top-1 0.384 
Mean F-score 0.823 
Mean Reciprocal Rank 
(MRR) 
0.485 
Mean Average Precision 
(MAP)ref 
0.380 
MAP10 0.16 
MAPsys 0.325 
 
Table 4. Results of the non-standard run2 
 
In both the non-standard runs, we have used 
an English-Hindi bilingual database of approx-
imately 961, 890 examples that have been col-
lected from the web2. This database contains the 
(frequency) of the corresponding English-Hindi 
name pair. Along with the outputs of three mod-
els, the output obtained from this bilingual data-
base has been also provided for each English 
word. In the first non-standard run, only the most 
frequent transliteration has been considered. But, 
in the second non-standard run all the possible 
transliteration have been considered. It is to be 
noted that in these two non-standard runs, the 
transliterations obtained from the bilingual data-
base have been kept first in the ranking. Results 
of the tables show quite similar performance in 
both the runs. But the non-standard runs resulted 
in substantially worse performance than the stan-
dard run. The reasons for this are the ranking 
algorithm used for the output and the types of 
tokens present in the test set. The additional da-
                                                 
2http://www.eci.gov.in/DevForum/Fullname.asp  
82
taset used for the non-standard runs is mainly 
census data consisting of only Indian person 
names. The NEWS 2009 Machine Transliteration 
Shared Task training set is well distributed with 
foreign names (Ex. Sweden, Warren), common 
nouns (Mahfuz, Darshanaa) and a few non 
named entities. Hence the training set for the 
non-standard runs was biased towards the Indian 
person name transliteration pattern. Additional 
training set was quite larger (961, 890) than the 
shared task training set (9,975). Actually outputs 
of non-standard runs have more alternative trans-
literation outputs than the standard set. That 
means non-standard sets are superset of standard 
set. Our observation is that the ranking algorithm 
used for the output and biased training are the 
main reasons for the worse performance of the 
non-standard runs. 
4 Conclusion  
This paper reports about our works as part of the 
NEWS 2009 Machine Transliteration Shared 
Task. We have used the modified joint source-
channel model along with two other alternatives 
to generate the Hindi transliteration from an Eng-
lish word (to generate more spelling variations of 
Hindi names). We have also devised some post-
processing rules to remove the errors. During 
standard run, we have obtained the word accura-
cy of 0.471 and mean F-score of 0.831. In non-
standard rune, we have used a bilingual database 
obtained from the web. The non-standard runs 
yielded the word accuracy and mean F-score 
values of 0.389 and 0.831 respectively in the first 
run and 0.384 and 0.823 respectively in the 
second run. 
 
References  
Al-Onaizan, Y. and Knight, K. 2002a. Named 
Entity Translation: Extended Abstract. In 
Proceedings of the Human Language Tech-
nology Conference, 122? 124. 
Al-Onaizan, Y. and Knight, K. 2002b. Translat-
ing Named Entities using Monolingual and 
Bilingual Resources. In Proceedings of the 
40th Annual Meeting of the ACL, 400?408, 
USA. 
Ekbal, A. Naskar, S. and Bandyopadhyay, S. 
2007. Named Entity Transliteration. Interna-
tional Journal of Computer Processing of 
Oriental Languages (IJCPOL), Volume 
(20:4), 289-310, World Scientific Publishing 
Company, Singapore. 
Ekbal, A., Naskar, S. and Bandyopadhyay, S. 
2006. A Modified Joint Source Channel 
Model for Transliteration. In Proceedings of 
the COLING-ACL 2006, 191-198, Australia. 
Goto, I., Kato, N., Uratani, N. and Ehara, T. 
2003. Transliteration Considering Context 
Information based on the Maximum Entropy 
Method. In Proceeding of the MT-Summit 
IX, 125?132, New Orleans, USA.  
Jung, Sung Young , Sung Lim Hong and Eunok 
Paek. 2000. An English to Korean Translite-
ration Model of Extended Markov Window. 
In Proceedings of International Conference 
on Computational Linguistics (COLING 
2000), 383-389. 
Knight, K. and Graehl, J. 1998. Machine Transli-
teration, Computational Linguistics, Volume 
(24:4), 599?612. 
Kumaran, A. and Tobias Kellner. 2007. A gener-
ic framework for machine transliteration. In 
Proc. of the 30th SIGIR. 
Li, Haizhou, A Kumaran, Min Zhang and Vla-
dimir Pervouchine. 2009. Whitepaper of 
NEWS 2009 Machine Transliteration Shared 
Task. In Proceedings of ACL-IJCNLP 2009 
Named Entities Workshop (NEWS 2009), Sin-
gapore. 
Li, Haizhou, A Kumaran, Vladimir Pervouchine 
and Min Zhang. 2009.  Report on NEWS 2009 
Machine Transliteration Shared Task. In Pro-
ceedings of ACL-IJCNLP 2009  amed Entities 
Workshop (NEWS 2009), Singapore. 
Li, Haizhou, Min Zhang and Su Jian. 2004. A 
Joint Source-Channel Model for Machine 
Transliteration. In Proceedings of the 42nd 
Annual Meeting of the ACL, 159-166. Spain. 
Marino, J. B., R. Banchs, J. M. Crego, A. de 
Gispert, P. Lambert, J. A. Fonollosa and M. 
Ruiz. 2005.  Bilingual n-gram Statistical 
Machine Translation. In Proceedings of the 
MT-Summit X, 275?282. 
Surana, Harshit, and Singh, Anil Kumar. 2008. A 
More Discerning and Adaptable Multilingual 
Transliteration Mechanism for Indian Lan-
guages. In Proceedings of the 3rd Interna-
tional Joint Conference on Natural Lan-
guage Processing (IJCNLP-08), 64-71, In-
dia. 
Vigra, Paola and Khudanpur, S. 2003. Translite-
ration of Proper Names in Cross-Lingual In-
formation Retrieval. In Proceedings of the 
ACL 2003 Workshop on Multilingual and 
Mixed-Language Named Entity Recognition, 
57?60. 
83
Coling 2010: Poster Volume, pages 232?240,
Beijing, August 2010
Topic-Based Bengali Opinion Summarization 
Amitava Das 
Department of Computer Science 
and Engineering  
Jadavpur University 
amitava.santu@gmail.com 
Sivaji Bandyopadhyay 
Department of Computer Science 
and Engineering  
Jadavpur University 
sivaji_cse_ju@yahoo.com 
 
Abstract 
In this paper the development of an opi-
nion summarization system that works on 
Bengali News corpus has been described. 
The system identifies the sentiment in-
formation in each document, aggregates 
them and represents the summary infor-
mation in text. The present sys-tem fol-
lows a topic-sentiment model for senti-
ment identification and aggregation. Top-
ic-sentiment model is designed as dis-
course level theme identification and the 
topic-sentiment aggregation is achieved 
by theme clustering (k-means) and Doc-
ument level Theme Relational Graph re-
presentation. The Document Level 
Theme Relational Graph is finally used 
for candidate summary sentence selection 
by standard page rank algorithms used in 
Information Retrieval (IR). As Bengali is 
a resource constrained language, the 
building of annotated gold standard cor-
pus and acquisition of linguistics tools 
for lexico-syntactic, syntactic and dis-
course level features extraction are de-
scribed in this paper. The reported accu-
racy of the Theme detection technique is 
83.60% (precision), 76.44% (recall) and 
79.85% (F-measure). The summarization 
system has been evaluated with Precision 
of 72.15%, Recall of 67.32% and F-
measure of 69.65%. 
1 Introduction 
The Web has become a rich source of various 
opinions in the form of product reviews, travel 
advice, social issue discussions, consumer com-
plaints, movie review, stock market predictions, 
real estate market predictions, etc. Present com-
putational systems need to extend the power of 
understanding the sentiment/opinion expressed in 
an electronic text to act properly in the society 
rather than dealing with the topic of a document. 
The topic-document model of information re-
trieval has been studied for a long time and sys-
tems are available publicly since last decade. On 
the contrary Opinion Mining/Sentiment Analysis 
is still an unsolved research problem. Although a 
few systems like Twitter Sentiment Analysis 
Tool1, TweetFeel2 are available in World Wide 
Web since last few years still more research ef-
forts are necessary to match the user satisfaction 
level and social need. 
Researchers have taken multiple approaches 
towards the problem of Opinion Summarization 
like Topic-sentiment model, Textual summaries 
at single document or multiple document pers-
pective and graphical summaries or visualization. 
The works on opinion tracking systems have ex-
plicitly incorporated temporal dimension. The 
topic-sentiment model is well established for 
opinion retrieval. 
The concept of reputation system was first in-
troduced in (Resnick et al, 2000). Reputation 
systems for both buyers and sellers are needed to 
earn each other?s trust in online interactions.  
Ku et al, (2005) selects representative words 
from a document set to identify the main con-
cepts in the document set. A term is considered 
to represent a topic if it appears frequently across 
documents or in each document. Different me-
thodologies have been used to assign weights to 
each word both at document level and paragraph 
level. The precision and recall values of the sys-
tem have been reported as 0.56 and 0.85. 
                                                 
1
 http://twittersentiment.appspot.com/ 
2http://www.tweetfeel.com/ 
232
Zhou et al (2006) have proposed the architec-
ture for generative summary from blogosphere. 
Typical multi-document summarization (MDS) 
systems focus on content selection followed by 
synthesis by removing redundancy across mul-
tiple input documents. The online discussion 
summarization system (Zhou et al, 2006) work 
on an online discussion corpus involving mul-
tiple participants and discussion topics are passed 
back and forth by various participants. MDS sys-
tems are insufficient in representing this aspect 
of the interactions. Due to the complex structure 
of the dialogue, similar subtopic structure identi-
fication in the participant-written dialogues is 
essential. Maximum Entropy Model (MEMM) 
and Support Vector Machine (SVM) have been 
used with a number of relevant features. 
Carenini et al (2006) present and compare 
two approaches to the task of multi document 
opinion summarization on evaluative texts. The 
first is a sentence extraction based approach 
while the second one is a natural language gener-
ation-based approach. Relevant extracted fea-
tures are categorized in two types: User Defined 
Features (UDF) and Crude Features (CF) as de-
scribed in (Hu and Liu, 2004).  
The summary generation technique uses the 
aggregation of the extracted features, CF and 
UDF. Opinion aggregation has been done by the 
two relevant features: opinion strength and polar-
ity. A new opinion distribution function feature 
has been introduced to capture the overall opi-
nion distributed in corpus. 
Kawai et al (2007) developed a news portal 
site called Fair News Reader (FNR) that recom-
mends news articles with different sentiments for 
a user in each of the topics in which the user is 
interested. FNR can detect various sentiments of 
news articles and determine the sentimental pre-
ferences of a user based on the sentiments of 
previously read articles by the user. News ar-
ticles crawled from various news sites are stored 
in a database. The contents are integrated as 
needed and the summary is presented on one 
page. A sentiment vector on the basis of word 
lattice model has been generated for every doc-
ument. A user sentiment model has been pro-
posed based on user sentiment state. The user 
sentiment state model works on the browsing 
history of the user. The intersection of the docu-
ments under User Vector and Sentiment Vector 
are the results. 
2 Resource Organization 
Resource acquisition is one of the most challeng-
ing obstacles to work with resource constrained 
languages like Bengali. Bengali is the fifth popu-
lar language in the World, second in India and 
the national language in Bangladesh. Extensive 
NLP research activities in Bengali have started 
recently but resources like annotated corpus, var-
ious linguistic tools are still unavailable for Ben-
gali in the required measure. The manual annota-
tion of gold standard corpus and acquisition of 
various tools used in the feature extraction for 
Bengali are described in this section. 
2.1 Gold Standard Data Acquisition 
2.1.1 Corpus 
For the present task a Bengali news corpus has 
been developed from the archive of a leading 
Bengali news paper available on the Web 
(http://www.anandabazar.com/). A portion of the 
corpus from the editorial pages, i.e., Reader?s 
opinion section or Letters to the Editor Section 
containing 28K word forms has been manually 
annotated with sentence level subjectivity and 
discourse level theme words. Detailed reports 
about this news corpus development in Bengali 
can be found in (Das and Bandyopadhyay, 
2009b). 
2.1.2 Annotation 
From the collected document set (Letters to the 
Editor Section), some documents have been cho-
sen for the annotation task. Some statistics about 
the Bengali news corpus is represented in the 
Table 1. Documents that have appeared within an 
interval of four months are chosen on the hypo-
thesis that these letters to the editors will be on 
related events. A simple annotation tool has been 
designed for annotating the sentences considered 
to be important for opinion summarization. 
Three annotators (Mr. X, Mr. Y and Mr. Z) par-
ticipated in the present task.  
<Story> 
?????????????????????.. 
?????????????????????.. 
<SS><TW>Sargeant O?Leary</TW> said ?the 
<TW>incident</TW> took place at 2:00pm.?</SS> 
?????????????????????.. 
</Story> 
Figure 1: XML Annotation Format 
 Annotators were asked to annotate sentences 
for summary and to mark the theme words (topi-
cal expressions) in those sentences. The docu-
ments with such annotated sentences are saved in 
233
XML format. Figure 1 shows the XML annota-
tion format. ?<SS>? marker denotes subjective 
sentences and ?<TW>? denotes the theme words. 
 Bengali NEWS Corpus Statistics 
Total number of  documents in the corpus 100 
Total number of sentences in the corpus 2234 
Average number of sentences in a document 22 
Total number of wordforms in the corpus 28807 
Average number of wordforms in a document 288 
Total number of distinct wordforms in the 
corpus 
17176 
Table 1: Bengali News Corpus Statistics 
The annotation tool highlights the sentiment 
words (Das and Bandyopadhyay, 2010a)3 by four 
different colors within a document according to 
their POS categories (Noun, Adjective, Adverb 
and Verb). This technique helps to increase the 
speed of annotation process. Finally 100 anno-
tated documents have been developed. 
2.1.3 Inter-annotator Agreement 
The agreement of annotations among three anno-
tators has been evaluated. The agreements of tag 
values at theme words level and sentence levels 
are listed in Tables 2 and 3 respectively. 
 
Annotators X vs. Y X Vs. Z Y Vs. Z Avg 
Percentage 82.64% 71.78% 80.47% 78.30% 
All Agree 69.06% 
Table 2: Agreement of annotators at theme 
words level 
 
Annotators X vs. Y X Vs. Z Y Vs. Z Avg 
Percentage 73.87% 69.06% 60.44% 67.8% 
All Agree 58.66% 
Table 3: Agreement of annotators at sentence 
level 
From the analysis of inter-annotator agree-
ment, it is observed that the agreement drops fast 
as the number of annotator?s increases. It is less 
possible to have consistent annotations when 
more annotators are involved. In the present task 
the inter-annotator agreement is better for theme 
words annotation rather than candidate sentence 
identification for summary though a small num-
ber of documents have been considered. 
Further discussion with annotators reveals that 
the psychology of annotators is to grasp as many 
as possible theme words identification during 
annotation but the same groups of annotators are 
more cautious during sentence identification for 
summary as they are very conscious to find out 
the most concise set of sentences that best de-
scribe the opinionated snapshot of any document. 
                                                 
3
 http://www.amitavadas.com/sentiwordnet.php 
The annotators were working independent of 
each other and they were not trained linguists.  
2.2 Subjectivity Classifier 
Work in opinion mining and classification often 
assumes the incoming documents to be opinio-
nated. Opinion mining system makes false hits 
while attempting to summarize non-subjective or 
factual sentences or documents. It becomes im-
perative to decide whether a given document 
contains subjective information or not as well as 
to identify which portions of the document are 
subjective or factual. This task is termed as sub-
jectivity detection in sentiment literature. The 
subjectivity classifier that uses SVM machine 
learning technique and described in (Das and 
Bandyopadhyay, 2009a) has been used here. The 
recall measure of the present classifier is greater 
than its precision value. The evaluation results of 
the classifier are 72.16% (Precision) and 76.00 
(recall) on the News Corpus.  
2.3 Feature Organization 
The set of features used in the present task have 
been categorized as Lexico-Syntactic, Syntactic 
and Discourse level features. These are listed in 
the Table 4 below and have been described in the 
subsequent subsections. 
 
Types Features 
Lexico-Syntactic 
POS 
SentiWordNet 
Frequency 
Stemming 
Syntactic Chunk Label Dependency Parsing Depth 
Discourse Level 
Title of the Document 
First Paragraph 
Term Distribution 
Collocation 
Table 4: Features 
2.3.1 Lexico-Syntactic Features 
2.3.1.1 Part of Speech (POS) 
It has been shown in (Hatzivassiloglou et. al., 
2000), (Chesley et. al., 2006) etc. that opinion 
bearing words in sentences are mainly adjective, 
adverb, noun and verbs. Many opinion mining 
tasks, like (Nasukawa et. al., 2003) are mostly 
based on adjective words. Details of the Bengali 
POS tagger used can be found in (Das and Ban-
dyopadhyay 2009b). 
234
2.3.1.2 SentiWordNet (Bengali) 
Words that are present in the SentiWordNet car-
ry opinion information. The developed Senti-
WordNet (Bengali) (Das and Bandyopadhyay, 
2010a) is used as an important feature during the 
learning process. These features are individual 
sentiment words or word n-grams (multiword 
entities) with strength measure as strong subjec-
tive or weak subjective. Strong and weak subjec-
tive measures are treated as a binary feature in 
the supervised classifier. Words which are col-
lected directly from SentiWordNet (Bengali) are 
tagged with positivity or negativity score. The 
subjectivity score of these words are calculated 
as:                 
| | | |s p nE S S= +  
where sE  is the resultant subjective measure 
and pS , nS  are the positivity and negativity 
scores respectively. 
2.3.1.3 Frequency 
Frequency always plays a crucial role in identify-
ing the importance of a word in the document. 
The system generates four separate high frequent 
word lists for four POS categories: Adjective, 
Adverb, Verb and Noun after function words are 
removed. Word frequency values are then effec-
tively used as a crucial feature in the Theme De-
tection technique. 
2.3.1.4 Stemming 
Several words in a sentence that carry opinion 
information may be present in inflected forms 
and stemming is necessary for them before they 
can be searched in appropriate lists. Due to non 
availability of good stemmers in Indian languag-
es especially in Bengali, a stemmer (Das and 
Bandyopadhyay, 2010b) based on stemming 
cluster technique has been used. This stemmer 
analyzes prefixes and suffixes of all the word 
forms present in a particular document. Words 
that are identified to have the same root form are 
grouped in a finite number of clusters with the 
identified root word as cluster center.  
2.3.2 Syntactic Features 
2.3.2.1 Chunk Label 
Chunk level information is effectively used as a 
feature in supervised classifier. Chunk labels are 
defined as B-X (Beginning), I-X (Intermediate) 
and E-X (End), where X is the chunk label. In 
the task of identification of Theme expressions, 
chunk label markers play a crucial role. Further 
details of development of chunking system could 
be found in (Das and Bandyopadhyay 2009b).  
2.3.2.2 Dependency Parser 
Dependency depth feature is very useful to iden-
tify Theme expressions. A particular Theme 
word generally occurs within a particular range 
of depths in a dependency tree. Theme expres-
sions may be a Named Entity (NE: person, or-
ganization or location names), a common noun 
(Ex: accident, bomb blast, strike etc) or words of 
other POS categories. It has been observed that 
depending upon the nature of Theme expressions 
it can occur within a certain depth in the depen-
dency tree for the sentence. A statistical depen-
dency parser has been used for Bengali as de-
scribed in (Ghosh et al, 2009). 
2.3.3 Discourse Level Features 
2.3.3.1 Positional Aspect 
Depending upon the position of the thematic 
clue, every document is divided into a number of 
zones. The features considered for each docu-
ment are Title words of the document, the first 
paragraph words and the words from the last two 
sentences. A detailed study was done on the 
Bengali news corpus to identify the roles of the 
positional aspect features of a document (first 
paragraph, last two sentences) in the detection of 
theme words and subjective sentences for gene-
rating the summary of the document. The impor-
tance of these positional features is shown in 
Tables 5 on the Bengali gold standard set. 
2.3.3.2 Title Words 
Title words of a document always carry some 
meaningful thematic information. The title word 
feature has been used as a binary feature during 
CRF based machine learning. 
2.3.3.3 First Paragraph Words 
People usually give a brief idea of their beliefs 
and speculations in the first paragraph of the 
document and subsequently elaborate or support 
them with relevant reasoning or factual informa-
tion. Hence first paragraph words are informative 
in the detection of Thematic Expressions.  
2.3.3.4 Words From Last Two Sentences 
Generally every document concludes with a 
summary of the opinions expressed in the docu-
ment. 
235
Positional Factors Bengali 
First Paragraph 56.80% 
Last Two Sentences 78.00% 
Table 5: Statistics on Positional Aspect. 
2.3.3.5 Term Distribution Model 
An alternative to the classical TF-IDF weighting 
mechanism of standard IR has been proposed as 
a model for the distribution of a word. The model 
characterizes and captures the informativeness of 
a word by measuring how regularly the word is 
distributed in a document. As discussed in Sec-
tion 1, Carenini et al (2006) have introduced the 
opinion distribution function feature to capture 
the overall opinion distributed in the corpus. 
Thus the objective is to estimate ( )d if w  that 
measures the distribution pattern of the k occur-
rences of the word wi in a document d. Zipf's law 
describes distribution patterns of words in an 
entire corpus. In contrast, term distribution mod-
els capture regularities of word occurrence in 
subunits of a corpus (e.g., documents, paragraphs 
or chapters of a book). A good understanding of 
the distribution patterns is useful to assess the 
likelihood of occurrences of a word in some spe-
cific positions (e.g., first paragraph or last two 
sentences) of a unit of text. Most term distribu-
tion models try to characterize the informative-
ness of a word identified by inverse document 
frequency (IDF). In the present work, the distri-
bution pattern of a word within a document for-
malizes the notion of topic-sentiment informa-
tiveness. This is based on the Poisson distribu-
tion. Significant Theme words are identified us-
ing TF, Positional and Distribution factor. The 
distribution function for each theme word in a 
document is evaluated as follows: 
( )1 1
1 1
( ) / ( ) /
n n
d i i i i i
i i
f w S S n TW TW n
? ?
= =
= ? + ?? ?
 
where n=number of sentences in a document 
with a particular theme word, Si=sentence id of 
the current sentence containing the theme word 
and Si-1=sentence id of the previous sentence 
containing the query term, iTW is the positional id 
of current Theme word and 1iTW ? is the positional 
id of the previous Theme word. 
2.3.3.6 Collocation 
Collocation with other thematic word/expression 
is undoubtedly an important clue for identifica-
tion of theme sequence patterns in a document. A 
window size of 5 including the present word is 
considered during training to capture the colloca-
tion with other thematic words/expressions. 
 
3 Theme Detection 
Term Frequency (TF) plays a crucial role to 
identify document relevance in Topic-Based In-
formation Retrieval. The motivation behind de-
veloping Theme detection technique is that in 
many documents relevant words may not occur 
frequently or irrelevant words may occur fre-
quently. Moreover for sentiment analysis topic 
words should have sentiment conceptuality. The 
Theme detection technique has been proposed to 
resolve these issues to identify discourse level 
relevant topic-semantic nodes in terms of word 
or expressions using a standard machine learning 
technique. The machine learning technique used 
here is Conditional Random Field (CRF)4. The 
theme word detection is defined as a sequence 
labeling problem. Depending upon the series of 
input feature, each word is tagged as either 
Theme Word (TW) or Other (O). 
4 Theme Clustering 
Theme clustering algorithms partition a set of 
documents into finite number of topic based 
groups or clusters in terms of theme 
words/expressions. The task of document cluster-
ing is to create a reasonable set of clusters for a 
given set of documents. A reasonable cluster is 
defined as the one that maximizes the within-
cluster document similarity and minimizes be-
tween-cluster similarities. There are two princip-
al motivations for the use of this technique in 
theme clustering setting: efficiency, and the clus-
ter hypothesis. 
The cluster hypothesis (Jardine and van Rijs-
bergen, 1971) takes this argument a step further 
by asserting that retrieval from a clustered col-
lection will not only be more efficient, but will in 
fact improve retrieval performance in terms of 
recall and precision. The basic notion behind this 
hypothesis is that by separating documents ac-
cording to topic, relevant documents will be 
found together in the same cluster, and non-
relevant documents will be avoided since they 
will reside in clusters that are not used for re-
trieval. Despite the plausibility of this hypothe-
sis, there is only mixed experimental support for 
it. Results vary considerably based on the clus-
                                                 
4
 http://crfpp.sourceforge.net 
236
tering algorithm and document collection in use 
(Willett, 1988; Shaw et al, 1996). 
Application of the clustering technique to the 
three sample documents results in the following 
theme-by-document matrix, A, where the rows 
represent Docl, Doc7 and Doc13 and the col-
umns represent the themes politics, sport, and 
travel.  
election cricket hotel
A parliament sachin vacation
governor soccer tourist
? ?
? ?
= ? ?
? ?? ?
 
The similarity between vectors is calculated 
by assigning numerical weights to these words 
and then using the cosine similarity measure as 
specified in the following equation.  
, ,
1
, .
N
k j k j i k i j
i
s q d q d w w
? ? ? ?
=
? ?
= = ?? ?? ? ? ---- (1) 
This equation specifies what is known as the 
dot product between vectors.  Now, in general, 
the dot product between two vectors is not par-
ticularly useful as a similarity metric, since it is 
too sensitive to the absolute magnitudes of the 
various dimensions. However, the dot product 
between vectors that have been length norma-
lized has a useful and intuitive interpretation: it 
computes the cosine of the angle between the 
two vectors. When two documents are identical 
they will receive a cosine of one; when they are 
orthogonal (share no common terms) they will 
receive a cosine of zero. Note that if for some 
reason the vectors are not stored in a normalized 
form, then the normalization can be incorporated 
directly into the similarity measure as follows.  
, ,1
2 2
, ,1 1
,
N
i k i ji
k j N N
i k i ki i
w w
s q d
w w
? ?
=
= =
?? ?
=? ?? ? ?
?
? ?  ----(2) 
Of course, in situations where the document 
collection is relatively static, it makes sense to 
normalize the document vectors once and store 
them, rather than include the normalization in the 
similarity metric. 
Calculating the similarity measure and using a 
predefined threshold value, documents are classi-
fied using standard bottom-up soft clustering k-
means technique. The predefined threshold value 
is experimentally set to 0.5 as shown in Table 6. 
A set of initial cluster centers is necessary in 
the beginning. Each document is assigned to the 
cluster whose center is closest to the document. 
After all documents have been assigned, the cen-
ter of each cluster is recomputed as the centroid 
or mean ?
?
 (where ?
?
 is the clustering coeffi-
cient) of its members, that 
is ( )1/
jj x c
c x?
? ?
?
= ? . The distance function is 
the cosine vector similarity function. 
ID Themes 1 2 3 
1  (administration) 0.63 0.12 0.04 
1  (good-government) 0.58 0.11 0.06 
1  (Society) 0.58 0.12 0.03 
1  (Law) 0.55 0.14 0.08 
2  !"# (Research) 0.11 0.59 0.02 
2 & ' (College) 0.15 0.55 0.01 
2 	
 (Higher Study) 0.12 0.66 0.01 
3  +,- (Jehadi) 0.13 0.05 0.58 
3 ,- (Mosque) 0.05 0.01 0.86 
3 23 (Musharaf) 0.05 0.01 0.86 
3 Proceedings of the ACL-HLT 2011 Student Session, pages 52?57,
Portland, OR, USA 19-24 June 2011. c?2011 Association for Computational Linguistics
PsychoSentiWordNet 
 
Amitava Das 
Department of Computer Science and Engineering  
Jadavpur University 
amitava.santu@gmail.com 
 
Abstract 
Sentiment analysis is one of the hot 
demanding research areas since last few 
decades. Although a formidable amount of 
research has been done but still the existing 
reported solutions or available systems are 
far from perfect or to meet the satisfaction 
level of end user's. The main issue may be 
there are many conceptual rules that govern 
sentiment, and there are even more clues 
(possibly unlimited) that can convey these 
concepts from realization to verbalization 
of a human being. Human psychology 
directly relates to the unrevealed clues; 
govern the sentiment realization of us. 
Human psychology relates many things 
like social psychology, culture, pragmatics 
and many more endless intelligent aspects 
of civilization. Proper incorporation of 
human psychology into computational 
sentiment knowledge representation may 
solve the problem. PsychoSentiWordNet is 
an extension over SentiWordNet that holds 
human psychological knowledge and 
sentiment knowledge simultaneously. 
1 Introduction 
In order to identify sentiment from a text, lexical 
analysis plays a crucial role. For example, words 
like love, hate, good and favorite directly indicate 
sentiment or opinion. Various previous works 
(Pang et al, 2002; Wiebe and Mihalcea, 2006; 
Baccianella et. al., 2010) have already proposed 
techniques for making dictionaries for those 
sentiment words. But polarity assignment of such 
sentiment lexicons is a hard semantic 
disambiguation problem. The regulating aspects 
which govern the lexical level semantic orientation 
are natural language context (Pang et al, 2002), 
language properties (Wiebe and Mihalcea, 2006), 
domain pragmatic knowledge (Aue and Gamon, 
2005), time dimension (Read, 2005), colors and 
culture (Strapparava and Ozbal, 2010) and many 
more unrevealed hidden aspects. Therefore it is a 
challenging and enigmatic research problem. 
What previous studies proposed is to attach prior 
polarity to each sentiment lexicon level. Prior 
polarity is an approximation value based on corpus 
heuristics based statistics and not exact. The 
probabilistic fixed point prior polarity scores do 
not solve the problem completely rather it shoves 
the problem into next level, called contextual 
polarity classification.  
The hypothesis we started with is that the 
summation of all the regulating aspects of 
sentiment orientation is human psychology and 
thus it is called multi-faceted problem (Liu, 2010). 
More precisely what we meant by human 
psychology is the all known and unknown aspects, 
directly or indirectly govern the sentiment 
orientation knowledge of us. The regulating 
aspects wrapped in the present 
PsychoSentiWordNet are Gender, Age, City, 
Country, Language and Profession.  
The PsychoSentiWordNet is an extension over 
the existing SentiWordNet to hold the possible 
psychological ingredients, governs the sentiment 
understandability of us. The PsychoSentiWordNet 
holds variable prior polarity scores, could be 
fetched depending upon those psychological 
regulating aspects. An example may illustrate the 
definition better for the concept ?Rock_Climbing?:  
Aspects (Age)  Polarity 
-----------------------------  ------------- 
Null   Positive 
50-54   Negative 
26-29   Positive 
52
In the previous example the described concept 
?Rock_Climbing? is generally positive as it is 
adventurous and people have it to make fun or 
excursion. But it demands highly physical ability 
thus may be not as good for aged people like the 
younger people.  
PsychoSentiWordNet provides good coverage as 
it an extension over SentiWordNet 3.0 
(Baccianella et. al., 2010). In this paper, we 
propose an interactive gaming (Dr Sentiment) 
technology to collect psycho-sentimental polarity 
for lexicons. 
In this section we have philosophically argued 
about the necessity of developing 
PsychoSentiWordNet. In the next section we will 
describe about the technical proposed architecture 
for building the lexical resource. Section 3 explains 
about some exciting outcomes that support the 
usefulness of the PsychoSentiWordNet. What we 
believe is the developed PsychoSentiWordNet will 
help automatic sentiment analysis research in many 
aspect and other disciplines as well, described in 
the section 4.The data structure and organization is 
described in section 5 and finally the present paper 
concluded with section 6. 
2 Dr Sentiment 
Dr Sentiment 1  is a template based interactive 
online game, which collects player?s sentiment by 
asking a set of simple template based questions and 
finally reveals a player?s sentimental status. Dr 
Sentiment fetches random words from 
SentiWordNet synsets and asks every player to tell 
about his/her sentiment polarity understanding 
regarding the concept behind.  
There are several motivations behind developing 
an intuitive game to automatically collect human 
psycho-sentimental orientation information.  
In the history of Information Retrieval research 
there is a milestone when ESP game2 (Ahn et al, 
2004) innovate the concept of a game to 
automatically label images available in the World 
Wide Web. It has been identified as the most 
reliable strategy to automatically annotate the 
online images. We are highly motivated by the 
success of the Image Labeler game.  
A number of research endeavors could be found 
in literature for creation of Sentiment Lexicon in 
                                                           
1
 http://www.amitavadas.com/Sentiment%20Game/ 
2
 http://www.espgame.org/ 
several languages and domains. These techniques 
can be broadly categorized in two genres, one 
follows classical manual annotation (Andreevskaia 
and Bergler, 2006);(Wiebe and Riloff, 2006); 
(Mohammad et al, 2008) techniques and the others 
proposed various automatic techniques (Tong, 
2001). Both types of techniques have few 
limitations. Manual annotation techniques are 
undoubtedly trustable but it generally takes time. 
Automatic techniques demands manual validations 
and are dependent on the corpus availability in the 
respective domain. Manual annotation technique 
required a large number of annotators to balance 
one?s sentimentality in order to reach agreement. 
But human annotators are quite unavailable and 
costly. 
But sentiment is a property of human 
intelligence and is not entirely based on the 
features of a language. Thus people?s involvement 
is required to capture the sentiment of the human 
society. We have developed an online game to 
attract internet population for the creation of 
PsychoSentiWordNet automatically. Involvement 
of Internet population is an effective approach as 
the population is very high in number and ever 
growing (approx. 360,985,492) 3 . Internet 
population consists of people with various 
languages, cultures, age etc and thus not biased 
towards any domain, language or particular 
society. The Sign Up form of the ?Dr Sentiment? 
game asks the player to provide personal 
information such as Sex, Age, City, Country, 
Language and Profession.  
The lexicons tagged by this system are credible 
as it is tagged by human beings. In either way it is 
not like a static sentiment lexicon set as it is 
updated regularly. Almost 100 players per day are 
currently playing it throughout the world in 
different languages. 
The game has four types of question templates. 
For further detailed description the question 
templates are named as Q1, Q2, Q3 and Q4. To 
make the gaming interface more interesting images 
has been added with the help of Google image 
search API 4  and to avoid biasness we have 
randomized among the first ten images retrieved 
by Google. Snapshots of different screens from the 
game are presented in Figure 1. 
                                                           
3
 http://www.internetworldstats.com/stats.htm 
4
 http://code.google.com/apis/imagesearch/ 
53
 Figure 1: Snapshots from Dr Sentiment Game 
 
2.1 Gaming Strategy 
There are four types of questions: Q1, Q2, Q3 and 
Q4. Dr Sentiment asks 30 questions to each 
player.There are predefined distributions of each 
question type as 11 for Q1, 11 for Q2, 4 for Q3 and 
4 for Q4. There is no thumb rule behind the 
cardinals rather they are arbitrarily chosen and 
randomly changed for experimentation. The 
questions are randomly asked to keep the game 
more interesting. 
2.2 Q1 
An English word from the English SentiWordNet 
synset is randomly chosen. The Google image 
search API is fired with the word as a query. An 
image along with the word itself is shown in the 
Q1 page of the game.  
Players press the different emoticons (Fig 2) to 
express their sentimentality. The interface keeps 
log records of each interaction. 
 
Extreme 
Positive Positive Neutral Negative 
Extreme 
Negative 
 
 
 
 
 
Figure 2: Emoticons to Express Player?s 
Sentiment 
2.3 Q2 
This question type is specially designed for relative 
scoring technique. For example: good and better 
both are positive but we need to know which one is 
more positive than other. Table 1 shows how in 
SentiWordNet relative scoring has been made. 
With the present gaming technology relative 
polarity scoring has been assigned to each n-n 
word pair combination. 
Now about the technical solution how we did it. 
Randomly n (presently 2-4) words have been 
chosen from the source SentiWordNet synsets 
along with their images as retrieved by Google 
API. Each player is then asked to select one of 
them that he/she likes most. The relative score is 
calculated and stored in the corresponding log log 
table. 
Word Positivity Negativity 
Good 0.625 0.0 
Better 0.875 0.0 
Best 0.980 0.0 
Table 1: Relative Sentiment Scores from 
SentiWordNet 
2.4 Q3 
The player is asked for any positive word in his/her 
mind. This technique helps to increase the 
coverage of existing SentiWordNet. The word is 
then added to the PsychoSentiWordNet and further 
used in Q1 to other users to note their 
sentimentality about the particular word. 
2.5 Q4 
A player is asked by Dr Sentiment about any 
negative word. The word is then added to the 
PsychoSentiWordNet and further used in Q1 to 
54
other users to note their sentimentality about the 
particular word. 
2.6 Comment Architecture 
There are three types of Comments, Comment type 
1 (CMNT1), Comment type 2 (CMNT2) and the 
final comment as Dr Sentiment?s prescription. 
CMNT1 type and CMNT2 type comments are 
associated with question types Q1 and Q2 
respectively. 
2.7 CMNT1 
Comment type 1 has 5 variations as shown in the 
Comment table in Table 3. Comments are 
randomly retrieved from comment type table 
according to their category. 
? Positive word has been tagged as negative (PN) 
? Positive word has been tagged as positive (PP) 
? Negative word has been tagged as positive (NP) 
? Negative word has been tagged as negative (NN) 
? Neutral (NU) 
2.8 CMNT2 
The strategy here is as same as the CMNT 1. 
Comment type 2 has only 2 variations as. 
? Positive word has been tagged as negative. (PN) 
? Negative word has been tagged as positive (NP) 
2.9 Dr Sentiment?s Prescription 
The final prescription depends on various factors 
such as total number of positive, negative or 
neutral comments and the total time taken by any 
player. The final prescription also depends on the 
range of the values of accumulating all the above 
factors.  
This is only the appealing factor to a player. The 
provoking message for players is Dr Sentiment can 
reveal their sentimental status: whether they are 
extreme negative or positive or very much neutral 
or diplomatic etc. A word previously tagged by a 
player is avoided by the tracking system for the 
next time playing as our intension is to tag more 
and more words involving Internet population. We 
observe that the strategy helps to keep the game 
interesting as a large number of players return to 
play the game after this strategy was implemented. 
We are not demanding that the revealed status of 
a player by Dr Sentiment is exact or ideal. It is 
only to make fun but the outcomes of the game 
effectively help to store human sentimental 
psychology in terms of computational lexicon. 
3 Senti-Mentality  
PsychoSentiWordNet gives a good sketch to 
understand the psycho-sentimental behavior of 
society depending upon proposed psychological 
dimensions. The PsychoSentiWordNet is basically 
the log records of every player?s tagged words. 
3.1 Concept-Culture-Wise Analysis 
 
Figure 3: Geospatial Senti-Mentality 
 
The word ?blue? get tagged by different players 
around the world. But surprisingly it has been 
tagged as positive from one part of the world and 
negative from another part of the world. The 
graphical illustration in Figure 3 explains the 
situation. The observation is that most of the 
negative tags are coming from the middle-east and 
especially from the Islamic countries. We found a 
line in Wiki5 (see in Religion Section) that may 
give a good explanation: ?Blue in Islam: In verse 
20:102 of the Qur?an, the word ??? zurq (plural of 
azraq 'blue') is used metaphorically for evil doers 
whose eyes are glazed with fear?. But other 
explanations may be there for this. This is an 
interesting observation that supports the 
effectiveness of PsychoSentiWordNet. This 
information could be further retrieved from the 
developed source by giving information like (blue, 
Italy), (blue, Iraq) or (blue, USA) etc. 
3.2 Age-Wise Analysis 
Another interesting observation is that 
sentimentality may vary age-wise. For better 
understanding we look at the total statistics and the 
                                                           
5
 http://en.wikipedia.org/wiki/Blue 
55
age wise distribution of all the players. Total 533 
players have taken part till date. The total number 
of players for each range of age is shown at top of 
every bar. In the Figure 4 the horizontal bars are 
divided into two colors (Green depicts the 
Positivity and Red depicts the negativity) 
according to the total positivity and negativity 
scores, gathered during playing. This sociological 
study gives an idea that variation of sentimentality 
with age.  This information could be further 
retrieved from the developed source by giving 
information like (X, 36-39) or (X, 45-49) etc.  
 
Figure 4: Age-Wise Senti-Mentality 
3.3 Gender Specific 
It is observed from the statistics collected that 
women are more positive than a man. The 
variations in sentimentality among men and 
women are shown in the following Figure 5.  
 
Figure 5: Gender Specific Senti-Mentality 
3.4 Other-Wise 
We have described several important observations 
in the previous sections and there are other 
important observations as well. Studies on the 
combinations of the proposed psychological 
dimensions, such as, location-age, location-
profession and gender-location may reveal some 
interesting results. 
4 Expected Impact of the Resource 
Undoubtedly the generated PsychoSentiWordNet 
are important resource for sentiment/opinion or 
emotion analysis task. Moreover the other non 
linguistic psychological dimensions are very much 
important for further analysis and in several newly 
discovered sub-disciplines such as: Geospatial 
Information retrieval (Egenhofer, 2002), 
Personalized search (Gaucha et al, 2003) and 
Recommender System (Adomavicius and Tuzhilin, 
2005) etc. 
5 The Data Structure and Organization 
Deciding about the data structure of this kind of 
special requirement was not trivial. Presently 
RDBMS (Relational Database Management 
System) has been used. Several tables are being 
used to keep user?s clicking log and their personal 
information.  
As one of the research motivations was to 
generate up-to-date prior polarity scores thus we 
decided to generate web service API by that people 
could access latest prior polarity scores. We do 
believe this method will over perform than a static 
sentiment lexicon set. 
6 Conclusion & Future Direction 
In the present paper the development of the 
PsychoSentiWordNet has been described. No 
evaluation has been done yet as there is no data 
available for this kind of experimentation and to 
the best of our knowledge this is the first endeavor 
where sentiment meets psychology.  
Our present goal is to collect such corpus and 
experiment to check whether variable prior polarity 
score of PsychoSentiWordNet excel over the fixed 
point prior polarity score of SentiWordNet. 
Acknowledgments 
The work reported in this paper was supported by a 
grant from the India-Japan Cooperative Program 
(DST-JST) 2009 Research project entitled 
?Sentiment Analysis where AI meets Psychology? 
funded by Department of Science and Technology 
(DST), Government of India. 
56
References  
Andreevskaia Alina and Bergler Sabine. CLaC and 
CLaC-NB: Knowledge-based and corpus-based ap-
proaches to sentiment tagging. In the Proc. of the 4th 
SemEval-2007, Pages 117?120, Prague, June 2007. 
Ahn Luis von and Laura Dabbish. Labeling Images with 
a Computer Game. In the Proc. of ACM-CHI 2004. 
Aue A. and Gamon M., Customizing sentiment classifi-
ers to new domains: A case study. In the Proc. of 
RANLP, 2005. 
Baccianella Stefano, Andrea Esuli, and Fabrizio Sebas-
tiani. SENTIWORDNET 3.0: An Enhanced Lexical 
Resource for Sentiment Analysis and Opinion Min-
ing. In the Proc. of LREC-10. 
Bing Liu. Sentiment Analysis: A Multi-Faceted Prob-
lem.In the IEEE Intelligent Systems, 2010. 
Mohammad Saif, Dorr Bonnie and Hirst Graeme. Com-
puting Word-Pair Antonymy. In the Proc. of 
EMNLP-2008. 
Pang Bo, Lee Lillian, and Vaithyanathan Shivakumar. 
Thumbs up? Sentiment classification using machine 
learning techniques. In the Proc. of EMNLP, Pages 
79?86, 2002. 
Read Jonathon. Using emoticons to reduce dependency 
in machine learning techniques for sentiment classi-
fication. In the Proc. of the ACL Student Research 
Workshop, 2005. 
Strapparava, C. and Valitutti, A. WordNet-Affect: an 
affective extension of WordNet. In Proc. of LREC 
2004, Pages 1083 ? 1086 
Wiebe Janyce and Mihalcea Rada. Word sense and 
subjectivity. In the Proc. of COLING/ACL-06. Pages 
1065-1072. 
Wiebe Janyce and Riloff Ellen. Creating Subjective and 
Objective Sentence Classifiers from Unannotated 
Texts. In the Proc. CICLING, Pages 475?486, 2006. 
Richard M. Tong. An operational system for detecting 
and tracking opinions in online discussion. In the 
Proc. of the Workshop on Operational Text Classifi-
cation (OTC), 2001. 
57
Proceedings of the ACL-HLT 2011 System Demonstrations, pages 50?55,
Portland, Oregon, USA, 21 June 2011. c?2011 Association for Computational Linguistics
Dr Sentiment Knows Everything! 
 
Amitava Das and Sivaji Bandyopadhyay 
Department of Computer Science and Engineering 
Jadavpur University 
India 
amitava.santu@gmail.com sivaji_cse_ju@yahoo.com 
 
 
Abstract 
Sentiment analysis is one of the hot de-
manding research areas since last few dec-
ades. Although a formidable amount of 
research have been done, the existing re-
ported solutions or available systems are 
still far from perfect or do not meet the sa-
tisfaction level of end users?. The main is-
sue is the various conceptual rules that 
govern sentiment and there are even more 
clues (possibly unlimited) that can convey 
these concepts from realization to verbali-
zation of a human being. Human psycholo-
gy directly relates to the unrevealed clues 
and governs the sentiment realization of us. 
Human psychology relates many things 
like social psychology, culture, pragmatics 
and many more endless intelligent aspects 
of civilization. Proper incorporation of hu-
man psychology into computational senti-
ment knowledge representation may solve 
the problem. In the present paper we pro-
pose a template based online interactive 
gaming technology, called Dr Sentiment to 
automatically create the PsychoSenti-
WordNet involving internet population. 
The PsychoSentiWordNet is an extension 
of SentiWordNet that presently holds hu-
man psychological knowledge on a few as-
pects along with sentiment knowledge. 
1 Introduction 
In order to identify sentiment from a text, lexical 
analysis plays a crucial role. For example, words 
like love, hate, good and favorite directly indicate 
sentiment or opinion. Previous works (Pang et al, 
2002; Wiebe and Mihalcea, 2006; Baccianella et. 
al., 2010) have already proposed various tech-
niques for making dictionaries for those sentiment 
words. But polarity assignment of such sentiment 
lexicons is a hard semantic disambiguation prob-
lem. The regulating aspects which govern the lexi-
cal level semantic orientation are natural language 
context (Pang et al, 2002), language properties 
(Wiebe and Mihalcea, 2006), domain pragmatic 
knowledge (Aue and Gamon, 2005), time dimen-
sion (Read, 2005), colors and culture (Strapparava 
and Ozbal, 2010) and many more unrevealed hid-
den aspects. Therefore it is a challenging and 
enigmatic research problem. 
The current trend is to attach prior polarity to 
each entry at the sentiment lexicon level. Prior po-
larity is an approximation value based on heuristics 
based statistics collected from corpus and not ex-
act. The probabilistic fixed point prior polarity 
scores do not solve the problem completely rather 
it places the problem into next level, called contex-
tual polarity classification.  
We start with the hypothesis that the summation 
of all the regulating aspects of sentiment orienta-
tion is human psychology and thus it is a multi-
faceted problem (Liu, 2010). More precisely what 
we mean by human psychology is the union of all 
known and unknown aspects that directly or indi-
rectly govern the sentiment orientation knowledge 
of us. The regulating aspects wrapped in the 
present PsychoSentiWordNet are Gender, Age, 
City, Country, Language and Profession.  
The PsychoSentiWordNet is an extension of the 
existing SentiWordNet 3.0 (Baccianella et. al., 
2010) to hold the possible psychological ingre-
dients and govern the sentiment understandability 
of us. The PsychoSentiWordNet holds variable 
prior polarity scores that could be fetched depend-
ing upon those psychological regulating aspects. 
50
An example with the input word ?High? may illu-
strate the definition better:  
 
Aspects (Profession)   Polarity 
Null     Positive 
Businessman    Negative 
Share Broker   Positive 
 
In this paper, we propose an interactive gaming 
(Dr Sentiment) technology to collect psycho-
sentimental polarity for lexicons. This technology 
has proven itself as an excellent technique to col-
lect psychological sentiment of human society 
even at multilingual level. Dr Sentiment presently 
supports 56 languages and therefore we may call it 
Global PsychoSentiWordNet. The supported lan-
guages by Dr Sentiment are reported in Table 1. 
In this section we have philosophically argued 
about the necessity of developing PsychoSenti-
WordNet. In the next section 2 we will describe the 
technical details of the proposed architecture for 
building the lexical resource. Section 3 explains 
about some exciting outcomes of PsychoSenti-
WordNet. The developed PsychoSentiWordNet(s) 
are expected to help automatic sentiment analysis 
research in many aspects and other disciplines as 
well and have been described in section 4.The data 
structure and the organization are described in sec-
tion 5. The conclusion is drawn in section 6. 
2 Dr Sentiment 
Dr Sentiment1 is a template based interactive on-
line game, which collects player?s sentiment by 
asking a set of simple template based questions and 
finally reveals a player?s sentimental status. Dr 
Sentiment fetches random words from Senti-
WordNet synsets and asks every player to tell 
about his/her sentiment polarity understanding re-
garding the concept behind the word fetched by it.  
There are several motivations behind developing 
the intuitive game to automatically collect human 
psycho-sentimental orientation information.  
In the history of Information Retrieval research 
there is a milestone when ESP game2 (Ahn et al, 
2004) innovated the concept of a game to automat-
ically label images available in the World Wide 
Web. It has been identified as the most reliable 
strategy to automatically annotate the online im-
                                                          
1
 http://www.amitavadas.com/Sentiment%20Game/index.php 
2
 http://www.espgame.org/ 
ages. We are highly motivated by the success of 
the Image Labeler game.  
A number of research endeavors could be found 
in the literature for creation of Sentiment Lexicon 
in several languages and domains. These tech-
niques can be broadly categorized into two classes, 
one follows classical manual annotation techniques  
(Andreevskaia and Bergler, 2006);(Wiebe and Ri-
loff, 2006) while the other follows various auto-
matic techniques (Mohammad et al, 2008). Both 
types of techniques have few limitations. Manual 
annotation techniques are undoubtedly trustable 
but it generally takes time. Automatic techniques 
demand manual validations and are dependent on 
the corpus availability in the respective domain. 
Manual annotation techniques require a large num-
ber of annotators to balance one?s sentimentality in 
order to reach agreement. But human annotators 
are quite unavailable and costly. 
Sentiment is a property of human intelligence 
and is not entirely based on the features of a lan-
guage. Thus people?s involvement is required to 
capture the sentiment of the human society. We 
have developed an online game to attract internet 
population for the creation of PsychoSentiWord-
Net automatically. Involvement of Internet popula-
tion is an effective approach as the population is 
very high in number and ever growing (approx. 
360,985,492) 3 . Internet population consists of 
people with various languages, cultures, age etc 
and thus not biased towards any domain, language 
or particular society. A detailed statistics on the 
Internet usage and population has been reported in 
the Table 2. 
The lexicons tagged by this system are credible 
as it is tagged by human beings. It is not a static 
sentiment lexicon set [polarity changes with time 
(Read, 2005)] as it is updated regularly. Around 
10-20 players each day are playing it throughout 
the world in different languages. The average 
number of tagging per word is about 7.47 till date. 
The Sign Up form of the ?Dr Sentiment? game 
asks the player to provide personal information 
such as Sex, Age, City, Country, Language and 
Profession. These collected personal details of a 
player are kept as a log record in the database. 
The gaming interface has four types of question 
templates. The question templates are named as 
Q1, Q2, Q3 and Q4. 
                                                          
3
 http://www.internetworldstats.com/stats.htm 
51
Languages 
Afrikaans Bulgarian Dutch German Irish Malay Russian Thai 
Albanian Catalan Estonian Greek Italian Maltese Serbian Turkish 
Arabic Chinese Filipino Haitian Japanese Norwegian Slovak Ukrainian 
Armenian Croatian Finnish Hebrew Korean Persian Slovenian Urdu 
Azerbaijani Creole French Hungarian Latvian Polish Spanish Vietnamese 
Basque Czech Galician Icelandic Lithuanian Portuguese Swahili Welsh 
Belarusian Danish Georgian Indonesian Macedonian Romanian Swedish Yiddish 
Table 1: Languages
 
WORLD INTERNET USAGE AND POPULATION STATISTICS 
World Regions Population ( 2010 Est.) 
Internet Users 
Dec. 31, 2000 
Internet Users 
Latest Data 
Penetration 
(Population) 
Growth 
2000-2010 
Users % 
of Table 
Africa 1,013,779,050 4,514,400 110,931,700 10.9 % 2,357.3 % 5.6 % 
Asia 3,834,792,852 114,304,000 825,094,396 21.5 % 621.8 % 42.0 % 
Europe  813,319,511 105,096,093 475,069,448 58.4 % 352.0 % 24.2 % 
Middle East  212,336,924 3,284,800 63,240,946 29.8 % 1,825.3 % 3.2 % 
North America 344,124,450 108,096,800 266,224,500 77.4 % 146.3 % 13.5 % 
Latin America/Caribbean  592,556,972 18,068,919 204,689,836 34.5 % 1,032.8 % 10.4 % 
Oceania / Australia  34,700,201 7,620,480 21,263,990 61.3 % 179.0 % 1.1 % 
WORLD TOTAL 6,845,609,960 360,985,492 1,966,514,816 28.7 % 444.8 % 100.0 % 
Table 2: Internet Usage and Population Statistics 
 
To make the gaming interface more interesting 
images have been added. These images have been 
retrieved by Google image search API 4  and to 
avoid biasness we have randomized among the 
first ten images retrieved by Google. 
2.1 Gaming Strategy 
Dr Sentiment asks 30 questions to each player. 
There are predefined distributions of each question 
type as 11 for Q1, 11 for Q2, 4 for Q3 and 4 for 
Q4. These numbers are arbitrarily chosen and ran-
domly changed for experimentation. The questions 
are randomly asked to keep the game more inter-
esting. For word based translation Google transla-
tion5 service has been used. At each Question (Q) 
level translation service has been used to display 
the sentiment word into player?s own language. 
Google API provides multiple senses for word lev-
el translation and currently only the first sense has 
been picked automatically.  
2.2 Q1 
An English word from the English SentiWordNet 
synset is randomly chosen. The Google image 
search API is fired with the word as a query. An 
image along with the word itself is shown in the 
Q1 page of the game.  
                                                          
4
 http://code.google.com/apis/imagesearch/ 
5
 http://translate.google.com/ 
Players press the different emoticons (Figure 1) 
to express their sentimentality. The interface keeps 
log records of each interaction. 
Extreme 
Positive Positive Neutral Negative 
Extreme 
Negative 
 
 
 
 
 
Figure 1: Emoticons to Express Player?s Senti-
ment 
2.3 Q2 
This question type is specially designed for relative 
scoring technique. For example: good and better 
both are positive but we need to know which one is 
more positive than other. Table 3 shows how in 
SentiWordNet relative scoring has been made. 
With the present gaming technology relative polar-
ity scoring has been assigned to each n-n word pair 
combination. 
Randomly n (presently 2-4) words have been 
chosen from the source SentiWordNet synsets 
along with their images as retrieved by Google 
API. Each player is then asked to select one of 
them that he/she likes most. The relative score is 
calculated and stored in the corresponding log ta-
ble. 
Word Positivity Negativity 
Good 0.625 0.0 
Better 0.875 0.0 
Best 0.980 0.0 
Table 3: Relative Sentiment Scores in Senti-
WordNet 
52
2.4 Q3 
The player is asked for any positive word in his/her 
mind. This technique helps to increase the cover-
age of existing SentiWordNet. The word is then 
added to the existing PsychoSentiWordNet and 
further used in Q1 to other users to note their sen-
timentality about the particular word. 
2.5 Q4 
A player is asked by Dr Sentiment about any nega-
tive word. The word is then added to the existing 
PsychoSentiWordNet and further used in Q1 to 
other users to note their sentimentality about the 
particular word. 
2.6 Comment Architecture 
There are three types of Comments, Comment type 
1 (CMNT1), Comment type 2 (CMNT2) and the 
final comment as Dr Sentiment?s prescription. 
CMNT1 type and CMNT2 type comments are as-
sociated with question types Q1 and Q2 respective-
ly. 
2.6.1 CMNT1 
Comment type 1 has 5 variations as shown in the 
Comment table in Table 4. Comments are random-
ly retrieved from comment type table according to 
their category: 
 
? Positive word has been tagged as negative (PN) 
? Positive word has been tagged as positive (PP) 
? Negative word has been tagged as positive (NP) 
? Negative word has been tagged as negative (NN) 
? Neutral. (NU) 
2.6.2 CMNT2 
The strategy here is as same as the CMNT 1. 
Comment type 2 has only two variations as. 
? Positive word has been tagged as negative (PN) 
? Negative word has been tagged as positive (NP) 
2.7 Dr Sentiment?s Prescription 
The final prescription depends on various factors 
such as total number of positive, negative or neu-
tral comments and the total time taken by any 
player. The final prescription also depends on the 
range of the accumulated values of all the above 
factors.  
This is the most important appealing factor to a 
player. The motivating message for players is that 
Dr Sentiment can reveal their sentimental status: 
whether they are extreme negative or positive or 
very much neutral or diplomatic etc. It is not 
claimed that the revealed status of a player by Dr 
Sentiment is exact or ideal. It is only to make the 
players motivated but the outcomes of the game 
effectively helps to store human sentimental psy-
chology in terms of computational lexicon. 
A word previously tagged by a player is avoided 
by the tracking system during subsequent turns by 
the same player. The intension is to tag more and 
more words involving Internet population. We ob-
serve that the strategy helps to keep the game in-
teresting as a large number of players return to 
play the game after this strategy was implemented. 
3 Senti-Mentality 
PsychoSentiWordNet gives a good sketch to un-
derstand the psycho-sentimental behavior of the 
human society depending upon proposed psycho-
logical dimensions. The PsychoSentiWordNet is 
basically the log records of every player?s tagged 
words.  
3.1 Concept-Culture-Wise Analysis 
The word ?blue? gets tagged by different players 
around the world. But surprisingly it has been 
tagged as positive from one part of the world and 
negative from another part of the world. The 
graphical illustration in Figure 2 may explain the 
situation better. The observation is that most of the 
negative tags are coming from the middle-east and 
especially from the Islamic countries. 
PN PP NP NN NU 
You don?t like 
<word>! 
Good you have a good 
choice! Is <word> good! 
Yes <word> is too 
bad! 
You should speak out 
frankly! 
You should like 
<word>! I love <word> too! 
I hope it is a bad 
choice! You are quite right! 
You are too diplomat-
ic! 
But <word> is a good 
itself! I support your view! 
I don?t agree with 
you! 
I also don?t like 
<word>! 
Why you hiding from 
me? I am Dr Senti-
ment. 
Table 4: Comments 
53
We found a line in Wiki6 (see in Religion Section) 
that may provide a good explanation: ?Blue in Is-
lam: In verse 20:102 of the Qur?an, the word ??? 
zurq (plural of azraq 'blue') is used metaphorically 
for evil doers whose eyes are glazed with fear?. 
But other explanations may be there for this situa-
tion. This is an interesting observation that sup-
ports the effectiveness of the developed 
PsychoSentiWordNet. This information could be 
further retrieved from the developed source by giv-
ing information like (blue, Italy), (blue, Iraq) or 
(blue, USA) etc. 
 
Figure 2: Geospatial Senti-Mentality 
3.2 Age-Wise Analysis 
Another interesting observation is that sentimental-
ity may vary age-wise. For better understanding we 
look at the total statistics and the age wise distribu-
tion of all the players. Total 533 players have taken 
part till date. The total number of players for each 
range of age is shown at the top of every bar.  
 
Figure 3: Age-Wise Senti-Mentality 
In Figure 3 the horizontal bars are divided into two 
colors (Green depicts the Positivity and Red de-
picts the negativity) according to the total positivi-
ty and negativity scores, gathered during playing. 
                                                          
6
 http://en.wikipedia.org/wiki/Blue 
This sociological study gives an idea on the varia-
tion of sentimentality with age.  This information 
may be retrieved from the developed source by 
giving information like (X, 36-39) or (X, 45-49) 
etc where X denotes any arbitrary lexicon synset. 
3.3 Gender-Wise Analysis 
It is observed from the collected statistics that 
women are more positive than men! The variations 
in sentimentality among men and women are 
shown in the following Figure 4.  
 
Figure 4: Gender Specific Senti-Mentality 
3.4 Other-Wise 
We have described several important observations 
in the previous sections and there are other impor-
tant observations as well. Studies on the combina-
tions of the proposed psychological dimensions, 
such as, location-age, location-profession and 
gender-location may reveal some interesting re-
sults.  
4 Expected Impact of the Resource 
Undoubtedly the generated PsychoSentiWord-
Net(s) are important resources for senti-
ment/opinion or emotion analysis task. Moreover 
the other non linguistic psychological dimensions 
are very much important for further analysis as 
well as for several newly discovered sub-
disciplines such as: Geospatial Information retriev-
al (Egenhofer, 2002), Personalized search (Gaucha 
et al, 2003), Recommender System (Adomavicius 
and Tuzhilin, 2005), Sentiment Tracking (Tong, 
2001) etc. 
5 The Data Structure and Organization 
Deciding on the data structure for the PsychoSen-
tiWordNet was not trivial. Presently RDBMS (Re-
lational Database Management System) has been 
54
used. Several tables are being used to keep user?s 
clicking log and their personal information.  
As one of the research motivations was to gen-
erate up-to-date prior polarity scores across various 
dimensions, we decided to generate web service 
API through which the people can access latest 
prior polarity scores. The developed PsychoSenti-
WordNet is expected to perform better than a static 
sentiment lexicon. 
6 Conclusion and Future Directions 
In the present paper the development of the Psy-
choSentiWordNet for 56 languages has been de-
scribed. No evaluation has been done yet as there 
is no data available for this kind of experimenta-
tion and to the best of our knowledge this is the 
first endeavor where sentiment analysis meets AI 
and psychology.  
Our present goal is to collect such corpus and 
carry out experiments to check whether variable 
prior polarity scores of PsychoSentiWordNet excel 
over the fixed point prior polarity score of Senti-
WordNet. 
Automatically picked first sense from Google 
translation API may cause difficulties for cross 
lingual projection of sentiment synsets. Erroneous 
outputs from API may also cause some problems. 
But these problems lead to another research issue 
that may be termed as cross lingual sentiment syn-
set linking. Presently we are giving a closer look to 
the qualitative analysis of developed multilingual 
psycho-sentiment lexicons. 
Acknowledgment 
The work reported in this paper was supported by a 
grant from the India-Japan Cooperative Program 
(DST-JST) Research project entitled ?Sentiment 
Analysis where AI meets Psychology? funded by 
Department of Science and Technology (DST), 
Government of India. 
References  
Adomavicius Gediminas and Alexander Tuzhilin. To-
ward the Next Generation of Recommender Systems: 
A Survey of the State-of-the-Art and Possible Exten-
sions. In the Proc. of IEEE Transactions on Know-
ledge and Data Engineering, VOL. 17, NO. 6, June 
2005. ISSN 1041-4347/05. Pages 734-749. 
Ahn Luis von and Laura Dabbish. Labeling Images with 
a Computer Game.In the Proc. of ACM CHI 2004. 
Andreevskaia Alina and Bergler Sabine. CLaC and 
CLaC-NB: Knowledge-based and corpus-based ap-
proaches to sentiment tagging. In the Proc. of the 4th 
SemEval-2007, Pages 117?120, Prague, June 2007. 
Aue A. and Gamon M., Customizing sentiment classifi-
ers to new domains: A case study. In the Proc. Of 
RANLP, 2005. 
Baccianella Stefano, Andrea Esuli, and Fabrizio Sebas-
tiani. SENTIWORDNET 3.0: An Enhanced Lexical 
Resource for Sentiment Analysis and Opinion Min-
ing. In the Proc. of LREC-10. 
Bo Pang, Lee Lillian, and Vaithyanathan Shivakumar. 
Thumbs up? Sentiment classification using machine 
learning techniques. In the Proc. of EMNLP, Pages 
79?86, 2002. 
Egenhofer M.. Toward the Semantic Geospatial Web. 
ACM-GIS 2002, McLean, VI A. Voisard and S.-C. 
Chen (eds.), Pages. 1-4, November 2002. 
Gaucha Susan, Jason Chaffeeb and Alexander Pret-
schnerc. Ontology-based personalized search and 
browsing. In Proc. of Web Intelligence and Agent 
Systems: An international journal. 2003. Pages 219?
234. ISSN 1570-1263/03. 
Liu Bing . Sentiment Analysis: A Multi-Faceted Prob-
lem.In the IEEE Intelligent Systems, 2010. 
Read Jonathon. Using emoticons to reduce dependency 
in machine learning techniques for sentiment classi-
fication. In the Proc. of the ACL Student Research 
Workshop, 2005. 
Richard M. Tong. An operational system for detecting 
and tracking opinions in online discussion. In the 
Proc. of the Workshop on Operational Text Classifi-
cation (OTC), 2001. 
Saif Mohammad, Dorr Bonnie and Hirst Graeme. Com-
puting Word-Pair Antonymy. In the Proc. of 
EMNLP-2008. 
Strapparava, C. and Valitutti, A. WordNet-Affect: an 
affective extension of WordNet. In Proc. of LREC 
2004, Pages 1083 ? 1086 
Wiebe Janyce and Mihalcea Rada. Word sense and sub-
jectivity. In the Proc. of COLING/ACL-06. Pages 
1065-1072. 
55
Proceedings of the 2010 Named Entities Workshop, ACL 2010, pages 71?75,
Uppsala, Sweden, 16 July 2010. c?2010 Association for Computational Linguistics
English to Indian Languages Machine Transliteration System at 
NEWS 2010 
Amitava Das1, Tanik Saikh2, Tapabrata Mondal3, Asif Ekbal4, Sivaji Bandyopadhyay5 
Department of Computer Science and Engineering1,2,3,5 
Jadavpur University,  
Kolkata-700032, India  
amitava.santu@gmail.com1, tanik4u@gmail.com2, tapabratamon-
dal@gmail.com3, sivaji_cse_ju@yahoo.com5  
Department of Computational Linguistics4 
University of Heidelberg 
Im Neuenheimer Feld 325 
69120 Heidelberg, Germany 
ekbal@cl.uni-heidelberg.de 
 
Abstract 
 
This paper reports about our work in the 
NEWS 2010 Shared Task on Transliteration 
Generation held as part of ACL 2010. One 
standard run and two non-standard runs were 
submitted for English to Hindi and Bengali 
transliteration while one standard and one non-
standard run were submitted for Kannada and 
Tamil. The transliteration systems are based 
on Orthographic rules and Phoneme based 
technology. The system has been trained on 
the NEWS 2010 Shared Task on Translitera-
tion Generation datasets. For the standard run, 
the system demonstrated mean F-Score values 
of 0.818 for Bengali, 0.714 for Hindi, 0.663 
for Kannada and 0.563 for Tamil. The reported 
mean F-Score values of non-standard runs are 
0.845 and 0.875 for Bengali non-standard run-
1 and 2, 0.752 and 0.739 for Hindi non-
standard run-1 and 2, 0.662 for Kannada non-
standard run-1 and 0.760 for Tamil non-
standard run-1. Non-Standard Run-2 for Ben-
gali has achieved the highest score among all 
the submitted runs. Hindi Non-Standard Run-1 
and Run-2 runs are ranked as the 5th and 6th 
among all submitted Runs. 
1 Introduction 
Transliteration is the method of translating one 
source language word into another target lan-
guage by expressing and preserving the original 
pronunciation in their source language. Thus, the 
central problem in transliteration is predicting the 
pronunciation of the original word. Translitera-
tion between two languages that use the same set 
of alphabets is trivial: the word is left as it is. 
However, for languages those use different al-
phabet sets the names must be transliterated or 
rendered in the target language alphabets. Trans-
literation of words is necessary in many applica-
tions, such as machine translation, corpus align-
ment, cross-language Information Retrieval, in-
formation extraction and automatic lexicon ac-
quisition. In the literature, a number of translite-
ration algorithms are available involving English 
(Li et al, 2004; Vigra and Khudanpur, 2003; Go-
to et al, 2003), European languages (Marino et 
al., 2005) and some of the Asian languages, 
namely Chinese (Li et al, 2004; Vigra and Khu-
danpur, 2003), Japanese (Goto et al, 2003; 
Knight and Graehl, 1998), Korean (Jung et al, 
2000) and Arabic (Al-Onaizan and Knight, 
2002a; Al-Onaizan and Knight, 2002c). Recent-
ly, some works have been initiated involving 
Indian languages (Ekbal et al, 2006; Ekbal et al, 
2007; Surana and Singh, 2008). The detailed re-
port of our participation in NEWS 2009 could be 
found in (Das et al, 2009).  
One standard run for Bengali (Bengali 
Standard Run: BSR), Hindi (Hindi Standard 
Run: HSR), Kannada (Kannada Standard Run: 
KSR) and Tamil (Tamil Standard Run: TSR) 
were submitted. Two non-standard runs for Eng-
lish to Hindi (Hindi Non-Standard Run 1 & 2: 
HNSR1 & HNSR2) and Bengali (Bengali Non-
Standard Run 1 & 2: BNSR1 & BNSR1) transli-
teration were submitted. Only one non-standard 
run were submitted for Kannada (Kannada Non-
Standard Run-1: KNSR1) and Tamil (Tamil 
Non-Standard Run-1: TNSR1). 
71
2 Machine Transliteration Systems  
Five different transliteration models have been 
proposed in the present report that can generate 
the transliteration in Indian language from an 
English word. The transliteration models are 
named as Trigram Model (Tri), Joint Source-
Channel Model (JSC), Modified Joint Source-
Channel Model (MJSC), Improved Modified 
Joint Source-Channel Model (IMJSC) and Inter-
national Phonetic Alphabet Based Model (IPA). 
Among all the models the first four are catego-
rized as orthographic model and the last one i.e. 
IPA based model is categorized as phoneme 
based model. 
An English word is divided into Translitera-
tion Units (TUs) with patterns C*V*, where C 
represents a consonant and V represents a vowel. 
The targeted words in Indian languages are di-
vided into TUs with patterns C+M?, where C 
represents a consonant or a vowel or a conjunct 
and M represents the vowel modifier or matra. 
The TUs are the basic lexical units for machine 
transliteration. The system considers the English 
and Indian languages contextual information in 
the form of collocated TUs simultaneously to 
calculate the plausibility of transliteration from 
each English TU to various Indian languages 
candidate TUs and chooses the one with maxi-
mum probability. The system learns the map-
pings automatically from the bilingual NEWS 
2010 training set being guided by linguistic fea-
tures/knowledge. The output of the mapping 
process is a decision-list classifier with collo-
cated TUs in the source language and their 
equivalent TUs in collocation in the target lan-
guage along with the probability of each decision 
obtained from the training set. A Direct example 
base has been maintained that contains the bilin-
gual training examples that do not result in the 
equal number of TUs in both the source and tar-
get sides during alignment. The Direct example 
base is checked first during machine translitera-
tion of the input English word. If no match is 
obtained, the system uses direct orthographic 
mapping by identifying the equivalent TU in In-
dian languages for each English TU in the input 
and then placing the target language TUs in or-
der. The IPA based model has been used for 
English dictionary words. Words which are not 
present in the dictionary are handled by other 
orthographic models as Trigram, JSC, MJSC and 
IMJSC. 
The transliteration models are described below 
in which S and T denotes the source and the tar-
get words respectively: 
3 Orthographic Transliteration models 
The orthographic models work on the idea of 
TUs from both source and target languages. The 
orthographic models used in the present system 
are described below. For transliteration, P(T), 
i.e., the probability of transliteration in the target 
language, is calculated from a English-Indian 
languages bilingual database If, T is not found in 
the dictionary, then a very small value is 
assigned to P(T). These models have been 
desribed in details in Ekbal et al (2007). 
3.1 Trigram 
This is basically the Trigram model where the 
previous and the next source TUs are considered 
as the context.  
( | ) ( , | )1, 11
K
P S T P s t s sk k kk
= < >?
? +
=
 
( ) arg max { ( ) ( | )}S T S P T P S T
T
? = ?  
3.2  Joint Source-Channel Model (JSC) 
This is essentially the Joint Source-Channel 
model (Hazhiou et al, 2004) where the 
previous TUs with reference to the current TUs 
in both the source (s) and the target sides (t) are 
considered as the context.  
( | ) ( , | , )11
K
P S T P s t s tk kk
= < > < >?
?
=
 
( ) arg max { ( ) ( | )}S T S P T P S T
T
? = ?  
3.3 Modified Joint Source-Channel Model 
(MJSC) 
In this model, the previous and the next TUs in 
the source and the previous target TU are 
considered as the context. This is the Modified 
Joint Source-Channel model. 
( | ) ( , | , )1, 11
K
P S T P s t s t sk k kk
= < > < >?
? +
=
 
( ) arg max { ( ) ( | )}S T S P T P S T
T
? = ?  
3.4 Improved Modified Joint Source-
Channel Model (IMJSC) 
In this model, the previous two and the next TUs 
in the source and the previous target TU are 
considered as the context. This is the  Improved 
Modified Joint Source-Channel model. 
72
( | ) ( , | , )1 1, 11
K
P S T P s t s s t sk k k kk
= < > < >? + ? +
=
 
( ) arg max { ( ) ( | )}S T S P T P S T
T
? = ?  
4 International Phonetic Alphabet 
(IPA) Model 
The NEWS 2010 Shared Task on Transliteration 
Generation challenge addresses general domain 
transliteration problem rather than named entity 
transliteration. Due to large number of dictionary 
words as reported in Table 1 in NEWS 2010 data 
set a phoneme based transliteration algorithm  
has been devised.  
 Train Dev Test 
Bengali 7.77% 5.14% 6.46% 
Hindi 27.82% 15.80% 3.7% 
Kannada 27.60% 14.63% 4.4% 
Tamil 27.87% 17.31% 3.0% 
Table 1: Statistics of Dictionary Words 
The International Phonetic Alphabet (IPA) is a 
system of representing phonetic notations based 
primarily on the Latin alphabet and devised by 
the International Phonetic Association as a 
standardized representation of the sounds of 
spoken language. The machine-readable 
Carnegie Mellon Pronouncing Dictionary 1  has 
been used as an external resource to capture 
source language IPA structure. The dictionary 
contains over 125,000 words and their 
transcriptions with mappings from words to their 
pronunciations in the given phoneme set. The 
current phoneme set contains 39 distinct 
phonemes. As there is no such parallel IPA 
dictionary available for Indian languages, 
English IPA structures have been mapped to TUs 
in Indian languages during training. An example 
of such mapping between phonemes and TUs are 
shown in Table 3, for which the vowels may 
carry lexical stress as reported in Table 2. This 
phone set is based on the ARPAbet2 symbol set 
developed for speech recognition uses.  
Representation Stress level 
0 No 
1 Primary 
2 Secondary 
Table 2: Stress Level on Vowel 
A pre-processing module checks whether a 
targeted source English word is a valid 
dictionary word or not. The dictionary words are 
then handled by phoneme based transliteration 
module. 
                                                 
1
 www.speech.cs.cmu.edu/cgi-bin/cmudict 
2
 http://en.wikipedia.org/wiki/Arpabet 
Phoneme Example Translation TUs 
AA odd AA0-D - 
AH hut HH0-AH-T - 
D dee D-IY1 -?	 
Table 3: Phoneme Map Patterns of English 
Words and TUs 
In the target side we use our TU segregation 
logic to get phoneme wise transliteration pattern. 
We present this problem as a sequence labelling 
problem, because transliteration pattern changes 
depending upon the contextual phonemes in 
source side and TUs in the target side. We use a 
standard machine learning based sequence 
labeller Conditional Random Field (CRF)3 here. 
IPA based model increased the performance 
for Bengali, Hindi and Tamil languages as 
reported in Section 6. The performance has 
decreased for Kannada. 
5 Ranking 
The ranking among the transliterated outputs 
follow the order reported in Table 4: The ranking 
decision is based on the experiments as described 
in (Ekbal et al, 2006) and additionally based on 
the experiments on NEWS 2010 development 
dataset. 
Word Type  Ranking Order 1 2 3 4 5 
Dictionary IPA IMJSC MJSC JSC Tri 
Non-
Dictionary IMJSC MJSC JSC Tri - 
Table 4: Phoneme Patterns of English Words 
In BSR, HSR, KSR and TSR the orthographic 
TU based models such as: IMJSC, MJSC, JSC 
and Tri have been used only trained by NEWS 
2010 dataset. In BNSR1 and HNSR1 all the or-
thographic models have been trained with addi-
tional census dataset as described in Section 6. In 
case of BNSR2, HNSR2, KNSR1 and TNSR1 
the output of the IPA based model has been add-
ed with highest priority. As no census data is 
available for Kannada and Tamil therefore there 
is only one Non-Standard Run was submitted for 
these two languages only with the output of IPA 
based model along with the output of Standard 
Run. 
6 Experimental Results  
We have trained our transliteration models using 
the NEWS 2010 datasets obtained from the 
NEWS 2010 Machine Transliteration Shared 
Task (Li et al, 2010). A brief statistics of the 
                                                 
3
 http://crfpp.sourceforge.net 
73
datasets are presented in Table 5. During train-
ing, we have split multi-words into collections of 
single word transliterations. It was observed that 
the number of tokens in the source and target 
sides mismatched in various multi-words and 
these cases were not considered further. Follow-
ing are some examples:  
Paris Charles de Gaulle  ???? 
???? ??	?
 ? ?????  
Suven Life Scie  ??? ??
	??
 
Delta Air Lines  ???? 
???	
 
In the training set, some multi-words were 
partly translated and not transliterated. Such ex-
amples were dropped from the training set. In the 
following example the English word ?National? 
is being translated in the target as ??????. 
Australian National Univer-
sity  ????? ???? 
?Proceedings of the 8th Workshop on Asian Language Resources, pages 56?63,
Beijing, China, 21-22 August 2010. c?2010 Asian Federation for Natural Language Processing
SentiWordNet for Indian Languages 
Amitava Das1 and Sivaji Bandyopadhyay2 
Department of Computer Science and Engineering  
Jadavpur University 
amitava.santu@gmail.com1 sivaji_cse_ju@yahoo.com2 
 
Abstract 
The discipline where sentiment/ opi-
nion/ emotion has been identified and 
classified in human written text is well 
known as sentiment analysis. A typical 
computational approach to sentiment 
analysis starts with prior polarity lex-
icons where entries are tagged with 
their prior out of context polarity as 
human beings perceive using their 
cognitive knowledge. Till date, all re-
search efforts found in sentiment lex-
icon literature deal mostly with English 
texts. In this article, we propose mul-
tiple computational techniques like, 
WordNet based, dictionary based, cor-
pus based or generative approaches for 
generating SentiWordNet(s) for Indian 
languages. Currently, SentiWordNet(s) 
are being developed for three Indian 
languages: Bengali, Hindi and Telugu. 
An online intuitive game has been de-
veloped to create and validate the de-
veloped SentiWordNet(s) by involving 
Internet population. A number of au-
tomatic, semi-automatic and manual 
validations and evaluation methodolo-
gies have been adopted to measure the 
coverage and credibility of the devel-
oped SentiWordNet(s). 
1 Introduction 
Sentiment analysis and classification from 
electronic text is a hard semantic disambigua-
tion problem. The regulating aspects of seman-
tic orientation of a text are natural language 
context information (Pang et al, 2002) lan-
guage properties (Wiebe and Mihalcea, 2006), 
domain pragmatic knowledge (Aue and Ga-
mon, 2005) and lastly most challenging is the 
time dimension (Read, 2005). 
The following example shows that the polar-
ity tag associated with a sentiment word de-
pends on the time dimension. During 90?s mo-
bile phone users generally reported in various 
online reviews about their color phones but in 
recent times color phone is not just enough. 
People are fascinated and influenced by touch 
screen and various software(s) installation fa-
cilities on these new generation gadgets. 
In typical computational approaches (Higa-
shinaka et al, 2007; Hatzivassiloglou et al, 
2000) to sentiment analysis researchers con-
sider the problem of learning a dictionary that 
maps semantic representations to verbaliza-
tions, where the data comes from opinionated 
electronic text. Although lexicons in these dic-
tionaries are not explicitly marked up with re-
spect to their contextual semantics, they con-
tain only explicit polarity rating and aspect 
indicators. Lexicon-based approaches can be 
broadly classified into two categories firstly 
where the discriminative polarity tag of lex-
icons is determined on labeled training data 
and secondly where the lexicons are manually 
compiled, the later constitutes the main effec-
tive approach.  
It is undoubted that the manual compilation 
is always the best way to create monolingual 
semantic lexicons, but manual methods are 
expensive in terms of human resources, it in-
volves a substantial number of human annota-
tors and it takes lot of time as well. In this pa-
per we propose several computational tech-
niques to generate sentiment lexicons in Indian 
languages automatically and semi-
automatically. In the present task, SentiWord-
56
Net(s) are being developed for the Bengali, 
Hindi and Telugu languages.  
Several prior polarity sentiment lexicons are 
available for English such as SentiWordNet 
(Esuli et. al., 2006), Subjectivity Word List 
(Wilson et. al., 2005), WordNet Affect list 
(Strapparava et al, 2004), Taboada?s adjective 
list (Taboada et al, 2006).  
Among these publicly available sentiment 
lexicon resources we find that SentiWordNet is 
most widely used (number of citation is higher 
than other resources1) in several applications 
such as sentiment analysis, opinion mining and 
emotion analysis. Subjectivity Word List is 
most trustable as the opinion mining system 
OpinionFinder2 that uses the subjectivity word 
list has reported highest score for opi-
nion/sentiment subjectivity (Wiebe and Riloff, 
2006). SentiWordNet is an automatically con-
structed lexical resource for English that as-
signs a positivity score and a negativity score 
to each WordNet synset.  
The subjectivity word list is compiled from 
manually developed resources augmented with 
entries learned from corpora. The entries in the 
subjectivity word list have been labeled with 
part of speech (POS) tags as well as either 
strong or weak subjective tag depending on the 
reliability of the subjective nature of the entry.  
These two resources have been merged au-
tomatically and the merged resource is used for 
SentiWordNet(s) generation in the present 
task.  
The generated sentiment lexicons or Senti-
WordNet(s) for several Indian languages most-
ly contain synsets (approximately 60%) of re-
spective languages. Synset based method is 
robust for any kind of monolingual lexicon 
creation and useful to avoid further word sense 
disambiguation problem in application domain.  
Additionally we have developed an online 
intuitive game to create and validate the devel-
oped SentiWordNet(s) by involving Internet 
population.  
The proposed approaches in this paper are 
easy to adopt for any new language. To meas-
ure the coverage and credibility of generated 
SentiWordNet(s) in Indian languages we have 
                                                 
1
 http://citeseerx.ist.psu.edu/ 
2
 http://www.cs.pitt.edu/mpqa/ 
developed several automatic and semi-
automatic evaluation methods. 
2 Related Works 
Various methods have been used in the litera-
ture such as WordNet based, dictionary based, 
corpus based or generative approaches for sen-
timent lexicon generation in a new target lan-
guage.  
Andreevskaia and Bergler, (2006) present a 
method for extracting sentiment-bearing adjec-
tives from WordNet using the Sentiment Tag 
Extraction Program (STEP). They did 58 
STEP runs on unique non-intersecting seed 
lists drawn from manually annotated list of 
positive and negative adjectives and evaluated 
the results against other manually annotated 
lists.  
The proposed methods in (Wiebe and Riloff, 
2006) automatically generate resources for 
subjectivity analysis for a new target language 
from the available resources for English. Two 
techniques have been proposed for the genera-
tion of target language lexicon from English 
subjectivity lexicon. The first technique uses a 
bilingual dictionary while the second method 
is a parallel corpus based approach using exist-
ing subjectivity analysis tools for English. 
Automatically or manually created lexicons 
may have limited coverage and do not include 
most semantically contrasting word pairs like 
antonyms. Antonyms are broadly categorized 
(Saif Mohammed, 2008) as gradable adjec-
tives (hot?cold, good?bad, friend?enemy) and 
productive adjectives (normal?abnormal, for-
tune?misfortune, implicit?explicit). The first 
type contains the semantically contrasting 
word pairs but the second type includes ortho-
graphic suffix/affix as a clue. The second type 
is highly productive using very less number of 
affixation rules.  
Degree of antonymy (Mohammad et al, 
2008) is defined to encompass the complete 
semantic range as a combined measure of the 
contrast in meaning conveyed by two antony-
my words and is identified by distributional 
hypothesis. It helps to measure relative senti-
ment score of a word and its antonym.   
Kumaran et al, (2008) introduced a beauti-
ful method for automatic data creation by on-
line intuitive games. A methodology has been 
57
proposed for community creation of linguistic 
data by community collaborative framework 
known as wikiBABEL3. It may be described as 
a revolutionary approach to automatically 
create large credible linguistic data by involv-
ing Internet population for content creation.  
For the present task we prefer to involve all 
the available methodologies to automatically 
and semi-automatically create and validate 
SentiWordNet(s) for three Indian languages. 
Automatic methods involve only computation-
al methods. Semi-automatic methods involve 
human interruption to validate system?s output. 
3 Source Lexicon Acquisition  
SentiWordNet and Subjectivity Word List 
have been identified as the most reliable source 
lexicons. The first one is widely used and the 
second one is robust in terms of performance. 
A merged sentiment lexicon has been devel-
oped from both the resources by removing the 
duplicates. It has been observed that 64% of 
the single word entries are common in the Sub-
jectivity Word List and SentiWordNet. The 
new merged sentiment lexicon consists of 
14,135 numbers of tokens. Several filtering 
techniques have been applied to generate the 
new list. 
A subset of 8,427 sentiment words has been 
extracted from the English SentiWordNet, by 
selecting those whose orientation strength is 
above the heuristically identified threshold 
value of 0.4. The words whose orientation 
strength is below 0.4 are ambiguous and may 
lose their subjectivity in the target language 
after translation. A total of weakly subjective 
2652 words are discarded from the 
Subjectivity word list as proposed in (Wiebe 
and Riloff, 2006). 
In the next stage the words whose POS 
category in the Subjectivity word list is 
undefined and tagged as ?anypos? are 
considered. These words may generate sense 
ambiguity issues in the next stages of 
subjectivity detection. The words are checked 
in the SentiWordNet list for validation. If a 
match is found with certain POS category, the 
word is added to the new merged sentiment 
                                                 
3
 http://research.microsoft.com/en-
us/projects/wikibabel/ 
lexicon. Otherwise the word is discarded to 
avoid ambiguities later. 
Some words in the Subjectivity word list are 
inflected e.g., memories. These words would 
be stemmed during the translation process, but 
some words present no subjectivity property 
after stemming (memory has no subjectivity 
property). A word may occur in the 
subjectivity list in many inflected forms. 
Individual clusters for the words sharing the 
same root form are created and then checked in 
the SentiWordNet for validation. If the root 
word exists in the SentiWordNet then it is 
assumed that the word remains subjective after 
stemming and hence is added to the new list. 
Otherwise the cluster is completely discarded 
to avoid any further ambiguities. 
Various statistics of the English Senti-
WordNet and Subjectivity Word List are re-
ported in Table 1.  
 SentiWordNet Subjectivity Word List 
En
tr
ie
s Single Multi Single Multi 
115424 79091 5866 990 
U
am
bi
-
gu
o
u
s 
W
o
rd
s 
20789 30000 4745 963 
D
isc
ar
de
d 
A
m
-
bi
gu
o
u
s 
W
o
rd
s Threshold Orientation  Strength 
Subjectivity 
Strength POS 
86944 30000 2652 928 
Table 1: English SentiWordNet and Subjec-
tivity Word List Statistics 
4 Target Lexicon Generation 
4.1 Bilingual Dictionary Based Approach 
A word-level translation process followed by 
error reduction technique has been adopted for 
generating the Indian languages 
SentiWordNet(s) from the English sentiment 
lexicon merged from the English 
SentiWordNet and the Subjectivity Word List.  
English to Indian languages synsets are be-
ing developed under Project English to Indian 
Languages Machine Translation Systems 
58
(EILMT)4, a consortia project funded by De-
partment of Information Technology (DIT), 
Government of India. These synsets are robust 
and reliable as these are created by native 
speakers as well as linguistics experts of the 
specific languages. For each language we have 
approximately 9966 synsets along with the 
English WordNet offset. These bilingual syn-
set dictionaries have been used along with lan-
guage specific dictionaries. 
A word level synset/lexical transfer tech-
nique is applied to each English synset/word in 
the merged sentiment lexicon. Each dictionary 
search produces a set of Indian languages syn-
sets/words for a particular English synset/word.  
4.1.1 Hindi 
Two available manually compiled English-
Hindi electronic dictionaries have been identi-
fied for the present task. First is the SHABD-
KOSH5  and the second one is Shabdanjali6 .  
These two dictionaries have been merged au-
tomatically by replacing the duplicates. The 
merged English-Hindi dictionary contains ap-
proximately 90,872 unique entries. The posi-
tive and negative sentiment scores for the Hin-
di words are copied from their English Senti-
WordNet.  
The bilingual dictionary based translation 
process has resulted 22,708 Hindi entries.  
4.1.2 Bengali 
An English-Bengali dictionary (approx-
imately 102119 entries) has been developed 
using the Samsad Bengali-English dictionary7. 
The positive and negative sentiment scores for 
the Bengali words are copied from their Eng-
lish SentiWordNet equivalents.  
The bilingual dictionary based translation 
process has resulted in 35,805 Bengali entries. 
A manual checking is done to identify the re-
liability of the words generated from automatic 
process. After manual checking only 1688 
                                                 
4
 http://www.cdacmumbai.in/e-ilmt 
5
 http://www.shabdkosh.com/ 
6
 
http://www.shabdkosh.com/content/category/downl
oads/ 
7
 
http://dsal.uchicago.edu/dictionaries/biswas_bengal
i/ 
words are discarded i.e., the final list consists 
of 34,117 words.  
4.2 Telugu 
Charles Philip Brown English-Telugu Dictio-
nary 8 , Aksharamala 9  English-Telugu Dictio-
nary and English-Telugu Dictionary 10  devel-
oped by Language Technology Research Cen-
ter (LTRC), International Institute of Hydera-
bad (IITH) have been chosen for the present 
task. There is no WordNet publicly available 
for Telugu and the corpus (Section 4.5) we 
used is small in size. Dictionary based ap-
proach is the main process for Telugu Senti-
WordNet generation.  
These three dictionaries have been merged 
automatically by replacing the duplicates. The 
merged English-Telugu dictionary contains 
approximately 112310 unique entries. The pos-
itive and negative sentiment scores for the Te-
lugu words are copied from their English Sen-
tiWordNet equivalents. 
The dictionary based translation process has 
resulted in 30,889 Telugu entries, about 88% 
of final Telugu SentiWordNet synsets. An on-
line intuitive game has been proposed in Sec-
tion 4.6 to automatically validate the devel-
oped Telugu SentiWordNet by involving In-
ternet population.  
4.3 WordNet Based Approach 
WordNet(s) are available for Hindi11  (Jha et 
al., 2001) and Bengali12 (Robkop et al, 2010) 
but publicly unavailable for Telugu. 
A WordNet based lexicon expansion strate-
gy has been adopted to increase the coverage 
of the generated SentiWordNet(s) through the 
dictionary based approach. The present algo-
rithm starts with English SentiWordNet syn-
sets that is expanded using synonymy and an-
tonymy relations in the WordNet. For match-
ing synsets we keep the exact score as in the 
source synset in the English SentiWordNet. 
The calculated positivity and negativity score 
                                                 
8
 http://dsal.uchicago.edu/dictionaries/brown/ 
9
 https://groups.google.com/group/aksharamala 
10
 
http://ltrc.iiit.ac.in/onlineServices/Dictionaries/Dict
_Frame.html 
11
 http://www.cfilt.iitb.ac.in/wordnet/webhwn/ 
12
 http://bn.asianwordnet.org/ 
59
for any target language antonym synset is cal-
culated as: 
1
1
p p
n n
T S
T S
= ?
= ?
 
where pS , nS  are the positivity and negativ-
ity score for the source language (i.e, English) 
and pT , nT  are the positivity and negativity 
score for target languages (i.e., Hindi and Ben-
gali) respectively. 
4.3.1 Hindi 
Hindi WordNet is a well structured and ma-
nually compiled resource and is being updated 
since last nine years. There is an available 
API13  for accessing the Hindi WordNet. Al-
most 60% of final SentiWordNet synsets in 
Hindi are generated by this method. 
4.3.2 Bengali 
The Bengali WordNet is being developed by 
the Asian WordNet (AWN) community. It on-
ly contains 1775 noun synsets as reported in 
(Robkop et al, 2010). A Web Service14  has 
been provided for accessing the Bengali 
WordNet. There are only a few number of 
noun synsets in the Bengali WordNet and other 
important POS category words for sentiment 
lexicon such as adjective, adverb and verb are 
absent. Only 5% new lexicon entries have been 
generated in this process.  
4.4 Antonym Generation 
Automatically or manually created lexicons 
have limited coverage and do not include most 
semantically contrasting word pairs. To over-
come the limitation and increase the coverage 
of the SentiWordNet(s) we present automatic 
antonymy generation technique followed by 
corpus validation to check orthographically 
generated antonym does really exist. Only 16 
hand crafted rules have been used as reported 
in Table 2. About 8% of Bengali, 7% of Hindi 
and 11% of Telugu SentiWordNet entries are 
generated in this process. 
 
                                                 
13
 
http://www.cfilt.iitb.ac.in/wordnet/webhwn/API_do
wnloaderInfo.php 
14
 http://bn.asianwordnet.org/services 
Affix/Suffix Word Antonym 
abX  Normal  Ab-normal 
misX Fortune Mis-fortune 
imX-exX  Im-plicit Ex-plicit 
antiX Clockwise Anti-clockwise 
nonX  Aligned Non-aligned 
inX-exX  In-trovert Ex-trovert 
disX Interest Dis-interest 
unX  Biased Un-biased 
upX-downX  Up-hill Down-hill 
imX  Possible Im-possible 
illX  Legal Il-legal 
overX-underX  Overdone Under-done 
inX  Consistent In-consistent 
rX-irX  Regular Ir-regular 
Xless-Xful  Harm-less Harm-ful 
malX  Function Mal-function 
Table 2: Rules for Generating Productive An-
tonyms 
4.5 Corpus Based Approach 
Language/culture specific words such as those 
listed below are to be captured in the devel-
oped SentiWordNet(s). But sentiment lexicon 
generation techniques via cross-lingual projec-
tion are unable to capture these words. As ex-
ample:  
????? (Sahera: A marriage-
wear) 
 (Durgapujo: A festiv-
al of Bengal) 
To increase the coverage of the developed 
SentiWordNet(s) and to capture the lan-
guage/culture specific words an automatic cor-
pus based approach has been proposed. At this 
stage the developed SentiWordNet(s) for the 
three Indian languages have been used as a 
seed list. Language specific corpus is automat-
ically tagged with these seed words and we 
have a simple tagset as SWP (Sentiment Word 
Positive) and SWN (Sentiment Word Nega-
tive). Although we have both positivity and 
negativity scores for the words in the seed list 
but we prefer a word level tag as either posi-
tive or negative following the highest senti-
ment score. 
A Conditional Random Field (CRF15) based 
Machine Learning model is then trained with 
the seed list corpus along with multiple lin-
guistics features such as morpheme, parts-of-
                                                 
15
 http://crfpp.sourceforge.net 
60
speech, and chunk label. These linguistics fea-
tures have been extracted by the shallow pars-
ers16  for Indian languages. An n-gram (n=4) 
sequence labeling model has been used for the 
present task.  
The monolingual corpuses used have been 
developed under Project English to Indian 
Languages Machine Translation Systems 
(EILMT). Each corpus has approximately 10K 
of sentences. 
4.6 Gaming Methodology 
There are several motivations behind develop-
ing an intuitive game to automatically create 
multilingual SentiWordNet(s). The assigned 
polarity scores to each synset may vary in time 
dimension. Language specific polarity scores 
may vary and it should be authenticated by 
numbers of language specific annotators. 
In the history of Information Retrieval re-
search there is a milestone when ESP17 game 
(Ahn et al, 2004) innovate the concept of a 
game to automatically label images available 
in World Wide Web. Highly motivated by the 
historical research we proposed a intuitive 
game to create and validate SentiWordNet(s) 
for Indian languages by involving internet 
population. 
 
Figure 1: Intuitive Game for SentiWordNet(s) 
Creation 
In the gaming interface a simple picture (re-
trieved by Google Image API18) along with a 
sentiment bearing word (retrieved randomly 
                                                 
16
 
http://ltrc.iiit.ac.in/showfile.php?filename=downloa
ds/shallow_parser.php 
17
 http://www.espgame.org/ 
18
 
http://code.google.com/apis/ajaxsearch/multimedia.
html 
from SentiWordNet) is displayed to a player 
and he/she is then been asked to capture his 
immediate sentiment as extreme positive, posi-
tive, extreme negative, negative or neutral by 
pressing appropriate emoticon buttons. A snap 
of the game is shown in the Figure 1. The sen-
timent score is calculated by the different emo-
ticons based on the inputs from the different 
players and then is assigned the scale as fol-
lows: extreme positive (pos: 0.5, neg: 0.0), 
positive (pos: 0.25, neg: 0.0), neutral (pos: 0.0, 
neg: 0.0), negative (pos: 0.0, 0.25), extreme 
negative (pos: 0.0, neg: 0.5). 
The score of a particular player is calculated 
on the basis of pre-stored sentiment lexicon 
scores in the generated SentiWordNet(s).  
5 Evaluation 
Andera Esuli and Fabrizio Sebastiani (2006) 
have calculated the reliability of the sentiment 
scores attached to every synsets in the English 
SentiWordNet. They have tagged sentiment 
words in the English WordNet with positive 
and negative sentiment scores. In the present 
task, these sentiment scores from English 
WordNet have been directly copied to the In-
dian language SentiWordNet(s).  
Two extrinsic evaluation strategies have 
been adopted for the developed Bengali Sen-
tiWordNet based on the two main usages of 
the sentiment lexicon as subjectivity classifier 
and polarity identifier. The Hindi and Telugu 
SentiWordNet(s) have not completely been 
evaluated.  
5.1 Coverage 
 NEWS BLOG 
Total number of  documents 100 - 
Total number of sentences 2234 300 
Avgerage number of sentences in 
a document 22 - 
Total number of wordforms 28807 4675 
Avgerage number of wordforms 
in a document 288 - 
Total number of distinct 
wordforms 17176 1235 
Table 3: Bengali Corpus Statistics 
We experimented with NEWS and BLOG 
corpora for subjectivity detection. Sentiment 
lexicons are generally domain independent but 
it provides a good baseline while working with 
sentiment analysis systems. The coverage of 
61
the developed Bengali SentiWordNet is eva-
luated by using it in a subjectivity classifier 
(Das and Bandyopadhyay, 2009). The statistics 
of the NEWS and BLOG corpora is reported in 
Table 3. 
For comparison with the coverage of Eng-
lish SentiWordNet the same subjectivity clas-
sifier (Das and Bandyopadhyay, 2009) has 
been applied on Multi Perspective Question 
Answering (MPQA) (NEWS) and IMDB Mov-
ie review corpus along with English Senti-
WordNet. The result of the subjectivity clas-
sifier on both the corpus proves that the cover-
age of the Bengali SentiWordNet is reasonably 
good. The subjectivity word list used in the 
subjectivity classifier is developed from the 
IMDB corpus and hence the experiments on 
the IMDB corpus have yielded high precision 
and recall scores. The developed Bengali Sen-
tiWordNet is domain independent and still its 
coverage is very good as shown in Table 4. 
 
Languages Domain Precision Recall 
English MPQA 76.08% 83.33% 
IMDB 79.90% 86.55% 
Bengali NEWS 72.16% 76.00% BLOG 74.6% 80.4% 
Table 4: Subjectivity Classifier using Senti-
WordNet 
5.2 Polarity Scores 
This evaluation metric measures the reliability 
of the associated polarity scores in the senti-
ment lexicons. To measure the reliability of 
polarity scores in the developed Bengali Sen-
tiWordNet, a polarity classifier (Das and Ban-
dyopadhyay, 2010) has been developed using 
the Bengali SentiWordNet alng with some 
other linguistic features. 
 
Features Overall Performance Incremented By 
SentiWordNet 47.60% 
Table 5: Polarity Performance Using Bengali 
SentiWordNet 
 
Feature ablation method proves that the as-
sociated polarity scores in the developed Ben-
gali SentiWordNet are reliable. Table 5 shows 
the performance of a polarity classifier using 
the Bengali SentiWordNet. The polarity wise 
overall performance of the polarity classifier is 
reported in Table 6. 
 
Polarity Precision Recall 
Positive 56.59% 52.89% 
Negative 75.57% 65.87% 
Table 6: Polarity-wise Performance Using 
Bengali SentiWordNet 
 
Comparative study with a polarity classifier 
that works with only prior polarity lexicon is 
necessary but no such works have been identi-
fied in literature.  
An arbitrary 100 words have been chosen 
from the Hindi SentiWordNet for human eval-
uation. Two persons are asked to manually 
check it and the result is reported in Table 7. 
The coverage of the Hindi SentiWordNet has 
not been evaluated, as no manually annotated 
sentiment corpus is available. 
 
Polarity Positive Negative 
Percentage 88.0% 91.0% 
Table 7: Evaluation of Polarity Score of De-
veloped Hindi SentiWordNet 
 
For Telugu we created a version of the game 
with Telugu words on screen. Only 3 users 
have played the Telugu language specific 
game till date. Total 92 arbitrary words have 
been tagged and the accuracy of the polarity 
scores is reported in Table 8. The coverage of 
Telugu SentiWordNet has not been evaluated, 
as no manually annotated sentiment corpus is 
available. 
Polarity Positive Negative 
Percentage 82.0% 78.0% 
Table 8: Evaluation of Polarity Score of De-
veloped Telugu SentiWordNet 
6 Conclusion 
SentiWordNet(s) for Indian languages are be-
ing developed using various approaches. The 
game based technique may be directed towards 
a new way for the creation of linguistic data 
not just only for SentiWordNet(s) but in either 
areas of NLP too.  
Presently only the Bengali SentiWordNet19 
is downloadable from the author?s web page. 
                                                 
19
 http://www.amitavadas.com/sentiwordnet.php 
62
References 
Andreevskaia Alina and Bergler Sabine. CLaC and 
CLaC-NB: Knowledge-based and corpus-based 
approaches to sentiment tagging. In Proceedings 
of the 4th International Workshop on Semantic 
Evaluations (SemEval-2007), pages 117?120, 
Prague, June 2007. 
Aue A. and Gamon M., Customizing sentiment 
classifiers to new domains: A case study. In Pro-
ceedings of Recent Advances in Natural Lan-
guage Processing (RANLP), 2005. 
Das A. and Bandyopadhyay S. (2010). Phrase-level 
Polarity Identification for Bengali, In Interna-
tional Journal of Computational Linguistics and 
Applications (IJCLA), Vol. 1, No. 1-2, Jan-Dec 
2010, ISSN 0976-0962, Pages 169-182. 
Das A. and Bandyopadhyay S. Subjectivity Detec-
tion in English and Bengali: A CRF-based Ap-
proach. In the Proceeding of ICON 2009. 
Esuli Andrea and Sebastiani Fabrizio. SentiWord-
Net: A publicly available lexical resource for 
opinion mining. In Proceedings of Language Re-
sources and Evaluation (LREC), 2006. 
Hatzivassiloglou, Vasileios and Wiebe Janyce. Ef-
fects of adjective orientation and gradability on 
sentence subjectivity. In Proceedings of COL-
ING-00, 18th International Conference on Com-
putational Linguistics. Saarbru?cken, GE. Pages 
299-305. 2000. 
Higashinaka Ryuichiro, Walker Marilyn, and Pra-
sad Rashmi. Learning to generate naturalistic ut-
terances using reviews in spoken dialogue sys-
tems. ACM Transactions on Speech and Lan-
guage Processing (TSLP), 2007. 
Jha S., Narayan D., Pande P. and Bhattacharyya P. 
A WordNet for Hindi, International Workshop 
on Lexical Resources in Natural Language 
Processing, Hyderabad, India, January 2001. 
Kumaran A., Saravanan K. and Maurice Sandor. 
WikiBABEL: Community Creation of Multilin-
gual Data, in the WikiSYM 2008 Conference, 
Porto, Portugal, Association for Computing Ma-
chinery, Inc., September 2008. 
Mihalcea Rada, Banea Carmen and Wiebe Janyce. 
Learning multilingual subjective language via-
cross-lingual projections. In Proceedings of the 
Association for Computational Linguistics 
(ACL), pages 976?983, Prague, Czech Republic, 
June 2007. 
Mohammad Saif, Dorr Bonnie, and Hirst Graeme. 
Computing Word-Pair Antonymy. In Proceed-
ings of the Conference on Empirical Methods in 
Natural Language Processing and Computational 
Natural Language Learning (EMNLP-2008), Oc-
tober 2008, Waikiki, Hawaii. 
Pang Bo, Lee Lillian, and Vaithyanathan Shivaku-
mar. Thumbs up? Sentiment classification using 
machine learning techniques. In Proceedings of 
the Conference on Empirical Methods in Natural 
Language Processing (EMNLP), pages 79?86, 
2002. 
Read Jonathon. Using emoticons to reduce depen-
dency in machine learning techniques for senti-
ment classification. In Proceedings of the ACL 
Student Research Workshop, 2005. 
Robkop Kergrit, Thoongsup Sareewan, Charoen-
porn Thatsanee, Sornlertlamvanich Virach and 
Isahara Hitoshi.WNMS: Connecting the Distri-
buted WordNet in the Case of Asian WordNet. . 
In the Proceeding of 5th International Confe-
rence of the Global WordNet Association 
(GWC-2010), Mumbai, India , 31st Jan. - 4th 
Feb., 2010. 
Wiebe Janyce and Mihalcea Rada. Word sense and 
subjectivity. In Proceedings of COLING/ACL-
06 the 21st Conference on Computational Lin-
guistics/Association for Computational Linguis-
tics. Sydney, Australia. Pages 1065--1072. 
Wiebe Janyce and Riloff Ellen. Creating Subjective 
and Objective Sentence Classifiers from Unan-
notated Texts. In Proceeding of International 
Conference on Intelligent Text Processing and 
Computational Linguistics, Mexico City, Pages 
475?486, 2006. 
Wilson Theresa, Wiebe Janyce and Hoffmann Paul 
(2005). Recognizing Contextual Polarity in 
Phrase-Level Sentiment Analysis. In Proceed-
ings of HLT/EMNLP 2005, Vancouver, Canada. 
63
Proceedings of the 2nd Workshop on Cognitive Aspects of the Lexicon (CogALex 2010), pages 2?11,
Beijing, August 2010
SemanticNet-Perception of Human Pragmatics   
Amitava Das1 and Sivaji Bandyopadhyay2 
Department of Computer Science and Engineering  
Jadavpur University 
amitava.santu@gmail.com1 sivaji_cse_ju@yahoo.com2  
 
Abstract 
SemanticNet is a semantic network of 
lexicons to hold human pragmatic 
knowledge. So far Natural Language 
Processing (NLP) research patronized 
much of manually augmented lexicon 
resources such as WordNet. But the 
small set of semantic relations like 
Hypernym, Holonym, Meronym and 
Synonym etc are very narrow to cap-
ture the wide variations human cogni-
tive knowledge. But no such informa-
tion could be retrieved from available 
lexicon resources. SemanticNet is the 
attempt to capture wide range of con-
text dependent semantic inference 
among various themes which human 
beings perceive in their pragmatic 
knowledge, learned by day to day cog-
nitive interactions with the surrounding 
physical world. SemanticNet holds 
human pragmatics with twenty well es-
tablished semantic relations for every 
pair of lexemes. As every pair of rela-
tions cannot be defined by fixed num-
ber of certain semantic relation labels 
thus additionally contextual semantic 
affinity inference in SemanticNet could 
be calculated by network distance and 
represented as a probabilistic score. 
SemanticNet is being presently devel-
oped for Bengali language. 
1 Historical Motivation 
Semantics (from Greek "??????????" - seman-
tikos) is the study of meaning, usually in lan-
guage. The word "semantics" itself denotes a 
range of ideas, from the popular to the highly 
technical. It is often used in ordinary language 
to denote a problem of understanding that 
comes down to word selection or connotation. 
We studied with various Psycholinguistics ex-
periments to understand how human natural 
intelligence helps to understand general se-
mantic from nature. Our study was to under-
stand the human psychology about semantics 
beyond language. We were haunting for the 
intellectual structure of the psychological and 
neurobiological factors that enable humans to 
acquire, use, comprehend and produce natural 
languages. Let?s come with an example of 
simple conversation about movie between two 
persons. 
Person A: Have you seen the 
movie ?No Man's Land?? How 
is it? 
Person B: Although it is 
good but you should see 
?The Hurt Locker?? 
May be the conversation looks very casual, 
but our intension was to find out the direction 
of the decision logic on the Person B?s brain. 
We start digging to find out the nature of hu-
man intelligent thinking. A prolonged discus-
sion with Person B reveals that the decision 
logic path to recommend a good movie was as 
the Figure 1. The highlighted red paths are the 
shortest semantic affinity distances of the hu-
man brain. 
We call it semantic thinking. Although the 
derivational path of semantic thinking is not 
such easy as we portrait in Figure 1 but we 
keep it easier for understandability. Actually a 
human try to figure out the closest semantic 
affinity node into his pragmatics knowledge by 
natural intelligence. In the previous example 
Person B find out with his intelligence that No 
Man's Land is a war movie and got Oscar 
2
award. Oscar award generally cracked by Hol-
lywood movies and thus Person B start search-
ing his pragmatics network to find out a movie 
fall into war genre, from Hollywood and may 
be got Oscar award. Person B finds out the 
name of a movie The Hurt Locker at nearer 
distance into his pragmatics knowledge net-
work which is an optimized recommendation 
that satisfy all the criteria. Noticeably Person B 
didn?t choice the other paths like Bollywood, 
Foreign movie etc. 
 
Figure 1: Semantic Thinking 
And thus our aim was to develop a computa-
tional lexicon structure for semantics as human 
pragmatics knowledge. We spare long time to 
find out the most robust structure to represent 
pragmatics knowledge properly and it should 
be easy understandable for next level of search 
and usability. 
We look into literature that probably direct 
to the direction of our ideological thinking. We 
found that in the year of 1996 Push Singh and 
Marvin Minsky proposed the field has shat-
tered into subfields populated by researchers 
with different goals and who speak very differ-
ent technical languages. Much has been 
learned, and it is time to start integrating what 
we've learned, but few researchers are widely 
versed enough to do so. They had a proposal 
for how to do so in their ConceptNet work. 
They developed lexicon resources like Con-
ceptNet (Liu and Singh, 2004). ConceptNet- 
ConceptNet is a large-scale semantic network 
(over 1.6 million links) relating a wide variety 
of ordinary objects, events, places, actions, and 
goals by only 20 different link types, mined 
from corpus. 
The present task of developing SemanticNet 
is to capture semantic affinity knowledge of 
human pragmatics as a lexicon database.  We 
extend our vision from the human common 
sense (as in ConceptNet) to human pragmatics 
and have proposed semantic relations for every 
pair of lexemes that cannot be defined by fixed 
number of certain semantic relation labels. 
Contextual semantic affinity inference in Se-
manticNet could be calculated by network dis-
tance and represented as a probabilistic score. 
SemanticNet is being presently developed for 
Bengali language. 
2 Semantic Roles 
The ideological study of semantic roles started 
age old ago since Panini?s karaka theory that 
assigns generic semantic roles to words in a 
natural language sentence. Semantic roles are 
generally domain specific in nature such as 
FROM_DESTINATION,TO_DESTINATION, 
DEPARTURE_TIME etc. Verb-specific se-
mantic roles have also been defined such as 
EATER and EATEN for the verb eat. The 
standard datasets that are used in various Eng-
lish SRL systems are: PropBank (Palmer et al, 
2005), FrameNet (Fillmore et al, 2003) and 
VerbNet (Kipper et al, 2006). These collec-
tions contain manually developed well-trusted 
gold reference annotations of both syntactic 
and predicate-argument structures.  
PropBank defines semantic roles for each 
verb. The various semantic roles identified 
(Dowty, 1991) are Agent, patient or theme etc. 
In addition to verb-specific roles, PropBank 
defines several more general roles that can ap-
ply to any verb (Palmer et al, 2005). 
FrameNet is annotated with verb frame se-
mantics and supported by corpus evidence. 
The frame-to-frame relations defined in Fra-
meNet are Inheritance, Perspective_on, Sub-
frame, Precedes, Inchoative_of, Causative_of 
and Using. Frame development focuses on pa-
raphrasability (or near paraphrasability) of 
words and multi-words.  
VerbNet annotated with thematic roles refer 
to the underlying semantic relationship be-
tween a predicate and its arguments. The se-
mantic tagset of VerbNet consists of tags as 
agent, patient, theme, experiencer, stimulus, 
instrument, location, source, goal, recipient, 
benefactive etc. 
It is evident from the above discussions that 
no adequate semantic role set exists that can be 
defines across various domains. Hence pro-
3
posed SemanticNet does not only rely on fixed 
type of semantics roles as ConceptNet. For 
semantic relations we followed the 20 relations 
defined in ConceptNet. Additionally we pro-
posed semantic relations for every pair of lex-
icons cannot be defined by exact semantic role 
and thus we formulated a probabilistic score 
based technique. Semantic affinity in Seman-
ticNet could be calculated by network distance. 
Details could be found in relevant Section 8. 
3 Corpus 
Present SemanticNet has been developed for 
Bengali language. Resource acquisition is one 
of the most challenging obstacles to work with 
electronically resource constrained languages 
like Bengali. Although Bengali is the sixth1 
popular language in the World, second in India 
and the national language in Bangladesh.  
There was another issue drive us long way 
to find out the proper corpus for the develop-
ment of SemanticNet. As the notion is to cap-
ture and store human pragmatic knowledge so 
the hypothesis was chosen corpus should not 
be biased towards any specific domain know-
ledge as human pragmatic knowledge is not 
constricted to any domain rather it has a wide 
spread range over anything related to universe 
and life on earth. Additionally it must be larger 
in size to cover mostly available general con-
cepts related to any topic. After a detail analy-
sis we decided it is better to choose NEWS 
corpus as various domains knowledge like Pol-
itics, Sports, Entertainment, Social Issues, 
Science, Arts and Culture, Tourism, Adver-
tisement, TV schedule, Tender, Comics and 
Weather etc are could be found only in NEWS 
corpus.  
Statistics NEWS 
Total no. of news documents in the 
corpus 108,305 
Total no. of sentences in the corpus 2,822,737 
Avg no. of sentences in a document 27 
Total no. of wordforms in the corpus 33,836,736 
Avg. no. of wordforms in a document 313 
Total no. of distinct wordforms in the 
corpus 467,858 
Table 1:  Bengali Corpus Statistics 
                                                 
1
 
http://en.wikipedia.org/wiki/List_of_languages_by_
number_of_native_speakers 
Fortunately such corpus development could 
be found in (Ekbal and Bandyopadhyay, 2008) 
for Bengali. We obtained the corpus from the 
authors. The Bengali NEWS corpus consisted 
of consecutive 4 years of NEWS stories with 
various sub domains as reported above. For the 
present task we have used the Bengali NEWS 
corpus, developed from the archive of a lead-
ing Bengali NEWS paper 2  available on the 
Web. The NEWS corpus is quite larger in size 
as reported in Table 1. 
4 Annotation 
From the collected document set 200 docu-
ments have been chosen randomly for the an-
notation task. Three annotators (Mr. X, Mr. Y 
and Mr. Z) participated in the present task.  
Annotators were asked to annotate the theme 
words (topical expressions) which best de-
scribe the topical snapshot of the document. 
The agreement of annotations among three 
annotators has been evaluated. The agreements 
of tag values at theme words level is reported 
in Table 2. 
 
Annotators X vs. Y X Vs. Z Y Vs. Z Avg 
Percentage 82.64% 71.78% 80.47% 78.3% 
All Agree 75.45% 
Table 2: Agreement of annotators at theme 
words level 
5 Theme Identification 
Term Frequency (TF) plays a crucial role to 
identify document relevance in Topic-Based 
Information Retrieval. The motivation behind 
developing Theme detection technique is that 
in many documents relevant words may not 
occur frequently or irrelevant words may occur 
frequently. Moreover for the lexicon affinity 
inference, topic or theme words are the only 
strong clue to start with. The Theme detection 
technique has been proposed to resolve these 
issues to identify discourse level most relevant 
thematic nodes in terms of word or lexicon 
using a standard machine learning technique. 
The machine learning technique used here is 
Conditional Random Field (CRF)3. The theme 
word detection has been defined as a sequence 
                                                 
2
 http://www.anandabazar.com/ 
3
 http://crfpp.sourceforge.net 
4
labeling problem using various useful depend-
ing features. Depending upon the series of in-
put features, each word is tagged as either 
Theme Word (TW) or Other (O). 
5.1 Feature Organization 
The set of features used in the present task 
have been categorized as Lexico-Syntactic, 
Syntactic and Discourse level features. These 
are listed in the Table 3 below and have been 
described in the subsequent subsections. 
 
Types Features 
Lexico-Syntactic 
POS 
Frequency 
Stemming 
Syntactic Chunk Label Dependency Parsing Depth 
Discourse Level 
Title of the Document 
First Paragraph 
Term Distribution 
Collocation 
Table 3: Features 
5.2 Lexico-Syntactic Features 
5.2.1 Part of Speech (POS) 
It has been shown by Das and Bandyopadhyay, 
(2009), that theme bearing words in sentences 
are mainly adjective, adverb, noun and verbs 
as other POS categories like pronoun, preposi-
tion, conjunct, article etc. have no relevance 
towards thematic semantic of any document. 
The detail of the POS tagging system chosen 
for the present task could be found in (Das and 
Bandyopadhyay 2009). 
5.3 Frequency 
Frequency always plays a crucial role in identi-
fying the importance of a word in the docu-
ment or corpus. The system generates four 
separate high frequent word lists after function 
words are removed for four POS categories: 
adjective, adverb, verb and noun. Word fre-
quency values are then effectively used as a 
crucial feature in the Theme Detection tech-
nique. 
5.4 Stemming 
Several words in a sentence that carry thematic 
information may be present in inflected forms. 
Stemming is necessary for such inflected 
words before they can be searched in appropri-
ate lists. Due to non availability of good stem-
mers in Indian languages especially in Bengali, 
a stemmer based on stemming cluster tech-
nique has been used as described in (Das and 
Bandyopadhyay, 2010). This stemmer analyz-
es prefixes and suffixes of all the word forms 
present in a particular document. Words that 
are identified to have the same root form are 
grouped in a finite number of clusters with the 
identified root word as cluster center.  
5.5 Syntactic Features 
5.5.1 Chunk Label 
We found that Chunk level information is very 
much effective to identify lexicon inference 
affinity. As an example: 
 
( 	)/NP (
 
)/NP ()/NP 
()/JJP (?)/SYM 
The movies released by Sa-
tyajit Roy are excellent. 
 
In the above example two lexicons 
?/release? and ?/movie? are collo-
cated in a chunk and they are very much se-
mantically neighboring in human pragmatic 
knowledge. Chunk feature effectively used in 
supervised classifier. Chunk labels are defined 
as B-X (Beginning), I-X (Intermediate) and E-
X (End), where X is the chunk label. In the 
task of identification of Theme expressions, 
chunk label markers play a crucial role. Fur-
ther details of development of chunking sys-
tem could be found in (Das and Bandyopad-
hyay 2009).  
5.5.2 Dependency Parser 
Dependency depth feature is very useful to 
identify Theme expressions. A particular 
Theme word generally occurs within a particu-
lar range of depth in a dependency tree. Theme 
expressions may be a Named Entity (NE: per-
son, organization or location names), a com-
mon noun (Ex: accident, bomb blast, strike etc) 
or words of other POS categories. It has been 
observed that depending upon the nature of 
Theme expressions it can occur within a cer-
tain depth in the dependency tree in the sen-
tences. A statistical dependency parser has 
5
been used for Bengali as described in (Ghosh 
et al, 2009). 
5.6 Discourse Level Features 
5.6.1 Positional Aspect 
Depending upon the position of the thematic 
clue, every document is divided into a number 
of zones. The features considered for each 
document are Title words of the document, the 
first paragraph words and the words from the 
last two sentences. A detailed study was done 
on the Bengali news corpus to identify the 
roles of the positional aspect features of a doc-
ument (first paragraph, last two sentences) in 
the detection of theme words. The importance 
of these positional features has been described 
in the following section.  
5.6.2 Title Words 
It has been observed that the Title words of a 
document always carry some meaningful the-
matic information. The title word feature has 
been used as a binary feature during CRF 
based machine learning. 
5.6.3 First Paragraph Words 
People usually give a brief idea of their beliefs 
and speculations about any related topic or 
theme in the first paragraph of the document 
and subsequently elaborate or support their 
ideas with relevant reasoning or factual infor-
mation. Hence first paragraph words are in-
formative in the detection of Thematic Expres-
sions.  
5.6.4 Words From Last Two Sentences 
It is a general practice of writing style that 
every document concludes with a summary of 
the overall story expressed in the document. 
We found that it is very obvious that every 
document ended with dense theme/topic words 
in the last two sentences. 
5.6.5 Term Distribution Model 
An alternative to the classical TF-IDF weight-
ing mechanism of standard IR has been pro-
posed as a model for the distribution of a word. 
The model characterizes and captures the in-
formativeness of a word by measuring how 
regularly the word is distributed in a document. 
Thus the objective is to estimate  that measures 
the distribution pattern of the k occurrences of 
the word wi in a document d. Zipf's law de-
scribes distribution patterns of words in an en-
tire corpus. In contrast, term distribution mod-
els capture regularities of word occurrence in 
subunits of a corpus (e.g., documents, para-
graphs or chapters of a book). A good under-
standing of the distribution patterns is useful to 
assess the likelihood of occurrences of a theme 
word in some specific positions (e.g., first pa-
ragraph or last two sentences) of a unit of text. 
Most term distribution models try to character-
ize the informativeness of a word identified by 
inverse document frequency (IDF). In the 
present work, the distribution pattern of a word 
within a document formalizes the notion of 
theme inference informativeness. This is based 
on the Poisson distribution. Significant Theme 
words are identified using TF, Positional and 
Distribution factor. The distribution function 
for each theme word in a document is eva-
luated as follows: 
( )1 1
1 1
( ) / ( ) /
n n
d i i i i i
i i
f w S S n TW TW n
? ?
= =
= ? + ?? ?  
where n=number of sentences in a document 
with a particular theme word Si=sentence id of 
the current sentence containing the theme word 
and Si-1=sentence id of the previous sentence 
containing the query term, iTW is the positional 
id of current Theme word and 1iTW ? is the posi-
tional id of the previous Theme word. 
5.6.6 Collocation 
Collocation with other thematic 
words/expressions is undoubtedly an important 
clue for identification of theme sequence pat-
terns in a document. As we used chunk level 
collocation to capture thematic words (as de-
scribed in 5.5.1) and in this section we are in-
troducing collocation feature as inter-chunk 
collocation or discourse level collocation with 
various granularity as sentence level, para-
graph level or discourse level.  
6 Theme Clustering 
Theme clustering algorithms partition a set of 
documents into finite number of topic based 
groups or clusters in terms of theme 
words/expressions. The task of document clus-
tering is to create a reasonable set of clusters 
6
for a given set of documents. A reasonable 
cluster is defined as the one that maximizes the 
within-cluster document similarity and mini-
mizes between-cluster similarities. There are 
two principal motivations for the use of this 
technique in the theme clustering setting: effi-
ciency, and the cluster hypothesis. 
The cluster hypothesis (Jardine and van 
Rijsbergen, 1971) takes this argument a step 
further by asserting that retrieval from a clus-
tered collection will not only be more efficient, 
but will in fact improve retrieval performance 
in terms of recall and precision. The basic no-
tion behind this hypothesis is that by separat-
ing documents according to topic, relevant 
documents will be found together in the same 
cluster, and non-relevant documents will be 
avoided since they will reside in clusters that 
are not used for retrieval. Despite the plausibil-
ity of this hypothesis, there is only mixed ex-
perimental support for it. Results vary consi-
derably based on the clustering algorithm and 
document collection in use (Willett, 1988). We 
employ the clustering hypothesis only to 
measure inter-document level thematic affinity 
inference on semantics. 
Application of the clustering technique to 
the three sample documents results in the fol-
lowing theme-by-document matrix, A, where 
the rows represent various documents and the 
columns represent the themes politics, sport, 
and travel.  
election cricket hotel
A parliament sachin vacation
governor soccer tourist
? ?
? ?
= ? ?
? ?? ?
 
The similarity between vectors is calculated 
by assigning numerical weights to these words 
and then using the cosine similarity measure as 
specified in the following equation.  
, ,
1
, .
N
k j k j i k i j
i
s q d q d w w
? ? ? ?
=
? ?
= = ?? ?? ? ? ---- (1) 
This equation specifies what is known as the 
dot product between vectors.  Now, in general, 
the dot product between two vectors is not par-
ticularly useful as a similarity metric, since it is 
too sensitive to the absolute magnitudes of the 
various dimensions. However, the dot product 
between vectors that have been length norma-
lized has a useful and intuitive interpretation: it 
computes the cosine of the angle between the 
two vectors. When two documents are identic-
al they will receive a cosine of one; when they 
are orthogonal (share no common terms) they 
will receive a cosine of zero. Note that if for 
some reason the vectors are not stored in a 
normalized form, then the normalization can 
be incorporated directly into the similarity 
measure as follows.  
Of course, in situations where the document 
collection is relatively static, it makes sense to 
normalize the document vectors once and store 
them, rather than include the normalization in 
the similarity metric. 
, ,1
2 2
, ,1 1
,
N
i k i ji
k j N N
i k i ki i
w w
s q d
w w
? ?
=
= =
?? ?
=? ?? ? ?
?
? ?  ----(2) 
Calculating the similarity measure and using 
a predefined threshold value, documents are 
classified using standard bottom-up soft clus-
tering k-means technique. The predefined thre-
shold value is experimentally set as 0.5 as 
shown in Table 4. 
 
ID Theme 1 2 3 
1 	
 (administration) 0.63 0.12 0.04 
1  (good-government) 0.58 0.11 0.06 
1  (society) 0.58 0.12 0.03 
1  (law) 0.55 0.14 0.08 
2 Proceedings of the 1st Workshop on South and Southeast Asian Natural Language Processing (WSSANLP), pages 17?25,
the 23rd International Conference on Computational Linguistics (COLING), Beijing, August 2010
Clause Identification and Classification in Bengali  
Aniruddha Ghosh1 Amitava Das2 Sivaji Bandyopadhyay3 
Department of Computer Science and Engineering  
Jadavpur University 
arghyaonline@gmail.com1 amitava.santu@gmail.com2 si-
vaji_cse_ju@yahoo.com3  
 
Abstract 
This paper reports about the develop-
ment of clause identification and classi-
fication techniques for Bengali language. 
A syntactic rule based model has been 
used to identify the clause boundary. For 
clause type identification a Conditional 
random Field (CRF) based statistical 
model has been used. The clause identi-
fication system and clause classification 
system demonstrated 73% and 78% pre-
cision values respectively.  
1 Introduction 
The clause identification is one of the shallow 
semantic parsing tasks, which is important in 
various NLP applications such as Machine 
Translation, parallel corpora alignment, Informa-
tion Extraction and speech applications. Gram-
matically a clause is a group of words having a 
subject and a predicate of its own, but forming 
part of a sentence. Clause boundary identifica-
tion of natural language sentences poses consi-
derable difficulties due to the ambiguous nature 
of natural languages. Clause classification is a 
convoluted task as natural language is generally 
syntactically rich in formation of sentences or 
clauses. 
By the classical theory of Panini (Paul and 
Staal, 1969) a clause is the surface level basic 
syntactic element which holds the basic depen-
dent semantics (i.e. lexical semantic have no 
dependency) to represent the overall meaning of 
any sentence. This syntactic to semantic deriva-
tion proceeds through two intermediate stages: 
the level of karaka relations, which are compa-
rable to the thematic role types and the level of 
inflectional or derivational morphosyntax. 
Fillmore?s Case Grammar (Fillmore et. al, 
2003), and much subsequent work, revived the 
Panini?s proposals in a modern setting. A main 
objective of Case Grammar was to identify syn-
tactic positions of semantic arguments that may 
have different realizations in syntax.  
In the year of 1996 Bharati et al (1996) de-
fines the idea of Chunk or local word group for 
Indian languages. After the successful imple-
mentation of Shakti1 , the first publicly available 
English-Hindi machine translation system the 
idea of chunk became the most acceptable syn-
tactic/semantic representation format for Indian 
languages, known as Shakti Standard Format 
(SSF).   
In 2009 Bali et al (2009) redefines the idea of 
chunk and establishes that the idea of chunking 
varies with prosodic structure of a language. 
Boundary of chunk level is very ambiguous it-
self and can differ by writer or speaker accord-
ing to their thrust on semantic. 
Therefore it is evident that automatic clause 
identification for Indian languages needs more 
research efforts. In the present task, clause 
boundary identification is attempted using the 
classical theory of Panini and the Case Grammar 
approach of Fillmore on the shallow parsed out-
put in SSF structure. It may be worth mentioning 
that several basic linguistic tools in Indian lan-
guages such as part of speech tagger, chunker, 
and shallow parser follow SSF2  as a standard.  
Previous research on clause identification was 
done mostly on the English language (Sang and 
Dejean, 2001). There have been limited efforts 
on clause identification for Indian languages. 
One such effort is proposed in Ram and Devi, 
                                                 
1
 http://shakti.iiit.ac.in/ 
2
 http://ltrc.iiit.ac.in/MachineTrans/research/tb/shakti-
analy-ssf.pdf 
17
(2008) with statistical method. The idea of ge-
nerative grammar based on rule-based descrip-
tions of syntactic structures introduced by 
Chomsky (Chomsky, 1956) points out that every 
language has its own peculiarities that cannot be 
described by standard grammar. Therefore a new 
concept of generative grammar has been pro-
posed by Chomsky. Generative grammar can be 
identified by statistical methods. In the present 
task, conditional random field (CRF) 3  -based 
machine learning method has been used in 
clause type classification. According to the best 
of our knowledge this is the first effort to identi-
fy and classify clauses in Bengali. 
The present system is divided into two parts. 
First, the clause identification task aims to iden-
tify the start and the end boundaries of the claus-
es in a sentence. Second, Clause classification 
system identifies the clause types. 
Analysis of corpus and standard grammar of 
Bengali revealed that clause boundary identifica-
tion depends mostly on syntactic dependency. 
For this reason, the present clause boundary 
identification system is rule based in nature. 
Classification of clause is a semantic task and 
depends on semantic properties of Bengali lan-
guage. Hence we follow the theory of 
Chomsky?s generative grammar to disambiguate 
among possible clause types. The present classi-
fication system of clause is a statistics-based 
approach. A conditional random field (CRF) 
based machine learning method has been used in 
the clause classification task. The output of the 
rule based identification system is forwarded to 
the machine learning model as input. 
The rest of the paper is organized as follows. 
In section 2 we elaborate the rule based clause 
boundary identification. The next section 3 de-
scribes the implementation detail with all identi-
fied features for the clause classification prob-
lem. Result section 4 reports about the accuracy 
of the hybrid system. In error analysis section 
we reported the limitations of the present sys-
tem. The conclusion is drawn in section 5 along 
with the future task direction. 
2 Resource Acquisition 
Bengali belongs to Indo-Aryan language family. 
A characteristic of Bengali is that it is under-
                                                 
3
 http://crf.sourceforge.net/ 
resourced. Language research for Bengali got 
attention recently. Resources like annotated cor-
pus and linguistics tools for Bengali are very 
rarely available in the public domain. 
2.1 Corpus 
We used the NLP TOOLS CONTEST: ICON 
20094 dependency relation marked training data-
set of 980 sentences for training of the present 
system. The data has been further annotated at 
the clause level. According to the standard 
grammar there are two basic clause types such as 
Principal clause and Subordinate clause. Subor-
dinate clauses have three variations as Noun 
clause, Adjective clause and Adverbial clause. 
The tagset defined for the present task consists 
of four tags as Principal clause (PC), Noun 
clause (NC), Adjective clause (AC) and Adver-
bial clause (RC). The annotation tool used for 
the present task is Sanchay5. The detailed statis-
tics of the corpus are reported in Table 1. 
 
 Train Dev Test 
No of Sentences 980 150 100 
Table 1: Statistics of Bengali Corpus 
2.1.1 Annotation Agreement 
Two annotators (Mr. X and Mr. Y) participated 
in the present task. Annotators were asked to 
identify the clause boundaries as well as the type 
of the identified clause. The agreement of anno-
tations among two annotators has been eva-
luated. The agreements of tag values at clause 
boundary level and clause type levels are listed 
in Table 2. 
 
 
Boundary Type 
Percentage 76.54% 89.65% 
Table 2: Agreement of annotators at clause 
boundary and type level 
It is observed from the Table 2 that clause 
boundary identification task has lower agree-
ment value. A further analysis reveals that there 
are almost 9% of cases where clause boundary 
has nested syntactic structure. These types of 
clause boundaries are difficult to identify. One 
of such cases is Inquisitive semantic (Groenen-
dijk, 2009) cases, ambiguous for human annota-
                                                 
4
 http://ltrc.iiit.ac.in/nlptools2009/ 
5
 http://ltrc.iiit.ac.in/nlpai_contest07/Sanchay/ 
18
tors too. It is better to illustrate with some spe-
cific example. 
If John goes to the party, 
will Mary go as well? 
In an inquisitive semantics for a language of 
propositional logic the interpretation of disjunc-
tion is the source of inquisitiveness. Indicative 
conditionals and conditional questions are 
treated both syntactically and semantically. The 
semantics comes with a new logical-
pragmatically notion that judges and compares 
the compliance of responses to an initiative in 
inquisitive dialogue (Groenendijk, 2009). Hence 
it is evident that these types of special cases 
need special research attention. 
2.2 Shallow Parser 
Shallow parser6 for Indian languages, developed 
under a Government of India funded consortium 
project named Indian Language to Indian Lan-
guage Machine Translation System (IL-ILMT), 
are now publicly available. It is a well developed 
linguistic tool and produce good credible analy-
sis. For the present task the linguistic analysis is 
done by the tool and it gives output as pruned 
morphological analysis at each word level, part 
of speech at each word level, chunk boundary 
with type-casted chunk label, vibhakti computa-
tion and chunk head identification. 
2.3 Dependency parser 
A dependency parser for Bengali has been used 
as described in Ghosh et al (2009). The depen-
dency parser follows the tagset7  identified for 
Indian languages as a part of NLP TOOLS 
CONTEST 2009 as a part of ICON 2009. 
3 Rule-based Clause Boundary Identi-
fication 
Analysis of a Bengali corpus and standard 
grammar reveals that clause boundaries are di-
rectly related to syntactic relations at sentence 
level. The present system first identifies the 
number of verbs present in a sentence and sub-
sequently finds out dependant chunks to each 
verb. The set of identified chunks that have rela-
tion with a particular verb is considered as a 
clause. But some clauses have nested syntactic 
                                                 
6
 http://ltrc.iiit.ac.in/analyzer/bengali/ 
7
 http://ltrc.iiit.ac.in/nlptools2009/CR/intro-husain.pdf 
formation, known as inquisitive semantic. These 
clauses are difficult to identify by using only 
syntactic relations. The present system has limi-
tations on those inquisitive types of clauses. 
Bengali is a verb final language. Most of the 
Bengali sentences follow a Subject-Object-Verb 
(SOV) pattern. In Bengali, subject can be miss-
ing in a clause formation. Missing subjects and 
missing keywords lead to ambiguities in clause 
boundary identification. In sentences which do 
not follow the SOV pattern, chunks that appear 
after the finite verb are not considered with that 
clause. For example:  
 
wAra AyZawana o parimANa 
xeKe buJawe asubiXA hayZa ei  
paWa hAwi geCe. 
 
After seeing the size and 
effect, it is hard to under-
stand that an elephant went 
through this way. 
 
In the above example, there is hardly any clue 
to find beginning of subordinate clause. To solve 
this type of problem, capturing only the tree 
structure of a particular sentence has been 
treated as the key factor to the goal of disambig-
uation. One way to capture the regularity of 
chunks over different sentences is to learn a ge-
nerative grammar that explains the structure of 
the chunks one finds. These types of language 
properties make the clause identification prob-
lem difficult.  
3.1 Karaka relation 
Dependency parsing generates the inter chunk 
relation and generates the tree structure. The de-
pendency parser as described in Section 2.3 used 
as a supportive tool for the present problem.   
In the output of the dependency parsing sys-
tems, most of the chunks have a dependency 
relation with the verb chunk. These relations are 
called as karaka relation. Using dependency re-
lations, the chunks having dependency relation 
i.e. karaka relation with same verb chunk are 
grouped. The set of chunks are the members of a 
clause. Using this technique, identification of 
chunk members of a certain clause becomes in-
dependent of SOV patterns of sentences. An ex-
ample is shown in Figure 1. 
19
 Figure 1: Karaka Relations 
3.2 Compound verbs  
In Bengali language a noun chunk with an infi-
nite verb chunk or a finite verb chunk can form a 
compound verb. An example is shown in Figure 
2. 
 
Figure 2: Compound Verb 
In the above example, the noun chunk and the 
VGF chunk form a compound verb. These two 
consecutive noun and verb chunks appearing in 
a sentence are merged to form a compound verb. 
These chunks are connected with a part-of rela-
tion in Dependency Parsing. The set of related 
chunks with these noun and verb chunks are 
merged.  
3.3 Shasthi Relation (r6) 
In dependency parsing the genitive relation are 
marked with shasthi (r6) relation. The chunk 
with shasthi (r6) (see the tagset of NLP Tool 
Contest: ICON 2009) relation always has a rela-
tion with the succeeding chunk. An example is 
shown in Figure 3. 
In the example as mentioned in Figure 3, the 
word ?wadera?(their) has a genitive relation 
with the word in the next chunk ?manera?(of 
mind). These chunks are placed in a set. It forms 
a set of two chunks members. The system gene-
rates two different types of set. In one forms a 
set of members having relation with verb 
chunks. Another set contains two noun chunks 
with genitive relation. Now the sets containing 
only noun chunks with genitive relation does not 
form a clause. Those sets are merged with the set 
containing verb chunk and having dependency 
relation with the noun chunks. An example is 
shown in Figure 3. 
 
Figure 3: Shasthi Relation 
 
Consider ? is set of all sets containing two 
chunk members connected with genitive marker. 
Consider ? is a set of all sets consisting of re-
lated chunks with a verb chunk. ? is a element of 
?. ? is a element of ?. Now, If a set ? which can 
have common chunks from a ? set then ? set is 
associated with the proper ? set. So, ? ? ? ? 
Null then ? = ? ? ?. If a set ? which can have 
common chunks from two ? sets which leads to 
ambiguity of associability of the ? set with the 
proper ? set. If ? ? ? = verb chunk, then ? set 
will be associated with ? set containing the verb 
chunk. From the related set of chunk of verb 
chunks, system has identified the clauses in the 
sentence. Afterwards, the clauses are marked 
with the B-I-E (Beginning-Intermediate-End) 
notation.  
4 Case Grammar-Identification of Ka-
raka relations 
The classical Sanskrit grammar Astadhyayi 8  
(?Eight Books?), written by the Indian gramma-
                                                 
8
 
http://en.wikipedia.org/wiki/P%C4%81%E1%B9%87
ini 
20
rian Panini sometime during 600 or 300 B.C. 
(Robins, 1979), includes a sophisticated theory 
of thematic structure that remains influential till 
today. Panini?s Sanskrit grammar is a system of 
rules for converting semantic representations of 
sentences into phonetic representations (Ki-
parsky, 1969). This derivation proceeds through 
two intermediate stages: the level of karaka rela-
tions, which are comparable to the thematic role 
types described above; and the level of morpho-
syntax. 
Fillmore?s Case Grammar (Fillmore, 1968), 
and much subsequent work, revived the Panini?s 
proposals in a modern setting. A main objective 
of Case Grammar was to identify semantic ar-
gument positions that may have different realiza-
tions in syntax. Fillmore hypothesized ?a set of 
universal, presumably innate, concepts which 
identify certain types of judgments human be-
ings are capable of making about the events that 
are going on around them?. He posited the fol-
lowing preliminary list of cases, noting however 
that ?additional cases will surely be needed?.  
? Agent: The typically animate perceived 
instigator of the action. 
? Instrument: Inanimate force or object 
causally involved in the action or state. 
? Dative: The animate being affected by 
the state or action. 
? Factitive: The object or being resulting 
from the action or state. 
? Locative: The location or time-spatial 
orientation of the state or action. 
? Objective: The semantically most neu-
tral case, the concept should be limited to 
things which are affected by the action or 
state. 
The SSF specification handles this syntactic 
dependency by a coarse-grain tagset of Nomini-
tive, Accusative, Genitive and Locative case 
markers. Bengali shallow parser identifies the 
chunk heads as part of the chunk level analysis. 
Dependency parsing followed by a rule based 
module has been developed to analyze the inter-
chunk relationships depending upon each verb 
present in a sentence. Described theoretical as-
pect can well define the problem definition of 
clause boundary identification but during prac-
tical implementation of the solution we found 
some difficulties. Bengali has explicit case 
markers and thus long distant chunk relations are 
possible as valid grammatical formation. As an 
example: 
bAjAre yAoyZAra samayZa xeKA 
kare gela rAma. 
 
bAjAre yAoyZAra samayZa rAma 
xeKA kare gela. 
 
rAma bAjAre yAoyZAra samayZa 
xeKA kare gela. 
 
Rama came to meet when he 
was going to market. 
 
In the above example rAma could be placed 
anywhere and still all the three syntactic forma-
tion are correct. For these feature of Bengali 
many dependency relation could be missed out 
located at far distance from the verb chunk in a 
sentence. Searching for uncountable numbers of 
chunks have dependency relation with a particu-
lar verb may have good idea theoretically but we 
prefer a checklist strategy to resolve the problem 
in practice. At this level we decided to check all 
semantic probable constituents by the definition 
of universal, presumably innate, concepts list. 
We found this is a nice fall back strategy to iden-
tify the clause boundary. Separately rules are 
written as described below. 
4.1 Agent 
Bengali is a verb final language. Most of the 
Bengali sentences follow a Subject-Object-Verb 
(SOV) pattern. In Bengali, subject can be miss-
ing in a clause formation. Missing subjects and 
missing keywords lead to ambiguities in clause 
boundary identification. 
 
  	? 
Close the door. 
 
In the previous case system marks 
?/door? as an ?Agent? whereas the 
?Agent? is ?you? (2nd person singular number), 
silent here.  
We developed rules using case marker, Gend-
er-Number-Person (GNP), morphological fea-
ture and modality features to disambiguate these 
21
types of phenomena. These rules help to stop 
false hits by identifying no 2nd person phrase 
was there in the example type sentences and em-
power to identify proper phrases by locating 
proper verb modality matching with the right 
chunk.  
4.2 Instrument 
Instrument identification is ambiguous for the 
same type of case marker (nominative) taken by 
agent and instrument. There is no ani-
mate/inanimate information is available at syn-
tactic level. 

	 
  ? 
The music of Shyam?s messme-
rized me. 
 ? 
The umbrella of Sumi. 
 
Bengali sentences follow a Subject-Object-
Verb (SOV) pattern. Positional information is 
helpful to disambiguate between agent and in-
strument roles. 
4.3 Dative 
G
en
er
a
l 
Bengali English Gloss 
/	//	
... 
Morn-
ing/evening/night/da
wn? 
_ 
//JU_CSE_GREC10: Named Entity Generation at GREC 2010 
Amitava Das1, Tanik Saikh2, Tapabrata Mondal3, Sivaji Bandyopadhyay4 
Department of Computer Science and Engineering 
Jadavpur University,  
Kolkata-700032, India  
amitava.santu@gmail.com1, tanik4u@gmail.com2, tapabratamon-
dal@gmail.com3, sivaji_cse_ju@yahoo.com4  
 
Abstract 
 
This paper presents the experiments carried 
out at Jadavpur University as part of the par-
ticipation in the GREC Named Entity Genera-
tion Challenge 2010. The Baseline system is 
based on the SEMCAT, SYNCAT and SYN-
FUNC features of REF and REG08-TYPE and 
CASE features of REFEX elements. The dis-
course level system is based on the additional 
positional features: paragraph number, sen-
tence number, word position in the sentence 
and mention number of a particular named ent-
ity in the document. The inclusion of discourse 
level features has improved the performance 
of the system. 
1 Baseline System 
The baseline system is based on the following 
linguistic features of REF elements: SEMCAT 
(Semantic Category), SYNCAT (Syntactic Cate-
gory) and SYNFUNC (Syntactic Function) (Anja 
Belz, 2010) and the following linguistic features 
of REFEX elements: REG08-TYPE (Entity type) 
and CASE (Case marker). The baseline system 
has been separately trained on the training set 
data for the three domains: chefs, composers and 
inventors. The system has been tested on each 
development set by identifying the most probable 
REFEX element among the possible alternatives 
based on the REF element feature combination. 
The probability assigned to a REFEX element 
corresponding to a certain feature combination of 
REF element is calculated as follows:  
( )
i
i
D
REFEX
v D
REF
Np R
N
=  
where ( )vp R is the probability of the targeted 
REFEX element to be assigned, iDREFN is the total 
number of occurrences of REF element feature 
combinations, iD denotes the domain i.e., Chefs, 
Composers and Inventors and iDREFN denotes the 
total number of occurrences of the REFEX ele-
ment corresponding to the REF feature combina-
tion. 
It has been observed that many times the most 
probable REFEX element as identified from the 
training set is not present among the alternative 
REFEX elements. In these cases the system as-
signs the next highest probable REFEX element 
learnt from the training set that matches with one 
of the REFEX elements among the alternatives. 
In some cases more than one REFEX element get 
same probability in the training set. In these cas-
es, the REFEX element that occurs earlier in the 
alternative set is assigned. The experimental re-
sult of Baseline system is reported in Table 1. 
 Chefs Composers Inventors 
Precision 0.63 0.68 0.70 
Recall 0.69 0.60 0.64 
F-Measure 0.66 0.64 0.68 
Table 1: Result of Baseline System 
2 Discourse Level System 
The discourse level features like paragraph num-
ber, sentence number and position of a particular 
word in a sentence have been added with the fea-
tures considered in the baseline system. As men-
tioned in Section 1, more than one REFEX ele-
ment can have the same probability value. This 
happens as REFEX elements are identified by 
two features only REG08-TYPE and CASE.  
 Nam
e 
Pro-
noun 
Com-
mon 
Emp-
ty 
Chefs 2317 3071 55 646 
Composers 2616 4037 92 858 
Inventors 1959 2826 75 621 
Table 2: Distribution of REFEX Types among 
three domains. 
The above problem occurs mainly for Name 
type. Pronouns are very frequent in all the three 
domains but they have small number of varia-
tions as: he, her, him, himself, his, she, who, 
whom and whose. Common type REFEX ele-
ments are too infrequent in the training set and 
they are very hard to generalize. Empty type has 
only one REFEX value as: ?_?.  The distribution 
of the various REFEX types among the three 
domains in the training set is shown in Table 2. 
2.1 Analysis of Name type entities 
Table 2 shows that name types are very frequent 
in all the three domains. Name type entities are 
further differentiated by adding more features 
derived from the analysis of the name type ele-
ment.  
Firstly, the full name of each named entity has 
been identified by Entity identification number 
(id), maximum length among all occurrences of 
that named entity and case marker as plain. For 
example, in Figure 1, the REFEX element of id 3 
has been chosen as a full name of entity ?0? as it 
has the longest string with case ?plain?. 
After identification of full name of each RE-
FEX entity, the following features are identified 
for each occurrence of an entity:: Complete 
Name Genitive (CNG), Complete Name (CN), 
First Name Genitive (FNG), First Name (FN), 
Last Name Genitive (LNG), Last Name (LN), 
Middle Name Genitive (MNG) and Middle 
Name (MN). These features are binary in nature 
and for each occurrence of an entity only one of 
the above features will be true. 
Pronouns are kept as the REFEX element fea-
ture with its surface level pattern as they have 
only 9 variations. Common types are considered 
with tag level ?common? as they hard to general-
ize. Empty types are tagged as ?empty? as they 
have only one tag value ?_?.  
1 
<REFEX ENTITY="0" REG08-
TYPE="name" CASE="genitive">Alain 
Senderens's</REFEX> 
CNG 
2 
<REFEX ENTITY="0" REG08-
TYPE="name" 
CASE="genitive">Senderens's</REFEX> 
LNG 
3 
<REFEX ENTITY="0" REG08-
TYPE="name" CASE="plain">Alain 
Senderens</REFEX> 
CN 
4 
<REFEX ENTITY="0" REG08-
TYPE="name" 
CASE="plain">Senderens</REFEX> 
LN 
Figure 1: Example of Full Name Identification 
3 Experimental Results 
The experimental results of the discourse level 
system on the development set are reported in the 
Table 3 and Table 4 respectively. Table 3 reports 
the results when the system has been trained sep-
arately with domain specific training set and Ta-
ble 4 reports the results when the training has 
been carried out on the complete training set.  
The comparison of the results of the baseline 
and the discourse level system shows an overall 
improvement. But there are some interesting ob-
servations when comparing the results in Table 3 
and Table 4. Currently detailed analyses of the 
results are being carried out.
 Chefs Composers Inventors 
P R F P R F P R F 
Name 0.69 0.74 0.71 0.78 0.61 0.69 0.77 0.67 0.71 
Pronoun 0.81 0.76 0.79 0.70 0.84 0.76 0.76 0.87 0.81 
Common 0.76 0.87 0.81 0.37 0.44 0.40 0.44 0.65 0.68 
Empty 0.92 0.88 0.90 0.86 0.92 0.89 0.72 0.65 0.68 
 
Table 3: Experimental Results of Discourse Level System on the Development Set (Training with 
Domain Specific Training Set) 
 
 
Reg08 Type 
   String  
Accuracy 
BLEU 
NIST 
String Edit 
Distance 
Precision Recall Mean Mean Normalized 1 2 3 4 
Chefs 0.66 0.70 0.57 0.68 0.70 0.76 0.81 3.70 0.77 0.38 
Composers 0.63 0.67 0.56 0.61 0.57 0.54 0.50 3.34 1.07 0.40 
Inventors 0.60 0.62 0.50 0.55 0.54 0.52 0.49 2.90 1.25 0.47 
Total 0.63 0.66 0.54 0.61 0.58 0.57 0.55 3.83 1.03 0.42 
 
Table 4: Table 4: Experimental Results of Discourse Level System on the Development Set (Training 
with Complete Training Set) 
References 
Anja Belz. 2010. GREC Named Entity Generation Challenge 2010: Participants? Pack. 
Proceedings of the 3rd Workshop on Computational Approaches to Subjectivity and Sentiment Analysis, pages 38?46,
Jeju, Republic of Korea, 12 July 2012. c?2012 Association for Computational Linguistics
Sentimantics: Conceptual Spaces for  
Lexical Sentiment Polarity Representation with Contextuality 
 
Amitava Das                           Bj?rn Gamb?ck 
Department of Computer and Information Science 
Norwegian University of Science and Technology 
Sem S?lands vei 7-9, NO-7094 Trondheim, Norway 
amitava.santu@gmail.com    gamback@idi.ntnu.no  
 
Abstract 
Current sentiment analysis systems rely on 
static (context independent) sentiment 
lexica with proximity based fixed-point 
prior polarities. However, sentiment-
orientation changes with context and these 
lexical resources give no indication of 
which value to pick at what context. The 
general trend is to pick the highest one, but 
which that is may vary at context. To 
overcome the problems of the present 
proximity-based static sentiment lexicon 
techniques, the paper proposes a new way 
to represent sentiment knowledge in a 
Vector Space Model. This model can store 
dynamic prior polarity with varying 
contextual information. The representation 
of the sentiment knowledge in the 
Conceptual Spaces of distributional 
Semantics is termed Sentimantics. 
1  Introduction 
Polarity classification is the classical problem 
from where the cultivation of Sentiment Analysis 
(SA) started. It involves sentiment / opinion 
classification into semantic classes such as 
positive, negative or neutral and/or other fine-
grained emotional classes like happy, sad, anger, 
disgust,surprise and similar. However, for the 
present task we stick to the standard binary 
classification, i.e., positive and/or negative.   
The Concept of Prior Polarity: Sentiment 
polarity classification (?The text is positive or 
negative??) started as a semantic orientation 
determination problem: by identifying the semantic 
orientation of adjectives, Hatzivassiloglou et al 
(1997) proved the effectiveness of empirically 
building a sentiment lexicon. Turney (2002) 
suggested review classification by Thumbs Up and 
Thumbs Down, while the concept of prior polarity 
lexica was firmly established with the introduction 
of SentiWordNet (Esuli et al, 2004). 
More or less all sentiment analysis researchers 
agree that prior polarity lexica are necessary for 
polarity classification, and prior polarity lexicon 
development has been attempted for other 
languages than English as well, including for 
Chinese (He et al, 2010), Japanese (Torii et al, 
2010), Thai (Haruechaiyasak et al, 2010), and 
Indian languages (Das and Bandyopadhyay, 2010). 
Polarity Classification Using the Lexicon: High 
accuracy for prior polarity identification is very 
hard to achieve, as prior polarity values are 
approximations only. Therefore the prior polarity 
method may not excel alone; additional techniques 
are required for contextual polarity 
disambiguation. The use of other NLP methods or 
machine learning techniques over human produced 
prior polarity lexica was pioneered by Pang et al 
(2002). Several researches then tried syntactic-
statistical techniques for polarity classification, 
reporting good accuracy (Seeker et al, 2009; 
Moilanen et al, 2010), making the two-step 
methodology (sentiment lexicon followed by 
further NLP techniques) the standard method for 
polarity classification. 
Incorporating Human Psychology: The 
existing reported solutions or available systems are 
still far from perfect or fail to meet the satisfaction 
level of the end users. The main issue may be that 
there are many conceptual rules that govern 
sentiment and there are even more clues (possibly 
unlimited) that can convey these concepts from 
realization to verbalization of a human being (Liu, 
38
2010). The most recent trends in prior polarity 
adopt an approach to sentiment knowledge 
representation which lets the mental lexicon model 
hold the contextual polarity, as in human mental 
knowledge representation. 
Cambria et al (2011) made an important 
contribution in this direction by introducing a new 
paradigm: Sentic Computing1, in which they use an 
emotion representation and a Common Sense-
based approach to infer affective states from short 
texts over the web. Grassi (2009) conceived the 
Human Emotion Ontology as a high level ontology 
supplying the most significant concepts and 
properties constituting the centerpiece for the 
description of human emotions.  
The Proposed Sentimantics: The present paper 
introduces the concept of Sentimantics which is 
related to the existing prior polarity concept, but 
differs from it philosophically in terms of 
contextual dynamicity. It ideologically follows the 
path of Minsky (2006), Cambria et al (2011) and 
(Grassi, 2009), but with a different notion.  
Sentiment analysis research started years ago, 
but still the question ?What is sentiment or 
opinion?? remains unanswered! It is very hard to 
define sentiment or opinion, and to identify the 
regulating or the controlling factors of sentiment; 
an analytic definition of opinion might even be 
impossible (Kim and Hovy, 2004). Moreover, no 
concise set of psychological forces could be 
defined that really affect the writers? sentiments, 
i.e., broadly the human sentiment.  
Sentimantics tries to solve the problem with a 
practical necessity and to overcome the problems 
of the present proximity-based static sentiment 
lexicon techniques. 
As discussed earlier, the two-step methodology 
is the most common one in practice. As described 
in Section 3, a syntactic-polarity classifier was 
therefore developed, to examine the impact of 
proposed Sentimantics concept, by comparing it to 
the standard polarity classification technique. The 
strategy was tested on both English and Bengali. 
The intension behind choosing two distinct 
language families is to establish the credibility of 
the proposed methods.  
                                                           
1
 http://sentic.net/sentics/ 
For English we choose the widely used MPQA3 
corpus, but for the Bengali we had to create our 
own corpus as discussed in the following section. 
The remainder of the paper then concentrates on 
the problems with using prior polarity values only, 
in Section 4, while the Sentimantics concept proper 
is discussed in Section 5. Finally, some initial 
conclusions are presented in Section 6.  
2 Bengali Corpus  
News text can be divided into two main types: (1) 
news reports that aim to objectively present factual 
information, and (2) opinionated articles that 
clearly present authors? and readers? views, 
evaluation or judgment about some specific events 
or persons (and appear in sections such as 
?Editorial?, ?Forum? and ?Letters to the editor?). A 
Bengali news corpus has been acquired for the 
present task, based on 100 documents from the 
?Reader?s opinion? section (?Letters to the Editor?) 
from the web archive of a popular Bengali 
newspaper. 4   In total, the corpus contains 2,235 
sentences (28,805 word forms, of which 3,435 are 
distinct). The corpus has been annotated with 
positive and negative phrase polarities using 
Sanchay5, the standard annotation tool for Indian 
languages. The annotation was done semi-
automatically: a module marked the sentiment 
words from SentiWordNet (Bengali)6 and then the 
corpus was corrected manually. 
3 The Syntactic Polarity Classifier 
Adhering to the standard two-step methodology 
(i.e., prior polarity lexicon followed by any NLP 
technique), a Syntactic-Statistical polarity 
classifier based on Support Vector Machines 
(SVMs) has been quickly developed using 
SVMTool.7 The intension behind the development 
of this syntactic polarity classifier was to examine 
the effectiveness and the limitations of the standard 
two-step methodology at the same time. 
The selection of an appropriate feature set is 
crucial when working with Machine Learning 
techniques such as SVM. We decided on a feature  
                                                           
3
 http://www.cs.pitt.edu/mpqa/ 
4
 http://www.anandabazar.com/  
5
 http://ltrc.iiit.ac.in/nlpai_contest07/Sanchay/  
6
 http://www.amitavadas.com/sentiwordnet.php 
7
 http://www.lsi.upc.edu/~nlp/SVMTool/  
39
Polarity Precision Recall 
Eng. Bng. Eng. Bng. 
Total 76.03% 70.04% 65.8% 63.02% 
Positive 58.6% 56.59% 54.0% 52.89% 
Negative 76.3% 75.57% 69.4% 65.87% 
Table 1: Overall and class-wise results of 
syntactic polarity classification 
set including Sentiment Lexicon, Negative Words, 
Stems, Function Words, Part of Speech and 
Dependency Relations, as most previous research 
agree that these are the prime features to detect the 
sentimental polarity from text (see, e.g., Pang and 
Lee, 2005; Seeker et al, 2009; Moilanen et al, 
2010; Liu et. al., 2005). 
Sentiment Lexicon: SentiWordNet 3.0 8  for 
English and SentiWordNet (Bengali) for Bengali. 
Negative Words: Manually created. Contains 
80 entries collected semi-automatically from both 
the MPQA9 corpus and the Movie Review dataset10 
by Cornell for English. 50 negative words were 
collected manually for Bengali. 
Stems: The Porter Stemmer11 for English. The 
Bengali Shallow Parser12 was used to extract root 
words (from morphological analysis output). 
Function Words: Collected from the web. 13 
Only personal pronouns are dropped for the 
present task. A list of 253 entries was collected 
manually from the Bengali corpus. 
POS, Chunking and Dependency 
Relations:The Stanford Dependency parser 14  for 
English. The Bengali Shallow Parser was used to 
extract POS, chunks and dependency relations. 
 
The results of SVM-based syntactic classification 
for English and Bengali are presented in Table 1, 
both in total and for each polarity class separately.  
To understand the effects of various features on 
the performance of the system, we used the feature 
ablation method. The dictionary-based approach 
using only SentiWordNet gave a 50.50% precision 
                                                           
8
 http://sentiwordnet.isti.cnr.it/  
9
 http://www.cs.pitt.edu/mpqa/  
10
 http://www.cs.cornell.edu/People/pabo/movie-review-data/  
11
 http://tartarus.org/martin/PorterStemmer/java.txt  
12ltrc.iiit.ac.in/showfile.php?filename=downloads/shallow_par
ser.php  
13
 http://www.flesl.net/Vocabulary/Single-
word_Lists/function_word_list.php  
14
 http://nlp.stanford.edu/software/lex-parser.shtml  
Features Precision Eng. Bng. 
Sentiment Lexicon 50.50% 47.60% 
+Negative Words 55.10% 50.40% 
+Stemming 59.30% 56.02% 
+ Function Words 63.10% 58.23% 
+ Part of Speech 66.56% 61.90% 
+Chunking 68.66% 66.80% 
+Dependency Relations 76.03% 70.04% 
Table 2: Performance of the syntactic polarity 
classifier by feature ablation 
(Eng.) and 47.60% (Bng.) which can be considered 
as baselines. As seen in Table 2, incremental use of 
other features like negative words, function words, 
part of speech, chunks and tools like stemming 
improved the precision of the system to 68.66% 
(Eng.) and 66.80% (Bng.). Further use of syntactic 
features in terms of dependency relations improved 
the system precision to 76.03% (Eng.) and 70.04% 
(Bng.). The feature ablation proves the 
accountability of the two-step polarity 
classification technique. The prior polarity lexicon 
(completely dictionary-based) approach gives 
about 50% precision; the further improvements of 
the system are obtained by other NLP techniques. 
To support our argumentation for choosing 
SVM, we tested the same classification problem 
with another machine learning technique, 
Conditional Random Fields (CRF)15 with the same 
data and setup. The performance of the CRF-based 
model is much worse than the SVM, with a 
precision of 70.04% and recall of 67.02% for 
English, resp. 61.23% precision and 55.00% recall 
for Bengali. The feature ablation method was also 
tested for the CRF model and the performance was 
more or less the same when the dictionary features 
and lexical features were used (i.e., SentiWordNet 
+ Negative Words + Stemming + Function Words 
+ Part of Speech). But it was difficult to increase 
the performance level for the CRF by using 
syntactic features like chunking and dependency 
relations. SVMs work excellent to normalize this 
dynamic situation. 
It has previously been noticed that multi-engine 
based methods work well for this type of 
heterogeneous tagging task, e.g., in Named Entity 
                                                           
15
 http://crfpp.googlecode.com/svn/trunk/doc/index.html 
40
Recognition (Ekbal and Bandyopadhyay, 2010) 
and POS tagging (Shulamit et al, 2010). We have 
not tested with that kind of setup, but rather looked 
at the problem from a different perspective, 
questioning the basics: Is the two-step methodology 
for the classification task ideal or should we look 
for other alternatives? 
4 What Knowledge at What Level? 
In this section we address some limitations 
regarding the usage of prior polarity values from 
existing of prior polarity lexical resources. Dealing 
with unknown/new words is a common problem. It 
becomes more difficult for sentiment analysis 
because it is very hard to find out any contextual 
clue to predict the sentimental orientation of any 
unknown/new word. There is another problem: 
word sense disambiguation, which is indeed a 
significant subtask when applying a resource like 
SentiWordNet (Cem et al, 2011).  
A prior polarity lexicon is attached with two 
probabilistic values (positivity and negativity), but 
according to the best of our knowledge no previous 
research clarifies which value to pick in what 
context? ? and there is no information about this in 
SentiWordNet. The general trend is to pick the 
highest one, but which may vary by context. An 
example may illustrate the problem better: Suppose 
a word ?high? (Positivity: 0.25, Negativity: 0.125 
from SentiWordNet) is attached with a positive 
polarity (its positivity value is higher than its 
negativity value) in the sentiment lexicon, but the 
polarity of the word may vary in any particular use. 
Sensex reaches high+. 
Prices go high-. 
Hence further processing is required to 
disambiguate these types of words. Table 3 shows 
how many words in the SentiWordNet(s) are 
ambiguous and need special care. There are 6,619 
(Eng.) and 7,654 (Bng.) lexicon entries in 
SentiWordNet(s) where both the positivity and the 
negativity values are greater than zero. Therefore 
these entries are ambiguous because there is no 
clue in the SentiWordNet which value to pick in 
what context. Similarly, there are 3,187 (Eng.) and 
2,677 (Bng.) lexical entries in SentiWordNet(s) 
whose positivity and negativity value difference is 
less than 0.2. These are also ambiguous words. 
Types 
Eng. Bng. 
Numbers (%) 
English: n/28,430 
Bengali: n/30,000 
Total Token 115,424 30,000 
Positivity > 0 ? Negativity > 0 28,430  30,000 
Positivity > 0 ? Negativity > 0 6619 (23.28 %) 
7,654 
(25.51 %) 
Positivity > 0 ? Negativity = 0 10,484 (36.87 %) 
8,934 
(29.78 %) 
Positivity = 0 ? Negativity > 0 11,327 (39.84 %) 
11,780 
(39.26 %) 
Positivity > 0 ? Negativity > 0 ? 
 |Positivity-Negativity| ? 0.2 
3,187 
(11.20 %) 
2,677 
(8.92 %) 
Table 3: SentiWordNet(s) statistics 
The main concern of the present task is the 
ambiguous entries from SentiWordNet(s). The 
basic hypothesis is that if we can add some sort of 
contextual information with the prior polarity 
scores in the sentiment lexicon, the updated rich 
lexicon network will serve better than the existing 
one, and reduce or even remove the need for 
further processing to disambiguate the contextual 
polarity. How much contextual information would 
be needed and how this knowledge should be 
represented could be a perpetual debate. To answer 
these questions we introduce Sentimantics: 
Distributed Semantic Lexical Models to hold the 
sentiment knowledge with context. 
5 Technical Solutions for Sentimantics 
In order to propose a model of Sentimantics we 
started with existing resources such as 
ConceptNet 16  (Havasi et al, 2007) and 
SentiWordNet for English, and SemanticNet (Das 
and Bandyopadhyay, 2010) and SentiWordNet 
(Bengali) for Bengali. The common sense lexica 
like ConceptNet and SemanticNet are developed 
for general purposes, and to formalize 
Sentimantics from these resources is problematic 
due to lack of dimensionality. Section 5.1 presents 
a more rational explanation with empirical results.  
In the end we developed a Syntactic Co-
Occurrence Based Vector Space Model to hold the 
Sentimantics from scratch by a corpus driven semi-
supervised method (Section 5.2). This model 
performs better than the previous one and quite 
satisfactory. Generally extracting knowledge from 
                                                           
16
  http://csc.media.mit.edu/conceptnet 
 
41
this kind of VSM is very expensive algorithmically 
because it is a very high dimensional network. 
Another important limitation of this type of model 
is that it demands very well defined processed 
input to extract knowledge, e.g., Input: (high) 
Context: (sensex, share market, point). 
Philosophically, the motivation of Sentimantics is 
to provide a rich lexicon network which will serve 
better than the existing one and reduce the 
requirement of further language processing 
techniques to disambiguate the contextual polarity. 
This model consists of relatively fewer 
dimensions. The final model is the best performing 
lexicon network model, which could be described 
as the acceptable solution for the Sentimantics 
problem. The details of the proposed models are 
described in the following. 
5.1 Semantic Network Overlap, SNO 
We started experimentation with network overlap 
techniques. The network overlap technique finds 
overlaps of nodes between two lexical networks: 
namely ConceptNet-SentiWordNet for English and 
SemanticNet-SentiWordNet (Bengali) for Bengali. 
The working principle of the network overlap 
technique is very simple. The algorithm starts with 
any SentiWordNet node and finds its closest 
neighbours from the commonsense networks 
(ConceptNet or SemanticNet). If, for example, a 
node chosen from SentiWordNet is ?long/?, the 
closest neighbours of this concept extracted from 
the commonsense networks are: ?road (40%) / 
waiting (62%) / car (35%) / building (54%) / queue 
(70%) ?? The association scores (as the previous 
example) are also extracted to understand the 
semantic similarity association. Hence the desired 
Sentimantics lexical network is developed by this 
network overlap technique. The next prime 
challenge is to assign contextual polarity to each 
association. For this a corpus-based method was 
used; based on the MPQA17 corpus for English and 
the corpus developed by us for. The corpora are 
pre-processed with dependency relations and 
stemming using the same parsers and stemmers as 
in Section 3. The dependency relations are 
necessary to understand the relations between the 
evaluative expression and other modifier-modified 
chunks in any subjective sentence. Stemming is 
                                                           
17
 http://www.cs.pitt.edu/mpqa/ 
necessary to understand the root form of any word 
and for dictionary comparison. The corpus-driven 
method assigns each sentiment word in the 
developed lexical network a contextual prior 
polarity, as shown in Figure 1. 
Semantic network-based polarity calculation  
Once the desired lexical semantic network to hold 
the Sentimantics has been developed, we look 
further to leverage the developed knowledge for 
the polarity classification task. The methodology 
of contextual polarity extraction from the network 
is very simple, and only a dependency parser and 
stemmer are required. For example, consider the 
following sentence. 
We have been waiting in a long queue. 
To extract the contextual polarity from this 
sentence it must be known that waiting-long-queue 
are interconnected with dependency relations, and 
stemming is a necessary pre-processing step for 
dictionary matching. To extract contextual polarity 
from the developed network the desired input is 
(long) with its context (waiting, queue). The 
accumulated contextual polarity will be Neg: 
(0.50+0.35)=0.85. For comparison if the score was 
extracted from SentiWordNet (English) it would be 
Pos: 0.25 as this is higher than the negative score 
(long: Pos: 0.25, Neg: 0.125 in SentiWordNet). 
SNO performance and limitations 
An evaluation proves that the present Network 
Overlap technique outperforms the previous 
syntactic polarity classification technique. The 
precision scores for this technique are 62.3% for 
English and 59.7% for Bengali on the MPQA and  
Figure 1: The Sentimantics Network 
42
Type Number 
Solved By 
Semantic 
Overlap 
Technique 
Positivity > 0 ? 
Negativity > 0 
Eng. 6,619 2,304 (34.80 %) 
Bng. 7,654 2,450 (32 %) 
|Positivity -
Negativity| ? 0.2 
Eng. 3,187 957 (30 %) 
Bng. 2,677 830 (31.5 %) 
Table 4: Results of Semantic Overlap 
Bengali corpora: clearly higher than the baselines 
based on SentiWordNet (50.5 and 47.6%; Table 2). 
Still, the overall goal to ?reduce/remove the 
requirement to use further NLP techniques to 
disambiguate the contextual polarity? could not be 
established empirically. To understand why, we 
performed an analysis of the errors and missed 
cases of the semantic network overlap technique: 
most of the errors were caused by lack of coverage. 
ConceptNet and SemanticNet were both developed 
from the news domain and for a different task. The 
comparative coverage of SentiWordNet (English) 
and MPQA is 74%, i.e., if we make a complete set 
of sentiment words from MPQA then altogether 
74% of that set is covered by SentiWordNet, which 
is very good and an acceptable coverage. For 
Bengali the comparative coverage is 72%, which is 
also very good.  However, the comparative 
coverage of SentiWordNet (English)-ConceptNet 
and SentiWordNet (Bengali)-SemanticNet is very 
low: 54% and 50% respectively: only half of the 
sentiment words in the SentiWordNets are covered 
by ConceptNet (Eng) resp. SemanticNet (Bng). 
Now look at the evaluation in Table 4 which we 
report to support our empirical reasoning behind 
the question ?What knowledge to keep at what 
level?? It shows how much fixed point-based static 
prior polarity is being resolved by the Semantic 
Network Overlap technique. The comparative 
results are noteworthy but not satisfactory: only 
34% (Eng.) and 32% (Bng.) of the cases of 
?Positivity > 0 ? Negativity > 0? resp. 30% (Eng.) 
and 31.5 % (Bng.) of the cases of ?|Positivity - 
Negativity| ? 0.2? are resolved by this technique. 
The results are presented in Table 4. 
As a result of the error analysis, we instead 
decided to develop a Vector Space Model from 
scratch in order to solve the Sentimantics problem 
and to reach a satisfactory level of coverage. The 
experiments in this direction are reported below. 
5.2 Starting from Scratch: Syntactic Co-
Occurrence Network Construction 
A syntactic word co-occurrence network was 
constructed for only the sentimental words from 
the corpora. The syntactic network is defined in a 
way similar to previous work such the Spin Model 
(Takamura et al, 2005) and Latent Semantic 
Analysis to compute the association strength with 
seed words (Turney and Litman, 2003). The 
hypothesis is that all the words occurring in the 
syntactic territory tend to have similar semantic 
orientation.  In order to reduce dimensionality 
when constructing the network, only the open word 
classes noun, verb, adjective and adverb are 
included, as those classes tend to have maximized 
sentiment properties. Involving fewer features 
generates VSMs with fewer dimensions. 
For the network creation we again started with 
SentiWordNet 3.0 to mark the sentiment words in 
the MPQA corpus. As the MPQA corpus is marked 
at expression level, SentiWordNet was used to 
mark only the lexical entries of the subjective 
expressions in the corpus. As before, the Stanford 
POS tagger and the Porter Stemmer were used to 
get POS classes and stems of the English terms, 
while SentiWordNet (Bengali), the Bengali corpus 
and the Bengali processors were used for Bengali. 
Features were extracted from a ?4 word window 
around the target terms. To normalize the extracted 
words from the corpus we used CF-IOF, concept 
frequency-inverse opinion frequency (Cambria et 
al., 2011), while a Spectral Clustering technique 
(Dasgupta and Ng, 2009) was used for the in-depth 
analysis of word co-occurrence patterns and their 
relationships at discourse level. The clustering 
algorithm partitions a set of lexica into a finite 
number of groups or clusters in terms of their 
syntactic co-occurrence relatedness.  
Numerical weights were assigned to the words 
and then the cosine similarity measure was used to 
calculate vector similarity: 
s qk
?
,d j
??
??
?
??
= qk
?
.d j
?
= wi ,k
i=1
N? ? wi , j  -----(1)
 
When the lexicon collection is relatively static, it 
makes sense to normalize the vectors once and 
store them, rather than include the normalization in 
the similarity metric (as in Equation 2). 
s qk
?
,d j
??
??
?
??
=
wi ,k ? wi , ji=1
N?
i ,k
2
wi=1
N? ? j ,k2wj=1N?
   -------(2)
 
43
ID Lexicon 1 2 3 
1 Broker 0.63 0.12 0.04 
1 NASDAQ 0.58 0.11 0.06 
1 Sensex 0.58 0.12 0.03 
1 High 0.55 0.14 0.08 
2 India 0.11 0.59 0.02 
2 Population 0.15 0.55 0.01 
2 High 0.12 0.66 0.01 
3 Market 0.13 0.05 0.58 
3 Petroleum 0.05 0.01 0.86 
3 UAE 0.12 0.04 0.65 
3 High 0.03 0.01 0.93 
Table 5: Five example cluster centroids  
 
After calculating the similarity measures and using 
a predefined threshold value (experimentally set to 
0.5), the lexica are classified using a standard 
spectral clustering technique: Starting from a set of 
initial cluster centers, each document is assigned to 
the cluster whose center is closest to the document. 
After all documents have been assigned, the center 
of each cluster is recomputed as the centroid or 
mean j
?
?
 (where j?
?
 is the clustering coefficient) 
of its members: 
 ?
?
= 1/ jc( ) ?xx?c j?
Table 5 gives an example of cluster centroids by 
spectral clustering. Bold words in the lexicon name 
column are cluster centers. Comparing two 
members of Cluster2, ?India? and ?Population?, it 
can be seen that ?India? is strongly associated with 
Cluster2 (p=0.59), but has some affinity with the 
other clusters as well (e.g., p=0.11 with Cluster1). 
These non-zero values are still useful for 
calculating vertex weights during the contextual 
polarity calculation. 
Polarity Calculation using the Syntactic Co-
Occurrence Network 
The relevance of the semantic lexicon nodes was 
computed by summing up the edge scores of those 
edges connecting a node with other nodes in the 
same cluster. As the cluster centers also are 
interconnected with weighted vertices, inter-cluster 
relations could be calculated in terms of weighted 
network distance between two nodes within two 
separate clusters.  
 
Figure 2: Semantic affinity graph for contextual 
prior polarity 
As an example, the lexicon level semantic 
orientation from Figure 2 could be calculated as 
follows: 
Sd (wi ,wj ) =
vkk=0
n?
k
 *  wj
p
            ----(3) or
                 =
vkk=0
n?
kc=0
m?  * lc
c=0
m? *  w jp---(4)
 
Where Sd(wi,wj) is the semantic orientation of wi 
with wj given as context. Equations (3) and (4) are 
for intra-cluster and inter-cluster semantic distance 
measure respectively. k is the number of weighted 
vertices between two lexica wi and wj. vk the 
weighted vertex between two lexica, m the number 
of cluster centers between them, lc the distance 
between their cluster centers, and wpj the polarity 
of the known word wj. 
This network was created and used in particular 
to handle unknown words. For the prediction of 
semantic orientation of an unknown word, a bag-
of-words method was adopted: the bag-of-words 
chain was formed with most of the known words, 
syntactically co-located. 
A classifier based on Conditional Random 
Fields was then trained on the corpus with a small 
set of features: co-occurrence distance, ConceptNet 
similarity scores, known or unknown based on 
SentiWordNet. With the help of these very simple 
features, the CRF classifier identifies the most 
probable bag-of-words to predict the semantic 
orientation of an unknown word. As an example: 
Suppose X marks the unknown words and that the 
probable bag-of-words are: 
 
9_11-X-Pentagon-USA-Bush 
Discuss-Terrorism-X-President 
Middle_East-X-Osama 
 
44
Once the target bag-of-words has been identified, 
the following equation can be used to calculate the 
polarity of the unknown word X. 
 
Discuss-0.012-Terrorism-0.0-X-0.23-
President 
 
The scores are extracted from ConceptNet and 
the equation is:  
w
x
p
= ei
i=0
n? *  pi
j=1
n?  -----(5)  
Where ei is the edge distances extracted from 
ConceptNet and Pi is the polarity information of 
the lexicon in the bag-of-words. 
The syntactic co-occurrence network gives 
reasonable performance increment over the normal 
linear sentiment lexicon and the Semantic Network 
Overlap technique, but it has some limitations: it is 
difficult to formulate a good equation to calculate 
semantic orientation within the network. The 
formulation we use produced a less distinguishing 
value for different bag of words. As example in 
Figure 2: 
(High, Sensex)=
0.3 0.3 0.3
2
+
=
 
(Price, High)= 
0.22 0.35 0.29
2
+
=
 
 
The main problem is that it is nearly impossible 
to predict polarity for an unknown word. Standard 
polarity classifiers generally degrade in 
performance in the presence of unknown words, 
but the Syntactic Co-Occurrence Network is very 
good at handling unknown or new words. 
The performance of the syntactic co-occurrence 
measure on the corpora is shown in Table 6, with a 
70.0% performance for English and 68.0% for 
Bengali; a good increment over the Semantic 
Network Overlap technique: about 45% (Eng.) and 
41% (Bng.) of the ?Positivity > 0 ? Negativity > 0? 
cases and 43% (Eng.) and 38% (Bng.) of the 
?|Positivity ? Negativity| ? 0.2? cases were resolved 
by the Syntactic co-occurrence based technique.  
To better aid our understanding of the developed 
lexical network to hold Sentimantics we visualized 
this network using the Fruchterman Reingold force 
directed graph layout algorithm (Fruchterman and 
Reingold, 1991) and the NodeXL 18  network 
analysis tool (Smith et al, 2009). 
                                                           
18
 http://www.codeplex.com/NodeXL  
Type Number 
Solved By 
Syntactic 
Co-Occurrence 
Network 
Positivity>0 && 
Negativity>0 
Eng. 6,619 2978  (45 %) 
Bng. 7,654 3138  (41 %) 
|Positivity-
Negativity|>=0.2 
Eng. 3,187 1370 (43 %) 
Bng. 2,677 1017 (38 %) 
 Table 6: Results of the syntactic co-occurrence 
based technique 
 
6 Conclusions 
The paper has introduced Sentimantics, a new way 
to represent sentiment knowledge in the 
Conceptual Spaces of distributional Semantics by 
using in a Vector Space Model. This model can 
store dynamic prior polarity with varying 
contextual information. It is clear from the 
experiments presented that developing the Vector 
Space Model from scratch is the best solution to 
solving the Sentimantics problem and to reach a 
satisfactory level of coverage. Although it could 
not be claimed that the two issues ?What 
knowledge to keep at what level?? and 
?reduce/remove the requirement of using further 
NLP techniques to disambiguate the contextual 
polarity? were fully solved, our experiments show 
that a proper treatment of Sentimantics can 
radically increase sentiment analysis performance. 
As we showed by the syntactic classification 
technique the lexicon model only provides 50% 
accuracy and further NLP techniques increase it to 
70%, whereas by the VSM based technique it 
reaches 70% accuracy while utilizing fewer 
language processing resources and techniques.  
To the best of our knowledge this is the first 
research endeavor which enlightens the necessity 
of using the dynamic prior polarity with context. It 
is an ongoing task and presently we are exploring 
its possible applications to multiple domains and 
languages. The term Sentimantics may or may not 
remain in spotlight with time, but we do believe 
that this is high time to move on for the dynamic 
prior polarity lexica. 
  
45
References  
Cambria Erik, Amir Hussain and Chris Eckl. 2011. 
Taking Refuge in Your Personal Sentic Corner. 
SAAIP, IJCNLP, pp. 35-43. 
Cem Akkaya, Janyce Wiebe, Conrad Alexander and 
Mihalcea Rada. 2011. Improving the Impact of 
Subjectivity Word Sense Disambiguation on 
Contextual Opinion Analysis. CoNLL.  
Das Amitava and Bandyopadhyay S. 2010. 
SemanticNet-Perception of Human Pragmatics. 
COGALEX-II, COLING, pp 2-11. 
Das Amitava Bandyopadhyay S. 2010. SentiWordNet 
for Indian Languages. ALR, COLING, pp 56-63. 
Dasgupta, Sajib and Vincent Ng. 2009. Topic-wise, 
Sentiment-wise, or Otherwise? Identifying the 
Hidden Dimension for Unsupervised Text 
Classification. EMNLP. 
Ekbal A. and Bandyopadhyay S. 2010. Voted NER 
System using Appropriate Unlabeled Data. 
Lingvisticae Investigationes Journal. 
Esuli Andrea and Fabrizio Sebastiani. 2006. 
SentiWordNet: A Publicly Available Lexical 
Resource for Opinion Mining. LREC, pp. 417-422.   
Fruchterman Thomas M. J. and Edward M. Reingold. 
1991. Graph drawing by force-directed placement. 
Software: Practice and Experience, 21(11):1129?
1164. 
Grassi, Marco. 2009. Developing HEO Human 
Emotions Ontology. Joint International Conference 
on Biometric ID management and Multimodal 
Communication, vol. 5707 of LNCS, pp 244?251.  
Haruechaiyasak Choochart, Alisa Kongthon, Palingoon 
Pornpimon and Sangkeettrakarn Chatchawal. 2010. 
Constructing Thai Opinion Mining Resource: A Case 
Study on Hotel Reviews. ALR, pp 64?71. 
Hatzivassiloglou Vasileios and Kathleen R. McKeown. 
1997. Predicting the Semantic Orientation of 
Adjectives. ACL, pp. 174?181.  
Havasi, C., Speer, R., Alonso, J. 2007. ConceptNet 3: a 
Flexible, Multilingual Semantic Network for 
Common Sense Knowledge.  RANLP. 
He Yulan, Alani Harith and Zhou Deyu. 2010. 
Exploring English Lexicon Knowledge for Chinese 
Sentiment Analysis. CIPS-SIGHAN, pp 28-29. 
Kim Soo-Min and Eduard Hovy. 2004. Determining the 
Sentiment of Opinions. COLING, pp. 1367-1373. 
Liu Bing. 2010. NLP Handbook. Chapter: Sentiment 
Analysis and Subjectivity, 2nd Edition. 
Liu Hugo, Henry Lieberman and Ted Selker. 2003. A 
Model of Textual Affect Sensing using Real-World 
Knowledge. IUI, pp. 125-132. 
Minsky Marvin. 2006. The Emotion Machine. Simon 
and Schuster, New York. 
Moilanen Karo, Pulman Stephen and Zhang Yue. 2010. 
Packed Feelings and Ordered Sentiments: Sentiment 
Parsing with Quasi-compositional Polarity 
Sequencing and Compression. WASSA, pp. 36--43. 
Ohana Bruno and Brendan Tierney. 2009. Sentiment 
classification of reviews using SentiWordNet. In the 
9th IT&T Conference. 
Pang Bo, Lillian Lee and Vaithyanathan Shivakumar. 
2002. Thumbs up? Sentiment Classification using 
Machine Learning Techniques. EMNLP, pp 79-86.  
Pang, Bo and Lillian Lee. 2005. Seeing stars: Exploiting 
class relationships for sentiment categorization with 
respect to rating scales. ACL, pp. 115-124. 
Seeker Wolfgang, Adam Bermingham, Jennifer Foster 
and Deirdre Hogan. 2009. Exploiting Syntax in 
Sentiment Polarity Classification. National Centre for 
Language Technology Dublin City University, 
Ireland. 
Shulamit Umansky-Pesin, Roi Reichart and Ari 
Rappoport. 2010. A Multi-Domain Web-Based 
Algorithm for POS Tagging of Unknown Words. 
COLING.  
Smith Marc, Ben Shneiderman, Natasa Milic-Frayling, 
Eduarda Mendes Rodrigues, Vladimir  Barash, Cody 
Dunne, Tony Capone, Adam Perer, and Eric Gleave. 
2009. Analyzing (social media) networks with 
NodeXL. 4th International Conference on 
Communities and Technologies, pp. 255-264. 
Takamura Hiroya, Inui Takashi and Okumura Manabu. 
2005. Extracting Semantic Orientations of Words 
using Spin Model. ACL, pp. 133-140. 
Torii Yoshimitsu, Das Dipankar, Bandyopadhyay Sivaji 
and Okumura Manabu. 2011. Developing Japanese 
WordNet Affect for Analyzing Emotions. WASSA, 
ACL, pp. 80-86 
Turney Peter and Michael Littman. 2003. Measuring 
praise and criticism: Inference of semantic 
orientation from association. ACM Transactions on 
Information Systems, 21(4):315?346. 
Turney Peter. 2002. Thumbs up or thumbs down? 
Semantic orientation applied to unsupervised 
classification of reviews. ACL, pp. 417?424. 
Turney Peter. 2006. Similarity of Semantic Relations. 
Computational Linguistics, 32(3):379-416.  
46
Proceedings of The First Workshop on Computational Approaches to Code Switching, pages 13?23,
October 25, 2014, Doha, Qatar.
c?2014 Association for Computational Linguistics
Code Mixing: A Challenge for Language Identification in the Language of
Social Media
Utsab Barman, Amitava Das
?
, Joachim Wagner and Jennifer Foster
CNGL Centre for Global Intelligent Content, National Centre for Language Technology
School of Computing, Dublin City University, Dublin, Ireland
?
Department of Computer Science and Engineering
University of North Texas, Denton, Texas, USA
{ubarman,jwagner,jfoster}@computing.dcu.ie
amitava.das@unt.edu
Abstract
In social media communication, multilin-
gual speakers often switch between lan-
guages, and, in such an environment, au-
tomatic language identification becomes
both a necessary and challenging task.
In this paper, we describe our work in
progress on the problem of automatic
language identification for the language
of social media. We describe a new
dataset that we are in the process of cre-
ating, which contains Facebook posts and
comments that exhibit code mixing be-
tween Bengali, English and Hindi. We
also present some preliminary word-level
language identification experiments using
this dataset. Different techniques are
employed, including a simple unsuper-
vised dictionary-based approach, super-
vised word-level classification with and
without contextual clues, and sequence la-
belling using Conditional Random Fields.
We find that the dictionary-based approach
is surpassed by supervised classification
and sequence labelling, and that it is im-
portant to take contextual clues into con-
sideration.
1 Introduction
Automatic processing and understanding of Social
Media Content (SMC) is currently attracting much
attention from the Natural Language Processing
research community. Although English is still by
far the most popular language in SMC, its domi-
nance is receding. Hong et al. (2011), for exam-
ple, applied an automatic language detection algo-
rithm to over 62 million tweets to identify the top
10 most popular languages on Twitter. They found
that only half of the tweets were in English. More-
over, mixing multiple languages together (code
mixing) is a popular trend in social media users
from language-dense areas (C?ardenas-Claros and
Isharyanti, 2009; Shafie and Nayan, 2013). In
a scenario where speakers switch between lan-
guages within a conversation, sentence or even
word, the task of automatic language identifica-
tion becomes increasingly important to facilitate
further processing.
Speakers whose first language uses a non-
Roman alphabet write using the Roman alphabet
for convenience (phonetic typing) which increases
the likelihood of code mixing with a Roman-
alphabet language. This can be especially ob-
served in South-East Asia and in the Indian sub-
continent. The following is a code mixing com-
ment taken from a Facebook group of Indian uni-
versity students:
Original: Yaar tu to, GOD hain. tui JU
te ki korchis? Hail u man!
Translation: Buddy you are GOD. What
are you doing in JU? Hail u man!
This comment is written in three languages: En-
glish, Hindi (italics), and Bengali (boldface). For
Bengali and Hindi, phonetic typing has been used.
We follow in the footsteps of recent work on
language identification for SMC (Hughes et al.,
2006; Baldwin and Lui, 2010; Bergsma et al.,
2012), focusing specifically on the problem of
word-level language identification for code mixing
SMC. Our corpus for this task is collected from
Facebook and contains instances of Bengali(BN)-
English(EN)-Hindi(HI) code mixing.
The paper is organized as follows: in Section 2,
we review related research in the area of code
mixing and language identification; in Section 3,
we describe our code mixing corpus, the data it-
13
self and the annotation process; in Section 4, we
list the tools and resources which we use in our
language identification experiments, described in
Section 5. Finally, in Section 6, we conclude
and provide suggestions for future research on this
topic.
2 Background and Related Work
The problem of language identification has been
investigated for half a century (Gold, 1967) and
that of computational analysis of code switching
for several decades (Joshi, 1982), but there has
been less work on automatic language identifi-
cation for multilingual code-mixed texts. Before
turning to that topic, we first briefly survey studies
on the general characteristics of code mixing.
Code mixing is a normal, natural product of
bilingual and multilingual language use. Signif-
icant studies of the phenomenon can be found
in the linguistics literature (Milroy and Muysken,
1995; Alex, 2008; Auer, 2013). These works
mainly discuss the sociological and conversational
necessities behind code mixing as well as its lin-
guistic nature. Scholars distinguish between inter-
sentence, intra-sentence and intra-word code mix-
ing.
Several researchers have investigated the rea-
sons for and the types of code mixing. Initial stud-
ies on Chinese-English code mixing in Hong Kong
(Li, 2000) and Macao (San, 2009) indicated that
mainly linguistic motivations were triggering the
code mixing in those highly bilingual societies.
Hidayat (2012) showed that Facebook users tend
to mainly use inter-sentential switching over intra-
sentential, and report that 45% of the switching
was instigated by real lexical needs, 40% was used
for talking about a particular topic, and 5% for
content clarification. The predominance of inter-
sentential code mixing in social media text was
also noted in the study by San (2009), which com-
pared the mixing in blog posts to that in the spoken
language in Macao. Dewaele (2010) claims that
?strong emotional arousal? increases the frequency
of code mixing. Dey and Fung (2014) present
a speech corpus of English-Hindi code mixing in
student interviews and analyse the motivations for
code mixing and in what grammatical contexts
code mixing occurs.
Turning to the work on automatic analysis of
code mixing, there have been some studies on de-
tecting code mixing in speech (Solorio and Liu,
2008a; Weiner et al., 2012). Solorio and Liu
(2008b) try to predict the points inside a set of spo-
ken Spanish-English sentences where the speak-
ers switch between the two languages. Other
studies have looked at code mixing in differ-
ent types of short texts, such as information re-
trieval queries (Gottron and Lipka, 2010) and SMS
messages (Farrugia, 2004; Rosner and Farrugia,
2007). Yamaguchi and Tanaka-Ishii (2012) per-
form language identification using artificial mul-
tilingual data, created by randomly sampling text
segments from monolingual documents. King
and Abney (2013) used weakly semi-supervised
methods to perform word-level language identifi-
cation. A dataset of 30 languages has been used
in their work. They explore several language
identification approaches, including a Naive Bayes
classifier for individual word-level classification
and sequence labelling with Conditional Random
Fields trained with Generalized Expectation crite-
ria (Mann and McCallum, 2008; Mann and Mc-
Callum, 2010), which achieved the highest scores.
Another very recent work on this topic is (Nguyen
and Do?gru?oz, 2013). They report on language
identification experiments performed on Turkish
and Dutch forum data. Experiments have been
carried out using language models, dictionaries,
logistic regression classification and Conditional
Random Fields. They find that language models
are more robust than dictionaries and that contex-
tual information is helpful for the task.
3 Corpus Acquisition
Taking into account the claim that code mixing is
frequent among speakers who are multilingual and
younger in age (C?ardenas-Claros and Isharyanti,
2009), we choose an Indian student community
between the 20-30 year age group as our data
source. India is a country with 30 spoken lan-
guages, among which 22 are official. code mix-
ing is very frequent in the Indian sub-continent
because languages change within very short geo-
distances and people generally have a basic knowl-
edge of their neighboring languages.
A Facebook group
1
and 11 Facebook users
(known to the authors) were selected to obtain
publicly available posts and comments. The Face-
book graph API explorer was used for data collec-
tion. Since these Facebook users are from West
Bengal, the most dominant language is Bengali
1
https://www.facebook.com/jumatrimonial
14
(Native Language), followed by English and then
Hindi (National Language of India). The posts
and comments in Bengali and Hindi script were
discarded during data collection, resulting in 2335
posts and 9813 comments.
3.1 Annotation
Four annotators took part in the annotation task.
Three were computer science students and the
other was one of the authors. The annotators are
proficient in all three languages of our corpus. A
simple annotation tool was developed which en-
abled these annotators to identify and distinguish
the different languages present in the content by
tagging them. Annotators were supplied with 4
basic tags (viz. sentence, fragment, inclusion and
wlcm (word-level code mixing)) to annotate differ-
ent levels of code mixing. Under each tag, six at-
tributes were provided, viz. English (en), Bengali
(bn), Hindi (hi), Mixed (mixd), Universal (univ)
and Undefined (undef). The attribute univ is as-
sociated with symbols, numbers, emoticons and
universal expressions (e.g. hahaha, lol). The at-
tribute undef is specified for a sentence or a word
for which no language tags can be attributed or
cannot be categorized as univ. In addition, anno-
tators were instructed to annotate named entities
separately. What follows are descriptions of each
of the annotation tags.
Sentence (sent): This tag refers to a sentence
and can be used to mark inter-sentential code mix-
ing. Annotators were instructed to identify a sen-
tence with its base language (e.g. en, bn, hi and
mixd) or with other types (e.g. univ, undef ) as the
first task of annotation. Only the attribute mixd is
used to refer to a sentence which contains multi-
ple languages in the same proportion. A sentence
may contain any number of inclusions, fragments
and word-level code mixing. A sentence can be at-
tributed as univ if and only if it contains symbols,
numbers, emoticons, chat acronyms and no other
words (Hindi, English or Bengali). A sentence can
be attributed as undef if it is not a sentence marked
as univ and has words/tokens that can not be cate-
gorized as Hindi, English or Bengali. Some exam-
ples of sentence-level annotations are the follow-
ing:
1. English-Sentence:
[sent-lang=?en?] what a.....6 hrs long...but re-
ally nice tennis.... [/sent]
2. Bengali-Sentence:
[sent-lang=?bn?] shubho nabo borsho.. :)
[/sent]
3. Hindi Sentence:
[sent-lang=?hi?] karwa sachh ..... :( [/sent]
4. Mixed-Sentence:
[sent-lang=?mixd?] [frag-lang=?hi?] oye
hoye ..... angreji me kahte hai ke [/frag]
[frag-lang=?en?] I love u.. !!! [/frag] [/sent]
5. Univ-Sentence:
[sent-lang=?univ?] hahahahahahah....!!!!!
[/sent]
6. Undef-Sentence:
[sent-lang=?undef?] Hablando de una triple
amenaza. [/sent]
Fragment (frag): This refers to a group of for-
eign words, grammatically related, in a sentence.
The presence of this tag in a sentence conveys that
intra-sentential code mixing has occurred within
the sentence boundary. Identification of fragments
(if present) in a sentence was the second task of
annotation. A sentence (sent) with attribute mixd
must contain multiple fragments (frag) with a spe-
cific language attribute. In the fourth example
above, the sentence contains a Hindi fragment oye
hoye ..... angreji me kahte hai ke and an English
fragment I love u.. !!!, hence it is considered as a
mixd sentence. A fragment can have any number
of inclusions and word-level code mixing. In the
first example below, Jio is a popular Bengali word
appearing in the English fragment Jio.. good joke,
hence tagged as a Bengali inclusion. One can ar-
gue that the word Jio could be a separate Bengali
inclusion (i.e. can be tagged as a Bengali inclu-
sion outside the English fragment). But looking
at the syntactic pattern and the sense expressed by
the comment, the annotator kept it as a single unit.
In the second example below, an instance of word-
level code mixing, typer, has been found in an En-
glish fragment (where the root English word type
has the Bengali suffix r).
1. Fragment with Inclusion:
[sent-lang=?mixd?] [frag-lang=?en?] [incl-
lang=?bn?] Jio.. [/incl] good joke [/frag] [frag
lang=?bn?] ?amar Babin? [/frag] [/sent]
2. Fragment with Word-Level code mixing:
[sent-lang=?mixd?] [frag-lang=?en?] ? I will
find u and marry you ? [/frag] [frag-
lang=?bn?] [wlcm-type=?en-and-bn-suffix?]
typer [/wlcm] hoe glo to! :D [/frag] [/sent]
15
Inclusion (incl): An inclusion is a foreign word
or phrase in a sentence or in a fragment which
is assimilated or used very frequently in native
language. Identification of inclusions can be per-
formed after annotating a sentence and fragment
(if present in that sentence). An inclusion within a
sentence or fragment also denotes intra-sentential
code mixing. In the example below, seriously is an
English inclusion which is assimilated in today?s
colloquial Bengali and Hindi. The only tag that an
inclusion may contain is word-level code mixing.
1. Sentence with Inclusion:
[sent-lang=?bn?] Na re [incl-lang=?en?] seri-
ously [/incl] ami khub kharap achi. [/sent]
Word-Level code mixing (wlcm): This is the
smallest unit of code mixing. This tag was in-
troduced to capture intra-word code mixing and
denotes cases where code mixing has occurred
within a single word. Identifying word-level code
mixing is the last task of annotation. Annotators
were told to mention the type of word-level code
mixing in the form of an attribute (Base Language
+ Second Language) format. Some examples are
provided below. In the first example below, the
root word class is English and e is an Bengali suf-
fix that has been added. In the third example be-
low, the opposite can be observed ? the root word
Kando is Bengali, and an English suffix z has been
added. In the second example below, a named en-
tity suman is present with a Bengali suffix er.
1. Word-Level code mixing (EN-BN):
[wlcm-type=?en-and-bn-suffix?] classe
[/wlcm]
2. Word-Level code mixing (NE-BN):
[wlcm-type=?NE-and-bn-suffix?] sumaner
[/wlcm]
3. Word-Level code mixing (BN-EN):
[wlcm-type=?bn-and-en-suffix?] kandoz
[/wlcm]
3.1.1 Inter Annotator Agreement
We calculate word-level inter annotator agreement
(Cohen?s Kappa) on a subset of 100 comments
(randomly selected) between two annotators. Two
annotators are in agreement about a word if they
both annotate the word with the same attribute
(en, bn, hi, univ, undef ), regardless of whether
the word is inside an inclusion, fragment or sen-
tence. Our observations that the word-level anno-
tation process is not a very ambiguous task and
that annotation instruction is also straightforward
are confirmed in a high inter-annotator agreement
(IAA) with a Kappa value of 0.884.
3.2 Data Characteristics
Tag-level and word-level statistics of annotated
data that reveal the characteristics of our data set
are described in Table 1 and in Table 2 respec-
tively. More than 56% of total sentences and al-
most 40% of total tokens are in Bengali, which is
the dominant language of this corpus. English is
the second most dominant language covering al-
most 33% of total tokens and 35% of total sen-
tences. The amount of Hindi data is substantially
lower ? nearly 1.75% of total tokens and 2% of to-
tal sentences. However, English inclusions (84%
of total inclusions) are more prominent than Hindi
or Bengali inclusions and there are a substantial
number of English fragments (almost 52% of total
fragments) present in our corpus. This means that
English is the main language involved in the code
mixing.
Statistics of Different Tags
Tags En Bn Hi Mixd Univ Undef
sent 5,370 8,523 354 204 746 15
frag 288 213 40 0 6 0
incl 7,377 262 94 0 1,032 1
wlcm 477
Name Entity 3,602
Acronym 691
Table 1: Tag-level statistics
Word-Level Tag Count
EN 66,298
BN 79,899
HI 3,440
WLCM 633
NE 5,233
ACRO 715
UNIV 39,291
UNDEF 61
Table 2: Word-level statistics
3.2.1 Code Mixing Types
In our corpus, inter- and intra-sentential code mix-
ing are more prominent than word-level code mix-
ing, which is similar to the findings of (Hidayat,
2012) . Our corpus contains every type of code
mixing in English, Hindi and Bengali viz. in-
ter/intra sentential and word-level as described in
the previous section. Some examples of different
types of code mixing in our corpus are presented
below.
16
1. Inter-Sentential:
[sent-lang=?hi?] Itna izzat diye aapne mujhe
!!! [/sent]
[sent-lang=?en?] Tears of joy. :?( :?( [/sent]
2. Intra-Sentential:
[sent-lang=?bn?] [incl-lang=?en?] by d way
[/incl] ei [frag-lang=?en?] my craving arms
shall forever remain empty .. never hold u
close .. [/frag] line ta baddo [incl-lang=?en?]
cheezy [/incl] :P ;) [/sent]
3. Word-Level:
[sent-lang=?bn?] [incl-lang=?en?] 1st yr
[/incl] eo to ei [wlcm-type=?en+bnSuffix?]
tymer [/wlcm] modhye sobar jute jay ..
[/sent]
3.2.2 Ambiguous Words
Annotators were instructed to tag an English word
as English irrespective of any influence of word
borrowing or foreign inclusion but an inspection of
the annotations revealed that English words were
sometimes annotated as Bengali or Hindi. To un-
derstand this phenomenon we processed the list
of language (EN,BN and HI) word types (total
26,475) and observed the percentage of types that
were not always annotated with the one language
throughout the corpus. The results are presented in
Table 3. Almost 7% of total types are ambiguous
(i.e. tagged in different languages during annota-
tion). Among them, a substantial amount (5.58%)
are English/Bengali.
Label(s) Count Percentage
EN 9,109 34.40
BN 14,345 54.18
HI 1,039 3.92
EN or BN 1,479 5.58
EN or HI 61 0.23
BN or HI 277 1.04
EN or BN or HI 165 0.62
Table 3: Statistics of ambiguous and monolingual
word types
There are two reasons why this is happening:
Same Words Across Languages Some words
are the same (e.g. baba, maa, na, khali) in Hindi
and Bengali because both of the languages orig-
inated from a single language Sanskrit and share
a good amount of common vocabulary. It also
occurred in English-Hindi and English-Bengali as
a result of word borrowing. Most of these are
commonly used inclusions like clg, dept, ques-
tion, cigarette, and topic. Sometimes the anno-
tators were careful enough to tag such words as
English and sometimes these words were tagged
in the annotators? native languages. During cross
checking of the annotated data the same error pat-
terns were observed for multiple annotators, i.e.
tagging commonly used foreign words into native
language. It only demonstrates that these English
words are highly assimilated in the conversational
vocabulary of Bengali and Hindi.
Phonetic Similarity of Spellings Due to pho-
netic typing some words share the same surface
form across two and sometimes across three lan-
guages. As an example, to is a word in the three
languages: it has occurred 1209 times as English,
715 times as Bengali and 55 times as Hindi in our
data. The meaning of these words (e.g. to, bolo,
die) are different in different languages. This phe-
nomenon is perhaps exacerbated by the trend to-
wards short and noisy spelling in SMC.
4 Tools and Resources
We have used the following resources and tools in
our experiment.
Dictionaries
1. British National Corpus (BNC): We com-
pile a word frequency list from the BNC (As-
ton and Burnard, 1998).
2. SEMEVAL 2013 Twitter Corpus (Se-
mevalTwitter): To cope with the language
of social media we use the SEMEVAL 2013
(Nakov et al., 2013) training data for the
Twitter sentiment analysis task. This data
comes from a popular social media site and
hence is likely to reflect the linguistic proper-
ties of SMC.
3. Lexical Normalization List (LexNorm-
List): Spelling variation is a well-known
phenomenon in SMC. We use a lexical nor-
malization dictionary created by Han et al.
(2012) to handle the different spelling vari-
ations in our data.
Machine Learning Toolkits
1. WEKA: We use the Weka toolkit (Hall et
al., 2009) for our experiments in decision tree
training.
2. MALLET: CRF learning is applied using the
MALLET toolkit (McCallum, 2002).
17
3. Liblinear: We apply Support Vector Ma-
chine (SVM) learning with a linear kernel us-
ing the Liblinear package (Fan et al., 2008).
NLP Tools For data tokenization we used the
CMU Tweet-Tokenizer (Owoputi et al., 2013).
5 Experiments
Since our training data is entirely labelled at the
word-level by human annotators, we address the
word-level language identification task in a fully
supervised way.
Out of the total data, 15% is set aside as a
blind test set, while the rest is employed in our ex-
periments through a 5-fold cross-validation setup.
There is a substantial amount of token overlap be-
tween the cross-validation data and the test set ?
88% of total EN tokens, 86% of total Bengali to-
kens and 57% of total Hindi tokens of the test set
are present in the cross-validation data.
2
We address the problem of word-level in three
different ways:
1. A simple heuristic-based approach which
uses a combination of our dictionaries to clas-
sify the language of a word
2. Word-level classification using supervised
machine learning with SVMs but no contex-
tual information
3. Word-level classification using supervised
machine learning with SVMs and sequence
labelling using CRFs, both employing con-
textual information
Named entities and instances of word-level
code mixing are excluded from evaluation. For
systems which do not take the context of a word
into account, i.e. the dictionary-based approach
(Section 5.1) and the SVM approach without con-
textual clues (Section 5.2), named entities and in-
stances of word-level code mixing can be safely
excluded from training. For systems which do
take context into account, the CRF system (Sec-
tion 5.3.1) and the SVM system with contextual
clues (Section 5.3.2), these are included in train-
ing, because to exclude them would result in un-
realistic contexts. This means that these systems
2
We found 25 comments and 17 posts common between
the cross-validation data and the test set. The reason for this
is that users of social media often express themselves in a
concise way. Almost all of these common data consisted of 1
to 3 token(s). In most of the cases these tokens were emoti-
cons, symbols or universal expressions such as wow and lol.
As the percentage of these comments is low, we keep these
comments as they are.
can classify a word to be a named entity or an in-
stance of word-level code mixing. To avoid this,
we implement a post-processor which backs off in
these cases to a system which hasn?t seen named
entities or word-level code mixing in training (see
Section 5.3).
5.1 Dictionary-Based Detection
We start with dictionary-based language detec-
tion. Generally a dictionary-based language de-
tector predicts the language of a word based on
its frequency in multiple language dictionaries. In
our data the Bengali and Hindi tokens are phoneti-
cally typed. As no such transliterated dictionary is,
to our knowledge, available for Bengali and Hindi,
we use the training set words as dictionaries. For
words that have multiple annotations in training
data (ambiguous words), we select the majority
tag based on frequency, e.g. the word to will al-
ways be tagged as English.
Our English dictionaries are those described
in Section 4 (BNC, LexNormList, SemEvalTwit-
ter) and the training set words. For LexNorm-
List, we have no frequency information, and so
we consider it as a simple word list. To pre-
dict the language of a word, dictionaries with nor-
malized frequency were considered first (BNC,
SemEvalTwitter, Training Data), if not found,
word list look-up was performed. The predicted
language is chosen based on the dominant lan-
guage(s) of the corpus if the word appears in mul-
tiple dictionaries with same frequency or if the
word does not appear in any dictionary or list.
A simple rule-based method is applied to pre-
dict universal expressions. A token is considered
as univ if any of the following conditions satisfies:
? All characters of the token are symbols or
numbers.
? The token contains certain repetitions identi-
fied by regular expressions.(e.g. hahaha).
? The token is a hash-tag or an URL or
mention-tags (e.g. @Sumit).
? Tokens (e.g. lol) identified by a word list
compiled from the relevant 4/5th of the train-
ing data.
Table 4 shows the results of dictionary-based
detection obtained from 5-fold cross-validation
averaging. We try different combinations and fre-
quency thresholds of the above dictionaries. We
find that using a normalized frequency is helpful
18
and that a combination of LexNormList and Train-
ing Data dictionaries is suited best for our data.
Hence, we consider this as our baseline language
identification system.
Dictionary Accuracy(%)
BNC 80.09
SemevalTwitter 77.61
LexNormList 79.86
Training Data 90.21
LexNormList+TrainingData (Baseline) 93.12
Table 4: Average cross-validation accuracy of
dictionary-based detection
5.2 Word-Level Classification without
Contextual Clues
The following feature types are employed:
1. Char-n-grams (G): We start with a character
n-gram-based approach (Cavnar and Tren-
kle, 1994), which is most common and fol-
lowed by many language identification re-
searchers. Following the work of King and
Abney (2013), we select character n-grams
(n=1 to 5) and the word as the features in our
experiments.
2. Presence in Dictionaries (D): We use pres-
ence in a dictionary as a features for all avail-
able dictionaries in previous experiments.
3. Length of words (L): Instead of using the
raw length value as a feature, we follow our
previous work (Rubino et al., 2013; Wagner
et al., 2014) and create multiple features for
length using a decision tree (J48). We use
length as the only feature to train a decision
tree for each fold and use the nodes obtained
from the tree to create boolean features.
4. Capitalization (C): We use 3 boolean fea-
tures to encode capitalization information:
whether any letter in the word is capitalized,
whether all letters in the word are capitalized
and whether the first letter is capitalized.
We perform experiments with an SVM classifier
(linear kernel) for different combination of these
features.
3
Parameter optimizations (C range 2
-15
to 2
10
) for SVM are performed for each feature
3
According to (Hsu et al., 2010) the SVM linear kernel
with parameter C optimization is good enough when dealing
with a large number of features. Though an RBF kernel can
be more effective than a linear one, it is possible only after
proper optimization of C and ? parameters, which is compu-
tational expensive for such a large feature set.
Features Accuracy Features Accuracy
G 94.62 GD 94.67
GL 94.62 GDL 94.73
GC 94.64 GDC 94.72
GLC 94.64 GDLC 94.75
Table 5: Average cross-validation accuracy for
SVM word-level classification (without context),
G = char-n-gram, L = binary length features, D
= presence in dictionaries and C = capitalization
features
GDLC: 94.75%
GLC: 94.64% GDL: 94.73% GDC: 94.72%
GL: 94.62% GC: 94.64% GD: 94.67%
G: 94.62%
Figure 1: Average cross-validation accuracy for
SVM word-level classification (without context),
G = char-n-gram, L = binary length features, D
= presence in dictionaries and C = capitalization
features: cube visualization
set and best cross-validation accuracy is found for
the GDLC-based run (94.75%) at C=1 (see Table 5
and Fig. 1).
We also investigate the use of a dictionary-to-
char-n-gram back-off model ? the idea is to ap-
ply the char-n-gram model SVM-GDLC for those
words for which a majority-based decision is taken
during dictionary-based detection. However, it
does not outperform the SVM. Hence, we select
SVM-GDLC for the next steps of our experiments
as the best exemplar of our individual word-level
classifier (without contextual clues).
5.3 Language Identification with Contextual
Clues
Contextual clues can play a very important role in
word-level language identification. As an exam-
ple, a part of a comment is presented from cross-
validation fold 1 that contains the word die which
is wrongly classified by the SVM classifier. The
frequency of die in the training set of fold 1 is 6
for English, 31 for Bengali and 0 for Hindi.
Gold Data: ..../univ the/en movie/en
for/en which/en i/en can/en die/en for/en
19
Features Order-0 Order-1 Order-2
G 92.80 95.16 95.36
GD 93.42 95.59 95.98
GL 92.82 95.14 95.41
GDL 93.47 95.60 95.94
GC 92.07 94.60 95.05
GDC 93.47 95.62 95.98
GLC 92.36 94.53 95.02
GDLC 93.47 95.58 95.98
Table 6: Average cross-validation accuracy of
CRF-based predictions where G = char-n-gram, L
= length feature, D = single dictionary-based la-
bels (baseline system) and C = capitalization fea-
tures
...../univ
SVM Output: ..../univ the/en
movie/en for/en which/en i/en can/en
die/bn for/en ...../univ
We now investigate whether contextual informa-
tion can correct the mis-classified tags.
Although named entities and word-level code
mixing are excluded from evaluation, when deal-
ing with context it is important to consider named
entity and word-level code mixing during training
because these may contain some important infor-
mation. We include these tokens in the training
data for our context-based experiments, labelling
them as other. The presence of this new label may
affect the prediction for a language token during
classification and sequence labelling. To avoid this
situation, a 4-way (bn, hi, en, univ) backoff classi-
fier is trained separately on English, Hindi, Ben-
gali and universal tokens. During evaluation of
any context-based system we discard named en-
tity and word-level code mixing from the predic-
tion of that system. If any of the remaining tokens
is predicted as other we back off to the decision
of the 4-way classifier for that token. For the CRF
experiments (Section 5.3.1), the backoff classifier
is a CRF system, and, for the SVM experiments
(Section 5.3.2), the backoff classifier is an SVM
system.
5.3.1 Conditional Random Fields (CRF)
As our goal is to apply contextual clues, we first
employ Conditional Random Fields (CRF), an ap-
proach which takes history into account in pre-
dicting the optimal sequence of labels. We em-
ploy a linear chain CRF with an increasing or-
der (Order-0, Order-1 and Order-2) with 200 it-
erations for different feature combinations (used
GDLC: 95.98%
GLC: 95.02% GDL: 95.94% GDC: 95.98%
GL: 95.41% GC: 95.05% GD: 95.98%
G: 95.36%
Figure 2: CRF Order-2 results: cube visualisation
G = char-n-gram, L = binary length features, D
= presence in dictionaries and C = capitalization
features
Context Accuracy (%)
GDLC + P
1
94.66
GDLC + P
2
94.55
GDLC + N
1
94.53
GDLC + N
2
94.37
GDLC + P
1
N
1
95.14
GDLC + P
2
N
2
94.55
Table 7: Average cross-validation accuracy of
SVM (GDLC) context-based runs, where P-i =
previous i word(s) , N-i = next i word(s)
in SVM-based runs). However, we observe that
accuracy of CRF based runs decreases when bi-
narized length features (see Section 5.2 and dic-
tionary features (a feature for each dictionary) are
involved. Hence, we use the dictionary-based pre-
dictions of the baseline system to generate a single
dictionary feature for each token and only the raw
length value of a token instead of binarized length
features. The results are presented in Table 6 and
the second order results are visualized in Fig. 2.
As expected, the performance increases as the
order increases from zero to one and two. The use
of a single dictionary feature is also helpful. The
results for GDC, GDLC, and GD based runs are
almost similar (95.98%). However, we choose the
GDC system because it performed slightly better
(95.989%) than the GDLC (95.983%) and the GD
(95.983%) systems.
5.3.2 SVM with Context
We also add contextual clues to our SVM classi-
fier. To obtain contextual information we include
the previous and next two words as features in
the SVM-GDLC-based run.
4
All possible com-
4
We also experimented with extracting all GDLC features
for the context words but this did not help.
20
binations are considered during experiments (Ta-
ble 7). After C parameter optimization, the best
cross-validation accuracy is found for the P
1
N
1
(one word previous and one word next) run with
C=0.125 (95.14%).
5.4 Test Set Results
We apply our best dictionary-based system, our
best SVM system (with and without context) and
our best CRF system to the held-out test set. The
results are shown in Table 8. Our best result is
achieved using the CRF model (95.76%).
5.5 Error Analysis
Manual error analysis shows the limitations of
these systems. The word-level classifier without
contextual clues does not perform well with Hindi
data. The number of Hindi tokens is quite low.
Only 2.4% (4,658) of total tokens of the training
data are Hindi, out of which 55.36% are bilin-
gually ambiguous and 29.51% are tri-lingually
ambiguous tokens. Individual word-level systems
often fail to assign proper labels to ambiguous
words, but adding context information helps to
overcome this problem. Considering the previ-
ous example of die, both context-based SVM and
CRF systems classify it properly. Though the final
system CRF-GDC performs well, it also has some
limitations, failing to identify the language for the
tokens which appear very frequently in three lan-
guages (e.g. are, na, pic).
6 Conclusion
We have presented an initial study on automatic
language identification with Indian language code
mixing from social media communication. We
described our dataset of Bengali-Hindi-English
Facebook comments and we presented the results
of our word-level classification experiments on
this dataset. Our experimental results lead us to
conclude that character n-gram features are useful
for this task, contextual information is also impor-
tant and that information from dictionaries can be
effectively incorporated as features.
In the future we plan to apply the techniques
and feature sets that we used in these experiments
to other datasets. We have already started this by
applying variants of the systems presented here to
the Nepali-English and Spanish-English datasets
which were introduced as part of the 2014 code
mixing shared task (Solorio et al., 2014; Barman
et al., 2014).
We did not include word-level code mixing in
our experiments ? in our future experiments we
will explore ways to identify and segment this type
of code mixing. It will be also important to find the
best way to handle inclusions since there is a fine
line between word borrowing and code mixing.
Acknowledgements
This research is supported by the Science Founda-
tion Ireland (Grant 12/CE/I2267) as part of CNGL
(www.cngl.ie) at Dublin City University. The
authors wish to acknowledge the DJEI/DES/SFI/
HEA for the provision of computational facili-
ties and support. Our special thanks to Soumik
Mandal from Jadavpur University, India for co-
ordinating the annotation task. We also thank the
administrator of JUMatrimonial and the 11 Face-
book users who agreed that we can use their posts
for their support and permission.
References
Beatrice Alex. 2008. Automatic detection of English
inclusions in mixed-lingual data with an application
to parsing. Ph.D. thesis, School of Informatics, The
University of Edinburgh, Edinburgh, UK.
Guy Aston and Lou Burnard. 1998. The BNC hand-
book: exploring the British National Corpus with
SARA. Capstone.
Peter Auer. 2013. Code-Switching in Conversation:
Language, Interaction and Identity. Routledge.
Timothy Baldwin and Marco Lui. 2010. Language
identification: The long and the short of the mat-
ter. In Human Language Technologies: The 2010
Annual Conference of the North American Chap-
ter of the Association for Computational Linguistics,
pages 229?237. Association for Computational Lin-
guistics.
Utsab Barman, Joachim Wagner, Grzegorz Chrupa?a,
and Jennifer Foster. 2014. DCU-UVT: Word-
level language classification with code-mixed data.
In Proceedings of the First Workshop on Com-
putational Approaches to Code-Switching. EMNLP
2014, Conference on Empirical Methods in Natural
Language Processing, Doha, Qatar. Association for
Computational Linguistics.
Shane Bergsma, Paul McNamee, Mossaab Bagdouri,
Clayton Fink, and Theresa Wilson. 2012. Language
identification for creating language-specific Twitter
collections. In Proceedings of the Second Workshop
on Language in Social Media, pages 65?74. Associ-
ation for Computational Linguistics.
21
System
Precision (%) Recall (%) Accuracy
(%)EN BN HI UNIV EN BN HI UNIV
Baseline (Dictionary) 92.67 90.73 80.64 99.67 92.28 94.63 43.47 94.99 93.64
SVM-GDLC 92.49 94.89 80.31 99.34 96.23 94.28 44.92 97.07 95.21
SVM-P
1
N
1
93.51 95.56 83.18 99.42 96.63 95.23 55.94 96.95 95.52
CRF-GDC 94.77 94.88 91.86 99.34 95.65 96.22 55.65 97.73 95.76
Table 8: Test set results for Baseline (Dictionary), SVM-GDLC, SVM-P1N1 and CRF-GDC
MS C?ardenas-Claros and N Isharyanti. 2009. Code-
switching and code-mixing in internet chatting:
Between?yes,?ya,?and?si?-a case study. The Jalt Call
Journal, 5(3):67?78.
William B. Cavnar and John M. Trenkle. 1994. N-
gram-based text categorization. In Theo Pavlidis,
editor, Proceedings of SDAIR-94, Third Annual
Symposium on Document Analysis and Information
Retrieval, pages 161?175.
Jean-Marc Dewaele. 2010. Emotions in Multiple Lan-
guages. Palgrave Macmillan.
Anik Dey and Pascale Fung. 2014. A Hindi-
English code-switching corpus. In Proceedings of
the Ninth International Conference on Language Re-
sources and Evaluation (LREC?14), pages 2410?
2413, Reykjavik, Iceland. European Language Re-
sources Association (ELRA).
Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh, Xiang-
Rui Wang, and Chih-Jen Lin. 2008. LIBLINEAR:
A library for large linear classification. Journal of
Machine Learning Research, 9:1871?1874.
Paulseph-John Farrugia. 2004. TTS pre-processing is-
sues for mixed language support. In Proceedings
of CSAW?04, the second Computer Science Annual
Workshop, pages 36?41. Department of Computer
Science & A.I., University of Malta.
E Mark Gold. 1967. Language identification in the
limit. Information and control, 10(5):447?474.
Thomas Gottron and Nedim Lipka. 2010. A compar-
ison of language identification approaches on short,
query-style texts. In Advances in Information Re-
trieval, pages 611?614. Springer.
Mark Hall, Eibe Frank, Geoffrey Holmes, Bernhard
Pfahringer, Peter Reutemann, and Ian H. Witten.
2009. The WEKA data mining software: An up-
date. SIGKDD Explor. Newsl., 11(1):10?18.
Bo Han, Paul Cook, and Timothy Baldwin. 2012.
Automatically constructing a normalisation dictio-
nary for microblogs. In Proceedings of the 2012
Joint Conference on Empirical Methods in Natu-
ral Language Processing and Computational Natu-
ral Language Learning, pages 421?432. Association
for Computational Linguistics.
Taofik Hidayat. 2012. An analysis of code switch-
ing used by facebookers: a case study in a
social network site. Student essay for the
study programme ?Pendidikan Bahasa Ing-
gris? (English Education) at STKIP Siliwangi
Bandung, Indonesia, http://publikasi.
stkipsiliwangi.ac.id/files/2012/
10/08220227-taofik-hidayat.pdf.
Lichan Hong, Gregorio Convertino, and Ed H. Chi.
2011. Language matters in twitter: A large scale
study. In Proceedings of the Fifth International
AAAI Conference on Weblogs and Social Media
(ICWSM-11), pages 518?521, Barcelona, Spain. As-
sociation for the Advancement of Artificial Intelli-
gence.
Chih-Wei Hsu, Chih-Chung Chang, and Chih-
Jen Lin. 2010. A practical guide to sup-
port vector classification. Technical re-
port. Department of Computer Science, Na-
tional Taiwan University, Taiwan, https:
//www.cs.sfu.ca/people/Faculty/
teaching/726/spring11/svmguide.pdf.
Baden Hughes, Timothy Baldwin, Steven Bird, Jeremy
Nicholson, and Andrew MacKinlay. 2006. Recon-
sidering language identification for written language
resources. In Proc. of the 5th edition of the Interna-
tional Conference on Language Resources and Eval-
uation (LREC 2006), pages 485?488, Genoa, Italy.
Aravind K. Joshi. 1982. Processing of sentences with
intra-sentential code-switching. In J. Horeck?y, ed-
itor, Proceedings of the 9th conference on Compu-
tational linguistics - Volume 1 (COLING?82), pages
145?150. Academia Praha, North-Holland Publish-
ing Company.
Ben King and Steven Abney. 2013. Labeling the lan-
guages of words in mixed-language documents us-
ing weakly supervised methods. In Proceedings of
the 2013 Conference of the North American Chap-
ter of the Association for Computational Linguis-
tics: Human Language Technologies, pages 1110?
1119, Atlanta, Georgia. Association for Computa-
tional Linguistics.
David C. S. Li. 2000. Cantonese-English code-
switching research in Hong Kong: a Y2K review.
World Englishes, 19(3):305?322.
Gideon S. Mann and Andrew McCallum. 2008.
Generalized expectation criteria for semi-supervised
learning of conditional random fields. In Proceed-
ings of ACL-08: HLT, pages 870?878, Columbus,
Ohio. Association for Computational Linguistics.
22
Gideon S. Mann and Andrew McCallum. 2010.
Generalized expectation criteria for semi-supervised
learning with weakly labeled data. The Journal of
Machine Learning Research, 11:955?984.
Andrew Kachites McCallum. 2002. Mal-
let: A machine learning for language toolkit.
http://mallet.cs.umass.edu.
Lesley Milroy and Pieter Muysken, editors. 1995. One
speaker, two languages: Cross-disciplinary per-
spectives on code-switching. Cambridge University
Press.
Preslav Nakov, Sara Rosenthal, Zornitsa Kozareva,
Veselin Stoyanov, Alan Ritter, and Theresa Wilson.
2013. Semeval-2013 task 2: Sentiment analysis in
twitter. In Second Joint Conference on Lexical and
Computational Semantics (*SEM), Volume 2: Pro-
ceedings of the Seventh International Workshop on
Semantic Evaluation (SemEval 2013), pages 312?
320, Atlanta, Georgia, USA. Association for Com-
putational Linguistics.
Dong Nguyen and A. Seza Do?gru?oz. 2013. Word
level language identification in online multilingual
communication. In Proceedings of the 2013 Con-
ference on Empirical Methods in Natural Language
Processing (EMNLP 2013), pages 857?862, Seattle,
Washington, USA. Association for Computational
Linguistics.
Olutobi Owoputi, Brendan O?Connor, Chris Dyer,
Kevin Gimpel, Nathan Schneider, and Noah A.
Smith. 2013. Improved part-of-speech tagging for
online conversational text with word clusters. In
Proceedings of the 2013 Conference of the North
American Chapter of the Association for Computa-
tional Linguistics: Human Language Technologies
(NAACL-HLT 2013), pages 380?390, Atlanta, Geor-
gia. Association for Computational Linguistics.
Mike Rosner and Paulseph-John Farrugia. 2007. A
tagging algorithm for mixed language identifica-
tion in a noisy domain. In INTERSPEECH-2007,
8th Annual Conference of the International Speech
Communication Association, pages 190?193. ISCA
Archive.
Raphael Rubino, Joachim Wagner, Jennifer Foster, Jo-
hann Roturier, Rasoul Samad Zadeh Kaljahi, and
Fred Hollowood. 2013. DCU-Symantec at the
WMT 2013 quality estimation shared task. In Pro-
ceedings of the Eighth Workshop on Statistical Ma-
chine Translation, pages 392?397, Sofia, Bulgaria.
Association for Computational Linguistics.
Hong Ka San. 2009. Chinese-English code-switching
in blogs by Macao young people. Master?s the-
sis, The University of Edinburgh, Edinburgh, UK.
http://hdl.handle.net/1842/3626.
Latisha Asmaak Shafie and Surina Nayan. 2013.
Languages, code-switching practice and primary
functions of facebook among university students.
Study in English Language Teaching, 1(1):187?
199. http://www.scholink.org/ojs/
index.php/selt.
Thamar Solorio and Yang Liu. 2008a. Learning to pre-
dict code-switching points. In Proceedings of the
Conference on Empirical Methods in Natural Lan-
guage Processing, pages 973?981. Association for
Computational Linguistics.
Thamar Solorio and Yang Liu. 2008b. Part-of-speech
tagging for English-Spanish code-switched text. In
Proceedings of the Conference on Empirical Meth-
ods in Natural Language Processing, pages 1051?
1060. Association for Computational Linguistics.
Thamar Solorio, Elizabeth Blair, Suraj Maharjan, Steve
Bethard, Mona Diab, Mahmoud Gonheim, Abdelati
Hawwari, Fahad AlGhamdi, Julia Hirshberg, Alison
Chang, and Pascale Fung. 2014. Overview for the
first shared task on language identification in code-
switched data. In Proceedings of the First Workshop
on Computational Approaches to Code-Switching.
EMNLP 2014, Conference on Empirical Methods in
Natural Language Processing, Doha, Qatar. Associ-
ation for Computational Linguistics.
Joachim Wagner, Piyush Arora, Santiago Cortes, Utsab
Barman, Dasha Bogdanova, Jennifer Foster, and
Lamia Tounsi. 2014. DCU: Aspect-based polarity
classification for SemEval task 4. In Proceedings
of the International Workshop on Semantic Evalu-
ation (SemEval-2014), pages 392?397, Dublin, Ire-
land. Association for Computational Linguistics.
Jochen Weiner, Ngoc Thang Vu, Dominic Telaar, Flo-
rian Metze, Tanja Schultz, Dau-Cheng Lyu, Eng-
Siong Chng, and Haizhou Li. 2012. Integration
of language identification into a recognition system
for spoken conversations containing code-switches.
In Proceedings of the 3rd Workshop on Spoken Lan-
guage Technologies for Under-resourced Languages
(SLTU?12), Cape Town, South Africa. International
Research Center MICA.
Hiroshi Yamaguchi and Kumiko Tanaka-Ishii. 2012.
Text segmentation by language using minimum de-
scription length. In Proceedings of the 50th An-
nual Meeting of the Association for Computational
Linguistics: Long Papers-Volume 1, pages 969?978.
Association for Computational Linguistics.
23
Proceedings of the 8th International Natural Language Generation Conference, pages 103?107,
Philadelphia, Pennsylvania, 19-21 June 2014. c?2014 Association for Computational Linguistics
A Framework for Health Behavior Change using Companionable Robots
Bandita Sarma
University of North Texas
banditasarma@my.unt.edu
Amitava Das
University of North Texas
Amitava.Das@unt.edu
Rodney D. Nielsen
University of North Texas
Rodney.Nielsen@unt.edu
Abstract
In this paper, we describe a dialogue
system framework for a companionable
robot, which aims to guide patients to-
wards health behavior changes via natu-
ral language analysis and generation. The
framework involves three broad stages,
rapport building and health topic identifi-
cation, assess patient?s opinion of change,
and designing plan and closing session.
The framework uses concepts from psy-
chology, computational linguistics, and
machine learning and builds on them. One
of the goals of the framework is to ensure
that the Companionbot builds and main-
tains rapport with patients.
1 Introduction
Human beings engage in many activities or be-
haviors that can aggravate existing health prob-
lems or lead to new ones. Abandoning such be-
haviors with the help of motivational interview-
ing or counseling sessions is called health behav-
ior change. Providing counseling sessions that re-
sult in behavior change is a difficult task even for
expert practitioners, and hence poses a great chal-
lenge for automated dialogue systems. The pro-
cess demands constant monitoring and an in-depth
understanding of the patient1. A wrong move on
the counselor?s part could undo what might oth-
erwise be a successful attempt to bring about the
targeted health behavior change. This could re-
quire the counselor to return to the first stage of
the process and regain the patient?s trust.
In this paper, we describe a framework for a
companionable robot, which can counsel its hu-
man companion and assist them in making health-
ier choices for a better life. In our previous work
1The terms patient and user are used interchangeably
throughout the paper to refer to the human using the Com-
panionbot.
(Nielsen et al., 2010), we described an overall ar-
chitecture for a companion robot capable of proac-
tive dialogue with patients. This paper focuses
on a natural language processing framework for
health behavior change dialogue. Bickmore and
Sidner (2006) outline a plan-based framework,
COLLAGEN, based on the transtheoretical model
and motivational interviewing to generate health
behavior change counseling dialogue for physical
activity promotion. COLLAGEN conducts a ses-
sion in four steps: greeting exchange, discussion
of previous day?s exercise, discussion of plans for
next day, and finally, farewell exchange. In addi-
tion to having similar steps, our framework also
discusses in detail the natural language process-
ing modules that are involved in judging the user?s
mindset at each step and guiding him/her towards
making a decision on changing health behavior. In
their follow up work (Bickmore et al., 2009), they
discuss several issues such as minimizing repet-
itiveness in the behavioral, linguistic and visual
aspect of the agent, establishing a therapeutic al-
liance between the user and the agent for a suc-
cessful dialogue, maintaining continuity over mul-
tiple sessions, and the challenge of open-ended
question generation. In addition to these issues,
there might be verbal resistance from the patient
to the suggestions by the Companionbot.
Use of telemedicine is becoming a common
practice in providing remote clinical health care. It
involves the use of various technologies like tele-
phone, Facsimile, e-mail, video meetings, etc. to
provide medical services. However, telemedicine
is not flexible and adaptive, or when it is, it re-
quires a human in the loop. It might also require
long wait times on the patient side to receive a
response from a health expert. Using compan-
ionable robots to provide guidance for health be-
havior change can provide greater flexibility com-
pared to standard telemedicine practices. Bajwa
(2010) described a virtual medical expert system
103
that leverages natural language processing tech-
niques to provide medical help to user queries
from a knowledge base using pattern matching.
In case the query does not have a match in the
knowledge base, it is directed to an expert. The
present work is along similar lines in terms of pro-
viding medical advice but in case of an unknown
health condition, the Companionbot provides in-
formation through Web search. In addition to this,
our framework adds the capability of generating
small talk, which will help the user overcome inhi-
bitions that might arise in talking to a robot instead
of a human. The medical advice provided by the
Companionbot will be in the form of suggestions
rather than instructions. This is intended to make
users reflect on their own choices comfortably in-
stead of receiving instructions from the Compan-
ionbot?s advice. The Nursebot project (Roy et al.,
2000) discussed five different functions to assist
the elderly through personal robots. One of the
functions is to provide virtual telemedicine based
facilities. Another robot called Paro (Kidd et al.,
2006) was developed to cater to the social needs of
elderly in nursing homes and was capable of gen-
erating a small set of vocal utterances in addition
to limited voice recognition and body movement.
Our framework, when implemented successfully,
will be capable of engaging the user in a complete
conversation, both casual and therapeutic.
2 Framework
The proposed dialogue system framework con-
sists of three broad stages. The first stage aims
to build rapport with the patient and identify the
health topic to be discussed. The second stage
involves identifying the issues and challenges the
patient perceives associated with enacting relevant
health behavior changes and motivating the patient
to make the most appropriate change(s). The final
stage summarizes the overall plans and goals, and
encourages the patient to follow through. The en-
tire process from building rapport with the patient
through motivating changes in health-related be-
havior may span several sessions, and of course, is
likely to be repeated for other behaviors.
2.1 Build rapport & identify health topic
In order to initiate a counseling session it is essen-
tial to build and maintain rapport with the patient.
This helps the patient feel more comfortable with
the situation, which facilitates more open commu-
nication. Reasonable rapport needs to be estab-
lished in the initial stages when the Companionbot
is first introduced to the patient. However, since
the Companionbot is meant to be present con-
stantly with its human companion, rapport build-
ing and maintenance is expected to be an on-
going process. Throughout both the casual and
health behavior change dialogue, the Companion-
bot will identify the patient?s interpersonal rela-
tions, health conditions and beliefs, likes and dis-
likes, habits, hobbies, and routines. These will be
stored in a user model, which the language gen-
eration engine will exploit to engage the user in
dialogue that is guided by, and infused with, per-
sonal context. A language understanding compo-
nent will constantly assess the user?s engagement
level in the conversation. If the user seems to
be disinterested at any point, a Typical Day strat-
egy (Mason and Butler, 2010) is used to deal with
the situation where the Companionbot will ask the
user what a typical day for them is like.
When the system has achieved an adequate level
of rapport, the next step is to identify a health
topic of concern to the patient, so that there can
be a focused discussion geared towards health be-
havior change. The present project will start with
a small list of conditions and the behaviors that,
when altered, can bring about an improvement in
the condition. For example, heart disease includes
diet and exercise, among others, as the associ-
ated behaviors. These conditions will be identi-
fied primarily using named-entity recognition and
keyword spotting. If the Companionbot identi-
fies heart disease as the topic, then the discussion
could focus on food habits or exercise related is-
sues.
2.2 Assess patient?s opinion of change
Once a health concern is identified, the next step
is to determine how important the patient thinks
it is to change the associated behaviors and how
confident they are about enacting those changes.
This is an important stage because not all people
have the same mindset regarding behavior change.
Some might understand the importance of it but
are not confident about achieving it while others
might not consider it important at all. In order to
understand the user?s mindset, Mason and Butler
(2010) suggest asking the user to assign cardinal
values to quantify these opinions. The values may
be on a scale of 0 to 10, where 0 is the lowest and
104
Figure 1: Block diagram for assessing patient?s
opinion of change
10 is the highest.
If there is a large difference between the user
ratings of importance and confidence, the Com-
panionbot will discuss the lower-ranked factor first
(Mason and Butler, 2010). If the scores are ap-
proximately equal (e.g., the patient gives both im-
portance and confidence a medium rating), then
the Companionbot?s dialogue will focus on help-
ing the user understand the importance of the be-
havior change (Mason and Butler, 2010). Low val-
ues for both importance and confidence scores in-
dicate that the user is not ready for these health
behavior changes (Mason and Butler, 2010), and
the Companionbot should move on to a different
health topic or behavior. If both the scores are
high, the Companionbot can move on to the next
stage, summarizing the discussion and motivating
the behavior changes. Figure 1 shows the block
diagram representation for this module.
2.3 Design plan & close the session
The Companionbot moves toward concluding the
conversation by asking an open-ended question re-
garding how the user feels about the health be-
havior changes that they have been discussing.
A user?s attitude can be categorized into one of
three categories, ready for change, not ready for
change, or ambivalent. If the patient is ready for
change, the Companionbot will provide sugges-
tions on how to bring about the change in the be-
Figure 2: Block diagram for designing plan and
closing the session
havior in previous step by leveraging knowledge
from the user model and the conversation history.
There may be patients who belong to the second
category and are not ready for the health behav-
ior change. We have already discussed ways on
how to tackle such a situation in Subsection 2.2.
If the patient is ambivalent about changing a be-
havior, the Companionbot will close by providing
information to help the patient reflect on the pros
and cons of the health behavior change until it is
appropriate to bring it up again in a future ses-
sion. A knowledge base will be maintained about
the behaviors associated with common and criti-
cal health conditions. Information about a health
condition, which is outside the domain of cur-
rent knowledge base, will be retrieved using Web
search. Figure 2 shows the block diagram repre-
sentation of this stage.
If a session exceeds a pre-defined time, deemed
to be the limit of most patients? ability to stay ad-
equately focused on health behavior change, or
if the system recognizes that the patient is los-
ing their focus, the Companionbot will check-in
with the patient, and if appropriate, will bring the
session to a close following strategies that parallel
those described in the preceding paragraph.
3 Challenges
Automatic generation of dialogue becomes a par-
ticularly challenging task when its purpose is to
105
guide people through sensitive or personal issues
like health behavior change. Some patients may
not like to be told what is good or bad for them.
In such a case, the patient might begin resisting
suggestions for change (Mason and Butler, 2010).
This places the entire counseling session in a pre-
carious position and any wrong move on the Com-
panionbot?s part could push the patient to a higher
level of resistance. To mitigate this scenario, the
Companionbot will include patient resistance de-
tection in the framework. If mild resistance is de-
tected, the discourse is automatically directed to-
wards bringing the user back on track. Whereas if
there is high resistance, the Companionbot moves
on to a different topic In case the user continues re-
sisting then the Companionbot will close the ses-
sion.
For successful implementation of therapeutic
dialogue systems, it is essential to ensure that they
do not sound monotonous. This is possible only
if the responses are generated dynamically and
hardcoding is limited. During rapport building
and user modeling, questions will be generated by
the Companionbot from various sources like the
Internet, medical forms, information provided by
physicians, family members, etc. At other times,
responses will be constructed using both syntactic
and semantic information from the user utterances.
Since multiple sessions might be held with the
user to discuss a specific behavior, it is neces-
sary to maintain continuity between the sessions
(Bickmore et al., 2009). Bickmore and Sidner
(2006) advocate dedicating a part of the dialogue
to reviewing prior discussions, associated actions,
and patient plans, as well as discussing what the
patient has done since the last session to follow
though on their plans. The Companionbot main-
tains a detailed user model including logs of the
previous sessions, which will be used to review
prior discussions, plans and actions and to guide
ongoing motivational interviews.
Another challenge is choosing appropriate eval-
uation measures to determine the system?s use-
fulness in bringing about the desired change in
the patient. The efficacy of the system will be
judged by monitoring the users behavior regularly.
Any noticeable changes, such as weight gain or
loss and increased or decreased smoking, will be
tracked. How frequently a patient interacts with
the Companionbot is an implicit qualitative mea-
sure of how much they appreciate it. We also plan
to use questionnaires to elicit user ratings of the
system for its acceptability and quality on a Lick-
ert scale (Lickert, 1932).
4 Conclusion
In this paper we proposed a novel framework
for automatic health behavior change counsel-
ing. Successful implementation of this frame-
work would mean that the Companionbot could be
used to guide patients towards bringing changes in
their behavior for a healthier life. This can reduce
the long wait period in conventional telemedicine
practices from the time the patients contact the
remote heatlh care provider to the instance they
receive the instruction (Bajwa, 2010). Since the
Companionbot will be capable of small talk aimed
at connecting with the user on an emotional level,
we hypothesize it will be perceived as being much
more natural than existing conversational robots.
References
Cory D. Kidd, Will Taggart and Sherry Turkle. 2006.
A Sociable Robot to Encourage Social Interaction
among the Elderly. IEEE International Conference
on Robotics and Automation, 3972?3976.
Imran S. Bajwa. 2010. Virtual Telemedicine Using
Natural Language Processing. International Jour-
nal of Information Technology and Web Engineer-
ing, 5(1):43?55.
Nicholas Roy, Gregory Baltus, Dieter Fox, Francine
Gemperle, Jennifer Goetz, Tad Hirsch, Dimitris
Margaritis, Michael Montemerlo, Joelle Pineau,
Jamieson Schulte and Sebastian Thrun. 2000. To-
wards Personal Service Robots for the Elderly.
Workshop on Interactive Robots and Entertainment.
Pip Mason and Christopher C. Butler. 2010. Health
Behavior Change. Elsevier Health Sciences.
Rensis Likert. 1932. A Technique for the Measurement
of Attitudes. Archives of Psychology, 140:1?55.
Rodney D. Nielsen, Richard Voyles, Daniel Bolanos,
Mohammad H. Mahoor, Wilson D. Pace, Katie A.
Siek and Wayne H. Ward. 2010. A Platform for
Human-Robot Dialog Systems Research. In Pro-
ceedings of AAAI Fall Symposium, Dialog with
Robots, 161?162.
Timothy W. Bickmore and Candace L. Sidner.
2006. Towards Plan-based Health Behavior Change
Counseling Systems. In proceedings of AAAI
Spring Symposium on Argumentation for Con-
sumers of Healthcare.
106
Timothy Bickmore, Daniel Mauer, Francisco Crespo
and Thomas Brown. 2008. Negotiating Task In-
terruptions with Virtual Agents for Health Behav-
ior Change. In Proceedings of the 7th International
Joint Conference on Autonomous Agents and Mul-
tiagent Systems, 1241?1244.
Timothy Bickmore, Daniel Schulman and Candace
Sidner. 2009. Issues in Designing Agents for Long
Term Behavior Change. CHI?09 Workshop on En-
gagement by Design.
107
