Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 443?451,
Gothenburg, Sweden, April 26-30 2014.
c
?2014 Association for Computational Linguistics
Is Machine Translation Getting Better over Time?
Yvette Graham Timothy Baldwin Alistair Moffat Justin Zobel
Department of Computing and Information Systems
The University of Melbourne
{ygraham,tbaldwin,ammoffat,jzobel}@unimelb.edu.au
Abstract
Recent human evaluation of machine
translation has focused on relative pref-
erence judgments of translation quality,
making it difficult to track longitudinal im-
provements over time. We carry out a
large-scale crowd-sourcing experiment to
estimate the degree to which state-of-the-
art performance in machine translation has
increased over the past five years. To fa-
cilitate longitudinal evaluation, we move
away from relative preference judgments
and instead ask human judges to provide
direct estimates of the quality of individ-
ual translations in isolation from alternate
outputs. For seven European language
pairs, our evaluation estimates an aver-
age 10-point improvement to state-of-the-
art machine translation between 2007 and
2012, with Czech-to-English translation
standing out as the language pair achiev-
ing most substantial gains. Our method
of human evaluation offers an economi-
cally feasible and robust means of per-
forming ongoing longitudinal evaluation
of machine translation.
1 Introduction
Human evaluation provides the foundation for em-
pirical machine translation (MT), whether human
judges are employed directly to evaluate system
output, or via the use of automatic metrics ?
validated through correlation with human judg-
ments. Achieving consistent human evaluation
is not easy, however. Annual evaluation cam-
paigns conduct large-scale human assessment but
report ever-decreasing levels of judge consistency
? when given the same pair of translations to
repeat-assess, even expert human judges will wor-
ryingly often contradict both the preference judg-
ment of other judges and their own earlier prefer-
ence (Bojar et al., 2013). For this reason, human
evaluation has been targeted within the commu-
nity as an area in need of attention, with increased
efforts to develop more reliable methodologies.
One standard platform for human evaluation is
WMT shared tasks, where assessments have (since
2007) taken the form of ranking five alternate sys-
tem outputs from best to worst (Bojar et al., 2013).
This method has been shown to produce more con-
sistent judgments compared to fluency and ade-
quacy judgments on a five-point scale (Callison-
Burch et al., 2007). However, relative preference
judgments have been criticized for being a sim-
plification of the real differences between trans-
lations, not sufficiently taking into account the
large number of different types of errors of vary-
ing severity that occur in translations (Birch et al.,
2013). Relative preference judgments do not take
into account the degree to which one translation is
better than another ? there is no way of knowing if
a winning system produces far better translations
than all other systems, or if that system would have
ranked lower if the severity of its inferior transla-
tion outputs were taken into account.
Rather than directly aiming to increase human
judge consistency, some methods instead increase
the number of reference translations available to
automatic metrics. HTER (Snover et al., 2006)
employs humans to post-edit each system out-
put, creating individual human-targeted reference
translations which are then used as the basis for
computing the translation error rate. HyTER, on
the other hand, is a tool that facilitates creation
of very large numbers of reference translations
(Dreyer and Marcu, 2012). Although both ap-
proaches increase fairness compared to automatic
metrics that use a single generic reference transla-
tion, even human post-editors will inevitably vary
in the way they post-edit translations, and the pro-
cess of creating even a single new reference trans-
443
lation for each system output is often too resource-
intensive to be used in practice.
With each method of human evaluation, a trade-
off exists between annotation time and the number
of judgments collected. At one end of the spec-
trum, the WMT human evaluation collects large
numbers of quick judgments (approximately 3.5
minutes per screen, or 20 seconds per label) (Bojar
et al., 2013).
1
In contrast, HMEANT (Lo and Wu,
2011) uses a more time-consuming fine-grained
semantic-role labeling analysis at a rate of approx-
imately 10 sentences per hour (Birch et al., 2013).
But even with this detailed evaluation methodol-
ogy, human judges are inconsistent (Birch et al.,
2013).
Although the trend appears to be toward more
fine-grained human evaluation of MT output, it
remains to be shown that this approach leads to
more reliable system rankings ? with a main rea-
son to doubt this being that far fewer judgments
will inevitably be possible. We take a counter-
approach and aim to maintain the speed by which
assessments are collected in shared task evalua-
tions, but modify the evaluation set-up in two main
ways: (1) we structure the judgments as monolin-
gual tasks, reducing the cognitive load involved
in assessing translation quality; and (2) we ap-
ply judge-intrinsic quality control and score stan-
dardization, to minimize noise introduced when
crowd-sourcing is used to leverage numbers of as-
sessments and to allow for the fact that human
judges will vary in the way they assess transla-
tions. Assessors are regarded as reliable as long
as they demonstrate consistent judgments across a
range of different quality translations.
We elicit direct estimates of quality from
judges, as a quantitative estimate of the magni-
tude of each attribute of interest (Steiner and Nor-
man, 1989). Since we no longer look for rela-
tive preference judgments, we revert back to the
original fluency and adequacy criteria last used in
WMT 2007 shared task evaluation. Instead of five-
point fluency/adequacy scales, however, we use
a (100-point) continuous rating scale, as this fa-
cilitates more sophisticated statistical analyses of
score distributions for judges, including worker-
intrinsic quality control for crowd-sourcing. The
latter does not depend on agreement with ex-
perts, and is made possible by the reduction in
1
WMT 2013 reports 361 hours of labor to collect 61,695
labels, with approximately one screen of five pairwise com-
parisons each yielding a set of 10 labels.
information-loss when a continuous scale is used.
In addition, translations are assessed in isolation
from alternate system outputs, so that judgments
collected are no longer relative to a set of five
translations. This has the added advantage of elim-
inating the criticism made of WMT evaluations
that systems sometimes gain advantage from luck-
of-the-draw comparison with low quality output,
and vice-versa (Bojar et al., 2011).
Based on our proposed evaluation methodology,
human judges are able to work quickly, on average
spending 18 and 13 seconds per single segment ad-
equacy and fluency judgment, respectively. Addi-
tionally, when sufficiently large volumes of such
judgments are collected, mean scores reveal sig-
nificant differences between systems. Further-
more, since human evaluation takes the form of di-
rect estimates instead of relative preference judg-
ments, our evaluation introduces the possibility
of large-scale longitudinal human evaluation. We
demonstrate the value of longitudinal evaluation
by investigating the improvement made to state-
of-the-art MT over a five year time period (be-
tween 2007 and 2012) using the best participating
WMT shared task system output. Since it is likely
that the test data used for shared tasks has varied
in difficulty over this time period, we additionally
propose a simple mechanism for scaling system
scores relative to task difficulty.
Using the proposed methodology for measur-
ing longitudinal change in MT, we conclude that,
for the seven European language pairs we evalu-
ate, MT has made an average 10% improvement
over the past 5 years. Our method uses non-expert
monolingual judges via a crowd-sourcing portal,
with fast turnaround and at relatively modest cost.
2 Monolingual Human Evaluation
There are several reasons why the assessment of
MT quality is difficult. Ideally, each judge should
be a native speaker of the target language, while
at the same time being highly competent in the
source language. Genuinely bilingual people are
rare, however. As a result, judges are often peo-
ple with demonstrated skills in the target language,
and a working knowledge ? often self-assessed ?
of the source language. Adding to the complexity
is the discipline that is required: the task is cog-
nitively difficult and time-consuming when done
properly. The judge is, in essence, being asked to
decide if the supplied translations are what they
444
would have generated if they were asked to do the
same translation.
The assessment task itself is typically structured
as follows: the source segment (a sentence or
a phrase), plus five alternative translations and a
?reference? translation are displayed. The judge
is then asked to assign a rank order to the five
translations, from best to worst. A set of pairwise
preferences are then inferred, and used to generate
system rankings, without any explicit formation of
stand-alone system ?scores?.
This structure introduces the risk that judges
will only compare translations against the refer-
ence translation. Certainly, judges will vary in
the degree they rely on the reference translation,
which will in turn impact on inter-judge inconsis-
tency. For instance, even when expert judges do
assessments, it is possible that they use the ref-
erence translation as a substitute for reading the
source input, or do not read the source input at
all. And if crowd-sourcing is used, can we really
expect high proportions of workers to put the ad-
ditional effort into reading and understanding the
source input when a reference translation (proba-
bly in their native language) is displayed? In re-
sponse to this potential variability in how annota-
tors go about the assessment task, we trial assess-
ments of adequacy in which the source input is not
displayed to human judges. We structure assess-
ments as a monolingual task and pose them in such
a way that the focus is on comparing the meaning
of reference translations and system outputs.
2
We therefore ask human judges to assess the de-
gree to which the system output conveys the same
meaning as the reference translation. In this way,
we focus the human judge indirectly on the ques-
tion we wish to answer when assessing MT: does
the translation convey the meaning of the source?
The fundamental assumption of this approach is
that the reference translation accurately captures
the meaning of the source; once that assumption
is made, it is clear that the source is not required
during the evaluation.
Benefits of this change are that the task is both
easier to describe to novice judges, and easier
to answer, and that it requires only monolingual
speakers, opening up the evaluation to a vastly
larger pool of genuinely qualified workers.
With this set-up in place for adequacy, we also
2
This dimension of the assessment is similar but not iden-
tical to the monolingual adequacy assessment in early NIST
evaluation campaigns (NIST, 2002).
re-introduce a fluency assessment. Fluency rat-
ings can be carried out without the presence of a
reference translation, reducing any remnant bias
towards reference translations in the evaluation
setup. That is, we propose a judgment regime in
which each task is presented as a two-item fluency
and adequacy judgment, evaluated separately, and
with adequacy restructured into a monolingual
?similarity of meaning? task.
When fluency and adequacy were originally
used for human evaluation, each rating used a 5-
point adjective scale (Callison-Burch et al., 2007).
However, adjectival scale labels are problematic
and ratings have been shown to be highly depen-
dent on the exact wording of descriptors (Seymour
et al., 1985). Alexandrov (2010) provides a sum-
mary of the extensive problems associated with the
use of adjectival scale labels, including bias result-
ing from positively- and negatively-worded items
not being true opposites of one another, and items
intended to have neutral intensity in fact proving
to have specific conceptual meanings.
It is often the case, however, that the question
could be restructured so that the rating scale no
longer requires adjectival labels, by posing the
question as a statement such as The text is fluent
English and asking the human assessor to specify
how strongly they agree or disagree with that state-
ment. The scale and labels can then be held con-
stant across experimental set-ups for all attributes
evaluated ? meaning that if the scale is still biased
in some way it will be equally so across all set-ups.
3 Assessor Consistency
One way of estimating the quality of a human
evaluation regime is to measure its consistency:
whether or not the same outcome is achieved if
the same question is asked a second time. In
MT, annotator consistency is commonly measured
using Cohen?s kappa coefficient, or some variant
thereof (Artstein and Poesio, 2008). Originally de-
veloped as a means of establishing assessor inde-
pendence, it is now commonly used in the reverse
sense, with high numeric values being used as ev-
idence of agreement. Two different measurements
can be made ? whether a judge is consistent with
other judgments performed by themselves (intra-
annotator agreement), and whether a judge is con-
sistent with other judges (inter-annotator agree-
ment).
Cohen?s kappa is intended for use with categor-
445
ical judgments, but is also commonly used with
five-point adjectival-scale judgments, where the
set of categories has an explicit ordering. One
particular issue with five-point assessments is that
score standardization cannot be applied. As such,
a judge who assigns two neighboring intervals is
awarded the same ?penalty? for being ?different?
as the judge who chooses the extremities. The
kappa coefficient cannot be directly applied to
many-valued interval or continuous data.
This raises the question of how we should eval-
uate assessor consistency when a continuous rat-
ing scale is in place. No judge, when given the
same translation to judge twice on a continuous
rating scale, can be expected to give precisely the
same score for each judgment (where repeat as-
sessments are separated by a considerable number
of intervening ones). A more flexible tool is thus
required. We build such a tool by starting with two
core assumptions:
A: When a consistent assessor is presented with
a set of repeat judgments, the mean of the
initial set of assessments will not be signifi-
cantly different from the mean score of repeat
assessments.
B: When a consistent judge is presented with a
set of judgments for translations from two
systems, one of which is known to produce
better translations than the other, the mean
score for the better system will be signifi-
cantly higher than that of the inferior system.
Assumption B is the basis of our quality-control
mechanism, and allows us to distinguish between
Turkers who are working carefully and those who
are merely going through the motions. We use a
100-judgment HIT structure to control same-judge
repeat items and deliberately-degraded system
outputs (bad reference items) used for worker-
intrinsic quality control (Graham et al., 2013).
bad reference translations for fluency judgments
are created as follows: two words in the translation
are randomly selected and randomly re-inserted
elsewhere in the sentence (but not as the initial or
final words of the sentence).
Since adding duplicate words will not degrade
adequacy in the same way, we use an alternate
method to create bad reference items for adequacy
judgments: we randomly delete a short sub-string
of length proportional to the length of the origi-
nal translation to emulate a missing phrase. Since
total fltrd Assum A total fltrd
wrkrs wrkrs holds segs segs
F 557 321 (58%) 314 (98.8%) 122k 78k (64%)
A 542 283 (52%) 282 (99.6%) 102k 62k (61%)
Table 1: Total quality control filtered workers and
assessments (F = fluency; A = adequacy).
this is effectively a new degradation scheme, we
tested against experts. For low-quality transla-
tions, deleting just two words from a long sentence
often made little difference. The method we even-
tually settled on removes a sequence of k words,
as a function of sentence length n:
2 ? n ? 3 ? k = 1
4 ? n ? 5 ? k = 2
6 ? n ? 8 ? k = 3
9 ? n ? 15 ? k = 4
16 ? n ? 20 ? k = 5
n > 20 ? k =
?
n
5
?
To filter out careless workers, scores for
bad reference pairs are extracted, and a
difference-of-means test is used to calculate
a worker-reliability estimate in the form of a
p-value. Paired tests are then employed using the
raw scores for degraded and corresponding system
outputs, using a reliability significance threshold
of p < 0.05. If a worker does not demonstrate
the ability to reliably distinguish between a bad
system and a better one, the judgments from
that worker are discarded. This methodology
means that careless workers who habitually rate
translations either high or low will be detected,
as well as (with high probability) those that click
(perhaps via robots) randomly. It also has the
advantage of not filtering out workers who are
internally consistent but whose scores happen not
to correspond particularly well to a set of expert
assessments.
Having filtered out users who are unable to reli-
ably distinguish between better and worse sets of
translations (p ? 0.05), we can now examine how
well Assumption A holds for the remaining users,
i.e. the extent to which workers apply consistent
scores to repeated translations. We compute mean
scores for the initial and repeat items and look for
even very small differences in the two distribu-
tions for each worker. Table 1 shows numbers of
workers who passed quality control, and also that
446
Si
S
i+5
1 bad reference its corresponding system output
1 system output a repeat of it
1 reference its corresponding system output
Above in reverse for S
i
and S
i+5
4 system outputs 4 system outputs
Table 2: Control of repeat item pairs. S
i
denotes
the i
th
set of 10 translations assessed within a 100
translation HIT.
the vast majority (around 99%) of reliable work-
ers have no significant difference between mean
scores for repeat items.
4 Five Years of Machine Translation
To estimate the improvement in MT that took
place between 2007 and 2012, we asked work-
ers on Amazon?s Mechanical Turk (MTurk) to rate
the quality of translations produced by the best-
reported participating system for each of WMT
2007 and WMT 2012 (Callison-Burch et al., 2007;
Callison-Burch et al., 2012). Since it is likely that
the test set has changed in difficulty over this time
period, we also include in the evaluation the orig-
inal test data for 2007 and 2012, translated by a
single current MT system. We use the latter to cal-
ibrate the results for test set difficulty, by calcu-
lating the average difference in rating, ?, between
the 2007 and 2012 test sets. This is then added
to the difference in rating for the best-reported
systems in 2012 and 2007, to arrive at an over-
all evaluation of the 5-year gain in MT quality for
a given language pair, separately for fluency and
adequacy.
Experiments were carried out for each of Ger-
man, French and Spanish into and out of English,
and also for Czech-to-English. English-to-Czech
was omitted because of a low response rate on
MTurk. For language pairs where two systems tied
for first place in the shared task, a random selec-
tion of translations from both systems was made.
HIT structure
To facilitate quality control, we construct each
HIT on MTurk as an assessment of 100 trans-
lations. Each individual translation is rated in
isolation from other translations with workers re-
quired to iterate through 100 translations without
the opportunity to revisit earlier assessments. A
100-translation HIT contains the following items:
70 randomly selected system outputs made up of
roughly equal proportions of translations for each
evaluated system, 10 bad reference translations
(each based on one of the 70 system outputs), 10
exact repeats and 10 reference translations. We di-
vide a 100-translation HIT into 10 sets of 10 trans-
lations. Table 2 shows how the content of each set
is determined. Translations are then randomized
only within each set (of 10 translations), with the
original sequence order of the sets preserved. In
this way, the order of quality control items is un-
predictable but controlled so pairs are separated by
a minimum of 40 intervening assessments (4 sets
of translations). The HIT structure results in 80%
of assessed translations corresponding to genuine
outputs of a system (including exact repeat assess-
ments), which is ultimately what we wish to ob-
tain, with 20% of assessments belonging to quality
control items (bad reference or reference transla-
tions).
Assessment set-up
Separate HITs were provided for evaluation of flu-
ency and adequacy. For fluency, a single system
output was displayed per screen, with a worker re-
quired to rate the fluency of a translation on a 100-
point visual analog scale with no displayed point
scores. A similar set-up was used for adequacy but
with the addition of a reference translation (dis-
played in gray font to distinguish it from the sys-
tem output being assessed). The Likert-type state-
ment that framed the judgment was Read the text
below and rate it by how much you agree that:
? [for fluency] the text is fluent English
? [for adequacy] the black text adequately ex-
presses the meaning of the gray text.
In neither case was the source language string pro-
vided to the workers.
Tasks were published on MTurk, with no re-
gion restriction but the stipulation that only na-
tive speakers of the target language should com-
plete HITs, and with a qualification of an MTurk
prior HIT-approval rate of at least 95%. Instruc-
tions were always presented in the target language.
Workers were paid US$0.50 per fluency HIT, and
US$0.60 per adequacy HIT.
3
3
Since insufficient assessments were collected for French
and German evaluations in the initial run, a second and ulti-
mately third set of HITs were needed for these languages with
increased payment per HIT of US$1.0 per 100-judgment ade-
quacy HIT, US$0.65 per 100-judgment fluency HIT and later
again to US$1.00 per 100-judgment fluency HIT.
447
Close to one thousand individual Turkers con-
tributed to this experiment (some did both flu-
ency and adequacy assessments), providing a to-
tal of more than 220,000 translations, of which
140,000 were provided by workers meeting the
quality threshold.
In general, it cost approximately US$30 to as-
sess each system, with low-quality workers ap-
proximately doubling the cost of the annotation.
We rejected HITs where it was clear that random-
clicking had taken place, but did not reject solely
on the basis of having not met the quality control
threshold, to avoid penalizing well-intentioned but
low-quality workers.
Overall change in performance
Table 3 shows the overall gain made in five years,
from WMT 07 to WMT 12. Mean scores for the
two top-performing systems from each shared task
(BEST
07
, BEST
12
) are included, as well as scores
for the benchmark current MT system on the two
test sets (CURR
07
, CURR
12
). For each language
pair, a 100-translation HIT was constructed by
randomly selecting translations from the pool of
(3003+2007)?2 that were available, and this re-
sults in apparently fewer assessments for the 2007
test set. In fact, numbers of evaluated translations
are relative to the size of each test set. Average z
scores for each system are also presented, based on
the mean and standard deviation of all assessments
provided by an individual worker, with positive
values representing deviations above the mean of
workers. In addition, we include mean BLEU (Pa-
pineni et al., 2001) and METEOR (Banerjee and
Lavie, 2005) automatic scores for the same system
outputs.
The CURR benchmark shows fluency scores
that are 5.9 points higher on the 2007 data set than
they are on the 2012 test data, with a larger dif-
ference in adequacy of 8.3 points. As such, the
2012 test data is more challenging than the 2007
test data. Despite this, both fluency and adequacy
scores for the best system in 2012 have increased
by 4.5 and 2.0 points respectively, amounting to
estimated average gains of 10.4 points in fluency
and 10.3 points in adequacy for state-of-the-art
MT across the seven language pairs.
Looking at the standardized scores, it is appar-
ent that the presence of the CURR translations for
the 2007 test set pushes the mean score for the
2007 best systems below zero. The presence in
the HITs of reference translations also shifts stan-
dardized system evaluations below zero, because
they are not attributable to any of the systems be-
ing assessed.
4
Results for automatic metrics lead to similar
conclusions: that the test set has indeed increased
in difficulty; and that, in spite of this, substantial
improvements have been made according to auto-
matic metrics, +13.5 using BLEU, and +7.1 on
average using METEOR.
Language pairs
Table 4 shows mean fluency and adequacy scores
by language pair for translation into English. Rel-
ative gains in both adequacy and fluency for the to-
English language pairs are in agreement with the
estimates generated through the use of the two au-
tomatic metrics. Most notably, Czech-to-English
translation appears to have made substantial gains
across the board, achieving more than double the
gain made by some of the other language pairs; re-
sults for best participating 2007 systems show that
this may in part be caused by the fact that Czech-
to-English translation had a lower 2007 baseline
to begin with (BEST
07
F:40.8; A:41.7) in compar-
ison to, for example, Spanish-to-English transla-
tion (BEST
07
F:56.7; A:59.0).
Another notable result is that although the test
data for each year?s shared task is parallel across
five languages, test set difficulty increases by dif-
ferent degrees according to human judges and au-
tomatic metrics, with BLEU scores showing sub-
stantial divergence across the to-English language
pairs. Comparing BLEU scores achieved by the
benchmark system for Spanish to English and
Czech-to-English, for example, the benchmark
system achieves close scores on the 2007 test data
with a difference of only |52.3 ? 51.2| = 1.1,
compared to the score difference for the bench-
mark scores for translation of the 2012 test data of
|25.0 ? 38.3| = 13.3. This may indicate that the
increase in test set difficulty that has taken place
over the years has made the shared task dispro-
portionately more difficult for some language pairs
than for others. It does seem that some language
pairs are harder to translate than others, and the
differential change may be a consequence of the
fact that increasing test set complexity for all lan-
guages in parallel has a greater impact on transla-
tion difficulty for language pairs that are intrinsi-
cally harder to translate between.
4
Scores for reference translations can optionally be omit-
ted for score standardization.
448
CURR
07
CURR
12
?
BEST
07
BEST
12
5-Year Gain
(CURR
07
? CURR
12
) (BEST
12
? BEST
07
+ ?)
fl
u
e
n
c
y
score 64.1 58.2 5.9 53.5 58.0 (+4.5) 10.4
z 0.18 0.00 0.18 ?0.16 0.00 (+0.16) 0.34
n 12,334 18,654 12,513 18,579
a
d
e
q
u
a
c
y
score 65.0 56.7 8.3 54.0 56.0 (+2.0) 10.3
z 0.18 ?0.07 0.25 ?0.16 ?0.09 (+0.07) 0.32
n 10,022 14,870 10,049 14,979
m
e
t
r
i
c
s
BLEU 41.5 30.0 11.4 25.6 27.7 (+2.1) 13.5
METEOR 49.2 41.1 8.1 41.1 40.1 (?1.0) 7.1
Table 3: Average human evaluation results for all language pairs; mean and standardized z scores are
computed in each case for n translations. In this table, and in Tables 4 and 5, all reported fluency and
adequacy values are in points relative to the 100-point assessment scale.
CURR
07
CURR
12
?
BEST
07
BEST
12
5-Year Gain
(CURR
07
? CURR
12
) (BEST
12
? BEST
07
+ ?)
D
E
-
E
N
fluency
score 65.3
???
57.9 7.4 52.8 55.0
?
(+2.2) 9.6
n 2,164 3,381 2,242 3,253
adequacy
score 63.8
???
52.8 11.0 46.5 49.8
??
(+3.3) 14.3
n 1,458 2,175 1,454 2,193
metrics
BLEU 38.3 26.5 11.8 21.1 23.8 (+2.7) 14.5
METEOR 40.3 32.7 7.6 33.4 31.7 (?1.7) 5.9
F
R
-
E
N
fluency
score 65.9
???
58.0 7.9 57.8 60.2
??
(+2.4) 10.3
n 2,172 3,267 2,203 3,238
adequacy
score 61.0
???
52.3 8.7 52.7 51.5 (?1.2) 7.5
n 1,754 2,651 1,763 2,712
metrics
BLEU 39.4 32.0 7.4 28.6 31.5 (+2.9) 10.3
METEOR 39.8 34.6 5.2 35.9 34.3 (?1.6) 3.6
E
S
-
E
N
fluency
score 68.4
???
59.2 9.2 56.7 56.7 (+0.0) 9.2
n 1,514 2,234 1,462 2,230
adequacy
score 68.0
???
56.9 11.1 59.0
???
55.7 (?3.3) 7.8
n 1,495 2,193 1,492 2,180
metrics
BLEU 51.2 38.3 12.9 35.1 33.5 (?1.6) 11.3
METEOR 45.4 37.0 8.4 39.9 36.0 (?3.9) 4.5
C
S
-
E
N
fluency
score 62.3
???
49.9 12.4 40.8 50.5
???
(+9.7) 22.1
n 1,873 2,816 1,923 2,828
adequacy
score 62.4
???
47.5 14.9 41.7 47.4
???
(+5.7) 20.6
n 1,218 1,830 1,257 1,855
metrics
BLEU 52.3 25.0 27.3 25.1 22.4 (?2.7) 24.6
METEOR 44.7 31.6 13.1 34.3 30.8 (?3.5) 9.6
Table 4: Human evaluation of WMT 2007 and 2012 best systems for to-English language pairs. Mean
scores are computed in each case for n translations. In this table and in Table 5,
?
denotes significance at
p < 0.05;
??
significance at p < 0.01; and
???
significance at p < 0.001.
Table 5 shows results for translation out-of En-
glish, and once again human evaluation scores are
in agreement with automatic metrics with English-
to-Spanish translation achieving most substantial
gains for the three out-of-English language pairs,
an increase of 12.4 points for fluency, and 11.8
points with respect to adequacy, while English-
to-French translation achieves a gain of 8.8 for
449
CURR
07
CURR
12
?
BEST
07
BEST
12
5-Year Gain
(CURR
07
? CURR
12
) (BEST
12
? BEST
07
+ ?)
E
N
-
E
S
fluency
score 77.2
???
73.4 3.8 63.3 71.9
???
(+8.6) 12.4
n 2,286 3,318 2,336 3,420
adequacy
score 75.2
???
68.1 7.1 62.5 67.2 (+4.7) 11.8
n 1,410 2,039 1,399 2,112
metrics
BLEU 48.2 38.7 9.5 29.1 35.3 (+6.2) 15.7
METEOR 69.9 59.6 10.3 57.0 58.1 (+1.1) 11.4
E
N
-
F
R
fluency
score 57.1 55.2 1.9 49.5 56.4 (+6.9) 8.8
n 1,008 1,645 1,039 1,588
adequacy
score 64.2
?
61.9 2.3 57.2 62.3 (+5.1) 7.4
n 1,234 1,877 1,274 1,775
metrics
BLEU 37.2 30.8 6.4 25.3 29.9 (+4.6) 11.0
METEOR 59.4 52.9 6.5 50.4 52.0 (+1.6) 8.1
E
N
-
D
E
fluency
score 52.3 54.1
?
?1.8 53.7 55.5 (+1.8) 0.0
n 1,317 1,993 1,308 2,022
adequacy
score 60.3
??
57.4 2.9 58.3 58.3 (+0.0) 2.9
n 1,453 2,105 1,410 2,152
metrics
BLEU 23.6 18.7 4.9 14.6 17.2 (+2.6) 7.5
METEOR 44.7 39.1 5.6 36.7 38.0 (+1.3) 6.9
Table 5: Human evaluation of WMT 2007 and 2012 best systems for out of English language pairs.
Mean scores are computed in each case for n translations.
fluency and 7.4 points for adequacy. English-to-
German translation achieves the lowest gain of
all languages, with apparently no improvement
in fluency, as the human fluency evaluation of
the benchmark system on the supposedly easier
2007 data receives a substantially lower score than
the same system over the 2012 data. This result
demonstrates why fluency, evaluated without a ref-
erence translation, should not be used to evalu-
ate MT systems without an adequacy assessment,
since it is entirely possible for a low-adequacy
translation to achieve a high fluency score.
For all language pairs, Figure 1 plots the net
gain in fluency, adequacy and F
1
against increase
in test data difficulty.
5 Conclusion
We carried out a large-scale human evaluation
of best-performing WMT 2007 and 2012 shared
task systems in order to estimate the improvement
made to state-of-the-art machine translation over
this five year time period. Results show significant
improvements have been made in machine trans-
lation of European language pairs, with Czech-
to-English recording the greatest gains. It is also
clear from our data that the difficulty of the task
has risen over the same period, to varying degrees
0 5 10 15
0
5
10
15
Best12 ? Best 07
? ( C
urr  07
 
?
 
Curr
12 ) de?enfr?en
es?en
cs?en
en?de
en?fr
en?es
F1
Fluency
Adequacy
Figure 1: Mean fluency, adequacy and combined
F
1
scores for language pairs.
for individual language pairs.
Researchers interested in making use of the
dataset are invited to contact the first author.
Acknowledgments This work was supported by
the Australian Research Council.
450
References
A. Alexandrov. 2010. Characteristics of single-item
measures in Likert scale format. The Electronic
Journal of Business Research Methods, 8:1?12.
R. Artstein and M. Poesio. 2008. Inter-coder agree-
ment for computational linguistics. Computational
Linguistics, 34(4):555?596.
S. Banerjee and A. Lavie. 2005. METEOR: An au-
tomatic metric for mt evaluation with improved cor-
relation with human judgements. In Proc. Wkshp.
Intrinsic and Extrinsic Evaluation Measures for Ma-
chine Translation and/or Summarization, pages 65?
73, Ann Arbor, MI.
A. Birch, B. Haddow, U. Germann, M. Nadejde,
C. Buck, and P. Koehn. 2013. The feasibility of
HMEANT as a human MT evaluation metric. In
Proc. 8th Wkshp. Statistical Machine Translation,
pages 52?61, Sofia, Bulgaria. ACL.
O. Bojar, M. Ercegov?cevic, M. Popel, and O. Zaidan.
2011. A grain of salt for the WMT manual evalua-
tion. In Proc. 6th Wkshp. Statistical Machine Trans-
lation, pages 1?11, Edinburgh, Scotland. ACL.
O. Bojar, C. Buck, C. Callison-Burch, C. Federmann,
B. Haddow, P. Koehn, C. Monz, M. Post, R. Soricut,
and L. Specia. 2013. Findings of the 2013 Work-
shop on Statistical Machine Translation. In Proc.
8th Wkshp. Statistical Machine Translation, pages
1?44, Sofia, Bulgaria. ACL.
C. Callison-Burch, C. Fordyce, P. Koehn, C. Monz, and
J. Schroeder. 2007. (Meta-) evaluation of machine
translation. In Proc. 2nd Wkshp. Statistical Machine
Translation, pages 136?158, Prague, Czech Repub-
lic. ACL.
C. Callison-Burch, P. Koehn, C. Monz, M. Post,
R. Soricut, and L. Specia. 2012. Findings of the
2012 Workshop on Statistical Machine Translation.
In Proc. 7th Wkshp. Statistical Machine Translation,
pages 10?51, Montreal, Canada. ACL.
M. Dreyer and D. Marcu. 2012. HyTER: Meaning-
equivalent semantics for translation evaluation. In
Proc. 2012 Conf. North American Chapter of the
ACL: Human Language Technologies, pages 162?
171, Montreal, Canada. ACL.
Y. Graham, T. Baldwin, A. Moffat, and J. Zobel. 2013.
Continuous measurement scales in human evalua-
tion of machine translation. In Proc. 7th Linguis-
tic Annotation Wkshp. & Interoperability with Dis-
course, pages 33?41, Sofia, Bulgaria. ACL.
C. Lo and D. Wu. 2011. MEANT: An inexpensive,
high-accuracy, semi-automatic metric for evaluating
translation utility based on semantic roles. In Proc.
49th Annual Meeting of the ACL: Human Language
Techologies, pages 220?229, Portland, OR. ACL.
NIST. 2002. The 2002 NIST machine translation
evaluation plan. National Institute of Standards and
Technology. http://www.itl.nist.gov/
iad/894.01/tests/mt/2003/doc/mt03_
evalplan.v2.pdf.
K. Papineni, S. Roukos, T. Ward, and W.-J. Zhu.
2001. BLEU: A method for automatic evaluation
of machine translation. Technical Report RC22176
(W0109-022), IBM Research, Thomas J. Watson
Research Center.
R. A. Seymour, J. M. Simpson, J. E. Charlton, and
M. E. Phillips. 1985. An evaluation of length and
end-phrase of visual analogue scales in dental pain.
Pain, 21:177?185.
M. Snover, B. Dorr, R. Scwartz, J. Makhoul, and
L. Micciula. 2006. A study of translation error rate
with targeted human annotation. In Proc. 7th Bien-
nial Conf. of the Assoc. Machine Translaiton in the
Americas, pages 223?231, Boston, MA.
D. L. Steiner and G. R. Norman. 1989. Health Mea-
surement Scales, A Practical Guide to their Devel-
opment and Use. Oxford University Press, Oxford,
UK, fourth edition.
451
Proceedings of the 7th Linguistic Annotation Workshop & Interoperability with Discourse, pages 33?41,
Sofia, Bulgaria, August 8-9, 2013. c?2013 Association for Computational Linguistics
Continuous Measurement Scales in
Human Evaluation of Machine Translation
Yvette Graham Timothy Baldwin Alistair Moffat Justin Zobel
Department of Computing and Information Systems, The University of Melbourne
{ygraham,tbaldwin,ammoffat,jzobel}@unimelb.edu.au
Abstract
We explore the use of continuous rat-
ing scales for human evaluation in the
context of machine translation evaluation,
comparing two assessor-intrinsic quality-
control techniques that do not rely on
agreement with expert judgments. Ex-
periments employing Amazon?s Mechan-
ical Turk service show that quality-control
techniques made possible by the use of
the continuous scale show dramatic im-
provements to intra-annotator agreement
of up to +0.101 in the kappa coefficient,
with inter-annotator agreement increasing
by up to +0.144 when additional standard-
ization of scores is applied.
1 Introduction
Human annotations of language are often required
in natural language processing (NLP) tasks for
evaluation purposes, in order to estimate how well
a given system mimics activities traditionally per-
formed by humans. In tasks such as machine
translation (MT) and natural language generation,
the system output is a fully-formed string in a tar-
get language. Annotations can take the form of
direct estimates of the quality of those outputs or
be structured as the simpler task of ranking com-
peting outputs from best-to-worst (Callison-Burch
et al, 2012).
A direct estimation method of assessment, as
opposed to ranking outputs from best-to-worst,
has the advantage that it includes in annotations
not only that one output is better than another,
but also the degree to which that output was bet-
ter than the other. In addition, direct estimation
of quality within the context of machine transla-
tion extends the usefulness of the annotated data
to other tasks such as quality-estimation (Callison-
Burch et al, 2012).
For an evaluation to be credible, the annotations
must be credible. The simplest way of establish-
ing this is to have the same data point annotated by
multiple annotators, and measure the agreement
between them. There has been a worrying trend
in recent MT shared tasks ? whether the evalu-
ation was structured as ranking translations from
best-to-worst, or by direct estimation of fluency
and adequacy ? of agreement between annotators
decreasing (Callison-Burch et al, 2008; Callison-
Burch et al, 2009; Callison-Burch et al, 2010;
Callison-Burch et al, 2011; Callison-Burch et al,
2012). Inconsistency in human evaluation of ma-
chine translation calls into question conclusions
drawn from those assessments, and is the target
of this paper: by revising the annotation process,
can we improve annotator agreement, and hence
the quality of human annotations?
Direct estimates of quality are intrinsically con-
tinuous in nature, but are often collected using an
interval-level scale with a relatively low number
of categories, perhaps to make the task cognitively
easier for human assessors. In MT evaluation,
five and seven-point interval-level scales are com-
mon (Callison-Burch et al, 2007; Denkowski and
Lavie, 2010). However, the interval-level scale
commonly used for direct estimation of translation
quality (and other NLP annotation tasks) forces
human judges to discretize their assessments into
a fixed number of categories, and this process
could be a cause of inconsistency in human judg-
ments. In particular, an assessor may be repeatedly
forced to choose between two categories, neither
of which really fits their judgment. The contin-
uous nature of translation quality assessment, as
well as the fact that many statistical methods ex-
ist that can be applied to continuous data but not
interval-level data, motivates our trial of a contin-
uous rating scale.
We use human judgments of translation fluency
as a test case and compare consistency levels when
33
the conventional 5-point interval-level scale and a
continuous visual analog scale (VAS) are used for
human evaluation. We collected data via Ama-
zon?s Mechanical Turk, where the quality of an-
notations is known to vary considerably (Callison-
Burch et al, 2010). As such, we test two quality-
control techniques based on statistical significance
? made possible by the use of the continuous rating
scale ? to intrinsically assess the quality of individ-
ual human judges. The quality-control techniques
are not restricted to fluency judgments and are rel-
evant to more general MT evaluation, as well as
other NLP annotation tasks.
2 Machine Translation Fluency
Measurement of fluency as a component of MT
evaluation has been carried out for a number of
years (LDC, 2005), but it has proven difficult
to acquire consistent judgments, even from ex-
pert assessors. Evaluation rounds such as the an-
nual Workshop on Statistical Machine Translation
(WMT) use human judgments of translation qual-
ity to produce official rankings in shared tasks, ini-
tially using an two-item assessment of fluency and
adequacy as separate attributes, and more recently
by asking judges to simply rank system outputs
against one another according to ?which transla-
tion is better?. However, the latter method also re-
ports low levels of agreement between judges. For
example, the 2007 WMT reported low levels of
consistency in fluency judgments in terms of both
intra-annotator agreement (intra-aa), with a kappa
coefficient of ? = 0.54 (moderate), and inter-
annotator agreement (inter-aa), with ? = 0.25
(slight). Adequacy judgments for the same data
received even lower scores: ? = 0.47 for intra-aa,
and ? = 0.23 for inter-aa.
While concerns over annotator agreement have
seen recent WMT evaluations move away from us-
ing fluency as an evaluation component, there can
be no question that fluency is a useful means of
evaluating translation output. In particular, it is not
biased by reference translations. The use of auto-
matic metrics is often criticized by the fact that
a system that produces a good translation which
happens not to be similar to the reference trans-
lations will be unfairly penalized. Similarly, if
human annotators are provided with one or more
reference sentences, they may inadvertently favor
translations that are similar to those references. If
fluency is judged independently of adequacy, no
reference translation is needed, and the bias is re-
moved.
In earlier work, we consider the possibility
that translation quality is a hypothetical construct
(Graham et al, 2012), and suggest applying meth-
ods of validating measurement of psychological
constructs to the validation of measurements of
translation quality. In psychology, a scale that em-
ploys more items as opposed to fewer is consid-
ered more valid. Under this criteria, a two-item
(fluency and adequacy) scale is more valid than a
single-item translation quality measure.
3 Measurement Scales
Direct estimation methods are designed to elicit
from the subject a direct quantitative estimate of
the magnitude of an attribute (Streiner and Nor-
man, 1989). We compare judgments collected
on a visual analog scale (VAS) to those using an
interval-level scale presented to the human judge
as a sequence of radio-buttons. The VAS was first
used in psychology in the 1920?s, and prior to the
digital age, scales used a line of fixed length (usu-
ally 100mm in length), with anchor labels at both
ends, and to be marked by hand with an ?X? at the
desired location (Streiner and Norman, 1989).
When an interval-scale is used in NLP evalua-
tion or other annotation tasks, it is commonly pre-
sented in the form of an adjectival scale, where
categories are labeled in increasing/decreasing
quality. For example, an MT evaluation of fluency
might specify 5 = ?Flawless English?, 4 = ?Good
English?, 3 = ?Non-native English?, 2 = ?Disfluent
English?, and 1 = ?Incomprehensible? (Callison-
Burch et al, 2007; Denkowski and Lavie, 2010).
With both a VAS and an adjectival scale, the
choice of labels can be critical. In medical re-
search, patients? ratings of their own health have
been shown to be highly dependent on the ex-
act wording of descriptors (Seymour et al, 1985).
Alexandrov (2010) provides a summary of the ex-
tensive literature on the numerous issues associ-
ated with adjectival scale labels, including bias
resulting from positively and negatively worded
items not being true opposites of one another, and
items intended to have neutral intensity in fact
proving to have unique conceptual meanings.
Likert scales avoid the problems associated with
adjectival labels, by structuring the question as
a simple statement that the respondent registers
their level of (dis)agreement with. Figure 1 shows
34
Figure 1: Amazon Mechanical Turk interface for fluency judgments with a Likert-type scale.
Figure 2: Continuous rating scale for fluency judgments with two anchors.
the Likert-type interval-level scale we use to col-
lect fluency judgments of MT output, and Fig-
ure 2 shows an equivalent VAS using the two
most extreme anchor labels, strongly disagree and
strongly agree.
4 Crowd-sourcing Judgments
The volume of judgments required for evaluation
of NLP tasks can be large, and employing experts
to undertake those judgments may not always be
feasible. Crowd-sourcing services via the Web of-
fer an attractive alternative, and have been used in
conjunction with a range of NLP evaluation and
annotation tasks. Several guides exist for instruct-
ing researchers from various backgrounds on us-
ing Amazon?s Mechanical Turk (AMT) (Gibson et
al., 2011; Callison-Burch, 2009), and allowance
for the use of AMT is increasingly being made
in research grant applications, as a cost-effective
way of gathering data. Issues remain in connec-
tion with low payment levels (Fort et al, 2011);
nevertheless, Ethics Approval Boards are typically
disinterested in projects that make use of AMT, re-
garding AMT as being a purchased service rather
than a part of the experimentation that may affect
human subjects.
The use of crowd-sourced judgments does,
however, introduce the possibility of increased in-
consistency, with service requesters typically hav-
ing no specific or verifiable knowledge about any
given worker. Hence, the possibility that a worker
is acting in good faith but not performing the task
well must be allowed for, as must the likelihood
that some workers will quite ruthlessly seek to
minimize the time spent on the task, by deliber-
ately giving low-quality or fake answers. Some
workers may even attempt to implement auto-
mated responses, so that they get paid without hav-
ing to do the work they are being paid for.
For example, if the task at hand is that of assess-
ing the fluency of text snippets, it is desirable to
employ native speakers. With AMT the requester
has the ability to restrict responses to only workers
who have a specified skill. But that facility does
not necessarily lead to confidence ? there is noth-
ing stopping a worker employing someone else
to do the test for them. Devising a test that reli-
ably evaluates whether or not someone is a native
speaker is also not at all straightforward.
Amazon allow location restrictions, based on
the registered residential address of the Turker,
which can be used to select in favor of those likely
to have at least some level of fluency (Callison-
Burch et al, 2010). We initially applied this re-
striction to both sets of judgments in experiments,
setting the task up so that only workers regis-
tered in Germany could evaluate the to-German
translations, for example. However, very low re-
35
sponse rates for languages other than to-English
were problematic, and we also received a number
of apparently-genuine requests from native speak-
ers residing outside the target countries. As a
result, we removed all location restrictions other
than for the to-English tasks.1
Crowd-sourcing judgments has the obvious risk
of being vulnerable to manipulation. On the other
hand, crowd-sourced judgments also offer the po-
tential of being more valid than those of experts,
since person-in-the-street abilities might be a more
useful yardstick for some tasks than informed aca-
demic judgment, and because a greater number of
judges may be available.
Having the ability to somehow evaluate the
quality of the work undertaken by a Turker is thus
highly desirable. We would like to be able to put
in place a mechanism that filters out non-native
speakers; native speakers with low literacy levels;
cheats; and robotic cheats. That goal is considered
in the next section.
5 Judge-Intrinsic Quality Control
One common method of quality assessment for a
new process is to identify a set of ?gold-standard?
items that have been judged by experts and whose
merits are agreed, present them to the new process
or assessor, and then assess the degree to which
the new process and the experts ?agree? on the
outcomes (Snow et al, 2008; Callison-Burch et
al., 2010). A possible concern is that even experts
can be expected to disagree (and hence have low
inter-aa levels), meaning that disagreement with
the new process will also occur, even if the new
process is a reliable one. In addition, the qual-
ity of the judgments collected is also assessed via
agreement levels, meaning that any filtering based
on a quality-control measure that uses agreement
will automatically increase consistency, even to
the extent of recalibrating non-expert workers? re-
sponses to more closely match expert judgments
(Snow et al, 2008). Moreover, if an interval-level
scale is used, standardized scores cannot be em-
ployed, so a non-expert who is more lenient than
the experts, but in a reliable and systematic man-
ner, might still have their assessments discarded.
For judgments collected on a continuous scale,
statistical tests based on difference of means (over
assessors) are possible. We structure our human
1It has also been suggested that AMT restricts Turker reg-
istration by country; official information is unclear about this.
T1 initial :
T1 repeat :
d1 
Bad Ref for T2 :
d2 
T2 initial :
Figure 3: Intrinsic quality-control distributions for
an individual judge.
intelligence tasks (HITs) on Mechanical Turk in
groups of 100 in a way that allows us to control
assignment of repeat item pairs to workers, so that
statistical tests can later be applied to an individ-
ual worker?s score distributions for repeat items.
Workers were made aware of the task structure
before accepting it ? the task preview included a
message This HIT consists of 100 fluency assess-
ments, you have 0 so far complete.
We refer to the repeat items in a HIT as
ask again translations. In addition, we inserted a
number of bad reference pairs into each HIT, with
a bad reference pair consisting of a genuine MT
system output, and a distorted sentence derived
from it, expecting that its fluency was markedly
worse than that of the corresponding system out-
put. This was done by randomly selecting two
words in the sentence and duplicating them in ran-
dom locations not adjacent to the original word
and not in the initial or sentence-final position.
Any other degradation method could also be used,
so long as it has a high probability of reducing the
fluency of the text, and provided that it is not im-
mediately obvious to the judges.
Insertion of ask again and bad reference pairs
into the HITs allowed two measurements to be
made for each worker: when presented with
an ask again pair, we expect a conscientious
judge to give similar scores (but when using
a continuous scale, certainly not identical), and
on bad reference pairings a conscientious judge
should reliably give the altered sentence a lower
score. The wide separation of the two appear-
ances of an ask again pair makes it unlikely that
a judge would remember either the sentence or
their first reaction to it, and backwards movement
through the sentences comprising each HIT was
not possible. In total, each HIT contained 100 sen-
36
Figure 4: Welch?s t-test reliability estimates plot-
ted against mean seconds per judgment.
tences, including 10 bad reference pairs, and 10
ask again pairs.
Figure 3 illustrates these two types of pairs,
presuming that over the course of one or more
HITs each worker has assessed multiple ask again
pairs generating the distribution indicated by d1,
and also multiple bad reference pairs, generating
the distribution indicated by d2. As an estimate
of the reliability of each individual judge we ap-
ply a t-test to compare ask again differences with
bad reference differences, with the expectation
that for a conscientious worker the latter should
be larger than the former. Since there is no guar-
antee that the two distributions of d1 and d2 have
the same variance, we apply Welch?s adaptation of
the Student t-test.
The null hypothesis to be tested for each AMT
worker is that the score difference for ask again
pairs is not less than the score difference for
bad reference pairs. Lower p values mean more
reliable workers; in the experiments that are re-
ported shortly, we use p < 0.05 as a threshold
of reliability. We also applied the non-parametric
Mann-Whitney test to the same data, for the pur-
pose of comparison, since there is no guarantee
that d1 and d2 will be normally distributed for a
given assessor.
The next section provides details of the experi-
mental structure, and then describes the outcomes
in terms of their effect on overall system rank-
ings. As a preliminary indication of Turker be-
havior, Figure 4 summarizes some of the data that
was obtained. Each plotted point represents one
AMT worker who took part in our experiments,
and the horizontal axis reflects their average per-
judgment time (noting that this is an imprecise
measurement, since they may have taken phone
calls or answered email while working through a
HIT, or simply left the task idle to help obscure
a lack of effort). The vertical scale is the p value
obtained for that worker when the ask again distri-
bution is compared to their bad reference distribu-
tion, with a line at p = 0.05 indicating the upper
limit of the zone for which we are confident that
they had a different overall response to ask again
pairs than they did to bad reference pairs. Note the
small number of very fast, very inaccurate work-
ers at the top left; we have no hesitation in call-
ing them unconscientious (and declining to pay
them for their completed HITs). Note also the very
small number of workers for which it was possi-
ble to reliably distinguish their ask again behavior
from their bad reference behavior.
6 Experiments
HIT Structure
A sample of 560 translations was selected at
random from the WMT 2012 published shared
task dataset for a range of language pairs, with
segments consisting of 70 translations, each as-
signed to a total of eight distinct HITs. The sen-
tences were generated as image files, as recom-
mended for judgment of translations (Callison-
Burch, 2009). Each HIT was presented to a worker
as a set of 100 sentences including a total of 30
quality control items, with only one sentence visi-
ble on-screen at any given time. Each quality con-
trol item comprised a pair of corresponding trans-
lations, widely separated within the HIT. Three
kinds of quality control pairs were used:
? ask again: system output and exact repeat;
? bad reference: system output and an altered
version of it with noticeably lower fluency;
and
? good reference: system output and the corre-
sponding human produced reference transla-
tion (as provided in the released WMT data).
Each HIT consisted of 10 groups, each containing
10 sentences: 7 ?normal? translations, plus one
of each type of quality control translation drawn
37
from one of the other groups in the HIT in such
a way that 40?60 judgments would be completed
between the elements of any quality-control pair.
Consistency of Human Judgments
Using judgments collected on the continuous rat-
ing scale, we first examine assessor consistency
based on Welch?s t-test and the non-parametric
Mann-Whitney U-test. In order to examine the de-
gree to which human assessors assign consistent
scores, we compute mean values of d1 (Figure 3)
when ask again pairs are given to the same judge,
and across pairs of judges. Three sets of results
are shown: the raw unfiltered data; data filtered
according to p < 0.05 according to the quality-
control regime described in the previous section
using the Welch?s t-test; and data filtered using
the Mann-Whitney U-test. Table 1 shows that the
t-test indicates that only 13.1% of assessors meet
quality control hurdle, while a higher proportion,
35.7%, of assessors are deemed acceptable.
The stricter filter, Welch?s t-test, yields more
consistent scores for same-judge repeat items: de-
creases of 4.5 (mean) and 4.2 (sd) are observed
when quality control is applied. In addition, re-
sults for Welch?s t-test show high levels of con-
sistency for same-judge repeat items: an average
difference of only 9.5 is observed, which is not
unreasonable, given that the scale is 100 points
in length and a 10-point difference corresponds to
just 60 pixels on the screen.
For repeat items rated by distinct judges, both
filtering methods decrease the mean difference in
scores compared to the unfiltered baseline, with
the two tests giving similar improvements.
When an interval-level scale is used to evaluate
the data, the Kappa coefficient is commonly used
to evaluate consistency levels of human judges
(Callison-Burch et al, 2007), where Pr(a) is the
relative observed agreement among raters, and
Pr(e) is the hypothetical probability of chance
agreement:
? =
Pr(a)? Pr(e)
1? Pr(e)
In order to use the Kappa coefficient to compare
agreement levels for the interval-level and contin-
uous scales, we convert continuous scale scores to
a target number of interval categories. We do this
primarily for a target number of five, as this best
provides a comparison between scores for the 5-
point interval-level scale. But we also present re-
sults for targets of four and two categories, since
the continuous scale is marked at the midway and
quarter points, providing implicit intervals. A two-
category is also interesting if the assessment pro-
cess is regarded as dichotomizing to only include
for each translation whether or not the judge con-
sidered it to be ?good? or ?bad?. Use of statisti-
cal difference of means tests on interval-level data
is not recommended; but for the purpose of illus-
tration, we also applied Welch?s t-test to quality
control workers that completed the interval-level
HITs, with the same threshold of p < 0.05.
Tables 2 and 3 show intra-annotator agreement
for the five-point interval scale and continuous
scales, with and without quality control.2 Results
for repeat items on the interval-level scale show
that quality control only alters intra-aa marginally
(Pr(a) increases by 1%), and that inter-aa levels
worsen (Pr(a) decreases by 6.2%). This confirms
that applying statistical tests to interval-level data
is not a suitable way of filtering out low quality
workers.
When comparing consistency levels of asses-
sors using the interval-level scale to those of the
continuous scale, we observe marginally lower ?
coefficients for both intra-aa (?0.009) and inter-
aa (?0.041) for the continuous scale. However,
this is likely to be in part due to the fact that the
continuous scale corresponds more intuitively to 4
categories, and agreement levels for the unfiltered
4-category continuous scale are higher than those
collected on the interval-level scale by +0.023
intra-aa and +0.014 inter-aa.
Applying quality-control on the continuous
scale results in dramatic increases in intra-aa lev-
els: +0.152 for 5-categories (5-cat), +0.100 for
4-categories (4-cat) and +0.096 for 2-categories
(2-cat). When considering inter-aa levels, quality-
control does not directly result in as dramatic an
increase, as inter-aa levels increase by +0.010
for 5-cat, +0.006 for 4-cat and +0.004 for 2-cat.
It is likely, however, that apparent disagreement
between assessors might be due to different as-
sessors judging fluency generally worse or better
than one another. The continuous scale allows for
scores to be standardized by normalizing scores
with respect to the mean and standard deviation
of all scores assigned by a given individual judge.
We therefore transform scores of each judge into
2Note that the mapping from continuous scores to cate-
gories was not applied for quality control.
38
same judge distinct judges
workers judgments mean sd mean sd
Unfiltered 100.0% 100.0% 14.0 18.4 28.9 23.5
Welch?s t-test 13.1% 23.5% 9.5 14.2 25.2 21.0
Mann-Whitney U-test 35.7% 48.8% 13.1 17.7 25.0 22.6
Table 1: Mean and standard deviation of score differences for continuous scale with ask again items
within a given judge and across two distinct judges, for no quality control (unfiltered), Welch?s t-test and
Mann-Whitney U-test with a quality-control threshold of p < 0.05.
# 5-pt. interval 5-pt. interval continuous continuous
categ- unfiltered filtered unfiltered filtered
ories Pr(a) ? Pr(a) ? Pr(a) ? Pr(a) ?
5 60.4% 0.505 61.4% 0.517 59.7% 0.496 71.8% 0.647
4 - - - - 64.6% 0.528 72.1% 0.629
2 - - - - 85.2% 0.704 90.0% 0.800
Table 2: Intra-annotator (same judge) agreement levels for 5-point interval and continuous scales for
unfiltered judgments and judgments of workers with p < 0.05 for Welch?s t-test.
corresponding z-scores and use percentiles of the
combined set of all scores to map z-scores to cat-
egories where a score falling in the bottom 20 th
percentile corresponds to strongly disagree, scores
between the 20 th and 40 th percentile to disagree,
and so on. Although this method of transformation
is somewhat harsh on the continuous scale, since
scores no longer correspond to different locations
on the original scale, it nevertheless shows an in-
crease in consistency of +0.05 (5-cat), +0.086 (4-
cat) and +0.144 (2-cat). However, caution must
be taken when interpreting consistency for stan-
dardized scores, as can be seen from the increase
in agreement observed when unfiltered scores are
standardized.
Table 4 shows a breakdown by target language
of the proportion of judgments collected whose
scores met the significance threshold of p < 0.05.
Results appear at first to have shockingly low lev-
els of high quality work, especially for English and
German. When running the tasks in Mechanical
Turk, it is worth noting that we did not adopt statis-
tical tests to automatically accept/reject HITs and
we believe this would be rather harsh on workers.
Our method of quality control is a high bar to reach
and it is likely that many workers that do not meet
the significance threshold would still have been
working in good faith. In practice, we individually
examined mean scores for reference translation,
system outputs and bad reference pairs, and only
declined payment when there was no doubt the re-
English German French Spanish
10.0% 0% 57.9% 62.5%
Table 4: High quality judgments, by language.
sponse was either automatic or extremely careless.
The structure of the task and the fact that the
quality-control items were somewhat hidden may
have lulled workers into a false sense of compla-
cency, and perhaps encouraged careless responses.
However, even taking this into consideration, the
fact that none of the German speaking asses-
sors and just 10% of English speaking assessors
reached our standards serves to highlight the im-
portance of good quality-control techniques when
employing services like AMT. In addition, the risk
of getting low quality work for some languages
might be more risky than for others. The response
rate for high quality work for Spanish and French
was so much higher than German and English,
perhaps by chance, or perhaps the result of factors
that will be revealed in future experimentation.
System Rankings
As an example of the degree to which system
rankings are affected by applying quality control,
for the language direction for which we achieved
the highest number of high quality assessments,
English-to-Spanish, we include system rankings
by mean score with each measurement scale, with
and without quality control and for mean z-scores
39
# 5-pt. interval 5-pt. interval continuous continuous cont. standrdzed. cont. standrdzed.
categ- unfiltered qual.-controlled unfiltered qual.-controlled unfiltered qual.-controlled
ories Pr(a) ? Pr(a) ? Pr(a) ? Pr(a) ? Pr(a) ? Pr(a) ?
5 33.0% 0.16 26.8% 0.084 29.5% 0.119 30.3% 0.128 30.2% 0.1272 33.5% 0.169
4 - - - - 38.1% 0.174 38.5% 0.180 35.5% 0.1403 44.5% 0.260
2 - - - - 66.5% 0.331 66.8% 0.335 75.5% 0.5097 73.8% 0.475
Table 3: Inter-annotator (distinct judge) agreement levels for 5-point interval and continuous scales for
unfiltered judgments and judgments of workers with p < 0.05 for Welch?s t-test.
z-scores
5-pt. 5-pt. continuous continuous continuous
unfiltered qual.-controlled unfiltered qual.-controlled qual.-controlled
Sys A 2.00 Sys A 2.00 Sys E 69.60 Sys E 74.39 Sys E 0.43
Sys B 1.98 Sys D 1.97 Sys B 61.78 Sys F 65.07 Sys B 0.16
Sys C 1.98 Sys F 1.95 Sys G 60.21 Sys G 64.51 Sys G 0.08
Sys D 1.98 Sys C 1.95 Sys F 59.38 Sys B 63.68 Sys D 0.06
Sys E 1.98 Sys E 1.95 Sys D 59.05 Sys D 63.52 Sys C 0.02
Sys F 1.97 Sys B 1.94 Sys A 57.44 Sys C 61.33 Sys F 0.01
Sys G 1.97 Sys G 1.93 Sys I 56.31 Sys A 58.43 Sys H ?0.03
Sys H 1.96 Sys H 1.90 Sys C 55.82 Sys I 57.46 Sys I ?0.07
Sys I 1.96 Sys I 1.88 Sys H 55.27 Sys H 57.04 Sys A ?0.10
Sys J 1.94 Sys J 1.81 Sys J 50.46 Sys J 50.73 Sys J ?0.23
Sys K 1.90 Sys K 1.76 Sys K 44.62 Sys K 41.25 Sys K ?0.47
Table 5: WMT system rankings based on approximately 80 randomly-selected fluency judgments per
system, with and without quality control for radio button and continuous input types, based on German-
English. The quality control method applied is annotators who score worsened system output and gen-
uine system outputs with statistically significant lower scores according to paired Student?s t-test.
when raw scores are normalized by individual as-
sessor mean and standard deviation. The results
are shown in Table 5. (Note that we do not claim
that these rankings are indicative of actual system
rankings, as only fluency of translations was as-
sessed, using an average of just 55 translations per
system.)
When comparing system rankings for unfiltered
versus quality-controlled continuous scales, firstly
the overall difference in ranking is not as dramatic
as one might expect, as many systems retain the
same rank order, with only a small number of sys-
tems changing position. This happens because
random-clickers cannot systematically favor any
system, and positive and negative random scores
tend to cancel each other out. However, even hav-
ing two systems ordered incorrectly is of concern;
careful quality control, and the use of normaliza-
tion of assessors? scores may lead to more consis-
tent outcomes. We also note that incorrect system
orderings may lead to flow-on effects for evalua-
tion of automatic metrics.
The system rankings in Table 5 also show how
the use of the continuous scale can be used to rank
systems according to z-scores, so that individual
assessor preferences over judgments can be ame-
liorated. Interestingly, the system that scores clos-
est to the mean, Sys F, corresponds to the baseline
system for the shared task with a z-score of 0.01.
7 Conclusion
We have compared human assessor consistency
levels for judgments collected on a five-point
interval-level scale to those collected on a contin-
uous scale, using machine translation fluency as
a test case. We described a method for quality-
controlling crowd-sourced annotations that results
in marked increases in intra-annotator consistency
and does not require judges to agree with experts.
In addition, the use of a continuous scale allows
scores to be standardized to eliminate individual
judge preferences, resulting in higher levels of
inter-annotator consistency.
40
Acknowledgments
This work was funded by the Australian Research
Council. Ondr?ej Bojar, Rosa Gog, Simon Gog,
Florian Hanke, Maika Vincente Navarro, Pavel
Pecina, and Djame Seddah provided translations
of task instructions, and feedback on published
HITs.
References
A. Alexandrov. 2010. Characteristics of single-item
measures in Likert scale format. The Electronic
Journal of Business Research Methods, 8:1?12.
C. Callison-Burch, C. Fordyce, P. Koehn, C. Monz, and
J. Schroeder. 2007. (Meta-) evaluation of machine
translation. In Proc. 2nd Wkshp. Statistical Machine
Translation, pages 136?158, Prague, Czech Repub-
lic.
C. Callison-Burch, C. Fordyce, P. Koehn, C. Monz,
and J. Schroeder. 2008. Further meta-evaluation of
machine translation. In Proc. 3rd Wkshp. Statisti-
cal Machine Translation, pages 70?106, Columbus,
Ohio.
C. Callison-Burch, P. Koehn, C. Monz, and
J. Schroeder. 2009. Findings of the 2009
Workshop on Statistical Machine Translation. In
Proc. 4th Wkshp. Statistical Machine Translation,
pages 1?28, Athens, Greece.
C. Callison-Burch, P. Koehn, C. Monz, K. Peterson,
M. Przybocki, and O. Zaidan. 2010. Findings of the
2010 Joint Workshop on Statistical Machine Trans-
lation and Metrics for Machine Translation. In Proc.
5th Wkshp. Statistical Machine Translation, pages
17?53, Uppsala, Sweden.
C. Callison-Burch, P. Koehn, C. Monz, and O. Zaidan.
2011. Findings of the 2011 Workshop on Statistical
Machine Translation. In Proc. 6th Wkshp. Statisti-
cal Machine Translation, pages 22?64, Edinburgh,
Scotland.
C. Callison-Burch, P. Koehn, C. Monz, M. Post,
R. Soricut, and L. Specia. 2012. Findings of the
2012 Workshop on Statistical Machine Translation.
In Proc. 7th Wkshp. Statistical Machine Translation,
pages 10?51, Montreal, Canada.
C. Callison-Burch. 2009. Fast, cheap, and creative:
Evaluating translation quality using Amazon?s Me-
chanical Turk. In Proc. Conf. Empirical Methods in
Natural Language Processing, pages 286?295, Sin-
gapore.
M. Denkowski and A. Lavie. 2010. Choosing the right
evaluation for machine translation: An examination
of annotator and automatic metric performance on
human judgement tasks. In Proc. 9th Conf. Assoc.
Machine Translation in the Americas (AMTA), Den-
ver, Colorado.
K. Fort, G. Adda, and K. B. Cohen. 2011. Amazon
Mechanical Turk: Gold mine or coal mine? Com-
putational Linguistics, 37(2):413?420.
E. Gibson, S. Piantadosi, and K. Fedorenko. 2011. Us-
ing Mechanical Turk to obtain and analyze English
acceptability judgments. Language and Linguistics
Compass, 5/8:509?524.
Y. Graham, T. Baldwin, A. Harwood, A. Moffat, and
J. Zobel. 2012. Measurement of progress in ma-
chine translation. In Proc. Australasian Language
Technology Wkshp., pages 70?78, Dunedin, New
Zealand.
LDC. 2005. Linguistic data annotation specification:
Assessment of fluency and adequacy in translations.
Technical report, Linguistic Data Consortium. Re-
vision 1.5.
R. A. Seymour, J. M. Simpson, J. E. Charlton, and
M. E. Phillips. 1985. An evaluation of length and
end-phrase of visiual analogue scales in dental pain.
Pain, 21:177?185.
R. Snow, B. O?Connor, D. Jursfsky, and A. Y. Ng.
2008. Cheap and fast ? but is it good? Evalu-
ating non-expert annotations for natural language
tasks. In Proc. Conf. Empirical Methods in Natu-
ral Language Processing, pages 254?263, Honolulu,
Hawaii.
D. L. Streiner and G. R. Norman. 1989. Health Mea-
surement Scales: A Practical Guide to their Devel-
opment and Use. Oxford University Press, fourth
edition.
41
