Proceedings of NAACL HLT 2007, Companion Volume, pages 161?164,
Rochester, NY, April 2007. c?2007 Association for Computational Linguistics
Simultaneous Identification of Biomedical Named-Entity and
Functional Relations Using Statistical Parsing Techniques ?
Zhongmin Shi and Anoop Sarkar and Fred Popowich
School of Computing Science
Simon Fraser University
{zshi1,anoop,popowich}@cs.sfu.ca
Abstract
In this paper we propose a statistical pars-
ing technique that simultaneously iden-
tifies biomedical named-entities (NEs)
and extracts subcellular localization re-
lations for bacterial proteins from the
text in MEDLINE articles. We build
a parser that derives both syntactic and
domain-dependent semantic information
and achieves an F-score of 48.4% for the
relation extraction task. We then propose
a semi-supervised approach that incor-
porates noisy automatically labeled data
to improve the F-score of our parser to
83.2%. Our key contributions are: learn-
ing from noisy data, and building an an-
notated corpus that can benefit relation ex-
traction research.
1 Introduction
Relation extraction from text is a step beyond
Named-Entity Recognition (NER) and generally de-
mands adequate domain knowledge to build rela-
tions among domain-specific concepts. A Biomedi-
cal Functional Relation (relation for short) states in-
teractions among biomedical substances. In this pa-
per we focus on one such relation: Bacterial Protein
Localization (BPL), and introduce our approach for
identifying BPLs from MEDLINE1 articles.
BPL is a key functional characteristic of pro-
teins. It is essential to the understanding of the func-
tion of different proteins and the discovery of suit-
able drugs, vaccines and diagnostic targets. We are
collaborating with researchers in molecular biology
with the goal of automatically extracting BPLs from
?This research was partially supported by NSERC, Canada.
1MEDLINE is a bibliographic database of biomedical
scientific articles at National Library of Medcine (NLM,
http://www.nlm.nih.gov/).
text with BioNLP techniques, to expand their pro-
tein localization database, namely PSORTdb2(Rey
et al, 2005). Specifically, the task is to produce as
output the relation tuple BPL(BACTERIUM, PRO-
TEIN, LOCATION) along with source sentence and
document references. The task is new to BioNLP
in terms of the specific biomedical relation being
sought. Therefore, we have to build annotated cor-
pus from scratch and we are unable to use existing
BioNLP shared task resources in our experiments.
In this paper we extract from the text of biomedical
articles a relation among: a LOCATION (one of the
possible locations shown in Figure 1 for Gram+ and
Gram- bacteria); a particular BACTERIUM, e.g. E.
Coli, and a PROTEIN name, e.g. OprF.
(Nair and Rost, 2002) used the text taken from
Swiss-Prot annotations of proteins, and trained a
subcellular classifier on this data. (Hoglund et al,
2006) predicted subcellular localizations using an
SVM trained on both text and protein sequence data,
by assigning each protein name a vector based on
terms co-occurring with the localization name for
each organism. (Lu and Hunter, 2005) applied a hi-
erarchical architecture of SVMs to predict subcel-
lular localization by incorporating a semantic hier-
archy of localization classes modeled with biolog-
ical processing pathways. These approaches either
ignore the actual location information in their pre-
dicted localization relations, or only focus on a small
portion of eukaryotic proteins. The performance of
these approaches are not comparable due to different
tasks and datasets.
2 System Outline
During our system?s preprocessing phase, sentences
are automatically annotated with both syntactic in-
formation and domain-specific semantic informa-
tion. Syntactic annotations are provided by a statis-
tical parser (Charniak and Johnson, 2005). Domain-
2http://db.psort.org.
161
cytoplasm cytoplasm 
Gram+ Gram- 
cytoplasmic 
membrane 
cell wall 
periplasm 
outer 
membrane secreted 
inner 
membrane 
Figure 1: Illustration of possible locations of pro-
teins with respect to the bacterial cell structure.
specific semantic information includes annotations
on PROTEIN, BACTERIUM and LOCATION NEs
by dictionary lookups from UMLS3, NCBI Taxon-
omy4 and SwissProt5, and two automatic Bio-NE
recognizers: MMTx6 and Lingpipe7.
We propose the use of a parser that simultane-
ously identifies NEs and extracts the BPL relations
from each sentence. We define NEs to be Relevant
to each other only if they are arguments of a BPL re-
lation, otherwise they are defined to be Irrelevant.
A sentence may contain multiple PROTEIN (LO-
CATION or ORGANISM) NEs, e.g., there are two
PROTEIN NEs in the sentence below but only one,
OmpA, is relevant. Our system aims to identify the
correct BPL relation among all possible BPL tuples
(candidate relations) in the sentence by only recog-
nizing relevant NEs. Each input sentence is assumed
to have at least one BPL relation.
Nine of 10 monoclonal antibodies mapped within the carboxy-
terminal region of [PROTEIN OprF] that is homologous to
the [ORGANISM Escherichia coli] [LOCATION outer membrane]
protein [PROTEIN OmpA].
3 Statistical Syntactic and Semantic Parser
Similar to the approach in (Miller et al, 2000) and
(Kulick et al, 2004), our parser integrates both syn-
tactic and semantic annotations into a single annota-
tion as shown in Figure 2. A lexicalized statistical
parser (Bikel, 2004) is applied to the parsing task.
The parse tree is decorated with two types of seman-
3http://www.nlm.nih.gov/research/umls/
4http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?db=Taxonomy
5http://www.ebi.ac.uk/swissprot/
6MetaMap Transfer, http://mmtx.nlm.nih.gov/
7http://www.alias-i.com/
PO_LNK/NP 
PO_PTR/PP PO_PTR/PP 
DT PO_PTR/NP PRN NN 
The PROTEIN_R/NP 
PROTEIN_R/JJ 
phospholipase 
PROTEIN_R/NNP 
C 
-LRB- 
-LRB- 
NP 
NNP 
PLC 
gene 
IN 
of 
PO_PTR/NP 
ORGANISM_R/NP 
ORGANISM_R/NNP ORGANISM_R/NNP 
Pseudomonas aeruginosa 
-RRB- 
-RRB- 
Figure 2: An example of parsing results
tic annotations:
1) Annotations on relevant PROTEIN, BAC-
TERIUM and LOCATION NEs. Tags are PRO-
TEIN R, BACTERIUM R and LOCATION R respec-
tively.
2) Annotations on paths between relevant NEs. The
lower-most node that spans both NEs is tagged as
LNK and all nodes along the path to the NEs are
tagged as PTR.
Binary relations are apparently much easier to
represent on the parse tree, therefore we split the
BPL ternary relation into two binary relations: BP
(BACTERIUM and PROTEIN) and PL (PROTEIN
and LOCATION). After capturing BP and PL rela-
tions, we will predict BPL as a fusion of BP and PL,
see ?4.1. In contrast to the global inference done us-
ing our generative model, heavily pipelined discrim-
inative approaches usually have problems with error
propagation. A more serious problem in a pipelined
system when using syntactic parses for relation ex-
traction is the alignment between the named enti-
ties produced by a separate system and the syntac-
tic parses produced by the statistical parser. This
alignment issue is non-trivial and we could not pro-
duce a pipelined system that dealt with this issue
satisfactorily for our dataset. As a result, we did
not directly compare our generative approach to a
pipelined strategy.
4 Experiment Settings and Evaluations
The training and test sets are derived from a small
expert-curated corpus. Table 1 lists numbers of sen-
tences and relevant NEs in each BP/PL/BPL set.
Since the parsing results include both NE and path
tags (note that we do not use any external NER sys-
tem), there are two metrics to produce and evalu-
ate PL or BP relations: Name-only and Name-path
metrics. The name-only metric only measures Rel-
162
PL BP BPL
Training set 289 / 605 258 / 595 352 / 852
Test set 44 / 134 28 / 127 62 / 182
Table 1: Sizes of training and test sets (number of
sentences / number of relevant NEs)
evant PROTEIN, BACTERIUM and LOCATION
NEs (see Section 2). It does not take path annota-
tions into account. The name-only metric is mea-
sured in terms of Precision, Recall and F-score, in
which True Positive (TP ) is the number of correctly
identified NEs, False Positive (FP ) is the number of
incorrectly identified NEs and False Negative (FN )
is the number of correct NEs that are not identified.
The name-path measures nodes being annotated
as LNK, PTR or R along the path between NEs
on the parse tree, therefore it represents confidence
of NEs being arguments of the relation. The name-
path metric is a macro-average measure, which is
the average performance of all sentences in data set.
In measurement of the name-path metric, TP is the
number of correctly annotated nodes on the path be-
tween relevant NEs. FP is the number of incor-
rectly annotated nodes on the path and FN is the
number of correct nodes that are not identified.
4.1 Fusion of BP and PL
The BPL relation can be predicted by a fusion of
BP and PL once they are extracted. Specifically, a
BP and a PL that are extracted from the same sen-
tence are merged into a BPL. The predicted BPL
relations are then evaluated by the same name-only
and name-path metrics as for binary relations. In the
name-path metric, nodes on both PL and BP paths
are counted. Note that we do not need a common
protein NER to merge the BP and PL relations. E.g.,
for name-only evaluation, assume true BPL(B1, P1,
L1): if we predict BP(B1, ) and PL(P1, L2), then
TP=2 due to B1, P1; FP=1 due to L2; and FN=1
due to P1.
5 NER and BPL Extraction
Baseline: An intuitive method for relation extrac-
tion would assume that any sentence containing
PROTEIN, ORGANISM and LOCATION NEs has
the relation. We employ this method as a baseline
system, in which NEs are identified by the auto-
matic NE recognizers and dictionary lookups as in-
troduced in ?2. The system is evaluated against the
test set in Table 1. Results in Table 2 show low pre-
cision for PROTEIN NER and the name-path metric.
Extraction using Supervised Parsing: We first ex-
periment a fully supervised approach by training the
parser on the BP/PL training set and evaluate on the
test set (see Table 1). The name-only and name-path
evaluation results in Table 2 show poor syntactic
parsing annotation quality and low recall on PRO-
TEIN NER. The major reason of these problems is
the lack of training data.
Extraction using Semi-supervised Parsing: Ex-
periments with purely supervised learning show that
our generative model requires a large curated set
to minimize the sparse data problem, but domain-
specific annotated corpora are always rare and ex-
pensive. However, there is a huge source of unla-
beled MEDLINE articles available that may meet
our needs, by assuming that any sentence contain-
ing BACTERIUM, PROTEIN and LOCATION NEs
has the BPL relation. We then choose such sentences
from a subset of the MEDLINE database as the
training data. These sentences, after being parsed
and BPL relations inserted, are in fact the very noisy
data when used to train the parser, since the assumed
relations do not necessarily exist. The reason this
noisy data works at all is probably because we can
learn a preference for structural relations between
entities that are close to each other in the sentence,
and thus distinguish between competing relations in
the same sentence. In future work, we hope to ex-
plore explicit bootstrapping from the labeled data to
improve the quality of the noisy data.
Two experiments were carried out corresponding
to choices of the training set: 1) noisy data only, 2)
noisy data and curated training data. Evaluation re-
sults given in Table 2.
Evaluation results on the name-only metric show
that, compared to supervised parsing, our semi-
supervised method dramatically improves recall for
NER. For instance, recall for PROTEIN NER in-
creases from 25.0% to 81.3%; recall on BAC-
TERIUM and LOCATION NERs increases about
30%. As for the name-path metric, the over-
all F-score is much higher than our fully super-
vised method increasing from 39.9% to 74.5%. It
shows that the inclusion of curated data in the semi-
163
Name-only Evaluation (%) Name-Path Evaluation (%)
Method Measure PL BP BPL PL BP BPL
PROT LOC PROT BACT
P 42.3 78.6 41.9 81.3 40.7 27.1 38.9 31.0
Baseline R 92.5 97.3 87.8 97.4 90.9 56.5 69.0 60.7
F 58.0 87.0 56.7 88.6 56.2 36.6 49.8 41.0
Supervised P 66.7 87.5 66.7 72.7 76.9 45.9 41.2 43.9
(training data R 25.0 56.0 10.5 47.1 35.3 36.7 36.3 36.5
only) F 36.4 68.3 18.2 57.1 48.4 40.8 38.6 39.9
Semi-supervised P 66.7 95.5 70.6 94.1 80.8 76.2 83.5 79.3
(noisy data R 84.2 80.8 80.0 84.2 81.8 67.8 72.4 67.0
only) F 74.4 87.5 75.0 88.9 81.3 71.7 77.5 74.2
Semi-supervised P 73.9 95.5 76.5 94.1 84.8 77.0 81.1 78.7
(noisy data + R 81.0 80.8 81.3 84.2 81.7 68.5 73.7 70.7
training data) F 77.3 87.5 78.8 88.9 83.2 72.5 77.2 74.5
Table 2: Name-only and name-path evaluation results. PROTEIN, LOCATION and BACTERIUM are
PROT, LOC and BACT for short. The training data is the subset of curated data in Table 1.
supervised method does not improve performance
much. Precision of PROTEIN NER increases 6.5%
on average, while F-score of overall BPL extraction
increases only slightly. We experimented with train-
ing the semi-supervised method using noisy data
alone, and testing on the entire curated set, i.e., 333
and 286 sentences for BP and PL extractions respec-
tively. Note that we do not directly train from the
training set in this method, so it is still ?unseen? data
for this model. The F-scores of path-only and path-
name metrics are 75.5% and 67.1% respectively.
6 Discussion and Future Work
In this paper we introduced a statistical parsing-
based method to extract biomedical relations from
MEDLINE articles. We made use of a large un-
labeled data set to train our relation extraction
model. Experiments show that the semi-supervised
method significantly outperforms the fully super-
vised method with F-score increasing from 48.4%
to 83.2%. We have implemented a discriminative
model (Liu et al, 2007) which takes as input the ex-
amples with gold named entities and identifies BPL
relations on them. In future work, we plan to let the
discriminative model take the output of our parser
and refine our current results further. We also plan
to train a graphical model based on all extracted BP,
PL and BPL relations to infer relations from multi-
ple sentences and documents.
References
D. Bikel. 2004. A distributional analysis of a lexicalized statis-
tical parsing model. In Proc. of EMNLP ?04, pages 182?189.
E. Charniak and M. Johnson. 2005. Coarse-to-fine n-best pars-
ing and maxent discriminative reranking. In Proc. of ACL
?05, pages 173?180.
A. Hoglund, T. Blum, S. Brady, P. Donnes, J. Miguel,
M. Rocheford, O. Kohlbacher, and H. Shatkay. 2006. Sig-
nificantly improved prediction of subcellular localization by
integrating text and protein sequence data. In Proc. of PSB
?06, volume 11, pages 16?27.
S. Kulick, A. Bies, M. Libeman, M. Mandel, R. McDonald,
M. Palmer, A. Schein, and L. Ungar. 2004. Integrated an-
notation for biomedical information extraction. In Proc. of
HLT/NAACL ?04, pages 61?68, Boston, May.
Y. Liu, Z. Shi, and A. Sarkar. 2007. Exploiting rich syntactic
information for relation extraction from biomedical articles.
In NAACL-HLT ?07, poster track, Rochester, NY, April.
Z. Lu and L. Hunter. 2005. Go molecular function terms are
predictive of subcellular localization. In Proc. of PSB ?05,
volume 10, pages 151?161.
S. Miller, H. Fox, L. Ramshaw, and R. Weischedel. 2000. A
novel use of statistical parsing to extract information from
text. In Proc. of NAACL ?06, pages 226?233.
R. Nair and B. Rost. 2002. Inferring subcellular localization
through automated lexical analysis. In Bioinformatics, vol-
ume 18, pages 78?86.
S. Rey, M. Acab, J. Gardy, M. Laird, K. deFays, C. Lam-
bert, and F. Brinkman. 2005. Psortdb: A database of sub-
cellular localizations for bacteria. Nucleic Acids Research,
33(D):164?168.
164
Flexible Speech Act Based Dialogue Management 
El i  Hagen and F red  Popowich  
School of Comput ing Science 
Simon Fraser University 
Canada V5A 1S6 
{hagen, popowich}@cs, sfu. ca 
Abstract 
We present an application independent dialogue 
engine that reasons on application dependent 
knowledge sources to calculate predictions about 
how a dialogue might continue. Predictions are 
language independent and are translated into lan- 
guage dependent structures for recognition and 
synthesis. Further, we discuss how the predic- 
tions account for different kinds of dialogue, e.g., 
question-answer or mixed initiative. 
1 In t roduct ion  
The computerized spoken information systems (or 
Spoken Dialogue System--SDS) that we will con- 
sider in this paper are systems where a computer 
acts as the operator of some service and inter- 
acts with a user in natural anguage, e.g., switch 
board, directory assistance, or ticket service. Be- 
fore an SDS can provide its information, it needs 
to acquire data from the user, e.g., customer name 
and number, birth date, service location, or ser- 
vice date. We call these parameter values. In an 
SDS they are acquired orally and speech recogni- 
tion is used to decode the speech signal into words. 
A dialogue manager facilitates the negotiation 
of parameter values between a user and an SDS. 
We emphasize keeping our dialogue manager ap- 
plication and language independent, hus we fac- 
tored out the independent information into two 
components. A dialogue ngine calculates pre- 
dictions for how to continue a dialogue from de- 
pendent knowledge sources (e.g., dialogue gram- 
mar and history, application description). A prag- 
matic interpreter maps syntactic/semantic inter- 
pretation results onto predictions. 
Our predictions are called dialogue primitives; 
GEN-primitives predict system utterances and 
REC-primitives predict user utterances. They are 
language independent and on both the recogni- 
tion and the generation side, other modules trans- 
late them into language dependent s ructures. In 
this paper, we will discuss the kinds of primi- 
tives our dialogue manager calculates and how 
REC-primitives 
REC- ~, 
SIDE I speech 1 \[ syntac./sem. \] 
recognizer ~ interpreter 
/ \ 
'telephone\] primitives pragm, int. 
microphone I ~o~e 
GEN I engine 
pr imi t iv~ 
~ \[ response i applicati?n 
I generator description+ dialogue 
strategies 
GEN- 
SIDE 
Figure 1: System architecture of our SDS. The 
arcs indicate information flow. 
they account for different kinds of dialogue, e.g., 
question-answer or mixed initiative. 
2 Background 
First, we discuss our system architecture and data 
flow between modules. Second, we present the ap- 
plication description of a movie service, which we 
will use for the examples in later sections. Third, 
we present some of our current primitives, and fi- 
naUy, we describe the dialogue engine and how it 
uses the application description and other sources 
to calculate dialogue primitives. 
2.1 System Architecture 
Our system architecture is presented in Figure 1. 
The dialogue manager takes an application de- 
scription (Section 2.2) and a set of dialogue strate- 
gies (Sections 3and 4) as input--both provided by 
the service designer. The application description 
describes the parameters needed by the service 
and is necessarily application dependent. The di- 
alogue strategies contain directions for how the di- 
alogue shall proceed in certain situations. For ex- 
ample, whether to ask for confirmation or spelling 
of a badly recognized parameter value or whether 
131 
to generate system or user directed dialogue. 
The output of our dialogue manager is a bag 
of abstract, language independent primitives. On 
the generation side they encode the next sys- 
tem utterance and a response generator trans- 
lates the GEN-primitives into text, which is then 
synthesized. On the recognition side, the REC- 
primitives represent the dialogue manager's pre- 
dictions about the next user utterance. REC- 
primitives are translated into (recognition) con- 
texts and grammars for speech recognition and 
they may activate sub-components of a synsem 
grammar. After speech recognition has taken 
place, the dialogue ngine must be told which pre- 
dictions came true, thus the pragmatic interpreter 
maps the output of synsem interpreter onto a sub- 
bag of REC-primitives, which is then returned to 
the dialogue engine for further processing (Sec- 
tion 2.4). 
2.2 Appl icat ion Descr ipt ion 
The application description (AD) specifies the 
tasks that a service can solve and the parame- 
ter values needed to solve them. The AD for a 
movie service is presented in Figure 2. Our repre- 
sentation is an extended version of and-or trees 1
and in Figure 2, the U-shaped symbols represent 
and-relations, while the V-shaped symbols repre- 
sent or-relations. Thus, this movie service can 
perform three tasks: selling tickets or providing 
movie or theatre information. If the user wants 
to buy tickets, the system needs to acquire six pa- 
rarneter values, e.g., the show time, the date, and 
the name of the film. Date and show time can 
be acquired in several ways. For example, a date 
can be a simple date (e.g., "November 17th ~) or 
a combination of day of the week and week (e.g., 
"Wednesday this week."). 
The nodes keep state information. Open nodes 
have not yet been negotiated, topic nodes are be- 
ing negotiated, and closed nodes have been negoti- 
ated. The currently active task has status active. 
Parameters can be retrieved through the func- 
tions activeTask(AD), openParams(AD), closed- 
Params(AD), and topicParams(AD). Status(p) 
returns the status of parameter p. tasks(AD) and 
params(AD) return the task and parameter nodes. 
Similar hierarchical domain descriptions have 
been suggested in (Young et al, 1990) for a naval 
domain and in (Caminero-Gil et al, 1996) for an 
e-mall assistance domain. A tree-like organiza- 
tion of the domain is sufficent for the information 
retrieval domains, which we are currently consid- 
ering. We expect, however, that in future work we 
1Extensions include has-a relations. 
movie service 
movieInfot buyTicketst theatreInfot 
Ticketsp 
Timep timep dat% k~_~we e 
wee kpDay P 
F igure 2: A description of a movie service. No- 
tation: U and v represent and/or-relations. Sub- 
scripts t and p denote tasks and parameters. 
will need to switch to a semantic network struc- 
ture or since our future research includes auto- 
matic generation of system utterances from our 
dialogue primitives, we hope to be able to uti- 
lize the ontology and domain organization work, 
which has proven so useful for text generation 
(Bateman et al, 1994; Bateman et al, 1995), for 
both dialogue management and text generation. 
2.3 Dialogue Pr imi t ives  
Following the procedure outlined in Section 2.4, 
the dialogue manager calculates a bag of primi- 
tives for each turn and speaker. Our current col- 
lection is motivated through our experience with 
several domains, e.g., movie service, horoscope 
service, and directory assistance. The collection 
is not exhaustive and we will add primitives as 
wider dialogue coverage is required. 
Notat ion:  A primitive is written prim- 
Name(p=v,n), where primName is its name; p E 
params(AD) U {aTask}; aTask is a special param- 
eter whose values E tasks(AD); v is the value of p; 
and n is an integer denoting the number of times 
a primitive has been uttered. If v is uninstanti- 
ated, it is left out for readability. Unless otherwise 
stated, p E params(AD). 
2.3.1 GEN-Pr imi t ives  
Our current GEN-primitives: 
salutation(p=v): system opens or closes the inter- 
action, p E {hello, goodbye}, v E {morning, day, 
evening}. 
requestValue(p): system requests a value for the 
paramter p. p E params(AD) U {aTask}. 
requestValue(p=v): system asks whether the value 
v of parameter p is correct. If this form is used, the 
system has a list of alternative values for p, and 
132 
v is not a recognition result (e.g., Frankfurt am 
Main or Frankfurt an der Oder where Frankfurt 
is the recognition result.) 
requestValue(aTask=v), v E tasks(AD) U {repeat- 
PreServiceTask, useService, repeatService}: system 
requests a value for aTask. I f  v E {repeatPre- 
ServiceTask, useService, repeatService}, the system 
requests whether the user wants the pre-service 
task repeated, the service started (first task after 
pre-service task), or a new task started. 
requestConfirm(p=v): system asks whether the 
value v of parameter p is correct, v is a recog- 
nition result, p E params(AD) U {aTask}. Am- 
bignous results not resulting from speech recog- 
nition, e.g., Frankfurt am Main vs. ~zauldurt an 
der Oder, would yield multiple requestValue(p=v) 
primitives. 
requestValueABC(p): system requests the spelling 
of the value of parameter p. 
requestParam(p=v): system asks whether the 
value v is a value for parameter p. 
evaluate(p=v) : system acknowledges value v of 
parameter p. 
promise(p=v): system promises to attempt to 
answer the user's request, p E params(AD) U 
{aTask}. v E {pleaseWait}. Only used after nav- 
igate(), requestParam 0 or requestAIternative 0 if 
the user has to wait long for a reply. 
inform(aTask=v): system informs about the ac- 
quired database results, v E aetiveTask(AD) U
{tooMany, zero}. If v = activeTask(AD), there 
are several answers, if v = tooMany/zero, there 
are either too many answers to be enumerated or 
zero answers. 
inform(aTask=n): system presents the n'th answer 
to the query t. n > 0 
inforrnAIternative(p): system informs that there 
are several possible values for p. p E params(AD) 
U {aTask}. v E {tooMany, null}. If v = tooMany, 
there are too many alternatives to be enumerated. 
v = null, means that v is uninstantiated, not that 
there are zero alternatives. 
inforrnAIternative(p=v): system informs that a 
possible value of p is v. p E params(AD) U 
{aTask}. 
informNegative(p): system infolds that the user 
misrecognized something, p E params(AD) U 
{aTask}. 
informPositive(p): system informs that the user 
recognized something correctly, p E params(AD) 
U {aTask}. 
withdraw(p): system withdraws from dialogue for 
reason p E {error} before it has started negotia- 
tions. 
withdrawOffer(aTask=v): system withdraws an of- 
fer for reason v E {error}. 
withdrawPrornise(aTask=v): system withdraws a 
promise for reason v E {error}. 
In Section 3, we present several sample instan- 
tiations of the primitives. 
2.3.2 REC-Pr imit ives 
Our current REC-primitives: 
requestParam(p): user requests which parameter 
the system requested, p E params(AD) U {null}. 
requestAIternatives(p): user requests possible val- 
ues for parameter p. 
requestConffirm(aTask=n): user asks system to 
confirm an answer that it has given, e.g., "Was 
the first answer $30?" 0 < n < no of query results. 
informValue(p=v): user provides value v for pa- 
rameter p. p was requested. 2 
informExtraValue(p=v): user provides value v for 
parameter p. p was not requested in the preceeding 
system utterance. 
informValueABC(p=v): user spells the value v of 
parameter p. The spelling is expanded by synsem 
and expansions are presented to the dialogue man- 
ager. 2 
inforrnPositive(p=v): user confirms that the value 
of parameter p is v. p E params(AD) U {aTask}. 
informNegative(p=v): user disconfirms that the 
value of parameter p is v. p E params(AD) U 
{aTask}. 
correctValue(p=v): user corrects a misrecognized 
value. Often used together with informNegative. 
For example, "Hamburg, not Homburg. "2 
informGarbage(p): user says something but recog- 
nizer and/or  synsem could not make sense out of 
it. 
changeValue(p=v): user changes the value of pa- 
rameter p to v instead of v'. 2 
repeatValue(p=v): user repeats the value v of pa- 
rameter p.2 
correctPararn(p=v): user corrects that v is the 
value of p, not p'. 
disambiguate(p=v): user chooses v as the value of 
p when presented with a choice between several 
values for p. p E params(AD) U {aTask}. 
2The pragmatic interpreter instantiates v. 
133 
rejectValue(p=v): the user has been given a se- 
ries of alternatives and chooses p=:v'. Primitive is 
combined with disambiguate(p=v'). 
navigate(aTask=v): user navigates in the query re- 
sults, v E {forward, backward, repeat, n} where 0 
n < no of query results. 2
rejectRequest(p=v): user ignores or does not hear 
the system request, v E {null, didNotHear}. 
rejectOffer(aTask=v): user ignores or does not 
hear the system offer, v E tasks(AD) U {null, did- 
NotHear}. 
evaluate(t=v): user evaluates an answer she has 
received, v E {positive, neutral, negative, cancel}. 
cancel is used to end the current dialogue after at 
least one answer has been given ~md start a new 
one without calling again. 
promise(p): user promises to find a value for p. 
withdrawAccept(aTask=v): user ,mthdraws from 
the conversation for reason v E {cancel, hangup}. 
With cancel, the user ends the current dialogue 
before an answer has been given and starts a new 
task Without calling again. 2
withdrawPromise(p=v): user withd.raws a promise 
to provide a value for reason v E {cancel, 
hangup}. 2
withdrawRequest(p=v): user withdraws a request. 
p E params(AD) U {forward, backward, repeat, and 
n}. 2 
null(): returned to the dialogue manager if the 
,user does not say anything and is not ezpected to 
say anything, e.g., after a greeting or promise. 
In Section 3, we present several sample instan- 
tiations of the primitives. 
2.4 D ia logue Eng ine  
The dialogue engine (Hagen, 1999) consists of a 
reasoning engine and several knowledge sources: 
An AD defines an application's data-needs, a di- 
alogue grammar defines how a dialogue may pro- 
ceed at the level of speech acts, and a dialogue 
history is a dynamically growing parse tree of an 
on-going dialogue with respect to the dialogue 
grammar. Other knowledge sources may be re- 
quired, for instance, recognition confidence or dis- 
ambiguation of city names. 
The dialogue ngine calculates the next turn by 
consulting and combining information from the 
knowledge sources. It consults with the dialogue 
history and the dialogue grammar in order to cal- 
culate which speech acts may continue a dialogue. 
Speech acts have no propositional content, thus 
in the context of the current dialogue history and 
the state of the application description, they are 
translated into dialogue primitives, which have 
content, for example, the name of a parameter 
and a potential value for this parameter. Here we 
will walk through an example of how some prim- 
itives are calculated in a simple question-answer 
dialogue. 
Example :  For our example we will use the AD 
in Figure 2. Assume that the task has already 
been negotiated and set to theatre information 
(i.e., activeTask(AD) = theatrelnfo), i.e., the sys- 
tem needs to acquire the name of the theatre and 
the name of the city. All other nodes in the AD 
are closed since they are not relevant o this task. 
The speech act  g rammar  used in our system 
is presented in Appendix A but we will use a 
trivial grammar for the example. It can account 
for simple question-answer dialogues where a 
request from the system (sys) is followed by an 
inform from the user (usr). The system can 
respond to the inform with a sub-dialogue: s 
Dialogue(sys)--~(request(sys) + Inform(usr))* 
Inform(usr)--+inform(usr) + \[Dialogue(sys)\] 
The d ia logue h i s to ry  reflects all previous ne- 
gotiations (here: task theatreinfo). 
Dialogue(sys) 
request(sys) ..  Inform(usr) 
requestValue(task) 
intprm.(usr) . . , 
informValue(task=theatrelnto) 
The next turn can be rooted in either the 
Inform(usr) after the inform(usr) or in the Dia- 
Iogue(sys) after Inform(usr). 
With all the above knowledge sources in place, 
the calculation of the next dialogue turn can start: 
1. The last speech act in the dialogue history 
gives us a starting point in the grammar, thus 
moving forward from inform(usr), the next atomic 
speech act is request(sys)--either as a flat struc- 
ture (i.e., request(sys) off Dialogue(sys)) or in a 
sub-dialogue (i.e., Dialogue(sys)-I-request(sys) off 
Inform(usr)). 
2. Knowing that  the system can request some- 
thing, the dialogue engine consults with the AD 
for what the system can ask about. The flat 
strucutre (request(us)) represents negotiation of 
the task but since we assume that negotiation of 
the task is complete (i.e., Status(theatrelnfo) = ac- 
tive), this speech act is not interpreted into a prim- 
SThe star ( ')  means that a dialogue may contain 
several request(sys) -I- Inform(usr) sequences. Lower- 
case speech acts are atomic, while others are complex. 
The dialogue in square brackets (\[\]) is optional. 
134 
itive. Next we consider the sub-diaJogue struc- 
ture. Both children of theatrelnfo are open (i.e., 
they have not been negotiatied yet) thus the sys- 
tem randomly chooses to pursue city whose state 
is changed to topic. The speech act and the pa- 
rameter are combined into the primitive request- 
Value(city)--request a value for the parameter city 
(e.g., "In which city is the theatre?"). We chose 
to use the sub-dialogue structure instead of the 
flat strucutre to represent negotiation of parame- 
ter values since they are subordinate to the task 
in the sense that the task dictates which parame- 
ter values are needed. This is also the case for the 
real gammar (Appendix A). 
3. The primitive requestValue(city) is added to 
the dialogue history: 
Dialogue(sys) 
J 
request(sys) I nform(usr) 
requestValue(task) j 
intorm(usr) Dialogue(sys) 
inform Value(task=theatrelnfo) 
request(sys) . 
requestValue(city) 
4. Starting from request(sys), the grammar 
states that inform(usr) (i.e., Inform(usr) + in- 
form(usr)) is the next speech act in the dialogue. 
requestValue(city) was the last primitive spoken. 
Reasoning that a user-inform in response to a sys- 
tem requestValue should involve the same parame- 
ter as the system's requestValue, the information is
combined to form the primitive informValue(city), 
i.e., the user should respond to the system request 
with a value for the parameter city. Let's assume 
that the user replied "Hong Kong", thus the dia- 
logue history is expanded: 
Dialogue(sys) 
request (sys) ., Inform(usr) 
req uestVa lue (task),...-. 
intorm(u.sr).  . o .Dialogue(sys) 
in formValue(task=theatre lnto)  / 
request(sys) . Inform(usr) 
requestValue(city) 
intorm(usr~ 
informValue(city& Hohg Kong) 
5. Starting ~om inform(usr), the grammar re- 
turns reques't(sys) and Dialogue(sys)-t-request(sys). 
Since a recogniton result is available from the pre- 
vious turn, the engine checks its recogution con- 
fidence. If it is high, it would consider the nego- 
tiation of city finished, change its state to closed, 
and discard Dialogue(sys)+request(sys) since there 
is nothing to be requested about a closed param- 
eter. It would translate request(sys) into request- 
Value(theatre) since theatre is the only remaining 
open parameter. 
If confidence is low, the dialogue engine 
may decide to ask the user to confirm 
the recognized value. In which case, Dia- 
Iogue(sys)+request(sys) would be interpreted into 
requestConfirm(city=Hong Kong). Whether re- 
quest(sys) would be interpreted or not depends 
on the dialogue strategies chosen by the service 
designer (see Sections 3 and 4). 
If confidence is extremely low, the dialogue n- 
gine may decide to repeat he question. In which 
case, request(sys) would be interpreted into re- 
questValue(city, 2), while the sub-dialogue struc- 
ture would be discarded. 
6. Any interpretation f the flat strucutre would 
result in the following addition to the last Dia- 
Iogue(sys) in the dialogue history. 
I 
Dialogue(sys) 
request(sys) . Inform(usr) recluest(sys) 
requestValue(city) i 
infgrm(u.sr) . ? 
informValue(city= Hong I~.ong) 
Our example shows how a speech act can result 
in several primitives depending on the context and 
thus how the dialogue manager dynamically reacts 
to external events. Although this brief description 
may not show it, our dialogue manager can handle 
mixed initiative dialogue (Hagen, 1999). In (Ha- 
gen, 1999), we also present our theory of taking, 
keeping, and relinquishing the initiative. 
Heisterkamp and McGlashan (1996) presented 
an approach that uses a similar division of func- 
tionality as we do: task (=application), contex- 
tual (=synsem + pragmatic), and pragmatic in- 
terpretation (=dialogue ngine). They also use 
abstract parameterized units similar to ours, but 
they do not use a speech act grammar to cal- 
culate the units. Rather, they map contex- 
tual functions onto dialogue goals, e.g., the func- 
tion new_for_system(gaalcity:munich) ntroduces 
the dialogue goal confirm(goalcity:munich). In 
terms of our primitievs this could be expressed as 
requestConfirm 0 follows informValue 0. We choose 
not to start our modelling at this level since we 
want to be able to vary what follows informValue0, 
e.g., requestConfirmO, requestValueABCO, or eval- 
uate().  
3 Pr imi t ives  in  Use  
Conceptually, GEN-primitives are calculated first 
and then a bag of possible responses (REC- 
primitives). One dialogue primitive corresponds 
to one information unit or communicative goal, 
135 
GEN-Pr imi t ive  
requestValue(film) 
requestConfi rm 
(theatre=Ridge) 
REC-Primit ives 
informValue(filrn) 
rejectRequest(film) 
withd rawAccept (aTask=hangup) 
withd rawAccept(aTask=cancel) 
inform Positive(theatre=Ridge) 
inform Negative(theatre=Ridge) 
rejectReq uest(theatre=Ridge) 
withdrawAccept(aTask=ha ngup) 
withdrawAccept(aTask=cancel) 
Table 1: REC-primitives calculated in response 
to two GEN-primitives in Dialogue 1. 
e.g., in an information retrieval setting: provid- 
ing or requesting one piece of infi)rmation. Prim- 
itives can be used individually or combined to ac- 
count for more complex dialogue. Whether and 
how they are combined epends on the dialogue 
strategies specified by the service designer. In this 
and the following section, we will examine several 
such strategies and show how the primitives are 
combined to achieve them. 
3.1 Quest ion-Answer Dialogue 
In the simplest case, the service designer wants a 
strickt question-answer dialogue: 4 
Dia logue 1: Question-Answer 
Sys: "Which film do you want to see?" 
req uest:Value(film) 
Usr: "The Matrix. At the Ridge." 
Int: informValue(film= Matrix) 
Sys: "Which theatre?" 
requestValue(theatre) 
Usr: "Ridge. R I D G E." 
Int: "mformValue(theatre=Ridge). ,, 
Sys: "Did you say The Ridge? 
requestConfi rm(theatre= Ridge) 
Usr: ~Yes. R I D G E." 
Int: inform Positive(theatre= Ridge.) 
For this type of dialogue, only the REC- 
primitives representing direct answers, rejects, 
and withdraws are calculated. In Table 1, we 
present those calculated in response to the 
first and the third system turn. We see that, 
after requestValue(film), only iinformValue(film) 
is calculated and the pragmatic', interpreter has 
no chance to detect "At the Ridge" (even if 
synsem parsed it correctly) since there is no in- 
formExtraValue(theatre) available to map it onto. 
Similarly, after requescConfirm(theatre=Ridge) 
only informPositive(theatre=Ridge) and in- 
formNegative(theatre=Ridge) are available and 
"R I D G E" cannot be detected since there is no 
informValueABC(city) primitive present. 
41n the sample dialogues, 'Sys' means system turn, 
'Usr' means user turn, and 'Int' means primitives rec- 
ognized and sent back to the dialogue engine from the 
pragmatic interpreter. 
GEN-Primit ive 
requestValue(film) 
requestConfirm 
(theatre=Ridge) 
PdgC-Primitives 
inforrnValue(film) 
rejectReq uest(film) 
inform ExtraValueValue(time) 
informExtraValue(theatre) 
inforrn Ext raVal ue(city) 
inform ExtraValue(noOfTickets) 
inforrnExtraValue(date) 
withd rawAccept(aTask=v) = 
inform Positive(theatre= Ridge) 
inform Negative(theatre=Ridge) 
rejectReq uest(theatre=Ridge) 
inform Ext raVal ue(ti m e) 
inform ExtraValue(city) 
informExtraValue(noOfTickets) 
inform ExtraValue(date) 
withdrawAccept(aTask=v) a 
=Vv E {cancel, hangup} 
Table 2: REC-primitives calculated in response 
to two GEN-primitives in Dialogue 2. 
3.2 Over -Answer ing  
In our experience, users frequently provide more 
information than explicitly asked for, thus a more 
flexible dialogue strategy would be to allow over- 
answering and Dialogue 1 could have developed as 
follows: 
Dia logue 2: Over-Answering 
Sys: "Which film do you want to see?" 
requestValue(film) 
Usr: =Matrix. At the Ridge. R I D G E." 
Int: informValue(film= Matrix) 
+ informExtraValue(theatre=The Ridge) 
Sys: "Did you say The Ridge?" 
req uestConfi rrn (theatre= Ridge) 
Usr: "Yes, and I want the late show." 
Int: informPositive(theatre=Ridge) 
+ informExtraValue(time=9P M) 
In Table 2, we present he REC-primitives cal- 
culated in response to the same system turns as 
in Dialogue 1. In Dialogue 2, only over-answering 
of requestValue 0 primitives were allowed, thus 
"R I D G E" could still not be accounted for. 
3.3 Complex Mixed Init iative 
Here we consider the most complex dialogue strat- 
egy that we can currently offer: The system is able 
to account for complex mixed initiative dialogue 
(at least from a dialogue point of view), i.e., the 
user can requst clarifications, over-answer, change 
values, repeat values, correct values, spell values, 
and reject requests as she pleases. 
D ia logue 3: Complex Mixed Initiative 
Sys: "Which ~Irn do you want to see?" 
requestValue(film) 
Usr: =Sorry, did you ask for the time?" 
Int: requestParam(time) 
Sys: =No. Which film do you want to see?" 
informNegative(time) + requestValue(film, 2) 
136 
Dia logue 3 cont 'd .  
Usr: "Matrix. At the Ridge." 
Int: informValue(film= M atrix) 
+ informExtraValue(theatre=The Ridge) 
Sys: "Did you say The Ridge?" 
requestConfirm (theatre=Ridge, 1) 
Usr: "Sorry, I didn't hear that." 
Int: rejectReq uest(theatre=didNotHear) 
Sys: "Did you say The Ridge?" 
requestConfirm(theatre= Ridge, 2) 
Usr: "Yes, The Ridge. R I D G E." 
Int: inform Positive(theatre= Ridge) 
+ repeatValue(theatre= Ridge) 
+ informValueABC(theatre=Ridge) 
Sys: "Ok. What time?" 
evaluate(theatre=Ridge) 
+ requestValue(time) 
Usr: "I don't know. What are the alternatives?" 
Int: req uestAIternatives(time) 
Sys: "18:30 or 21:00." 
informAlternative(time=18:30) 
+ informAIternative(time=21:00) 
Usr: "Ok, two tickets for the late show tomorrow." 
Int: evaluate(time=neutral) 
-l.- inform ExtraValue(noOfTickets=2) 
+ informValue(time=21:00) 
+ informExtraValue(date=July 4)
Sys: "Did you say two tickets?" 
req uestConfirm(noOfTickets=2) 
? Usr: "Yes, but I change to the early show." 
Int: inform Positive(noOfTickets=2) 
+ changeValue(time=18:30) 
In Table 3, we present he REC-primitives cal- 
culated in response to two system utterances. 
3.4 Mu l t i - Funct iona l  Turns  
It has been argued that speech act grammars can- 
not be used to describe dialogue since utterances 
can be multi-functional or encode more than one 
speech act; Speech act grammars can typically be 
in only one state at a time, thus they cannot cap- 
ture this phenomenon (Levinson, 1981). In an 
information retrieval setting such situations oc- 
cur, for example, when users disregard the system 
utterance and provide unrelated information or 
when a recogniton mistake occured and the sytem 
asks for confirmation. Instead of answering yes or 
no, users frequently answer with the correct value, 
which implicitly disconfirms the previous value: 
D ia logue 4: Multi-Functional Utterances 
Sys: "How many tickets?" 
req uestVal ue(noOfTickets) 
Usr: "I want tickets for July 4." 
I.ut: reject Request (noOf'l'ickets) 
+ informExtraValue(date-~July 3) 
Sys: "Did you say July 3?" 
requestConfirm (date=July 3) 
Usr: "Tomorrow!" 
Int: informNegative(date=July 3)
-t- correctValue(date=July 4) 
In the first utterance, the user both ignores the 
system utterance and provides ome information. 
In the second one, she negated and correctd the 
system suggestion with a single word. 
GEN-Pr imit ive 
requestValue(film) 
requestConfirm 
(theatre=Ridge) 
REC-Primit ives 
informValue(film) 
informValueABC(film) 
requestAIternatives(film) 
promise(film) 
rejectRequest(film=v) = 
informGarbage(film) 
requestParam(p) b
informExtraValue(p) b 
informValueABC(p) b
repeatValue(p) ~ 
changeValue(p) c
withdrawAccept(aTask=v) d 
inform Positive(theatre---- Ridge) 
repeatValue(theatre=Ridge) 
informNegative(theatre----Ridge) 
correctValue(theatre) 
informValueABC (theatre) 
rejectRequest(theatre=v) a 
inform Garbage(theatre) 
informExtraValue(p) b 
informValueABC(p) b
repeatValue(p) ?
changeValue(p) c
withd rawAccept(aTask=v) 'd 
=Vv E {null, didNotHear} 
~VpE openParams(AD) 
VpE closedParams(AD) 
~Vv E {cancel, hangup} 
Table 3: REC-primitives calculated in response 
to two GEN-primitives in Dialogue 3. 
Since we are not using the speech act grammar 
directly and instead interpret he speech acts into 
a bag of primitives, we can assign as many prim- 
itives to an utterance as necessary and are not 
bound by the states dictated by a grammar. This 
aspect of our approach becomes even more inter- 
esting when the system combines everal primi- 
tives in its utterance (Section 4). 
4 D ia logue  St ra teg ies  
Although, the procedure outlined in Section 2.4, 
only shows how to calculate one primitive per sys- 
tem turn, the approach is, of course, not limited 
to this. The service designer can decide to em- 
ploy mixed initiative dialogue strategies for the 
system utterances as well, for example, requesting 
or confirming several values at once or implicitly 
confirming values. The dialogue strategies for sys- 
tem utterances include choosing nodes in the ap- 
plication description, dealing with speech recogni- 
tion results, or dealing with ambiguous data from 
other knowledge sources. Here we present a few 
examples of how the dialogue manager would com- 
bine hypotheses (for more information see (Hagen, 
2001)). 
137 
4.1 Conf i rmation Strategies 
We illustrate implicit and multiple confirmation, 
i.e., the system realizes requestValue and request- 
Confirm or multiple requestConfirm primitives in 
one utterance: 
Dialogue 5: Confirmation Strategies 
Sys: "Which showing of The Mal;rix do you want?" 
requestValue(time) 
-I- requestConfi rm (film= Matrix) 
Usr: "(No.) Buena Vista!" 
Int: informNegative(film=The Mzttrix) 
+ correct:Value(film=Buena Vista) 
+ reject:Request(time) 
Sys: "Which showing of Buena Vista do you want?" 
requestConfirm(film=Buena 'Vista) 
+ requestValue(time) 
Usr: =The late show. Tomorrow. :~ 
Int: inforrnPositive(film=Buena Vista) 
informValue(time=21:00) 
+ infformExtraValue(date=8 October) 
Sys: "Did you say 21:00 today?" 
requestConflrm (time=21:00) =
requestConfirm(date=October 7) 
Usr: "No. Tomorrow." 
Int: inform Positive(time=21:00) 
-I- informNegative(date= October 7) 
+ correctValue(date=October 8) 
For the first two utterances, the system has a 
recognition result for the parameter film with a 
low recognition score. Consequently, it calculates 
requestConfirm(film=Matrix/Buena Vista). Addi- 
tionally, there are still open parameter nodes in 
the AD, thus the dialogue engine picks one (ei- 
ther at random or if the service designer has 
ordered them, the next one) and calculates a 
requestValue primitive, here requestValue(time). 
If the service designer allows implicit confirma- 
tion, the two primitives are combined and ut- 
tered together in one turn. If the service de. 
signer does not allow implicit confirmation, the 
dialogue engine continues the dialogue with the 
topic that has alread been introduced, i.e., re. 
questConfirm (film= Matrix/Buena Vista). 5 
For its last utterance, the system has two recog- 
nition results with a low recognition score, thus 
for each one of them it calculates a requestConfirm 
primitive. If the service designer, allows multiple 
confirmations, they are combined and realized as 
one utterance. If not, the dialogue ngine chooses 
requestConfirm(time=21:00), since this topic was 
introduces first. If topics are introduced in the 
same utterance, it pickes one at random. 
4.2 AD Based Strategies 
When requesting parameter values from the user, 
the system consults the application description for 
SThis is a conceptual account. In the implemen- 
tation, the requestValue primitive would not be cal- 
cualted at all, if the service designer does not allow 
implicit confirmation. 
open nodes. If there are several open nodes, the 
dialogue manager can decide to keep the initiative 
and produce several primitives, which can be com- 
bined into one turn. If the nodes are joined with 
an or-relation, the text generator would trans- 
late the primitives into an utterance offering al- 
ternative ways of entering the same information. 
For example, "Please tell me the show time or 
early or late show." (requestValue(time) + re- 
questValue(namedTime)). If the nodes are joined 
with an and- or a has-a relation, the text gen- 
erator would translate the primitives into an ut- 
terances requesting several different pieces of in- 
formation. For example, "What is the name of 
the city and the theatre?" (requestValue(city) + 
requestValue(theatre)). 
As seen in the application descriptions there 
may be several ways of acquiring aparticular value 
e.g., date and time in Figure 2. If a parameter 
value is recognized with a low score, the service 
designer can decide whether the system shall con- 
tinue processing the original parameter o  whether 
it shall switch to one of the alternative ones. Thus 
after a bad recognition of date, the system can 
switch strategy and request weekDay and week in- 
stead. 
Which strategies to follow is decided by the ser- 
vice designer through a set of switches in the dia- 
logue strategies specification file (Figure 1). 
5 P ragmat ic  In terpreter  
After synsem interpretation, the user utterance 
must be mapped onto dialogue primitives. A bag 
of REC-primitives is calculated for each user ut- 
terance and the pragmatic interpreter must assure 
that the utterance is mapped onto primitives in 
this bag. There is always a mapping. The reject 
and withdraw primitives are always part of the bag 
thus in the worst case, the user utterance would 
be mapped onto one of these. 
Since primitives in their uninstantiated form are 
application independent, we can develop generic 
rules for this mapping. In other words, the rules 
define how the dialogue strategies presented in 
Section 3 are mapped onto primitives and how we 
account for several primitives per utterance. 
A rule has the form: GEN-Primitives A user ut- 
terance =~- REC-primitives. In Table 4, we present 
two rules for implicit confirmation. The first one 
corresponds tothe first sys/usr pair in Dialogue 5. 
The user responds with a new value vs (Buena 
Vista) for P2 (film) in requestConfirm(p2=v2) 
and thereby disconfirms v2 (Matrix) and re- 
jects the request for a value for Pl (time) in 
requestValue(pl). The second rule corresponds to 
138 
GEN-Primitives 
requestValue(p~) 
V1 < i _< maxi 
req uestConf.(pj =vj ) 
Vl < j < max# 
requestValue(pi) 
V1 < i < maxi 
requestConf.(p# =vj) 
V1 _< j < max# 
I nput  
(no) 
pj~vt~ 
vz ~ vj, 
Vj 
l< j _< 
k ~ ma.xj 
Vi 
l_<i_< 
k _< =axi 
l~EC-Pr imit ives 
informNeg.(pj =v./) 
Y j l< j<k  
correctVal.(pj =vl) 
Y j l< j<k  
informPos. (pj =v# )
W k < j < max j 
rejectRequest(pi) 
V1 < i _< maxl 
informVal.(pi=vi) 
Vi l< i<k  
informPos.(pj =vj) 
Vl _< j < maxj 
rejectRequest(pi) 
Vi k < i < maxi 
Table 4: Mapping of user 
primitives, p=v means that 
value v for param p. 
input onto REC- 
the user provided 
the second sys/usr pair in Dialogue 5. The user 
provides value vx (the late show) for Pl (time) 
in requestValue(pl) and thus confirms v2 (Buena 
Vista) in requestConfirm(p2=v2). For instanti- 
afion of the primitives, see Dialogue 5. Here, 
we only presented two examples. Similar rules 
were developed for all our primitives and dialogue 
strategies (see (Hagen, 2001)). 
One reviewer asked whether we can modify the 
approach such that expectations can be overrid- 
den if there is sufficently good information from 
the synsem module. The short answer is that we 
could (re-)calcnlate the primitives pretending that 
the service designer allowed mixed initiative re- 
gardless of the dialogue strategies actually chosen. 
W~, however, think it is important to give her the 
right to decide. For example, if she has decided 
that over-answering is allowed, informExtraValue0 
primitives for all parameters whose status is still 
open would be calculated and thus there is noth- 
ing to override. If, however, the service designer 
has decided that over-answering is not allowed, we 
assume that she had good reasons for doing that 
and the dialogue manager will not try to overrule 
this decision. 
6 Conclusion 
We have presented some results from our research 
on spoken dialogue management. We concen- 
trated on how to dynamically calculate a collec- 
tion of predictions for how to continue a dialogue 
(dialogue primitives), how to account for differ- 
ent dialogue strategies and utterances with sev- 
eral communicative goals through combinations of
primitives, and how to map the user utterances 
onto primitives. The approach as been imple- 
mented and tested in several prototype systems, 
e.g., horoscope, movie, and telephone rate service 
(Feldes et al, 1998). 
Dialogue grammars have previously been used 
to manage dialogue (Bunt, 1989; Bilange, 1991; 
Traum and Hinkelman, 1992; JSnsson, 1993; Mast 
et al, 1994; Novick and Sutton, 1994; Chino and 
Tsuboi, 1996), but we are not aware of an ap- 
proach where speech acts are translated into a 
collection of primitives with propositional content. 
Previous grammar approaches use the speech acts 
directly or assume a one-to-one correspondence 
between utterance and speech act. 
Through the natural division of the knowledge 
into type and content, we have achieved a flex- 
ible dialogue manager that adapts to users' be- 
haviour. We can take advantage of the predictive 
capabilites of speech act grammars and still be 
able to account for multi-functional utterances. 
We have also demonstrated that our approach 
is flexible: 1. the dialogue engine, the pragmatic 
interpreter, the primitives and the algorithm for 
mapping user utterances onto predictions are ap- 
plication and language independent, which makes 
it easy to reuse our dialogue manager in new ap- 
plications, and 2. the dialogue manager can easily 
account for several types of dialogue, e.g., strict 
question-answer or mixed initiative. We give the 
service designer the freedom to decide which kind 
of dialogue she wants---on a high level--and the 
dialogue manager combines the basic primitives 
accordingly. 
Future work includes empirical testing to ver- 
ify whether we are calculating appropriate predic- 
tions. Also, several aspects of our dialogue gram- 
mar have not yet been translated into primitives, 
for example, the frequent use of assert in natu- 
ral dialogue. As a wider dialogue coverage is re- 
quired, we will add primitives accordingly. We are 
also working on using the primities as input to a 
multi-lingual automatic text generation system. 
Acknowledgements 
The author thanks the three anonymous reviewers 
for their helpful comments on the first draft of 
this paper. Financial support from the Norwegian 
Research Council, project number 116578/410 is
greatly appreciated. 
References 
J.A. Bateman, B. Magnini, and F. Rinaldi. 1994. 
The generalized {Italian, German, English} up- 
per model. In Proe. of the ECAI9J Workshop: 
Comparison of Implemented Ontologies, Ams- 
terdam, The Netherlands. 
139 
J.A. Bateman, B. Maguini, and G. Fabris. 
1995. The generalized upper model knowledge 
base: Organization and use. In Proc. of the 
Conf. on Knowledge Representation a d Shar- 
ing, Twente, The Netherlands. 
E- Bilange. 1991. A task independent oral dia- 
logue model. In Proc. of the Euro. Conf. of the 
ACL, pages 83-87. 
H.C. Bunt. 1989. Information dialogues as com- 
municative action in relation to. partner model- 
ing and information processing. In M.M. Tay- 
lor, F. Neel, and D.G. BouwhLfis, editors, The 
Structure of Multimodal Dialogue, pages 47-73. 
North-Holland, Amsterdam. 
J. Caminero-Gil, J. Alvarez-Cercadillo, C. Crespo- 
Casas, and D. Tapias-Merino. 1996. Data- 
driven discourse modeling for semantic in- 
terpretation. In Proe. of 1996 Intl. Conf. 
on Acoustics, Speech, and Signal Processing 
(ICASSP'96), pages 401-404. 
T. Chino and H. Tsuboi. 1996. A new discourse 
model for spontaneous spoken dialogue. In 
1021-1024, editor, Proc. of the 1996 Intl. Conf. 
on Spoken Language Processing {ICSLP'96). 
S. Feldes, G. Fries, E. Hagen, and A. Wirth. 1998. 
A novel service creation enviromnent for speech 
enabled database access. In Proc. ~th IEEE 
Workshop on Interactive Voice Technology for 
Telecommunications Applications (IVTTA '98), 
29-30 Sept. 1998, Torino, Italy. 
E. Hagen. 1999. An approach to mixed ini- 
: tiative spoken information retrieval dialogue. 
User Modeling and User-Adapted Interaction, 
9(1/2):167-213. 
E. Hagen. 2001. Mixed Initiative Spoken Dialogue 
Management in Information Systems. Ph.D. 
thesis, School of Computing Science, Simon 
Fraser University, Burnaby, BC, Canada. Jan. 
2001 expected. 
P. Heisterkamp and S. McGlashan. 1996. Units 
of dialogue management. In Proc. of the 1996 
Intl. Conf. on Spoken Language Processing (IC- 
SLP'96). 
A. JSnsson. 1993. A dialogue manager using 
initiative-response units and distributed con- 
trol. In Proc. of 6th Euro. Conf. of the A CL, 
pages 233-238. 
S.C. Levinson. 1981. Some pre-observations on
the modelling of dialogue. Discourse Processes, 
4:93-116. 
M. Mast, F. Kummert, U. Ehrlich, G.A. Fink, 
T. Kuhn, H. Niemann, and G. Sagerer. 1994. 
Prosody takes over: Towards a prosodically 
guided dialog system. Speech Communication, 
15(1-2):155-167. 
D.G. Novick and S. Sutton. 1994. An empirical 
model of acknowledgement for spoken-language 
systems. In Proc. of the 32nd Annual Meeting 
of the ACL, pages 96-101. 
S. Sitter and A. Stein. 1992. Modelling the il- 
locutionary aspects of information-seeking di-
alogues. Information Processing and Manage- 
ment, 8(2):165-180. 
D. 'I~aum and E. Hinkelman. 1992. Conversation 
acts in task-oriented spoken dialogue. Compu- 
tational Intelligence, 8(3):575-599. 
S. Young, A. Hauptmann, W. Ward, E. Smith, 
and P. Werner. 1990. High-level knowledge 
sources in usable speech recognition systems. 
In A. Walbel and K. Lee, editors, Readings 
in Speech Recognition, pages 538-549. Morgan 
Kaufman, San Mateo, CA. 
A .  The  Complete  D ia logue  Grammar  
The complete grammar is a slightly modified 
version of the grammar presented in (Sitter and 
Stein, 1992). Notat ion:  Complex dialogue 
moves begin with an upper case (e.g., Request); 
atom/c dialogue acts are all lower case (e.g., re- 
quest). S and K mean seeker and knower. Square 
brackets (\[\]) mean optional. X + means one or 
more instances of X. All moves except Inform and 
Assert are representated bythe abstraction Move. 
Subscript i means that move and act must be of 
the same type, e.g., Request and request. 
Dialogue(S) --+ (Cycle(S)) + 
Cycle(S) -). Request(S), Promise(K), Inform(K), 
Evaluate(S). 
Cycle(S) ~ Request(S), \[Promise(K)\], 
WithdrawRequest(S). 
Cycle(S) ~ Request(S), Promise(K), 
WithdrawPromise(K). 
Cycle(S) ~ Request(S), RejectRequest(K). 
Cycle(S) ~ Offer(K), Accept(S), Inform(K), Evaluate(S), 
Cycle(S) ~ Offer(K), \[Accept(S)\], WithdrawOffer(K). 
Cycle(S) ---y Offer(K), Accept(S), WithdrawAccept(S). 
Cycle(S) -+ Offer(K), RejectOffer(S). 
Cycle(S) -+ Withdraw(usr). 
Cycle(S) ~ Withdraw(system). 
inform(K) .-.). inform(K), \[Dialogue(S)\]. 
Assert(S/K) ,-.), assert(S/K), \[Dialogue(K/S)\]. 
Movei(S/K). 
Movei(S/K) ~ act~(S/K), \[Dialogue(K/S)\]. 
Movei(S/K) ~ acti(S/K), \[Assert(S/K)\]. 
Mov~ (S/K) -+ Dialogue(K/S). 
Moves(S/K) ~ Assert(S/K), \[acti(S/K)\]. 
Move~(S/K) --~. Assert(S/K), \[Dialogue(K/S)\]. 
140 
Proceedings of the Second Workshop on Statistical Machine Translation, pages 17?24,
Prague, June 2007. c?2007 Association for Computational Linguistics
Integration of an Arabic Transliteration Module into a Statistical 
Machine Translation System
Mehdi M. Kashani+, Eric Joanis++, Roland Kuhn++, George Foster++, Fred Popowich+
+ School of Computing Science
Simon Fraser University
8888 University Drive
Burnaby, BC V5A 1S6, Canada
mmostafa@sfu.ca
 popowich@sfu.ca
++ NRC Institute for Information Technology
101 St-Jean-Bosco Street 
Gatineau, QC K1A 0R6, Canada
firstname.lastname@cnrc-nrc.gc.ca
Abstract
We provide an in-depth analysis of the in-
tegration of an Arabic-to-English translit-
eration system into a general-purpose 
phrase-based statistical machine translation 
system. We study the integration from dif-
ferent aspects and evaluate the improve-
ment that can be attributed to the integra-
tion using the BLEU metric. Our experi-
ments show that a transliteration module 
can help significantly in the situation where 
the test data is rich with previously unseen 
named entities. We obtain 70% and 53% of 
the theoretical maximum improvement we 
could achieve, as measured by an oracle on 
development and test sets respectively for 
OOV words (out of vocabulary source 
words not appearing in the phrase table).
1 Introduction
Transliteration is the practice of transcribing a 
word or text written in one writing system into an-
other writing system. The most frequent candidates 
for transliteration are person names, locations, or-
ganizations and imported words. The lack of a 
fully comprehensive bilingual dictionary including 
the entries for all named entities (NEs) renders the 
task of transliteration necessary for certain natural 
language processing applications dealing with 
named entities. Two applications where translitera-
tion can be particularly useful are machine transla-
tion (MT) and cross lingual information retrieval. 
While transliteration itself is a relatively well-
studied problem, its effect on the aforementioned 
applications is still under investigation.
Transliteration as a self-contained task has its 
own challenges, but applying it to a real applica-
tion introduces new challenges. In this paper we 
analyze the efficacy of integrating a transliteration 
module into a real MT system and evaluate the 
performance.
When working on a limited domain, given a suf-
ficiently large amount of training data, almost all 
of the words in the unseen data (in the same do-
main) will have appeared in the training corpus. 
But this argument does not hold for NEs, because 
no matter how big the training corpus is, there will 
always be unseen names of people and locations. 
Current MT systems either leave such unknown 
names as they are in the final target text or remove 
them in order to obtain a better evaluation score. 
None of these methods can give the reader who is 
not familiar with the source language any informa-
tion about those out-of-vocabulary (OOV) words, 
especially when the source and target languages 
use different scripts. If these words are not names, 
one can usually guess what they are, by using the 
partial information of other parts of speech. But, in 
the case of names, there is no way to determine the 
individual or location the sentence is talking about. 
So, to improve the usability of a translation, it is 
particularly important to handle NEs well.
The importance of NEs is not yet reflected in the 
evaluation methods used in the MT community, 
the most common of which is the BLEU metric. 
BLEU (Papineni et al 2002) was devised to pro-
vide automatic evaluation of MT output. In this 
metric n-gram similarity of the MT output is com-
puted with one or more references made by human 
17
translators. BLEU does not distinguish between 
different words and gives equal weight to all. In 
this paper, we base our evaluation on the BLEU 
metric and show that using transliteration has im-
pact on it (and in some cases significant impact). 
However, we believe that such integration is more 
important for practical uses of MT than BLEU in-
dicates.
Other than improving readability and raising the 
BLEU score, another advantage of using a translit-
eration system is that having the right translation 
for a name helps the language model select a better 
ordering for other words. For example, our phrase 
table1 does not have any entry for ?????? (Dulles) 
and when running MT system on the plain Arabic 
text we get
and this trip was cancelled [?] by the american 
authorities responsible for security at the airport 
???? .
We ran our MT system twice, once by suggest-
ing ?dallas? and another time ?dulles? as English 
equivalents for ?????? and the decoder generated 
the following sentences, respectively:
and this trip was cancelled [?] by the american 
authorities responsible for security at the airport 
at dallas .
and this trip was cancelled [?] by the american 
authorities responsible for security at dulles air-
port .2
Every statistical MT (SMT) system assigns a 
probability distribution to the words that are seen 
in its parallel training data, including proper names. 
The richer the training data, the higher the chance 
for a given name in the test data to be found in the 
translation tables. In other words, an MT system 
with a relatively rich phrase table is able to trans-
late many of the common names in the test data, 
with all the remaining words being rare and foreign. 
So unlike a self-contained transliteration module, 
which typically deals with a mix of ?easy? and 
                                                
1 A table where the conditional probabilities of target 
phrases given source phrases (and vice versa) is kept.
2 Note that the language model can be trained on more 
text, and hence can know more NEs than the translation 
model does.
?hard? names, the primary use for a transliteration 
module embedded in an SMT system will be to 
deal with the ?hard? names left over after the 
phrase tables have provided translations for the 
?easy? ones. That means that when measuring the 
performance improvements caused by embedding 
a transliteration module in an MT system, one 
must keep in mind that such improvements are dif-
ficult to attain: they are won mainly by correctly 
transliterating ?hard? names. 
Another issue with OOV words is that some of 
them remained untranslated due to misspellings in 
the source text. For example, we encountered 
??????? (?Hthearow?) instead of ??????? 
(?Heathrow?) or ??????? (?Brezer?) instead of 
??????? (?Bremer?) in our development test set. 
Also, evaluation by BLEU (or a similar auto-
matic metric) is problematic. Almost all of the MT 
evaluations use one or more reference translations 
as the gold standard and, using some metrics, they 
give a score to the MT output. The problem with 
NEs is that they usually have more than a single 
equivalent in the target language (especially if they 
don't originally come from the target language) 
which may or may not have been captured in the 
gold standard. So even if the transliteration module 
comes up with a correct interpretation of a name it 
might not receive credit as far as the limited num-
ber of correct names in the references are con-
cerned.
Our first impression was that having more inter-
pretations for a name in the references would raise 
the transliteration module?s chance to generate at 
least one of them, hence improving the perform-
ance. But, in practice, when references do not 
agree on a name?s transliteration that is the sign of 
an ambiguity. In these cases, the transliteration 
module often suggests a correct transliteration that 
the decoder outputs correctly, but which fails to 
receive credit from the BLEU metric because this 
transliteration is not found in the references. As an 
example, for the name ?????????, four references 
came up with four different interpretations: 
swerios, swiriyus, severius, sweires. A quick query 
in Google showed us another four acceptable in-
terpretations (severios, sewerios, sweirios, saw-
erios).
Machine transliteration has been an active re-
search field for quite a while (Al-Onaizan and 
Knight, 2002; AbdulJaleel and Larkey, 2003; Kle-
mentiev and Roth, 2006; Sproat et al 2006) but to 
18
our knowledge there is little published work on 
evaluating transliteration within a real MT system.
The closest work to ours is described in (Hassan 
and Sorensen, 2005) where they have a list of 
names in Arabic and feed this list as the input text 
to their MT system. They evaluate their system in 
three different cases: as a word-based NE transla-
tion, phrase-based NE translation and in presence 
of a transliteration module. Then, they report the 
BLEU score on the final output. Since their text is 
comprised of only NEs, the BLEU increase is quite 
high. Combining all three models, they get a 24.9 
BLEU point increase over the na?ve baseline. The 
difference they report between their best method 
without transliteration and the one including trans-
literation is 8.12 BLEU points for person names 
(their best increase).
In section 2, we introduce different methods for 
incorporating a transliteration module into an MT 
system and justify our choice. In section 3, the 
transliteration module is briefly introduced and we 
explain how we prepared its output for use by the 
MT system. In section 4, an evaluation of the inte-
gration is provided. Finally, section 5 concludes 
the paper.
2 Our Approach
Before going into details of our approach, an 
overview of Portage (Sadat et al 2005), the 
machine translation system that we used for our 
experiments and some of its properties should be 
provided.
Portage is a statistical phrase-based SMT system 
similar to Pharaoh (Koehn et al 2003).  Given a 
source sentence, it tries to find the target sentence 
that maximizes the joint probability of a target sen-
tence and a phrase alignment according to a loglin-
ear model. Features in the loglinear model consist 
of a phrase-based translation model with relative-
frequency and lexical probability estimates; a 4-
gram language model using Kneser-Ney smooth-
ing, trained with the SRILM toolkit; a single-
parameter distortion penalty on phrase reordering; 
and a word-length penalty. Weights on the loglin-
ear features are set using Och's algorithm (Och, 
2003) to maximize the system's BLEU score on a 
development corpus. To generate phrase pairs from 
a parallel corpus, we use the "diag-and" phrase 
induction algorithm described in (Koehn et al 
2003), with symmetrized word alignments gener-
ated using IBM model 2 (Brown et al 1993).
Portage allows the use of SGML-like markup 
for arbitrary entities within the input text. The 
markup can be used to specify translations 
provided by external sources for the entities, such 
as rule-based translations of numbers and dates, or 
a transliteration module for OOVs in our work. 
Many SMT systems have this capability, so 
although the details given here pertain to Portage, 
the techniques described can be used in many 
different SMT systems.
As an example, suppose we already have two 
different transliterations with their probabilities for 
the Arabic name ??????. We can replace every 
occurrence of the ?????? in the Arabic input text 
with the following:
<NAME target="mohammed|mohamed"
prob=".7|.3"> ???? </NAME>
By running Portage on this marked up text, the 
decoder chooses between entries in its own phrase 
table and the marked-up text. One thing that is 
important for our task is that if the entry cannot be 
found in Portage?s phrase tables, it is guaranteed 
that one of the candidates inside the markup will 
be chosen. Even if none of the candidates exist in 
the language model, the decoder still picks one of 
them, because the system assigns a small arbitrary 
probability (we typically use e-18) as unigram 
probability of each unseen word.
We considered four different methods for 
incorporating the transliteration module into the 
MT system. The first and second methods need an 
NE tagger and the other two do not require any 
external tools.
Method 1: use an NE tagger to extract the 
names in the Arabic input text. Then, run the 
transliteration module on them and assign 
probabilities to top candidates. Use the markup 
capability of Portage and replace each name in the 
Arabic text with the SGML-like tag including 
different probabilities for different candidates. 
Feed the marked-up text to Portage to translate.
Method 2: similar to method 1 but instead of 
using the marked-up text, a new phrase table, only 
containing entries for the names in the Arabic input 
text is built and added to Portage?s existing phrase 
tables. A weight is given to this phrase table and 
19
then the decoder uses this phrase table as well as 
its own phrase tables to decide which translation to 
choose when encountering the names in the 
text.  The main difference between methods 1 and 
2 is that in our system, method 2 allows for a bleu-
optimal weight to be learned for the NE phrase 
table, whereas the weight on the rules for method 1 
has to be set by hand.
Method 3: run Portage on the plain Arabic text. 
Extract all untranslated Arabic OOVs and run the 
transliteration module on them. Replace them with 
the top candidate.
Method 4: run Portage on the plain Arabic text. 
Extract all untranslated Arabic OOVs and run the 
transliteration module on them. Replace them with 
SGML-like tags including different probabilities 
for different candidates, as described previously. 
Feed the marked-up text to Portage to translate.
The first two methods need a powerful NE 
tagger with a high recall value. We computed the 
recall value on the development set OOVs using 
two different NE taggers, Tagger A and Tagger B 
(each from a different research group). Taggers A 
and B showed a recall of 33% and 53% respec-
tively, both being low for our purposes. Another 
issue with these two methods is that for many of 
the names the transliteration module will compete 
with the internal phrase table. Our observations 
show that if a name exists in the phrase table, it is 
likely to be translated correctly. In general, 
observed parallel data (i.e. training data) should be 
a more reliable source of information than 
transliteration, encouraging us to use transliteration 
most appropriately as a ?back-off? method. In a 
few cases, the Arabic name is ambiguous with a 
common word and is mistakenly translated as such. 
For example, ????? ??? ???? is an Arabic name that 
should be transliterated as ?Hani Abu Nahl? but 
since ????? also means ?solve?, the MT system 
outputs ?Hani Abu Solve?. The advantage of the 
first two methods is that they can deal with such 
cases. But considering the noise in the NE 
detectors, handling them increases the risk of 
losing already correct translations of other names.
The third method is simple and easy to use but 
not optimal: it does not take advantage of the 
decoder?s internal features (notably the language 
models) and only picks up the highest scoring 
candidate from the transliteration module.
The fourth method only deals with those words 
that the MT system was unable to deal with and 
had to leave untranslated in the final text. 
Therefore whatever suggestions the transliteration 
module makes do not need to compete with the 
internal phrase tables, which is good because we 
expect the phrase tables to be a more reliable 
source of information. It is guaranteed that the 
translation quality will be improved (in the worst 
case, a bad transliteration is still more informative 
than the original word in Arabic script). Moreover, 
unlike the third method, we take advantage of all 
internal decoder features on the second pass. We 
adopt the fourth method for our experiment. The 
following example better illustrates how this 
approach works:
Example: Suppose we have the following sentence 
in the Arabic input text: 
???? ???? ????? ????? ???????.
Portage is run on the Arabic plain text and yields 
the following output:
blair accepts ????? report in full .
The Arabic word ??????? (Hutton) is extracted and 
fed to the transliteration module. The 
transliteration module comes up with some English 
candidates, each with different probabilities as 
estimated by the HMM. They are rescaled (as will 
be explained in section 3) and the following 
markup text will be generated to replace the 
untranslated ??????? in the first plain Arabic 
sentence:
<NAME target="hoton|hutton|authon" 
prob="0.1|0.00028|4.64e-05">?????</NAME> 
Portage is then run on this newly marked up text 
(second pass). From now on, with the additional 
guidance of the language models, it is the 
decoder?s task to decide between different markup 
suggestions. For the above example, the following 
output will be generated:
blair accepts hutton report in full .
20
3 Transliteration System
In this section we provide a brief overview of the 
embedded transliteration system we used for our 
experiment. For the full description refer to 
(Kashani et al 2007).
3.1 Three Phase Transliteration
The transliteration module follows the noisy 
channel framework. The adapted spelling-based 
generative model is similar to (Al-Onaizan and 
Knight, 2002). It consists of three consecutive 
phases, the first two using HMMs and the Viterbi 
algorithm, and the third using a number of 
monolingual dictionaries to match the close entries 
or to filter out some invalid candidates from the 
first two phases.
Since in Arabic, the diacritics are usually 
omitted in writing, a name like ?????? (Mohamed) 
would have an equivalent like ?mhmd? if we only 
take into account the written letters. To address 
this issue, we run Viterbi in two different passes 
(each called a phase), using HMMs trained on data 
prepared in different ways.
In phase 1, the system tries to find the best 
transliterations of the written word, without caring 
about what the hidden diacritics would be (in our 
example, mhmd).
In phase 2, given the Arabic input and the output 
candidates from phase 1, the system fills in the 
possible blanks in between using the character-
based language model (yielding ?mohamed? as a 
possible output, among others).
To prepare the character-level translation model 
for both phases we adopted an approach similar to 
(AbdulJaleel and Larkey, 2003).
In phase 3, the Google unigram model 
(LDC2006T13 from the LDC catalog) is first used 
to filter out the noise (i.e. those candidates that do 
not exist in the Google unigram are removed from 
the candidate list). Then a combination of some 
monolingual dictionaries of person names is used 
to find close matches between their entries and the 
HMM output candidates based on the Levenshtein 
distance metric.
3.2 Task-specific Changes to the Module
Due to the nature of the task at hand and by 
observing the development test set and its 
references, the following major changes became 
necessary:
Removing Part of Phase Three: By observing the 
OOV words in the development test set, we 
realized that having the monolingual dictionary in 
the pipeline and using the Levensthtein distance as 
a metric for adding the closest dictionary entries to 
the final output, does not help much, mainly 
because OOVs are rarely in the dictionary. So, the 
dictionary part not only slows down the execution 
but would also add noise to the final output (by 
adding some entries that probably are not the 
desired outputs). However, we kept the Google 
unigram filtering in the pipeline.
Rescaling HMM Probabilities: Although the 
transliteration module outputs HMM probability 
score for each candidate, and the MT system also 
uses probability scores, in practice the translitera-
tion scores have to be adjusted.  For example, if 
three consecutive candidates have log probabilities 
-40, -42 and -50, the decoder should be given val-
ues with similar differences in scale, comparable 
with the typical differences in its internal features 
(eg. Language Models). Knowing that the entries 
in the internal features usually have exponential 
differences, we adopted the following conversion 
formula:
p'i = 0.1*(pi/pmax)?
Equation 1
where pi = 10(output of HMM for candidate i) and max is the 
best candidate.
We rescale the HMM probability so that the top 
candidate is (arbitrarily) given a probability of p'max
= 0.1.  It immediately follows that the rescaled 
score would be 0.1 * pi / pmax.  Since the decoder
combines its models in a log-linear fashion, we 
apply an exponent ? to the HMM probabilities be-
fore scaling them, as way to control the weight of 
those probabilities in decoding.  This yields equa-
tion 1.  Ideally, we would like the weight ? to be 
optimized the same way other decoder weights are 
optimized, but our decoder does not support this 
yet, so for this work we arbitrarily set the weight to 
? = 0.2, which seems to work well. For the above 
example, the distribution would be 0.1, 0.039 and 0.001.
21
Prefix Detachment: Arabic is a morphologically 
rich language. Even after performing tokenization, 
some words still remain untokenized. If the 
composite word is frequent, there is a chance that it 
exists in the phrase table but many times it does 
not, especially if the main part of that word is a 
named entity. We did not want to delve into the 
details of morphology: we only considered two 
frequent prefixes: ??? (?va? meaning ?and?) and 
???? (?al? determiner in Arabic). If a word starts 
with either of these two prefixes, we detach them 
and run the transliteration module once on the 
detached name and a second time on the whole 
word. The output candidates are merged 
automatically based on their scores, and the 
decoder decides which one to choose.
Keeping the Top 5 HMM Candidates: The 
transliteration module uses the Google unigram 
model to filter out the candidate words that do not 
appear above a certain threshold (200 times) on the 
Internet. This helps eliminate hundreds of 
unwanted sequences of letters. But, we decided to 
keep top-5 candidates on the output list, even if 
they are rejected by the Google unigram model 
because sometimes the transliteration module is
unable to suggest the correct equivalent or in other 
cases the OOV should actually be translated rather 
than transliterated 3 . In these cases, the closest 
literal transliteration will still provide the end user 
more information about the entity than the word in 
Arabic script would.
4 Evaluation
Although there are metrics that directly address NE 
translation performance4, we chose to use BLEU 
because our purpose is to assess NE translation 
within MT, and BLEU is currently the standard 
metric for MT.
                                                
3 This would happen especially for ancient names or 
some names that underwent sophisticated morphologi-
cal transformations (For example, Abraham in English 
and ??????? (Ibrahim) in Arabic).
4 NIST?s NE translation task 
(http://www.nist.gov/speech/tests/ace/index.htm) is an 
example.
4.1 Training Data
We used the data made available for the 2006 
NIST Machine Translation Evaluation. Our bilin-
gual training corpus consisted of 4M sentence pairs
drawn mostly from newswire and UN domains. 
We trained one language model on the English half 
of this corpus (137M running words), and another 
on the English Gigaword corpus (2.3G running 
words). For tuning feature weights, we used LDC's 
"multiple translation part 1" corpus, which contains 
1,043 sentence pairs. 
4.2 Test Data
We used the NIST MT04 evaluation set and the 
NIST MT05 evaluation set as our development and 
blind test sets. The development test set consists of 
1353 sentences, 233 of which contain OOVs. 
Among them 100 sentences have OOVs that are 
actually named entities. The blind test set consists 
of 1056 sentences, 189 of them having OOVs and 
131 of them having OOV named entities. The 
number of sentences for each experiment is 
summarized in table 1.
Whole Text OOV 
Sentences
OOV-NE 
Sentences
Dev test set 1353 233 100
Blind test set 1056 189 131
Table 1: Distribution of sentences in test sets.
4.3 Results
As the baseline, we ran the Portage without the 
transliteration module on development and blind 
test sets. The second column of table 2 shows 
baseline BLEU scores. We applied method 4 as 
outlined in section 2 and computed the BLEU 
score, also in order to compare the results we 
implemented method 3 on the same test sets. The 
BLEU scores obtained from methods 3 and 4 are 
shown in columns 3 and 4 of table 2.
baseline Method 3 Method 4 Oracle
Dev 44.67 44.71 44.83 44.90
Blind 48.56 48.62 48.80 49.01
Table 2: BLEU score on different test sets.
Considering the fact that only a small portion of 
the test set has out-of-vocabulary named entities, 
22
we computed the BLEU score on two different 
sub-portions of the test set: first, on the sentences 
with OOVs; second, only on the sentences 
containing OOV named entities. The BLEU 
increase on different portions of the test set is 
shown in table 3.
baseline Method 4
Dev OOV sentences 39.17 40.02
OOV-NE Sentences 44.56 46.31
blind OOV sentences 43.93 45.07
OOV-NE Sentences 42.32 44.87
Table 3: BLEU score on different 
portions of the test sets.
To set an upper bound on how much applying 
any transliteration module can contribute to the 
overall results, we developed an oracle-like 
dictionary for the OOVs in the test sets, which was 
then used to create a markup Arabic text. By 
feeding this markup input to the MT system we 
obtained the result shown in column 5 of table 2. 
This is the performance our system would achieve 
if it had perfect accuracy in transliteration, 
including correctly guessing what errors the human 
translators made in the references.  Method 4 
achieves 70% of this maximum gain on dev, and 
53% on blind.
5 Conclusion
This paper has described the integration of a trans-
literation module into a state-of-the-art statistical 
machine translation (SMT) system for the Arabic 
to English task. The final version of the translitera-
tion module operates in three phases. First, it gen-
erates English letter sequences corresponding to 
the Arabic letter sequence; for the typical case 
where the Arabic omits diacritics, this often means 
that the English letter sequence is incomplete (e.g., 
vowels are often missing). In the next phase, the 
module tries to guess the missing English letters. 
In the third phase, the module uses a huge collec-
tion of English unigrams to filter out improbable or 
impossible English words and names. We de-
scribed four possible methods for integrating this
module in an SMT system. Two of these methods 
require NE taggers of higher quality than those 
available to us, and were not explored experimen-
tally. Method 3 inserts the top-scoring candidate 
from the transliteration module in the translation 
wherever there was an Arabic OOV in the source. 
Method 4 outputs multiple candidates from the
transliteration module, each with a score; the SMT 
system combines these scores with language model 
scores to decide which candidate will be chosen. In 
our experiments, Method 4 consistently outper-
formed Model 3. Note that although we used 
BLEU as the metric for all experiments in this pa-
per, BLEU greatly understates the importance of
accurate transliteration for many practical SMT 
applications.
References
Nasreen AbdulJaleel and Leah S. Larkey, 2003. Statisti-
cal Transliteration for English-Arabic Cross Lan-
guage Information Retrieval, Proceedings of the 
Twelfth International Conference on Information and 
Knowledge Management, New Orleans, LA
Yaser Al-Onaizan and Kevin Knight, 2002. Machine 
Transliteration of Names in Arabic Text, Proceedings 
of the ACL Workshop on Computational Approaches 
to Semitic Languages 
Peter F. Brown, Vincent J. Della Pietra, Stephen A. 
Della Pietra, and Robert L. Mercer, 1993. The 
Mathematics of Statistical Machine Translation: Pa-
rameter Estimation, Computational Linguistics
Hany Hassan and Jeffrey Sorensen, 2005. An Integrated 
Approach for Arabic-English Named Entity Transla-
tion, Proceedings of the ACL Workshop on Compu-
tational Approaches to Semitic Languages (ACL), 
University of Michigan, Ann Arbor
Mehdi M. Kashani, Fred Popowich, and Anoop Sarkar, 
2007. Automatic Transliteration of Proper Nouns 
from Arabic to English, Proceedings of the Second 
Workshop on Computational Approaches to Arabic 
Script-based Languages
Alexandre Klementiev and Dan Roth, 2006. Named 
Entity Transliteration and Discovery from Multilin-
gual Comparable Corpora, COLING-ACL, Sidney, 
Australia
Philipp Koehn, Franz Josef Och, and Daniel Marcu, 
2003. Statistical Phrase-based Translation, In Pro-
ceedings of HLT-NAACL, Edmonton, Canada
Franz Josef Och, 2003. Minimum Error Rate Training 
for Statistical Machine Translation, In Proceedings 
of the 41th Annual Meeting of the Association for 
Computation Linguistics, Sapporo
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu, 2002. BLEU: a Method for Automatic 
Evaluation of Machine Translation. In Proceedings 
23
of the 40th Annual Conference of the Association for 
Computational Linguistics (ACL), Philadelphia, PA
Fatiha Sadat, Howard Johnson, Akakpo Agbago, 
George Foster, Roland Kuhn, Aaron Tikuisis, 2005. 
Portage: A Phrase-base Machine Translation System.
In Proceedings of the ACL Workshop on Building 
and Using Parallel Texts, Ann Arbor, Michigan
Richard Sproat, Tao Tao, and ChengXiang Zhai, 2006, 
Named Entity Transliteration with Comparable Cor-
pora, COLING-ACL, Sidney, Australia
24
Pre-processing Closed Captions for Machine Translation 
Dav ide  Turcato  Fred Popowich  Paul  McFet r idge  
Dev lan  N icho lson  Jan ine  Too le  
Natural Language Laboratory, School of Computing Science, Simon Fraser University 
8888 University Drive, Burnaby, British Columbia, V5A 1S6, Canada 
and 
gavagai Technology Inc. 
P.O. 374, 3495 Cambie Street, Vancouver, British Columbia, V5Z 4R3, Canada 
{turk, popowich, mcfet, devl an, toole}?cs, sfu. ca 
Abst rac t  
We describe an approach to Machine Transla- 
tion of transcribed speech, as found in closed 
captions. We discuss how the colloquial nature 
and input format peculiarities of closed captions 
are dealt with in a pre-processing pipeline that 
prepares the input for effective processing by 
a core MT system. In particular, we describe 
components for proper name recognition and 
input segmentation. We evaluate the contribu- 
tion of such modules to the system performance. 
The described methods have been implemented 
on an MT system for translating English closed 
captions to Spanish and Portuguese. 
1 In t roduct ion  
Machine Translation (MT) technology can be 
embedded in a device to perform real time 
translation of closed captions included in TV 
signals. While speed is one factor associated 
with the construction of such a device, another 
factor is the language type and format. The 
challenges posed by closed captions to MT can 
be attributed to three distinct characteristics: 
Firstly, closed captions are transcribed 
speech. Although closed captions are not a com- 
pletely faithful transcription of TV programs, 
they render spoken language and therefore the 
language used is typically colloquial (Nyberg 
and Mitamura, 1997). They contain many of 
the phenomena which characterize spoken lan- 
guage: interjections, repetitions, tuttering, el- 
lipsis, interruptions, hesitations. Linguistically 
and stylistically they differ from written lan- 
guage: sentences are shorter and poorly struc- 
tured, and contain idiomatic expressions, un- 
grammaticality, etc. The associated ifficulties 
stem from the inherently colloquial nature of 
closed captions, and, to different degrees, of 
all forms of transcribed speech (Hindle, 1983). 
Such difficulties require a different approach 
than is taken for written documents. 
Secondly, closed captions come in a specific 
format, which poses problems for their optimal 
processing. Closed-captioners may often split 
a single utterance between two screens, if the 
character limit for a screen has been exceeded. 
The split is based on consideration about string 
length, rather than linguistic considerations, 
hence it can happen at non-constituent bound- 
aries (see Table 1), thus making the real time 
processing of the separate segments problem- 
atic. Another problem is that captions have no 
upper/lower case distinction. This poses chal- 
lenges for proper name recognition since names 
cannot be identified by an initial capital. Addi- 
tionally, we cannot rely on the initial uppercase 
letter to identify a sentence initial word. This 
problematic aspect sets the domain of closed 
captions apart from most text-to-text MT do- 
mains, making it more akin, in this respect, to 
speech translation systems. Although, from a 
technical point of view, such input format char- 
acteristics could be amended, most likely they 
are not under a developer's control, hence they 
have to be presumed. 
Thirdly, closed captions are used under oper- 
ational constraints. Users have no control over 
the speed of the image or caption flow so (s)he 
must comprehend the caption in the limited 
time that the caption appears on the screen. 
Accordingly, the translation of closed captions 
is a "time-constrained" application, where the 
user has limited time to comprehend the system 
output. Hence, an MT system should produce 
translations comprehensible within the limited 
time available to the viewer. 
In this paper we focus on the first two fac- 
tors, as the third has been discussed in (Toole 
et al, 1998). We discuss how such domain- 
38 
good evening, i'm jim lehrer. 
on the "newshour" tonight, four members of congress debate the 
u.n. deal with iraq; paul solman tells the troubled story of 
indonesia's currency; mark 
shields and paul gigot analyze the political week; 
and elizabeth farnsworth explains how the universe is getting 
larger. 
Table 1: Closed caption script fragment. 
dependent, problematic factors are dealt with 
in a pre-processing pipeline that prepares the 
input for processing by a core MT system. The 
described methods have been implemented for 
an MT system that translates English closed 
captions to Spanish and Portuguese. All the 
examples here refer to the Spanish module. 
2 P re -process ing  des ign  
Input pre-processing is essential in an embedded 
real time system, in order to simplify the core 
processing and make it both time- and memory- 
effective. In addition to this, we followed the 
guideline of separating domain-dependent pro- 
cesses and resources from general purpose ones. 
On the one hand, grammars and lexicons are 
costly resources. It would be desirable for them 
to be domain-independent a d portable across 
different domains, as well as declarative and 
bidirectional. On the other hand, a domain with 
distinctive characteristics requires ome specific 
treatment, if a system aims at robustness. We 
decided to have a domain independent core MT 
system, locating the domain dependent process- 
ing in a pipeline of low-level components, easy  
to implement, aiming at fast and robust pro- 
cessing and using limited linguistic knowledge. 
We use declarative and bidirectional gram- 
mars and lexicons. The lexicMist approach is 
indeed suitable to the closed caption domain, 
e.g. in terms of its capability of handling loosely 
structured or incomplete sentences. Also, the 
linguistic resources are geared towards this do- 
main in terms of grammatical nd lexical cover- 
age. However, our system architecture and for- 
malism make them equally usable in any other 
domain and translation direction, as the linguis- 
tic knowledge therein contained is valid in any 
domain. For the architecture we refer the reader 
to (Popowich et al, 1997). In the rest of this 
paper we focus on the pre-processing module 
39 
and how it deals with the issues discussed in 
the introduction. 
The task of the pre-processing pipeline is to 
make the input amenable to a linguistically- 
principled, domain independent reatment. 
This task is accomplished in two ways: 
1. By normalizing the input, i.e. removing 
noise, reducing the input to standard typo- 
graphical conventions, and also restructur- 
ing and simplifying it, whenever this can be 
done in a reliable, meaning-preserving way. 
2. By annotating the input with linguistic in- 
formation, whenever this can be reliably 
done with a shallow linguistic analysis, to 
reduce input ambiguity and make a full lin- 
guistic analysis more manageable. 
Figure (1) shows the system architecture, 
with a particular emphasis on the pre- 
processing pipeline. The next section describes 
the pipeline up to tagging. Proper name 
recognition and segmentation, which deal more 
specifically with the problems described in the 
introduction, are discussed in further sections. 
3 Normal i za t ion  and  tagg ing  
The label normalization groups three compo- 
nents, which clean up and tokenize the input. 
The text-level normalization module performs 
operations at the string level, such as remov- 
ing extraneous text and punctuation (e.g. curly 
brackets , used to mark off sound effects), or re- 
moving periods from abbreviations. E.g.: 
(I) "I went to high school in the u.s." 
"I went to high school in the usa." 
The tokenizer breaks a line into words. The 
token-level normalization recognizes and an- 
notates tokens belonging to special categories 
Pre-processing 
Normalization 
\[Text-level normalization \]
\[ Tokenization ) 
\[Token-level normalization I 
+ 
\[ Proper name recognition \] 
\[ Segmentation "') 
Core MT 
system 
Anal' ,sis \] 
+ 
i I Oo.o a, on I 
,1 
\ ]Pos t -process ing  ) 
Figure 1: System architecture. 
(times, numbers, etc.), expands contractions, 
recognizes, normalizes and annotates tutters 
(e.g. b-b-b-bright), identifies compound words 
and converts number words into digits. E.g.: 
(2) "I" "went" "to" "high" "school" 
"in" "the" "usa" " " 
"I" "went" "to" "high school" "in" 
"the" "usa" " " 
(3) "W-wh-wha~'s" "that" "?"0  
"what"/stutter "is" "that" "?" 
Note that annotations associated with tokens 
are carried along the entire translation process, 
so as to be used in producing the output (e.g. 
stutters are re-inserted in the output). 
The tagger assigns parts of speech to tokens. 
Part of speech information is used by the subse- 
quent pre-processing modules, and also in pars- 
ing, to prioritize the most likely lexical assign- 
ments of ambiguous items. 
4 P roper  name recogn i t ion  
Proper names are ubiquitous in closed captions 
(see Table 1). Their recognition is important 
for effective comprehension of closed captions, 
particularly in consideration of two facts: (i) 
users have little time to mentally rectify a mis- 
translation; (ii) a name can occur repeatedly 
in a program (e.g. a movie), with an annoy- 
ing effect if it is systematically mistranslated 
(e.g. a golf tournament where the golfer named 
Tiger Woods is systematically referred to as los 
bosques del tigre, lit. 'the woods of the tiger'). 
Name recognition is made harder in the closed 
caption domain by the fact that no capitaliza- 
tion information is given, thus making unusable 
all methods that rely on capitalization as the 
main way to identify candidates (Wolinski et al, 
1995) (Wacholder et al, 1997). For instance, an 
expression like 'mark sh ie lds ' ,  as occurs in Ta- 
ble (1), is problematic in the absence of capital- 
ization, as both 'mark' and ' sh ie lds '  are three- 
way ambiguous (proper name, common noun 
and verb). Note that this identical problem may 
be encountered if an MT system is embedded 
in a speech-to-speech translation as well. This 
situation forced us to explore different ways of 
identifying proper names. 
The goal of our recognizer is to identify 
proper names in a tagged line and annotate 
them accordingly, in order to override any 
other possiblelexical assignment in the follow- 
ing modules. The recognizer also overrides pre- 
vious tokenization, by possibly compounding 
two or more tokens into a single one, which 
will be treated as such thereafter. Besides part 
of speech, the only other information used by 
the recognizer is the lexical status of words, i.e. 
their ambiguity class (i.e. the range of possible 
syntactic ategories it can be assigned) or their 
status as an unknown word (i.e. a word that 
is not in the lexicon). The recognizer scans an 
input line from left to right, and tries to match 
40 
each item against a sequences of patterns. Each 
pattern expresses constraints (in terms of word, 
part of speech tag and lexical status) on the 
item under inspection and its left and right con- 
texts. Any number of items can be inspected to 
the left and right of the current item. Such pat- 
terns also make use of regular expression bpera- 
tors (conjunction, disjunction, negation, Kleene 
star). For instance (a simplified version of) a 
pattern might look like the following: 
(4) /the/DEW (NOUNIADJ)*\] X' \['NOUN\] 
where we adopt the convention of representing 
words by lowercase strings, part of speech tags 
by uppercase strings and variables by primed 
Xs. The left and right context are enclosed 
in square brackets, respectively to the left and 
right of the current item. They can also con- 
tain special markers for the beginning and end 
of a line, and for the left or right boundary of 
the proper name being identified. This way to- 
kenization can be overridden and separate to- 
kens joined into a single name. Constraints on 
the lexical status of items are expressed as pred- 
icates associated with pattern elements, e.g.: 
(5) proper_and_common (X') 
A pattern like the one above (4-5) would 
match a lexically ambiguous proper/common 
noun preceded by a determiner (with any num- 
ber of nouns or adjectives in between), and not 
followed by a noun (e.g. ' the b i l l  i s . . . ' ) .  Be- 
sides identifying proper names, some patterns 
may establish that a given item is not a name 
(as in the case above). A return value is as- 
sociated with each pattern, specifying whether 
the current match is or is not a proper name. 
Once a successful match occurs, no further pat- 
terns are tried. Patterns are ordered from more 
to less specific. At the bottom of the pattern 
sequence are the simplest patterns, e.g.: 
(6) ( \[\] X' \[\] ), proper_and_common(X') 
yes 
which is the default assignment for words like 
'b i l l '  if no other pattern matched. However 
(6) is overridden by more specific patterns like: 
(7) ( \[x''\] x' \[\] ) ,  
proper_and_common (X'), common(X") 
no 
41 
(s) ( \[x' \]  x' \[\] ) ,  
proper_and_common(X'), proper(X")  
yes 
The former pattern covers cases like 
' te lecommunicat ions  b i l l ' ,  preventing 
'b i l l '  from being interpreted as a proper 
name, the latter covers cases like 'damian 
b i l l ' ,  where 'b i l l '  is more likely to be a name. 
In general, the recognizer tries to disam- 
biguate lexically ambiguous nouns or to as- 
sign a category to unknown words on the ba- 
sis of the available context. However, in prin- 
ciple any word could be turned into a proper 
name. For instance, verbs or adjectives can 
be turned into proper names, when the con- 
text contains strong cues, like a title. Increas- 
ingly larger contexts provide evidence for more 
informed guesses, which override guesses based 
on narrower contexts. Consider the following 
examples that show how a word or expression 
is treated ifferently depending on the available 
context. Recognized names are in italics. 
(9) biZ~ ~ 
(i0) the bill is ... 
(11) the b i l l  clinton is . . .  
(12) the  b i l l  c l in ton  admin is t ra t ion  is  
The lexically ambiguous bill, interpreted as 
a proper name in isolation, becomes a common 
noun if preceded by a determiner. However, 
the interpretation reverts to proper name if an- 
other noun follows. Likewise the unknown word 
clinton is (incorrectly) interpreted as a com- 
mon noun in (11), as it is the last item of a 
noun phrase introduced by a determiner, but it 
becomes a proper name if another noun follows. 
We also use a name memory ,  which patterns 
have access to. As proper names are found in an 
input stream, they are added to the name mem- 
ory. A previous occurrence of a proper name is 
used as evidence in making decisions about fur- 
ther occurrences. The idea is to cache names 
occurred in an 'easy' context (e.g. a name pre- 
ceded by a title, which provides trong evidence 
for its status as a proper name), to use them 
later to make decisions in 'difficult' contexts, 
where the internal evidence would not be suffi- 
cient to support a proper name interpretation. 
Hence, what typically happens is that the same 
name in the same context is interpreted iffer- 
ently at different imes, if previously the name 
has occurred in an 'easy' context and has been 
memorized. E.g.: 
(13) the individual title went to tiger 
woods. 
mr. tiger woods struggled today 
with a final round 80. 
name-memory 
the short well publicized 
professional life of t iger  woods 
has been an open book. 
The name memory was designed to suit the 
peculiarity of closed captions. Typically, in this 
domain proper names have a low dispersion. 
They are concentrated in sections of an input 
stream (e.g. the name of the main characters 
in a movie), then disappear for long sections 
(e.g. after the movie is over). Therefore, a 
name memory needs to be reset to reflect such 
changes. However, it is problematic to decide 
when to reset the name memory. Even if it was 
possible to detect when a new program starts, 
one should take into account the possible sce- 
nario of an MT system embedded in a consumer 
product, in which case the user might unpre- 
dictably change channel at any time. In or- 
der to keep a name memory aligned with the 
current program, without any detection of pro- 
gram changes, we structured the name memory 
as a relatively short queue (first in, first out). 
Every time a new item is added to the end of 
the queue, the first item is removed and all the 
other items are shifted. Moreover, we do not 
check whether a name is already in the mem- 
ory. Every time a suitable item is found, we 
add it to the memory, regardless of whether it 
is already there. Hence, the same item could 
be present wice or more in the memory at any 
given time. The result of this arrangement is 
that a name only remains in the memory :for a 
relatively short time. It can only remain :\[or a 
longer time if it keeps reappearing frequently in 
the input stream (as typically happens), other- 
wise it is removed shortly after it stopped ap- 
pearing. In this way, the name memory is kept 
42 
# of items 
Proper names correctly identified 
False positives 
False negatives 
152 
8 
57 
Table 2: Name recognition evaluation results. 
aligned with the current program, with only a 
short transition period, during which names no 
longer pertinent are still present in the memory, 
before getting replaced by pertinent ones. 
The recognizer currently contains 63 pat- 
terns. We tested the recognizer on a sample of 
1000 lines (5 randomly chosen continuous frag- 
ments of 200 lines each). The results, shown in 
table (2), illustrate a recall of 72.7% and a pre- 
cision of 95.0%. These results reflect our cau- 
tious approach to name recognition. Since the 
core MT system has its own means of identify- 
ing some proper names (either in the lexicon or 
via default assignments o unknown words) we 
aimed at recognizing names in pre-processing 
only when this could be done reliably. Note 
also that 6 out of the 8 false positives were iso- 
lated interjections that would be better left un- 
translated (e.g. p f foo ,  e l  smacko), or closed 
captioner's typos (e.g. yo4swear).  
5 Segmentation 
Segmentation breaks a line into one or more 
segments, which are passed separately to sub- 
sequent modules (Ejerhed, 1996) (Beeferman et 
al., 1997). In translation, segmentation is ap- 
plied to split a line into a sequence of transla- 
tionally self-contained units (Lavie et al, 1996). 
In our system, the translation units we iden- 
tify are syntactic units, motivated by cross- 
linguistic considerations. Each unit is a con- 
stituent that dan be translated independently. 
Its translation is insensitive to the context in 
which the unit occurs, and the order of the units 
is preserved by translation. 
One motivation for segmenting is that pro- 
cessing is faster: syntactic ambiguity is reduced, 
and backtracking from a module to a previ- 
ous one does not involve re-processing an en- 
tire line, but only the segment hat failed. A 
second motivation is robustness: a failure in 
one segment does not involve a failure in the 
entire line, and error-recovery can be limited 
only to a segment. Further motivations are pro- 
vided by the colloquial nature of closed cap- 
tions. A line often contains fragments with a 
loose syntactic relation to each other and to the 
main clause: vocatives, false starts, tag ques- 
tions, etc. These are most easily translated as 
individual segments. Parenthetical expressions 
are often also found in the middle of a main 
clause, thus making complete parses problem- 
atic. However, the solution involves a heavier 
intervention than just segmenting. Dealing with 
parentheticals requires restructuring a line, and 
reducing it to a 'normal' form which ideally al- 
ways has parenthetical expressions at one end of 
a sentence (under the empirical assumption that 
the overall meaning is not affected). We will 
see how this kind of problem is handled in seg- 
mentation. A third motivation is given by the 
format of closed captions, with input lines split 
across non-constituent boundaries. One solu- 
tion would be delaying translation until a sen- 
tence boundary is found, and restructuring the 
stored lines in a linguistically principled way. 
However, the requirements of real time transla- 
tion (either because of real time captioning at 
the source, or because the MT system is embed- 
ded in a consumer product), together with the 
requirement that translations be aligned with 
the source text and, above all, with the images, 
makes this solution problematic. The solution 
we are left with, if we want lines to be bro- 
ken along constituent boundaries, is to further 
segment a sentence, even at the cost of some- 
times separating elements that should go to- 
gether for an optimal translation. We also ar- 
gued elsewhere (Toole et al, 1998) that in a 
time-constrained application the output gram- 
maticality is of paramount importance, even at 
the cost of a complete meaning equivalence with 
the source. For this reason, we also simplify 
likely problematic input, when a simplification 
is possible without affecting the core meaning. 
To sum up, the task at hand is broader than 
just segmentation: re-ordering of constituents 
and removal of words are also required, to syn- 
tactically 'normalize' the input. As with name 
recognition, we aim at using efficient and easy 
to implement techniques, relying on limited lin- 
guistic information. The segmenter works by 
matching input lines against a set of templates 
represented by pushdown transducers. Each 
transducer is specified in a fairly standard way 
(Gazdar and Mellish, 1989, 82), by defining an 
initial state, a final state, and a set of transitions 
of the following form: 
(14) (State I, State2, Label, Transducer> 
Such a transition specifies that Transducer 
can move from Statel to State2 when the in- 
put specified by Label is found. Label can be 
either a pair (InputSymbol, OutputSymbol) or 
the name of another transducer, which needs 
to be entirely traversed for the transition from 
State l  to State2 to take place. An input sym- 
bol is a <Word, Tag> pair. An output symbol 
is an integer anging from 0 to 3, specifying to 
which of two output segments an input sym- 
bol is assigned (0 = neither segment, 3 = both 
segments, 1 and 2 to be interpreted in the ob- 
vious way). The output codes are then used to 
perform the actual split of a line. A successful 
match splits a line into two segments at most. 
However, on a successful split, the resulting seg- 
ments are recursively fed to the segmenter, until 
no match is found. Therefore, there is no limit 
to the number of segments obtained from an 
input line. The segmenter currently contains 
37 top-level transducers, i.e. segmenting pat- 
terns. Not all of them are used at the same time. 
The implementation of patterns is straightfor- 
ward and the segmenter can be easily adapted 
to different domains, by implementing specific 
patterns and excluding others. For instance, a 
very simple patterns plit a line at every comma, 
a slightly more sophisticated one, splits a line at 
every comma, unless tagged as a coordination; 
other patterns plit a final adverb, interjection, 
prepositional phrase, etc. 
Note that a segment can be a discontinuous 
part of a line, as the same output code can be 
assigned to non-contiguous elements. This fea- 
ture is used, e.g., in restructuring a sentence, as 
when a parenthetical expression is encountered. 
Thefollowing example shows an input sentence, 
an assignment, and a resulting segmentation. 
(15) this, however, is a political 
science course. 
(16) this/2 ,/0 however/l ,/i is/2 a/2 
political/2 science/2 course/2. 
(17) I. however , 
43 
2. this is a po l i t ica l  sc ience 
course 
We sometimes use the segmenter's ability to 
simplify the input, e.g. with adverbs like just, 
which are polysemous and difficult to translate, 
but seldom contribute to the core meaning of a 
sentence. 
6 Per fo rmance  
We ran a test to evaluate how the recognizer 
and segmenter affected the quality of transla- 
tions. We selected asample of 200 lines of closed 
captioning, comprising four continuous sections 
of 50 lines each. The sample was run through 
the MT system twice, once with the recognizer 
and segmenter activated and once without. The 
results were evaluated by two native Spanish 
speakers. We adopted a very simple evalua- 
tion measure, asking the subjects to tell whether 
one translation was better than the other. The 
translations differed for 32 input lines out of 200 
(16%). Table (3) shows the evaluation results, 
with input lines as the unit of measurement. 
The third column shows the intersection of the 
two evaluations, i.e. the evaluations on which 
the two subjects agreed. The three rows show 
how often the translation was better (i) with 
pre-processing, (ii) without pre-processing, or 
(iii) no difference could be appreciated. 
The results show a discrepancy in the evalu- 
ations. One evaluator also pointed out that it 
is hard to make sense of transcribed closed cap- 
tions, without the audio-visual context. These 
two facts seem to point out that an appropri- 
ate evaluation should be done in the operational 
context in which closed captions are normally 
used. Still, the intersection of the subjects' eval- 
uations shows that pre-processing improves the 
output quality. In three of the four cases where 
the two evaluators agreed that pre-processing 
yielded a worse result, the worse performance 
was due to an incorrect name recognition oi" seg- 
mentation. However, in two of the three cases, 
the original problem was an incorrect agging. 
Note that even when the name recognizer 
and segmenter are off, the system can identify 
some names, and recover from translation fail- 
ures by piecing together translations of frag- 
ments. Therefore, what was being tested was 
not so much name recognition and segmenting 
44 
per se, but the idea of having separate modules 
for such tasks in the system front end. 
Finally, the test did not take into account 
speed, as we set higher time thresholds than 
an embedded application would require. Since 
segmentation reduces processing time, it is also 
expected to reduce the impact of tighter time 
thresholds, all other things being equal. 
We are planning to conduct an operational 
evaluation of the system. The goal is to evalu- 
ate the system output in its proper visual con- 
text, and compare the results with parallel re- 
sults for human translated closed captions. Dif- 
ferent groups of participants will watch a video 
With either human- or machine-translated sub- 
titles, and complete a questionnaire based on 
the subtitles in the video. The questionnaire 
will contain a set of questions to elicit the sub- 
ject's assessment on the translation quality, and 
a set of questions to assess the subject's level of 
comprehension f the program. 
7 Conc lus ion  
It is apparent hat the peculiarity of closed 
captions, both in terms of transcribed speech 
characteristic and constraints due to the input 
format, require an ad hoc treatment, consider- 
ably different from the approaches suitable for 
written documents. Yet the knowledge about 
a language (or the bilingual knowledge about 
a language-pair) is largely invariant across dif- 
ferent applications domains and should there- 
fore be portable from one application domain 
to another. The architecture we have proposed 
strives to combine the need for domain indepen- 
dent linguistic resources and linguistically prin- 
cipled methods with the need for robust MT 
systems tuned to real world, noisy and idiosyn- 
cratic input, as encountered when embedding 
MT in real woi:ld devices. 
In terms of adequacy, a standard evaluation 
and a comparison among different MT systems 
frtom different domains is hard, as the ade- 
quacy of a system depends on its application 
(Church and Hovy, 1993). This is even truer 
with-closed captions, where the use of transla- 
tion output is heavily influenced by operational 
constraints (time constraints, the presence of 
images, sound, etc.). In some cases such con- 
straints may place a heavier burden on a system 
(e.g. the time constraint), in some other cases 
Judge 1 Judge 2 Both agreed 
Better with pre~processing 
Better without pre-processing 
No difference 
21 16 15 
4 12 4 
7 4 3 
Table 3: Evaluation results. 
they can make an imperfect ranslation accept- 
able (e.g. the presence of images and sounds). 
We did not attempt an assessment in absolute 
terms, which we believe should take into ac- 
count the operational environment and involve 
real-world users. More modestly, we aimed at 
showing that our pre-processing techniques pro- 
vide an improvement in performance. 
Our work on closed captions also shows that 
the challenges coming from this domain, even 
in terms on low-level issues of input format, can 
lead to interesting developments of new linguis- 
tic techniques. We believe that our solutions to 
specific problems (namely, proper name recog- 
nition and segmentation) in the closed caption 
domain bear relevance to a wider context, and 
offer techniques that can be usefully employed 
in a wider range of applications. 
Re ferences  
Doug Beeferman, Adam Berger, and John Laf- 
ferty. 1997. Text segmentation using expo- 
nential models. In Proceedings of the Second 
Conference on Empirical Methods in Natu- 
ral Language Processing (EMNLP-2), Prov- 
idence, USA. 
Kenneth W. Church and Eduard H. Hovy. 
1993. Good applications for crummy machine 
translation. Machine Translation, 8:239-258. 
Eva Ejerhed. 1996. Finite state segmentation 
of discourse into clauses. In A. Kornai, ed- 
itor, Proceedings of the ECAI-96 Workshop 
Extended Finite State Models of Language, 
Budapest,Hungary. 
Gerald Gazdar and Christopher S. Mellish. 
1989. Natural Language Processing in PRO- 
LOG: an Introduction to Computational Lin- 
guistics. Addison-Wesley Publishing Com- 
pany, Wokingham, England. 
Donald Hindle. 1983. Deterministic parsing of 
syntactic non-fluencies. In Proceedings ofthe 
21st Annual Meeting of the Association for 
Computational Linguistics (ACL-83), pages 
123-128, Cambridge, Massachusetts, USA. 
Alon Lavie, Donna Gates, Noah Coccaro, and 
Lori Levin. 1996. Input segmentation of 
spontaneous peech in janus: a speech- 
to-speech translation system. In Proceed- 
ings of ECAI-96 Workshop on Dialogue Pro- 
cessing in Spoken Language Systems, Bu- 
dapest,Hungary. 
Eric Nyberg and Teruko Mitamura. 1997. A 
real-time MT system for translating broad- 
cast captions. In Proceedings ofthe Sixth Ma- 
chine Translation Summit, pages 51-57, San 
Diego, California, USA. 
Fred Popowich, Davide Turcato, Olivier Lau- 
rens, Paul McFetridge, J. Devlan Nicholson, 
Patrick McGivern, Maricela Corzo-Pena, Lisa 
Pidruchney, and Scott MacDonald. 1997. A 
lexicalist approach to the translation of collo- 
quial text. In Proceedings ofthe 7th Interna- 
tional Conference on Theoretical nd Method- 
ological Issues in Machine Translation, pages 
76-86, Santa Fe, New Mexico, USA. 
Janine Toole, Davide Turcato, Fred Popowich, 
Dan Fass, and Paul McFetridge. 1998. Time- 
constrained Machine Translation. In Proceed- 
ings of the Third Conference of the Associa- 
tion for Machine Translation in the Ameri- 
cas (AMTA-98), pages 103-112, Langhorne, 
Pennsylvania, USA. 
Nina Wacholder, Yael Ravin, and Misook Choi. 
? 1997. Disambiguation of proper names in 
texts. In Proceedings of the Fifth Confer- 
ence on Applied Natural Language Processing 
(ANLP-97), pages 202-208, Washington, DC, 
USA. Association for Computational Linguis- 
tics. 
Francis Wolinski, Frantz Vichot, and Bruno Dil- 
let. 1995. Automatic processing of proper 
names in texts. In Proceedings of the 7th 
Conference of the European Chapter of the 
Asscociation for Computational Linguistics 
(EACL-95), pages 23-30, Dublin, Ireland. 
45 
Adapt ing a synonym database to specif ic domains 
Dav ide  Turcato  Fred Popowich  Jan ine  Toole  
Dan Pass Dev lan  N icho lson  Gordon T i sher  
gavagai Technology Inc. 
P.O. 374, 3495 Ca~abie Street, Vancouver, British Columbia, V5Z 4R3, Canada 
and 
Natural Language Laboratory, School of Computing Science, Simon Fraser University 
8888 University Drive, Burnaby, British Columbia, V5A 1S6, Canada 
{turk, popowich ,toole, lass, devl an, gt i sher}@{gavagai, net, as, sfu. ca} 
Abst ract  
This paper describes a method for 
adapting ageneral purpose synonym 
database, like WordNet, to a spe- 
cific domain, where only a sub- 
set of the synonymy relations de- 
fined in the general database hold. 
The method adopts an eliminative 
approach, based on incrementally 
pruning the original database. The 
method is based on a preliminary 
manual pruning phase and an algo- 
rithm for automatically pruning the 
database. This method has been im- 
plemented and used for an Informa- 
tion Retrieval system in the aviation 
domain. 
1 In t roduct ion  
Synonyms can be an important resource for 
Information Retrieval (IR) applications, and 
attempts have been made at using them to 
expand query terms (Voorhees, 1998). In 
expanding query terms, overgeneration is as 
much of a problem as incompleteness or lack 
of synonym resources. Precision can dramat- 
ically drop because of false hits due to in- 
correct synonymy relations. This problem is 
particularly felt when IR is applied to docu- 
ments in specific technical domains. In such 
cases, the synonymy relations that hold in the 
specific domain are only a restricted portion 
of the synonymy relations holding for a given 
language at large. For instance, a set of syn- 
onyms like 
(1) {cocaine, cocain, coke, snow, C} 
valid for English, would be detrimental in a 
specific domain like weather eports, where 
both snow and C (for Celsius) occur very fre- 
quently, but never as synonyms of each other. 
We describe a method for creating a do- 
main specific synonym database from a gen- 
eral purpose one. We use WordNet (Fell- 
baum, 1998) as our initial database, and we 
draw evidence from a domain specific corpus 
about what synonymy relations hold in the 
domain. 
Our task has obvious relations to word 
sense disambiguation (Sanderson, 1997) (Lea- 
cock et al, 1998), since both tasks are based 
on identifying senses of ambiguous words in 
a text. However, the two tasks are quite dis- 
tinct. In word sense disambiguation, a set of 
candidate senses for a given word is checked 
against each occurrence of the relevant word 
in a text, and a single candidate sense is se- 
lected for each occurrence ofthe word. In our 
synonym specialization task a set of candidate 
senses for a given word is checked against an 
entire corpus, and a subset of candidate senses 
is selected. Although the latter task could be 
reduced to the former (by disambiguating all
occurrences of a word in a test and taking 
the union of the selected senses), alternative 
approaches could also be used. In a specific 
domain, where words can be expected to be 
monosemous to a large extent, synonym prun- 
ing can be an effective alternative (or a com- 
plement) to word sense disambiguation. 
From a different perspective, our 
task is also related to the task of as- 
signing Subject Field Codes (SFC) to 
a terminological resource, as done by 
Magnini and Cavagli~ (2000) for WordNet. 
Assuming that a specific domain corresponds 
to a single SFC (or a restricted set of SFCs, 
at most), the difference between SFC as- 
signment and our task is that the former 
assigns one of many possible values to a given 
synset (one of all possible SFCs), while the 
latter assigns one of two possible values (the 
words belongs or does not belong to the SFC 
representing the domain). In other words, 
SFC assignment is a classification task, while 
ours can be seen as either a filtering or 
ranking task. 
Adopting a filtering/ranking perspective 
makes apparent hat the synonym pruning 
task can also be seen as an eliminative pro- 
cess, and as such it can be performed incre- 
mentally. In the following section we will 
show how such characteristics have been ex- 
ploited in performing the task. 
In section 2 we describe the pruning 
methodology, while section 3 provides a prac- 
tical example from a specific domain. Con- 
clusions are offered in section 4. 
2 Methodo logy  
2.1 Out l ine  
The synonym pruning task aims at improv- 
ing both the accuracy and the speed of a syn- 
onym database. In order to set the terms of 
the problem, we find it useful to partition the 
set of synonymy relations defined in WordNet 
into three classes: 
. Relations irrelevant to the specific do- 
main (e.g. relations involving words that 
seldom or never appear in the specific do- 
main) 
. Relations that are relevant but incorrect 
in the specific domain (e.g. the syn- 
onymy of two words that do appear in the 
specific domain, but are only synonyms 
in a sense irrelevant o the specific do- 
main); 
3. Relations that are relevant and correct in 
the specific domain. 
The creation of a domain specific database 
aims at removing relations in the first two 
classes (to improve speed and accuracy, re- 
spectively) and including only relations in the 
third class. 
The overall goal of the described method 
is to inspect all synonymy relations in Word- 
Net and classify each of them into one of the 
three aforementioned classes. We define a 
synonymy relation as a binary relation be- 
tween two synonym terms (with respect to 
? a particular sense). Therefore, a WordNet 
synset containing n terms defines ~11 k syn- 
onym relations. The assignment of a syn- 
onymy relation to a class is based on evidence 
drawn from a domain specific corpus. We use 
a tagged and lemmatized corpus for this pur- 
pose. Accordingly, all frequencies used in the 
rest of the paper are to be intended as fre- 
quencies of ( lemma, tag) pairs. 
The pruning process is carried out in three 
steps: (i) manual pruning; (ii) automatic 
pruning; (iii) optimization. The first two 
steps focus on incrementally eliminating in- 
correct synonyms, while the third step focuses 
on removing irrelevant synonyms. The three 
steps are described in the following sections. 
2.2 Manua l  p run ing  
Different synonymy relations have a different 
impact on the behavior of the application in 
which they are used, depending on how fre- 
quently each synonymy relation is used. Rela- 
tions involving words frequently appearing in 
either queries or corpora have a much higher 
impact (either positive or negative) than re- 
lations involving rarely occurring words. E.g. 
the synonymy between snow and C has a 
higher impact on the weather eport domain 
(or the aviation domain, discussed in this pa- 
per) than the synonymy relation between co- 
caine and coke. Consequently, the precision of 
a synonym database obviously depends much 
more on frequently used relations than on 
rarely used ones. Another important consid- 
eration is that judging the  correctness of a 
given synonymy relation in a given domain is 
often an elusive issue: besides clearcut cases, 
there is a large gray area where judgments 
may not be trivial even for humans evalua- 
tots. E.g. given the following three senses of 
the noun approach 
(2) a. {approach, approach path, glide 
path, glide slope} 
(the final path followed by an air- 
craft as it is landing) 
b. {approach, approach shot} 
(a relatively short golf shot in- 
tended to put the ball onto the 
putting green) 
c. {access, approach} 
(a way of entering or leaving) 
it would be easy to judge the first and second 
senses respectively relevant and irrelevant o 
the aviation domain, but the evaluation of the 
third sense would be fuzzier. 
The combination of the two remarks above 
induced us to consider a manual pruning 
phase for the terms of highest 'weight' as a 
good investment of human effort, in terms of 
rate between the achieved increase in preci- 
sion and the amount of work involved. A 
second reason for performing an initial man- 
ual pruning is that its outcome can be used 
as a reliable test set against which automatic 
pruning algorithms can be tested. 
Based on such considerations, we included a 
manual phase in the pruning process, consist- 
ing of two steps: (i) the ranking of synonymy 
relations in terms of their weight in the spe- 
cific domain; (ii) the actual evaluation of the 
correctness of the top ranking synonymy re- 
lation, by human evaluators. 
2.2.1 Rank ing  of  synonymy re lat ions  
The goal of ranking synonymy relations is 
to associate them with a score that estimates 
how often a synonymy relation is likely to 
be used in the specific domain. The input 
database is sorted by the assigned scores, and 
the top ranking words are checked for manual 
pruning. Only terms appearing in the domain 
specific corpus are considered at this stage. 
In this way the benefit of manual pruning is 
maximized. Ranking is based on three sorting 
criteria, listed below in order of priority. 
Cr i te r ion  1. Since a term that does ap- 
pear in the domain corpus must have at least 
one valid sense in the specific domain, words 
with only one sense are not good candidates 
for pruning (under the assumption of com- 
pleteness of the synonym database). There- 
fore .polysemous terms are prioritized over 
monosemous terms. 
Cr i te r ion  2. The second and third sort- 
ing criteria axe similar, the only difference be- 
ing that the second criterion assumes the ex- 
istence of some inventory of relevant queries 
(a term list, a collection of previous queries, 
etc.), ff such an inventory is not available, the 
second sorting criterion can be omitted. If the 
inventory is available, it is used to check which 
synonymy relations are actually to be used in 
queries to the domain corpus. Given a pair 
(ti,tj) of synonym terms, a score (which we 
name scoreCQ) is assigned to their synonymy 
relation, according to the following formula: 
(3) scoreCQij = 
(fcorpusi * fqueryj) + 
(fcorpusj ? fqueryi) 
where fcorpusn and fqueryn are, respec- 
tively, the frequencies of a term in the domain 
corpus and in the inventory of query terms. 
The above formula aims at estimating how 
often a given synonymy relation is likely to 
be actually used. In particular, each half of 
the formula estimates how often a given term 
in the corpus is likely to be matched as a syn- 
onym of a given term in a query. Consider, 
e.g., the following situation (taken form the 
aviation domain discussed in section 3.1): 
(4) fcorpuSsnow = 3042 
f querysnow = 2 
fcorpusc = 9168 
f queryc = 0 
It is estimated that C would be matched 
18336 times as a synonym for snow (i.e 9168 
* 2), while snow would never be matched as 
a synonym for C, because C never occurs as 
a query term. Therefore scoreCQs,~ow,c is 
18336 (i.e. 18336 + 0). 
Then, for each polysemous term i and 
synset s such that i E s, the following score is 
computed: 
Table 1: Frequencies of sample synset erms. 
j fcorpusj fqueryj 
cocaine 1 0 
cocain 0 0 
coke 8 0 
C 9168 0 
(5) scorePolyCQ i,~ = 
E{scoreCQi,~lj ~ s A i ? j} 
E.g., i f  ,5' is the synset in (1), then 
scorePolyCQs~ow,s is "the sum of 
scoreCQsnow,coc~ine, scoreCQsnow,eocain, 
scoreCQsnow,eoke and scoreCQ,no~o,c. Given 
the data in Table 1 (taken again from our 
aviation domain) the following scoreCQ 
would result: 
(6) scoreCQsnow,cocaine -~2 
scoreCQsnow,cocain = 0 
scoreCQs~ow,cok~ = 16 
scoreCQsno~o,c = 18336 
Therefore, scorePolyCQsnow,s would equal 
18354. 
The final score assigned to each polysemous 
term tl is the highest scorePolyCQi,s. For 
snow, which has the following three senses 
(7) a. {cocaine, cocaine, coke, C, snow} 
(a narcotic (alkaloid) extracted 
from coca leaves) 
b. {snow} 
(a layer of snowflakes (white crys- 
tals of frozen water) covering the 
ground) 
c. {snow, snowfall} 
(precipitation falling from clouds 
in the form of ice crystals) 
the highest score would be the one computed 
above. 
Cr i ter ion 3. The third criterion assigns 
a score in terms of domain corpus frequency 
alone. It is used to further rank terms that 
do not occur in the query term inventory (or 
when no query term inventory is available). It 
is computed in the same way as the previous 
score, with the only difference that a value of 
1 is conventionally assumed for fquery (the 
frequency of a term in the inventory of query 
terms). 
2.2.2 Correctness  evaluat ion 
All the synsets containing the top rank- 
ing terms, according to the hierarchy of crite- 
ria described above, are manuMly checked for 
pruning. For each term, all the synsets con- 
taining the term are clustered together and 
presented to a human operator, who exam- 
ines each (term, synset) pair and answers the 
question: does the term belong to the synset 
in the specific domain? Evidence about the 
answer is drawn from relevant examples auto- 
matically extracted from the domain specific 
corpus. E.g., following up on our example in 
the previous section, the operator would be 
presented with the word snow associated with 
each of the synsets in (7) and would have to 
provide a yes/no answer for each of them. In 
the specific case, the answer would be likely 
to be 'no' for (7a) and 'yes' for (75) and (7c). 
The evaluator is presented with all the 
synsets involving a relevant term (even 
those that did not rank high in terms of 
scorePoIyCQ) in order to apply a contrastive 
approach. It might well be the case that the 
correct sense for a given term is one for which 
the term has no synonyms at all (e.g. 7b in 
the example), therefore all synsets for a given 
term need to be presented to the evaiuator 
in order to make an informed choice. The 
evaluator provides a yes/no answer for all the 
(term, synset) he/she is presented with (with 
some exceptions, as explained in section 3.1). 
2.3 Automat ic  p run ing  
The automatic pruning task is analogous to 
manual pruning in two respects: (i) its in- 
put is the set of synonymy relations involving 
WordNet polysemous words appearing in the 
domain specific orpus; (ii) it is performed by 
examining all (term, synset) input pairs and 
answering the question: does the term belong 
to the synset in the specific domain? How- 
ever, while the manual pruning task was re- 
garded as a filtering task, where a human eval- 
4 
uator assigns a boolean value to each pruning 
candidate, the automatic pruning task can 
be more conveniently regarded as a ranking 
task, where all the pruning candidates are as- 
signed a score, measuring how appropriate a 
given sense is for a given word, in the do- 
main at hand. The actual pruning is left as 
a subsequent step. Different pruning thresh- 
olds can be applied to the ranked list, based 
on different considerations (e.g. depending on 
whether astronger emphasis i  put on the pre- 
cision or the recall of the resulting database). 
The score is based on the frequencies of both 
words in the synset (except the word under 
consideration) and words in the sense gloss. 
We also remove from the gloss all words be- 
longing to a stoplist (a stoplist provided with 
WordNet was used for this purpose). The fol- 
lowing scoring formula is used: 
(8) (average_synset_frequeney/ 
synset_cardinality k) .4- 
(average_gloss_frequency~ 
gloss_cardinality :) 
Note that the synset cardinality does not 
include the word under consideration, reflect- 
ing the fact the word's frequency is not used 
in calculating the score. Therefore a synset 
only containing the word under consideration 
and no synonyms is assigned cardinality 0. 
The goal is to identify (term, sense) pairs 
not pertaining to the domain. For this rea- 
son we tend to assign high scores to candi- 
dates for which we do not have enough evi- 
dence about their inappropriateness. This is 
why average frequencies are divided by some 
factor which is function of the number of av- 
eraged frequencies, in order to increase the 
Scores based on little evidence (i.e. fewer av- 
eraged numbers). In the sample application 
described in section 3 the value of k was set 
to 2. For analogous reasons, we convention- 
ally assign a very high score to candidates for 
which we have no evidence (i.e. no words in 
both the synset and the gloss). If either the 
synset or the gloss is empty, we conventionally 
double the score for the gloss or the synset, 
respectively. We note at this point that our 
final ranking list are sorted in reverse order 
with respect o the assigned scores, since we 
are focusing on removing incorrect items. At 
the top of the list are the items that receive 
the lowest score, i.e. that are more likely to 
be incorrect (term, sense) associations for our 
domain (thus being the best candidates to be 
pruned out). 
Table 2 shows the ranking of the senses 
for the word C in the aviation domain. In 
the table, each term is followed by its corpus 
frequency, separated by a slash. From each 
synset the word C itself has been removed, 
as well as the gloss words found in the stop 
list. Therefore, the table only contains the 
words that contribute to the calculation of the 
sense's core. E.g. the score for the first sense 
in the list is obtained from the following ex- 
pression: 
(9) ((0 + 57)/2/22) + 
( (8+0+0+ 198+9559+0+1298)/7/72 ) 
The third sense in the list exemplifies the 
case of an empty synset (i.e. a synset orig- 
inally containing only the word under con- 
sideration). In this case the score obtained 
from the gloss is doubled. Note that the ob- 
viously incorrect sense of C as a narcotic is 
in the middle of the list. This is due to a tag- 
ging problem, as the word leaves in the gloss 
was tagged as verb instead of noun. Therefore 
it was assigned a very high frequency, as the 
verb leave, unlike the noun leaf, is very com- 
mon in the aviation domain. The last sense 
in the list also requires a brief explanation. 
The original word in the gloss was 10S. How- 
ever, the pre-processor that was used before 
tagging the glosses recognized S as an abbre- 
viation for South and expanded the term ac- 
cordingly. It so happens that both words 10 
and South are very frequent in the aviation 
corpus we used, therefore the sense was as- 
signed a high score. 
2.4 Optimization 
The aim of this phase is to improve the access 
speed to the synonym database, by removing 
all information that is not likely to be used. 
The main idea is to minimize the size of the 
Score 
Table 2: Ranking of synsets containing the word C 
Frequencies 
39.37 
62.75 
224.28 
synset: 
gloss: 
synset: 
gloss: 
synset: 
gloss: 
241.69 synset: 
gloss: 
585.17 synset: 
gloss: 
743.28 synset: 
gloss: 
1053.43 synset: 
gloss: 
ATOMIC_NUMBEK_6/O, CAKBON/57 
ABUNDANT/8, NONMETALLIC/O, TETRAVALENT/O, ELEMENT/198 
0CCUR/9559, ALLOTROPIC/O, FOKM/1298 
AMPEre-SECOND/O, COULOMB/O 
UNIT/3378, ELECTRICAL/2373, CHARGE/523, EQUAL/153 
AMOUNT/1634, CHARGE/523, TKANSFEK/480, CUKKENT/242, 1/37106 
AMPEre/4, 1/37106 
0 
GENEKAL-PUKPOSE/O, PROGRAMING/O, LANGUAGE/445, CLOSELY/841 
ASSOCIATE/543, UNIX/O, OPEKATE/5726, SYSTEM/49863 
COCAIN/O, COCAINE/i, COKE/8, SNOW/3042 
NARCOTIC/i, ALKALOID/O, EXTKACT/31, COCA/I, LEAVE/24220 
LIGHT_SPEED/I, SPEED_OF_LIGHT/O 
SPEED/14665, LIGHT/22481, TRAVEL/f05, VACUUM/192 
DEGREE_CELSIUS/24, DEGREEiENTIGRADE/28 
DEGKEE/43617, CENTIGRADE/34, SCALE/540, TEMPERATURE/2963 
I00/0, CENTRED/O, CENTUKY/31, HUNDRED/O, ONE_C/O 
TEN/Z3, 10/16150, SOUTH/12213 
database in such a way that the database be- 
havior remains unchanged. Two operations 
are performed at the stage: (i) a simple rel- 
evance  tes t  to remove irrelevant erms (i.e. 
terms not pertaining to the domain at hand); 
(ii) a redundancy check, to remove informa- 
tion that, although perhaps relevant, does not 
affect the database behavior. 
2.4.1 Re levance  tes t  
Terms not appearing in the domain cor- 
pus are considered not relevant o the spe- 
cific domain and removed from the synonym 
database. The rationale underlying this step 
is to remove from the synonym database syn- 
onymy relations that are never going to be 
used in the specific domain. In this way the ef- 
ficiency of the module can be increased, by re- 
ducing the size of the database and the num- 
ber of searches performed (synonyms that are 
known to never appear are not searched for), 
without affecting the system's matching at- 
curacy. E.g., the synset in (10a) would be 
reduced to the synset in (10b). 
(10) a. AMPERE-SECOND/O, COULOMB/O, 
C/9168 
b. C/9168 
2.4.2 Redundancy  check 
The final step is the removal of redundant 
synsets, possibly as a consequence of the pre- 
vious pruning steps. Specifically, the follow- 
ing synsets are removed: 
? Synsets containing a single term (al- 
though the associated sense might be a 
valid one for that term, in the specific 
domain). 
? Duplicate synsets, i.e. identical (in terms 
of synset elements) to some other synset 
not being removed (the choice of the only 
synset o be preserved is arbitrary). 
E.g., the synset in (10b) would be finMly 
removed at this stage. 
3 Sample  app l i ca t ion  
The described methodology was applied to 
the aviation domain. We used the Aviation 
Safety Information System (ASRS) corpus 
(h 'e tp : / /as rs .  a rc .nasa .gov / )  as our avia- 
tion specific corpus. The resulting domain- 
specific database is being used in an IR ap- 
plication that retrieves documents relevant 
to user defined queries, expressed as phrase 
patterns, and identifies portions of text that 
are instances of the relevant phrase patterns. 
The application makes use of Natural Lan- 
guage Processing (NLP) techniques (tagging 
and partial parsing) to annotate documents. 
User defined queries are matched against such 
annotated corpora. Synonyms are used to 
expand occurrences of specific words in such 
queries. In the following two sections we de- 
scribe how the pruning process was performed 
and provide some results. 
3.1 Adapt ing  Wordnet  to the  
av iat ion  domain  
A vocabulary of relevant query terms was 
made available by a user of our IR applica- 
tion and was used in our ranking of synonymy 
relations. Manual pruning was performed on 
the 1000 top ranking terms, with which 6565 
synsets were associated overall. The manual 
pruning task was split between two human 
evaluators. The evaluators were programmers 
members of our staff. They were English na- 
tive speakers who had acquaintance with our 
IR application and with the goals of the man- 
ual pruning process, but no specific training 
or background on lexicographic or WordNet- 
related tasks. For each of the 1000 terms, 
the evaluators were provided with a sample 
of 100 (at most) sentences where the rele- 
vant word occurred in the ASRS corpus. 100 
of the 1000 manually checked clusters (i.e. 
groups of synsets referring to the same head 
term) were submitted to both evaluators (576 
synsets overall), in order to check the rate 
of agreement of their evaluations. The eval- 
uators were allowed to leave synsets unan- 
swered, when the synsets only contained the 
head term (and at least one other synset in 
the cluster had been deemed correct). Leav- 
ing out the cases when one or both evalua- 
tors skipped the answer, there remained 418 
synsets for which both answered. There was 
agreement in 315 cases (75%) and disagree- 
ment in 103 cases (25%). A sample of senses 
on which the evaluators disagreed is shown in 
(11). In each case, the term being evaluated 
is the first in the synset. 
(11) a. {about, around} 
(in the area or vicinity) 
b. {accept, admit, take, take on} 
(admit into a group or commu- 
nity) 
c. {accept, consent, go for} 
(give an affirmative reply to) 
d. {accept, swallow} 
(tolerate or accommodate oneself 
to) 
e. {accept, take} 
(be designed to hold or take) 
f. {accomplished, effected, estab- 
lished} 
(settled securely and uncondi- 
tionally) 
g. {acknowledge, know, recognize} 
(discern) 
h. {act, cognitive operation, cogni- 
tive process, operation, process} 
(the performance of some com- 
posite cognitive activity) 
i. {act, act as, play} 
(pretend to have certain qualities 
or state of mind) 
j. {action, activeness, activity} 
(the state of being active) 
k. {action, activity, natural action, 
natural process} 
(a process existing in or produced 
by nature (rather than by the in- 
tent of human beings)) 
It should be noted that the 'yes' and 'no' 
answers were not evenly distributed between 
the evaluators. In 80% of the cases of dis- 
agreement, i  was evaluator A answering 'yes' 
and evaluator B answering 'no'. This seems 
to suggest han one of the reasons for dis- 
agreement was a different degree of strictness 
in evaluating. Since the evaluators matched 
a sense against an entire corpus (represented 
by a sample of occurrences), one common sit- 
uation may have been that a sense did oc- 
cur, but very rarely. Therefore, the evaluators 
may have applied different criteria in judging 
how many occurrences were needed to deem 
a sense correct. This discrepancy, of course, 
may compound with the fact that the differ- 
ences among WordNet senses can sometimes 
be very subtle. 
Automatic pruning was performed on 
the entire WordNet database, regardless of 
whether candidates had already been manu- 
ally checked or not. This was done for test- 
ing purposes, in order to check the results of 
automatic pruning against the test set ob- 
tained from manual pruning. Besides asso- 
ciating ASRS frequencies with all words in 
synsets and glosses, we also computed fre- 
quencies for collocations (i.e. multi-word 
terms) appearing in synsets. The input to 
automatic pruning was constituted by 10352 
polysemous terms appearing at least once in 
ASRS the corpus. Such terms correspond to 
37494 (term, synset) pairs. Therefore, the 
latter was the actual number of pruning can- 
didates that  were ranked. 
The check of WordNet senses against ASRS 
senses was only done unidirectionally, i.e. 
we only checked whether WordNet senses 
were attested in ASRS. Although it would 
be interesting to see how often the appropri- 
ate, domain-specific senses were absent from 
WordNet, no check of this kind was done. We 
took the simplifying assumption that Word- 
Net be complete, thus aiming at assigning at 
least one WordNet sense to each term that 
appeared in both WordNet and ASRS. 
3.2 Resu l ts  
In order to test the automatic pruning per- 
formance, we ran the ranking procedure on 
a test set taken from the manually checked 
files. This file had been set apart and had 
not been used in the preliminary tests on the 
automatic pruning algorithm. The test set 
included 350 clusters, comprising 2300 candi- 
dates. 1643 candidates were actually assigned 
an evaluation during manual pruning. These 
were used for the test. We extracted the 1643 
relevant items from our ranking list, then we 
incrementally computed precision and recall 
in terms of the items that had been manually 
checked by our human evaluators. The re- 
sults are shown in figure 1. As an example of 
how this figure can be interpreted, taking into 
consideration the top 20% of the ranking list 
(along the X axis), an 80% precision (Y axis) 
means that 80% of the items encountered so 
far had been removed in manual pruning; a 
27% recall (Y axis) means that 27% of the 
overall manually removed items have been en- 
countered so far. 
The automatic pruning task was intention- 
ally framed as a ranking problem, in order to 
leave open the issue of what pruning threshold 
would be optimal. This same approach was 
taken in the IR application in which the prun- 
ing procedure was embedded. Users are given 
the option to set their own pruning threshold 
(depending on whether they focus more on 
precision or recall), by setting a value spec- 
ifying what precision they require. Pruning 
is performed on the top section of the rank- 
ing list that guarantees the required precision, 
according to the correlation between precision 
and amount of pruning shown in figure 1. 
A second test was designed to check 
whether there is a correlation between the 
levels of confidence of automatic and man- 
ual pruning. For this purpose we used the 
file that had been manually checked by both 
human evaiuators. We took into account he 
candidates that had been removed by at least 
one evaluator: the candidates that were re- 
moved by both evaluators were deemed to 
have a high level of confidence, while those 
removed by only one evaluator were deemed 
to have a lower level of confidence. Then we 
checked whether the two classes were equally 
distributed in the automatic pruning ranking 
list, or whether higher confidence candidates 
tended to be ranked higher than lower con- 
fidence ones. The results are shown in fig- 
ure 2, where the automatic pruning recall for 
each class is shown. For any given portion 
of the ranking list higher confidence candi- 
dates (solid lines) have a significantly higher 
recall than lower confidence candidates (dot- 
Table 3: WordNet optimization results. 
DB Synsets Word-senses 
Full WN 99,642 174,008 
Reduced WN 9,441 23,368 
ted line). 
Finally, table 3 shows the result of applying 
the described optimization techniques alone, 
i.e. without any prior pruning, with respect 
to the ASRS corpus. The table shows how 
many synsets and how many word-senses are 
contained in the full Wordnet database and in 
its optimized version. Note that such reduc- 
tion does not involve any loss of accuracy. 
4 Conc lus ions  
There is a need for automatically or semi- 
automatically adapting NLP components o 
specific domain, if such components are to be 
effectively used in IR applications without in- 
volving labor-intensive manual adaptation. A 
key part of adapting NLP components ospe- 
cific domains is the adaptation of their lexical 
and terminological resources. It may often be 
the case that a consistent section of a general 
purpose terminological resource is irrelevant 
to a specific domain, thus involving an unnec- 
essary amount of ambiguity that affects both 
the accuracy and efficiency of the overall NLP 
component. In this paper we have proposed 
a method for adapting a general purpose syn- 
onym database to a specific domain. 
Evaluating the performance of the pro- 
posed pruning method is not a straightfor- 
ward task, since there are no other results 
available on a similar task, to the best of our 
knowledge. However, a comparison between 
the results of manual and automatic pruning 
provides ome useful hints. In particular: 
? The discrepancy between the evaluation 
of human operators hows that the task 
is elusive even for humans (the value of 
the agreement evaluation statistic n for 
our human evaluators was 0.5); 
? however, the correlation between the 
level of confidence of human evaluations 
and scores assigned by the automatic 
pruning procedure shows that the auto- 
matic pruning algorithm captures ome 
significant aspect of the problem. 
Although there is probably room for im- 
proving the automatic pruning performance, 
the preliminary results how that the current 
approach is pointing in the right direction. 
Re ferences  
Christiane Fellbaum, editor. 1998. Wordnet: An 
Electronic Lexical Database. MIT Press Books. 
Claudia Leacock, Martin Chodorow, and 
George A. Miller. 1998. Using corpus tatistics 
and WordNet relations for sense identification. 
Computational Linguistics, 24(1):147-165. 
Bernardo Magnini and Gabriela Cavaglih. 2000. 
Integrating Subject Field Codes into WordNet. 
In Maria Gavrilidou, George Carayannis, Stella 
Markantonatou, Stelios Piperidis, and Gregory 
Stainhaouer, editors, Proceedings of the Sec- 
ond International Conference on Language Re- 
sources and Evaluation (LREC-PO00), pages 
1413-1418, Athens, Greece. 
Mark Sanderson. 1997. Word Sense Disambigua- 
tion and Information Retrieval. Ph.D. thesis, 
Department ofComputing Science at the Uni- 
versity of Glasgow, Glasgow G12. Technical 
Report (TR-1997-7). 
Ellen M. Voorhees. 1998. Using WordNet for text 
retrieval. In Fellbaum (Fellbaum, 1998), chap- 
ter 12, pages 285-303. 
9 
~2 
~9 
100 i I I I 
95 
90 
85 
+? i 
75 
70 
65 
60 
55 
0 
100 
80 
60 
40 
20 
0 I I I I 
0 20 40 60 80 100 
Top % of ranking list 
F igure 1: Precision and recall of automat ic  pruning 
10 
~9 
cg 
100 
80 
60 
40 
20 
I I I I t - - ' - /  
-- _ t / f  j" _ 
-- r l  j --  
/ J _  ; _ 
- - J "  I I I I 
0 20 40 60 80 100 
Top % of ranking list 
Figure 2: A recall comparison for different confidence rates 
11 
INLG 2012 Proceedings of the 7th International Natural Language Generation Conference, pages 115?119,
Utica, May 2012. c?2012 Association for Computational Linguistics
Interactive Natural Language Query Construction for Report Generation?
Fred Popowich
School of Computing Science
Simon Fraser University
Burnaby, BC, CANADA
popowich@sfu.ca
Milan Mosny
Response42 Inc
North Vancouver, BC, Canada
Milan.Mosny
@response42.com
David Lindberg
School of Computing Science
Simon Fraser University
Burnaby, BC, CANADA
dll4@sfu.ca
Abstract
Question answering is an age old AI chal-
lenge. How we approach this challenge is de-
termined by decisions regarding the linguis-
tic and domain knowledge our system will
need, the technical and business acumen of
our users, the interface used to input ques-
tions, and the form in which we should present
answers to a user?s questions. Our approach
to question answering involves the interactive
construction of natural language queries. We
describe and evaluate a question answering
system that provides a point-and-click, web-
based interface in conjunction with a seman-
tic grammar to support user-controlled natural
language question generation. A preliminary
evaluation is performed using a selection of 12
questions based on the Adventure Works sam-
ple database.
1 Introduction
There is a long history of systems that allow users
to pose questions in natural language to obtain ap-
propriate responses from information systems (Katz,
1988; El-Mouadib et al, 2009). Information sys-
tems safeguard a wealth of information, but tradi-
tional interfaces to these systems require relatively
sophisticated technical know-how and do not always
present results in the most useful or intuitive way for
non-technical users. Simply put, people and com-
puters do not speak the same language. The ques-
tion answering challenge is thus the matter of devel-
oping a method that allows users with varying levels
?This research was supported in part by a discovery grant
from the Natural Sciences and Engineering Research Council
of Canada. The authors would also like to thank the referees for
their insights and suggestions.
of technical proficiency to ask questions using natu-
ral language and receive answers in an appropriate,
intuitive format. Using natural language to ask these
questions may be easy for users, but is challenging
due to the ambiguity inherent in natural language
anaylsis. Proposals involving controlled natural lan-
guage, such as (Nelken and Francez, 2000), can deal
with some of the challenges, but the task becomes
more difficult when we seek to answer natural lan-
guage questions in a way that is domain portable.
Before we can attempt to design and implement a
question answering system, we need to address sev-
eral key issues. First, we need to decide what knowl-
edge our system needs. Specifically, we must decide
what linguistic knowledge is needed to properly in-
terpret users? questions. Then we need to consider
what kind of domain-specific knowledge the system
must have and how that knowledge will be stored
and accessed. We must address the challenges posed
by users with varying levels of technical sophistica-
tion and domain knowledge. The sophistication of
the user and the environment in which the system
is used will also affect how users will give input to
the system. Will we need to process text, speech,
or will a simpler point-and-click interface be suf-
ficient? Finally, we must decide how to best an-
swer the user?s questions, whether it be by fetch-
ing pre-existing documents, dynamically generat-
ing structured database reports, or producing nat-
ural language sentences. These five issues do not
present us with a series of independent choices that
are merely stylistic or cosmetic. The stance we take
regarding each of these issues strongly influences
design decisions, ease of installation/configuration,
and the end-user experience.
Here we solve this problem in the context of ac-
115
cessing information from a structured database ? a
natural language interface to a database (NLIDB)
(Kapetanios et al, 2010). However, instead of treat-
ing it as a natural language analysis problem, we
will consider it as a task involving natural language
generation (NLG) where users build natural lan-
guage questions by making choices that add words
and phrases. Using our method, users construct
queries in a menu driven manner (Tennant et al,
1983; Evans and Power, 2003) to ask questions that
are always unambiguous and easy for anyone to un-
derstand, getting answers in the form of interactive
database reports (not textual reports) that are both
immediate and consistent.
This approach retains the main advantage of tra-
ditional NLIDBs that allow input of a question in a
free form text ? the ability for the user to communi-
cate with the information system in English. There
is no need for the user to master a computer query
langauge such as SQL or MDX. Many disadvant-
ges of traditional free input NLIDBs are removed
(Tennant et al, 1983). Traditional NLIDBs fail to
analyze some questions and indicate so to the user,
greatly decreasing the user?s confidence in the sys-
tem. The problem is even worse when the NLIDB
analyzes the question incorrectly and produces a
wrong or unpexpected result. In contrast, our system
is able to answer every question correctly. In tradi-
tional free input NLIDBs, the user can make gram-
matical or spelling mistakes that may lead to other
errors. Using a menu-based technique, the user is
forced to input only valid and wellformed queries.
The complexity of the system is greatly reduced as
the language that the system has to process is sim-
ple and unambiguous. Portability to other domains
is improved because there is no need for vocabulary
that fully covers the domain.
2 Our approach
We begin with an overview of our approach to this
question answering problem involving NLG. We de-
scribe how we address each of the afore-mentioned
issues and give our rationale for each of those
choices. Following a brief discussion of our use
of online analytical processing (OLAP) (Janus and
Fouche, 2009) in section 2.2, we then decribe how
we use the OLAP model as the basis for interactive
natural query generation, and describe the database
used in our evaluation, along with the grammar used
for NLG.
2.1 Overview
Our approach to the question answering problem is
based on the following decisions and assumptions:
Linguistic knowledge We use a semantic grammar
to support user-controlled NLG rather than language
analysis. By guiding the construction process, we
avoid difficult analysis tasks, such as resolving am-
biguities and clarifying vague language. We also
eliminate the possibility of out-of-domain queries.
Domain-specific knowledge We model domain
knowledge using an OLAP cube, a widely-used
approach to model domain-specific data. OLAP
cubes provide a standard semantic representation
that is well-suited to historical business data and
allows us to automatically generate both the lexicon
and the semantic grammar for our system.
Users The prototypical user of our system is famil-
iar with business issues but does not have a high-
degree of technical expertise. We provide a simple
and intuitive interface suitable for such users but still
powerful enough for users of any level of technical
proficiency.
Input A web-based, point-and-click interface will
guide users in the creation of a natural language
query string. Users click on words and phrases to
construct a question in plain English.
Answers We will answer questions with an interac-
tive database report. Users can click on parts of the
report to get detailed information, making it more of
an interactive dashboard rather than a report.
An approach governed by these principles offers
many benefits. It simplifies database report creation
and lowers the associated costs, allows businesses to
leverage existing investments in data warehouse and
reporting technology, offers a familiar and comfort-
able interface, does not require installation on client
machines, and is simple to install and configure.
2.2 Role of OLAP
An OLAP cube is produced as a result of process-
ing a datawarehouse into datastructures optimized
116
for query processing. The OLAP query language
makes reference to measure groups (that roughly
correspond to fact tables), measures (that come from
the numerical values in the fact tables) and dimen-
sions (that come from dimension tables). For ex-
ample, the order fact table might include total or-
der price, order quantity, freight cost, and discount
amount. These are the essential figures that describe
orders, but to know more we need to examine these
facts along one or more dimensions. Accordingly,
the dimension tables associated with this fact table
include time (order date, year, quarter, and month),
customer (name, address, city, and zip code), and
product (name, category, and price).
2.3 Interactive Natural Language Generation
At the heart of the system is a semantic grammar.
Our goal was to create a grammar that is suitable to
database querying application, but is simple enough
so that it can be automatically adapted to different
domains. The semantic model makes use of both
entities (unary predicates) and relationships (binary
predicates) that are automatically derived from the
OLAP model. These entities and relationships can
be directly and automatically mapped to the lexical
items and phrases that the user sees on the screen
during query construction. Once a user has com-
pleted the construction of a natural language query,
a corresponding first order logic formula is created
which can then be translated into a database query
in SQL or MDX.
Our assumption was that many database queries
can be expressed within the following template
Show <Show> and ... and <Show> for
each <GroupBy> and ... and for
each <GroupBy> limit to <LimitTo>
and ... and to <LimitTo>
where <Show>, <GroupBy> and <LimitTo> are
different classes of nominals. <Show> may refer to
a measure or to a level in a dimension which may
take an additional constraint in a form of a preposi-
tional clause. <GroupBy> may refer to a level in
a dimension which may take a constraint in a form
of a prepositional phrase or to a set of members of a
dimension. <LimitTo> may refer to a set of mem-
bers of a dimension. A prepositional phrase express-
ing a constraint has a form
with <NounPhrase>
QuestionElement
Terminal
EntityTerminal
GroupByEntityTerminal
Nonterminal
TopLevel
GroupBy
LimitTo
Show
PrepositionalClause
Determiner
NounPhrase
List
Figure 1: Semantic Grammar Element Classes
where the noun phrase consists of a determiner such
as ?some?, ?no?, ?at least N?, ?exactly N? and a
noun referring to a measure.
The semantic grammar makes use of classes in an
inheritance hierarchy as shown in Figure 1. Each
question element corresponds to a parametrized ter-
minal or nonterminal. That is, it can play a role of
one of multiple terminals or nonterminals depend-
ing on its initialization parameters. There are alto-
gether 13 classes that comprise the elements of the
grammar. The implementations of the different class
elements make use of semantic constraints as appro-
priate. Only minimal human intervention is required
when adapting the system to a new OLAP cube. The
intervention consists of ?cleaning up? the automat-
ically generated terminal symbols of the semantic
grammar so that the plural and singular forms that
were present in the cube metadata are used consis-
tently and so that the mass vs. countable attribute of
each measure is set appropriately.
3 Evaluation
An evaluation of this kind of system requires an
examination of three performance metrics: domain
coverage, ease of use, and query efficiency. How
well the system covers the target domain is crucially
important. In order to measure domain coverage, we
need to determine how many answerable questions
can actually be answered using the system. We can
answer this question in part by examining the user
interface. Does the interface restrict users? access
to domain elements and relationships? A more thor-
ough assessment of domain coverage requires exten-
117
sive user studies.
Ease of use is often thought of as a qualitative
measure of performance, but a systematic, objective
evaluation requires us to define a quantitative mea-
sure. The primary action used to generate queries
in our system is the ?click.? Users click on items to
refine their queries, so the number of clicks required
to generate queries seems like a reasonable starting
point for evaluating ease of use. The time it takes
users to make those clicks is important. A four-click
query sounds efficient, but if it takes the user two
minutes to figure out which four clicks need to be
made, not much is gained. It would be ideal if the
number of clicks and the time needed to make those
clicks grow proportionally. That is, we do not want
to penalize users who need to build longer queries.
Query efficiency is measured by the time between
the user submitting a query and the system present-
ing the answer. How long must a user wait while
data is being fetched and the report generated? Un-
like ease of use, this is objectively measurable and
easy to benchmark.
In our initial evaluation, we applied these metrics
to a selection of 12 natural language questions about
the data in the Adventure Works (Codeplex Open
Source Community, 2008) database that could be
answered by our natural language query construc-
tion system. These questions were generated by a
user with prior exposure to the Adventure Works
database but no prior exposure to the query construc-
tion software system or its design or algorithms, so
the questions are not purposely fine-tuned to yield
artificially optimal results. Eight of these questions
were directly answerable, while four were indirectly
answerable. For each of these questions, we mea-
sured the number of clicks required to generate the
query string, the time it took to make the required
clicks, and the time required to retrieve the needed
records and generate a report. The distinction be-
tween directly answerable and indirectly answerable
questions deserves a short explanation. A question
is deemed directly answerable if the answer is the
sole result returned in the report or if the answer is
included in a group of results returned. A question is
deemed indirectly answerable if the report generated
based on a related query can be used to calculate the
answer or if the information relevant to the answer
is a subset of the information returned. So, the ques-
tion What are the top 20 products based on inter-
net sales was directly answerable through the con-
structed query Show products with one of 20 highest
internet sales amount, while the question What is the
averagefreight cost for internet orders over $1000
could only be answered Show internet freight cost
for customers with more than 1000 dollars of inter-
net sales amount and for each date.
We found that a user was able to construct nat-
ural language queries using between 2 and 6 clicks
which required 10 and 57 seconds of elaspsed time
for the construction process. On average 3.3 clicks
were required to create a query with an average time
of 33 seconds, where the time grew in a linear man-
ner based on the number of clicks. Once a query was
constructed, the average time to generate a report
was 6.7 seconds with the vast majority of queries
producing a report from the database system in 4
seconds or less. The median values for query con-
struction was 2.5 clicks, query construction was 31.5
seconds, and report generation was 4 seconds..
4 Analysis and Conclusions
Our evaluation suggests that the menu driven NLG
approach results in the rapid creation of unambigu-
ous queries that can retrieve the relevant database
information corresponding to the query. It has been
embedded in a system that uses OLAP cubes to
produce database reports (and dashboards) that al-
low user interaction with the retrieved information.
The system was automatically adapded to a given
OLAP cube (only minimal human intervention was
required) and can be equally easily adapted to other
OLAP cubes serving other domains.
Our results build on semantic web related work
(Paiva et al, 2010) that shows that use of NLG for
guided queries construction can be an effective al-
ternative to a natural language interface to an in-
formation retrieval system. We deal with a highly
constrained natural language (cf. the analysis gram-
mars used by (Nelken and Francez, 2000; Thorne
and Calvanese, 2012)) that is effective in generation
of database queries and the generation (not analysis)
of natural language. Like (Paiva et al, 2010), we
rely on a semantic grammar, but instead build on the
information that can be automatically extracted from
the database model, rather than leveraging knowl-
118
edge from semantic web resources. Furthermore, we
provide a more detailed evaluation as to the effec-
tiveness of the guided query construction technique.
Use of OLAP in NLG has also been explored in
the context of content planning (Favero and Robin,
2000), and can play an important role in dealing with
domain portability issues not only in the context of
NLG but also in other natural language database ap-
plications. Our technique for leveraging the data
model and OLAP cube avoids human customization
techniques like those reported by (Minock, 2010)
where an explicit mapping between phrases and
database relations and entities needs to be provided,
and (Evans and Power, 2003) where explicit domain
information needs to be entered.
The NLG query construction approach does have
limitations, since users will likely have questions
that either cannot be constructed by the seman-
tic grammar, or that cannot be answered from the
underlying database. However, issues related to
choice or ambiguity that are frequently encountered
by NLG systems in particular, and natural language
processing systems in general, can be avoided by
having a human ?in the loop.?
Efficiency and effectiveness is derived from how
we leverage human knowledge, both in query com-
position and result interpretation. In traditional,
non-intelligent query scenarios, users know what
they want to ask but not necessarily how to ask it.
By guiding the user through the NLG process, the
user can focus on the what not the how. Database
reports are generated quickly, providing unambigu-
ous answers in a clear, flexible format. and in a fa-
miliar, comfortable, un-intimidating web-based en-
vironment. Aside from usability benefits, this web-
based approach has the added benefit of minimizing
configuration and maintenance.
Our results are only suggestive, since they involve
only 12 questions. They suggest it would be worth-
while to expend the resources for a full study that
includes multiple users with different levels of ex-
perience, multiple domains and larger sets of ques-
tions. A more fine-grained analysis of the differ-
ence between the results sets of constructed English
queries and the expected answers to original ques-
tions should also be performed along with an evalu-
ation of how easy it is for the user to find the answer
to the question within the database report.
References
Codeplex Open Source Community. 2008. Adventure-
works SQL Database Product Samples. CODEPLEX.
http://msftdbprodsamples.codeplex.com.
Faraj A. El-Mouadib, Zakaria S. Zubi, Ahmed A. Alma-
grous, and Irdess S. El-Feghi. 2009. Generic inter-
active natural language interface to databases (GIN-
LIDB). Int Journal of Computers, 3:301?310.
Roger Evans and Richard Power. 2003. WYSIWYM
- building user interfaces with natural language feed-
back. In Proc. of EACL 2003, 10th Conf. of the Euro-
pean Chapter of the ACL, pages 203?206, Budapest,
Hungary.
Eloi Favero and Jacques Robin. 2000. Using OLAP
and data mining for content planning in natural lan-
guage generation. In NLDB ?00 Proc. 5th Interna-
tional Conference on Applications of Natural Lan-
guage to Information Systems-Revised Papers, pages
164?175. Springer-Verlag, London.
Phil Janus and Guy Fouche. 2009. Introduction to olap.
In Pro SQL Server 2008 Analysis Services, pages 1?
14. Springer-Verlag.
Epaminondas Kapetanios, Vijayan Sugumaran, and Myra
Spiliopoulou. 2010. Special issue: 13th international
conference on natural language and information sys-
tems (NLDB 2008) five selected and extended papers.
Data and Knowledge Engineering, 69.
Boris Katz. 1988. Using english for indexing and re-
trieving. In Proceedings of the First RIAO Conference
on User-Oriented Content-Based Text and Image Han-
dling (RIAO ?88). CID.
Michael Minock. 2010. C-PHRASE: a system for build-
ing robust natural language interfaces to databases.
Data and Knowledge Engineering, 69:290?302.
Rani Nelken and Nissim Francez. 2000. Querying
temporal databases using controlled natural language.
In Proc 18th International Conference on Computa-
tional Linguistics (COLING 2000), pages 1076?1080,
Saarbru?cken, Germany, August.
Sara Paiva, Manuel Ramos-Cabrer, and Alberto Gil-
Solla. 2010. Automatic query generation in guided
systems: natural language generation from graphically
built query. In Proc 11th ACIS Intl Conf on Software
Engineering, Artificial Intelligence, Networking and
Parallel/Distributed Computing (SNPD 2010), pages
165?170. IEEE Conf Publishing Services.
Harry Tennant, Kenneth Ross, Richard Saenz, Craig
Thompson, and James Miller. 1983. Menu-based
natural language understanding. In Proc 21st annual
meeting of the Association of Computational Linguis-
tics, pages 151?158. ACL.
Camilo Thorne and Diego Calvanese. 2012. Tractabil-
ity and intractability of controlled languages for data
access. Studia Logica, to appear.
119
Proceedings of the 14th European Workshop on Natural Language Generation, pages 105?114,
Sofia, Bulgaria, August 8-9 2013. c?2013 Association for Computational Linguistics
Generating Natural Language Questions to Support Learning On-Line
David Lindberg Fred Popowich
School of Computing Science
Simon Fraser University
Burnaby, BC, CANADA
dll4,popowich@sfu.ca
John Nesbit Phil Winne
Faculty of Education
Simon Fraser University
Burnaby, BC, CANADA
nesbit,winne@sfu.ca
Abstract
When instructors prepare learning materi-
als for students, they frequently develop
accompanying questions to guide learn-
ing. Natural language processing technol-
ogy can be used to automatically generate
such questions but techniques used have
not fully leveraged semantic information
contained in the learning materials or the
full context in which the question genera-
tion task occurs. We introduce a sophisti-
cated template-based approach that incor-
porates semantic role labels into a system
that automatically generates natural lan-
guage questions to support online learn-
ing. While we have not yet incorporated
the full learning context into our approach,
our preliminary evaluation and evaluation
methodology indicate our approach is a
promising one for supporting learning.
1 Introduction
Ample research (e.g., Callender and McDaniel,
2007) shows that learners learn more, and more
deeply, if they are prompted to examine their
learning materials while and after they study. Of-
ten, these prompts consist of questions related to
the learning materials. After reading a given pas-
sage or section of text, learners are familiar with
learning exercises which consist of questions they
need to answer.
Questioning is one of the most common and in-
tensively studied instructional strategies used by
teachers (Rus and Graesser, 1989). Questions em-
bedded in text, or presented while learners are
studying text, are hypothesized to promote self-
explanation which is known to increase compre-
hension and enhance transfer of learning (e.g.,
Rittle-Johnson, 2006).
Traditionally, these questions have been con-
structed by educators. Recent research, though,
has investigated how natural language processing
techniques can be used to automatically generate
these questions (Kalady et al, 2010; Varga and
Ha, 2010; Ali et al, 2010; Mannem et al, 2010).
While the automated approaches have generally
focussed on syntactic features, we propose an ap-
proach that also takes semantic features into ac-
count, in conjunction with domain dependent and
domain independent templates motivated by ed-
ucational research. After introducing our ques-
tion generation system, we will provide a prelimi-
nary analysis of the performance of the system on
educational material, and then outline our future
plans to tailor the questions to the needs of spe-
cific learners and specific learning outcomes.
2 Question Generation from Text
The task of question generation (QG) from text
can be broadly divided into three (not entirely dis-
joint) categories: syntax-based, semantics-based,
and template-based. Systems in the syntactic cat-
egory often use elements of semantics and vice-
versa. A system we would call template-based
must to some extent use syntactic and/or seman-
tic information. Regardless of the approach taken,
systems must perform at least four tasks:
1. content selection: picking spans of source
text (typically single sentences) from which
questions can be generated
2. target identification: determining which spe-
cific words and/or phrases should be asked
about
3. question formulation: determining the appro-
priate question(s) given the content identified
4. surface form generation: producing the final
surface-form realization
Task 2 need not always precede task 3; target
identification can drive question formulation and
105
vice-versa. A system constrained to generating
specific kinds of questions will select only the tar-
gets appropriate for those kinds of questions. Con-
versely, a system with broader generation capa-
bilities might pick targets more freely and (ide-
ally) generate only the questions that are appro-
priate for those targets. We consider the methods
used in performing tasks 2 and 4 to be the pri-
mary discriminators in determining the category
into which a given method is best placed. This is
not the only way one might classify a QG system.
However, we believe this method allows us to best
compare and contrast our approach with previous
approaches.
Syntax-based methods comprise a large portion
of the existing literature. Kalady et al (2012),
Varga and Ha (2010), Wolfe (1976), and Ali et
al. (2010) provide a sample of these methods. Al-
though each of these efforts has differed on a few
details, they have followed the same basic strat-
egy: parse sentences using a syntactic parser, sim-
plify complex sentences, identify key phrases, and
apply syntactic transformation rules and question
word replacement.
The methods we have labeled ?semantics-
based? use method(s) of target identification (task
2) that are primarily semantic, using techniques
such as semantic role labeling (SRL). Given a sen-
tence, a semantic role labeler identifies the pred-
icates (relations and actions) along with the se-
mantic entities associated with each predicate. Se-
mantic roles, as defined in PropBank (Palmer et
al., 2005), include Arg0, Arg1, ..., Arg5, and
ArgA. A set of modifiers is also defined and in-
cludes ArgM-LOC (location), ArgM-EXT (ex-
tent), ArgM-DIS (discourse), ArgM-ADV (adver-
bial), ArgM-NEG (negation), ArgM-MOD (modal
verb), ArgM-CAU (cause), ArgM-TMP (time),
ArgM-PNC (purpose), ArgM-MNR (manner), and
ArgM-DIR (direction). We adopt the shorter
CoNLL SRL shared task naming conventions
(Carreras and Ma`rquez, 2005) (e.g., A0 and AM-
LOC).
Mannem et al (2010), for example, introduce a
semantics-based system that combines SRL with
syntactic transformations. In the content selec-
tion stage, a single sentence is first parsed with
a semantic role labeler to identify potential tar-
gets. Targets are selected using simple selec-
tion criteria. Any of the predicate-specific se-
mantic arguments (A0-A5), if present, are consid-
ered valid targets. Mannem et al further iden-
tify modifiers AM-MNR, AM-PUNC, AM-CAU,
AM-TMP, AM-LOC, and AM-DIS as potential
targets. These roles are used to generate addi-
tional questions that cannot be attained using only
the A0-A5 roles. For example, AM-LOC can be
used to generate a where question, and an AM-
TMP can be used to generate a when question. Af-
ter targets have been identified, these, along with
the complete SRL parse of the sentence are passed
to the question formulation stage. Two heuristics
are used to rank the generated questions. Ques-
tions are ranked first by the depth of their predi-
cate in the dependency parse of the original ques-
tion. This is based on the assumption that ques-
tions arising from main clauses are more desir-
able than those generated from deeper predicates.
In the second stage, questions with the same rank
are re-ranked according to the number of pronouns
they contain, with questions with fewer pronouns
having higher rank.
One limitation of the syntax and semantics-
based methods is that they generate questions by
rearranging the surface form of sentences. Ques-
tion templates offer the ability to ask questions that
are not so tightly-coupled to the exact wording of
the source text. A question template is any pre-
defined text with placeholder variables to be re-
placed with content from the source text. Ques-
tion templates allow question generation systems
to leverage human expertise in language genera-
tion.
The template-based system of Cai et al (2006)
uses Natural Language Generation Markup Lan-
guage (NLGML), a language that can be used to
generate not only questions but any natural lan-
guage expression. NLGML uses syntactic pattern
matching and semantic features for content selec-
tion and question templates to guide question for-
mulation and surface-form realization. Note that
a pattern need not specify a complete syntax tree.
Additionally, patterns can impose semantic con-
straints. However, simple ?copy and paste? tem-
plates are not a panacea for surface-form real-
ization. Mechanisms for changing capitalization
of words and changing verb conjugation (when
source sentence verbs are to appear in the output
text) need to be provided: NLGML provides some
such functions.
106
3 Our Approach
We develop a template-based framework for QG.
The primary motivation for this decision is the
ability of a template-based approach to generate
questions that are not merely declarative to in-
terrogative transformations. We aim to address
some of the limitations of the existing approaches
outlined in the previous section while leveraging
some of their strengths in novel ways. We com-
bine the benefits of a semantics-based approach,
the most important of which is not being tightly-
constrained by syntax, with the surface-form flex-
ibility of a template-based approach.
The data used to develop our approach was ob-
tained from a collection of 25 documents prepared
for educational research purposes within the Fac-
ulty of Education at SFU. All hand-coded rules
we describe below were motivated by patterns ob-
served in this development data. This collection
was modeled after a high-school science curricu-
lum on global warming, with vocabulary and dis-
course appropriate for learners in that age group.
Although the collection included a glossary of key
terms and their definitions, this resource was used
only for evaluation purposes as described in Sec-
tion 4.
3.1 Semantic-based templates
Previous template-based methods have used syn-
tactic pattern matching, which does provide a
great deal of flexibility in specifying sentences
appropriate for generating certain types of ques-
tions. However, this flexibility comes at the ex-
pense of generality. As seen in Wyse and Piwek
(2009), who use Stanford Tregex (Levy and An-
drew, 2006) for pattern matching, the specificity of
syntactic patterns can make it difficult to specify
a syntactic pattern of the desired scope. Further-
more, semantically similar entities can span dif-
ferent syntactic structures, and matching these re-
quires either multiple patterns (in the case of Cai
et al, 2006) or a more complicated pattern (in the
case of Wyse and Piwek, 2009).
If we want to develop templates that are se-
mantically motivated, more flexible in terms of
the content they successfully match, and more ap-
proachable for non-technical users, we need to
move away from syntactic pattern matching. In-
stead, we match semantic patterns. We define a
semantic pattern as the SRL parse of a sentence
and the named entities (if any) contained within
the span of each semantic role. We use Stanford
NER (Finkel et al, 2005) for named entity recog-
nition. Figure 1 shows a sentence and its corre-
sponding semantic pattern. Notice this sentence
has two predicates, each with its own semantic ar-
guments. Each of these predicate-argument struc-
tures is a distinct predicate frame.
Figure 1: A sentence and its semantic pattern
Even the shallow semantics of SRL can identify
the semantically interesting portions of a sentence,
and these semantically-meaningful substrings can
span a range of syntactic patterns. Figure 2
shows a clear example of this phenomenon. In
this example, we see two sentences expressing
the same semantic relationship between two con-
cepts, namely, the fact that trapped heat causes
the Earth?s temperature to increase. In one case,
this causation is expressed in an adjective phrase,
while the other uses a sentence-initial preposi-
tional phrase. The parse trees are generated using
the Stanford Parser (Klein and Manning, 2003).
The AM-CAU semantic role captures the cause in
both sentences. It is impossible to accomplish the
same feat with a single NLGML pattern. However,
it is possible to capture both with a single Tregex
pattern.
The principle advantage of semantic pattern
matching is that a single semantic pattern casts a
narrow semantic net while casting a large syntactic
net. This means fewer patterns need to be defined
by the template author, and the patterns are more
compact.
Our templates have three components: plain-
text, slots, and slot options. Plaintext forms the
107
Figure 2: Two different syntax subtrees subsumed
by a single semantic role
skeleton into which semantically-meaningful sub-
strings of a source sentence are inserted to create a
question. The only restrictions on the plaintext is
that it cannot contain any text that looks like a slot
but is not intended as one, and it cannot contain
the character sequence used to delineate the plain-
text from the slots appearing outside the plaintext.
Aside from these restrictions, any desired text is
valid.
Slots facilitate sentence and template matching.
They accept specific semantic arguments, and can
appear inside or outside the plaintext. These pro-
vide the semantic pattern against which a source
sentence is matched. A slot inside the plaintext
acts as a variable to be replaced by the correspond-
ing semantic role text from a matching sentence,
while any slots appearing outside the plaintext
serve only to provide additional pattern match-
ing criteria. The template author does not need
to specify the complete semantic pattern in each
template. Instead, only the portions relevant to the
desired question need to be specified. This is an
important point of contrast between our template-
based approach vs. syntax and semantics-based
approaches. We can choose to generate questions
that do not include any predicates from the source
sentence but instead ask more abstract or general
questions about other semantic constituents. We
believe these kinds of questions are better able to
escape the realm of the factoid, because they are
not constrained to the actions and relations de-
scribed by predicates.
Slot options function much like NLGML func-
tions and are of two types: modifiers and filters.
Modifiers apply transformations to the role text in-
serted into a slot, and filters enforce finer-grained
matching criteria. Predicate slots have their own
distinct set of options, while the other semantic
roles share a common set of options. A template?s
slots and filters describe the necessary conditions
for the template to be matched with a source sen-
tence semantic pattern.
3.2 Predicate slot options
The predicate filter options restrict the predicates
that can match a predicate slot. With no filter
options specified, any predicate is considered a
match. Table 1 shows the complete list of filters.
Filter Description
be predicate lemma must not be ?be?
!be predicate lemma must be ?be?
!have predicate lemma must not be ?have?
Table 1: Predicate filters
The selection of predicate filters might at first
seem oddly limited. Failing to consider the func-
tional differences between various types of verbs
(particularly auxiliary and copula) would indeed
produce low-quality questions and should in fact
be ignored in most cases. For example, consider
the sentence ?Dinosaurs, along with many other
animals, became extinct approximately 65 mil-
lion years ago.? A question such as ?What did
dinosaurs, along with many other animals, be-
come?? is not particularly useful. We can rec-
ognize copula predicates by their surrounding se-
mantic pattern, so in the broad sense, we do not
need to adopt any copula-specific rules.
The one exception to the above rule is any cop-
ula whose lemma is be. The be and !be filters
allow the presence or absence of such a predicate
to be detected. This capability is useful for two
reasons. First, the presence of such a predicate
gives us an inexpensive way to generate defini-
tion questions, even if the source text is not writ-
ten in the form of a definition. Although this will
over-generate definition questions, non-predicate
108
filters can be used to add additional mitigating
constraints. Second, requiring the absence of such
a predicate allows us to actively avoid generat-
ing certain kinds of ungrammatical or meaning-
less questions. Whether using one of these predi-
cates results in ungrammatical questions depends
on the wording of the underlying template, so we
provide the !be filter for the template author to
use as needed. Consider the sentence ?El Nino
is caused when the westerly winds are unusually
weak.? Without the !be filter, one of our tem-
plates would generate the question ?When can El
Nino be?? Applying the !be filter prevents this
question from being generated.
Like copula, auxiliary verbs are often not suit-
able for question generation. Fortunately, many
auxiliary verbs are also modal and are assigned
the label AM-MOD and so do not form predi-
cate frames of their own. Instead, they are in-
cluded in the frame of the predicate they modify.
In other cases auxiliary verbs are not modal, such
as in the sentence ?So far, scientists have not been
able to predict the long term effects of this wob-
ble.? In this case, the auxiliary have is treated as
a separate predicate, but importantly, the span of
its A1 includes the predicate been. We provide
a non-predicate filter to prevent generation when
this overlap is present.
The !have filter is motivated by the observa-
tion that the predicate have can appear as a full,
non-copula predicate (with an A0 and A1) but of-
ten does not yield high-quality questions. For ex-
ample, consider the sentence ?This effect can have
a large impact on the Earth?s climate.? Without the
!have filter, one of our templates would gener-
ate the question ?What can this effect have?? With
the !have filter, that template does not yield any
questions from the given sentence.
Predicate modifiers allow the template author to
explicitly force a change in conjugation. See Ta-
ble 2 for the complete set of predicate modifiers,
where fps is an abbreviation for first person sin-
gular, sps for second person singular, and so on.
The lemma modifier can appear on its own. How-
ever, all other conjugation changes must specify
both a tense and a person. If no modifiers are used,
the predicate is copied as-is from the source sen-
tence. Although perfect is an aspect rather than a
tense, MorphAdorner1, which we use to conjugate
predicates, defines it as a tense, so we have imple-
1http://morphadorner.northwestern.edu
mented it as a tense filter.
Modifier Tense Modifier
lemma lemma (dictionary form) fps
pres present sps
prespart present participle tps
past past fpp
pastpart past participle spp
perf perfect tpp
pastperf past perfect
pastperfpart past perfect participle
Table 2: Predicate modifiers
3.3 Non-predicate slot options
The filters for non-predicate slots impose addi-
tional syntactic and named entity restrictions on
any matching role text. As with predicates, the
absence of any non-predicate filters results in the
mere presence of the corresponding semantic role
being sufficient for matching. See Table 3 for the
complete list of non-predicate filters describing re-
strictions on the role text (RT), role span (RS), and
predicate frame (PF) in terms of the semantic type
of named entities (and in some cases in terms of
non-semantic features).
Filter Description
null PF must not contain this semantic role.
!nv RS must not contain a predicate
dur RT must contain DURATION
date RT must contain DATE
!date RT must not contain a DATE
loc RT must contain a LOCATION.
ne RT must contain a named entity
misc RT must contain a MISC
comp RT must contain a comparison
!comma RT must not contain a comma
singular RT must be singular
plural RT must be plural
Table 3: Non-predicate filters
The choice of filters again requires some expla-
nation. The null and !nv filters were foreshad-
owed above. For slots appearing outside the tem-
plate?s plaintext, the null filter explicitly requires
that the corresponding semantic role not be present
in a source sentence semantic pattern. An A0 slot
paired with the null filter is the mechanism al-
luded to earlier that allows for the recognition of
copula predicates without the need to examine the
predicate itself. The !nv filter can be used to pre-
vent ungrammatical questions. We observe that
if a role span does include a predicate, resulting
questions are often ungrammatical due to the con-
jugation of that predicate. Applying this filter to
109
the A1 of a predicate prevents generation from a
predicate frame whose predicate is a non-modal
auxiliary verb.
The named entity filters (dur, !dur, date,
loc, ne, and misc) are those most relevant to
the corpus we have used to evaluate our approach
and thus the easiest to experiment with effectively.
Because named entities are used only for filtering,
expanding the set of named entity filters is a trivial
task.
The filters comp, !comma, singular, and
plural are syntax-based filters. With the ex-
ception of !comma, these filters force the exam-
ination of the part-of-speech (POS) tags to de-
tect the desired features. The singular and
plural filters let templates be tailored to singu-
lar and plural arguments in any desired way, be-
yond simply selecting appropriate auxiliary verbs.
The type of comparison we search for when the
comp filter is used is quite specific. We search
for phrases that describe conditions that are atypi-
cal. These can be seen in phrases such ?unusually
weak,? ?unseasonably warm,? ?strangely absent,?
and so on. These phrases are present when a word
whose POS tag is RB (adverb) is followed by a
word whose tag is JJ (adjective). Consider a sen-
tence such as ?El Nino is caused when the westerly
winds are unusually weak.? The comp filter allows
us to generate questions such as ?What data would
indicate El Nino?? or ?How do the conditions that
cause El Nino differ from normal conditions?? Al-
though this heuristic does produce both false pos-
itives and false negatives, other syntactic features
such as comparative adverbs and comparative ad-
jectives are less semantically constrained. Further
investigation is needed to determine more flexible
ways to recognize descriptions of atypical condi-
tions.
We see two situations in which a comma ap-
pears within the span of a single semantic role.
The first situation occurs when a list of nouns is
serving the role, such as in ?Climate change in-
cludes changes in precipitation, temperature, and
pressure.? Here, ?changes in precipitation, temper-
ature, and pressure? is the A1 of the predicate in-
cludes. In cases where a question is only appro-
priate for single concept (e.g. temperature) rather
than a set of concepts, the !comma filter pre-
vents such a question from being generated from
the sentence above. This has implications for role
text containing appositives, the second situation in
which a comma appears within a single role span.
Such roles are rejected when !comma is used.
This is not ideal, as removing appositives does not
cause semantic roles to be lost from a semantic
pattern. Future work will address this problem.
The non-predicate modifiers (Table 4) serve two
purposes: to create more fluent questions and to
remove non-essential text. Note that the -tpp,
which forces the removal of trailing prepositional
phrases, can have undesired results when applied
to certain modifier roles, such as AM-LOC, AM-
MNR, and AM-TMP, when they appear in the tem-
plate plaintext. These modifiers often contain only
a prepositional phrase, and in such cases, -tpp
will result in an empty string being placed into the
template.
Modifier Effect
-lp If initial token is prep, remove it
-tpp If RT ends with PP, remove PP
-ldt If initial token is determiner, remove it
Table 4: Non-predicate modifiers
3.4 Our QG system
Figure 3 shows the architecture and data flow of
our QG system. One of the most important things
to observe about this architecture is that the tem-
plates are an external input. They are in no way
coupled to the system and can be modified as
needed without any system modifications.
Compared to most other approaches, we per-
form very little pre-processing. Syntax-based
methods in particular have been motivated to per-
form sentence simplification, because their meth-
ods are more likely to generate meaningful ques-
tions from short, succinct sentences. We have cho-
sen not to perform any sentence simplification.
This decision was motivated by the observation
that common methods of sentence simplification
can eliminate useful semantic content. For exam-
ple, Kalady et al (2010) claim that prepositional
phrases are often not fundamental to the meaning
of a sentence, so they remove them when simpli-
fying a sentence. However, as Figure 4 shows,
a prepositional phrase can contain important se-
mantic information. In that example, removing the
prepositional phrase causes temporal information
to be lost.
One pre-processing step we do perform is
pronominal anaphora resolution (Charniak and El-
sner, 2009). Even though we do not split com-
110
Figure 3: System architecture and data flow
Figure 4: Semantic information can be lost dur-
ing sentence simplification. Removing the prepo-
sitional phrase from the first sentence leaves the
simpler second sentence, but the AM-TMP modi-
fier is lost.
plex sentences and therefore do not create new
sentences in which pronouns are separated from
their antecedents, this kind of anaphora resolution
remains an important step in limiting the number
of vague questions.
Each source sentence is tokenized and anno-
tated with POS tags, named entities, lemmata, and
its SRL parse. SRL is the cornerstone of our ap-
proach. We generate the SRL parse (Collobert
et al, 2011) in order to extract a set of predicate
frames. Questions are generated from individ-
ual predicate frames rather than entire sentences
(unless the sentence contains only one predicate
frame). Given a sentence, the semantic pattern of
each of its predicate frames is compared against
that of each template. Algorithm 1 describes the
process of matching a single predicate frame (pf )
to a single template (t). Although it is not stated in
Algorithm 1, the sentence-level tokenization, lem-
mata, named entities and POS tags are checked
as needed according to the template?s slot filters.
If a predicate frame and template are matched,
they are passed to Algorithm 2, which fills tem-
plate slots with role text to produce a question.
Even in the absence of modifiers, all role text re-
ceives some additional processing before being in-
serted into its corresponding slot. These additional
steps include the removal of colons and the things
they introduce and the removal of text contained
in parentheses. We observe that these extra steps
lead to questions that are more meaningful.
Algorithm 1 patternsMatch(pf ,t)
for all slot ? t do
if pf does not have slot.role then
if null 6? slot.filters then
return false
end if
else
for all filter ? slot.filters do
if pf.role does not match filter then
return false
end if
end for
end if
end for
return true
Because we generate questions from predicate
frames rather than entire sentences, two sentences
describing the same semantic entities might result
in duplicate questions. To avoid duplicates we
keep only the first occurrence of a question.
Using slots and filters, we can now create some
interesting templates and see the questions they
111
Algorithm 2 fillTemplate(t,pf )
question text? t.plaintext
for all slot ? t.plaintext slots do
role text? pf.role(slot.role).text
for all modifier ? slot.modifiers do
applyModifier(role text,modifier)
end for
In question text, replace slot with role text
end for
return question text
yield. Table 5 shows some templates (T) that
match the sentence in Figure 1 and the questions
(Q) that result. Although the questions that are
generated are not answerable from the original
sentence, they were judged answerable from the
source document in our evaluation. The full set of
templates is provided in (Lindberg, 2013).
As recently as 12,500 years ago, the Earth was in the
midst of a glacial age referred to as the Last Ice Age.
T: How would you describe [A2 -lp misc]?
Q: How would you describe the Last Ice Age?
T: Summarize the influence of [A1 -lp !comma !nv] on
the environment.
Q: Summarize the influence of a glacial age on the envi-
ronment.
T: What caused [A2 -lp !nv misc]? ## [A0 null]
Q: What caused the Last Ice Age?
Table 5: A few sample templates and questions
4 Evaluation
There remains no standard set of evaluation met-
rics for assessing the quality of question gener-
ation output. Some present no evaluation at all
(Wyse and Piwek, 2009; Stanescu et al, 2008).
Among those who do perform an evaluation, there
does appear to be a consensus that some form
of human evaluation is necessary. Despite this
agreement in principle, approaches tend to diverge
thereafter. There are differences in the evaluation
criteria and the evaluation procedure.
Most previous efforts in QG have not gone be-
yond manual evaluation. While some have gone
a step further and built models for ranking based
on the probability of a question being acceptable
(Heilman and Smith, 2010), these models have not
had a strong basis in pedagogy. While a question
that is both syntactically and semantically well-
formed is considered acceptable in some evalua-
tion schemes, such questions can greatly outnum-
ber the questions that we can reasonably expect a
student would want or have time to answer. We
implement a classifier that attempts to identify the
questions that are the most pedagogically useful.
For our initial evaluation of the performance of
our QG system, we selected a subset of 10 doc-
uments from the collection described in the previ-
ous section. On average, each document contained
25 sentences. From the 10 documents, our system
generated 1472 questions in total, an average of
5.9 questions per sentence. Due to the educational
nature of this material, we needed evaluators with
educational training rather than naive ones. Ac-
cordingly, the questions we generated were evalu-
ated by a graduate student from the Faculty of Ed-
ucation. She was asked to give binary judgements
for grammaticality, semantic validity, vagueness,
answerability, and learning value. For each ques-
tion, two aspects of answerability were evaluated.
The first aspect was whether the question was an-
swerable from the source sentence from which
it was generated. The second was whether the
question was answerable given the source docu-
ment as a whole. The evaluator was given no pre-
determined guidelines regarding the relationships
among the evaluation criteria (e.g., the influence
of vagueness and answerability on learning value).
This aspect of the evaluation was left to her dis-
cretion as an educator. She found that 85% of the
questions were grammatical, with 66% of them ac-
tually making sense. It was determined that 14%
of the questions were answerable from the sen-
tence used to generate them, while 20% of them
were answerable from the document. Finally, she
determined that 17% of the questions had learn-
ing value according to the prescribed learning out-
comes for the curriculum being modeled. Aside
from performing this evaluation, the evaluator was
not involved in this research.
Given this evaluation, we then built a classi-
fier which used logistic regression (L2 regular-
ized log-likelihood) to classify on learning value.
We used length, language model, SRL, named en-
tity, glossary, and syntax features. Length and
language model features measure the token count
and grammaticality of a question and the sentence
from which it was generated. SRL features in-
clude the token count of each semantic role in the
generating predicate frame, whether each role is
required by the matching template, and whether
each role?s text is used. Named entity features
indicate the presence of each of nine named en-
tity types in both the source sentence and gener-
ated question. Glossary features note the number
112
of glossary terms that appear in a sentence and
question and a measure of the average importance
of each term, which we calculated from a sim-
ple in-terms-of graph (Winne and Hadwin, 2013)
we constructed from the glossary. This graph has
directed edges between each glossary term and
the terms that appear in its gloss. Syntax fea-
tures identify the depth of the generating predi-
cate frame in the source sentence and the POS tag
of its predicate. Without adding noise, the train-
ing set had 217 questions with learning value and
1101 questions without learning value. The clas-
sifier obtained precision and recall scores of 0.47
and 0.22 respectively for questions with learning
value, along with scores of 0.79 and 0.92 for ques-
tions with no learning value. We then added noise
to the training set by relabelling any grammati-
cal question that made sense as having learning
value. This relabelling resulted in a training set
of 778 questions with learning value and only 540
questions without learning value. The classifier
trained on this noisy set showed a precision score
on learning value questions decreased to 0.29 but
a dramatic increase in recall to 0.81. For questions
with no learning value, the precision increased
slightly to 0.86 which was offset by a dramatic de-
crease in recall to 0.38. So when the system gener-
ates a poor quality question, we have a high prob-
ability of knowing that it is a poor question which
allows us to then filter or discard it.
5 Conclusions
We have shown how a template-based method,
using predominately semantic information, can
be used to generate natural language questions
for use in an on-line learning system. Our tem-
plates are based on semantic patterns, which cast
a wide syntactic net and a narrow semantic net.
The template mechanism supports rich selectional
and generational capabilities, generating a large
pool from which questions for learners can be
selected. A simple automated technique for se-
lecting questions with learning value was intro-
duced. Although this automated technique shows
promise for some applications, future investiga-
tion into what constitutes a useful question in the
context of a specific task and an individual learner
is needed. Some might argue that it is risky to
generate questions that cannot be answered from
the source sentence from which they were gener-
ated. Although some questions are generated that
are not answered elsewhere in a document, there
is a benefit in learners being able to recognize that
a particular question is not answerable. Our future
work will expand both on the types of potential
questions generated, and on the selection from the
set of potential questions based on the information
an individual learner (a) knows, (b) has available
in a ?library? of saved sources, (c) has operated
on while studying online (e.g., tagged), and (d)
might find in the Internet. To facilitate this further
research, we will be integrating question genera-
tion into the nStudy system (Hadwin et al, 2010;
Winne and Hadwin, 2013). We will also be per-
forming thorough user studies which will evalu-
ate the generated questions from the learner?s per-
spective in addition to the educator?s perspective.
Acknowledgments
This research was supported by an Insight De-
velopment Grant (#430-2012-044) from the So-
cial Sciences and Humanities Research Council
of Canada and a Discovery Grant from the Nat-
ural Sciences and Engineering Research Council
of Canada. The authors are extremely grateful to
Kiran Bisra from the Faculty of Education for pro-
viding information for the evaluation. Finally, spe-
cial thanks to the reviewers for their comments and
suggestions.
References
Husam Ali, Yllias Chali, and Sadid A Hasan. 2010.
Automation of question generation from sentences.
In Proceedings of QG2010: The Third Workshop on
Question Generation, pages 58?67.
Zhiqiang Cai, Vasile Rus, Hyun-Jeong Joyce Kim,
Suresh C. Susarla, Pavan Karnam, and Arthur C.
Graesser. 2006. Nlgml: A markup language
for question generation. In Thomas Reeves and
Shirley Yamashita, editors, Proceedings of World
Conference on E-Learning in Corporate, Govern-
ment, Healthcare, and Higher Education 2006,
pages 2747?2752, Honolulu, Hawaii, USA, Octo-
ber. AACE.
Aimee A. Callender and Mark A. McDaniel. 2007.
The benefits of embedded question adjuncts for low
and high structure builders. Journal Of Educational
Psychology (2007), pages 339?348.
Xavier Carreras and Llu??s Ma`rquez. 2005. Introduc-
tion to the conll-2005 shared task: Semantic role la-
beling. In Proceedings of the Ninth Conference on
Computational Natural Language Learning, pages
152?164. Association for Computational Linguis-
tics.
113
Eugene Charniak and Micha Elsner. 2009. Em works
for pronoun anaphora resolution. In Proceedings
of the 12th Conference of the European Chapter
of the Association for Computational Linguistics,
pages 148?156. Association for Computational Lin-
guistics.
Ronan Collobert, Jason Weston, Le?on Bottou, Michael
Karlen, Koray Kavukcuoglu, and Pavel Kuksa.
2011. Natural language processing (almost) from
scratch. The Journal of Machine Learning Re-
search, 12:2493?2537.
Jenny Rose Finkel, Trond Grenager, and Christopher
Manning. 2005. Incorporating non-local informa-
tion into information extraction systems by gibbs
sampling. In Proceedings of the 43rd Annual Meet-
ing on Association for Computational Linguistics,
pages 363?370. Association for Computational Lin-
guistics.
A.F. Hadwin, M. Oshige, C.L.Z. Gress, and P.H.
Winne. 2010. Innovative ways for using nstudy
to orchestrate and research social aspects of self-
regulated learning. Computers in Human Behaviour
(2010), pages 794?805.
Michael Heilman and Noah A Smith. 2010. Good
question! statistical ranking for question genera-
tion. In Human Language Technologies: The 2010
Annual Conference of the North American Chap-
ter of the Association for Computational Linguistics,
pages 609?617. Association for Computational Lin-
guistics.
Saidalavi Kalady, Ajeesh Elikkottil, and Rajarshi Das.
2010. Natural language question generation using
syntax and keywords. In Proceedings of QG2010:
The Third Workshop on Question Generation, pages
1?10.
Dan Klein and Christopher D Manning. 2003. Ac-
curate unlexicalized parsing. In Proceedings of the
41st Annual Meeting on Association for Computa-
tional Linguistics-Volume 1, pages 423?430. Asso-
ciation for Computational Linguistics.
Roger Levy and Galen Andrew. 2006. Tregex and tsur-
geon: tools for querying and manipulating tree data
structures. In LREC 2006.
David Lindberg. 2013. Automatic question generation
from text for self-directed learning. Master?s thesis,
Simon Fraser University, Canada.
Prashanth Mannem, Rashmi Prasad, and Aravind Joshi.
2010. Question generation from paragraphs at
upenn: Qgstec system description. In Proceedings
of QG2010: The Third Workshop on Question Gen-
eration, pages 84?91.
Martha Palmer, Daniel Gildea, and Paul Kingsbury.
2005. The proposition bank: An annotated cor-
pus of semantic roles. Computational Linguistics,
31(1):71?106.
Bethany Rittle-Johnson. 2006. Promoting transfer:
Effects of self-explanation and direct instruction.
Child Development (2006), pages 1?15.
Vasile Rus and Arthur C Graesser. 1989. Classroom
questioning. In School improvement research series.
Liana Stanescu, Cosmin Stoica Spahiu, Anca Ion, and
Andrei Spahiu. 2008. Question generation for
learning evaluation. In Computer Science and In-
formation Technology, 2008. IMCSIT 2008. Interna-
tional Multiconference on, pages 509?513. IEEE.
Andrea Varga and Le An Ha. 2010. Wlv: A ques-
tion generation system for the qgstec 2010 task b.
In Proceedings of QG2010: The Third Workshop on
Question Generation, pages 80?83.
Philip H Winne and Allyson F Hadwin. 2013. nstudy:
Tracing and supporting self-regulated learning in the
internet. In International handbook of metacog-
nition and learning technologies, pages 293?308.
Springer.
John H Wolfe. 1976. Automatic question gener-
ation from text-an aid to independent study. In
ACM SIGCUE Outlook, volume 10, pages 104?112.
ACM.
Brendan Wyse and Paul Piwek. 2009. Generating
questions from openlearn study units. In AIED
2009 Workshop Proceedings Volume 1: The 2nd
Workshop on Question Generation, 6-9 July 2009,
Brighton, UK.
114
