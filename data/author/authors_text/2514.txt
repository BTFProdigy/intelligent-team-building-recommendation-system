Handling Information Access Dialogue through QA Technologies
? A novel challenge for open-domain question answering ?
Tsuneaki Kato
The University of Tokyo
kato@boz.c.u-tokyo.ac.jp
Fumito Masui
Mie University
masui@ai.info.mie-u.ac.jp
Jun?ichi Fukumoto
Ritsumeikan University
fukumoto@media.ritsumei.ac.jp
Noriko Kando
National Institute of Informatics
kando@nii.ac.jp
Abstract
A novel challenge for evaluating open-domain
question answering technologies is proposed.
In this challenge, question answering systems
are supposed to be used interactively to an-
swer a series of related questions, whereas in
the conventional setting, systems answer iso-
lated questions one by one. Such an interac-
tion occurs in the case of gathering informa-
tion for a report on a specific topic, or when
browsing information of interest to the user. In
this paper, first, we explain the design of the
challenge. We then discuss its reality and show
how the capabilities measured by the challenge
are useful and important in practical situations,
and that the difficulty of the challenge is proper
for evaluating the current state of open-domain
question answering technologies.
1 Introduction
Open-domain question answering technologies allow
users to ask a question in natural language and obtain
the answer itself rather than a list of documents that con-
tain the answer. These technologies make it possible
to retrieve information itself rather than merely docu-
ments, and will lead to new styles of information access
(Voorhees, 2000).
The recent research on open-domain question answer-
ing concentrates on answering factoid questions one by
one in isolation from each other. Such systems that an-
swer isolated factoid questions are the most basic level of
question answering technologies, and will lead to more
sophisticated technologies that can be used by profes-
sional reporters and information analysts. On some stage
of that sophistication, a cub reporter writing an article on
a specific topic will be able to translate the main issue ad-
dressed by his report into a set of simpler questions and
then pose those questions to the question answering sys-
tem (Burger et al, 2001).
In addition, there is a relation between multi-document
summarization and question answering. In his lecture,
Eduard Hovy mentioned that multi-document summa-
rization may be able to be reduced into a series of ques-
tion answering (Hovy, 2001). In SUMMAC, an intrinsic
evaluation was conducted which measures the extent to
which a summary provides answers to a set of obligatory
questions on a given topic (Mani et al, 1998). Those sug-
gest such question answering systems that can answer a
series of related questions would surely be a useful aid to
summarization work by human and by machine.
Against this background, question answering systems
need to be able to answer a series of questions, which
have a common topic and/or share a local context. In
this paper, we propose a challenge to measure objectively
and quantitatively such an ability of question answering
systems. We call this challenge QACIAD (Question An-
swering Challenge for Information Access Dialogue). In
this challenge, question answering systems are used in-
teractively to participate in dialogues for accessing infor-
mation. Such information access dialogue occurs such
as when gathering information for a report on a specific
topic, or when browsing information of interest to the
user. Actually, in QACIAD, the interaction is only simu-
lated and systems answer a series of questions in a batch
mode. Although such a simulation may neglect the in-
herent dynamics of dialogue, it is a practical compromise
for objective evaluation and, as a result, the test sets of
the challenge are reusable.
Question answering systems need a wide range of abil-
ities in order to participate in information access dia-
logues (Burger et al, 2001). First, the systems must re-
spond in real time to make interaction possible. They
must also properly interpret a given question within the
context of a specific dialogue, and also be cooperative
by adding appropriate information not mentioned explic-
itly by the user. Moreover, the systems should be able
to pose a question for clarification to resolve ambiguity
concerning the user?s goal and intentions, and to partici-
pate in mixed initiative dialogue by making suggestions
and leading the user toward solving the problem. Among
these various capabilities, QACIAD focuses on the most
fundamental aspect of dialogue, that is, interpreting a
given question within the context of a specific dialogue.
It measures context processing abilities of systems such
as anaphora resolution and ellipses handling.
This paper is organized as follows. The next chap-
ter explains the design of QACIAD. The following three
chapters discuss the reality of the challenge. First, we ex-
plain the process of constructing the test set of the chal-
lenge and introduce the results of a study conducted dur-
ing this process which show the validity of QACIAD.
That is, QACIAD measures valid abilities needed for
participating in information access dialogues. In other
words, the ability measured by the challenge is crucial
to the systems for realizing information access dialogues
for writing reports and summaries. Second, we show the
statistics of pragmatic phenomena in the constructed test
set, and demonstrate that the challenge covers a wide va-
riety of pragmatic phenomena observed in real dialogues.
Third, based on a preliminary analysis of the QACIAD
run, we show that the challenge has a proper difficulty
for evaluating the current state of open-domain question
answering technologies. In the last two chapters, we dis-
cuss problems identified while constructing the test set
and conducting the run, and draw some conclusions.
2 Design of QACIAD
2.1 History
The origin of QACIAD comes from QAC1 (Question
Answering Challenge), one of the tasks of the NTCIR3
workshop conducted from March 2001 through October
2002 (NTCIR, 2001). QACIAD was originally proposed
in March 2001 as the third subtask of QAC1, its formal
run was conducted in May 2002 (Fukumoto et al, 2001;
Fukumoto et al, 2002; Fukumoto et al, 2003), and the re-
sults were reported at the NTCIR3 workshop meeting in
October 2002. The current design of QACIAD reported
in this paper is based on that challenge and is the result
of extensive elaboration. The design of the challenge and
construction of the test set were performed from January
2003 through December 2003. The formal run was con-
ducted in December 2003, as a subtask of QAC2, which
in turn is a task of the NTCIR4 workshop (NTCIR, 2003).
2.2 QAC as a common ground
QAC is a challenge for evaluating question answering
technologies in Japanese. It consists of three subtasks
including QACIAD, and the common scope of those sub-
tasks covers factoid questions that have names as an-
swers. Here, names mean not only names of proper items
(named entities) including date expressions and monetary
values, but also common names such as names of species
and names of body parts. Although the syntactical range
of the names approximately corresponds to compound
nouns, some of them, such as the titles of novels and
movies, deviate from that range. The underlying docu-
ment set consists of two years of articles of two newspa-
pers in QAC2, and one newspaper in QAC1. Using those
documents as the data source, the systems answer various
open-domain questions.
From the outset, QAC has focused on question answer-
ing technologies that can be used as components of larger
intelligent systems and technologies that can handle re-
alistic problems. It persists in requesting exact answers
rather than the text snippets that contain them with the
cost of avoiding handling definition questions and why
questions, because such answers are crucial in order to be
used as inputs to other intelligent systems such as multi-
document summarization systems. Moreover, as such a
situation is considered to be more realistic, the systems
must collect all the possible correct answers and detect
the absence of an answer. Therefore two subtasks, one of
which is QACIAD, request systems to return one list of
answers that contains all and only correct answers, while
the other subtask requests systems to return a ranked list
of possible answers as in TREC-8. In both subtasks, the
presence of answers in the underlying documents is not
guaranteed and the number of answers is not specified, so
these subtasks are similar to the list question task in the
TREC-2003 style rather than the TREC-10 style (TREC,
2003).
2.3 Information access dialogue
Considering scenes in which those question answering
systems participate in a dialogue, we classified informa-
tion access dialogues into the following two categories.
As discussed later, dialogues in a real situation may have
different features in their different portions; the classifi-
cation just shows two extremes.
Gathering Type The user has a concrete objective such
as writing a report and summary on a specific topic,
and asks a system a series of questions all concern-
ing that topic. The dialogue has a common global
topic, and, as a result, each consecutive question
shares a local context.
Browsing Type The user does not have any fixed topic
of interest; the topic of interest varies as the dialogue
progresses. No global topic covers a whole dialogue
but each consecutive question shares a local context.
This paper proposes the design of the challenge, which
can measure the abilities of question answering systems
useful in such dialogues.
2.4 The setting
QACIAD requests participant systems to return all pos-
sible answers to a series of questions, each of which is a
factoid question that has names as answer. This series of
questions and the answers to those questions comprise an
information access dialogue. Two examples of the series
of questions are shown in Figure 1, which were picked
up from our test set discussed in the next chapter. Se-
ries 14 is a series of a typical gathering type, while series
22 of a typical browsing type. In QACIAD, a number of
series (in the case of our test set, 36 series) are given to
the system at once and systems are requested to answer
those series in a batch mode. One series consists of seven
questions on average. The systems must identify the type
to which a series belongs, as it is not given. The systems
need not identify the changes of series, as the boundary
of series is given. Those, however, must not look ahead
to the questions following the one currently being han-
dled. This restriction reflects the fact that QACIAD is a
simulation of interactive use of question answering sys-
tems in dialogues. This restriction, accompanied with the
existence of two types of series, increases the complexity
of the context processing that the systems must employ.
For example, the systems need to identify that series 22
is a browsing type and the focus of the second question is
Yankee stadium rather than New York Yankees without
looking ahead to the following questions. Especially in
Japanese, since anaphora are not realized often and the
definite and indefinite are not clearly distinguished, those
problems are more serious.
2.5 Evaluation measure
In QACIAD, as the systems are requested to return one
list consisting all and only correct answers and the num-
ber of correct answers differs for each question1, mod-
ified F measure is used for the evaluation, which takes
account of both precision and recall. Two modifications
were needed. The first is for the case where an answer
list returned by a system contains the same answer more
than once or answers in different expressions denoting
the same item. In that case, only one answer is regarded
as the correct one, and so the precision of such answer
list decreases. Cases regarded as different expressions
denoting the same item include a person?s name with and
without the position name, variations of foreign name no-
tation, differences of monetary units used, differences of
time zone referred to, and so on. The second modifica-
tion is for questions with no answer. For those questions,
modified F measure is 1.0 if a system returns an empty
list as the answer, and is 0.0 otherwise.
1It is a special case that the number of answers is just one
for all questions shown in Figure 1.
Series 14
When was Seiji Ozawa born?
Where was he born?
Which university did he graduate from?
Who did he study under?
Who recognized him?
Which orchestra was he conducting in 1998?
Which orchestra will he begin to conduct in 2002?
Series 22
Which stadium is home to the New York Yankees?
When was it built?
How many persons? monuments have been
displayed there?
Whose monument was displayed in 1999?
When did he come to Japan on honeymoon?
Who was the bride at that time?
Who often draws pop art using her as a motif?
What company?s can did he often draw also?
Figure 1: Examples of series of questions
The judgment as to whether a given answer is correct
or not takes into account not only an answer itself but
also the accompanying article from which the answer was
extracted. When the article does not validly support the
answer, that is, assessors cannot understand that the an-
swer is the correct one for a given question by reading
that article, it is regarded as incorrect even though the
answer itself is correct. The correctness of an answer
is determined according to the interpretation of a given
question done by human assessors within the given con-
text. The system?s answers to previous questions, and its
understanding of the context from which those answers
were derived, are irrelevant. For example, the correct an-
swer to the second question of series 22, namely when the
Yankee stadium was built, is 1923. If the system wrongly
answers the Shea stadium to the first question, and then
?correctly? answers the second question 1964, the year
when the Shea stadium was built, that answer to the sec-
ond question is not correct. On the other hand, if the
system answers 1923 to the second question with an ap-
propriate article supporting it, that answer is correct no
matter how the system answered the first question.
3 Constructing a Test Set and Usefulness
of the Challenge
We collected and analyzed questions for two purposes.
The first purpose was to establish a methodology for con-
structing a test set based on the design of QACIAD dis-
cussed in the previous chapter. The second purpose was
to confirm the reality of the challenge, that is, to deter-
mine whether it is useful for information access dialogues
to use question answering systems that can answer ques-
tions that have names as answers.
3.1 Collecting questions
Questions were collected as follows. Subjects were pre-
sented various topics, which included persons, organiza-
tions, and events selected from newspaper articles, and
were requested to make questions that ask for informa-
tion to be used in the report on that topic. The report is
supposed to describe facts on a given topic, rather than
contain opinions or prospects on the topic. The ques-
tions are restricted to wh-type questions, and natural se-
ries of questions containing anaphoric expressions and so
on were constructed. The topics were presented in three
different ways: only by a short description of the topic,
which corresponds to the title part of the TREC topic def-
inition; with a short article or the lead of a longer article,
which is representative of that topic and corresponds to
the narrative part of the TREC topic definition; and with
five articles concerning that topic. The number of top-
ics was 60, selected from two years of newspaper arti-
cles. Thirty subjects participated in the experiment. Each
subject made questions for ten topics for each topic pre-
sentation pattern, and was instructed to make around ten
questions for each topic. It is worth noting that the ques-
tions obtained were natural in both content and expres-
sion since in this experiment the subjects did not consider
whether the answers to their questions would be found in
the newspapers, and some subjects did not read the arti-
cles at all.
This time, for the test set construction and preliminary
analysis, 1,033 questions on 40 topics, made by three sub-
jects for each topic with different topic presentation pat-
terns, were used. All of the questions collected are now
being analyzed extensively, especially on the differences
among questions according to the topic presentation pat-
tern.
3.2 Analysis of the questions
Our main concern here is how many of the questions
collected fall into the category of questions that the cur-
rent question answering systems could answer. In other
words, how many of the questions can be answered by a
list of names? In the case the majority of them fall into
such a category, it is realistic to use question answering
systems for information access dialogues and the chal-
lenge on such abilities must be useful.
Table 1 shows the classification of questions according
to the subject asked. In the case where users ask ques-
tions to get information for a report, the number of why
questions is relatively small. Moreover, there were fewer
questions requesting an explanation or definition than ex-
Table 1: Categorization of questions by subject
Asking about
4W (Who, When, Where, What)
70%incl. several types of numerical values
Why 4%
How, for a procedure or method 10%
Definitions, descriptions or explanations 16%
Table 2: Categorization of questions by answer type
Answered in
Numerical values or date expressions 28%
Proper names 22%
Common names (in compound nouns) 8%
Names probably 14%
Clauses, sentences, or texts 28%
pected, probably because questions such as ?Who is Seiji
Ozawa? were decomposed into relatively concrete ques-
tions such as those asking for his birthday and birth place.
However, not all questions that were categorized as
4W questions could be answered by names. For exam-
ple, whereas questions asking where, such as ?Where was
Shakespeare born??, could be answered by a place name,
questions like ?Where do lobsters like to live?? need a
description and not a proper name as the answer. Table 2
shows the result of categorization according to this as-
pect. This categorization was conducted by inspecting
questions only, and some of the questions were hard to
determine decisively whether those could be answered
by names or not, and so were categorized as ?Names
probably?. For example, the question ?Where does the
name ?AIBO? come from?? could be answered by name
if AIBO is an acronym, but there may be a long story as
to its origin. Although such cases happened in other com-
binations of categories, those questions were categorized
into a more complex category as only the border of names
and descriptions are important in the current analysis.
As Table 2 shows, 58% to 72% of questions could be
answered by names. The amount of those questions is al-
most same as the amount of 4W questions, since while
some 4W questions could not be answered by names,
some definition and explanation questions might be able
to be answered by names. The fact that 58% to 72% of
questions for writing reports could be answered by names
demonstrates that question answering systems that an-
swer these questions are useful in such situations.
In addition, the answers to 84% of those 72% questions
could be found by humans from newspaper articles. This
indicates that the setting is realistic where users write re-
ports through interacting with a question answering sys-
tem that uses newspaper articles as its data source.
3.3 Constructing a test set
Using the questions collected, we constructed a test set
as follows. We selected 26 from 40 topics, and chose
appropriate questions and rearranged them for construct-
ing gathering type series. Some of the questions were
edited in order to resolve semantic or pragmatic ambigui-
ties, though we tried to use the questions without modifi-
cation where possible. The topics of the gathering series
consisted of 5 persons, 2 organizations, 11 events, 5 ar-
tifacts, and 3 animals and fishes, among which 4 topics
concerned sets of organizations and events, such as the
big three companies in the beer industry, simultaneous
terrorist attacks, and annual festival events.
Browsing type series were constructed by using some
of the remaining questions as seeds of a sequence and by
adding new questions to create a flow to/from those ques-
tions. For example, series 22 shown in Figure 1 was com-
posed by adding the last four newly created questions to
the first four questions which were collected for the Yan-
kee stadium2. For such seeds, we also used the collection
of questions for evaluating summarization constructed for
TSC (Text Summarization Challenge), another challenge
in the NTCIR workshop (TSC, 2003). Some topics used
for the question collection were the same as the topics
used in TSC also. We made 10 browsing series in this
way.
Finally, the test set constructed this time contained 36
series and 251 questions, with 26 series of the gather-
ing type and 10 series of the browsing type. The average
number of questions in one series was 6.92.
4 Characteristics of the Test Set
This chapter describes the pragmatic characteristics of
the constructed test set. Japanese has four major types
of anaphoric devices: pronouns, zero pronouns, definite
noun phrases, and ellipses. Zero pronouns are very com-
mon in Japanese in which pronouns are not realized on
the surface. As Japanese also has a completely different
determiner system from English, the difference between
definite and indefinite is not apparent on the surface,
and definite noun phrases usually have the same form
as generic noun phrases. Table 3 shows the summary
of such pragmatic phenomena observed in 215 questions
obtained by removing the first one of each series from
the 251 questions in the test set. The total number is
more than 215 as 12 questions contain more than one phe-
nomenon. The sixth question in series 22, ?Who was the
bride at that time?? is an example of such a question with
2The question focus of the first one was changed.
Table 3: Pragmatic phenomena observed in the test set
Type Occurence
Pronouns 76 (21)
Zero pronouns 134 (33)
Definite noun phrases 11 (4)
Ellipses 7
multiple anaphoric expressions. The numbers in paren-
theses show the number of cases in which the referenced
item is an event. As the table indicates, a wide range of
pragmatic phenomena is observed in the test set.
Precisely speaking, the series in the test set can be
characterized through the pragmatic phenomena that they
contain. Gathering type series consist of questions that
have a common referent in a broad sense, which is a
global topic mentioned in the first question of the series.
Strictly gathering type series can be distinguished as a
special case of gathering type series. In those series, all
questions refer exactly to the same item mentioned in the
first question and do not have any other anaphoric ex-
pression. In other words, questions about the common
topic introduced by the first question comprise a whole
sequence. Series 14 in Figure 1 is an example of the
strictly gathering type and all questions can be interpreted
by supplying Seiji Ozawa, who is introduced in the first
question. The test set has 5 series of the strictly gathering
type. Other gathering type series have other two types of
questions. The first type of questions not only has a ref-
erence to the global topic but also refers to other items or
has an ellipsis. The second type of questions has a refer-
ence to a complex item, such as an event that contains the
global topic as its component. Series 20 shown in Fig-
ure 2 is such a series. The third question refers not only
to the global topic, George Mallory, in this case, but also
to his famous phrase. The sixth one refers to an event
George Mallory was concerned in.
On the other hand, the questions of a browsing type
series do not have such a global topic. Sometimes the
referent is the answer of the immediately preceding ques-
tion, such as the fifth, seventh and eighth questions in
series 22 in Figure 1. No series, however, consists solely
of questions that have only a reference to the answer to
the immediately previous questions. All series contain
references to the answers to non-immediately previous
questions or items mentioned in the previous questions,
or more than one pragmatic phenomenon. In series 22,
the third, fourth and sixth questions belong to such a case.
In both types, therefore, the shifting pattern of the fo-
cus is not simple, and so a sophisticated way is needed to
track it. Such focus tracking is indispensable to get cor-
rect answers. Systems cannot even retrieve articles con-
Series 20
In which country was George Mallory born?
What was his famous phrase?
When did he say it?
How old was he when he started climbing
mountains?
On which expedition did he go missing near the
top of Everest?
When did it happen?
At what altitude on Everest was he seen last?
Who found his body?
Figure 2: Another example of series of questions
taining the answer just by accumulating keywords. This
is clear for the browsing type, as an article is unlikely to
mention both the New York Yankees and Campbell soup.
In the gathering type, since the topics mentioned in rela-
tively many articles were chosen, it is not easy to locate
the answer to a question from those articles retrieved us-
ing that topic as the keyword. For example, there are 155
articles mentioning Seiji Ozawa in our document sets, of
which 22 articles mention his move to the Vienna Phil-
harmonic Orchestra, and only two articles also mention
his birthday. An extensive, quantitative analysis is now
in progress.
5 Difficulty of the Challenge and the
Current State of Technologies
Seven teams and fourteen systems participated in the run
using the test set mentioned in the previous chapter con-
ducted in December 2003. In this chapter, based on a
preliminary analysis of the run, the difficulty of the chal-
lenge and the current state of technologies for addressing
the challenge are discussed. The techniques employed in
the participant systems have not yet been published, but
will be published by the NTCIR workshop 4 meeting at
the latest.
Figure 3 shows the mean modified F measures of the
top 10 participant systems. The chart shows the mean
modified F measure of three categories: all of the test set
questions, the questions of the first of each series, and
questions of the second and after. As anticipated, it is
more difficult to answer correctly the questions other than
the first question of each series. This indicates that more
sophisticated context processing is needed.
The mean modified F measure is not high even for the
top systems. This is probably because of not only the
difficulties of context processing but also the difficulties
of returning the list of all and only correct answers. It
is difficult to achieve high recall since some of the ques-
 
 
  
  
  
  
  	

 
    
    
Proceedings of the Interactive Question Answering Workshop at HLT-NAACL 2006, pages 9?16,
New York City, NY, USA. June 2006. c?2006 Association for Computational Linguistics
WoZ Simulation of Interactive Question Answering
Tsuneaki Kato
The University of Tokyo
kato@boz.c.u-tokyo.ac.jp
Fumito Masui
Mie University
masui@ai.info.mie-u.ac.jp
Jun?ichi Fukumoto
Ritsumeikan University
fukumoto@media.ritsumei.ac.jp
Noriko Kando
National Institute of Informatics
kando@nii.ac.jp
Abstract
QACIAD (Question Answering Chal-
lenge for Information Access Dialogue)
is an evaluation framework for measur-
ing interactive question answering (QA)
technologies. It assumes that users inter-
actively collect information using a QA
system for writing a report on a given
topic and evaluates, among other things,
the capabilities needed under such cir-
cumstances. This paper reports an ex-
periment for examining the assumptions
made by QACIAD. In this experiment, di-
alogues under the situation that QACIAD
assumes are collected using WoZ (Wiz-
ard of Oz) simulating, which is frequently
used for collecting dialogue data for de-
signing speech dialogue systems, and then
analyzed. The results indicate that the set-
ting of QACIAD is real and appropriate
and that one of the important capabilities
for future interactive QA systems is pro-
viding cooperative and helpful responses.
1 Introduction
Open-domain question answering (QA) technolo-
gies allow users to ask a question using natural lan-
guage and obtain the answer itself rather than a list
of documents that contain the answer (Voorhees et
al.2000). While early research in this field concen-
trated on answering factoid questions one by one in
an isolated manner, recent research appears to be
moving in several new directions. Using QA sys-
tems in an interactive environment is one of those
directions. A context task was attempted in order
to evaluate the systems? ability to track context for
supporting interactive user sessions at TREC 2001
(Voorhees 2001). Since TREC 2004, questions in
the task have been given as collections of questions
related to common topics, rather than ones that are
isolated and independent of each other (Voorhees
2004). It is important for researchers to recognize
that such a cohesive manner is natural in QA, al-
though the task itself is not intended for evaluating
context processing abilities since, as it is given the
common topic, sophisticated context processing is
not needed.
Such a direction has also been envisaged as a re-
search roadmap, in which QA systems become more
sophisticated and can be used by professional re-
porters and information analysts (Burger et al2001).
At some stage of that sophistication, a young re-
porter writing an article on a specific topic will be
able to translate the main issue into a set of simpler
questions and pose those questions to the QA sys-
tem.
Another research trend in interactive QA has been
observed in several projects that are part of the
ARDA AQUAINT program. These studies concern
scenario-based QA, the aim of which is to handle
non-factoid, explanatory, analytical questions posed
by users with extensive background knowledge. Is-
sues include managing clarification dialogues in or-
der to disambiguate users? intentions and interests;
and question decomposition to obtain simpler and
more tractable questions (Small et al2003)(Hickl et
9
al.2004).
The nature of questions posed by users and pat-
terns of interaction vary depending on the users who
use a QA system and on the environments in which
it is used (Liddy 2002). The user may be a young re-
porter, a trained analyst, or a common man without
special training. Questions can be answered by sim-
ple names and facts, such as those handled in early
TREC conferences (Chai et al2004), or by short
passages retrieved like some systems developed in
the AQUAINT program do (Small et al2003). The
situation in which QA systems are supposed to be
used is an important factor of the system design and
the evaluation must take such a factor into account.
QACIAD (Question Answering Challenge for Infor-
mation Access Dialogue) is an objective and quan-
titative evaluation framework to measure the abil-
ities of QA systems used interactively to partici-
pate in dialogues for accessing information (Kato et
al.2004a)(Kato et al2006). It assumes the situation
in which users interactively collect information us-
ing a QA system for writing a report on a given topic
and evaluates, among other things, the capabilities
needed under such circumstances, i.e. proper inter-
pretation of questions under a given dialogue con-
text; in other words, context processing capabilities
such as anaphora resolution and ellipses handling.
We are interested in examining the assumptions
made by QACIAD, and conducted an experiment,
in which the dialogues under the situation QACIAD
assumes were simulated using the WoZ (Wizard of
Oz) technique (Fraser et al1991) and analyzed. In
WoZ simulation, which is frequently used for col-
lecting dialogue data for designing speech dialogue
systems, dialogues that become possible when a sys-
tem has been developed are simulated by a human, a
WoZ, who plays the role of the system, as well as a
subject who is not informed that a human is behav-
ing as the system and plays the role of its user. An-
alyzing the characteristics of language expressions
and pragmatic devices used by users, we confirm
whether QACIAD is a proper framework for eval-
uating QA systems used in the situation it assumes.
We also examine what functions will be needed for
such QA systems by analyzing intelligent behavior
of the WoZs.
2 QACIAD and the previous study
QACIAD was proposed by Kato et al as a task of
QAC, which is a series of challenges for evaluat-
ing QA technologies in Japanese (Kato et al2004b).
QAC covers factoid questions in the form of com-
plete sentences with interrogative pronouns. Any
answers to those questions should be names. Here,
?names? means not only names of proper items
including date expressions and monetary values
(called ?named entities?), but also common names
such as those of species and body parts. Although
the syntactical range of the names approximately
corresponds to compound nouns, some of them,
such as the titles of novels and movies, deviate from
that range. The underlying document set consists
of newspaper articles. Being given various open-
domain questions, systems are requested to extract
exact answers rather than text snippets that contain
the answers, and to return the answer along with the
newspaper article from which it was extracted. The
article should guarantee the legitimacy of the answer
to a given question.
In QACIAD, which assumes interactive use of
QA systems, systems are requested to answer series
of related questions. The series of questions and the
answers to those questions comprise an information
access dialogue. All questions except the first one of
each series have some anaphoric expressions, which
may be zero pronouns, while each question is in the
range of those handled in QAC. Although the sys-
tems are supposed to participate in dialogue inter-
actively, the interaction is only simulated; systems
answer a series of questions in batch mode. Such
a simulation may neglect the inherent dynamics of
dialogue, as the dialogue evolution is fixed before-
hand and therefore not something that the systems
can control. It is, however, a practical compromise
for an objective evaluation. Since all participants
must answer the same set of questions in the same
context, the results for the same test set are compa-
rable with each other, and the test sets of the task are
reusable by pooling the correct answers.
Systems are requested to return one list consisting
of all and only correct answers. Since the number of
correct answers differs for each question and is not
given, a modified F measure is used for the evalu-
ation, which takes into account both precision and
10
recall.
Two types of series were included in the QA-
CIAD, which correspond to two extremes of infor-
mation access dialogue: a gathering type in which
the user has a concrete objective such as writing a
report and summary on a specific topic, and asks
a system a series of questions related to that topic;
and a browsing type in which the user does not
have any fixed topic of interest. Although the QA-
CIAD assumes that users are interactively collect-
ing information on a given topic and the gathering-
type dialogue mainly occurs under such circum-
stances, browsing-type series are included in the task
based on the observation that even when focusing
on information access dialogue for writing reports,
the systems must handle focus shifts appearing in
browsing-type series. The systems must identify the
type of series, as it is not given, although they need
not identify changes of series, as the boundary is
given. The systems must not look ahead to questions
following the one currently being handled. This re-
striction reflects the fact that the QACIAD is a simu-
lation of interactive use of QA systems in dialogues.
Examples of series of QACIAD are shown in Fig-
ure 1. The original questions are in Japanese and the
figure shows their direct translations.
The evaluation of QA technologies based on QA-
CIAD were conducted twice in QAC2 and QAC3,
which are a part of the NTCIR-4 and NTCIR-5
workshops1, respectively (Kato et al2004b)(Kato et
al.2005). It was one of the three tasks of QAC2 and
the only task of QAC3. On each occasion, several
novel techniques were proposed for interactive QA.
Kato et al conducted an experiment for confirm-
ing the reality and appropriateness of QACIAD, in
which subjects were presented various topics and
were requested to write down series of questions
in Japanese to elicit information for a report on
that topic (Kato et al2004a)(Kato et al2006). The
report was supposed to describe facts on a given
topic, rather than state opinions or prospects on the
topic. The questions were restricted to wh-type
questions, and a natural series of questions that may
contain anaphoric expressions and ellipses was con-
1The NTCIR Workshop is a series of evaluation workshops
designed to enhance research in information access technolo-
gies including information retrieval, QA, text summarization,
extraction, and so on (NTCIR 2006).
Series 30002
What genre does the ?Harry Potter? series belong to?
Who is the author?
Who are the main characters in the series?
When was the first book published?
What was its title?
How many books had been published by 2001?
How many languages has it been translated into?
How many copies have been sold in Japan?
Series 30004
When did Asahi breweries Ltd. start selling their low-malt
beer?
What is the brand name?
How much did it cost?
What brands of low-malt beer were already on the
market at that time?
Which company had the largest share?
How much low-malt beer was sold compared to regular
beer?
Which company made it originally?
Series 30024
Where was Universal Studio Japan constructed?
What is the nearest train station?
Which actor attended the ribbon-cutting ceremony on the
opening day?
Which movie that he featured in was released in the New
Year season of 2001?
What movie starring Kevin Costner was released in the
same season?
What was the subject matter of that movie?
What role did Costner play in that movie?
Figure 1: Examples of Series in QACIAD
structed. Analysis of the question series collected
in such a manner showed that 58% to 75% of ques-
tions for writing reports could be answered by val-
ues or names; a wide range of reference expres-
sions is observed in questions in such a situation;
and sequences of questions are sometimes very com-
plicated and include subdialogues and focus shifts.
From these observations they concluded the reality
and appropriateness of the QACIAD, and validated
the needs of browsing-type series in the task.
One of the objectives of our experiment is to con-
firm these results in a more realistic situation. The
previous experiment setting is far from the actual
situations in which QA systems are used, in which
subjects have to write down their questions without
getting the answers. Using WoZ simulation, it is
confirmed whether or not this difference affected the
result. Moreover, observing the behavior of WoZs,
the capabilities and functions needed for QA sys-
11
tems used in such a situation are investigated.
3 Setting
Referring to the headlines in Mainichi and Yomi-
uri newspapers from 2000 and 2001, we selected
101 topics, which included events, persons, and or-
ganizations. On each of those topics, a summary
of between 800 and 1600 characters long and an
abstract of around 100 characters long were con-
structed using a full text search system on the news-
paper articles.2 Four experts shared this prepara-
tion work. Twenty topics were selected from among
the original 101 on the basis that enough informa-
tion was gathered and compiled into the summary.3
The topics consisted of 5 persons, 2 organizations,
7 events, 5 artifacts, and 1 syndrome, which include
Mr. Colin Powell, Charles, Prince of Wales, An ac-
cident of a Russian nuclear-powerd submarine, Con-
corde, ?Harry Potter?, and Windows XP.
The four experts who shared the preparation work
acted as a WoZ in the simulated QA systems, and
each expert participated in dialogues on the topic for
which she/he wrote the summary and abstract, and
tried to answer questions from users using the sum-
mary, a full text search system, and her/his memory.
Six subjects participated in the experiment and 10
topics were assigned to each subject. That is, each
topic was handled by three subjects. The subjects
behaved as users of simulated QA systems. Pre-
sented with the topic and abstract, the subjects were
directed to think about questions beforehand assum-
ing that they had to write a report on that topic, and
then they participated in an information access dia-
logue with the ?QA system.? The suggested number
of questions for each topic was around 10.
The dialogues were conducted through keyboards
and character displays. The subjects were told that
the system could answer only simple factoid ques-
tions, and the WoZs were instructed not to answer
complicated questions such as asking for reasons
and opinions. No restrictions on the usage of lan-
guage expressions were imposed but were left to
the discretion of the WoZs, though canned phrases
2Those lengths of Japanese texts roughly correspond to 300
to 600 words and 40 words of English text, respectively.
3We prepared 101 topics to meet the additional needs of
other experiments. It is absolutely not the case that only 20
out of 101 were acceptable for our experiments.
USR: What is the title of the first book of the ?Harry
Potter? series?
SYS: ?Harry Potter and the Sorcerer?s Stone.?
USR: How many books were sold?
SYS: The Japanese translation sold 600 thousand copies
in the first half year after its publication.
USR: Who is the author?
SYS: J. K. Rowling.
USR: Who are the main characters (in that series)?
SYS: Harry Potter, Ron Weasley, Hagrid and so on.
USR: What kind of person is Harry Potter?
SYS: A child with a wizard and a witch for parents.
USR: What is the plot?
SYS: It is an adventure story involving Harry Potter and his
friends who enter a school for witchcraft and wizardry.
USR: How many people went to the movie?
SYS: In the US, box-office sales of the movie ?Harry Potter
and the Sorcerer?s Stone? reached 188 million
dollars in the first 10 days after release.
Figure 2: Example of dialogues collected
such as ?Please wait a moment? and ?Sorry, the an-
swer could not be found? were prepared in advance.
The WoZs were also instructed that they could clar-
ify users? questions when they were ambiguous or
vague, and that their answers should be simple but
cooperative and helpful responses were not forbid-
den.
An example of the dialogues collected is shown in
Figure 2. In the figure, SYS stands for utterances of
the QA system simulated by a WoZ and USR repre-
sents that of the user, namely a subject. In the rest of
the paper, these are referred to as system?s utterances
and user?s utterances, respectively.
4 Coding and Results
Excluding meta-utterances for dialogue control such
as ?Please wait a moment? and ?That?s all,? 620
pairs of utterances were collected, of which 22 sys-
tem utterances were for clarification. Among the re-
maining 598 cases, the system gave some answers in
502 cases, and the other 94 utterances were negative
responses: 86 utterances said that the answer could
not found; 10 utterances said that the question was
too complicated or that they could not answer such
type of question.
4.1 Characteristics of questions and answers
The syntactic classification of user utterances and its
distribution is shown in Table 1. The numbers in
12
Table 1: Syntactic classification of user utterances
Syntactic form
Wh-type Question 87.7% (544)
Yes-no Question 9.5% (59)
Imperative (Information request) 2.6% (16)
Declarative (Answer to clarification) 0.2% (1)
Table 2: Categorization of user utterances by subject
Asking about
Who, Where, What 32.5% (201)
When 16.3% (101)
How much/many 16.8% (104)(for several types of numerical values)
Why 6.5% (40)
How (for procedures or situations) 17.0% (105)
Definitions, Descriptions, Explanations 10.8% (67)
Other (Multiple Whs) 0.2% (1)
parentheses are numbers of occurrences. In spite of
the direction of using wh-type questions, more than
10% of utterances are yes-no questions and impera-
tives for requesting information. Most of the user
responses to clarification questions from the sys-
tem are rephrasing of the question concerned; only
one response has a declarative form. Examples of
rephrasing will be shown in section 4.3.
The classification of user questions and requests
according to the subject asked or requested is shown
in Table 2; the classification of system answers ac-
cording to their syntactic and semantic categoriza-
tion is shown in Table 3. In Table 2, the classification
of yes-no questions was estimated based on the in-
formation provided in the helpful responses to those.
The classification in Table 3 was conducted based on
the syntactic and semantic form of the exact part of
the answer itself rather than on whole utterances of
the system. For example, the categorization of the
system utterance ?He was born on April 5, 1935,?
which is the answer to ?When was Mr. Colin Powell
born?? is not a sentence but a date expression.
4.2 Pragmatic phenomena
Japanese has four major types of anaphoric devices:
pronouns, zero pronouns, definite noun phrases,
Table 3: Categorization of user utterances by answer
type
Answered in
Numerical values 14.3% (72)
Date expressions 16.7% (84)
Proper names 22.1% (111)
Common names 8.8% (44)
Compound nouns except names 4.2% (21)
Noun phrases 6.2% (31)
Clauses, sentences, or texts 27.7% (139)
Table 4: Pragmatic phenomena observed
Type
No reference expression 203
Pronouns 14
Zero pronouns 317
Definite noun phrases 104
Ellipses 1
and ellipses. Zero pronouns are very common in
Japanese, in which pronouns are not apparent on the
surface. As Japanese also has a completely different
determiner system from English, the difference be-
tween definite and indefinite is not apparent on the
surface, and definite noun phrases usually have the
same form as generic noun phrases. Table 4 shows
a summary of such pragmatic phenomena observed.
The total number is more than 620 as some utter-
ances contain more than one anaphoric expression.
?How many crew members were in the submarine
when the accident happened?? is an example of such
a question with multiple anaphoric expressions.
Among 203 questions with no reference expres-
sion, 60 questions (30%) are the first one of a series
of utterances on the same topic. The others are di-
vided into two cases. In the first and major case, the
current foci appear literally rather than in pronouns
or definite noun phrases. The second case, which is
relatively minor, is the beginning of and the resume
from a subdialogue. For example, when a user him-
self resumed talking about Charles, Prince of Wales
after a few questions about his children, the user
used a literal expression like ?Who is the long-term
partner of Charles, Prince of Wales??; after talking
13
about the details of the accident, a user asked ?What
is the major characteristic of Concorde??
Out of 437 reference expressions excluding el-
lipses in Table 4, 94 expressions (22%) refer to items
other than the global topic. Moreover, 76 expres-
sions of those (81%) are a sole reference expression
in the utterance. The existence of questions with-
out referring to the global topic, accompanied with
the existence of some questions with no reference
expression, means that the focus shifts in those dia-
logues, and then the local focus can differ from the
global topic.
4.3 Clarifications
As mentioned above, only 22 clarification utterances
were observed in the experiment. They are catego-
rized into those asking about an ambiguous referent
(13 utterances) and those asking about a vague ques-
tion subject (9 utterances). Examples of each case
are shown below.
Clarification asking about an ambiguous referent
USR: When was he [Mr. Colin Powell] installed as head of
security of the United States?
SYS: December 6, 2000.
USR: Who was the president at that time?
SYS: Mr. George W. Bush.
USR: Which political party did he belong to?
SYS: Whose political party are you asking about?
USR: Which political party did Mr. Colin Powell belong to?
Clarification asking about a vague question subject
USR: Who developed Windows XP?
SYS: Are you asking for a company name?
USR: What company developed Windows XP?
4.4 Cooperative and helpful responses
Rather than simple and literal answers to the users?
questions and requests, cooperative and helpful re-
sponses, in which, for example, information not re-
quested directly by a user was additionally provided
(Kaplan 1983), were observed frequently. The re-
sults of analyzing such cooperative and helpful be-
havior in the system responses are shown in Table
5. Questions asking reasons, procedures, situations,
definitions, descriptions or explanations, which are
categorized into the last three categories except oth-
ers in Table 2, were excluded from this analysis,
since for those questions it is difficult to judge objec-
Table 5: Cooperative and helpful behavior observed
Type of behavior
Providing additional information 52 (15.6%)
Providing detailed information 38 (11.4%)
Elaboration of subject 38 (11.4%)
Providing alternative information 47 (14.1%)
Exemplification 28 (8.4%)
Providing speculations 3 (0.9%)
tively whether some information is additional or not
and, in addition, most of those are out of the scope
of QACIAD, which is our main interest. Table 5
shows the number of occurrences and its percentage
among positive responses. Sometimes, more than
one behavior was observed in one utterance. 148
utterances out of 334 positive responses (44%) con-
tained some helpful behavior. The definitions and
manually constructed examples of cooperative and
helpful behavior listed in Table 5 are shown below.
Providing additional information Information
not requested directly from a user is addition-
ally provided.
USR: What year was Mr. Colin Powell born?
SYS: He was born in 1935 in New York.
Providing detailed information More detailed in-
formation than that requested directly by a user
is provided.
USR: What year was Mr. Colin Powell born?
SYS: He was born on April 5, 1935.
Elaboration of subject A relatively vague subject
of a question is elaborated and made concrete.
In the following example, a question with a
vague expression, ?how well? is answered by
a value as a concrete statistic.
USR: How well did the movie do?
SYS: In the US, box-office sales reached 188 million
dollars in the first 10 days after release.
Providing alternative information Rather than
exact information a user requested, alternative
related information is provided. An alternative
may be proposed by relaxing the condition
imposed by the user. In the following example,
attendance figures were altered to box-office
sales.
USR: How many people went to the movie?
14
SYS: In the US, box-office sales reached 188 million
dollars in the first 10 days after release.
Exemplification Instead of a comprehensive list or
abstracted general description, some major in-
stances are listed.
USR: What kinds of character goods are on sale?
SYS: Stuffed owls, uniforms for schools of witchcraft and
wizardry, and so on.
Providing speculation Rather than fact, specula-
tions and opinions are provided, which can be
considered a special case of providing alterna-
tive information. Although there are few cases
of such behavior in Table 5, there were many
such answers in response to questions asking
for reasons and situations.
USR: What exploded?
SYS: It is surmised that torpedoes in the shed at the fore
exploded.
Detecting and correcting erroneous user assump-
tions is a famous helpful behavior discussed in the
literature (Pollack 1989) (Kaplan 1983). Only two
occurrences of such behavior were observed, one of
which is shown below.
USR: How long did it take from the explosion until the
shipwreck?
SYS: The explosion happened while the submarine was going
down, and ...
5 Discussion
The analysis of dialogue collected by the WoZ sim-
ulation showed very similar results to those obtained
in a previous study, in which sequences of questions
were written down by subjects without knowing the
answers to questions. That is, as shown in Table 2,
when users asked questions to get information for a
report, the number of why-questions was relatively
small. Moreover, there were fewer questions re-
questing an explanation or definition than expected,
probably because definition questions such as ?Who
is Mr. Colin Powell?? were decomposed into rela-
tively concrete questions such as those asking for his
birthday and birthplace. The remainder (65%) could
be answered in values and names. Table 3 indicates
that 62% of the questions in our experiments were
answered by values or names. If compound nouns
describing events or situations, which are usually
distinguished from names, are considered to be in
the range of answers, the percentage of answerable
questions reaches 68%. From these results, the set-
ting of QACIAD looks realistic where users write re-
ports interacting with a QA system handling factoid
questions that have values and names as answers.
A wide range of reference expressions is observed
in information access dialogues for writing reports.
Moreover, our study confirmed that those sequences
of questions were sometimes very complicated and
included subdialogues and focus shifts. It is ex-
pected that using an interactive QA system that can
manage those pragmatic phenomena will enable flu-
ent information access dialogue for writing reports.
In this sense, the objective of QACIAD is appropri-
ate.
It could be concluded from these results that the
reality and appropriateness of QACIAD was recon-
firmed in a more realistic situation. And yet suspi-
cion remains that even in our WoZ simulation, the
subjects were not motivated appropriately, as sug-
gested by the lack of dynamic dialogue development
in the example shown in Figure 2. Especially, the
users often gave up too easily when they did not
obtain answers to prepared questions.4 The truth,
however, may be that in the environment of gath-
ering information for writing reports, dynamic dia-
logue development is limited compared to the case
when trained analysts use QA systems for problem
solving. If so, research on this type of QA systems
represents a proper milestone toward interactive QA
systems in a broad sense.
Another finding of our experiment is the impor-
tance of cooperative and helpful responses. Nearly
half of WoZ utterances were not simple literal re-
sponses but included some cooperative and helpful
behavior. This situation contrasts with a relatively
small number of clarification dialogues. The im-
portance of this behavior, which was emphasized
in research on dialogues systems in the 80s and
90s, was reconfirmed in the latest research, although
question-answering technologies were redefined in
the late 90s. Some behavior such as providing alter-
native information could be viewed as a second-best
4It is understandable, however, that there were few rephras-
ing attempts since users were informed that paraphrasing such
as ?What is the population of the US?? to ?How many people
are living in the US?? are usually in vain.
15
strategy of resource-bounded human WoZs. Even
so, it is impossible to eliminate completely the need
for such a strategy by improving core QA technolo-
gies. In addition, intrinsic cooperative and helpful
behavior such as providing additional information
was also often observed. These facts, accompanied
by the fact that such dialogues are perceived as fluent
and felicitous, suggest that the capability to behave
cooperatively and helpfully is essential for interac-
tive QA technologies.
6 Conclusion
Through WoZ simulation, the capabilities and func-
tions needed for interactive QA systems used as a
participant in information access dialogues for writ-
ing reports were examined. The results are compati-
ble with those of previous research, and reconfirmed
the reality and appropriateness of QACIAD. A new
finding of our experiment is the importance of coop-
erative and helpful behavior of QA systems, which
was frequently observed in utterances of the WoZs
who simulated interactive QA systems. Designing
such cooperative functions is indispensable. While
this fact is well known in the context of past research
on dialogue systems, it has been reconfirmed in the
context of the latest interactive QA technologies.
References
Joyce Y. Chai and Rong Jin. 2004. Discource Struc-
ture for Context Question Answering. Proceedings of
HLT-NAACL2004 Workshop on Pragmatics of Ques-
tion Answering, pp. 23-30.
John Burger, Claire Cardie, Vinay Chaudhri, et al 2001.
Issues, Tasks and Program Structures to Roadmap Re-
search in Question & Answering (Q&A)
http://www-nlpir.nist.gov/projrcts/
duc/roadmpping.html.
Norma M. Fraser and G. Nigel Gilbert. 1991. Simulating
speech systems. Computer Speech and Language, Vol
5, No.1, pp. 81-99.
Andrew Hickl, Johm Lehmann, John Williams, and
Sanda Harabagiu. 2004. Experiments with Interactive
Question Answering in Complex Scenarios. Proceed-
ings of HLT-NAACL2004 Workshop on Pragmatics of
Question Answering, pp. 60-69.
Joerrold Kaplan. 1983. Cooperative Responses from a
Portable Natural Language Database Query System.
Michael Brady and Robert C. Berwick eds. Compu-
tational Models of Discourse, pp. 167?208, The MIT
Press.
Tsuneaki Kato, Jun?ichi Fukumoto, Fumito Masui and
Noriko Kando. 2004a. Handling Information Access
Dialogue through QA Technologies ? A novel chal-
lenge for open-domain question answering ?. Pro-
ceedings of HLT-NAACL2004 Workshop on Pragmat-
ics of Question Answering, pp. 70-77.
Tsuneaki Kato, Jun?ici Fukumoto and Fumito Masui.
2004b. Question Answering Challenge for Informa-
tion Access Dialogue ? Overview of NTCIR4 QAC2
Subtask 3 ?. Proceedings of NTCIR-4 Workshop Meet-
ing.
Tsuneaki Kato, Jun?ici Fukumoto and Fumito Masui.
2005. An Overview of NTCIR-5 QAC3. Proceedings
of Fifth NTCIR Workshop Meeting, pp. 361?372.
Tsuneaki Kato, Jun?ici Fukumoto, Fumito Masui and
Noriko Kando. 2006. Are Open-domain Question
Answering Technologies Useful for Information Ac-
cess Dialogues? ? An empirical study and a proposal
of a novel challenge ? ACL Trans. of Asian Language
Information Processing, In Printing.
Elizabeth D. Liddy. 2002. Why are People Asking
these Questions? : A Call for Bringing Situation into
Question-Answering System Evaluation. LREC Work-
shop Proceedings on Question Answering ? Strategy
and Resources, pp. 5-8.
NTCIR Project Home Page. 2006.
http://research.nii.ac.jp/?ntcadm/
index-en.html
Martha E. Pollack. 1989. Plans as Complex Mental At-
titudes. Philip R. Cohen, Jerry Morgan and Martha E.
Pollack eds. Intentions in Communication, pp. 77?103,
The MIT Press.
Sharon Small, Nobuyuki Shimizu, Tomek Strzalkowski,
and Liu Ting 2003. HITIQA: A Data Driven Ap-
proach to Interactive Question Answering: A Prelim-
inary Report AAAI 2003 Spring Symposium New Di-
rections in Question Answering, pp. 94-104.
Ellen M. Voorhees and Dawn M. Tice. 2000. Building a
Question Answering Test Collection the Proceedings
of the 23rd Annual International ACM SIGIR Confer-
ence on Research and Development in Information Re-
trieval, pp. 200 - 207.
Ellen M. Voorhees. 2001. Overview of the TREC 2001
Question Answering Track. Proceedings of TREC
2001.
Ellen M. Voorhees. 2004. Overview of the TREC 2004
Question Answering Track. Proceedings of TREC
2004.
16
Proceedings of the Interactive Question Answering Workshop at HLT-NAACL 2006, pages 41?48,
New York City, NY, USA. June 2006. c?2006 Association for Computational Linguistics
Answering questions of Information Access Dialogue (IAD) task
using ellipsis handling of follow-up questions
Junichi Fukumoto
Department of Media Technology
Ritsumeikan University
1-1-1 Nojihigashi, Kusatsu, Shiga 525-8577 Japan
fukumoto@media.ritsumei.ac.jp
Abstract
In this paper, we propose ellipsis han-
dling method for follow-up questions in
Information Access Dialogue (IAD) task
of NTCIR QAC3. In this method, our sys-
tem classifies ellipsis patterns of question
sentences into three types and recognizes
elliptical elements using ellipsis handling
algorithm for each type. In the evalua-
tion using Formal Run and Reference Run
data, there were several cases which our
algorithm could not handle ellipsis cor-
rectly. According to the analysis of evalu-
ation results, the main reason of low per-
formance was lack of word information
for recognition of referential elements. If
our system can recognize word meanings
correctly, some errors will not occur and
ellipsis handling works well.
1 Introduction
In question answering task QAC of NTCIR (Kato
et al, 2005)(Kato et al, 2004), interactive use of
question answering is proposed as one of evaluation
task called Information Access Dialogue (IAD) task,
which was called subtask3 in QAC1,2. In IAD task,
a set of question consists of one first question and
several follow-up questions. These series of ques-
tions and answers comprise an information access
dialogue. In QAC1, there was only one follow-up
question in a series of questions, but in QAC2 and 3
there were several follow-up questions.
All follow-up questions have anaphoric expres-
sions including zero anaphora which is frequently
occurs in Japanese. There were several approaches
to answer follow-up questions. One approach was
to extract answers of follow-up questions from doc-
uments which were retrieved using clue words of the
first question (Sasaki et al, 2002). In the other ap-
proach, they added clue words extracted from the
previous questions to clue words of follow-up ques-
tion for document retrieval (Murata et al, 2002).
However, when topic was changed in a series of
questions, these approaches did not work well be-
cause clue words of the previous questions were
not always effective to extract answer of the current
question.
Our approach is to handle ellipses of follow-up
questions and apply the processed questions to ordi-
nary question answering system which extracts an-
swers of a question (Fukumoto et al, 2002)(Fuku-
moto et al, 2004)(Matsuda and Fukumoto, 2005).
For QAC3, we have improved our previous approach
to handle follow-up questions, that is, we have ex-
panded ellipsis handling rules more precisely. Based
on the analysis of evaluation results of QAC2, we
have classified ellipsis pattern of question sentences
into three types. The first type is ellipsis using pro-
noun. This is the case that a word used in previ-
ous questions is replaced with pronoun. The second
type is ellipsis of word in verb?s obligatory case el-
ements in the follow-up question. Some obligatory
case elements of a verb of a follow-up question will
be omitted and such elements also used in the previ-
ous question. The last type is ellipsis of a modifier
or modificand in a follow-up question. Such an ele-
41
ment appears in the previous question and has mod-
ification relationship with some word in the follow-
up question sentence. In order to handle the above
three ellipsis types, we utilized case information of
main verb of a question and co-occurrence of nouns
to recognize which case information is omitted. We
used co-occurrence dictionary which was developed
by Japan Electric Dictionary Research Inc. (EDR)
(EDR, ).
As for core QA system which is our main ques-
tion answering system, we have integrated previous
systems modules which are developed for QAC2.
One module is to handle numeric type questions. It
analyzes co-occurrence data of unit expression and
their object names and detects an appropriate nu-
meric type. Another module uses detailed classifica-
tion of Named Entity for non numerical type ques-
tions such as person name, organization name and so
on to extract an answer element of a given question.
In the following sections, we will show the de-
tails of analysis of elliptical question sentences and
our new method of ellipsis handling. We will also
discuss our system evaluation on ellipsis handling.
2 Ellipsis handling
In this section, we explain what kinds of ellipsis pat-
terns exist in the follow-up questions of a series of
questions and how to resolve each ellipsis to apply
them to core QA system.
2.1 Ellipsis in questions
We have analyzed 319 questions (46sets) which
were used in subtask3 of QAC1 and QAC2 and then,
classified ellipsis patterns into 3 types as follows:
Replacing with pronoun
In this pattern, pronoun is used in a follow-up ques-
tion and this pronoun refers an element or answer of
the previous question.
Ex1-1  
	
(Who is the president of America?)
Ex1-2 