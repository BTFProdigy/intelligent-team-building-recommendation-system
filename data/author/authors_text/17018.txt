Proceedings of the 7th Workshop on Language Technology for Cultural Heritage, Social Sciences, and Humanities, pages 74?83,
Sofia, Bulgaria, August 8 2013. c?2013 Association for Computational Linguistics
Towards Creating Precision Grammars from Interlinear Glossed Text:
Inferring Large-Scale Typological Properties
Emily M. Bender Michael Wayne Goodman Joshua Crowgey Fei Xia
Department of Linguistics
University of Washington
Seattle WA 98195-4340
{ebender,goodmami,jcrowgey,fxia}@uw.edu
Abstract
We propose to bring together two kinds of
linguistic resources?interlinear glossed
text (IGT) and a language-independent
precision grammar resource?to automat-
ically create precision grammars in the
context of language documentation. This
paper takes the first steps in that direction
by extracting major-constituent word or-
der and case system properties from IGT
for a diverse sample of languages.
1 Introduction
Hale et al (1992) predicted that more than 90%
of the world?s approximately 7,000 languages will
become extinct by the year 2100. This is a crisis
not only for the field of linguistics?on track to
lose the majority of its primary data?but also a
crisis for the social sciences more broadly as lan-
guages are a key piece of cultural heritage. The
field of linguistics has responded with increased
efforts to document endangered languages. Lan-
guage documentation not only captures key lin-
guistic data (both primary data and analytical
facts) but also supports language revitalization ef-
forts. It must include both primary data collec-
tion (as in Abney and Bird?s (2010) universal cor-
pus) and analytical work elucidating the linguistic
structures of each language. As such, the outputs
of documentary linguistics are dictionaries, de-
scriptive (prose) grammars as well as transcribed
and translated texts (Woodbury, 2003).
Traditionally, these outputs were printed ar-
tifacts, but the field of documentary linguistics
has increasingly realized the benefits of producing
digital artifacts as well (Nordhoff and Poggeman,
2012). Bender et al (2012a) argue that the docu-
mentary value of electronic descriptive grammars
can be significantly enhanced by pairing them with
implemented (machine-readable) precision gram-
mars and grammar-derived treebanks. However,
the creation of such precision grammars is time
consuming, and the cost of developing them must
be brought down if they are to be effectively inte-
grated into language documentation projects.
In this work, we are interested in leveraging
existing linguistic resources of two distinct types
in order to facilitate the development of precision
grammars for language documentation. The first
type of linguistic resource is collections of inter-
linear glossed text (IGT), a typical format for dis-
playing linguistic examples. A sample of IGT
from Shona is shown in (1).
(1) Ndakanga
ndi-aka-nga
SBJ.1SG-RP-AUX
ndakatenga
ndi-aka-teng-a
SBJ.1SG-RP-buy-FV
muchero
mu-chero
CL3-fruit
?I had bought fruit.? [sna] (Toews, 2009:34)
The annotations in IGT result from deep linguistic
analysis and represent much effort on the part of
field linguists. These rich annotations include the
segmentation of the source line into morphemes,
the glossing of those individual morphemes, and
the translation into a language of broader commu-
nication. The IGT format was developed to com-
pactly display this information to other linguists.
Here, we propose to repurpose such data in the au-
tomatic development of further resources.
The second resource we will be working with
is the LinGO Grammar Matrix (Bender et al,
2002; 2010), an open source repository of imple-
mented linguistic analyses. The Grammar Matrix
pairs a core grammar, shared across all grammars
it creates, with a series of libraries of analyses
of cross-linguistically variable phenomena. Users
access the system through a web-based question-
naire which elicits linguistic descriptions of lan-
guages and then outputs working HPSG (Pol-
lard and Sag, 1994) grammar fragments compat-
ible with DELPH-IN (www.delph-in.net) tools
based on those descriptions. For present purposes,
this system can be viewed as a function which
maps simple descriptions of languages to preci-
74
sion grammar fragments. These fragments are rel-
atively modest, yet they relate linguistic strings to
semantic representations (and vice versa) and are
ready to be built out to broad coverage.
Thus we ask whether the information encoded
by documentary linguists in IGT can be lever-
aged to answer the Grammar Matrix?s question-
naire and create a precision grammar fragment
automatically. The information required by the
Grammar Matrix questionnaire concerns five dif-
ferent aspects of linguistic systems: (i) constituent
ordering (including the presence/absence of con-
stituent types), (ii) morphosyntactic systems, (iii)
morphosyntactic features, (iv) lexical types and
their instances and (v) morphological rules. In this
initial work, we target examples of types (i) and
(ii): the major constituent word order and the gen-
eral type of case system in a language. The Gram-
mar Matrix and other related work are described
in further in ?2. In ?3 we present our test data and
experimental set-up. ??4?5 describe our method-
ology and results for the two tasks, respectively,
with further discussion and outlook in ??6?7.
2 Background and Related Work
2.1 The Grammar Matrix
The Grammar Matrix produces precision gram-
mars on the basis of description of languages
that include both high-level typological informa-
tion and more specific detail. Among the for-
mer are aspects (i)?(iii) listed in ?1. The third
of these (morphosyntactic features) concerns the
type and range of grammaticized information that
a language marks in its morphology and/or syn-
tax. This includes person/number systems (e.g.,
is there an inclusive/exclusive distinction in non-
singular first person forms?), the range of aspec-
tual distinctions a language marks, and the range
of cases (if any) in a language, inter alia. The an-
swers to these questions in turn cause the system
to provide relevant features that the user can ref-
erence in providing the more specific information
elicited by the questionnaire ((iv) and (v) above),
viz., the definition of both lexical types (e.g., first
person dual exclusive pronouns) and morphologi-
cal rules (e.g., nominative case marking on nouns).
The information input by the user to the Gram-
mar Matrix questionnaire is stored in a file called
a ?choices file?. The choices file is used both in
the dynamic definition of the html pages (so that
the features available for lexical definitions de-
pend on earlier choices) and as the input to the cus-
tomization script that actually produces the gram-
mar fragments to spec. The customization sys-
tem distinguishes between choices files which are
complete and consistent (and can be used to cre-
ate working grammar fragments) and those which
do not yet have answers to required questions or
give answers which are inconsistent according to
the underlying grammatical theory. The ultimate
goal of the present project is to be able to automat-
ically create complete and consistent choices files
on the basis of IGT, and in fact to create complete
and consistent choices files which take maximal
advantage of the analyses stored in the Grammar
Matrix customization system, answering not only
the minimal set of questions required but in fact all
which are relevant and possible to answer based on
the information in the IGT.
Creating such complete and consistent choices
files is a long-term project, with different ap-
proches required for the different types of ques-
tions outlined in ?1. Bender et al (2012b) take
some initial steps towards answering the questions
which define lexical rules. We envision answering
the questions regarding morphosyntactic features
through an analysis of the grams that appear on the
gloss line, with reference to the GOLD ontology
(Farrar and Langendoen, 2003). The implementa-
tion of such systems in such a way that they are
robust to potentially noisy data will undoubtedly
be non-trivial. The contribution of this paper is
the development of systems to handle one example
each of the questions of types (i) and (ii), namely
detecting major constituent word order and the un-
derlying case system. For the first, we build di-
rectly on the work of Lewis and Xia (2008) (see
?2.2). Our experiment can be viewed as an at-
tempt to reproduce their results in the context of
the specific view of word order possibilities devel-
oped in the Grammar Matrix. The second question
(that of case systems) is in some ways more sub-
tle, requiring not only analysis of IGT instances in
isolation and aggregation of the results, but also
identification of particular kinds of IGT instances
and comparison across them.
2.2 RiPLes
The RiPLes project has two intertwined goals.
The first goal is to create a framework that allows
the rapid development of resources for resource-
poor languages (RPLs), which is accomplished by
75
Figure 1: Welsh IGT with alignment and projected
syntactic structure
bootstrapping NLP tools with initial seeds created
by projecting syntactic information from resource-
rich languages to RPLs through IGT. Projecting
syntactic structures has two steps. First, the words
in the language line and the translation line are
aligned via the gloss line. Second, the transla-
tion line is parsed by a parser for the resource-rich
language and the parse tree is then projected to
the language line using word alignment and some
heuristics as illustrated in Figure 1 (adapted from
Xia and Lewis (2009)).1 Previous work has ap-
plied these projected trees to enhance the perfor-
mance of statistical parsers (Georgi et al, 2012).
Though the projected trees are noisy, they contain
enough information for those tasks.
The second goal of RiPLes is to use the au-
tomatically created resources to perform cross-
lingual study on a large number of languages
to discover linguistic knowledge. For instance,
Lewis and Xia (2008) showed that IGT data en-
riched with the projected syntactic structure could
be used to determine the word order property of a
language with a high accuracy (see ?4). Naseem
et al (2012) use this type of information (in their
case, drawn from the WALS database (Haspel-
math et al, 2008)) to improve multilingual depen-
dency parsing. Here, we build on this aspect of
RiPLes and begin to extend it towards the wider
range of linguistic phenomena and more detailed
classification within phenomena required by the
Grammar Matrix questionnaire.
2.3 Other Related Work
Our work is also situated with respect to attempts
to automatically characterize typological proper-
1The details of the algorithm and experimental results
were reported in (Xia and Lewis, 2007).
ties of languages, including Daume? III and Camp-
bell?s (2007) Bayesian approach to discovering ty-
pological implications and Georgi et al?s (2010)
work on predicting (unknown) typological proper-
ties by clustering languages based on known prop-
erties. Both projects use the typological database
WALS (Haspelmath et al, 2008), which has in-
formation about 192 different typological proper-
ties and about 2,678 different languages (though
the matrix is very sparse). This approach is com-
plementary to ours, and it remains an interesting
question whether our results could be improved
by bringing in information about other typological
properties of the language (either extracted from
the IGT or looked up in a typological database).
Another strand of related work concerns the col-
lection and curation of IGT, including the ODIN
project (Lewis, 2006; Xia and Lewis, 2008),
which harvests IGT from linguistics publications
available over the web and TypeCraft (Beermann
and Mihaylov, 2009), which facilitates the collab-
orative development of IGT annotations. TerraL-
ing/SSWL2 (Syntactic Structures of the World?s
Languages) has begun a database which combines
both typological properties and IGT illustrating
those properties, contributed by linguists.
Finally, Beerman and Hellan (2011) represents
another approach to inducing grammars from IGT,
by bringing the hand-built linguistic knowledge
sources closer together: On the one hand, their
cross-linguistic grammar resource (TypeGram) in-
cludes a mechanism for mapping from strings
specifying verb valence and valence-altering lex-
ical rules to sets of grammar constraints. On
the other hand, their IGT authoring environment
(TypeCraft) provides support for annotating exam-
ples with those strings. The approach advocated
here attempts to bridge the gap between IGT and
grammar specification algorithmically, instead.
3 Development and Test Data
Our long-term goal is to produce working gram-
mar fragments from IGT produced in documen-
tary linguistics projects. However, in order to
evaluate the performance of approaches to answer-
ing the high-level questions in the Grammar Ma-
trix questionnaire, we need both IGT and gold-
standard answers for a reasonably-sized sample of
languages. We have constructed development and
test data for this purpose on the basis of work done
2
http://sswl.railsplayground.net/, accessed 4/25/13
76
Sets of languages DEV1 (n=10) DEV2 (n=10) TEST (n=11)
Range of testsuite sizes 16?359 11?229 48?216
Median testsuite size 91 87 76
Language families Indo-European (4), Niger- Indo-European (3), Indo-European (2), Afro-Asiatic,
Congo (2), Afro-Asiatic, Dravidian (2), Algic, Austro-Asiatic, Austronesian,
Japanese, Nadahup, Creole, Niger-Congo, Arauan, Carib, Karvelian,
Sino-Tibetan Quechuan, Salishan N. Caucasian, Tai-Kadai, Isolate
Table 1: Language families and testsuites sizes (in number of grammatical examples)
by students in a class that uses the Grammar Ma-
trix (Bender, 2007). In this class, students work
with descriptive resources for languages they are
typically not familiar with to create testsuites (cu-
rated collections of grammatical and ungrammat-
ical examples) and Grammar Matrix choices files.
Later on in the class, the students extend the gram-
mar fragments output by the customization system
to handle a broader fragment of the language. Ac-
cordingly, the testsuites cover phenomena which
go beyond the customization system.
Testsuites for grammars, especially in their
early stages of development, require examples that
are simple (isolating the phenomena illustrated by
the examples to the extent possible), built out of
a small vocabulary, and include both grammati-
cal and ungrammatical examples (Lehmann et al,
1996). The examples included in descriptive re-
sources often don?t fit these requirements exactly.
As a result, the data we are working with include
examples invented by the students on the basis of
the descriptive statements in their resources.3
In total, we have testsuites and associated
choices files for 31 languages, spanning 17 lan-
guage families (plus one creole and one language
isolate). The most well-represented family is
Indo-European, with nine languages. We used 20
languages, in two dev sets, for algorithm develop-
ment (including manual error analysis), and saved
11 languages as a held-out test set to verify the
generalizability of our approach. Table 1 lists the
language families and the range of testsuite sizes
for each of these sets of languages.
4 Inferring Word Order
Lewis and Xia (2008) show how IGT from ODIN
(Lewis, 2006) can be used to determine, with high
accuracy, the word order properties of a language.
They identify 14 typological parameters related to
word order for which WALS (Haspelmath et al,
2008) or other typological resources provide in-
3Such examples are flagged in the testsuites? meta-data.
formation. The parameter most closely relevant to
the present work is Order of Words in a Sentence
(Dryer, 2011). For this parameter, Lewis and Xia
tested their method in 97 languages and found that
their system had 99% accuracy provided the IGT
collections had at least 40 instances per language.
The Grammar Matrix?s word order questions
differ somewhat from the typological classifi-
cation that Lewis and Xia (2008) were using.
Answering the Grammar Matrix questionnaire
amounts to more than making a descriptive state-
ment about a language. The Grammar Matrix cus-
tomization system translates collections of such
descriptive statements into working grammar frag-
ments. In the case of word order, this most di-
rectly effects the number and nature of phrase
structure rules included in the output grammar, but
can also interact with other aspects of the gram-
mar (e.g., the treatment of argument optionality).
More broadly, specifying the word order system
of a grammar determines both grammaticality (ac-
cepting some strings, ruling out others) and, for
the fixed word orders at least, aspects of the map-
ping of syntactic to semantic arguments.
Lewis and Xia (2008), like Dryer (2011), gave
the six fixed orders of S, O and V plus ?no dom-
inant order?. In contrast, the Grammar Matrix
distinguishes Free (pragmatically constrained), V-
final, V-initial, and V2 orders, in addition to the
six fixed orders. It is important to note that the
relationship between the word order type of a lan-
guage and the actual orders attested in sentences
can be somewhat indirect. For a fixed word order
language, we would expect the order declared as
its type to be the most common in running text,
but not the only type available. English, for exam-
ple, is an SVO language, but several constructions
allow for other orders, including subject-auxiliary
inversion, so-called topicalization, and others:
(2) Did Kim leave?
(3) The book, Kim forgot.
In a language with more word order flexibility in
general, there may still be a preferred word order
77
which is the most common due to pragmatic or
other constraints. Users of the Grammar Matrix
are advised to choose one of the fixed word orders
if the deviations from that order can generally be
accounted for by specific syntactic constructions,
and a freer word order otherwise.
The relationship between the correct word or-
der choice for the Grammar Matrix customization
system and the distribution of actual token word
orders in our development and test data is affected
by another factor, related to Lewis and Xia?s ?IGT
bias? which we dub ?testsuite bias?. The collec-
tions of IGT we are using were constructed as test-
suites for grammar engineering projects and thus
comprise examples selected or constructed to il-
lustrate specific grammatical properties in a test-
ing regime where one example is enough to repre-
sent each sentence type of interest. Therefore, they
do not represent a natural distribution of word or-
der types. For example, the testsuite authors may
show the full range of possible word orders in the
word order section of the testsuite and then default
to one particular choice for other portions (those
illustrating e.g., case systems or negation).
4.1 Methodology
Our first stpes mirror the RiPLes approach, pars-
ing parse the English translation of each sentence
and projecting the parsed structure onto the source
language line. Functional tags, such as SBJ and
OBJ, are added to the NP nodes on the English
side based on our knowledge of English word or-
der and then carried over to the source language
side during the projection of parse trees. The trees
are then searched for any of ten patterns: SOV,
SVO, OSV, OVS, VSO, VOS, SV, VS, OV, and
VO. The six ternary patterns match when both ver-
bal arguments are present in the same clause. The
four binary patterns are for intransitive sentences
or those with dropped arguments. These ten pat-
terns make up the observed word orders.
Given our relatively limited data set (each lan-
guage is one data point), we present an initial
approach to determining underlying word order
based on heuristics informed by general linguis-
tic knowledge. We compare the distribution of ob-
served word orders to distributions we expect to
see for canonical examples of underlying word or-
ders. We accomplish this by first deconstructing
the ternary observed-word-orders into binary pat-
terns (the four above plus SO and OS). This gives
us three axes: one for the tendency to exhibit VS
or SV order, another for VO or OV order, and an-
other for OS or SO order. By counting the ob-
served word orders in the IGT examples, we can
place the language in this three-dimensional space.
Figure 4.1 depicts this space with the positions of
canonical word orders.4 The canonical word order
positions are those found under homogeneous ob-
servations. For example, the canonical position for
SOV order is when 100% of the sentences exhibit
SO, OV, and SV orders; and the canonical position
for Free word order is when each observed order
occurs with equal frequency to its opposite order
(on the same axis; e.g. VO and OV). We select the
underlying word order by finding which canoni-
cal word order position has the shortest Euclidean
distance to the observed word order position.
When a language is selected as Free word or-
der, we employ a secondary heuristic to decide if
it is actually V2 word order. The V2 order cannot
be easily recognized only with the binary word or-
ders, so it is not given a unique point in the three-
dimensional space. Rather, we try to recognize it
by comparing the ternary orders. A Free-order lan-
guage is reclassified as V2 if SVO and OVS occur
more frequently than SOV and OSV.5
OVS
SOV
V-finalVS
OV
OSV
SVV-initial
VOS
OS
SO
SVO
VSO
VO
Free/V2
Figure 2: Three axes of basic word order and the
positions of canonical word orders.
4.2 Results
Table 2 shows the results we obtained for our dev
and test sets. For comparison, we use a most-
4Of the eight vertices of this cube, six represent canoni-
cal word orders the other two impossible combinations: The
vertex for (SV, VO, OS) (e.g.) has S both before and after O.
5The VOS and VSO patterns are excluded from this com-
parison, since they can go either way?there may be un-
aligned constituents (i.e. not a S, O, or V) before the verb
which are ignored by our system.
78
frequent-type baseline, selecting SOV for all lan-
guages, based on Dryer?s (2011) survey. We get
high accuracy for DEV1, low accuracy for DEV2,
and moderate accuracy for TEST, but all are sig-
nificantly higher than the baseline.
Dataset Inferred WO Baseline
DEV1 0.900 0.200
DEV2 0.500 0.100
TEST 0.727 0.091
Table 2: Accuracy of word-order inference
Hand analysis of the errors in the dev sets
show that some languages fall victim to the test-
suite bias, such as Russian, Quechua, and Tamil.
All of these languages have Free word order, but
our system infers SVO for Russian and SOV for
Quechua and Tamil, because the authors of the
testsuites used one order significantly more than
the others. Similarly, the Free word order lan-
guage Nishnaabemwin is inferred as V2 because
there are more SVO and OVS patterns given than
others. We also see errors due to misalignment
from RiPLes? syntactic projection. The VSO lan-
guage Welsh is inferred as SVO because the near-
ubiquitous sentence-initial auxiliary doesn?t align
to the main verb of the English translation.
5 Inferring Case Systems
Case refers to linguistic phenomena in which the
form of a noun phrase (NP) varies depending on
the function of the NP in a sentence (Blake, 2001).
The Grammar Matrix?s case library (Drellishak,
2009) focuses on case marking of core arguments
of verbs. Specifying a grammar for case involves
both choosing the high-level case system to be
modeled as well as associating verb types with
case frames and defining the lexical items or lex-
ical rules which mark the case on the NPs. Here,
we focus on the high-level case system question
as it is logically prior, and in some ways more in-
teresting than the lexical details: Answering this
question requires identifying case frames of verbs
in particular examples and then comparing across
those examples, as described below.
The high-level case system of a language con-
cerns the alignment of case marking between tran-
sitive and intransitive clauses. The three ele-
ments in question are the subjects of intransi-
tives (dubbed S), the subjects (or agent-like ar-
guments) of transitives (dubbed A) and the ob-
jects (or patient-like arguments) of intransitives
Case Case grams present
system NOM ? ACC ERG ? ABS
none
nom-acc X
erg-abs X
split-erg X X
(conditioned on V)
Table 3: GRAM case system assignment rules
(O). Among languages which make use of case,
the most common alignment type is a nominative-
accusative system (Comrie 2011a,b). In this
type, S takes the same kind of marking as A.6
The Grammar Matrix case library provides nine
options, including none, nominative-accusative,
ergative-absolutive (S marked like O), tripartite (S,
A and O all distinct) and several more intricate
types. For example, in a language with one type
of split case system the alignment is nominative-
accusative in non-past tense clauses, but ergative-
absolutive in past tense ones.
As with major constituent word order, the con-
straints implementing a case system in a grammar
serve to model both grammaticality and the map-
ping between syntactic and semantic arguments.
Here too, the distribution of tokens may be some-
thing other than a pure expression of the case
alignment type. Sources of noise in the distri-
bution include: argument optionality (e.g., tran-
sitives with one or more covert arguments), ar-
gument frames other than simple intransitives or
transitives, and quirky case (verbs that use a non-
standard case frame for their arguments, such as
the German verb helfen which selects a dative ar-
gument, though the language?s general system is
nominative-accusative (Drellishak, 2009)).
5.1 Methodology
We explore two possible methodologies for infer-
ring case systems, one relatively na??ve and one
more elaborate, and compare them to a most-
frequent-type baseline. Method 1, called GRAM,
considers only the gloss line of the IGT and as-
sumes that it complies with the Leipzig Glossing
Rules (Bickel et al, 2008). These rules not only
prescribe formatting aspects of IGT but also pro-
vide a set of licensed ?grams?, or tags for grammat-
ical properties that appear in the gloss line. GRAM
scans for the grams associated with case, and as-
signs case systems according to Table 3.
This methodology is simple to implement and
6English?s residual case system is of this type.
79
expected to work well given Leipzig-compliant
IGT. However, since it does not model the func-
tion of case, it is dependent on the IGT authors?
choice of gram symbols, and may be confused by
either alternative case names (e.g., SBJ and OBJ for
nominative and accusative or LOC for ergative in
languages where it is homophonous with the loca-
tive case) or by other grams which collide with the
case name-space (such as NOM for nominalizer).
It also only handles four of the nine case systems
(albeit the most frequent ones).
Method 2, called SAO, is more theoretically
motivated, builds on the RiPLes approach used
in inferring word order, and is designed to be
robust to idiosyncratic glossing conventions. In
this methodology, we first identify the S, A and
O arguments by projecting the information from
the parse of the English translation (including
the function tags) to the source sentence (and its
glosses). We discard all items which do not appear
to be simple transitive or intransitive clauses with
all arguments overt, and then collect all grams for
each argument type (from all words within in the
NP, including head nouns as well as determiners
and adjectives). While there are many grammati-
cal features that can be marked on NPs (such as
number, definiteness, honorifics, etc.), the only
ones that should correlate strongly with grammat-
ical function are case-marking grams. Further-
more, in any given NP, while case may be multi-
ply marked, we only expect one type of case gram
to appear. We thus assume that the most frequent
gram for each argument type is a case marker (if
there are any) and assign the case system accord-
ing to the following rules, where Sg, Og and Ag de-
note the most frequent grams associated with these
argument positions, respectively:
? Nominative-accusative: Sg=Ag, Sg 6=Og
? Ergative-absolutive: Sg=Og, Sg 6=Ag
? No case: Sg=Ag=Og, or Sg 6=Ag 6=Og and Sg,
Ag, Og also present on each of the other ar-
gument types
? Tripartite: Sg 6=Ag 6=Og, and Sg, Ag, Og (vir-
tually) absent from the other argument types
? Split-S: Sg 6=Ag 6=Og, and Ag and Og are both
present in the list for the S argument type
Here, we?re using Split-S to stand in for both
Split-S and Fluid-S. These are both systems where
some S arguments are marked like A, and some
like O. In Split-S, which is taken depends on the
verb. In Fluid-S, it depends on the interpretation of
the verb. These could be distinguished by looking
for intransitive verbs that appear more than once in
the data and checking whether their S arguments
all have consistently A or O marking.
This system is agnostic as to the spelling of the
case grams. By relying on more analysis of the
IGT than GRAM, it also introduces new kinds of
brittleness. Recognizing the difference between
grams being present and (virtually) absent makes
the system susceptible to noise.
5.2 Results
Table 4 shows the results for the inference of case-
marking systems. Currently GRAM performs best,
but both methods generally perform better than
the baseline. The better performance of GRAM
is expected, given the small size and generally
Leipzig-compliant glossing of our data sets. In
future work, we plan to incorporate data from
ODIN, which is likely less consistently annotated
but more voluminous, and we expect SAO to be
more robust than GRAM to this kind of data.
Dataset GRAM SAO Baseline
DEV1 0.900 0.700 0.400
DEV2 0.900 0.500 0.500
TEST 0.545 0.545 0.455
Table 4: Accuracy of case-marking inference
We find that GRAM is sometimes able to do well
when RiPLes gives alignment errors. For exam-
ple, Old Japanese is a NOM-ACC language, but the
case-marking grams (associated to postpositions)
are not aligned to the NP arguments, so SAO is not
able to judge their distribution. On the other hand,
SAO prevails when non-standard grams are used,
such as the NOM-ACC language Hupdeh, which is
annotated with SUBJ and OBJ grams. This comple-
mentarity suggests scope for system combination,
which we leave to future work.
6 Discussion and Future Work
Our initial results are promising, but also show
remaining room for improvement. Error analysis
suggests two main directions to pursue:
Overcoming testsuite bias In both the word or-
der and case system tasks, we see the effect of
testsuite bias on our system results. The testsuites
for freer word order languages can be artificially
dominated by a particular word order that the test-
suite author found convenient. Further, the re-
stricted vocabulary used in testsuites, combined
80
with a general preference for animates as subjects,
leads to stems and certain grams potentially being
misidentified as case markers.
We believe that these aspects of testsuite bias
are not typical of our true target input data, viz.,
the larger collections of IGT created by field
projects. On the other hand, there may be other as-
pects of testsuites which are simplifying the prob-
lem and to which our current methods are over-
fitted. To address these issues, we intend to look
to larger datasets in future work, both IGT collec-
tions from field projects and IGT from ODIN. For
the field projects, we will need to construct choices
files. For ODIN, we can search for data from the
languages we already have choices files for.
As we move from testsuites to test corpora
(e.g., narratives collected in documentary linguis-
tics projects), we expect to find different distribu-
tions of word order types. Our current methodol-
ogy for extracting word order is based on idealized
locations in our word order space for each strict
word order type. Working with naturally occurring
corpora it should be possible to gain a more em-
pirically based understanding of the relationship
between underlying word order and sentence type
distributions. It will be particularly interesting to
see how stable these relationships are across lan-
guages with the same underlying word order type
but from different language families and/or with
differences in other typological characteristics.
Better handling of unaligned words The other
main source of error is words that remain un-
aligned in the projected syntactic structure and
thus only loosely incorporated into the syntax
trees. This includes items like case marking adpo-
sitions in Japanese, which are unaligned because
there is no corresponding word in English, and
auxiliaries in Welsh, which are unaligned when
the English translation doesn?t happen to use an
auxiliary. In the former case, our SAO method
for case system extraction doesn?t include the case
grams in the set of grams for each NP. In the latter,
the word order inference system is unable to pick
up on the VSO order represented as Aux+S+[VP].
Simply fixing the attachment of the auxiliaries will
not be enough in this case, as the word order infer-
ence algorithm will need to be extended to han-
dle auxiliaries, but fixing the alignment is the first
step. Alignment problems are also the main reason
our initial attempts to extract information about
the order of determiners and nouns haven?t yet
been able to beat the most-frequent-type baseline.
Better handling of these unaligned words is
a non-trivial task, and will require bringing in
sources of knowledge other than the structure of
the English translation. The information we have
to leverage in this regard comes mainly from the
gloss line and from general linguistic/typological
knowledge which can be added to the algorithm.
That is, there are types of grams which are canon-
ically associated with verbal projections and types
of grams canonically associated with nominal pro-
jections. When these grams occur on unaligned
elements, we can hypothesize that the elements
are auxiliaries and case-marking adpositions re-
spectively. Further typological considerations will
motivate heuristics for modifying tree structures
based on these classifications.
Other directions for future work include extend-
ing this methodology to other aspects of grammat-
ical description, including additional high-level
systems (e.g., argument optionality), discovering
the range of morphosyntactic features active in a
language, and describing and populating lexical
types (e.g., common nouns with a particular gen-
der). Once we are able to answer enough of the
questionnaire that the customization system is able
to output a grammar, interesting options for de-
tailed evaluation will become available. In par-
ticular, we will be able to parse the IGT (includ-
ing held-out examples) with the resulting gram-
mar, and then compare the resulting semantic rep-
resentations to those produced by parsing the En-
glish translations with tools that produce compara-
ble semantic representations for English (using the
English Resource Grammar (Flickinger, 2000)).
7 Conclusions and Future Work
In this paper we have presented an approach to
combining two types of linguistic resources?IGT,
as produced by documentary linguists and a cross-
linguistic grammar resource supporting preci-
sion parsing and generation?to create language-
specific resources which can help enrich language
documentation and support language revitaliza-
tion efforts. In addition to presenting the broad vi-
sion of the project, we have reported initial results
in two case studies as a proof-of-concept. Though
there is still a ways to go, we find these initial re-
sults a promising indication of the approach?s abil-
ity to assist in the preservation of the key type of
cultural heritage that is linguistic systems.
81
Acknowledgments
We are grateful to the students in Ling 567 at the
University of Washington who created the test-
suites and choices files used as development and
test data in this work and to the three anonymous
reviewers for helpful comments and discussion.
This material is based upon work supported
by the National Science Foundation under Grant
No. BCS-1160274. Any opinions, findings, and
conclusions or recommendations expressed in this
material are those of the author(s) and do not nec-
essarily reflect the views of the National Science
Foundation.
References
Steven Abney and Steven Bird. 2010. The human
language project: building a universal corpus of the
world?s languages. In Proceedings of the 48th An-
nual Meeting of the Association for Computational
Linguistics, ACL ?10, pages 88?97, Stroudsburg,
PA, USA. Association for Computational Linguis-
tics.
Dorothee Beerman and Lars Hellan. 2011. Induc-
ing grammar from IGT. In Proceedings of the 5th
Language and Technology Conference: Human Lan-
guage Technologies as a Challenge for Computer
Science and Linguistics (LTC 2011).
Dorothee Beermann and Pavel Mihaylov. 2009. Type-
Craft: Linguistic data and knowledge sharing, open
access and linguistic methodology. Paper presented
at the Workshop on Small Tools in Cross-linguistic
Research, University of Utrecht. The Netherlands.
Emily M. Bender, Dan Flickinger, and Stephan Oepen.
2002. The grammar matrix: An open-source starter-
kit for the rapid development of cross-linguistically
consistent broad-coverage precision grammars. In
John Carroll, Nelleke Oostdijk, and Richard Sut-
cliffe, editors, Proceedings of the Workshop on
Grammar Engineering and Evaluation at the 19th
International Conference on Computational Lin-
guistics, pages 8?14, Taipei, Taiwan.
Emily M. Bender, Scott Drellishak, Antske Fokkens,
Laurie Poulson, and Safiyyah Saleem. 2010. Gram-
mar customization. Research on Language & Com-
putation, pages 1?50. 10.1007/s11168-010-9070-1.
Emily M. Bender, Sumukh Ghodke, Timothy Baldwin,
and Rebecca Dridan. 2012a. From database to tree-
bank: Enhancing hypertext grammars with grammar
engineering and treebank search. In Sebastian Nord-
hoff and Karl-Ludwig G. Poggeman, editors, Elec-
tronic Grammaticography, pages 179?206. Univer-
sity of Hawaii Press, Honolulu.
Emily M. Bender, David Wax, and Michael Wayne
Goodman. 2012b. From IGT to precision grammar:
French verbal morphology. In LSA Annual Meeting
Extended Abstracts 2012.
Emily M. Bender. 2007. Combining research and
pedagogy in the development of a crosslinguistic
grammar resource. In Tracy Holloway King and
Emily M. Bender, editors, Proceedings of the GEAF
2007 Workshop, Stanford, CA. CSLI Publications.
Balthasar Bickel, Bernard Comrie, and Martin Haspel-
math. 2008. The Leipzig glossing rules: Con-
ventions for interlinear morpheme-by-morpheme
glosses. Max Planck Institute for Evolutionary An-
thropology and Department of Linguistics, Univer-
sity of Leipzig.
Barry J. Blake. 2001. Case. Cambridge University
Press, Cambridge, second edition.
Bernard Comrie. 2011a. Alignment of case marking of
full noun phrases. In Matthew S. Dryer and Martin
Haspelmath, editors, The World Atlas of Language
Structures Online. Max Planck Digital Library, Mu-
nich.
Bernard Comrie. 2011b. Alignment of case marking of
pronouns. In Matthew S. Dryer and Martin Haspel-
math, editors, The World Atlas of Language Struc-
tures Online. Max Planck Digital Library, Munich.
Hal Daume? III and Lyle Campbell. 2007. A Bayesian
model for discovering typological implications. In
Proceedings of the 45th Annual Meeting of the As-
sociation of Computational Linguistics, pages 65?
72, Prague, Czech Republic, June. Association for
Computational Linguistics.
Scott Drellishak. 2009. Widespread But Not Uni-
versal: Improving the Typological Coverage of the
Grammar Matrix. Ph.D. thesis, University of Wash-
ington.
Matthew S. Dryer. 2011. Order of subject, object and
verb. In Matthew S. Dryer and Martin Haspelmath,
editors, The World Atlas of Language Structures On-
line. Max Planck Digital Library, Munich.
Scott Farrar and Terry Langendoen. 2003. A linguistic
ontology for the semantic web. Glot International,
7:97?100.
Dan Flickinger. 2000. On building a more efficient
grammar by exploiting types. Natural Language
Engineering, 6 (1) (Special Issue on Efficient Pro-
cessing with HPSG):15 ? 28.
Ryan Georgi, Fei Xia, and William Lewis. 2010.
Comparing language similarity across genetic and
typologically-based groupings. In Proceedings of
the 23rd International Conference on Computa-
tional Linguistics (Coling 2010), pages 385?393,
Beijing, China, August. Coling 2010 Organizing
Committee.
82
Ryan Georgi, Fei Xia, and William Lewis. 2012. Im-
proving dependency parsing with interlinear glossed
text and syntactic projection. In Proceedings of
COLING 2012: Posters, pages 371?380, Mumbai,
India, December.
Ken Hale, Michael Krauss, Lucille J. Wata-
homigie, Akira Y. Yamamoto, Colette Craig,
LaVerne Masayesva Jeanne, and Nora C. England.
1992. Endangered languages. Language, 68(1):pp.
1?42.
Martin Haspelmath, Matthew S. Dryer, David Gil, and
Bernard Comrie, editors. 2008. The World Atlas
of Language Structures Online. Max Planck Digital
Library, Munich. http://wals.info.
Sabine Lehmann, Stephan Oepen, Sylvie Regnier-
Prost, Klaus Netter, Veronika Lux, Judith Klein,
Kirsten Falkedal, Frederik Fouvry, Dominique Esti-
val, Eva Dauphin, Herve` Compagnion, Judith Baur,
Lorna Balkan, and Doug Arnold. 1996. TSNLP:
Test suites for natural language processing. In Pro-
ceedings of the 16th conference on Computational
linguistics - Volume 2, COLING ?96, pages 711?
716, Stroudsburg, PA, USA. Association for Com-
putational Linguistics.
William D. Lewis and Fei Xia. 2008. Automati-
cally identifying computationally relevant typolog-
ical features. In Proceedings of the Third Interna-
tional Joint Conference on Natural Language Pro-
cessing, pages 685?690, Hyderabad, India.
William D. Lewis. 2006. ODIN: A model for adapting
and enriching legacy infrastructure. In Proceedings
of the e-Humanities Workshop, Held in cooperation
with e-Science, Amsterdam.
Tahira Naseem, Regina Barzilay, and Amir Globerson.
2012. Selective sharing for multilingual dependency
parsing. In Proceedings of the 50th Annual Meet-
ing of the Association for Computational Linguis-
tics (Volume 1: Long Papers), pages 629?637, Jeju
Island, Korea, July. Association for Computational
Linguistics.
Sebastian Nordhoff and Karl-Ludwig G. Poggeman,
editors. 2012. Electronic Grammaticography. Uni-
versity of Hawaii Press, Honolulu.
Carl Pollard and Ivan A. Sag. 1994. Head-Driven
Phrase Structure Grammar. Studies in Contempo-
rary Linguistics. The University of Chicago Press
and CSLI Publications, Chicago, IL and Stanford,
CA.
Carmela Toews. 2009. The expression of tense and as-
pect in Shona. Selected Proceedings of the 39th An-
nual Converence on African Linguistics, pages 32?
41.
Tony Woodbury. 2003. Defining documentary lin-
guistics. Language documentation and description,
1(1):35.
Fei Xia and William D. Lewis. 2007. Multilin-
gual structural projection across interlinear text.
In Proc. of the Conference on Human Language
Technologies (HLT/NAACL 2007), pages 452?459,
Rochester, New York.
Fei Xia and William D. Lewis. 2008. Repurposing
theoretical linguistic data for tool development and
search. In Proceedings of the Third International
Joint Conference on Natural Language Processing,
pages 529?536, Hyderabad, India.
Fei Xia and William Lewis. 2009. Applying NLP
technologies to the collection and enrichment of lan-
guage data on the web to aid linguistic research. In
Proceedings of the EACL 2009 Workshop on Lan-
guage Technology and Resources for Cultural Her-
itage, Social Sciences, Humanities, and Education
(LaTeCH ? SHELT&R 2009), pages 51?59, Athens,
Greece, March. Association for Computational Lin-
guistics.
83
Proceedings of the 2014 Workshop on the Use of Computational Methods in the Study of Endangered Languages, pages 43?53,
Baltimore, Maryland, USA, 26 June 2014.
c
?2014 Association for Computational Linguistics
Learning Grammar Specifications from IGT: A Case Study of Chintang
Emily M. Bender Joshua Crowgey Michael Wayne Goodman Fei Xia
Department of Linguistics
University of Washington
Seattle, WA 98195-4340 USA
{ebender,jcrowgey,goodmami,fxia}@uw.edu
Abstract
We present a case study of the methodol-
ogy of using information extracted from
interlinear glossed text (IGT) to create of
actual working HPSG grammar fragments
using the Grammar Matrix focusing on
one language: Chintang. Though the re-
sults are barely measurable in terms of
coverage over running text, they nonethe-
less provide a proof of concept. Our expe-
rience report reflects on the ways in which
this task is non-trivial and on mismatches
between the assumptions of the methodol-
ogy and the realities of IGT as produced in
a large-scale field project.
1 Introduction
We explore the possibility of learning precision
grammar fragments from existing products of doc-
umentary linguistic work. A precision grammar is
a grammar which encodes a sharp notion of gram-
maticality and furthermore relates strings to elabo-
rate semantic representations. Such objects are of
interest in the context of documentary linguistics
because: (1) they are valuable tools in the explo-
ration of linguistic hypotheses (especially regard-
ing the interaction of various phenomena); (2) they
facilitate the search for examples in corpora which
are not yet understood; and (3) they can support
the development of treebanks (see Bender et al.,
2012a). However, they are expensive to build.
The present work is carried out in the context of
the AGGREGATION project,
1
which is exploring
whether such grammars can be learned on the ba-
sis of data already collected and enriched through
the work of descriptive linguists, specifically, col-
lections of IGT (interlinear glossed text).
The grammars themselves are not likely targets
for machine learning, especially in the absence of
1
http://depts.washington.edu/uwcl/aggregation/
treebanks, which are not generally available for
languages that are the focus of descriptive and
documentary linguistics. Instead, we take advan-
tage of the LinGO Grammar Matrix customiza-
tion system (Bender et al., 2002; Bender et al.,
2010) which maps from collections of statements
of linguistic properties (encoded in choices files)
to HPSG (Pollard and Sag, 1994) grammar frag-
ments which in turn can be used to parse strings
into semantic representations in the format of Min-
imal Recursion Semantics (MRS; Copestake et al.,
2005) and conversely, to generate strings from
MRS representations. The choices files are a
much simpler representation than the grammars
derived from them and therefore a more approach-
able learning target. Furthermore, using the Gram-
mar Matrix customization system to produce the
grammars results in much less noise in the auto-
matically derived grammar code than would arise
in a system learning grammars directly.
Here, we focus on a case study of Chintang, a
Kiranti language of Nepal, described by the Chin-
tang Language Research Project (CLRP) (Bickel
et al., 2009). Where Lewis and Xia (2008) and
Bender et al. (2013) apply similar methodologies
to extract large scale properties for many lan-
guages, we focus on a case study of a single lan-
guage, looking at both the large scale properties
and the lexical details. This is important for two
reasons: First, it gives us a chance to look in-
depth at the possible sources of difficulty in ex-
tracting the large scale properties. Second, while
large-scale properties are undoubtedly important,
the bulk of the information specified in a preci-
sion grammar is far more fine-grained. In this
case study we apply the methodology of Bender et
al. (2013) to extract general word order and case
properties and examine the sources of error affect-
ing those results. We also explore extensions of
those methodologies and that of Wax (2014) to ex-
tract lexical entries and specifications for morpho-
43
logical rules. Together with a few default spec-
ifications, this information is enough to allow us
to define grammars through the Grammar Matrix
customization system and thus evaluate the results
in terms of parsing coverage, accuracy and am-
biguity over running text. Chintang is particu-
larly well-suited for this case study because it is
an actual endangered language subject to active
descriptive research, making the evaluation of our
techniques realistic. Furthermore, the descriptive
research on Chintang is fairly advanced, having
produced both large corpora of high-quality IGT
and sophisticated linguistic descriptions, making
the evaluation and error analysis possible.
2 Related Work
This work can be understood as a task related to
both grammar induction and grammar extraction,
though it is distinct from both. It also connects
with and extends previous work using interlinear
glossed text to extract grammatical properties.
Grammar induction (Clark, 2001; Klein and
Manning, 2002; Klein and Manning, 2004;
Haghighi and Klein, 2006; Smith and Eisner,
2006; Snyder et al., 2009, inter alios) involves the
learning of grammars from unlabeled sentences.
Here, unlabeled means that the sentences are of-
ten POS tagged, but no syntactic structures for
the sentences are available. Most of those stud-
ies choose probabilistic context-free grammars
(PCFGs) or dependency grammars as the gram-
mar framework, and estimate the probability of
the context-free rules or dependency arcs from the
data. These studies improve parsing performance
significantly over some baselines such as the EM
algorithm, but the induced grammars are very dif-
ferent from precision grammars with respect to
content, quality, and grammar framework.
Grammar extraction, on the other hand, learns
grammars (sets of rules) from treebanks. Here the
idea is to use heuristics to convert the syntactic
structures in a treebank into derivation trees con-
forming to a particular framework, and then ex-
tract grammars from those trees. This has been
done in a wide range of grammar frameworks, in-
cluding PCFG (e.g. Krotov et al., 1998), LTAG
(e.g. Xia, 1999; Chen and Vijay-Shanker, 2000),
LFG (e.g. Cahill et al., 2004), CCG (e.g. Hock-
enmaier and Steedman, 2002, 2007), and HPSG
(e.g. Miyao et al., 2004; Cramer and Zhang, 2009).
However, this approach is not applicable to work
word-order=v-final
has-dets=yes
noun-det-order=det-noun
...
case-marking=erg-abs
erg-abs-erg-case-name=erg
erg-abs-abs-case-name=abs
...
verb4_valence=erg-abs
verb4_stem1_orth=sams-i-ne
verb4_stem1_pred=_sams-i-ne_v_re
...
verb-pc3_inputs=verb-pc2
verb-pc3_lrt1_name=2nd-person-subj
verb-pc3_lrt1_feat1_name=pernum
verb-pc3_lrt1_feat1_value=2nd
verb-pc3_lrt1_feat1_head=subj
verb-pc3_lrt1_lri1_inflecting=yes
verb-pc3_lrt1_lri1_orth=a-
Figure 1: Excerpts from a choices file
on endangered language documentation, as tree-
banks are not available for such languages.
A third line of research attempts to bootstrap
NLP tools for resource-poor languages by taking
advantage of IGT data and resources for resource-
rich languages. The canonical form of an IGT in-
stance includes a language line, a word-to-word
or morpheme-to-morpheme gloss line, and a trans-
lation line (typically in a resource-rich language).
The bootstrapping process starts with word align-
ment of the language line and translation line with
the help of the gloss line. Then the translation line
is parsed and the parse tree is projected to the lan-
guage line using the alignments (Xia and Lewis,
2007). The projected trees can be used to answer
linguistic questions such as word order (Lewis
and Xia, 2008) or bootstrap parsers (Georgi et al.,
2013). Our work extends this methodology to the
construction of precision grammars.
3 Methodology
Our goal in this work is to automatically create
choices files on the basis of IGT data. The choices
files encode both general properties about the lan-
guage we are trying to model as well as more spe-
cific information including lexical classes, lexical
items within lexical classes and definitions of lexi-
cal rules. Lexical rule definitions can include both
morphotactic information (ordering of affixes) as
well as morphosyntactic information, though here
our focus is on the former. Sample excerpts from
a choices file are given in Fig 1. These choices
files are then input into the Grammar Matrix cus-
tomization system
2
which produces HPSG gram-
2
SVN revision (for reproducibility): 27678.
44
mar fragments that meet the specifications in the
choices files. The Grammar Matrix customization
system provides analyses of a range of linguistic
phenomena. Here, we focus on a few that we con-
sider the most basic: major constituent word or-
der, the general case system, case frames for spe-
cific verbs, case marking on nouns, and morpho-
tactics for verbs. In ?3.1 we describe the dataset
we are working with. ?3.2 describes the different
approaches we take to building choices files on the
basis of this dataset. ?3.3 explains the metrics we
will use to evaluate the resulting grammars in ?4.
3.1 The Chintang Dataset
Chintang (ISO639-3: ctn) is a language spoken by
about 5000 people in Nepal and believed to be-
long to the Eastern subgroup of the Kiranti lan-
guages, which in turn are argued to belong to the
larger Tibeto-Burman family (Bickel et al., 2007;
Schikowski et al., in press). Here we briefly sum-
marize properties of the language that relate to
the information we are attempting to automatically
detect in the IGT, and in many cases make the
problem interestingly difficult.
Schikowski et al. (in press) describe Chintang
as exhibiting information-structurally constrained
word order: All permutations of the major senten-
tial constituents are expected to be valid, with the
different orders subject to different felicity condi-
tions. They state, however, that no detailed analy-
sis of word order has yet been carried out, and so
this description should be taken as preliminary.
In contrast, much detailed work has been done
on the marking of arguments, both via agree-
ment on the verb and via case marking of depen-
dents (Bickel et al., 2010; Stoll and Bickel, 2012;
Schikowski et al., in press). The case marking sys-
tem can be understood as following an ergative-
absolutive pattern, but with several variations from
that theme. In an ergative-absolutive pattern, the
sole argument of an intransitive verb (here called
S) is marked the same as the most patient-like ar-
gument of a transitive verb (here called O) and
differentiated from the most agent-like argument
of a transitive verb (here called A). Most A ar-
guments are marked with an overt case marker
called ergative, while S and O arguments appear
without a case marker. In most writing about the
language, this unmarked case is called nomina-
tive; here we will use the term absolutive. Simi-
larly, verbs agree with up to two arguments, and
the agreement markers for S and O are generally
shared and distinguished from those for A.
Divergences from the ergative-absolutive pat-
tern include variable marking of ergative case on
first and second person pronouns as well as va-
lence alternations such as one that licenses oc-
currences of transitive verbs with two absolutive
arguments (and S-style agreement with the A ar-
gument) when the O argument is of an indefinite
quantity (Schikowski et al., in press). Further-
more, the language allows dropping of arguments
(A, S, and O). Finally, there are of course valences
beyond simple intransitive and transitive, as well
as case frames even for two-argument verbs other
than { ERG, ABS }. As a result of the combination of
these facts, the actual occurrence of ergative-case-
marked arguments in speech is relatively low: Ex-
amining a corpus of speech spoken to and around
children, Stoll and Bickel (2012) find that only
11% of (semantically) transitive verb tokens have
an overt, ergative-marked NP A argument. As dis-
cussed below, these properties make it difficult for
automated methods to detect both the overall case
system of the language and accurate information
regarding the case frames of individual verbs.
The dataset we are using contains 9793 (8863
train, 930 test) IGT instances which come from
the corpus of narratives and other speech col-
lected, transcribed, translated and glossed by the
CLRP.
3
An example is shown in Fig. 2. As can
be seen in Fig. 2, the glossing in this dataset is ex-
tremely thorough. It is also supported by a detailed
Toolbox lexicon that encodes not only alternative
forms for each lemma as well as glosses in English
and Nepali, but also valence frames for most verb
entries which list the expected case marking on
the arguments. Finally, note that morphosyntactic
properties without a morphological reflex are sys-
tematically unglossed in the data, so that ABS never
appears (nor does SG for singular nouns, etc.).
In our experiments, we abstract away from the
problem of morphophonological analysis in order
to focus on morphosyntax and lexical acquisition.
Accordingly, our grammars target the second line
of the IGT, which represents each form as a se-
quence of phonologically regularized morphemes.
3.2 Grammars
In this section, we describe the different means we
use for extracting the different kinds of informa-
3
http://www.spw.uzh.ch/clrp
45
unisaNa
u-nisa-Na
3sPOSS-younger.brother-ERG.A
khatte
khatt-e
take-IND.PST
mo
mo
DEM.DOWN
kosi
kosi-i
river-LOC
moba
mo-pe
DEM.DOWN-LOC
?The younger brother took it to the river.? [ctn] (Bickel et al., 2013c)
Figure 2: Sample IGT
tion required to build the choices files (see Fig 1
above). We first describe our points of comparison
(oracle, ?3.2.1 and baseline, ?3.2.2), and then con-
sider different ways of detecting the large-scale
properties (word order, ?3.2.3; overall case sys-
tem, ?3.2.4). Next we turn to different ways of ex-
tracting two kinds of lexical information: the con-
straints on case (i.e. case frames of verbs and the
case marking on nouns, ?3.2.5) and verbal mor-
photactics (?3.2.6). Finally, we describe a small
set of hand-coded ?choices? which are added to all
choices files (except the oracle one) in order to cre-
ate working grammars (?3.2.7).
The alternative approaches to extracting the var-
ious kinds of information can be cross-classified
with each other, giving the set of choices files de-
scribed in Table 1. The first column gives iden-
tifiers for the choices files. The second specifies
how the lexicon was created, the third how the
value for major constituent word order was deter-
mined, and the fourth how the values for case were
determined, including the overall case system, the
case frames, and the case values for nouns. These
options are all described in more detail below.
3.2.1 Oracle choices file
As an upper-bound, we use the choices file de-
veloped in Bender et al., 2012b. This file in-
cludes hand-specified definitions of lexical rules
for nouns and verbs as well as lexical entries cre-
ated by importing lexical entries from the Tool-
box lexicon developed by the CLRP. This lex-
icon, as noted above, lists valence frames for
most verbal entries. As the Grammar Matrix
customization system currently only provides for
simple transitive and intransitive verbs, only two
verb classes were defined: intransitives with the
case frame { ABS } and transitives with the case
frame { ERG, ABS }. In addition, there is one class
of nouns. Finally, the choices file includes hand-
coded lexical entries for pronouns. As an upper-
bound, this choices file can be expected to repre-
sent high precision and moderate recall: verbs that
don?t fit the two classes defined aren?t imported.
Note that the Grammar Matrix customization
system does not currently support the definition of
adjectives, adverbs, or other parts of speech out-
side of verb, noun, determiner, (certain) adposi-
tions, conjunctions and auxiliaries. Thus while we
expect each grammar to be able to parse at least
some sentences in the corpus, to the extent that
sentences tend to include words outside the classes
noun, verb and determiner, we expect relatively
low coverage, even from our upper-bound.
3.2.2 Baseline choices file
Our baseline choices file is designed to create a
working grammar, without particular high-level
information about Chintang, that focuses on cov-
erage at the expense of precision. We hand-
specified the (counter-factual) assertion that there
is no case marking in Chintang, and in addi-
tion that Chintang allows free word order (on the
grounds that this is the least constrained word or-
der possibility). It also defines bare-bones classes
of nouns, determiners and transitive verbs, and
then populates the lexicon by using a variant of the
methodology in Xia and Lewis 2007. In particu-
lar, we parse the translation line using the Char-
niak parser (Charniak, 1997) and then use the cor-
respondences inherent in IGT to create a projected
tree structure for the language line, following Xia
and Lewis. An example of the result for Chintang
is shown in Fig 3. The projected trees include part
of speech tags for each word that can be aligned.
For each such word tagged as noun, verb, or deter-
miner, we create an instance in the corresponding
lexical type. In this baseline grammar, all verbs
are assumed to be transitive, but since all argu-
ments can (optionally) be dropped, the grammar is
expected to be able to cover intransitive sentences,
even if the semantic representation is wrong.
Since this baseline choices file models Chintang
as if it had no case marking, we expect it the re-
sulting grammar to have relatively high recall in
terms of the combination of nominal and verbal
constituents. On the other hand, since it is build-
ing a full-form lexicon and Chintang is a morpho-
logically complex language, we expect it to have
relatively low lexical coverage on held-out data.
46
Choices file Lexicon Word order Case
ORACLE Manual Manual Manual
BASELINE Fullform Default None
FF-AUTO-NONE Fullform Auto None
FF-DEFAULT-GRAM Fullform Default Auto (GRAM)
FF-AUTO-GRAM Fullform Auto Auto (GRAM)
FF-DEFAULT-SAO* Fullform Default Auto (SAO)
FF-AUTO-SAO* Fullform Auto Auto (SAO)
MOM-DEFAULT-NONE MOM Default None
MOM-AUTO-NONE MOM Auto None
MOM-DEFAULT-GRAM* MOM Default Auto (GRAM)
MOM-AUTO-GRAM* MOM Auto Auto (GRAM)
MOM-DEFAULT-SAO* MOM Default Auto (SAO)
MOM-AUTO-SAO* MOM Auto Auto (SAO)
Table 1: Choices files generated
s
vp
np-obj
pp
np
nn vbd
jutta khet-a-N-e
shoe buy-PST-1sS/P-IND.PST
I bought a pair of shoe .
prp vbd dt nn in nn
np-subj-prp np np
pp .
np-obj
vp
s
Figure 3: Projected tree structure (ex. from (Bickel
et al., 2013d))
3.2.3 Word order
We applied the methodology of Bender et al.
(2013) for determining major constituent order.
For our dataset, the algorithm chose ?v-final?,
which matches what is in the ORACLE choices file,
but is not necessarily correct. We created two ver-
sions of each of the other choices files, one with
the default (baseline) answer of ?free word order?
and one with this automatically supplied answer.
3.2.4 Case system
Similarly, we applied extended versions of the two
methods for automatically discovering case sys-
tems from Bender et al. 2013: GRAM which looks
for known case grams in glosses (not using pro-
jected trees) and SAO which extends the structure-
projection methodology of Xia and Lewis (2007)
to detect S, A and O arguments and then looks
for the most frequent gram associated with each
of these.
4
The GRAM method determines the
case system of Chintang to be ergative-absolutive,
while the SAO method indicates ?none? (no case).
Specifying a case system in a choices file has no
effect on the coverage or precision of the resulting
grammar if the lexical items don?t constrain case.
Thus the case system choices only make sense in
combination with the case frames choices (?3.2.5).
3.2.5 Case frames and case values
The HPSG analysis of case involves a feature CASE
which is constrained by both verbs and nouns:
Nouns constrain their own CASE value, while verbs
constrain the CASE value of the arguments they se-
lect for.
5
In order to constrain verbs and nouns
appropriately, we first need a range of possible
case values. For choices files built based on the
GRAM system, we consider case markers to be any
of those included in the set of grams defined by
the Leipzig Glossing Rules (Bickel et al., 2008):
ABL, ABS, ACC, ALL, COM, DAT, ERG, GEN, INS, LOC,
and OBL. For choices files built based on the SAO
system, we consider as case markers only those
grams (automatically) identified as marking S, A,
or O. In the present study, that should only be erga-
tive; as there is no marked case for absolutive, all
other nouns were treated as absolutive (regardless
of their actual case marking, since the SAO system
has no way to detect other case grams).
4
Our extensions involved making the system able to han-
dle the situation where one or more of S, A and O are morpho-
logically unmarked and therefore unreflected in the glosses.
5
For the details of the analyses of case systems provided
by the Grammar Matrix, see Drellishak 2009.
47
In choices files which specify case systems, we
constrain the case value for nouns by creating one
noun class for every case value, and then assigning
the lexical entries for nouns to those lexical classes
based on the grams in the gloss of the noun.
6
Similarly, we create lexical classes for each
case frame identified for transitive and intransitive
verbs: We look for case grams on each argument
of the verb, as determined by the function tags in
the projected tree (e.g. NP-SUBJ-PRP in Fig 3).
7
For
each case frame we identify, we create a lexical
class, and we create lexical entries for verbs based
on the case frames we extract for them. When
the system identifies both an overt subject and an
overt object, it considers the verb to be transitive
and constrains the case of its two arguments based
on the observed case values. If either argument
is overt but not marked for case, the verb is con-
strained to select for the default case on that argu-
ment, according to the detected case marking sys-
tem (i.e. ergative for transitive subjects and absolu-
tive for transitive objects, in this instance). When
there is an overt subject but no overt object, the
verb is treated as intransitive and is constrained to
select for a subject of the observed case (or the
default case, here absolutive, if the overt subject
bears no case marker). When there is an overt ob-
ject but no subject, the verb is assumed to be tran-
sitive and the object?s case assigned as with other
transitives but the subject?s case is constrained to
the default (i.e. ergative, in this instance). Verbs
with no overt arguments are not matched.
3.2.6 MOM choices file: Automatically
extracted lemmas and lexical rules
The final refinement we try on our baseline is
to apply the ?Matrix-ODIN Morphology? (MOM)
methodology of Wax 2014. This methodology at-
tempts to automatically identify affixes and cre-
ate appropriate descriptions of lexical rules in a
choices file to model those affixes. As a result,
it also identifies stems. Thus we use the same ba-
sic choices as in the baseline choices file, but now
populate the lexicon with stems rather than full-
forms. Compared to BASELINE, this one should re-
sult in a grammar with better lexical coverage on
held-out data, to the extent that the MOM system
6
In future work, we plan to extend the MOM approach
(?3.2.6) from verbs to nouns, but for now, the nouns are
treated as full-form lexical entries across all choices files.
7
While the GRAM method doesn?t require the projected
trees to determine the overall case system, we do need them
here to find case frames for particular verbs.
is able to correctly extract both stems and inflec-
tional rules. We note that while the MOM system
uses the same conceptual approach to alignment as
that in the BASELINE, GRAM and SAO approaches, the
implementation is separate, and so does not find
exactly the same set of verbs.
3.2.7 Shared choices
The ORACLE choices file ran as-is. For the re-
maining choices files, we also needed to answer
the questions about determiners (whether there are
any, position with respect to the noun). Based on
initial experiments, we chose ?yes? for the pres-
ence of determiners and ?det-noun? order. In an
attempt to boost coverage generally, we also coded
the choices that allow any argument to be dropped.
While the determiner-related choices are specific
to Chintang, the latter set of choices could be ex-
pected to boost coverage (at the cost of some pre-
cision) for any language.
3.2.8 Summary
Table 1 shows the 10 logical possibilities that arise
from combining the methods discussed in this sec-
tion, in addition to the ORACLE grammar and the
BASELINE grammar. However, we test only a subset
of these possibilities for the following reasons:
8
The SAO system chose no case as the case system
for Chintang. As a result, this makes FF-DEFAULT-
SAO and FF-AUTO-SAO the same as BASELINE and FF-
AUTO-NONE, respectively. In future work, we aim
to improve the SAO system but until it is effec-
tive enough to pick some case system for Chin-
tang, these options do not require further testing.
Secondly, while it is possible in principle to com-
bine the output of the MOM system (which classi-
fies verbs based on their morphological combina-
toric potential) with the output of the system be-
hind the GRAM choices files (which classifies verbs
based on their case frames), doing so is non-trivial
because these classifications are orthogonal, yet
each verb must inherit from each dimension. We
thus leave the exploration of MOM-DEFAULT-GRAM
and MOM-AUTO-GRAM (and likewise MOM-DEFAULT-
SAO and MOM-AUTO-SAO) for future work.
3.3 Evaluation
We evaluate the grammars generated by the
choices files over both the data used to develop
them (?training?; 8863 items) as well as data not
included in the development process (held-out
8
Untested choices files are marked with an * in the table.
48
?test? data; 930 items). We run both of these eval-
uations because we are actually testing two sepa-
rate questions. The first is whether the grammars
generated in this way can provide useful analyti-
cal tools to linguists. In this primary use-case, we
expect a linguist to provide the system with all of
their IGT and then use the generated grammars in
order to gain insights into that same data. This
does not amount to a case of testing on the train-
ing data because the annotations provided to the
system (IGT) are not the same as those produced
by the system (full parses, including semantic rep-
resentations). However, we are still interested in
also testing on held-out data in order to answer the
second question: whether grammars generated in
this way can also generalize to further texts.
We evaluate the grammars generated by the
choices files we create in terms of lexical cov-
erage, parse coverage, parse accuracy and am-
biguity. Lexical coverage measures how many
items consist only of word forms recognized by
the grammar. Any item with unknown lexical
items won?t parse.
9
Parse coverage is the num-
ber of items that receive any analysis at all, where
ambiguity is the number of different analyses each
item receives. To measure parse accuracy, we
examined the items that parse and determined
which parses had semantic representations whose
predicate-argument structures plausibly matched
what was indicated in the gloss.
4 Results
Table 2 compares the lexical information encoded
in each of the choices files in a quantitative fash-
ion. The first thing to note is that the grammars
vary widely in the size of their lexicons. The BASE-
LINE/FF lexicons are expected to be larger than the
others because they take each fully inflected form
encountered as a separate lexical entry. On the
other hand, the ORACLE choices file was built on the
basis of the Toolbox lexicon (dictionary) from the
CLRP and thus is effectively created on the basis
of a much larger dataset. The GRAM choices files
only contain verbs for which a case frame could
be identified. If the projected tree was not inter-
pretable by our extraction heuristics or if the ex-
ample had no overt arguments, then the verb will
not be extracted. The MOM choices files, on the
9
There are methods for handling unknown lexical items
(e.g. Adolphs et al., 2008) in more mature grammars of this
type, but these are not applicable at this stage.
other hand, only need to identify verbs in the string
to be able to extract them, and should be able to
generalize across different inflected forms of the
same verb. This gives a number of verb entries
intermediate between that for BASELINE/FF and the
GRAM files. For nouns, there is less variation: the
MOM files use the same data as the BASELINE, while
the GRAM method faces as simpler problem than
for verbs: it only needs to identify the case gram
(if any) in a noun?s gloss. The slightly larger num-
bers of nouns in the GRAM files v. the others can be
explained by the same form being glossed in two
different ways in the training data.
The remaining differences can be briefly ex-
plained as follows: The ORACLE choices file does
not contain any entries for determiners. The oth-
ers all contain the same 240 entries; one for any
word aligned by the algorithm to a determiner in
the English translation. Only the ORACLE and MOM
choices files attempt to handle morphology, and so
far MOM only does verbal morphology.
Table 3 presents the results of parsing training
and test data with the various grammars, in abso-
lute numbers and in percentages of the entire data
set. The ?lexical coverage? columns indicate for
how many items the grammars were able to rec-
ognize each constituent word form. The ?items
parsed? columns show the number of items that
received any analysis at all, while ?items correct?
show the number of items that were judged (by
one of the authors) to have a predicate-argument
structure that plausibly reflects the gloss given in
the IGT. The final column shows the average num-
ber of distinct analyses the grammars find for the
items they parse at all.
The results are in fact barely measurable with
these metrics (especially on the test data), but
nonetheless speak to the differences between the
grammars. Regarding lexical coverage, the ORA-
CLE grammar does best on the test data set. This
is because it is the only choices file not derived
from the training data. Not surprisingly, the BASE-
LINE grammar has the highest number of readings
per item parsed, followed closely by FF-AUTO-NONE
which adds only a minor constraint on word or-
der.
10
On the other hand, comparing the number
of items parsed to the number judged correct, ex-
cept for the MOM choices files, the ?survival rate?
was over 50% for all other tests.
11
This suggests
10
It is in this relative lack of constraint that BASELINE
mostly clearly forms a baseline to improve upon.
11
The vast majority of the incorrect parses for the MOM
49
Choices file # verb entries # noun entries # det entries # verb affixes # noun affixes
ORACLE 900 4751 0 160 24
BASELINE 3005 1719 240 0 0
FF-AUTO-NONE 3005 1719 240 0 0
FF-DEFAULT-GRAM 739 1724 240 0 0
FF-AUTO-GRAM 739 1724 240 0 0
MOM-DEFAULT-NONE 1177 1719 240 262 0
MOM-AUTO-NONE 1177 1719 240 262 0
Table 2: Amount of lexical information in each choices file
Training Data (N = 8863) Test Data (N = 930)
lexical items items average lexical items items average
choices file coverage (%) parsed (%) correct (%) readings coverage (%) parsed (%) correct (%) readings
ORACLE 1165 (13) 174 (3.5) 132 (1.5) 2.17 116 (12.5) 20 (2.2) 10 (1.1) 1.35
BASELINE 1276 (14) 398 (7.9) 216 (2.4) 8.30 41 (4.4) 15 (1.6) 8 (0.9) 28.87
FF-AUTO-NONE 1276 (14) 354 (4.0) 196 (2.2) 7.12 41 (4.4) 13 (1.4) 7 (0.8) 13.92
FF-DEFAULT-GRAM 911 (10) 126 (1.4) 84 (0.9) 4.08 18 (1.9) 4 (0.4) 2 (0.2) 5.00
FF-AUTO-GRAM 911 (10) 120 (1.4) 82 (0.9) 3.84 18 (1.9) 4 (0.4) 2 (0.2) 5.00
MOM-DEFAULT-NONE 1102 (12) 814 (9.2) 52 (0.6) 6.04 39 (4.2) 16 (1.7) 3 (0.3) 10.81
MOM-AUTO-NONE 1102 (12) 753 (8.5) 49 (0.6) 4.20 39 (4.2) 10 (1.1) 3 (0.3) 9.20
Table 3: Results
that, despite the noise introduced by the automatic
methods of lexical extraction, the precision gram-
mar backbone provided by the Grammar Matrix
can still provide high-quality parses.
For example, the BASELINE grammar produces
six parses of the string in (1):
(1) din
din
day
khiptukum
khipt-u-kV-m
count-3P-IND.NPST-1/2nsA
?(We) count days.? [ctn] (Bickel et al., 2013b)
Among these six is one which produces the se-
mantic representation in (2). While this grammar
does not yet capture any of the agreement mor-
phology that indicates that the subject is first per-
son plural, it does correctly link the ?day? to the
semantic ARG2 of ?count?.
(2)
? h
1
,
h
3
: din n day(x
4
),
h
5
: exist q rel(x
4
, h
6
, h
7
),
h
6
: khipt-u-kv-m v count(e
2
, x
9
, x
4
)
{ h
6
=
q
h
3
} ?
Finally, we note that the longest items we are
able to parse consist of one verb and two NPs, each
of which can have only up to two words (a deter-
miner and a noun). Most of the examples that do
parse consist of only one or two words, while the
full data set ranges from items of length 1 to items
of length 25 (average 4.5 words/item in training,
choices files involved analyses of words for ?yes?, ?well?,
?what? and the like as verbs. Note that one form of ?yes? is the
copula, and such examples were accepted. Another source of
incorrect parses for many grammars involves homophony be-
tween the focus particle and a verb meaning ?come?.
5 words/item in test). The Grammar Matrix al-
ready supports some longer sentences in the form
of coordination, so one avenue for future work is
to explore the automatic detection of coordination
strategies. Otherwise, branching out to longer sen-
tences will require additions to the Grammar Ma-
trix allowing the specification of modifiers and a
wider range of valence types for verbs.
5 Error Analysis
The opportunity to work closely with one lan-
guage has allowed us to observe several ways
in which the assumptions of the systems we are
building on do not match what we find in the data.
Here we briefly review some of those mismatches
and reflects on what could be done to handle them.
The first observation concerns the non-glossing
of zero-marked morphosyntactic features, such as
absolutive case in Chintang. From the point of
view of a consumer of IGT it is certainly desirable
to have as much information as possible made ex-
plicit in the glossing. From the point of view of
a project creating IGT in the context of on-going
fieldwork, however, it is likely often difficult to
reliably gloss zero morphemes and thus the de-
cision to leave them systematically unglossed is
quite sensible. Both the GRAM method and espe-
cially the SAO method for detecting case systems,
which we extended to extracting case frames for
particular verbs, are not yet fully robust to the
possibility that certain case values are unmarked
morphologically and thus not glossed in the data.
50
While we extended them to a certain extent in this
work, there is still more to be done on this front.
A second observation concerns the glossing of
proper names, as in (3):
(3) pailego
paile-ko
first-GEN
ubhiyauti
u-bhiya
3A-marriage
paphuma
paphu-ma
a.clan.of.Rai.people-F
?His first marriage was with a Phuphu woman.?
[ctn] (Bickel et al., 2013a)
We use statistical alignment between the trans-
lation line and the gloss line and between the
gloss line and the language line in order to project
information from the analysis of the translation
line onto the language line. Glosses such as
?a.clan.of.Rai.people? tend to confuse this align-
ment process, though they are very informative to
a human reader of the IGT. Error analysis of sen-
tences for which we were unable to extract subject
and object arguments at all suggested that many
of the errors were caused by misalignments likely
due to the aligner not being able to cope with this
kind of glossing. Future work will explore how to
train the aligner to function better in such cases.
In addition to properties of the glossing conven-
tions, there are also properties of the language that
proved challenging for our system. The first is the
intricate nature of the case-marking system as dis-
cussed in ?3.1. In particular, our system does not
model any distinction between 1st and 2nd per-
son pronouns and other nouns, such that when the
pronouns appear without a case marker, they are
taken to be in the unmarked case (i.e. absolutive),
though this is not necessarily so. The second prop-
erty of the language that our system found diffi-
cult is the optionality of arguments. We were able
to adapt our case frame extraction strategy to han-
dle dropped subjects, but dropped objects are more
confounding: our system is unable so far to distin-
guish such verbs from intransitives. One possible
way forward in this case is to draw more informa-
tion from the English translation in the IGT: En-
glish tends not to drop arguments, and so when we
find an object (especially a pronominal object) in
the English translation that is not aligned to any-
thing in the language line, we would have evidence
that the verb in question may be transitive.
Finally, we looked closely at the items in the test
data for which we had complete lexical analysis,
but which still failed to parse. We did this both for
the fullform and MOM-based lexicons. The goal
here was to evaluate whether (a) our assignment of
items to lexical categories was correct (and there
was some other issue standing in the way of an-
alyzing the item) or (b) we should have parsed a
given item, but our system had misidentified the
words in question in such a way that no syntactic
analysis could be found. For the baseline system,
we found that although some items had misidenti-
fied categories (specifically, pronouns and adverbs
were sometimes misidentified as determiners), the
two major obstacles to parsing came from multi-
verb constructions or sentential fragments. Of the
26 unparsed items with lexical coverage, 10 con-
tained multiple verbs and 12 were NP or interjec-
tory fragments (eg: ?Yes, yes, yes.?). We observed
a similar pattern among 23 unparsed items from
the MOM-based lexicon. We can take two lessons
from this assessment: (1) since much of our data
comes from naturally occurring speech, it may be
useful to rerun our tests with an NP fragment as
a valid root symbol in our grammars; (2) proper
identification of auxiliary verbs is an important
next step for improving our system.
6 Conclusion
In this paper we have taken the first steps towards
creating actual precision grammars by creating
Grammar Matrix customization system choices
files on the basis of automated analysis of IGT.
Measured in terms of coverage over held-out data,
the results are hardly impressive and might seem
discouraging. However, we see in these initial for-
ays rather a proof-of-concept. Moreover, the pro-
cess of digging into the details of getting an IGT-
to-grammar system working for one particular lan-
guage has been a very rich source of information
on the mismatches between the assumptions of
systems built to handle high-level properties and
the linguistic facts and glossing conventions of the
kind of data they are meant to handle.
7 Acknowledgments
This material is based upon work supported by
the National Science Foundation under Grant No.
BCS-1160274. Any opinions, findings, and con-
clusions or recommendations expressed in this
material are those of the author(s) and do not nec-
essarily reflect the views of the NSF.
We would like to thank David Wax for his as-
sistance in setting up the MOM system, Olga Za-
maraeva for general discussion, and especially the
CRLP for providing access to the Chintang data.
51
References
Peter Adolphs, Stephan Oepen, Ulrich Callmeier,
Berthold Crysmann, Dan Flickinger, and Bernd
Kiefer. 2008. Some fine points of hybrid natural
language parsing. Marrakech, Morocco, May.
Emily M. Bender, Dan Flickinger, and Stephan Oepen.
2002. The grammar matrix: An open-source starter-
kit for the rapid development of cross-linguistically
consistent broad-coverage precision grammars. In
John Carroll, Nelleke Oostdijk, and Richard Sut-
cliffe, editors, Proceedings of the Workshop on
Grammar Engineering and Evaluation at the 19th
International Conference on Computational Lin-
guistics, pages 8?14, Taipei, Taiwan.
Emily M. Bender, Scott Drellishak, Antske Fokkens,
Laurie Poulson, and Safiyyah Saleem. 2010. Gram-
mar customization. Research on Language & Com-
putation, pages 1?50. 10.1007/s11168-010-9070-1.
Emily M. Bender, Sumukh Ghodke, Timothy Baldwin,
and Rebecca Dridan. 2012a. From database to tree-
bank: Enhancing hypertext grammars with grammar
engineering and treebank search. In Sebastian Nord-
hoff and Karl-Ludwig G. Poggeman, editors, Elec-
tronic Grammaticography, pages 179?206. Univer-
sity of Hawaii Press, Honolulu.
Emily M. Bender, Robert Schikowski, and Balthasar
Bickel. 2012b. Deriving a lexicon for a precision
grammar from language documentation resources:
A case study of Chintang. In Proceedings of COL-
ING 2012, pages 247?262, Mumbai, India, Decem-
ber. The COLING 2012 Organizing Committee.
Emily M. Bender, Michael Wayne Goodman, Joshua
Crowgey, and Fei Xia. 2013. Towards creating pre-
cision grammars from interlinear glossed text: Infer-
ring large-scale typological properties. In Proceed-
ings of the 7th Workshop on Language Technology
for Cultural Heritage, Social Sciences, and Human-
ities, pages 74?83, Sofia, Bulgaria, August. Associ-
ation for Computational Linguistics.
Balthasar Bickel, Goma Banjade, Martin Gaenszle,
Elena Lieven, Netra Paudyal, Ichchha Rai, Manoj
Rai, Novel Kishor Rai, and Sabine Stoll. 2007. Free
prefix ordering in Chintang. Language, 83(1):43?
73.
Balthasar Bickel, Bernard Comrie, and Martin Haspel-
math. 2008. The Leipzig glossing rules: Con-
ventions for interlinear morpheme-by-morpheme
glosses. Max Planck Institute for Evolutionary An-
thropology and Department of Linguistics, Univer-
sity of Leipzig.
Balthasar Bickel, Martin Gaenszle, Novel Kishore Rai,
Elena Lieven, Goma Banjade, Toya Nath Bhatta,
Netra Paudyal, Judith Pettigrew, Ichchha P. Rai,
Manoj Rai, Robert Schikowski, and Sabine Stoll.
2009. Audiovisual corpus of the chintang lan-
guage, including a longitudinal corpus of language
acquisition by six children, plus a trilingual dic-
tionary, paradigm sets, grammar sketches, ethno-
graphic descriptions, and photographs. DOBES
Archive, http://www.mpi.nl/DOBES.
Balthasar Bickel, Manoj Rai, Netra P. Paudyal, Goma
Banjade, Toya N. Bhatta, Martin Gaenszle, Elena
Lieven, Ichchha Purna Rai, Novel Kishore Rai, and
Sabine Stoll. 2010. The syntax of three-argument
verbs in Chintang and Belhare (Southeastern Ki-
ranti). In Studies in Ditransitive Constructions: A
Comparative Handbook, pages 382?408. Mouton de
Gruyter, Berlin.
Balthasar Bickel, Martin Gaenszle, Novel Kishore
Rai, Vishnu Singh Rai, Elena Lieven, Sabine Stoll,
G. Banjade, T. N. Bhatta, N Paudyal, J Pettigrew,
and M Rai, I. P.and Rai. 2013a. Hatuwa. Accessed:
15 January 2013.
Balthasar Bickel, Martin Gaenszle, Novel Kishore
Rai, Vishnu Singh Rai, Elena Lieven, Sabine Stoll,
G. Banjade, T. N. Bhatta, N Paudyal, J Pettigrew,
and M Rai, I. P.and Rai. 2013b. Khadak?s daily life.
Accessed: 15 January 2013.
Balthasar Bickel, Martin Gaenszle, Novel Kishore
Rai, Vishnu Singh Rai, Elena Lieven, Sabine Stoll,
G. Banjade, T. N. Bhatta, N Paudyal, J Pettigrew,
and M Rai, I. P.and Rai. 2013c. Tale of a poor guy.
Accessed: 15 January 2013.
Balthasar Bickel, Martin Gaenszle, Novel Kishore
Rai, Vishnu Singh Rai, Elena Lieven, Sabine Stoll,
G. Banjade, T. N. Bhatta, N Paudyal, J Pettigrew,
and M Rai, I. P.and Rai. 2013d. Talk of kazi?s trip.
Accessed: 15 January 2013.
Aoife Cahill, Michael Burke, Ruth O?Donovan, Josef
Van Genabith, and Andy Way. 2004. Long-distance
dependency resolution in automatically acquired
wide-coverage pcfg-based lfg approximations. In
Proceedings of the 42nd Meeting of the Association
for Computational Linguistics (ACL?04), Main Vol-
ume, pages 319?326, Barcelona, Spain, July.
Eugene Charniak. 1997. Statistical parsing with a
context-free grammar and word statistics. In Pro-
ceedings of AAAI-1997.
John Chen and K. Vijay-Shanker. 2000. Automated
Extraction of TAGs from the Penn Treebank. In
Proc. of the 6th International Workshop on Parsing
Technologies (IWPT-2000), Italy.
Alexander Clark. 2001. Unsupervised induction
of stochastic context-free grammars using distri-
butional clustering. In Proc. of the 5th Confer-
ence on Computational Natural Language Learning
(CoNLL-2001).
Ann Copestake, Dan Flickinger, Carl Pollard, and
Ivan A. Sag. 2005. Minimal recursion semantics:
An introduction. Research on Language & Compu-
tation, 3(4):281?332.
52
Bart Cramer and Yi Zhang. 2009. Construction of a
german hpsg grammar from a detailed treebank. In
Proceedings of the 2009 Workshop on Grammar En-
gineering Across Frameworks (GEAF 2009), pages
37?45, Suntec, Singapore.
Scott Drellishak. 2009. Widespread But Not Uni-
versal: Improving the Typological Coverage of the
Grammar Matrix. Ph.D. thesis, University of Wash-
ington.
Ryan Georgi, Fei Xia, and William D. Lewis. 2013.
Enhanced and portable dependency projection algo-
rithms using interlinear glossed text. In Proceedings
of ACL 2013 (Volume 2: Short Papers), pages 306?
311, Sofia, Bulgaria, August.
Aria Haghighi and Dan Klein. 2006. Prototype-
driven grammar induction. In Proceedings of the
21st International Conference on Computational
Linguistics and 44th Annual Meeting of the Associ-
ation for Computational Linguistics (COLING/ACL
2006), pages 881?888, Sydney, Australia, July. As-
sociation for Computational Linguistics.
Julia Hockenmaier and Mark Steedman. 2002.
Acquiring compact lexicalized grammars from a
cleaner treebank. In Proc. of LREC-2002, pages
1974?1981.
Julia Hockenmaier and Mark Steedman. 2007. Ccg-
bank: A corpus of ccg derivations and dependency
structures extracted from the penn treebank. Com-
putational Linguistics, 33(3):355?396.
Dan Klein and Christopher Manning. 2002. A gen-
eral constituent context model for improved gram-
mar induction. In Proceedings of the 40th Annual
Meeting of the Association for Computational Lin-
guistics (ACL-2002), Philadelphia, PA.
Dan Klein and Christopher Manning. 2004. Corpus-
based induction of syntactic structure: models of de-
pendency and constituency. In Proceedings of the
42nd Annual Meeting of the Association for Compu-
tational Linguistics (ACL-2004), Barcelona, Spain.
Alexander Krotov, Mark Hepple, Robert Gaizauskas,
and Yorick Wilks. 1998. Compacting the Penn
Treebank Grammar. In Proc. of the 36th Annual
Meeting of the Association for Computational Lin-
guistics (ACL-1998), Montreal, Quebec, Canada.
William D. Lewis and Fei Xia. 2008. Automati-
cally identifying computationally relevant typolog-
ical features. In Proceedings of the Third Interna-
tional Joint Conference on Natural Language Pro-
cessing, pages 685?690, Hyderabad, India.
Yusuke Miyao, Takashi Ninomiya, and Junichi Tsu-
jii. 2004. Corpus-oriented grammar development
for acquiring a head-driven phrase structure gram-
mar from the penn treebank. In Proc. of the First In-
ternational Joint Conference on Natural Language
Processing (IJCNLP-2004), Hainan, China.
Carl Pollard and Ivan A. Sag. 1994. Head-Driven
Phrase Structure Grammar. Studies in Contempo-
rary Linguistics. The University of Chicago Press
and CSLI Publications, Chicago, IL and Stanford,
CA.
Robert Schikowski, Balthasar Bickel, and Netra
Paudyal. in press. Flexible valency in Chintang.
In B. Comrie and A. Malchukov, editors, Valency
Classes: A Comparative Handbook. Mouton de
Gruyter, Berlin.
Noah A. Smith and Jason Eisner. 2006. Annealing
structural bias in multilingual weighted grammar in-
duction. In Proceedings of the 21st International
Conference on Computational Linguistics and 44th
Annual Meeting of the Association for Computa-
tional Linguistics (ACL/COLING 2006), pages 569?
576, Sydney, Australia, July. Association for Com-
putational Linguistics.
Benjamin Snyder, Tahira Naseem, and Regina Barzi-
lay. 2009. Unsupervised multilingual grammar in-
duction. In Proceedings of the Joint Conference of
the 47th Annual Meeting of the ACL and the 4th In-
ternational Joint Conference on Natural Language
Processing of the AFNLP, pages 73?81, August.
Sabine Stoll and Balthasar Bickel. 2012. How to
measure frequency? Different ways of counting
ergatives in Chintang (Tibeto-Burman, Nepal) and
their implications. In Frank Seifart, Geoffrey Haig,
Nikolaus P. Himmelmann, Dagmar Jung, Anna Mar-
getts, and Paul Trilsbeek, editors, Potentials of Lan-
guage Documentation: Methods, Analyses, and Uti-
lization, pages 83?89. University of Hawai?i Press,
Manoa.
David Wax. 2014. Automated grammar engineering
for verbal morphology. Master?s thesis, University
of Washington.
Fei Xia and William D. Lewis. 2007. Multilin-
gual structural projection across interlinear text.
In Proc. of the Conference on Human Language
Technologies (HLT/NAACL 2007), pages 452?459,
Rochester, New York.
Fei Xia. 1999. Extracting Tree Adjoining Gram-
mars from Bracketed Corpora. In Proc. of 5th Nat-
ural Language Processing Pacific Rim Symposium
(NLPRS-1999), Beijing, China.
53
