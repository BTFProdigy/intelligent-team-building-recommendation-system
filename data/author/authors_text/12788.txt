The Importance of Narrative and Other Lessons from an Evaluation of an
NLG System that Summarises Clinical Data
Ehud Reiter, Albert Gatt, Franc?ois Portet
Dept of Computing Science
University of Aberdeen, UK
{e.reiter,a.gatt,fportet}@
abdn.ac.uk
Marian van der Meulen?
Dept of Psychology
University of Edinburgh, UK
m.a.van-der-meulen@
sms.ed.ac.uk
Abstract
The BABYTALK BT-45 system generates tex-
tual summaries of clinical data about babies in
a neonatal intensive care unit. A recent task-
based evaluation of the system suggested that
these summaries are useful, but not as effec-
tive as they could be. In this paper we present
a qualitative analysis of problems that the
evaluation highlighted in BT-45 texts. Many
of these problems are due to the fact that BT-
45 does not generate good narrative texts; this
is a topic which has not previously received
much attention from the NLG research com-
munity, but seems to be quite important for
creating good data-to-text systems.
1 Introduction
Data-to-text NLG systems produce textual output
based on the analysis and interpretation of non-
linguistic data (Reiter, 2007). Systems which pro-
duce short summaries of small amounts of data, such
as weather-forecast generators (Reiter et al, 2005),
have been one of the most successful applications of
NLG, and there is growing interest in creating sys-
tems which produce longer summaries of larger data
sets.
We have recently carried out an evaluation of one
such system, BT-45 (Portet et al, 2007), which gen-
erates multi-paragraph summaries of clinical data
from a Neonatal Intensive Care Unit (NICU). The
summaries cover a period of roughly 45 minutes,
and describe both sensor data (heart rate, blood oxy-
gen saturation, etc, sampled at 1 sec intervals) as
well as discrete events such as drug administration;
?Now at the Department of Clinical Neurosciences, Univer-
sity Hospital, Geneva, Switzerland
they are intended to help medical staff make treat-
ment decisions. This evaluation showed that from a
decision-support perspective, the BT-45 texts were
as effective as visualisations of the data, but less ef-
fective than human-written textual summaries.
In addition to quantitative performance data,
which is presented elsewhere (van der Meulen et
al., submitted), the evaluation also gave us valuable
clues about what aspects of data-to-text technology
need to be improved in order to make texts gener-
ated by such systems more effective as decision sup-
port aids; this is the subject of this paper. Somewhat
to our surprise, many of the problems identified in
the evaluation relate to the fact that BT-45 could not
produce a good narrative describing the data. Gen-
eration of non-fictional narratives is not something
which has been the focus of much NLG research in
the past, but our results suggest it is important, at
least in the context of producing texts which are ef-
fective decision-support aids.
1.1 Background: Data-to-Text
Data-to-text systems are motivated by the belief that
(brief) linguistic summaries of datasets may in some
cases be more effective than more traditional pre-
sentations of numeric data, such as tables, statistical
analyses, and graphical visualisations (even simple
visual/graphical displays require relatively complex
cognitive processing (Carpenter and Shah, 1998)).
Also linguistic summaries can be delivered in some
contexts where visualisations are not possible, such
as text messages on a mobile phone, or when the
user is visually impaired (Ferres et al, 2006). In the
NICU domain, Law et al (2005) conducted an ex-
periment which showed that medical professionals
were more likely to make the correct treatment deci-
147
sion when shown a human-written textual summary
of the data than when they were shown a graphical
visualisation of the data.
A number of data-to-text systems have been de-
veloped and indeed fielded, especially in the domain
of weather forecasts (Goldberg et al, 1994; Reiter
et al, 2005). Most of these systems have gener-
ated short (paragraph-length or smaller) summaries
of relatively small data sets (less than 1KB). Some
research has been on systems that summarise larger
data sets (Yu et al, 2007; Turner et al, 2008), but
these systems have also generated paragraph-length
summaries; we are not aware of any previous re-
search on generating multi-paragraph summaries in
a data-to-text system.
Data-to-texts systems have been evaluated in a
number of ways, including human ratings (the most
common technique) (Reiter et al, 2005), BLEU-like
scores against human texts (Belz and Reiter, 2006),
post-edit analyses (Sripada et al, 2005), and per-
suasive effectiveness (Carenini and Moore, 2006).
However, again to the best of our knowledge no pre-
vious data-to-text system has been evaluated by ask-
ing users to make decisions based on the generated
texts, and measuring the quality of these decisions.
2 BabyTalk and BT-45
Law et al (2005) showed that human-written textual
summaries were effective decision-support aids in
NICU, but of course it is not practical to expect medi-
cal professionals to routinely write such summaries,
especially considering that the summaries used by
Law et al in some cases took several hours to write.
The goal of the BABYTALK research project is to
use NLG and data-to-text technology to automati-
cally generate textual summaries of NICU data, for a
variety of audiences and purposes. The first system
developed in BABYTALK, and the subject of this pa-
per, is BT-45 (Portet et al, 2007), which generates
summaries of 30-60 minute chunks of clinical data,
for the purpose of helping nurses and doctors make
appropriate treatment decisions.
An example of BABYTALK input data is shown in
Figures 1 (sensor data) and 2 (selected event data).
Figure 3 shows the human-written corpus text for
this scenario, and Figure 4 shows the BT-45 text gen-
erated for this scenario. Note that for the purposes of
Figure 1: Example Babytalk Input Data: Sensors
HR = Heart Rate; TcPO2 = blood O2 level; TCPCO2 =
blood CO2 level; SaO2 = oxygen saturation; T1 = chest
temperature; T2 = toe temperature; mean BP = blood
pressure. The bars and triangles at the bottom show the
time of discrete events (Figure 2).
event time
Blood transfusion 13.35
Intermittent crying 13.38
FiO2 (oxygen level) changed to 50% 13.51
Incubator temperature changed to 36.4 13.52
Attempt to insert line 13.53
Line removed 13.57
Attempt to insert line 13.58
Line removed 14.00
FiO2 (oxygen level) changed to 45% 14.03
Attempt to insert line 14.08
Line removed 14.09
Figure 2: Example Babytalk Input Data: Selected Dis-
crete Events
148
You saw the baby between 13:30 and 14:10.
Over the course of the monitoring period the HR in-
creases steadily from 140 to 165; the BP varies between
41 and 49.
At the start of the period T1 is 37 and T2 is 35.8C.
During the first 15 minutes the pO2 is 7.8-9.2 and the
pCO2 is 5.9-7.3.
At 13:35 a blood transfusion is commenced.
At 13:38 the baby is crying and there are a few up-
ward spikes in the pO2 trace corresponding to downward
spikes in the pCO2 trace. At 13.45 the humidity on the
incubator walls is wiped away and T1 and T2 fall to 36.3
and 35.4 respectively
At 13:50 the baby is examined. There is a desaturation to
72% and the FiO2 is changed to 50%. Between now and
14.10 there are several attempts to site a peripheral can-
nula. The pO2 and pCO2 both have spikes in the traces,
pO2 reaching 19.2 and pCO2 reaching 0.4. There are sev-
eral episodes of artefact in the oxygen saturation trace.
T1 and T2 fall to 36.2 and 35.7 and the oxygen saturation
falls to 65%. The FiO2 is adjusted to 50%. Also at this
time the incubator temperature is adjusted to 36.4C.
At 14:03 with the pO2 at 13.9 and oxygen saturation at
100%, the FiO2 is reduced to 45
At 14:04 T1 rises sharply to 40, then drops fairly steeply
to 28.5C. Between 14.06 and 14.10 there are several
spikes in the pO2 and pCO2 traces but by 14.10 the pO2
is 8, pCO2 is 3.7, the oxygen saturation is 71%, the HR
is 163, the BP 39, T1 29C and T2 35.4C.
Figure 3: Corpus Text for Fig 1, 2 data
this paper, we have deliberately selected a relatively
poor quality BT-45 output text.
The processing performed by BT-45 is described
by Portet et al (2007). Very briefly, BT-45 generates
texts in four stages:
? signal analysis: for example, detecting spikes
in the sensor channels; this is done using adap-
tations of standard pattern detection and recog-
nition algorithms.
? data abstraction: for example, identifying
three line-insertion-attempt and line-removal
events within a short span of time and
grouping these into the higher-level con-
cept LINE-INSERTION-PROCESS-FAILED (BT-
You saw the baby between 13:30 and 14:10. Heart Rate
(HR) = 149. Core Temperature (T1) = 37.0. Periph-
eral Temperature (T2) = 35.8. Transcutaneous Oxygen
(TcPO2 ) = 9.5. Transcutaneous CO2 (TcPCO2) = 6.7.
Mean Blood Pressure (mean BP) = 41. Oxygen Satura-
tion (SaO2) = 94.
Over the next 39 minutes SaO2 decreased to 81, T2 de-
creased to 34.5, HR stayed at around 151, T1 decreased
to 36.2 and mean BP stayed at around 40.
A blood transfusion was given to the baby at 13:35.
At 13:50 there was a desaturation down to 65. As a result,
Fraction of Inspired Oxygen (FIO2) was set to 50
There were 3 failed attempts to insert a peripheral venous
line at 13:53. TcPO2 suddenly decreased to 8.1. SaO2
suddenly increased to 92. TcPO2 suddenly decreased to
9.3. There was a spike in TcPO2 up to 14.8. There had
been another spike in T1 up to 40.5. FIO2 had been low-
ered to 45%. Previously the baby had been examined.
Figure 4: BT-45 Text for Fig 1, 2 data
45 includes a domain ontology of such con-
cepts); this is done using knowledge-based
techniques.
? document planning: for example, deciding not
to mention most of the spikes in O2 and CO2;
this is primarily done in a bottom-up fashion,
using information (computed by the data ab-
straction module) on the medical importance of
events, and also on causal and other relation-
ships between events.
? microplanning and realisation: producing the
actual text shown in Figure 4; this is mostly
done using relatively standard NLG techniques,
although we have developed new techniques
for communicating temporal information and
relationships.
3 Evaluation of BT-45
BT-45 was evaluated by asking medical staff to
make decisions about what actions they should take
with regard to a baby, after viewing either a BT-45
text, a human-written textual summary, or a visual-
isation of the baby?s data; this was similar in gen-
eral terms to the experiment described by Law et al
(2005). van der Meulen et al (submitted) gives de-
149
tails about the evaluation design and quantitative re-
sults of the evaluation; in this paper we just briefly
summarise these aspects of the evaluation
Material: Our medical collaborators selected 24
scenarios (data sets), and defined 18 types of ac-
tions. For each of the data sets, they specified which
of the 18 actions were appropriate, inappropriate, or
neutral (neither appropriate nor inappropriate); one
appropriate action was identified as the main target
action. For the data set shown in Figures 1 and 2, for
example:
? Main target action: Adjust monitoring equip-
ment
? Other appropriate actions: calm/comfort baby,
manage temperature, analyse blood sample
? Neutral actions: adjust CPAP (ventilation) set-
tings, baby care (e.g., nappy change)
? Inappropriate actions: all other actions (e.g.
blood transfusion, order X-Ray) (12 in all)
For each scenario, we created three presentations: a
visualisation (similar to Figure 1), a human-written
text summary written by our collaborators, and the
summary produced by BT-45. Our collaborators
were asked not to include any explicit medical in-
terpretation of the data in their human-written sum-
maries. For each scenario, our collaborators also
prepared a text which gave background information
about the baby.
When developing BT-45, we had access to the
data collection that the scenario data sets were taken
from (which includes several months of data), but
did not know ahead of time which specific scenarios
would be used in the experiment.
Subjects: 35 medical professionals, including ju-
nior nurses, senior nurses, junior doctors, and senior
doctors.
Procedure: Each subject was shown 8 scenarios
in each condition (visualisation, human text, BT-45
text) in a Latin Square design; all subjects were also
shown the background texts. Subjects were asked to
specify which actions should be taken for this baby,
selecting actions from a fixed set of check-boxes;
they were given three minutes to make this deci-
sion. Subjects were not explicitly asked for free-text
comments, but any comments spontaneously made
by subjects were recorded. Subject responses were
scored by computing the percentage of appropriate
actions they selected, and subtracting from this the
percentage of inappropriate actions.
Results: The highest score was achieved by the
human texts (mean score of 0.39); there was no sig-
nificant difference between the BT-45 texts (mean
score of 0.34) and the visualisations (mean score of
0.33). The largest differences occurred in the ju-
nior nurses group. van der Meulen et al (submitted)
present a detailed statistical analysis of the results.
Discussion: This shows that BT-45 texts were as
effective as visualisation, but less effective than the
human texts. This suggests that data-to-text technol-
ogy as it stands could be useful as a supplement to
visualisations (since some individuals do better with
texts and some with graphics; also some data sets are
visualised effectively and some are not), and in con-
texts where visualisation is not possible. But it also
suggests that if we can improve the technology so
that computer-generated texts are as effective as hu-
man texts, we should have a very effective decision-
support technology.
4 Quantitative Comparison of BT-45 and
Corpus Texts
In addition to the task-based evaluation described
above, we also quantitatively compared the BT-45
and human texts, and qualitatively analysed prob-
lems in the BT-45 texts. Quantitative comparison
was done by annotating the BT-45 and human texts
to identify which events they mentioned. For each
scenario, we computed the MASI coefficient (Pas-
sonneau, 2006) between the set of events mentioned
in the BT-45 and human texts. The average MASI
score was 0.21 (SD = 0.13), which is low; this
suggests that BT-45 and the human writers choose
different content. We also checked whether similar
human and BT-45 texts (as judged by MASI score)
obtained similar evaluation scores; in fact there was
no significant correlation between MASI similarity
of human and BT-45 texts and the difference be-
tween their evaluation scores.
We performed a second analysis based on com-
paring the structure (e.g., number and size of para-
graphs) of the BT-45 and human texts, using a
tree-edit-distance metric to compare text structures.
150
Again this showed that there were large differences
between the structure of the BT-45 and human texts,
and that these differences did correlate with differ-
ences in evaluation scores.
In other words, simple metrics of content and
structural differences do not seem to be good pre-
dictors of text effectiveness; this is perhaps not sur-
prising given the complexity of the texts and the in-
formation they are communicating.
5 Qualitative Analysis of Problems in
BT-45 texts
The final step in our evaluation was to qualitatively
analyse the BT-45 texts and the results of the task-
based evaluation, in order to highlight problems in
the BT-45 texts. Of course we were aware of numer-
ous ways in which the software could be improved,
but the evaluation gave us information about which
of these mattered most in terms of overall effective-
ness. We report this analysis below, including issues
identified from subjects? comments, issues identi-
fied from scenarios where BT-45 texts did poorly,
and problems identified via manual inspection of the
texts. We do not distinguish between ?linguistic? and
?reasoning? problems, in part because it is usually
difficult (and indeed somewhat artificial) to separate
these aspects of BT-45.
5.1 Problems Identified by Subjects
Subjects made a number of comments during the ex-
periment. Two aspects of BT-45 were repeatedly
criticised in these comments.
5.1.1 Layout and bullet lists
Subjects wanted better layout and formatting, in
the human texts as well as the BT-45 texts (BT-45
texts do not currently include any visual formatting).
In particular, they wanted bullet lists to be used, es-
pecially for lab results. Such issues have been exten-
sively discussed by other researchers (e.g., (Power et
al., 2003)), we will not further discuss them here.
5.1.2 Continuity
BT-45 sometimes described changes in signals (or
other events) which didn?t make sense because they
omitted intermediate events. For example, consider
the last paragraph in the BT-45 text shown in Fig-
ure 4 (with italics added):
There were 3 failed attempts to insert a pe-
ripheral venous line at 13:53. TcPO2 sud-
denly decreased to 8.1. SaO2 suddenly in-
creased to 92. TcPO2 suddenly decreased
to 9.3. There was a spike in TcPO2 up
to 14.8. There had been another spike in
T1 up to 40.5. FIO2 had been lowered to
45%. Previously the baby had been exam-
ined.
Subjects complained that it made no sense for
TcPO2 to decrease to 9.3 when the last value men-
tioned for this parameter was 8.1
In this case (and in many others like it), BT-45
had identified the decrease events as being medically
important, but had not assigned as much importance,
and hence not mentioned, the increase event (TcPO2
went up to 19) between these decrease events. This
is partially because BT-45 believed that a TcPO2 of
19 is a sensor artefact (not a real reading of blood
oxygen), since 19kPa is a very high value for this
channel. In fact this is a correct inference on BT-
45?s part, but the text is still confusing for readers.
We call this problem continuity, making an anal-
ogy to the problems that film-makers have in ensur-
ing that scenes in a film (which maybe shot in very
different times and locations) fit together in the eyes
of the viewer. It is interesting to note that some of
the human texts also seem to have continuity prob-
lems (for example, the text in Figure 3 says T2 falls
to 35.4, and then says T2 falls to 35.7), but none of
the subjects complained about continuity problems
in the human texts. So some kinds of continuity vio-
lations seem more problematical to readers than oth-
ers. Perhaps this depends on the proximity of the
events both in the document structure and in time;
we hope to empirically explore this hypothesis.
Continuity is just one aspect of the broader prob-
lem of deciding which events need to be explicitly
mentioned in the text, and which can be omitted.
Making such decisions is perhaps one of the hard-
est aspects of data-to-text.
5.2 Scenarios Where BT-45 did Badly
When analysing the results of the experiment, we
noticed that BT-45 texts did as well as the human
texts for scenarios based on five of the eight target
actions; however they did significantly worse than
151
main target action human BT-45 diff
Adjust CPAP 0.37 0.37 0
Adjust monitoring equip 0.59 0.22 0.37
Adjust ventilation 0.22 0.23 -0.01
Extubate 0.14 0.12 0.02
Manage temperature 0.55 0.33 0.22
No action 0.61 0.43 0.18
Suction 0.34 0.42 -0.08
Support blood pressure 0.45 0.55 -0.10
Table 1: Average evaluation score by main target action
the human texts on the scenarios based on the other
three actions (Adjust Monitoring Equipment, Man-
age Temperature, and No Action). Details are shown
in Table 1; an ANOVA confirms that there is a sig-
nificant effect of main target action on scores (p <
.001). We have identified a number of reasons why
we believe this is the case, which we discuss below.
5.2.1 Too much focus on medically important
events
Content-selection in BT-45 is largely driven by
rules that assess the medical importance of events
and patterns. In particular, BT-45 tends to give low
importance to events which it believes are due to
sensor artefacts. While this strategy makes sense
in many cases, it leads to poor performance in sce-
narios where the target action is Adjust Monitor-
ing Equipment, when sensor problems need to be
pointed out to the reader.
This can be seen in the example scenario used in
this paper. The TcPO2 and TcPCO2 traces shown in
Figure 1 are full of sensor artefacts (such as the im-
plausibly high values of TcPO2 mentioned above).
The human text shown in Figure 3 explicitly men-
tions these, for example (italics added)
At 13:50 the baby is examined. There
is a desaturation to 72% and the FiO2 is
changed to 50%. Between now and 14.10
there are several attempts to site a periph-
eral cannula. The pO2 and pCO2 both
have spikes in the traces, pO2 reaching
19.2 and pCO2 reaching 0.4. There are
several episodes of artefact in the oxygen
saturation trace.
The BT-45 text shown in Figure 4, in contrast, only
mentions one spike in TcPO2, and does not mention
any artefacts.
This is a difficult problem to solve, because in
a context where medical intervention was needed,
BT-45 would be correct to ignore the sensor prob-
lems. One solution would be for BT-45 to perform
a top-level diagnosis itself, and adjust its texts based
on whether it believed staff should focus on medi-
cal intervention or adjusting sensors. Whether this
is desirable or even feasible is unclear; it relates to
the more general issue of how a data-summarisation
system such as BT-45 should be integrated with
the kind of diagnosis systems developed by the
AI/Medicine community.
5.2.2 Poor description of related channels
BT-45 essentially describes each channel inde-
pendently. For temperature, however, it is often bet-
ter to describe the two temperature channels together
and even contrast them, which is what the human
texts do; this contributes to BT-45?s poor perfor-
mance in Manage Temperature scenarios.
For example, in one of the Manage Temperature
scenarios, the BT-45 text says
Core Temperature (T1) = 36.4. Peripheral
Temperature (T2) = 34.0. . . .
(new paragraph)Over the next 44 minutes
T2 decreased to 33.4.
The human text says
He is warm centrally but T2 drifts down
over the 45 minutes from 34 to 33.3C.
The information content of the two texts is quite
similar, but the human text describes temperature
in an integrated fashion. Similar problems occur in
other scenarios. In fact, over the 24 scenarios as
a whole, the human texts include only three para-
graphs which mention just one of the temperatures
(T1 or T2, but not both), while the BT-45 texts in-
clude 18 such paragraphs.
BT-45?s document planner is mostly driven by
medical importance and causal relationships; al-
though it does try to group together information
about related channels, this is done as a secondary
optimisation, not as a primary organising principle.
The human texts place a much higher priority on
152
grouping ?physiological systems? (to use NICU ter-
minology) of related channels and events together,
including the respiratory and cardiac systems as well
as the temperature system. We suspect that BT-45
should place more emphasis on systems in its docu-
ment planning.
5.2.3 Poor long-term overview
BT-45 does not do a good job of summarising a
channel?s behaviour over the entire scenario. This
isn?t a problem in eventful scenarios, where the key
is to describe the events; but it does reduce the effec-
tiveness of texts in uneventful scenarios where the
main target action is No Action (i.e., do nothing).
This problem can be seen in the text extracts
shown in the previous section. Even at the level of
individual channels, He is warm centrally is a better
overview than Core Temperature (T1) = 36.4; and
T2 drifts down over the 45 minutes from 34 to 33.3C
is better than Peripheral Temperature (T2) = 34.0.
. . . Over the next 44 minutes T2 decreased to 33.4.
At a signal analysis level, BT-45 also does not
do a good job of detecting patterns (such as spikes)
with a duration of minutes instead of seconds. This
contributes to the system?s poor performance in
Manage Temperature scenarios, because tempera-
ture changes relatively slowly.
We believe these problems can be solved, by
putting more emphasis on analysis and reporting of
long time-scale events in the BT-45 modules.
5.3 Other Problems
We manually examined the texts, looking for cases
where the BT-45 texts did not seem clear. This high-
lighted a number of additional issues.
5.3.1 Describing events at different temporal
time-scales
BT-45 does not always do a good job of correctly
identifying long-term trends in a context where there
are also short-term patterns such as spikes. In fact
accurately detecting simultaneous events at different
time-scales is one of the major signal analysis chal-
lenges in BT-45. There are linguistic issues as well
as signal analysis ones; for example, should long-
duration and short-duration events be described in
separate paragraphs?
5.3.2 Poor communication of time
BT-45 texts often did not communicate time well.
This is for a number of reasons, of which the
most fundamental is problems describing the time
of long-duration events. For instance, in our ex-
ample scenario, the sequence of insert/remove line
events in Figure 2 is analysed by the data-abstraction
module as the abstract event LINE-INSERTION-
PROCESS-FAILED, with a start time of 13.53 (first
insertion event) and an end time of 14.09 (last re-
moval event). BT-45 expresses this as There were 3
failed attempts to insert a peripheral venous line at
13:53; the time given is the time the abstract event
started, which is reasonable in this case. Now, if the
final insertion attempt at 14.08 had been successful,
the BT-45 data abstraction module would have in-
stead produced the abstract event LINE-INSERTION-
PROCESS-SUCCEEDED, with similar times, and BT-
45 would have produced the text
After three attempts, at 13.53 a peripheral
venous line was inserted successfully.
In other words, the time given would still be the time
that the abstract event started; but this is mislead-
ing, because readers of the above text expect that
the stated time is the time of the successful inser-
tion (14.08), not the time at which the sequence of
insert/remove events started.
We need a much better model of how to communi-
cate time, and how this communication depends on
the semantics and linguistic expression of the events
being described. An obvious first step, which we are
currently working on, is to include a linguistically-
motivated temporal ontology (Moens and Steedman,
1988), which will be separate from the existing do-
main ontology. We also need better techniques for
communicating the temporal relationships between
events in cases where they are not listed in chrono-
logical order (Oberlander and Lascarides, 1992).
6 Discussion
Two discourse analysts from Edinburgh University,
Dr. Andy McKinlay and Dr Chris McVittie, kindly
examined and compared some of the human and BT-
45 texts. Their top-level comment was that the hu-
man texts had much better narrative structures than
the BT-45 texts. They use the term ?narrative? in
153
the sense of Labov (1972, Chapter 9); that is story-
like structures which describe real experiences, and
which go beyond just describing the events and in-
clude information that helps listeners make sense of
what happened, such as abstracts, evaluatives, cor-
relatives, and explicatives.
Dr McKinlay and Dr. McVittie pointed out many
of the problems mentioned above, but they also
pointed out a number of other narrative deficiencies
in the BT-45 texts. The most fundamental was that
the human texts did a much better job of linking re-
lated events into a coherent whole. Other deficien-
cies include the lack of any kind of conclusion in the
BT-45 texts.
We agree with this analysis; it is striking that
many of the specific problems identified are related
to the problem of generating narratives. Continu-
ity, description of related channels, overview of be-
haviour over time, and communication of time are
all aspects of narrative in the broad sense; they are
things we need to get right in order to turn a text
into a story. This point is especially significant in
light of the fact that many of our medical collabora-
tors at Edinburgh have informally told us that they
believe stories are valuable when presenting infor-
mation about the babies, and indeed that a major
problem with data visualisation systems compared
to written notes (which they used many years ago) is
that the visualisation systems do not tell stories.
Unfortunately, we are not aware of any previ-
ous research in the NLG community about these is-
sues. Researchers in the creativity community have
looked at issues such as plot and character develop-
ment in systems that generate fictional stories (Perez
y Perez and Sharples, 2004); but this is not relevant
to our problem, which is presenting non-fictional
events as a narrative. Callaway and Lester (2002)
looked at microplanning issues in narrative genera-
tion, including reference, lexical variation, and ag-
gregation; but none of these were identified in our
evaluation as major problems in text quality.
7 Future Work
The BABYTALK project continues until August
2010, and during this period we hope to investigate
most of the issues identified above, especially the
ones related to narrative. We are currently conduct-
ing experiments to improve the way we communi-
cate time, and we have started redoing the docu-
ment planner to do a better job of describing sys-
tems of related channels in a unified manner. We
are also investigating top-down data abstraction and
document planning approaches which we hope will
address continuity problems, and which may assist
in better overviews and narrative structures. We are
also working on many issues not directly related to
narrative, such as reasoning about and communicat-
ing uncertainty, use of vague language, generation
of texts for non-specialists (e.g., parents), and HCI
issues.
We would welcome interest by other researchers
in these topics (there is more that needs investigat-
ing than we can do on our own!), and we would be
happy to assist such people, for example by sharing
some of our code and data resources.
8 Conclusion
We believe that there is enormous potential in sys-
tems such as BABYTALK which generate textual
summaries of data; the world desperately needs bet-
ter techniques to help people understand data sets,
and our experiments suggest that good textual sum-
maries really can help communicate data sets, at
least in some contexts. However, building good
data summarisation systems requires the NLG re-
search community to address a number of problems
which it has not traditionally focused on, many of
which have to do with generating good narratives.
We intend to focus much of our energy on these
issues, and would welcome research contributions
from other members of the community.
Acknowledgements
Many thanks to our colleagues in the BabyTalk
project, and to the doctors and nurses who partic-
ipated in the evaluation; this work would not have
been possible without them. Special thanks to Dr
McKinlay and Dr McVittie for agreeing to anal-
yse the texts for us. We are also grateful to our
colleagues in the Aberdeen NLG group, and to the
anonymous reviewers, for their helpful comments.
This research was funded by the UK Engineer-
ing and Physical Sciences Research Council, under
grant EP/D049520/1.
154
References
A Belz and E Reiter. 2006. Comparing automatic and
human evaluation of NLG systems. In Proceedings of
EACL-2006, pages 313?320.
C Callaway and J Lester. 2002. Narrative prose genera-
tion. Artificial Intelligence, 139:213?252.
G Carenini and J Moore. 2006. Generating and eval-
uating evaluative arguments. Artificial Intelligence,
170:925?952.
P Carpenter and P Shah. 1998. A model of the percep-
tual and conceptual processes in graph comprehension.
Journal of Experimental Psychology: Applied, 4:74?
100.
L Ferres, A Parush, S Roberts, and G Lindgaard. 2006.
Helping people with visual impairments gain access to
graphical information through natural language: The
iGraph system. In Proceedings of ICCHP-2008.
E Goldberg, N Driedger, and R Kittredge. 1994. Using
natural-language processing to produce weather fore-
casts. IEEE Expert, 9(2):45?53.
W Labov. 1972. Language in the Inner City. University
of Pennsylvania Press.
A Law, Y Freer, J Hunter, R Logie, N McIntosh, and
J Quinn. 2005. A comparison of graphical and textual
presentations of time series data to support medical de-
cision making in the neonatal intensive care unit. Jour-
nal of Clinical Monitoring and Computing, 19:183?
194.
M Moens and M Steedman. 1988. Temporal ontology
and temporal reference. Computational Linguistics,
14(2):15?28.
J Oberlander and A Lascarides. 1992. Preventing false
temporal implicatures: Interactive defaults for text
generation. In Proceedings of COLING-1992, pages
721?727.
R Passonneau. 2006. Measuring agreement on set-
valued items (MASI) for semantic and pragmatic an-
notation. In Proceedings of LREC-2006.
R Perez y Perez and M Sharples. 2004. Three computer-
based models of storytelling: Brutus, Minstrel, and
Mexica. Knowledge-Based Systems, 17:15?29.
F Portet, E Reiter, J Hunter, and S Sripada. 2007. Auto-
matic generation of textual summaries from neonatal
intensive care data. In Proceedings of AIME 2007.
R Power, D Scott, and N Bouayad-Agha. 2003. Doc-
ument structure. Computational Linguistics, 29:211?
260.
E Reiter, S Sripada, J Hunter, J Yu, and I Davy. 2005.
Choosing words in computer-generated weather fore-
casts. Artificial Intelligence, 167:137?169.
E Reiter. 2007. An architecture for Data-to-Text sys-
tems. In Proceedings of ENLG-07, pages 97?104.
S Sripada, E Reiter, and L Hawizy. 2005. Evaluation of
an NLG system using post-edit data: Lessons learned.
In Proceedings of ENLG-2005, pages 133?139.
R Turner, S Sripada, E Reiter, and I Davy. 2008. Using
spatial reference frames to generate grounded textual
summaries of georeferenced data. In Proceedings of
INLG-2008.
M van der Meulen, R Logie, Y Freer, C Sykes, N McIn-
tosh, and J Hunter. submitted. When a graph is poorer
than 100 words: A comparison of computerised natu-
ral language generation, human generated descriptions
and graphical displays in neonatal intensive care.
J Yu, E Reiter, J Hunter, and C Mellish. 2007. Choosing
the content of textual summaries of large time-series
data sets. Natural Language Engineering, 13:25?49.
155
Textual Properties and Task Based Evaluation: Investigating the Role of
Surface Properties, Structure and Content.
Albert Gatt
Institute of Linguistics
University of Malta
albert.gatt@um.edu.mt
Franc?ois Portet
Laboratoire d?Informatique de Grenoble
Grenoble Institute of Technology
francois.portet@imag.fr
Abstract
This paper investigates the relationship be-
tween the results of an extrinsic, task-
based evaluation of an NLG system and
various metrics measuring both surface
and deep semantic textual properties, in-
cluding relevance. The latter rely heav-
ily on domain knowledge. We show that
they correlate systematically with some
measures of performance. The core ar-
gument of this paper is that more domain
knowledge-based metrics shed more light
on the relationship between deep semantic
properties of a text and task performance.
1 Introduction
Evaluation methodology in NLG has generated a
lot of interest. Some recent work suggested that
the relationship between various intrinsic and ex-
trinsic evaluation methods (Spa?rck-Jones and Gal-
liers, 1996) is not straightforward (Reiter and
Belz, 2009; Gatt and Belz, to appear), leading to
some arguments for more domain-specific intrin-
sic metrics (Foster, 2008). One reason why these
issues are important is that reliable intrinsic eval-
uation metrics that correlate with performance in
an extrinsic, task-based setting can inform system
development. Indeed, this is often the stated pur-
pose of evaluation metrics such as BLEU (Papineni
et al, 2002) and ROUGE (Lin and Hovy, 2003),
which were originally characterised as evaluation
?understudies?.
In this paper we take up these questions in the
context of a knowledge-based NLG system, BT-45
(Portet et al, 2009), which summarises medical
data for decision support purposes in a Neona-
tal Intensive Care Unit (NICU). Our extrinsic
data comes from an experiment involving com-
plex medical decision making based on automati-
cally generated and human-authored texts (van der
Meulen et al, 2009). This gives us the oppor-
tunity to directly compare the textual character-
istics of generated and human-written summaries
and their relationship to decision-making perfor-
mance. The present work uses data from an ear-
lier study (Gatt and Portet, 2009), which presented
some preliminary results along these lines for the
system in question. We extend this work in a num-
ber of ways. Our principal aim is to test the va-
lidity not only of general-purpose metrics which
measure surface properties of text, but also of met-
rics which make use of domain knowledge, in the
sense that they attempt to relate the ?deep seman-
tics? of the texts to extrinsic factors, based on an
ontology for the BT-45 domain.
After an overview of related work in section 2,
the BT-45 system, its domain ontology and the ex-
trinsic evaluation are described in section 3. The
ontology plays an important role in the evaluation
metrics presented in Section 5. Finally, the eval-
uation of the methods is presented in Section 6,
before discussing and concluding in Section 7.
2 Related Work
In NLG evaluation, extrinsic, task-based methods
play a significant role (Reiter et al, 2003; Karasi-
mos and Isard, 2004; Stock et al, 2007). De-
pending on the study design, these studies often
leave open the question of precisely which as-
pects of a system (and of the text it generates)
contribute to success or failure. Intrinsic NLG
evaluations often involve ratings of text quality
or responses to questionnaires (Lester and Porter,
1997; Callaway and Lester, 2002; Foster, 2008),
with some studies using post-editing by human ex-
perts (Reiter et al, 2005). Automatically com-
puted metrics exploiting corpora, such as BLEU,
NIST and ROUGE, have mainly been used in eval-
uations of the coverage and quality of morphosyn-
tactic realisers (Langkilde-Geary, 2002; Callaway,
2003), though they have recently also been used
for subtasks such as Referring Expression Gener-
ation (Gatt and Belz, to appear) as well as end-to-
end weather forecasting systems (Reiter and Belz,
2009). The widespread use of these metrics in
NLP partly rests on the fact that they are quick
and cheap, but there is controversy about their re-
liability both in MT (Calliston-Burch et al, 2006)
and summarisation (Dorr et al, 2005; Liu and Liu,
2008). As noted in Section 1, similar questions
have been raised in NLG. One of the problems
associated with these metrics is that they rely on
the notion of a ?gold standard?, which is not al-
ways precisely definable given multiple solutions
to the same generation, summarisation or transla-
tion task. These observations underlie recent de-
velopments in Summarisation evaluation such as
the Pyramid method (Nenkova and Passonneau,
2004), which in addition also emphasises content
overlap with a set of reference summaries, rather
than n-gram matches.
It is interesting to note that, with some excep-
tions (Foster, 2008), most of the methodologi-
cal studies on intrinsic evaluation cited here have
focused on ?generic? metrics (corpus-based au-
tomatic measures being foremost among them),
none of which use domain knowledge to quantify
those aspects of a text related to its content. There
is some work in Summarisation that suggests that
incorporating more knowledge improves results.
For example, Yoo and Song (Yoo et al, 2007)
used the Medical Subject Headings (MeSH) to
construct graphs representing the high-level con-
tent of documents, which are then used to clus-
ter documents by topic, each cluster being used to
produce a summary. In (Plaza et al, 2009), the
authors have proposed a summarisation method
based on WordNet concepts and showed that this
higher level representation improves the summari-
sation task.
The principal aim of this paper is to develop
metrics with which to compare texts using domain
knowledge ? in the form of the ontology used in
the BT-45 system ? and to correlate results to hu-
man decision-making performance. The resulting
metrics focus on aspects of content, structure and
relevance that are shown to correlate meaningfully
with task performance, in contrast to other, more
surface-oriented ones (such as ROUGE).
3 The BT-45 System
BT-45(Portet et al, 2009) was designed to gen-
erate a textual summary of 45 minutes of patient
data in a Neonatal Intensive Care Unit (NICU), of
the kind shown in Figure 1(a). The corresponding
summary for the same data shown in Figure 1(b)
is a two-step consensus summary written by two
expert neonatologists. These two summaries cor-
respond to two of the conditions in the task-based
evaluation experiment described below.
In BT-45, summaries such as Figure 1(a) were
generated from raw input data consisting of (a)
physiological signals measured using sensors for
various parameters (such as heart rate); and (b)
discrete events logged by medical staff (e.g. drug
administration). The system was based on a
pipeline architecture which extends the standard
NLG tasks such as document planning and mi-
croplanning with preliminary stages for data anal-
ysis and reasoning. The texts generated were de-
scriptive, that is, they kept interpretation to a min-
imum (for example, the system did not make di-
agnoses). Nor were they generated with a bias to-
wards specific problems or actions that could be
considered desirable for a clinician to take in a par-
ticular context.
Every stage of the generation process made use
of a domain-specific ontology of around 550 con-
cepts, an excerpt of which is shown in Figure 1(c).
The ontology classified objects of type EVENT
and ENTITY into several subtypes; for example,
a DRUG ADMINISTRATION is an INTERVENTION,
which means it involves an agent and a patient.
The ontology functioned as a repository of declar-
ative knowledge, on the basis of which produc-
tion rules were defined to support reasoning in or-
der to make abstractions and to identify relations
(such as causality) between events detected in the
data based on their ontological class and their spe-
cific properties. In addition to the standard IS-A
links, the ontology contains functional relation-
ships which connect events to concepts represent-
ing physiological systems (such as the respiratory
or cardiovascular systems); these are referred to as
functional concepts. For example, in Figure 1(c), a
FEED event is linked to NUTRITION, meaning that
it is primarily relevant to the nutritional system.
These links were included in the ontology follow-
ing consultation with a senior neonatal consultant
after the development of BT-45 was completed.
Their inclusion was motivated by the knowledge-
based evaluation metrics developed for the pur-
poses of the present study, and discussed further
Over the next 38 minutes T1 stayed at around
37.4.
By 13:33 TcPO2 had rapidly decreased to 2.7.
Previously HR had increased to 173.
By 13:35 there had been 2 successive desatura-
tions down to 56. Previously T2 had increased
to 35.5.
By 13:40 SaO2 had rapidly decreased to 79.
(a) BT-45 summary
At the start of the monitoring period: HR
baseline is 145-155, oxygen saturation is
99%, pO2 = 4.9 and CO2 = 10.3 Mean BP is
37-47; T1 and T2 are 37.3degC and 34.6degC
respectively.
At 13:33 there is a desaturation to 59%, which
is accompanied by a drop in pO2 to 1.3 and
a decrease in HR to 122. The blood pressure
rises toward the end of this episode to 49. These
parameters return to their baselines by 13:37.
(b) Human summary (c) Ontology excerpt
Figure 1: Excerpts from Human and BT-45 summaries, and ontology example.
in Section 5.
The task-based experiment to evaluate BT-45
was conducted off-ward and involved a group of
35 clinicians, who were exposed to 24 scenarios,
each covering approximately 45 minutes of patient
data, together with a short introductory text which
gave some background about the patient. The pa-
tient data was then presented in one of three condi-
tions: graphically using a time-series plot, and tex-
tually in the form of a consensus summary written
by human experts (H; Figure 1(b)) and one gener-
ated automatically by BT-45(C; Figure 1(a)). Like
the BT-45 texts, the H texts did not give interpre-
tations or diagnoses and every effort was made not
to bias a reader in favour of certain courses of ac-
tion. A Latin Square design was used to ensure
that each scenario was shown to an equal number
of participants in each condition, while no partici-
pant saw the same scenario in more than one con-
dition.
For each scenario, the task was to select one
or more appropriate clinical actions from a prede-
fined set of 18, one of which was ?no action?. Se-
lections had to be made within three minutes, after
which the scenario timed out. The same choice of
18 actions was given in each scenario s, but for
each one, two neonatal experts identified the sub-
sets of appropriate (APs), inappropriate/potentially
harmful (INAPs) and neutral actions. One of the
appropriate actions was also deemed to be the ?tar-
get?, that is, the most important action to take.
In three scenarios, the ?target? was ?no action?.
For each participant p and scenario s, the perfor-
mance score P ps was based on the proportion PAPs
of actions selected out of APs, and the proportion
PINAPs selected out of the set of inappropriate ac-
tions INAPs: P
p
s = PAPs ? PINAPs ? [?1, 1].
Overall, decision making in the H condition was
better (Ps = .45SD=.10) than either C (Ps =
.41SD=.13) or G (Ps = .40SD=.15). No sig-
nificant difference was found between the latter
two, but the H texts were significantly better than
the C texts, as revealed in a by-subjects ANOVA
(F (1, 31) = 5.266, p < 0.05). We also performed
a post-hoc analysis, comparing the proportions of
appropriate actions selected, PAP and that of inap-
propriate actions PINAP in the H and C conditions
across scenarios. In addition, we computed a dif-
ferent score SPAP, defined as the proportion of ap-
propriate actions selected by a participant within
a scenario out of the total number of actions se-
lected (effectively a measure of ?precision?). A
comparison between means for these three scores
obtained across scenarios showed no significant
differences.
In the analysis reported in Section 6, we com-
pare our textual metrics to both the global score
P as well as to these three other performance in-
dicators. In various follow-up analyses (van der
Meulen et al, 2009; Reiter et al, 2008), it was
found that the three scenarios in which the tar-
get action was ?no action? may have misled some
participants, insofar as this option was included
among a set of other actions, some of which were
themselves deemed appropriate or at least neutral
(in the sense that they could be carried out without
harming the patient). We shall therefore exclude
these scenarios from our analyses.
<P>
At 14:15 hours
<EVENT TYPE="HEEL_PRICK" ID="e11">
a heel prick is done.
</EVENT>
<EVENT TYPE="TREND" SOURCE="HR" DIRECTION="increasing" ID="e12">
The HR increases
</EVENT>
at this point and for 7 minutes from the start of this procedure
<EVENT CARDINALITY="3" SOURCE="SaO2" TYPE="ARTIFACT" ID="e13">
there is a lot of artefact in the oxygen saturation trace.
</EVENT>
</P>
<TREL ARG0="e11" ARG1="TIMESTAMP" RELATION="at" />
<TREL ARG0="e12" ARG1="e11" RELATION="starts" />
<TREL ARG0="e13" ARG1="e11" RELATION="starts" />
(a) Annotation (b) Normalised tree
Figure 2: Fragment of an annotated summary and normalised tree representation.
4 Corpus Annotation
For this study, we annotated the H and C texts
from our experiment using the ontology, in or-
der to make both their semantic content and struc-
ture explicit. Figure 2(a) shows an excerpt from
an annotated text. Every paragraph of the text is
marked up explicitly. All segments of the text cor-
responding to an ontology EVENT are marked up
with a TYPE (the name of the concept in the on-
tology) and other properties, such as DIRECTION
and SOURCE in the case of trends in physiolog-
ical parameters. The CARDINALITY attribute is
used to indicate that a single text segment abstracts
over several occurrences in the data; for example,
the statement about artefacts in the example corre-
sponds to three such episodes in the data.
In addition to events, the markup also includes
separate nodes for all the temporal (TREL) and
discourse (DREL) relations which are explicitly
mentioned in the text, typically using adverbial or
prepositional phrases or verbs of causality. Ev-
ery TREL and DREL points to two arguments and
has a RELATION attribute. In the case of a TREL,
the value is one of the temporal relations de-
fined by (Allen, 1983). For DRELs, values were
restricted to CAUSE and CONTRAST (Mann and
S.Thompson, 1988). One of the arguments of a
TREL can be a timestamp, rather than an event.
This is the case for the first sentence in the frag-
ment, where event e11 is specified as having oc-
curred at a specific time (at 14:15). By contrast,
r4 is a relation between e11 and e12, where
the RELATION is STARTS, indicating that the text
specifies that e11 is used by the author as the an-
chor point to specify the start of e12, as reflected
by the expression at this point.
The markup provided the basis on which many
of the metrics described in the following section
were computed. Based on the annotation, we
used a normalised structural representation of the
texts as shown in Figure 2(b), consisting of PARA-
GRAPH (P) nodes which subsume events and rela-
tions. Relations dominate their event arguments.
For example, the starts TREL holding between
e12 and e11 is represented by a STARTS node
subsuming the two events. In case an event is
dominated by more than one relation (for exam-
ple, it is temporally related to two events, as e11
is in Figure 2(a), we maintain the tree structure by
creating two copies of the event, which are sub-
sumed by the two relations. Thus, the normalised
tree representation is a ?compiled out? version of
the graph representing all events and their rela-
tions. The tree representation is better suited to
our needs, given the complexity of comparing two
graphs.
5 Metrics
The evaluation metrics used to score texts written
by domain experts and those generated by the BT-
45 system fall into three main classes, described
below.
Semantic content and structure To compare
both the content and the structure of texts, we used
three measures. The first quantifies the number
of EVENT nodes in an annotated text, defined as
?
e?E c, where E is the set of events mentioned,
and c is the value of the CARDINALITY attribute
of an event e ? E. Similarly, we computed the
number of temporal (TREL) and discourse (DREL)
relations mentioned in a text. We also used the
Tree Edit Distance metric to compute the distance
between the tree representations of the H and C
texts (see Figure 2(b)). This measure computes
the minimum number of node insertions, dele-
tions and substitutions required to transform one
tree into another and therefore takes into account
not only the content (events and relations) but
also its structural arrangement in the text. The
edit distance between two trees is computed using
the standard Levenshtein edit distance algorithm,
computed over a string that represents the preorder
traversal of the two trees, using a cost of 1 for in-
sertions and deletions, and 2 for substitutions.
N-gram overlap As a measure of n-gram over-
lap, we use ROUGE-n, which measures simple
n?gram overlap (in the present paper we use
n = 4). We also use ROUGE-SU, in which over-
lap is computed using skip-bigrams while also ac-
counting for unigrams that a text has in common
with its reference (in order to avoid bias against
texts which share several unigrams but few skip
bigrams).
Domain-dependent relevance metrics The
metrics described so far make use of domain
knowledge only to the extent that this is reflected
in the textual markup. We now consider a family
of metrics which are much more heavily reliant
on domain-specific knowledge structures and
reasoning. In our domain, the relevance of a
text in a given experimental scenario s can be
defined in terms of whether the events it mentions
have some relationship to the appropriate clinical
actions (APs). We attempt to model some aspects
of this using a weighting strategy and reasoning
rules.
Recall from Section 3 that fc?s represent the
various physiological systems to which an event
or action can be related. Therefore, each event e
mentioned in a text can be related to a set of pos-
sible actions using the functional concepts fc(e)
to which that event is linked in the ontology. Let
Es,t be the set of events mentioned in text t for
scenario s. An event e ? Es,t references an action
a iff FC(e) ? FC(a) 6= ?. Our hypothesis is that
an appropriate action is more likely to be taken if
there are events which reference it in the text ? that
is, if the text mentions things which are directly or
indirectly relevant to the action. For instance, if a
text mentions events related to the RESPIRATION
fc, a clinician might be more likely to make a de-
cision to manage a patient?s ventilation support.
It is worth emphasising that, since both the BT-
45 and human-authored texts were descriptive and
were not written or generated with the appropriate
actions in mind, the hypothesis that the relevance
of the content to the appropriate actions might in-
crease the likelihood of these actions being chosen
is far from a foregone conclusion.
Part of the novelty in this way of quantifying
relevance lies in its use of the knowledge (i.e. the
ontology) that is already available to the system,
rather than asking human experts to rate the rele-
vance of a text, a time-consuming process which
could be subject to experts? personal biases. How-
ever, this way of conceptualising relevance gener-
ates links to too many actions for one event. It is
often the case that an event, through its association
with a functional concept, references more than
one action, but not all of these are appropriate.
For example, a change in oxygen saturation can
be related to RESPIRATION, which itself is related
to several respiration-related actions in a scenario,
only some of which are appropriate. Clearly, rele-
vance depends not only on a physiological connec-
tion between an event and a phsiological system
(functional concept), but also on the context, that
is, the other events and their relative importance
in a given scenario. Another factor that needs to
be taken into account is the overall probability of
an action. Some actions are performed routinely,
while others tend to be associated with emergen-
cies (e.g. a nappy change is much more frequent
over all than resuscitating a patient). This means
that some actions ? even appropriate ones ? may
have been less likely to be selected even though
they were referenced by the text and were appro-
priate.
We prune unwarranted connections between
events and actions by taking into account (a) a pa-
tient?s current status (described in the text and in
the background information given to experimental
participants); (b) the fact that some actions have
much higher prior probabilities than others be-
cause they are performed more routinely; (c) the
fact that some events may be more important than
others (e.g. resuscitation is much more important
than a nappy change). Based on this, we define the
weight of an action a as follows:
Wa =
P
e?E
Pr(a)?e.importance
P
a?Ae
Pr(a)
P
e?E e.importance
(1)
Where E is the set of events in the text, Ae the
set of actions related to event e, e.importance ?
N+ the importance of the event e and Pr(a) the
prior probability of action a. All weights are nor-
malised so that the following inequalities hold:
X
a?Ae
Pr(a) ? e.importance
P
a?Ae
Pr(a)
= e.importance (2)
X
a?A
Wa = 1 (3)
where A is the set of all possible actions. The idea
is that an event e makes some contribution (pos-
sibly 0) to the relevance of some actions Ae, and
the total weight of the event is distributed among
all actions related to it using (a) the prior probabil-
ity Pr(a) of each action (the most frequent action
will have more weight) and (b) the importance of
the event. At the end of the process each action
would be assigned a score representing the accu-
mulated weights of the events, which is then nor-
malised, so that
?
a?A Wa = 1.
The prior probability in the equation is meant
to reflect our earlier observation that clinical ac-
tions differ in the frequency with which they are
performed and this may bias their selection. Pri-
ors were computed using maximum likelihood es-
timates from a large database containing exhaus-
tive annotations of clinical actions recorded by an
on-site research nurse over a period of 4 months in
a NICU, which contains a total of 43,889 records
of actions (Hunter et al, 2003).
The importance value in equation (1) is meant
to reflect the fact that events in the text do not
attract the attention of a reader to the same ex-
tent, since they do not have the same degree
of ?severity? or ?surprise?. We operationalise
this by identifying the superconcepts in the on-
tology (PATHOLOGICAL-FUNCTION, DISORDER,
SURGICAL-INTERVENTION, etc.) which could be
thought of as representing ?drastic? occurrences.
To these we added the concept of a TREND which
corresponds to a change in a physiological param-
eter (such as an increase in heart rate), based on the
rationale that the primary aim of NICU staff is to
keep a patient stable, so that any physiological in-
stability warrants an intervention. The importance
of events subsumed by these superconcepts was
then set to be three times that of ?normal? events.
Finally, we apply knowledge-based rules to
prune the number of actions Ae related to an event
e. As an example, a decision to intubate a baby
depends not only on events in the text which ref-
erence this action, but also on whether the baby is
already intubated. This can be assessed by check-
ing whether s/he is on CMV (a type of ventila-
tion which is only used after intubation). The rule
is represented as INTUBATE ? ?on(baby, CMV).
Although such rules are extremely rough, they do
help to prune inconsistencies.
Two scores were computed for both human
and computer texts using equation (1). RELs,t
is the sum of weights of actions referenced in
a text t for scenario s which are appropriate:
RELs,t =
?
a?Aap Wa. Conversely, IRRELs,t
quantifies the weights of actions referenced in t
for scenario s which are inappropriate: IRRELs,t =?
a?Ainap
Wa.
6 Results
In what follows, we report two-tailed Pearson?s r
correlations to compare our metrics to the three
performance measures discussed in Section 3: P ,
the global performance score; PAPP and PINAPP, the
proportion of appropriate (resp. inappropriate) ac-
tions selected from the subsets of in/appropriate
(resp. inappropriate) actions in a scenario; and
SPAPP, the proportion of appropriate actions se-
lected by a participant out of the set of actions se-
lected. The last three are included because they
shed light more directly on the extent to which
experimental participants chose correctly or incor-
rectly. In case a metric measures similarity or dif-
ference between texts, the correlation reported is
with the difference between the H scores and the C
scores. Where relevant, we also report correlations
with the absolute mean performance scores within
the H and/or C conditions. Correlations exclude
the three scenarios which had ?no action? as the
target appropriate action, though where relevant,
we will indicate whether the correlations change
when these scenarios are also included.
6.1 Content and Structure
Overall, the C texts mentioned significantly fewer
events than the H texts (t20 = 2.44, p = .05),
and also mentioned fewer temporal and discourse
relations explicitly (t20 = 3.70, p < .05). In
P (H-C) PAPP (H-C) SPAPP (H-C) PINAP (H-C)
Events (H-C) .43? .42? .02 -.09
Relations (H-C) .34 .30 0 -.15
Tree Edit .36 .33 .09 -.14
Table 1: Correlations between performance differences and content/structure measures. ?significant at
p = .05; ?approaches significance at p = .06
the case of the H texts, the number of events
and relations did not correlate significantly with
any of the performance scores. In the case of
the C texts, the number of relations mentioned
was significantly negatively correlated to PINAP
(r = ?.49, p < .05), and positively correlated
to SPAPP (r = .7, p < .001). This suggests
that temporal and discourse relations made texts
more understandable and resulted in more appro-
priate actions being taken. More unexpectedly, the
number of events mentioned was negatively cor-
related to PAPP (r = ?.53, p < .05) and to P
(r = ?.5, p < .05). This may have been due to the
C texts mentioning a number of events that were
relatively unimportant and/or irrelevant to the ap-
propriate actions.
Table 1 displays correlations between perfor-
mance differences between H and C, and differ-
ences in number of events and relations, as well
as Tree Edit Distance. The positive correlation
between the number of events mentioned and P
suggests that a larger amount of content in the
H texts is partially responsible for the difference
in decision-making accuracy by experimental par-
ticipants. This is further supported by the fact
that the correlation with the difference in PAPP ap-
proaches significance. It is worth noting that none
of these correlations are significant when means
from the three ?no action? scenarios are included
in the computation. This further supports our ear-
lier conclusion that these three scenarios are out-
liers. Somewhat surprisingly, Tree Edit Distance
does not correlate significantly with any of the per-
formance differences, though the correlations go
in the expected directions (positive in the case of
P , SPAPP and PAPP, negative in the case of PINAP).
This may be due to the high variance in the Edit
Distance scores (mean: 66.5; SD: 34.8).
Overall, these results show that differences in
both content and structure made the H texts supe-
rior and human texts did a much better job at ex-
plicitly relating events or situating them in time,
which is crucial for comprehension and correct
decision-making. This point has previously been
Absolute Scores (C) Differences (H-C)
P PAP PINAP SPAP P PAP PINAP SPAP
R-4 .33 .38 .2 -.03 -.19 -.2 -.01 -.1
R-SU -.03 -.02 .05 -.31 .04 .01 -.1 .13
Table 2: Correlations between ROUGE and perfor-
mance scores in the C condition. ?significant at
p = .05.
made in relation to the same data on the basis of a
qualitative study (Reiter et al, 2008).
6.2 N-gram Overlap
Correlations with ROUGE-4 and ROUGE-SU are
shown in Table 2 both for absolute performance
scores on the C texts, and for the differences be-
tween H and C. This is because ROUGE can be
interpreted in two ways: on the one hand, it mea-
sures the ?quality? of C texts relative to the ref-
erence human texts; on the other it also indicates
similarity between C and H.
There are no significant correlations between
ROUGE and any of our performance measures. Al-
though this leaves open the question of whether a
different set of performance measures, or a differ-
ent experiment, would evince a more systematic
covariation, the results suggest that it is not sur-
face similarity (to the extent that this is measured
by ROUGE) that is contributing to better decision
making. It is however worth noting that some cor-
relations with ROUGE-4, namely those involving
P and PAPP, do turn out significant when the ?no
action? scenarios are included. This turns out to
be solely due to one of the ?no action? scenar-
ios, which had a much higher ROUGE-4 score than
the others, possibly because the corresponding hu-
man text was comparatively brief and the number
of events mentioned in the two texts was roughly
equal (11 for the C text, 12 for the H text).
6.3 Knowledge Based Relevance Metrics
Finally, we compare our knowledge-based mea-
sures of the relevance of the content to appropri-
ate actions (REL) and to inappropriate actions (IR-
REL). The correlations between each measure and
Human (H) BT-45 (C)
P PAP PINAP SPAP P PAP PINAP SPAP
REL .14 .11 -.14 .60? .33 .24 -.49? .7?
IRREL -.25 -.22 .1 -.56? -.34 -.26 .43 -.62?
Table 3: Correlations between knowledge-based relevance scores and absolute performance scores in the
C and H conditions. ?significant at p ? .05.
the absolute performance scores in each condition
are displayed in Table 3.
The absolute scores in Table 3 show that both
REL and IRREL are significantly correlated to
SPAPP, the proportion of appropriate actions out
of the actions selected by participants. The cor-
relations are in the expected direction: there is a
strong tendency for participants to choose more
appropriate actions when REL is high, and the re-
verse is true for IRREL. In the case of the C texts,
there is also a negative correlation (as expected)
between REL and PINAP, though this is the only
one that reaches significance with this variable. It
therefore appears that the knowledge-based rele-
vance measures evince a meaningful relationship
with at least some of the more ?direct? measures of
performance (those assessing the relative prefer-
ence of participants for appropriate actions based
on a textual summary), though not with the global
preference score P . One possible reason for the
low correlations with the latter is that the two mea-
sures attempt to quantify directly the relevance of
the content units in a text to in/appropriate courses
of action; hence, they have a more direct relation-
ship to measures of proportions of the courses of
actions chosen.
7 Discussion and Conclusions
We conclude this paper with some observations
about the relative merit of different measures of
textual characteristics. ?Standard?, surface-based
measures such as (ROUGE) do not display any sys-
tematic relationship with our extrinsic measures
of performance, recalling similar observations in
the NLG literature (Gatt and Belz, to appear) and
in MT and Summarisation (Calliston-Burch et al,
2006; Dorr et al, 2005). Some authors have also
reported that ROUGE does not correlate well with
human judgements of NLG texts (Reiter and Belz,
2009). On the other hand, we do find some evi-
dence that the amount of content in texts, and the
extent to which they explicitly relate content el-
ements temporally and rhetorically, may impact
decision-making. The significant correlations ob-
served between the number of relations in a text
and the extrinsic measures are worth emphasis-
ing, as they suggest a significant role not only for
content, but also rhetorical and temporal structure,
something that many metrics do not take into ac-
count.
Perhaps the most important contribution of this
paper has been to emphasise knowledge-based as-
pects of textual evaluation, not only by measur-
ing content units and structure, but also by de-
veloping a motivated relevance metric, the cru-
cial assumption being that the utility of a sum-
mary is contingent on its managing to convey in-
formation that will motivate a reader to take the
?right? course of action. The strong correlations
between the relevance measures and the extent to
which people chose the correct actions (or more
accurately, chose more correct actions) vindicates
this assumption.
Some of the correlations which turned out not to
be significant may be due to ?noise? in the data, in
particular, high variance in the performance scores
(as suggested by the standard deviations for P
given in Section 3). They therefore do not war-
rant the conclusion that no relationship exists be-
tween a particular measure and extrinsic task per-
formance; nevertheless, where other studies have
noted similar gaps, the trends in question may be
systematic and general. This, however, can only
be ascertained in further follow-up studies.
This paper has investigated the relationship be-
tween a number of intrinsic measures of text qual-
ity and decision-making performance based on an
external task. Emphasis was placed on metrics
that quantify aspects of semantics, relevance and
structure. We have also compared generated texts
to their human-authored counterparts to identify
differences which can motivate further system im-
provements. Future work will focus on further ex-
ploring metrics that reflect the relevance of a text,
as well as the role of temporal and discourse struc-
ture in conveying the intended meaning.
References
J. F. Allen. 1983. Maintaining knowledge about
temporal intervals. Communications of the ACM,
26(11):832?843.
Charles B. Callaway and James C. Lester. 2002.
Narrative prose generation. Artificial Intelligence,
139(2):213?252.
C. B. Callaway. 2003. Evaluating coverage for large
symbolic NLG grammars. In Proc. IJCAI?03.
C. Calliston-Burch, M. Osborne, and P. Koehn. 2006.
Re-evaluating the role of BLEU in machine transla-
tion research. In Proc. EACL?06.
B. J. Dorr, C. Monz, S. President, R. Schwartz, and
D. Zajic. 2005. A methodology for extrinsic evalu-
ation of text summarization: Does ROUGE correlate?
In Proc. Workshop on Intrinsic and Extrinsic Evalu-
ation Measures.
M.E. Foster. 2008. Automated metrics that agree with
human judgements on generated output for an em-
bodied conversational agent. In Proc. INLG?08.
A. Gatt and A. Belz. to appear. Introducing shared task
evaluation to NLG: The TUNA shared task evalua-
tion challenges. In E. Krahmer and M. Theune, ed-
itors, Empirical Methods in Natural Language Gen-
eration. Springer.
A. Gatt and F. Portet. 2009. Text content and task
performance in the evaluation of a natural language
generation system. In Proc. RANLP?09.
J. Hunter, G. Ewing, L. Ferguson, Y. Freer, R. Logie,
P. McCue, and N. McIntosh. 2003. The NEONATE
database. In Proc. IDAMAP?03.
A. Karasimos and A. Isard. 2004. Multilingual eval-
uation of a natural language generation system. In
Proc. LREC?04.
I. Langkilde-Geary. 2002. An empirical verification of
coverage and correctness for a general-purpose sen-
tence generator. In Proc. INLG?02.
J.C. Lester and B.W. Porter. 1997. Developing and
empirically evaluating robust explanation genera-
tors: The KNIGHT experiments. Computational Lin-
guistics, 23(1):65?101.
C-Y Lin and E. Hovy. 2003. Automatic evaluation of
summaries using n-gram co-occurrence statistics. in.
In Proc. of HLT-NAACL?03.
F. Liu and Y. Liu. 2008. Correlation between rouge
and human evaluation of extractive meeting sum-
maries. In Proc. ACL?08.
W. C. Mann and S.Thompson. 1988. Rhetorical struc-
ture theory: Towards a functional theory of text or-
ganisation. Text, 8(3):243?281.
A. Nenkova and R. Passonneau. 2004. Evaluating
content selection in summarisation: The Pyramid
method. In Proc. NAACL-HLT?04.
S. Papineni, T. Roukos, W. Ward, and W. Zhu. 2002.
BLEU: A method for automatic evaluation of ma-
chine translation. In Proc. ACL?02.
L Plaza, A D??az, and P Gerva?s P. 2009. Auto-
matic summarization of news using wordnet concept
graphs. best paper award. In Proc. IADIS?09.
F. Portet, E. Reiter, A. Gatt, J. Hunter, S. Sripada,
Y. Freer, and C. Sykes. 2009. Automatic generation
of textual summaries from neonatal intensive care
data. Artificial Intelligence, 173(7?8):789?816.
E. Reiter and A. Belz. 2009. An investigation into the
validity of some metrics for automatically evaluat-
ing Natural Language Generation systems. Compu-
tational Linguistics, 35(4):529?558.
E. Reiter, R. Robertson, and L. Osman. 2003. Lessons
from a failure: Generating tailored smoking cessa-
tion letters. Artificial Intelligence, 144:41?58.
E. Reiter, S. Sripada, J. Hunter, J. Yu, and I. Davy.
2005. Choosing words in computer-generated
weather forecasts. Artificial Intelligence, 167:137?
169.
E. Reiter, A. Gatt, F. Portet, and M. van der Meulen.
2008. The importance of narrative and other lessons
from an evaluation of an NLG system that sum-
marises clinical data. In Proc. INLG?08.
K. Spa?rck-Jones and J. R. Galliers. 1996. Evaluating
natural language processing systems: An analysis
and review. Springer, Berlin.
O. Stock, M. Zancanaro, P. Busetta, C. Callaway,
A. Krueger, M. Kruppa, T. Kuflik, E. Not, and
C. Rocchi. 2007. Adaptive, intelligent presen-
tation of information for the museum visitor in
PEACH. User Modeling and User-Adapted Interac-
tion, 17(3):257?304.
M. van der Meulen, R. H. Logie, Y. Freer, C. Sykes,
N. McIntosh, and J. Hunter. 2009. When a graph is
poorer than 100 words. Applied Cognitive Psychol-
ogy, 24(1):77?89.
I. Yoo, X. Hu, and I-Y Song. 2007. A coherent
graph-based semantic clustering and summarization
approach for biomedical literature and a new sum-
marization evaluation method. BMC Bioinformat-
ics, 8(9).
