Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 356?360,
Dublin, Ireland, August 23-24, 2014.
IUCL: Combining Information Sources for SemEval Task 5
Alex Rudnick, Levi King, Can Liu, Markus Dickinson, Sandra K?ubler
Indiana University
Bloomington, IN, USA
{alexr,leviking,liucan,md7,skuebler}@indiana.edu
Abstract
We describe the Indiana University sys-
tem for SemEval Task 5, the L2 writ-
ing assistant task, as well as some exten-
sions to the system that were completed
after the main evaluation. Our team sub-
mitted translations for all four language
pairs in the evaluation, yielding the top
scores for English-German. The system
is based on combining several information
sources to arrive at a final L2 translation
for a given L1 text fragment, incorporating
phrase tables extracted from bitexts, an L2
language model, a multilingual dictionary,
and dependency-based collocational mod-
els derived from large samples of target-
language text.
1 Introduction
In the L2 writing assistant task, we must translate
an L1 fragment in the midst of an existing, nearly
complete, L2 sentence. With the presence of this
rich target-language context, the task is rather dif-
ferent from a standard machine translation setting,
and our goal with our design was to make effec-
tive use of the L2 context, exploiting collocational
relationships between tokens anywhere in the L2
context and the proposed fragment translations.
Our system proceeds in several stages: (1) look-
ing up or constructing candidate translations for
the L1 fragment, (2) scoring candidate transla-
tions via a language model of the L2, (3) scoring
candidate translations with a dependency-driven
word similarity measure (Lin, 1998) (which we
call SIM), and (4) combining the previous scores
in a log-linear model to arrive at a final n-best
list. Step 1 models transfer knowledge between
This work is licenced under a Creative Commons Attribution
4.0 International License. Page numbers and proceedings
footer are added by the organizers. License details: http:
//creativecommons.org/licenses/by/4.0/
the L1 and L2; step 2 models facts about the L2
syntax, i.e., which translations fit well into the lo-
cal context; step 3 models collocational and se-
mantic tendencies of the L2; and step 4 gives dif-
ferent weights to each of the three sources of in-
formation. Although we did not finish step 3 in
time for the official results, we discuss it here, as
it represents the most novel aspect of the system ?
namely, steps towards the exploitation of the rich
L2 context. In general, our approach is language-
independent, with accuracy varying due to the size
of data sources and quality of input technology
(e.g., syntactic parse accuracy). More features
could easily be added to the log-linear model, and
further explorations of ways to make use of target-
language knowledge could be promising.
2 Data Sources
The data sources serve two major purposes for our
system: For L2 candidate generation, we use Eu-
roparl and BabelNet; and for candidate ranking
based on L2 context, we use Wikipedia and the
Google Books Syntactic N-grams.
Europarl The Europarl Parallel Corpus (Eu-
roparl, v7) (Koehn, 2005) is a corpus of pro-
ceedings of the European Parliament, contain-
ing 21 European languages with sentence align-
ments. From this corpus, we build phrase tables
for English-Spanish, English-German, French-
English, Dutch-English.
BabelNet In the cases where the constructed
phrase tables do not contain a translation for a
source phrase, we need to back off to smaller
phrases and find candidate translations for these
components. To better handle sparsity, we extend
look-up using the multilingual dictionary Babel-
Net, v2.0 (Navigli and Ponzetto, 2012) as a way to
find translation candidates.
356
Wikipedia For German and Spanish, we use re-
cent Wikipedia dumps, which were converted to
plain text with the Wikipedia Extractor tool.
1
To
save time during parsing, sentences longer than 25
words are removed. The remaining sentences are
POS-tagged and dependency parsed using Mate
Parser with its pre-trained models (Bohnet, 2010;
Bohnet and Kuhn, 2012; Seeker and Kuhn, 2013).
To keep our English Wikipedia dataset to a man-
ageable size, we choose an older (2006), smaller
dump. Long sentences are removed, and the re-
maining sentences are POS-tagged and depen-
dency parsed using the pre-trained Stanford Parser
(Klein and Manning, 2003; de Marneffe et al.,
2006). The resulting sizes of the datasets are
(roughly): German: 389M words, 28M sentences;
Spanish: 147M words, 12M sentences; English:
253M words, 15M sentences. Dependencies ex-
tracted from these parsed datasets serve as training
for the SIM system described in section 3.3.
Google Books Syntactic N-grams For English,
we also obtained dependency relationships for our
word similarity statistics using the arcs dataset of
the Google Books Syntactic N-Grams (Goldberg
and Orwant, 2013), which has 919M items, each
of which is a small ?syntactic n-gram?, a term
Goldberg and Orwant use to describe short de-
pendency chains, each of which may contain sev-
eral tokens. This data set does not contain the ac-
tual parses of books from the Google Books cor-
pus, but counts of these dependency chains. We
converted the longer chains into their component
(head, dependent, label) triples and then collated
these triples into counts, also for use in the SIM
system.
3 System Design
As previously mentioned, at run-time, our system
decomposes the fragment translation task into two
parts: generating many possible candidate transla-
tions, then scoring and ranking them in the target-
language context.
3.1 Constructing Candidate Translations
As a starting point, we use phrase tables con-
structed in typical SMT fashion, built with the
training scripts packaged with Moses (Koehn et
al., 2007). These scripts preprocess the bitext, es-
timate word alignments with GIZA++ (Och and
1
http://medialab.di.unipi.it/wiki/
Wikipedia_Extractor
Ney, 2000) and then extract phrases with the
grow-diag-final-and heuristic.
At translation time, we look for the given
source-language phrase in the phrase table, and if
it is found, we take all translations of that phrase
as our candidates.
When translating a phrase that is not found in
the phrase table, we try to construct a ?synthetic
phrase? out of the available components. This
is done by listing, combinatorially, all ways to
decompose the L1 phrase into sub-phrases of at
least one token long. Then for each decomposi-
tion of the input phrase, such that all of its compo-
nents can be found in the phrase table, we gen-
erate a translation by concatenating their target-
language sides. This approach naively assumes
that generating valid L2 text requires no reorder-
ing of the components. Also, since there are 2
n?1
possible ways to split an n-token phrase into sub-
sequences (i.e., each token is either the first token
in a new sub-sequence, or it is not), we perform
some heuristic pruning at this step, taking only
the first 100 decompositions, preferring those built
from longer phrase-table entries. Every phrase in
the phrase table, including these synthetic phrases,
has both a ?direct? and ?inverse? probability score;
for synthetic phrases, we estimate these scores by
taking the product of the corresponding probabili-
ties for the individual components.
In the case that an individual word cannot be
found in the phrase table, the system attempts to
look up the word in BabelNet, estimating the prob-
abilities as uniformly distributed over the available
BabelNet entries. Thus, synthetic phrase table
entries can be constructed by combining phrases
found in the training data and words available in
BabelNet.
For the evaluation, in cases where an L1 phrase
contained words that were neither in our train-
ing data nor BabelNet (and thus were simply out-
of-vocabulary for our system), we took the first
translation for that phrase, without regard to con-
text, from Google Translate, through the semi-
automated Google Docs interface. This approach
is not particularly scalable or reproducible, but
simulates what a user might do in such a situation.
3.2 Scoring Candidate Translations via a L2
Language Model
To model how well a phrase fits into the L2 con-
text, we score candidates with an n-gram lan-
357
guage model (LM) trained on a large sample of
target-language text. Constructing and querying
a large language model is potentially computa-
tionally expensive, so here we use the KenLM
Language Model Toolkit and its Python interface
(Heafield, 2011). Here our models were trained
on the Wikipedia text mentioned previously (with-
out filtering long sentences), with KenLM set to
5-grams and the default settings.
3.3 Scoring Candidate Translations via
Dependency-Based Word Similarity
The candidate ranking based on the n-gram lan-
guage model ? while quite useful ? is based on
very shallow information. We can also rank the
candidate phrases based on how well each of the
components fits into the L2 context using syntactic
information. In this case, the fitness is measured in
terms of dependency-based word similarity com-
puted from dependency triples consisting of the
the head, the dependent, and the dependency la-
bel. We slightly adapted the word similarity mea-
sure by Lin (1998):
SIM(w
1
, w
2
) =
2 ? c(h, d, l)
c(h,?, l) + c(?, d, l)
(1)
where h = w
1
and d = w
2
and c(h, d, l)
is the frequency with which a particular
(head, dependent, label) dependency triple
occurs in the L2 corpus. c(h,?, l) is the fre-
quency with which a word occurs as a head
in a dependency labeled l with any dependent.
c(?, d, l) is the frequency with which a word
occurs as a dependent in a dependency labeled
l with any head. In the measure by Lin (1998),
the numerator is defined as the information of all
dependency features that w
1
and w
2
share, com-
puted as the negative sum of the log probability of
each dependency feature. Similarly, the denom-
inator is computed as the sum of information of
dependency features for w
1
and w
2
.
To compute the fitness of a word w
i
for its
context, we consider a set D of all words that are
directly dependency-related to w
i
. The fitness of
w
i
is thus computed as:
FIT (w
i
) =
?
D
w
j
SIM(w
i
, w
j
)
|D|
(2)
The fitness of a phrase is the average word sim-
ilarity over all its components. For example, the
fitness of the phrase ?eat with chopsticks? would
be computed as:
FIT (eat with chopsticks) =
FIT (eat) + FIT (with) + FIT (chopsticks)
3
(3)
Since we consider the heads and dependents
of a target phrase component, these may be situ-
ated inside or outside the phrase. Both cases are
included in our calculation, thus enabling us to
consider a broader, syntactically determined local
context of the phrase. By basing the calculation on
a single word?s head and dependents, we attempt
to avoid data sparseness issues that we might get
from rare n-gram contexts.
Back-Off Lexical-based dependency triples suf-
fer from data sparsity, so in addition to computing
the lexical fitness of a phrase, we also calculate the
POS fitness. For example, the POS fitness of ?eat
with chopsticks? would be computed as follows:
FIT (eat/VBG with/IN chopsticks/NNS) =
FIT (VBG) + FIT (IN) + FIT (NNS)
3
(4)
Storing and Caching The large vocabulary
and huge number of combinations of our
(head, dependent, label) triples poses an effi-
ciency problem when querying the dependency-
based word similarity values. Thus, we stored
the dependency triples in a database with a
Python programming interface (SQLite3) and
built database indices on the frequent query types.
However, for frequently searched dependency
triples, re-querying the database is still inefficient.
Thus, we built a query cache to store the recently-
queried triples. Using the database and cache sig-
nificantly speeds up our system.
This database only stores dependency triples
and their corresponding counts; the dependency-
based similarity value is calculated as needed, for
each particular context. Then, these FIT scores
are combined with the scores from the phrase ta-
ble and language model, using weights tuned by
MERT.
358
system acc wordacc oofacc oofwordacc
run2 0.665 0.722 0.806 0.857
SIM 0.647 0.706 0.800 0.852
nb 0.657 0.717 0.834 0.868
Figure 1: Scores on the test set for English-
German; here next-best is CNRC-run1.
system acc wordacc oofacc oofwordacc
run2 0.633 0.72 0.781 0.847
SIM 0.359 0.482 0.462 0.607
best 0.755 0.827 0.920 0.944
Figure 2: Scores on the test set for English-
Spanish; here best is UEdin-run2.
3.4 Tuning Weights with MERT
In order to rank the various candidate translations,
we must combine the different sources of infor-
mation in some way. Here we use a familiar log-
linear model, taking the log of each score ? the di-
rect and inverse translation probabilities, the LM
probability, and the surface and POS SIM scores ?
and producing a weighted sum. Since the original
scores are either probabilities or probability-like
(in the range [0, 1]), their logs are negative num-
bers, and at translation time we return the trans-
lation (or n-best) with the highest (least negative)
score.
This leaves us with the question of how to
set the weights for the log-linear model; in this
work, we use the ZMERT package (Zaidan, 2009),
which implements the MERT optimization algo-
rithm (Och, 2003), iteratively tuning the feature
weights by repeatedly requesting n-best lists from
the system. We used ZMERT with its default
settings, optimizing our system?s BLEU scores
on the provided development set. We chose, for
convenience, BLEU as a stand-in for the word-
level accuracy score, as BLEU scores are maxi-
mized when the system output matches the refer-
ence translations.
4 Experiments
In figures 1-4, we show the scores on this year?s
test set for running the two variations of our sys-
tem: run2, the version without the SIM exten-
sions, which we submitted for the evaluation, and
SIM, with the extensions enabled. For compar-
ison, we also include the best (or for English-
German, next-best) submitted system. We see here
system acc wordacc oofacc oofwordacc
run2 0.545 0.682 0.691 0.800
SIM 0.549 0.687 0.693 0.800
best 0.733 0.824 0.905 0.938
Figure 3: Scores on the test set for French-English;
here best is UEdin-run1.
system acc wordacc oofacc oofwordacc
run2 0.544 0.679 0.634 0.753
SIM 0.540 0.676 0.635 0.753
best 0.575 0.692 0.733 0.811
Figure 4: Scores on the test set for Dutch-English;
here best is UEdin-run1.
that the use of the SIM features did not improve
the performance of the base system, and in the
case of English-Spanish caused significant degra-
dation, which is as of yet unexplained, though we
suspect difficulties parsing the Spanish test set, as
for all of the other language pairs, the effects of
adding SIM features were small.
5 Conclusion
We have described our entry for the initial run-
ning of the ?L2 Writing Assistant? task and ex-
plained some possible extensions to our base log-
linear model system.
In developing the SIM extensions, we faced
some interesting software engineering challenges,
and we can now produce large databases of depen-
dency relationship counts for various languages.
Unfortunately, these extensions have not yet led
to improvements in performance on this particu-
lar task. The databases themselves seem at least
intuitively promising, capturing interesting infor-
mation about common usage patterns of the tar-
get language. Finding a good way to make use
of this information may involve computing some
measure that we have not yet considered, or per-
haps the insights captured by SIM are covered ef-
fectively by the language model.
We look forward to future developments around
this task and associated applications in helping
language learners communicate effectively.
359
References
Bernd Bohnet and Jonas Kuhn. 2012. The best of
both worlds ? A graph-based completion model for
transition-based parsers. In Proceedings of the 13th
Conference of the European Chapter of the Associ-
ation for Computational Linguistics (EACL), pages
77?87, Avignon, France.
Bernd Bohnet. 2010. Top accuracy and fast depen-
dency parsing is not a contradiction. In Proceedings
of the 23rd International Conference on Computa-
tional Linguistics (COLING), pages 89?97, Beijing,
China.
Marie-Catherine de Marneffe, Bill MacCartney, and
Christopher Manning. 2006. Generating typed de-
pendency parses from phrase structure parses. In
Proceedings of LREC-06, Genoa, Italy.
Yoav Goldberg and Jon Orwant. 2013. A dataset of
syntactic-ngrams over time from a very large cor-
pus of English books. In Second Joint Conference
on Lexical and Computational Semantics (*SEM),
pages 241?247, Atlanta, GA.
Kenneth Heafield. 2011. KenLM: faster and smaller
language model queries. In Proceedings of the
EMNLP 2011 Sixth Workshop on Statistical Ma-
chine Translation, pages 187?197, Edinburgh, Scot-
land, United Kingdom, July.
Dan Klein and Christopher Manning. 2003. Accurate
unlexicalized parsing. In Proceedings of ACL-2003,
pages 423?430, Sapporo, Japan.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, Chris Dyer, Ondrej Bojar, Alexan-
dra Constantin, and Evan Herbst. 2007. Moses:
Open source toolkit for statistical machine transla-
tion. In Proceedings of the 45th Annual Meeting of
the Association for Computational Linguistics Com-
panion Volume Proceedings of the Demo and Poster
Sessions, Prague, Czech Republic.
Philipp Koehn. 2005. Europarl: A parallel corpus for
statistical machine translation. In MT Summit.
Dekang Lin. 1998. An information-theoretic defini-
tion of similarity. In International Conference on
Machine Learning (ICML), volume 98, pages 296?
304.
Roberto Navigli and Simone Paolo Ponzetto. 2012.
BabelNet: The automatic construction, evaluation
and application of a wide-coverage multilingual se-
mantic network. Artificial Intelligence, 193:217?
250.
Franz Josef Och and Hermann Ney. 2000. Improved
statistical alignment models. In Proceedings of the
38th Annual Meeting of the Association for Com-
putational Linguistics (ACL), pages 440?447, Hong
Kong.
Franz Josef Och. 2003. Minimum error rate train-
ing in statistical machine translation. In Proceed-
ings of the 41st Annual Meeting of the Association
for Computational Linguistics, pages 160?167, Sap-
poro, Japan, July.
Wolfgang Seeker and Jonas Kuhn. 2013. Morphologi-
cal and syntactic case in statistical dependency pars-
ing. Computational Linguistics, 39(1):23?55.
Omar F. Zaidan. 2009. Z-MERT: A fully configurable
open source tool for minimum error rate training of
machine translation systems. The Prague Bulletin of
Mathematical Linguistics, 91:79?88.
360
Proceedings of The First Workshop on Computational Approaches to Code Switching, pages 102?106,
October 25, 2014, Doha, Qatar.
c?2014 Association for Computational Linguistics
The IUCL+ System: Word-Level Language Identification via Extended
Markov Models
Levi King, Eric Baucom, Timur Gilmanov, Sandra K
?
ubler, Daniel Whyatt
Indiana University
{leviking,eabaucom,timugilm,skuebler,dwhyatt}@indiana.edu
Wolfgang Maier
Universita?t Du?sseldorf
maierw@hhu.de
Paul Rodrigues
University of Maryland
prr@umd.edu
Abstract
We describe the IUCL+ system for the shared
task of the First Workshop on Computational
Approaches to Code Switching (Solorio et al.,
2014), in which participants were challenged
to label each word in Twitter texts as a named
entity or one of two candidate languages. Our
system combines character n-gram probabili-
ties, lexical probabilities, word label transition
probabilities and existing named entity recog-
nition tools within a Markovmodel framework
that weights these components and assigns a
label. Our approach is language-independent,
and we submitted results for all data sets
(five test sets and three ?surprise? sets, cov-
ering four language pairs), earning the high-
est accuracy score on the tweet level on two
language pairs (Mandarin-English, Arabic-
dialects 1 & 2) and one of the surprise sets
(Arabic-dialects).
1 Introduction
This shared task challenged participants to perform
word level analysis on short, potentially bilingual Twit-
ter and blog texts covering four language pairs: Nepali-
English, Spanish-English,Mandarin-English andMod-
ern Standard Arabic-Arabic dialects. Training sets
ranging from 1,000 to roughly 11,000 tweets were pro-
vided for the language pairs, where the content of the
tweets was tokenized and labeled with one of six la-
bels. The goal of the task is to accurately replicate
this annotation automatically on pre-tokenized texts.
With an inventory of six labels, however, the task is
more than a simple binary classification task. In gen-
eral, the most common labels observed in the train-
ing data are lang1 and lang2, with other (mainly
covering punctuation and emoticons) also common.
Named entities (ne) are also frequent, and accounting
for them adds a significant complication to the task.
Less common are mixed (to account for words that
may e.g., apply L1 morphology to an L2 word), and
ambiguous (to cover a word that could exist in either
language, e.g., no in the Spanish-English data).
Traditionally, language identification is performed
on the document level, i.e., on longer segments of
text than what is available in tweets. These methods
are based on variants of character n-grams. Seminal
work in this area is by Beesley (1988) and Grefenstette
(1995). Lui and Baldwin (2014) showed that character
n-grams also perform on Twitter messages. One of a
few recent approaches working on individual words is
by King et al. (2014), who worked on historical data;
see also work by Nguyen and Dogruz (2013) and King
and Abney (2013).
Our system is an adaptation of a Markov model,
which integrates lexical, character n-gram, and la-
bel transition probabilities (all trained on the provided
data) in addition to the output of pre-existing NER
tools. All the information sources are weighted in the
Markov model.
One advantage of our approach is that it is language-
independent. We use the exact same architecture for
all language pairs, and the only difference for the indi-
vidual language pairs lies in a manual, non-exhaustive
search for the best weights. Our results show that the
approachworks well for the one language pair with dif-
ferent writing systems (Mandarin-English) as well as
for the most complex language pair, the Arabic set. In
the latter data set, the major difficulty consists in the
extreme skewing with an overwhelming dominance of
words in Modern Standard Arabic.
2 Method
Our system uses an extension of a Markov model to
perform the task of word level language identification.
The system consists of three main components, which
produce named entity probabilities, emission probabil-
ities and label transition probabilities. The outputs of
these three components are weighted and combined in-
side the extended Markov model (eMM), where the
best tag sequence for a given tweet (or sentence) is de-
termined via the Viterbi algorithm.
In the following sections, we will describe these
components in more detail.
2.1 Named Entity Recognition
We regard named entity recognition (NER) as a stand-
alone task, independent of language identification. For
this reason, NER is performed first in our system.
In order to classify named entities in the tweets, we
employ two external tools, Stanford-NER and Twit-
terNLP. Both systems are used in a black box approach,
102
without any attempt at optimization. I.e., we use the
default parameters where applicable.
Stanford NER (Finkel et al., 2005) is a state-of-the-
art named entity recognizer based on conditional ran-
dom fields (CRF), which can easily be trained on cus-
tom data.
1
For all of the four language pairs, we train a
NER model on a modified version of the training data
in which we have kept the label ?ne? as our target la-
bel, but replaced all others with the label ?O?. Thus, we
create a binary classification problem of distinguishing
named entities from all other words. This method is
applicable for all data sets.
For the Arabic data, we additionally employ a
gazetteer, namely ANERgazet (Benajiba and Rosso,
2008).
2
However, we do not use the three classes (per-
son, location, organization) available in this resource.
The second NER tool used in our system is the Twit-
terNLP package.
3
This system was designed specifi-
cally for Twitter data. It deals with the particular dif-
ficulties that Twitter-specific language (due to spelling,
etc.) poses to named entity recognition. The system has
been shown to be very successful: Ritter et al. (2011,
table 6) achieve an improvement of 52% on segmen-
tation F-score in comparison with Stanford NER on
hand-annotated Twitter data, which is mainly due to a
considerably increased recall.
The drawback of using TwitterNLP for our task is
that it was developed for English, and adapting it to
other languages would involve a major redesign and
adaptation of the system. For this reason, we decided
to use it exclusively on the language pairs that include
English. An inspection of the training data showed that
for all language pairs involving English, a majority of
the NEs are written in English and should thus be rec-
ognizable by the system.
TwitterNLP is an IOB tagger. Since we do not dis-
tinguish between the beginning and the rest of a named
entity, we change all corresponding labels to ?ne? in
the output of the NER system.
In testing mode, the NER tools both label each word
in a tweet as either ?O? or ?ne?. We combine the output
such that ?ne? overrides ?O? in case of any disagree-
ments, and pass this information to the eMM. This out-
put is weighted with optimized weights unique to each
language pair that were obtained through 10-fold cross
validation, as discussed below. Thus, the decisions of
the NER systems is not final, but they rather provide
evidence that can be overruled by other system compo-
nents.
2.2 Label Transition Models
The label transition probability component models lan-
guage switches on the sequence of words. It is also
1
See http://nlp.stanford.edu/software/
CRF-NER.shtml.
2
As available from http://users.dsic.upv.es/
grupos/nle/.
3
See https://github.com/aritter/
twitter_nlp.
trained on the provided training data. In effect, this
component consists of unigram, bigram, and trigram
probability models of the sequences of labels found
in the training data. Our MM is second order, thus
the transition probabilities are linear interpolations of
the uni-, bi-, and trigram label transition probabili-
ties that were observed in the training data. We add
two beginning-of-sentence buffer labels and one end-
of-sentence buffer label to assist in deriving the start-
ing and ending probabilities of each label during the
training.
2.3 Emission Probabilities
The emission probability component is comprised of
two subcomponents: a lexical probability component
and a character n-gram probability component. Both
are trained on the provided training data.
Lexical probabilities: The lexical probability com-
ponent consists of a dictionary for each label contain-
ing the words found under that label and their rel-
ative frequencies. Each word type and its count of
tokens are added to the total for each respective la-
bel. After training, the probability of a given label
emitting a word (i.e., P (word|label)) is derived from
these counts. To handle out-of-vocabulary words, we
use Chen-Goodman ?one-count? smoothing, which ap-
proximates the probabilities of unknownwords as com-
pared to the occurrence of singletons (Chen and Good-
man, 1996).
Character n-gram probabilities: The character-
based n-grammodel serves mostly as a back-off in case
a word is out-of-vocabulary, in which case the lexi-
cal probability may not be reliable. However, it also
provides important information in the case of mixed
words, which may use morphology from one language
added to a stem from the other one. In this setting, un-
igrams are not informative. For this reason, we select
longer n-grams, with n ranging between 2 and 5.
Character n-gram probabilities are calculated as fol-
lows: For each training set, the words in that training
set are sorted into lists according to their labels. In
training models for each value of n, n ? 1 buffer char-
acters are added to the beginning and end of each word.
For example, in creating a trigram character model
for the lang1 (English) words in the Nepali-English
training set, we encounter the word star. We first gen-
erate the form $$star##, then derive the trigrams. The
trigrams from all training words are counted and sorted
into types, and the counts are converted to relative fre-
quencies.Thus, using four values of n for a data set
containing six labels, we obtain 24 character n-gram
models for that language pair. Note that because this
component operates on individual words, character n-
grams never cross a word boundary.
In testing mode, for each word and for each value of
n, the component generates a probability that the word
occurred under each of the six labels. These values
103
are passed to the eMM, which uses manually optimized
weights for each value of n to combine the four n-gram
scores for each label into a single n-gram score for each
label. In cases where an n-gram from the test word
was not present in the training data, we use a primitive
variant of LaPlace smoothing, which returns a fixed,
extremely low non-zero probability for that n-gram.
2.4 The Extended Markov Model
Our approach is basically a trigram Markov model
(MM), in which the observations are the words in
the tweet (or blog sentence) and the underlying states
correspond to the sequence of codeswitching labels
(lang1, lang2, ne, mixed, ambiguous,
other). The MM, as usual, also uses starting
and ending probabilities (in our case, derived from
standard training of the label transition model, due
to our beginning- and end-of-sentence buffer labels),
label/state transition probabilities, and probabilities
that the state labels will emit particular observations.
The only difference is that we modify the standard
HMM emission probabilities. We call this resulting
Markov model extended (eMM).
First, for every possible state/label in the sequence,
we linearly interpolate ?lexical (emission) probabil-
ities? P
lex
(the standard emission probabilities for
HMMs) with character n-gram probabilities P
char
.
That is, we choose 0 ? ?
lex
? 1 and 0 ? ?
char
? 1
such that ?
lex
+ ?
char
= 1. We use them to derive
a new emission probability P
combined
= ?
lex
? P
lex
+
?
char
?P
char
. This probability represents the likelihood
that the given label in the hidden layer will emit the lex-
ical observation, along with its corresponding character
n-gram sequence.
Second, only for ne labels in the hidden layer, we
modify the probabilities that they will emit the ob-
served word if that word has been judged by our NER
module to be a named entity. Since the NER compo-
nent exhibits high precision but comparatively low re-
call, we boost the P
combined
(label = ne|word) if the
observedword is judged to be a named entity, but we do
not penalize the regular P
combined
if not. This boosting
is accomplished via linear interpolation and another set
of parameters, 0 ? ?
ne
? 1 and 0 ? ?
combined
? 1
such that ?
ne
+ ?
combined
= 1. Given a positive de-
cision from the NER module, the new probability for
the ne label emitting the observed word is derived as
P
ne+combined
= ?
ne
? 0.80 + ?
combined
? P
combined
,
i.e., we simply interpolate the original probability with
a high probability. All lambda values, as well as the
weights for the character n-gram probabilities, were set
via 10-fold cross-validation, discussed below.
2.5 Cross Validation & Optimization
In total, the system uses 11 weights, each of which is
optimized for each language pair. In labeling named
entities, the output of the NER component is given one
weight and the named entity probabilities of the other
sources (emission and label transition components) is
given another weight, with these weights summing to
one. For the label transition component, the uni-, bi-
and trigram scores receive weights that sum to one.
Likewise, the emission probability component is com-
prised of the lexical probability and the character n-
gram probability, with weights that sum to one. The
character n-gram component is itself comprised of the
bi-, tri-, four- and five-gram scores, again with weights
that sum to one.
For each language pair, these weights were opti-
mized using a 10-fold cross validation script that splits
the original training data into a training file and a test
file, runs the split files through the system and averages
the output. As time did not allow an exhaustive search
for optimal weights in this multi-dimensional space, we
narrowed the space by first manually optimizing each
subset of weights independently, then exploring com-
binations of weights in the resulting neighborhood.
3 Results
3.1 Main Results
The results presented in this section are the official re-
sults provided by the organizers. The evaluation is split
into two parts: a tweet level evaluation and a token level
evaluation. On the tweet level, the evaluation concen-
trates on the capability of systems to distinguish mono-
lingual from multilingual tweets. The token level eval-
uation is concerned with the classification of individ-
ual words into the different classes: lang1, lang2,
ambiguous, mixed, ne, and other.
Our results for the tweet level evaluation, in com-
parison to the best or next-best performing system are
shown in table 1. They show that our system is ca-
pable of discriminating monolingual from multilingual
tweets with very high precision. This resulted in the
best results in the evaluation with regard to accuracy
for Mandarin-English and for both Arabic-dialects set-
tings. We note that for the latter setting, reaching good
results is exceedingly difficult without any Arabic re-
sources. This task is traditionally approached by us-
ing a morphological analyzer, but we decided to use
a knowledge poor approach. This resulted in a rather
high accuracy but in low precision and recall, espe-
cially for the first Arabic test set, which was extremely
skewed, with only 32 out of 2332 tweets displaying
codeswitching.
Our results for the token level evaluation, in com-
parison to the best performing system per language,
are shown in table 2. They show that our system sur-
passed the baseline for both language pairs for which
the organizers provided baselines. In terms of accu-
racy, our system is very close to the best performing
system for the pairs Spanish-English andMandarin En-
glish. For the other language pairs, we partially suffer
from a weak NER component. This is especially obvi-
ous for the Arabic dialect sets. However, this is also a
problem that can be easily fixed by using a more com-
104
lang. pair system Acc. Recall Precision F-score
Nep.-Eng. IUCL+ 91.2 95.6 94.9 95.2
dcu-uvt 95.8 99.4 96.1 97.7
Span.-Eng. IUCL+ 83.8 51.4 87.7 64.8
TAU 86.8 72.0 80.3 75.9
Man.-Eng. IUCL+ 82.4 94.3 85.0 89.4
MSR-India 81.8 95.5 83.7 89.2
Arab. dia. IUCL+ 97.4 12.5 11.1 11.8
MSR-India 94.7 34.4 9.7 15.2
Arab. dia. 2 IUCL+ 76.6 24.9 27.1 26.0
MSR-India 71.4 21.2 18.3 19.6
Table 1: Tweet level results in comparison to the system with (next-)highest accuracy.
lang1 lang2 mixed ne
lang. pair system Acc. R P F R P F R P F R P F
Nep.-Eng. IUCL+ 75.2 85.1 89.1 87.1 68.9 97.6 80.8 1.7 100 3.3 55.1 48.7 51.7
dcu-uvt 96.3 97.9 95.2 96.5 98.8 96.1 97.4 3.3 50.0 6.3 45.6 80.4 58.2
base 70.0 57.1 76.5 65.4 92.3 62.8 74.7 0.0 100 0.0 0.0 100 0.0
Span.-Eng. IUCL+ 84.4 88.9 82.3 85.5 85.1 89.9 87.4 0.0 100 0.0 30.4 48.5 37.4
TAU 85.8 90.0 83.0 86.4 86.9 91.4 89.1 0.0 100 0.0 31.3 54.1 39.6
base 70.3 85.1 67.6 75.4 78.1 72.8 75.4 0.0 100 0.0 0.0 100 0.0
Man.-Eng. IUCL+ 89.5 98.3 97.8 98.1 83.9 66.6 74.2 0.0 100 0.0 70.1 50.3 58.6
MSR-India 90.4 98.4 97.6 98.0 89.1 66.6 76.2 0.0 100 0.0 67.7 65.2 66.4
Arab. dia. IUCL+ 78.8 96.1 81.6 88.2 34.8 8.9 14.2 ? ? ? 3.3 23.4 5.8
CMU 91.0 92.2 97.0 94.6 57.4 4.9 9.0 ? ? ? 77.8 70.6 74.0
Arab. dia. 2 IUCL+ 51.9 90.7 43.8 59.0 47.7 78.3 59.3 0.0 0.0 0.0 8.5 28.6 13.1
CMU 79.8 85.4 69.0 76.3 76.1 87.3 81.3 0.0 100 0.0 68.7 78.8 73.4
Table 2: Token level results in comparison to the system with highest accuracy (results for ambiguous and
other are not reported).
lang1 lang2 ne
lang. pair system Acc. R P F R P F R P F
Nep.-Eng. IUCL+ 80.5 86.1 78.8 82.3 97.6 80.9 88.5 29.9 80.9 43.7
JustAnEagerStudent 86.5 91.3 80.2 85.4 93.6 91.1 92.3 39.4 83.3 53.5
Span.-Eng. IUCL+ 91.8 87.4 81.9 84.5 84.5 87.4 85.9 28.5 47.4 35.6
dcu-uvt 94.4 87.9 80.5 84.0 84.1 86.7 85.4 22.4 55.2 31.9
Arab. dia. IUCL+ 48.9 91.7 33.3 48.8 48.4 81.9 60.9 3.3 17.6 5.5
CMU 77.5 87.6 55.5 68.0 75.6 89.8 82.1 52.3 73.8 61.2
Table 3: Token level results for the out-of-domain data.
petitive, language dependent system. Another problem
constitutes the mixed cases, which cannot be reliably
annotated.
3.2 Out-Of-Domain Results
The shared task organizers provided ?surprise? data,
from domains different from the training data. Our re-
sults on those data sets are shown in table 3. For space
reasons, we concentrate on the token level results only.
The results show that our system is very robust with
regard to out-of-domain settings. For Nepali-English
and Spanish-English, we reach higher results than on
the original test sets, and for the Arabic dialects, the re-
sults are only slightly lower. These results need further
analysis for us to understand how our system performs
in such situations.
4 Conclusions
We have presented the IUCL+ system for word level
language identification. Our system is based on a
Markov model, which integrates different types of in-
formation, including the named entity analyses, lexical
and character n-gram probabilities as well as transition
probabilities. One strength of the system is that it is
completely language independent. The results of the
shared task have shown that the system generally pro-
vides reliable results, and it is fairly robust in an out-
of-domain setting.
105
References
Kenneth R. Beesley. 1988. Language identifier: A
computer program for automatic natural-language
identification of on-line text. In Proceedings of the
29th Annual Conference of the American Translators
Association, volume 47, page 54.
Yassine Benajiba and Paolo Rosso. 2008. Arabic
named entity recognition using conditional random
fields. In Proceedings of Workshop on HLT & NLP
within the Arabic World, LREC 2008, Marakech,
Morroco.
Stanley F. Chen and Joshua Goodman. 1996. An em-
pirical study of smoothing techniques for language
modeling. In Proceedings of the 34th annual meet-
ing on Association for Computational Linguistics,
pages 310?318.
Jenny Rose Finkel, Trond Grenager, and Christopher
Manning. 2005. Incorporating non-local informa-
tion into information extraction systems by gibbs
sampling. In Proceedings of the 43rd Annual Meet-
ing of the Association for Computational Linguistics
(ACL?05), pages 363?370.
Gregory Grefenstette. 1995. Comparing two language
identification schemes. In Proceedings of the Third
International Conference on Statistical Analysis of
Textual Data (JADT), volume 2.
Ben King and Steven Abney. 2013. Labeling the lan-
guages of words in mixed-languagedocuments using
weakly supervised methods. In Proceedings of the
2013 Conference of the North American Chapter of
the Association for Computational Linguistics: Hu-
man Language Technologies, pages 1110?1119. As-
sociation for Computational Linguistics.
Levi King, Sandra Ku?bler, and Wallace Hooper. 2014.
Word-level language identification in The Chymistry
of Isaac Newton. Literary and Linguistic Comput-
ing.
Marco Lui and Timothy Baldwin. 2014. Accurate lan-
guage identification of Twitter messages. In Pro-
ceedings of the 5th Workshop on Language Analysis
for Social Media (LASM), pages 17?25, Gothenburg,
Sweden.
Dong Nguyen and A. Seza Dogruz. 2013. Word level
language identification in online multilingual com-
munication. In Proceedings of the 2013 Conference
on Empirical Methods in Natural LanguageProcess-
ing, pages 857?862.
Alan Ritter, Sam Clark, Mausam, and Oren Etzioni.
2011. Named entity recognition in tweets: An ex-
perimental study. In Proceedings of the 2011 Con-
ference on Empirical Methods in Natural Language
Processing, pages 1524?1534, Edinburgh, Scotland,
UK., July. Association for Computational Linguis-
tics.
Thamar Solorio, Elizabeth Blair, Suraj Maharjan, Steve
Bethard, Mona Diab, Mahmoud Gonheim, Abdelati
Hawwari, Julia Hirshberg, Alison Chang, and Pas-
cale Fung. 2014. Overview for the first shared
task on language identification in code-switched
data. In Proceedings of the First Workshop on Com-
putational Approaches to Code-Switching. EMNLP
2014, Conference on Empirical Methods in Natural
Language Processing, Doha, Qatar.
106
Proceedings of the Second Workshop on Natural Language Processing for Social Media (SocialNLP), pages 2?11,
Dublin, Ireland, August 24 2014.
Feature Selection for Highly Skewed Sentiment Analysis Tasks
Can Liu
Indiana University
Bloomington, IN, USA
liucan@indiana.edu
Sandra K?ubler
Indiana University
Bloomington, IN, USA
skuebler@indiana.edu
Ning Yu
University of Kentucky
Lexington, KY, USA
ning.yu@uky.edu
Abstract
Sentiment analysis generally uses large feature sets based on a bag-of-words approach, which
results in a situation where individual features are not very informative. In addition, many data
sets tend to be heavily skewed. We approach this combination of challenges by investigating
feature selection in order to reduce the large number of features to those that are discriminative.
We examine the performance of five feature selection methods on two sentiment analysis data
sets from different domains, each with different ratios of class imbalance.
Our finding shows that feature selection is capable of improving the classification accuracy only
in balanced or slightly skewed situations. However, it is difficult to mitigate high skewing ratios.
We also conclude that there does not exist a single method that performs best across data sets and
skewing ratios. However we found that TF ? IDF
2
can help in identifying the minority class
even in highly imbalanced cases.
1 Introduction
In recent years, sentiment analysis has become an important area of research (Pang and Lee, 2008;
Bollen et al., 2011; Liu, 2012). Sentiment analysis is concerned with extracting opinions or emotions
from text, especially user generated web content. Specific tasks include monitoring mood and emotion;
differentiating opinions from facts; detecting positive or negative opinion polarity; determining opinion
strength; and identifying other opinion properties. At this point, two major approaches exists: lexicon
and machine learning based. The lexicon-based approach uses high quality, often manually generated
features. The machine learning-based approach uses automatically generated feature sets, which are from
various sources of evidence (e.g., part-of-speech, n-grams, emoticons) in order to capture the nuances of
sentiment. This means that a large set of features is extracted, out of which only a small subset may be
good indicators for the sentiment.
One major problem associated with sentiment analysis of web content is that for many topics, these
data sets tend to be highly imbalanced. There is a general trend that users are willing to submit positive
reviews, but they are much more hesitant to submit reviews in the medium to low ranges. For example,
for the YouTube data set that we will use, we collected comments for YouTube videos from the comedy
category, along with their ratings. In this data set, more than 3/4 of all ratings consist of the highest rating
of 5. For other types of user generated content, they opposite may be true.
Heavy skewing in data sets is challenging for standard classification algorithms. Therefore, the data
sets generally used for research on sentiment analysis are balanced. Researchers either generate balanced
data sets during data collection, by sampling a certain number of positive and negative reviews, or by se-
lecting a balanced subset for certain experiments. Examples for a balanced data set are the movie review
data set (Pang and Lee, 2004) or the IMDB review data set (Maas et al., 2011). Using a balanced data set
allows researchers to focus on finding robust methods and feature sets for the problem. Particularly, the
movie review data set has been used as a benchmark data set that allows for comparisons of various sen-
timent analysis models. For example, Agarwal and Mittal (2012), Agarwal and Mittal (2013), Kummer
This work is licenced under a Creative Commons Attribution 4.0 International License. Page numbers and proceedings footer
are added by the organizers. License details: http://creativecommons.org/licenses/by/4.0/
2
and Savoy (2012), O?Keefe and Koprinska (2009), and Paltoglou and Thelwall (2010) all proposed com-
petitive feature selection methods evaluated on the movie review data set. However, the generalizability
of such feature selection methods to imbalanced data sets, which better represent real world situations,
has not been investigated in much detail. Forman (2003) provides an extensive study of feature selection
methods for highly imbalanced data sets, but he uses document classification as task.
This current paper investigates the robustness of three feature selection methods that Forman (2003)
has shown to be successful, as well as two variants of TF ? IDF . The three methods are Odds-Ratio
(OR), Information Gain (IG), and Binormal Separation (BNS). BNS has been found to perform signif-
icantly better than other methods in more highly skewed tasks (Forman, 2003). The two variants of
TF ? IDF differ in the data set used for calculating document frequency. We investigate the behavior
of these methods on a subtask of sentiment analysis, namely the prediction of user ratings. For this, we
will use data sets from two different domains in order to gain insight into whether or not these feature
selection methods are robust across domains and across skewing ratios: One set consists of user reviews
from Epicurious
1
, an online community where recipes can be exchanged and reviewed, the other set
consists of user reviews of YouTube comedy videos.
The remainder of this paper is organized as follows: In section 2, we explain the rationale for applying
feature selection and introduce the feature selection methods that are examined in this paper. Section
3 introduces the experimental settings, including a description of the two data sets, data preprocessing,
feature representation, and definition of the binary classification tasks. In section 4, we present and
discuss the results for the feature selection methods, and in section 5, we conclude.
2 Feature Selection and Class Skewing
In a larger picture, feature selection is a method (applicable both in regression and classification prob-
lems) to identify a subset of features to achieve various goals: 1) to reduce computational cost, 2) to
avoid overfitting, 3) to avoid model failure, and 4) to handle skewed data sets for classification tasks.
We concentrate on the last motivation, even though an improvement of efficiency and the reduction of
overfitting are welcome side effects. The feature selection methods studied in this paper have been used
in text classification as well, which is a more general but similar task using n-gram features. However,
since all measures are intended for binary classification problems, we reformulate the rating prediction
into a binary classification problem (see section 3.5).
Feature selection methods can be divided into wrapper and filter methods. Wrapper methods use
the classification outcome on a held-out data set to score feature subsets. Standard wrapper methods
include forward selection, backward selection, and genetic algorithms. Filter methods, in contrast, use
an independent measure rather than the error rate on the held-out data. This means that they can be
applied to larger feature sets, which may be unfeasible with wrapper methods. Since sentiment analysis
often deals with high dimensional feature representation, we will concentrate on filter methods for our
feature selection experiments.
Previous research (e.g. (Brank et al., 2002b; Forman, 2003)) has shown that Information Gain and
Odds Ratio have been used successfully across different tasks and that Binormal Separation has good
recall for the minority class under skewed class distributions. So we will investigate them in this paper.
Other filter methods are not investigated in this paper due to two main concerns: We exclude Chi-
squared and Z-score, statistical tests because they require a certain sample size. Our concern is that
their estimation for rare words may not be accurate. We also exclude Categorical Proportion Difference
and Probability Proportion Difference since they do not normalize over the sample of size of positive
and negative classes. Thus, our concern is that they may not provide a fair estimate for features from a
skewed data sets.
2.1 Notation
Following Zheng et al. (2004), feature selection methods can be divided into two groups: one-sided and
two-sided measures. One-sided measures assign a high score to positively-correlated features and a low
1
www.epicurious.com
3
score to negative features while two-sided measures prefer highly distinguishing features, independent
of whether they are positively or negatively correlated. Zheng et al. (2004) note that the ratio of positive
and negative features affects precision and recall of the classification, especially for the minority class.
For one-sided methods, we have control over this ratio by selecting a specified number of features one
each side; for two-sided methods, however, we do not have this control. In this paper, we will keep a 1:1
ratio for one-sided methods. For example, if we select 1 000 features, we select the 500 highest ranked
features for the positive class, and the 500 highest ranked features for the negative class. When using
two-sided methods, the 1 000 highest ranked features are selected.
For the discussion of the feature selection methods, we use the following notations:
? S: target or positive class.
? S: negative class.
? D
S
: The number of documents in class S.
? D
S
: The number of documents in class S.
? D
Sf
: The number of documents in class S where feature f occurs.
? D
Sf
: The number of documents in class S where feature f occurs.
? T
Sf
: The number of times feature f occurs in class S.
2.2 Feature Selection Methods
In addition to Information Gain, Odds Ratio and Bi-Normal Separation, TF ? IDF is included for
comparison purposes. We define these measures for binary classification as shown below.
Information Gain (IG): IG is a two-sided measure that estimates how much is known about an unob-
served random variable given an observed variable. It is defined as the entropy of one random variable
minus the conditional entropy of the observed variable. Thus, IG is the reduced uncertainty of class S
given a feature f :
IG = H(S)?H(S|f) =
?
f?{0,1}
?
S?{0,1}
P (f, S)log
P (f, S)
P (f)P (S)
Brank et al. (2002b) analyzed feature vector sparsity and concluded that IG prefers common features
over extremely rare ones. IG can be regarded as the weighted average of Mutual Information, and rare
features are penalized in the weighting. Thus they are unlikely to be chosen (Li et al., 2009). Forman
(2003) observed that IG performs better when only few features (100-500) are used. Both authors agreed
that IG has a high precision with respect to the minority class.
Odds Ratio (OR): OR (Mosteller, 1968) is a one-sided measure that is defined as the ratio of the odds
of feature f occurring in class S to the odds of it occurring in class S. A value larger than 1 indicates that
a feature is positively correlated with class S, a value smaller than 1 indicates it is negatively correlated:
OR = log
P (f, S)(1? P (f, S))
P (f, S)(1? P (f, S))
Brank et al. (2002b) showed that OR requires a high number of features to achieve a given feature
vector sparsity because it prefers rare terms. Features that occur in very few documents of class S and
do not occur in S have a small denominator, and thus a rather large OR value.
4
Bi-Normal Separation (BNS): BNS (Forman, 2003) is a two-sided measure that regards the proba-
bility of feature f occurring in class S as the area under the normal distribution bell curve. The whole
area under the bell curve corresponds to 1, and the area for a particular feature has a corresponding
threshold along the x-axis (ranging from negative infinite to positive infinite). For a feature f , one can
find the threshold that corresponds to the probability of occurring in the positive class, and the threshold
corresponding to the probability of occurring in S. BNS measures the separation in these two thresholds:
BNS = |F
?1
(
D
Sf
D
S
)? F
?1
(
D
Sf
D
S
)|
where F
?1
is the inverse function of the standard normal cumulative probability distribution. As we can
see, the F
?1
function exaggerates an input more dramatically when the input is close to 0 or 1 which
means that BNS perfers rare words.
Term Frequency * Inverse Document Frequency (TF*IDF): TF ? IDF was originally proposed
for information retrieval tasks, where it measures how representative a term is for the document in which
it occurs. When TF ? IDF is adopted for binary classification, we calculate the TF ? IDF of a feature
w.r.t. the positive class (normalized) and the TF ? IDF w.r.t. the negative class (normalized). We obtain
the absolute value of the difference of these two measures. If a feature is equally important in both
classes and thus would not contribute to classification, it receives a small value. The larger the value,
the more discriminative the feature. We apply two variants of TF ? IDF , depending on how IDF is
calculated:
TF ? IDF
1
= (0.5 +
0.5? T
Sf
max
i
(T
Sf
i
)
)? log(
D
S
+ D
S
D
Sf
)
TF ? IDF
2
= (0.5 +
0.5? T
Sf
max
i
(T
Sf
i
)
)? log(
D
S
D
Sf
)
In the first variant, TF ? IDF
1
, document frequency is based on the whole set of examples while in
the second variant, TF ? IDF
2
, document frequency is based only on the class under consideration, S.
3 Experimental Setup
3.1 Data Sets
Epicurious Data Set: We developed a web crawler to scrape user reviews for 10 146 recipes, published
on the Epicurious website before and on April 02, 2013. On the website, each recipe is assigned a rating
of 1 to 4 forks, including the intermediate values of 1.5, 2.5, and 3.5. This is an accumulated rating over
all user reviews. (Reviews with ratings of 0 were excluded, they usually indicate that recipes have not
received any ratings.) We rounded down all the half ratings, e.g., 1.5 forks counts as 1 fork, based on
the observation that users are generous when rating recipes. Our experiments classify each recipe by
aggregating over all its reviews. While a little more than half of the recipes received 1 to 10 reviews,
there are recipes with more than 100 reviews. To avoid an advantage for highly reviewed recipes, we
randomly selected 10 reviews if a recipe has more than 10 reviews. Recipes with less than 3 reviews
were eliminated since they do not provide enough information. After these clean-up steps, the data set
has the distribution of ratings shown in table 1.
YouTube Data Set: Using the Google YouTube Data API, we collected average user ratings and user
comments for a set of YouTube videos in the category Comedy. Each video is rated from 1 to 5. The
distribution of ratings among all YouTube videos is very skewed, as illustrated in figure 1. Most videos
are rated highly; very few are rated poorly. The 1% quantile is 1.0; the 6.5% quantile is 3.0; the 40%
quantile is 4.75; the 50% quantile is 4.85; and the 77% quantile is 5.0. We selected a set of 3 000 videos.
Videos with less than 5 comments or with non-English comments are discarded.
5
rating no.
1 fork 44 recipes
2 forks 304 recipes
3 forks 1416 recipes
4 forks 1368 recipes
Table 1: The distribution of ratings in the Epicurious data set.
1 2 3 4 5
0.0
0.5
1.0
1.5
2.0
2.5
Video Rating
Den
sity
Figure 1: Skewing in the YouTube data set.
3.2 Data Preprocessing
Before feature extraction, basic preprocessing is conducted for both data sets individually. For the Epi-
curious data set, we perform stemming using the Porter Stemmer (Porter, 1980) to normalize words, and
rare words (? 4 occurrences) are removed. On the YouTube data set, we perform spelling correction
and normalization because the writing style is rather informal. Our normalizer collapses repetitions into
their original forms plus a suffix ?RPT?, thus retaining this potentially helpful clue for reviewer?s strong
emotion without increasing the features due to creative spelling. For example, ?loooooove? is changed
to ?loveRPT? and?lolololol? to ?lolRPT?. The normalizer also replaces all emoticons by either?EMOP?
for positive emoticons or ?EMON? for negative emoticons. Besides a standard English dictionary, we
also use the Urban Dictionary
2
since it has a better coverage of online abbreviations.
We do not filter stop words for two reasons: 1) Stop words are domain dependent, and some En-
glish stop words may be informative for sentiment analysis, and 2) uninformative words that are equally
common in both classes will be excluded by feature selection if the method is successful.
3.3 Feature representation
Since our focus is on settings with high numbers of features, we use a bag-of-words approach, in which
every word represents one feature, and its term frequency serves as its value. Different feature weighting
methods, including binary weighting, term frequency, and TF?IDF have been adopted in past sentiment
analysis studies (e.g., (Pang et al., 2002; Paltoglou and Thelwall, 2010)). (Pang et al., 2002) found that
simply using binary feature weighting performed better than using more complicated weightings in a
task of classifying positive and negative movie reviews. However, movie reviews are relatively short, so
there may not be a large difference between binary features and others. Topic classification usually uses
term frequency as feature weighting. TF ? IDF and variants were shown to perform better than binary
weighting and term frequency for sentiment analysis (Paltoglou and Thelwall, 2010).
Since our user rating prediction tasks aggregate all user comments into large documents and predict
ratings per recipe/YouTube video, term frequency tends to capture richer information than binary fea-
2
http://www.urbandictionary.com/
6
Epicurious YouTube
ratio no. NEG no. POS ratio no. NEG no. POS
1:8 348 2 784 1:10 56 559
1:1.57 348 547 1:1.57 356 559
1:1 348 348 1:1 559 559
Table 2: Skewing ratios and sizes of positive and negative classes for both data sets.
tures. Thus, we use term frequency weighting for simplicity, not to deviate from the focus of feature
selection methods. Since there is a considerable variance in term frequency in the features, we normalize
the feature values to [0,1] to avoid large feature values from dominating the vector operations in classifier
optimization.
For the Epicurious data, the whole feature set consists of 10 677 unigram features. For YouTube, the
full feature set of features consists of 23 232 unigram features. We evaluate the performance of feature
selection methods starting at 500 features, at a step-size of 500. For the Epicurious data, we include up to
10 500 features. For the YouTube data, we stop at 15 000 features due to prohibitively long classification
times.
3.4 Classifier
The classifier we use in this paper is Support Vector Machines (SVMs) in the implementation of
SVM
light
(Joachims, 1999). Because algorithm optimization is not the focus of this study, we use the
default linear kernel and other default parameter values. Classification results are evaluated by accuracy
as well as precision and recall for individual classes.
3.5 Binary Classification
Since all feature selection methods we use in our experiments are defined under a binary classification
scenario, we need to redefine the rating prediction task. For both data sets, this means, we group the
recipes and videos into a positive and a negative class. A baseline classifier predicts every instance as the
majority class. For both data sets, the majority class is positive.
For the Epicurious data set, 1-fork and 2-fork recipes are grouped into the negative class (NEG), and
3-fork and 4-fork recipes are grouped into the positive class (POS), yielding a data set of 348 NEG and
2 784 POS recipes (skewing ratio: 1:8). The different skewing ratios we use are shown in table 2. 2/3 of
the data is used for training, and 1/3 for testing, with the split maintaining the class ratio. Note that for
the less skewed settings, all NEG instances were kept while POS instances were sampled randomly.
For the YouTube data set, we sample from all videos with rating 5 for the positive class and from
all videos with ratings between and including 1 and 3 for the negative class. This yields 559 POS and
559 NEG videos. The different skewing ratios we use are shown in table 2. 7/8 of the data is used for
training, and 1/8 for testing, with the split maintaining the class ratio.
4 Results
4.1 Results for the Epicurious Data Set
The results for the Epicurious data set with different skewing ratios are shown in figure 2. The accuracy
of the baseline is 50% for the 1:1 ratio, 61% for 1:1.57, and 88.9% for 1:8.
The results show that once we use a high number of features, all the feature selection methods perform
the same. This point where they conflate is reached at around 4 000 features for the ratios of 1:1 and
1:1.57. There, accuracy reaches around 71%. For the experiment with the highest skewing, this point is
reached much later, at around 8 000 features. For this setting, we also reach a higher accuracy of around
89%, which is to be expected since we have a stronger majority class. Note that once the conflation point
is reached, the accuracy also corresponds to the accuracy when using the full feature set. This accuracy
is always higher than that of the baseline.
7
2000 4000 6000 8000 10000
0.66
0.68
0.70
0.72
0.74
0.76
Number of Features Chosen
Accura
cy l
l
l
l l l
l l l l l l l l l l l l l l l
l
l l l
l
l
l
l
l l
Epicurious 1:1? Accuracy
l
ll
IGBNSORTFIDF1TFIDF2baseline = 0.5
2000 4000 6000 8000 100000
.60
0.65
0.70
0.75
0.80
Number of Features Chosen
Accura
cy
l
l
l
l
l
l
l l l l l l l l l l l l l l l
l
l
l
l
l
l l
l l l
l l l l
Epicurious 1:1.57 ? Accuracy
l
ll
IGBNSORTFIDF1TFIDF2baseline
2000 4000 6000 8000 100000
.880
0.885
0.890
0.895
0.900
Number of Features Chosen
Accura
cy
l l l l l l l l l l
l l l l l l l l l l
l
l l
l l
l
l l l
l
l
l l l
l l l
Epicurious 1:8 ? Accuracy
l
ll
IGBNSORTFIDF1TFIDF2baseline
Figure 2: The results for the Epicurious data set.
2000 4000 6000 8000 10000
0.0
0.2
0.4
0.6
0.8
1.0
Number of Features Chosen
Prec
ision
l l l l l l l l l l l l l l l l l l l l l
l
l
l
Epicurious 1:8 ? Precision NEG class
l
l
l
IGBNSORTFIDF1TFIDF2
2000 4000 6000 8000 10000
0.00
0.02
0.04
0.06
0.08
0.10
Number of Features Chosen
Reca
ll
l l l l l l l l l l l l l l l l l l l l l
l
l
l
Epicurious 1:8 ? Recall NEG class
l
l
l
IGBNSORTFIDF1TFIDF2
Figure 3: Precision and recall for the negative cases in the Epicurious set, given a 1:8 skewing.
The results also show that the most pronounced differences between feature selection methods occur in
the balanced data set. In the set with the highest skewing, the differences are minor, and only TF ?IDF
2
improves over the baseline when using 1 000 features.
Another surprising result is that TF ?IDF
2
, OR, and BNS have a tendency to fluctuate between higher
and lower results than the setting using all features. This means that it is difficult to find a good cut-off
point for these methods. TF ? IDF
1
and IG show clear performance gains for the balanced setting, but
they also show more fluctuation in settings with higher skewing.
From these results, we can conclude that for sentiment analysis tasks, feature selection is useful only
in a balanced or slightly skewed cases if we are interested in accuracy. However, a look at the precision
and recall given the highest skewing (see figure 3) shows that TF ? IDF
2
in combination with a small
number of features is the only method that finds at least a few cases of the minority class. Thus if a
good performance on the minority class examples is more important than overall accuracy, TF ? IDF
2
is a good choice. One explanation is that TF ? IDF
2
concentrates on one class and can thus ignore the
otherwise overwhelming positive class completely. TF ? IDF
1
and OR have the lowest precision, and
BNS fluctuates. Where recall is concerned, TF ? IDF
1
and IG reach the highest recall given a small
feature set.
4.2 Results for the YouTube Data Set
The results for the YouTube data set with different skewing ratios are shown in figure 4. The accuracy of
the baseline is 50% for the 1:1 ratio, 61.08% for 1:1.57, and 90.9% for 1:10.
The results show that even though the YouTube data set is considerably smaller than the Epicurious
one, is does profit from larger numbers of selected features: For the balanced and the low skewing, there
8
0 5000 10000 15000
0.50
0.55
0.60
0.65
0.70
0.75
Number of Features Chosen
Accura
cy
l
l l l
l
l l l
l l l l l
l l l l l l l l l l l l l l l l l
l l
l l l l
l
l
l
l
l l l
l l l l l l
l l
l l
l
l l l
l
l l
l
l l
l l l l l l l
l
Youtube Equal ? Accuracy
l
ll
IGBNSORTFIDF1TFIDF2baseline
0 5000 10000 150000
.60
0.65
0.70
0.75
Number of Features Chosen
Accura
cy
l
l l l l
l l
l l
l l l l
l
l l l l
l
l l l
l l l
l l l l l
l
l l
l l l l l l
l l l
l
l l
l l
l
l l
l l
l
l l l l
l l
l l
l
l l l l l l l l l l l l l l l
Youtube 1:1.57 ? Accuracy
l
ll
IGBNSORTFIDF1TFIDF2baseline
0 5000 10000 150000
.80
0.85
0.90
0.95
Number of Features Chosen
Accura
cy
l l l l l l l l l l l l l l l l l l l l l l l l l l l l l l
l
l
Youtube 1:10 ? Accuracy
l
ll
IGBNSORTFIDF1TFIDF2baseline
Figure 4: The results for the YouTube set.
0 5000 10000 15000
0.70
0.75
0.80
0.85
0.90
0.95
1.00
Number of Features Chosen
Prec
ision
l
l
l
l
l
l
l l
l l l l l l l l l l
l l l l
l
l l l l l l l
l l l l l l l l l
l l l l l l l l
l l
l l
l
l
l
l
l l l l l l l l l l
Youtube 1:1.57 ? Precision NEG class
l
l
l
IGBNSORTFIDF1TFIDF2
0 5000 10000 15000
0.0
0.1
0.2
0.3
0.4
0.5
Number of Features Chosen
Reca
ll
l l
l l l
l l
l l
l l l l
l
l l l l
l
l l l l l l l l l l l
l
l l
l l l l l l
l l l
l
l l
l l
l
l l
l l
l
l l l l l l l l l l l l l l l l l l l l l l l l
Youtube 1:1.57 ? Recall NEG class
l
l
l
IGBNSORTFIDF1TFIDF2
Figure 5: Precision and recall for the negative cases in the YouTube set, given a 1:1.57 skewing.
is no point at which the methods conflate. The results for the highly skewed case show that no feature
selection method is capable of finding cases of the minority class: all methods consistently identify only
one instance of the negative class. However, this may be a consequence of the small data set. In terms of
accuracy, we see that a combination of a small number of features with either IG or TF ? IDF
1
provides
the best results. For the YouTube data set, BNS also performs well but requires a larger set of features for
reaching its highest accuracy. We assume that this is the case because BNS has a tendency to prefer rare
words. Note that we did not test for number of features greater than 15 000 because of the computation
cost, but we can see that the performance curve for different feature selection methods tends to conflate
to the point that represents the full feature set.
If we look at the performance of different feature selection methods on identification of minority class
instances, we find that TF ? IDF
2
again manages to increase recall in the highly skewed case, but this
time at the expense of precision. For the 1:1.57 ratio, all methods reach a perfect precision when a high
number of features is selected, see figure 5. TF ? IDF
2
is the only method that reaches this precision
with small numbers of features, too. However, this is not completely consistent.
4.3 Discussion
If we compare the performance curves across data sets and skewing ratios and aim for high accuracy,
we see that there is no single feature selection method that is optimal in all situations. Thus, we have to
conclude that the choice of feature selection method is dependent on the task. In fact, the performance
of a feature selection method could depend on many factors, such as the difficulty of the classification
task, the preprocessing step, the feature generation decision, the data representation scheme (Brank et
al., 2002a), or the classification model (e.g., SVM, Maximum Entropy, Naive Bayes).
9
We have also shown on two different data sets, each with three skewing ratios, that it is difficult for
feature selection methods to mitigate the effect of highly skewed class distributions while we can still
improve performance by using a reduced feature set for slightly skewed cases. Thus, the higher the skew-
ing of the data set is, the more difficult it is to find a feature selection method that has a positive effect,
and parameters, such as feature set size, have to be optimized very carefully. Thus, feature selection is
much less effective in highly skewed user rating tasks than in document classification.
However, if the task requires recall of the minority class, our experiments have shown that TF ?IDF
2
is able to increase this measure with a small feature set, even for highly imbalanced cases.
5 Conclusion and Future Work
In this paper, we investigated whether feature selection methods reported to be successful for document
classification perform robustly in sentiment classification problems with a highly skewed class distribu-
tion. Our findings show that feature selection methods are most effective when the data sets are balanced
or moderately skewed, while for highly imbalanced cases, we only saw an improvement in recall for the
minority class.
In the future, we will extend feature selection methods ? originally defined for binary classification
scenarios ? to handle multi-class classification problems. A simple way of implementing this is be to
break multi-class classification into several 1-vs.-all or 1-vs.-1 tasks, perform feature selection on these
binary tasks and then aggregate them. Another direction that we want to take is integrating more complex
features, such as parsing or semantic features, into the classification task to investigate how they influence
feature selection. In addition, we will compare other approaches against feature selection for handling
highly skewed data sets, including classification by rank, ensemble learning methods, and memory-
based learning with a instance-specific weighting. Finally, a more challenging task is to select features
for sentiment analysis problems where no annotation (feature selection under unsupervised learning) or
few annotations (feature selection under semi-supervised learning) are available.
References
Basant Agarwal and Namita Mittal. 2012. Categorical probability proportion difference (CPPD): A feature selec-
tion method for sentiment classification. In Proceedings of the 2nd Workshop on Sentiment Analysis where AI
meets Psychology (SAAIP), pages 17?26.
Basant Agarwal and Namita Mittal. 2013. Sentiment classification using rough set based hybrid feature selection.
In Proceedings of the 4th Workshop on Computational Approaches to Subjectivity, Sentiment & Social Media
Analysis (WASSA), pages 115?119, Atlanta, GA.
Johan Bollen, Huina Mao, and Xiao-Jun Zeng. 2011. Twitter mood predicts the stock market. Journal of Compu-
tational Science, 2:1?8.
Janez Brank, Marko Grobelnik, Nata?sa Milic-Frayling, and Dunja Mladenic. 2002a. An extensive empirical study
of feature selection metrics for text classification. In Proceedings of the Third International Conference on Data
Mining Methods and Databases for Engineering, Finance and Other Fields, Bologna, Italy.
Janez Brank, Marko Grobelnik, Nata?sa Milic-Frayling, and Dunja Mladenic. 2002b. Feature selection using
Linear Support Vector Machines. Technical Report MSR-TR-2002-63, Microsoft Research.
George Forman. 2003. An extensive empirical study of feature selection metrics for text classification. Journal of
Machine Learning Research, 3:1289?1305.
Thorsten Joachims. 1999. Making large-scale SVM learning practical. In B. Sch?olkopf, C. Burges, and A. Smola,
editors, Advances in Kernel Methods - Support Vector Learning. MIT-Press.
Olena Kummer and Jaques Savoy. 2012. Feature selection in sentiment analysis. In Proceeding of the Conf?erence
en Recherche d?Infomations et Applications (CORIA), pages 273?284, Bordeaux, France.
Shoushan Li, Rui Xia, Chengqing Zong, and Chu-Ren Huang. 2009. A framework of feature selection methods
for text categorization. In Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the
4th International Joint Conference on Natural Language Processing of the AFNLP, pages 692?700, Suntec,
Singapore.
10
Bing Liu. 2012. Sentiment Analysis and Opinion Mining. Synthesis Lectures on Human Language Technologies.
Morgan & Claypool Publishers.
Andrew L. Maas, Raymond E. Daly, Peter T. Pham, Dan Huang, Andrew Y. Ng, and Christopher Potts. 2011.
Learning word vectors for sentiment analysis. In Proceedings of the 49th Annual Meeting of the Association for
Computational Linguistics: Human Language Technologies, pages 142?150, Portland, OR.
Frederick Mosteller. 1968. Association and estimation in contingency tables. Journal of the American Statistical
Association, 63(321):1?28.
Tim O?Keefe and Irena Koprinska. 2009. Feature selection and weighting methods in sentiment analysis. In Pro-
ceedings of the 14th Australasian Document Computing Symposium (ADCS), pages 67?74, Sydney, Australia.
Georgios Paltoglou and Mike Thelwall. 2010. A study of information retrieval weighting schemes for sentiment
analysis. In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages
1386?1395, Uppsala, Sweden.
Bo Pang and Lillian Lee. 2004. A sentimental education: Sentiment analysis using subjectivity summarization
based on minimum cuts. In Proceedings of the 42nd Annual Meeting on Association for Computational Lin-
guistics, Barcelona, Spain.
Bo Pang and Lillian Lee. 2008. Opinion mining and sentiment analysis. Foundations and Trends in Information
Retrieval, 2(1-2):1?135.
Bo Pang, Lillian Lee, and Shivakumar Vaithyanathan. 2002. Thumbs up? Sentiment classification using ma-
chine learning techniques. In Proceedings of the 2002 Conference on Empirical Methods in Natural Language
Processing (EMNLP), pages 79?86, Philadelphia, PA.
Martin Porter. 1980. An algorithm for suffix stripping. Program, 14(3):130?137.
Zhaohui Zheng, Xiayun Wu, and Rohini Srihari. 2004. Feature selection for text categorization on imbalanced
data. ACM SIGKDD Explorations Newsletter, 6(1):80?89.
11
Proceedings of the Second Workshop on Natural Language Processing for Social Media (SocialNLP), pages 12?21,
Dublin, Ireland, August 24 2014.
?My Curiosity was Satisfied, but not in a Good Way?:
Predicting User Ratings for Online Recipes
Can Liu, Chun Guo, Daniel Dakota , Sridhar Rajagopalan, Wen Li, Sandra K
?
ubler
Indiana University
{liucan, chunguo, ddakota, srrajago, wl9, skuebler}@indiana.edu
Ning Yu
University of Kentucky
ning.yu@uky.edu
Abstract
In this paper, we develop an approach to automatically predict user ratings for recipes at Epicuri-
ous.com, based on the recipes? reviews. We investigate two distributional methods for feature se-
lection, Information Gain and Bi-Normal Separation; we also compare distributionally selected
features to linguistically motivated features and two types of frameworks: a one-layer system
where we aggregate all reviews and predict the rating vs. a two-layer system where ratings of
individual reviews are predicted and then aggregated. We obtain our best results by using the
two-layer architecture, in combination with 5 000 features selected by Information Gain. This
setup reaches an overall accuracy of 65.60%, given an upper bound of 82.57%.
1 Introduction
Exchanging recipes over the internet has become popular over the last decade. There are numerous sites
that allow us to upload our own recipes, to search for and to download others, as well as to rate and
review recipes. Such sites aggregate invaluable information. This raises the question how such sites can
select good recipes to present to users. Thus, we need to automatically predict their ratings.
Previous work (Yu et al., 2013) has shown that the reviews are the best rating predictors, in comparison
to ingredients, preparation steps, and metadata. In this paper, we follow their approach and investigate
how to use the information contained in the reviews to its fullest potential. Given that the rating classes
are discrete and that the distances between adjacent classes are not necessarily equivalent, we frame this
task as a classification problem, in which the class distribution is highly skewed, posing the question of
how to improve precision and recall especially for the minority classes to achieve higher overall accuracy.
One approach is to identify n-gram features of the highest discriminating power among ratings, from a
large number of features, many of which are equally distributed over ratings. An alternative strategy is
to select less surface-oriented, but rather linguistically motivated features. Our second question concerns
the rating predictor architecture. One possibility is to aggregate all reviews for a recipe, utilizing rich
textual information at one step (one-layer architecture). The other possibility is to rate individual reviews
first, using shorter but more precise language clues, and then aggregate them (two-layer). The latter
approach avoids the problem of contradictory reviews for a given review, but it raises the question on
how to aggregate over individual ratings. We will investigate all these approaches.
The remainder of the paper is structured as follows: First, we review related work in section 2. Then, in
section 3, we motivate our research questions in more detail. Section 4 describes the experimental setup,
including the data preparation, feature extraction, classifier, and evaluation. In section 5, we present
the results for the one-layer experiments, and in section 6 for the two-layer experiments. Section 7
investigates a more realistic gold standard. We then conclude in section 8.
2 Related Work
This section provides a brief survey for sentiment analysis on online reviews.
This work is licenced under a Creative Commons Attribution 4.0 International License. Page numbers and proceedings footer
are added by the organizers. License details: http://creativecommons.org/licenses/by/4.0/
12
During the last decade or more, there has been significant body of sentiment analysis studies on online
reviews. Two major approaches exist: lexicon-based and machine learning. A lexicon-based approach
requires prior knowledge of important sentiment features to build a list of sentiment-bearing words (or
phrases), which are often domain independent. Examples of such lexicons include the Multi-Perspective
Question Answering (MPQA) subjectivity lexicon (Wilson et al., 2005) and the Linguistic Inquiry and
Word Count (LIWC) dictionary (Pennebaker et al., 2014). The sentiment of a review is determined
by various ways of aggregating information about the sentiment-bearing words (phrases), such as their
frequency and sentiment scores. The machine learning approach dominantly adopts supervised learning
algorithms, which treat sentiment analysis as a text classification task. In this case, sentiment features
are generated from a pre-labeled corpus. Given the lack of annotated data, semi-supervised learning is
adopted (Yu, 2014; Yu and Ku?bler, 2011). For this study, we focus on a specific language domain of
online recipe reviews, which has user ratings, thus we choose supervised learning. We also adopt one
existing linguistic lexicon to provide extra features for our classification models.
The earliest sentiment analysis on online reviews was done by Pang et al. (2002); they applied several
supervised learning algorithms to classify online movie reviews into a positive and a negative class.
This study found that machine learning methods outperformed human annotators. It also found that
bigrams did not improve the classification performance, whether used alone or with unigrams, which has
been confirmed by many following studies. However, Cui et al. (2006) later conjectured that when the
training corpus is large enough, adding bigrams to unigrams improved the accuracy of binary product
review classification. A great number of diverse features were proven to be beneficial to capture subtle
sentiments across studies and a ?kitchen sink? approach is often adopted for sentiment analysis (Yang
et al., 2008). However, when features are noisy and redundant, researcher have found it beneficial to
identify the most telling ones (Gamon, 2004; Ng et al., 2006).
While it is useful to differentiate positive and negative reviews, a finer level of distinction can help
users better compare online reviews. As a matter of fact, even extra half star ratings can have dramatic
economic impact (Anderson and Magruder, 2012). To predict multi-level ratings, either multiclass clas-
sification or regression methods can be applied (Koppel and Schler, 2006; Yu et al., 2013). Pang and Lee
(2005) have also proposed an alternative meta-algorithm based on metric labeling for predicting three or
four sentiment classes for movie reviews. In their experiments, the meta-algorithm outperformed SVMs
in either one-versus-all or regression mode. In order to adopt this meta-algorithm, however, one needs to
determine an effective review similarity measure, which is not always straightforward.
If an item receives multiple reviews and/or comes from multiple sources, an overall rating needs to be
generated for this item. Yu et al. (2013) generated this overall rating by treating all the reviews from one
recipe as one long review. In this study, we are going to investigate how to integrate review-level rating
predictions to generate a recipe-level prediction. Rating aggregation has been studied intensively for
collaborative filtering, where the user/rater?s bias is adjusted (e.g., the trustworthy user?s rating has more
influence than others (McGlohon et al., 2010)). Since our current study does not take raters? information
into consideration, we are going to stay with the sample aggregation method. A study by Garcin et al.
(2009) suggests that among mean, median, and mode, the median is often a better choice as it is not as
sensitive to outliers as the mean.
3 Research Questions
As described in the previous section, many studies use only word unigrams or bigrams. We use word
and part-of-speech (POS) n-grams, with n ranging from 1 to 3. This approach generates a large number
of features, creating a very noisy and high dimensional data set, which also makes classifier training
and testing slow. For this reason, we first investigate the effect of feature selection. The next question
concerns the usefulness of linguistically and socio-linguistically motivated features. This results in a
small, but ideally meaningful set of features. The last research question that we approach in this paper
concerns whether classifying recipes on the recipe level is too coarse. In general, we have a wide range
of reviews, each of which is accompanied by a user rating. Thus, it is possible to conduct review-level
classification and then aggregate the ratings.
13
3.1 Feature Selection
Our primary feature set is based on word and POS n-grams. This results in an extremely large feature
set of 449 144 features, many of which do not serve any discriminatory function. A common first step to
trimming the feature set is to delete stop words. However, in the cooking domain, it is unclear whether
stop words would help. Feature selection is used to identify n-grams tightly associated with individual
ratings. Additionally, a extremely high dimensional feature representation makes model training and
testing more time consuming, and is likely to suffer from overfitting - given a large number of parameters
needed to describe the model. Due to the exponential computation time required by wrapper approaches
for feature selection, we use filtering approaches which are based on statistics about the distribution of
features. Previous research (Liu et al., 2014) indicates that Bi-Normal Separation (BNS) (Forman, 2003)
and Information Gain (IG) yield best results for this task. Information Gain is defined as follows:
IG = H(S) ? H(S|f) =
?
f?{0,1}
?
S?{0,1}
P (f, S)log
P (f, S)
P (f)P (S)
where S is the positive class, f a feature, and P (f, S) the joint probability of the feature f occurring
with class S. Bi-Normal Separation finds the separation of the probability of a feature occurring in the
positive class vs. the negative class, normalized by F
?1
, which is the inverse function of the standard
normal cumulative probability distribution. Bi-Normal Separation is defined as follows:
BNS = |F
?1
(
D
Sf
D
S
) ? F
?1
(
D
Sf
D
S
)|
where D
S
is the number of documents in class S, D
S
the number of documents in class S, D
Sf
the
number of documents in class S where feature f occurs, and D
S,f
the number of documents in class
S where feature f occurs. The F
?1
function exaggerates an input more dramatically when the input is
close to 0 or 1, which means that BNS prefers rare words.
Since both metrics are defined for binary classification, the features are chosen in terms of a separation
of the recipes into ?bad? ratings (1-fork and 2-fork) versus ?good? ratings (3-fork and 4-fork), on the
assumption that the selected features will be predictive for the more specific classes as well. For
review-based experiments, the features are chosen with regard to ?good? and ?bad? individual reviews.
3.2 Linguistically Motivated Features
Linguistic features In order to examine whether linguistic information can improve prediction accu-
racy, linguistically motivated features were extracted from the data. We selected seven features based
on the assumption that they reveal a sense of involvedness or distance of the reviewer, i.e., that authors
distance themselves from a recipe to indicate negative sentiment and show more involvedness to indicate
positive sentiment. These seven features are:
1. The percentage of personal pronouns per sentence.
2. The number of words per sentence.
3. The total number of words in the review.
4. The percentage of passive sentences per review.
5. The number of punctuation marks per sentence.
6. The number of capitalized characters per sentence.
7. The type/token ratio per review.
Features such as words per sentence, total words, and the type/token ratio are seen as indicating the
complexity of the review.
14
Our hypothesis is that the longer the review, the more likely it indicates a negative sentiment as the
review may go at lengths to indicate why something was negative.
Similarly, using the passive voice can be viewed as distancing oneself from the review indicating a
sense of impartial judgement, most likely associated with negativity, as one tends to actively like some-
thing (i.e. ?We liked it? versus ?It wasn?t well seasoned.?). Since some reviews with strong emotions are
written in all capital letters as well as contain many punctuation marks (particularly ?!?), these features
are also collected as possible indicators of sentiment.
Lexicon-based features In addition, we used an existing lexicon, the Linguistic Inquiry and Word
Count (LIWC) dictionary (Pennebaker et al., 2014), to analyze several emotional and cognitive dimen-
sions in the recipe reviews. This lexicon is chosen over other sentiment lexicons because it covers a broad
range of categories beyond simply positive and negative emotions. Briefly, it contains general descriptor
categories (e.g., percentage of words captured by the dictionary), standard linguistic dimensions (e.g.,
percentage of words in the text that are pronouns), word categories tapping psychological constructs
(e.g., biological processes), personal concern categories (e.g., work), spoken language dimensions (e.g.,
accent), and punctuation categories. Details of these dimensions can be found in the LIWC 2007 manual.
For our study, we first extracted all the features from a review set independent from our training/test
set. We then selected the LIWC features with highest power to differentiate four rating classes based
on Information Gain. Below are the 15 selected features. Note that the linguistic features here are
document-level features, not sentence-level features, as proposed above.
? Linguistic Processes
? Negations (e.g., never, no): 57 words
? 1st person plural (e.g., we, us): 12 words
? Exclamation mark
? Psychological Processes
? Affective process: this high level category contains 915 positive/negative emotions, anxiety,
anger and sadness related terms.
? Positive emotion (e.g., love, nice, sweet): 406 words
? Negative emotion (e.g., hurt, ugly, nasty): 499 words
? Sadness (e.g., crying, grief): 101 words
? Exclusive (e.g., but, exclude): 17 words
? Tentative (e.g., maybe, perhaps, guess): 155 words
? Causation (e.g., because, hence): 108 words
? Discrepancy (e.g., should, would, could) : 76 words
? Certainty (e.g., always, never): 83 words
? Sexual (e.g., love): 96 words
? Feel (e.g., feel, touch): 75 words
? Personal Concerns
? Leisure (e.g, cook, chat, movie): 229 words
It is not surprising that emotion related features are selected, but it is interesting to see that cognitive
processes features (i.e., causation, tentative, discrepancy, certainty and exclusive) are also highly related
to ratings. Taking a close look at the means of feature values across four ratings, we observe that people
tend to use words in the tentative, discrepancy, exclusive categories when they write negative recipe
reviews. For terms in causation, however, it is the opposite: People write about reasons when writing
positive reviews. Some further investigation is needed to explain why this is the case. We also see that the
higher the rating, the more likely it is that people use first person plural pronouns. This may be due to the
fact that only when people like a recipe, they will tend to share the food with others. Other observations
15
1 fork 108
2 fork 787
3 fork 5 648
4 fork 3 546
Table 1: The distribution of ratings in the Epicurious data.
include: The sexual features are positively correlated with high ratings, which is mainly due to the word
?love? in its non-sexual meaning. People tend to use more words from the perception processes category
feel when they complain about a recipe.
3.3 One-Layer Prediction Versus Two-Layer Prediction
The one-layer or recipe-based approach consider all reviews per recipe as a single document. This
approach has rich textual information, especially when a large number of reviews exist for a recipe.
However, the concern with this approach is that the reviews in themselves may be varied. There are
recipes whose reviews range from the lowest to the highest rating. Given such a range of individual
ratings, we can assume that the recipe-based approach will be faced with a contradictory feature set for
certain recipes. For this reason, we also investigate a two-layer or review-based approach. Here, every
individual review is rated automatically. In a second step, we aggregate over all reviews per recipe.
Aggregation can either take the form of majority voting, average, or of a second classifier which takes
the aggregated ratings as features to make a final decision. However, this approach will suffer from often
very short reviews, which do not allow the extraction of sufficient features as well as from the inequality
in the number of reviews per recipe.
4 Experimental Setup
4.1 Data Set
We scraped user reviews for 10 089 recipes, published on the Epicurious website
1
before and on April 02,
2013. Typically, a recipe contains three parts: ingredients, cooking instructions, and user reviews. In our
experiments, we focus exclusively on the reviews. Each user review has a rating for this recipe, ranging
from 1 fork to 4 forks. There is also an overall rating per recipe, which is the average of all reviews
ratings as well as ratings submitted without reviews. Half forks are possible for recipe rating but not for
review ratings. These recipes were pre-processed to remove reviews with zero ratings. Recipes that had
no reviews were then subsequently removed. In order to counter the effect of the wide variance in the
number of reviews per recipe, we randomly sampled 10 reviews from recipes with more than 10 reviews.
We had performed initial experiments with all reviews, which resulted in only minor differences. At the
review level, rare words (unigrams occurring less than four times) were removed for two reasons: 1)
Extremely rare words are likely to be noise rather than sentiment-bearing clues; 2) the feature selection
method BNS is biased towards rare words; 3) such words do not generalize well. The recipes were then
tagged using the Stanford POS Tagger (Toutanova et al., 2003).
The data set is severely skewed with regard to the number of recipes per fork: Users seem to be more
willing to review good recipes. To lessen the effect of imbalance in the rating classifier, all half fork
reviews were added to their corresponding full star reviews (i.e., 1.5 fork was added to the 1 fork data).
This resulted in the data split of 10 089 recipes shown in table 1. Even after collapsing the half stars,
there is still a very large skewing of the data towards the higher ratings. This means, feature selection is
important to mitigate the imbalance to a certain degree.
4.2 Features
In addition to the linguistic features described in section 3.2, we also extracted n-gram features: word
unigrams, bigrams, and trigrams as well as POS tag unigrams, bigrams, and trigrams. Since the data
1
http://www.epicurious.com
16
Method 750 900 1 000 1 500 3 000 6 000
BNS ? ? 31.33 ? 42.00 50.67
IG 62.00 62.00 62.33 62.33 61.00 58.67
Table 2: Results for feature selection based on Bi-Normal Separation (BNS) and Information Gain (IG).
includes tokens particular to the web, modifications were made to the data to help with the processing of
these types of tokens. URLs were replaced with a single URL token and tagged with a unique ?URL? tag.
Emoticons were defined as either positive or negative and subsequently replaced by EMOP or EMON
respectively. Since it is unclear for this task whether more frequent feature should receive a larger weight,
we normalized features values to a range of [0,1].
4.3 Classifiers
Preliminary experiments were run to determine the best classifier for the task. We decided on Support
Vector Machines in the implementation of SVM multi-class V1.01 (Crammer and Singer, 2002) for both
review-level and recipe-level rating prediction. Initial experiments showed that SVM multi-class V1.01
reaches higher results on our skewed data set than the current V2.20. For this reason, all experiments
reported in this paper are based on V1.01 with its default settings, i.e., using a linear kernel.
To aggregate review-level ratings into a recipe-level prediction, we experimented with both the max-
imum entropy classifier in the implementation of the Stanford Classifier (Manning and Klein, 2003)
and the SVM multi-class classifier. We included Maxent Classifier because given the small number of
features it is no longer clear whether SVM is advantageous.
4.4 Baseline
The baseline was established following Yu et al. (2013) as selecting the label of majority class (3-fork)
to tag all recipes, producing an accuracy of 56.00% for both one-layer and two-layer systems.
4.5 Evaluation
Evaluation was performed using 3-fold cross validation. Since the data is skewed, we report Precision
(P), Recall (R), and F-Scores (F) for all classes across each experiment, along with standard accuracy.
5 Results for One-Layer Prediction
5.1 Feature Selection
We first investigated the effect of feature selection, varying the number of included features from 750 to
6 000. Results for the two methods and different feature thresholds are shown in table 2. Since previous
work (Liu et al., 2014) showed that BNS has a tendency to select infrequent n-grams and would need
a larger number of features than IG to achieve good performance, we tested the higher ranges of 1 000,
3 000, 6 000 features. None of these experiments yields an accuracy higher than the baseline of 56.00%.
On the other hand, the performance of Information Gain peaks at 1 000 and 1 500 features, and we reach
an absolute increase in accuracy of 6.33%. Given these experiments, for all following experiments, we
use the combination of Information Gain and 1 000 n-gram features.
5.2 Linguistically Motivated Features
Here, we test the contribution of the linguistically motivated features introduced in section 3.2. To allow
a comparison to previous experiments, we report the baseline and the results for using Information Gain.
For the two sets of linguistically motivated features, we used the following combination of features:
1. Lexicon-based features (Lex) combined with linguistic features (Ling) (22 features).
2. Lexicon-based features (Lex) combined with the 1 000 features selected by Information Gain (IG)
(1015 features).
17
1 fork 2 fork 3 fork 4 fork
P R F P R F P R F P R F Acc.
Base 0.00 0.00 0.00 0.00 0.00 0.00 56.00 100.00 72.00 0.00 0.00 0.00 56.00
IG 33.33 1.00 2.00 31.33 12.00 17.33 66.00 73.67 69.67 58.33 58.00 58.00 62.33
Lex+Ling 0.00 0.00 0.00 0.00 0.00 0.00 56.00 100.00 72.00 0.00 0.00 0.00 56.00
IG+Lex 39.00 2.00 3.67 31.67 10.00 15.00 65.33 75.33 69.67 59.67 55.67 57.33 62.67
IG+Ling 0.00 0.00 0.00 32.00 3.33 6.00 63.67 81.00 71.33 62.67 49.67 55.33 63.33
IG+Lex+Ling 0.00 0.00 0.00 32.00 3.33 6.00 63.67 81.00 71.33 62.67 49.67 55.33 63.33
Table 3: Results for manually selected features.
1 fork 2 fork 3 fork 4 fork
no. feat. P R F P R F P R F P R F Acc.
1000 61.57 58.11 59.79 53.70 37.42 44.11 63.04 42.14 50.51 71.30 89.08 79.20 67.80
2000 61.65 58.27 59.91 52.96 39.40 45.18 63.37 43.19 51.37 71.86 88.70 79.40 68.11
3000 62.50 58.51 60.44 52.98 40.88 46.15 62.90 44.49 52.12 72.45 88.10 79.51 68.34
4000 62.45 58.45 60.38 52.38 41.05 46.03 62.99 45.54 52.86 72.83 87.70 79.58 68.46
5000 62.32 57.00 59.54 51.66 41.17 45.82 62.21 46.15 52.99 73.05 87.24 79.52 68.31
Table 4: Results on individual reviews for the two-layer experiments.
3. Linguistic features (Ling) combined with the 1 000 features selected by Information Gain (IG)
(1007 features).
4. A combination of all three sets of features (IG+Lex+Ling) (1022 features).
The results for these experiments are reported in table 3. These results show that a combination of
the two sets of linguistically motivated features does not increase accuracy over the baseline. In fact, the
classification is identical to the baseline, i.e., all recipes are grouped into the majority class of 3-fork. We
assume that the linguistically motivated features are too rare to be useful. If we add the lexicon-based
features to the ones selected by Information Gain, we reach a minimal improvement over only the IG
features: accuracy increases from 62.33% to 62.67%. This increase is mostly due to a better performance
on the minority class of 1 fork. If we add the 7 linguistic features to the IG features, we reach the highest
accuracy of 63.33%. However, this is due to a more pronounced preference for selecting the majority
class. Adding the lexicon-based features to this feature set does not give any further improvements.
6 Results for Two-Layer Prediction
In this section, we investigate the two-layer or review-based prediction. For these experiments, we per-
formed feature selection on the individual reviews using IG. Adding the linguistically motivated features
considerably decreased performance. We assume that these features do not generalize well on the shorter
reviews.
Note that the task approached here is a difficult task since the recipe rating on Epicurious is not the
average over all the ratings associated to the individual reviews but also includes ratings by user who did
not write a review. If we average over all the sampled gold standard review ratings per recipe, we reach
an accuracy of 82.57%. This is the upper bound that we can reach in these experiments.
6.1 Classifying Individual Reviews
First, we look at the phase in which individual reviews are classified. The results of this set of experiments
is shown in table 4. Note that there are three important trends here: 1) The accuracy of the SVM classifier
is higher than for classifying recipes. The comparison needs to be taken with a grain of salt because
these are two different tasks. However, this is an indication that it is possible to reach higher results
based on aggregating over individual reviews. 2) For this task, we reach the highest results by using
4 000 features, i.e., a considerably higher number of features than the optimal set for the recipe-based
experiments, where 1 000 features sufficed. We suspect that we need more features in this setting because
the individual reviews are shorter so that individual features do not generalize as well as for complete
recipes. 3) The classification of individual reviews is less skewed than for complete recipes. The F-scores
18
1 fork 2 fork 3 fork 4 fork
no. f. sys. P R F P R F P R F P R F Acc.
1000 avg 44.64 36.98 40.45 60.00 23.03 33.28 75.72 51.91 61.59 52.38 86.01 65.11 61.48
maxent 43.87 33.33 37.88 58.30 19.73 29.48 73.90 58.77 65.47 55.03 81.40 65.67 63.41
svm 62.21 62.21 62.21 56.18 16.03 24.94 72.24 58.14 64.43 53.92 80.82 68.68 62.21
2000 avg 44.61 38.83 41.52 61.45 24.29 34.82 76.12 53.96 63.15 53.45 85.53 65.79 62.58
maxent 43.17 34.27 38.21 61.47 21.23 31.56 74.43 60.93 67.01 56.27 80.93 66.38 64.58
svm 63.29 63.29 63.29 56.53 16.79 25.89 72.65 60.32 65.91 55.14 80.26 65.37 63.29
3000 avg 42.71 38.86 40.69 61.08 26.57 37.03 75.62 54.33 63.23 53.71 84.60 65.71 62.64
maxent 42.40 35.20 38.47 61.90 23.40 33.96 74.00 61.90 67.41 56.83 79.73 66.36 64.84
svm 63.53 63.53 63.53 54.09 17.41 26.34 72.14 61.45 66.37 55.80 79.02 65.41 63.53
4000 avg 38.64 34.22 36.30 61.09 24.89 35.37 75.23 55.91 64.15 54.34 83.81 65.93 63.07
maxent 38.37 31.47 34.58 60.67 22.47 32.79 73.80 63.03 67.99 57.47 79.00 66.54 65.16
svm 64.03 64.03 64.03 52.92 17.53 26.33 72.24 62.85 67.22 56.51 78.17 65.60 64.03
5000 avg 39.23 37.05 38.11 59.20 24.89 35.05 75.38 56.25 64.42 54.54 83.64 66.03 63.23
maxent 38.03 33.40 35.56 58.37 22.00 31.96 73.97 64.17 68.72 58.13 78.57 66.82 65.60
svm 64.68 64.68 64.68 50.19 17.40 25.84 72.52 64.18 68.10 57.45 77.95 66.15 64.68
Table 5: Results on aggregating reviews for the two-layer experiments.
for the non-majority classes are considerably higher than in the recipe-based setting. Thus, we expect to
obtain more balanced results across classes in the aggregation as well.
6.2 Predicting Recipe Ratings by Aggregating Reviews
When aggregating review predictions to recipe rating, we use three methods: 1) Taking the average of
the review ratings from the previous step; 2) using SVM; and 3) using a maximum entropy classifier
(Maxent), the Stanford Classifier. When calculating the average over review rating predictions, the final
average is rounded up. The results are reported in table 5. When using SVM and the maximum entropy
classifier, we use four features, corresponding to the four ratings. The feature values are calculated as
the percentage of reviews from the target recipe that were assigned to this fork rating by our review-level
classifier.
Overall, the maximum entropy classifier yields the best performance, independent of the number of
features used for the review-level classifier. The highest performance we reach by using 5 000 features
and the maximum entropy classifier. Calculating the average results in the worst performance. Although
Epicurious calculates the average user ratings based on review ratings and singular ratings, keep in mind
that we use at most 10 reviews per recipe, hence only capture part of the image. This may explain why
simply calculating the average does not work well. When looking at the F-scores for each fork in table 5,
however, the maximum entropy classifier produces lower performance than average and SVM classifier
for the 1 fork and 2 fork classes. For 1 fork, SVM has the highest F-scores for different numbers of
features, followed by the averaging approach while for 2 fork, the average approach produced the highest
F-scores. One possible explanation is that recipes with lower ratings have relatively small numbers of
reviews and thus may be less impacted by our sampling.
7 Towards a More Realistic Gold Standard
When we aggregate over the individual review rating using the average, the results are only slightly better
than the one-layer results. For example, the best performance using the average reaches an accuracy of
63.23%, as opposed to the one-layer accuracy of 62.33% in table 2 (note that these settings use only IG
features). One reason for this low performance is that Epicurious averages all review ratings to generate
a recipe rating, independent of whether there is review attached to the rating or not. Since our text-based
classifiers make their decisions only based on the reviews, the question is how well we actually predict
the average rating if only ratings attached to reviews were used in the calculation. In this way, we can
evaluate how well our approach works if we assume that all the information is available to the classifier.
Consequently, we calculated a new gold standard, averaging gold ratings of individual reviews in the
recipe sample. We investigate this effect based on the two-layer setting where reviews are aggregated
via averaging. The results of this set of experiments are shown in table 6 for the two-layer approach and
in table 7 for the one-liner approach. We report results using the gold label based on the ratings from
19
1 fork 2 fork 3 fork 4 fork
sys. P R F P R F P R F P R F Acc.
EPI 39.23 37.05 38.11 59.20 24.89 35.05 75.38 56.25 64.42 54.54 83.64 66.03 63.23
EPI-AVG 56.41 51.16 53.66 62.73 62.73 62.73 76.89 65.66 70.83 67.10 85.39 75.15 71.10
Table 6: Evaluation on a more realistic gold standard for two-layer experiments.
1 fork 2 fork 3 fork 4 fork
P R F P R F P R F P R F Acc.
Base 0.00 0.00 0.00 0.00 0.00 0.00 52.00 100.00 68.00 0.00 0.00 0.00 52.00
IG 8.33 1.00 1.67 29.67 11.00 16.00 64.33 70.00 67.00 64.00 66.00 64.67 63.33
Lex+Ling 0.00 0.00 0.00 0.00 0.00 0.00 51.33 99.33 67.67 41.33 1.00 2.00 51.00
IG+Lex 11.00 1.00 2.00 29.00 9.67 14.67 64.00 70.33 67.00 64.00 65.33 64.67 63.33
IG+Ling 16.67 1.00 2.00 31.00 6.33 10.67 63.00 72.67 67.67 65.00 63.33 64.00 63.33
IG+Lex+Ling 16.67 1.00 2.00 31.33 6.67 11.00 63.00 72.67 67.33 65.00 63.33 64.00 63.33
Table 7: Evaluation on a more realistic gold standard for one-layer experiments.
Epicurious (EPI) and based on the new gold standard (EPI-AVG). These results show that based on this
more realistic gold standard, averaging over the individual reviews results in an accuracy of 71.10%,
however with an upper bound of 100% instead of 82.57%. The results for the on-layer experiments are
not as sensitive to this new gold standard. The baseline, which loses 4%, shows that now, the task is
more difficult. All combinations involving IG selected features reach an accuracy of 63.33%, the same
as for the Epicurious gold standard (see table 3).
8 Conclusion and Future Work
In this study, we have explored various strategies for predicting recipe ratings based on user reviews.
This is a difficult task due to systemic reasons, user bias, as well as exogenous factors: 1) There are
user ratings that do not come with reviews, which means that they constitute hidden information for our
classifiers (so that we have an upper bound of 82.57% in overall accuracy). 2) Ratings are not entirely
supported by text, i.e., some ratings seem to be independent from the review text, due to user behavior
(e.g., people tend to give higher ratings in good weather than in bad weather (Bakhshi et al., 2014)).
Our experiments suggest that a two-layer approach, which predicts review-level ratings and aggregates
them for the recipe-level rating, reaches a higher accuracy than the one-layer approach that aggregates
all reviews and predicts on the recipe level directly, with a 3.6% absolute improvement in accuracy. If
we evaluate the two-layer results on a more realistic gold standard, we achieve an even higher increase
of 12.3%.
Our experiments also suggest that with feature selection, automatically generated n-gram features can
produce reasonable results without manually generated linguistic cues and lexicons, although the latter
does show a slight improvement, especially for minority classes.
A few directions can be taken for our future study: 1) Handling short reviews with better methods for
dealing with sparse features. 2) The feature selection is conducted within a binary classification scenario
(1- and 2-forks vs. 3- and 4-forks). It is worth exploring the effect of feature selection within four 1 vs.
all scenarios (i.e., 1-fork against the rest, etc.). 3) We will explore aspect-level sentiment classification
to provide a finer-grained summary of the recipes.
References
Michael Anderson and Jeremy Magruder. 2012. Learning from the crowd: Regression discontinuity estimates of
the effects of an online review database. The Economic Journal, 122:957?989.
Saeideh Bakhshi, Partha Kanuparthy, and Eric Gilbert. 2014. Demographics, weather and online reviews: A study
of restaurant recommendations. In Proceedings of the WWW conference, Seoul, Korea.
Koby Crammer and Yoram Singer. 2002. On the algorithmic implementation of multiclass kernel-based vector
machines. Journal of Machine Learning Research, 2:265?292.
20
Hang Cui, Vibhu Mittal, and Mayur Datar. 2006. Comparative experiments on sentiment classification for online
product reviews. In Proceedings of the 21st National Conference on Artificial Intelligence, AAAI?06, pages
1265?1270, Boston, Massachusetts.
George Forman. 2003. An extensive empirical study of feature selection metrics for text classification. Journal of
Machine Learning Research, 3:1289?1305.
Michael Gamon. 2004. Sentiment classification on customer feedback data: Noisy data, large feature vectors,
and the role of linguistic analysis. In Proceedings of the 20th International Conference on Computational
Linguistics (COLING), pages 841?847, Geneva, Switzerland.
Florent Garcin, Boi Faltings, Radu Jurca, and Nadine Joswig. 2009. Rating aggregation in collaborative filtering
systems. In Proceedings of the Third ACM Conference on Recommender Systems, pages 349?352, New York,
NY.
Moshe Koppel and Jonathan Schler. 2006. The importance of neutral examples in learning sentiment. Computa-
tional Intelligence Journal, 22:100?109. Special Issue on Sentiment Analysis.
Can Liu, Sandra Ku?bler, and Ning Yu. 2014. Feature selection for highly skewed sentiment analysis tasks. In
Proceedings of the Second Workshop on Natural Language Processing for Social Media (SocialNLP), Dublin,
Ireland.
Christopher Manning and Dan Klein. 2003. Optimization, maxent models, and conditional estimation without
magic. Tutorial at HLT-NAACL 2003 and ACL 2003.
Mary McGlohon, Natalie Glance, and Zach Reiter. 2010. Star quality: Aggregating reviews to rank products and
merchants. In Proceedings of Fourth International Conference on Weblogs and Social Media (ICWSM), pages
114?121, Washington, DC.
Vincent Ng, Sajib Dasgupta, and S. M. Niaz Arifin. 2006. Examining the role of linguistic knowledge sources
in the automatic identification and classification of reviews. In Proceedings of COLING/ACL, pages 611?618,
Sydney, Australia.
Bo Pang and Lillian Lee. 2005. Seeing stars: Exploiting class relationships for sentiment categorization with re-
spect to rating scales. In Proceedings of the 43rd AnnualMeeting on Association for Computational Linguistics,
ACL, pages 115?124, Ann Arbor, MI.
Bo Pang, Lillian Lee, and Shivakumar Vaithyanathan. 2002. Thumbs up?: Sentiment classification using machine
learning techniques. In Proceedings of the Conference on Empirical Methods in Natural Language Processing,
EMNLP, pages 79?86, Philadelphia, PA.
James Pennebaker, Roger Booth, and Martha Francis, 2014. Linguistic inqury and word count: LIWC 2007
operator?s manual. http://homepage.psy.utexas.edu/HomePage/Faculty/Pennebaker/
Reprints/LIWC2007_OperatorManual.pdf.
Kristina Toutanova, Dan Klein, Christopher Manning, and Yoram Singer. 2003. Feature-rich part-of-speech
tagging with a cyclic dependency network. In Proceedings of HLT-NAACL 2003, pages 252?259, Edmonton,
Canada.
Theresa Wilson, Janyce Wiebe, and Paul Hoffmann. 2005. Recognizing contextual polarity in phrase-level senti-
ment analysis. In Proceedings of the Conference on Empirical Methods in Natural Language Processing, pages
347?354, Vancouver, Canada.
Kiduk Yang, Ning Yu, and Hui Zhang. 2008. WIDIT in TREC2007 blog track: Combining lexicon-basedmethods
to detect opinionated blogs. In Proceedings of the 16th Text Retrieval Conference, Gaithersburg, MD.
Ning Yu and Sandra Ku?bler. 2011. Filling the gap: Semi-supervised learning for opinion detection across domains.
In Proceedings of the Fifteenth Conference on Computational Natural Language Learning, CoNLL, pages 200?
209, Portland, OR.
Ning Yu, Desislava Zhekova, Can Liu, and Sandra Ku?bler. 2013. Do good recipes need butter? Predicting user
ratings of online recipes. In Proceedings of the IJCAI Workshop on Cooking with Computers, Beijing, China.
Ning Yu. 2014. Exploring co-training strategies for opinion detection. Journal of the Association for Information
Science and Technology.
21
First Joint Workshop on Statistical Parsing of Morphologically Rich Languages
and Syntactic Analysis of Non-Canonical Languages, pages 1?14 Dublin, Ireland, August 23-29 2014.
Parsing German: How Much Morphology Do We Need?
Wolfgang Maier
Heinrich-Heine-Universita?t Du?sseldorf
Du?sseldorf, Germany
maierw@hhu.de
Sandra Ku?bler
Indiana University
Bloomington, IN, USA
skuebler@indiana.edu
Daniel Dakota
Indiana University
Bloomington, IN, USA
ddakota@indiana.edu
Daniel Whyatt
Indiana University
Bloomington, IN, USA
dwhyatt@indiana.edu
Abstract
We investigate how the granularity of POS tags influences POS tagging, and furthermore, how
POS tagging performance relates to parsing results. For this, we use the standard ?pipeline?
approach, in which a parser builds its output on previously tagged input. The experiments are
performed on two German treebanks, using three POS tagsets of different granularity, and six
different POS taggers, together with the Berkeley parser. Our findings show that less granularity
of the POS tagset leads to better tagging results. However, both too coarse-grained and too
fine-grained distinctions on POS level decrease parsing performance.
1 Introduction
German is a non-configurational language with a moderately free word order in combination with a case
system. The case of a noun phrase complement generally is a direct indicator of the phrase?s grammatical
function. For this reason, a morphological analysis seems to be a prerequisite for a syntactic analysis.
However, in computational linguistics, parsing was developed for English without the use of morpho-
logical information, and this same architecture is used for other languages, including German (Ku?bler et
al., 2006; Petrov and Klein, 2008). An easy way of introducing morphological information into parsing,
without modifying the architecture, is to attach morphology to the part-of-speech (POS) tagset. However,
this makes POS tagging more complex and thus more difficult.
In this paper, we investigate the following questions: 1) How well do the different POS taggers work
with tagsets of a varying level of morphological granularity? 2) Do the differences in POS tagger per-
formance translate into similar differences in parsing quality? Complementary POS tagging results and
preliminary parsing results have been published in German in Ku?bler and Maier (2013).
Our experiments are based on two different treebanks for German, TiGer (Brants et al., 2002) and
Tu?Ba-D/Z (Telljohann et al., 2012). Both treebanks are based on the same POS tagset, the Stuttgart-
Tu?bingen Tagset (STTS) (Schiller et al., 1995). We perform experiments with three variants of the tagset:
The standard STTS, the Universal Tagset (UTS) (Petrov et al., 2012) (a language-independent tagset),
and an extended version of the STTS that also includes morphological information from the treebanks
(STTSmorph). STTS consists of 54 tags, UTS uses 12 basic tags, and the morphological variants of the
STTS comprise 783 and 524 POS tags respectively. We use a wide range of POS taggers, which are
based on different strategies: Morfette (Chrupala et al., 2008) and RF-Tagger (Schmid and Laws, 2008)
are designed for large morphological tagsets, the Stanford tagger (Toutanova et al., 2003) is based on a
maximum entropy model, SVMTool (Gime?nez and Ma`rquez, 2004) is based on support vector machines,
TnT (Brants, 2000) is a Markov model trigram tagger, and Wapiti (Lavergne et al., 2010) a conditional
random field tagger. For our parsing experiments, we use the Berkeley parser (Petrov and Klein, 2007b;
Petrov and Klein, 2007a).
This work is licenced under a Creative Commons Attribution 4.0 International License. Page numbers and proceedings footer
are added by the organizers. License details: http://creativecommons.org/licenses/by/4.0/
1
Our findings for POS tagging show that Morfette reaches the highest accuracy on UTS and overall on
unknown words while TnT reaches the best performance for STTS and the RF-Tagger for STTSmorph.
These trends are stable across both treebanks. As for the parsing results, using STTS results in the best
accuracies. For TiGer, POS tags assigned by the parser perform better in combination with UTS and
STTSmorph. For TiGer in combination with STTS and all variants in Tu?Ba-D/Z, there are only minor
differences between the parser assigned POS tags and those by TnT.
The remainder of the article is structured as follows. In section 2, we review previous work. Section
3 presents the different POS tagsets. Section 4 describes our experimental setup. The POS tagging and
parsing results are discussed in the sections 5 and 6, respectively. Section 7 concludes the article.
2 Previous Work
In this section, we present a review of the literature that has previously examined the correlation of
POS tagging and parsing under different aspects. While this overview is not exhaustive, it presents the
major findings related to our work. The issues examined can be regarded under two orthogonal aspects,
namely, the parsing model used (data-driven or grammar-based), and the question of how to disambiguate
between various tags for a single word.
Some work has been done on investigating different tagsets for individual languages. Collins et al.
(1999) adapt the parser of Collins (1999) for the Czech Prague Dependency Treebank. Using an external
lexicon to reduce data sparseness for word forms did not result in any improvement, but adding case to the
POS tagset had a positive effect. Seddah et al. (2009) investigate the use of different parsers on French.
They also investigate two tagsets with different granularity and come to the conclusion that the finer
grained tagset leads to higher parser performance. The work that is closest to ours is work by Marton et
al. (2013), who investigate the optimal POS tagset for parsing Arabic. They come to the conclusion that
adding definiteness, person, number, gender, and lemma information to the POS tagset improve parsing
accuracy. Both Dehdari et al. (2011) and Sza?nto? and Farkas (2014) investigate automatic methods for
selecting the best subset of morphological features, the former for Arabic, the latter for Basque, French,
German, Hebrew, and Hungarian. However, note that Sza?nto? and Farkas (2014) used the data from the
SPMRL shared task 2013, which does not contain grammatical functions in the syntactic annotations.
Both approaches found improvements for subsets of morphological features.
Other works examine, also within a ?pipeline? method, possibilities for ambiguity reduction through
modification of tagsets, or of the lexicon by tagset reduction, or through word-clustering. Lakeland
(2005) uses lexicalized parsing a` la Collins (1999). Similarly to the more recent work by Koo et al.
(2008) or Candito and Seddah (2010), he addresses the question of how to optimally disambiguate for
parsing on the lexical level by clustering. A word cluster is thereby seen as an equivalence class of
words and assumes to a certain extent the function of a POS tag, but can be adapted to the training
data. Le Roux et al. (2012) address the issue of data sparseness on the lexical level with PCFG parsing
with the morphologically rich language Spanish. The authors use a reimplementation of the Berkeley
parser. They show that parsing results can be improved by simplifying the POS tagset, as well as by
lemmatization, since both approaches reduce data sparseness.
As already mentioned, a POS tag can be seen as an equivalence class of words. Since in the ?pipeline?
approach, the parse tree is built on POS tags, it is possible that a POS tagset is optimal from a linguistic
point of view, but that its behavior is not optimal with respect to parsing results, because relevant lexical
information is hidden from the parse tree by the POS tagset. While Koo et al. (2008) overcome this
deficit by automatically searching for ?better? clusters, other works copy certain lexical information into
the actual tree, e.g., by using grammatical function annotation (Versley, 2005; Versley and Rehbein,
2009). Seeker and Kuhn (2013) complement the ?pipeline? model (using a dependency parser (Bohnet,
2010)) by an additional component that uses case information as a filter for the parser. They achieve
improvements for Hungarian, German and Czech.
A number of works develop models for simultaneous POS tagging or morphological segmentation
and parsing. Based on work by Ratnaparkhi (1996) and Toutanova and Manning (2000), Chen and Kit
(2011) investigate disambiguation on the lexical level. They assume that local, i.e., sequential but not
2
tag description tag description tag description
NOUN noun PRON pronoun CONJ conjunction
VERB verb DET determiner, article PRT particle
ADJ adjective ADP preposition, postposition . punctuation
ADV adverb NUM numeral X everything else
Table 1: The 12 tags of the Universal Tagset.
hierarchical, features are decisive for the quality of POS tagging and note that a ?pipeline? model does not
take this into account since the parser effectively performs the POS disambiguation. On these grounds,
they present a factorized model for PCFG parsing which separates parsing into a discriminative lexical
model (with local features) and the actual parsing model, to be combined with a product-of-experts
(Hinton, 1999).
Particularly in the dependency parsing literature, combined models for simultaneous POS tagging and
parsing can be found. Research has concentrated on languages that require additional segmentation on
the word level, such as Chinese (Hatori et al., 2011) or Hebrew (Goldberg and Tsarfaty, 2008). A new
approach by Bohnet and Nivre (2012) was also evaluated on German. Results for POS tagging and
parsing of German by means of a constraint grammar can be found in Daum et al. (2003) as well as
in Foth et al. (2005). However, since these approaches are only marginally related to our approach, we
forego a further overview.
3 The Three Tagset Variants
In our experiments, we use three POS tagset variants: The standard Stuttgart-Tu?bingen Tagset (STTS),
the Universal Tagset (UTS) (Petrov et al., 2012), and an extended version of the STTS that also includes
morphological information from the treebanks (STTSmorph). Since the two treebanks differ in their
morphological annotation, in this variant, the tags differ between the two treebanks: For TiGer, we have
783 possible complex POS tags, and for Tu?Ba-D/Z, there are 524. By complex tags, we mean a combi-
nation of an STTS tag with the morphological tag. Also, note that not all of the possible combinations
are attested in the treebanks.
The UTS consists of 12 basic POS tags, shown in table 11. It was developed for multilingual appli-
cations, in which a common tagset is of importance, such as for a multilingual POS tagger. The UTS
only represents the major word classes. Thus, this tagset should result in a high POS tagging accuracy
since only major distinctions are made. However, it is unclear whether these coarse distinctions provide
enough information for a syntactic analysis.
The STTS is based on distributional regularities of German. It contains 54 tags and thus models more
fine grained distinctions than the UTS. For a list of tags, see Schiller et al. (1995). The finer distinctions
in STTS mostly concern word classes, but there is also a distinction between finite and infinite verbs.
This distinction is important for the syntactic analysis, especially in Tu?Ba-D/Z, but it can be difficult to
make by a POS tagger with a limited context.
The STTS can be extended by a morphological component. Both treebanks provide a morphological
analysis, but the analyses model different decisions. In TiGer, a set of 585 different feature combinations
is used, which can be combined from the features listed in table 2. The sentence in (1) gives an example
of the combination of the STTS and morphology, which are separated by the % sign. The feature ?
means that there are no morphological features for the given POS tag.
(1) Konzernchefs
NN%Nom.Pl.Masc
lehnen
VVFIN%3.Pl.Pres.Ind
den
ART%Acc.Sg.Masc
Milliarda?r
NN%Acc.Sg.Masc
als
APPR%?
US-Pra?sidenten
NN%Acc.Sg.Masc
ab
PTKVZ%?
/
$(%?
?Corporate CEOs disapprove of the billionaire as US president /?
1For a mapping from STTS to UTS, cf. https://code.google.com/p/universal-pos-tags/.
3
feature description
ambiguous: *
gender masculine (Masc), feminine (Fem), neuter (Neut)
gradation positive (Pos), comparative (Comp), superlative (Sup)
case nominative (Nom), genitive (Gen), dative (Dat), accusative (Akk)
mode indicative (Ind), conjunctive (Subj), imperative (Imp)
number singular (Sg), plural (Pl)
person 1., 2., 3.
tense present (Pres), past (Past)
Table 2: The morphological categories in TiGer.
feature description
ambiguous *
gender masculine (m), feminine (f), neuter (n)
case nominative (n), genitive (g), dative (d), accusative (a)
number singular (s), plural (p)
person 1., 2., 3.
tense present (s), past (t)
mode indicative (i), conjunctive (k)
Table 3: The morphological categories in Tu?Ba-D/Z.
Out of the 585 possible combinations of morphological features, 271 are attested in TiGer. In combi-
nation with the STTS, this results in 783 combinations of STTS and morphological tags. Out of those,
761 occur in the training set. However, we expect data sparseness during testing because of the high
number of possible tags. For this reason, we calculated which percentage of the tags in the development
and test set are known combinations. We found that 25% and 30%, respectively, do not occur in the train-
ing set. However, note that the number of tags in the development and test sets is considerably smaller
than the number of tags in the training set.
In Tu?Ba-D/Z, there are 132 possible morphological feature combinations which can be combined from
the features listed in table 3. The sentence in (2) gives an example of the combination of the STTS and
morphology.
(2) Aber
KON%?
Bremerhavens
NE%gsn
AfB
NE%nsf
fordert
VVFIN%3sis
jetzt
ADV%?
Untersuchungsausschu?
NN%asm
?But the Bremerhaven AfB now demands a board of inquiry?
Out of the 132 possible feature combinations, 105 are attested in Tu?Ba-D/Z. In combination with the
STTS, this results in 524 combinations of STTS and morphological tags. Out of those, 513 occur in the
training set. For the development and test set, we found that 16% and 18% respectively do not occur in
the training set. These percentages are considerably lower than the ones for TiGer.
Since the tagsets that include morphology comprise several hundred different POS tags, we expect
tagging to be more difficult, resulting in lower accuracies. We also expect that the Tu?Ba-D/Z tagset
is better suited for POS tagging than the TiGer set because of its smaller tagset size and its higher
coverage on the development and test set. It is, however, unknown whether this information can be used
successfully in parsing.
4
dieses
PDAT
Acc.Sg.Neut
Buch
NN
Acc.Sg.Neut
finden
VVFIN
3.Pl.Pres.Ind
vor
APPR
allem
PIS
Dat.Sg.Neut
diejenigen
PDS
Nom.Pl.*
schwierig
ADJD
Pos
,
$,
die
PRELS
Nom.Pl.*
am
PTKA
meisten
PIS
*.*.*
Bildung
NN
Acc.Sg.Fem
haben
VAFIN
3.Pl.Pres.Ind
,
$,
vor
APPR
allem
PIS
Dat.Sg.Neut
psychoanalytische
ADJA
Pos.Acc.Sg.Fem
Bildung
NN
Acc.Sg.Fem
(
$(
...
$(
)
$(
NK NK
NP
AC NK
PP
PM HD
AA
AC NK
PP
MO NK NK
NP
NK NK APP
NP
SB OAHD
S
MO NK RC
NP
OA HD SBMO
S
VROOT
Figure 1: A sentence from TiGer.
4 Experimental Setup
4.1 Treebanks
We use the treebanks TiGer (Brants et al., 2002), version 2.2, and Tu?Ba-D/Z (Telljohann et al., 2012),
release 8. Both are built on newspaper text, Frankfurter Rundschau for TiGer and taz for Tu?Ba-D/Z.
Both treebanks use the same POS tagset with only one minor difference in the naming of one POS label.
However, the treebanks differ considerably in the syntactic annotation scheme. While TiGer uses a very
flat annotation involving crossing branches, the annotations in Tu?Ba-D/Z are more hierarchical, and long
distance relations are modeled via grammatical function labels rather than via attachment. Figures 1 and
2 show examples.
For preprocessing, we follow the standard practices from the parsing community. In both treebanks,
punctuation and other material, such as parentheses, are not included in the annotation, but attached to a
virtual root node. We attach the respective nodes to the tree using the algorithm described by Maier et
al. (2012) so that every sentence corresponds to exactly one tree. In a nutshell, this algorithm uses the
left and right terminal neighbors as attachment targets. In TiGer, we then remove the crossing branches
using a two-stage process. In a first step, we apply the transformation described by Boyd (2007). This
transformation introduces a new non-terminal for every continuous block of a discontinuous constituent.
We keep a flag on each of the newly introduced nodes that indicates if it dominates the head daughter of
the original discontinuous node. Subsequently, we delete all those nodes for which this flag is false.2
For both POS tagging and parsing, we use the same split for training, development, and test. We use
the first half of the last 10 000 sentences in TiGer for development and the second half for testing. The
remaining 40 472 sentences are used for training. Accordingly, in order to ensure equal conditions, we
use the first 40 472 sentences in Tu?Ba-D/Z for training, and the first and second half of the following
10 000 sentences for development and testing. The remaining sentences in Tu?Ba-D/Z are not used.
4.2 POS Taggers
We employ six different POS tagger, each of them using a different tagging technique. Morfette (Chru-
pala et al., 2008), in its current implementation based on averaged Perceptron, is a tool designed for the
annotation of large morphological tagsets. Since none of the other POS taggers have access to lemmas,
we only provide full word forms to Morfette as well, which may inhibit its generalization capability.
The RF-Tagger (Schmid and Laws, 2008) assumes a tagset in a factorized version. I.e., the POS tag
VVFIN%3sis in sentence (2) would be represented as VVFIN.3.s.i.s, where the dots indicate different
subcategories, which are then treated separately by the POS tagger. It is based on a Markov model, but
the context size is determined by a decision tree. The Stanford tagger (Toutanova et al., 2003) is based
on a maximum entropy model, and SVMTool (Gime?nez and Ma`rquez, 2004) is based on support vector
machines. TnT (Brants, 2000; Brants, 1998), short for trigrams and tags, is a Markov model POS tagger.
2An implementation of all transformations is available at http://github.com/wmaier/treetools.
5
Beides
PIS
nsn
sind
VAFIN
3pis
Liedformen
NN
npf
,
$,
die
PRELS
np*
am
APPRART
dsn
Ende
NN
dsn
des
ART
gsn
achtzehnten
ADJA
gsn
Jahrhunderts
NN
gsn
die
ART
apn
ersten
ADJA
apn
Anzeichen
NN
apn
eines
ART
gsm
Verschmelzungsprozesses
NN
gsm
zeigen
VVFIN
3pis
.
$.
HD
NX
HD
VXFIN
HD
NX
HD
NX
HD
NX
HD
ADJX
HD
ADJX
- HD
NX
HD
VXFIN
PRED
VF
HD
LK
ON
MF
ON
C
- - HD
NX
- - HD
NX
HD
VC
HD -
NX
HD -
NX
- HD
PX
V-MOD OA
MF
- - -
R-SIMPX
ON-MO|
NF
- - - -
SIMPX
VROOT
Figure 2: A sentence from Tu?Ba-D/Z.
It uses an interpolation between uni-, bi- and trigrams as probability model. TnT has a sophisticated
mechanism for tagging unknown words. We also use Wapiti (Lavergne et al., 2010) a conditional ran-
dom field tagger. Since conditional random fields were developed for sequence tagging, this POS tagger
is expected to perform well.
All POS taggers are used with default settings. For the Stanford tagger, we use the bi-directional model
based on a context of 5 words. For SVMTool, we use the processing from left to right in combination
with features based on word and POS trigrams and word length, prefix and suffix information. Wapiti is
trained on uni-, bi-, and trigrams. Features used in training consist of tests concerning the alphanumeric,
upper or lower case characteristics, prefixes and suffixes of length three, and all possible POS tags for a
word.
For POS tagging evaluation, we use the script provided by TnT since it also allows us to calculate
accuracy on known and unknown words.
4.3 Parser
We use the Berkeley parser (Petrov and Klein, 2007b; Petrov and Klein, 2007a). We chose the Berke-
ley parser because we are aware of the fact that there are considerable differences in the tagset sizes,
which a plain PCFG parser cannot process successfully. The Berkeley parser split/merge capabilities
provide a way of smoothing over these differences. For parser evaluation, we use our own implementa-
tion of the PARSEVAL metrics.3 We report labeled precision (LP), labeled recall (LR), and the labeled
F-score(LF1). Note that the labeled evaluation does not only look at constituent labels but also at gram-
matical functions attached to the constituents, e.g. NP-SBJ for a subject NP. This is a considerably more
difficult task for German because of the relatively free word order. We also provide POS tagging ac-
curacy in the parse trees since the Berkeley parser adapts POS tags from the input if they do not fit its
syntax model.
5 POS Tagging Results
5.1 The Three Tagset Variants
The results for the POS tagging evaluation are shown in table 4. We are aware of the fact that the results
are not directly comparable across the different POS tagsets and across different treebanks since the
3The implementation is available at http://github.com/wmaier/evalb-lcfrs. Note that we evaluate the trees
as they are, i.e., we do not collapse or ignore tags.
6
TiGer Tu?Ba-D/Z
Tagset Tagger dev test dev test
UTS Morfette 98.51 98.09 98.25 98.49
RF-Tagger 97.89 97.41 97.69 97.96
Stanford 97.88 96.83 97.11 97.26
SVMTool 98.54 98.01 98.09 98.28
TnT 97.94 97.48 97.72 97.92
Wapiti 97.54 96.67 97.47 97.80
STTS Morfette 94.12 93.23 92.95 93.41
RF-Tagger 97.04 96.24 96.68 96.84
RF-Tagger (fact.) 97.05 96.26 96.69 96.85
Stanford 96.26 95.15 95.63 95.79
SVMTool 97.06 96.22 96.46 96.69
TnT 97.15 96.29 96.92 97.00
Wapiti 92.93 91.62 90.99 91.81
STTSmorph Morfette 82.71 80.10 81.19 82.26
RF-Tagger 86.56 83.90 85.68 86.31
Stanford ? ? ? ?
SVMTool 82.47 79.53 80.33 81.31
TnT 85.77 82.77 84.67 85.45
Wapiti 79.83 75.92 77.27 78.29
STTSmorph? STTS TnT 97.08 96.15 96.78 96.82
Table 4: POS tagging results using three versions of the German POS tagset and two treebanks.
corresponding tagging tasks differ in the level of difficulty. Any interpretation must therefore be taken
with a grain of salt, but we think that it is important to evaluate POS tagging on its own, especially
since it is not always the case that a larger label set automatically results in a more difficult task. The
results show that UTS, i.e., the variant with the least information, results in the highest POS tagging
results, between 96.67% and 98.54%. In tagging with the STTS, we reach a lower accuracy between
90.99% and 97.15%. When we include the morphological information, we reach considerably lower
results, between 75.92% and 86.56%. In other words, this shows that the more information there is in
the POS tagset, the harder the POS tagging task is. POS tagging with morphological information is the
most difficult task. We also see that there are no results for the Stanford POS tagger in the morphological
setting. We were unable to run these experiments, even when we used a high-memory cluster with access
to 120g of memory. It seems that the Stanford tagger is incapable of handling the large tagset sizes in the
setting using morphological information. Additionally, our assumption that the morphological tagset of
Tu?Ba-D/Z is less difficult to annotate because of its smaller tagset size is not borne out. The variation of
results on Tu?Ba-D/Z is often less than between the treebanks, across POS taggers.
If we compare the result of the different POS taggers, we see that for the different tagset variants,
different POS taggers perform best: For UTS, surprisingly, Morfette reaches the highest results, with
the exception of the TiGer development set, for which SVMTool performs slightly better. In general,
SVMTool is very close in accuracy to Morfette for this tagset variant. For STTS, TnT outperforms
all other POS taggers, and SVMTool is a close second. For STTSmorph, the RF-Tagger reaches the
highest results. For the RF-Tagger in combination with the STTS, we performed 2 experiments, one
using the standard STTS and one in which the STTS tags are factored, such that VVFIN is factored
into V.V.FIN. The latter variant reaches minimally higher results. In all settings, Wapiti is the weakest
approach; the difference between Wapiti and the best performing POS tagger reaches 6-7 percent points
for STTSmorph. This is rather surprising given that POS tagging is a typical sequence tagging task, for
which CRFs were developed.
Another fact worth mentioning is that there are considerable differences in POS tagging accuracy
7
TiGer Tu?Ba-D/Z
dev test dev test
Tagset Tagger Known Unkn. Known Unkn. Known Unkn. Known Unkn.
UTS Morfette 98.66 96.74 98.32 96.04 98.54 95.46 98.69 96.39
RF-Tagger 98.15 94.64 97.82 93.65 98.28 92.02 98.35 93.85
Stanford 99.05 91.85 98.78 87.70 98.94 79.30 98.92 79.69
SVMTool 98.81 95.26 98.41 94.45 98.63 92.89 98.66 94.27
TnT 98.06 96.50 97.67 95.74 98.07 94.28 98.25 95.25
Wapiti 98.94 80.71 98.51 80.04 98.68 85.79 98.83 86.91
STTS Morfette 94.42 90.60 93.56 90.24 93.17 90.83 93.59 91.57
RF-Tagger 97.80 87.92 97.30 86.71 97.62 87.59 97.73 87.52
RF-T. (fact.) 97.78 88.21 97.28 87.09 97.63 87.65 97.73 87.51
Stanford 98.16 73.56 97.75 71.60 97.96 73.04 97.97 72.64
SVMTool 97.86 87.41 97.26 86.82 97.50 86.47 97.60 87.05
TnT 97.80 89.25 97.21 87.95 97.65 89.78 97.72 89.33
Wapiti 94.51 73.78 93.48 74.83 93.21 69.45 93.71 71.74
STTSmorph Morfette 84.30 63.50 82.43 58.98 82.91 64.53 83.95 64.42
RF-Tagger 88.34 65.09 86.38 61.47 87.70 66.20 88.25 65.80
SVMTool 84.67 55.89 82.40 53.58 82.87 55.81 83.61 57.01
TnT 87.62 63.41 85.55 57.65 86.91 62.95 87.61 62.55
Wapiti 83.91 30.51 81.43 26.08 82.05 31.05 82.83 30.29
Table 5: Results for the different POS taggers for known and unknown words.
between the development and test set in both treebanks. For both STTS variants, these differences are
often larger than the differences between individual POS taggers on the same data set. Thus, in the
STTSmorph setting, the difference for TnT between the development and test set in TiGer is 3 percent
points while the differences between TnT and SVMTool and Morfette respectively are less.
One last question that we investigated concerns the effect of the morphological information on POS
tagging accuracy. We know that when we use morphological information, the POS tagging task is more
difficult. However, it is possible that the mistakes that occur concern only the morphological information
while the POS tags minus morphology may be predicted with equal or even higher accuracy. In order to
investigate this problem, we used the STTSmorph output of TnT and deleted all the morphological infor-
mation, thus leaving only the STTS POS tags. We then evaluated these POS tags against the gold STTS
tags. The results are shown in the last row in table 4, marked as STTSmorph? STTS. A comparison of
these results with the TnT results for STTS shows that the POS tagger reaches a higher accuracy when
trained directly on STTS rather than on STTSmorph, with a subsequent deletion of the morphological
information. This means that the morphological information is not useful but rather harmful in POS
tagging.
5.2 Evaluating on Known and Unknown Words
In a next set of experiments, we investigate how the different POS taggers perform on known and un-
known words. We define all words from the development and test set as known if the appear in the
training set. If they do not, they are considered unknown words. Note, however, that even if a word is
known, we still may not have the full set of POS tags in its ambiguity set. This is especially relevant for
the larger tagsets where the ambiguity rate per word is higher.
In TiGer, 7.64% of the words in the development set are unknown, 9.96% in the test set. In Tu?Ba-D/Z,
9.36% of the words in the development set are unknown, 8.64% in the test set. Note that this corresponds
to the levels of accuracy in table 4.
The results of the evaluation on known and unknown words are shown in table 5. These results show
that the Stanford POS tagger produces the highest accuracies for known words for UTS and STTS (note
8
TiGer Tu?Ba-D/Z
Morphology dev test dev test
STTS 97.15 96.29 96.92 97.00
STTSmorph 85.77 82.77 84.67 85.45
agreement 86.04 83.08 84.96 85.77
case 88.10 86.47 87.48 87.91
number 95.60 94.19 95.24 95.41
number + person 95.55 94.11 95.18 95.24
verbal features 97.03 96.02 96.55 96.44
Table 6: The results for TnT with different morphological variants.
that it could not be used for STTSmorph). For unknown words, Morfette reaches the highest results for
UTS and STTS, with TnT reaching the second highest results. For STTSmorph, the RF-Tagger reaches
the highest accuracy on both known and unknown words. The results for the RF-Tagger for STTS show
that the factored version performs better on unknown words than the standard one. It is also noticeable
that Wapiti, the CRF POS tagger, has the lowest performance on unknown words: For UTS, the results
are 10-16 percent points lower that the ones by Morfette; for STTS, the difference reaches 16-23 percent
points, and for STTSmorph, about 35 percent points. This shows that in order to reach a reasonable
accuracy rate, Wapiti?s unknown word handling model via regular expressions must be extended further.
However, note that Wapiti?s results on known words are also lower than the best performing system?s,
thus showing that CRFs are less well suited for POS tagging than originally expected.
5.3 Evaluating Morphological Variants
In this set of experiments, we investigate whether there are subsets of STTSmorph that are relevant for
parsing and that would allow us to reach higher POS tagging and parsing accuracies than on the full set
of morphological features. The subsets were chosen manually to model our intuition on which features
may be relevant for parsing. We investigate the following subsets: all agreement features, case only,
number only, number + person, and only verbal features. In this set of experiments, we concentrate on
TnT because it has been shown to be the most robust across the different settings. The results of these
experiments are shown in table 6. For comparison, we also list the results for the original STTS and
STTSmorph settings from table 4.
The results show that there are morphological subsets that allow reliable POS accuracies: If we use
verbal features, we reach results that are only slightly below the STTS results. For the subset using
number + person features, the difference is around 2 percent points. However, all subsets perform worse
than the STTS. The subsets that include case or all agreement features, which are the subsets most
relevant for parsing, reach accuracies that are slightly above STTSmorph, but still more than 10 percent
points below the original STTS.
6 Parsing Results
In this section, we report parsing results for TiGer in table 7 and for Tu?Ba-D/Z in table 8. We again
use the three POS tag variants as input, and we report results for 1) gold POS tags, 2) for tags assigned
by TnT, which proved to be the most reliable POS tagger across different settings, and 3) for POS tags
assigned by the Berkeley parser. Since the parser is known to alter POS tags given as input if they do not
fit the syntax model, we also report POS tagging accuracy. Note that this behavior of the parser explains
why we do not necessarily have a 100% POS tagging accuracy in the gold POS tag setting.
A first glance at the POS tagging results in the gold POS setting in tables 7 and 8 shows that for UTS
and STTS, the decrease in accuracy is minimal. In other words, the parser only changes a few POS tags.
When we compared the differences in POS tags between the output of the parser and the gold standard,
we found that most changes constitute a retagging of common nouns (NN) as proper nouns (NE). In
the STTSmorph setting, POS tagging accuracy is considerably lower, showing that the parser changed
9
dev test
Tag source Tagset POS LP LR LF1 POS LP LR LF1
gold UTS 100.00 77.97 77.23 77.60 99.97 71.80 70.26 71.02
STTS 99.98 78.09 77.55 77.82 99.97 71.90 71.11 71.50
STTSmorph 91.67 74.72 75.21 74.97 88.70 67.68 67.99 67.83
parser UTS 98.55 77.75 76.84 77.29 97.83 71.13 69.50 70.30
STTS 97.25 78.03 77.19 77.60 96.18 71.16 69.84 70.49
STTSmorph 83.06 75.53 75.24 75.39 79.05 67.67 67.02 67.34
TnT UTS 96.56 74.16 73.28 73.72 96.01 68.37 66.78 67.57
STTS 97.26 78.03 77.19 77.60 96.19 71.16 69.84 70.49
STTSmorph 77.94 73.06 72.69 72.88 75.05 65.43 64.78 65.10
Table 7: Parsing results for TiGer.
dev test
Tag source Tagset POS LP LR LF1 POS LP LR LF1
gold UTS 99.98 81.39 81.12 81.26 99.98 82.24 81.94 82.09
STTS 100.00 83.60 83.58 83.59 99.99 84.54 84.46 84.50
STTSmorph 89.75 82.27 78.85 80.53 90.55 83.57 79.91 81.70
parser UTS 98.35 79.97 79.61 79.79 98.58 81.07 80.66 80.87
STTS 97.20 81.84 81.65 81.74 97.39 82.93 82.78 82.85
STTSmorph 81.03 80.85 77.22 78.99 81.68 81.89 78.20 80.00
TnT UTS 98.35 79.97 79.61 79.79 98.58 81.07 80.66 80.87
STTS 97.21 81.84 81.65 81.74 97.39 82.93 82.78 82.85
STTSmorph 81.03 80.85 77.22 78.99 81.68 81.89 78.20 80.00
Table 8: Parsing results for Tu?Ba-D/Z.
between 8% (UTS) and 25% (STTSmorph) of the POS tags. This is a clear indication that the parser
suffers from data sparseness and has to adapt the POS tags in order to be able to parse the sentences.
We need to compare the POS tagging results based on automatically assigned POS tags; they show
the following trends: For TiGer in the STTS setting, the results based on TnT and on the parser are
very similar. For UTS and STTSmorph, the POS tags assigned by the parser reach a higher accuracy.
For Tu?Ba-D/Z, all the results are extremely similar.4 If we compare the POS tagging accuracies of the
parsed sentences and the accuracies of the original POS tags assigned by the tagger, we see that for
TiGer, the accuracy decreases by approximately 1.5 percent points for UTS, 0.1 percent points for STTS
and 9 percent points for STTSmorph. For Tu?Ba-D/Z, the loss in the STTSmorph setting is smaller, at
around 4 percent points. For UTS and STTS, there is a small improvement in POS tagging accuracy.
When we look at the parsing results, we see that gold POS tags always lead to the highest parsing
results, across treebanks and POS tagsets. We also see that across all conditions, the parsing results
for STTS are the highest. For TiGer, the results for UTS are only marginally lower, which seems to
indicate that some of the distinctions made in STTS are important, but not all of them. For Tu?Ba-D/Z,
the loss for UTS is more pronounced, at around 2 percent points. This suggests that for the Tu?Ba-D/Z
annotation scheme, the more fined grained distinctions in STTS are more important than for UTS. One
example would be the distinction between finite and infinite verbs, which is directly projected to the verb
group in Tu?Ba-D/Z (see the verb groups VXFIN and VXINF in figure 2). Note also that for Tu?ba-D/Z,
the parsing based on automatic POS tagging outperforms parsing based on gold UTS tags, thus again
confirming how important the granularity of STTS is for this treebank.
When we look at the parsing results for STTSmorph, it is obvious that this POS tagset variant leads
to the lowest parsing results, even in the gold POS setting. This means that even though agreement
4Because of the (almost) identical results, we checked our results with extreme care but could not find any errors.
10
information should be helpful for assigning grammatical functions, the information seems to be presented
to the parser in a form that it cannot exploit properly. We also performed preliminary experiments using
the morphological variants discussed in section 5.3 in parsing, but the results did not improve over the
STTS baseline.
When we compare the two sets of automatically assigned POS tags for TiGer, we see that the difference
in POS accuracy for UTS is 1.8 percent points while the difference in F-scores is 2.5 percent points. This
means that TnT tagging errors have a more negative impact on parsing quality than those in the POS
tags assigned by the parser itself. For STTSmorph, the difference is more pronounced in POS accuracy
(4 points as opposed to 2.2 in F-scores), which means that for STTSmorph, TnT errors are less harmful
than for UTS. We assume that this is the case because in many instances, the POS tags themselves will
be correct, and the error occurs in the morphological features. For Tu?Ba-D/Z, the difference between
UTS and STTSmorph is marginal; this is due to the fact that UTS results are much lower than for TiGer.
Thus, the difference between STTS and STTSmorph is stable across both treebanks.
A more in-depth investigation of the results shows that the aggregate EVALB score tends to hide indi-
vidual large differences between single sentences in the results. For example, in the results for the TiGer
dev set with gold POS tags, there are 119 sentences in STTSmorph which have an STTS counterpart with
an F-score that is at least 50 points higher. However, there are also 28 sentences for which the opposite
holds, i.e., for which STTSmorph wins over STTS. In Tu?Ba-D/Z, there are fewer sentences with such
extreme differences. There are 28 / 11 sentences with a score difference of 50 points or more between
STTS and STTSmorph in the Tu?Ba-D/Z development set, and vice versa. A manual inspection of the
results indicates that in some cases, the morphology is passed up into the tree and thereby contributes
to a correct grammatical function of a phrase label (such as for case information) while in other cases,
it causes an over-differentiation of grammatical functions and thereby has a detrimental effect (such as
for PPs, which are attached incorrectly). In the case of Tu?Ba-D/Z, this leads to trees with substructures
that are too flat, while in the case of TiGer, it leads to more hierarchical substructures. This finding is
corroborated by a further comparison of the number of edges produced by the parser, which reveals that
for the case of TiGer, the number of edges grows with the size of the POS tagset, while for the case of
Tu?Ba-D/Z, the number of edges produced with STTS is higher than with UTS, but drops considerably
for STTSmorph. The large differences in results for single sentences look more pronounced in TiGer due
to the average number of edges per sentence (7.60/8.72 for dev/test gold), which is much lower than for
Tu?Ba-D/Z (20.93/21.16 for dev/test gold); in other words, because of its flat annotation. We suspect that
there is data sparsity involved, but this needs to be investigated further.
7 Conclusion and Future Work
We have investigated how the granularity of POS tags influences POS tagging, and furthermore, how POS
tagging performance relates to parsing results, on the basis of experiments on two German treebanks,
using three POS tagsets of different granularity (UTS, STTS, and STTSmorph), and six different POS
taggers, together with the Berkeley parser.
We have shown that the tagging task is easier the less granular the tagset is. Furthermore, we have
shown that both too coarse-grained and too fine-grained distinctions on POS level hurt parsing perfor-
mance. The results for the morphological tagset are thus in direct contrast to previous studies, such as
(Dehdari et al., 2011; Marton et al., 2013; Seddah et al., 2009; Sza?nto? and Farkas, 2014), which show
for different languages that adding morphological information increases parsing accuracy. Surprisingly,
given the STTS tagset, the Berkeley parser itself was able to deliver a POS tagging performance which
was almost identical to the performance of the best tagger, TnT. Additionally, we can conclude that the
choice of the tagset and of the best POS tagger for a given treebank does not only depend on the language
but also on the annotation scheme.
In future work, we will undertake a systematic investigation of tag clustering methods in order to find a
truly optimally granular POS tagset. We will also investigate the exact relation between annotation depth
and the granularity of the POS tagset with regard to parsing accuracy and data sparsity. The latter may
elucidate reasons behind the differences between our results and those of the studies mentioned above.
11
References
Bernd Bohnet and Joakim Nivre. 2012. A transition-based system for joint part-of-speech tagging and labeled
non-projective dependency parsing. In Proceedings of the Joint Conference on Empirical Methods in Natural
Language Processing and Computational Natural Language Learning (EMNLP-CoNLL), pages 1455?1465,
Jeju Island, Korea.
Bernd Bohnet. 2010. Very high accuracy and fast dependency parsing is not a contradiction. In Proceedings of
the 23rd International Conference on Computational Linguistics (IJCNLP), pages 89?97, Beijing, China.
Adriane Boyd. 2007. Discontinuity revisited: An improved conversion to context-free representations. In Pro-
ceedings of The Linguistic Annotation Workshop (LAW) at ACL 2007, pages 41?44, Prague, Czech Republic.
Sabine Brants, Stefanie Dipper, Silvia Hansen, Wolfgang Lezius, and George Smith. 2002. The TIGER treebank.
In Proceedings of the First Workshop on Treebanks and Linguistic Theories (TLT), pages 24?41, Sozopol,
Bulgaria.
Thorsten Brants, 1998. TnT?A Statistical Part-of-Speech Tagger. Universita?t des Saarlandes, Computational
Linguistics, Saarbru?cken, Germany.
Thorsten Brants. 2000. TnT?a statistical part-of-speech tagger. In Proceedings of the 1st Conference of the North
American Chapter of the Association for Computational Linguistics and the 6th Conference on Applied Natural
Language Processing (ANLP/NAACL), pages 224?231, Seattle, WA.
Marie Candito and Djame? Seddah. 2010. Parsing word clusters. In Proceedings of the NAACL HLT 2010 First
Workshop on Statistical Parsing of Morphologically-Rich Languages, pages 76?84, Los Angeles, CA.
Xiao Chen and Chunyu Kit. 2011. Improving part-of-speech tagging for context-free parsing. In Proceedings of
5th International Joint Conference on Natural Language Processing (IJCNLP), pages 1260?1268, Chiang Mai,
Thailand.
Grzegorz Chrupala, Georgiana Dinu, and Josef van Genabith. 2008. Learning morphology with Morfette. In
Proceedings the Fifth International Conference on Language Resources and Evaluation (LREC), Marrakech,
Morocco.
Michael Collins, Jan Hajic?, Lance Ramshaw, and Christoph Tillmann. 1999. A statistical parser for Czech. In
Proceedings of the 37th Annual Meeting of the Association for Computational Linguistics, pages 505?512,
College Park, MD.
Michael Collins. 1999. Head-Driven Statistical Models for Natural Language Parsing. Ph.D. thesis, University
of Pennsylvania, Philadelphia, PA.
Michael Daum, Kilian Foth, and Wolfgang Menzel. 2003. Constraint based integration of deep and shallow
parsing techniques. In Proceedings of the 10th Conference of the European Chapter of the Association for
Computational Linguistics (EACL), Budapest, Hungary.
Jon Dehdari, Lamia Tounsi, and Josef van Genabith. 2011. Morphological features for parsing morphologically-
rich languages: A case of Arabic. In Proceedings of the Second Workshop on Statistical Parsing of Morpholog-
ically Rich Languages, pages 12?21, Dublin, Ireland.
Kilian Foth, Michael Daum, and Wolfgang Menzel. 2005. Parsing unrestricted German text with defeasible
constraints. In H. Christiansen, P. R. Skadhauge, and J. Villadsen, editors, Constraint Solving and Language
Processing, pages 140?157. Springer.
Jesu?s Gime?nez and Llu??s Ma`rquez. 2004. SVMTool: A general POS tagger generator based on Support Vector
Machines. In Proceedings of the 4th International Conference on Language Resources and Evaluation (LREC),
pages 43?46, Lisbon, Portugal.
Yoav Goldberg and Reut Tsarfaty. 2008. A single generative model for joint morphological segmentation and
syntactic parsing. In Proceedings of The 46th Annual Meeting of the Association for Computational Linguistics:
Human Language Technologies (ACL:HLT), pages 371?379, Columbus, OH.
Jun Hatori, Takuya Matsuzaki, Yusuke Miyao, and Jun?ichi Tsujii. 2011. Incremental joint POS tagging and
dependency parsing in Chinese. In Proceedings of 5th International Joint Conference on Natural Language
Processing (IJCNLP), pages 1216?1224, Chiang Mai, Thailand.
Geoffrey Hinton. 1999. Products of experts. In Proceedings of the Ninth International Conference on Artificial
Neural Networks, pages 1?6, Stockholm, Sweden.
12
Terry Koo, Xavier Carreras, and Michael Collins. 2008. Simple semi-supervised dependency parsing. In Pro-
ceedings of The 46th Annual Meeting of the Association for Computational Linguistics: Human Language
Technologies (ACL:HLT), pages 595?603, Columbus, OH.
Sandra Ku?bler and Wolfgang Maier. 2013. U?ber den Einfluss von Part-of-Speech-Tags auf Parsing-Ergebnisse.
Journal for Language Technology and Computational Linguistics. Special Issue on ?Das Stuttgart-Tu?bingen
Wortarten-Tagset ? Stand und Perspektiven?, 28(1):17?44.
Sandra Ku?bler, Erhard W. Hinrichs, and Wolfgang Maier. 2006. Is it really that difficult to parse German? In
Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages
111?119, Sydney, Australia.
Corrin Lakeland. 2005. Lexical Approaches to Backoff in Statistical Parsing. Ph.D. thesis, University of Otago,
New Zealand.
Thomas Lavergne, Olivier Cappe?, and Franc?ois Yvon. 2010. Practical very large scale CRFs. In Proceedings
the 48th Annual Meeting of the Association for Computational Linguistics (ACL), pages 504?513, Uppsala,
Sweden.
Joseph Le Roux, Benoit Sagot, and Djame? Seddah. 2012. Statistical parsing of Spanish and data driven lemma-
tization. In Proceedings of the ACL 2012 Joint Workshop on Statistical Parsing and Semantic Processing of
Morphologically Rich Languages, pages 55?61, Jeju, Republic of Korea.
Wolfgang Maier, Miriam Kaeshammer, and Laura Kallmeyer. 2012. Data-driven PLCFRS parsing revisited:
Restricting the fan-out to two. In Proceedings of the Eleventh International Conference on Tree Adjoining
Grammars and Related Formalisms (TAG+11), Paris, France.
Yuval Marton, Nizar Habash, and Owen Rambow. 2013. Dependency parsing of Modern Standard Arabic with
lexical and inflectional features. Computational Linguistics, 39(1):161?194.
Slav Petrov and Dan Klein. 2007a. Improved inference for unlexicalized parsing. In Proceedings of Human Lan-
guage Technologies 2007: The Conference of the North American Chapter of the Association for Computational
Linguistics, pages 404?411, Rochester, NY.
Slav Petrov and Dan Klein. 2007b. Learning and inference for hierarchically split PCFGs. In Proceedings of
AAAI (Nectar Track), Vancouver, Canada.
Slav Petrov and Dan Klein. 2008. Parsing German with language agnostic latent variable grammars. In Proceed-
ings of the ACL Workshop on Parsing German, pages 33?39, Columbus, OH.
Slav Petrov, Dipanjan Das, and Ryan McDonald. 2012. A universal part-of-speech tagset. In Proceedings of the
Eight International Conference on Language Resources and Evaluation (LREC), Istanbul, Turkey.
Adwait Ratnaparkhi. 1996. A maximum entropy model for part-of-speech tagging. In Proceedings of the Empiri-
cal Methods in Natural Language Processing Conference (EMNLP), pages 133?142, Philadelphia, PA.
Anne Schiller, Simone Teufel, and Christine Thielen. 1995. Guidelines fu?r das Tagging deutscher Textkorpora mit
STTS. Technical report, Universita?t Stuttgart and Universita?t Tu?bingen.
Helmut Schmid and Florian Laws. 2008. Estimation of conditional probabilities with decision trees and an
application to fine-grained POS tagging. In Proceedings of the 22nd International Conference on Computational
Linguistics (COLING), pages 777?784, Manchester, UK.
Djame? Seddah, Marie Candito, and Beno??t Crabbe?. 2009. Cross parser evaluation: A French Treebanks study.
In Proceedings of the 11th International Conference on Parsing Technologies (IWPT), pages 150?161, Paris,
France.
Wolfgang Seeker and Jonas Kuhn. 2013. Morphological and syntactic case in statistical dependency parsing.
Computational Linguistics, 39(1):23?55.
Zsolt Sza?nto? and Richa?rd Farkas. 2014. Special techniques for constituent parsing of morphologically rich lan-
guages. In Proceedings of the 14th Conference of the European Chapter of the Association for Computational
Linguistics (EACL), pages 135?144, Gothenburg, Sweden.
Heike Telljohann, Erhard W. Hinrichs, Sandra Ku?bler, Heike Zinsmeister, and Kathrin Beck, 2012. Stylebook for
the Tu?bingen Treebank of Written German (Tu?Ba-D/Z). Seminar fu?r Sprachwissenschaft, Universita?t Tu?bingen,
Germany.
13
Kristina Toutanova and Christopher D. Manning. 2000. Enriching the knowledge sources used in a maximum
entropy part-of-speech tagger. In Proceedings of the Joint SIGDAT Conference on Empirical Methods in Natural
Language Processing and Very Large Corpora (EMNLP/VLC), Hong Kong.
Kristina Toutanova, Dan Klein, Christopher Manning, and Yoram Singer. 2003. Feature-rich part-of-speech
tagging with a cyclic dependency network. In Proceedings of the Human Language Technology Conference of
the North American Chapter of the Association for Computational Linguistics (HLT-NAACL), pages 252?259,
Edmonton, Canada.
Yannick Versley and Ines Rehbein. 2009. Scalable discriminative parsing for German. In Proceedings of the 11th
International Conference on Parsing Technologies (IWPT), pages 134?137, Paris, France.
Yannick Versley. 2005. Parser evaluation across text types. In Fourth Workshop on Treebanks and Linguistic
Theories (TLT 2005), Barcelona, Spain.
14
First Joint Workshop on Statistical Parsing of Morphologically Rich Languages
and Syntactic Analysis of Non-Canonical Languages, pages 103?109 Dublin, Ireland, August 23-29 2014.
Introducing the SPMRL 2014 Shared Task on Parsing
Morphologically-Rich Languages
Djame? Seddah
INRIA & Univ. Paris Sorbonne
Paris, France
djame.seddah@paris-sorbonne.fr
Sandra Ku?bler
Indiana University
Bloomington, IN, USA
skuebler@indiana.edu
Reut Tsarfaty
Weizman Institute
Rehovot, Israel
reut.tsarfaty@weizmann.ac.il
1 Introduction
This first joint meeting on Statistical Parsing of Morphologically Rich Languages and Syntactic Analysis
of Non-Canonical English (SPMRL-SANCL) featured a shared task on statistical parsing of morpholog-
ically rich languages (SPMRL). The goal of the shared task is to allow to train and test different partic-
ipating systems on comparable data sets, thus providing an objective measure of comparison between
state-of-the-art parsing systems on data data sets from a range of different languages. This 2014 SPMRL
shared task is a continuation and extension of the SPMRL shared task, which was co-located with the
SPMRL meeting at EMNLP 2013 (Seddah et al., 2013).
This paper provides a short overview of the 2014 SPMRL shared task goals, data sets, and evaluation
setup. Since the SPMRL 2014 largely builds on the infrastructure established for the SPMRL 2013
shared task, we start by reviewing the previous shared task (?2) and then proceed to the 2014 SPMRL
evaluation settings (?3), data sets (?4), and a task summary (?5). Due to organizational constraints,
this overview is published prior to the submission of all system test runs, and a more detailed overview
including the description of participating systems and the analysis of their results will follow as part of
(Seddah et al., 2014), once the shared task is completed.
2 The SPMRL Shared Task 2013
The SPMRL Shared Task 2013 (Seddah et al., 2013) was organized with the goal of providing standard
data sets, streamlined evaluation metrics, and a set of strong baselines for parsing morphologically rich
languages (MRLs). The goals were both to provide a focal point for researchers interested in parsing
MRLs and consequently to advance the state of the art in this area of research.
The shared task focused on parsing nine morphologically rich languages, from different typological
language families, in both a constituent-based and a dependency-based format. The set of nine typolog-
ically diverse languages comprised data sets for Arabic, Basque, French, German, Hebrew, Hungarian,
Korean, Polish, and Swedish. Compared to previous multilingual shared tasks (Buchholz and Marsi,
2006; Nivre et al., 2007), the SPMRL shared task targeted parsing in realistic evaluation scenarios, in
which the analysis of morphologically ambiguous input tokens is not known in advance. An additional
novelty of the SPMRL shared task is that it allowed for both a dependency-based and a constituent-
based parse representation. This setting relied on an intricate and careful data preparation process which
ensured consistency between the constituent and the dependency version by aligning the two representa-
tion types at the token level and at the level of part-of-speech tags. For all languages, we provided two
versions of the data sets: an all data set, identical in size to the one made available by the individual
treebank providers, and a small data set, with a training set of 5,000 sentences, and a test set of about 500
sentences. Controlling the set sizes across languages allows us to level the playing field across languages
and treebanks.
This work is licenced under a Creative Commons Attribution 4.0 International License. Page numbers and proceedings footer
are added by the organizers. License details: http://creativecommons.org/licenses/by/4.0/
103
The shared task also advanced the state of the art by introducing different levels of complexity in
parsing. In general, parsing is reduced to the parsing proper step, assuming gold segmentation of the
text into sentences and words as well as gold POS tags and morphological analyses. This is a serious
simplification of the task since especially in Semitic languages, the segmentation into input tokens is a
task that is best performed in combination with parsing because of the ambiguities involved.
The shared task deviated from this standard configuration by adding conditions in which more realistic
settings were given: In the gold setting, unambiguous gold morphological segmentation, POS tags, and
morphological features for each input token were given. In the predicted setting, disambiguated morpho-
logical segmentation was provided, but the POS tags and morphological features for each input segment
were not. In the raw setting, there was no gold information, i.e., morphological segmentation, POS tags
and morphological features for each input token had to be predicted as part of the parsing task. To lower
the entry cost, participants were provided with reasonable baseline (if not state-of-the-art) morphological
predictions (either disambiguated ? in most cases? or ambiguous prediction in lattice forms).
As a consequence of the raw scenario, it was not possible to (only) rely on the accepted parsing met-
rics, labeled bracket evaluation via EVALB1 (Black et al., 1991), Leaf-Ancestor (Sampson and Babarczy,
2003) for constituents and CONLL X?s Labeled/Unlabeled Attachment Score for dependencies (Buch-
holz and Marsi, 2006). When the segmentation of words into input tokens is not given, there may be
discrepancies on the lexical levels, which neither EVALB and LEAF-ANCESTOR nor LAS/UAS are pre-
pared to handle. Thus, we also used TedEval, a distance-based metric that evaluates a morphosyntactic
structure as a complete whole (Tsarfaty et al., 2012b). Note that given the workload brought to the par-
ticipants, we did not try to enforce function label evaluation for constituent parsing. We hope that further
shared tasks will try to generalize such an evaluation. Indeed, having predicted function labels would
ease labeled TEDEVAL evaluation and favor a full parsing chain evaluation. Nevertheless, the choice of
TEDEVAL allowed us to go beyond the standard cross-parser evaluation within one setting and approach
cross-framework (constituent vs. dependency (Tsarfaty et al., 2012a)) and cross-language evaluation,
thus pushing the envelope on parsing evaluation. Additionally, we performed a specialized evaluation of
multi-word expressions in the French treebank.
The SPMRL Shared Task 2013 featured seven teams who approached the dependency parsing task
and one team that approached constituent parsing. The best performing system (Bjo?rkelund et al., 2013)
in either framework consisted of an ensemble system, combining several dependency parsers or sev-
eral instantiations of a PCFG-LA parser by a (re-)ranker, both on top of state-of-the-art morphological
analyses. The results show that parser combination helps to reach a robust performance across lan-
guages. However, the integration of morphological analysis into the parsing needs to be investigated
thoroughly, and new, morphologically aware approaches are needed. The cross-parser, cross-scenario,
and cross-framework evaluation protocols show that performance on gold morphological input is signif-
icantly higher than that in more realistic scenarios, and more training data is beneficial. Additionally,
differences between dependency and constituents are smaller than previously assumed, and languages
which are typologically farthest from English, such as Semitic and Asian languages, are still amongst
the hardest to parse, regardless of the parsing method used.
3 SPMRL 2014 Parsing Scenarios
As in the previous edition, this year, we consider three parsing scenarios, depending on how much of the
morphological information is provided. The scenarios are listed below, in increasing order of difficulty.
? Gold: In this scenario, the parser is provided with unambiguous gold morphological segmentation,
POS tags, and morphological features for each input token.
? Predicted: In this scenario, the parser is provided with disambiguated morphological segmentation.
However, the POS tags and morphological features for each input segment are unknown.
? Raw: In this scenario, the parser is provided with morphologically ambiguous input. The morpho-
logical segmentation, POS tags, and morphological features for each input token are unknown.
1We extended the usualEVALB to penalize unparsed sentences.
104
Scenario Segmentation PoS+Feat. Tree
Gold X X ?
Predicted X 1-best ?
Raw (1-best) 1-best 1-best ?
Raw (all) ? ? ?
Table 1: A summary of the parsing and evaluation scenarios. X depicts gold information, ? depicts
unknown information, to be predicted by the system.
The Predicted and Raw scenarios require predicting morphological analyses. This may be done using
a language-specific morphological analyzer, or it may be done jointly with parsing. We provide inputs
that support these different scenarios:
? Predicted: Gold treebank segmentation is given to the parser. The POS tags assignment and mor-
phological features are automatically predicted by the parser or by an external resource.
? Raw (1-best): The 1-best segmentation and POS tags assignment is predicted by an external re-
source and given to the parser.
? Raw (all): All possible segmentations and POS tags are specified by an external resource. The
parser selects jointly a segmentation and a tree.
An overview of all scenarios is shown in table 1. For languages in which terminals equal tokens, only
Gold and Predicted scenarios are considered. For the Semitic languages, we further provide input for
both Raw (1-best) and Raw (all) scenarios.2
4 SPMRL 2014 Data Sets
The main innovation of the SPMRL 2014 shared task with respect to the previous edition is the availabil-
ity of additional, unannotated data, for the purpose of semi-supervised training. This section provides
a description of the unlabeled-data preparation that is required in the context of parsing MRLs, and the
core labeled data that is used in conjunction with it.
4.1 SPMRL Unlabeled Data Set
One of the common problems when dealing with morphologically rich languages (MRLs) is lexical data
sparseness due to the high level of variation in word forms (Tsarfaty et al., 2010; Tsarfaty et al., 2012c).
The use of large, unlabeled corpora in a semi-supervised setting, in addition to the relatively small MRL
data sets, can become a valid option to overcome such issues. For instance, using Brown clusters (Brown
et al., 1992) has been shown to boost the performance of a PCFG-LA based parser for French (Candito
and Crabbe?, 2009; Candito and Seddah, 2010). External lexical acquisition was successfully used for
Arabic (Habash, 2008) and Hebrew (Goldberg et al., 2009), self-training increased accuracy for parsing
German (Rehbein, 2011), and more recently, the use of word embeddings led to some promising results
for some MRLs (Cirik and S?ensoy, 2013).
By releasing large, unlabeled data sets and by providing accurate pre-annotation in a format directly
compatible with models trained on the SPMRL Shared Task treebanks, we hope to foster the development
of interesting and feature-rich parsing models that build on larger, morphologically rich, lexicons. Table
2 presents basic facts about the data sets. Details on the unlabeled data and their pre-annotations will
be provided in (Seddah et al., 2014). Note that we could not ensure the same volume of data for all
languages, nor we could run the same parser, or morphology prediction, on all data. Potential future work
could focus on ensuring a stricter level of comparability of these data or on investigating the feasibility
of such a normalization of procedures.
2The raw Arabic lattices were made available later than the other data. They are now included in the shared task release.
105
Language Source (main) type size (tree tokens) morph parsed
Arabic news domain news 120M X* X*
Basque web balanced 150M X X
French news domain newswire 120M X+mwe X*
German Wikipedia wiki (edited) 205M X X
Hebrew Wikipedia wiki (edited) 160M X X
Hungarian news domain newswire 100M X X
Korean news domain newswire 40M X X*
Polish Wikipedia wiki (edited) 100M X X
Swedish PAROLE balanced 24M X X
Table 2: Unlabeled data set properties.*: made available mid-july
4.2 SPMRL Core Labeled Data Set
In order to provide a faithful evaluation of the impact of these additional sets of unlabeled data, we used
the exact same data sets for training and testing as in the previous edition. Specifically, we used an Arabic
data set, originally provided by the LDC (Maamouri et al., 2004), in a dependency form, derived from the
Columbia Catib Treebank (Habash and Roth, 2009; Habash et al., 2009) and in a constituency instance,
following the Stanford pre-processing scheme (Green and Manning, 2010) and extended according to the
SPMRL 2013 extension scheme (Seddah et al., 2013). For Basque, the data was provided by Aduriz et
al. (2003) in both dependency and constituency, we removed sentences with non-projective trees so both
instances could be aligned at the token level. Regarding French, we used a new instance of the French
Treebank (Abeille? et al., 2003) that includes multi-word expression (MWE) annotations, annotated at the
morpho-syntactic level in both instances. Predicted MWEs were added this year, using the same tools as
Constant et al. (2013). The German data are based on the Tiger corpus (Brants et al., 2002), and converted
to constituent and dependency following (Seeker and Kuhn, 2012). The Hebrew data set is based on the
Modern Hebrew Treebank (Sima?an et al., 2001), with the Goldberg (2011) dependency version, in turn
aligned with the phrase structure instance described in (Tsarfaty, 2010; Tsarfaty, 2013). Note that in
order to match the Hebrew unlabeled data encoding, the Hebrew treebank was converted back to UTF-8.
The Hungarian data are derived from the Szeged treebank (Csendes et al., 2005; Vincze et al., 2010),
while the Korean data originate from the Kaist Treebank (Choi et al., 1994) which was converted to
dependency for the SPMRL shared task by Choi (2013). The Polish treebank we used is described in
(Wolin?ski et al., 2011; S?widzin?ski and Wolin?ski, 2010; Wro?blewska, 2012). Compared to the last year?s
edition, we added explicit feature names in the relevant data fields. The Swedish data originate from
(Nivre et al., 2006), we added function labels extracted from the original Swedish XML data. Note
that in addition to constituency and dependency versions, the Polish, German and Swedish data sets are
also available in the Tiger XML format (Mengel and Lezius, 2000), allowing a direct representation of
discontinuous structures in their phrase-based structures.
5 Conclusion
At the time of writing this short introduction, the shared task is ongoing, and neither results nor the final
submitting teams are known. At this point, we can say that 15 teams registered for the 2014 shared
task edition, indicating an increased awareness of and continued interest in the topic of the shared task.
Results, cross-parser and cross-data analysis, and shared task description papers will be made available
at http://www.spmrl.org/spmrl2014-sharedtask.html.
Acknowledgments
We would like to express our gratitude to the original treebank labeled and unlabeled data contribu-
tors for the considerable time they devoted to our shared task. Namely, Arabic: Nizar Habash, Ryan
Roth (Columbia University); Spence Green (Stanford University) , Ann Bies, Seth Kulick, Mohamed
Maamouri (the Linguistic Data Consortium) ; Basque: Koldo Gojenola, Iakes Goenaga (University of
the Basque Country) ; French: Marie Candito (Univ. Paris 7 & Inria), Djame? Seddah (Univ. Paris
Sorbonne & Inria) , Matthieu Constant (Univ. Marne la Valle?e) ; German: Wolfgang Seeker (IMS
106
Stuttgart), Wolfgang Maier (Univ. of Dusseldorf), Yannick Versley (Univ. of Tuebingen) ; Hebrew:
Yoav Goldberg (Bar Ilan Univ.), Reut Tsarfaty (Weizmann Institute of Science) ; Hungarian: Richa`rd
Farkas, Veronika Vincze (Univ. of Szeged) ; Korean: Jinho D. Choi (Univ. of Massachusetts Amherst),
Jungyeul Park (Kaist); Polish: Adam Przepio?rkowski, Marcin Wolin?ski, Alina Wro?blewska (Institute
of Computer Science, Polish Academy of Sciences) ; Swedish: Joakim Nivre (Uppsala Univ.), Marco
Kuhlmann (Linko?ping University).
We gratefully acknowledge the contribution of Spra?kbanken and the University of Gothenburg for
providing the PAROLE corpus. We are also very grateful to the Philosophical Faculty of the Heinrich-
Heine Universita?t Du?sseldorf for hosting the shared task data via their dokuwiki.
References
Anne Abeille?, Lionel Cle?ment, and Franc?ois Toussenel. 2003. Building a treebank for French. In Anne Abeille?,
editor, Treebanks. Kluwer, Dordrecht.
I. Aduriz, M. J. Aranzabe, J. M. Arriola, A. Atutxa, A. D??az de Ilarraza, A. Garmendia, and M. Oronoz. 2003.
Construction of a Basque dependency treebank. In Proceedings of the Second Workshop on Treebanks and
Linguistic Theories, pages 201?204, Va?xjo?, Sweden.
Anders Bjo?rkelund, Ozlem Cetinoglu, Richa?rd Farkas, Thomas Mueller, and Wolfgang Seeker. 2013. (Re)ranking
meets morphosyntax: State-of-the-art results from the SPMRL 2013 shared task. In Proceedings of the Fourth
Workshop on Statistical Parsing of Morphologically-Rich Languages, pages 134?144, Seattle, WA.
Ezra Black, Steven Abney, Dan Flickinger, Claudia Gdaniec, Ralph Grishman, Philip Harrison, Donald Hindle,
Robert Ingria, Frederick Jelinek, Judith Klavans, Mark Liberman, Mitchell Marcus, Salim Roukos, Beatrice
Santorini, and Tomek Strzalkowski. 1991. A procedure for quantitatively comparing the syntactic coverage
of English grammars. In Proceedings of the DARPA Speech and Natural Language Workshop 1991, pages
306?311, Pacific Grove, CA.
Sabine Brants, Stefanie Dipper, Silvia Hansen, Wolfgang Lezius, and George Smith. 2002. The TIGER treebank.
In Proceedings of the First Workshop on Treebanks and Linguistic Theories (TLT), pages 24?41, Sozopol,
Bulgaria.
Peter F. Brown, Vincent J. Della, Peter V. Desouza, Jennifer C. Lai, and Robert L. Mercer. 1992. Class-based
n-gram models of natural language. Computational Linguistics, 18(4):467?479.
Sabine Buchholz and Erwin Marsi. 2006. CoNLL-X shared task on multilingual dependency parsing. In Proceed-
ings of CoNLL, pages 149?164, New York, NY.
Marie Candito and Beno??t Crabbe?. 2009. Improving generative statistical parsing with semi-supervised word
clustering. In Proceedings of the 11th International Conference on Parsing Technologies (IWPT?09), pages
138?141, Paris, France.
Marie Candito and Djame? Seddah. 2010. Parsing word clusters. In Proceedings of the NAACL/HLT Workshop on
Statistical Parsing of Morphologically Rich Languages (SPMRL 2010), Los Angeles, CA.
Key-sun Choi, Young S. Han, Young G. Han, and Oh W. Kwon. 1994. KAIST Tree Bank Project for Korean:
Present and Future Development. In In Proceedings of the International Workshop on Sharable Natural Lan-
guage Resources, pages 7?14, Nara, Japan.
Jinho D. Choi. 2013. Preparing Korean data for the shared task on parsing morphologically rich languages.
arXiv:1309.1649.
Volkan Cirik and Hu?snu? S?ensoy. 2013. The AI-KU system at the SPMRL 2013 shared task: Unsupervised features
for dependency parsing. In Proceedings of the Fourth Workshop on Statistical Parsing of Morphologically-Rich
Languages, pages 68?75, Seattle, WA.
Matthieu Constant, Marie Candito, and Djame? Seddah. 2013. The LIGM-Alpage architecture for the SPMRL
2013 shared task: Multiword expression analysis and dependency parsing. In Proceedings of the Fourth Work-
shop on Statistical Parsing of Morphologically-Rich Languages, pages 46?52, Seattle, WA.
Do?ra Csendes, Ja?nos Csirik, Tibor Gyimo?thy, and Andra?s Kocsor. 2005. The Szeged treebank. In Proceedings
of the 8th International Conference on Text, Speech and Dialogue (TSD), Lecture Notes in Computer Science,
pages 123?132, Berlin / Heidelberg. Springer.
107
Yoav Goldberg, Reut Tsarfaty, Meni Adler, and Michael Elhadad. 2009. Enhancing unlexicalized parsing per-
formance using a wide coverage lexicon, fuzzy tag-set mapping, and EM-HMM-based lexical probabilities.
In Proceedings of the 12th Conference of the European Chapter of the ACL (EACL), pages 327?335, Athens,
Greece.
Yoav Goldberg. 2011. Automatic syntactic processing of Modern Hebrew. Ph.D. thesis, Ben Gurion University of
the Negev.
Spence Green and Christopher D. Manning. 2010. Better Arabic parsing: Baselines, evaluations, and analysis. In
Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 394?402,
Beijing, China.
Nizar Habash and Ryan Roth. 2009. CATiB: The Columbia Arabic Treebank. In Proceedings of ACL-IJCNLP,
pages 221?224, Suntec, Singapore.
Nizar Habash, Reem Faraj, and Ryan Roth. 2009. Syntactic Annotation in the Columbia Arabic Treebank. In
Proceedings of MEDAR International Conference on Arabic Language Resources and Tools, Cairo, Egypt.
Nizar Habash. 2008. Four techniques for online handling of out-of-vocabulary words in Arabic-English statistical
machine translation. In Proceedings of ACL-08: HLT, Short Papers, pages 57?60, Columbus, OH.
Mohamed Maamouri, Ann Bies, Tim Buckwalter, and Wigdan Mekki. 2004. The Penn Arabic treebank: Build-
ing a large-scale annotated Arabic corpus. In Proceedings of NEMLAR International Conference on Arabic
Language Resources and Tools, pages 102?109, Cairo, Egypt.
Andreas Mengel and Wolfgang Lezius. 2000. An XML-based encoding format for syntactically annotated cor-
pora. In Proceedings of the Second International Conference on Language Resources and Engineering (LREC
2000), pages 121?126, Athens, Greece.
Joakim Nivre, Jens Nilsson, and Johan Hall. 2006. Talbanken05: A Swedish treebank with phrase structure and
dependency annotation. In Proceedings of LREC, pages 1392?1395, Genoa, Italy.
Joakim Nivre, Johan Hall, Sandra Ku?bler, Ryan McDonald, Jens Nilsson, Sebastian Riedel, and Deniz Yuret.
2007. The CoNLL 2007 shared task on dependency parsing. In Proceedings of the CoNLL Shared Task Session
of EMNLP-CoNLL 2007, pages 915?932, Prague, Czech Republic.
Ines Rehbein. 2011. Data point selection for self-training. In Proceedings of the Second Workshop on Statistical
Parsing of Morphologically Rich Languages, pages 62?67, Dublin, Ireland.
Geoffrey Sampson and Anna Babarczy. 2003. A test of the leaf-ancestor metric for parse accuracy. Natural
Language Engineering, 9(04):365?380.
Djame? Seddah, Reut Tsarfaty, Sandra Ku?bler, Marie Candito, Jinho D. Choi, Richa?rd Farkas, Jennifer Foster, Iakes
Goenaga, Koldo Gojenola Galletebeitia, Yoav Goldberg, Spence Green, Nizar Habash, Marco Kuhlmann, Wolf-
gang Maier, Joakim Nivre, Adam Przepio?rkowski, Ryan Roth, Wolfgang Seeker, Yannick Versley, Veronika
Vincze, Marcin Wolin?ski, Alina Wro?blewska, and Eric Villemonte de la Clergerie. 2013. Overview of the
SPMRL 2013 shared task: A cross-framework evaluation of parsing morphologically rich languages. In Pro-
ceedings of the Fourth Workshop on Statistical Parsing of Morphologically-Rich Languages, pages 146?182,
Seattle, WA.
Djame? Seddah, Reut Tsarfaty, Sandra Ku?bler, Marie Candito, Jinho Choi, Matthieu Constant, Richa?rd Farkas,
Iakes Goenaga, Koldo Gojenola, Yoav Goldberg, Spence Green, Nizar Habash, Marco Kuhlmann, Wolfgang
Maier, Joakim Nivre, Adam Przepiorkowski, Ryan Roth, Wolfgang Seeker, Yannick Versley, Veronika Vincze,
Marcin Wolin?ski, Alina Wro?blewska, and Eric Villemonte de la Cle?rgerie. 2014. Overview of the spmrl 2014
shared task on parsing morphologically rich languages. In Notes of the SPMRL 2014 Shared Task on Parsing
Morphologically-Rich Languages, Dublin, Ireland.
Wolfgang Seeker and Jonas Kuhn. 2012. Making Ellipses Explicit in Dependency Conversion for a German
Treebank. In Proceedings of the 8th International Conference on Language Resources and Evaluation, pages
3132?3139, Istanbul, Turkey.
Khalil Sima?an, Alon Itai, Yoad Winter, Alon Altmann, and Noa Nativ. 2001. Building a tree-bank of Modern
Hebrew text. Traitement Automatique des Langues, 42:347?380.
Marek S?widzin?ski and Marcin Wolin?ski. 2010. Towards a bank of constituent parse trees for Polish. In Proceed-
ings of Text, Speech and Dialogue, pages 197?204, Brno, Czech Republic.
108
Reut Tsarfaty, Djame Seddah, Yoav Goldberg, Sandra Ku?bler, Marie Candito, Jennifer Foster, Yannick Versley,
Ines Rehbein, and Lamia Tounsi. 2010. Statistical parsing for morphologically rich language (SPMRL): What,
how and whither. In Proceedings of the First workshop on Statistical Parsing of Morphologically Rich Lan-
guages (SPMRL), Los Angeles, CA.
Reut Tsarfaty, Joakim Nivre, and Evelina Andersson. 2012a. Cross-framework evaluation for statistical parsing.
In Proceeding of EACL, Avignon, France.
Reut Tsarfaty, Joakim Nivre, and Evelina Andersson. 2012b. Joint evaluation for segmentation and parsing. In
Proceedings of ACL, Jeju, Korea.
Reut Tsarfaty, Djame? Seddah, Sandra Ku?bler, and Joakim Nivre. 2012c. Parsing morphologically rich languages:
Introduction to the special issue. Computational Linguistics, 39(1):15?22.
Reut Tsarfaty. 2010. Relational-Realizational Parsing. Ph.D. thesis, University of Amsterdam.
Reut Tsarfaty. 2013. A unified morpho-syntactic scheme of Stanford dependencies. In Proceedings of ACL, Sofia,
Bulgaria.
Veronika Vincze, Do?ra Szauter, Attila Alma?si, Gyo?rgy Mo?ra, Zolta?n Alexin, and Ja?nos Csirik. 2010. Hungarian
Dependency Treebank. In Proceedings of LREC, Valletta, Malta.
Marcin Wolin?ski, Katarzyna G?owin?ska, and Marek S?widzin?ski. 2011. A preliminary version of Sk?adnica?a
treebank of Polish. In Proceedings of the 5th Language & Technology Conference, pages 299?303, Poznan?,
Poland.
Alina Wro?blewska. 2012. Polish Dependency Bank. Linguistic Issues in Language Technology, 7(1):1?15.
109
