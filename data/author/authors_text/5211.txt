139
140
141
142
143
144
145
146
Learning to predict pitch accents and prosodic boundaries in Dutch
Erwin Marsi1, Martin Reynaert1, Antal van den Bosch1,
Walter Daelemans2, Ve?ronique Hoste2
1 Tilburg University
ILK / Computational Linguistics and AI
Tilburg, The Netherlands
{e.c.marsi,reynaert,
antal.vdnbosch}@uvt.nl
2 University of Antwerp,
CNTS
Antwerp, Belgium
{daelem,hoste}@uia.ua.ac.be
Abstract
We train a decision tree inducer (CART)
and a memory-based classifier (MBL)
on predicting prosodic pitch accents and
breaks in Dutch text, on the basis of shal-
low, easy-to-compute features. We train
the algorithms on both tasks individu-
ally and on the two tasks simultaneously.
The parameters of both algorithms and
the selection of features are optimized per
task with iterative deepening, an efficient
wrapper procedure that uses progressive
sampling of training data. Results show
a consistent significant advantage of MBL
over CART, and also indicate that task
combination can be done at the cost of
little generalization score loss. Tests on
cross-validated data and on held-out data
yield F-scores of MBL on accent place-
ment of 84 and 87, respectively, and on
breaks of 88 and 91, respectively. Accent
placement is shown to outperform an in-
formed baseline rule; reliably predicting
breaks other than those already indicated
by intra-sentential punctuation, however,
appears to be more challenging.
1 Introduction
Any text-to-speech (TTS) system that aims at pro-
ducing understandable and natural-sounding out-
put needs to have on-board methods for predict-
ing prosody. Most systems start with generating
a prosodic representation at the linguistic or sym-
bolic level, followed by the actual phonetic real-
ization in terms of (primarily) pitch, pauses, and
segmental durations. The first step involves plac-
ing pitch accents and inserting prosodic boundaries
at the right locations (and may involve tune choice
as well). Pitch accents correspond roughly to pitch
movements that lend emphasis to certain words in
an utterance. Prosodic breaks are audible interrup-
tions in the flow of speech, typically realized by a
combination of a pause, a boundary-marking pitch
movement, and lengthening of the phrase-final seg-
ments. Errors at this level may impede the listener
in the correct understanding of the spoken utterance
(Cutler et al, 1997). Predicting prosody is known to
be a hard problem that is thought to require informa-
tion on syntactic boundaries, syntactic and seman-
tic relations between constituents, discourse-level
knowledge, and phonological well-formedness con-
straints (Hirschberg, 1993). However, producing all
this information ? using full parsing, including es-
tablishing semanto-syntactic relations, and full dis-
course analysis ? is currently infeasible for a real-
time system. Resolving this dilemma has been the
topic of several studies in pitch accent placement
(Hirschberg, 1993; Black, 1995; Pan and McKe-
own, 1999; Pan and Hirschberg, 2000; Marsi et al,
2002) and in prosodic boundary placement (Wang
and Hirschberg, 1997; Taylor and Black, 1998). The
commonly adopted solution is to use shallow infor-
mation sources that approximate full syntactic, se-
mantic and discourse information, such as the words
of the text themselves, their part-of-speech tags, or
their information content (in general, or in the text
at hand), since words with a high (semantic) infor-
mation content or load tend to receive pitch accents
(Ladd, 1996).
Within this research paradigm, we investigate
pitch accent and prosodic boundary placement for
Dutch, using an annotated corpus of newspaper text,
and machine learning algorithms to produce classi-
fiers for both tasks. We address two questions that
have been left open thus far in previous work:
1. Is there an advantage in inducing decision trees
for both tasks, or is it better to not abstract from
individual instances and use a memory-based
k-nearest neighbour classifier?
2. Is there an advantage in inducing classifiers for
both tasks individually, or can both tasks be
learned together.
The first question deals with a key difference be-
tween standard decision tree induction and memory-
based classification: how to deal with exceptional
instances. Decision trees, CART (Classification
and Regression Tree) in particular (Breiman et al,
1984), have been among the first successful machine
learning algorithms applied to predicting pitch ac-
cents and prosodic boundaries for TTS (Hirschberg,
1993; Wang and Hirschberg, 1997). Decision tree
induction finds, through heuristics, a minimally-
sized decision tree that is estimated to generalize
well to unseen data. Its minimality strategy makes
the algorithm reluctant to remember individual out-
lier instances that would take long paths in the tree:
typically, these are discarded. This may work well
when outliers do not reoccur, but as demonstrated
by (Daelemans et al, 1999), exceptions do typically
reoccur in language data. Hence, machine learn-
ing algorithms that retain a memory trace of indi-
vidual instances, like memory-based learning algo-
rithms based on the k-nearest neighbour classifier,
outperform decision tree or rule inducers precisely
for this reason.
Comparing the performance of machine learning
algorithms is not straightforward, and deserves care-
ful methodological consideration. For a fair com-
parison, both algorithms should be objectively and
automatically optimized for the task to be learned.
This point is made by (Daelemans and Hoste, 2002),
who show that, for tasks such as word-sense dis-
ambiguation and part-of-speech tagging, tuning al-
gorithms in terms of feature selection and classifier
parameters gives rise to significant improvements in
performance. In this paper, therefore, we optimize
both CART and MBL individually and per task, us-
ing a heuristic optimization method called iterative
deepening.
The second issue, that of task combination, stems
from the intuition that the two tasks have a lot
in common. For instance, (Hirschberg, 1993) re-
ports that knowledge of the location of breaks facil-
itates accent placement. Although pitch accents and
breaks do not consistently occur at the same posi-
tions, they are to some extent analogous to phrase
chunks and head words in parsing: breaks mark
boundaries of intonational phrases, in which typi-
cally at least one accent is placed. A learner may
thus be able to learn both tasks at the same time.
Apart from the two issues raised, our work is also
practically motivated. Our goal is a good algorithm
for real-time TTS. This is reflected in the type of
features that we use as input. These can be com-
puted in real-time, and are language independent.
We intend to show that this approach goes a long
way towards generating high-quality prosody, cast-
ing doubt on the need for more expensive sentence
and discourse analysis.
The remainder of this paper has the following
structure. In Section 2 we define the task, describe
the data, and the feature generation process which
involves POS tagging, syntactic chunking, and com-
puting several information-theoretic metrics. Fur-
thermore, a brief overview is given of the algorithms
we used (CART and MBL). Section 3 describes the
experimental procedure (ten-fold iterative deepen-
ing) and the evaluation metrics (F-scores). Section 4
reports the results for predicting accents and major
prosodic boundaries with both classifiers. It also re-
ports their performance on held-out data and on two
fully independent test sets. The final section offers
some discussion and concluding remarks.
2 Task definition, data, and machine
learners
To explore the generalization abilities of machine
learning algorithms trained on placing pitch accents
and breaks in Dutch text, we define three classifica-
tion tasks:
Pitch accent placement ? given a word form in its
sentential context, decide whether it should be
accented. This is a binary classification task.
Break insertion ? given a word form in its senten-
tial context, decide whether it should be fol-
lowed by a boundary. This is a binary classi-
fication task.
Combined accent placement and break insertion
? given a word form in its sentential context,
decide whether it should be accented and
whether it should be followed by a break. This
is a four-class task: no accent and no break; an
accent and no break; no accent and a break;
an accent and a break.
Finer-grained classifications could be envisioned,
e.g. predicting the type of pitch accent, but we assert
that finer classification, apart from being arguably
harder to annotate, could be deferred to later pro-
cessing given an adequate level of precision and re-
call on the present task.
In the next subsections we describe which data we
selected for annotation and how we annotated it with
respect to pitch accents and prosodic breaks. We
then describe the implementation of memory-based
learning applied to the task.
2.1 Prosodic annotation of the data
The data used in our experiments consists of 201
articles from the ILK corpus (a large collection of
Dutch newspaper text), totalling 4,493 sentences
and 58,097 tokens (excluding punctuation). We set
apart 10 articles, containing 2,905 tokens (excluding
punctuation) as held-out data for testing purposes.
As a preprocessing step, the data was tokenised by
a rule-based Dutch tokeniser, splitting punctuation
from words, and marking sentence endings.
The articles were then prosodically annotated,
without overlap, by four different annotators, and
were corrected in a second stage, again without over-
lap, by two corrector-annotators. The annotators?
task was to indicate the locations of accents and/or
breaks that they preferred. They used a custom an-
notation tool which provided feedback in the form
of synthesized speech. In total, 23,488 accents were
placed, which amounts to roughly one accent in two
and a half words. 8627 breaks were marked; 4601
of these were sentence-internal breaks; the remain-
der consisted of breaks at the end of sentences.
2.2 Generating shallow features
The 201 prosodically-annotated articles were subse-
quently processed through the following 15 feature
construction steps, each contributing one feature per
word form token. An excerpt of the annotated data
with all generated symbolic and numeric1 features is
presented in Table 1.
Word forms (Wrd) ? The word form tokens form
the central unit to which other features are added.
Pre- and post-punctuation ? All punctuation
marks in the data are transferred to two separate fea-
tures: a pre-punctuation feature (PreP) for punctua-
tion marks such as quotation marks appearing before
the token, and a post-punctuation feature (PostP) for
punctuation marks such as periods, commas, and
question marks following the token.
Part-of-speech (POS) tagging ? We used MBT
version 1.0 (Daelemans et al, 1996) to develop a
memory-based POS tagger trained on the Eindhoven
corpus of written Dutch, which does not overlap
with our base data. We split up the full POS tags into
two features, the first (PosC) containing the main
POS category, the second (PosF) the POS subfea-
tures.
Diacritical accent ? Some tokens bear an ortho-
graphical diacritical accent put there by the author to
particularly emphasize the token in question. These
accents were stripped off the accented letter, and
transferred to a binary feature (DiA).
NP and VP chunking (NpC & VpC) ? An ap-
proximation of the syntactic structure is provided by
simple noun phrase and verb phrase chunkers, which
take word and POS information as input and are
based on a small number of manually written reg-
ular expressions. Phrase boundaries are encoded per
word using three tags: ?B? for chunk-initial words,
?I? for chunk-internal words, and ?O? for words out-
side chunks. The NPs are identified according to the
base principle of one semantic head per chunk (non-
recursive, base NPs). VPs include only verbs, not
the verbal complements.
IC ? Information content (IC) of a word w is
given by IC(w) = ?log(P (w)), where P(w) is esti-
1Numeric features were rounded off to two decimal points,
where appropriate.
mated by the observed frequency of w in a large dis-
joint corpus of about 1.7 GB of unannotated Dutch
text garnered from various sources. Word forms not
in this corpus were given the highest IC score, i.e.
the value for hapax legomenae (words that occur
once).
Bigram IC ? IC on bigrams (BIC) was calculated
for the bigrams (pairs of words) in the data, accord-
ing to the same formula and corpus material as for
unigram IC.
TF*IDF ? The TF*IDF metric (Salton, 1989) es-
timates the relevance of a word in a document. Doc-
ument frequency counts for all token types were ob-
tained from a subset of the same corpus as used
for IC calculations. TF*IDF and IC (previous two
features) have been succesfully tested as features
for accent prediction by (Pan and McKeown, 1999),
who assert that IC is a more powerful predictor than
TF*IDF.
Phrasometer ? The phrasometer feature (PM) is
the summed log-likelihood of all n-grams the word
form occurs in, with n ranging from 1 to 25, and
computed in an iterative growth procedure: log-
likelihoods of n + 1-grams were computed by ex-
panding all stored n-grams one word to the left
and to the right; only the n + 1-grams with higher
log-likelihood than that of the original n-gram are
stored. Computations are based on the complete ILK
Corpus.
Distance to previous occurrence ? The distance,
counted in the number of tokens, to previous occur-
rence of a token within the same article (D2P). Un-
seen words were assigned the arbitrary high default
distance of 9999.
Distance to sentence boundaries ? Distance of
the current token to the start of the sentence (D2S)
and to the end of the sentence (D2E), both measured
as a proportion of the total sentence length measured
in tokens.
2.3 CART: Classification and regression trees
CART (Breiman et al, 1984) is a statistical method
to induce a classification or regression tree from a
given set of instances. An instance consists of a
fixed-length vector of n feature-value pairs, and an
information field containing the classification of that
particular feature-value vector. Each node in the
CART tree contains a binary test on some categor-
ical or numerical feature in the input vector. In the
case of classification, the leaves contain the most
likely class. The tree building algorithm starts by
selecting the feature test that splits the data in such a
way that the mean impurity (entropy times the num-
ber of instances) of the two partitions is minimal.
The algorithm continues to split each partition recur-
sively until some stop criterion is met (e.g. a mini-
mal number of instances in the partition). Alterna-
tively, a small stop value can be used to build a tree
that is probably overfitted, but is then pruned back
to where it best matches some amount of held-out
data. In our experiments, we used the CART imple-
mentation that is part of the Edinburgh Speech Tools
(Taylor et al, 1999).
2.4 Memory-based learning
Memory-based learning (MBL), also known as
instance-based, example-based, or lazy learning
(Stanfill and Waltz, 1986; Aha et al, 1991), is a
supervised inductive learning algorithm for learning
classification tasks. Memory-based learning treats
a set of training instances as points in a multi-
dimensional feature space, and stores them as such
in an instance base in memory (rather than perform-
ing some abstraction over them). After the instance
base is stored, new (test) instances are classified
by matching them to all instances in memory, and
by calculating with each match the distance, given
by a distance function between the new instance
X and the memory instance Y . Cf. (Daelemans
et al, 2002) for details. Classification in memory-
based learning is performed by the k-NN algorithm
(Fix and Hodges, 1951; Cover and Hart, 1967) that
searches for the k ?nearest neighbours? according
to the distance function. The majority class of the
k nearest neighbours then determines the class of
the new case. In our k-NN implementation2 , equi-
distant neighbours are taken as belonging to the
same k, so this implementation is effectively a k-
nearest distance classifier.
3 Optimization by iterative deepening
Iterative deepening (ID) is a heuristic search algo-
rithm for the optimization of algorithmic parameter
2All experiments with memory-based learning were per-
formed with TiMBL, version 4.3 (Daelemans et al, 2002).
Wrd PreP PostP PosC PosF DiA NpC VpC IC BIC Tf*Idf PM D2P D2S D2E A B AB
De = = Art bep,zijdofmv,neut 0 B O 2.11 5.78 0.00 4 9999 0.00 0.94 - - - -
bomen = = N soort,mv,neut 0 I O 4.37 7.38 0.16 4 17 0.06 0.89 A - A-
rondom = = Prep voor 0 O O 4.58 5.09 0.04 4 17 0.11 0.83 - - - -
de = = Art bep,zijdofmv,neut 0 B O 1.31 5.22 0.00 5 20 0.17 0.78 - - - -
molen = = N soort,ev,neut 0 I O 5.00 7.50 0.18 5 9 0.22 0.72 A - A-
bij = = Prep voor 0 O O 2.50 3.04 0.00 6 9999 0.28 0.67 - - - -
de = = Art bep,zijdofmv,neut 0 B O 1.31 6.04 0.00 6 3 0.33 0.61 - - - -
scheepswerf = = N soort,ev,neut 0 I O 5.63 8.02 0.03 4 9999 0.39 0.56 - - - -
Verolme = = N eigen,ev,neut 0 I O 6.38 7.59 0.05 0 9999 0.44 0.50 A - A-
moeten = = V trans,ott,3,ev 0 B O 2.99 6.77 0.01 4 9999 0.61 0.33 - - - -
verkassen = , V trans,inf 0 I O 5.75 5.99 0.02 4 9999 0.67 0.28 A B AB
vindt = = V trans,ott,3,ev 0 O B 3.51 8.50 0.00 6 9999 0.72 0.22 - - - -
molenaar = = N soort,ev,neut 0 B O 5.95 8.50 0.05 0 9999 0.78 0.17 - - - -
Wijbrand = = N eigen,ev,neut 0 I O 7.89 8.50 0.11 0 38 0.83 0.11 A - A-
Table 1: Symbolic and numerical features and class for the sentence De bomen rondom de scheepswerf Verolme moeten verkassen,
vindt molenaar Wijbrandt. ?Miller Wijbrand thinks that the trees surrounding the mill near shipyard Verolme have to relocate.?
and feature selection, that combines classifier wrap-
ping (using the training material internally to test ex-
perimental variants) (Kohavi and John, 1997) with
progressive sampling of training material (Provost et
al., 1999). We start with a large pool of experiments,
each with a unique combination of input features
and algorithmic parameter settings. In the first step,
each attempted setting is applied to a small amount
of training material and tested on a fixed amount
of held-out data (which is a part of the full train-
ing set). Only the best settings are kept; all others
are removed from the pool of competing settings.
In subsequent iterations, this step is repeated, ex-
ponentially decreasing the number of settings in the
pool, while at the same time exponentially increas-
ing the amount of training material. The idea is that
the increasing amount of time required for training
is compensated by running fewer experiments, in ef-
fect keeping processing time approximately constant
across iterations. This process terminates when only
the single best experiment is left (or, the n best ex-
periments).
This ID procedure can in fact be embedded in a
standard 10-fold cross-validation procedure. In such
a 10-fold CV ID experiment, the ID procedure is car-
ried out on the 90% training partition, and the result-
ing optimal setting is tested on the remaining 10%
test partition. The average score of the 10 optimized
folds can then be considered, as that of a normal 10-
fold CV experiment, to be a good estimation of the
performance of a classifier optimized on the full data
set.
For current purposes, our specific realization of
this general procedure was as follows. We used folds
of approximately equal size. Within each ID ex-
periment, the amount of held-out data was approx-
imately 5%; the initial amount of training data was
5% as well. Eight iterations were performed, dur-
ing which the number of experiments was decreased,
and the amount of training data was increased, so
that in the end only the 3 best experiments used
all available training data (i.e. the remaining 95%).
Increasing the training data set was accomplished
by random sampling from the total of training data
available. Selection of the best experiments was
based on their F-score (van Rijsbergen, 1979) on
the target class (accent or break). F-score, the har-
monic mean of precision and recall, is chosen since
it directly evaluates the tasks (placement of accents
or breaks), in contrast with classification accuracy
(the percentage of correctly classified test instances)
which is biased to the majority class (to place no ac-
cent or break). Moreover, accuracy masks relevant
differences between certain inappropriate classifiers
that do not place accents or breaks, and better clas-
sifiers that do place them, but partly erroneously.
The initial pool of experiments was created by
systematically varying feature selection (the input
features to the classifier) and the classifier set-
tings (the parameters of the classifiers). We re-
stricted these selections and settings within reason-
able bounds to keep our experiments computation-
ally feasible. In particular, feature selection was lim-
ited to varying the size of the window that was used
to model the local context of an instance. A uni-
form window (i.e. the same size for all features) was
applied to all features except DiA, D2P, D2S, and
D2E. Its size (win) could be 1, 3, 5, 7, or 9, where
win = 1 implies no modeling of context, whereas
win = 9 means that during classification not only
the features of the current instance are taken into ac-
count, but also those of the preceding and following
four instances.
For CART, we varied the following parameter val-
ues, resulting in a first ID step with 480 experiments:
? the minimum number of examples for leaf
nodes (stop): 1, 10, 25, 50, and 100
? the number of partitions to split a float feature
range into (frs): 2, 5, 10, and 25
? the percentage of training material held out for
pruning (held-out): 0, 5, 10, 15, 20, and 25 (0
implies no pruning)
For MBL, we varied the following parameter val-
ues, which led to 1184 experiments in the first ID
step:
? the number of nearest neighbours (k): 1, 4, 7,
10, 13, 16, 19, 22, 25, and 28
? the type of feature weighting: Gain Ratio (GR),
and Shared Variance (SV)
? the feature value similarity metric: Overlap,
or Modified Value Difference Metric (MVDM)
with back-off to Overlap at value frequency
tresholds 1 (L=1, no back-off), 2, and 10
? the type of distance weighting: None, Inverse
Distance, Inverse Linear Distance, and Expo-
nential Decay with ? = 1.0 (ED1) and ? = 4.0
(ED4)
4 Results
4.1 Tenfold iterative deepening results
We first determined two sharp, informed baselines;
see Table 2. The informed baseline for accent place-
ment is based on the content versus function word
distinction, commonly employed in TTS systems
(Taylor and Black, 1998). We refer to this baseline
as CF-rule. It is constructed by accenting all content
words, while leaving all function words (determin-
ers, prepositions, conjunctions/complementisers and
auxiliaries) unaccented. The required word class in-
formation is obtained from the POS tags. The base-
line for break placement, henceforth PUNC-rule, re-
lies solely on punctuation. A break is inserted after
any sequence of punctuation symbols containing one
Target : Method : Prec : Rec : F :
Accent CF-rule 66.7 94.9 78.3
CART 78.6 ?2.8 85.7 ?1.1 82.0 ?1.7
MBL 80.0 ?2.7 86.6 ?1.4 83.6 ?1.6?
CARTC 78.7 ?3.0 85.6 ?0.8 82.0 ?1.6
MBLC 81.0 ?2.7 86.1 ?1.1 83.4 ?1.5?
Break PUNC-rule 99.2 75.7 85.9
CART 93.1 ?1.5 82.2 ?3.0 87.3 ?1.5
MBL 95.1 ?1.4 81.9 ?2.8 88.0 ?1.5?
CARTC 94.5 ?0.8 80.2 ?3.1 86.7 ?1.6
MBLC 95.7 ?1.1 80.7 ?3.1 87.6 ?1.7?
Table 2: Precision, recall, and F-scores on accent, break
and combined prediction by means of CART and MBL, for
baselines and for average results over 10 folds of the Iterative
Deepening experiment; a ? indicates a significant difference
(p < 0.01) between CART and MBL according to a paired
t-test. Superscript C refers to the combined task.
or more characters from the set {,!?:;()}. It should
be noted that both baselines are simple rule-based
algorithms that have been manually optimized for
the current training set. They perform well above
chance level, and pose a serious challenge to any ML
approach.
From the results displayed in Table 2, the follow-
ing can be concluded. First, MBL attains the highest
F-scores on accent placement, 83.6, and break place-
ment, 88.0. It does so when trained on the ACCENT
and BREAK tasks individually. On these tasks, MBL
performs significantly better than CART (paired t-
tests yield p < 0.01 for both differences).
Second, the performances of MBL and CART on
the combined task, when split in F-scores on accent
and break placement, are rather close to those on the
accent and break tasks. For both MBL and CART,
the scores on accent placement as part of the com-
bined task versus accent placement in isolation are
not significantly different. For break insertion, how-
ever, a small but significant drop in performance can
be seen with MBL (p < 0.05) and CART (p < 0.01)
when it is performed as part of the COMBINED task.
As is to be expected, the optimal feature selec-
tions and classifier settings obtained by iterative
deepening turned out to vary over the ten folds for
both MBL and CART. Table 3 lists the settings pro-
ducing the best F-score on accents or breaks. A win-
dow of 7 (i.e. the features of the three preceding and
following word form tokens) is used by CART and
MBL for accent placement, and also for break in-
sertion by CART, whereas MBL uses a window of
Target: Method: Setting:
Accent CART win=7, stop=50, frs=5, held-out=5
MBL win=7, MVDM with L=5, k=25, GR, ED4
Break CART win=7, stop=25, frs=2, held-out=5
MBL win=3, MVDM with L=2, k=28, GR, ED4
Table 3: Optimal parameter settings for CART and MBL with
respect to accent and break prediction
just 3. Both algorithms (stop in CART, and k in
MBL) base classifications on minimally around 25
instances. Furthermore, MBL uses the Gain Ratio
feature weighting and Exponential Decay distance
weighting. Although no pruning was part of the Iter-
ative Deepening experiment, CART prefers to hold
out 5% of its training material to prune the decision
tree resulting from the remaining 95%.
4.2 External validation
We tested our optimized approach on our held-out
data of 10 articles (2,905 tokens), and on an indepen-
dent test corpus (van Herwijnen and Terken, 2001).
The latter contains two types of text: 2 newspaper
texts (55 sentences, 786 words excluding punctua-
tion), and 17 email messages (70 sentences, 1133
words excluding punctuation). This material was an-
notated by 10 experts, who were asked to indicate
the preferred accents and breaks. For the purpose
of evaluation, words were assumed to be accented if
they received an accent by at least 7 of the annota-
tors. Furthermore, of the original four break levels
annotated (i.e. no break, light, medium, or heavy ),
only medium and heavy level breaks were consid-
ered to be a break in our evaluation. Table 4 lists the
precision, recall, and F-scores obtained on the two
tasks using the single-best scoring setting from the
10-fold CV ID experiment per task. It can be seen
that both CART and MBL outperformed the CF-rule
baseline on our own held-out data and on the news
and email texts, with similar margins as observed in
our 10-fold CV ID experiment. MBL attains an F-
score of 86.6 on accents, and 91.0 on breaks; both
are improvements over the cross-validation estima-
tions. On breaks, however, both CART and MBL
failed to improve on the PUNC-rule baseline; on the
news and email texts they perform even worse. In-
specting MBLs output on these text, it turned out
that MBL does emulate the PUNC-rule baseline,
but that it places additional breaks at positions not
Target : Test set Method : Prec : Rec : F :
Accent Held-out CF-rule 73.5 94.8 82.8
CART 84.3 86.1 85.2
MBL 87.0 86.3 86.6
News CF-rule 52.2 92.9 66.9
CART 62.7 92.5 74.6
MBL 66.3 89.2 76.0
Email CF-rule 54.3 91.0 68.0
CART 66.8 88.5 76.1
MBL 71.0 88.5 78.8
Break Held-out PUNC-rule 99.5 83.7 90.9
CART 92.6 88.9 90.7
MBL 95.5 87.0 91.0
News PUNC-rule 98.8 93.1 95.9
CART 80.6 95.4 87.4
MBL 89.3 95.4 92.2
Email PUNC-rule 93.9 87.0 90.3
CART 81.6 90.2 85.7
MBL 83.0 91.1 86.8
Table 4: Precision, recall, and F-scores on accent and break
prediction for our held-out corpus and two external corpora of
news and email texts, using the best settings for CART and
MBL as determined by the ID experiments.
marked by punctuation. A considerable portion of
these non-punctuation breaks is placed incorrectly ?
or at least different from what the annotators pre-
ferred ? resulting in a lower precision that does not
outweigh the higher recall.
5 Conclusion
With shallow features as input, we trained machine
learning algorithms on predicting the placement of
pitch accents and prosodic breaks in Dutch text,
a desirable function for a TTS system to produce
synthetic speech with good prosody. Both algo-
rithms, the memory-based classifier MBL and de-
cision tree inducer CART, were automatically opti-
mized by an Iterative Deepening procedure, a classi-
fier wrapper technique with progressive sampling of
training data. It was shown that MBL significantly
outperforms CART on both tasks, as well as on the
combined task (predicting accents and breaks simul-
taneously). This again provides an indication that
it is advantageous to retain individual instances in
memory (MBL) rather than to discard outlier cases
as noise (CART).
Training on both tasks simultaneously, in one
model rather than divided over two, results in
generalization accuracies similar to that of the
individually-learned models (identical on accent
placement, and slightly lower for break placement).
This shows that learning one task does not seriously
hinder learning the other. From a practical point of
view, it means that a TTS developer can resort to one
system for both tasks instead of two.
Pitch accent placement can be learned from shal-
low input features with fair accuracy. Break in-
sertion seems a harder task, certainly in view of
the informed punctuation baseline PUNC-rule. Es-
pecially the precision of the insertion of breaks at
other points than those already indicated by com-
mas and other ?pseudo-prosodic? orthographic mark
up is hard. This may be due to the lack of crucial
information in the shallow features, to inherent lim-
itations of the ML algorithms, but may as well point
to a certain amount of optionality or personal pref-
erence, which puts an upper bound on what can be
achieved in break prediction (Koehn et al, 2000).
We plan to integrate the placement of pitch ac-
cents and breaks in a TTS system for Dutch, which
will enable the closed-loop annotation of more data
using the TTS itself and on-line (active) learning.
Moreover, we plan to investigate the perceptual
cost of false insertions and deletions of accents and
breaks in experiments with human listeners.
Acknowledgements
Our thanks go out to Olga van Herwijnen and Jacques Terken
for the use of their TTS evaluation corpus. All research in
this paper was funded by the Flemish-Dutch Committee (VNC)
of the National Foundations for Research in the Netherlands
(NWO) and Belgium (FWO).
References
D. W. Aha, D. Kibler, and M. Albert. 1991. Instance-based
learning algorithms. Machine Learning, 6:37?66.
A.W. Black. 1995. Comparison of algorithms for predicting
pitch accent placement in English speech synthesis. In Pro-
ceedings of the Spring Meeting of the Acoustical Society of
Japan.
L. Breiman, J. Friedman, R. Ohlsen, and C. Stone. 1984.
Classification and regression trees. Wadsworth International
Group, Belmont, CA.
C.J. van Rijsbergen. 1979. Information Retrieval. Butter-
sworth, London.
T. M. Cover and P. E. Hart. 1967. Nearest neighbor pattern
classification. Institute of Electrical and Electronics Engi-
neers Transactions on Information Theory, 13:21?27.
A. Cutler, D. Dahan, and W.A. Van Donselaar. 1997. Prosody
in the comprehension of spoken language: A literature re-
view. Language and Speech, 40(2):141?202.
W. Daelemans and V. Hoste. 2002. Evaluation of machine
learning methods for natural language processing tasks. In
Proceedings of LREC-2002, the third International Confer-
ence on Language Resources and Evaluation, pages 755?
760.
W. Daelemans, J. Zavrel, P. Berck, and S. Gillis. 1996. MBT:
A memory-based part of speech tagger generator. In E. Ejer-
hed and I. Dagan, editors, Proc. of Fourth Workshop on Very
Large Corpora, pages 14?27. ACL SIGDAT.
W. Daelemans, A. van den Bosch, and J. Zavrel. 1999. For-
getting exceptions is harmful in language learning. Ma-
chine Learning, Special issue on Natural Language Learn-
ing, 34:11?41.
W. Daelemans, J. Zavrel, K. van der Sloot, and
A. van den Bosch. 2002. TiMBL: Tilburg Memory
Based Learner, version 4.3, reference guide. Technical
Report ILK-0210, ILK, Tilburg University.
E. Fix and J. L. Hodges. 1951. Discriminatory analysis?
nonparametric discrimination; consistency properties. Tech-
nical Report Project 21-49-004, Report No. 4, USAF School
of Aviation Medicine.
J. Hirschberg. 1993. Pitch accent in context: Predicting intona-
tional prominence from text. Artificial Intelligence, 63:305?
340.
P. Koehn, S. Abney, J. Hirschberg, and M. Collins. 2000. Im-
proving intonational phrasing with syntactic information. In
ICASSP, pages 1289?1290.
R. Kohavi and G. John. 1997. Wrappers for feature subset
selection. Artificial Intelligence Journal, 97(1?2):273?324.
D. R. Ladd. 1996. Intonational phonology. Cambridge Uni-
versity Press.
E. Marsi, G.J. Busser, W. Daelemans, V. Hoste, M. Reynaert,
and A. van den Bosch. 2002. Combining information
sources for memory-based pitch accent placement. In Pro-
ceedings of the International Conference on Spoken Lan-
guage Processing, ICSLP-2002, pages 1273?1276.
S. Pan and J. Hirschberg. 2000. Modeling local context for
pitch accent prediction. In Proceedings of the 35th Annual
Meeting of the Association for Computational Linguistics,
Hong Kong.
S. Pan and K. McKeown. 1999. Word informativeness
and automatic pitch accent modeling. In Proceedings of
EMNLP/VLC?99, New Brunswick, NJ, USA. ACL.
F. Provost, D. Jensen, and T. Oates. 1999. Efficient progressive
sampling. In Proceedings of the Fifth International Con-
ference on Knowledge Discovery and Data Mining, pages
23?32.
G. Salton. 1989. Automatic text processing: The transfor-
mation, analysis, and retrieval of information by computer.
Addison?Wesley, Reading, MA, USA.
C. Stanfill and D. Waltz. 1986. Toward memory-based reason-
ing. Communications of the ACM, 29(12):1213?1228, De-
cember.
P. Taylor and A. Black. 1998. Assigning phrase breaks from
part-of-speech sequences. Computer Speech and Language,
12:99?117.
P. Taylor, R. Caley, A. W. Black, and S. King, 1999. Edin-
burgh Speech Tools Library, System Documentation Edition
1.2. CSTR, University of Edinburgh.
O. van Herwijnen and J. Terken. 2001. Evaluation of pros-3 for
the assignment of prosodic structure, compared to assign-
ment by human experts. In Proceedings Eurospeech 2001
Scandinavia, Vol.1, pages 529?532.
M. Q. Wang and J. Hirschberg. 1997. Automatic classification
of intonational phrasing boundaries. Computer Speech and
Language, 6(2):175?196.
Proceedings of ACL-08: HLT, Short Papers (Companion Volume), pages 193?196,
Columbus, Ohio, USA, June 2008. c?2008 Association for Computational Linguistics
Query-based sentence fusion is better defined and leads to
more preferred results than generic sentence fusion?
Emiel Krahmer
Tilburg University
Tilburg, The Netherlands
E.J.Krahmer@uvt.nl
Erwin Marsi
Tilburg University
Tilburg, The Netherlands
E.C.Marsi@uvt.nl
Paul van Pelt
Tilburg University
Tilburg, The Netherlands
paul.vanpelt@gmail.com
Abstract
We show that question-based sentence fu-
sion is a better defined task than generic sen-
tence fusion (Q-based fusions are shorter, dis-
play less variety in length, yield more identi-
cal results and have higher normalized Rouge
scores). Moreover, we show that in a QA set-
ting, participants strongly prefer Q-based fu-
sions over generic ones, and have a preference
for union over intersection fusions.
1 Introduction
Sentence fusion is a text-to-text generation applica-
tion, which given two related sentences, outputs a
single sentence expressing the information shared
by the two input sentences (Barzilay and McKeown
2005). Consider, for example, the following pair of
sentences:1
(1) Posttraumatic stress disorder (PTSD) is a
psychological disorder which is classified as
an anxiety disorder in the DSM-IV.
(2) Posttraumatic stress disorder (abbrev.
PTSD) is a psychological disorder caused by
a mental trauma (also called psychotrauma)
that can develop after exposure to a terrifying
event.
?Thanks are due to Ed Hovy for discussions on the Rouge
metrics and to Carel van Wijk for statistical advice. The data-
set described in this paper (2200 fusions of pairs of sentences)
is available upon request. This research was carried out within
the Deaso project (http://daeso.uvt.nl/).
1All examples are English translations of Dutch originals.
Fusing these two sentences with the strategy de-
scribed by Barzilay and McKeown (based on align-
ing and fusing the respective dependency trees)
would result in a sentence like (3).
(3) Posttraumatic stress disorder (PTSD) is a
psychological disorder.
Barzilay and McKeown (2005) argue convincingly
that employing such a fusion strategy in a multi-
document summarization system can result in more
informative and more coherent summaries.
It should be noted, however, that there are multi-
ple ways to fuse two sentences. Besides fusing the
shared information present in both sentences, we can
conceivably also fuse them such that all information
present in either of the sentences is kept, without any
redundancies. Marsi and Krahmer (2005) refer to
this latter strategy as union fusion (as opposed to
intersection fusion, as in (3)). A possible union fu-
sion of (1) and (2) would be:
(4) Posttraumatic stress disorder (PTSD) is a
psychological disorder, which is classified
as an anxiety disorder in the DSM-IV,
caused by a mental trauma (also called psy-
chotrauma) that can develop after exposure
to a terrifying event.
Marsi and Krahmer (2005) propose an algorithm
which is capable of producing both fusion types.
Which type is more useful is likely to depend on
the kind of application and information needs of the
user, but this is essentially still an open question.
193
However, there is a complication. Daume? III &
Marcu (2004) argue that generic sentence fusion is
an ill-defined task. They describe experimental data
showing that when participants are given two con-
secutive sentences from a single document and are
asked to fuse them (in the intersection sense), differ-
ent participants produce very different fusions. Nat-
urally, if human participants cannot reliably perform
fusions, evaluating automatic fusion strategies is al-
ways going to be a shaky business. The question
is why different participants come to different fu-
sions. One possibility, which we explore in this pa-
per, is that it is the generic nature of the fusion which
causes problems. In particular, we hypothesize that
fusing two sentences in the context of a preceding
question (the natural setting in QA applications) re-
sults in more agreement among humans. A related
question is of course what the results would be for
union fusion. Will people agree more on the unions
than on the intersections? And is the effect of a pre-
ceding question the same for both kinds of fusion?
In Experiment I, below, we address these questions,
by collecting and comparing four different fusions
for various pairs of related sentences, both generic
and question-based ones, and both intersection and
union ones.
While it seems a reasonable hypothesis that
question-based fusions will lead to more agreement
among humans, the really interesting question is
which fusion strategy (if any) is most appreciated
by users in a task-based evaluation. Given that Ex-
periment I gives us four different fusions per pair of
sentence, an interesting follow-up question is which
leads to the best answers in a QA setting. Do par-
ticipants prefer concise (intersection) or complete
(union) answers? And does it matter whether the
fusion was question-based or not? In Experiment
II, we address these questions via an evaluation
experiment using a (simulated) medical question-
answering system, in which participants have to rank
four answers (resulting from generic and question-
based intersection and union fusions) for different
medical questions.
2 Experiment I: Data-collection
Method To collect pairs of related sentences to be
fused under different conditions, we proceeded as
Fusion type Length M (SD) # Id.
Generic Intersection 15.6 (2.9) 73
Q-Based Intersection 8.1 (2.5) 189
Generic Union 31.2 (7.8) 109
Q-Based Union 19.2 (4.7) 134
Table 1: Mean sentence length (plus Standard Deviation)
and number of identical fusion results as a function of
fusion type (n = 550 for each type).
follows. As our starting point we used a set of
100 medical questions compiled as a benchmark for
evaluating medical QA systems, where all correct
answers were manually retrieved from the available
text material. Based on this set, we randomly se-
lected 25 questions for which more than one answer
could be found (otherwise there would be nothing
to fuse), and where the first two answer sentences
shared at least some information (otherwise inter-
section fusion would be impossible).
Participants were 44 native speakers of Dutch (20
women) with an average age of 30.1 years, none
with a background in sentence fusion research. Ex-
periment I has a mixed between-within subjects de-
sign. Participants were randomly assigned to either
the intersection or the union condition, and within
each condition they first had to produce 25 generic
and then 25 question-based fusions. In the latter
case, participants were given the original question
used to retrieve the sentences to be fused.
The experiment was run using a web-based
script. Participants were told that the purpose of the
experiment was merely to gather data, they were not
informed about our interest in generic vs question
based fusion. Before participants could start with
their task, the concept of sentence fusion (either
fusion or intersection, depending on the condition)
was explained, using a number of worked examples.
After this, the actual experiment started.
Results First consider the descriptive statistics in Ta-
ble 1. Naturally, intersection fusion leads to shorter
sentences on average than union fusion. More in-
terestingly, question (Q)-based fusions lead to sig-
nificantly shorter sentences than their generic coun-
terparts (intersection t = 9.1, p < .001, union:
t = 6.1, p < .001, two-tailed). Also note that
194
Generic Q-Based Generic Q-Based
Intersection Intersection Union Union
Rouge-1 .036 .068 .035 .041
Rouge-SU4 .014 .038 .018 .020
Rouge-SU9 .014 .040 .016 .020
Table 2: Average Rouge-1, Rouge-SU4 and Rouge-SU9 (normalized for sentence length) as a function of fusion type.
the variation among participants decreases in the Q-
based conditions (lower standard deviations). This
suggests that participants in the Q-based conditions
indeed show less variety in their fusions than partic-
ipants in the generic conditions. This is confirmed
by the number of identical (i.e., duplicated) fusions,
which is indeed higher in the Q-based conditions,
although the difference is only significant for inter-
sections (?2(1) = 51.3, p < .001).
We also computed average Rouge-1, Rouge-SU4
and Rouge-SU9 scores for each set of fusions, to
be able to quantify the overlap between participants
in the various conditions. One complication is that
these metrics are sensitive to sentence-length (longer
sentences are more likely to contain overlapping
words than shorter ones), hence in Table 2 we report
on Rouge scores that are normalized with respect
sentence length. The resulting picture is surprisingly
consistent: Q-based fusion on all three metrics re-
sults in higher normalized Rouge scores, where the
difference is generally small in the case of union,
and rather substantial for intersection.
3 Experiment II: Evaluation
The previous experiment indicates that Q-based
fusion is indeed a better-defined summarization task
than generic fusion, in this experiment we address
the question which kind of fusion participants prefer
in a QA application.
Method We selected 20 from the 25 questions
used in Experiment I, for which we made sure
that the fusions in the four categories resulted
in sentences with a sufficiently different content.
For each question, one representative sentence
was selected from the 22 fusions produced by
participants in Experiment I, for each of the four
categories (Q-based intersection, Q-based union,
Generic intersection and Generic union). This
Fusion type Mean Rank
Q-Based Union 1.888
Q-Based Intersection 2.471
Generic Intersection 2.709
Generic Union 2.932
Table 4: Mean rank from 1 (= ?best?) to 4 (=?worst?) as
a function of fusion type.
representative sentence was the most frequent result
for that particular category. When no such sentence
was present for a particular task, a random selection
was made.
Participants were 38 native speakers of Dutch (17
men), with an average age of 39.4 years. None
had participated in Experiment I and none had a
background in sentence fusion research. Participants
were confronted with the selected 20 questions, one
at a time. For each question, participants saw four
alternative answers (one from each category). Fig-
ure 3 shows one question, with four different fusions
derived by participants from example sentences (1)
and (2). Naturally, the labels for the 4 fusion strate-
gies were not part of the experiment. Participants
were asked to rank the 4 answers from ?best? (rank
1) to ?worst? (rank 4), via a forced choice paradigm
(i.e., they also had to make a choice if they felt that
two answers were roughly as good). Experiment II
had a within-subjects design, which means that all
38 participants ranked the answers for all 20 ques-
tions.
Results Table 4 gives the mean rank for the four
fusion types. To test for significance, we per-
formed a repeated measures Analysis of Variance
(ANOVA) with fusion type and question as the in-
dependent variables and average rank as the depen-
dent variable. A main effect was found of fusion
type (F (3, 111) = 20.938, p < .001, ?2 = .361).
195
What is PTSD?
Generic Intersection Posttraumatic stress disorder (PTSD) is a psychological disorder.
Q-based Intersection PTSD stands for posttraumatic stress disorder and is a psychological disorder.
Generic Union Posttraumatic stress disorder (PTSD) is a psychological disorder, which is classified as an
anxiety disorder in the DSM-IV, caused by a mental trauma (also called psychotrauma) that
can develop after exposure to a terrifying event.
Q-based Union PTSD (posttraumatic stress disorder) is a psychological disorder caused by a mental trauma
(also called psychotrauma) that can develop after exposure to a terrifying event.
Table 3: Example question from Experiment II, with four possible answers, based on different fusions strategies
(obtained in Experiment I).
Pairwise comparisons using the Bonferroni method
show that all comparisons are statistically significant
(at p < .001) except for the one between Generic In-
tersection and Generic Union. Thus, in particular:
Q-based union is ranked significantly higher than
Q-based intersection, which in turn is ranked sig-
nificantly higher than both Generic union and inter-
section (whose respective ranks are not significantly
different).
The ANOVA analysis also revealed a significant
interaction between question and type of fusion
(F (57, 2109) = 7.459, p < .001, ?2 = .168).2
What this means is that relative ranking varies for
different questions. To better understand this inter-
action, we performed a series of Friedman tests for
each question (the Friedman test is a standard non-
parametric test for ranked data). The Friedman anal-
yses revealed that the overall pattern (Q-based union
> Q-based intersection > Generic Union / Intersec-
tion) was found to be significant for 13 out of the
20 questions. For four of the remaining seven ques-
tions, Q-based union ranked first as well, while for
two questions Q-based intersection was ranked as
the best answer. For the remaining question, there
was no significant difference between the four fu-
sion types.
4 Conclusion and discussion
In this paper we have addressed two questions. First:
is Q-based fusion a better defined task than generic
fusion? Here, the answer seems to be ?yes?: Q-
based fusions are shorter, display less variety in
length, result in more identically fused sentences
2Naturally, there can be no main effect of question, since
there is no variance; the ranks 1-4 are fixed for each question.
and have higher normalized Rouge scores, where the
differences are larger for intersection than for union.
Inspection of the fused sentences reveals that there
is simply more potential variation on the word level
(do I select this word from one input sentence or
from the other?) for union fusion than for inter-
section fusion. Second: which kind of fusion (if
any) do users of a medical QA system prefer? Here
a consistent preference order was found, with rank
1 = Q-based union, rank 2 = Q-based Intersection,
rank 3/4 = Generic intersection / union. Thus: par-
ticipants clearly prefer Q-based fusions, and prefer
more complete answers over shorter ones.
In future research, we intend to collect new data
with different questions per sentence pair, to find out
to what extent the question and its phrasing drive
the fusion process. In addition, we will also let sen-
tences from different domains be fused, based on the
hypothesis that fusion strategies may differ across
domains.
References
Regina Barzilay and Kathleen McKeown. 2005. Sen-
tence Fusion for Multidocument News Summariza-
tion. Computational Linguistics, 31(3), 297-328.
Hal Daume? III and Daniel Marcu. 2004. Generic Sen-
tence Fusion is an Ill-Defined Summarization Task.
Proceedings of the ACL Text Summarization Branches
Out Workshop, Barcelona, Spain.
Chin-Yew Lin and Eduard Hovy. 2003. Automatic evalu-
ation of summaries using N-gram co-occurrence statis-
tics. Proceedings of NAACL ?03, Edmonton, Canada.
Erwin Marsi and Emiel Krahmer. 2005. Explorations
in Sentence Fusion. Proceedings of the 10th Euro-
pean Workshop on Natural Language Generation, Ab-
erdeen, UK.
196
Proceedings of the ACL Workshop on Computational Approaches to Semitic Languages, pages 1?8,
Ann Arbor, June 2005. c?2005 Association for Computational Linguistics
Memory-based morphological analysis generation and
part-of-speech tagging of Arabic
Erwin Marsi, Antal van den Bosch
ILK, Tilburg University
P.O. Box 90153
NL-5000 LE Tilburg
The Netherlands
{E.C.Marsi,Antal.vdnBosch}@uvt.nl
Abdelhadi Soudi
Center for Computational Linguistics
Ecole Nationale de L?Industrie Mine?rale
Rabat,
Morocco,
asoudi@gmail.com/asoudi@enim.ac.ma
Abstract
We explore the application of memory-
based learning to morphological analy-
sis and part-of-speech tagging of written
Arabic, based on data from the Arabic
Treebank. Morphological analysis ? the
construction of all possible analyses of
isolated unvoweled wordforms ? is per-
formed as a letter-by-letter operation pre-
diction task, where the operation encodes
segmentation, part-of-speech, character
changes, and vocalization. Part-of-speech
tagging is carried out by a bi-modular tag-
ger that has a subtagger for known words
and one for unknown words. We report on
the performance of the morphological an-
alyzer and part-of-speech tagger. We ob-
serve that the tagger, which has an accu-
racy of 91.9% on new data, can be used to
select the appropriate morphological anal-
ysis of words in context at a precision of
64.0 and a recall of 89.7.
1 Introduction
Memory-based learning has been successfully ap-
plied to morphological analysis and part-of-speech
tagging in Western and Eastern-European languages
(van den Bosch and Daelemans, 1999; Daelemans et
al., 1996). With the release of the Arabic Treebank
by the Linguistic Data Consortium (current version:
3), a large corpus has become available for Ara-
bic that can act as training material for machine-
learning algorithms. The data facilitates machine-
learned part-of-speech taggers, tokenizers, and shal-
low parsing units such as chunkers, as exemplified
by Diab et al (2004).
However, Arabic appears to be a special challenge
for data-driven approaches. It is a Semitic language
with a non-concatenative morphology. In addition to
prefixation and suffixation, inflectional and deriva-
tional processes may cause stems to undergo infixa-
tional modification in the presence of different syn-
tactic features as well as certain consonants. An
Arabic word may be composed of a stem consist-
ing of a consonantal root and a pattern, affixes, and
clitics. The affixes include inflectional markers for
tense, gender, and number. The clitics may be ei-
ther attached to the beginning of stems (proclitics)
or to the end of stems (enclitics) and include pos-
sessive pronouns, pronouns, some prepositions, con-
junctions and determiners.
Arabic verbs, for example, can be conjugated ac-
cording to one of the traditionally recognized pat-
terns. There are 15 triliteral forms, of which at least
9 are in common. They represent very subtle dif-
ferences. Within each conjugation pattern, an entire
paradigm is found: two tenses (perfect and imper-
fect), two voices (active and passive) and five moods
(indicative, subjunctive, jussive, imperative and en-
ergetic). Arabic nouns show a comparably rich and
complex morphological structure. The broken plu-
ral system, for example, is highly allomorphic: for
a given singular pattern, two different plural forms
may be equally frequent, and there may be no way
to predict which of the two a particular singular will
take. For some singulars as many as three further
1
statistically minor plural patterns are also possible.
Various ways of accounting for Arabic morphol-
ogy have been proposed. The type of account of
Arabic morphology that is generally accepted by
(computational) linguists is that proposed by (Mc-
Carthy, 1981). In his proposal, stems are formed
by a derivational combination of a root morpheme
and a vowel melody. The two are arranged accord-
ing to canonical patterns. Roots are said to inter-
digitate with patterns to form stems. For exam-
ple, the Arabic stem katab (?he wrote?) is com-
posed of the morpheme ktb (?the notion of writ-
ing?) and the vowel melody morpheme ?a-a?. The
two are integrated according to the pattern CVCVC
(C=consonant, V=vowel). This means that word
structure in this morphology is not built linearly as
is the case in concatenative morphological systems.
The attempts to model Arabic morphology in a
two-level system (Kay?s (1987) Finite State Model,
Beesley?s (1990; 1998) Two-Level Model and Ki-
raz?s (1994) Multi-tape Two-Level Model) reflect
McCarthy?s separation of levels. It is beyond the
scope of this paper to provide a detailed description
of these models, but see (Soudi, 2002).
In this paper, we explore the use of memory-
based learning for morphological analysis and part-
of-speech (PoS) tagging of written Arabic. The next
section summarizes the principles of memory-based
learning. The following three sections describe our
exploratory work on memory-based morphological
analysis and PoS tagging, and integration of the two
tasks. The final two sections contain a short discus-
sion of related work and an overall conclusion.
2 Memory-based learning
Memory-based learning, also known as instance-
based, example-based, or lazy learning (Aha et al,
1991; Daelemans et al, 1999), extensions of the k-
nearest neighbor classifier (Cover and Hart, 1967),
is a supervised inductive learning algorithm for
learning classification tasks. Memory-based learn-
ing treats a set of labeled (pre-classified) training
instances as points in a multi-dimensional feature
space, and stores them as such in an instance base
in memory. Thus, in contrast to most other ma-
chine learning algorithms, it performs no abstrac-
tion, which allows it to deal with productive but low-
frequency exceptions (Daelemans et al, 1999).
An instance consists of a fixed-length vector of
n feature-value pairs, and the classification of that
particular feature-value vector. After the instance
base is stored, new (test) instances are classified by
matching them to all instances in the instance base,
and by calculating with each match the distance,
given by a distance kernel function. Classification
in memory-based learning is performed by the k-
NN algorithm that searches for the k ?nearest neigh-
bours? according to the ?(X,Y ) kernel function1 .
The distance function and the classifier can be
refined by several kernel plug-ins, such as feature
weighting (assigning larger distance to mismatches
on important features), and distance weighting (as-
signing a smaller vote in the classification to more
distant nearest neighbors). Details can be found in
(Daelemans et al, 2004).
3 Morphological analysis
We focus first on morphological analysis . Training
on data extracted from the Arabic Treebank, we in-
duce a morphological analysis generator which we
control for undergeneralization (recall errors) and
overgeneralization (precision errors).
3.1 Data
3.1.1 Arabic Treebank
Our point of departure is the Arabic Treebank 1
(ATB1), version 3.0, distributed by LDC in 2005,
more specifically the ?after treebank? PoS-tagged
data. Unvoweled tokens as they appear in the orig-
inal news paper are accompanied in the treebank
by vocalized versions; all of their morphological
analyses are generated by means of Tim Buckwal-
ter?s Arabic Morphological Analyzer (Buckwalter,
2002), and the appropriate morphological analysis is
singled out. An example is given in Figure 1. The in-
put token (INPUT STRING) is transliterated (LOOK-UP
WORD) according to Buckwalter?s transliteration sys-
tem. All possible vocalizations and their morpho-
logical analyzes are listed (SOLUTION). The analysis
is rule-based, and basically consists of three steps.
First, all possible segmentations of the input string
1All experiments with memory-based learning were per-
formed with TiMBL, version 5.1 (Daelemans et al, 2004),
available from http://ilk.uvt.nl.
2
INPUT STRING: \331\203\330\252\330\250
LOOK-UP WORD: ktb
Comment:
INDEX: P2W38
SOLUTION 1: (kataba) [katab-u_1] katab/PV+a/PVSUFF_SUBJ:3MS
(GLOSS): write + he/it [verb]
* SOLUTION 2: (kutiba) [katab-u_1] kutib/PV_PASS+a/PVSUFF_SUBJ:3MS
(GLOSS): be written/be fated/be destined + he/it [verb]
SOLUTION 3: (kutub) [kitAb_1] kutub/NOUN
(GLOSS): books
SOLUTION 4: (kutubu) [kitAb_1] kutub/NOUN+u/CASE_DEF_NOM
(GLOSS): books + [def.nom.]
SOLUTION 5: (kutuba) [kitAb_1] kutub/NOUN+a/CASE_DEF_ACC
(GLOSS): books + [def.acc.]
SOLUTION 6: (kutubi) [kitAb_1] kutub/NOUN+i/CASE_DEF_GEN
(GLOSS): books + [def.gen.]
SOLUTION 7: (kutubN) [kitAb_1] kutub/NOUN+N/CASE_INDEF_NOM
(GLOSS): books + [indef.nom.]
SOLUTION 8: (kutubK) [kitAb_1] kutub/NOUN+K/CASE_INDEF_GEN
(GLOSS): books + [indef.gen.]
SOLUTION 9: (ktb) [DEFAULT] ktb/NOUN_PROP
(GLOSS): NOT_IN_LEXICON
SOLUTION 10: (katb) [DEFAULT] ka/PREP+tb/NOUN_PROP
(GLOSS): like/such as + NOT_IN_LEXICON
Figure 1: Example token from ATB1
in terms of prefixes (0 to 4 characters long), stems (at
least one character), and suffixes (0 to 6 characters
long) are generated. Next, dictionary lookup is used
to determine if these segments are existing morpho-
logical units. Finally, the numbers of analyses is fur-
ther reduced by checking for the mutual compatibil-
ity of prefix+stem, stem+suffix, and prefix+stem
in three compatibility tables. The resulting analy-
ses have to a certain extent been manually checked.
Most importantly, a star (*) preceding a solution in-
dicates that this is the correct analysis in the given
context.
3.1.2 Preprocessing
We grouped the 734 files from the treebank into
eleven parts of approximately equal size. Ten parts
were used for training and testing our morphological
analyzer, while the final part was used as held-out
material for testing the morphological analyzer in
combination with the PoS tagger (described in Sec-
tion 4).
In the corpus the number of analyses per word
is not entirely constant, either due to the automatic
generation method or to annotator edits. As our ini-
tial goal is to predict all possible analyses for a given
word, regardless of contextual constraints, we first
created a lexicon that maps every word to all anal-
yses encountered and their respective frequencies
From the 185,061 tokens in the corpus, we extracted
16,626 unique word types ? skipping punctuation to-
kens ? and 129,655 analyses, which amounts to 7.8
analyses per type on average.
= = = = = k t b = = = ka/PREP+;ka;k;ku
= = = = k t b = = = = a/PREP+t;uti;ata;t;utu
= = = k t b = = = = = ab/PV+a/PVSUFF_SUBJ:3MS+;
b/NOUN_PROP+;ub/NOUN+i/CASE_DEF_GEN+;
ub/NOUN+a/CASE_DEF_ACC+;
ub/NOUN+K/CASE_INDEF_GEN+;
ib/PV_PASS+a/PVSUFF_SUBJ:3MS+;
ub/NOUN+N/CASE_INDEF_NOM+;ub/NOUN+;
ub/NOUN+u/CASE_DEF_NOM+
Figure 2: Instances for the analyses of the word ktb
in Figure 1.
3.1.3 Creating instances
These separate lexicons were created for training
and testing material. The lexical entries in a lexi-
con were converted to instances suitable to memory-
based learning of the mapping from words to their
analyses (van den Bosch and Daelemans, 1999). In-
stances consist of a sequence of feature values and a
corresponding class, representing a potentially com-
plex morphological operation.
The features are created by sliding a window over
the unvoweled look-up word, resulting in one in-
stance for each character. Using a 5-1-5 window
yields 11 features, i.e. the input character in focus,
plus the five preceding and five following characters.
The equal sign (=) is used as a filler symbol.
The instance classes represent the morphological
analyses. The classes corresponding to a word?s
characters should enable us to derive all associated
analyses. This implies that the classes need to en-
code several aspects simultaneously: vocalization,
morphological segmentation and tagging. The fol-
lowing template describes the format of classes:
class = subanalysis; subanalysis; ...
subanalysis = preceding vowels & tags +
input character +
following vowels & tags
For example, the classes of the instances in Fig-
ure 2 encode the ten solutions for the word ktb in
Figure 1. The ratio behind this encoding is that
it allows for a simple derivation of the solution,
akin to the way that the pieces of a jigsaw puz-
zle can be combined. We can exhaustively try all
combinations of the subanalyses of the classes, and
check if the right side of one subanalysis matches
the left side of a subsequent subanalysis. This re-
construction process is illustrated in Figure 3 (only
two reconstructions are depicted, corresponding to
SOLUTION 1 and SOLUTION 4). For exam-
ple, the subanalysis ka from the first class in Fig-
ure 2 matches the subanalysis ata from the sec-
3
ka ku
ata utu
ab/PV+a/PVSUFF_SUBJ:3MS ub/NOUN+u/CASE_DEF_NOM
-------------------------- + ------------------------- +
katab/PV+a/PVSUFF_SUBJ:3MS kutub/NOUN+u/CASE_DEF_NOM
Figure 3: Illustration of how two morphological
analyses are reconstructed from the classes in Fig-
ure 2.
ond class, which in turn matches the subanaly-
sis ab/PV+a/PVSUFF SUBJ:3MS from the third
class; together these constitute the complete analysis
katab/PV+a/PVSUFF SUBJ:3MS.
3.2 Initial Experiments
To test the feasibility of our approach, we first train
and test on the full data set. Timbl is used with its de-
fault settings (overlap distance function, gain-ratio
feature weighting, k = 1). Rather than evaluating
on the accuracy of predicting the complex classes,
we evaluate on the complete correctness of all recon-
structed analyses, in terms of precision, recall, and
F-score (van Rijsbergen, 1979). As expected, this
results in a near perfect recall (97.5). The precision,
however, is much lower (52.5), indicating a substan-
tial amount of analysis overgeneration; almost one
in two generated analyses is actually not valid. With
an F-score of only 68.1, we are clearly not able to
reproduce the training data perfectly.
Next we split the data in 9 parts for training and
1 part for testing. The k-NN classifier is again used
with its default settings. Table 1 shows the results
broken down into known and unknown words. As
known words can be looked up in the lexicon derived
from the training material, the first row presents
the results with lookup and the second row without
lookup (that is, with prediction). The fact that even
with lookup the performance is not perfect shows
that the upper bound for this task is not 100%. The
reason is that apparantly some words in the test ma-
terial have received analyses that never occur in the
training material and vice versa. For known words
without lookup, the recall is still good, but the preci-
sion is low. This is consistent with the initial results
mentioned above. For unknown words, both recall
and precison are much worse, indicating rather poor
generalization.
To sum up, there appear to be problems with both
the precision and the recall. The precision is low for
known words and even worse for unknown words.
#Wrds Prec Rec F
Known with lookup 3220 92.6 98.1 95.3
Known without lookup 3220 49.9 95.0 65.5
Unknown 847 22.8 26.8 24.7
Table 1: Results of initial experiments split into
known and unknown words, and with and without
lookup of known words.
#Wrds Prec Rec F
Known 3220 15.6 99.0 26.9
Unknown 847 3.9 66.8 7.5
Table 2: Results of experiments for improving the
recall, split into known and unknown words.
Analysis overgeneration seems to be a side effect
of the way we encode and reconstruct the analyses.
The recall is low for unknown words only. There
appear to be at least two reasons for this undergen-
eration problem. First, if just one of the predicted
classes is incorrect (one of the pieces of the jigsaw
puzzle is of the wrong shape) then many, or even all
of the reconstructions fail. Second, some generaliza-
tions cannot be made, because infrequent classes are
overshadowed by more frequent ones with the same
features. Consider, for example, the instance for the
third character (l) of the word jEl:
= = = j E l = = = = =
Its real class in the test data is:
al/VERB_PERFECT+;ol/NOUN+
When the k-NN classifier is looking for its nearest
neighbors, it finds three; two with a ?verb imperfect?
tag, and one with a ?noun? tag.
{ al/VERB_IMPERFECT+ 2, ol/NOUN+ 1}
Therefore, the class predicted by the classifier is
al/VERB IMPERFECT+, because this is the majority
class in the NN-set. So, although a part of the cor-
rect solution is present in the NN-set, simple major-
ity voting prevents it from surfacing in the output.
3.3 Improving recall
In an attempt to address the low recall, we revised
our experimental setup to take advantage of the com-
plete NN-set. As before, the k-NN classifier is used,
4
Prec Rec F
Known 58.6 (0.4) 66.6 (0.5) 62.4 (0.3)
Unknown 28.7 (3.7) 37.2 (1.2) 32.2 (2.5)
All 53.4 (1.2) 62.2 (0.6) 57.5 (0.8)
Table 3: Average results and SD of the 10-fold CV
experiment, split into known and unknown words
but rather than relying on the classifier to do the ma-
jority voting over the (possibly weighted) classes in
the k-NN set and to output a single class, we perform
a reconstruction of analyses combining all classes in
the k-NN set. To allow for more classes in k-NN?s
output, we increase k to 3 while keeping the other
settings as before. As expected, this approach in-
creases the number of analyses. This, in turn, in-
creases the recall dramatically, up to nearly perfect
for known words; see Table 2. However, this gain
in recall is at the expense of the precision, which
drops dramatically. So, although our revised ap-
proach solves the issues above, it introduces massive
overgeneration.
3.4 Improving precision
We try to tackle the overgeneration problem by fil-
tering the analyses in two ways. First, by ranking
the analyses and limiting output to the n-best. The
ranking mechanism relies on the distribution of the
classes in the NN-set. Normally, some classes occur
more frequently than others in the NN-set. During
the reconstruction of a particular analysis, we sum
the frequencies of the classes involved. The result-
ing score is then used to rank the analyses in de-
creasing order, which we filter by taking the n-best.
The second filter employs the fact that only cer-
tain sequences of morphological tags are valid. Tag
bigrams are already implicit in the way that the
classes are constructed, because a class contains
the tags preceding and following the input charac-
ter. However, cooccurrence restrictions on tags may
stretch over longer distances; tag trigram informa-
tion is not available at all. We therefore derive a
frequency list of all tag trigrams occurring in the
training data. This information is then used to filter
analyses containing tag trigrams occurring below a
certain frequency threshold in the training data.
Both filters were optimized on the fold that was
used for testing so far, maximizing the overall F-
score. This yieled an n-best value of 40 and tag
frequency treshold of 250. Next, we ran a 10-fold
cross-validation experiment on all data (except the
held out data) using the method described in the pre-
vious section in combination with the filters. Aver-
age scores of the 10 folds are given in Table 3. In
comparison with the initial results, both precision
and recall on unknown words has improved, indi-
cating that overgeneration and undergeneration can
be midly counteracted.
3.5 Discussion
Admittedly, the performance is not very impressive.
We have to keep in mind, however, that the task is
not an easy one. It includes vowel insertion in am-
biguous root forms, which ? in contrast to vowel in-
sertion in prefixes and suffixes ? is probably irreg-
ular and unpredictable, unless the appropriate stem
would be known. As far as the evaluation is con-
cerned, we are unsure whether the analyses found
in the treebank for a particular word are exhaus-
tive. If not, some of the predictions that are currently
counted as precision errors (overgeneration) may in
fact be correct alternatives.
Since instances are generated for each type rather
than for each token in the data, the effect of to-
ken frequency on classification is lost. For exam-
ple, instances from frequent tokens are more likely
to occur in the k-NN set, and therefore their (par-
tial) analyses will show up more frequently. This is
an issue to explore in future work. Depending on
the application, it may also make sense to optimize
on the correct prediction of unkown words, or on in-
creasing only the recall.
4 Part-of-speech tagging
We employ MBT, a memory-based tagger-generator
and tagger (Daelemans et al, 1996) to produce a
part-of-speech (PoS) tagger based on the ATB1 cor-
pus2. We first describe how we prepared the corpus
data. We then describe how we generated the tag-
ger (a two-module tagger with a module for known
words and one for unknown words), and subse-
quently we report on the accuracies obtained on test
material by the generated tagger. We conclude this
2In our experiments we used the MBT software pack-
age, version 2 (Daelemans et al, 2003), available from
http://ilk.uvt.nl/.
5
w CONJ
bdA VERB_PERFECT
styfn NOUN_PROP
knt NOUN_PROP
nHylA ADJ+NSUFF_MASC_SG_ACC_INDEF
jdA ADV
, PUNC
AlA ADV
>n FUNC_WORD
...
Figure 4: Part of an ATB1 sentence with unvoweled
words (left) and their respective PoS tags (right).
section by describing the effect of using the output
of the morphological analyzer as extra input to the
tagger.
4.1 Data preparation
While the morphological analyzer attempts to gener-
ate all possible analyses for a given unvoweled word,
the goal of PoS tagging is to select one of these
analyses as the appropriate one given the context,
as the annotators of the ATB1 corpus did using the
* marker. We developed a PoS tagger that is trained
to predict an unvoweled word in context, a concate-
nation of the PoS tags of its morphemes. Essentially
this is the task of the morphological analyzer with-
out segmentation and vocalization. Figure 4 shows
part of a sentence where for each word the respective
tag is given in the second column. Concatenation is
marked by the delimiter +.
We trained on the full ten folds used in the previ-
ous sections, and tested on the eleventh fold. The
training set thus contains 150,966 words in 4,601
sentences; the test set contains 15,102 words in 469
sentences. 358 unique tags occur in the corpus. In
the test set 947 words occur that do not occur in the
training set.
4.2 Memory-based tagger generator
Memory-based tagging is based on the idea that
words occurring in similar contexts will have the
same PoS tag. A particular instantiation, MBT, was
proposed in (Daelemans et al, 1996). MBT has three
modules. First, it has a lexicon module which stores
for all words occurring in the provided training cor-
pus their possible PoS tags (tags which occur below
a certain threshold, default 5%, are ignored). Sec-
ond, it generates two distinct taggers; one for known
words, and one for unknown words.
The known-word tagger can obviously benefit
from the lexicon, just as a morphological analyzer
could. The input on which the known-word tag-
ger bases its prediction for a given focus word con-
sists of the following set of features and parameter
settings: (1) The word itself, in a local context of
the two preceding words and one subsequent word.
Only the 200 most frequent words are represented
as themselves; other words are reduced to a generic
string ? cf. (Daelemans et al, 2003) for details. (2)
The possible tags of the focus word, plus the pos-
sible tags of the next word, and the disambiguated
tags of two words to the left (which are available be-
cause the tagger operates from the beginning to the
end of the sentence). The known-words tagger is
based on a k-NN classifier with k = 15, the modi-
fied value difference metric (MVDM) distance func-
tion, inverse-linear distance weighting, and GR fea-
ture weighting. These settings were manually opti-
mized on a held-out validation set (taken from the
training data).
The unknown-word tagger attempts to derive as
much information as possible from the surface form
of the word, by using its suffix and prefix letters as
features. The following set of features and param-
eters are used: (1) The three prefix characters and
the four suffix characters of the focus word (possi-
bly encompassing the whole word); (2) The possible
tags of the next word, and the disambiguated tags
of two words to the left. The unknown-words tag-
ger is based on a k-NN classifier with k = 19, the
modified value difference metric (MVDM) distance
function, inverse-linear distance weighting, and GR
feature weighting ? again, manually tuned on vali-
dation material.
The accuracy of the tagger on the held-out cor-
pus is 91.9% correctly assigned tags. On the 14155
known words in the test set the tagger attains an ac-
curacy of 93.1%; on the 947 unknown words the ac-
curacy is considerably lower: 73.6%.
5 Integrating morphological analysis and
part-of-speech tagging
While morphological analysis and PoS tagging are
ends in their own right, the usual function of the
two modules in higher-level natural-language pro-
cessing or text mining systems is that they jointly
determine for each word in a text the appropriate
single morpho-syntactic analysis. In our setup, this
6
All words Known words Unknown words
Part-of-speech source Precision Recall Precision Recall Precision Recall
Gold standard 70.1 97.8 75.8 99.5 30.2 73.4
Predicted 64.0 89.7 69.8 92.0 23.9 59.0
Table 4: Precision and recall of the identification of the contextually appropriate morphological analysis,
measured on all test words and split on known words and unknown words. The top line represents the upper-
bound experiment with gold-standard PoS tags; the bottom line represents the experiment with predicted PoS
tags.
amounts to predicting the solution that is preceded
by ?*? in the original ATB1 data. For this purpose,
the PoS tag predicted by MBT, as described in the
previous section, serves to select the morphological
analysis that is compatible with this tag. We em-
ployed the following two rules to implement this:
(1) If the input word occurs in the training data,
then look up the morphological analyses of the word
in the training-based lexicon, and return all mor-
phological analyses with a PoS content matching
the tag predicted by the tagger. (2) Otherwise, let
the memory-based morphological analyzer produce
analyses, and return all analyses with a PoS content
matching the predicted tag.
We first carried out an experiment integrating the
output of the morphological analyzer and the PoS
tagger, faking perfect tagger predictions, in order to
determine the upper bound of this approach. Rather
than predicting the PoS tag with MBT, we directly
derived the PoS tag from the annotations in the tree-
bank. The upper result line in Table 4 displays the
precision and recall scores on the held-out data of
identifying the appropriate morphological analysis,
i.e. the solution marked by *. Unsurprisingly, the
recall on known words is 99.5%, since we are us-
ing the gold-standard PoS tag which is guaranteed
to be among the training-based lexicon, except for
some annotation discrepancies. More interestingly,
about one in four analyses of known words matching
on PoS tags actually mismatches on vowel or conso-
nant changes, e.g. because it represents a different
stem ? which is unpredictable by our method.
About one out of four unknown words has mor-
phological analyses that do not match the gold-
standard PoS (a recall of 73.4); at the same time,
a considerable amount of overgeneration of analy-
ses accounts for the low amount of analyses that
matches (a precision of 30.2).
Next, the experiment was repeated with predicted
PoS tags and morphological analyses. The results
are presented in the bottom result line of Table 4.
The precision and recall of identifying correct anal-
yses of known words degrades as compared to the
upper-bounds results due to incorrect PoS tag pre-
dictions. On unknown words the combination of
heavy overgeneration by the morphological analyzer
and the 73.6% accuracy of the tagger leads to a low
precision of 23.9 and a fair recall of 59.0. On both
known and unknown words the integration of the
morphological analyzer and the tagger is able to nar-
row down the analyses by the analyzer to a subset of
matching analyses that in about nine out of ten cases
contains the ?* SOLUTION? word.
6 Related work
The application of machine learning methods to
Arabic morphology and PoS tagging appears to
be somewhat limited and recent, compared to the
vast descriptive and rule-based literature particularly
on morphology (Kay, 1987; Beesley, 1990; Kiraz,
1994; Beesley, 1998; Cavalli-Sfora et al, 2000;
Soudi, 2002).
We are not aware of any machine-learning ap-
proach to Arabic morphology, but find related is-
sues treated in (Daya et al, 2004), who propose a
machine-learning method augmented with linguistic
constraints to identifying roots in Hebrew words ?
a related but reverse task to ours. Arabic PoS tag-
ging seems to have attracted some more attention.
Freeman (2001) describes initial work in developing
a PoS tagger based on transformational error-driven
learning (i.e. the Brill tagger), but does not provide
performance analyses. Khoja (2001) reports a 90%
accurate morpho-syntactic statistical tagger that uses
7
the Viterbi algorithm to select a maximally-likely
part-of-speech tag sequence over a sentence. Diab
et al (2004) describe a part-of-speech tagger based
on support vector machines that is trained on tok-
enized data (clitics are separate tokens), reporting a
tagging accuracy of 95.5%.
7 Conclusions
We investigated the application of memory-based
learning (k-nearest neighbor classification) to mor-
phological analysis and PoS tagging of unvoweled
written Arabic, using the ATB1 corpus as training
and testing material. The morphological analyzer
was shown to attain F-scores of 0.32 on unknown
words when predicting all aspects of the analysis,
including vocalization (a partly unpredictable task,
certainly if no context is available). The PoS tag-
ger attains an accuracy of about 74% on unknown
words, and 92% on all words (including known
words). A combination of the two which selects
from the set of generated analyses a subset of anal-
yses with the PoS predicted by the tagger, yielded
a recall of the contextually appropriate analysis of
0.90 on test words, yet a low precision of 0.64
largely caused by overgeneration of invalid analy-
ses.
We make two final remarks. First, memory-
based morphological analysis of Arabic words ap-
pears feasible, but its main limitation is its inevitable
inability to recognize the appropriate stem of un-
known words on the basis of the ambiguous root
form input; our current method simply overgener-
ates vocalizations, keeping high recall at the cost of
low precision. Second, memory-based PoS tagging
of written Arabic text also appears to be feasible; the
observed performances are roughly comparable to
those observed for other languages. The PoS tagging
task as we define it is deliberately separated from the
problem of vocalization, which is in effect the prob-
lem of stem identification. We therefore consider the
automatic identification of stems as a component of
full morpho-syntactic analysis of written Arabic an
important issue for future research.
References
D. W. Aha, D. Kibler, and M. Albert. 1991. Instance-based
learning algorithms. Machine Learning, 6:37?66.
K. Beesley. 1990. Finite-state description of Arabic morphol-
ogy. In Proceedings of the Second Cambridge Conference:
Bilingual Computing in Arabic and English.
K. Beesley. 1998. Consonant spreading in Arabic stems. In
Proceedings of COLING-98.
T. Buckwalter. 2002. Buckwalter Arabic morpho-
logical analyzer version 1.0. Technical Report
LDC2002L49, Linguistic Data Consortium. available
from http://www.ldc.upenn.edu/.
V. Cavalli-Sfora, A. Soudi, and M. Teruko. 2000. Arabic
morphology generation using a concatenative strategy. In
Proceedings of the First Conference of the North-American
Chapter of the Association for Computational Linguistics,
Seattle, WA, USA.
T. M. Cover and P. E. Hart. 1967. Nearest neighbor pattern
classification. Institute of Electrical and Electronics Engi-
neers Transactions on Information Theory, 13:21?27.
W. Daelemans, J. Zavrel, P. Berck, and S. Gillis. 1996. MBT: A
memory-based part of speech tagger generator. In E. Ejerhed
and I. Dagan, editors, Proceedings of Fourth Workshop on
Very Large Corpora, pages 14?27. ACL SIGDAT.
W. Daelemans, A. van den Bosch, and J. Zavrel. 1999. For-
getting exceptions is harmful in language learning. Ma-
chine Learning, Special issue on Natural Language Learn-
ing, 34:11?41.
W. Daelemans, J. Zavrel, A. van den Bosch, and K. van der
Sloot. 2003. MBT: Memory based tagger, version 2.0, ref-
erence guide. ILK Technical Report 03-13, Tilburg Univer-
sity.
W. Daelemans, J. Zavrel, K. van der Sloot, and
A. van den Bosch. 2004. TiMBL: Tilburg memory
based learner, version 5.1, reference guide. ILK Technical
Report 04-02, Tilburg University.
E. Daya, D. Roth, and S. Wintner. 2004. Learning Hebrew
roots: Machine learning with linguistic constraints. In Pro-
ceedings of EMNLP?04, Barcelona, Spain.
M. Diab, K. Hacioglu, and D. Jurafsky. 2004. Automatic tag-
ging of arabic text: From raw text to base phrase chunks. In
Proceedings of HLT/NAACL-2004.
A. Freeman. 2001. Brill?s POS tagger and a morphology parser
for Arabic. In ACL/EACL-2001 Workshop on Arabic Lan-
guage Processing: Status and Prospects, Toulouse, France.
M. Kay. 1987. Non-concatenative finite-state morphology. In
Proceedings of the third Conference of the European Chap-
ter of the Association for Computational Linguistics, pages
2?10, Copenhagen, Denmark.
S. Khoja. 2001. APT: Arabic part-of-speech tagger. In Pro-
ceedings of the Student Workshop at NAACL-2001.
G. Kiraz. 1994. Multi-tape two-level morphology: A case
study in semitic non-linear morphology. In Proceedings of
COLING?94, volume 1, pages 180?186.
J. McCarthy. 1981. A prosodic theory of non-concatenative
morphology. Linguistic Inquiry, 12:373?418.
A. Soudi. 2002. A Computational Lexeme-based Treatment of
Arabic Morphology. Ph.D. thesis, Mohamed V University
(Morocco) and Carnegie Mellon University (USA).
A. van den Bosch and W. Daelemans. 1999. Memory-based
morphological analysis. In Proceedings of the 37th Annual
Meeting of the ACL, pages 285?292, San Francisco, CA.
Morgan Kaufmann.
C.J. van Rijsbergen. 1979. Information Retrieval. Butter-
sworth, London.
8
Proceedings of the ACL Workshop on Empirical Modeling of Semantic Equivalence and Entailment, pages 1?6,
Ann Arbor, June 2005. c?2005 Association for Computational Linguistics
Classification of semantic relations by humans and machines ?
Erwin Marsi and Emiel Krahmer
Communication and Cognition
Tilburg University, The Netherlands
{e.c.marsi, e.j.krahmer}@uvt.nl
Abstract
This paper addresses the classification of
semantic relations between pairs of sen-
tences extracted from a Dutch parallel cor-
pus at the word, phrase and sentence level.
We first investigate the performance of hu-
man annotators on the task of manually
aligning dependency analyses of the re-
spective sentences and of assigning one
of five semantic relations to the aligned
phrases (equals, generalizes, specifies, re-
states and intersects). Results indicate that
humans can perform this task well, with
an F-score of .98 on alignment and an F-
score of .95 on semantic relations (after
correction). We then describe and evalu-
ate a combined alignment and classifica-
tion algorithm, which achieves an F-score
on alignment of .85 (using EuroWordNet)
and an F-score of .80 on semantic relation
classification.
1 Introduction
An automatic method that can determine how two
sentences relate to each other in terms of seman-
tic overlap or textual entailment (e.g., (Dagan and
Glickman, 2004)) would be a very useful thing to
have for robust natural language applications. A
summarizer, for instance, could use it to extract
the most informative sentences, while a question-
answering system ? to give a second example ?
could use it to select potential answer string (Pun-
yakanok et al, 2004), perhaps preferring more spe-
cific answers over more general ones. In general, it
?This work was carried out within the IMIX-IMOGEN (In-
teractive Multimodal Output Generation) project, sponsored by
the Netherlands Organization of Scientific Research (NWO).
is very useful to know whether some sentence S is
more specific (entails) or more general than (is en-
tailed by) an alternative sentence S?, or whether the
two sentences express essentially the same informa-
tion albeit in a different way (paraphrasing).
Research on automatic methods for recognizing
semantic relations between sentences is still rela-
tively new, and many basic issues need to be re-
solved. In this paper we address two such related is-
sues: (1) to what extent can human annotators label
semantic overlap relations between words, phrases
and sentences, and (2) what is the added value of
linguistically informed analyses.
It is generally assumed that pure string overlap
is not sufficient for recognizing semantic relations;
and that using some form of syntactic analysis may
be beneficial (e.g., (Herrera et al, 2005), (Vander-
wende et al, 2005)). Our working hypothesis is that
semantic overlap at the word and phrase levels may
provide a good basis for deciding the semantic re-
lation between sentences. Recognising semantic re-
lations between sentences then becomes a two-step
procedure: first, the words and phrases in the re-
spective sentences need to be aligned, after which
the relations between the pairs of aligned words and
phrases should be labeled in terms of semantic rela-
tions.
Various alignment algorithms have been devel-
oped for data-driven approaches to machine trans-
lation (e.g. (Och and Ney, 2000)). Initially work
focused on word-based alignment, but more and
more work is also addressing alignment at the higher
levels (substrings, syntactic phrases or trees), e.g.,
(Meyers et al, 1996), (Gildea, 2003). For our pur-
poses, an additional advantage of aligning syntac-
tic structures is that it keeps the alignment feasible
(as the number of arbitrary substrings that may be
aligned grows exponentially to the number of words
1
in the sentence). Here, following (Herrera et al,
2005) and (Barzilay, 2003), we will align sentences
at the level of dependency structures. In addition,
we will label the alignments in terms of five basic
semantic relations to be defined below. We will per-
form this task both manually and automatically, so
that we can address both of the issues raised above.
Section 2 describes a monolingual parallel cor-
pus consisting of two Dutch translations, and for-
malizes the alignment-classification task to be per-
formed. In section 3 we report the results on align-
ment, first describing interannotator agreement on
this task and then the results on automatic alignment.
In section 4, then, we address the semantic relation
classification; again, first describing interannotator
results, followed by results obtained using memory-
based machine learning techniques. We end with a
general discussion.
2 Corpus and Task definition
2.1 Corpus
We have developed a parallel monolingual corpus
consisting of two different Dutch translations of the
French book ?Le petit prince? (the little prince) by
Antoine de Saint-Exupe?ry (published 1943), one by
Laetitia de Beaufort-van Hamel (1966) and one by
Ernst Altena (2000). For our purposes, this proved
to be a good way to quickly find a large enough
set of related sentence pairs, which differ semanti-
cally in interesting and subtle ways. In this work,
we used the first five chapters, with 290 sentences
and 3600 words in the first translation, and 277 sen-
tences and 3358 words in the second translation.
The texts were automatically tokenized and split into
sentences, after which errors were manually cor-
rected. Corresponding sentences from both trans-
lations were manually aligned; in most cases this
was a one-to-one mapping, but occasionally a sin-
gle sentence in one translation mapped onto two or
more sentences in the other: this occurred 23 times
in all five chapters. Next, the Alpino parser for
Dutch (e.g., (Bouma et al, 2001)) was used for part-
of-speech tagging and lemmatizing all words, and
for assigning a dependency analysis to all sentences.
The POS labels indicate the major word class (e.g.
verb, noun, adj, and adv). The dependency rela-
tions hold between tokens and are identical to those
used in the Spoken Dutch Corpus. These include de-
pendencies such as head/subject, head/modifier and
coordination/conjunction. If a full parse could not
be obtained, Alpino produced partial analyses col-
lected under a single root node. Errors in lemmati-
zation, POS tagging, and syntactic dependency pars-
ing were not subject to manual correction.
2.2 Task definition
The task to be performed can be described infor-
mally as follows: given two dependency analyses,
align those nodes that are semantically related. More
precisely: For each node v in the dependency struc-
ture for a sentence S, we define STR(v) as the sub-
string of all tokens under v (i.e., the composition of
the tokens of all nodes reachable from v). An align-
ment between sentences S and S? pairs nodes from
the dependency graphs for both sentences. Aligning
node v from the dependency graph D of sentence
S with node v? from the graph D? of S? indicates
that there is a semantic relation between STR(v) and
STR(v?), that is, between the respective substrings
associated with v and v?. We distinguish five po-
tential, mutually exclusive, relations between nodes
(with illustrative examples):
1. v equals v? iff STR(v) and STR(v?) are literally
identical (abstracting from case). Example: ?a
small and a large boa-constrictor? equals ?a
large and a small boa-constrictor?;
2. v restates v? iff STR(v) is a paraphrase of
STR(v?) (same information content but differ-
ent wording). Example: ?a drawing of a boa-
constrictor snake? restates ?a drawing of a boa-
constrictor?;
3. v specifies v? iff STR(v) is more specific than
STR(v?). Example: ?the planet B 612? specifies
?the planet?;
4. v generalizes v? iff STR(v?) is more specific
than STR(v). Example: ?the planet? general-
izes ?the planet B 612?;
5. v intersects v? iff STR(v) and STR(v?) share
some informational content, but also each ex-
press some piece of information not expressed
in the other. Example: ?Jupiter and Mars? in-
tersects ?Mars and Venus?
Figure 1 shows an example alignment with seman-
tic relations between the dependency structures of
2
hebben
komen
hebben
ik
ik op in in aanraking met
zo contact met in de loop van
veel
heel
persoon
serieus veel
massa
gewichtig
heel
leven
mijn
leven
het
manier
die mens
Figure 1: Dependency structures and alignment for the sentences Zo heb ik in de loop van mijn leven heel
veel contacten gehad met heel veel serieuze personen. (lit. ?Thus have I in the course of my life very
many contacts had with very many serious persons?) and Op die manier kwam ik in het leven met massa?s
gewichtige mensen in aanraking.. (lit. ?In that way came I in the life with mass-of weighty/important people
in touch?). The alignment relations are equals (dotted gray), restates (solid gray), specifies (dotted black),
and intersects (dashed gray). For the sake of transparency, dependency relations have been omitted.
two sentences. Note that there is an intuitive rela-
tion with entailment here: both equals and restates
can be understood as mutual entailment (i.e., if the
root nodes of the analyses corresponding S and S?
stand in an equal or restate relation, S entails S? and
S? entails S), if S specifies S? then S also entails S?
and if S generalizes S? then S is entailed by S?.
In remainder of this paper, we will distinguish two
aspects of this task: alignment is the subtask of pair-
ing related nodes ? or more precise, pairing the to-
ken strings corresponding to these nodes; classifica-
tion of semantic relations is the subtask of labeling
these alignments in terms of the five types of seman-
tic relations.
2.3 Annotation procedure
For creating manual alignments, we developed a
special-purpose annotation tool which shows, side
by side, two sentences, as well as their respective
dependency graphs. When the user clicks on a node
v in the graph, the corresponding string (STR(v)) is
shown at the bottom. The tool enables the user to
manually construct an alignment graph on the basis
of the respective dependency graphs. This is done by
focusing on a node in the structure for one sentence,
and then selecting a corresponding node (if possible)
in the other structure, after which the user can select
the relevant alignment relation. The tool offers addi-
tional support for folding parts of the graphs, high-
lighting unaligned nodes and hiding dependency re-
lation labels.
All text material was aligned by the two authors.
They started with annotating the first ten sentences
of chapter one together in order to get a feel for
the task. They continued with the remaining sen-
tences from chapter one individually (35 sentences
and 521 in the first translation, and 35 sentences and
481 words in the second translation). Next, both
annotators discussed annotation differences, which
triggered some revisions in their respective annota-
tion. They also agreed on a single consensus annota-
tion. Interannotator agreement will be discussed in
the next two sections. Finally, each author annotated
two additional chapters, bringing the total to five.
3 Alignment
3.1 Interannotator agreement
Interannotator agreement was calculated in terms of
precision, recall and F-score (with ? = 1) on aligned
3
(A1, A2) (A1? , A2?) (Ac, A1?) (Ac, A2?)
#real: 322 323 322 322
#pred: 312 321 323 321
#correct: 293 315 317 318
precision: .94 .98 .98 .99
recall: .91 .98 .98 .99
F-score: .92 .98 .98 .99
Table 1: Interannotator agreement with respect
to alignment between annotators 1 and 2 before
(A1, A2) and after (A1? , A2?) revision , and between
the consensus and annotator 1 (Ac, A1?) and annota-
tor 2 (Ac, A2?) respectively.
node pairs as follows:
precision = | Areal ? Apred | / | Apred | (1)
recall = | Areal ? Apred | / | Areal | (2)
F -score = (2 ? prec ? rec) / (prec + rec) (3)
where Areal is the set of all real alignments (the ref-
erence or golden standard), Apred is the set of all
predicted alignments, and Apred?Areal is the set al
correctly predicted alignments. For the purpose of
calculating interannotator agreement, one of the an-
notations (A1) was considered the ?real? alignment,
the other (A2) the ?predicted?. The results are sum-
marized in Table 1 in column (A1, A2).1
As explained in section 2.3, both annotators re-
vised their initial annotations. This improved their
agreement, as shown in column (A1? , A2?). In ad-
dition, they agreed on a single consensus annotation
(Ac). The last two columns of Table 1 show the re-
sults of evaluating each of the revised annotations
against this consensus annotation. The F-score of
.98 can therefore be regarded as the upper bound on
the alignment task.
3.2 Automatic alignment
Our tree alignment algorithm is based on the dy-
namic programming algorithm in (Meyers et al,
1996), and similar to that used in (Barzilay, 2003).
It calculates the match between each node in de-
pendency tree D against each node in dependency
tree D?. The score for each pair of nodes only de-
pends on the similarity of the words associated with
the nodes and, recursively, on the scores of the best
1Note that since there are no classes, we can not calculate
change agreement rethe Kappa statistic.
matching pairs of their descendants. The node simi-
larity function relies either on identity of the lemmas
or on synonym, hyperonym, and hyponym relations
between them, as retrieved from EuroWordNet.
Automatic alignment was evaluated with the con-
sensus alignment of the first chapter as the gold
standard. A baseline was constructed by aligning
those nodes which stand in an equals relation to each
other, i.e., a node v in D is aligned to a node v?
in D? iff STR(v) =STR(v?). This baseline already
achieves a relatively high score (an F-score of .56),
which may be attributed to the nature of our mate-
rial: the translated sentence pairs are relatively close
to each other and may show a sizeable amount of lit-
eral string overlap. In order to test the contribution
of synonym and hyperonym information for node
matching, performance is measured with and with-
out the use of EuroWordNet. The results for auto-
matic alignment are shown in Table 2. In compari-
son with the baseline, the alignment algorithm with-
out use of EuroWordnet loses a few points on preci-
sion, but improves a lot on recall (a 200% increase),
which in turn leads to a substantial improvement on
the overall F-score. The use of EurWordNet leads to
a small increase (two points) on both precision and
recall, and thus to small increase in F-score. How-
ever, in comparison with the gold standard human
score for this task (.95), there is clearly room for
further improvement.
4 Classification of semantic relations
4.1 Interannotator agreement
In addition to alignment, the annotation procedure
for the first chapter of The little prince by two anno-
tators (cf. section 2.3) also involved labeling of the
semantic relation between aligned nodes. Interanno-
tator agreement on this task is shown Table 3, before
and after revision. The measures are weighted preci-
sion, recall and F-score. For instance, the precision
is the weighted sum of the separate precision scores
for each of the five relations. The table also shows
the ?-score. The F-score of .97 can be regarded as
the upper bound on the relation labeling task. We
think these numbers indicate that the classification
of semantic relations is a well defined task which
can be accomplished with a high level of interanno-
tator agreement.
4
Alignment : Prec : Rec : F-score:
baseline .87 .41 .56
algorithm without wordnet .84 .82 .83
algorithm with wordnet .86 .84 .85
Table 2: Precision, recall and F-score on automatic
alignment
(A1, A2) (A1? , A2?) (Ac, A1?) (Ac, A2?)
precision: .86 .96 .98 .97
recall: .86 .95 .97 .97
F-score: .85 .95 .97 .97
?: .77 .92 .96 .96
Table 3: Interannotator agreement with respect to se-
mantic relation labeling between annotators 1 and 2
before (A1, A2) and after (A1? , A2?) revision , and
between the consensus and annotator 1 (Ac, A1?)
and annotator 2 (Ac, A2?) respectively.
4.2 Automatic classification
For the purpose of automatic semantic relation la-
beling, we approach the task as a classification prob-
lem to be solved by machine learning. Alignments
between node pairs are classified on the basis of the
lexical-semantic relation between the nodes, their
corresponding strings, and ? recursively ? on previ-
ous decisions about the semantic relations of daugh-
ter nodes. The input features used are:
? a boolean feature representing string identity
between the strings corresponding to the nodes
? a boolean feature for each of the five semantic
relations indicating whether the relation holds
for at least one of the daughter nodes;
? a boolean feature indicating whether at least
one of the daughter nodes is not aligned;
? a categorical feature representing the lexical se-
mantic relation between the nodes (i.e. the
lemmas and their part-of-speech) as found in
EuroWordNet, which can be synonym, hyper-
onym, or hyponym.2
To allow for the use of previous decisions, the
nodes of the dependency analyses are traversed in
a bottom-up fashion. Whenever a node is aligned,
the classifier assigns a semantic label to the align-
ment. Taking previous decisions into account may
2These three form the bulk of all relations in Dutch Eu-
roWordnet. Since no word sense disambiguation was involved,
we simply used all word senses.
Prec : Rec : F-score:
equals .93? .06 .95? .04 .94? .02
restates .56? .08 .78? .04 .65? .05
specifies n.a. 0 n.a.
generalizes .19? .06 .37? .09 .24? .05
intersects n.a. 0 n.a.
Combined: .62? .01 .70? .02 .64? .02
Table 4: Average precision, recall and F-score (and
SD) over all 5 folds on automatic classification of
semantic relations
cause a proliferation of errors: wrong classification
of daughter nodes may in turn cause wrong classifi-
cation of the mother node. To investigate this risk,
classification experiments were run both with and
without (i.e. using the annotation) previous deci-
sions.
Since our amount of data is limited, we used
a memory-based classifier, which ? in contrast to
most other machine learning algorithms ? performs
no abstraction, allowing it to deal with productive
but low-frequency exceptions typically occurring in
NLP tasks(Daelemans et al, 1999). All memory-
based learning was performed with TiMBL, version
5.1 (Daelemans et al, 2004), with its default set-
tings (overlap distance function, gain-ratio feature
weighting, k = 1).
The five first chapters of The little prince were
used to run a 5-fold cross-validated classification ex-
periment. The first chapter is the consensus align-
ment and relation labeling, while the other four were
done by one out of two annotators. The alignments
to be classified are those from to the human align-
ment. The baseline of always guessing equals ? the
majority class ? gives a precision of 0.26, a recall of
0.51, and an F-score of 0.36. Table 4 presents the re-
sults broken down to relation type. The combined F-
score of 0.64 is almost twice the baseline score. As
expected, the highest score goes to equals, followed
by a reasonable score on restates. Performance on
the other relation types is rather poor, with even no
predictions of specifies and intersects at all.
Faking perfect previous decisions by using the
annotation gives a considerable improvement, as
shown in Table 5, especially on specifies, general-
izes and intersects. This reveals that the prolifera-
tion of classification errors is indeed a problem that
should be addressed.
5
Prec : Rec : F-score:
equals .99? .02 .97? .02 .98? .01
restates .65? .04 .82? .04 .73? .03
specifies .60? .12 .48? .10 .53? .09
generalizes .50? .11 .52? .10 .50? .09
intersects .69? .27 .35? .12 .46? .16
Combined: .82? .02 .81? .02 .80? .02
Table 5: Average precision, recall and F-score (and
SD) over all 5 folds on automatic classification of
semantic relations without using previous decisions.
In sum, these results show that automatic classifi-
cation of semantic relations is feasible and promis-
ing ? especially when the proliferation of classifica-
tion errors can be prevented ? but still not nearly as
good as human performance.
5 Discussion and Future work
This paper presented an approach to detecting se-
mantic relations at the word, phrase and sentence
level on the basis of dependency analyses. We inves-
tigated the performance of human annotators on the
tasks of manually aligning dependency analyses and
of labeling the semantic relations between aligned
nodes. Results indicate that humans can perform this
task well, with an F-score of .98 on alignment and an
F-score of .92 on semantic relations (after revision).
We also described and evaluated automatic methods
addressing these tasks: a dynamic programming tree
alignment algorithm which achieved an F-score on
alignment of .85 (using lexical semantic information
from EuroWordNet), and a memory-based seman-
tic relation classifier which achieved F-scores of .64
and .80 with and without using real previous deci-
sions respectively.
One of the issues that remains to be addressed
in future work is the effect of parsing errors. Such
errors were not corrected, but during manual align-
ment, we sometimes found that substrings could not
be properly aligned because the parser had failed to
identify them as syntactic constituents. As far as
classification of semantic relations is concerned, the
proliferation of classification errors is an issue that
needs to be solved. Classification performance may
be further improved with additional features (e.g.
phrase length information), optimization, and more
data. Also, we have not yet tried to combine au-
tomatic alignment and classification. Yet another
point concerns the type of text material. The sen-
tence pairs from our current corpus are relatively
close, in the sense that both translations more or less
convey the same information. Although this seems a
good starting point to study alignment, we intend to
continue with other types of text material in future
work. For instance, in extending our work to the ac-
tual output of a QA system, we expect to encounter
sentences with far less overlap.
References
R. Barzilay. 2003. Information Fusion for Multidocu-
ment Summarization. Ph.D. Thesis, Columbia Univer-
sity.
G. Bouma, G. van Noord, and R. Malouf. 2001. Alpino:
Wide-coverage computational analysis of Dutch. In
Computational Linguistics in The Netherlands 2000,
pages 45?59.
W. Daelemans, A. Van den Bosch, and J. Zavrel. 1999.
Forgetting exceptions is harmful in language learning.
Machine Learning, Special issue on Natural Language
Learning, 34:11?41.
W. Daelemans, J. Zavrel, K. Van der Sloot, and
A. van den Bosch. 2004. TiMBL: Tilburg memory
based learner, version 5.1, reference guide. ILK Tech-
nical Report 04-02, Tilburg University.
I. Dagan and O. Glickman. 2004. Probabilistic textual
entailment: Generic applied modelling of language
variability. In Learning Methods for Text Understand-
ing and Mining, Grenoble.
D. Gildea. 2003. Loosely tree-based alignment for ma-
chine translation. In Proceedings of the 41st Annual
Meeting of the ACL, Sapporo, Japan.
J. Herrera, A. Pe nas, and F. Verdejo. 2005. Textual
entailment recognition based on dependency analy-
sis and wordnet. In Proceedings of the 1st. PASCAL
Recognision Textual Entailment Challenge Workshop.
Pattern Analysis, Statistical Modelling and Computa-
tional Learning, PASCAL.
A. Meyers, R. Yangarber, and R. Grisham. 1996. Align-
ment of shared forests for bilingual corpora. In Pro-
ceedings of 16th International Conference on Com-
putational Linguistics (COLING-96), pages 460?465,
Copenhagen, Denmark.
F.J. Och and H. Ney. 2000. Statistical machine trans-
lation. In EAMT Workshop, pages 39?46, Ljubljana,
Slovenia.
V. Punyakanok, D. Roth, and W. Yih. 2004. Natural lan-
guage inference via dependency tree mapping: An ap-
plication to question answering. Computational Lin-
guistics, 6(9).
L. Vanderwende, D. Coughlin, and W. Dolan. 2005.
What syntax can contribute in entailment task. In Pro-
ceedings of the 1st. PASCAL Recognision Textual En-
tailment Challenge Workshop, Southampton, U.K.
6
Explorations in Sentence Fusion?
Erwin Marsi and Emiel Krahmer
Communication and Cognition
Faculty of Arts, Tilburg University
P.O.Box 90153, NL-5000 LE Tilburg, The Netherlands
{e.c.marsi, e.j.krahmer}@uvt.nl
Abstract
Sentence fusion is a text-to-text (revision-like) gen-
eration task which takes related sentences as input
and merges these into a single output sentence. In
this paper we describe our ongoing work on de-
veloping a sentence fusion module for Dutch. We
propose a generalized version of alignment which
not only indicates which words and phrases should
be aligned but also labels these in terms of a small
set of primitive semantic relations, indicating how
words and phrases from the two input sentences re-
late to each other. It is shown that human label-
ers can perform this task with a high agreement (F-
score of .95). We then describe and evaluate our
adaptation of an existing automatic alignment al-
gorithm, and use the resulting alignments, plus the
semantic labels, in a generalized fusion and gen-
eration algorithm. A small-scale evaluation study
reveals that most of the resulting sentences are ad-
equate to good.
1 Introduction
Traditionally, Natural Language Generation (NLG) is defined
as the automatic production of ?meaningful texts in (...) hu-
man language from some underlying non-linguistic represen-
tation of information? [Reiter and Dale, 2000, xvii]. Re-
cently, there is an increased interest in NLG applications
that produce meaningful text from meaningful text rather than
from abstract meaning representations. Such applications
are sometimes referred to as text-to-text generation applica-
tions (e.g., [Chandrasekar and Bangalore, 1997], [Knight and
Marcu, 2002], [Lapata, 2003]), and may be likened to ear-
lier revision-based generation strategies, e.g. [Robin, 1994]
[Callaway and Lester, 1997]. Text-to-text generation is often
motivated from practical applications such as summarization,
sentence simplification, and sentence compression. One rea-
son for the interest in such generation systems is the possi-
bility to automatically learn text-to-text generation strategies
from corpora of parallel text.
?This work was carried out within the IMIX-IMOGEN (Inter-
active Multimodal Output Generation) project, sponsored by the
Netherlands Organization of Scientific Research (NWO).
In this paper, we take a closer look at sentence fusion
[Barzilay, 2003][Barzilay et al, 1999], one of the interesting
variants in text-to-text generation. A sentence fusion module
takes related sentences as input, and generates a single sen-
tence summarizing the input sentences. The general strategy
described in [Barzilay, 2003] is to first align the dependency
structures of the two input sentences to find the common in-
formation in both sentences. On the basis of this alignment,
the common information is framed into an fusion tree (i.e.,
capturing the shared information), which is subsequently re-
alized in natural language by generating all traversals of the
fusion tree and scoring their probability using an n-gram lan-
guage model. Of the sentences thus generated the one with
the lowest (length normalized) entropy is selected.
Barzilay and co-workers apply sentence fusion in the con-
text of multi-document summarization, where the input sen-
tences typically come from multiple documents describing
the same event, but sentence fusion seems to be useful for
other applications as well. In question-answering, for in-
stance, sentence fusion could be used to generate more com-
plete answers. Many current QA systems use various parallel
answer-finding strategies, each of which may produce an N-
best list of answers (e.g., [Maybury, 2004]) In response to a
question like ?What causes RSI?? one potential answer sen-
tence could be:
RSI can be caused by repeating the same sequence
of movements many times an hour or day.
And another might be:
RSI is generally caused by a mixture of poor er-
gonomics, stress and poor posture.
These two incomplete answers might be fused into a more
complete answer such as:
RSI can be caused by a mixture of poor er-
gonomics, stress, poor posture and by repeating the
same sequence of movements many times an hour
or day.
The same process of sentence fusion can of course be applied
to the whole list of N-best answers in order to derive a more
specific, or even the most specific, answer, akin to taking the
union of a number of sets. Likewise, we can rely on sentence
fusion to derive a more general answer, or even the most gen-
eral one (cf. intersection), in the hope that this will filter out
irrelevant parts of the answer.
Arguably, such applications call for a generalized version
of sentence fusion, which may have consequences for the var-
ious components (alignment, fusion and generation) of the
sentence fusion pipeline. At the alignment level, we would
like to have a better understanding of how words and phrases
in the input sentences relate to each other. Rather than a bi-
nary choice (align or not), one might want to distinguish more
fine-grained relations such as overlap (if two phrases share
some but not all of their content), paraphrases (if two phrases
express the same information in different ways), entailments
(if one phrase entails the other, but not vice versa), etc. Such
an alignment strategy would be especially useful for applica-
tions such as question answering and information extraction,
where it is often important to know whether two sentences
are paraphrases or stand in an entailment relation [Dagan and
Glickman, 2004]. In the fusion module, we are interested in
the possibilities to generate various kinds of fusions depend-
ing on the relations between the respective sentences, e.g., se-
lecting the more specific or the more general phrase depend-
ing on whether the fusion tree is an intersection or a union
one. Finally, the generation may be more complicated in the
generalized version, and it is an interesting question whether
the use of language models is equally suitable for different
kinds of fusion.
In this paper, we will explore some of these issues re-
lated to a generalized version of sentence fusion. We start
with the basic question whether it is possible at all to reli-
ably align sentences, including different potential relations
between words and phrases (section 2). We then present our
ongoing work on sentence fusion, describing the current sta-
tus and performance of the alignment algorithm (section 3),
as well as the fusion and generation components (section 4).
We end with discussion and description of future plans in sec-
tion 5.
2 Data collection and Annotation
2.1 General approach
Alignment has become standard practice in data-driven ap-
proaches to machine translation (e.g. [Och and Ney, 2000]).
Initially work focused on word-based alignment, but more re-
cent research also addresses alignment at the higher levels
(substrings, syntactic phrases or trees), e.g.,[Gildea, 2003].
The latter approach seems most suitable for current purposes,
where we want to express that a sequence of words in one
sentence is related to a non-identical sequence of words in
another sentence (a paraphrase, for instance). However, if
we allow alignment of arbitrary substrings of two sentences,
then the number of possible alignments grows exponentially
to the number of tokens in the sentences, and the process of
alignment ? either manually or automatically ? may become
infeasible. An alternative, which seems to occupy the middle
ground between word alignment on the one hand and align-
ment of arbitrary substrings on the other, is to align syntac-
tic analyses. Here, following [Barzilay, 2003], we will align
sentences at the level of dependency structures. Unlike to
[Barzilay, 2003], we are interested in a number of different
alignment relations between sentences, and pay special atten-
tion to the feasibility of this alignment task.
verb:
hebben
verb:
hebben
hd/vc
pron:
ik
hd/su
adv:
zo
hd/mod hd/su
noun:
contact
hd/obj1
prep:
met
hd/pc
prep:
in de loop van
hd/mod
det:
veel
hd/det
adv:
heel
hd/mod
noun:
persoon
hd/obj1
adj:
serieus
hd/mod
det:
veel
hd/det
adv:
heel
hd/mod
noun:
leven
hd/obj1
det:
mijn
hd/det
Figure 1: Example dependency structure for the sentence Zo
heb ik in the loop van mijn leven heel veel contacten gehad
met heel veel serieuze personen. (lit. ?Thus have I in the
course of my life very many contacts had with very many
serious persons?).
2.2 Corpus
For evaluation and parameter estimation we have developed
a parallel monolingual corpus consisting of two different
Dutch translations of the French book ?Le petit prince? (the
little prince) by Antoine de Saint-Exupe?ry (published 1943),
one by Laetitia de Beaufort-van Hamel (1966) and one by
Ernst Altena (2000). The texts were automatically tokenized
and split into sentences, after which errors were manually
corrected. Corresponding sentences from both translations
were manually aligned; in most cases this was a one-to-one
mapping but occasionally a single sentence in one version
mapped onto two sentences in the other: Next, the Alpino
parser for Dutch (e.g., [Bouma et al, 2001]) was used for
part-of-speech tagging and lemmatizing all words, and for
assigning a dependency analysis to all sentences. The POS
labels indicate the major word class (e.g. verb, noun, pron,
and adv). The dependency relations hold between tokens
and are the same as used in the Spoken Dutch Corpus (see
e.g., [van der Wouden et al, 2002]). These include depen-
dencies such as head/subject, head/modifier and coordina-
tion/conjunction. See Figure 1 for an example. If a full parse
could not be obtained, Alpino produced partial analyses col-
lected under a single root node. Errors in lemmatization, POS
tagging, and syntactic dependency parsing were not subject to
manual correction.
2.3 Task definition
A dependency analysis of a sentence S yields a labeled di-
rected graph D = ?V,E?, where V (vertices) are the nodes,
and E (edges) are the dependency relations. For each node
v in the dependency structure for a sentence S, we define
STR(v) as the substring of all tokens under v (i.e., the com-
position of the tokens of all nodes reachable from v). For
example, the string associated with node persoon in Figure 1
is heel veel serieuze personen (?very many serious persons?).
An alignment between sentences S and S? pairs nodes from
the dependency graphs for both sentences. Aligning node v
from the dependency graph D of sentence S with node v?
from the graph D? of S? indicates that there is a relation be-
tween STR(v) and STR(v?), i.e., between the respective sub-
strings associated with v and v?. We distinguish five potential,
mutually exclusive, relations between nodes (with illustrative
examples):
1. v equals v? iff STR(v) and STR(v?) are literally identical
(abstracting from case and word order)
Example: ?a small and a large boa-constrictor? equals
?a large and a small boa-constrictor?;
2. v restates v? iff STR(v) is a paraphrase of STR(v?) (same
information content but different wording),
Example: ?a drawing of a boa-constrictor snake? re-
states ?a drawing of a boa-constrictor?;
3. v specifies v? iff STR(v) is more specific than STR(v?),
Example: ?the planet B 612? specifies ?the planet?;
4. v generalizes v? iff STR(v?) is more specific than
STR(v),
Example: ?the planet? generalizes ?the planet B 612?;
5. v intersects v? iff STR(v) and STR(v?) share some in-
formational content, but also each express some piece of
information not expressed in the other,
Example: ?Jupiter and Mars? intersects ?Mars and
Venus?
Note that there is an intuitive relation with entailment here:
both equals and restates can be understood as mutual entail-
ment (i.e., if the root nodes of the analyses corresponding S
and S? stand in an equal or restate relation, S entails S? and
S? entails S), if S specifies S? then S also entails S? and if S
generalizes S? then S is entailed by S?.
An alignment between S and S? can now formally be
defined on the basis of the respective dependency graphs
D = ?V,E? and D? = ?V ?, E?? as a graph A = ?VA, EA?,
such that
EA = {?v, l, v?? | v ? V & v? ? V ? & l(STR(v), STR(v?))},
where l is one of the five relations defined above. The nodes
of A are those nodes from D en D? which are aligned, for-
mally defined as
VA = {v | ?v??l?v, l, v?? ? EA}?{v? | ?v?l?v, l, v?? ? EA}
A complete example alignment can be found in the Appendix,
Figure 3.
(A1, A2) (A1? , A2?) (Ac, A1?) (Ac, A2?)
#real: 322 323 322 322
#pred: 312 321 323 321
#correct: 293 315 317 318
precision: .94 .98 .98 .99
recall: .91 .98 .98 .99
F-score: .92 .98 .98 .99
Table 1: Interannotator agreement with respect to align-
ment between annotators 1 and 2 before (A1, A2) and after
(A1? , A2?) revision , and between the consensus and annota-
tor 1 (Ac, A1?) and annotator 2 (Ac, A2?) respectively.
2.4 Alignment tool
For creating manual alignments, we developed a special-
purpose annotation tool called Gadget (?Graphical Aligner of
Dependency Graphs and Equivalent Tokens?). It shows, side
by side, two sentences, as well as their respective dependency
graphs. When the user clicks on a node v in the graph, the cor-
responding string (STR(v)) is shown at the bottom. The tool
enables the user to manually construct an alignment graph on
the basis of the respective dependency graphs. This is done
by focusing on a node in the structure for one sentence, and
then selecting a corresponding node (if possible) in the other
structure, after which the user can select the relevant align-
ment relation. The tool offers additional support for folding
parts of the graphs, highlighting unaligned nodes and hiding
dependency relation labels. See Figure 4 in the Appendix for
a screen shot of Gadget.
2.5 Results
All text material was aligned by the two authors. They started
doing the first ten sentences of chapter one together in order
to get a feel for the task. They continued with the remaining
sentences from chapter one individually. The total number
of nodes in the two translations of the chapter was 445 and
399 respectively. Inter-annotator agreement was calculated
for two aspects: alignment and relation labeling. With respect
to alignment, we calculated the precision, recall and F-score
(with ? = 1) on aligned node pairs as follows:
precision(Areal, Apred) = | Areal ?Apred || Apred | (1)
recall(Areal, Apred) = | Areal ?Apred || Areal | (2)
F -score = 2? precision? recallprecision+ recall (3)
where Areal is the set of all real alignments (the reference or
golden standard), Apred is the set of all predicted alignments,
and Apred?Areal is the set al correctly predicted alignments.
For the purpose of calculating inter-annotator agreement, one
of the annotations (A1) was considered the ?real? alignment,
the other (A2) the ?predicted?. The results are summarized in
Table 1 in column (A1, A2).
Next, both annotators discussed the differences in align-
ment, and corrected mistaken or forgotten alignments. This
improved their agreement as shown in column (A1? , A2?). In
(A1, A2) (A1? , A2?) (Ac, A1?) (Ac, A2?)
precision: .86 .96 .98 .97
recall: .86 .95 .97 .97
F-score: .85 .95 .97 .97
?: .77 .92 .96 .96
Table 2: Inter-annotator agreement with respect to alignment
relation labeling between annotators 1 and 2 before (A1, A2)
and after (A1? , A2?) revision , and between the consensus and
annotator 1 (Ac, A1?) and annotator 2 (Ac, A2?) respectively.
addition, they agreed on a single consensus annotation (Ac).
The last two columns of Table 1 show the results of evalu-
ating each of the revised annotations against this consensus
annotation. The F-score of .96 can therefore be regarded as
the upper bound on the alignment task.
In a similar way, the agreement was calculated for the task
of labeling the alignment relations. Results are shown in Ta-
ble 2, where the measures are weighted precision, recall and
F-score. For instance, the precision is the weighted sum of
the separate precision scores for each of the five relations.
The table also shows the ?-score, which is another commonly
used measure for inter-annotator agreement [Carletta, 1996].
Again, the F-score of .97 can be regarded as the upper bound
on the relation labeling task.
We think these numbers indicate that the labeled alignment
task is well defined and can be accomplished with a high level
of inter-annotator agreement.
3 Automatic alignment
In this section, we describe the alignment algorithm that we
use (section 3.1), and evaluate its performance (section 3.2).
3.1 Tree alignment algorithm
The tree alignment algorithm is based on [Meyers et al,
1996], and similar to that used in [Barzilay, 2003]. It cal-
culates the match between each node in dependency tree D
against each node in dependency tree D?. The score for each
pair of nodes only depends on the similarity of the words
associated with the nodes and, recursively, on the scores of
the best matching pairs of their descendants. For an efficient
implementation, dynamic programming is used to build up a
score matrix, which guarantees that each score will be calcu-
lated only once.
Given two dependency trees D and D?, the algorithm
builds up a score function S(v, v?) for matching each node
v in D against each node v? in D?, which is stored in a ma-
trix M . The value S(v, v?) is the score for the best match
between the two subtrees rooted at v in D and at v? in D?.
When a value for S(v, v?) is required, and is not yet in the
matrix, it is recursively computed by the following formula:
S(v, v?) = max
?
?
?
TREEMATCH(v, v?)
maxi=1,...,n S(vi, v?)
maxj=1,...,m S(v, v?j)
(4)
where v1, . . . , vn denote the children of v and v?1, . . . , v?m de-
note the children of v?. The three terms correspond to the
three ways that nodes can be aligned: (1) v can be directly
aligned to v?; (2) any of the children of v can be aligned to v?;
(3) v can be aligned to any of the children of v?. Notice that
the last two options imply skipping one or more edges, and
leaving one or more nodes unaligned.1
The function TREEMATCH(v, v?) is a measure of how well
the subtrees rooted at v and v? match:
TREEMATCH(v, v?) = NODEMATCH(v, v?) +
max
p ? P(v,v?)
?
? ?
(i,j) ? p
(
RELMATCH(??v i,??v ?j) + S(vi, v?j)
)
?
?
Here ??v i denotes the dependency relation from v to vi.
P(v, v?) is the set of all possible pairings of the n children
of v against the m children of v?, which is the power set of
{1, . . . , n} ? {1, . . . ,m}. The summation in (5) ranges over
all pairs, denoted by (i, j), which appear in a given pairing
p ? P(v, v?). Maximizing this summation thus amounts to
finding the optimal alignment of children of v to children of
v?.
NODEMATCH(v, v?) ? 0 is a measure of how well the
label of node v matches the label of v?.
RELMATCH(??v i,??v ?j) ? 0 is a measure for how well the
dependency relation between node v and its child vi matches
that of the dependency relation between node v? and its child
vj .
Since the dependency graphs delivered by the Alpino
parser were usually not trees, they required some modifica-
tion in order to be suitable input for the tree alignment al-
gorithm. We first determined a root node, which is defined
as a node from which all other nodes in the graph can be
reached. In the rare case of multiple root nodes, an arbi-
trary one was chosen. Starting from this root node, any cyclic
edges were temporarily removed during a depth-first traver-
sal of the graph. The resulting directed acyclic graphs may
still have some amount of structure sharing, but this poses no
problem for the algorithm.
3.2 Evaluation of automatic alignment
We evaluated the automatic alignment of nodes, abstracting
from relation labels, as we have no algorithm for automatic
labeling of these relations yet. The baseline is achieved by
aligning those nodes with stand in an equals relation to each
other, i.e., a node v in D is aligned to a node v? in D? iff
STR(v) =STR(v?). This alignment can be constructed rela-
tively easy.
The alignment algorithm is tested with the following
NODEMATCH function:
NODEMATCH(v, v?) =
?
??????
??????
10 if STR(v) = STR(v?)
5 if LABEL(v) = LABEL(v?)
2 if LABEL(v) is a synonym
hyperonym or hyponym
of LABEL(v?)
0 otherwise
1In the original formulation of the algorithm by [Meyers et al,
1996], there is a penalty for skipping edges.
Alignment : Prec : Rec : F-score:
baseline .87 .41 .56
algorithm without wordnet .84 .82 .83
algorithm with wordnet .86 .84 .85
Table 3: Precision, recall and F-score on automatic alignment
It reserves the highest value for a literal string match, a some-
what lower value for matching lemmas, and an even lower
value in case of a synonym, hyperonym or hyponym relation.
The latter relations are retrieved from the Dutch part of Eu-
roWordnet [Vossen, 1998]. For the RELMATCH function, we
simply used a value of 1 for identical dependency relations,
and 0 otherwise. These values were found to be adequate in a
number of test runs on two other, manually aligned chapters
(these chapters were not used for the actual evaluation). In the
future we intend to experiment with automatic optimizations.
We measured the alignment accuracy defined as the per-
centage of correctly aligned node pairs, where the consen-
sus alignment of the first chapter served as the golden stan-
dard. The results are summarized in Table 3. In order to test
the contribution of synonym and hyperonym information for
node matching, performance is measured with and without
the use of Eurowordnet. The results show that the algorithm
improves substantially on the baseline. The baseline already
achieves a relatively high score (an F-score of .56), which
may be attributed to the nature of our material: the translated
sentence pairs are relatively close to each other and may show
a sizeable amount of literal string overlap. The alignment al-
gorithm (without use of EuroWordnet) loses a few points on
precision, but improves a lot on recall (a 200% increase with
respect to the baseline), which in turn leads to a substantial
improvement on the overall F-score. The use of Euroword-
net leads to a small increase (two points) on both precision
and recall (and thus to small increase on F-score). Yet, in
comparison with the gold standard human score for this task
(.95), there is clearly room for further improvement.
4 Merging and generation
The remaining two steps in the sentence fusion process are
merging and generation. In general, merging amounts to de-
ciding which information from either sentence should be pre-
served, whereas generation involves producing a grammat-
ically correct surface representation. In order to get an idea
about the baseline performance, we explored a simple, some-
what naive string-based approach. Below, the pseudocode
is shown for merging two dependency trees in order to get
restatements. Given a labeled alignment A between depen-
dency graphs D and D?, if there is a restates relation between
node v from D and node v? from D?, we add the string real-
ization of v? as an alternative to those of v.
RESTATE(A)
1 for each edge ?v, l, v?? ? EA
2 do if l = restates
3 then STR(v) ? STR(v) ? STR(v?)
The same procedure is followed in order to get specifications:
SPECIFY(A)
1 for each edge ?v, l, v?? ? EA
2 do if l = generalizes
3 then STR(v) ? STR(v) ? STR(v?)
The generalization procedure adds the option to omit the re-
alization of a modifier that is not aligned:
GENERALIZE(D,A)
1 for each edge ?v, l, v?? ? EA
2 do if l = specifies
3 then STR(v) ? STR(v) ? STR(v?)
4 for each edge ?v, l, v?? ? ED
5 do if l ? MOD-DEP-RELS and v /? EA
6 then STR(v) ? STR(v) ? NIL
where MOD-DEP-REL is the set of dependency relations be-
tween a node and a modifier (e.g. head/mod and head/predm).
Each procedure is repeated twice, once adding substrings
from D into D? and once the other way around. Next, we
traverse the dependency trees and generate all string realiza-
tions, extending the list of variants for each node that has mul-
tiple realizations. Finally, we filter out multiple copies of the
same string, as well as strings that are identical to the input
sentences.
This procedure for merging and generation was applied to
the 35 sentence pairs from the consensus alignment of chapter
one of ?Le Petit Prince?. Overall this gave rise to 194 restate-
ment, 62 specifications and 177 generalizations, with some
sentence pairs leading to many variants and others to none at
all. Some output showed only minor variations, for instance,
substitution of a synonym. However, others revealed surpris-
ingly adequate generalizations or specifications. Examples of
good and bad output are given in Figure 2.
As expected, many of the resulting variants are ungram-
matical, because constraints on word order, agreement or sub-
categorisation are violated. Following work on statistical sur-
face generation [Langkilde and Knight, 1998] and other work
on sentence fusion [Barzilay, 2003], we tried to filter un-
grammatical variants with an n-gram language model. The
Cambridge-CMU Statistical Modeling Toolkit v2 was used to
train a 3-gram model on over 250M words from the Twente
Newscorpus , using back-off and Good-Turing smoothing.
Variants were ranked in order of increasing entropy. We
found, however, that the ranking was often inadequate, show-
ing ungrammatical variants at the top and grammatical vari-
ants in the lower regions.
To gain some insight into the general performance of the
merging and generation strategy, we performed a small eval-
uation test in which the two authors independently judged all
generated variants in terms of three categories:
1. Perfect: no problems in either semantics or syntax;
2. Acceptable: understandable, but with some minor flaws
in semantics or grammar;
3. Nonsense: serious problems in semantics or grammar
Table 4 shows the number of sentences in each of the three
categories per judge, broken down in restatements, general-
ization and specifications. The ?-score on this classification
Input1: Zo
Thus
heb
have
ik
I
in
in
de
the
loop
course
van
of
mijn
my
leven
life
heel
very
veel
many
contacten
contacts
gehad
had
met
with
heel
very
veel
many
serieuze
serious
personen
persons
Input2: Op
In
die
that
manier
way
kwam
came
ik
I
in
in
het
the
leven
life
met
with
massa?s
masses-of
gewichtige
weighty/important
mensen
people
in
in
aanraking
touch
Restate: op
in
die
that
manier
way
heb
have
ik
I
in
in
de
the
loop
course
van
of
mijn
my
leven
life
heel
very
veel
many
contacten
contacts
gehad
had
met
with
heel
very
veel
many
serieuze
serious
personen
persons
Specific: op
in
die
that
manier
way
kwam
have
ik
I
in
in
de
the
loop
course
van
of
mijn
my
leven
life
met
with
massa?s
masses-of
gewichtige
weighty/important
mensen
people
in
in
aanraking
touch
General: zo
thus
heb
have
ik
I
in
in
het
the
leven
life
veel
many
contacten
contacts
gehad
had
met
with
veel
many
serieuze
serious
personen
persons
Input1: En
And
zo
so
heb
have
ik
I
op
at
mijn
my
zesde
sixth
jaar
year
een
a
prachtige
wonderful
loopbaan
career
als
as
kunstschilder
art-painter
laten
let
varen
sail
.
Input2: Zo
Thus
kwam
came
het
it
,
,
dat
that
ik
I
op
at
zesjarige
six-year
leeftijd
age
een
a
schitterende
bright
schildersloopbaan
painter-career
liet
let
varen
sail
.
Specific: en
and
zo
so
heb
have
ik
I
op
at
mijn
my
zesde
sixth
jaar
year
als
as
kunstschilder
art-painter
laten
let
een
a
schitterende
bright
schildersloopbaan
painter-career
varen
sail
General: zo
so
kwam
came
het
it
dat
that
ik
I
op
at
leeftijd
age
een
a
prachtige
wonderful
loopbaan
career
liet
let
varen
sail
Figure 2: Examples of good (top) and bad (bottom) sentence fusion output
Restate: Specific: General:
J1 J2 J1 J2 J1 J2
Perfect: 109 104 28 22 89 86
Acceptable: 44 58 15 16 34 24
Nonsense: 41 32 19 24 54 67
Total: 194 62 177
Table 4: Results of the evaluation of the sentence fusion out-
put as the number of sentences in each of the three categories
perfect, acceptable and nonsense per judge (J1 and J2), bro-
ken down in restatements, generalizations and specifications.
task is .75, indicating a moderate to good agreement between
the judges. Roughly half of the generated restatements and
generalization are perfect, while this is not the case for spec-
ifications. We have no plausible explanation for this yet.
We think we can conclude from this evaluation that sen-
tence fusion is a viable and interesting approach for produc-
ing restatements, generalization and specifications. However,
there is certainly further work to do; the procedure for merg-
ing dependency graphs should be extended, and the realiza-
tion model clearly requires more linguistic sophistication in
particular to deal with word order, agreement and subcate-
gorisation constraints.
5 Discussion and Future work
In this paper we have described our ongoing work on sen-
tence fusion for Dutch. Starting point was the sentence fusion
model proposed by [Barzilay et al, 1999; Barzilay, 2003]
in which dependency analyses of pairs of sentences are first
aligned, after which the aligned parts (representing the com-
mon information) are fused. The resulting fused dependency
tree is subsequently transfered into natural language. Our
new contributions are primarily in two areas. First, we carried
out an explicit evaluation of the alignment ? both human and
automatic alignment ? whereas [Barzilay, 2003] only evalu-
ates the output of the complete sentence fusion process. We
found that annotators can reliably align phrases and assign
relation labels to them, and that good results can be achieved
with automatic alignment, certainly above an informed base-
line, albeit still below human performance. Second, Barzi-
lay and co-workers developed their sentence fusion model in
the context of multi-document summarization, but arguably
the approach could also be applicable for applications such
as question answering or information extraction. This seems
to call for a more refined version of sentence fusion, which
has consequences for alignment, merging and realization. We
have therefore introduced five different types of semantic re-
lations between strings, namely equals, restates, specifies,
generalizes and intersects. This increases the expressiveness
of the representation, and supports generating restatements,
generalizations and specifications. Finally, we described and
evaluated our first results on sentence realization based on
these refined alignments, with promising results.
Similar work is described in [Pang et al, 2003], who de-
scribe a syntax-based algorithm that builds word lattices from
parallel translations which can be used to generate new para-
phrases. Their alignment algorithm is less refined, and there
is only type of alignment and hence output (only restate-
ments), but their mapping of aligned trees to a word lattice
(or FSA) seems worthwhile to explore in combination with
the approach we have proposed here.
One of the issues that remains to be addressed in future
work is the effect of parsing errors. Such errors were not
manually corrected, but during manual alignment, however,
we sometimes found that substrings could not be properly
aligned because the parser failed to identify them as syntac-
tic constituents. The repercussions of this for the generation
should be investigated by comparing the results obtained here
with alignments on perfect parses. Furthermore, our work on
automatic alignment so far only concerned the alignment of
nodes, not the determination of the relation type. We intend
to address this task with machine learning, initially relying
on shallow features such as the length of the respective token
strings and the amount of overlap. It is also clear that more
work is needed on merging and surface realization. One pos-
sible direction here is to exploit the relatively rich linguistic
representation of the input sentences (POS tags, lemmas and
dependency structures), for instance, along the lines of [Ban-
galore and Rambow, 2000]. Yet another issue concerns the
type of text material. The sentence pairs from our current cor-
pus are relatively close, in the sense that there is usually a 1-
to-1 mapping between sentences, and both translations more
or less convey the same information. Although this seems a
good starting point to study alignment, we intend to continue
with other types of text material in future work. For instance,
in extending our work to the actual output of a QA system,
we expect to encounter sentences with far less overlap. Of
particular interest to us is also whether sentence fusion can
be shown to improve the quality of QA system output.
References
[Bangalore and Rambow, 2000] Srinivas Bangalore and
Owen Rambow. Exploiting a probabilistic hierar-
chical model for generation. In Proceedings of the
17th conference on Computational linguistics, pages
42?48, Morristown, NJ, USA, 2000. Association for
Computational Linguistics.
[Barzilay et al, 1999] R. Barzilay, K. McKeown, and M. El-
haded. Information fusion in the context of multi-
document summarization. In Proceedings of the 37th An-
nual Meeting of the Association for Computational Lin-
guistics (ACL-99), Maryland, 1999.
[Barzilay, 2003] R. Barzilay. Information Fusion for Multi-
document Summarization. Ph.D. Thesis, Columbia Uni-
versity, 2003.
[Bouma et al, 2001] Gosse Bouma, Gertjan van Noord, and
Robert Malouf. Alpino: Wide-coverage computational
analysis of dutch. In Computational Linguistics in The
Netherlands 2000. 2001.
[Callaway and Lester, 1997] C. Callaway and J. Lester. Dy-
namically improving explanations: A revision-based ap-
proach to explanation generation. In Proceedings of the
15th International Joint Conference on Artificial Intelli-
gence (IJCAI 1997), pages 952?958, Nagoya, Japan, 1997.
[Carletta, 1996] Jean Carletta. Assessing agreement on clas-
sification tasks: the kappa statistic. Comput. Linguist.,
22(2):249?254, 1996.
[Chandrasekar and Bangalore, 1997] R. Chandrasekar and
S. Bangalore. Automatic induction of rules for text simpli-
fication. Knowledge-based Systems, 10(3):183?190, 1997.
[Dagan and Glickman, 2004] I. Dagan and O. Glickman.
Probabilistic textual entailment: Generic applied mod-
elling of language variability. In Learning Methods for
Text Understanding and Mining, Grenoble, 2004.
[Gildea, 2003] D. Gildea. Loosely tree-based alignment for
machine translation. In Proceedings of the 41st Annual
Meeting of the ACL, Sapporo, Japan, 2003.
[Imamura, 2001] K. Imamura. Hierarchical phrase align-
ment harmonized with parsing. In Proceedings of the
6th Natural Language Processing Pacific Rim Symposium
(NLPRS 2001), pages 377?384, Tokyo, Japan, 2001.
[Knight and Marcu, 2002] K. Knight and D. Marcu. Sum-
marization beyond sentence extraction: A probabilistic ap-
proach to sentence compression. Artificial Intelligence,
139(1):91?107, 2002.
[Langkilde and Knight, 1998] Irene Langkilde and Kevin
Knight. Generation that exploits corpus-based statistical
knowledge. In Proceedings of the 36th conference on As-
sociation for Computational Linguistics, pages 704?710,
Morristown, NJ, USA, 1998. Association for Computa-
tional Linguistics.
[Lapata, 2003] M. Lapata. Probabilistic text structuring: Ex-
periments with sentence ordering. In Proceedings of the
41st Annual Meeting of the Association for Computational
Linguistics, pages 545?552, Sapporo, 2003.
[Maybury, 2004] M. Maybury. New Directions in Question
Answering. AAAI Press, 2004.
[Meyers et al, 1996] A. Meyers, R. Yangarber, and R. Gr-
isham. Alignment of shared forests for bilingual cor-
pora. In Proceedings of 16th International Conference on
Computational Linguistics (COLING-96), pages 460?465,
Copenhagen, Denmark, 1996.
[Och and Ney, 2000] Franz Josef Och and Hermann Ney.
Statistical machine translation. In EAMT Workshop, pages
39?46, Ljubljana, Slovenia, 2000.
[Pang et al, 2003] Bo Pang, Kevin Knight, and Daniel
Marcu. Syntax-based alignment of multiple translations:
Extracting paraphrases and generating new sentences. In
HLT-NAACL, 2003.
[Reiter and Dale, 2000] E. Reiter and R. Dale. Building Nat-
ural Language Generation Systems. Cambridge University
Press, Cambridge, 2000.
[Robin, 1994] J. Robin. Revision-based generation of Nat-
ural Language Summaries Providing Historical Back-
ground. Ph.D. Thesis, Columbia University, 1994.
[van der Wouden et al, 2002] T. van der Wouden, H. Hoek-
stra, M. Moortgat, B. Renmans, and I. Schuurman. Syntac-
tic analysis in the spoken dutch corpus. In Proceedings of
the third International Conference on Language Resources
and Evaluation, pages 768?773, Las Palmas, Canary Is-
lands, Spain, 2002.
[Vossen, 1998] Piek Vossen, editor. EuroWordNet: a multi-
lingual database with lexical semantic networks. Kluwer
Academic Publishers, Norwell, MA, USA, 1998.
6 Appendix
he
bb
en
ko
m
en
he
bb
en
ik
ik
o
p
in
in
 a
an
ra
ki
ng
m
e
t
zo
co
n
ta
ct
m
e
t
in
 d
e 
lo
op
 v
an
ve
e
l
he
el
pe
rs
oo
n
se
rie
us
ve
e
l
m
a
ss
a
ge
wi
ch
tig
he
el
le
ve
n m
ijn
le
ve
n
he
t
m
a
n
ie
r
di
e
m
e
n
s
Fi
gu
re
3:
D
ep
en
de
nc
y
st
ru
ct
ur
es
an
d
al
ig
nm
en
tf
or
th
e
se
n
te
nc
es
Zo
he
b
ik
in
de
lo
op
va
n
m
ijn
le
ve
n
he
el
ve
el
co
n
ta
ct
en
ge
ha
d
m
et
he
el
ve
el
se
ri
eu
ze
pe
rs
o
n
en
.
(li
t.?
Th
us
ha
v
e
Ii
n
th
e
co
u
rs
e
o
fm
y
lif
e
v
er
y
m
an
y
co
n
ta
ct
s
ha
d
w
ith
v
er
y
m
an
y
se
rio
us
pe
rs
on
s?
)a
n
d
O
p
di
em
a
n
ie
rk
wa
m
ik
in
he
tl
ev
en
m
et
m
a
ss
a
?s
ge
w
ic
ht
ig
e
m
en
se
n
in
a
a
n
ra
ki
ng
.
.
(li
t.
?
In
th
at
w
ay
ca
m
e
Ii
n
th
e
lif
e
w
ith
m
as
s-
o
fw
ei
gh
ty
/im
po
rta
nt
pe
op
le
in
to
uc
h?
).
Th
e
al
ig
nm
en
tr
el
at
io
ns
ar
e
eq
ua
ls
(do
tte
dg
ra
y),
re
st
at
es
(so
lid
gr
ay
),s
pe
ci
fie
s(
do
tte
db
la
ck
),a
n
d
in
te
rs
ec
ts
(da
sh
ed
gr
ay
).
Fo
r
th
e
sa
ke
o
ft
ra
ns
pa
re
nc
y,
de
pe
nd
en
cy
re
la
tio
ns
ha
v
e
be
en
o
m
itt
ed
.
Fi
gu
re
4:
Sc
re
en
sh
ot
o
fG
ad
ge
t,
th
e
to
ol
u
se
d
fo
ra
lig
ni
ng
de
pe
nd
en
cy
st
ru
ct
ur
es
o
fs
en
te
nc
es
.
Proceedings of the 10th Conference on Computational Natural Language Learning (CoNLL-X),
pages 149?164, New York City, June 2006. c?2006 Association for Computational Linguistics
CoNLL-X shared task on Multilingual Dependency Parsing
Sabine Buchholz
Speech Technology Group
Cambridge Research Lab
Toshiba Research Europe
Cambridge CB2 3NH, UK
sabine.buchholz@crl.toshiba.co.uk
Erwin Marsi
Communication & Cognition
Tilburg University
5000 LE Tilburg, The Netherlands
e.c.marsi@uvt.nl
Abstract
Each year the Conference on Com-
putational Natural Language Learning
(CoNLL)1 features a shared task, in which
participants train and test their systems on
exactly the same data sets, in order to bet-
ter compare systems. The tenth CoNLL
(CoNLL-X) saw a shared task on Multi-
lingual Dependency Parsing. In this pa-
per, we describe how treebanks for 13 lan-
guages were converted into the same de-
pendency format and how parsing perfor-
mance was measured. We also give an
overview of the parsing approaches that
participants took and the results that they
achieved. Finally, we try to draw gen-
eral conclusions about multi-lingual pars-
ing: What makes a particular language,
treebank or annotation scheme easier or
harder to parse and which phenomena are
challenging for any dependency parser?
Acknowledgement
Many thanks to Amit Dubey and Yuval Kry-
molowski, the other two organizers of the shared
task, for discussions, converting treebanks, writing
software and helping with the papers.2
1see http://ilps.science.uva.nl/?erikt/signll/conll/
2Thanks also to Alexander Yeh for additional help with the
paper reviews. His work was made possible by the MITRE Cor-
poration?s Sponsored Research Program.
1 Introduction
Previous CoNLL shared tasks focused on NP chunk-
ing (1999), general chunking (2000), clause iden-
tification (2001), named entity recognition (2002,
2003), and semantic role labeling (2004, 2005). This
shared task on full (dependency) parsing is the log-
ical next step. Parsing is an important preprocess-
ing step for many NLP applications and therefore
of considerable practical interest. It is a complex
task and as it is not straightforwardly mappable to a
?classical? segmentation, classification or sequence
prediction problem, it also poses theoretical chal-
lenges to machine learning researchers.
During the last decade, much research has been
done on data-driven parsing and performance has in-
creased steadily. For training these parsers, syntac-
tically annotated corpora (treebanks) of thousands
to tens of thousands of sentences are necessary; so
initially, research has focused on English. Dur-
ing the last few years, however, treebanks for other
languages have become available and some parsers
have been applied to several different languages.
See Section 2 for a more detailed overview of re-
lated previous research.
So far, there has not been much comparison be-
tween different dependency parsers on exactly the
same data sets (other than for English). One of the
reasons is the lack of a de-facto standard for an eval-
uation metric (labeled or unlabeled, separate root ac-
curacy?), for splitting the data into training and test-
ing portions and, in the case of constituency tree-
banks converted to dependency format, for this con-
version. Another reason are the various annotation
149
schemes and logical data formats used by different
treebanks, which make it tedious to apply a parser to
many treebanks. We hope that this shared task will
improve the situation by introducing a uniform ap-
proach to dependency parsing. See Section 3 for the
detailed task definition and Section 4 for information
about the conversion of all 13 treebanks.
In this shared task, participants had two to three
months3 to implement a parsing system that could be
trained for all these languages and four days to parse
unseen test data for each. 19 participant groups sub-
mitted parsed test data. Of these, all but one parsed
all 12 required languages and 13 also parsed the op-
tional Bulgarian data. A wide variety of parsing
approaches were used: some are extensions of pre-
viously published approaches, others are new. See
Section 5 for an overview.
Systems were scored by computing the labeled
attachment score (LAS), i.e. the percentage of
?scoring? tokens for which the system had predicted
the correct head and dependency label. Punctuation
tokens were excluded from scoring. Results across
languages and systems varied widely from 37.8%
(worst score on Turkish) to 91.7% (best score on
Japanese). See Section 6 for detailed results.
However, variations are consistent enough to al-
low us to draw some general conclusions. Section 7
discusses the implications of the results and analyzes
the remaining problems. Finally, Section 8 describes
possible directions for future research.
2 Previous research
Tesnie`re (1959) introduced the idea of a dependency
tree (a ?stemma? in his terminology), in which
words stand in direct head-dependent relations, for
representing the syntactic structure of a sentence.
Hays (1964) and Gaifman (1965) studied the for-
mal properties of projective dependency grammars,
i.e. those where dependency links are not allowed to
cross. Mel?c?uk (1988) describes a multistratal de-
pendency grammar, i.e. one that distinguishes be-
tween several types of dependency relations (mor-
phological, syntactic and semantic). Other theories
related to dependency grammar are word grammar
3Some though had significantly less time: One participant
registered as late as six days before the test data release (reg-
istration was a prerequisite to obtain most of the data sets) and
still went on to submit parsed test data in time.
(Hudson, 1984) and link grammar (Sleator and Tem-
perley, 1993).
Some relatively recent rule-based full depen-
dency parsers are Kurohashi and Nagao (1994) for
Japanese, Oflazer (1999) for Turkish, Tapanainen
and Ja?rvinen (1997) for English and Elworthy
(2000) for English and Japanese.
While phrase structure parsers are usually evalu-
ated with the GEIG/PARSEVAL measures of preci-
sion and recall over constituents (Black et al, 1991),
Lin (1995) and others have argued for an alterna-
tive, dependency-based evaluation. That approach is
based on a conversion from constituent structure to
dependency structure by recursively defining a head
for each constituent.
The same idea was used by Magerman (1995),
who developed the first ?head table? for the Penn
Treebank (Marcus et al, 1994), and Collins (1996),
whose constituent parser is internally based on prob-
abilities of bilexical dependencies, i.e. dependencies
between two words. Collins (1997)?s parser and
its reimplementation and extension by Bikel (2002)
have by now been applied to a variety of languages:
English (Collins, 1999), Czech (Collins et al, 1999),
German (Dubey and Keller, 2003), Spanish (Cowan
and Collins, 2005), French (Arun and Keller, 2005),
Chinese (Bikel, 2002) and, according to Dan Bikel?s
web page, Arabic.
Eisner (1996) introduced a data-driven depen-
dency parser and compared several probability mod-
els on (English) Penn Treebank data. Kudo and
Matsumoto (2000) describe a dependency parser for
Japanese and Yamada and Matsumoto (2003) an ex-
tension for English. Nivre?s parser has been tested
for Swedish (Nivre et al, 2004), English (Nivre and
Scholz, 2004), Czech (Nivre and Nilsson, 2005),
Bulgarian (Marinov and Nivre, 2005) and Chinese
Cheng et al (2005), while McDonald?s parser has
been applied to English (McDonald et al, 2005a),
Czech (McDonald et al, 2005b) and, very recently,
Danish (McDonald and Pereira, 2006).
3 Data format, task definition
The training data derived from the original treebanks
(see Section 4) and given to the shared task partic-
ipants was in a simple column-based format that is
150
an extension of Joakim Nivre?s Malt-TAB format4
for the shared task and was chosen for its processing
simplicity. All the sentences are in one text file and
they are separated by a blank line after each sen-
tence. A sentence consists of one or more tokens.
Each token is represented on one line, consisting of
10 fields. Fields are separated from each other by a
TAB.5 The 10 fields are:
1) ID: Token counter, starting at 1 for each new
sentence.
2) FORM: Word form or punctuation symbol.
For the Arabic data only, FORM is a concatenation
of the word in Arabic script and its transliteration in
Latin script, separated by an underscore. This rep-
resentation is meant to suit both those that do and
those that do not read Arabic.
3) LEMMA: Lemma or stem (depending on the
particular treebank) of word form, or an underscore
if not available. Like for the FORM, the values for
Arabic are concatenations of two scripts.
4) CPOSTAG: Coarse-grained part-of-speech
tag, where the tagset depends on the treebank.
5) POSTAG: Fine-grained part-of-speech tag,
where the tagset depends on the treebank. It is iden-
tical to the CPOSTAG value if no POSTAG is avail-
able from the original treebank.
6) FEATS: Unordered set of syntactic and/or
morphological features (depending on the particu-
lar treebank), or an underscore if not available. Set
members are separated by a vertical bar (|).
7) HEAD: Head of the current token, which is
either a value of ID, or zero (?0?) if the token links
to the virtual root node of the sentence. Note that
depending on the original treebank annotation, there
may be multiple tokens with a HEAD value of zero.
8) DEPREL: Dependency relation to the HEAD.
The set of dependency relations depends on the par-
ticular treebank. The dependency relation of a to-
ken with HEAD=0 may be meaningful or simply
?ROOT? (also depending on the treebank).
9) PHEAD: Projective head of current token,
which is either a value of ID or zero (?0?), or an un-
derscore if not available. The dependency structure
4http://w3.msi.vxu.se/ nivre/research/MaltXML.html
5Consequently, field values cannot contain TABs. In the
shared task data, field values are also not supposed to con-
tain any other whitespace (although unfortunately some spaces
slipped through in the Spanish data).
resulting from the PHEAD column is guaranteed to
be projective (but is not available for all data sets),
whereas the structure resulting from the HEAD col-
umn will be non-projective for some sentences of
some languages (but is always available).
10) PDEPREL: Dependency relation to the
PHEAD, or an underscore if not available.
As should be obvious from the description above,
our format assumes that each token has exactly one
head. Some dependency grammars, and also some
treebanks, allow tokens to have more than one head,
although often there is a distinction between primary
and optional secondary relations, e.g. in the Danish
Dependency Treebank (Kromann, 2003), the Dutch
Alpino Treebank (van der Beek et al, 2002b) and
the German TIGER treebank (Brants et al, 2002).
For this shared task we decided to ignore any ad-
ditional relations. However the data format could
easily be extended with additional optional columns
in the future. Cycles do not occur in the shared task
data but are scored as normal if predicted by parsers.
The character encoding of all data files is Unicode
(specifically UTF-8), which is the only encoding to
cover all languages and therefore ideally suited for
multilingual parsing.
While the training data contained all 10 columns
(although sometimes only with dummy values, i.e.
underscores), the test data given to participants con-
tained only the first 6. Participants? parsers then
predicted the HEAD and DEPREL columns (any
predicted PHEAD and PDEPREL columns were ig-
nored). The predicted values were compared to the
gold standard HEAD and DEPREL.6 The official
evaluation metric is the labeled attachment score
(LAS), i.e. the percentage of ?scoring? tokens for
which the system has predicted the correct HEAD
and DEPREL. The evaluation script defines a non-
scoring token as a token where all characters of the
FORM value have the Unicode category property
?Punctuation?.7
6The official scoring script eval.pl, data sets for some
languages and instructions on how to get the rest, the software
used for the treebank conversions, much documentation, full
results and other related information will be available from the
permanent URL http://depparse.uvt.nl (also linked
from the CoNLL web page).
7See man perlunicode for the technical details and the
shared task website for our reasons for this decision. Note
that an underscore and a percentage sign also have the Unicode
?Punctuation? property.
151
We tried to take a test set that was representative
of the genres in a treebank and did not cut through
text samples. We also tried to document how we
selected this set.8 We aimed at having roughly the
same size for the test sets of all languages: 5,000
scoring tokens. This is not an exact requirement as
we do not want to cut sentences in half. The rel-
atively small size of the test set means that even
for the smallest treebanks the majority of tokens is
available for training, and the equal size means that
for the overall ranking of participants, we can sim-
ply compute the score on the concatenation of all
test sets.
4 Treebanks and their conversion
In selecting the treebanks, practical considerations
were the major factor. Treebanks had to be actually
available, large enough, have a license that allowed
free use for research or kind treebank providers who
temporarily waived the fee for the shared task, and
be suitable for conversion into the common format
within the limited time. In addition, we aimed at a
broad coverage of different language families.9 As
a general rule, we did not manually correct errors in
treebanks if we discovered some during the conver-
sion, see also Buchholz and Green (2006), although
we did report them to the treebank providers and
several got corrected by them.
4.1 Dependency treebanks
We used the following six dependency treebanks:
Czech: Prague Dependency Treebank10 (PDT)
(Bo?hmova? et al, 2003); Arabic: Prague Arabic De-
pendency Treebank11 (PADT) (Hajic? et al, 2004;
Smrz? et al, 2002); Slovene: Slovene Dependency
Treebank12 (SDT) (Dz?eroski et al, 2006); Danish:
8See the shared task website for a more detailed discussion.
9That was also the reason why we decided not to include
a fifth Germanic language (English) although the freely avail-
able SUSANNE treebank (Sampson, 1995) or possibly the Penn
Treebank would have qualified otherwise.
10Many thanks to Jan Hajic? for granting the temporary li-
cense for CoNLL-X and talking to LDC about it, to Christo-
pher Cieri for arranging distribution through LDC and to Tony
Castelletto for handling the distribution.
11Many thanks to Yuval Krymolowski for converting the tree-
bank, Otakar Smrz? for valuable help during the conversion and
thanks again to Jan Hajic?, Christopher Cieri and Tony Castel-
letto.
12Many thanks to the SDT people for granting the special
license for CoNLL-X and to Tomaz? Erjavec for converting the
Danish Dependency Treebank13 (Kromann, 2003);
Swedish: Talbanken0514 (Teleman, 1974; Einars-
son, 1976; Nilsson et al, 2005); Turkish: Metu-
Sabanc? treebank15 (Oflazer et al, 2003; Atalay et
al., 2003).
The conversion of these treebanks was the easi-
est task as the linguistic representation was already
what we needed, so the information only had to be
converted from SGML or XML to the shared task
format. Also, the relevant information had to be dis-
tributed appropriately over the CPOSTAG, POSTAG
and FEATS columns.
For the Swedish data, no predefined distinction
into coarse and fine-grained PoS was available, so
the two columns contain identical values in our for-
mat. For the Czech data, we sampled both our train-
ing and test data from the official ?training? partition
because only that one contains gold standard PoS
tags, which is also what is used in most other data
sets. The Czech DEPREL values include the suf-
fixes to mark coordination, apposition and parenthe-
sis, while these have been ignored during the con-
version of the much smaller Slovene data. For the
Arabic data, sentences with missing annotation were
filtered out during the conversion.
The Turkish treebank posed a special problem
because it analyzes each word as a sequence of
one or more inflectional groups (IGs). Each IG
consists of either a stem or a derivational suffix
plus all the inflectional suffixes belonging to that
stem/derivational suffix. The head of a whole word
is not just another word but a specific IG of another
word.16 One can easily map this representation to
one in which the head of a word is a word but that
treebank for us.
13Many thanks to Matthias Trautner Kromann and assistants
for creating the DDT and releasing it under the GNU General
Public License and to Joakim Nivre, Johan Hall and Jens Nils-
son for the conversion of DDT to Malt-XML.
14Many thanks to Jens Nilsson, Johan Hall and Joakim Nivre
for the conversion of the original Talbanken to Talbanken05
and for making it freely available for research purposes and to
Joakim Nivre again for prompt and proper respons to all our
questions.
15Many thanks to Bilge Say and Kemal Oflazer for grant-
ing the license for CoNLL-X and answering questions and to
Gu?ls?en Eryig?it for making many corrections to the treebank and
discussing some aspects of the conversion.
16This is a bit like saying that in ?the usefulness of X for
Y?, ?for Y? links to ?use-? and not to ?usefulness?. Only that
in Turkish, ?use?, ?full? and ?ness? each could have their own
inflectional suffixes attached to them.
152
mapping would lose information and it is not clear
whether the result is linguistically meaningful, prac-
tically useful, or even easier to parse because in the
original representation, each IG has its own PoS and
morphological features, so it is not clear how that in-
formation should be represented if all IGs of a word
are conflated. We therefore chose to represent each
IG as a separate token in our format. To make the
result a connected dependency structure, we defined
the HEAD of each non-word-final IG to be the fol-
lowing IG and the DEPREL to be ?DERIV?. We as-
signed the stem of the word to the first IG?s LEMMA
column, with all non-first IGs having LEMMA ? ?,
and the actual word form to the last IG, with all non-
last IGs having FORM ? ?. As already mentioned in
Section 3, the underscore has the punctuation char-
acter property, therefore non-last IGs (whose HEAD
and DEPREL were introduced by us) are not scoring
tokens. We also attached or reattached punctuation
(see the README available at the shared task web-
site for details.)
4.2 Phrase structure with functions for all
constituents
We used the following five treebanks of this type:
German: TIGER treebank17 (Brants et al, 2002);
Japanese: Japanese Verbmobil treebank18 (Kawata
and Bartels, 2000); Portuguese: The Bosque part
of the Floresta sinta?(c)tica19 (Afonso et al, 2002);
Dutch: Alpino treebank20 (van der Beek et al,
2002b; van der Beek et al, 2002a); Chinese: Sinica
17Many thanks to the TIGER team for allowing us to use the
treebank for the shared task and to Amit Dubey for converting
the treebank.
18Many thanks to Yasuhiro Kawata, Julia Bartels and col-
leagues from Tu?bingen University for the construction of the
original Verbmobil treebank for Japanese and to Sandra Ku?bler
for providing the data and granting the special license for
CoNLL-X.
19Many thanks to Diana Santos, Eckhard Bick and other
Floresta sint(c)tica project members for creating the treebank
and making it publicly available, for answering many questions
about the treebank (Diana and Eckhard), for correcting prob-
lems and making new releases (Diana), and for sharing scripts
and explaining the head rules implemented in them (Eckhard).
Thanks also to Jason Baldridge for useful discussions and to
Ben Wing for independently reporting problems which Diana
then fixed.
20Many thanks to Gertjan van Noord and the other people at
the University of Groningen for creating the Alpino Treebank
and releasing it for free, to Gertjan van Noord for answering all
our questions and for providing extra test material and to Antal
van den Bosch for help with the memory-based tagger.
treebank21 (Chen et al, 2003).
Their conversion to dependency format required
the definition of a head table. Fortunately, in con-
trast to the Penn Treebank for which the head ta-
ble is based on POS22 we could use the gram-
matical functions annotated in these treebanks.
Therefore, head rules are often of the form: the
head child of a VP/clause is the child with the
HD/predicator/hd/Head function. The DEPREL
value for a token is the function of the biggest con-
stituent of which this token is the lexical head. If the
constituent comprising the complete sentence did
not have a function, we gave its lexical head token
the DEPREL ?ROOT?.
For the Chinese treebank, most functions are not
grammatical functions (such as ?subject?, ?object?)
but semantic roles (such as ?agent?, ?theme?). For
the Portuguese treebank, the conversion was compli-
cated by the fact that a detailed specification existed
which tokens should be the head of which other to-
kens, e.g. the finite verb must be the head of the
subject and the complementzier but the main verb
must be the head of the complements and adjuncts.23
Given that the Floresta sinta?(c)tica does not use tra-
ditional VP constituents but rather verbal chunks
(consisting mainly of verbs), a simple Magerman-
Collins-style head table was not sufficient to derive
the required dependency structure. Instead we used
a head table that defined several types of heads (syn-
tactic, semantic) and a link table that specified what
linked to which type of head.24
Another problem existed with the Dutch tree-
bank. Its original PoS tag set is very coarse and
the PoS and the word stem information is not very
reliable.25 We therefore decided to retag the tree-
bank automatically using the Memory-Based Tag-
ger (MBT) (Daelemans et al, 1996) which uses a
very fine-grained tag set. However, this created a
problem with multiwords. MBT does not have the
concept of multiwords and therefore tags all of their
21Many thanks to Academia Sinica for granting the tempo-
rary license for CoNLL-X, to Keh-Jiann Chen for answering
our questions and to Amit Dubey for converting the treebank.
22containing rules such as: the head child of a VP is the left-
most ?to?, or else the leftmost past tense verb, or else etc.
23Eckhard Bick, p.c.
24See the conversion script bosque2MALT.py and the
README file at the shared task website for details.
25http://www.let.rug.nl/vannoord/trees/Papers/diffs.pdf
153
components individually. As Alpino does not pro-
vide an internal structure for multiwords, we had
to treat multiwords as one token. However, we
then lack a proper PoS for the multiword. After
much discussion, we decided to assign each multi-
word the CPOSTAG ?MWU? (multiword unit) and
a POSTAG which is the concatenation of the PoS
of all the components as predicted by MBT (sepa-
rated by an underscore). Likewise, the FEATS are
a concatenation of the morphological features of all
components. This approach resulted in many dif-
ferent POSTAG values for the training set and even
in unseen values in the test set. It remains to be
tested whether our approach resulted in data sets bet-
ter suited for parsing than the original.
4.3 Phrase structure with some functions
We used two treebanks of this type: Spanish:
Cast3LB26 (Civit Torruella and Mart?? Anton??n,
2002; Navarro et al, 2003; Civit et al, 2003); Bul-
garian: BulTreeBank27 (Simov et al, 2002; Simov
and Osenova, 2003; Simov et al, 2004; Osenova and
Simov, 2004; Simov et al, 2005).
Converting a phrase structure treebank with only
a few functions to a dependency format usually re-
quires linguistic competence in the treebank?s lan-
guage in order to create the head table and miss-
ing function labels. We are grateful to Chanev et
al. (2006) for converting the BulTreeBank to the
shared task format and to Montserrat Civit for pro-
viding us with a head table and a function mapping
for Cast3LB.28
4.4 Data set characteristics
Table 1 shows details of all data sets. Following
Nivre and Nilsson (2005) we use the following def-
inition: ?an arc (i, j) is projective iff all nodes oc-
curring between i and j are dominated by i (where
dominates is the transitive closure of the arc rela-
26Many thanks to Montserrat Civit and Toni Mart?? for allow-
ing us to use Cast3LB for CoNLL-X and to Amit Dubey for
converting the treebank.
27Many thanks to Kiril Simov and Petya Osenova for allow-
ing us to use the BulTreeBank for CoNLL-X.
28Although unfortunately, due to a bug, the function list was
not used and the Spanish data in the shared task ended up with
many DEPREL values being simply ? ?. By the time we dis-
covered this, the test data release date was very close and we
decided not to release new bug-fixed training material that late.
tion)?.29
5 Approaches
Table 2 tries to give an overview of the wide variety
of parsing approaches used by participants. We refer
to the individual papers for details. There are several
dimensions along which to classify approaches.
5.1 Top-down, bottom-up
Phrase structure parsers are often classified in terms
of the parsing order: top-down, bottom-up or var-
ious combinations. For dependency parsing, there
seem to be two different interpretations of the term
?bottom-up?. Nivre and Scholz (2004) uses this
term with reference to Yamada and Matsumoto
(2003), whose parser has to find all children of a
token before it can attach that token to its head.
We will refer to this as ?bottom-up-trees?. An-
other use of ?bottom-up? is due to Eisner (1996),
who introduced the notion of a ?span?. A span
consists of a potential dependency arc r between
two tokens i and j and all those dependency arcs
that would be spanned by r, i.e. all arcs between
tokens k and l with i ? k, l ? j. Parsing in
this order means that the parser has to find all chil-
dren and siblings on one side of a token before it
can attach that token to a head on the same side.
This approach assumes projective dependency struc-
tures. Eisner called this approach simply ?bottom-
up?, while Nivre, whose parser implicitly also fol-
lows this order, called it ?top-down/bottom-up? to
distinguish it from the pure ?bottom-up(-trees)? or-
der of Yamada and Matsumoto (2003). To avoid
confusion, we will refer to this order as ?bottom-up-
spans?.
5.2 Unlabeled parsing versus labeling
Given that the parser needs to predict the HEAD as
well as the DEPREL value, different approaches are
possible: predict the (probabilities of the) HEADs
of all tokens first, or predict the (probabilities of
the) DEPRELs of all tokens first, or predict the
HEAD and DEPREL of one token before predict-
ing these values for the next token. Within the
first approach, each dependency can be labeled in-
dependently (Corston-Oliver and Aue, 2006) or a
29Thanks to Joakim Nivre for explaining this.
154
Ar Ch Cz Da Du Ge Ja Po Sl Sp Sw Tu Bu
lang. fam. Sem. Sin. Sla. Ger. Ger. Ger. Jap. Rom. Sla. Rom. Ger. Ura. Sla.
genres 1: ne 6 3 8+ 5+ 1: ne 1: di 1: ne 1: no 9 4+ 8 12
annotation d c+f d d dc+f dc+f c+f dc+f d c(+f) dc+f/d d c+t
training data
tokens (k) 54 337 1249 94 195 700 151 207 29 89 191 58 190
%non-scor. 8.8 a0.8 14.9 13.9 11.3 11.5 11.6 14.2 17.3 12.6 11.0 b33.1 14.4
units (k) 1.5 57.0 72.7 5.2 13.3 39.2 17.0 9.1 1.5 3.3 11.0 5.0 12.8
tokens/unit c37.2 d5.9 17.2 18.2 14.6 17.8 e8.9 22.8 18.7 27.0 17.3 11.5 14.8
LEMMA f(+) ? + ? + ? ? + + + ? + ?
CPOSTAGs 14 13+9 12 10 13 g52 20 15 11 15 37 14 11
POSTAGs 19 h294+9 63 24 i302 52 77 21 28 38 37 30 53
FEATS 19 ? 61 47 81 ? 4 146 51 33 ? 82 50
DEPRELs 27 82 78 52 26 46 7 55 25 21 56 25 18
D.s H.=0 15 1 14 1 1 1 1 6 6 1 1 1 1
%HEAD=0 5.5 16.9 6.7 6.4 8.9 6.3 18.6 5.1 5.9 4.2 6.5 13.4 7.9
%H. preced. 82.9 24.8 50.9 75.0 46.5 50.9 8.9 60.3 47.2 60.8 52.8 6.2 62.9
%H. follow. 11.6 58.2 42.4 18.6 44.6 42.7 72.5 34.6 46.9 35.1 40.7 80.4 29.2
H.=0/unit 1.9 1.0 1.0 1.0 1.2 1.0 1.5 1.0 j0.9 1.0 1.0 1.0 1.0
%n.p. arcs 0.4 0.0 1.9 1.0 5.4 2.3 k1.1 1.3 1.9 l0.1 1.0 1.5 0.4
%n.p. units 11.2 0.0 23.2 15.6 36.4 27.8 5.3 18.9 22.2 1.7 9.8 11.6 5.4
test data
scor. tokens 4990 4970 5000 5010 4998 5008 5003 5009 5004 4991 5021 5021 5013
%new form 17.3 9.3 5.2 18.1 20.7 6.5 0.96 11.6 22.0 14.7 18.0 41.4 14.5
%new lem. 4.3 n/a 1.8 n/a 15.9 n/a n/a 7.8 9.9 9.7 n/a 13.2 n/a
Table 1: Characteristics of the data sets for the 13 languages (abbreviated by their first two letters): language family (Semitic,
Sino-Tibetan, Slavic, Germanic, Japonic (or language isolate), Romance, Ural-Altaic); number of genres, and genre if only one
(news, dialogue, novel); type of annotation (d=dependency, c=constituents, dc=discontinuous constituents, +f=with functions,
+t=with types). For the training data: number of tokens (times 1000); percentage of non-scoring tokens; number of parse tree units
(usually sentences, times 1000); average number of (scoring and non-scoring) tokens per parse tree unit; whether a lemma or stem
is available; how many different CPOSTAG values, POSTAG values, FEATS components and DEPREL values occur for scoring
tokens; how many different values for DEPREL scoring tokens with HEAD=0 can have (if that number is 1, there is one designated
label (e.g. ?ROOT?) for tokens with HEAD=0); percentage of scoring tokens with HEAD=0, a head that precedes or a head that
follows the token (this nicely shows which languages are predominantly head-initial or head-final); the average number of scoring
tokens with HEAD=0 per parse tree unit; the percentage of (scoring and non-scoring) non-projective relations and of parse tree
units with at least one non-projective relation. For the test data: number of scoring tokens; percentage of scoring tokens with a
FORM or a LEMMA that does not occur in the training data.
afinal punctuation was deliberately left out during the conversion (as it is explicitly excluded from the tree structure)
bthe non-last IGs of a word are non-scoring, see Section 4.1
cin many cases the parse tree unit in PADT is not a sentence but a paragraph
din many cases the unit in Sinica is not a sentence but a comma-separated clause or phrase
ethe treebank consists of transcribed dialogues, in which some sentences are very short, e.g. just ?Hai.? (?Yes.?)
fonly part of the Arabic data has non-underscore values for the LEMMA column
gno mapping from fine-grained to coarse-grained tags was available; same for Swedish
h9 values are typos; POSTAGs also encode subcategorization information for verbs and some semantic information for con-
junctions and nouns; some values also include parts in square brackets which in hindsight should maybe have gone to FEATS
idue to treatment of multiwords
jprobably due to some sentences consisting only of non-scoring tokens, i.e. punctuation
kthese are all disfluencies, which are attached to the virtual root node
lfrom co-indexed items in the original treebank; same for Bulgarian
155
algorithm ver. hor. search lab. non-proj learner pre post opt
all pairs
McD MST/Eisner b-s irr. opt/approx. 2nd + a MIRA ? ? ?
Cor MST/Eisner b-s irr. optimal 2nd ? BPMb+ME [SVM] + c ? ?
Shi MST/CLE irr. irr. optimal 1st +, CLE MIRA ? ? ?
Can own algorithm irr. irr. approx.(?) int. + d TiMBL ? ? +
Rie ILP irr. irr. increment. int. + e MIRA ? ? +
Bic CG-inspired mpf mpf backtrack(?) int. + f MLE(?) + g + h ?
stepwise
Dre hagi/Eisner/rerank b-s irr. best 1st exh 2nd ? MLE ? ? + j
Liu own algorithm b-t mpf det./local int. ? MLE ? ? ?
Car Eisner b-s irr. approx. int. ? perceptron ? ? ?
stepwise: classifier-based
Att Y&M b-t for. determin. int. + k ME [MBL,SVM,...] stem ? ?
Cha Y&M b-t for. local 2nd ? l perceptron (SNoW) proj ? ?
Yur own algorithm b-s irr. determin. int. ? decision list (GPA)m ? ? ?
Che chunker+Nivre b-s for. determin. int.n ? SVM + ME [CRF] ? ? ?
Niv Nivre b-s for. determin. int. +, ps-pr SVM proj deproj +
Joh Nivre+MST/CLE b-s f+bo N-best int.p +, CLE SVM (LIBSVM) ? ?
Wu Nivre+root parser b-s f/bq det.[+exh.] int. ? [+] SVM (SVMLight) ? [+] r ?
other
Sch PCFG/CKY b-t irr. opt. int. +, traces MLE [ME] d2c c2d ?
Table 2: Overview of parsing approaches taken by participating groups (identified by the first three letters
of the first author): algorithm (Y&M: Yamada and Matsumoto (2003), ILP: Integer Linear Programming),
vertical direction (irrelevant, mpf: most probable first, bottom-up-spans, bottom-up-trees), horizontal direc-
tion (irrelevant, mpf: most probable first, forward, backward), search (optimal, approximate, incremental,
best-first exhaustive, deterministic), labeling (interleaved, separate and 1st step, separate and 2nd step),
non-projective (ps-pr: through pseudo-projective approach), learner (ME: Maximum Entropy; learners in
brackets were explored but not used in the official submission), preprocessing (projectivize, d2c: dependen-
cies to constituents), postprocessing (deprojectivize, c2d: constituents to dependencies), learner parameter
optimization per language
anon-projectivity through approximate search, used for some languages
b20 averaged perceptrons combined into a Bayes Point Machine
cintroduced a single POS tag ?aux? for all Swedish auxiliary and model verbs
dby having no projectivity constraint
eselective projectivity constraint for Japanese
fseveral approaches to non-projectivity
gusing some FEATS components to create some finer-grained POSTAG values
hreattachment rules for some types of non-projectivity
ihead automaton grammar
jdetermined the maximally allowed distance for relations
kthrough special parser actions
lpseudo-projectivizing training data only
mGreedy Prepend Algorithm
nbut two separate learners used for unlabeled parsing versus labeling
oboth foward and backward, then combined into a single tree with CLE
pbut two separate SVMs used for unlabeled parsing versus labeling
qforward parsing for Japanese and Turkish, backward for the rest
rattaching remaining unattached tokens through exhaustive search (not for submitted runs)
156
sequence classifier can label all children of a token
together (McDonald et al, 2006). Within the third
approach, HEAD and DEPREL can be predicted si-
multaneously, or in two separate steps (potentially
using two different learners).
5.3 All pairs
At the highest level of abstraction, there are two fun-
damental approaches, which we will call ?all pairs?
and ?stepwise?. In an ?all pairs? approach, every
possible pair of two tokens in a sentence is consid-
ered and some score is assigned to the possibility
of this pair having a (directed) dependency relation.
Using that information as building blocks, the parser
then searches for the best parse for the sentence.
This approach is one of those described in Eisner
(1996). The definition of ?best? parse depends on
the precise model used. That model can be one that
defines the score of a complete dependency tree as
the sum of the scores of all dependency arcs in it.
The search for the best parse can then be formalized
as the search for the maximum spanning tree (MST)
(McDonald et al, 2005b). If the parse has to be pro-
jective, Eisner?s bottom-up-span algorithm (Eisner,
1996) can be used for the search. For non-projective
parses, McDonald et al (2005b) propose using the
Chu-Liu-Edmonds (CLE) algorithm (Chu and Liu,
1965; Edmonds, 1967) and McDonald and Pereira
(2006) describe an approximate extension of Eis-
ner?s algorithm. There are also alternatives to MST
which allow imposing additional constraints on the
dependency structure, e.g. that at most one depen-
dent of a token can have a certain label, such as ?sub-
ject?, see Riedel et al (2006) and Bick (2006). By
contrast, Canisius et al (2006) do not even enforce
the tree constraint, i.e. they allow cycles. In a vari-
ant of the ?all pairs? approach, only those pairs of
tokens are considered that are not too distant (Cani-
sius et al, 2006).
5.4 Stepwise
In a stepwise approach, not all pairs are considered.
Instead, the dependency tree is built stepwise and
the decision about what step to take next (e.g. which
dependency to insert) can be based on information
about, in theory all, previous steps and their results
(in the context of generative probabilistic parsing,
Black et al (1993) call this the history). Stepwise
approaches can use an explicit probability model
over next steps, e.g. a generative one (Eisner, 1996;
Dreyer et al, 2006), or train a machine learner to
predict those. The approach can be deterministic (at
each point, one step is chosen) or employ various
types of search. In addition, parsing can be done in
a bottom-up-constituent or a bottom-up-spans fash-
ion (or in another way, although this was not done in
this shared task). Finally, parsing can start at the first
or the last token of a sentence. When talking about
languages that are written from left to right, this dis-
tinction is normally referred to as left-to-right ver-
sus right-to-left. However, for multilingual parsing
which includes languages that are written from right
to left (Arabic) or sometimes top to bottom (Chi-
nese, Japanese) this terminology is confusing be-
cause it is not always clear whether a left-to-right
parser for Arabic would really start with the left-
most (i.e. last) token of a sentence or, like for other
languages, with the first (i.e. rightmost). In general,
starting with the first token (?forward?) makes more
sense from a psycholinguistic point of view but start-
ing with the last (?backward?) might be beneficial
for some languages (possibly related to them being
head-initial versus head-final languages). The pars-
ing order directly determines what information will
be available from the history when the next decision
needs to be made. Stepwise parsers tend to inter-
leave the prediction of HEAD and DEPREL.
5.5 Non-projectivity
All data sets except the Chinese one contain some
non-projective dependency arcs, although their pro-
portion varies from 0.1% to 5.4%. Participants took
the following approaches to non-projectivity:
? Ignore, i.e. predict only projective parses. De-
pending on the way the parser is trained, it
might be necessary to at least projectivize the
training data (Chang et al, 2006).
? Always allow non-projective arcs, by not im-
posing any projectivity constraint (Shimizu,
2006; Canisius et al, 2006).
? Allow during parsing under certain conditions,
e.g. for tokens with certain properties (Riedel
et al, 2006; Bick, 2006) or if no alternative
projective arc has a score above the threshold
157
(Bick, 2006) or if the classifier chooses a spe-
cial action (Attardi, 2006) or the parser predicts
a trace (Schiehlen and Spranger, 2006).
? Introduce through post-processing, e.g.
through reattachment rules (Bick, 2006) or
if the change increases overall parse tree
probability (McDonald et al, 2006).
? The pseudo-projective approach (Nivre and
Nilsson, 2005): Transform non-projective
training trees to projective ones but encode
the information necessary to make the inverse
transformation in the DEPREL, so that this in-
verse transformation can also be carried out on
the test trees (Nivre et al, 2006).
5.6 Data columns used
Table 3 shows which column values have
been used by participants. Nobody used the
PHEAD/PDEPREL column in any way. It is likely
that those who did not use any of the other columns
did so mainly for practical reasons, such as the
limited time and/or the difficulty to integrate it into
an existing parser.
5.6.1 FORM versus LEMMA
Lemma or stem information has often been ig-
nored in previous dependency parsers. In the shared
task data, it was available in just over half the data
sets. Both LEMMA and FORM encode lexical in-
formation. There is therefore a certain redundancy.
Participants have used these two columns in differ-
ent ways:
? Use only one (see Table 3).
? Use both, in different features. Typically, a fea-
ture selection routine and/or the learner itself
(through weights) will decide about the impor-
tance of the resulting features.
? Use a variant of the FORM as a substitute for
a missing LEMMA. Bick (2006) used the low-
ercased FORM if the LEMMA is not available,
Corston-Oliver and Aue (2006) a prefix and At-
tardi (2006) a stem derived by a rule-based sys-
tem for Danish, German and Swedish.
form lem. cpos pos feats
McD ++ a + b ?? + +, co+cr.pr.
Cor + + + c ++ +, co+cr.pr.d
Shi + ? + ? ?
Can + ? ? + ?
Rie + e + + + f + cr.pr.
Bic (+) + + g + (+)
Dre ++ h + rer. rer. ?
Liu (+) + ++ + ?
Car ++ + ++ + + comp.
Att (+) + + ? (+)
Cha ? + ? + + atomic
Yur + + + + + comp.
Che + + + + + atomic?
Niv + + + + + comp.
Joh + ? + + + comp.
Wu + ? + + ?
Sch ? (+)i ? (+) (+)
Table 3: Overview of data columns used by partici-
pating groups. ???: a column value was not used at
all. ?+?: used in at least some features. ?(+)?: Vari-
ant of FORM used only if LEMMA is missing, or
only parts of FEATS used. ?++?: used more exten-
sively than another column containing related infor-
mation (where FORM and LEMMA are related, as
are CPOSTAG and POSTAG), e.g. also in combina-
tion features or features for context tokens in addi-
tion to features for the focus token(s). ?rer.?: used
in the reranker only. For the last column: atomic,
comp. = components, cr.pr. = cross-product.
aalso prefix and suffix for labeler
binstead of form for Arabic and Spanish
cinstead of POSTAG for Dutch and Turkish
dfor labeler; unlab. parsing: only some for global features
ealso prefix
falso 1st character of POSTAG
gonly as backoff
hreranker: also suffix; if no lemma, use prefix of FORM
iLEMMA, POSTAG, FEATS only for back-off smoothing
5.6.2 CPOSTAG versus POSTAG
All data sets except German and Swedish had dif-
ferent values for CPOSTAG and POSTAG, although
the granularity varied widely. Again, there are dif-
ferent approaches to dealing with the redundancy:
? Use only one for all languages.
158
? Use both, in different features. Typically, a fea-
ture selection routine and/or the learner itself
(through weights) will decide about the impor-
tance of the resulting features.
? Use one or the other for each language.
5.6.3 Using FEATS
By design, a FEATS column value has internal
structure. Splitting it at the ?|?30 results in a set of
components. The following approaches have been
used:
? Ignore the FEATS.
? Treat the complete FEATS value as atomic, i.e.
do not split it into components.
? Use only some components, e.g. Bick (2006)
uses only case, mood and pronoun subclass and
Attardi (2006) uses only gender, number, per-
son and case.
? Use one binary feature for each component.
This is likely to be useful if grammatical func-
tion is indicated by case.
? Use one binary feature for each cross-product
of the FEATS components of i and the FEATS
components of j. This is likely to be useful for
agreement phenomena.
? Use one binary feature for each FEATS com-
ponent of i that also exists for j. This is a more
explicit way to model agreement.
5.7 Types of features
When deciding whether there should be a depen-
dency relation between tokens i and j, all parsers
use at least information about these two tokens. In
addition, the following sources of information can
be used (see Table 4): token context (tc): a limited
number (determined by the window size) of tokens
directly preceding or following i or j; children: in-
formation about the already found children of i and
j; siblings: in a set-up where the decision is not ?is
there a relation between i and j? but ?is i the head of
j? or in a separate labeling step, the siblings of i are
the already found children of j; structural context
30or for Dutch, also at the ? ?
tc ch si sc di in gl co ac la op
McD + l + l ? l l + ? l (+)a
Cor + l b l + p ? + + ? ? (+)c
Shi + ? ? ? + ? ? + ? + ?
Can + ? ? ? + ? ? ? ? ? ?
Rie + ? + d ? ? ? ? + ? + e +
Bic + + f + g ? + + h ? + ? ++ (+)i
Dre r r + r + r ? + ? r r
Liu ? + ? + + ? ? + ? ? ?
Car + ? + ? + + ? + ? + ?
Att ? + + + ? ? ? ? + + (+)j
Cha + + ? l ? ? ? + + ? ?
Yur + + ? ? ? ? ? ? ? ? +
Che ? + + + + ? ? ? ? ? ?
Niv + + ? + ? ? ? ? ? + +
Joh + + ? + ? ? ? ? ? + ?
Wu + + ? + ? ? ? + ? + ?
Sch ? + ? ? ? ? ? ? ? + ?
Table 4: Overview of features used by participating
groups. See the text for the meaning of the column
abbreviations. For separate HEAD and DEPREL as-
signment: p: only for unlabeled parsing, l: only for
labeling, r: only for reranking.
aFORM versus LEMMA
bnumber of tokens governed by child
cPOSTAG versus CPOSTAG
dfor arity constraint
efor arity constraint
ffor ?full? head constraint
gfor uniqueness constraint
hfor barrier constraint
iof constraints
jPOS window size
(sc) other than children/siblings: neighboring sub-
trees/spans, or ancestors of i and j; distance from i
to j; information derived from all the tokens in be-
tween i and j (e.g. whether there is an intervening
verb or how many intervening commas there are);
global features (e.g. does the sentence contain a fi-
nite verb); explicit feature combinations (depending
on the learner, these might not be necessary, e.g. a
polynomial kernel routinely combines features); for
classifier-based parsers: the previous actions, i.e.
classifications; whether information about labels is
used as input for other decisions. Finally, the pre-
cise set of features can be optimized per language.
159
6 Results
Table 5 shows the official results for submitted
parser outputs.31 The two participant groups with
the highest total score are McDonald et al (2006)
and Nivre et al (2006). As both groups had much
prior experience in multilingual dependency pars-
ing (see Section 2), it is not too surprising that they
both achieved good results. It is surprising, how-
ever, how similar their total scores are, given that
their approaches are quite different (see Table 2).
The results show that experiments on just one or two
languages certainly give an indication of the useful-
ness of a parsing approach but should not be taken
as proof that one algorithm is better for ?parsing? (in
general) than another that performs slightly worse.
The Bulgarian scores suggest that rankings would
not have been very different had it been the 13th
obligatory languages.
Table 6 shows that the same holds had we used an-
other evaluation metric. Note that a negative number
in both the third and fifth column indicates that er-
rors on HEAD and DEPREL occur together on the
same token more often than for other parsers. Fi-
nally, we checked that, had we also scored on punc-
tuation tokens, total scores as well as rankings would
only have shown very minor differences.
7 Result analysis
7.1 Across data sets
The average LAS over all data sets varies between
56.0 for Turkish and 85.9 for Japanese. Top scores
vary between 65.7 for Turkish and 91.7 for Japanese.
In general, there is a high correlation between the
best scores and the average scores. This means that
data sets are inherently easy or difficult, no mat-
ter what the parsing approach. The ?easiest? one is
clearly the Japanese data set. However, it would be
wrong to conclude from this that Japanese in general
is easy to parse. It is more likely that the effect stems
from the characteristics of the data. The Japanese
Verbmobil treebank contains dialogue within a re-
stricted domain (making business appointments). As
31Unfortunately, urgent other obligations prevented two par-
ticipants (John O?Neil and Kenji Sagae) from submitting a pa-
per about their shared task work. Their results are indicated by
a smaller font. Sagae used a best-first probabilistic version of
Y&M (p.c.).
LAS unlabeled label acc.
McD 80.3 = 86.6 ?1 86.7
Niv 80.2 = 85.5 +1 86.8
O?N 78.4 = 85.3 ?1 85.0
Rie 77.9 = 85.0 ?1 84.9
Sag 77.8 ?2 83.7 +2 85.6
Che 77.7 +1 84.6 = 84.2
Cor 76.9 +1 84.4 ?1 84.0
Cha 76.8 = 83.5 +1 84.1
Joh 74.9 ?1 80.4 = 83.7
Car 74.7 +1 81.2 = 83.5
Wu 71.7 ?1 78.4 ?1 79.1
Can 70.8 +1 78.4 ?1 78.6
Bic 70.0 = 77.5 a+2 80.3
Dre 65.2 ?1 74.5 ?1 75.2
Yur 65.0 ?1 73.5 ?2 70.9
Liu 63.3 ?2 70.7 = 73.6
Sch 62.8 = 72.1 b+3 75.7
Att 61.2 c+4 76.2 = 70.7
Shi 34.2 = 38.7 = 39.7
Table 6: Differences in ranking depending on the
evaluation metric. The second column repeats the
official metric (LAS). The third column shows how
the ranking for each participant changes (or not: ?=?)
if the unlabeled attachment scores, as shown in the
fourth column, are used. The fifth column shows
how the ranking changes (in comparison to LAS) if
the label accuracies, as shown in the sixth column,
are used.
aIn Bick?s method, preference is given to the assignment of
dependency labels.
bSchiehlen derived the constituent labels for his PCFG ap-
proach from the DEPREL values.
cDue to the bug (see footnote with Table 5).
can be seen in Table 1, there are very few new
FORM values in the test data, which is an indica-
tion of many dialogues in the treebank being sim-
ilar. In addition, parsing units are short on aver-
age. Finally, the set of DEPREL values is very small
and consequently the ratio between (C)POSTAG and
DEPREL values is extremely favorable. It would
be interesting to apply the shared task parsers to
the Kyoto University Corpus (Kurohashi and Nagao,
1997), which is the standard treebank for Japanese
and has also been used by Kudo and Matsumoto
160
Ar Ch Cz Da Du Ge Ja Po Sl Sp Sw Tu Tot SD Bu
McD 66.9 85.9 80.2 84.8 79.2 87.3 90.7 86.8 73.4 82.3 82.6 63.2 80.3 8.4 87.6
Niv 66.7 86.9 78.4 84.8 78.6 85.8 91.7 87.6 70.3 81.3 84.6 65.7 80.2 8.5 87.4
O?N 66.7 86.7 76.6 82.8 77.5 85.4 90.6 84.7 71.1 79.8 81.8 57.5 78.4 9.4 85.2
Rie 66.7 90.0 67.4 83.6 78.6 86.2 90.5 84.4 71.2 77.4 80.7 58.6 77.9 10.1 0.0
Sag 62.7 84.7 75.2 81.6 76.6 84.9 90.4 86.0 69.1 77.7 82.0 63.2 77.8 9.0 0.0
Che 65.2 84.3 76.2 81.7 71.8 84.1 89.9 85.1 71.4 80.5 81.1 61.2 77.7 8.7 86.3
Cor 63.5 79.9 74.5 81.7 71.4 83.5 90.0 84.6 72.4 80.4 79.7 61.7 76.9 8.5 83.4
Cha 60.9 85.1 72.9 80.6 72.9 84.2 89.1 84.0 69.5 79.7 82.3 60.5 76.8 9.4 0.0
Joh 64.3 72.5 71.5 81.5 72.7 80.4 85.6 84.6 66.4 78.2 78.1 63.4 74.9 7.7 0.0
Car 60.9 83.7 68.8 79.7 67.3 82.4 88.1 83.4 68.4 77.2 78.7 58.1 74.7 9.7 83.3
Wu 63.8 74.8 59.4 78.4 68.5 76.5 90.1 81.5 67.8 73.0 71.7 55.1 71.7 9.7 79.7
Can 57.6 78.4 60.9 77.9 74.6 77.6 87.4 77.4 59.2 68.3 79.2 51.1 70.8 11.1 78.7
Bic 55.4 76.2 63.0 74.6 69.5 74.7 84.8 78.2 64.3 71.4 74.1 53.9 70.0 9.3 79.2
Dre 53.4 71.6 60.5 66.6 61.6 71.0 82.9 75.3 58.7 67.6 67.6 46.1 65.2 9.9 74.8
Yur 52.4 72.7 51.9 71.6 62.8 63.8 84.4 70.4 55.1 69.6 65.2 60.3 65.0 9.5 73.5
Liu 50.7 75.3 58.5 77.7 59.4 68.1 70.8 71.1 57.2 65.1 63.8 41.7 63.3 10.4 67.6
Sch 44.4 66.2 53.3 76.1 72.1 68.7 83.4 71.0 50.7 47.0 71.1 49.8 62.8 13.0 0.0
Att 53.8 54.9 59.8 66.4 58.2 69.8 65.4 75.4 57.2 67.4 68.8 37.8 a61.2 9.9 72.9
Shi 62.8 0.0 0.0 75.8 0.0 0.0 0.0 0.0 64.6 73.2 79.5 54.2 34.2 36.3 0.0
Av 59.9 78.3 67.2 78.3 70.7 78.6 85.9 80.6 65.2 73.5 76.4 56.0 80.0
SD 6.5 8.8 8.9 5.5 6.7 7.5 7.1 5.8 6.8 8.4 6.5 7.7 6.3
Table 5: Labeled attachment scores of parsers on the 13 test sets. The total score (Tot) and standard devia-
tions (SD) from the average per participant are calculated over the 12 obligatory languages (i.e. excluding
Bulgarian). Note that due to the equal sizes of the test sets for all languages, the total scores, i.e. the LAS
over the concatenation of the 12 obligatory test sets, are identical (up to the first decimal digit) to the average
LAS over the 12 test sets. Averages and standard deviations per data set are calculated ignoring zero scores
(i.e. results not submitted). The highest score for each column and those not significantly worse (p < 0.05)
are shown in bold face. Significance was computed using the official scoring script eval.pl and Dan
Bikel?s Randomized Parsing Evaluation Comparator, which implements stratified shuffling.
aAttardi?s submitted results contained an unfortunate bug which caused the DEPREL values of all tokens with HEAD=0 to
be an underscore (which is scored as incorrect). Using the simple heuristic of assigning the DEPREL value that most frequently
occured with HEAD=0 in training would have resulted in a total LAS of 67.5.
(2000), or to the domain-restricted Japanese dia-
logues of the ATR corpus (Lepage et al, 1998).32
Other relatively ?easy? data sets are Portuguese
(2nd highest average score but, interestingly, the
third-longest parsing units), Bulgarian (3rd), Ger-
man (4th) and Chinese (5th). Chinese also has the
second highest top score33 and Chinese parsing units
32Unfortunately, both these treebanks need to be bought, so
they could not be used for the shared task. Note also that
Japanese dependency parsers often operate on ?bunsetsus? in-
stead of words. Bunsetsus are related to chunks and consist of
a content word and following particles (if any).
33Although this seems to be somewhat of a mystery com-
pared to the ranking according to the average scores. Riedel et
are the shortest. and Chinese parsing units are the
shortest. We note that all ?easier? data sets offer
large to middle-sized training sets.
The most difficult data set is clearly the Turkish
one. It is rather small, and in contrast to Arabic
and Slovene, which are equally small or smaller, it
covers 8 genres, which results in a high percentage
of new FORM and LEMMA values in the test set.
It is also possible that parsers get confused by the
high proportion (one third!) of non-scoring tokens
al. (2006)?s top score is more than 3% absolute above the sec-
ond highest score and they offer no clear explanation for their
success.
161
and the many tokens with ? ? as either the FORM or
LEMMA. There is a clear need for further research
to check whether other representations result in bet-
ter performance.
The second-most difficult data set is Arabic. It is
quite small and has by far the longest parsing units.
The third-most difficult data set is Slovene. It has
the smallest training set. However, its average as
well as top score far exceed those for Arabic and
Turkish, which are larger. Interestingly, although the
treebank text comes from a single source (a transla-
tion of Orwell?s novel ?1984?), there is quite a high
proportion of new FORM and LEMMA values in the
test set. The fourth-most difficult data set is Czech
in terms of the average score and Dutch in terms of
the top score. The diffence in ranking for Czech is
probably due to the fact that it has by far the largest
training set and ironically, several participants could
not train on all data within the limited time, or else
had to partition the data and train one model for each
partition. Likely problems with the Dutch data set
are: noisy (C)POSTAG and LEMMA, (C)POSTAG
for multiwords, and the highest proportion of non-
projectivity.
Factors that have been discussed so far are: the
size of the training data, the proportion of new
FORM and LEMMA values in the test set, the ra-
tio of (C)POSTAG to DEPREL values, the average
length of the parsing unit the proportion of non-
projective arcs/parsing units. It would be interest-
ing to derive a formula based on those factors that
fits the shared task data and see how well it pre-
dicts results on new data sets. One factor that seems
to be irrelevant is the head-final versus head-initial
distinction, as both the ?easiest? and the most dif-
ficult data sets are for head-final languages. There
is also no clear proof that some language families
are easier (with current parsing methods) than oth-
ers. It would be interesting to test parsers on the
Hebrew treebank (Sima?an et al, 2001), to compare
performance to Arabic, the other Semitic language
in the shared task, or on the Hungarian Szeged Cor-
pus (Csendes et al, 2004), for another agglutinative
language.
7.2 Across participants
For most parsers, their ranking for a specific lan-
guage differs at most a few places from their over-
all ranking. There are some outliers though. For
example, Johansson and Nugues (2006) and Yuret
(2006) are seven ranks higher for Turkish than over-
all, while Riedel et al (2006) are five ranks lower.
Canisius et al (2006) are six and Schiehlen and
Spranger (2006) even eight ranks higher for Dutch
than overall, while Riedel et al (2006) are six ranks
lower for Czech and Johansson and Nugues (2006)
also six for Chinese. Some of the higher rankings
could be related to native speaker competence and
resulting better parameter tuning but other outliers
remain a mystery. Even though McDonald et al
(2006) and Nivre et al (2006) obtained very simi-
lar overall scores, a more detailed look at their per-
formance shows clear differences. Taken over all 12
obligatory languages, both obtain a recall of more
than 89% on root tokens (i.e. those with HEAD=0)
but Nivre?s precision on them is much lower than
McDonald?s (80.91 versus 91.07). This is likely to
be an effect of the different parsing approaches.
7.3 Across part-of-speech tags
When breaking down by part-of-speech the results
of all participants on all data sets, one can observe
some patterns of ?easy? and ?difficult? parts-of-
speech, at least in so far as tag sets are compara-
ble across treebanks. The one PoS that everybody
got 100% correct are the German infinitival mark-
ers (tag PTKZU; like ?to? in English). Accuracy on
the Swedish equivalent (IM) is not far off at 98%.
Other easy PoS are articles, with accuracies in the
nineties for German, Dutch, Swedish, Portuguese
and Spanish. As several participants have remarked
in their papers, prepositions are much more difficult,
with typical accuracies in the fifties or sixties. Simi-
larly, conjunctions typically score low, with accura-
cies even in the forties for Arabic and Dutch.
8 Future research
There are many directions for interesting research
building on the work done in this shared task. One
is the question which factors make data sets ?easy?
or difficult. Another is finding out how much of
parsing performance depends on annotations such
as the lemma and morphological features, which
are not yet routinely part of treebanking efforts. In
this respect, it would be interesting to repeat ex-
162
periments with the recently released new version of
the TIGER treebank which now contains this in-
formation. One line of research that does not re-
quire additional annotation effort is defining or im-
proving the mapping from coarse-grained to fine-
grained PoS tags.34 Another is harvesting and using
large-scale distributional data from the internet. We
also hope that by combining parsers we can achieve
even better performance, which in turn would facili-
tate the semi-automatic enlargement of existing tree-
banks and possibly the detection of remaining er-
rors. This would create a positive feedback loop.
Finally one must not forget that almost all of the
LEMMA, (C)POSTAG and FEATS values and even
part of the FORM column (the multiword tokens
used in many data sets and basically all tokeniza-
tion for Chinese and Japanese, where words are nor-
mally not delimited by spaces) have been manually
created or corrected and that the general parsing task
has to integrate automatic tokenization, morphologi-
cal analysis and tagging. We hope that the resources
created and lessons learned during this shared task
will be valuable for many years to come but also
that they will be extended and improved by others
in the future, and that the shared task website will
grow into an informational hub on multilingual de-
pendency parsing.
References
A. Arun and F. Keller. 2005. Lexicalization in crosslinguistic
probabilistic parsing: The case of French. In Proc. of the
43rd Annual Meeting of the ACL.
D. Bikel. 2002. Design of a multi-lingual, parallel-processing
statistical parsing engine. In Proc. of the Human Language
Technology Conf. (HLT).
E. Black, S. Abney, D. Flickenger, et al 1991. A procedure for
quantitatively comparing the syntactic coverage of English
grammars. In Speech and Natural Language: Proceedings
of a Workshop Held at Pacific Grove, California.
E. Black, F. Jelinek, J. Lafferty, D. Magerman, R. Mercer, and
S. Roukos. 1993. Towards history-based grammars: Using
richer models for probabilistic parsing. In Proc. of the 31rd
Annual Meeting of the ACL.
S. Buchholz and D. Green. 2006. Quality control of treebanks:
documenting, converting, patching. In LREC 2006 work-
shop on Quality assurance and quality measurement for lan-
guage and speech resources.
34For the Swedish Talbanken05 corpus, that work has been
done after the shared task (see the treebank?s web site).
A. Chanev, K. Simov, P. Osenova, and S. Marinov. 2006. De-
pendency conversion and parsing of the BulTreeBank. In
Proc. of the LREC-Workshop Merging and Layering Lin-
guistic Information.
Y. Cheng, M. Asahara, and Y. Matsumoto. 2005. Chinese
deterministic dependency analyzer: Examining effects of
global features and root node finder. In Proc. of SIGHAN-
2005.
Y.J. Chu and T.H. Liu. 1965. On the shortest arborescence of a
directed graph. Science Sinica, 14:1396?1400.
M. Collins, J. Hajic, L. Ramshaw, and C. Tillmann. 1999.
A statistical parser for Czech. In Proc. of the 37th Annual
Meeting of the ACL.
M. Collins. 1996. A new statistical parser based on bigram
lexical dependencies. In Proc. of the 34th Annual Meeting
of the ACL.
M. Collins. 1997. Three generative, lexicalised models for sta-
tistical parsing. In Proc. of the 35th Annual Meeting of the
ACL.
M. Collins. 1999. Head-Driven Statistical Models for Natural
Language Parsing. Ph.D. thesis, University of Pennsylvania.
B. Cowan and M. Collins. 2005. Morphology and reranking for
the statistical parsing of Spanish. In Proc. of the Joint Conf.
on Human Language Technology and Empirical Methods in
Natural Language Processing (HLT/EMNLP).
D. Csendes, J. Csirik, and T. Gyimo?thy. 2004. The Szeged cor-
pus: a POS tagged and syntactically annotated Hungarian
natural language corpus. In Proc. of the 5th Intern. Work-
shop on Linguistically Interpreteted Corpora (LINC).
W. Daelemans, J. Zavrel, P. Berck, and S. Gillis. 1996. MBT:
A memory-based part of speech tagger-generator. In Proc.
of the 4th Workshop on Very Large Corpora (VLC).
A. Dubey and F. Keller. 2003. Probabilistic parsing for German
using sister-head dependencies. In Proc. of the 41st Annual
Meeting of the ACL.
J. Edmonds. 1967. Optimum branchings. Journal of Research
of the National Bureau of Standards, 71B:233?240.
J. Einarsson. 1976. Talbankens skriftspra?kskonkordans.
J. Eisner. 1996. Three new probabilistic models for depen-
dency parsing: An exploration. In Proc. of the 16th Intern.
Conf. on Computational Linguistics (COLING), pages 340?
345.
D. Elworthy. 2000. A finite-state parser with dependency struc-
ture output. In Proc. of the 6th Intern. Workshop on Parsing
Technologies (IWPT).
H. Gaifman. 1965. Dependency systems and phrase-structure
systems. Information and Control, 8:304?337.
D. Hays. 1964. Dependency theory: A formalism and some
observations. Language, 40:511?525.
R. Hudson. 1984. Word Grammar. Blackwell.
163
T. Kudo and Y. Matsumoto. 2000. Japanese dependency struc-
ture analysis based on support vector machines. In Proc. of
the Joint Conf. on Empirical Methods in Natural Language
Processing and Very Large Corpora (EMNLP/VLC).
S. Kurohashi and M. Nagao. 1994. KN parser: Japanese depen-
dency/case structure analyzer. In Proceedings of the Work-
shop on Sharable Natural Language, pages 48?55.
S. Kurohashi and M. Nagao. 1997. Kyoto University text cor-
pus project. In Proc. of the 5th Conf. on Applied Natural
Language Processing (ANLP), pages 115?118.
Y. Lepage, S. Ando, S. Akamine, and H. Iida. 1998. An anno-
tated corpus in Japanese using Tesnie`re?s structural syntax.
In ACL-COLING Workshop on Processing of Dependency-
Based Grammars, pages 109?115.
D. Lin. 1995. A dependency-based method for evaluating
broad-coverage parsers. In Proc. of the International Joint
Conference on Artificial Intelligence (IJCAI).
D. Magerman. 1995. Statistical decision-tree models for pars-
ing. In Proc. of the 33rd Annual Meeting of the ACL, pages
276?283.
M. Marcus, G. Kim, M. Marcinkiewicz, R. Mac-Intyre, A. Bies,
M. Ferguson, K. Katz, and B. Schasberger. 1994. The Penn
treebank: Annotating predicate argument structure. In Proc.
of the Workshop on Human Language Technology (HLT).
S. Marinov and J. Nivre. 2005. A data-driven dependency
parser for Bulgarian. In Proc. of the 4th Workshop on Tree-
banks and Linguistic Theories (TLT), pages 89?100.
R. McDonald and F. Pereira. 2006. Online learning of approx-
imate dependency parsing algorithms. In Proc. of the 11th
Conf. of the European Chapter of the ACL (EACL).
R. McDonald, K. Crammer, and F. Pereira. 2005a. Online
large-margin training of dependency parsers. In Proc. of the
43rd Annual Meeting of the ACL.
R. McDonald, F. Pereira, K. Ribarov, and J. Hajic. 2005b.
Non-projective dependency parsing using spanning tree al-
gorithms. In Proc. of the Joint Conf. on Human Language
Technology and Empirical Methods in Natural Language
Processing (HLT/EMNLP).
I. Mel?c?uk. 1988. Dependency Syntax: Theory and Practice.
The SUNY Press, Albany, N.Y.
J. Nivre and J. Nilsson. 2005. Pseudo-projective dependency
parsing. In Proc. of the 43rd Annual Meeting of the ACL,
pages 99?106.
J. Nivre and M. Scholz. 2004. Deterministic dependency pars-
ing of English text. In Proc. of the 20th Intern. Conf. on
Computational Linguistics (COLING), pages 64?70.
J. Nivre, J. Hall, and J. Nilsson. 2004. Memory-based depen-
dency parsing. In Proc. of the 8th Conf. on Computational
Natural Language Learning (CoNLL), pages 49?56.
K. Oflazer. 1999. Dependency parsing with an extended finite
state approach. In Proc. of the 37th Annual Meeting of the
ACL, pages 254?260.
G. Sampson. 1995. English for the Computer: The SUSANNE
Corpus and analytic scheme. Clarendon Press.
K. Sima?an, A. Itai, Y. Winter, A. Altman, and N. Nativ. 2001.
Building a tree-bank of modern Hebrew text. In Journal
Traitement Automatique des Langues (t.a.l.) ? Special Issue
on Natural Language Processing and Corpus Linguistics.
D. Sleator and D. Temperley. 1993. Parsing English with a link
grammar. In Proc. of the 3rd Intern. Workshop on Parsing
Technologies (IWPT).
P. Tapanainen and T. Ja?rvinen. 1997. A non-projective depen-
dency parser. In Proc. of the 5th Conf. on Applied Natural
Language Processing (ANLP).
U. Teleman, 1974. Manual fo?r grammatisk beskrivning av talad
och skriven svenska (MAMBA).
L. Tesnie`re. 1959. Ele?ment de syntaxe structurale. Klinck-
sieck, Paris.
H. Yamada and Y. Matsumoto. 2003. Statistical dependency
analysis with support vector machines. In Proc. of the 8th In-
tern. Workshop on Parsing Technologies (IWPT), pages 195?
206.
Papers by participants of CoNLL-X (this volume)
G. Attardi. 2006. Experiments with a multilanguage non-
projective dependency parser.
E. Bick. 2006. LingPars, a linguistically inspired, language-
independent machine learner for dependency treebanks.
S. Canisius, T. Bogers, A. van den Bosch, J. Geertzen, and
E. Tjong Kim Sang. 2006. Dependency parsing by infer-
ence over high-recall dependency predictions.
M. Chang, Q. Do, and D. Roth. 2006. A pipeline model for
bottom-up dependency parsing.
S. Corston-Oliver and A. Aue. 2006. Dependency parsing with
reference to Slovene, Spanish and Swedish.
M. Dreyer, D. Smith, and N. Smith. 2006. Vine parsing and
minimum risk reranking for speed and precision.
R. Johansson and P. Nugues. 2006. Investigating multilingual
dependency parsing.
R. McDonald, K. Lerman, and F. Pereira. 2006. Multilingual
dependency analysis with a two-stage discriminative parser.
J. Nivre, J. Hall, J. Nilsson, G. Eryig?it, and S. Marinov. 2006.
Labeled pseudo-projective dependency parsing with support
vector machines.
S. Riedel, R. C?ak?c?, and I. Meza-Ruiz. 2006. Multi-lingual de-
pendency parsing with incremental integer linear program-
ming.
M. Schiehlen and K. Spranger. 2006. Language indepen-
dent probabilistic context-free parsing bolstered by machine
learning.
N. Shimizu. 2006. Maximum spanning tree algorithm for non-
projective labeled dependency parsing.
D. Yuret. 2006. Dependency parsing as a classication problem.
164
Proceedings of the Workshop on Textual Entailment and Paraphrasing, pages 83?88,
Prague, June 2007. c?2007 Association for Computational Linguistics
Dependency-based paraphrasing for recognizing textual entailment
Erwin Marsi, Emiel Krahmer
Communication & Cognition
Tilburg University
The Netherlands
e.c.marsi@uvt.nl
e.j.krahmer@uvt.nl
Wauter Bosma
Human Media Interaction
University of Twente
The Netherlands
w.e.bosma@ewi.utwente.nl
Abstract
This paper addresses syntax-based para-
phrasing methods for Recognizing Textual
Entailment (RTE). In particular, we de-
scribe a dependency-based paraphrasing al-
gorithm, using the DIRT data set, and its
application in the context of a straightfor-
ward RTE system based on aligning depen-
dency trees. We find a small positive effect
of dependency-based paraphrasing on both
the RTE3 development and test sets, but the
added value of this type of paraphrasing de-
serves further analysis.
1 Introduction
Coping with paraphrases appears to be an essential
subtask in Recognizing Textual Entailment (RTE).
Most RTE systems incorporate some form of lex-
ical paraphrasing, usually relying on WordNet to
identify synonym, hypernym and hyponym rela-
tions among word pairs from text and hypothesis
(Bar-Haim et al, 2006, Table 2). Many systems
also address paraphrasing above the lexical level.
This can take the form of identifying or substitut-
ing equivalent multi-word strings, e.g., (Bosma and
Callison-Burch, 2006). A drawback of this approach
is that it is hard to cope with discontinuous para-
phrases containing one or more gaps. Other ap-
proaches exploit syntactic knowledge in the form
of parse trees. Hand-crafted transformation rules
can account for systematic syntactic alternation like
active-passive form, e.g., (Marsi et al, 2006). Al-
ternatively, such paraphrase rules may be automati-
cally derived from huge text corpora (Lin and Pan-
tel, 2001). There are at least two key advantages of
syntax-based over string-based paraphrasing which
are relevant for RTE: (1) it can cope with discontin-
uous paraphrases; (2) syntactic information such as
dominance relations, phrasal syntactic labels and de-
pendency relations, can be used to refine the coarse
matching on words only.
Here we investigate paraphrasing on the basis of
of syntactic dependency analyses. Our sole resource
is the DIRT data set (Lin and Pantel, 2001), an exten-
sive collection of automatically derived paraphrases.
These have been used for RTE before (de Salvo Braz
et al, 2005; Raina et al, 2005), and similar ap-
proaches to paraphrase mining have been applied
as well (Nielsen et al, 2006; Hickl et al, 2006).
However, in these approaches paraphrasing is al-
ways one factor in a complex system, and as a result
little is known of the contribution of paraphrasing
for the RTE task. In this paper, we focus entirely
on dependency-based paraphrasing in order to get a
better understanding of its usefulness for RTE. In the
next Section, we describe the DIRT data and present
an algorithm for dependency-based paraphrasing in
order to bring a pair?s text closer to its hypothesis.
We present statistics on coverage as well as qual-
itative discussion of the results. Section 3 then de-
scribes our RTE system and results with and without
dependency-based paraphrasing.
2 Dependency-based paraphrasing
2.1 Preprocessing RTE data
Starting from the text-hypothesis pairs in the RTE
XML format, we first preprocess the data. As the
text part may consist of more than one sentence,
we first perform sentence splitting using Mxtermi-
nator (Reynar and Ratnaparkhi, 1997), a maximum
83
entropy-based end of sentence classifier trained on
the Penn Treebank data. Next, each sentence is to-
kenized and syntactically parsed using the Minipar
parser (Lin, 1998). From the parser?s tabular output
we extract the word forms, lemmas, part-of-speech
tags and dependency relations. This information is
then stored in an ad-hoc XML format which repre-
sents the trees as an hierarchy of node elements in
order to facilitate tree matching.
2.2 DIRT data
The DIRT (Discovering Inference Rules from Text)
method is based on extending Harris Distributional
Hypothesis, which states that words that occurred in
the same contexts tend to be similar, to dependency
paths in parse trees (Lin and Pantel, 2001). Each
dependency path consists of at least three nodes: a
root node, and two non-root terminal nodes, which
are nouns. The DIRT data set we used consists of
over 182k paraphrase clusters derived from 1GB of
newspaper text. Each cluster consists of a unique
dependency path, which we will call the paraphrase
source, and a list of equivalent dependency paths,
which we will refer to as the paraphrase transla-
tions, ordered in decreasing value of point-wise mu-
tual information. A small sample in the original for-
mat is
(N:by:V<buy>V:obj:N (sims
N:to:V<sell>V:obj:N 0.211704
N:subj:V<buy>V:obj:N 0.198728
...
))
The first two lines represent the inference rule: X
bought by Y entails X sold to Y.
We preprocess the DIRT data by restoring prepo-
sitions, which were originally folded into a depen-
dency relation, to individual nodes, as this eases
alignment with the parsed RTE data. For the same
reason, paths are converted to the same ad-hoc XML
format as the parsed RTE data.
2.3 Paraphrase substitution
Conceptually, our paraphrase substitution algorithm
takes a straightforward approach. For the purpose of
explanation only, Figure 1 presents pseudo-code for
a naive implementation. The main function takes
two arguments (cf. line 1). The first is a prepro-
cessed RTE data set in which all sentences from text
and hypothesis are dependency parsed. The second
is a collection of DIRT paraphrases, each one map-
ping a source path to one or more translation paths.
For each text/hypothesis pair (cf. line 2), we look
at all the subtrees of the text parses (cf. line 3-4)
and attempt to find a suitable paraphrase of this sub-
tree (cf. line 5). We search the DIRT paraphrases
(cf. line 8) for a source path that matches the text
subtree at hand (cf. line 9). If found, we check
if any of the corresponding paraphrase translation
paths (cf. line 10) matches a subtree of the hypoth-
esis parse (cf. line 11-12). If so, we modify the
text tree by substituting this translation path (cf. line
13). The intuition behind this is that we only accept
paraphrases that bring the text closer to the hypothe-
sis. The DIRT paraphrases are ordered in decreasing
likelihood, so after a successful paraphrase substitu-
tion, we discard the remaining possibilities and con-
tinue with the next text subtree (cf. line 14).
The Match function, which is used for matching
the source path to a text subtree and the translation
path to an hypothesis subtree, requires the path to
occur in the subtree. That is, all lemmas, part-of-
speech tags and dependency relations from the path
must have identical counterparts in the subtree; skip-
ping nodes is not allowed. As the path?s terminals
specify no lemma, the only requirement is that their
counterparts are nouns.
The Substitute function replaces the matched path
in the text tree by the paraphrase?s translation path.
Intuitively, the path ?overlays? a part of the sub-
tree, changing lemmas and dependency relations,
but leaving most of the daughter nodes unaffected.
Note that the new path may be longer or shorter than
the original one, thus introducing or removing nodes
from the text tree.
As an example, we will trace our algorithm as ap-
plied to the first pair of the RTE3 dev set (id=1).
Text: The sale was made to pay Yukos? US$ 27.5 billion tax
bill, Yuganskneftegaz was originally sold for US$ 9.4 bil-
lion to a little known company Baikalfinansgroup which
was later bought by the Russian state-owned oil company
Rosneft.
Hypothesis: Baikalfinansgroup was sold to Rosneft.
Entailment: Yes
While traversing the parse tree of the text, our
algorithm encounters a node with POS tag V and
lemma buy. The relevant part of the parse tree is
shown at the right top of Figure 2. The logical argu-
ments inferred by Minipar are shown between curly
84
(1) def Paraphrase(parsed-rte-data, dirt-paraphrases):
(2) for pair in parsed-rte-data:
(3) for text-tree in pair.text-parses:
(4) for text-subtree in text-tree:
(5) Paraphrase-subtree(text-subtree, dirt-paraphrases, pair.hyp-parse)
(6)
(7) def Paraphrase-subtree(text-subtree, dirt-paraphrases, hyp-tree):
(8) for (source-path, translations) in dirt-paraphrases:
(9) if Match(source-path, text-subtree):
(10) for trans-path in translations:
(11) for hyp-subtree in hyp-tree:
(12) if Match(trans-path, hyp-subtree):
(13) text-subtree = Substitute(trans-path, text-subtree)
(14) return
Figure 1: Pseudo-code for a naive implementation of the dependency-based paraphrase substitution algo-
rithm
brackets, e.g., US$ 9.4 billion. For this combination
of verb and lemma, the DIRT data contains 340 para-
phrase sets, with a total of 26950 paraphrases. The
algorithm starts searching for a paraphrase source
which matches the text. It finds the path shown
at the left top of Figure 2: buy with a PP modi-
fier headed by preposition by, and a nominal object.
This paraphrase source has 108 alternative transla-
tions. It searches for paraphrase translations which
match the hypothesis. The first, and therefore most
likely (probability is 0.22) path it finds is rooted in
sell, with a PP-modifier headed by to and a nominal
object. This translation path, as well as its alignment
to the hypothesis parse tree, is shown in the mid-
dle part of Figure 2. Finally, the source path in the
text tree is substituted by the translation path. The
bottom part of Figure 2 shows the updated text tree
as well as its improved alignment to the hypothesis
tree. The paraphrasing procedure can in effect be
viewed as making the inference that Baikalfinans-
group was bought by Rosneft, therefore Baikalfi-
nansgroup was sold to Rosneft.
The naive implementation of the algorithm is of
course not very efficient. Our actual implementa-
tion uses a number of shortcuts to reduce process-
ing time. For instance, the DIRT paraphrases are
indexed on the lemma of their root in order to speed
up retrieval. As another example, text nodes with
less than two child nodes (i.e. terminal and unary-
branching nodes) are immediately skipped, as they
will never match a paraphrase path.
2.4 Paraphrasing results
We applied our paraphrasing algorithm to the RTE3
development set. Table 1 gives an impression of how
many paraphrases were substituted. The first row
lists the total number of nodes in the dependency
trees of the text parts. The second row shows that
for roughly 15% of these nodes, the DIRT data con-
tains a paraphrase with the same lemma. The next
two rows show in how many cases the source path
matches the text and the translation path matches the
hypothesis (i.e. giving rise to a paraphrase substitu-
tion). Clearly, the number of actual paraphrase sub-
stitutions is relatively small: on average about 0.5%
of all text subtrees are subject to paraphrasing. Still,
about one in six sentences is subject to paraphras-
ing, and close to half of all pairs is paraphrased at
least once. Sentences triggering more than one para-
phrase do occur. Also note that paraphrasing occurs
more frequently in true entailment pairs than in false
entailment pairs. This is to be expected, given that
text and hypothesis are more similar when an entail-
ment relation holds.
2.5 Discussion on paraphrasing
Type of paraphrases A substantial number of the
paraphrases applied are single word synonyms or
verb plus particle combinations which might as well
be obtained from string-based substitution on the ba-
sis of a lexical resource like WordNet. Some ran-
domly chosen examples include X announces Y en-
tails X supports Y, X makes Y entails X sells Y, and
locates X at Y, discovers X at Y. Nevertheless, more
interesting paraphrases do occur. In the pair below
(id=452), we find the paraphrase X wins Y entails X
85
Table 1: Frequency of (partial) paraphrase matches on the RTE3 dev set
IE: IR: QA: SUM: Total:
Text nodes: 8899 10610 10502 8196 38207
Matching paraphrase lemma: 1439 1724 1581 1429 6173
Matching paraphrase source: 566 584 543 518 2211
Matching paraphrase translation: 71 55 23 79 228
Text sentences: 272 350 306 229 1157
Paraphrased text sentences: 63 51 20 66 200
Paraphrased true-entailment pairs: 32 25 12 39 108
Paraphrased false-entailment pairs: 26 21 5 23 75
(is) Y champion.
Text: Boris Becker is a true legend in the sport of tennis. Aged
just seventeen, he won Wimbledon for the first time and
went on to become the most prolific tennis player.
Hypothesis: Boris Becker is a Wimbledon champion.
Entailment: True
Another intriguing paraphrase, which appears to be
false on first sight, is X flies from Y entails X makes
(a) flight to Y. However, in the context of the next
pair (id=777), it turns out to be correct.
Text: The Hercules transporter plane which flew straight here
from the first round of the trip in Pakistan, touched down
and it was just a brisk 100m stroll to the handshakes.
Hypothesis: The Hercules transporter plane made a flight to
Pakistan.
Entailment: True
Coverage Although the DIRT data constitutes a
relatively large collection of paraphrases, it is clear
that many paraphrases required for the RTE3 data
are missing. We tried to improve coverage to some
extent by relaxing the Match function: instead of
an exact match, we allowed for small mismatches
in POS tag and dependency relation, reversing the
order of a path?s left and right side, and even for
skipping nodes. However, subjective evaluation sug-
gested that the results deteriorated. Alternatively,
the coverage might be increased by deducing para-
phrases on the fly using the web as a corpus, e.g.,
(Hickl et al, 2006).
Somewhat surprisingly, the vast majority of para-
phrases concerns verbs. Even though the DIRT data
contains paraphrases for nouns, adjectives and com-
plementizers, the coverage of these word classes is
apparently not nearly as extensive as that of verbs.
Another observation is that fewer paraphrases oc-
cur in pairs from the QA task. We have no explana-
tion for this.
False paraphrases Since the DIRT data was au-
tomatically derived and was not manually checked,
it contains noise in the form of questionable or even
false paraphrases. While some of these surface in
paraphrased RTE3 data (e.g. X leaves for Y entails
X departs Y, and X feeds Y entails Y feeds X), their
number appears to be limited. We conjecture this is
because of the double constraint that a paraphrase
must match both text and hypothesis.
Relevance Not all paraphrase substitutions are rel-
evant for the purpose of recognizing textual entail-
ment. Evidently, paraphrases in false entailment
pairs are counterproductive. However, even in true
entailment pairs paraphrases might occur in parts
of the text that are irrelevant to the task at hand.
Consider the following pair from the RTE3 dev set
(id=417).
Text: When comparing Michele Granger and Brian Goodell,
Brian has to be the clear winner. In 1976, while still a
student at Mission Viejo High, Brian won two Olympic
gold medals at Montreal, breaking his own world records
in both the 400 - and 1,500 - meter freestyle events. He
went on to win three gold medals in he 1979 Pan Ameri-
can Games.
Hypothesis: Brian Goodell won three gold medals in the 1979
Pan American Games.
Entailment: True
The second text sentence and hypothesis match
the paraphrases: (1) X medal at Y entails X medal in
Y, and (2) X record in Y entails X medal in Y. Even
so, virtually all of the important information is in the
third text sentence.
3 Results on RTE3 data
Since our contribution focuses on syntactic para-
phrasing, our RTE3 system is a simplified version
86
Table 2: Percent accuracy on RTE3 set without
paraphrasing (?) and with paraphrasing (+)
Task Dev? Dev+ Test? Test+
IE 59.5 61.0 53.0 53.5
IR 67.0 68.0 58.5 61.5
QA 76.0 76.5 69.0 68.0
SUM 66.0 67.5 53.0 53.5
Overall 66.9 68.2 58.6 59.1
of our RTE2 system as described in (ref supressed
for blind reviewing) The core of the system is still
the tree alignment algorithm from (Meyers et al,
1996), but without normalization of node weights
and applied to Minipar instead of Maltparser out-
put. To keep things simple, we do not apply syntac-
tic normalization, nor do we use WordNet or other
resources to improve node matching. Instead, we
simply align each text tree to the corresponding hy-
pothesis tree and calculate the coverage, which is
defined as the proportion of aligned content words
in the hypothesis. If the coverage is above a task-
specific threshold, we say entailment is true, other-
wise it is false.
The results are summarized in Table 2. Overall
results on the test set are considerably worse than
on the development set, which is most likely due to
overfitting task-specific parameters for node match-
ing and coverage. Our main interest is to what extent
dependency-based paraphrasing improves our base-
line prediction. The improvement on the develop-
ment set is more than 1%. This is reduced to 0.5%
in the case of the test set.
Our preliminary results indicate a small positive
effect of dependency-based paraphrasing on the re-
sults of our RTE system. Unlike most earlier work,
we did not add resources other than Minipar depen-
dency trees and DIRT paraphrase trees, in order to
isolate the contribution of syntactic paraphrases to
RTE. Nevertheless, our RTE3 system may be im-
proved by using WordNet or other lexical resources
to improve node matching, both in the paraphrasing
step and in the tree-alignment step. In future work,
we hope to improve both the paraphrasing method
(along the lines discussed in Section 2.5) and the
RTE system itself.
Acknowledgments We would like to thank Dekang Lin and
Patrick Pantel for allowing us to use the DIRT data. This work
was jointly conducted within the DAESO project funded by the
Stevin program (De Nederlandse Taalunie) and the IMOGEN
project funded by the Netherlands Organization for Scientific
Research (NWO).
References
R. Bar-Haim, I. Dagan, B. Dolan, L. Ferro, D. Giampic-
colo, B. Magnini, and I. Szpektor. 2006. The second
pascal recognising textual entailment challenge. In
Proceedings of the Second PASCAL Challenges Work-
shop on Recognising Textual Entailment, pages 1?9,
Venice, Italy.
W. Bosma and C. Callison-Burch. 2006. Paraphrase sub-
stitution for recognizing textual entailment. In Pro-
ceedings of CLEF.
R. de Salvo Braz, R. Girju, V. Punyakanok, D. Roth, and
M. Sammons. 2005. An inference model for seman-
tic entailemnt in natural language. In Proceedings of
the First Pascal Challenge Workshop on Recognizing
Textual Entailment, pages 29?32.
A. Hickl, J. Williams, J. Bensley, K. Roberts, B. Rink,
and Y. Shi. 2006. Recognizing textual entailment
with lccs groundhog system. In Proceedings of the
Second PASCAL Challenges Workshop on Recognis-
ing Textual Entailment, pages 80?85, Venice, Italy.
Dekang Lin and Patrick Pantel. 2001. Discovery of infer-
ence rules for question answering. Natural Language
Engineering, 7(4):343?360.
Dekang Lin. 1998. Dependency-based evaluation of
minipar. In Proceedings of the Workshop on Evalua-
tion of Parsing Systems at LREC 1998, pages 317?330,
Granada, Spain.
E. Marsi, E. Krahmer, W. Bosma, and M. Theune. 2006.
Normalized alignment of dependency trees for detect-
ing textual entailment. In Proceedings of the Second
PASCAL Challenges Workshop on Recognising Tex-
tual Entailment, pages 56?61, venice, Italy.
Adam Meyers, Roman Yangarber, and Ralph Grisham.
1996. Alignment of shared forests for bilingual cor-
pora. In Proceedings of 16th International Conference
on Computational Linguistics (COLING-96), pages
460?465, Copenhagen, Denmark.
R. Nielsen, W. Ward, and J.H. Martin. 2006. Toward
dependency path based entailment. In Proceedings of
the Second PASCAL Challenges Workshop on Recog-
nising Textual Entailment, pages 44?49, Venice, Italy.
R. Raina, A. Haghighi, C. Cox, J. Finkel, J. Michels,
K. Toutanova, B. MacCartney, M.C. deMarneffe, C.D.
Manning, and A.Y. Ng. 2005. Robust textual infer-
ence using diverse knowledge sources. In Proceedings
of PASCAL Recognising Textual Entailment Workshop.
J. C. Reynar and A. Ratnaparkhi. 1997. A maximum en-
tropy approach to identifying sentence boundaries. In
Proceedings of the Fifth Conference on Applied Natu-
ral Language Processing, Washington, D.C.
87
buy
by
mod
...
obj
buy
...
pcomp-n
by
Rosneft
{US$ 9.4 billion}Baikalfinansgroup
s mod obj
{Baikalfinansgroup}
subj
known
mod
company
nn
{fin}
rel
which
whn
be
i
later
pred
{which}
subj
pcomp-n
the
det
Russian
mod
state-owned
mod
oil company
nn
state
lex-mod
-
lex-mod
oil
lex-mod
sell
to
mod
...
obj
sell
...
pcomp-n
to
Rosneft
{Baikalfinansgroup}Baikalfinansgroup
s
be
be mod obj
pcomp-n
sell
Baikalfinansgroup
s
to
mod
{US$ 9.4 billion}
obj
{Baikalfinansgroup}
subj
known
mod
company
nn
{fin}
rel
which
whn
be
i
later
pred
{which}
subj
Rosneft
pcomp-n
the
det
Russian
mod
state-owned
mod
oil company
nn
state
lex-mod
-
lex-mod
oil
lex-mod
sell
Baikalfinansgroup
s
be
be
to
mod
{Baikalfinansgroup}
obj
Rosneft
pcomp-n
Figure 2: Alignment of paraphrase source to text (top), alignment of paraphrase translation to hypothesis
(mid), and alignment of hypothesis to paraphrased text (bottom) for pair 1 from RTE3 dev set
88
Proceedings of the 12th European Workshop on Natural Language Generation, pages 25?32,
Athens, Greece, 30 ? 31 March 2009. c?2009 Association for Computational Linguistics
Is sentence compression an NLG task?
Erwin Marsi, Emiel Krahmer
Tilburg University
Tilburg, The Netherlands
e.j.krahmer@uvt.nl
e.c.marsi@uvt.nl
Iris Hendrickx, Walter Daelemans
Antwerp University
Antwerpen, Belgium
iris.hendrickx@ua.ac.be
walter.daelemans@ua.ac.be
Abstract
Data-driven approaches to sentence com-
pression define the task as dropping any
subset of words from the input sentence
while retaining important information and
grammaticality. We show that only 16%
of the observed compressed sentences in
the domain of subtitling can be accounted
for in this way. We argue that part of this
is due to evaluation issues and estimate
that a deletion model is in fact compat-
ible with approximately 55% of the ob-
served data. We analyse the remaining
problems and conclude that in those cases
word order changes and paraphrasing are
crucial, and argue for more elaborate sen-
tence compression models which build on
NLG work.
1 Introduction
The task of sentence compression (or sentence re-
duction) can be defined as summarizing a single
sentence by removing information from it (Jing
and McKeown, 2000). The compressed sentence
should retain the most important information and
remain grammatical. One of the applications
is in automatic summarization in order to com-
press sentences extracted for the summary (Lin,
2003; Jing and McKeown, 2000). Other appli-
cations include automatic subtitling (Vandeghin-
ste and Tsjong Kim Sang, 2004; Vandeghinste and
Pan, 2004; Daelemans et al, 2004) and displaying
text on devices with very small screens (Corston-
Oliver, 2001).
A more restricted version defines sentence
compression as dropping any subset of words
from the input sentence while retaining impor-
tant information and grammaticality (Knight and
Marcu, 2002). This formulation of the task pro-
vided the basis for the noisy-channel en decision-
tree based algorithms presented in (Knight and
Marcu, 2002), and for virtually all follow-up work
on data-driven sentence compression (Le and
Horiguchi, 2003; Vandeghinste and Pan, 2004;
Turner and Charniak, 2005; Clarke and Lapata,
2006; Zajic et al, 2007; Clarke and Lapata, 2008)
It makes two important assumptions: (1) only
word deletions are allowed ? no substitutions or
insertions ? and therefore no paraphrases; (2) the
word order is fixed. In other words, the com-
pressed sentence must be a subsequence of the
source sentence. We will call this the subsequence
constraint, and refer to the corresponding com-
pression models as word deletion models. Another
implicit assumption in most work is that the scope
of sentence compression is limited to isolated sen-
tences and that the textual context is irrelevant.
Under this definition, sentence compression is
reduced to a word deletion task. Although one
may argue that even this counts as a form of
text-to-text generation, and consequently an NLG
task, the generation component is virtually non-
existent. One can thus seriously doubt whether it
really is an NLG task.
Things would become more interesting from an
NLG perspective if we could show that sentence
compression necessarily involves transformations
beyond mere deletion of words, and that this re-
quires linguistic knowledge and resources typical
to NLG. The aim of this paper is therefore to chal-
lenge the deletion model and the underlying subse-
quence constraint. To use an analogy, our aim is to
show that sentence compression is less like carv-
ing something out of wood - where material can
only be removed - and more like molding some-
thing out of clay - where the material can be thor-
25
oughly reshaped. In support of this claim we pro-
vide evidence that the coverage of deletion models
is in fact rather limited and that word reordering
and paraphrasing play an important role.
The remainder of this paper is structured as fol-
lows. In Section 2, we introduce our text material
which comes from the domain of subtitling. We
explain why not all material is equally well suited
for studying sentence compression and motivate
why we disregard certain parts of the data. We
also describe the manual alignment procedure and
the derivation of edit operations from it. In Sec-
tion 3, an analysis of the number of deletions, in-
sertions, substitutions, and reorderings in our data
is presented. We determine how many of the com-
pressed sentences actually satisfy the subsequence
constraint, and how many of them could in prin-
ciple be accounted for. That is, we consider al-
ternatives with the same compression ratio which
do not violate the subsequence constraint. Next
is an analysis of the remaining problematic cases
in which violation of the subsequence constraint
is crucial to accomplish the observed compression
ratio. We single out (1) reordering after deletion
and (2) paraphrasing as important factors. Given
the importance of paraphrases, Section 3.4 dis-
cusses the perspectives for automatic extraction of
paraphrase pairs from large text corpora, and tries
to estimate how much text is required to obtain a
reasonable coverage. We finish with a summary
and discussion in Section 4.
2 Material
We study sentence compression in the context of
subtitling. The basic problem of subtitling is that
on average reading takes more time than listen-
ing, so subtitles can not be a verbatim transcrip-
tion of the speech without increasingly lagging be-
hind. Subtitles can be presented at a rate of 690
to 780 characters per minute, while the average
speech rate is considerably higher (Vandeghinste
and Tsjong Kim Sang, 2004). Subtitles are there-
fore often a compressed representation of the orig-
inal spoken text.
Our text material stems from the NOS Journaal,
the daily news broadcast of the Dutch public tele-
vision. It is parallel text with on one side the au-
tocue sentences (aut), i.e. the text the news reader
is reading, and on the other side the corresponding
subtitle sentences (sub). It was originally collected
and processed in two earlier research projects ?
Atranos and Musa ? on automatic subtitling (Van-
deghinste and Tsjong Kim Sang, 2004; Vandegh-
inste and Pan, 2004; Daelemans et al, 2004). All
text was automatically tokenized and aligned at
the sentence level, after which alignments were
manually checked.
The same material was further annotated in an
ongoing project called DAESO1, in which the gen-
eral goal is automatic detection of semantic over-
lap. All aligned sentences were first syntactically
parsed after which their parse trees were manually
aligned in more detail. Pairs of similar syntactic
nodes ? either words or phrases ? were aligned and
labeled according to a set of five semantic similar-
ity relations (Marsi and Krahmer, 2007). For cur-
rent purposes, only the alignment at the word level
is used, ignoring phrasal alignments and relation
labels.
Not all material in this corpus is equally well
suited for studying sentence compression as de-
fined in the introduction. As we will discuss in
more detail below, this prompted us to disregard
certain parts of the data.
Sentence deletion, splitting and merging For a
start, autocue and subtitle sentences are often not
in a one-to-one alignment relation. Table 1 speci-
fies the alignment degree (i.e. the number of other
sentences that a sentence is aligned to) for autocue
and subtitle sentences. The first thing to notice
is that there is a large number of unaligned sub-
titles. These correspond to non-anchor text from,
e.g., interviews or reporters abroad. More inter-
esting is that about one in five autocue sentences
is completely dropped. A small number of about
4 to 8 percent of the sentence pairs are not one-
to-one aligned. A long autocue sentence may be
split into several simpler subtitle sentences, each
containing only a part of the semantic content of
the autocue sentence. Conversely, one or more -
usually short - autocue sentences may be merged
into a single subtitle sentence.
These decisions of sentence deletion, splitting
and merging are worthy research topics in the con-
text of automatic subtitling, but they should not
be confused with sentence compression, the scope
of which is by definition limited to single sen-
tence. Accordingly we disregarded all sentence
pairs where autocue and subtitle are not in a one-
to-one relation with each other. This reduced the
data set from 15289 to 11034 sentence pairs.
1http://daeso.uvt.nl
26
Degree: Autocue: (%) Subtitle: (%)
0 3607 (20.74) 12542 (46.75)
1 12382 (71.19) 13340 (49.72)
2 1313 (7.55) 901 (3.36)
3 83 (0.48) 41 (0.15)
4 8 (0.05) 6 (0.02)
Table 1: Degree of sentence alignment
Word compression A significant part of the re-
duction in subtitle characters is actually not ob-
tained by deleting words but by lexical substitution
of a shorter token. Examples of this include sub-
stitution by digits (?7? for ?seven?), abbreviations
or acronyms (?US? for ?United States?), symbols
(euro symbol for ?Euro?), or reductions of com-
pound words (?elections? for ?state-elections?).
We will call this word compression. Although an
important part of subtitling, we prefer to abstract
from word compression and focus here on sen-
tence compression proper. Removing all sentence
pairs containing a word compression has the dis-
advantage of further reducing the data set. Instead
we choose to measure compression ratio (CR) in
terms of tokens2 rather than characters.
CR =
#toksub
#tokaut
(1)
This means that the majority of the word com-
pressions do not affect the sentence CR.
Variability in compression ratio The CR of
subtitles is not constant, but varies depending
(mainly) on the amount of provided autocue ma-
terial in a given time frame. The histogram in
Figure 1 shows the distribution of the CR (mea-
sured in words) for one-to-one aligned sentences.
In fact, autocue sentences are most likely not to
be compressed at all (thus belonging to the largest
bin, from 1.00 to 1.09 in the histogram).3 In order
to obtain a proper set of compression examples,
we retained only those sentence pairs where the
compression ratio is less than one.
Parsing failures As mentioned earlier detailed
alignment of autocue and subtitle sentences was
carried out on their syntactic trees. However,
for various reasons a small number of sentences
(0.2%) failed to pass the parser and received no
parse tree. As a consequence, their trees could not
2Throughout this study we ignore punctuation and letter
case.
3Some instances even show a CR larger than one, because
occasionally there is sufficient time/space to provide a clari-
fication, disambiguation, update, or stylistic enhancement.
Figure 1: Histogram of compression ratio
Min: Max: Sum: Mean: SD:
aut-tokens 2 43 80651 15.41 5.48
sub-tokens 1 29 53691 10.26 3.72
CR 0.07 0.96 nan 0.69 0.17
Table 2: Properties of the final data set of
5233 pairs of autocue-subtitle sentences: mini-
mum value, maximal value, total sum, mean and
standard deviation for number of tokens per au-
tocue/subtitle sentence and Compression Ratio
be aligned and there is no alignment at the word
level available either. Variability in CR and pars-
ing failures are together responsible for a further
reduction down to 5233 sentence pairs, the final
size of our data set, with an overall CR of 0.69.
Other properties of this data set are summarized in
Table 2.4
Word deletions, insertions and substitutions
Having a manual alignment of similar words in
both sentences allows us to simply deduce word
deletions, substitutions and insertions, as well as
word order changes, in the following way:
? if an autocue word is not aligned to a subtitle
word, then it is was deleted
? if a subtitle word is not aligned to an autocue
word, then it was inserted
? if different autocue and subtitle words are
aligned, then the former was substituted by
the latter
? if alignments cross each other, then the word
order was changed
The remaining option is where the aligned
words are identical (ignoring differences in case).
4We use the acronym nan (?not a number?) for unde-
fined/meaningless values.
27
Without the word alignment, we would have
to resort to automatically calculating the edit dis-
tance, i.e. the sum of the minimal number of
insertions, deletions and substitutions required to
transform one sentence in to the other. However,
this would result in different and often counter-
intuitive sequences of edit operations. Our ap-
proach clearly distinguishes word order changes
from the edit operations; the conventional edit dis-
tance, by contrast, can only account for changes
in word order by sequences of the edit operations.
Another difference is that substitution can also be
accomplished as deletion followed by insertion,
which means edit operations need to have an as-
sociated weight. Global tuning of these weights
turns out to be hard.
3 Analysis
3.1 Edit operations
The observed deletions, insertions, substitutions,
edit distances, and word order changes are shown
in Table 3. As expected, deletion is the most fre-
quent operation, with on average seven deletions
per sentence. Insertion and substitutions are far
less frequent. Note also that ? even though the task
is compression ? insertions are somewhat more
frequent than substitutions. Word order changes
occur in 1688 cases (32.26%). Here, reordering is
a binary variable ? i.e. the word order is changed
or not ? hence Min, Max and SD are undefined.
Another point of view is to look at the number
of sentence pairs containing a certain edit oper-
ation. Here we find 5233 pairs (100.00%) with
deletion, 2738 (52.32%) with substitution, 3263
(62.35%) with insertion, and 1688 (32.26%) with
reordering.
The average CR for subsequences is 0.68
(SD = 0.20) versus 0.69 (SD = 0.17)
for non-subsequences. A detailed inspection of
the relation between the subsequence/non ?
subsequence ratio and CR revealed no clear cor-
relation, so we did not find indications that non-
subsequences occur more frequently at higher
compression ratios.
3.2 Percentage of subsequences
The subtitle is a subsequence of the autocue if
there are no insertions, no substitutions, and no
word order changes. In contrast, if any of these
do occur, the subtitle is not a subsequence. It turns
Min: Max: Sum: Mean: SD:
del 1 34 34728 6.64 4.57
sub 0 6 4116 0.79 0.94
ins 0 17 7768 1.48 1.78
dist 1 46 46612 8.91 5.78
reorder nan nan 1688 0.32 nan
Table 3: Observed word deletions, insertions, sub-
stitutions, and edit distances
out that only 843 (16.11%) subtitles are a subse-
quence, which is rather low.
At first sight, this appears to be bad news for
any deletion model, as it seems to imply that the
model cannot account for close to 84% the ob-
served data. However, the important thing to keep
in mind is that compression of a given sentence
is a problem for which there are usually multiple
solutions (Belz and Reiter, 2006). This is exactly
what makes it so hard to perform automatic evalu-
ation of NLG systems. There may very well exist
semantically equivalent alternatives with the same
CR which do satisfy the subsequence constraint.
For this reason, a substantial part of the observed
non-subsequences may have subsequence counter-
parts which can be accounted for by a deletion
model. The question is: how many?
In order to address this question, we took a
random sample of 200 non-subsequence sentence
pairs. In each case we tried to come up with
an alternative subsequence subtitle with the same
meaning and the same CR (or when opportune,
even a lower CR). Table 4 shows the distribu-
tion of the difference in tokens between the orig-
inal non-subsequence subtitle and the manually-
constructed equivalent subsequence subtitle. Ap-
parently 95 out of 200 (47%) subsequence sub-
titles have the same (or even fewer) tokens, and
thus the same (or an even lower) compression ra-
tio. This suggests that the subsequence constraint
is not as problematic as it seemed and that the cov-
erage of a deletion model is in fact far better than
it appeared to be. Recall that 16% of the original
subtitles were already subsequences, so our anal-
ysis suggests that a deletion model is compatible
with 55% (16% plus 47% of 84%).
3.3 Problematic non-subsequences
Another result of this exercise in rewriting sub-
titles is that it allows us to identify those cases
where the attempt to create a proper subse-
quence fails. In (1), we show one representa-
tive example of a problematic subtitle, for which
28
(1) Aut de
the
bron
source
was
was
een
a
geriatrische
geriatric
patient
patient
die
who
zonder
without
het
it
zelf
self
te
to
merken
notice
uitzonderlijk
exceptionally
veel
many
larven
larvae
bij
with
zich
him
bleek
appeared
te
to
dragen
carry
en
and
een
a
grote
large
verspreiding
spreading
veroorzaakte
caused
?the source was a geriatric patient who unknowingly carried exceptionally many larvae and caused a wide spreading?
Sub een
a
geriatrische
geriatric
patient
patient
met
with
larven
larvae
heeft
has
de
the
verspreiding
spreading
veroorzaakt
caused
Seq de bron was een geriatrische patient die veel larven bij zich bleek te dragen en een verspreiding veroorzaakte
(2) Aut in
in
verband
relation
met
to
de
the
lawineramp
avalanche-disaster
in
in
galu?r
Galtu?r
hebben
have
de
the
politieke
political
partijen
parties
in
in
tirol
Tirol
gezamenlijk
together
besloten
decided
de
the
verkiezingscampagne
election-campaign
voor
for
het
the
regionale
regional
parlement
parliament
op
up
te
to
schorten
postpone
Sub de
the
politieke
political
partijen
parties
in
in
tirol
Tirol
hebben
have
besloten
decided
de
the
verkiezingen
elections
op
up
te
to
schorten
postpone
?Political parties in Tirol have decided to postpone the elections?
(3) Aut velen
many
van
of
hen
them
worden
are
door
by
de
the
servie?rs
Serbs
in
in
volgeladen
crammed
treinen
trains
gedeporteerd
deported
Sub vluchtelingen
refugees
worden
are
per
by
trein
train
gedeporteerd
deported
token-diff: count: (%:)
-2 4 2.00
-1 18 9.00
0 73 36.50
1 42 21.00
2 32 16.00
3 11 5.50
4 9 4.50
5 5 2.50
7 2 1.00
8 2 1.00
9 1 0.50
11 1 0.50
Table 4: Distribution of difference in tokens
between original non-subsequence subtitle and
equivalent subsequence subtitle
the best equivalent subsequence we could ob-
tain still has nine more tokens than the origi-
nal non-subsequence. These problematic non-
subsequences reveal where insertion, substitution
and/or word reordering are essential to obtain a
subtitle with a sufficient CR (i.e. the CR observed
in the real subtitles). At least three different types
of phenomena were observed.
Word order In some cases deletion of a con-
stituent necessitates a change in word order to ob-
tain a grammatical sentence. In example (2), the
autocue sentence has the PP modifier in verband
met de lawineramp in galu?r in its topic position
(first sentence position). Deleting this modifier, as
is done in the subtitle, results in a sentence that
starts with the verb hebben, which is interpreted as
a yes-no question. For a declarative interpretation,
we have to move the subject de politieke partijen
to the first position, as in the subtitle. Incidentally,
this indicates that it is instructive to apply sentence
compression models to multiple languages, as a
word order problem like this never arises in En-
glish.
Similar problems arise whenever an embedded
clause is promoted to a main clause, which re-
quires a change in the position of the finite verb
in Dutch. In total, a word order problem occurred
in 24 out 200 sentences.
Referring expressions Referring expressions
are on many occasions replaced by a shorter
one ? usually a little less precise. For
example, de belgische overheid ?the Belgian
authorities? is replaced by belgie ?Belgium?.
Extreme cases of this occur where a long
NP like deze tweede impeachment-procedure
in de amerikaanse geschiedenis ?this second
impeachment-procedure in the American history?
is replaced by an anaphor like het ?it?.
Since a referring expression or anaphor must be
appropriate in the given context, substitutions like
these transcend the domain of a single sentence
and require taking the preceding textual context
into account. This is especially clear in exam-
ples like (3) in which ?many of them? is replaced
the ?refugees?. It is questionable whether these
types of substitutions belong to the task of sen-
tence compression. We prefer to regard it as one of
the additional tasks in automatic subtitling, apart
from compression. Incidentally, it is interesting
that the challenge of generating referring expres-
sions is also relevant for automatic subtitling.
29
Paraphrasing Apart from the reduced referring
expressions, there are nominal paraphrases reduc-
ing a noun phrases like medewerkers van banken
?employees of banks? to a compound word like
bankmedewerkers ?bank-employees?. Likewise,
there are adverbial paraphrases such as sinds een
paar jaar ?since a few years? to tegenwoordig
?nowadays?, and van de afgelopen tijd ?of the past
time? to recent ?recent?. However, the majority of
the paraphrasing concerns verbs as in the two ex-
amples below.
(4) Aut X
X
neemt
takes
het
the
initiatief
initiative
tot
to
oprichting
raising
van
of
Y
Y
Sub X
X
zet
sets
Y
Y
op
up
(5) Aut X
X
om
for
zijn
his
uitlevering
extradition
vroeg
asked
maar
but
Y
Y
die
that
weigerde
refused
Sub Y
Y
hem
him
niet
not
wilde
wanted
uitleveren
extradite
aan
to
X
Y
?Y refused to extradite him to Y?
Even though not all paraphrases are actually
shorter, it seems that at least some of them boost
compression beyond what can be accomplished
with only word deletion. In the next Section, we
look at the possibilities of automatic extraction of
such paraphrases.
3.4 Perspectives for automatic paraphrase
extraction
There is a growing amount of work on automatic
extraction of paraphrases from text corpora (Lin
and Pantel, 2001; Barzilay and Lee, 2003; Ibrahim
et al, 2003; Dolan et al, 2004). One general pre-
requisite for learning a particular paraphrase pat-
tern is that it must occur in the text corpus with a
sufficiently high frequency, otherwise the chances
of learning the pattern are proportionally small. In
this section, we investigate to what extent the para-
phrases encountered in our random sample of 200
pairs can be retrieved from a reasonably large text
corpus.
In a first step, we manually extracted 106
paraphrase patterns. We filtered these pat-
terns and excluded anaphoric expressions, general
verb alternation patterns like active/passive and
continuous/non-continuous, as well as verbal pat-
terns involving more than two slots. After this fil-
tering step, 59 pairs of paraphrases remained, in-
cluding the examples shown in the preceding Sec-
tion.
The aim was to estimate how big our corpus
has to be to cover the majority of these para-
phrase pairs. We started with counting for each
of the paraphrase pairs in our sample how often
they occur in a corpus of Dutch news texts, the
Twente News Corpus5, which contains approxi-
mately 325M tokens and 20M sentences. We em-
ployed regular expressions to count the number of
paraphrase pattern matches. The corpus turned out
to contain 70% percent of all paraphrase pairs (i.e.
both patterns in the pair occur at least once). We
also counted how many pairs have a frequencies of
at least 10 and 100. To study the effect of corpus
size on the percentage of covered paraphrases, we
performed these counts on 1, 2, 5, 10, 25, 50 and
100% of the corpus. Figure 2 shows the percent-
age of covered paraphrases dependent on the cor-
pus size. The most strict threshold that only counts
pairs that occur at least 100 times in our corpus,
does not retrieve any counts on 1% of the corpus
(3M words). At 10% of the corpus size only 4%
of the paraphases is found, and on the full data set
25% of the pairs is found.
For 51% percent of the patterns (with a fre-
quency of at least 10) we find substantial evidence
in our corpus of 325M tokens. We fitted a curve
through our data points, and found a logarithmic
line fit with adjusted R2 value of .943. This sug-
gests that in order to get 75% of the patterns, we
would need a corpus that is 18 times bigger than
our current one, which amounts to roughly 6 bil-
lion words. Although this seems like a lot of text,
using the WWW as our corpus would easily give
us these numbers. Today?s estimate of the Index
Dutch World Wide Web is 688 million pages6. If
we assume that each page contains at least 100 to-
kens on average, this implies a corpus size of 68
billion tokens.
The patterns used here are word-based and in
many cases they express a particular verb tense or
verb form (e.g. 3rd person singular), and word
order. This implies that our estimations are the
minimum number of matches one can find. For
more abstract matching, we would need syntacti-
cally parsed data (Lin and Pantel, 2001). We ex-
pect that this would also positively affect the cov-
erage.
5http://www.vf.utwente.nl/?druid/TwNC/
TwNC-main.html
6http://www.worldwidewebsize.com/
index.php?lang=NL, as measured in December
2008
30
Figure 2: Percentage of covered paraphrases as a
function of the corpus size
4 Discussion
We found that only 16.11% of 5233 subtitle sen-
tences were proper subsequences of the corre-
sponding autocue sentence, and therefore 84% can
not be accounted for by a deletion model. One
consequence appears to be that the subsequence
constraint greatly reduces the amount of avail-
able training material for any word deletion model.
However, an attempt to rewrite non-subsequences
to semantically equivalent sequences with the
same CR suggests that a deletion model could in
principle be adequate for 55% of the data. More-
over, in those cases where an application can toler-
ate a little slack in the CR, a deletion model might
be sufficient. For instance, if we are willing to tol-
erate up to two more tokens, we can account for as
much as 169 (84%) of the 200 non-subsequences
in our sample, which amounts to 87% (16% plus
84% of 84%) of the total data.
It should be noted that we have been very strict
regarding what counts as a semantically equiva-
lent subtitle: every piece of information occurring
in the non-subsequence subtitle must reoccur in
the sequence subtitle. However, looking at our
original data, it is clear that considerable liberty
is taken as far as conserving semantic content is
concerned: subtitles often drop substantial pieces
of information. If we relax the notion of seman-
tic equivalence a little, an even larger part of the
non-subsequences can be rewritten as proper se-
quences.
The remaining problematic non-subsequences
are those where insertion, substitution and/or word
reordering are essential to obtain a sufficient CR.
One of the issues we identified is that deletion
of certain constituents must be accompanied by a
change in word order to prevent an ungrammati-
cal sentence. Since changes in word order appear
to require grammatical modeling or knowledge,
this brings sentence compression closer to being
an NLG task.
Nguyen and Horiguchi (2003) describe an ex-
tension of the decision tree-based compression
model (Knight and Marcu, 2002) which allows for
word order changes. The key to their approach
is that dropped constituents are temporarily stored
on a deletion stack, from which they can later be
re-inserted in the tree where required. Although
this provides an unlimited freedom for rearranging
constituents, it also complicates the task of learn-
ing the parsing steps, which might explain why
their evaluation results show marginal improve-
ments at best.
In our data, most of the word order changes ap-
pear to be minor though, often only moving the
verb to second position after deleting a constituent
in the topic position. We believe that unrestricted
word order changes are perhaps not necessary and
that the vast majority of the word order problems
can be solved by a fairly restricted way of reorder-
ing. In particular, we plan to implement a parser-
based model with an additional swap operation
that swaps the two topmost items on the stack. We
expect that this is more feasible as a learning task
than a model with a deletion stack.
Apart from reordering, other problems for word
deletion models are the insertions and substitu-
tions as a result of paraphrasing. Within a deci-
sion tree-based model, paraphrasing of words or
continuous phrases may be modeled by a combi-
nation of a paraphrase lexicon and an extra opera-
tion which replaces the n topmost elements on the
stack by the corresponding paraphrase. However,
paraphrases involving variable arguments, as typ-
ical for verbal paraphrases, cannot be accounted
for in this way. More powerful compression mod-
els may draw on existing NLG methods for text
revision (Inui et al, 1992) to accommodate full
paraphrasing.
We also looked at the perspectives for auto-
matic paraphrase extraction from large text cor-
pora. About a quarter of the required paraphrase
patterns was found at least a hundred times in our
corpus of 325M tokens. Extrapolation suggests
that using the web at its current size would give us
a coverage of approximately ten counts for three
31
quarters of the paraphrases.
Incidentally, we identified two other tasks in
automatic subtitling which are closely related to
NLG. First, splitting and merging of sentences
(Jing and McKeown, 2000), which seems related
to content planning and aggregation. Second, gen-
eration of a shorter referring expression or an
anaphoric expression, which is currently one of
the main themes in data-driven NLG.
In conclusion, we have presented evidence that
deletion models for sentence compression are not
sufficient, and that more elaborate models in-
volving reordering and paraphrasing are required,
which puts sentence compression in the field of
NLG.
Acknowledgments
We would like to thank Nienke Eckhardt, Paul van Pelt, Han-
neke Schoormans and Jurry de Vos for the corpus annota-
tion work, and Erik Tsjong Kim Sang and colleagues for the
autocue-subtitle material from the ATRANOS project, and
Martijn Goudbeek for help with curve fitting. This work was
conducted within the DAESO project funded by the Stevin
program (De Nederlandse Taalunie).
References
Regina Barzilay and Lillian Lee. 2003. Learning to
paraphrase: an unsupervised approach using multiple-
sequence alignment. In Proceedings of the 2003 Confer-
ence of the North American Chapter of the Association for
Computational Linguistics on Human Language Technol-
ogy, pages 16?23, Morristown, NJ, USA.
Anja Belz and Ehud Reiter. 2006. Comparing automatic and
human evaluation of NLG systems. In Proceedings of the
11th Conference of the European Chapter of the Associa-
tion for Computational Linguistics, pages 313?320.
James Clarke and Mirella Lapata. 2006. Models for sentence
compression: a comparison across domains, training re-
quirements and evaluation measures. In Proceedings of
the 21st International Conference on Computational Lin-
guistics and the 44th annual meeting of the Association for
Computational Linguistics, pages 377?384, Morristown,
NJ, USA.
James Clarke and Mirella Lapata. 2008. Global inference
for sentence compression an integer linear programming
approach. Journal of Artificial Intelligence Research,
31:399?429.
Simon Corston-Oliver. 2001. Text compaction for display
on very small screens. In Proceedings of the Workshop
on Automatic Summarization (WAS 2001), pages 89?98,
Pittsburgh, PA, USA.
Walter Daelemans, Anita Ho?thker, and Erik Tjong Kim Sang.
2004. Automatic sentence simplification for subtitling in
Dutch and English. In Proceedings of the 4th Interna-
tional Conference on Language Resources and Evalua-
tion, pages 1045?1048.
Bill Dolan, Chris Quirk, and Chris Brockett. 2004. Unsuper-
vised construction of large paraphrase corpora: Exploiting
massively parallel news sources. In Proceedings of the
20th International Conference on Computational Linguis-
tics, pages 350?356, Morristown, NJ, USA.
Ali Ibrahim, Boris Katz, and Jimmy Lin. 2003. Extract-
ing structural paraphrases from aligned monolingual cor-
pora. In Proceedings of the 2nd International Work-
shop on Paraphrasing, volume 16, pages 57?64, Sapporo,
Japan.
Kentaro Inui, Takenobu Tokunaga, and Hozumi Tanaka.
1992. Text Revision: A Model and Its Implementation.
In Proceedings of the 6th International Workshop on Nat-
ural Language Generation: Aspects of Automated Natural
Language Generation, pages 215?230. Springer-Verlag
London, UK.
Hongyan Jing and Kathleen McKeown. 2000. Cut and paste
based text summarization. In Proceedings of the 1st Con-
ference of the North American Chapter of the Association
for Computational Linguistics, pages 178?185, San Fran-
cisco, CA, USA.
Kevin Knight and Daniel Marcu. 2002. Summarization be-
yond sentence extraction: A probabilistic approach to sen-
tence compression. Artificial Intelligence, 139(1):91?107.
Nguyen Minh Le and Susumu Horiguchi. 2003. A New Sen-
tence Reduction based on Decision Tree Model. In Pro-
ceedings of the 17th Pacific Asia Conference on Language,
Information and Computation, pages 290?297.
Dekang Lin and Patrick Pantel. 2001. Discovery of infer-
ence rules for question answering. Natural Language En-
gineering, 7(4):343?360.
Chin-Yew Lin. 2003. Improving summarization perfor-
mance by sentence compression - A pilot study. In Pro-
ceedings of the Sixth International Workshop on Informa-
tion Retrieval with Asian Languages, volume 2003, pages
1?9.
Erwin Marsi and Emiel Krahmer. 2007. Annotating a par-
allel monolingual treebank with semantic similarity re-
lations. In Proceedings of the 6th International Work-
shop on Treebanks and Linguistic Theories, pages 85?96,
Bergen, Norway.
Jenine Turner and Eugene Charniak. 2005. Supervised and
unsupervised learning for sentence compression. In Pro-
ceedings of the 43rd Annual Meeting of the Association
for Computational Linguistics, pages 290?297, Ann Ar-
bor, Michigan, June.
Vincent Vandeghinste and Yi Pan. 2004. Sentence com-
pression for automated subtitling: A hybrid approach. In
Proceedings of the ACL Workshop on Text Summarization,
pages 89?95.
Vincent Vandeghinste and Erik Tsjong Kim Sang. 2004.
Using a Parallel Transcript/Subtitle Corpus for Sentence
Compression. In Proceedings of LREC 2004.
David Zajic, Bonnie J. Dorr, Jimmy Lin, and Richard
Schwartz. 2007. Multi-candidate reduction: Sentence
compression as a tool for document summarization tasks.
Information Processing Management, 43(6):1549?1570.
32
Proceedings of the 12th European Workshop on Natural Language Generation, pages 122?125,
Athens, Greece, 30 ? 31 March 2009. c?2009 Association for Computational Linguistics
Clustering and Matching Headlines for Automatic Paraphrase Acquisition
Sander Wubben, Antal van den Bosch, Emiel Krahmer, Erwin Marsi
Tilburg centre for Creative Computing
Tilburg University
The Netherlands
{s.wubben,antal.vdnbosch,e.j.krahmer,e.c.marsi}@uvt.nl
Abstract
For developing a data-driven text rewriting
algorithm for paraphrasing, it is essential
to have a monolingual corpus of aligned
paraphrased sentences. News article head-
lines are a rich source of paraphrases; they
tend to describe the same event in vari-
ous different ways, and can easily be ob-
tained from the web. We compare two
methods of aligning headlines to construct
such an aligned corpus of paraphrases, one
based on clustering, and the other on pair-
wise similarity-based matching. We show
that the latter performs best on the task of
aligning paraphrastic headlines.
1 Introduction
In recent years, text-to-text generation has re-
ceived increasing attention in the field of Nat-
ural Language Generation (NLG). In contrast
to traditional concept-to-text systems, text-to-text
generation systems convert source text to target
text, where typically the source and target text
share the same meaning to some extent. Ap-
plications of text-to-text generation include sum-
marization (Knight and Marcu, 2002), question-
answering (Lin and Pantel, 2001), and machine
translation.
For text-to-text generation it is important to
know which words and phrases are semantically
close or exchangable in which contexts. While
there are various resources available that capture
such knowledge at the word level (e.g., synset
knowledge in WordNet), this kind of information
is much harder to get by at the phrase level. There-
fore, paraphrase acquisition can be considered an
important technology for producing resources for
text-to-text generation. Paraphrase generation has
already proven to be valuable for Question An-
swering (Lin and Pantel, 2001; Riezler et al,
2007), Machine Translation (Callison-Burch et al,
2006) and the evaluation thereof (Russo-Lassner
et al, 2006; Kauchak and Barzilay, 2006; Zhou et
al., 2006), but also for text simplification and ex-
planation.
In the study described in this paper, we make
an effort to collect Dutch paraphrases from news
article headlines in an unsupervised way to be
used in future paraphrase generation. News ar-
ticle headlines are abundant on the web, and
are already grouped by news aggregators such as
Google News. These services collect multiple arti-
cles covering the same event. Crawling such news
aggregators is an effective way of collecting re-
lated articles which can straightforwardly be used
for the acquisition of paraphrases (Dolan et al,
2004; Nelken and Shieber, 2006). We use this
method to collect a large amount of aligned para-
phrases in an automatic fashion.
2 Method
We aim to build a high-quality paraphrase corpus.
Considering the fact that this corpus will be the ba-
sic resource of a paraphrase generation system, we
need it to be as free of errors as possible, because
errors will propagate throughout the system. This
implies that we focus on obtaining a high precision
in the paraphrases collection process. Where pre-
vious work has focused on aligning news-items at
the paragraph and sentence level (Barzilay and El-
hadad, 2003), we choose to focus on aligning the
headlines of news articles. We think this approach
will enable us to harvest reliable training material
for paraphrase generation quickly and efficiently,
without having to worry too much about the prob-
lems that arise when trying to align complete news
articles.
For the development of our system we use
data which was obtained in the DAESO-project.
This project is an ongoing effort to build a Par-
allel Monolingual Treebank for Dutch (Marsi
122
Placenta sandwich? No, urban legend!
Tom wants to make movie with Katie
Kate?s dad not happy with Tom Cruise
Cruise and Holmes sign for eighteen million
Eighteen million for Tom and Katie
Newest mission Tom Cruise not very convincing
Latest mission Tom Cruise succeeds less well
Tom Cruise barely succeeds with MI:3
Tom Cruise: How weird is he?
How weird is Tom Cruise really?
Tom Cruise leaves family
Tom Cruise escapes changing diapers
Table 1: Part of a sample headline cluster, with
sub-clusters
and Krahmer, 2007) and will be made available
through the Dutch HLT Agency. Part of the data
in the DAESO-corpus consists of headline clusters
crawled from Google News Netherlands in the pe-
riod April?August 2006. For each news article,
the headline and the first 150 characters of the ar-
ticle were stored. Roughly 13,000 clusters were
retrieved. Table 1 shows part of a (translated) clus-
ter. It is clear that although clusters deal roughly
with one subject, the headlines can represent quite
a different perspective on the content of the arti-
cle. To obtain only paraphrase pairs, the clusters
need to be more coherent. To that end 865 clus-
ters were manually subdivided into sub-clusters of
headlines that show clear semantic overlap. Sub-
clustering is no trivial task, however. Some sen-
tences are very clearly paraphrases, but consider
for instance the last two sentences in the example.
They do paraphrase each other to some extent, but
their relation can only be understood properly with
world knowledge. Also, there are numerous head-
lines that can not be sub-clustered, such as the first
three headlines shown in the example.
We use these annotated clusters as development
and test data in developing a method to automat-
ically obtain paraphrase pairs from headline clus-
ters. We divide the annotated headline clusters in a
development set of 40 clusters, while the remain-
der is used as test data. The headlines are stemmed
using the porter stemmer for Dutch (Kraaij and
Pohlmann, 1994).
Instead of a word overlap measure as used by
Barzilay and Elhadad (2003), we use a modified
TF ?IDF word score as was suggested by Nelken
and Shieber (2006). Each sentence is viewed as a
document, and each original cluster as a collection
of documents. For each stemmed word i in sen-
tence j, TFi,j is a binary variable indicating if the
word occurs in the sentence or not. The TF ?IDF
score is then:
TF.IDFi = TFi,j ? log
|D|
|{dj : ti ? dj}|
|D| is the total number of sentences in the clus-
ter and |{dj : ti ? dj}| is the number of sen-
tences that contain the term ti. These scores are
used in a vector space representation. The similar-
ity between headlines can be calculated by using
a similarity function on the headline vectors, such
as cosine similarity.
2.1 Clustering
Our first approach is to use a clustering algorithm
to cluster similar headlines. The original Google
News headline clusters are reclustered into finer
grained sub-clusters. We use the k-means imple-
mentation in the CLUTO1 software package. The
k-means algorithm is an algorithm that assigns
k centers to represent the clustering of n points
(k < n) in a vector space. The total intra-cluster
variances is minimized by the function
V =
k?
i=1
?
xj?Si
(xj ? ?i)
2
where ?i is the centroid of all the points xj ? Si.
The PK1 cluster-stopping algorithm as pro-
posed by Pedersen and Kulkarni (2006) is used to
find the optimal k for each sub-cluster:
PK1(k) =
Cr(k)?mean(Cr[1...?K])
std(Cr[1...?K])
Here, Cr is a criterion function, which mea-
sures the ratio of withincluster similarity to be-
tweencluster similarity. As soon as PK1(k) ex-
ceeds a threshold, k?1 is selected as the optimum
number of clusters.
To find the optimal threshold value for cluster-
stopping, optimization is performed on the devel-
opment data. Our optimization function is an F -
score:
F? =
(1 + ?2) ? (precision ? recall)
(?2 ? precision + recall)
1http://glaros.dtc.umn.edu/gkhome/views/cluto/
123
We evaluate the number of aligments between pos-
sible paraphrases. For instance, in a cluster of four
sentences,
(4
2
)
= 6 alignments can be made. In
our case, precision is the number of alignments
retrieved from the clusters which are relevant, di-
vided by the total number of retrieved alignments.
Recall is the number of relevant retrieved alig-
ments divided by the total number of relevant
alignments.
We use an F?-score with a ? of 0.25 as we
favour precision over recall. We do not want to op-
timize on precision alone, because we still want to
retrieve a fair amount of paraphrases and not only
the ones that are very similar. Through optimiza-
tion on our development set, we find an optimal
threshold for the PK1 algorithm thpk1 = 1. For
each original cluster, k-means clustering is then
performed using the k found by the cluster stop-
ping function. In each newly obtained cluster all
headlines can be aligned to each other.
2.2 Pairwise similarity
Our second approach is to calculate the similarity
between pairs of headlines directly. If the similar-
ity exceeds a certain threshold, the pair is accepted
as a paraphrase pair. If it is below the thresh-
old, it is rejected. However, as Barzilay and El-
hadad (2003) have pointed out, sentence mapping
in this way is only effective to a certain extent.
Beyond that point, context is needed. With this
in mind, we adopt two thresholds and the Cosine
similarity function to calculate the similarity be-
tween two sentences:
cos(?) =
V 1 ? V 2
?V 1??V 2?
where V 1 and V 2 are the vectors of the two sen-
tences being compared. If the similarity is higher
than the upper threshold, it is accepted. If it is
lower than the lower theshold, it is rejected. In
the remaining case of a similarity between the two
thresholds, similarity is calculated over the con-
texts of the two headlines, namely the text snippet
that was retrieved with the headline. If this simi-
larity exceeds the upper threshold, it is accepted.
Threshold values as found by optimizing on the
development data using again an F0.25-score, are
Thlower = 0.2 and Thupper = 0.5. An optional
final step is to add alignments that are implied by
previous alignments. For instance, if headlineA is
paired with headline B, and headline B is aligned
to headline C, headline A can be aligned to C as
Type Precision Recall
k-means clustering 0.91 0.43
clusters only
k-means clustering 0.66 0.44
all headlines
pairwise similarity 0.93 0.39
clusters only
pairwise similarity 0.76 0.41
all headlines
Table 2: Precision and Recall for both methods
Playstation 3 more expensive than
competitor
Playstation 3 will become more
expensive than Xbox 360
Sony postpones Blu-Ray movies
Sony postpones coming of blu-ray dvds
Prices Playstation 3 known: from 499 euros
E3 2006: Playstation 3 from 499 euros
Sony PS3 with Blu-Ray for sale from
November 11th
PS3 available in Europe from
November 17th
Table 3: Examples of correct (above) and incorrect
(below) alignments
well. We do not add these alignments, because in
particular in large clusters when one wrong align-
ment is made, this process chains together a large
amount of incorrect alignments.
3 Results
The 825 clusters in the test set contain 1,751 sub-
clusters in total. In these sub-clusters, there are
6,685 clustered headlines. Another 3,123 head-
lines remain unclustered. Table 2 displays the
paraphrase detection precision and recall of our
two approaches. It is clear that k-means cluster-
ing performs well when all unclustered headlines
are artificially ignored. In the more realistic case
when there are also items that cannot be clustered,
the pairwise calculation of similarity with a back
off strategy of using context performs better when
we aim for higher precision. Some examples of
correct and incorrect alignments are given in Ta-
ble 3.
124
4 Discussion
Using headlines of news articles clustered by
Google News, and finding good paraphrases
within these clusters is an effective route for ob-
taining pairs of paraphrased sentences with rea-
sonable precision. We have shown that a cosine
similarity function comparing headlines and us-
ing a back off strategy to compare context can be
used to extract paraphrase pairs at a precision of
0.76. Although we could aim for a higher preci-
sion by assigning higher values to the thresholds,
we still want some recall and variation in our para-
phrases. Of course the coverage of our method is
still somewhat limited: only paraphrases that have
some words in common will be extracted. This
is not a bad thing: we are particularly interested
in extracting paraphrase patterns at the constituent
level. These alignments can be made with existing
alignment tools such as the GIZA++ toolkit.
We measure the performance of our approaches
by comparing to human annotation of sub-
clusterings. The human task in itself is hard. For
instance, is we look at the incorrect examples in
Table 3, the difficulty of distinguishing between
paraphrases and non-paraphrases is apparent. In
future research we would like to investigate the
task of judging paraphrases. The next step we
would like to take towards automatic paraphrase
generation, is to identify the differences between
paraphrases at the constituent level. This task has
in fact been performed by human annotators in the
DAESO-project. A logical next step would be to
learn to align the different constituents on our ex-
tracted paraphrases in an unsupervised way.
Acknowledgements
Thanks are due to the Netherlands Organization
for Scientific Research (NWO) and to the Dutch
HLT Stevin programme. Thanks also to Wauter
Bosma for originally mining the headlines from
Google News. For more information on DAESO,
please visit daeso.uvt.nl.
References
Regina Barzilay and Noemie Elhadad. 2003. Sentence
alignment for monolingual comparable corpora. In
Proceedings of the 2003 conference on Empirical
methods in natural language processing, pages 25?
32.
Chris Callison-Burch, Philipp Koehn, and Miles Os-
borne. 2006. Improved statistical machine transla-
tion using paraphrases. In Proceedings of the main
conference on Human Language Technology Con-
ference of the North American Chapter of the Asso-
ciation of Computational Linguistics, pages 17?24.
Bill Dolan, Chris Quirk, and Chris Brockett. 2004.
Unsupervised construction of large paraphrase cor-
pora: exploiting massively parallel news sources. In
COLING ?04: Proceedings of the 20th international
conference on Computational Linguistics, page 350.
David Kauchak and Regina Barzilay. 2006. Para-
phrasing for automatic evaluation. In Proceedings
of the Human Language Technology Conference of
the NAACL, Main Conference, pages 455?462, June.
Kevin Knight and Daniel Marcu. 2002. Summa-
rization beyond sentence extraction: a probabilis-
tic approach to sentence compression. Artif. Intell.,
139(1):91?107.
Wessel Kraaij and Rene Pohlmann. 1994. Porters
stemming algorithm for dutch. In Informatieweten-
schap 1994: Wetenschappelijke bijdragen aan de
derde STINFON Conferentie, pages 167?180.
Dekang Lin and Patrick Pantel. 2001. Dirt: Discov-
ery of inference rules from text. In KDD ?01: Pro-
ceedings of the seventh ACM SIGKDD international
conference on Knowledge discovery and data min-
ing, pages 323?328.
Erwin Marsi and Emiel Krahmer. 2007. Annotating
a parallel monolingual treebank with semantic sim-
ilarity relations. In he Sixth International Workshop
on Treebanks and Linguistic Theories (TLT?07).
Rani Nelken and Stuart M. Shieber. 2006. Towards ro-
bust context-sensitive sentence alignment for mono-
lingual corpora. In Proceedings of the 11th Confer-
ence of the European Chapter of the Association for
Computational Linguistics (EACL-06), 3?7 April.
Ted Pedersen and Anagha Kulkarni. 2006. Automatic
cluster stopping with criterion functions and the gap
statistic. In Proceedings of the 2006 Conference
of the North American Chapter of the Association
for Computational Linguistics on Human Language
Technology, pages 276?279.
Stefan Riezler, Alexander Vasserman, Ioannis
Tsochantaridis, Vibhu O. Mittal, and Yi Liu. 2007.
Statistical machine translation for query expansion
in answer retrieval. In ACL.
Grazia Russo-Lassner, Jimmy Lin, and Philip Resnik.
2006. A paraphrase-based approach to machine
translation evaluation. Technical report, University
of Maryland, College Park.
Liang Zhou, Chin-Yew Lin, and Eduard Hovy. 2006.
Re-evaluating machine translation results with para-
phrase support. In Proceedings of the 2006 Con-
ference on Empirical Methods in Natural Language
Processing, pages 77?84, July.
125
Proceedings of the 2009 Workshop on Language Generation and Summarisation, ACL-IJCNLP 2009, pages 63?66,
Suntec, Singapore, 6 August 2009.
c?2009 ACL and AFNLP
Reducing redundancy in multi-document summarization
using lexical semantic similarity
Iris Hendrickx, Walter Daelemans
University of Antwerp
Antwerpen, Belgium
iris.hendrickx@ua.ac.be
walter.daelemans@ua.ac.be
Erwin Marsi, Emiel Krahmer
Tilburg University
Tilburg, The Netherlands
e.j.krahmer@uvt.nl
e.c.marsi@uvt.nl
Abstract
We present an automatic multi-document
summarization system for Dutch based on
the MEAD system. We focus on redun-
dancy detection, an essential ingredient of
multi-document summarization. We in-
troduce a semantic overlap detection tool,
which goes beyond simple string match-
ing. Our results so far do not confirm
our expectation that this tool would out-
perform the other tested methods.
1 Introduction
One of the main issues in automatic multi-
document summarization is avoiding redundancy.
As the source documents are all related to the
same topic, at least some of their content is likely
to overlap. In fact, this is in part what makes
multi-document summarization feasible. For ex-
ample, news articles that report on a particular
event, or that are based on the same source, often
contain similar information expressed in differ-
ent ways. A multi-document summarizer should
include this overlapping information not more
than once. The backbone of most current ap-
proaches to automatic summarization is a vector
space model in which a sentence is regarded as
a bag of words and a weighted cosine similarity
measure is used to quantify the amount of shared
information between a pair of sentences. Cosine
similarity (in this context) essentially amounts to
calculating word overlap, albeit with weighting of
the terms and normalization for differences in sen-
tence length. It is clear that this approach to detect-
ing redundancy is far from satisfactory, because it
only covers redundancy in its most trivial form,
i.e., identical words. In contrast, the redundancy
that we ultimately want to avoid in summarization
is that at the semantic level. As an extreme case
in point, two sentences with no words in common
can still carry virtually the same meaning.
The remainder of this paper is structured in
the following way. In Section 2 we introduce a
tool for detecting semantic overlap. In section 3
we present a Dutch multi-document summariza-
tion system, based on the MEAD summarization
toolkit (Radev et al, 2004). Next, in section 4 we
describe the experimental setup and the data set
that we used. Section 5 reports on the results, and
we conclude in section 6.
2 Detecting semantic overlap
In this section, we detail the semantic overlap de-
tection tool and the resources we build on.
Parallel/comparable text corpus The basis for
our semantic overlap detection tool is a mono-
lingual parallel/comparable tree-bank of 1 million
words of Dutch text (Marsi and Krahmer, 2007).
Half of the text material has so far been manually
aligned at the sentence level. Subsequently, the
sentences have been parsed and the resulting parse
trees have been aligned at the level of syntactic
nodes. Moreover, aligned nodes have been labeled
according to a set of semantic similarity labels that
express the type of similarity relation between the
nodes. The following five labels are used: gen-
eralize, specify, intersect, restate, and equal. The
corpus serves as the basis for developing tools for
automatic alignment and relation labeling.
Word aligner The word alignment tool takes as
input a pair of source and target sentences and
produces a matching between the words, that is,
a (possibly partial) one-to-one mapping of source
to target words. This aligner is a part of the full
fledged tree aligner currently under development.
The alignment task comprises several subtasks.
First, the input sentences are tokenized and parsed
with the Alpino syntactic parser for Dutch (Bouma
et al, 2001). Apart from the syntactic analysis,
which we disregard in the current work, the parser
63
performs lemmatization, part-of-speech tagging
and compound analysis, all of which are used here.
In addition, the aligner uses lexical-semantic
knowledge from Cornetto, a lexical database for
Dutch (40K entries) similar to the well-known En-
glish WordNet (Vossen et al, 2008). The rela-
tions we use are synonym, hyperonym, and xpos-
near-synonym (align near synonyms with differ-
ent POS labels). In addition we check whether
a pair of content words has a least common sub-
sumer (LCS) in the hyperonym hierarchy. As path
length has been shown to be a poor predictor in
this respect, we calculate the Lin similarity, which
combines the Information Content of the words in-
volved (Lin, 1998). A current limitation is that
we lack word sense disambiguation, hence we take
the maximal score over all the senses of the words.
The components described above can be con-
sidered as experts which predict word alignments
with a certain probability. Since alignments can
support, complement or contradict each other, we
are faced with the problem of how to combine
the evidence. Our approach is to view the align-
ment as a weighted bipartite multigraph. That is,
a graph where source and target nodes are in dis-
joint sets, multiple edges are allowed between the
same pair of nodes, and edges have an associated
weight. Our goal is on the one hand to maximize
the sum of the edge weights, and on the other hand
to reduce this graph to a model in which every
node can have at most one associated edge. This
is a combinatorial optimization problem known as
the assignment problem for which efficient algo-
rithms exist. We use a variant of the The Hungar-
ian Algorithm
1
(Kuhn, 1955), for the computation
of the matches.
Sentence similarity score Given a word align-
ment between a pair of sentences, a similarity
score is required to measure the amount of se-
mantic overlap or redundancy. Evidently the sim-
ilarity score should be proportional to the relative
number of aligned words. However, some align-
ments are more important than others. For exam-
ple, the alignment between two determiners (e.g.
the) is less significant than that between two com-
mon nouns. This is modeled in our similarity score
by weighting alignments according to the idf (in-
verse document frequency) (Sp?arck Jones, 1972)
of the words involved.
1
Also known as the Munkres algorithm
sim(s
1
, s
2
) =
?
w
i
?A
idf(w
i
)
?
w
j
?S
idf(w
j
)
(1)
Here s
1
and s
2
are sentences, S is the longest of
the two sentences, w
j
are the words in S, A is the
subsequence of aligned words in S, and w
i
are the
words in A.
3 Multi-document summarization
The Dutch Multi-Document Summarizer pre-
sented here is based on the MEAD summariza-
tion toolkit (Radev et al, 2004), which offers a
wide range of summarization algorithms and has a
flexible structure. The system creates a summary
by extracting a subset of sentences from the orig-
inal documents. The summarizer reads in a clus-
ter of documents, i.e. a set of documents relevant
for the same topic, and for each sentence it ex-
tracts a set of features. These features are com-
bined to determine an importance score for each
sentence. Next the sentences are sorted accord-
ing to their importance score. The system starts a
summary by adding the sentence with the highest
weight. Then it examines the second most impor-
tant sentence and measures the similarity with the
sentence that is already added. If the overlap is
limited, the sentence is added to the summary, oth-
erwise it is disregarded. This process is repeated
until the intended summary size is reached. The
module that performs this last step of determining
which sentences end up in the final summary is
called the reranker.
We use two baseline systems: the random base-
line system randomly selects a set of sentences
and the lead-based system which selects a sub-
set of initial sentences as summary. We investi-
gated the following features. A simple and effec-
tive features is the position: each sentence gets a
score of 1/position where ?position? is the place
in the document. The length feature is a filter that
removes sentences shorter than the given thresh-
old. The simwf feature presents the overlap of a
sentence with the title of the document computed
with cosine similarity. One of MEAD?s main fea-
tures is centroid-based summarization. Centroids
of clusters are used to determine which words
are important for the cluster and sentences con-
taining these words are considered to be central
sentences. The words are weighted with tf*idf.
64
The aim of query-based summarization is to cre-
ate summaries that are relevant with respect to a
particular query. This can easily be done with fea-
tures that express the overlap between the query
and a source sentence. We examined three differ-
ent query-based features that measure simple word
overlap between the query and the sentence, co-
sine similarity with tf*idf weighting of words and
cosine similarity without tf*idf weighting.
The MEAD toolkit implements multiple
reranker modules, we investigated the following
three: the cosine-reranker, the mmr-reranker and
novelty-reranker. We compare these rerankers
against the semantic overlap detection (sod)
tool detailed in section 2. The cosine-reranker
represents two sentences as tf*idf weighted word
vectors and computes a cosine similarity score
between them. Sentences with a cosine similarity
above the threshold are disregarded. The mmr-
reranker module is based on the maximal margin
relevance criterion (Carbonell and Goldstein,
1998). MMR models the trade-off between a
focused summary and a summary with a wide
scope. The novelty-reranker is an extension of the
cosine-reranker and boosts sentences occurring
after an important sentence by multiplying with
1.2. The reranker tries to mimic human behavior
as people tend to pick clusters of sentences when
summarizing.
4 Experimental setup
To perform proper evaluation of the summariza-
tion system we constructed a new data set for eval-
uating Dutch multi-document summarization. It
consists of 30 query-based document clusters. The
document clusters were created manually follow-
ing the guidelines of DUC 2006 (Dang, 2006).
Each cluster contains a query description and 5 to
25 newspaper articles relevant for that particular
question. For each cluster five annotators wrote
an abstract of approximately 250 words. These
summaries serve as a gold standard for compari-
son with automatically generated extracts.
We split our data set in a test set of 20 clus-
ters and a development set of 10 clusters. We use
the development set for parameter tuning and fea-
ture selection for the summarizer. We try out each
of the characteristics discussed in section 3. The
best combination found on the development set is
the feature combination position, centroid, length
with cut-off 13, and queryCosine. We tested the
different rerankers and vary the similarity thresh-
olds to determine their optimal threshold value. As
the novelty-reranker scored lower than the other
rerankers on the development set, we did not in-
clude it in our experiments on the test set.
For the experiments on the development set, we
compare each of the automatically produced ex-
tracts with five manually written summaries and
report macro-average Rouge-2 and Rouge-SU4
scores (Lin and Hovy, 2003). For the experiments
on the test set, we also perform a manual evalu-
ation. We follow the DUC 2006 guidelines for
manual evaluation of responsiveness and the lin-
guistic quality of the produced summaries. The re-
sponsiveness scores express the information con-
tent of the summary with respect to the query. The
linguistic quality is evaluated on five different ob-
jectives: grammaticality, non-redundancy, coher-
ence, referential clarity and focus. The annotators
can choose a value on a five point scale where
1 means ?very poor? and 5 means ?very good?.
We use two independent annotators to evaluate the
summaries and we report the average scores.
5 Results
The evaluation of the results on the test set are
shown in table 1. The Rouge scores of the different
rerankers are all above both baselines, and they are
very close to each other. The scores for the content
measure and responsiveness show that the values
for the automatic summaries are between 2 (poor)
and 3 (barely acceptable). The optimized summa-
rizers score higher than the two baselines on this
point.
We are most interested in the aspect of ?non-
redundancy?. The random baseline system
achieves a good result here, and the optimized
summarizers all score lower. The chance of over-
lap between randomly selected sentences seems
to be lower than when an automatic summarizer
tries to select only the most important sentences.
When we compare the three optimized systems
with different rerankers on this aspect we see that
the scores are very close. Our semantic overlap de-
tection (sod) reranker does not do any better than
the other two. The optimized summarizers do per-
form better than the baseline systems with respect
to focus and structure.
65
setting Rouge-2 Rouge-SU4 gram redun ref focus struct respons
rand baseline 0.101 0.153 4.08 3.9 2.58 2.6 2 2.25
lead baseline 0.139 0.179 3.05 3.6 3.25 2.88 2.38 2.4
optim-cosine 0.152 0.193 3.9 3.18 2.65 3.15 2.43 2.75
optim-mmr 0.149 0.191 3.98 3.13 2.55 3.13 2.38 2.7
optim-sod 0.150 0.193 4.05 3.13 2.85 3.23 2.5 2.7
Table 1: Macro-average Rouge scores and manual evaluation on the test set on these aspects:
grammaticality, non-redundancy, referential clarity, focus, structure and responsiveness.
6 Discussion and conclusion
We presented an automatic multi-document sum-
marization system for Dutch based on the MEAD
system, supporting the claim that MEAD is largely
language-independent. We experimented with dif-
ferent features and parameter settings of the sum-
marizer, and optimized it for summarization of
Dutch newspaper text. We presented a semantic
overlap detection tool, developed on the basis of a
monolingual corpus of parallel/comparable Dutch
text, which goes beyond simple string matching.
We expected this tool to improve the sentence
reranking step, thereby reducing redundancy in the
summaries. However, we were unable to show a
significant effect. We have several possible expla-
nations for this. First, many of the sentence pairs
that share the same semantic content, also share a
number of identical words. To detect these cases,
therefore, computing cosine similarity may be just
as effective. Second, the accuracy of the align-
ment tool may not be good enough, partly because
of errors in the linguistic analysis or lack of cover-
age, and partly because certain types of knowledge
(word sense, syntactic structure) are not yet ex-
ploited. Third, reranking of sentences is unlikely
to improve the summary in cases where the pre-
ceding step of sentence ranking within documents
performs poorly. We are currently still investigat-
ing this matter and hope to obtain significant re-
sults with an improved version of our tool for de-
tecting semantic overlap.
We plan to work on a more refined version that
not only uses word alignment but also considers
alignments at the parse tree level. This idea is
in line with the work of Barzilay and McKeown
(2005) who use this type of technique to fuse sim-
ilar sentences for multi-document summarization.
Acknowledgements This work was conducted within the
DAESO http://daeso.uvt.nl project funded by the
Stevin program (De Nederlandse Taalunie). The construction
of the evaluation corpus described in this paper was financed
by KP BOF 2008, University of Antwerp. We would like to
thank NIST for kindly sharing their DUC 2006 guidelines.
References
Regina Barzilay and Kathleen R. McKeown. 2005. Sentence
fusion for multidocument news summarization. Compu-
tational Linguistics, 31(3):297?328.
Gosse Bouma, Gertjan van Noord, and Robert Malouf. 2001.
Alpino: Wide-coverage computational analysis of Dutch.
In Computational Linguistics in the Netherlands 2000.,
pages 45?59. Rodopi, Amsterdam, New York.
Jaime Carbonell and Jade Goldstein. 1998. The use of
mmr, diversity-based reranking for reordering documents
and producing summaries. In Proceedings of SIGIR 1998,
pages 335?336, New York, NY, USA. ACM.
H.T. Dang. 2006. Overview of DUC 2006. In Proceedings
of the Document Understanding Workshop, pages 1?10,
Brooklyn, USA.
Harold W. Kuhn. 1955. The Hungarian Method for the as-
signment problem. Naval Research Logistics Quarterly,
2:83?97.
C.-Y. Lin and E.H. Hovy. 2003. Automatic evaluation
of summaries using n-gram co-occurrence statistics. In
Proceedings of HLT-NAACL, pages 71 ? 78, Edmonton,
Canada.
D. Lin. 1998. An information-theoretic definition of similar-
ity. In Proceedings of the ICML, pages 296?304.
Erwin Marsi and Emiel Krahmer. 2007. Annotating a par-
allel monolingual treebank with semantic similarity re-
lations. In Proceedings of the 6th International Work-
shop on Treebanks and Linguistic Theories, pages 85?96,
Bergen, Norway.
Dragomir Radev et al 2004. Mead - a platform for multidoc-
ument multilingual text summarization. In Proceedings of
LREC 2004, Lisabon, Portugal.
Karen Sp?arck Jones. 1972. A statistical interpretation of
term specificity and its application in retrieval. Journal
of Documentation, 28(1):11?21.
P. Vossen, I. Maks, R. Segers, and H. van der Vliet. 2008.
Integrating lexical units, synsets and ontology in the Cor-
netto Database. In Proceedings of LREC 2008, Mar-
rakech, Morocco.
66
Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 752?760,
Beijing, August 2010
Automatic analysis of semantic similarity in comparable text
through syntactic tree matching
Erwin Marsi
TiCC, Tilburg University
e.c.marsi@uvt.nl
Emiel Krahmer
TiCC, Tilburg University
e.j .krahmer@uvt.nl
Abstract
We propose to analyse semantic similar-
ity in comparable text by matching syn-
tactic trees and labeling the alignments
according to one of five semantic simi-
larity relations. We present a Memory-
based Graph Matcher (MBGM) that per-
forms both tasks simultaneously as a com-
bination of exhaustive pairwise classifica-
tion using a memory-based learner, fol-
lowed by global optimization of the align-
ments using a combinatorial optimization
algorithm. The method is evaluated on a
monolingual treebank consisting of com-
parable Dutch news texts. Results show
that it performs substantially above the
baseline and close to the human reference.
1 Introduction
Natural languages allow us to express essentially
the same underlying meaning as many alterna-
tive surface forms. In other words, there are of-
ten many similar ways to say the same thing.
This characteristic poses a problem for many nat-
ural language processing applications. Automatic
summarizers, for example, typically rank sen-
tences according to their informativity and then
extract the top n sentences, depending on the re-
quired compression rate. Although the sentences
are essentially treated as independent of each
other, they typically are not. Extracted sentences
may have substantial semantic overlap, result-
ing in unintended redundancy in the summaries.
This is particularly problematic in the case of
multi-document summarization, where sentences
extracted from related documents are very likely
to express similar information in different ways
(Radev and McKeown, 1998). Therefore, if se-
mantic similarity between sentences could be de-
tected automatically, this would certainly help to
avoid redundancy in summaries.
Similar arguments can be made for many other
NLP applications. Automatic duplicate and pla-
giarism detection beyond obvious string overlap
requires recognition of semantic similarity. Au-
tomatic question-answering systems may benefit
from clustering semantically similar candidate an-
swers. Intelligent document merging software,
which supports a minimal but lossless merge of
several revisions of the same text, must handle
cases of paraphrasing, restructuring, compression,
etc. Recognizing textual entailments (Dagan et
al., 2005) could arguably be seen as a specific in-
stance of detecting semantic similarity.
In addition to merely detecting semantic simi-
larity, we can ask to what extent two expressions
share meaning. For instance, the meaning of one
sentence can be fully contained in that of another,
the meaning of one sentence can overlap only
partly with that of another, etc. This requires an
analysis of the semantic similarity between a pair
of expressions. Like detection, automatic analy-
sis of semantic similarity can play an important
role in NLP applications. To return to the case
of multi-document summarization, analysing the
semantic similarity between sentences extracted
from different documents provides the basis for
sentence fusion, a process where a new sentence
is generated that conveys all common information
from both sentences without introducing redun-
dancy (Barzilay and McKeown, 2005; Marsi and
Krahmer, 2005b).
752
Analysis of semantic similarity can be ap-
proached from different angles. A basic approach
is to use string similarity measures such as the
Levenshtein distance or the Jaccard similarity co-
efficient. Although cheap and fast, this fails to
account for less obvious cases such as synonyms
or syntactic paraphrasing. At the other extreme,
we can perform a deep semantic analysis of two
expressions and rely on formal reasoning to de-
rive a logical relation between them. This ap-
proach suffers from issues with coverage and ro-
bustness commonly associated with deep linguis-
tic processing. We therefore think that the middle
ground between these two extremes offers the best
option. In this paper we present a new method
for analysing semantic similarity in comparable
text. It relies on a combination of morphologi-
cal and syntactic analysis, lexical resources such
as word nets, and machine learning from exam-
ples. We propose to analyse semantic similarity
between sentences by aligning their syntax trees,
where each node is matched to the most similar
node in the other tree (if any). In addition, we
label these alignments according to the type of
similarity relation that holds between the aligned
phrases. The labeling supports further processing.
For instance, Marsi & Krahmer (2005b; 2008) de-
scribe how to generate different types of sentence
fusions on the basis of this relation labeling.
In the next Section we provide a more formal
definition of the task of matching syntactic trees
and labeling alignments, followed by a discusion
of related work in Section 3. Section 4 describes a
parallel, monolingual treebank used for develop-
ing and testing our approach. In Section 5 we pro-
pose a new algorithm for simultaneous node align-
ment and relation labeling. The results of several
evaluation experiments are presented in Section 6.
We finish with a conclusion.
2 Problem statement
Aligning a pair of similar syntactic trees is the pro-
cess of pairing those nodes that are most similar.
More formally: let v be a node in the syntactic
tree T of sentence S and v? a node in the syntactic
tree T ? of sentence S?. A labeled node alignment
is a tuple < v, v?, r > where r is a label from a set
of relations. A labeled tree alignment is a set of
labeled node alignments. A labeled tree matching
is a tree alignment in which each node is aligned
to at most one other node.
For each node v, its terminal yield STR(v) is de-
fined as the sequence of all terminal nodes reach-
able from v (i.e., a substring of sentence S).
Aligning node v to v? with label r indicates that
relation r holds between their yields STR(v) and
STR(v?). We label alignments according to a small
set of semantic similarity relations. As an exam-
ple, consider the following Dutch sentences:
(1) a. Dagelijks
Daily
koffie
coffee
vermindert
diminishes
risico
risk
op
on
Alzheimer
Alzheimer
en
and
Dementie.
Dementia.
b. Drie
Three
koppen
cups
koffie
coffee
per
a
dag
day
reduceert
reduces
kans
chance
op
on
Parkinson
Parkinson
en
and
Dementie.
Dementia.
The corresponding syntax trees and their (partial)
alignment is shown in Figure 1. We distinguish
the following five mutually exclusive similarity
relations:
1. v equals v? iff lower-cased STR(v) and
lower-cased STR(v?) are identical ? example:
Dementia equals Dementia;
2. v restates v? iff STR(v) is a proper para-
phrase of STR(v?) ? example: diminishes re-
states reduces;
3. v generalizes v? iff STR(v) is more general
than STR(v?) ? example: daily coffee gener-
alizes three cups of coffee a day;
4. v specifies v? iff STR(v) is more specific than
STR(v?) ? example: three cups of coffee a day
specifies dailly coffee;
5. v intersects v? iff STR(v) and STR(v?) share
meaning, but each also contains unique infor-
mation not expressed in the other ? example:
Alzheimer and Dementia intersects Parkin-
son and Dementia.
Our interpretation of these relations is one of
common sense rather than strict logic, akin to
the definition of entailment employed in the RTE
challenge (Dagan et al, 2005). Note also that re-
lations are prioritized: equals takes precedence
753
smain
np vermindert np
Dagelijks koffie
np
smaminpvemr
reduceert
dmrtntmr
risico pp
op conj
Altzheimer en Dementie
conj
Datmirmgtr
Dementie
l jknpr
smain
np
Drie koppen koffie pp
per dag
kans pp
op
Parkinson en
Figure 1: Example of two aligned and labeled syntactic trees. For expository reasons the alignment is
not exhaustive.
over restates, etc. Furthermore, equals, restates
and intersects are symmetrical, whereas general-
izes is the inverse of specifies. Finally, nodes con-
taining unique information, such as Alzheimer and
Parkinson, remain unaligned.
3 Related work
Many syntax-based approaches to machine trans-
lation rely on bilingual treebanks to extract trans-
fer rules or train statistical translation models. In
order to build bilingual treebanks a number of
methods for automatic tree alignment have been
developed, e.g., (Gildea, 2003; Groves et al,
2004; Tinsley et al, 2007; Lavie et al, 2008).
Most related to our approach is the work on dis-
criminative tree alignment by Tiedemann & Kotze?
(2009). However, these algorithms assume that
source and target sentences express the same in-
formation (i.e. parallel text) and cannot cope
with comparable text where parts may remain un-
aligned. See (MacCartney et al, 2008) for further
arguments and empirical evidence that MT align-
ment algorithms are not suitable for aligning par-
allel monolingual text.
MacCartney, Galley, and Manning (2008) de-
scribe a system for monolingual phrase alignment
based on supervised learning which also exploits
external resources for knowledge of semantic re-
latedness. In contrast to our work, they do not
use syntactic trees or similarity relation labels.
Partly similar semantic relations are used in (Mac-
Cartney and Manning, 2008) for modeling seman-
tic containment and exclusion in natural language
inference. Marsi & Krahmer (2005a) is closely
related to our work, but follows a more com-
plicated method: first a dynamic programming-
based tree alignment algorithm is applied, fol-
lowed by a classification of similarity relations us-
ing a supervised-classifier. Other differences are
that their data set is much smaller and consists
of parallel rather than comparable text. A major
drawback of this algorithmic approach it that it
cannot cope with crossing alignments. We are not
aware of other work that combines alignment with
semantic relation labeling, or algorithms which
perform both tasks simultaneously.
4 Data collection
For developing our alignment algorithm we use
the DAESO corpus1. This is a Dutch parallel
monolingual treebank of 1 million words, half
of which were manually annotated. The corpus
consists of pairs of sentences with different lev-
els of semantic overlap, ranging from high (dif-
ferent Dutch translations of books from Darwin,
Montaigne and Saint-Exupe?ry) to low (different
press releases from the two main news agencies
in The Netherlands, ANP and NOVUM). For this
paper, we concentrate on the latter part of the
DAESO corpus, where the proportion of Equals
and Restates is relatively low. This corpus seg-
ment consists of 8,248 pairs of sentences, contain-
ing 162,361 tokens (ignoring punctuation). All
sentences were tokenized and tagged, and subse-
quently parsed by the Alpino dependency parser
for Dutch (Bouma et al, 2001). Two annota-
1http://daeso.uvt.nl
754
Alignment: Labeling:
Eq: Re: Spec: Gen: Int: Macro: Micro:
Words: F: 95.38 95.48 58.50 65.81 65.00 25.85 62.11 88.72
SD: 2.16 2.69 7.63 13.05 11.25 18.74
Full trees: F: 88.31 95.83 71.38 60.21 66.71 62.67 71.36 81.92
SD: 1.15 2.27 3.77 7.63 8.17 6.14
Table 1: Average F-scores (in percentages, with Standard Deviations) for the six human annotators on
alignment and semantic relation labeling, for words and for full syntactic trees.
tors determined which sentences in the compa-
rable news reports contained semantic overlap.
Six other annotators produced manual alignments
of words and phrases in matched sentence pairs,
which resulted in 86,227 aligned pairs of nodes.
A small sample of 10 similar press releases
comprising a total of 48 sentence pairs was inde-
pendently annotated by all six annotators to deter-
mine inter-annotator agreement. We used preci-
sion, recall and F-score on alignment. To calcu-
late these scores for relation labeling, we simply
restrict the set of alignments to those labeled with
a particular relation, ignoring all others. Likewise,
we restrict these sets to terminal node alignments
in order to get scores on word alignment.
Given the six annotations A1, . . . , A6, we re-
peatedly took one as the True annotation against
which the five other annotations were evaluated.
We then computed the average scores over these
6 ? 5 = 30 scores (note that with this proce-
dure, precision, recall and F score end up being
equal). Table 1 summarizes the results, both for
word alignments and for full syntactic tree align-
ment. It can be seen that for alignment of words an
average F-score of over 95 % was obtained, while
alignment for full syntactic trees results in an F-
score of 88%. For relation labeling, the scores dif-
fered per relation, as is to be expected: the average
F-score for Equals was over 95% for both word
and full tree alignment2, and for the other rela-
tions average F-scores between 0.6 and 0.7 were
2At first sight, it may seem that labeling Equals is a trivial
and deterministic task, for which the F-score should always
be close to 100%. However, the same word may occur multi-
ple times in the source or target sentences, which introduces
ambiguity. This frequently occurs with function words such
as determiners and prepositions. Moreover, choosing among
several equivalent Equals alignments may sometimes involve
a somewhat arbitrary decision. This situation arises, for in-
stance, when a proper noun is mentioned just once in the
source sentence but twice in the target sentence.
obtained. The exception to note is Intersects on
word level, which only occurred a few times ac-
cording to a few of the annotators. The macro
and micro (weighted) F-score averages on labeled
alignment are 62.11% and 88.72% for words, and
71.36% and 81.92% for full syntactic trees.
5 Memory-based Graph Matcher
In order to automatically perform the alignment
and labeling tasks described in Section 2, we cast
these tasks simultaneously as a combination of ex-
haustive pairwise classification using a supervised
machine learning algorithm, followed by global
optimization of the alignments using a combina-
torial optimization algorithm. Input to the tree
matching algorithm is a pair of syntactic trees con-
sisting of a source tree Ts and a target tree Tt.
Step 1: Feature extraction For each possible
pairing of a source node ns in tree Ts and a target
node nt in tree Tt, create an instance consisting of
feature values extracted from the input trees. Fea-
tures can represent properties of individual nodes,
e.g. the category of the source node is NP, or rela-
tions between nodes, e.g. source and target node
share the same part-of-speech.
Step 2: Classification A generic supervised
classifier is used to predict a class label for each
instance. The class is either one of the seman-
tic similarity relations or the special class none,
which is interpreted as no alignment. Our im-
plementation employs the memory-based learner
TiMBL (Daelemans et al, 2009), a freely avail-
able, efficient and enhanced implementation of k-
nearest neighbour classification. The classifier is
trained on instances derived according to Step 1
from a parallel treebank of aligned and labeled
syntactic trees.
755
Step 3: Weighting Associate a cost with each
prediction so that high costs indicate low confi-
dence in the predicted class and vice versa. We
use the normalized entropy of the class labels in
the set of nearest neighbours (H) defined as
H = ?
?
c?C p(c) log2 p(c)
log2|C|
(1)
where C is the set of class labels encountered in
the set of nearest neighbours (i.e., a subset of the
five relations plus none), and p(c) is the probabil-
ity of class c, which is simply the proportion of
instances with class label c in the set of nearest
neighbours. Intuitively this means that the cost
is zero if all nearest neighbours are of the same
class, whereas the cost goes to 1 if the nearest
neighbours are equally distributed over all possi-
ble classes.
Step 4: Matching The classification step will
usually give rise to one-to-many alignment of
nodes. In order to reduce this to just one-to-one
alignments, we search for a node matching which
minimizes the sum of costs over all alignments.
This is a well-known problem in combinato-
rial optimization known as the Assignment Prob-
lem. The equivalent in graph-theoretical terms
is a minimum weighted bipartite graph match-
ing. This problem can be solved in polynomial
time (O(n3)) using e.g., the Hungarian algorithm
(Kuhn, 1955). The output of the algorithm is the
labeled tree matching obtained by removing all
node alignments labeled with the special none re-
lation.
6 Experiments
6.1 Experimental setup
Word alignment and full tree alignments are con-
ceptually different tasks, which require partly dif-
ferent features and may have different practical
applications. These are therefore addressed in
separate experiments.
Table 2 summarizes the respective sizes of de-
velopment and the held-out test set in terms of
number of aligned graph pairs, number of aligned
node pairs and number of tokens. The percentage
of aligned nodes over all graphs is calculated rela-
tive to the number of nodes over all graphs. Since
Data Graph Node Tokens Aligned
pairs pairs nodes (%)
word develop 2 664 13 027 45 149 15.71
word test 547 2 858 10 005 14.96
tree develop 2 664 22 741 45 149 47.20
tree test 547 4 894 10 005 47.05
Table 2: Properties of develop and test data sets
Data Eq Re Spec Gen Int
word develop 84.92 6.15 2.10 1.77 5.07
word test 85.62 6.09 2.17 1.99 4.13
tree develop 56.61 6.57 7.52 6.38 22.91
tree test 58.40 7.11 7.40 6.38 20.72
Table 3: Distribution of semantic similarity rela-
tions for word alignment and for full tree align-
ments in both develop and test data sets
alignments involving non-terminal nodes are ig-
nored in the task of word alignment, the number of
aligned node pairs and the percentage of aligned
nodes is lower in the word develop and word test
sets. Table 3 gives the distribution of semantic re-
lations in the development and test set, for word
and tree alignment. It can be observed that the
distribution if fairly skewed with Equals being the
majority class, even more so for word alignments.
Another thing to notice is that Intersects are much
more frequent at the level of non-terminal align-
ments.
Development was carried out using 10-fold
cross validation on the development data and con-
sequently reported scores on the development data
are averages over 10 folds. Only two parameters
were coarsely optimized on the development set.
First, the amount of downsampling of the none
class varied between 0.1 or 0.5. Second, the pa-
rameter k of the memory-based classifier ? the
number of nearest neighbours taken into account
during classification ? ranged from 1 to 15. Opti-
mal settings were finally applied when testing on
the held-out data.
A simple greedy alignment procedure served as
baseline. For word alignment, identical words are
aligned as Equals and identical roots as Restates.
For full tree alignment, this is extended to the level
of phrases so that phrases with identical words are
aligned as Equals and phrases with identical roots
as Restates. The baseline does not predict Spec-
756
ifies, Generalizes or Intersects relations, as that
would require a more involved, knowledge-based
approach.
All features used are described in Table 4.
The word-based features rely on pure string pro-
cessing and require no linguistic preprocessing.
The morphology-based features exploit the lim-
ited amount of morphological analysis provided
by the Alpino parser (Bouma et al, 2001). For
instance, it provides word roots and decomposes
compound words. Likewise the part-of-speech-
based features use the coarse-grained part-of-
speech tags assigned by the Alpino parser. The
lexical-semantic features rely on the Cornetto
database (Vossen et al, 2008), a recent exten-
sion to the Dutch WordNet, to look-up synonym
and hypernym relations among source and tar-
get lemmas. Unfortunately there is no word
sense disambiguation module to identify the cor-
rect senses. In addition, a background corpus
of over 500M words of (mainly) news text pro-
vides the word counts required to calculate the
Lin similarity measure (Lin, 1998). The syntax-
based features use the syntactic structure, which
is a mix of phrase-based and dependency-based
analysis. The phrasal features express similar-
ity between the terminal yields of source and tar-
get nodes. With the exception of same-parent-lc-
phrase, these features are only used for full tree
alignment, not for word alignment.
6.2 Results on word alignment
We evaluate our alignment model in two steps:
first focussing on word alignment and then on full
tree alignment. Table 5 summarizes the results for
MBGM on word alignment (50% downsampling
and k = 3), which we compare statistically to the
baseline performance, and informally with the hu-
man scores reported in Table 1 in Section 4 (note
that the human scores are only for a subset of the
data used for automatic evaluation).
The first thing to observe is that the MBGM
scores on the development and tests sets are
very similar throughout. For predicting word
alignments, the MBGM system performs signif-
icantly better than the baseline system (t(18) =
17.72, p < .0001). On the test set, MBGM ob-
tains an F-score of nearly 89%, which is almost
exactly halfway between the scores of the base-
line system and the human scores. In a similar
vein, the performance of the MBGM system on
relation labeling is considerably better than that
of the baseline system. For all semantic rela-
tions, MBGM performs significantly better than
the baseline (t(18) > 9.4138, p < .0001 for each
relation, trivially so for the Specifies, Generalizes
and Intersects relations, which the baseline system
never predicts).
The macro scores are plain averages over the 5
scores on each relation, whereas the micro scores
are weighted averages. As the Equals is the major-
ity class and at the same time easiest to predict, the
micro scores are higher. The macro scores, how-
ever, better reflect performance on the real chal-
lenge, that is, correctly predicting the relations
other than Equals. The MBGM macro average
is 27.37% higher than the baseline (but still some
10% below the human top line), while the micro
average is 5.83% higher and only 0.75% below
the human top line. Macro scores on the test set
are overall lower than those on the develop set,
presumably because of tuning on the development
data.
6.3 Results on tree alignment
Table 6 contains the results of full tree alignment
(50% downsampling and k = 5); here both termi-
nal and non-terminal nodes are aligned and clas-
sified in one pass. Again scores on the develop-
ment and test set are very similar, the latter being
slightly better. For full tree alignment, MBGM
once again performs significantly better than the
baseline, t(18) = 25.68, p < .0001. With an F-
score on the test set of 86.65, MBGM scores al-
most 20 percent higher than the baseline system.
This F-score is less than 2% lower than the aver-
age F-score obtained by our human annotators on
full tree alignment, albeit not on exactly the same
sample. The picture that emerges for semantic re-
lation labeling is closely related to the one we saw
for word alignments. MBGM significantly out-
performs the baseline, for each semantic relation
(t(18) > 12.6636, p < .0001). MBGM scores a
macro average F-score of 52.24% (an increase of
30.05% over the baseline) and a micro average of
80.03% (12.68% above the base score). It is inter-
757
Feature Type Description
Word
word-subsumption string indicate if source word equals, has as prefix, is a prefix of, has a suffix, is a
suffix of, has as infix or is an infix of target word
shared-pre-/in-/suffix-len int length of shared prefix/infix/suffix in characters
source/target-stop-word bool test if source/target word is in a stop word list of frequent function words
source/target-word-len int length of source/target word in characters
word-len-diff int word length difference in characters
source/target-word-uniq bool test if source/target word is unique in source/target sentence
same-words-lhs/rhs int no. of identical preceding/following words in source and target word contexts
Morphology
root-subsumption string indicate if source root equals, has as prefix, is a prefix of, has a suffix, is a suffix
of, has as infix or is an infix of target root
roots-share-pre-/in-/suffix bool source and target root share a prefix/infix/suffix
Part-of-speech
source/target-pos string source/target part-of-speech
same-pos bool test if source and target have same part-of-speech
source/target-content-word bool test if source/target word is a content word
both-content-word bool test if both source and target word are content words
Lexical-semantic using Cornetto
cornet-restates float 1.0 if source and target words are synonyms and 0.5 if they are near-synonyms,
zero otherwise
cornet-specifies float Lin similarity score if source word is a hyponym of target word, zero otherwise
cornet-generalizes float Lin similarity score if source word is a hypernym of target word, zero otherwise
cornet-intersects float Lin similarity score if source word share a common hypernym, zero otherwise
Syntax
source/target-cat string source/target syntactic category
same-cat bool test if source and target have same syntactic category
source/target-parent-cat string source/target syntactic category of parent node
source/target-deprel string source/target dependency relation
same-deprel bool test if source and target have same dependency relation
same-dephead-root bool test if the dependency heads of the source and target have same root
Phrasal
word-prec/rec float precision/recall on the yields of source and target nodes
same-lc-phrase bool test if lower-cased yields of source and target nodes are identical
same-parent-lc-phrase bool test if lower-cased yields of parents of source and target nodes are identical
source/target-phrase-len int length of source/target phrase in words
phrase-len-diff int phrase length difference in words
Table 4: Features (where slashes indicate multiple versions of the same feature, e.g. source/target-pos
represents the two features source-pos and target-pos)
esting to observe that MBGM obtains higher F-
scores on Equals and on Intersects (the two most
frequent relations) than the human annotators ob-
tained. As a result of this, the micro F-score of
the automatic full tree alignment is less than 2%
lower than the human reference score.
Tree alignment can also be implemented as a
two-step procedure, where in the first step align-
ments and semantic relation classifications at the
word level are produced, while in the second step
these are used to predict alignments and seman-
tic relations for non-terminals. We experimented
with such a two-step procedure as well, in one ver-
sion using the actual word alignments and in the
other the predicted word alignments. The scores
of the two-step prediction are only marginally dif-
ferent from those of one step prediction, both for
alignment and for relation classification, giving
improvements in the order of about 1% for both
subtasks. As is to be expected, the scores with
true word alignments are much better than those
with predicted word alignments. They are inter-
esting though, because they suggest that a fairly
good full tree alignment can be automatically ob-
758
Alignment: Labeling:
Eq: Re: Spec: Gen: Int: Macro: Micro:
Prec: 80.59 81.84 46.26 0.00 0.00 0.00 25.61 80.22
Develop baseline: Rec: 81.58 93.10 34.71 0.00 0.00 0.00 25.56 82.20
F: 81.08 87.11 39.66 0.00 0.00 0.00 25.35 80.70
Prec: 91.72 94.54 61.26 74.60 67.82 45.80 68.80 90.82
Develop MBGM: Rec: 87.82 95.91 46.19 40.87 43.22 27.27 50.61 86.96
F: 89.73 95.02 52.67 52.81 52.80 34.19 57.50 88.85
Prec: 82.45 83.83 43.12 0.00 0.00 0.00 25.39 82.17
Test baseline: Rec: 82.19 93.87 27.01 0.00 0.00 0.00 24.18 82.02
F: 82.32 88.57 33.22 0.00 0.00 0.00 24.36 82.14
Prec: 90.92 94.20 53.33 59.87 54.21 42.47 60.84 89.90
Test MBGM: Rec: 87.09 95.41 40.21 32.75 43.28 20.31 46.39 86.11
F: 88.96 94.80 45.85 42.34 48.17 27.48 51.73 87.97
Table 5: Scores (in percentages) on word alignment and semantic relation labeling
Alignment: Labeling:
Eq: Re: Spec: Gen: Int: Macro: Micro:
Prec: 82.50 83.76 46.72 0.00 0.00 0.00 26.10 82.18
Develop baseline: Rec: 54.54 93.66 20.01 0.00 0.00 0.00 22.74 54.34
F: 65.67 88.43 28.02 0.00 0.00 0.00 23.29 65.42
Prec: 92.23 96.15 55.90 54.40 56.15 70.33 66.59 84.99
Develop MBGM: Rec: 81.04 94.03 26.64 21.71 29.34 70.27 48.40 74.68
F: 86.27 95.08 36.08 31.03 38.54 70.30 54.21 79.50
Prec: 84.23 85.68 42.24 0.00 0.00 0.00 25.58 84.14
Test baseline: Rec: 56.21 94.44 14.08 0.00 0.00 0.00 21.70 56.15
F: 67.43 89.85 21.12 0.00 0.00 0.00 22.19 67.35
Prec: 92.27 96.67 60.25 46.92 56.85 68.64 65.87 85.23
Test MBGM: Rec: 81.67 94.54 27.87 19.55 30.94 71.01 48.87 75.44
F: 86.65 95.60 38.11 27.60 40.07 69.80 54.24 80.03
Table 6: Scores (in percentages) on full tree alignment and semantic relation labeling
tained given a manually checked word alignment.
7 Conclusions
We have proposed to analyse semantic similarity
between comparable sentences by aligning their
syntax trees, matching each node to the most sim-
ilar node in the other tree (if any). In addi-
tion, alignments are labeled with a semantic sim-
ilarity relation. We have presented a Memory-
based Graph Matcher (MBGM) that performs
both tasks simultaneously as a combination of ex-
haustive pairwise classification using a memory-
based learning algorithm, and global optimization
of alignments using a combinatorial optimization
algorithm. It relies on a combination of morpho-
logical/syntactic analysis, lexical resources such
as word nets, and machine learning using a par-
allel monolingual treebank. Results on aligning
comparable news texts from a monolingual paral-
lel treebank for Dutch show that MBGM consis-
tently and significantly outperforms the baseline,
both for alignment and labeling. This holds both
for word alignment and tree alignment.
In future research we will test MBGM on other
data, as the DAESO corpus contains sub-corpora
with various degrees of semantic overlap. In addi-
tion, we intend to explore alternative features from
word space models. Finally, we plan to evaluate
MBGM in the context of NLP applications such
as multi-document summarization. This includes
work on how to define similarity at the sentence
level in terms of the proportion of aligned con-
stituents. Both MBGM and the annotated data set
will be publicly released.2
759
Acknowledgments
This work was conducted within the DAESO
project funded by the Stevin program (De Ned-
erlandse Taalunie).
References
Barzilay, Regina and Kathleen R. McKeown. 2005. Sen-
tence fusion for multidocument news summarization.
Computational Linguistics, 31(3):297?328.
Bouma, Gosse, Gertjan van Noord, and Robert Malouf.
2001. Alpino: Wide-coverage computational analysis of
Dutch. In Daelemans, Walter, Khalil Sima?an, Jorn Veen-
stra, and Jakub Zavre, editors, Computational Linguistics
in the Netherlands 2000., pages 45?59. Rodopi, Amster-
dam, New York.
Daelemans, W., J. Zavrel, K. Van der Sloot, and
A. Van den Bosch. 2009. TiMBL: Tilburg Memory
Based Learner, version 6.2, reference manual. Technical
Report ILK 09-01, Induction of Linguistic Knowledge,
Tilburg University.
Dagan, I., O. Glickman, and B. Magnini. 2005. The PAS-
CAL Recognising Textual Entailment Challenge. In Pro-
ceedings of the PASCAL Challenges Workshop on Recog-
nising Textual Entailment, Southampton, U.K.
Gildea, Daniel. 2003. Loosely tree-based alignment for
machine translation. In Proceedings of the 41st Annual
Meeting on Association for Computational Linguistics,
pages 80?87, Sapporo, Japan.
Groves, D., M. Hearne, and A. Way. 2004. Robust sub-
sentential alignment of phrase-structure trees. In Pro-
ceedings of the 20th International Conference on Com-
putational Linguistics (CoLing ?04), pages 1072?1078.
Krahmer, Emiel, Erwin Marsi, and Paul van Pelt. 2008.
Query-based sentence fusion is better defined and leads
to more preferred results than generic sentence fusion. In
Moore, J., S. Teufel, J. Allan, and S. Furui, editors, Pro-
ceedings of the 46th Annual Meeting of the Association
for Computational Linguistics: Human Language Tech-
nologies, pages 193?196, Columbus, Ohio, USA.
Kuhn, Harold W. 1955. The Hungarian Method for the as-
signment problem. Naval Research Logistics Quarterly,
2:83?97.
Lavie, A., A. Parlikar, and V. Ambati. 2008. Syntax-
driven learning of sub-sentential translation equivalents
and translation rules from parsed parallel corpora. In Pro-
ceedings of the Second Workshop on Syntax and Structure
in Statistical Translation, pages 87?95.
Lin, D. 1998. An information-theoretic definition of similar-
ity. In Proceedings of the 15th International Conference
on Machine Learning, pages 296?304.
MacCartney, B. and C.D. Manning. 2008. Modeling seman-
tic containment and exclusion in natural language infer-
ence. In Proceedings of the 22nd International Confer-
ence on Computational Linguistics-Volume 1, pages 521?
528.
MacCartney, Bill, Michel Galley, and Christopher D. Man-
ning. 2008. A phrase-based alignment model for natural
language inference. In Proceedings of the 2008 Confer-
ence on Empirical Methods in Natural Language Process-
ing, pages 802?811, Honolulu, Hawaii, October.
Marsi, Erwin and Emiel Krahmer. 2005a. Classification of
semantic relations by humans and machines. In Proceed-
ings of the ACL 2005 workshop on Empirical Modeling
of Semantic Equivalence and Entailment, pages 1?6, Ann
Arbor, Michigan.
Marsi, Erwin and Emiel Krahmer. 2005b. Explorations in
sentence fusion. In Proceedings of the 10th European
Workshop on Natural Language Generation, Aberdeen,
GB.
Radev, D.R. and K.R. McKeown. 1998. Generating natural
language summaries from multiple on-line sources. Com-
putational Linguistics, 24(3):469?500.
Tiedemann, J. and G. Kotze?. 2009. Building a Large
Machine-Aligned Parallel Treebank. In Eighth Interna-
tional Workshop on Treebanks and Linguistic Theories,
page 197.
Tinsley, J., V. Zhechev, M. Hearne, and A. Way. 2007. Ro-
bust language-pair independent sub-tree alignment. Ma-
chine Translation Summit XI, pages 467?474.
Vossen, P., I. Maks, R. Segers, and H. van der Vliet. 2008.
Integrating lexical units, synsets and ontology in the Cor-
netto Database. In Proceedings of LREC 2008, Mar-
rakech, Morocco.
760
Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 1: Proceedings of the Main Conference
and the Shared Task, pages 66?73, Atlanta, Georgia, June 13-14, 2013. c?2013 Association for Computational Linguistics
NTNU-CORE: Combining strong features for semantic similarity
Erwin Marsi, Hans Moen, Lars Bungum, Gleb Sizov, Bjo?rn Gamba?ck, Andre? Lynum
Norwegian University of Science and Technology
Department of Computer and Information and Science
Sem S?lands vei 7-9
NO-7491 Trondheim, Norway
{emarsi,hansmoe,bungum,sizov,gamback,andrely}@idi.ntnu.no
Abstract
The paper outlines the work carried out at
NTNU as part of the *SEM?13 shared task
on Semantic Textual Similarity, using an ap-
proach which combines shallow textual, dis-
tributional and knowledge-based features by
a support vector regression model. Feature
sets include (1) aggregated similarity based
on named entity recognition with WordNet
and Levenshtein distance through the calcula-
tion of maximum weighted bipartite graphs;
(2) higher order word co-occurrence simi-
larity using a novel method called ?Multi-
sense Random Indexing?; (3) deeper seman-
tic relations based on the RelEx semantic
dependency relationship extraction system;
(4) graph edit-distance on dependency trees;
(5) reused features of the TakeLab and DKPro
systems from the STS?12 shared task. The
NTNU systems obtained 9th place overall (5th
best team) and 1st place on the SMT data set.
1 Introduction
Intuitively, two texts are semantically similar if they
roughly mean the same thing. The task of formally
establishing semantic textual similarity clearly is
more complex. For a start, it implies that we have
a way to formally represent the intended meaning of
all texts in all possible contexts, and furthermore a
way to measure the degree of equivalence between
two such representations. This goes far beyond the
state-of-the-art for arbitrary sentence pairs, and sev-
eral restrictions must be imposed. The Semantic
Textual Similarity (STS) task (Agirre et al, 2012,
2013) limits the comparison to isolated sentences
only (rather than complete texts), and defines sim-
ilarity of a pair of sentences as the one assigned by
human judges on a 0?5 scale (with 0 implying no
relation and 5 complete semantic equivalence). It is
unclear, however, to what extent two judges would
agree on the level of similarity between sentences;
Agirre et al (2012) report figures on the agreement
between the authors themselves of about 87?89%.
As in most language processing tasks, there are
two overall ways to measure sentence similarity, ei-
ther by data-driven (distributional) methods or by
knowledge-driven methods; in the STS?12 task the
two approaches were used nearly equally much.
Distributional models normally measure similarity
in terms of word or word co-occurrence statistics, or
through concept relations extracted from a corpus.
The basic strategy taken by NTNU in the STS?13
task was to use something of a ?feature carpet bomb-
ing approach? in the way of first automatically ex-
tracting as many potentially useful features as possi-
ble, using both knowledge and data-driven methods,
and then evaluating feature combinations on the data
sets provided by the organisers of the shared task.
To this end, four different types of features were
extracted. The first (Section 2) aggregates similar-
ity based on named entity recognition with WordNet
and Levenshtein distance by calculating maximum
weighted bipartite graphs. The second set of features
(Section 3) models higher order co-occurrence sim-
ilarity relations using Random Indexing (Kanerva
et al, 2000), both in the form of a (standard) sliding
window approach and through a novel method called
?Multi-sense Random Indexing? which aims to sep-
arate the representation of different senses of a term
66
from each other. The third feature set (Section 4)
aims to capture deeper semantic relations using ei-
ther the output of the RelEx semantic dependency
relationship extraction system (Fundel et al, 2007)
or an in-house graph edit-distance matching system.
The final set (Section 5) is a straight-forward gath-
ering of features from the systems that fared best in
STS?12: TakeLab from University of Zagreb (S?aric?
et al, 2012) and DKPro from Darmstadt?s Ubiqui-
tous Knowledge Processing Lab (Ba?r et al, 2012).
As described in Section 6, Support Vector Regres-
sion (Vapnik et al, 1997) was used for solving the
multi-dimensional regression problem of combining
all the extracted feature values. Three different sys-
tems were created based on feature performance on
the supplied development data. Section 7 discusses
scores on the STS?12 and STS?13 test data.
2 Compositional Word Matching
Compositional word matching similarity is based
on a one-to-one alignment of words from the two
sentences. The alignment is obtained by maximal
weighted bipartite matching using several word sim-
ilarity measures. In addition, we utilise named entity
recognition and matching tools. In general, the ap-
proach is similar to the one described by Karnick
et al (2012), with a different set of tools used. Our
implementation relies on the ANNIE components in
GATE (Cunningham et al, 2002) and will thus be
referred to as GateWordMatch.
The processing pipeline for GateWordMatch
is: (1) tokenization by ANNIE English Tokeniser,
(2) part-of-speech tagging by ANNIE POS Tagger,
(3) lemmatization by GATE Morphological Anal-
yser, (4) stopword removal, (5) named entity recog-
nition based on lists by ANNIE Gazetteer, (6) named
entity recognition based on the JAPE grammar by
the ANNIE NE Transducer, (7) matching of named
entities by ANNIE Ortho Matcher, (8) computing
WordNet and Levenstein similarity between words,
(9) calculation of a maximum weighted bipartite
graph matching based on similarities from 7 and 8.
Steps 1?4 are standard preprocessing routines.
In step 5, named entities are recognised based on
lists that contain locations, organisations, compa-
nies, newspapers, and person names, as well as date,
time and currency units. In step 6, JAPE grammar
rules are applied to recognise entities such as ad-
dresses, emails, dates, job titles, and person names
based on basic syntactic and morphological features.
Matching of named entities in step 7 is based on
matching rules that check the type of named entity,
and lists with aliases to identify entities as ?US?,
?United State?, and ?USA? as the same entity.
In step 8, similarity is computed for each pair
of words from the two sentences. Words that are
matched as entities in step 7 get a similarity value
of 1.0. For the rest of the entities and non-entity
words we use LCH (Leacock and Chodorow, 1998)
similarity, which is based on a shortest path between
the corresponding senses in WordNet. Since word
sense disambiguation is not used, we take the simi-
larity between the nearest senses of two words. For
cases when the WordNet-based similarity cannot be
obtained, a similarity based on the Levenshtein dis-
tance (Levenshtein, 1966) is used instead. It is nor-
malised by the length of the longest word in the pair.
For the STS?13 test data set, named entity matching
contributed to 4% of all matched word pairs; LCH
similarity to 61%, and Levenshtein distance to 35%.
In step 9, maximum weighted bipartite matching
is computed using the Hungarian Algorithm (Kuhn,
1955). Nodes in the bipartite graph represent words
from the sentences, and edges have weights that cor-
respond to similarities between tokens obtained in
step 8. Weighted bipartite matching finds the one-to-
one alignment that maximizes the sum of similarities
between aligned tokens. Total similarity normalised
by the number of words in both sentences is used as
the final sentence similarity measure.
3 Distributional Similarity
Our distributional similarity features use Random
Indexing (RI; Kanerva et al, 2000; Sahlgren, 2005),
also employed in STS?12 by Tovar et al (2012);
Sokolov (2012); Semeraro et al (2012). It is an
efficient method for modelling higher order co-
occurrence similarities among terms, comparable to
Latent Semantic Analysis (LSA; Deerwester et al,
1990). It incrementally builds a term co-occurrence
matrix of reduced dimensionality through the use of
a sliding window and fixed size index vectors used
for training context vectors, one per unique term.
A novel variant, which we have called ?Multi-
67
sense Random Indexing? (MSRI), inspired by
Reisinger and Mooney (2010), attempts to capture
one or more ?senses? per unique term in an unsu-
pervised manner, each sense represented as an indi-
vidual vector in the model. The method is similar to
classical sliding window RI, but each term can have
multiple context vectors (referred to as ?sense vec-
tors? here) which are updated individually. When
updating a term vector, instead of directly adding the
index vectors of the neighbouring terms in the win-
dow to its context vector, the system first computes a
separate window vector consisting of the sum of the
index vectors. Then cosine similarity is calculated
between the window vector and each of the term?s
sense vectors. Each similarity score is in turn com-
pared to a set similarity threshold: if no score ex-
ceeds the threshold, the sentence vector is added as
a new separate sense vector for the term; if exactly
one score is above the threshold, the window vector
is added to that sense vector; and if multiple scores
are above the threshold, all the involved senses are
merged into one sense vector, together with the win-
dow vector. This accomplishes an incremental clus-
tering of senses in an unsupervised manner while re-
taining the efficiency of classical RI.
As data for training the models we used the
CLEF 2004?2008 English corpus (approx. 130M
words). Our implementation of RI and MSRI is
based on JavaSDM (Hassel, 2004). For classical
RI, we used stopword removal (using a customised
versions of the English stoplist from the Lucene
project), window size of 4+4, dimensionality set to
1800, 4 non-zeros, and unweighted index vector in
the sliding window. For MSRI, we used a simi-
larity threshold of 0.2, a vector dimensionality of
800, a non-zero count of 4, and window size of
5+5. The index vectors in the sliding window were
shifted to create direction vectors (Sahlgren et al,
2008), and weighted by distance to the target term.
Rare senses with a frequency below 10 were ex-
cluded. Other sliding-window schemes, including
unweighted non-shifted vectors and Random Permu-
tation (Sahlgren et al, 2008), were tested, but none
outperformed the sliding-window schemes used.
Similarity between sentence pairs was calcu-
lated as the normalised maximal bipartite similar-
ity between term pairs in each sentence, resulting
in the following features: (1) MSRI-Centroid:
each term is represented as the sum of its sense
vectors; (2) MSRI-MaxSense: for each term
pair, the sense-pair with max similarity is used;
(3) MSRI-Context: for each term, its neigh-
bouring terms within a window of 2+2 is used as
context for picking a single, max similar, sense
from the target term to be used as its represen-
tation; (4) MSRI-HASenses: similarity between
two terms is computed by applying the Hungarian
Algorithm to all their possible sense pair mappings;
(5) RI-Avg: classical RI, each term is represented
as a single context vector; (6) RI-Hungarian:
similarity between two sentences is calculated us-
ing the Hungarian Algorithm. Alternatively, sen-
tence level similarity was computed as the cosine
similarity between sentence vectors composed of
their terms? vectors. The corresponding features
are (1) RI-SentVectors-Norm: sentence vec-
tors are created by summing their constituent terms
(i.e., context vectors), which have first been normal-
ized; (2) RI-SentVectors-TFIDF: same as be-
fore, but TF*IDF weights are added.
4 Deeper Semantic Relations
Two deep strategies were employed to accompany
the shallow-processed feature sets. Two existing
systems were used to provide the basis for these fea-
tures, namely the RelEx system (Fundel et al, 2007)
from the OpenCog initiative (Hart and Goertzel,
2008), and an in-house graph-edit distance system
developed for plagiarism detection (R?kenes, 2013).
RelEx outputs syntactic trees, dependency graphs,
and semantic frames as this one for the sentence
?Indian air force to buy 126 Rafale fighter jets?:
Commerce buy:Goods(buy,jet)
Entity:Entity(jet,jet)
Entity:Name(jet,Rafale)
Entity:Name(jet,fighter)
Possibilities:Event(hyp,buy)
Request:Addressee(air,you)
Request:Message(air,air)
Transitive action:Beneficiary(buy,jet)
Three features were extracted from this: first, if
there was an exact match of the frame found in s1
with s2; second, if there was a partial match until the
first argument (Commerce buy:Goods(buy);
and third if there was a match of the frame category
68
(Commerce buy:Goods).
In STS?12, Singh et al (2012) matched Universal
Networking Language (UNL) graphs against each
other by counting matches of relations and univer-
sal words, while Bhagwani et al (2012) calculated
WordNet-based word-level similarities and created
a weighted bipartite graph (see Section 2). The
method employed here instead looked at the graph
edit distance between dependency graphs obtained
with the Maltparser dependency parser (Nivre et al,
2006). Edit distance is the defined as the minimum
of the sum of the costs of the edit operations (in-
sertion, deletion and substitution of nodes) required
to transform one graph into the other. It is approx-
imated with a fast but suboptimal algorithm based
on bipartite graph matching through the Hungarian
algorithm (Riesen and Bunke, 2009).
5 Reused Features
The TakeLab ?simple? system (S?aric? et al, 2012) ob-
tained 3rd place in overall Pearson correlation and
1st for normalized Pearson in STS?12. The source
code1 was used to generate all its features, that is,
n-gram overlap, WordNet-augmented word overlap,
vector space sentence similarity, normalized differ-
ence, shallow NE similarity, numbers overlap, and
stock index features.2 This required the full LSA
vector space models, which were kindly provided
by the TakeLab team. The word counts required for
computing Information Content were obtained from
Google Books Ngrams.3
The DKPro system (Ba?r et al, 2012) obtained first
place in STS?12 with the second run. We used the
source code4 to generate features for the STS?12
and STS?13 data. Of the string-similarity features,
we reused the Longest Common Substring, Longest
Common Subsequence (with and without normaliza-
tion), and Greedy String Tiling measures. From the
character/word n-grams features, we used Charac-
ter n-grams (n = 2, 3, 4), Word n-grams by Con-
tainment w/o Stopwords (n = 1, 2), Word n-grams
1http://takelab.fer.hr/sts/
2We did not use content n-gram overlap or skip n-grams.
3http://storage.googleapis.com/books/
ngrams/books/datasetsv2.html, version 20120701,
with 468,491,999,592 words
4http://code.google.com/p/
dkpro-similarity-asl/
by Jaccard (n = 1, 3, 4), and Word n-grams by Jac-
card w/o Stopwords (n = 2, 4). Semantic similarity
measures include WordNet Similarity based on the
Resnik measure (two variants) and Explicit Seman-
tic Similarity based on WordNet, Wikipedia or Wik-
tionary. This means that we reused all features from
DKPro run 1 except for Distributional Thesaurus.
6 Systems
Our systems follow previous submissions to the STS
task (e.g., S?aric? et al, 2012; Banea et al, 2012) in
that feature values are extracted for each sentence
pair and combined with a gold standard score in or-
der to train a Support Vector Regressor on the result-
ing regression task. A postprocessing step guaran-
tees that all scores are in the [0, 5] range and equal 5
if the two sentences are identical. SVR has been
shown to be a powerful technique for predictive data
analysis when the primary goal is to approximate a
function, since the learning algorithm is applicable
to continuous classes. Hence support vector regres-
sion differs from support vector machine classifica-
tion where the goal rather is to take a binary deci-
sion. The key idea in SVR is to use a cost function
for building the model which tries to ignore noise in
training data (i.e., data which is too close to the pre-
diction), so that the produced model in essence only
depends on a more robust subset of the extracted fea-
tures.
Three systems were created using the supplied
annotated data based on Microsoft Research Para-
phrase and Video description corpora (MSRpar and
MSvid), statistical machine translation system out-
put (SMTeuroparl and SMTnews), and sense map-
pings between OntoNotes and WordNet (OnWN).
The first system (NTNU1) includes all TakeLab and
DKPro features plus the GateWordMatch feature
with the SVR in its default setting.5 The training
material consisted of all annotated data available,
except for the SMT test set, where it was limited to
SMTeuroparl and SMTnews. The NTNU2 system is
similar to NTNU1, except that the training material
for OnWN and FNWN excluded MSRvid and that
the SVR parameter C was set to 200. NTNU3 is
similar to NTNU1 except that all features available
are included.
5RBF kernel,  = 0.1, C = #samples, ? = 1#features
69
Data NTNU1 NTNU2 NTNU3
MSRpar 0.7262 0.7507 0.7221
MSRvid 0.8660 0.8882 0.8662
SMTeuroparl 0.5843 0.3386 0.5503
SMTnews 0.5840 0.5592 0.5306
OnWN 0.7503 0.6365 0.7200
mean 0.7022 0.6346 0.6779
Table 1: Correlation score on 2012 test data
7 Results
System performance is evaluated using the Pearson
product-moment correlation coefficient (r) between
the system scores and the human scores. Results on
the 2012 test data (i.e., 2013 development data) are
listed in Table 1. This basically shows that except
for the GateWordMatch, adding our other fea-
tures tends to give slightly lower scores (cf. NTNU1
vs NTNU3). In addition, the table illustrates that op-
timizing the SVR according to cross-validated grid
search on 2012 training data (here C = 200), rarely
pays off when testing on unseen data (cf. NTNU1
vs NTNU2).
Table 2 shows the official results on the test data.
These are generally in agreement with the scores on
the development data, although substantially lower.
Our systems did particularly well on SMT, holding
first and second position, reasonably good on head-
lines, but not so well on the ontology alignment data,
resulting in overall 9th (NTNU1) and 12th (NTNU3)
system positions (5th best team). Table 3 lists the
correlation score and rank of the ten best individual
features per STS?13 test data set, and those among
the top-20 overall, resulting from linear regression
on a single feature. Features in boldface are gen-
uinely new (i.e., described in Sections 2?4).
Overall the character n-gram features are the most
informative, particularly for HeadLine and SMT.
The reason may be that these not only capture word
overlap (Ahn, 2011), but also inflectional forms and
spelling variants.
The (weighted) distributional similarity features
based on NYT are important for HeadLine and SMT,
which obviously contain sentence pairs from the
news genre, whereas the Wikipedia based feature is
more important for OnWN and FNWN. WordNet-
based measures are highly relevant too, with variants
NTNU1 NTNU2 NTNU3
Data r n r n r n
Head 0.7279 11 0.5909 59 0.7274 12
OnWN 0.5952 31 0.1634 86 0.5882 32
FNWN 0.3215 45 0.3650 27 0.3115 49
SMT 0.4015 2 0.3786 9 0.4035 1
mean 0.5519 9 0.3946 68 0.5498 12
Table 2: Correlation score and rank on 2013 test data
relying on path length outperforming those based on
Resnik similarity, except for SMT.
As is to be expected, basic word and lemma uni-
gram overlap prove to be informative, with overall
unweighted variants resulting in higher correlation.
Somewhat surprisingly, higher order n-gram over-
laps (n > 1) seem to be less relevant. Longest com-
mon subsequence and substring appear to work par-
ticularly well for OnWN and FNWN, respectively.
GateWordMatch is highly relevant too, in
agreement with earlier results on the development
data. Although treated as a single feature, it is ac-
tually a combination of similarity features where an
appropriate feature is selected for each word pair.
This ?vertical? way of combining features can po-
tentially provide a more fine-grained feature selec-
tion, resulting in less noise. Indeed, if two words are
matching as named entities or as close synonyms,
less precise types of features such as character-based
and data-driven similarity should not dominate the
overall similarity score.
It is interesting to find that MSRI outper-
forms both classical RI and ESA (Gabrilovich and
Markovitch, 2007) on this task. Still, the more ad-
vanced features, such as MSRI-Context, gave in-
ferior results compared to MSRI-Centroid. This
suggests that more research on MSRI is needed
to understand how both training and retrieval can
be optimised. Also, LSA-based features (see
tl.weight-dist-sim-wiki) achieve better
results than both MSRI, RI and ESA. Then again,
larger corpora were used for training the LSA mod-
els. RI has been shown to be comparable to LSA
(Karlgren and Sahlgren, 2001), and since a relatively
small corpus was used for training the RI/MSRI
models, there are reasons to believe that better
scores can be achieved by both RI- and MSRI-based
features by using more training data.
70
HeadLine OnWN FNWN SMT Mean
Features r n r n r n r n r n
CharacterNGramMeasure-3 0.72 2 0.39 2 0.44 3 0.70 1 0.56 1
CharacterNGramMeasure-4 0.69 3 0.38 5 0.45 2 0.67 6 0.55 2
CharacterNGramMeasure-2 0.73 1 0.37 9 0.34 10 0.69 2 0.53 3
tl.weight-dist-sim-wiki 0.58 14 0.39 3 0.45 1 0.67 5 0.52 4
tl.wn-sim-lem 0.69 4 0.40 1 0.41 5 0.59 10 0.52 5
GateWordMatch 0.67 8 0.37 11 0.34 11 0.60 9 0.50 6
tl.dist-sim-nyt 0.69 5 0.34 28 0.26 23 0.65 8 0.49 7
tl.n-gram-match-lem-1 0.68 6 0.36 16 0.37 8 0.51 14 0.48 8
tl.weight-dist-sim-nyt 0.57 17 0.37 14 0.29 18 0.66 7 0.47 9
tl.n-gram-match-lc-1 0.68 7 0.37 10 0.32 13 0.50 17 0.47 10
MCS06-Resnik-WordNet 0.49 26 0.36 22 0.28 19 0.68 3 0.45 11
TWSI-Resnik-WordNet 0.49 27 0.36 23 0.28 20 0.68 4 0.45 12
tl.weight-word-match-lem 0.56 18 0.37 16 0.37 7 0.50 16 0.45 13
MSRI-Centroid 0.60 13 0.36 17 0.37 9 0.45 19 0.45 14
tl.weight-word-match-olc 0.56 19 0.38 8 0.32 12 0.51 15 0.44 15
MSRI-MaxSense 0.58 15 0.36 15 0.31 14 0.45 20 0.42 16
GreedyStringTiling-3 0.67 9 0.38 6 0.31 15 0.34 29 0.43 17
ESA-Wikipedia 0.50 25 0.30 38 0.32 14 0.54 12 0.42 18
WordNGramJaccard-1 0.64 10 0.37 12 0.25 25 0.33 30 0.40 19
WordNGramContainment-1-stopword 0.64 25 0.38 7 0.25 24 0.32 31 0.40 20
RI-Hungarian 0.58 16 0.33 31 0.10 34 0.42 22 0.36 24
RI-AvgTermTerm 0.56 20 0.33 32 0.11 33 0.37 28 0.34 25
LongestCommonSubstring 0.40 29 0.30 39 0.42 4 0.37 27 0.37 26
ESA-WordNet 0.11 43 0.30 40 0.41 6 0.49 18 0.33 29
LongestCommonSubsequenceNorm 0.53 21 0.39 4 0.19 27 0.18 37 0.32 30
MultisenseRI-ContextTermTerm 0.39 31 0.33 33 0.28 21 0.15 38 0.29 33
MultisenseRI-HASensesTermTerm 0.39 32 0.33 34 0.28 22 0.15 39 0.29 34
RI-SentVectors-Norm 0.34 35 0.35 26 -0.01 51 0.24 35 0.23 39
RelationSimilarity 0.31 39 0.35 27 0.24 26 0.02 41 0.23 40
RI-SentVectors-TFIDF 0.27 40 0.15 50 0.08 40 0.23 36 0.18 41
GraphEditDistance 0.33 38 0.25 46 0.13 31 -0.11 49 0.15 42
Table 3: Correlation score and rank of the best features
8 Conclusion and Future Work
The NTNU system can be regarded as continuation
of the most successful systems from the STS?12
shared task, combining shallow textual, distribu-
tional and knowledge-based features into a support
vector regression model. It reuses features from the
TakeLab and DKPro systems, resulting in a very
strong baseline.
Adding new features to further improve
performance turned out to be hard: only
GateWordMatch yielded improved perfor-
mance. Similarity features based on both classical
and innovative variants of Random Indexing were
shown to correlate with semantic textual similarity,
but did not complement the existing distributional
features. Likewise, features designed to reveal
deeper syntactic (graph edit distance) and semantic
relations (RelEx) did not add to the score.
As future work, we would aim to explore a
vertical feature composition approach similar to
GateWordMatch and contrast it with the ?flat?
composition currently used in our systems.
Acknowledgements
Thanks to TakeLab for source code of their ?simple?
system and the full-scale LSA models. Thanks to the
team from Ubiquitous Knowledge Processing Lab
for source code of their DKPro Similarity system.
71
References
Agirre, E., Cer, D., Diab, M., and Gonzalez-Agirre,
A. (2012). SemEval-2012 Task 6: A pilot on se-
mantic textual similarity. In *SEM (2012), pages
385?393.
Agirre, E., Cer, D., Diab, M., Gonzalez-Agirre, A.,
and Guo, W. (2013). *SEM 2013 Shared Task:
Semantic textual similarity, including a pilot on
typed-similarity. In *SEM 2013: The Second
Joint Conference on Lexical and Computational
Semantics. Association for Computational Lin-
guistics.
Ahn, C. S. (2011). Automatically detecting authors?
native language. PhD thesis, Monterey, Califor-
nia. Naval Postgraduate School.
Banea, C., Hassan, S., Mohler, M., and Mihalcea, R.
(2012). UNT: a supervised synergistic approach
to semantic text similarity. In *SEM (2012),
pages 635?642.
Ba?r, D., Biemann, C., Gurevych, I., and Zesch, T.
(2012). UKP: Computing semantic textual sim-
ilarity by combining multiple content similarity
measures. In *SEM (2012), pages 435?440.
Bhagwani, S., Satapathy, S., and Karnick, H. (2012).
sranjans : Semantic textual similarity using maxi-
mal weighted bipartite graph matching. In *SEM
2012: The First Joint Conference on Lexical and
Computational Semantics ? Volume 1: Proceed-
ings of the main conference and the shared task,
and Volume 2: Proceedings of the Sixth Interna-
tional Workshop on Semantic Evaluation (Sem-
Eval 2012), pages 579?585, Montre?al, Canada.
Association for Computational Linguistics.
Cunningham, H., Maynard, D., Bontcheva, K., and
Tablan, V. (2002). GATE: A framework and
graphical development environment for robust
NLP tools and applications. In Proceedings of the
40th Annual Meeting of the Association for Com-
putational Linguistics, pages 168?175, Philadel-
phia, Pennsylvania. ACL.
Deerwester, S., Dumais, S., Furnas, G., Landauer,
T., and Harshman, R. (1990). Indexing by latent
semantic analysis. Journal of the American Soci-
ety for Information Science, 41(6):391?407.
Fundel, K., Ku?ffner, R., and Zimmer, R. (2007).
RelEx - Relation extraction using dependency
parse trees. Bioinformatics, 23(3):365?371.
Gabrilovich, E. and Markovitch, S. (2007). Comput-
ing semantic relatedness using wikipedia-based
explicit semantic analysis. In Proceedings of The
Twentieth International Joint Conference for Ar-
tificial Intelligence., pages 1606?1611.
Hart, D. and Goertzel, B. (2008). Opencog: A soft-
ware framework for integrative artificial general
intelligence. In Proceedings of the 2008 confer-
ence on Artificial General Intelligence 2008: Pro-
ceedings of the First AGI Conference, pages 468?
472, Amsterdam, The Netherlands, The Nether-
lands. IOS Press.
Hassel, M. (2004). JavaSDM package.
Kanerva, P., Kristoferson, J., and Holst, A. (2000).
Random indexing of text samples for latent se-
mantic analysis. In Gleitman, L. and Josh, A.,
editors, Proceedings of the 22nd Annual Confer-
ence of the Cognitive Science Society, page 1036.
Erlbaum.
Karlgren, J. and Sahlgren, M. (2001). From Words
to Understanding. In Uesaka, Y., Kanerva, P., and
Asoh, H., editors, Foundations of real-world in-
telligence, chapter 26, pages 294?311. Stanford:
CSLI Publications.
Karnick, H., Satapathy, S., and Bhagwani, S. (2012).
sranjans: Semantic textual similarity using max-
imal bipartite graph matching. In *SEM (2012),
pages 579?585.
Kuhn, H. (1955). The Hungarian method for the as-
signment problem. Naval research logistics quar-
terly, 2:83?97.
Leacock, C. and Chodorow, M. (1998). Combin-
ing local context and WordNet similarity for word
sense identification. WordNet: An electronic lexi-
cal . . . .
Levenshtein, V. I. (1966). Binary codes capable of
correcting deletions, insertions and reversals. So-
viet Physics Doklady, 10(8):707?710.
Nivre, J., Hall, J., and Nilsson, J. (2006). Malt-
parser: A data-driven parser-generator for depen-
dency parsing. In In Proc. of LREC-2006, pages
2216?2219.
72
Reisinger, J. and Mooney, R. (2010). Multi-
prototype vector-space models of word meaning.
In Human Language Technologies: The 2010 An-
nual Conference of the North American Chapter
of the Association for Computational Linguistics,
number June, pages 109?117.
Riesen, K. and Bunke, H. (2009). Approximate
graph edit distance computation by means of bi-
partite graph matching. Image and Vision Com-
puting, 27(7):950?959.
R?kenes, H. (2013). Graph-Edit Distance Applied to
the Task of Detecting Plagiarism. Master?s the-
sis, Norwegian University of Science and Tech-
nology.
Sahlgren, M. (2005). An introduction to random in-
dexing. In Methods and Applications of Seman-
tic Indexing Workshop at the 7th International
Conference on Terminology and Knowledge En-
gineering, TKE, volume 5.
Sahlgren, M., Holst, A., and Kanerva, P. (2008). Per-
mutations as a Means to Encode Order in Word
Space. Proceedings of the 30th Conference of the
Cognitive Science Society.
S?aric?, F., Glavas?, G., Karan, M., S?najder, J., and
Bas?ic?, B. D. (2012). TakeLab: systems for mea-
suring semantic text similarity. In *SEM (2012),
pages 441?448.
*SEM (2012). Proceedings of the First Joint Con-
ference on Lexical and Computational Seman-
tics (*SEM), volume 2: Proceedings of the Sixth
International Workshop on Semantic Evaluation,
Montreal, Canada. Association for Computational
Linguistics.
Semeraro, G., Aldo, B., and Orabona, V. E. (2012).
UNIBA: Distributional semantics for textual sim-
ilarity. In *SEM (2012), pages 591?596.
Singh, J., Bhattacharya, A., and Bhattacharyya, P.
(2012). janardhan: Semantic textual similarity us-
ing universal networking language graph match-
ing. In *SEM 2012: The First Joint Conference
on Lexical and Computational Semantics ? Vol-
ume 1: Proceedings of the main conference and
the shared task, and Volume 2: Proceedings of the
Sixth International Workshop on Semantic Evalu-
ation (SemEval 2012), pages 662?666, Montre?al,
Canada. Association for Computational Linguis-
tics.
Sokolov, A. (2012). LIMSI: learning semantic simi-
larity by selecting random word subsets. In *SEM
(2012), pages 543?546.
Tovar, M., Reyes, J., and Montes, A. (2012). BUAP:
a first approximation to relational similarity mea-
suring. In *SEM (2012), pages 502?505.
Vapnik, V., Golowich, S. E., and Smola, A. (1997).
Support vector method for function approxima-
tion, regression estimation, and signal process-
ing. In Mozer, M. C., Jordan, M. I., and Petsche,
T., editors, Advances in Neural Information Pro-
cessing Systems, volume 9, pages 281?287. MIT
Press, Cambridge, Massachusetts.
73
Workshop on Monolingual Text-To-Text Generation, pages 27?33,
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 27?33,
Portland, Oregon, 24 June 2011. c?2011 Association for Computational Linguistics
Comparing Phrase-based and Syntax-based Paraphrase Generation
Sander Wubben
Tilburg University
P.O. Box 90135
5000 LE Tilburg
The Netherlands
s.wubben@uvt.nl
Erwin Marsi
NTNU
Sem Saelandsvei 7-9
NO-7491 Trondheim
Norway
emarsi@idi.ntnu.no
Antal van den Bosch
Tilburg University
P.O. Box 90135
5000 LE Tilburg
The Netherlands
antal.vdnbosch@uvt.nl
Emiel Krahmer
Tilburg University
P.O. Box 90135
5000 LE Tilburg
The Netherlands
e.j.krahmer@uvt.nl
Abstract
Paraphrase generation can be regarded as ma-
chine translation where source and target lan-
guage are the same. We use the Moses statisti-
cal machine translation toolkit for paraphras-
ing, comparing phrase-based to syntax-based
approaches. Data is derived from a recently
released, large scale (2.1M tokens) paraphrase
corpus for Dutch. Preliminary results indicate
that the phrase-based approach performs bet-
ter in terms of NIST scores and produces para-
phrases at a greater distance from the source.
1 Introduction
One of the challenging properties of natural lan-
guage is that the same semantic content can typically
be expressed by many different surface forms. As
the ability to deal with paraphrases holds great po-
tential for improving the coverage of NLP systems,
a substantial body of research addressing recogni-
tion, extraction and generation of paraphrases has
emerged (Androutsopoulos and Malakasiotis, 2010;
Madnani and Dorr, 2010). Paraphrase Generation
can be regarded as a translation task in which source
and target language are the same. Both Paraphrase
Generation and Machine Translation (MT) are in-
stances of Text-To-Text Generation, which involves
transforming one text into another, obeying certain
restrictions. Here these restrictions are that the gen-
erated text must be grammatically well-formed and
semantically/translationally equivalent to the source
text. Addionally Paraphrase Generation requires
that the output should differ from the input to a cer-
tain degree.
The similarity between Paraphrase Generation
and MT suggests that methods and tools originally
developed for MT could be exploited for Paraphrase
Generation. One popular approach ? arguably the
most successful so far ? is Statistical Phrase-based
Machine Translation (PBMT), which learns phrase
translation rules from aligned bilingual text corpora
(Och et al, 1999; Vogel et al, 2000; Zens et al,
2002; Koehn et al, 2003). Prior work has explored
the use of PBMT for paraphrase generation (Quirk et
al., 2004; Bannard and Callison-Burch, 2005; Mad-
nani et al, 2007; Callison-Burch, 2008; Zhao et al,
2009; Wubben et al, 2010)
However, since many researchers believe that
PBMT has reached a performance ceiling, ongo-
ing research looks into more structural approaches
to statistical MT (Marcu and Wong, 2002; Och and
Ney, 2004; Khalilov and Fonollosa, 2009). Syntax-
based MT attempts to extract translation rules in
terms of syntactic constituents or subtrees rather
than arbitrary phrases, presupposing syntactic struc-
tures for source, target or both languages. Syntactic
information might lead to better results in the area
of grammatical well-formedness, and unlike phrase-
based MT that uses contiguous n-grams, syntax en-
ables the modeling of long-distance translation pat-
terns.
While the verdict on whether or not this approach
leads to any significant performance gain is still
out, a similar line of reasoning would suggest that
syntax-based paraphrasing may offer similar advan-
tages over phrase-based paraphrasing. Considering
the fact that the success of PBMT can partly be at-
tributed to the abundance of large parallel corpora,
27
and that sufficiently large parallel corpora are still
lacking for paraphrase generation, using more lin-
guistically motivated methods might prove benefi-
cial for paraphrase generation. At the same time,
automatic syntactic analysis introduces errors in the
parse trees, as no syntactic parser is perfect. Like-
wise, automatic alignment of syntactic phrases may
be prone to errors.
The main contribution of this paper is a systematic
comparison between phrase-based and syntax-based
paraphrase generation using an off-the-shelf statis-
tical machine translation (SMT) decoder, namely
Moses (Koehn et al, 2007) and the word-alignment
tool GIZA++ (Och and Ney, 2003). Training data
derives from a new, large scale (2.1M tokens) para-
phrase corpus for Dutch, which has been recently
released.
The paper is organized as follows. Section 2 re-
views the paraphrase corpus from which provides
training and test data. Next, Section 3 describes the
paraphrase generation methods and the experimen-
tal setup. Results are presented in Section 4. In
Section 5 we discuss our findings and formulate our
conclusions.
2 Corpus
The main bottleneck in building SMT systems is
the need for a substantial amount of parallel aligned
text. Likewise, exploiting SMT for paraphrasing re-
quires large amounts of monolingual parallel text.
However, paraphrase corpora are scarce; the situa-
tion is more dire than in MT, and this has caused
some studies to focus on the automatic harvesting
of paraphrase corpora. The use of monolingual par-
allel text corpora was first suggested by Barzilay
and McKeown (2001), who built their corpus us-
ing various alternative human-produced translations
of literary texts and then applied machine learn-
ing or multi-sequence alignment for extracting para-
phrases. In a similar vein, Pang et al (2003) used a
corpus of alternative English translations of Chinese
news stories in combination with a syntax-based al-
gorithm that automatically builds word lattices, in
which paraphrases can be identified.
So-called comparable monolingual corpora, for
instance independently written news reports describ-
ing the same event, in which some pairs of sentences
exhibit partial semantic overlap have also been in-
vestigated (Shinyama et al, 2002; Barzilay and Lee,
2003; Shen et al, 2006; Wubben et al, 2009)
The first manually collected paraphrase corpus is
the Microsoft Research Paraphrase (MSRP) Corpus
(Dolan et al, 2004), consisting of 5,801 sentence
pairs, sampled from a larger corpus of news arti-
cles. However, it is rather small and contains no sub-
sentential allignments. Cohn et al (2008) developed
a parallel monolingual corpus of 900 sentence pairs
annotated at the word and phrase level. However, all
of these corpora are small from an SMT perspective.
Recently a new large-scale paraphrase corpus for
Dutch, the DAESO corpus, was released. The cor-
pus contains both samples of parallel and compa-
rable text in which similar sentences, phrases and
words are aligned. One part of the corpus is manu-
ally aligned, whereas another part is automatically
aligned using a data-driven aligner trained on the
first part. The DAESO corpus is extensively de-
scribed in (Marsi and Krahmer, 2011); the summary
here is limited to aspects relevant to the work at
hand.
The corpus contains the following types of text:
(1) alternative translations in Dutch of three liter-
ary works of fiction; (2) autocue text from televion
broadcast news as read by the news reader, and the
corresponding subtitles; (3) headlines from similar
news articles obtained from Google News Dutch;
(4) press releases about the same news topic from
two different press agencies; (5) similar answers re-
trieved from a document collection in the medical
domain, originally created for evaluating question-
answering systems.
In a first step, similar sentences were automati-
cally aligned, after which alignments were manu-
ally corrected. In the case of the parallel book texts,
aligned sentences are (approximate) paraphrases. To
a lesser degree, this is also true for the news head-
lines. The autocue-subtitle pairs are mostly exam-
ples of sentence compression, as the subtitle tends
to be a compressed version of the read autocue text.
In contrast, the press releases and the QA answers,
are characterized by a great deal of one-to-many
sentence alignments, as well as sentences left un-
aligned, as is to be expected in comparable text.
Most sentences in these types of text tend to have
only partial overlap in meaning.
28
Table 1: Properties of the manually aligned corpus
Autosub Books Headlines News QA Overall
aligned trees 18 338 6 362 32 627 11 052 118 68 497
tokens 217 959 115 893 179 629 162 361 2 230 678 072
tokens/sent 11.89 18.22 5.51 14.69 18.90 9.90
nodes 365 157 191 636 318 399 271 192 3734 1 150 118
nodes/tree 19.91 30.12 9.76 24.54 31.64 16.79
uniquely aligned trees (%) 92.93 92.49 84.57 63.61 50.00 84.10
aligned nodes (%) 73.53 66.83 73.58 53.62 38.62 67.62
Next, aligned sentences were tokenized and
parsed with the Alpino parser for Dutch (Bouma et
al., 2001). The parser provides a relatively theory-
neutral syntactic analysis which is a blend of phrase
structure analysis and dependency analysis, with a
backbone of phrasal constituents and arcs labeled
with syntactic function/dependency labels.
The alignments not only concern paraphrases in
the strict sense, i.e., expressions that are semanti-
cally equivalent, but extend to expressions that are
semantically similar in less strict ways, for instance,
where one phrase is either more specific or more
general than the related phrase. For this reason,
alignments are also labeled according to a limited
set of semantic similarity relations. Since these rela-
tions were not used in the current study, we will not
discuss them further here.
The corpus comprises over 2.1 million tokens,
678 thousand of which are manually annotated and
1,511 thousand are automatically processed.
To give a more complete overview of the sizes
of different corpus segments, some properties of the
manually aligned corpus are listed in Table 1. Prop-
erties of the automatically aligned part are similar,
except for the fact that it only contains text of the
news and QA type.
3 Paraphrase generation
Phrase-based MT models consider translation as a
mapping of small text chunks, with possible re-
ordering (Och and Ney, 2004). Operations such as
insertion, deletion and many-to-one, one-to-many
or many-to-many translation are all covered in the
structure of the phrase table. Phrase-based models
have been used most prominently in the past decade,
as they have shown to outperform other approaches
(Callison-Burch et al, 2009).
One issue with the phrase-based approach is that
recursion is not handled explicitly. It is gener-
ally acknowledged that language contains recursive
structures up to certain depths. So-called hierarchi-
cal models have introduced the inclusion of non-
terminals in the mapping rules, to allow for recur-
sion (Chiang et al, 2005). However, using a generic
non-terminal X can introduce many substitutions
in translations that do not make sense. By mak-
ing the non-terminals explicit, using syntactic cat-
egories such as NP s and V P s, this phenomenon
is constrained, resulting in syntax-based translation.
Instead of phrase translations, translation rules in
terms of syntactic constituents or subtrees are ex-
tracted, presupposing the availability of syntactic
structures for source, target, or both languages.
Incorporating syntax can guide the translation
process and unlike phrase-based MT syntax it en-
ables the modeling of long-distance translation pat-
terns. Syntax-based systems may parse the data on
the target side (string-to-tree), source side (tree-to-
string), or both (tree-to-tree).
In our experiments we use tree-to-tree syntax-
based MT. We also experiment with relaxing the
parses by a method proposed under the label of
syntax-augmented machine translation (SAMT), de-
scribed in (Zollmann and Venugopal, 2006). This
method combines any neighboring nodes and labels
previously unlabeled nodes, removing the syntactic
constraint on the grammar1.
We train all systems on the DAESO data (218,102
lines of aligned sentences) and test on a held-out
set consisting of manually aligned headlines that ap-
1This method is implemented in the Moses package in the
program relax-parse as option SAMT 4
29
Table 2: Examples of output of the phrase-based and syntax-based systems
Source jongen ( 7 ) zwaargewond na aanrijding boy (7) severely-injured after crash
Phrase-based 7-jarige gewond na botsing 7-year-old injured after collision
Syntax-based jongen ( 7 ) zwaar gewond na aanrijding boy (7) severely injured after crash
Source jeugdwerkloosheid daalt vooral bij voldoende opleiding youth-unemployment drops especially with adequate training
Phrase-based werkloosheid jongeren daalt , vooral bij voldoende studie unemployment youths drops, especially with sufficient study
Syntax-based * jeugdwerkloosheid daalt vooral in voldoende opleiding youth-unemployment drops especially in adequate training
Source kritiek op boetebeleid ns criticism of fining-policy ns
Phrase-based * kritiek op de omstreden boetebeleid en criticism of the controversial and
Syntax-based kritiek op omstreden boetebeleid nederlandse spoorwegen criticism of controversial fining-policy dutch railways
Source weer bestuurders radboud weg again directors radboud [hospital] leaving
Phrase-based * weer de weg ziekenhuis again the leaving hospital
Syntax-based alweer bestuurders ziekenhuis weg yet-again directors hospital leaving
peared in May 2006.2 We test on 773 headlines that
have three or more aligned paraphrasing reference
headlines. We use an SRILM (Stolcke, 2002) lan-
guage model trained on the Twente news corpus3.
To investigate the effect of the amount of training
data on results, we also train a phrase-based model
on more data by adding more aligned headlines orig-
inating from data crawled in 2010 and aligned using
tf.idf scores over headline clusters and Cosine sim-
ilarity as described in (Wubben et al, 2009), result-
ing in an extra 612,158 aligned headlines.
Evaluation is based on the assumption that a good
paraphrase is well-formed and semantically similar
but structurally different from the source sentence.
We therefore score the generated paraphrases not
only by an MT metric (we use NIST scores), but
also factor in the edit distance between the input
sentence and the output sentence. We take the 10-
best generated paraphrases and select from these the
one most dissimilar from the source sentence in term
of Levenshtein distance on tokens. We then weigh
NIST scores according to their corresponding sen-
tence Levenshtein Distance, to calculate a weighted
2Syntactic trees were converted to the XML format used by
Moses for syntax-based MT. A minor complication is that the
word order in the tree is different from the word order in the
corresponding sentence in about half of the cases. The technical
reason is that Alpino internally produces dependency structures
that can be non-projective. Conversion to a phrase structure tree
therefore necessitates moving some words to a different posi-
tion in the tree. We performed a subsequent reordering of the
trees, moving terminals to make the word order match the sur-
face word order.
3http://www.vf.utwente.nl/?druid/TwNC/
TwNC-main.html
average score. This implies that we penalize sys-
tems that provide output at Levenshtein distance 0,
which are essentially copies of the input, and not
paraphrases. Formally, the score is computed as fol-
lows:
NISTweightedLD = ?
?
i=LD(1..8)
(i ?Ni ?NISTi)
?
i=LD(1..8)
(i ?Ni)
where ? is the percentage of output phrases that have
a sentence Levenshtein Distance higher than 0. In-
stead of NIST scores, other MT evaluation scores
can be plugged into this formula, such as METEOR
(Lavie and Agarwal, 2007) for languages for which
paraphrase data is available.
4 Results
Figure 1 shows NIST scores per Levenshtein Dis-
tance. It can be observed that overall the NIST score
decreases as the distance to the input increases, indi-
cating that more distant paraphrases are of less qual-
ity. The relaxed syntax-based approach (SAMT)
performs mildly better than the standard syntax-
based approach, but performs worse than the phrase-
based approach. The distribution of generated para-
phrases per Levenshtein Distance is shown in Fig-
ure 2. It reveals that the Syntax-based approaches
tend to stay closer to the source than the phrase-
based approaches.
In Table 2 a few examples of output from both
Phrase- and Syntax-based systems are given. The
30
2 4 6 8 10
2
4
6
8
10
LevenshteinDistance
N
IS
T
sc
or
e
Phrase
Phrase extra data
Syntax
Syntax relaxed
Figure 1: NIST scores per Levenshtein distance
top two examples show sentences where the phrase-
based approach scores better, and the bottom two
show examples where the syntax-based approach
scores better. In general, we observe that the
phrase-based approach is often more drastic with its
changes, as shown also in Figure 2. The syntax-
based approach is less risky, and reverts more to
single-word substitution.
The weighted NIST score for the phrase-based
approach is 7.14 versus 6.75 for the syntax-based
approach. Adding extra data does not improve the
phrase-based approach, as it yields a score of 6.47,
but the relaxed method does improve the syntax-
based approach (7.04).
5 Discussion and conclusion
We have compared a phrase-based MT approach
to paraphrasing with a syntax-based MT approach.
The Phrase-based approach performs better in terms
of NIST score weighted by edit distance of the out-
put. In general, the phrase-based MT system per-
forms more edits and these edits seem to be more
reliable than the edits done by the Syntax-based ap-
proach. A relaxed Syntax-based approach performs
better, while adding more data to the Phrase-based
approach does not yield better results. To gain a bet-
ter understanding of the quality of the output gener-
ated by the different approaches, it would be desir-
able to present the output of the different systems to
human judges. In future work, we intend to com-
pare the effects of using manual word alignments
from the DAESO corpus instead of the automatic
alignments produced by GIZA++. We also wish to
0 2 4 6 8 10
0
100
200
300
LevenshteinDistance
N
Phrase
Phrase extra data
Syntax
Syntax relaxed
Figure 2: Distribution of generated paraphrases per Lev-
enshtein distance
further explore the effect of the nature of the data
that we train on: the DAESO corpus consists of var-
ious data sources from different domains. Our aim
is also to incorporate the notion of dissimilarity into
the paraphrase model, by adding dissimilarity scores
to the model.
31
References
Ion Androutsopoulos and Prodromos Malakasiotis.
2010. A survey of paraphrasing and textual entailment
methods. Journal of Artificial Intelligence Research,
38:135?187, May.
Colin Bannard and Chris Callison-Burch. 2005. Para-
phrasing with bilingual parallel corpora. In ACL ?05:
Proceedings of the 43rd Annual Meeting on Associ-
ation for Computational Linguistics, pages 597?604,
Morristown, NJ, USA. Association for Computational
Linguistics.
Regina Barzilay and Lillian Lee. 2003. Learning to
paraphrase: an unsupervised approach using multiple-
sequence alignment. In NAACL ?03: Proceedings of
the 2003 Conference of the North American Chapter of
the Association for Computational Linguistics on Hu-
man Language Technology, pages 16?23, Morristown,
NJ, USA. Association for Computational Linguistics.
Regina Barzilay and Kathleen McKeown. 2001. Extract-
ing paraphrases from a parallel corpus. In Proceedings
of Meeting of the Association for Computational Lin-
guistics, pages 50?57, Toulouse, France.
Gosse Bouma, Gertjan van Noord, and Robert Malouf.
2001. Alpino: Wide-coverage computational analy-
sis of Dutch. In Walter Daelemans, Khalil Sima?an,
Jorn Veenstra, and Jakub Zavre, editors, Computa-
tional Linguistics in the Netherlands 2000., pages 45?
59. Rodopi, Amsterdam, New York.
Chris Callison-Burch, Philipp Koehn, Christof Monz,
and Josh Schroeder. 2009. Findings of the 2009
Workshop on Statistical Machine Translation. In
Proceedings of the Fourth Workshop on Statistical
Machine Translation, pages 1?28, Athens, Greece,
March. Association for Computational Linguistics.
Chris Callison-Burch. 2008. Syntactic constraints
on paraphrases extracted from parallel corpora. In
Proceedings of the Conference on Empirical Meth-
ods in Natural Language Processing, EMNLP ?08,
pages 196?205, Stroudsburg, PA, USA. Association
for Computational Linguistics.
David Chiang, Adam Lopez, Nitin Madnani, Christof
Monz, Philip Resnik, and Michael Subotin. 2005. The
hiero machine translation system: extensions, evalua-
tion, and analysis. In Proceedings of the conference on
Human Language Technology and Empirical Methods
in Natural Language Processing, HLT ?05, pages 779?
786, Stroudsburg, PA, USA. Association for Compu-
tational Linguistics.
Trevor Cohn, Chris Callison-Burch, and Mirella Lapata.
2008. Constructing corpora for the development and
evaluation of paraphrase systems. Computational Lin-
guistics, 34(4):597?614.
Bill Dolan, Chris Quirk, and Chris Brockett. 2004.
Unsupervised construction of large paraphrase cor-
pora: Exploiting massively parallel news sources. In
Proceedings of the 20th International Conference on
Computational Linguistics, pages 350?356, Morris-
town, NJ, USA.
Maxim Khalilov and Jose? A. R. Fonollosa. 2009. N-
gram-based statistical machine translation versus syn-
tax augmented machine translation: comparison and
system combination. In Proceedings of the 12th Con-
ference of the European Chapter of the Association
for Computational Linguistics, EACL ?09, pages 424?
432, Stroudsburg, PA, USA. Association for Compu-
tational Linguistics.
Philip Koehn, Franz Josef Och, and Daniel Marcu. 2003.
Statistical phrase-based translation. In Proceedings of
the 2003 Conference of the North American Chapter
of the Association for Computational Linguistics on
Human Language Technology-Volume 1, pages 48?54.
Association for Computational Linguistics.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris C.
Burch, Marcello Federico, Nicola Bertoldi, Brooke
Cowan, Wade Shen, Christine Moran, Richard Zens,
Chris Dyer, Ondrej Bojar, Alexandra Constantin, and
Evan Herbst. 2007. Moses: Open source toolkit for
statistical machine translation. In ACL. The Associa-
tion for Computer Linguistics.
Alon Lavie and Abhaya Agarwal. 2007. Meteor: an au-
tomatic metric for mt evaluation with high levels of
correlation with human judgments. In Proceedings
of the Second Workshop on Statistical Machine Trans-
lation, StatMT ?07, pages 228?231, Stroudsburg, PA,
USA. Association for Computational Linguistics.
Nitin Madnani and Bonnie J. Dorr. 2010. Gener-
ating phrasal and sentential paraphrases: A survey
of data-driven methods. Computational Linguistics,
36(3):341?387.
Nitin Madnani, Necip Fazil Ayan, Philip Resnik, and
Bonnie J. Dorr. 2007. Using paraphrases for pa-
rameter tuning in statistical machine translation. In
Proceedings of the Second Workshop on Statistical
Machine Translation, StatMT ?07, pages 120?127,
Stroudsburg, PA, USA. Association for Computational
Linguistics.
Daniel Marcu and William Wong. 2002. A phrase-based,
joint probability model for statistical machine trans-
lation. In Proceedings of the ACL-02 conference on
Empirical methods in natural language processing -
Volume 10, EMNLP ?02, pages 133?139, Stroudsburg,
PA, USA. Association for Computational Linguistics.
Erwin Marsi and Emiel Krahmer. 2011. Construction of
an aligned monolingual treebank for studying seman-
tic similarity. (submitted for publication).
32
Franz J. Och and Hermann Ney. 2003. A systematic
comparison of various statistical alignment models.
Comput. Linguist., 29(1):19?51, March.
Franz Josef Och and Hermann Ney. 2004. The align-
ment template approach to statistical machine transla-
tion. Comput. Linguist., 30:417?449, December.
Franz J. Och, Christoph Tillmann, and Hermann Ney.
1999. Improved alignment models for Statistical Ma-
chine Translation. In Proceedings of the Joint Work-
shop on Empirical Methods in NLP and Very Large
Corpora, pages 20?28, Maryland, USA.
Bo Pang, Kevin Knight, and Daniel Marcu. 2003.
Syntax-based alignment of multiple translations: Ex-
tracting paraphrases and generating new sentences. In
HLT-NAACL.
Chris Quirk, Chris Brockett, and William Dolan. 2004.
Monolingual machine translation for paraphrase gen-
eration. In Dekang Lin and Dekai Wu, editors, Pro-
ceedings of EMNLP 2004, pages 142?149, Barcelona,
Spain, July. Association for Computational Linguis-
tics.
Siwei Shen, Dragomir R. Radev, Agam Patel, and Gu?nes?
Erkan. 2006. Adding syntax to dynamic program-
ming for aligning comparable texts for the generation
of paraphrases. In Proceedings of the COLING/ACL
2006 Main Conference Poster Sessions, pages 747?
754, Sydney, Australia, July. Association for Compu-
tational Linguistics.
Yusuke Shinyama, Satoshi Sekine, Kiyoshi Sudo, and
Ralph Grishman. 2002. Automatic paraphrase acqui-
sition from news articles. In Proceedings of Human
Language Technology Conference (HLT 2002), pages
313?318, San Diego, USA.
Andreas Stolcke. 2002. SRILM - An Extensible Lan-
guage Modeling Toolkit. In In Proc. Int. Conf. on
Spoken Language Processing, pages 901?904, Denver,
Colorado.
S. Vogel, Franz Josef Och, and Hermann Ney. 2000. The
statistical translation module in the verbmobil system.
In KONVENS 2000 / Sprachkommunikation, Vortrge
der gemeinsamen Veranstaltung 5. Konferenz zur Ve-
rarbeitung natrlicher Sprache (KONVENS), 6. ITG-
Fachtagung ?Sprachkommunikation?, pages 291?293,
Berlin, Germany, Germany. VDE-Verlag GmbH.
Sander Wubben, Antal van den Bosch, Emiel Krahmer,
and Erwin Marsi. 2009. Clustering and matching
headlines for automatic paraphrase acquisition. In
E. Krahmer and M. Theune, editors, The 12th Eu-
ropean Workshop on Natural Language Generation,
pages 122?125, Athens. Association for Computa-
tional Linguistics.
Sander Wubben, Antal van den Bosch, and Emiel Krah-
mer. 2010. Paraphrase generation as monolingual
translation: Data and evaluation. In B. Mac Namee
J. Kelleher and I. van der Sluis, editors, Proceedings of
the 10th International Workshop on Natural Language
Generation (INLG 2010), pages 203?207, Dublin.
Richard Zens, Franz Josef Och, and Hermann Ney. 2002.
Phrase-based statistical machine translation. In Pro-
ceedings of the 25th Annual German Conference on
AI: Advances in Artificial Intelligence, KI ?02, pages
18?32, London, UK. Springer-Verlag.
Shiqi Zhao, Xiang Lan, Ting Liu, and Sheng Li. 2009.
Application-driven statistical paraphrase generation.
In Proceedings of the Joint Conference of the 47th
Annual Meeting of the ACL and the 4th Interna-
tional Joint Conference on Natural Language Process-
ing of the AFNLP: Volume 2 - Volume 2, ACL ?09,
pages 834?842, Stroudsburg, PA, USA. Association
for Computational Linguistics.
Andreas Zollmann and Ashish Venugopal. 2006. Syntax
augmented machine translation via chart parsing. In
Proceedings of the Workshop on Statistical Machine
Translation, StatMT ?06, pages 138?141, Stroudsburg,
PA, USA. Association for Computational Linguistics.
33
Proceedings of the 9th Workshop on Multiword Expressions (MWE 2013), pages 21?30,
Atlanta, Georgia, 13-14 June 2013. c?2013 Association for Computational Linguistics
Improving Word Translation Disambiguation by
Capturing Multiword Expressions with Dictionaries
Lars Bungum, Bjo?rn Gamba?ck, Andre? Lynum, Erwin Marsi
Norwegian University of Science and Technology
Sem S?lands vei 7?9; NO?7491 Trondheim, Norway
{bungum,gamback,andrely,emarsi}@idi.ntnu.no
Abstract
The paper describes a method for identifying
and translating multiword expressions using a
bi-directional dictionary. While a dictionary-
based approach suffers from limited recall,
precision is high; hence it is best employed
alongside an approach with complementing
properties, such as an n-gram language model.
We evaluate the method on data from the
English-German translation part of the cross-
lingual word sense disambiguation task in the
2010 semantic evaluation exercise (SemEval).
The output of a baseline disambiguation sys-
tem based on n-grams was substantially im-
proved by matching the target words and their
immediate contexts against compound and
collocational words in a dictionary.
1 Introduction
Multiword expressions (MWEs) cause particular
lexical choice problems in machine translation
(MT), but can also be seen as an opportunity to both
generalize outside the bilingual corpora often used
as training data in statistical machine translation ap-
proaches and as a method to adapt to specific do-
mains. The identification of MWEs is in general
important for many language processing tasks (Sag
et al, 2002), but can be crucial in MT: since the se-
mantics of many MWEs are non-compositional, a
suitable translation cannot be constructed by trans-
lating the words in isolation. Identifying MWEs
can help to identify idiomatic or otherwise fixed lan-
guage usage, leading to more fluent translations, and
potentially reduce the amount of lexical choice an
MT system faces during target language generation.
In any translation effort, automatic or otherwise,
the selection of target language lexical items to in-
clude in the translation is a crucial part of the fi-
nal translation quality. In rule-based systems lex-
ical choice is derived from the semantics of the
source words, a process which often involves com-
plex semantic composition. Data-driven systems
on the other hand commonly base their translations
nearly exclusively on cooccurrences of bare words
or phrases in bilingual corpora, leaving the respon-
sibility of selecting lexical items in the translation
entirely to the local context found in phrase trans-
lation tables and language models with no explicit
notion of the source or target language semantics.
Still, systems of this type have been shown to pro-
duce reasonable translation quality without explic-
itly considering word translation disambiguation.
Bilingual corpora are scarce, however, and un-
available for most language pairs and target do-
mains. An alternative approach is to build systems
based on large monolingual knowledge sources and
bilingual lexica, as in the hybrid MT system PRE-
SEMT (Sofianopoulos et al, 2012). Since such
a system explicitly uses a translation dictionary, it
must at some point in the translation process decide
which lexical entries to use; thus a separate word
translation disambiguation module needs to be in-
corporated. To research available methods in such a
module we have identified a task where we can use
public datasets for measuring how well a method is
able to select the optimal of many translation choices
from a source language sentence.
21
In phrase-based statistical MT systems, the trans-
lation of multiword expressions can be a notable
source of errors, despite the fact that those systems
explicitly recognize and use alignments of sequen-
tial chunks of words. Several researchers have ap-
proached this problem by adding MWE translation
tables to the systems, either through expanding the
phrase tables (Ren et al, 2009) or by injecting the
MWE translations into the decoder (Bai et al, 2009).
Furthermore, there has been some interest in auto-
matic mining of MWE pairs from bilingual corpora
as a task in itself: Caseli et al (2010) used a dic-
tionary for evaluation of an automatic MWE extrac-
tion procedure using bilingual corpora. They also
argued for the filtering of stopwords, similarly to the
procedure described in the present paper. Sharoff
et al (2006) showed how MWE pairs can be ex-
tracted from comparable monolingual corpora in-
stead of from a parallel bilingual corpus.
The methodology introduced in this paper em-
ploys bilingual dictionaries as a source of multi-
word expressions. Relationships are induced be-
tween the source sentence and candidate transla-
tion lexical items based on their correspondence in
the dictionary. Specifically, we use a determinis-
tic multiword expression disambiguation procedure
based on translation dictionaries in both directions
(from source to target language and vice versa),
and a baseline system that ranks target lexical items
based on their immediate context and an n-gram
language model. The n-gram model represents a
high-coverage, low-precision companion to the dic-
tionary approach (i.e., it has complementary proper-
ties). Results show that the MWE dictionary infor-
mation substantially improves the baseline system.
The 2010 Semantic Evaluation exercise (Sem-
Eval?10) featured a shared task on Cross-Lingual
Word Sense Disambiguation (CL-WSD), where the
focus was on disambiguating the translation of a sin-
gle noun in a sentence. The participating systems
were given an English word in its context and asked
to produce appropriate substitutes in another lan-
guage (Lefever and Hoste, 2010b). The CL-WSD
data covers Dutch, French, Spanish, Italian and Ger-
man; however, since the purpose of the experiments
in this paper just was to assess our method?s abil-
ity to choose the right translation of a word given its
context, we used the English-to-German part only.
The next section details the employed disam-
biguation methodology and describes the data sets
used in the experiments. Section 3 then reports on
the results of experiments applying the methodology
to the SemEval datasets, particularly addressing the
impact of the dictionary MWE correspondences. Fi-
nally, Section 4 sums up the discussion and points to
issues that can be investigated further.
2 Methodology
The core of the disambiguation model introduced
in this paper is dictionary-based multiword extrac-
tion. Multiword extraction is done in both a direct
and indirect manner: Direct extraction uses adjacent
words in the source language in combination with
the word to be translated, if the combination has an
entry in the source-to-target language (SL?TL) dic-
tionary. Indirect extraction works in the reverse di-
rection, by searching the target-to-source (TL?SL)
dictionary and looking up translation candidates for
the combined words. Using a dictionary to identify
multiword expressions after translation has a low re-
call of target language MWEs, since often there ei-
ther are no multiword expressions to be discovered,
or the dictionary method is unable to find a trans-
lation for an MWE. Nevertheless, when an MWE
really is identified by means of the dictionary-based
method, the precision is high.
Due to the low recall, relying on multiword ex-
pressions from dictionaries would, however, not be
sufficient. Hence this method is combined with an
n-gram language model (LM) based on a large tar-
get language corpus. The LM is used to rank trans-
lation candidates according to the probability of the
n-gram best matching the context around the transla-
tion candidate. This is a more robust but less precise
approach, which servers as the foundation for the
high-precision but low-recall dictionary approach.
In the actual implementation, the n-gram method
thus first provides a list of its best suggestions
(currently top-5), and the dictionary method then
prepends its candidates to the top of this list. Con-
sequently, n-gram matching is described before
dictionary-based multiword extraction in the follow-
ing section. First, however, we introduce the data
sets used in the experiments.
22
(a) AGREEMENT in the form of an exchange of letters between
the European Economic Community and the Bank for Interna-
tional Settlements concerning the mobilization of claims held by
the Member States under the medium-term financial assistance
arrangements
{bank 4; bankengesellschaft 1; kreditinstitut 1; zentralbank 1; fi-
nanzinstitut 1}
(b) The Office shall maintain an electronic data bank with the par-
ticulars of applications for registration of trade marks and entries
in the Register. The Office may also make available the contents
of this data bank on CD-ROM or in any other machine-readable
form.
{datenbank 4; bank 3; datenbanksystem 1; daten 1}
(c) established as a band of 1 km in width from the banks of a
river or the shores of a lake or coast for a length of at least 3 km.
{ufer 4; flussufer 3}
Table 1: Examples of contexts for the English word bank
with possible German translations
2.1 The CL-WSD Datasets
The data sets used for the SemEval?10 Cross-
Lingual Word Sense Disambiguation task were con-
structed by making a ?sense inventory? of all pos-
sible target language translations of a given source
language word based on word-alignments in Eu-
roparl (Koehn, 2005), with alignments involving the
relevant source words being manually checked. The
retrieved target words were manually lemmatised
and clustered into translations with a similar sense;
see Lefever and Hoste (2010a) for details.
Trial and test instances were extracted from two
other corpora, JRC-Acquis (Steinberger et al, 2006)
and BNC (Burnard, 2007). The trial data for each
language consists of five nouns (with 20 sentence
contexts per noun), and the test data of twenty nouns
(50 contexts each, so 1000 in total per language,
with the CL-WSD data covering Dutch, French,
Spanish, Italian and German). Table 1 provides ex-
amples from the trial data of contexts for the English
word bank and its possible translations in German.
Gold standard translations were created by hav-
ing four human translators picking the contextually
appropriate sense for each source word, choosing 0?
3 preferred target language translations for it. The
translations are thus restricted to those appearing in
Europarl, probably introducing a slight domain bias.
Each translation has an associated count indicating
how many annotators considered it to be among their
top-3 preferred translations in the given context.
bank, bankanleihe, bankanstalt, bankdarlehen, bankenge-
sellschaft, bankensektor, bankfeiertag, bankgesellschaft, bankin-
stitut, bankkonto, bankkredit, banknote, blutbank, daten, daten-
bank, datenbanksystem, euro-banknote, feiertag, finanzinstitut,
flussufer, geheimkonto, geldschein, gescha?ftsbank, handelsbank,
konto, kredit, kreditinstitut, nationalbank, notenbank, sparkasse,
sparkassenverband, ufer, weltbank, weltbankgeber, west-bank,
westbank, westjordanien, westjordanland, westjordanufer, west-
ufer, zentralbank
Table 2: All German translation candidates for bank as
extracted from the gold standard
In this way, for the English lemma bank, for ex-
ample, the CL-WSD trial gold standard for German
contains the word Bank itself, together with 40 other
translation candidates, as shown in Table 2. Eight
of those are related to river banks (Ufer, but also,
e.g., Westbank and Westjordanland), three concern
databases (Datenbank), and one is for blood banks.
The rest are connected to different types of finan-
cial institutions (such as Handelsbank and Finanz-
institut, but also by association Konto, Weldbank-
geber, Banknote, Geldschein, Kredit, etc.).
2.2 N-Gram Context Matching
N-gram matching is used to produce a ranked list
of translation candidates and their contexts, both in
order to provide robustness and to give a baseline
performance. The n-gram models were built using
the IRSTLM toolkit (Federico et al, 2008; Bungum
and Gamba?ck, 2012) on the DeWaC corpus (Baroni
and Kilgarriff, 2006), using the stopword list from
NLTK (Loper and Bird, 2002). The n-gram match-
ing procedure consists of two steps:
1. An nth order source context is extracted and the
translations for each SL word in this context
are retrieved from the dictionary. This includes
stopword filtering of the context.
2. All relevant n-grams are inspected in order
from left to right and from more specific (5-
grams) to least specific (single words).
For each part of the context with matching n-grams
in the target language model, the appropriate target
translation candidates are extracted and ranked ac-
cording to their language model probability. This
results in an n-best list of translation candidates.
23
Since dictionary entries are lemma-based, lemma-
tization was necessary to use this approach in com-
bination with the dictionary enhancements. The
source context is formed by the lemmata in the sen-
tence surrounding the focus word (the word to be
disambiguated) by a window of up to four words
in each direction, limited by a 5-gram maximum
length. In order to extract the semantically most rel-
evant content, stopwords are removed before con-
structing this source word window. For each of the
1?5 lemmata in the window, the relevant translation
candidates are retrieved from the bilingual dictio-
nary. The candidates form the ordered translation
context for the source word window.
The following example illustrates how the trans-
lation context is created for the focus word ?bank?.
First the relevant part of the source language sen-
tence with the focus word in bold face:
(1) The BIS could conclude stand-by credit
agreements with the creditor countries? cen-
tral bank if they should so request.
For example, using a context of two words in front
and two words after the focus word, the following
source language context is obtained after a prepro-
cessing involving lemmatization, stopword removal,
and insertion of sentence start (<s>) and end mark-
ers (</s>):
(2) country central bank request </s>
From this the possible n-grams in the target side con-
text are generated by assembling all ordered com-
binations of the translations of the source language
words for each context length: the widest contexts
(5-grams) are looked up first before moving on to
narrower contexts, and ending up with looking up
only the translation candidate in isolation.
Each of the n-grams is looked up in the language
model and for each context part the n-grams are or-
dered according to their language model probability.
Table 3 shows a few examples of such generated n-
grams with their corresponding scores from the n-
gram language model.1 The target candidates (ital-
ics) are then extracted from the ordered list of target
language n-grams. This gives an n-best list of trans-
1There are no scores for 4- and 5-grams; as expected when
using direct translation to generate target language n-grams.
n n-gram LM score
5 land mittig bank nachsuchen </s> Not found
4 mittig bank nachsuchen </s> Not found
3 mittig bank nachsuchen Not found
3 kredit anfragen </s> -0.266291
2 mittig bank -3.382560
2 zentral blutbank -5.144870
1 bank -3.673000
Table 3: Target language n-gram examples from look-
ups of stopword-filtered lemmata country central bank
request reported in log scores. The first 3 n-grams were
not found in the language model.
lation candidates from which the top-1 or top-5 can
be taken. Since multiple senses in the dictionary can
render the same literal output, duplicate translation
candidates are filtered out from the n-best list.
2.3 Dictionary-Based Context Matching
After creating the n-gram based list of translation
candidates, additional candidates are produced by
looking at multiword entries in a bilingual dictio-
nary. The existence of multiword entries in the dic-
tionary corresponding to adjacent lemmata in the
source context or translation candidates in the target
context is taken as a clear indicator for the suitability
of a particular translation candidate. Such entries are
added to the top of the n-best list, which represents
a strong preference in the disambiguation system.
Dictionaries are used in all experiments to look up
translation candidates and target language transla-
tions of the words in the context, but this approach is
mining the dictionaries by using lookups of greater
length. Thus is, for example, the dictionary entry
Community Bank translated to the translation candi-
date Commerzbank; this translation candidate would
be put on top of the list of prioritized answers.
Two separate procedures are used to find such in-
dicators, a direct procedure based on the source con-
text and an indirect procedure based on the weaker
target language context. These are detailed in pseu-
docode in Algorithms 1 and 2, and work as follows:
Source Language (SL) Method (Algorithm 1)
If there is a dictionary entry for the source word
and one of its adjacent words, search the set
of translations for any of the translation candi-
dates for the word alone. Specifically, transla-
24
Algorithm 1 SL algorithm to rank translation candidates (tcands) for SL lemma b given list of tcands
1: procedure FINDCAND(list rlist,SL-lemma b, const tcands) . rlist is original ranking
2: comblemmas? list(previouslemma(b) + b, b + nextlemma(b)) . Find adjacent lemmata
3: for lem ? comblemmas do
4: c? sl-dictionary-lookup(lem) . Look up lemma in SL?TL dict.
5: if c ? tcands then rlist? list(c + rlist) . Push lookup result c onto rlist if in tcands
6: end if
7: end for
8: return rlist . Return new list with lemmata whose translations were in tcands on top
9: end procedure
Algorithm 2 TL algorithm to rank translation candidates (tcands) for SL lemma b given list of tcands
[The ready-made TL tcands from the dataset are looked up in TL-SL direction. It is necessary to keep a list of the
reverse-translation of the individual tcand as well as the original tcand itself, in order to monitor which tcand it was.
If the SL context is found in either of these reverse lookups the matching tcand is ranked high.]
1: procedure FINDCAND(list rlist,SL-lemma b, const tcands) . rlist is original ranking
2: for cand ? tcands do . Assemble list of TL translations
3: translist? list(cand, tl-dictionary-lookup(cand)) + translist
4: . Append TL?SL lookup results of tcands with cand as id
5: end for
6: for cand, trans ? translist do
7: if previouslemma(b)?nextlemma(b) ? trans then . If trans contains either SL lemma
8: rlist? list(cand) + rlist . append this cand onto rlist
9: end if
10: end for
11: return rlist
12: . Return tcands list; top-ranking tcands whose SL-neighbours were found in TL?SL lookup
13: end procedure
tions of the combination of the source word and
an adjacent word in the context are matched
against translation candidates for the word.
Target Language (TL) Method (Algorithm 2)
If a translation candidate looked up in the re-
verse direction matches the source word along
with one or more adjacent words, it is a good
translation candidate. TL candidates are looked
up in a TL?SL dictionary and multiword results
are matched against SL combinations of disam-
biguation words and their immediate contexts.
For both methods the dictionary entry for the tar-
get word or translation candidate is matched against
the immediate context. Thus both methods result
in two different lookups for each focus word, com-
bining it with the previous and next terms, respec-
tively. This is done exhaustively for all combina-
tions of translations of the words in the context win-
dow. Only one adjacent word was used, since very
few of the candidates were able to match the context
even with one word. Hence, virtually none would
be found with more context, making it very unlikely
that larger contexts would contribute to the disam-
biguation procedure, as wider matches would also
match the one-word contexts.
Also for both methods, translation candidates are
only added once, in case the same translation candi-
date generates hits with either (or both) of the meth-
ods. Looking at the running example, stopword fil-
tered and with lemmatized context:
(3) country central bank request
This example generates two source language multi-
word expressions, central bank and bank request. In
the source language method, these word combina-
25
tions are looked up in the dictionary where the zen-
tralbank entry is found for central bank, which is
also found as a translation candidate for bank.
The target language method works in the reverse
order, looking up the translation candidates in the
TL?SL direction and checking if the combined lem-
mata are among the candidates? translations into the
source language. In the example, the entry zentral-
bank:central bank is found in the dictionary, match-
ing the source language context, so zentralbank is
assumed to be a correct translation.
2.4 Dictionaries
Two English-German dictionaries were used in the
experiments, both with close to 1 million entries
(translations). One is a free on-line resource, while
the other was obtained by reversing an existing pro-
prietary German-English dictionary made available
to the authors by its owners:
? The GFAI dictionary (called ?D1? in Section 3
below) is a proprietary and substantially ex-
tended version of the Chemnitz dictionary, with
549k EN entries including 433k MWEs, and
552k DE entries (79k MWEs). The Chem-
nitz electronic German-English dictionary2 it-
self contains over 470,000 word translations
and is available under a GPL license.
? The freely available CC dictionary3 (?D2? be-
low) is an internet-based German-English and
English-German dictionary built through user
generated word definitions. It has 565k/440k
(total/MWE) EN and 548k/210k DE entries.
Note that the actual dictionaries are irrelevant to the
discussion at hand, and that we do not aim to point
out strengths or weaknesses of either dictionary, nor
to indicate a bias towards a specific resource.
3 Results
Experiments were carried out both on the trial and
test data described in Section 2.1 (5 trial and 20 test
words; with 20 resp. 50 instances for each word; in
total 1100 instances in need of disambiguation). The
results show that the dictionaries yield answers with
2http://dict.tu-chemnitz.de/
3http://www.dict.cc/
high precision, although they are robust enough to
solve the SemEval WSD challenge on their own.
For measuring the success rate of the developed
models, we adopt the ?Out-Of-Five? (OOF) score
(Lefever and Hoste, 2010b) from the SemEval?10
Cross-Lingual Word Sense Disambiguation task.
The Out-Of-Five criterion measures how well the
top five candidates from the system match the top
five translations in the gold standard:
OOF (i) =
?
a?Ai
freq i(a)
|Hi|
where Hi denotes the multiset of translations pro-
posed by humans for the focus word in each source
sentence si (1 ? i ? N , N being the number
of test items). Ai is the set of translations produced
by the system for source term i. Since each transla-
tion has an associated count of how many annotators
chose it, there is for each si a function freq i return-
ing this count for each term in Hi (0 for all other
terms), and max freq i gives the maximal count for
any term in Hi. For the first example in Table 1:
?
??????????
??????????
H1 = {bank, bank, bank, bank, zentralbank,
bankengesellschaft, kreditinstitut, finanzinstitut}
freq1(bank) = 4
. . .
freq1(finanzinstitut) = 1
maxfreq1 = 4
and the cardinality of the multiset is: |H1| = 8. This
equates to the sum of all top-3 preferences given to
the translation candidates by all annotators.
For the Out-Of-Five evaluation, the CL-WSD sys-
tems were allowed to submit up to five candidates
of equal rank. OOF is a recall-oriented measure
with no additional penalty for precision errors, so
there is no benefit in outputting less than five can-
didates. With respect to the previous example from
Table 1, the maximum score is obtained by system
output A1 = {bank, bankengesellschaft, kreditinstitut,
zentralbank, finanzinstitut}, which gives OOF (1) =
(4 + 1 + 1 + 1 + 1)/8 = 1, whereas A2 = {bank,
bankengesellschaft, nationalbank, notenbank, sparkasse}
would give OOF (1) = (4 + 1)/8 = 0.625.4
4Note that the maximum OOF score is not always 1 (i.e., it
is not normalized), since the gold standard sometimes contains
more than five translation alternatives.
26
Source language Target language All
Dictionary D1 D2 comb D1 D2 comb comb
Top 8.89 6.99 8.89 22.71 24.43 25.34 24.67
Low 0.00 0.00 0.00 0.00 0.00 0.00 0.00
Mean 2.71 0.99 3.04 8.35 7.10 9.24 10.13
Table 4: F1-score results for individual dictionaries
Source language Target language All
Dictionary D1 D2 comb D1 D2 comb comb
coach 1.00 0.00 1.00 0.21 0.00 0.21 0.21
education 0.83 0.67 0.83 0.47 0.62 0.54 0.53
execution 0.00 0.00 0.00 0.17 0.22 0.17 0.17
figure 1.00 0.00 1.00 0.51 0.57 0.55 0.55
job 0.88 0.80 0.94 0.45 0.78 0.46 0.44
letter 1.00 0.00 1.00 0.66 0.75 0.62 0.66
match 1.00 1.00 1.00 0.80 0.50 0.80 0.80
mission 0.71 0.33 0.71 0.46 0.37 0.36 0.36
mood 0.00 0.00 0.00 0.00 0.00 0.00 0.00
paper 0.68 0.17 0.68 0.53 0.35 0.55 0.55
post 1.00 1.00 1.00 0.39 0.48 0.45 0.48
pot 0.00 0.00 0.00 1.00 1.00 1.00 1.00
range 1.00 1.00 1.00 0.28 0.37 0.30 0.30
rest 1.00 0.67 1.00 0.60 0.56 0.56 0.58
ring 0.09 0.00 0.09 0.37 0.93 0.38 0.38
scene 1.00 0.00 1.00 0.50 0.42 0.44 0.50
side 1.00 0.00 1.00 0.21 0.16 0.23 0.27
soil 1.00 0.00 1.00 0.72 0.58 0.66 0.69
strain 0.00 0.00 0.00 0.51 0.88 0.55 0.55
test 1.00 1.00 1.00 0.62 0.52 0.57 0.61
Mean 0.84 0.74 0.84 0.50 0.56 0.49 0.51
Table 5: Precision scores for all terms filtering out those
instances for which no candidates were suggested
For assessing overall system performance in
the experiments, we take the best (?Top?), worst
(?Low?), and average (?Mean?) of the OOF scores
for all the SL focus words, with F1-score reported
as the harmonic mean of the precision and recall of
the OOF scores. Table 4 shows results for each dic-
tionary approach on the test set, with ?D1? being
the GFAI dictionary, ?D2? the CC dictionary, and
?comb? the combination of both. Target language
look-up contributes more to providing good transla-
tion candidates than the source language methodol-
ogy, and also outperforms a strategy combining all
dictionaries in both directions (?All comb?).
Filtering out the instances for which no candi-
date translation was produced, and taking the aver-
age precision scores only over these, gives the re-
sults shown in Table 5. Markedly different preci-
sion scores can be noticed, but the source language
Source language Target language
Dictionary D1 D2 D1 D2
Mean 3.25 1.5 12.65 11.45
Total 223 256 1,164 880
Table 6: Number of instances with a translation candidate
(?Mean?) and the total number of suggested candidates
Most Most Freq 5-gram 5-gram All Dict VSM
Freq Aligned + Dict Comb Model
Top 51.77 68.71 52.02 52.74 24.67 55.92
Low 1.76 9.93 14.09 15.40 0.00 10.73
Mean 21.18 34.61 30.36 36.38 10.13 30.30
Table 7: Overview of results (F1-scores) on SemEval data
method again has higher precision on the sugges-
tions it makes than the target language counterpart.
As shown in Table 6, this higher precision is offset
by lower coverage, with far fewer instances actually
producing a translation candidate with the dictionary
lookup methods. There is a notable difference in the
precision of the SL and TL approaches, coinciding
with more candidates produced by the latter. Several
words in Table 5 give 100% precision scores for at
least one dictionary, while a few give 0% precision
for some dictionaries. The word ?mood? even has
0% precision for both dictionaries in both directions.
Table 7 gives an overview of different approaches
to word translation disambiguation on the dataset.
For each method, the three lines again give both
the best and worst scoring terms, and the mean
value for all test words. The maximum attainable
score for each of those would be 99.28, 90.48 and
95.47, respectively, but those are perfect scores not
reachable for all items, as described above (OOF-
scoring). Instead the columns Most Freq and Most
Freq aligned give the baseline scores for the Sem-
Eval dataset: the translation most frequently seen
in the corpus and the translation most frequently
aligned in a word-aligned parallel corpus (Europarl),
respectively. Then follows the results when using
only a stopword-filtered 5-gram model built with the
IRSTLM language modeling kit (Federico and Cet-
tolo, 2007), and when combining the 5-gram model
with the dictionary approach (5-gram + Dict).
The next column (All Dict Comb) shows how the
dictionary methods fared on their own. The com-
27
bined dictionary approach has low recall (see Ta-
ble 6) and does not alone provide a good solution to
the overall problem. Due to high precision, however,
the approach is able to enhance the n-gram method
that already produces acceptable results. Finally, the
column VSM Model as comparison gives the results
obtained when using a Vector Space Model for word
translation disambiguation (Marsi et al, 2011).
Comparing the dictionary approach to state-of-
the-art monolingual solutions to the WTD problem
on this dataset shows that the approach performs bet-
ter for the Lowest and Mean scores of the terms, but
not for the Top scores (Lynum et al, 2012). As can
be seen in Table 7, the vector space model produced
the overall best score for a single term. However, the
method combining a 5-gram language model with
the dictionary approach was best both at avoiding
really low scores for any single term and when com-
paring the mean scores for all the terms.
4 Discussion and Conclusion
The paper has presented a method for using dictio-
nary lookups based on the adjacent words in both
the source language text and target language candi-
date translation texts to disambiguate word transla-
tion candidates. By composing lookup words by us-
ing both neighbouring words, improved disambigua-
tion performance was obtained on the data from the
SemEval?10 English-German Cross-Lingual Word
Sense Disambiguation task. The extended use of
dictionaries proves a valuable source of informa-
tion for disambiguation, and can introduce low-cost
phrase-level translation to quantitative Word Sense
Disambiguation approaches such as N-gram or Vec-
tor Space Model methods, often lacking the phrases-
based dimension.
The results show clear differences between the
source and target language methods of using dictio-
nary lookups, where the former has very high preci-
sion (0.84) but low coverage, while the TL method
compensates lower precision (0.51) with markedly
better coverage. The SL dictionary method pro-
vided answers to only between 1.5 and 3.25 of 50
instances per word on average, depending on the dic-
tionary. This owes largely to the differences in algo-
rithms, where the TL method matches any adjacent
lemma to the focus word with the translation of the
pre-defined translation candidates, whereas the SL
method matches dictionaries of the combined lem-
mata of the focus word and its adjacent words to the
same list of translation candidates. False positives
are expected with lower constraints such as these.
On the SemEval data, the contribution of the dictio-
nary methods to the n-grams is mostly in improving
the average score.
The idea of acquiring lexical information from
corpora is of course not new in itself. So did, e.g.,
Rapp (1999) use vector-space models for the pur-
pose of extracting ranked lists of translation can-
didates for extending a dictionary for word trans-
lation disambiguation. Chiao and Zweigenbaum
(2002) tried to identify translational equivalences
by investigating the relations between target and
source language word distributions in a restricted
domain, and also applied reverse-translation filtering
for improved performance, while Sadat et al (2003)
utilised non-aligned, comparable corpora to induce
a bilingual lexicon, using a bidirectional method
(SL?TL, TL?SL, and a combination of both).
Extending the method to use an arbitrary size win-
dow around all words in the context of each focus
word (not just the word itself) could identify more
multiword expressions and generate a more accurate
bag-of-words for a data-driven approach. Differ-
ences between dictionaries could also be explored,
giving more weight to translations found in two or
more dictionaries. Furthermore, the differences be-
tween the SL and TL methods could explored fur-
ther, investigating in detail the consequences of us-
ing a symmetrical dictionary, in order to study the
effect that increased coverage has on results. Test-
ing the idea on more languages will help verify the
validity of these findings.
Acknowledgements
This research has received funding from NTNU and from
the European Community?s 7th Framework Programme
under contract nr 248307 (PRESEMT). Thanks to the
other project participants and the anonymous reviewers
for several very useful comments.
28
References
Bai, M.-H., You, J.-M., Chen, K.-J., and Chang,
J. S. (2009). Acquiring translation equivalences of
multiword expressions by normalized correlation
frequencies. In Proceedings of the 2009 Confer-
ence on Empirical Methods in Natural Language
Processing, pages 478?486, Singapore. ACL.
Baroni, M. and Kilgarriff, A. (2006). Large
linguistically-processed web corpora for multiple
languages. In Proceedings of the 11th Conference
of the European Chapter of the Association for
Computational Linguistics, pages 87?90, Trento,
Italy. ACL.
Bungum, L. and Gamba?ck, B. (2012). Efficient n-
gram language modeling for billion word web-
corpora. In Proceedings of the 8th International
Conference on Language Resources and Evalua-
tion, pages 6?12, Istanbul, Turkey. ELRA. Work-
shop on Challenges in the Management of Large
Corpora.
Burnard, L., editor (2007). Reference Guide for the
British National Corpus (XML Edition). BNC
Consortium, Oxford, England. http://www.
natcorp.ox.ac.uk/XMLedition/URG.
Caseli, H. d. M., Ramisch, C., das Grac?as
Volpe Nunes, M., and Villavicencio, A. (2010).
Alignment-based extraction of multiword expres-
sions. Language Resources and Evaluation, 44(1-
2):59?77. Special Issue on Multiword expression:
hard going or plain sailing.
Chiao, Y.-C. and Zweigenbaum, P. (2002). Look-
ing for candidate translational equivalents in spe-
cialized comparable corpora. In Proceedings of
the 40th Annual Meeting of the Association for
Computational Linguistics, volume 2, pages 1?5,
Philadelphia, Pennsylvania. ACL. Also published
in AMIA Annual Symposium 2002, pp. 150?154.
Federico, M., Bertoldi, N., and Cettolo, M. (2008).
Irstlm: an open source toolkit for handling large
scale language models. In INTERSPEECH, pages
1618?1621. ISCA.
Federico, M. and Cettolo, M. (2007). Efficient han-
dling of n-gram language models for statistical
machine translation. In Proceedings of the 45th
Annual Meeting of the Association for Compu-
tational Linguistics, pages 88?95, Prague, Czech
Republic. ACL. 2nd Workshop on Statistical Ma-
chine Translation.
Koehn, P. (2005). Europarl: A parallel corpus for
statistical machine translation. In Proceedings of
the 10th Machine Translation Summit, pages 79?
86, Phuket, Thailand.
Lefever, E. and Hoste, V. (2010a). Construction
of a benchmark data set for cross-lingual word
sense disambiguation. In Proceedings of the 7th
International Conference on Language Resources
and Evaluation, pages 1584?1590, Valetta, Malta.
ELRA.
Lefever, E. and Hoste, V. (2010b). SemEval-2010
Task 3: Cross-lingual word sense disambiguation.
In Proceedings of the 48th Annual Meeting of the
Association for Computational Linguistics, pages
15?20, Uppsala, Sweden. ACL. 5th International
Workshop on Semantic Evaluation.
Loper, E. and Bird, S. (2002). NLTK: the natu-
ral language toolkit. In Proceedings of the ACL-
02 Workshop on Effective tools and methodolo-
gies for teaching natural language processing and
computational linguistics - Volume 1, ETMTNLP
?02, pages 63?70, Stroudsburg, PA, USA. Associ-
ation for Computational Linguistics.
LREC06 (2006). Proceedings of the 5th Interna-
tional Conference on Language Resources and
Evaluation, Genova, Italy. ELRA.
Lynum, A., Marsi, E., Bungum, L., and Gamba?ck,
B. (2012). Disambiguating word translations with
target language models. In Proceedings of the
15th International Conference on Text, Speech
and Dialogue, pages 378?385, Brno, Czech Re-
public. Springer.
Marsi, E., Lynum, A., Bungum, L., and Gamba?ck,
B. (2011). Word translation disambiguation with-
out parallel texts. In Proceedings of the Inter-
national Workshop on Using Linguistic Informa-
tion for Hybrid Machine Translation, pages 66?
74, Barcelona, Spain.
Rapp, R. (1999). Automatic identification of word
translations from unrelated English and German
corpora. In Proceedings of the 37th Annual Meet-
ing of the Association for Computational Linguis-
tics, pages 519?526, Madrid, Spain. ACL.
29
Ren, Z., Lu?, Y., Cao, J., Liu, Q., and Huang, Y.
(2009). Improving statistical machine translation
using domain bilingual multiword expressions. In
Proceedings of the 47th Annual Meeting of the
Association for Computational Linguistics, pages
47?54, Singapore. ACL. Workshop on Multiword
Expressions: Identification, Interpretation, Dis-
ambiguation and Applications.
Sadat, F., Yoshikawa, M., and Uemura, S. (2003).
Learning bilingual translations from comparable
corpora to cross-language information retrieval:
Hybrid statistics-based and linguistics-based ap-
proach. In Proceedings of the 41th Annual Meet-
ing of the Association for Computational Linguis-
tics, pages 57?64, Sapporo, Japan. ACL. 6th
International Workshop on Information Retrieval
with Asian languages; a shorter version published
in ACL Annual Meeting 2003, pp. 141?144.
Sag, I., Baldwin, T., Bond, F., Copestake, A., and
Flickinger, D. (2002). Multiword expressions:
A pain in the neck for NLP. In Gelbukh, A.,
editor, Computational Linguistics and Intelligent
Text Processing: Proceedings of the 3rd Interna-
tional Conference, number 2276 in Lecture Notes
in Computer Science, pages 189?206, Mexico
City, Mexico. Springer-Verlag.
Sharoff, S., Babych, B., and Hartley, A. (2006). Us-
ing collocations from comparable corpora to find
translation equivalents. In LREC06 (2006), pages
465?470.
Sofianopoulos, S., Vassiliou, M., and Tambouratzis,
G. (2012). Implementing a language-independent
MT methodology. In Proceedings of the 50th
Annual Meeting of the Association for Computa-
tional Linguistics, pages 1?10, Jeju, Korea. ACL.
First Workshop on Multilingual Modeling.
Steinberger, R., Pouliquen, B., Widiger, A., Ignat,
C., Erjavec, T., Tufis?, D., and Varga, D. (2006).
The JRC-Acquis: A multilingual aligned parallel
corpus with 20+ languages. In LREC06 (2006),
pages 2142?2147.
30
Proceedings of the Workshop on Continuous Vector Space Models and their Compositionality, pages 83?90,
Sofia, Bulgaria, August 9 2013. c?2013 Association for Computational Linguistics
Towards Dynamic Word Sense Discrimination with Random Indexing
Hans Moen, Erwin Marsi, Bjo?rn Gamba?ck
Norwegian University of Science and Technology
Department of Computer and Information and Science
Sem S?lands vei 7-9
NO-7491 Trondheim, Norway
{hansmoe,emarsi,gamback}@idi.ntnu.no
Abstract
Most distributional models of word sim-
ilarity represent a word type by a single
vector of contextual features, even though,
words commonly have more than one
sense. The multiple senses can be captured
by employing several vectors per word in a
multi-prototype distributional model, pro-
totypes that can be obtained by first con-
structing all the context vectors for the
word and then clustering similar vectors
to create sense vectors. Storing and clus-
tering context vectors can be expensive
though. As an alternative, we introduce
Multi-Sense Random Indexing, which per-
forms on-the-fly (incremental) clustering.
To evaluate the method, a number of mea-
sures for word similarity are proposed,
both contextual and non-contextual, in-
cluding new measures based on optimal
alignment of word senses. Experimental
results on the task of predicting semantic
textual similarity do, however, not show
a systematic difference between single-
prototype and multi-prototype models.
1 Introduction
Many terms have more than one meaning, or
sense. Some of these senses are static and can
be listed in dictionaries and thesauri, while other
senses are dynamic and determined by the con-
texts the terms occur in. Work in Word Sense Dis-
ambiguation often concentrate on the static word
senses, making the task of distinguishing between
them one of classification into a predefined set of
classes (i.e., the given word senses); see, e.g., Erk
et al (2013; Navigli (2009) for overviews of cur-
rent work in the area. The idea of fixed generic
word senses has received a fair amount of criti-
cism in the literature (Kilgarriff, 2000).
This paper instead primarily investigates dy-
namically appearing word senses, word senses that
depend on the actual usage of a term in a cor-
pus or a domain. This task is often referred to as
Word Sense Induction or Word Sense Discrimina-
tion (Schu?tze, 1998). This is, in contrast, essen-
tially a categorisation problem, distinguished by
different senses being more or less similar to each
other at a given time, given some input data. The
dividing line between Word Sense Disambigua-
tion and Discrimination is not necessarily razor
sharp though: also different senses of a term listed
in a dictionary tend to have some level of overlap.
In recent years, distributional models have been
widely used to infer word similarity. Most such
models represent a word type by a single vector of
contextual features obtained from co-occurrence
counts in large textual corpora. By assigning a
single vector to each term in the corpus, the re-
sulting model assumes that each term has a fixed
semantic meaning (relative to all the other terms).
However, due to homonomy and polysemy, word
semantics cannot be adequately represented by a
single-prototype vector.
Multi-prototype distributional models in con-
trast employ different vectors to represent different
senses of a word (Reisinger and Mooney, 2010).
Multiple prototypes can be obtained by first con-
structing context vectors for all words and then
clustering similar context vectors to create a sense
vector. This may be expensive, as vectors need to
stored and clustered. As an alternative, we propose
a new method called Multi-Sense Random Index-
ing (MSRI), which is based on Random Indexing
(Kanerva et al, 2000) and performs an on-the-fly
(incremental) clustering.
MSRI is a method for building a multi-
prototype / multi-sense vector space model, which
attempts to capture one or more senses per unique
term in an unsupervised manner, where each sense
is represented as a separate vector in the model.
83
This differs from the classical Random Indexing
(RI) method which assumes a static sense inven-
tory by restricting each term to have only one vec-
tor (sense) per term, as described in Section 2. The
MSRI method is introduced in Section 3.
Since the induced dynamic senses do not neces-
sarily correspond to the traditional senses distin-
guished by humans, we perform an extrinsic eval-
uation by applying the resulting models to data
from the Semantic Textual Similarity shared task
(Agirre et al, 2013), in order to compare MSRI
to the classical RI method. The experimental set-
up is the topic of Section 4, while the results of
the experiments are given in Section 5. Section 6
then sums up the discussion and points to ways in
which the present work could be continued.
2 Vector Space Models
With the introduction of LSA, Latent Semantic
Analysis (Deerwester et al, 1990), distributed
models of lexical semantics, built from unla-
belled free text data, became a popular sub-field
within the language processing research commu-
nity. Methods for building such semantic mod-
els rely primarily on term co-occurrence infor-
mation, and attempt to capture latent relations
from analysing large amounts of text. Most of
these methods represent semantic models as multi-
dimensional vectors in a vector space model.
After LSA, other methods for building seman-
tic models have been proposed, one of them being
Random Indexing (Kanerva et al, 2000). Com-
mon to these methods is that they generate a con-
text vector for each unique term in the training data
which represents the term?s ?contextual? meaning
in the vector space. By assigning a single con-
text vector to each term in the corpus, the resulting
model assumes that each term has a fixed semantic
meaning (relative to all other terms).
Random Indexing incrementally builds a co-
occurrence matrix of reduced dimensionality, by
first assigning index vectors to each unique term.
The vectors are of a predefined size (typically
around 1000), and consist of a few randomly
placed 1s and -1s. Context vectors of the same size
are also assigned to each term, initially consisting
of only zeros. When traversing a document corpus
using a sliding window of a fixed size, the context
vectors are continuously updated: the term in the
centre of the window (the target term), has the in-
dex vectors of its neighbouring terms (the ones in
the window) added to its context vector using vec-
tor summation. Then the cosine similarity mea-
sure can be used on term pairs to calculate their
similarity (or ?contextual similarity?).
Random Indexing has achieved promising re-
sults in various experiments, for example, on the
TOEFL test (?Test of English as a Foreign Lan-
guage?) (Kanerva et al, 2000). However, it is ev-
ident that many terms have more than one mean-
ing or sense, some being static and some dynamic,
that is, determined by the contexts the terms occur
in. Schu?tze (1998) proposed a method for clus-
tering the contextual occurrences of terms into in-
dividual ?prototype? vectors, where one term can
have multiple prototype vectors representing sep-
arate senses of the term. Others have adopted
the same underlying idea, using alternative meth-
ods and techniques (Reisinger and Mooney, 2010;
Huang et al, 2012; Van de Cruys et al, 2011; Dinu
and Lapata, 2010).
3 Multi-Sense Random Indexing, MSRI
Inspired by the work of Schu?tze (1998) and
Reisinger and Mooney (2010), this paper intro-
duces a novel variant of Random Indexing, which
we have called ?Multi-Sense Random Indexing?.
MSRI attempts to capture one or more senses per
unique term in an unsupervised and incremental
manner, each sense represented as an separate vec-
tor in the model. The method is similar to classical
sliding window RI, but each term can have mul-
tiple context vectors (referred to as sense vectors
here) which are updated separately.
When updating a term vector, instead of directly
adding the index vectors of the neighbouring terms
in the window to its context vector, the system first
computes a separate window vector consisting of
the sum of the index vectors. The similarity be-
tween the window vector and each of the term?s
sense vectors is calculated. Each similarity score
is then compared to a pre-set similarity threshold:
? if no score exceeds the threshold, the window
vector becomes a new separate sense vector
for the term,
? if exactly one score is above the threshold,
the window vector is added to that sense vec-
tor, and
? if multiple scores are above the threshold, all
the involved senses are merged into one sense
vector, together with the window vector.
84
Algorithm 1 MSRI training
for all terms t in a document D do
generate window vector ~win from the neigh-
bouring words? index vectors
for all sense vectors ~si of t do
sim(si) = CosSim( ~win,~si)
end for
if sim(si..k) ? ? then
Merge ~si..k and ~win through summing
else
if sim(si) ? ? then
~si+ = ~win
end if
else
if sim(si..n) < ? then
Assign ~win as new sense vector of t
end if
end if
end for
See Algorithm 1 for a pseudo code version. Here
? represents the similarity threshold.
This accomplishes an incremental (on-line)
clustering of senses in an unsupervised manner,
while retaining the other properties of classical RI.
Even though the algorithm has a slightly higher
complexity than classical RI, this is mainly a mat-
ter of optimisation, which is not the focus of this
paper. The incremental clustering that we apply
is somewhat similar to what is used by Lughofer
(2008), although we are storing in memory only
one element (i.e., vector) for each ?cluster? (i.e.,
sense) at any given time.
When looking up a term in the vector space, a
pre-set sense-frequency threshold is applied to fil-
ter out ?noisy? senses. Hence, senses that have
occurred less than the threshold are not included
when looking up a term and its senses for, for ex-
ample, similarity calculations.
As an example of what the resulting models
contain in terms of senses, Table 1 shows four dif-
ferent senses of the term ?round? produced by the
MSRI model. Note that these senses do not nec-
essarily correspond to human-determined senses.
The idea is only that using multiple prototype
vectors facilitates better modelling of a term?s
meaning than a single prototype (Reisinger and
Mooney, 2010).
round1 round2 round3 round4
finish camping inch launcher
final restricted bundt grenade
match budget dough propel
half fare thick antitank
third adventure cake antiaircraft
Table 1: Top-5 most similar terms for four dif-
ferent senses of ?round? using the Max similarity
measure to the other terms in the model.
3.1 Term Similarity Measures
Unlike classical RI, which only has a single con-
text vector per term and thus calculates similarity
between two terms directly using cosine similarity,
there are multiple ways of calculating the similar-
ity between two terms in MSRI. Some alternatives
are described in Reisinger and Mooney (2010). In
the experiment in this paper, we test four ways of
calculating similarity between two terms t and t?
in isolation, with the Average and Max methods
stemming from Reisinger and Mooney (2010).
Let ~si..n and ~s?j..m be the sets of sense vectors
corresponding to the terms t and t? respectively.
Term similarity measures are then defined as:
Centroid
For term t, compute its centroid vector by
summing its sense vectors ~si..n. The same is
done for t? with its sense vectors ~s?j..m. These
centroids are in turn used to calculate the co-
sine similarity between t and t?.
Average
For all ~si..n in t, find the pair ~si, ~s?j with high-
est cosine similarity:
1
n
n?
i=1
CosSimmax(~si, ~s?j)
Then do the same for all ~s?j..m in t?:
1
m
m?
j=1
CosSimmax(~s?j , ~si)
The similarity between t and t? is computed
as the average of these two similarity scores.
Max
The similarity between ti and t?i equals the
similarity of their most similar sense:
Sim(t, t?) = CosSimmaxij (~si, ~s?i)
85
Hungarian Algorithm
First cosine similarity is computed for each
possible pair of sense vectors ~si..n and ~s?j..m,
resulting in a matrix of similarity scores.
Finding the optimal matching from senses ~si
to ~s?j that maximises the sum of similarities
is known as the assignment problem. This
combinatorial optimisation problem can be
solved in polynomial time through the Hun-
garian Algorithm (Kuhn, 1955). The over-
all similarity between terms t and t? is then
defined as the average of the similarities be-
tween their aligned senses.
All measures defined so far calculate similarity be-
tween terms in isolation. In many applications,
however, terms occur in a particular context that
can be exploited to determine their most likely
sense. Narrowing down their possible meaning to
a subset of senses, or a single sense, can be ex-
pected to yield a more adequate estimation of their
similarity. Hence a context-sensitive measure of
term similarity is defined as:
Contextual similarity
Let ~C and ~C ? be vectors representing the con-
texts of terms t and t? respectively. These
context vectors are constructed by summing
the index vectors of the neighbouring terms
within a window, following the same proce-
dure as used when training the MSRI model.
We then find s? and s? ? as the sense vectors
best matching the context vectors:
s? = argmaxi CosSim(~si, ~C)
s? ? = argmaxj CosSim(~sj , ~C ?)
Finally, contextual similarity is defined as the
similarity between these sense vectors:
Simcontext(t, t
?) = CosSim(s?, s? ?)
3.2 Sentence Similarity Features
In the experiments reported on below, a range of
different ways to represent sentences were tested.
Sentence similarity was generally calculated by
the average of the maximum similarity between
pairs of terms from both sentences, respectively.
The different ways of representing the data in
combination with some sentence similarity mea-
sure will here be referred to as similarity features.
1. MSRI-TermCentroid:
In each sentence, each term is represented as
the sum of its sense vectors. This is similar
to having one context vector, as in classical
RI, but due to the sense-frequency filtering,
potentially ?noisy? senses are not included.
2. MSRI-TermMaxSense:
For each bipartite term pair in the two sen-
tences, their sense-pairs with maximum co-
sine similarity are used, one sense per term.
3. MSRI-TermInContext:
A 5 + 5 window around each (target) term
is used as context for selecting one sense of
the term. A window vector is calculated by
summing the index vectors of the other terms
in the window (i.e., except for the target term
itself). The sense of the target term which is
most similar to the window vector is used as
the representation of the term.
4. MSRI-TermHASenses:
Calculating similarity between two terms is
done by applying the Hungarian Algorithm
to all their bipartite sense pairs.
5. RI-TermAvg:
Classical Random Indexing ? each term is
represented as a single context vector.
6. RI-TermHA:
Similarity between two sentences is calcu-
lated by applying the Hungarian Algorithm to
the context vectors of each constituent term.
The parameters were selected based on a com-
bination of surveying previous work on RI (e.g.,
Sokolov (2012)), and by analysing how sense
counts evolved during training. For MSRI, we
used a similarity threshold of 0.2, a vector dimen-
sionality of 800, a non-zero count of 6, and a win-
dow size of 5 + 5. Sense vectors resulting from
less than 50 observations were removed. For clas-
sical RI, we used the same parameters as for MSRI
(except for a similarity threshold).
4 Experimental Setup
In order to explore the potential of the MSRI
model and the textual similarity measures pro-
posed here, experiments were carried out on data
from the Semantic Textual Similarity (STS) shared
task (Agirre et al, 2012; Agirre et al, 2013).
86
Given a pair of sentences, systems participating
in this task shall compute how semantically sim-
ilar the two sentences are, returning a similar-
ity score between zero (completely unrelated) and
five (completely semantically equivalent). Gold
standard scores are obtained by averaging multi-
ple scores obtained from human annotators. Sys-
tem performance is then evaluated using the Pear-
son product-moment correlation coefficient (?) be-
tween the system scores and the human scores.
The goal of the experiments reported here was
not to build a competitive STS system, but rather
to investigate whether MSRI can outperform clas-
sical Random Indexing on a concrete task such as
computing textual similarity, as well as to identify
which similarity measures and meaning represen-
tations appear to be most suitable for such a task.
The system is therefore quite rudimentary: a sim-
ple linear regression model is fitted on the training
data, using a single sentence similarity measure
as input and the similarity score as the dependent
variable. The implementations of RI and MSRI
are based on JavaSDM (Hassel, 2004).
As data for training random indexing models,
we used the CLEF 2004?2008 English corpus,
consisting of approximately 130M words of news-
paper articles (Peters et al, 2004). All text was
tokenized and lemmatized using the TreeTagger
for English (Schmid, 1994). Stopwords were re-
moved using a customized version of the stoplist
provided by the Lucene project (Apache, 2005).
Data for fitting and evaluating the linear re-
gression models came from the STS development
and test data, consisting of sentence pairs with
a gold standard similarity score. The STS 2012
development data stems from the Microsoft Re-
search Paraphrase corpus (MSRpar, 750 pairs),
the Microsoft Research Video Description cor-
pus (MSvid, 750 pairs), and statistical machine
translation output based on the Europarl corpus
(SMTeuroparl, 734 pairs). Test data for STS
2012 consists of more data from the same sources:
MSRpar (750 pairs), MSRvid (750 pairs) and
SMTeuroparl (459 pairs). In addition, different
test data comes from translation data in the news
domain (SMTnews, 399 pairs) and ontology map-
pings between OntoNotes and WordNet (OnWN,
750 pairs). When testing on the STS 2012 data, we
used the corresponding development data from the
same domain for training, except for OnWN where
we used all development data combined.
The development data for STS 2013 consisted
of all development and test data from STS 2012
combined, whereas test data comprised machine
translation output (SMT, 750 pairs), ontology
mappings both between WordNet and OntoNotes
(OnWN, 561 pairs) and between WordNet and
FrameNet (FNWN, 189 pairs), as well as news ar-
ticle headlines (HeadLine, 750 pairs). For sim-
plicity, all development data combined were used
for fitting the linear regression model, even though
careful matching of development and test data sets
may improve performance.
5 Results and Discussion
Table 2 shows Pearson correlation scores per fea-
ture on the STS 2012 test data using simple linear
regression. The most useful features for each data
set are marked in bold. For reference, the scores of
the best performing STS systems for each data set
are also shown, as well as baseline scores obtained
with a simple normalized token overlap measure.
There is large variation in correlation scores,
ranging from 0.77 down to 0.27. Part of this vari-
ation is due to the different nature of the data sets.
For example, sentence similarity in the SMT do-
main seems harder to predict than in the video
domain. Yet there is no single measure that ob-
tains the highest score on all data sets. There is
also no consistent difference in performance be-
tween the RI and MSRI measures, which seem
to yield about equal scores on average. The
MSRI-TermInContext measure has the low-
est score on average, suggesting that word sense
disambiguation in context is not beneficial in its
current implementation.
The corresponding results on the STS 2013 test
data are shown in Table 3. The same observations
as for the STS 2012 data set can be made: again
there was no consistent difference between the RI
and MSRI features, and no single best measure.
All in all, these results do not provide any ev-
idence that MSRI improves on standard RI for
this particular task (sentence semantic similarity).
Multi-sense distributional models have, however,
been found to outperform single-sense models on
other tasks. For example, Reisinger and Mooney
(2010) report that multi-sense models significantly
increase the correlation with human similarity
judgements. Other multi-prototype distributional
models may yield better results than their single-
prototype counterparts on the STS task.
87
Features: MSRpar MSRvid SMTeuroparl SMTnews OnWN Mean
Best systems 0.73 0.88 0.57 0.61 0.71 0.70
Baseline 0.43 0.30 0.45 0.39 0.59 0.43
RI-TermAvg 0.44 0.71 0.50 0.42 0.65 0.54
RI-TermHA 0.41 0.72 0.44 0.35 0.56 0.49
MSRI-TermCentroid 0.45 0.73 0.50 0.33 0.64 0.53
MSRI-TermHASenses 0.40 0.77 0.47 0.39 0.68 0.54
MSRI-TermInContext 0.33 0.55 0.36 0.27 0.42 0.38
MSRI-TermMaxSense 0.44 0.71 0.50 0.32 0.64 0.52
Table 2: Pearson correlation scores per feature on STS 2012 test data using simple linear regression
Feature Headlines SMT FNWN OnWN Mean
Best systems 0.78 0.40 0.58 0.84 0.65
Baseline 0.54 0.29 0.21 0.28 0.33
RI-TermAvg 0.60 0.37 0.21 0.52 0.42
RI-TermHA 0.65 0.36 0.27 0.52 0.45
MSRI-TermCentroid 0.60 0.35 0.37 0.45 0.44
MSRI-TermHASenses 0.63 0.35 0.33 0.54 0.46
MSRI-TermInContext 0.20 0.29 0.19 0.36 0.26
MSRI-TermMaxSense 0.58 0.35 0.31 0.45 0.42
Table 3: Pearson correlation scores per feature on STS 2013 test data using simple linear regression
Notably, the more advanced features used in our
experiment, such as MSRI-TermInContext,
gave very clearly inferior results when compared
to MSRI-TermHASenses. This suggests that
more research on MSRI is needed to understand
how both training and retrieval can be fully uti-
lized and optimized.
6 Conclusion and Future Work
The paper introduced a new method called Multi-
Sense Random Indexing (MSRI), which is based
on Random Indexing and performs on-the-fly
clustering, as an efficient way to construct multi-
prototype distributional models for word similar-
ity. A number of alternative measures for word
similarity were proposed, both context-dependent
and context-independent, including new measures
based on optimal alignment of word senses us-
ing the Hungarian algorithm. An extrinsic eval-
uation was carried out by applying the resulting
models to the Semantic Textual Similarity task.
Initial experimental results did not show a sys-
tematic difference between single-prototype and
multi-prototype models in this task.
There are many questions left for future work.
One of them is how the number of senses per word
evolves during training and how the distribution
of senses in the final model looks like. So far we
only know that on average the number of senses
keeps growing with more training material, cur-
rently resulting in about 5 senses per word at the
end of training (after removing senses with fre-
quency below the sense-frequency threshold). It
is worth noting that this depends heavily on the
similarity threshold for merging senses, as well as
on the weighting schema used.
In addition there are a number of model para-
meters that have so far only been manually tuned
on the development data, such as window size,
number of non-zeros, vector dimensionality, and
the sense frequency filtering threshold. A system-
atic exploration of the parameter space is clearly
desirable. Another thing that would be worth
looking into, is how to compose sentence vectors
and document vectors from the multi-sense vector
space in a proper way, focusing on how to pick
the right senses and how to weight these. It would
also be interesting to explore the possibilities for
combining the MSRI method with the Reflective
Random Indexing method by Cohen et al (2010)
in an attempt to model higher order co-occurrence
relations on sense level.
The fact that the induced dynamic word senses
do not necessarily correspond to human-created
senses makes evaluation in traditional word sense
disambiguation tasks difficult. However, correla-
88
tion to human word similarity judgement may pro-
vide a way of intrinsic evaluation of the models
(Reisinger and Mooney, 2010). The Usim bench
mark data look promising for evaluation of word
similarity in context (Erk et al, 2013).
It is also worth exploring ways to optimise the
algorithm, as this has not been the focus of our
work so far. This would also allow faster training
and experimentation on larger text corpora, such
as Wikipedia. In addition to the JavaSDM pack-
age (Hassel, 2004), Lucene (Apache, 2005) with
the Semantic Vectors package (Widdows and Fer-
raro, 2008) would be an alternative framework for
implementing the proposed MSRI algorithm.
Acknowledgements
This work was partly supported by the Re-
search Council of Norway through the EviCare
project (NFR project no. 193022) and by the
European Community?s Seventh Framework Pro-
gramme (FP7/20072013) under grant agreement
nr. 248307 (PRESEMT). Part of this work has
been briefly described in our contribution to the
STS shared task (Marsi et al, 2013).
References
Eneko Agirre, Daniel Cer, Mona Diab, and Aitor
Gonzalez-Agirre. 2012. SemEval-2012 Task 6: A
pilot on semantic textual similarity. In Proceedings
of the First Joint Conference on Lexical and Com-
putational Semantics (*SEM), volume 2: Proceed-
ings of the Sixth International Workshop on Seman-
tic Evaluation, pages 385?393, Montreal, Canada,
June. Association for Computational Linguistics.
Eneko Agirre, Daniel Cer, Mona Diab, Aitor Gonzalez-
Agirre, and Weiwei Guo. 2013. *SEM 2013 shared
task: Semantic textual similarity. In Second Joint
Conference on Lexical and Computational Seman-
tics (*SEM), volume 1: Proceedings of the Main
Conference and the Shared Task: Semantic Textual
Similarity, pages 32?43, Atlanta, Georgia, June. As-
sociation for Computational Linguistics.
Apache. 2005. Apache Lucene open source package.
http://lucene.apache.org/.
Trevor Cohen, Roger Schvaneveldt, and Dominic Wid-
dows. 2010. Reflective random indexing and indi-
rect inference: A scalable method for discovery of
implicit connections. Journal of Biomedical Infor-
matics, 43(2):240?256, April.
Scott Deerwester, Susan T. Dumais, George W. Fur-
nas, Thomas K. Landauer, and Richard Harshman.
1990. Indexing by latent semantic analysis. Jour-
nal of the American Society for Information Science,
41(6):391?407.
Georgiana Dinu and Mirella Lapata. 2010. Measur-
ing distributional similarity in context. In Proceed-
ings of the 2010 Conference on Empirical Methods
in Natural Language Processing, pages 1162?1172,
Cambridge, Massachusetts, October. Association for
Computational Linguistics.
Katrin Erk, Diana McCarthy, and Nicholas Gaylord.
2013. Measuring word meaning in context. Com-
putational Linguistics, 39(3):501?544.
Martin Hassel. 2004. JavaSDM package. http:
//www.nada.kth.se/?xmartin/java/.
School of Computer Science and Communication;
Royal Institute of Technology (KTH); Stockholm,
Sweden.
Eric H. Huang, Richard Socher, Christopher D. Man-
ning, and Andrew Y. Ng. 2012. Improving word
representations via global context and multiple word
prototypes. In Proceedings of the 50th Annual Meet-
ing of the Association for Computational Linguis-
tics: Long Papers - Volume 1, ACL ?12, pages 873?
882, Jeju Island, Korea. Association for Computa-
tional Linguistics.
Pentti Kanerva, Jan Kristoferson, and Anders Holst.
2000. Random indexing of text samples for latent
semantic analysis. In Proceedings of the 22nd An-
nual Conference of the Cognitive Science Society,
page 1036, Philadelphia, Pennsylvania. Erlbaum.
Adam Kilgarriff. 2000. I don?t believe in word senses.
Computers and the Humanities, 31(2):91?113.
Harold W. Kuhn. 1955. The Hungarian method for
the assignment problem. Naval Research Logistics
Quarterly, 2:83?97.
Edwin Lughofer. 2008. Extensions of vector quantiza-
tion for incremental clustering. Pattern Recognition,
41(3):995?1011, March.
Erwin Marsi, Hans Moen, Lars Bungum, Gleb Sizov,
Bjo?rn Gamba?ck, and Andre? Lynum. 2013. NTNU-
CORE: Combining strong features for semantic sim-
ilarity. In Second Joint Conference on Lexical and
Computational Semantics (*SEM), volume 1: Pro-
ceedings of the Main Conference and the Shared
Task: Semantic Textual Similarity, pages 66?73, At-
lanta, Georgia, June. Association for Computational
Linguistics.
Roberto Navigli. 2009. Word Sense Disambiguation:
a survey. ACM Computing Surveys, 41(2):1?69.
Carol Peters, Paul Clough, Julio Gonzalo, Gareth J.F.
Jones, Michael Kluck, and Bernardo Magnini, ed-
itors. 2004. Multilingual Information Access
for Text, Speech and Images, 5th Workshop of the
Cross-Language Evaluation Forum, CLEF 2004,
volume 3491 of Lecture Notes in Computer Science.
Springer-Verlag, Bath, England.
89
Joseph Reisinger and Raymond J. Mooney. 2010.
Multi-prototype vector-space models of word mean-
ing. In Human Language Technologies: The 2010
Annual Conference of the North American Chap-
ter of the Association for Computational Linguistics,
pages 109?117, Los Angeles, California, June.
Helmut Schmid. 1994. Probabilistic part-of-speech
tagging using decision trees. In Proceedings of the
1st International Conference on New Methods in
Natural Language Processing, pages 44?49, Univer-
sity of Manchester Institute of Science and Technol-
ogy, Manchester, England, September.
Hinrich Schu?tze. 1998. Automatic word sense dis-
crimination. Computational Linguistics, 24(1):97?
123, March.
Artem Sokolov. 2012. LIMSI: learning semantic
similarity by selecting random word subsets. In
Proceedings of the First Joint Conference on Lexi-
cal and Computational Semantics (*SEM), volume
2: Proceedings of the Sixth International Workshop
on Semantic Evaluation, pages 543?546, Montreal,
Canada, June. Association for Computational Lin-
guistics.
Tim Van de Cruys, Thierry Poibeau, and Anna Korho-
nen. 2011. Latent vector weighting for word mean-
ing in context. In Proceedings of the 2011 Con-
ference on Empirical Methods in Natural Language
Processing, pages 1012?1022, Edinburgh, Scotland,
July. Association for Computational Linguistics.
Dominic Widdows and Kathleen Ferraro. 2008. Se-
mantic vectors: a scalable open source package and
online technology management application. In Pro-
ceedings of the Sixth International Language Re-
sources and Evaluation (LREC?08), pages 1183?
1190, Marrakech, Morocco.
90
Proceedings of the 5th International Workshop on Health Text Mining and Information Analysis (Louhi) @ EACL 2014, pages 116?124,
Gothenburg, Sweden, April 26-30 2014.
c
?2014 Association for Computational Linguistics
Care Episode Retrieval
Hans Moen
1
, Erwin Marsi
1
, Filip Ginter
2
,
Laura-Maria Murtola
3,4
, Tapio Salakoski
2
, Sanna Salanter
?
a
3,4
,
1
Dept. of Computer and Information Science,
Norwegian University of Science and Technology, Norway
2
Dept. of Information Technology, University of Turku, Finland
3
Dept. of Nursing Science, University of Turku, Finland
4
Turku University Hospital, Finland
{hans.moen,emarsi}@idi.ntnu.no, ginter@cs.utu.fi,
{lmemur,tapio.salakoski,sansala}@utu.fi
Abstract
The documentation of a care episode con-
sists of clinical notes concerning patient
care, concluded with a discharge sum-
mary. Care episodes are stored electron-
ically and used throughout the health care
sector by patients, administrators and pro-
fessionals from different areas, primarily
for clinical purposes, but also for sec-
ondary purposes such as decision support
and research. A common use case is, given
a ? possibly unfinished ? care episode,
to retrieve the most similar care episodes
among the records. This paper presents
several methods for information retrieval,
focusing on care episode retrieval, based
on textual similarity, where similarity is
measured through domain-specific mod-
elling of the distributional semantics of
words. Models include variants of random
indexing and a semantic neural network
model called word2vec. A novel method is
introduced that utilizes the ICD-10 codes
attached to care episodes to better induce
domain-specificity in the semantic model.
We report on an experimental evaluation
of care episode retrieval that circumvents
the lack of human judgements regarding
episode relevance by exploiting (1) ICD-
10 codes of care episodes and (2) seman-
tic similarity between their discharge sum-
maries. Results suggest that several of the
methods proposed outperform a state-of-
the art search engine (Lucene) on the re-
trieval task.
1 Introduction
Information retrieval (IR) aims at retrieving and
ranking documents relative to a textual query ex-
pressing the information need of a user (Manning
et al., 2008). IR has become a crucial technology
for many organisations that deal with vast amounts
of partly structured and unstructured (free text)
data stored in electronic format, including hospi-
tals and other health care providers. IR is an es-
sential part of the clinical practice; e.g., on-line IR
systems are associated with substantial improve-
ments in clinicians decision-making concerning
clinical problems (Westbrook et al., 2005).
The different stages of the clinical care of a pa-
tient are documented in clinical care notes, con-
sisting mainly of free text. A care episode consists
of a sequence of individual clinical care notes,
concluded by a discharge summary, as illustrated
in Figure 1. Care episodes are stored in elec-
tronic format in electronic health record (EHR)
systems. These systems are used throughout the
health care sector by patients, administrators and
professionals from different areas, primarily for
clinical purposes, but also for secondary purposes
such as decision support and research (H?ayrinen et
al., 2008). IR from EHR in general is therefore a
common and important task.
This paper focuses on the particular task of re-
trieving those care episodes that are most similar
to the sequence of clinical notes for a given pa-
tient, which we will call care episode retrieval.
In conventional IR, the query typically consists of
several keywords or a short phrase, while the re-
trievable units are typically documents. In con-
trast, in care episode retrieval, the query consist of
the clinical notes contained in a care episode. The
discharge summary is used separately for evalu-
116
A
BTime
Clinical notes Dischargesummary
Figure 1: Illustration of care episode retrieval. The
two care episodes (A and B) are composed of
a number of individual clinical notes and a sin-
gle discharge summary. Given an ongoing care
episode (minus the discharge summary), the task
is to retrieve other, similar care episodes.
ation purposes, and is assumed to be unavailable
for constructing a query at retrieval time. Retriev-
able units are thus complete care episodes without
summaries.
We envision a number of different use cases for
a care episode retrieval system. Firstly, it could fa-
cilitate clinicians in decision-making. For exam-
ple, given a patient that is being treated in a hos-
pital, an involved clinician may want to find previ-
ous patients that are similar in terms of their health
history, symptoms or received treatments. Supple-
mentary input from the clinician would enable the
system to give heightened weight to keywords of
particular interest within the care episodes, which
would further be emphasized in the semantic sim-
ilarity calculation during IR. It may help consider-
ably to see what similar patients have received in
terms of medication and further treatment, what
related issues such as bi-conditions or risks oc-
curred, how other clinicians have described cer-
tain aspects, what clinical practice guidelines have
been utilized, and so on. This relates to the un-
derlying principle in textual case-based reasoning
(Lenz et al., 1998). Secondly, it could help man-
agement to get almost real time information con-
cerning the overall situation on the unit for a spe-
cific follow-up period. Such a system could for ex-
ample support managerial decision-making with
statistical information concerning care trends on
the unit, adverse events or infections. Thirdly, it
could facilitate knowledge discovery and research.
For instance, it could enable researchers to map
or cluster similar care episodes to find common
symptoms or conditions. In sum, care episode re-
trieval is likely to improve care quality and consis-
tency in hospitals.
From the perspective of NLP, care episode re-
trieval ? and IR from EHRs in general ? is a
challenging task. It differs from general-purpose
web search in that the vocabulary, the informa-
tion needs and the queries of clinicians are highly
specialised (Yang et al., 2011). Clinical notes
contain highly domain-specific terminology (Rec-
tor, 1999; Friedman et al., 2002; Allvin et al.,
2010) and generic text processing resources are
therefore often suboptimal or inadequate (Shatkay,
2005). At the same time, development of dedi-
cated clinical NLP tools and resources is often dif-
ficult and costly. For example, popular data-driven
approaches to NLP are based on supervised learn-
ing, which requires substantial amounts of tailored
training data, typically built through manual anno-
tation by annotators who need both linguistic and
clinical knowledge. Additionally, variations in the
language and terminology used in sub-domains
within and across health care organisations greatly
limit the scope of applicability of such training
data (Rector, 1999).
Recent work has shown that distributional mod-
els of semantics, induced in an unsupervised man-
ner from large corpora of clinical and/or medical
text, are well suited as a resource-light approach
to capturing and representing domain-specific ter-
minology (Pedersen et al., 2007; Koopman et al.,
2012; Henriksson et al., 2014). This raises the
question to what extent distributional models of
semantics can alleviate the aforementioned prob-
lems of NLP in the clinical domain. The work
reported here investigates to what extent distribu-
tional models of semantics, built from a corpus of
clinical text in an fully unsupervised manner, can
be used for care episode retrieval. Models include
several variants of random indexing and a seman-
tic neural network model called word2vec, which
will be described in more detail in Section 4.
It has been argued that clinical NLP should ex-
ploit existing knowledge resources such as knowl-
edge bases about medications, treatments, dis-
eases, symptoms and care plans, despite these not
having been explicitly built for doing clinical NLP
(Friedman et al., 2013). Along these lines, a novel
method is proposed here that utilizes the ICD-10
codes ? diagnostic labels attached to care episodes
by clinicians ? to better induce domain-specificity
in the semantic model. Experimental results sug-
gest that this method outperforms a state-of-the art
search engine (Lucene) on the task of care episode
117
retrieval.
Apart from issues related to clinical terminol-
ogy, another problem in care episode retrieval is
the lack of benchmark data, such as the relevance
scores produced by human judges commonly used
for evaluation of IR systems. Although collec-
tions of care episodes may be available, producing
gold standard similarity scores required for evalu-
ation is costly. Another contribution of this paper
is the proposal of evaluation procedures that cir-
cumvent the lack of human judgements regarding
episode similarity. This is accomplished by ex-
ploiting either (1) ICD-10 codes of care episodes
or (2) semantic similarity between their discharge
summaries. Despite our focus on the specific task
of care episode retrieval, we hypothesize that the
methods and models proposed here have the po-
tential to increase performance of IR on clinical
text in general.
2 Data
The data set used in this study consists of the elec-
tronic health records from patients with any type
of heart related problem that were admitted to one
particular university hospital in Finland between
the years 2005-2009. Of these, only the clini-
cal notes written by physician are used. A sup-
porting statement for the research was obtained
from the Ethics Committee of the Hospital District
(17.2.2009 ?67) and permission to conduct the re-
search was obtained from the Medical Director of
the Hospital District (2/2009). The total set consist
of 66884 care episodes, which amounts to 398040
notes and 64 million words in total. This full set
was used for training of the semantic models. To
make the experimentation more convenient, we
chose to use a subset for evaluation. This com-
prises 26530 care episodes, amounting to 155562
notes and 25.7 million words in total.
Notes are mostly unstructured, consisting of
free text in Finnish. Some meta-data ? such as
names of the authors, dates, wards, and so on ? is
present, but is not used for retrieval.
Care episodes have been manually labeled ac-
cording to the 10th revision of the International
Classification of Diseases (ICD-10) (World Health
Organization and others, 2013), a standardised
tool of diagnostic codes for classifying diseases.
Codes are normally applied at the end of the pa-
tient?s stay, or even after the patient has been dis-
charged from the hospital. Care episodes have
one primary ICD-10 code attached and optionally
a number of additionally relevant codes. In this
study, only the primary one is used, because ex-
traction of the secondary codes is non-trivial.
ICD-10 codes have an internal structure that re-
flects the classification system ranging from broad
categories down to fine-grained subjects. For ex-
ample, the first character (J) of the code J21.1
signals that it belongs to the broad category Dis-
eases of the respiratory system. The next two
digits (21) classify the subject as belonging to
the subcategory Acute bronchiolitis. Finally, the
last digit after the dot (1) means that it belongs
to the sub-subclass Acute bronchiolitis due to hu-
man metapneumovirus. There are 356 unique ?pri-
mary? ICD-10 codes in the evaluation data set.
3 Task
The task addressed in this study is retrieval of care
episodes that are similar to each other. In con-
trast to the normal IR setting, where the search
query is derived from a text stating the user?s in-
formation need, here the query is based on an-
other care episode, which we refer to as the query
episode. As the query episode may document on-
going treatment, and thus lack a discharge sum-
mary and ICD-10 code, neither of these informa-
tion sources can be relied upon for constructing
the query. The task is therefore to retrieve the most
similar care episodes using only the information
contained in the free text of the clinical notes in
the query episode.
Evaluation of retrieval results generally re-
quires an assessment of their relevancy to the
query. Since similarity judgements by humans
are currently lacking, and obtaining these is time-
consuming and costly, we explored alternative
ways of evaluating performance on the task. The
first alternative is to assume that care episodes are
similar if they have the same ICD-10 code. That is,
a retrieved care episode is considered correct if its
ICD-10 code is identical to the code of the query
episode. It should be noted that ICD-10 codes are
not used in the query in any of the experiments.
Closer inspection shows that the free text con-
tent in care episodes with the same ICD-10 code
is indeed quite similar in many cases, but not al-
ways. Considering all of them equally similar
amounts to an arguably coarse approximation of
relevance. The second alternative tries to remedy
this issue by measuring the similarity between dis-
118
charge summaries. That is, if the discharge sum-
mary of a retrieved episode is semantically simi-
lar to the discharge summary of the query episode,
the retrieved episode is assumed to be correct.
In practice, textual similarity between discharge
summaries, and therefore the relevance score, is
continuous rather than binary. It is measured using
the same models of distributional semantics used
for retrieval, which will be described in Section 4.
It should be stressed that the discharge summaries
are not taken into consideration during retrieval in
any of the experiments and are only used for eval-
uation.
4 Method
4.1 Semantic models
A crucial part in retrieving similar care episodes
is having a good similarity measure. Here similar-
ity between care episodes is measured as the sim-
ilarity between the words they contain (see Sec-
tion 4.2). Semantic similarity between words is in
turn measured through the use of word space mod-
els (WSM), without performing an explicit query
expansion step. Several variants of these models
were tested, utilizing different techniques and pa-
rameters for building them. The models trained
and tested in this paper are: (1) classic random
indexing with a sliding window using term in-
dex vectors and term context vectors (RI-Word);
(2) random indexing with index vectors for doc-
uments (RI-Doc); (3) random indexing with in-
dex vectors for ICD-10 codes (RI-ICD); (4) a ver-
sion of random indexing where only the term in-
dex vectors are used (RI-Index); and (5) a seman-
tic neural network model, using word2vec to build
word context vectors (Word2vec).
RI-Word
Random Indexing (RI) (Kanerva et al., 2000) is
a method for building a (pre) compressed WSM
with a fixed dimensionality, done in an incremen-
tal fashion. RI consist of the following two steps:
First, instead of allocating one dimension in the
multidimensional vector space to a single word,
each word is assigned an ?index vector? as its
unique signature in the vector space. Index vectors
are generated vectors consisting of mostly zeros
together with a randomly distributed set of several
1?s and -1?s, uniquely distributed for each unique
word; The second step is to induce ?context vec-
tors? for each word. A context vector represents
the contextual meaning of a word in the WSM.
This is done using a sliding window of a fixed size
to traverse a training corpus, inducing context vec-
tors for the center/target word of the sliding win-
dow by summing the index vectors of the neigh-
bouring words in the window.
As the dimensionality of the index vectors is
fixed, the dimensionality of the vector space will
not grow beyond the size W ?Dim, where W is
the number of unique words in the vocabulary, and
Dim being the pre-selected dimensionality to use
for the index vectors. As a result, RI models are
significantly smaller than plain word space mod-
els, making them a lot less computationally expen-
sive. Additionally, the method is fully incremental
(additional training data can be added at any given
time without having to retrain the existing model),
easy to parallelize, and scalable, meaning that it is
fast and can be trained on large amounts of text in
an on-line fashion.
RI-Doc
Contrary to sliding window approach used in RI-
Word, a RI model built with document index vec-
tors first assigns unique index vectors to every
document in the training corpus. In the training
phase, each word in a document get the respective
document vector added to its context vector. The
resulting WSM is thus a compressed version of a
term-by-document matrix.
RI-ICD
Based on the principle of RI with document index
vectors, we here explore a novel way of construct-
ing a WSM by exploiting the ICD-10 code classi-
fication done by clinicians. Instead of using doc-
ument index vectors, we here use ICD-code index
vectors. First, a unique index vector is assigned to
each chapter and sub-chapter in the ICD-10 taxon-
omy. This means assigning a unique index vector
to each ?node? in the ICD-10 taxonomy, as illus-
trated in Figure 2. For each clinical note in the
training corpus, the index vector of the their pri-
mary ICD-10 code is added to all words within it.
In addition, all the index vectors for the ICD-codes
higher in the taxonomy are added, each weighted
according to their position in the hierarchy. A
weight of 1 is given to the full code, while the
weight is halved for each step upwards in the hi-
erarchy. The motivation for the latter is to capture
a certain degree of similarity between codes that
share an initial path in the taxonomy. As a result,
119
J	 ?
0.125	 ? 0.25	 ? 0.5	 ? 1	 ? Weight	 ?
2	 ?
1	 ?
0	 ?
1	 ?
1	 ?
0	 ?
J21.1	 ?
0	 ?
Figure 2: Weighting applied to ICD-code index
vectors when training WSMs based on ICD-10
codes (RI-ICD).
this similarity is encoded in the resulting WSM.
As a example: for a clinical note labelled with the
code J21.1, we add the following index vectors
to the context vectors of all its constituting words:
iv(J)? 0.125, iv(J2)? 0.25, iv(J21)? 0.5 and
iv(J21.1) ? 1.0. The underlying hypothesis for
building a WSM in this way is that it may cap-
ture relations between words in a way that bet-
ter reflects the clinical domain, compared to the
other domain-independent methods for construct-
ing a WSM.
RI-Index
As an alternative to using word?s (semantic) con-
text vectors, we simply only use their index vec-
tors as their ?contextual meaning?. When con-
structing document vectors directly from word in-
dex vectors (see Section 4.2), the resulting docu-
ment vectors represent a compressed version of a
document-by-term matrix.
Word2vec
Recently, a novel method for inducing WSMs was
introduced by Mikolov et al. (2013a), stemming
from the research in deep learning and neural net-
work language models. While the overall objec-
tive of learning a continuous vector space repre-
sentation for each word based on its textual con-
text remains, the underlying algorithms are sub-
stantially different from traditional methods such
as Latent Semantic Analysis and RI. Considering,
in turn, every word in the training data as a target
word, the method induces the representations by
training a simplified neural network to predict the
nearby context words of each target word (skip-
gram architecture), or alternatively the target word
based on all words in its immediate context (BoW
architecture). The vector space representation is
subsequently extracted from the learned weights
within the neural network. One of the main prac-
tical advantages of the word2vec method lies in
its scalability, allowing quick training on large
amounts of text, setting it apart from the majority
of other methods of distributional semantics. Ad-
ditionally, the word2vec method has been shown
to produce representations that surpass in quality
traditional methods such as Latent Semantic Anal-
ysis, especially on tasks measuring the preserva-
tion of important linguistic regularities (Mikolov
et al., 2013b).
4.2 Computing care episode similarity
After having computed a WSM, the next step is
to build episode vectors to use for the actual re-
trieval task. This is done by first normalizing the
word vectors and multiplying them with a word?s
TF*IDF weight. An episode vector is then ob-
tained by summing the word vectors of all its
words and dividing the result by the total num-
ber of words in the episode. Similarity between
episodes is determined by computing the cosine
similarity between their vectors.
4.3 Baselines
Two baselines were used in this study. The first
one is random retrieval of care episodes, which
can be expected to give very low scores and serves
merely as a sanity check. The second one is
Apache Lucene (Cutting, 1999), a state-of-the-art
search engine based on look-up of similar docu-
ments through a reverse index and relevance rank-
ing based on a TF*IDF-weighted vector space
model. Care episodes were indexed using Lucene.
Similar to the other models/methods, all of the free
text in the query episode, excluding the discharge
summary, served as the query string provided to
Lucene. Being a state-of-the-art IR system, the
scores achieved by Lucene in these experiments
should indicate the difficulty of the task.
5 Experiments
In these experiments we strove to have a setup
that was as comparable as possible for all models
and systems, both in terms of text pre-processing
and in terms of the target model dimensionality
when inducing the vector space models. The clin-
120
ical notes are split into sentences, tokenized, and
lemmatized using a Constraint-Grammar based
morphological analyzer and tagger extended with
clinical vocabulary (Karlsson, 1995). After stop
words were removed
1
, the total training corpus
contained 39 million words (minus the query
episodes), while the evaluation subset contained
18.5 million words. The vocabulary consisted of
0.6 million unique terms. Twenty care episodes
were randomly selected to serve as the query
episodes during testing, with the requirement that
each had different ICD-10 codes and consisted of a
minimum of six clinical notes. The average num-
ber of words per query episode is 830.
RI-based and word2vec models have a prede-
fined dimensionality of 800. For RI-based mod-
els, 4 non-zeros were used in the index vectors.
For the RI-Word model, a narrow context win-
dow was employed (5 left + 5 right), weighting
index vectors according to their distance to the tar-
get word (weight
i
= 2
1?dist
it
). In addition, the
index vectors were shifted once left or right de-
pending on what side of the target word they were
located, similar to direction vectors as described
in (Sahlgren et al., 2008) These parameters for RI
were chosen based on previous work on semantic
textual similarity (Moen et al., 2013). Also a much
larger window of 20+20 was tested, but without
noteworthy improvements. The word2vec model
is trained with the BoW architecture and otherwise
default parameters. In addition to Apache Lucene
(version 4.2.0)
2
, the word2vec tool
3
was used to
train the word2vec model, and the RI-based meth-
ods utilized the JavaSDM package
4
. Scores were
calculated using the trec eval tool
5
.
5.1 Experiment 1: ICD-10 code overlap
In this experiment retrieved episodes with a pri-
mary ICD-10 code identical to that of the query
episode were considered to be correct. The num-
ber of correct episodes varies between 49 and
1654. The total is 7721, and the average is
386. The high total is mainly due to three query
episodes with ICD-10 codes that occur very fre-
quently in the episode collection (896, 1590, and
1
http://www.nettiapina.fi/
finnish-stopword-list/
2
http://archive.apache.org/dist/
lucene/java/
3
https://code.google.com/p/word2vec/
4
http://www.nada.kth.se/
?
xmartin/java/
5
http://trec.nist.gov/trec_eval/
IR model MAP P@10
Lucene 0.1379 0.3000
RI-Word 0.0911 0.2650
RI-Doc 0.1015 0.3300
RI-ICD 0.3261 0.5150
RI-Index 0.1187 0.3200
Word2vec 0.1768 0.3350
Random 0.0154 0.0200
Table 1: Mean average precision and precision at
10 for retrieval of care episodes with the same pri-
mary ICD-10 code as the query episode
1654 times). When conducting the experiment all
care episodes were retrieved for each of the 20
query episodes.
Performance was measured in terms of mean
average precision (MAP) and precision among
the top-10 results (P@10), averaged over all 20
queries, as shown in in Table 1. The best MAP
score is achieved by RI-ICD, almost twice that of
word2vec, which achieved the second best MAP
score, whereas RI-Word performed worst of all.
All models score well above the random baseline,
whereas RI-ICD outperforms Lucene by a large
margin. P@10 scores follow the same ranking.
The latter scores are more representative for most
use cases where users will only inspect the top-n
retrieval results.
5.2 Experiment 2: Discharge summary
overlap
In this experiment retrieved episodes with a dis-
charge summary similar to that of the query
episode were considered to be correct. Using the
discharge summaries of the query episodes, the
top 100 care episodes with the most similar dis-
charge summary were selected as the most simi-
lar care episodes (disregarding the query episode).
This was repeated for each of the methods ? i.e.
the five different semantic models and Lucene ?
resulting in six different tests. The top 100 was
used rather than a threshold on the similarity score,
because otherwise six different thresholds would
have to be chosen. This procedure thus resulted in
six different test collections, each consisting of 20
query episodes with their corresponding 100 most
similar collection episodes.
Subsequently a 6-by-6 experimental design was
followed where each retrieval method was tested
against each test set construction method. At re-
trieval time, for each query episode, the system re-
trieves and ranks 1000 care episodes. It can be ex-
pected that when identical methods are used for re-
121
trieval and test set construction, the resulting bias
gives rise to relatively high scores. In contrast,
averaging over the scores for all six construction
methods is assumed to be a less biased indicator
of performance.
Table 2 shows the number of correctly retrieved
episodes by the different models, with the maxi-
mum being 2000 (20 queries times 100 most sim-
ilar episodes). This gives an indication of the re-
call among a 1000 retrieved episodes per query,
but without caring about precision or ranking. In
general, the numbers are relatively good when the
same model is used for both retrieval and construc-
tion of the test set (cf. values on the diagonal), al-
though in a couple of cases (e.g. with word2vec)
results are better with different models. The RI-
ICD model performs best when used for both re-
trieval and test construction. Looking at the av-
erages, which presumably are less biased indica-
tors, RI-ICD and word2vec seem to have compa-
rable performance, with both of them outperform-
ing Lucene. Other models are less successful, al-
though still much better than the random baseline.
The MAP scores in Table 3 show similar re-
sults, although here RI-ICD yields the best aver-
age score. Both models RI-ICD and word2vec
outperform Lucene. Again the RI-ICD model per-
forms exceptionally well when used for both re-
trieval and test construction.
Finally Table 4 presents precision for top-10 re-
trieved care episodes. Here RI-Doc yields the best
average scores, while RI-ICD and word2vec both
perform slightly worse.
6 Discussion
The goal of the experiments was primarily to
determine which distributional semantic models
work best for care episode retrieval. The exper-
imental results show that several models outper-
form Lucene at the care episode retrieval task.
This suggests that models of higher order seman-
tics contribute positively to calculating document
similarities in the clinical domain, compared with
straight forward boolean word matching (cf. RI-
Index and Lucene).
The relatively good performance of the RI-ICD
model, particularly in Experiment 1, suggests that
exploiting structured or encoded information in
building semantic models for clinical NLP is a
promising direction that calls for further investi-
gation. This approach concurs with the arguments
in favor of reuse of existing information sources
in Friedman et al. (2013). On the one hand, it
may not be surprising that the RI-ICD model is
performing well on Experiment 1, given how it in-
duces semantic relations between words occurring
in episodes with the same ICD-10 code. On the
other hand, being able to accurately retrieve care
episodes with similar ICD-10 codes evidently has
practical value from a clinical perspective.
The different ranking of models in experiments
1 versus 2 confirms that there is a difference be-
tween the two indicators of episode similarity,
i.e. similarity in terms of their ICD-10 codes
versus similarity with regard to their discharge
summaries. In our data a single care episode
can potentially span across several hospital wards.
A better correlation between the similarity mea-
sures is to be expected when narrowing the def-
inition of a care episode to only a single ward.
Also, taking into consideration all ICD-10 codes
for care episodes ? not only the primary one ?
could potentially improve discrimination among
care episodes. This could be useful in two ways:
(1) to create more precise test sets of the type used
in Experiment 1; (2) to extend RI-ICD models
with index vectors also for the secondary ICD-10
codes.
Input to the models for training was limited to
the free text in the clinical notes, with the ex-
ception of the use of ICD-10 codes in the RI-
ICD model. Other sources of information could,
and probably should, be utilized in a practical
care episode retrieval system applied in a hospi-
tal, such as the structured and coded information
commonly found in EHR systems. Another po-
tential information source is the internal structure
of the care episodes, as episodes containing sim-
ilar notes in the same sequential order are intu-
itively more likely to be similar. We tried comput-
ing exhaustive pairwise similarities between the
individual notes from two episodes and then tak-
ing the average of these as a similarity measure
for the episodes. However, this did not improve
performance on any measure. An alternative ap-
proach may be to apply sequence alignment algo-
rithms, as commonly used in bioinformatics (Gus-
field, 1997), in order to detect if both episodes
contain similar notes in the same temporal order.
We leave this to future work.
122
IR model \ Test set Lucene RI-Word RI-Doc RI-ICD RI-Index Word2vec Average Rank
Lucene 889 700 670 687 484 920 725 2
RI-Word 643 800 586 600 384 849 644 5
RI-Doc 665 630 859 697 436 795 680 4
RI-ICD 635 459 659 1191 490 813 707 3
RI-Index 690 491 607 654 576 758 629 6
Word2vec 789 703 702 870 516 1113 782 1
Random 74 83 86 67 84 85 79 7
Table 2: Number of correctly retrieved episodes (max 2000) for different IR models (rows) when using
different models for measuring discharge summary similarity (columns)
IR model \ Test set Lucene RI-Word RI-Doc RI-ICD RI-Index Word2vec Average Rank
Lucene 0.0856 0.0357 0.0405 0.0578 0.0269 0.0833 0.0550 3
RI-Word 0.0392 0.0492 0.0312 0.0412 0.0151 0.0735 0.0416 6
RI-Doc 0.0493 0.0302 0.0677 0.0610 0.0220 0.0698 0.0500 4
RI-ICD 0.0497 0.0202 0.0416 0.1704 0.0261 0.0712 0.0632 1
RI-Index 0.0655 0.0230 0.0401 0.0504 0.0399 0.0652 0.0473 5
Word2vec 0.0667 0.0357 0.0404 0.0818 0.0293 0.1193 0.0622 2
Random 0.0003 0.0003 0.0005 0.0002 0.0003 0.0004 0.0003 7
Table 3: Mean average precision for different IR models (rows) when using different models for measur-
ing discharge summary similarity (columns)
IR model \ Test set Lucene RI-Word RI-Doc RI-ICD RI-Index Word2vec Average Rank
Lucene 0.2450 0.1350 0.1200 0.1650 0.0950 0.1900 0.1583 5
RI-Word 0.1350 0.1500 0.1000 0.1350 0.0600 0.2100 0.1316 6
RI-Doc 0.2000 0.1250 0.2050 0.2200 0.0900 0.2400 0.1800 1
RI-ICD 0.1700 0.0650 0.1350 0.3400 0.0950 0.2050 0.1683 2
RI-Index 0.2000 0.1250 0.1550 0.1250 0.1700 0.2050 0.1633 3
Word2vec 0.1800 0.1200 0.1150 0.2100 0.0850 0.2650 0.1625 4
Random 0.0000 0.0000 0.0050 0.0000 0.0000 0.0000 0.0008 7
Table 4: Precision at top-10 retrieved episodes for different IR models (rows) when using different
models for measuring discharge summary similarity (columns)
7 Conclusion and future work
In this paper we proposed the task of care episode
retrieval as a way of evaluating several distribu-
tional semantic models in their performance at IR.
As manually constructing a proper test set of clas-
sified care episodes is costly, we experimented
with building test sets by exploiting either ICD-10
code overlap or semantic similarity of discharge
summaries. A novel method for generating se-
mantic models utilizing the ICD-10 codes of care
episodes in the training corpus was presented (RI-
ICD). The models, as well as the Lucene search
engine, were applied to the care episode retrieval
task and their performance was evaluated against
the test sets using different evaluation measures.
The results suggest that the RI-ICD model is bet-
ter suited to IR tasks in the clinical domain com-
pared with models trained on local distributions of
words, or those relying on direct word matching.
The word2vec model performed relatively well
and outperformed Lucene in both experiments.
In the results reported here, the internal se-
quence of clinical notes is ignored. Future work
should focus on exploring the temporal (sub-) se-
quence similarities between care episode pairs for
doing care episode retrieval. Further work should
also focus on expanding on the RI-ICD method
by exploiting other types of structured and/or en-
coded information related to clinical notes for
training semantic models tailored for NLP in the
clinical domain.
Acknowledgments
This study was partly supported by the Research
Council of Norway through the EviCare project
(NFR project no. 193022), the Turku University
Hospital (EVO 2014), and the Academy of Fin-
land (project no. 140323). The study is a part
of the research projects of the Ikitik consortium
(http://www.ikitik.fi). We would like
to thank Juho Heimonen for assisting us in pre-
processing the data and the reviewers for their
helpful comments.
123
References
Helen Allvin, Elin Carlsson, Hercules Dalianis, Ri-
itta Danielsson-Ojala, Vidas Daudaravi?cius, Mar-
tin Hassel, Dimitrios Kokkinakis, Helj?a Lundgren-
Laine, Gunnar Nilsson, ?ystein Nytr?, et al. 2010.
Characteristics and analysis of finnish and swedish
clinical intensive care nursing narratives. In Pro-
ceedings of the NAACL HLT 2010 Second Louhi
Workshop on Text and Data Mining of Health Docu-
ments, pages 53?60. Association for Computational
Linguistics.
Doug Cutting. 1999. Apache Lucene open source
package.
Carol Friedman, Pauline Kra, and Andrey Rzhetsky.
2002. Two biomedical sublanguages: a description
based on the theories of zellig harris. Journal of
biomedical informatics, 35(4):222?235.
Carol Friedman, Thomas C Rindflesch, and Mil-
ton Corn. 2013. Natural language process-
ing: State of the art and prospects for significant
progress, a workshop sponsored by the national li-
brary of medicine. Journal of biomedical informat-
ics, 46(5):765?773.
Dan Gusfield. 1997. Algorithms on strings, trees and
sequences: computer science and computational bi-
ology. Cambridge University Press.
Kristiina H?ayrinen, Kaija Saranto, and Pirkko
Nyk?anen. 2008. Definition, structure, content, use
and impacts of electronic health records: a review
of the research literature. International journal of
medical informatics, 77(5):291?304.
Aron Henriksson, Hans Moen, Maria Skeppstedt, Vi-
das Daudaravi, Martin Duneld, et al. 2014. Syn-
onym extraction and abbreviation expansion with
ensembles of semantic spaces. Journal of biomed-
ical semantics, 5(1):6.
Pentti Kanerva, Jan Kristofersson, and Anders Holst.
2000. Random indexing of text samples for latent
semantic analysis. In Proceedings of 22nd Annual
Conference of the Cognitive Science Society, page
1036.
Fred Karlsson. 1995. Constraint grammar: a
language-independent system for parsing unre-
stricted text. Mouton de Gruyter, Berlin and New
York.
Bevan Koopman, Guido Zuccon, Peter Bruza, Lauri-
anne Sitbon, and Michael Lawley. 2012. An evalu-
ation of corpus-driven measures of medical concept
similarity for information retrieval. In Proceedings
of the 21st ACM international conference on Infor-
mation and knowledge management, pages 2439?
2442. ACM.
Mario Lenz, Andr?e H?ubner, and Mirjam Kunze. 1998.
Textual cbr. In Case-based reasoning technology,
pages 115?137. Springer.
Christopher D Manning, Prabhakar Raghavan, and
Hinrich Sch?utze. 2008. Introduction to information
retrieval, volume 1. Cambridge University Press
Cambridge.
Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S. Cor-
rado, and Jeff Dean. 2013a. Distributed representa-
tions of words and phrases and their compositional-
ity. In Advances in Neural Information Processing
Systems 26, pages 3111?3119.
Tomas Mikolov, Wen-tau Yih, and Geoffrey Zweig.
2013b. Linguistic regularities in continuous space
word representations. In Proceedings of the 2013
Conference of the North American Chapter of the
Association for Computational Linguistics: Human
Language Technologies, pages 746?751. Associa-
tion for Computational Linguistics, June.
Hans Moen, Erwin Marsi, and Bj?orn Gamb?ack. 2013.
Towards dynamic word sense discrimination with
random indexing. ACL 2013, page 83.
Ted Pedersen, Serguei VS Pakhomov, Siddharth Pat-
wardhan, and Christopher G Chute. 2007. Mea-
sures of semantic similarity and relatedness in the
biomedical domain. Journal of biomedical infor-
matics, 40(3):288?299.
Alan L Rector. 1999. Clinical terminology: why is
it so hard? Methods of information in medicine,
38(4/5):239?252.
Magnus Sahlgren, Anders Holst, and Pentti Kanerva.
2008. Permutations as a means to encode order in
word space. In Proceedings of the Annual Meeting
of the Cognitive Science Society.
Hagit Shatkay. 2005. Hairpins in bookstacks: infor-
mation retrieval from biomedical text. Briefings in
Bioinformatics, 6(3):222?238.
Johanna I Westbrook, Enrico W Coiera, and A So-
phie Gosling. 2005. Do online information retrieval
systems help experienced clinicians answer clinical
questions? Journal of the American Medical Infor-
matics Association, 12(3):315?321.
World Health Organization and others. 2013. Interna-
tional classification of diseases (icd).
Lei Yang, Qiaozhu Mei, Kai Zheng, and David A
Hanauer. 2011. Query log analysis of an electronic
health record search engine. In AMIA Annual Sym-
posium Proceedings, volume 2011, page 915. Amer-
ican Medical Informatics Association.
124
