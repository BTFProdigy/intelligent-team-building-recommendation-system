What?s the Trouble: Automatically Identifying Problematic Dialogues in
DARPA Communicator Dialogue Systems
Helen Wright Hastie, Rashmi Prasad, Marilyn Walker
AT& T Labs - Research
180 Park Ave, Florham Park, N.J. 07932, U.S.A.
hhastie,rjprasad,walker@research.att.com
Abstract
Spoken dialogue systems promise effi-
cient and natural access to information
services from any phone. Recently, spo-
ken dialogue systems for widely used ap-
plications such as email, travel informa-
tion, and customer care have moved from
research labs into commercial use. These
applications can receive millions of calls
a month. This huge amount of spoken
dialogue data has led to a need for fully
automatic methods for selecting a subset
of caller dialogues that are most likely
to be useful for further system improve-
ment, to be stored, transcribed and further
analyzed. This paper reports results on
automatically training a Problematic Di-
alogue Identifier to classify problematic
human-computer dialogues using a corpus
of 1242 DARPA Communicator dialogues
in the travel planning domain. We show
that using fully automatic features we can
identify classes of problematic dialogues
with accuracies from 67% to 89%.
1 Introduction
Spoken dialogue systems promise efficient and nat-
ural access to a large variety of information services
from any phone. Deployed systems and research
prototypes exist for applications such as personal
email and calendars, travel and restaurant informa-
tion, personal banking, and customer care. Within
the last few years, several spoken dialogue systems
for widely used applications have moved from re-
search labs into commercial use (Baggia et al, 1998;
Gorin et al, 1997). These applications can receive
millions of calls a month. There is a strong require-
ment for automatic methods to identify and extract
dialogues that provide training data for further sys-
tem development.
As a spoken dialogue system is developed, it is
first tested as a prototype, then fielded in a limited
setting, possibly running with human supervision
(Gorin et al, 1997), and finally deployed. At each
stage from research prototype to deployed commer-
cial application, the system is constantly undergoing
further development. When a system is prototyped
in house or first tested in the field, human subjects
are often paid to use the system and give detailed
feedback on task completion and user satisfaction
(Baggia et al, 1998; Walker et al, 2001). Even
when a system is deployed, it often keeps evolving,
either because customers want to do different things
with it, or because new tasks arise out of develop-
ments in the underlying application. However, real
customers of a deployed system may not be willing
to give detailed feedback.
Thus, the widespread use of these systems has
created a data management and analysis problem.
System designers need to constantly track system
performance, identify problems, and fix them. Sys-
tem modules such as automatic speech recognition
(ASR), natural language understanding (NLU) and
dialogue management may rely on training data col-
lected at each phase. ASR performance assessment
relies on full transcription of the utterances. Dia-
logue manager assessment relies on a human inter-
face expert reading a full transcription of the dia-
logue or listening to a recording of it, possibly while
examining the logfiles to understand the interaction
between all the components. However, because of
the high volume of calls, spoken dialogue service
providers typically can only afford to store, tran-
scribe, and analyze a small fraction of the dialogues.
                Computational Linguistics (ACL), Philadelphia, July 2002, pp. 384-391.
                         Proceedings of the 40th Annual Meeting of the Association for
Therefore, there is a great need for methods for
both automatically evaluating system performance,
and for extracting subsets of dialogues that provide
good training data for system improvement. This is
a difficult problem because by the time a system is
deployed, typically over 90% of the dialogue inter-
actions result in completed tasks and satisfied users.
Dialogues such as these do not provide very use-
ful training data for further system development be-
cause there is little to be learned when the dialogue
goes well.
Previous research on spoken dialogue evaluation
proposed the application of automatic classifiers for
identifying and predicting of problematic dialogues
(Litman et al, 1999; Walker et al, 2002) for the
purpose of automatically adapting the dialogue man-
ager. Here we apply similar methods to the dialogue
corpus data-mining problem described above. We
report results on automatically training a Problem-
atic Dialogue Identifier (PDI) to classify problem-
atic human-computer dialogues using the October-
2001 DARPA Communicator corpus.
Section 2 describes our approach and the dialogue
corpus. Section 3 describes how we use the DATE
dialogue act tagging scheme to define input features
for the PDI. Section 4 presents a method and results
for automatically predicting task completion. Sec-
tion 5 presents results for predicting problematic di-
alogues based on the user?s satisfaction. We show
that we identify task failure dialogues with 85% ac-
curacy (baseline 59%) and dialogues with low user
satisfaction with up to 89% accuracy. We discuss the
application of the PDI to data mining in Section 6.
Finally, we summarize the paper and discuss future
work.
2 Corpus, Methods and Data
Our experiments apply CLASSIFICATION and RE-
GRESSION trees (CART) (Brieman et al, 1984) to
train a Problematic Dialogue Identifier (PDI) from
a corpus of human-computer dialogues. CLASSI-
FICATION trees are used for categorical response
variables and REGRESSION trees are used for con-
tinuous response variables. CART trees are binary
decision trees. A CLASSIFICATION tree specifies
what queries to perform on the features to maximize
CLASSIFICATION ACCURACY, while REGRESSION
trees derive a set of queries to maximize the COR-
RELATION of the predicted value and the original
value. Like other machine learners, CART takes as
input the allowed values for the response variables;
the names and ranges of values of a fixed set of input
features; and training data specifying the response
variable value and the input feature values for each
example in a training set. Below, we specify how
the PDI was trained, first describing the corpus, then
the response variables, and finally the input features
derived from the corpus.
Corpus: We train and test the PDI on the DARPA
Communicator October-2001 corpus of 1242 dia-
logues. This corpus represents interactions with
real users, with eight different Communicator travel
planning systems, over a period of six months from
April to October of 2001. The dialogue tasks range
from simple domestic round trips to multileg inter-
national trips requiring both car and hotel arrange-
ments. The corpus includes logfiles with logged
events for each system and user turn; hand transcrip-
tions and automatic speech recognizer (ASR) tran-
scription for each user utterance; information de-
rived from a user profile such as user dialect region;
and a User Satisfaction survey and hand-labelled
Task Completion metric for each dialogue. We ran-
domly divide the corpus into 80% training (894 dia-
logues) and 20% testing (248 dialogues).
Defining the Response Variables: In principle,
either low User Satisfaction or failure to complete
the task could be used to define problematic dia-
logues. Therefore, both of these are candidate re-
sponse variables to be examined. The User Satisfac-
tion measure derived from the user survey ranges be-
tween 5 and 25. Task Completion is a ternary mea-
sure where no Task Completion is indicated by 0,
completion of only the airline itinerary is indicated
by 1, and completion of both the airline itinerary and
ground arrangements, such as car and hotel book-
ings, is indicated by 2. We also defined a binary ver-
sion of Task Completion, where Binary Task Com-
pletion=0 when no task or subtask was complete
(equivalent to Task Completion=0), and Binary Task
Completion=1 where all or some of the task was
complete (equivalent to Task Completion=1 or Task
Completion=2).
Figure 1 shows the frequency of dialogues for
varying User Satisfaction for cases where Task
Completion is 0 (solid line) and Task Completion
is greater than 0 (dotted lines). Note that Task Com-
pletion is 1 or 2 for a number of dialogues for which
User Satisfaction is low. Figure 2 illustrates such a
dialogue (system turns are labelled S, user turns as
U, and ASR hypotheses as REC). Here, low User
Satisfaction may be due to the fact that the user had
to repeat herself many times before the system un-
derstood the departure city. An automatic surrogate
for ASR accuracy (such as ASR confidence) would
Figure 1: User Satisfaction for dialogues where Task
Completion is 0, 1 or 2
not be adequate for identifying this problematic di-
alogue, because here either the dialogue manager or
the SLU component is at fault. Another dialogue
subset of interest in Figure 1 is one for which Task
Completion is 0 but User Satisfaction is high. A
common cause for non-Task Completion in these di-
alogues is database access problems. The fact that
the interaction went well until this point is not cap-
tured by the Task Completion metric. Thus we de-
cided to use both User Satisfaction and Task Com-
pletion as response variables in separate experiments
in order to allow the PDI to capture the relationship
between these two variables.
Input Features: Both User Satisfaction and Task
Completion predictors are trained using two types of
automatically extractable features: (1) logfile fea-
tures; and (2) system dialogue act types. The
logfile features include efficiency metrics such as
the number of system and user turns spent on the
task; the total time of the dialogue in milliseconds;
and situational metrics, such as phone type1 which
affects ASR. The system dialogue act types are
extracted from the logfiles using the DATE (Dia-
logue Act Tagging for Evaluation) automatic tag-
1We assume this is automatically derivable by automatic
number identification (ANI).
S13: Sorry, I must have misunderstood. I just heard you
say that the departure city was Minto, Arkansas.
Was the departure city Minneapolis-St-Paul, Minto,
Arkansas or Milan, Italy?
U14: THE DEPARTURE CITY IS MINNEAPOLIS
SAINT PAUL
REC: the departure city in minneapolis saint paul
S15: Here?s a summary of the second flight segment:
I?ve got you leaving from Minto, Arkansas on Fri-
day, October twelfth in the morning and arriving in
newark flying on continental. Is this correct?
U16: NO I WANT TO LEAVE FROM MINNEAPOLIS
REC: no i want to leave from minneapolis
Repeat dialogue segment 3 times
S25: Sorry, I must have misunderstood. I just heard you
say that the departure city was Minto, Arkansas. Was
the departure city Minto, Arkansas or Minneapolis-
St-Paul ?
U26: THE DEPARTURE CITY IS MINNEAPOLIS
SAINT PAUL
REC: the departure city minneapolis by all
S27: Recapping the second flight segment: I have you
leaving out of minneapolis-st-paul on Friday, Octo-
ber twelfth in the morning and arriving in newark
flying on continental. Is this correct?
U28: YES
REC: yes
Figure 2: Sample low user satisfaction dialogue
ging scheme (Walker et al, 2001). The purpose
of these features is to extract numerical correlates
of system dialogue behaviors. This dialogue act la-
belling procedure is detailed in Section 3.
Figure 3 summarizes the types of features used
to train the User Satisfaction predictor. In addition
to the efficiency metrics and the DATE labels, Task
Success can itself be used as a predictor. This can
either be the hand-labelled feature or an approxima-
tion as predicted by the Task Completion Predictor,
described in Section 4. Figure 4 shows the system
design for automatically predicting User Satisfac-
  Efficiency Measures
? Hand-labelled: WERR, SERR
? Automatic: TimeOnTask, TurnsOnTask, Nu-
mOverlaps, MeanUsrTurnDur, MeanWrdsPerUs-
rTurn, MeanSysTurnDur, MeanWrdsPerSysTurn,
DeadAlive, Phone-type, SessionNumber
  Qualitative Measures
? Automatic: DATE Unigrams, e.g. present-
info:flight, acknowledgement:flight booking etc.
? Automatic: DATE Bigrams, e.g. present-
info:flight+acknowledgement:flight booking etc.
  Task Success Features
? Hand-labelled: HL Task Completion
? Automatic: Auto Task Completion
Figure 3: Features used to train the User Satisfaction
Prediction tree
tion with the three types of input features.
DATE
Output 
of
SLS
Completion
Auto Task Completion
CART
Predictor
UserSatisfaction
Task 
Predictor
TAGGER
Automatic
Logfile 
Features
DATE
Rules
Figure 4: Schema for User Satisfaction prediction
3 Extracting DATE Features
The dialogue act labelling of the corpus follows
the DATE tagging scheme (Walker et al, 2001).
In DATE, utterance classification is done along
three cross-cutting orthogonal dimensions. The
CONVERSATIONAL-DOMAIN dimension specifies
the domain of discourse that an utterance is about.
The SPEECH ACT dimension captures distinctions
between communicative goals such as requesting
information (REQUEST-INFO) or presenting infor-
mation (PRESENT-INFO). The TASK-SUBTASK di-
mension specifies which travel reservation subtask
the utterance contributes to. The SPEECH ACT and
CONVERSATIONAL-DOMAIN dimensions are gen-
eral across domains, while the TASK-SUBTASK di-
mension is domain- and sometimes system-specific.
Within the conversational domain dimension,
DATE distinguishes three domains (see Figure 5).
The ABOUT-TASK domain is necessary for evaluat-
ing a dialogue system?s ability to collaborate with
a speaker on achieving the task goal. The ABOUT-
COMMUNICATION domain reflects the system goal
of managing the verbal channel of communication
and providing evidence of what has been under-
stood. All implicit and explicit confirmations are
about communication. The ABOUT-SITUATION-
FRAME domain pertains to the goal of managing the
user?s expectations about how to interact with the
system.
DATE distinguishes 11 speech acts. Examples of
each speech act are shown in Figure 6.
The TASK-SUBTASK dimension distinguishes
among 28 subtasks, some of which can also be
grouped at a level below the top level task. The
TOP-LEVEL-TRIP task describes the task which con-
tains as its subtasks the ORIGIN, DESTINATION,
Conversational Domain Example
ABOUT-TASK And what time didja wanna
leave?
ABOUT-
COMMUNICATION
Leaving from Miami.
ABOUT-SITUATION-
FRAME
You may say repeat, help me
out, start over, or, that?s wrong
Figure 5: Example utterances distinguished within
the Conversational Domain Dimension
Speech-Act Example
REQUEST-INFO And, what city are you flying to?
PRESENT-INFO The airfare for this trip is 390 dol-
lars.
OFFER Would you like me to hold this op-
tion?
ACKNOWLEDGMENT I will book this leg.
BACKCHANNEL Okay.
STATUS-REPORT Accessing the database; this
might take a few seconds.
EXPLICIT-
CONFIRM
You will depart on September 1st.
Is that correct?
IMPLICIT-
CONFIRM
Leaving from Dallas.
INSTRUCTION Try saying a short sentence.
APOLOGY Sorry, I didn?t understand that.
OPENING-
CLOSING
Hello. Welcome to the C M U
Communicator.
Figure 6: Example speech act utterances
DATE, TIME, AIRLINE, TRIP-TYPE, RETRIEVAL
and ITINERARY tasks. The GROUND task includes
both the HOTEL and CAR-RENTAL subtasks. The
HOTEL task includes both the HOTEL-NAME and
HOTEL-LOCATION subtasks.2
For the DATE labelling of the corpus, we imple-
mented an extended version of the pattern matcher
that was used for tagging the Communicator June
2000 corpus (Walker et al, 2001). This method
identified and labelled an utterance or utterance se-
quence automatically by reference to a database of
utterance patterns that were hand-labelled with the
DATE tags. Before applying the pattern matcher,
a named-entity labeler was applied to the system
utterances, matching named-entities relevant in the
travel domain, such as city, airport, car, hotel, airline
names etc.. The named-entity labeler was also ap-
plied to the utterance patterns in the pattern database
to allow for generality in the expression of com-
municative goals specified within DATE. For this
named-entity labelling task, we collected vocabulary
lists from the sites, which maintained such lists for
2ABOUT-SITUATION-FRAME utterances are not specific to
any particular task and can be used for any subtask, for example,
system statements that it misunderstood. Such utterances are
given a ?meta? dialogue act status in the task dimension.
developing their system.3 The extension of the pat-
tern matcher for the 2001 corpus labelling was done
because we found that systems had augmented their
inventory of named entities and utterance patterns
from 2000 to 2001, and these were not accounted
for by the 2000 tagger database. For the extension,
we collected a fresh set of vocabulary lists from the
sites and augmented the pattern database with ad-
ditional 800 labelled utterance patterns. We also
implemented a contextual rule-based postprocessor
that takes any remaining unlabelled utterances and
attempts to label them by looking at their surround-
ing DATE labels. More details about the extended
tagger can be found in (Prasad and Walker, 2002).
On the 2001 corpus, we were able to label 98.4 
of the data. A hand evaluation of 10 randomly se-
lected dialogues from each system shows that we
achieved a classification accuracy of 96  at the ut-
terance level.
For User Satisfaction Prediction, we found that
the distribution of DATE acts were better captured
by using the frequency normalized over the total
number of dialogue acts. In addition to these un-
igram proportions, the bigram frequencies of the
DATE dialogue acts were also calculated. In the fol-
lowing two sections, we discuss which DATE labels
are discriminatory for predicting Task Completion
and User Satisfaction.
4 The Task Completion Predictor
In order to automatically predict Task Comple-
tion, we train a CLASSIFICATION tree to catego-
rize dialogues into Task Completion=0, Task Com-
pletion=1 or Task Completion=2. Recall that a
CLASSIFICATION tree attempts to maximize CLAS-
SIFICATION ACCURACY, results for Task Comple-
tion are thus given in terms of percentage of dia-
logues correctly classified. The majority class base-
line is 59.3% (dialogues where Task Completion=1).
The tree was trained on a number of different in-
put features. The most discriminatory ones, how-
ever, were derived from the DATE tagger. We
use the primitive DATE tags in conjunction with a
feature called GroundCheck (GC), a boolean fea-
ture indicating the existence of DATE tags related
to making ground arrangements, specifically re-
quest info:hotel name, request info:hotel location,
offer:hotel and offer:rental.
Table 1 gives the results for Task Completion pre-
diction accuracy using the various types of features.
3The named entities were preclassified into their respective
semantic classes by the sites.
Baseline Auto ALF + ALF +
Logfile GC GC+ DATE
TC 59% 59% 79% 85%
BTC 86% 86% 86% 92%
Table 1: Task Completion (TC) and Binary Task
Completion (BTC) prediction results, using auto-
matic logfile features (ALF), GroundCheck (GC)
and DATE unigram frequencies
The first row is for predicting ternary Task Comple-
tion, and the second for predicting binary Task Com-
pletion. Using automatic logfile features (ALF) is
not effective for the prediction of either types of Task
Completion. However, the use of GroundCheck re-
sults in an accuracy of 79% for the ternary Task
Completion which is significantly above the base-
line (df = 247, t = -6.264, p  .0001). Adding in the
other DATE features yields an accuracy of 85%. For
Binary Task Completion it is only the use of all the
DATE features that yields an improvement over the
baseline of 92%, which is significant (df = 247, t =
5.83, p  .0001).
A diagram of the trained decision tree for ternary
Task Completion is given in Figure 7. At any junc-
tion in the tree, if the query is true then one takes
the path down the right-hand side of the tree, oth-
erwise one takes the left-hand side. The leaf nodes
contain the predicted value. The GroundCheck fea-
ture is at the top of the tree and divides the data
into Task Completion  2 and Task Completion  2.
If GroundCheck  1, then the tree estimates that
Task Completion is 2, which is the best fit for the
data given the input features. If GroundCheck  0
and there is an acknowledgment of a booking, then
probably a flight has been booked, therefore, Task
Completion is predicted to be 1. Interestingly, if
there is no acknowledgment of a booking then Task
Completion  0, unless the system got to the stage of
asking the user for an airline preference and if re-
quest info:top level trip  2. More than one of these
DATE types indicates that there was a problem in the
dialogue and that the information gathering phase
started over from the beginning.
The binary Task Completion decision tree simply
checks if an acknowledgement:flight booking
has occurred. If it has, then Binary Task Com-
pletion=1, otherwise it looks for the DATE act
about situation frame:instruction:meta situation info,
which captures the fact that the system has told
the user what the system can and cannot do, or
has informed the user about the current state of the
task. This must help with Task Completion, as the
tree tells us that if one or more of these acts are
observed then Task Completion=1, otherwise Task
Completion=0.
TC=1
GroundCheck =0
TC=2
request_info:airline <1
request_info:top_level_trip < 2
acknow.: flight_booking< 1
TC=0TC=1
TC=0 TC=1
Figure 7: Classification Tree for predicting Task
Completion (TC)
5 The User Satisfaction Predictor
Feature Log LF + LF +
used features unigram bigram
HL TC 0.587 0.584 0.592
Auto TC 0.438 0.434 0.472
HL BTC 0.608 0.607 0.614
Auto BTC 0.477 0.47 0.484
Table 2: Correlation results using logfile fea-
tures (LF), adding unigram proportions and bigram
counts, for trees tested on either hand-labelled (HL)
or automatically derived Task Completion (TC) and
Binary Task Completion (BTC)
Quantitative Results: Recall that REGRESSION
trees attempt to maximize the CORRELATION of the
predicted value and the original value. Thus, the re-
sults of the User Satisfaction predictor are given in
terms of the correlation between the predicted User
Satisfaction and actual User Satisfaction as calcu-
lated from the user survey. Here, we also provide R 
for comparison with previous studies. Table 2 gives
the correlation results for User Satisfaction for dif-
ferent feature sets. The User Satisfaction predictor
is trained using the hand-labelled Task Completion
feature for a topline result and using the automati-
cally obtained Task Completion (Auto TC) for the
fully automatic results. We also give results using
Binary Task Completion (BTC) as a substitute for
Task Completion. The first column gives results us-
ing features extracted from the logfile; the second
column indicates results using the DATE unigram
proportions and the third column indicates results
when both the DATE unigram and bigram features
are available.
The first row of Table 2 indicates that perfor-
mance across the three feature sets is indistinguish-
able when hand-labelled Task Completion (HL TC)
is used as the Task Completion input feature. A
comparison of Row 1 and Row 2 shows that the
PDI performs significantly worse using only auto-
matic features (z = 3.18). Row 2 also indicates that
the DATE bigrams help performance, although the
difference between R = .438 and R = .472 is not
significant. The third and fourth rows of Table 1
indicate that for predicting User Satisfaction, Bi-
nary Task Completion is as good as or better than
Ternary Task Completion. The highest correlation of
0.614 (   	

 ) uses hand-labelled Binary Task
Completion and the logfile features and DATE uni-
gram proportions and bigram counts. Again, we see
that the Automatic Binary Task Completion (Auto
BTC) performs significantly worse than the hand-
labelled version (z = -3.18). Row 4 includes the best
totally automatic system: using Automatic Binary
Task Completion and DATE unigrams and bigrams
yields a correlation of 0.484 ( 	 ).
Regression Tree Interpretation: It is interest-
ing to examine the trees to see which features are
used for predicting User Satisfaction. A metric
called Feature Usage Frequency indicates which fea-
tures are the most discriminatory in the CART tree.
Specifically, Feature Usage Frequency counts how
often a feature is queried for each data point, nor-
malized so that the sum of Feature Usage Frequency
values for all the features sums to one. The higher a
feature is in the tree, the more times it is queried. To
calculate the Feature Usage Frequency, we grouped
the features into three types: Task Completion, Log-
file features and DATE frequencies. Feature Us-
age Frequency for the logfile features is 37%. Task
Completion occurs only twice in the tree, however,
it makes up 31because it occurs at the top of the
tree. The Feature Usage Frequency for DATE cat-
egory frequency is 32%. We will discuss each of
these three groups of features in turn.
The most used logfile feature is TurnsOnTask
which is the number of turns which are task-
oriented, for example, initial instructions on how
to use the system are not taken as a TurnOnTask.
Shorter dialogues tend to have a higher User Sat-
isfaction. This is reflected in the User Satisfaction
scores in the tree. However, dialogues which are
long (TurnsOnTask  79 ) can be satisfactory (User
Satisfaction = 15.2) as long as the task that is com-
pleted is long, i.e., if ground arrangements are made
in that dialogue (Task Completion=2). If ground ar-
rangements are not made, the User Satisfaction is
lower (11.6). Phone type is another important fea-
ture queried in the tree, so that dialogues conducted
over corded phones have higher satisfaction. This
is likely to be due to better recognition performance
from corded phones.
As mentioned previously, Task Completion is at
the top of the tree and is therefore the most queried
feature. This captures the relationship between Task
Completion and User Satisfaction as illustrated in
Figure 1.
Finally, it is interesting to examine which DATE
tags the tree uses. If there have been more than
three acknowledgments of bookings, then several
legs of a journey have been successfully booked,
therefore User Satisfaction is high. In particular,
User Satisfaction is high if the system has asked
if the user would like a price for their itinerary
which is one of the final dialogue acts a system
does before the task is completed. The DATE act
about comm:apology:meta slu reject is a measure
of the system?s level of misunderstanding. There-
fore, the more of these dialogue act types the lower
User Satisfaction. This part of the tree uses length
in a similar way described earlier, whereby long di-
alogues are only allocated lower User Satisfaction
if they do not involve ground arrangements. Users
do not seem to mind longer dialogues as long as
the system gives a number of implicit confirma-
tions. The dialogue act request info:top level trip
usually occurs at the start of the dialogue and re-
quests the initial travel plan. If there are more than
one of this dialogue act, it indicates that a START-
OVER occurred due to system failure, and this leads
to lower User Satisfaction. A rule containing the
bigram request info:depart day month date+USER
states that if there is more than one occurrence of this
request then User Satisfaction will be lower. USER
is the single category used for user-turns. No auto-
matic method of predicting user speech act is avail-
able yet for this data. A repetition of this DATE
bigram indicates that a misunderstanding occurred
the first time it was requested, or that the task is
multi-leg in which case User Satisfaction is gener-
ally lower.
The tree that uses Binary Task Completion is
identical to the tree described above, apart from
one binary decision which differentiates dialogues
where Task Completion=1 and Task Completion=2.
Instead of making this distinction, it just uses dia-
logue length to indicate the complexity of the task.
In the original tree, long dialogues are not penalized
if they have achieved a complex task (i.e. if Task
Completion=2). The Binary Task Completion tree
has no way of making this distinction and therefore
just penalizes very long dialogues (where TurnsOn-
Task  110). The Feature Usage Frequency for the
Task Completion features is reduced from 31% to
21%, and the Feature Usage Frequency for the log-
file features increases to 47%. We have shown that
this more general tree produces slightly better re-
sults.
6 Results for Identifying Problematic
Dialogues for Data Mining
So far, we have described a PDI that predicts User
Satisfaction as a continuous variable. For data min-
ing, system developers will want to extract dialogues
with predicted User Satisfaction below a particular
threshold. This threshhold could vary during dif-
ferent stages of system development. As the sys-
tem is fine tuned there will be fewer and fewer dia-
logues with low User Satisfaction, therefore in order
to find the interesting dialogues for system develop-
ment one would have to raise the User Satisfaction
threshold. In order to illustrate the potential value
of our PDI, consider an example threshhold of 12
which divides the data into 73.4% good dialogues
where User Satisfaction  12 which is our baseline
result.
Table 3 gives the recall and precision for the PDIs
described above which use hand-labelled Task Com-
pletion and Auto Task Completion. In the data,
26.6% of the dialogues are problematic (User Sat-
isfaction is under 12), whereas the PDI using hand-
labelled Task Completion predicts that 21.8% are
problematic. Of the problematic dialogues, 54.5%
are classified correctly (Recall). Of the dialogues
that it classes as problematic 66.7% are problematic
(Precision). The results for the automatic system
show an improvement in Recall: it identifies more
problematic dialogues correctly (66.7%) but the pre-
cision is lower.
What do these numbers mean in terms of our orig-
inal goal of reducing the number of dialogues that
need to be transcribed to find good cases to use
Task Completion Dialogue Recall Prec.
Hand-labelled Good 90% 84.5%
Hand-labelled Problematic 54.5% 66.7%
Automatic Good 88.5% 81.3%
Automatic Problematic 66.7% 58.0%
Table 3: Precision and Recall for good and prob-
lematic dialogues (where a good dialogue has User
Satisfaction  12) for the PDI using hand-labelled
Task Completion and Auto Task Completion
for system improvement? If one had a budget to
transcribe 20% of the dataset containing 100 dia-
logues, then by randomly extracting 20 dialogues,
one would transcribe 5 problematic dialogues and 15
good dialogues. Using the fully automatic PDI, one
would obtain 12 problematic dialogues and 8 good
dialogues. To look at it another way, to extract 15
problematic dialogues out of 100, 55% of the data
would need transcribing. To obtain 15 problem-
atic dialogues using the fully automatic PDI, only
26% of the data would need transcribing. This is a
massive improvement over randomly choosing dia-
logues.
7 Discussion and Future Developments
This paper presented a Problematic Dialogue Identi-
fier which system developers can use for evaluation
and to extract problematic dialogues from a large
dataset for system development. We describe PDIs
for predicting both Task Completion and User Satis-
faction in the DARPA Communicator October 2001
corpus.
There has been little previous work on recogniz-
ing problematic dialogues. However, a number of
studies have been done on predicting specific errors
in a dialogue, using a variety of automatic and hand-
labelled features, such as ASR confidence and se-
mantic labels (Aberdeen et al, 2001; Hirschberg et
al., 2000; Levow, 1998; Litman et al, 1999). Pre-
vious work on predicting problematic dialogues be-
fore the end of the dialogue (Walker et al, 2002)
achieved accuracies of 87% using hand-labelled fea-
tures (baseline 67%). Our automatic Task Comple-
tion PDI achieves an accuracy of 85%.
Previous work also predicted User Satisfaction
by applying multi-variate linear regression features
with and without DATE features and showed that
DATE improved the model fit from   	
 to
 (Walker et al, 2001). Our best model
has an The Pragmatics of Taking a Spoken Language System Out of the Laboratory
Jody J. Daniels and Helen Wright Hastie
Lockheed Martin Advanced Technology Laboratories
1 Federal Street A&E 3-W
Camden, NJ 08102
 jdaniels, hhastie@atl.lmco.com
Abstract
Lockheed Martin?s Advanced Technology Lab-
oratories has been designing, developing, test-
ing, and evaluating spoken language under-
standing systems in several unique operational
environments over the past five years. Through
these experiences we have encountered numer-
ous challenges in making each system become
an integral part of a user?s operations. In this
paper, we discuss these challenges and report
how we overcame them with respect to a num-
ber of domains.
1 Introduction
Lockheed Martin?s Advanced Technology Laboratories
(LMATL) has been designing, developing, testing, and
evaluating spoken language understanding systems (SLS)
in several unique operational environments over the past
five years. This model of human interaction is referred to
as Listen, Communicate, Show (LCS). In an LCS system,
the computer listens for information requests, communi-
cates with the user and networked information resources
to compute user-centered solutions, and shows tailored
visualizations to individual users. Through developing
these systems, we have encountered numerous challenges
in making each system become an integral part of a user?s
operations. For example, Figure 1 shows the deployment
of a dialogue system for placing Marine supply requests,
which is being used in a tactical vehicle, a HMMWV.
Some of the challenges of creating such spoken lan-
guage systems include giving appropriate responses. This
involves managing the tension between utterance brevity
and giving enough context in a response to build the
user?s trust. Similarly, the length of user utterances must
be succinct enough to convey the correct information
without adding to the signature of the soldier. The system
must be robust when handling out of vocabulary terms
and concepts. It must also be able to adapt to noisy en-
vironments whose parameters change frequently and be
able use input devices and power access unique to each
situation.
Figure 1: LCS Marine on the move
2 Architecture
The LCS Spoken Language systems use the Galaxy ar-
chitecture (Seneff et al, 1999). This Galaxy architecture
consists of a central hub and servers. Each of the servers
performs a specific function, such as converting audio
speech into a text translation of that speech or combin-
ing the user?s past statements with what was said most
recently. The individual servers exchange information by
sending messages through the hub. These messages con-
tain information to be sent to other servers as well as in-
formation used to determine what server or servers should
be contacted next.
Various Galaxy Servers work together to develop a se-
mantic understanding of the user?s statements and ques-
tions. The sound spoken into the microphone, telephone,
or radio is collected by an Audio Server and sent on to
the recognizer. The recognizer translates this wave file
into text, which is sent to a natural language parser. The
parser converts the text into a semantic frame, a repre-
sentation of the statement?s meaning. This meaning rep-
resentation is passed on to another server, the Dialogue
Manager. This server monitors the current context of a
conversation and, based on this context, can prompt the
user for any necessary clarification and present intelligent
responses to the user. Since the Dialogue Manager is
aware of what information has been discussed thus far,
it is able to determine what information is still needed. A
semantic frame is created by the Dialogue Manager and
this is sent through the Language Generation Server to
generate a text response. The text response is then spo-
ken to the user through a speech synthesis server.
To solve the problem of retrieving or placing data
from/in remote and local sources, we gave the sys-
tems below the use of mobile software agents. If user-
requested information is not immediately available, an
agent can monitor the data sources until it is possible to
respond. Users may request a notification or alert when
a particular activity occurs, which may happen at an in-
determinate time in the future. Because of the potentially
significant time lag, it is important to manage dialogue
activity so that the user is only interrupted when the need
for information is more important than the current task
that the user is currently undertaking. This active man-
agement of interruptions aids task management and light-
ens cognitive load (Daniels et al, 2002).
3 Domains
3.1 LCS Marine
One of the first LCS systems to be tested out in the
field was our Marine Logistics spoken dialogue system.
This application sought to connect the Marine in the
field to the Small Unit Logistics (SUL) database, which
maintains current information about supply requisitions.
Warfighters wanted to be able to place requests as well
as check on the status of existing requests without the
need of additional hardware or communicating with a
third party. It was also highly desirable to use existing
communications procedures, so that the training time to
use the system was minimized. The system needed to
be speaker-independent and mixed initiative enabling the
warfighters to develop a sense of trust in the technology.
Marines using the system were able to perform several
tasks. They could create new requests for supplies, with
the system prompting them for information needed to fill
in a request form. They could also modify and delete
previously placed requests and could check on the status
of requests in one of two ways. They could directly ask
about the current status, or they could delegate an agent
to monitor the status of a particular request. It was an
easy addition to the system to add a constraint that the
agent return after a specified time period if no activity oc-
curs on the request, which is also valuable information for
the Marine. These delegated agents travel across a low-
bandwidth SINCGARS radio network from the Marine to
the database and access that database to place, alter, and
monitor supply requisitions.
The challenges in deploying this system to the field
were twofold - building trust in the system so that it
would become part of normal operations and in dealing
with the unique environmental factors. The former pre-
sented the conflicting goals of brevity versus confirming
user inputs. Marines want to restrict their time on the ra-
dio net as much as possible. At the same time they want
to ensure that what they requested is what they were go-
ing to receive. Much time went into defining and refining
system responses that met both needs as best possible.
This involved several sessions with a numerous Marines
evaluating possible dialogue responses. We also spent
much time ensuring that LCS Marine could handle both
proper and malformed radio protocols. Broad coverage of
potential expressions, especially those when under stress,
such as recognition of the liberal use of curse words, led
to greater user ability to successfully interact through the
system.
The second set of challenges, unique environmental
factors, included access while on the move, battlefield
noise, and coping with adverse conditions such as sand
storms. Accessing LCS Marine while on the move meant
using a SINCGARS radio as the input device. Attempts
to use the system by directly collecting speech from a
SINCGARS radio were dropped due to the technological
challenges presented by the distortion introduced by the
radio on the signal. Instead, we installed the majority of
the system on laptops and put these into the HMMWV.
We sent mobile agents over the SINCGARS data link
back to the data sources. This meant securing hardware
in a HMMWV and powering it off of the vehicle?s battery
as illustrated in Figure 1. (Only one laptop was damaged
during testing.) The mobile agents were able to easily
traverse a retransmission link and reach the remote data
source.
Dealing with hugely varying background noise sources
was less of a problem than originally predicted. Fortu-
nately, most of the time that one of these loud events
would occur, users would simply stop talking. Their hear-
ing was impaired and so they would wait for the noise to
abate and then continue the dialogue. On the other hand,
we did encounter several users who, because of the Lom-
bard effect, insisted upon always yelling at the system.
While we did not measure decibel levels, there were a
few times when the system was not able to understand
the user because of background noise.
3.2 Shipboard Information
An LCS system has also been developed to monitor ship-
board system information aboard the Sea Shadow (IX
529), a Naval test platform for stealth, automation, and
control technologies. From anywhere on the ship, per-
sonnel use the on-board intercom to contact this system,
SUSIE (Shipboard Ubiquitous Speech Interface Environ-
ment), to ask about the status of equipment that is located
throughout the ship. Crew members do not have to be
anywhere near the equipment being monitored in order
to receive data. Figure 2 illustrates a sailor using SUSIE
through the ship?s intercom.
Personnel can ask about pressures, temperatures, and
voltages of various pieces of equipment or can delegate
Figure 2: Sailor interacting with SUSIE through the
ship?s intercom
monitoring those measurements (sensor readings) to the
system. A user can request notification of an abnormal
reading by a sensor. This causes the LCS system to dele-
gate a persistent agent to monitor the sensor and to report
the requested data. Should an abnormal reading occur,
the user is contacted by the system, again using the inter-
com.
This domain presented several challenges and oppor-
tunities. Through numerous discussions with users and
presentation of possible dialogues, we learned that the
users would benefit from a system ability to remember,
between sessions, the most recent activity of each user.
This would permit a user to simply log in and request:
?What about now??. SUSIE would determine what had
been this user?s most recent monitoring activity, would
then seek out the current status, and then report it. While
this seems quite simple, there is significant behind-the-
scenes work to store context and make the interaction ap-
pear seamless.
It was necessary to use the organic intercom system in-
stalled in the Sea Shadow for communication with crew
members. Collecting speech data through the intercom
system to pass to SUSIE required linking two DSPs (and
adjusting them) to the hardware for the SLS. Once con-
nected in, the next significant challenge was that of the
varying noise levels. Background noise varied from one
room to the next and even within a single space. We were
not able to use a push-to-talk or hold-to-talk paradigm
because of the inconvenience to the crew members; they
leave the intercom open for the duration of the conversa-
tion. Fortunately, the recognizer (built on MIT?s SUM-
MIT) is able to handle a great deal of a noise and still
hypothesize accurately. To improve the system accuracy,
we will incorporate automatic retraining of the recognizer
on noise each time that a new session begins.
3.3 Battlefield Casualty Reporting System
We are currently developing a new LCS system known
as the Battlefield Casualty Reporting System or BCRS.
The goal of this system is to assist military personnel
in reporting battlefield casualties directly into a main
database. This involves intelligent dialogue to reduce am-
biguity, resolve constraints, and refine searches on indi-
vidual names and the circumstances surrounding the ca-
sualty. Prior knowledge of every individual?s name will
not be possible. The deployment of this system will be
again present many challenges such as noise effects on a
battlefield, effects of stress on the voice, and the ability
to integrate into existing military hardware.
4 Future Work
The areas of research needed to address needs for more
dynamic and robust systems include better, more robust
or partial parsing mechanisms. In addition, systems must
be able to cope with multi-sentence inputs, including the
system?s ability to insert back channel activity. Ease of
domain expansion is important as systems evolve. Vary-
ing environmental factors mean that the systems require
additional noise adaptation or mitigation techniques, in
addition, the ability to switch modes of communication if
one is not appropriate at a given time.
5 Conclusions
We have discussed the pragmatics involved with taking
an SLS system out of the laboratory or away from tele-
phony and placing it in a volatile environment. These sys-
tems have to be robust and be able to cope with varying
input styles and modes as well as be able to modify their
output to the appropriate situation. In addition, the sys-
tems must be an integral part of the technology that is in
current use and be able to withstand adverse conditions.
Satisfying all of these constraints involves active partici-
pation in the development process with the end-users as
well as creative solutions and technological advances.
6 Acknowledgments
This work was supported by DARPA contract N66001-
01-D-6011.
References
Jody Daniels, Susan Regli, and Jerry Franke. 2002. Sup-
port for intelligent interruption and augmented con-
text recovery. In Proceedings of 7th IEEE Conference
on Human Factors and Power Plants, Scottsdate, AZ,
September.
Stephanie Seneff, Ray Lau, and Joe Polifroni. 1999. Or-
ganization, communication, and control in the galaxy-
ii conversational system. In Proceedings for Eu-
rospeech ?98, Budapest, Hungary.
Bridging the Gap: Academic and Industrial Research in Dialog Technologies Workshop Proceedings, pages 84?88,
NAACL-HLT, Rochester, NY, April 2007. c?2007 Association for Computational Linguistics
WIRE: A Wearable Spoken Language Understanding System for the Military Helen Hastie Patrick Craven Michael Orr Lockheed Martin Advanced Technology Laboratories 3 Executive Campus Cherry Hill, NJ  08002 {hhastie, pcraven, morr}@atl.lmco.com 
    
Abstract 
In this paper, we present the WIRE system for human intelligence reporting and discuss chal-lenges of deploying spoken language under-standing systems for the military, particularly for dismounted warfighters. Using the PARADISE evaluation paradigm, we show that performance models derived using standard metrics can account for 68% of the variance of User Satisfaction. We discuss the implication of these results and how the evaluation paradigm may be modified for the military domain.  
1 Introduction Operation Iraqi Freedom has demonstrated the need for improved communication, intelligence, and information capturing by groups of dis-mounted warfighters (soldiers and Marines) at the company level and below. Current methods of col-lecting intelligence are cumbersome, inefficient and can endanger the safety of the collector. For example, a dismounted warfighter who is collect-ing intelligence may stop to take down notes, in-cluding his location and time of report or alternatively try to retain the information in mem-ory. This information then has to be typed into a report on return to base. The authors have devel-oped a unique, hands-free solution by capturing intelligence through spoken language understand-ing technology called WIRE or Wearable Intelli-gent Reporting Environment. Through WIRE, users simply speak what they see, WIRE under-stands the speech and automatically populates a 
report. The report format we have adopted is a SALUTE report which stands for the information fields: Size, Activity, Location, Unit, Time and Equipment. The military user is used to giving in-formation in a structure way, therefore, informa-tion entry is structured but the vocabulary is reasonably varied, an example report is ?Size is three insurgents, Activity is transporting weapons.? These reports are tagged by WIRE with GPS posi-tion and time of filing. The report can be sent in real-time over 802.11 or radio link or downloaded on return to base and viewed on a C2 Interface. WIRE will allow for increased amounts of digit-ized intelligence that can be correlated in space and time to predict adverse events. In addition, pre and post-patrol briefings will be more efficient, accu-rate and complete. Additionally, if reports are transmitted in real time, they have the potential to improve situational awareness in the field. This paper discusses the challenges of taking spoken language understanding technology out of the laboratory and into the hands of dismounted warfighters. We also discuss usability tests and results from an initial test with Army Reservists.  2 System Overview WIRE is a spoken language understanding system that has a plug-and-play architecture (Figure 1) that allows for easy technology refresh of the dif-ferent components. These components pass events to each other via an event bus. The speech is col-lected by an audio server and passed to the Auto-matic Speech Recognizer (ASR) server, which is responsible for converting the audio waveform into an N-best list. The Natural Language (NL) under  
84
 Figure 1.  WIRE System Architecture  standing component executes a named-entity tag-ger to tag and retain key text elements within the each candidate N-best list element. The sets of tagged entities are then parsed using a bottom-up chart parser. The chart parser validates each named entity tag sequence and generates a syntactic parse tree. A heuristic is then applied to select the best parse tree from the N-best list as the representative spoken text. After a parse tree is selected, a seman-tic parser is used to prune the parse tree and pro-duce a semantic frame?a data structure that represents the user's spoken text. The semantic frame is then passed through a rule-based filter that translates text as necessary for processing, e.g., converting text numbers to digits. The semantic frame is then passed to the Dia-logue Manager which decides what action to take based on the most recent utterance and its context. If the system is to speak a reply, the natural lan-guage generation component generates a string of text that is spoken by the Text-To-Speech engine (TTS).  The WIRE spoken language understanding sys-tem was fully developed by the authors with the exception of the ASR, called Dynaspeak?, which was developed by SRI International (Franco et al, 2002) and the TTS engine from Loquendo S.p.A. Grammars for the ASR and NL have to be written for each new domain and report type.  In order for the system to adapt to the user?s en-vironment, there are two modes of operation. In-teractive mode explicitly confirms what the user says and allows the user to ask the system to read back certain fields or the whole report. Alterna-tively, in stealth mode, the user simply speaks the report and WIRE files it immediately. In both 
cases, audio is recorded as a back-up for report accuracy. 3 Challenges of Deployment to Dis-mounted Warfighters The goal of WIRE is to provide a means of report-ing using an interface that is conceptually easy to use through natural language. This is particularly challenging given the fluid nature of war and the constant emergence of new concepts such as dif-ferent types of Improvised Explosive Devices (IEDs) or groups of insurgents. Another challenge is that each unit has its own idiosyncrasies, call signs and manner of speaking. Because WIRE is a limited-domain system and it is not possible to in-corporate all of this variability, we found training to be a key factor in user and system performance and acceptance. A new challenge that phone-based or desk-top systems have yet to face is the need for a mobile spoken language understanding system that can be worn by the user. From a software perspective, WIRE has to have a small footprint. From a hard-ware perspective, the system has to be lightweight, robust, and rugged and must integrate with existing equipment. Wearable computing is constantly evolving and eventually WIRE will be able to run on a system as small as a button. We have also been working with various companies to create a USB noise-canceling microphone similar to what the military user is accustomed to. 4 Experiment Design  Fifteen Army Reservists and three former Marines participated in WIRE usability tests in a laboratory environment. The Reservists predominately pro-vide drill-instructor support for Army basic train-ing groups. The session began with a brief introduction to the WIRE system. Following that, participants reviewed a series of self-paced training slides. They then completed two sets of four sce-narios, with one set completed in stealth mode and the other in interactive mode. A total of 523 utter-ances were collected. Participants were asked to complete five-question surveys at the end of each set of scenarios. For the regression model de-scribed below, we averaged User Satisfaction scores for both types of interaction modes.  
85
We adopted the PARADISE evaluation method (Walker et al, 1997). PARADISE is a ?decision-theoretic framework to specify the relative con-tribution of various factors to a system?s overall performance.? Figure 2 shows the PARADISE model which defines system performance as a weighted function of task-based success measures and dialogue-based cost measures. Dialogue costs are further divided into dialogue efficiency meas-ures and qualitative measures. Weights are calcu-lated by correlating User Satisfaction with performance.  
 Figure 2. PARADISE Model (Walker et al, 1997)   The set of metrics that were collected are:   ? Dialogue Efficiency Measures: User Turns, Average Length of Utterance, Average Re-sponse Latency and Platform. ? Dialogue Quality Measures: Word Accuracy. ? Task Success Measures: Report Accuracy, Field Correctness for Size, Activity, Location, Unit, Time and Equipment. ? User Satisfaction: Average of User Ex-pertise, User Confidence, System Trust, Task Ease, Future Use. User Satisfaction is the average of responses from a survey of five questions on a five-point Likert scale with five being the highest rating. These questions in-clude:  ? Q1: I knew what I could say at any point (User Expertise). ? Q2: I knew what I was doing at any point in the dialog (User Confidence). ? Q3: I trusted that WIRE accurately cap-tured my report information (System Trust). ? Q4: I felt like I could create and file a re-port quickly (Task Ease). 
? Q5: I would recommend that this system be fielded (Future Use). These questions are modified from the more tra-ditional User Satisfaction questions (Walker et al, 2001) that include TTS Performance and Expected Behavior. TTS Performance was substituted be-cause the voice is of such a high quality that it sounds just like a human; therefore, the question is no longer relevant. Expected Behavior was substi-tuted for this study because WIRE is mostly user initiative for the reporting domain.  The Task Success metric was captured by Re-port Accuracy. This was calculated by averaging the correctness of each field over the number of fields attempted. Field correctness was scored manually as either 1 or 0, depending on whether the report field was filled out completely correctly based on user?s intent. Partial credit was not given.  Various platforms were used in the experiment, including laptops, tablet PCs and wearable com-puters. The Platform metric reflects the processing power with 0 being the highest processing power and 1 the less powerful wearable computers. 5 Experimental Results We applied the PARADISE model using the met-rics described above by performing multiple linear regression using a backward coefficient selection method that iteratively removes coefficients that do not help prediction. The best model takes into ac-count 68% of the variance of User Satisfaction (p=.01). Table 1 gives the metrics in the model with their coefficients and p values. Note that the data set is quite small (N=18, df=17), which most likely affected the results.   Table 1. Predictive Power and Significance of Metrics Metric Standardized ?  Coefficients p value User Turns -0.633 0.01 Unit Field Correctness 0.735 0.00 Platform -0.24 0.141  Results show an average User Satisfaction of 3.9 that is broken down into 4.09 for interactive mode and 3.73 for stealth. The lowest medium user satis-faction score was for System Trust (3.5), the high-est for Task Ease (4.5). 
86
Speech recognition word accuracy is 79%, how-ever, Report Accuracy, which is after the speech has been processed by the NL, is 84%. Individual field correctness scores varied from 93% for Activ-ity to 75% for Location. From previous tests, we have found that word accuracy increases through user training and experience up to 95%. 6  Interpretation and Discussion These initial results show that the User Turns met-ric is negatively predictive of User Satisfaction. This is intuitive as the more user turns it takes to complete a report the less satisfied the user.  (Walker et al, 2001) have similar findings for the Communicator data where Task Duration is nega-tively predictive of User Satisfaction in their model (coefficient -0.15).   Secondly, Unit Field Correctness is predictive of User Satisfaction. Given this model and the limited data set, this metric may represent task completion better than overall Report Accuracy. During the test, the user can visually see the report before it is sent. If there are mistakes then this too will affect User Satisfaction. This is similar to findings by (Walker et al, 2001) who found that Task Comple-tion was positively predictive of User Satisfaction (coefficient 0.45).   Finally, Platform is negatively predictive, in other words: the higher the processing power (scored 0) the higher the User Satisfaction and the lower the processing power (scored 1) the lower the User Satisfaction. Not surprisingly, users prefer the system when it runs on a faster computer. This means that the success of the system is likely de-pendent on an advanced wearable computer. There have been recent advances in this field since this experiment. These systems are now available with faster Intel processors and acceptable form factor and battery life. The User Satisfaction results show that areas of improvement include increasing the trust in the user (Q3). This challenge has been discussed pre-viously for military applications in  (Miksch et al, 2004) and may reflect tentativeness of military personnel to accept new technology. Trust in the system can be improved by putting the system in ?interactive? mode, which explicitly confirms each utterance and allows the user to have the system read back the report before sending it. A Wilcoxon signed-rank test (Z = 2.12, p < .05) indicated that 
scores for this question were significantly higher for interactive mode (M = 3.93) than stealth mode (M=3.27). Our current evaluation model uses User Satis-faction as a response variable in line with previous PARADISE evaluations (Walker et al, 2001). However, User Satisfaction may not be the most appropriate metric for military applications. Unlike commercial applications, the goal of a military sys-tem is not to please the user but rather to complete a mission in a highly effective and safe manner. Therefore, a metric such as mission effectiveness may be more appropriate. Similarly, (Forbes-Riley and Litman, 2006) use the domain-specific re-sponse variable, of student learning in their evalua-tion model. An obvious extension to this study is to test in more realistic environments where the users may be experiencing stress in noisy environments. Ini-tial studies have been performed whereby users are physically exerted. These studies did not show a degradation in performance. In addition, initial tests outside in noisy and windy environments em-phasize the need for a high quality noise canceling microphone. Further, more extensive tests of this type are needed. In summary, we have presented the WIRE spo-ken language understanding system for intelligence reporting, and we have discussed initial evalua-tions using the PARADISE methods. Through ad-vances in spoken language understanding, hardware and microphones, this technology will soon transition out of the laboratory and into the field to benefit warfighters and improve security in conflict regions. Acknowledgments Thanks to the Army Reservist 1/417th Regt, 1st BDE 98th Div (IT).   References Forbes-Riley, K. and Litman, D.J. ?Modeling User Sat-isfaction and Student Learning in a Spoken Dialogue Tutoring System with Generic, Tutoring, and User Affect Parameters.? HLT-NAACL, 2006. Franco, H., Zheng, J., Butzberger, J., Cesari, F., Frand-sen, M., Arnold, J., Rao, R., Stolcke, A., and Abrash, V. ?Dynaspeak?: SRI International's scalable speech recognizer for embedded and mobile systems.? HLT, 2002. 
87
Miksch, D., Daniels, J.J., and Hastie, H. (2004). ?Estab-lishing Trust in a Deployed Spoken Language Sys-tem for Military Domains.? In Proc. of AAAI Workshop, 2004. Walker, M.A., Litman, D., Kamm, C. and Abella, A.  ?PARADISE: A Framework for Evaluating Spoken Dialogue Agents.? ACL, 1997. 
Walker, M.A., Passonneau, R., and Boland, J.E. ?Quantitative and Qualitative Evaluation of DARPA Communicator Spoken Dialogue Systems.? ACL, 2001.  
88
Coling 2008: Companion volume ? Posters and Demonstrations, pages 161?164
Manchester, August 2008
?Build Your Own? Spoken Dialogue Systems:
Automatically Generating ISU Dialogue Systems from Business User
Resources
Oliver Lemon, Xingkun Liu, and Helen Hastie
School of Informatics
University of Edinburgh
Informatics Forum
10 Crichton Street
Edinburgh, EH8 9AB
{olemon,xliu4,hhastie}@inf.ed.ac.uk
Abstract
Building effective spoken dialogue sys-
tems (SDS) is currently a complex task
requiring expert knowledge. Our tools
give control of SDS application develop-
ment to non-experts, who need only use
a Graphical User Interface or GUI to de-
velop state-of-the-art ?Information State
Update? (ISU) dialogue systems. Behind
the GUI is a set of Advanced Dialogue
Tools (ADT) that generate complete SDS
based on Business User Resources. These
resources include a database and a Pro-
cess Model that captures the structure of
an application, for example, banking or
restaurant information. Also generated
are speech recognition Language Models
and grammars for robust interpretation of
spontaneous speech. We will demonstrate
how our simple GUI allows developers to
easily and quickly create and modify SDS
without the need for expensive speech ap-
plication service providers. This demon-
stration shows the interface, the ADT com-
ponents, and discusses some of the re-
search issues involved. We also show an
example application built with the tools: a
tourist information system running on an
ultra-mobile PC.
1 Introduction
As automated call centres are becoming more and
more commonplace, new challenges are emerg-
ing such as having to rely on expensive service
c
? 2008. Licensed under the Creative Commons
Attribution-Noncommercial-Share Alike 3.0 Unported li-
cense (http://creativecommons.org/licenses/by-nc-sa/3.0/).
Some rights reserved.
providers to build systems, the inability to quickly
and easily modify live systems, and the time and
cost needed to create new SDS applications. This
paper describes a solution to these problems using
our Advanced Dialogue Tools (ADT). This pro-
totype system allows developers to take already
established business user resources such as Busi-
ness Process Models (BPM) and databases, and
use them to automatically generate spoken dia-
logue systems. Simple customisations can then
be made through the easy-to-use ADT interface or
GUI, which is example-driven. This radically new
way of creating spoken dialogue systems will put
control into the hands of the business user who is
familiar with customer needs and business goals,
thus improving usability and making spoken dia-
logue systems more widely and rapidly available.
Currently, VoiceXML is widely used for such
tasks. However, VoiceXML applications are dif-
ficult to build and maintain, because develop-
ers must anticipate and represent every possi-
ble dialogue path in the finite-state VoiceXML
model. ADT will generate VoiceXML dynami-
cally, but the easy-to-use interface allows devel-
opers to select, deploy, and monitor different ad-
vanced dialogue strategies without needing to code
VoiceXML directly. We apply the ?Information
State Update? (ISU) approach (Lemon, 2004) that
enables more robust, flexible and natural conver-
sations than VoiceXML. ISU uses a more concise
and maintainable representation of dialogue flow,
based on rules operating over dialogue contexts,
which can generalise to unforeseen states.
2 The ADT Architecture
Figure 1 shows the ADT architecture whereby the
main algorithm takes business user resources and
databases as input and uses these to automatically
161
generate the spoken dialogue system. Figure 2
shows part of one such resource, namely a BPM
for hotel bookings. First the caller will hear an
introduction, then they will be asked what price
range they want, and then whether they want a ho-
tel in the centre of town or not. Advantages of us-
ing BPMs include the fact that graphical interfaces
and authoring environments are widely available
for them, for example: Eclipse, IBM Websphere-
Process Server, BEA WeblogicWorkshop etc.. In
addition, Business User Resources can contain a
lot of additional information as well as call flows
including context, multi-media, and multiple cus-
tomer interactions.
Figure 1: The ADT Architecture
Figure 2: Part of an example Business Process
Model for searching for Hotels
The resulting spoken dialogue system deploys
the following main modules:
? Speech Recogniser module, e.g. ATK/HTK
(Young, 2007; Young, 1995) or Nuance (Nu-
ance, 2002)
? Spoken Language Understanding module,
e.g. Grammatical Framework (GF) parser
(Ranta, 2004)
? BPM and Database modules
? Speech synthesiser e.g. Festival (Taylor et al,
1998) or Cereproc (Aylett and Pidcock, 2007)
2.1 Generic Dialogue Modelling
Sophisticated research systems have been devel-
oped only for specific applications and cannot be
easily transferred to another, even very similar task
or domain. The problem of components being do-
main specific is especially prevalent in the core
area of dialogue management. For example MIT?s
Pegasus and Mercury systems (Seneff, 2002) have
dialogue managers (DM) that use approximately
350 domain-specific hand-coded rules each. The
sheer amount of labour required to construct sys-
tems prevents them from being more widely and
rapidly deployed. Our solution uses BPMs and
related authoring tools to specify domain-specific
dialogue interactions which are combined with a
domain-general dialogue manager. Specifically,
the DM consults the BPM to determine what task-
based steps to take next, such as asking for a cin-
ema name. General aspects of dialogue, such as
confirmation and clarification strategies, are han-
dled by the domain-general DM. Values for con-
straints on transitions and branching in the BPM,
for example ?present insurance option if the user is
business-class?, are compiled into domain-specific
parts of the DM?s update rules. XML format is
used for BPMs, and they are compiled into finite
state machines consulted by the spoken dialogue
system through the BPM module. The domain-
general DM was mostly abstracted from the TALK
system (Lemon et al, 2006).
2.2 Compiling Grammars for Business User
Resources and Databases
For Spoken Language Understanding, ADT cur-
rently uses Grammatical Framework (GF) (Ranta,
2004) which is a language for writing multilingual
grammars, on top of which various applications
such as machine translation and human-machine
interaction have been built. A GF grammar not
only defines syntactic well-formedness, but also
semantic content.
Using ADT, system developers do not have to
write a single line of GF grammar code. The sys-
162
tem compiles all database entries and their proper-
ties into the appropriate ?slot-filling? parts of the
GF grammar for each specific BPM.
For example, a generated GF rule is:
Bpm generalTypeRule 4:
town info hotels name->Utt=>{ s = np.s}
This rule was generated because ?name? is a
database field for the subtask hotels in the
?town info? BPM. It specifies that all hotel names
are valid utterances.
A core GF grammar has been developed to cover
basic information-seeking interactions. This is
combined with a domain-specific grammar which
is automatically generated from the BPM, database
and the example utterances provided by the devel-
oper in the GUI. Finally, GF is a robust parser ? it
skips all disfluencies and unknown words to pro-
duce an interpretation of the user input if one ex-
ists.
2.3 Speech Recognition and Text To Speech
The grammars for Spoken Language Understand-
ing generated by ADT are also compiled to
grammar-based language models (LM) for speech
recognition. ADT is plug-and-play and adheres to
industry standards such as GSML, GrXML. This
allows for greater flexibility since the application
developer is not tied to one recogniser or TTS en-
gine. For this demonstration, the speech recog-
niser is ATK (Young, 2007; Young, 1995) and
the speech synthesiser is Cereproc (Aylett and Pid-
cock, 2007). Future work will involve automati-
cally generating context sensitive language models
(Lemon and Gruenstein, 2004).
2.4 ADT GUI
As mentioned above, the prototype ADT GUI can
be used to define system prompts and add likely
user responses to the grammar. Figure 3 shows the
developer associating ?spotter? phrases with sub-
tasks in the BPM. Here the developer is associating
the phrases ?hotels, hotel, stay, room, night, sleep?
and ?rooms? with the hotels task. This means that,
for example, if the user says ?I need a place to
stay?, the hotel-booking BPM will be triggered.
Note that multi-word phrases may also be defined.
The defined spotters are automatically compiled
into the GF grammar for parsing and speech recog-
nition. By default all the lexical entries for answer-
types for the subtasks will already be present as
Figure 3: Example: using the ADT GUI to define
?spotter? phrases for different BPM subtasks
spotter phrases. ADT also checks for possible am-
biguities, for example whether ?pizza? is a spot-
ter for both cuisine type for a restaurant task and
food type for a shopping task, and it uses clarifica-
tion sub-dialogues to resolve them at runtime.
Figure 4 shows the developer?s overview of the
subtasks of a BPM, in this case hotel information.
The developer can navigate this representation and
edit it to define prompts and manipulate the asso-
ciated databases.
Figure 4: Sub-dialogue structure generated from
the Hotel booking BPM
Figure 5 shows the developer specifying the
required linguistic information to automate the
ask price subtask of the hotels BPM. Here the de-
veloper specifies the system prompt for the infor-
mation ?Do you want something cheap or expen-
sive??; a phrase for implicit confirmation of pro-
vided values ?a [X] hotel?, where [X] is the seman-
tics of the speech recognition hypothesis for the
user input; and a clarifying phrase for this subtask
163
Figure 5: Example: using ADT to define prompts,
answer sets, and database mappings for the
ask price subtask of the BPM in Figure 4
?Do you mean the hotel price?? for use when dis-
ambiguating between two or more tasks. The de-
veloper also specifies here the answer type that will
resolve the system prompt. There are many pre-
defined answer-types extracted from the databases
associated with the BPM, and the developer can
select and/or edit these. Optionally, they can give
additional example phrases that users might say
to answer the prompt, and these are automatically
added to the GF grammar.
2.5 Usability
Several demonstration systems have been built us-
ing ADT with an average development time of un-
der an hour. However, our planned evaluation will
test the ability of novice users, with some knowl-
edge of BPMs and databases, to iteratively develop
their own ISU dialogue systems.
3 Summary
This paper describes the Advanced Dialogue Tools
for creating Information State Update based dia-
logue systems automatically from Business User
Resources such as BPMs and databases. The tools
include automatic generation of grammars for ro-
bust interpretation of spontaneous speech, and uses
the application databases and BPMs to generate
lexical entries and grammar rules for speech recog-
nition language modelling. We also demonstrate
an easy-to-use prototype interface that allows the
user to easily and quickly modify aspects of the
dialogue, thus eliminating the need for third party
service providers. This paper describes ADT, its
main components, and some of the research issues
involved in its development.
4 Acknowledgement
This project is funded by a Scottish Enterprise
Proof of Concept Grant (project number 8-ELM-
004).
References
Aylett, Matthew P. and Christopher J. Pidcock. 2007.
The cerevoice characterful speech synthesiser sdk.
In AISB, pages 174?8.
Lemon, Oliver and Alexander Gruenstein. 2004. Mul-
tithreaded context for robust conversational inter-
faces: context-sensitive speech recognition and in-
terpretation of corrective fragments. ACM Trans-
actions on Computer-Human Interaction (ACM
TOCHI), 11(3):241? 267.
Lemon, Oliver, Kallirroi Georgila, James Henderson,
and Matthew Stuttle. 2006. An ISU dialogue system
exhibiting reinforcement learning of dialogue poli-
cies: generic slot-filling in the TALK in-car system.
In Proceedings of EACL, pages 119?122.
Lemon, Oliver. 2004. Context-sensitive speech recog-
nition in Information-State Update dialogue systems:
results for the grammar switching approach. In Pro-
ceedings of the 8th Workshop on the Semantics and
Pragmatics of Dialogue, CATALOG?04, pages 49?
55.
Nuance, 2002. http://www.nuance.com. As of 1 Feb
2002.
Ranta, A. 2004. Grammatical framework. a type-
theoretical grammar formalism. Journal of Func-
tional Programming, 14(2):145?189.
Seneff, Stephanie. 2002. Response Planning and Gen-
eration in the Mercury Flight Reservation System.
Computer Speech and Language, 16.
Taylor, P., A. Black, and R. Caley. 1998. The architec-
ture of the the Festival speech synthesis system. In
Third International Workshop on Speech Synthesis,
Sydney, Australia.
Young, Steve. 1995. Large vocabulary continuous
speech recognition: A review. In Proceedings of
the IEEE Workshop on Automatic Speech Recogni-
tion and Understanding, pages 3?28.
Young, Steve. 2007. ATK: An Application Toolkit
for HTK, Version 1.6. Technical report, Cambridge
University Engineering Department.
164
Proceedings of SIGDIAL 2009: the 10th Annual Meeting of the Special Interest Group in Discourse and Dialogue, pages 148?151,
Queen Mary University of London, September 2009. c?2009 Association for Computational Linguistics
Automatic Generation of Information State Update Dialogue Systems that
Dynamically Create Voice XML, as Demonstrated on the iPhone
Helen Hastie, Xingkun Liu and Oliver Lemon
School of Informatics
University of Edinburgh
{hhastie,xliu4,olemon}@inf.ed.ac.uk
Abstract
We demonstrate DUDE1 (Dialogue
and Understanding Development En-
vironment), a prototype development
environment that automatically generates
dialogue systems from business-user
resources and databases. These generated
spoken dialogue systems (SDS) are then
deployed on an industry standard Voice
XML platform. Specifically, the deployed
system works by dynamically generating
context-sensitive Voice XML pages. The
dialogue move of each page is determined
in real time by the dialogue manager,
which is an Information State Update
engine. Firstly, we will demonstrate the
development environment which includes
automatic generation of speech recogni-
tion grammars for robust interpretation of
spontaneous speech, and uses the appli-
cation database to generate lexical entries
and grammar rules. A simple graphical
interface allows users (i.e. developers) to
easily and quickly create and the modify
SDS without the need for expensive
service providers. Secondly, we will
demonstrate the deployed system which
enables participants to call up and speak
to the SDS recently created. We will also
show a pre-built application running on
the iPhone and Google Android phone for
searching for places such as restaurants,
hotels and museums.
1Patent Pending
1 Introduction
With the advent of new mobile platforms such as
the iPhone and Google Android, there is a need for
a new way to interact with applications and search
for information on the web. Google Voice Search
is one such example. However, we believe that
this simple ?one-shot? search using speech recog-
nition is not optimal for the user. A service that
allows the user to have a dialogue via their phone
opens up a wider set of possibilities. For exam-
ple, the user may be visiting a foreign city and
would like to have a discussion about the types
of restaurants, their cuisine, their price-range and
even ask for recommendations from the system or
their friends on social networking sites. The Di-
alogue Understanding Development Environment
or DUDE makes this possible by providing a flex-
ible, natural, mixed initiative dialogue using an in-
formation state update dialogue engine (Bos et al,
2003).
Currently, if a company wishes to deploy such
a spoken dialogue system, they have to employ
a costly service provider with a long turn around
time for any changes to the system, even minor
ones such as a special promotion offer. In addi-
tion, there is steep competition on application sites
such as Google Market Place and Apple App Store
which are populated with very cheap applications.
DUDE?s Development Environment takes existing
business-user resources and databases and auto-
matically generates the dialogue system. This re-
duces development time and, therefore, costs and
opens up the technology to a wider user-base. In
addition, the DUDE environment is so easy to use
that it gives the control back into the business-user
and away from independent services providers.
In this paper, we describe the architecture and
148
technology of the DUDE Development Environ-
ment and then discuss how the deployed system
works on a mobile platform.
2 The DUDE Development Environment
Figure 1 shows the DUDE Development Envi-
ronment architecture whereby the main algorithm
takes the business-user resources and databases as
input and uses these to automatically generate the
spoken dialogue system which includes a Voice
XML generator. Advantages of using business-
user resources such as Business Process Mod-
els (BPM) (Williams, 1967) include the fact that
graphical interfaces and authoring environments
are widely available (e.g. Eclipse). In addition,
business-user resources can contain a lot of addi-
tional information as well as call flow including
context, multi-media and multiple customer inter-
actions.
Figure 1: The DUDE Architecture
2.1 Spoken Dialogue System Generation
Many sophisticated research systems are devel-
oped for specific applications and cannot be eas-
ily transferred to another, even very similar task or
domain. The problem of components being do-
main specific is especially prevalent in the core
area of dialogue management. For example MIT?s
Pegasus and Mercury systems (Seneff, 2002) have
dialogue managers (DM) that use approximately
350 domain-specific hand-coded rules each. The
sheer amount of labour required to construct sys-
tems prevents them from being more widely and
rapidly deployed. We present a solution whereby
BPMs and related authoring tools are used to spec-
ify domain-specific dialogue interactions which
are combined with domain-general dialogue man-
agers. Specifically, the DM consults the BPM to
determine what task-based steps to take next, such
as asking for price range after establishing pre-
ferred cuisine type. General aspects of dialogue,
such as confirmation and clarification strategies,
are handled by the domain-general DM. Values
for constraints on transitions and branching in the
BPM, for example ?present insurance offer if the
user is business-class?, are compiled into domain-
specific parts of the Information State. XML for-
mat is used for BPMs, and they are compiled into
finite state machines consulted by the spoken dia-
logue system. The domain-general dialogue man-
ager was mostly abstracted from the TALK system
(Lemon et al, 2006).
Using DUDE, developers do not have to write
a single line of grammar code. There are three
types of grammars: (1) a core grammar, (2) a
grammar generated from the database and BPM,
and (3) dynamically generated grammars created
during the dialogue. The core grammar (1) was
developed to cover basic information-seeking in-
teractions. In addition (2), the system com-
piles relevant database entries and their proper-
ties into the appropriate ?slot-filling? parts of a
SRGS GRXML (Speech Recognition Grammar
Specification) grammar for each specific BPM
node. Task level grammars are used to allow a
level of mixed initiative, for example, if the sys-
tem asks ?what type of cuisine?? the user can
reply with cuisine and also any other slot type,
such as, ?cheap Italian?. The dynamically gen-
erated grammars (3), such as for restaurants cur-
rently being recommended, minimizes grammar
size and makes the system more efficient. In ad-
dition to the above-mentioned grammars, devel-
opers are able to provide task spotter phrases and
synonyms reflecting how users might respond by
using the DUDE Development Environment. If
these are not already covered by the existing gram-
mar, DUDE automatically generates rules to cover
them.
The generated SRGS GRXML grammars are
used to populate the Voice XML pages and conse-
quently used by the Voice XML Platform Speech
recogniser. In this case, we deploy our system to
the Voxeo Platform (http://www.voxeo.com). As
well as the W3C standard SRGS GRXML, DUDE
is able to generate alternative grammar specifica-
tions such as SRGS ABNF (Augmented Backus-
Naur Form), JSGF ABNF (Java Speech Grammar
Format) and Nuance?s GSL (Grammar Specifica-
149
Figure 2: Example: using the DUDE Development Environment to define spotter phrases and other
information for the different BPM tasks
tion Language).
2.2 The Development Environment
As mentioned above, the DUDEDevelopment En-
vironment can be used to define system prompts
and add task spotter phrases and synonyms to the
grammars. Figure 2 shows the GUI with the BPM
on the left hand side and the properties pane for
the restaurants task on the right hand side. In this
pane the developer can define the system prompt,
the information to be presented to the user and the
spotter phrases. Here the developer is associating
the phrases ?restaurants, restaurant, somewhere to
eat....? with the restaurant task. This means that
if the user says ?I want somewhere to eat?, the
restaurant part of the BPM will be triggered. Note
that multi-word phrases may also be defined. The
defined spotters are automatically compiled into
the grammar for parsing and speech recognition.
By default all the lexical entries for answer-types
for the subtasks will already be present as spotter
phrases. DUDE checks for possible ambiguities,
for example if ?pizza? is a spotter for both cui-
sine type for a restaurant task and food type for a
shopping task, the system uses a clarification sub-
dialogue to resolve them at runtime.
Figure 3 shows the developer specifying the re-
quired linguistic information to automate the cui-
sine subtask of the restaurants task. Here the de-
veloper specifies the system prompt ?What type
of cuisine do you want?? and a phrase for im-
plicit confirmation of provided values, e.g. ?a [X]
restaurant?, where [X] is a variable that will be
replaced with the semantics of the speech recogni-
tion hypothesis for the user input. The developer
also specifies here the answer type that will resolve
the system prompt. There are predefined answer-
types extracted from the databases, and the devel-
oper can select and/or edit these, adding phrases
and synonyms. In addition, they have the ability
to define their own answer-types.
Figure 3: Example: using the DUDE Develop-
ment Environment to define prompts, answer sets,
and database mappings for the cuisine subtask
150
3 Deployment of the Generated Spoken
Dialogue System
The second part of the demonstration shows
a pre-built multimodal application running on
the iPhone (http://www.apple.com) and Google
Android phone (http://code.google.com//android).
This application allows the user to have a dialogue
about places of interest using The List website
(http://www.list.co.uk). Figure 4 shows screen-
shots of the iPhone, firstly with The List home-
page and then a page with content on Bar Roma,
an ?italian restaurant in Edinburgh? as requested
by the user through spoken dialogue.
Figure 4: DUDE-generated iPhone List Applica-
tion pushing relevant web content
Figure 5 shows the architecture of this system
whereby the DUDE server runs the spoken dia-
logue system (as outputted from the DUDEDevel-
opment Environment). This system dynamically
generates Voice XML pages whose dialogue move
and grammar is determined by the Information
State Update Dialogue Model. These Voice XML
pages are sent in real time to the Voice XML plat-
form (in our case Voxeo) which the user talks to by
placing a regular phone call. In addition, DUDE
communicates the relevant URL via a server con-
nection.
4 Summary
This paper describes a demonstration of the
DUDE Development Environment and its result-
ing spoken dialogue systems as deployed on a mo-
bile phone, specifically the iPhone and Google
Android. With the emergence of web-enabled
smart-phones, a new and innovative interactive
method is needed that combines web-surfing and
Figure 5: Architecture of deployed DUDE Appli-
cation on a mobile phone (e.g. the iPhone)
dialogue in order to get the user exactly the infor-
mation required in real time.
5 Acknowledgement
This project is funded by a Scottish Enterprise
Proof of Concept Grant (project number 8-ELM-
004). We gratefully acknowledge The List for giv-
ing us data for our prototype application.
References
Johan Bos, Ewan Klein, Oliver Lemon, and Tetsushi
Oka. 2003. DIPPER: Description and Formalisa-
tion of an Information-State Update Dialogue Sys-
tem Architecture. In 4th SIGdial Workshop on Dis-
course and Dialogue, pages 115?124, Sapporo.
Adam Cheyer and David Martin. 2001. The Open
Agent Architecture. Journal of Autonomous Agents
and Multi-Agent Systems, 4(1/2):143?148.
Oliver Lemon, Kallirroi Georgila, James Henderson,
and Matthew Stuttle. 2006. An ISU dialogue sys-
tem exhibiting reinforcement learning of dialogue
policies: generic slot-filling in the TALK in-car sys-
tem. In Proceedings of EACL, pages 119?122.
Stephanie Seneff. 2002. Response Planning and Gen-
eration in the Mercury Flight Reservation System.
Computer Speech and Language, 16.
S Williams. 1967. Business process modeling im-
proves administrative control. Automation, pages
44?50.
151
Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural
Language Learning, pages 82?93, Jeju Island, Korea, 12?14 July 2012. c?2012 Association for Computational Linguistics
Optimising Incremental Dialogue Decisions Using Information Density for
Interactive Systems
Nina Dethlefs, Helen Hastie, Verena Rieser and Oliver Lemon
School of Mathematical and Computer Sciences
Heriot-Watt University, Edinburgh, Scotland
n.s.dethlefs | h.hastie | v.t.rieser | o.lemon@hw.ac.uk
Abstract
Incremental processing allows system design-
ers to address several discourse phenomena
that have previously been somewhat neglected
in interactive systems, such as backchannels
or barge-ins, but that can enhance the re-
sponsiveness and naturalness of systems. Un-
fortunately, prior work has focused largely
on deterministic incremental decision mak-
ing, rendering system behaviour less flexible
and adaptive than is desirable. We present a
novel approach to incremental decision mak-
ing that is based on Hierarchical Reinforce-
ment Learning to achieve an interactive op-
timisation of Information Presentation (IP)
strategies, allowing the system to generate
and comprehend backchannels and barge-ins,
by employing the recent psycholinguistic hy-
pothesis of information density (ID) (Jaeger,
2010). Results in terms of average rewards
and a human rating study show that our learnt
strategy outperforms several baselines that are
not sensitive to ID by more than 23%.
1 Introduction
Recent work on incremental systems has shown
that adapting a system?s turn-taking behaviour to be
more human-like can improve the user?s experience
significantly, based on incremental models of auto-
matic speech recognition (ASR) (Baumann et al
2011), dialogue management (Buss et al2010), and
speech generation (Skantze and Hjalmarsson, 2010).
All of these approaches are based on the same gen-
eral abstract architecture of incremental processing
(Schlangen and Skantze, 2011). While this archi-
tecture offers inherently incremental mechanisms to
update and revise input hypotheses, it is affected
by a number of drawbacks, shared by determinis-
tic models of decision making in general: they rely
on hand-crafted rules which can be time-consuming
and expensive to produce, they do not provide a
mechanism to deal with uncertainty introduced by
varying user behaviour, they are unable to gener-
alise and adapt flexibly to unseen situations, and
they do not use automatic optimisation. Statisti-
cal approaches to incremental processing that ad-
dress some of these problems have been suggested
by Raux and Eskenazi (2009), who use a cost matrix
and decision theoretic principles to optimise turn-
taking in a dialogue system under the constraint that
users prefer no gaps and no overlap at turn bound-
aries. Also, DeVault et al2009) use maximum en-
tropy classification to support responsive overlap in
an incremental system by predicting the completions
of user utterances. Selfridge et al2011) use logis-
tic regression models to predict the stability and ac-
curacy of incremental speech recognition results to
enhance performance without causing delay. For re-
lated work on (deterministic) incremental language
generation, please see (Kilger and Finkler, 1995;
Purver and Otsuka, 2003).
Recent years have seen a number of data-driven
approaches to interactive systems that automatically
adapt their decisions to the dialogue context us-
ing Reinforcement Learning (Levin et al2000;
Walker, 2000; Young, 2000; Singh et al2002;
Pietquin and Dutoit, 2006; Henderson et al2008;
Cuaya?huitl et al2010; Thomson, 2009; Young et
al., 2010; Lemon, 2011; Janarthanam and Lemon,
2010; Rieser et al2010; Cuaya?huitl and Dethlefs,
82
2011; Dethlefs and Cuaya?huitl, 2011). While these
approaches have been shown to enhance the perfor-
mance and adaptivity of interactive systems, unfor-
tunately none of them has yet been combined with
incremental processing.
In this paper, we present a novel approach to in-
cremental decision making for output planning that
is based on Hierarchical Reinforcement Learning
(HRL). In particular, we address the problem of op-
timising IP strategies while allowing the system to
generate and comprehend backchannels and barge-
ins based on a partially data-driven reward func-
tion. Generating backchannels can be beneficial for
grounding in interaction. Similarly, barge-ins can
lead to more efficient interactions, e.g. when a sys-
tem can clarify a bad recognition result immediately
before acting based on a misrecognition.
A central concept to our approach is Information
Density (ID) (Jaeger, 2010), a psycholinguistic hy-
pothesis that human utterance production is sensitive
to a uniform distribution of information across the
utterance. This hypothesis has also been adopted for
low level output planning recently, see e.g. Rajku-
mar and White (2011). Our results in terms of av-
erage rewards and a human rating study show that a
learning agent that is sensitive to ID can learn when
it is most beneficial to generate feedback to a user,
and outperforms several other agents that are not
sensitive to ID.
2 Incremental Information Presentation
2.1 Information Presentation Strategies
Our example domain of application is the Infor-
mation Presentation phase in an interactive system
for restaurant recommendations, extending previous
work by Rieser et al2010). This previous work
incrementally constructs IP strategies according to
the predicted user reaction, whereas our approach
focuses on whether and when to generate backchan-
nels and barge-ins and how to react to user barge-
ins in the context of dynamically changing input hy-
potheses. We therefore implement a simplified ver-
sion of Rieser et al model. Their system distin-
guished two steps: the selection of an IP strategy
and the selection of attributes to present to the user.
We assume here that the choice of attributes is deter-
mined by matching the types specified in the user in-
put, so that our system only needs to choose a strat-
egy for presenting its results. Attributes include cui-
sine, food quality, location, price range and service
quality of a restaurant. The system then performs a
database lookup and chooses among three main IP
strategies summary, comparison, recommendation
and several ordered combinations of these. Please
see Rieser et al2010) for details. Table 1 shows
examples of the main types of IP strategies that we
generate.
2.2 Backchannels and Barge-ins
An important advantage of incremental processing
can be the increased reactiveness of systems. In this
paper, we focus on the phenomena of backchannels
and barge-ins that can act as feedback in an interac-
tion for both user and system. Figure 1 shows some
examples. Backchannels can often be interpreted as
signals of grounding. Coming from the user, the sys-
tem may infer that the user is following the presenta-
tion of information or is confirming a piece of infor-
mation without trying to take the turn. Similarly, we
can allow a system to generate backchannels to the
user to confirm that it understands the user?s prefer-
ences, i.e. receives high confidence scores from the
ASR module. An important decision for a dialogue
system is then when to generate a backchannel?
Barge-ins typically occur in different situations.
The user may barge-in on the system to correct an
ASR error (such as ?Italian? instead of ?Indian? in
Figure 1) or the system may want to barge-in on the
user to confirm a low-confidence ASR hypothesis so
as to be able to start an immediate database look up
for results. In the former case, the user barging-in
on the system, we assume that the system has two
choices: yielding the turn to the user, or trying to
keep the turn. In the latter case, the system barging-
in on the user, the system would have to decide if and
when it would be beneficial to barge-in on a user ut-
terance. In the following sections, we will develop
a model of dialogue optimisation that can address
these question based on Hierarchical RL that opti-
mises system behaviour based on trade-offs defined
in terms of ID.
83
Type Example
Comparison The restaurant Roma is in the medium price range, but does not serve excellent food.
The restaurants Firenze and Verona both have great food but are more expensive. The
restaurant Verona has good service, too.
Recommendation Restaurant Verona has the best overall match with your query. It is a bit more expen-
sive, but has great food and service.
Summary I found 24 Italian restaurants in the city centre that match your query. 11 of them are
in the medium price range, 5 are cheap and 8 are expensive.
Table 1: Examples of IP as a comparison, recommendation and summary for a user looking for Italian restaurants in
the city centre that have a good price for value.
Backchannel 1 (the system backchannels)
USR I want Italian food [500 ms] in the city centre. . .
SYS uh-huh
SYS OK. I found 24 Italian restaurants in the city centre. The
restaurant Roma is in the medium price range, but does not
have great food. The restaurants Firenze and Verona . . .
Backchannel 2 (the user backchannels)
USR I want Italian food in the centre of town . . .
SYS OK. I found 35 central Italian restaurants . . .
USR OK.
SYS The restaurant Verona has great food but is also a bit
expensive. The Roma is cheaper, but not as central as Verona . . .
Barge-ins 1 (the user barges-in on system)
USR I want Italian food in the centre of town . . .
SYS I found 35 Indian . . .
USR Not Indian, I want Italian.
SYS OK, Italian . . .
SYS I have 24 Italian restaurants . . .
Barge-ins 2 (the system barges-in on user)
USR I need an Italian restaurant that is located . . .
SYS I?m sorry. Did you say
Indian or Italian?
USR I said Italian. And in the centre of town please.
SYS OK, let me see. I have 24 Italian restaurants . . .
Figure 1: Example phenomena generated with the learnt
policy. The agent has learnt to produce backchannels
and barge-ins at the appropriate moment and alternative
strategies to deal with user barge-ins.
3 Information Theory
Information Theory as introduced by Shannon
(1948) is based on two main concepts: a communi-
cation channel through which information is trans-
ferred in bits and the information gain, i.e. the in-
formation load that each bit carries. For natural lan-
guage, the assumption is that people aim to com-
municate according to the channel?s capacity, which
corresponds to the hearer?s capacity in terms of cog-
nitive load. If they go beyond that, the cognitive load
of the listener gets too high. If they stay (far) below,
too little information is transferred per bit (i.e., the
utterance is inefficient or uninformative). The in-
formation gain of each word, which is indicative of
how close we are to the channel?s capacity, can be
computed using entropy measures.
3.1 Information Density
Psycholinguistic research has presented evidence for
users distributing information across utterances uni-
formly, so that each word is carrying roughly the
same amount of information. This has been ob-
served for phonetic phenomena based on words
(Bell et al2003) and syllables (Aylett and Turk,
2004), and for syntactic phenomena (Levy and
Jaeger, 2007; Jaeger, 2010). Relating ID to likeli-
hood, we can say that the less frequent a word is, the
more information it is likely to carry (Jaeger, 2010).
For example the word ?the? often has a high corpus
frequency but a low ID.
The ID is defined as the log-probability of an
event (i.e. a word) (Shannon, 1948; Levy and Jaeger,
2007), so that for an utterance u consisting of the
word sequence w1 . . . wi?1, we can compute the ID
at each point during the utterance as:
log 1P (u) =
n
?
i=1
log 1P (wi|w1 . . . wi?1)
(1)
While typically the context of a word is given by
all preceding words of the utterance, we follow Gen-
zel and Charniak (2002) in restricting our computa-
tion to tri-grams for computability reasons. Given a
84
language model of the domain, we can therefore op-
timise ID in system-generated discourse, where we
treat ID as ?an optimal solution to the problem of
rapid yet error-free communication in a noisy envi-
ronment? (Levy and Jaeger (2007), p.2). We will
now transfer the notion of ID to IP and investigate
the distribution of information over user restaurant
queries.
3.2 Information Density in User Utterances
We aim to use ID for incremental IP in two ways:
(1) to estimate the best moment for generating
backchannels or barge-ins to the user, and (2) to de-
cide whether to yield or keep the current system turn
in case of a user barge-in. While we do not have spe-
cific data on human barge-in behaviour, we know
from the work of (Jaeger, 2010), e.g., that ID influ-
ences human language production. We therefore hy-
pothesise a relationship between ID and incremen-
tal phenomena. A human-human data collection is
planned for the near future.
To compute the ID of user and system utterances
at each time step, we estimated an n?gram lan-
guage model (using Kneser-Ney smoothing) based
on a transcribed corpus of human subjects interact-
ing with a system for restaurant recommendations of
Rieser et al2011).1 The corpus contained user ut-
terances as exemplified in Figure 1 and allowed us to
compute the ID at any point during a user utterance.2
In this way, we can estimate points of low density
which may be eligible for a barge-in or a backchan-
nel. Figure 2 shows some example utterances drawn
from the corpus and their ID including the first sen-
tence from Figure 1. These examples were typical
for what could generally be observed from the cor-
pus. We see that while information is transmitted
with varying amounts of density, the main bits of in-
formation are transmitted at a scale between 2 and
7.
Due to a lack of human data for the system utter-
ances, we use the same corpus data to compute the
ID of system utterances.3 The learning agent can use
1Available at http://www.macs.hw.ac.uk/
ilabarchive/classicproject/data/login.php.
2Note that our model does not currently handle out-of-
domain words. In future work, we will learn when to seek clar-
ification.
3We plan a data collection of such utterances for the future,
1 2 3 4 5 6 7 8 9 10 11 12
0
2
4
6
8
10
12
14
16
18
20
In
fo
rm
at
io
n 
De
ns
ity
Time 
 
 
I want Italian food in the city centre.
Yes, I need a moderately priced restaurant in the New Chesterton area.
I need the address of a Thai restaurant.
Figure 2: Information Density for example utterances,
where peaks indicate places of high density.
this information to consider the trade-off of yielding
a current turn to the user or trying to keep it, e.g., in
case of a user barge-in given the ID of its own turn
and of the user?s incoming turn. Such decisions will
be made incrementally in our domain given dynam-
ically changing hypotheses of user input.
4 Incremental Utterance Optimisation
To optimise incremental decision making for an in-
teractive system given the optimisation measure of
ID, we formalise the dialogue module as a Hierar-
chical Reinforcement Learning agent and learn an
optimal action policy by mapping states to actions
and optimising a long-term reward signal. The di-
alogue states can be seen as representing the sys-
tem?s knowledge about the task, the user and the
environment. The dialogue actions correspond to
the system?s capabilities, such as present the re-
sults or barge-in on the user. They also handle in-
cremental updates in the system. In addition, we
need a transition function that specifies the way
that actions change the environment (as expressed
in the state representation) and a reward function
which specifies a numeric value for each action
taken. In this way, decision making can be seen
as a finite sequence of states, actions and rewards
{s0, a0, r1, s1, a1, ..., rt?1, st}, where the goal is to
induce an optimal strategy automatically using Rein-
forcement Learning (RL) (Sutton and Barto, 1998).
We used Hierarchical RL, rather than flat RL, be-
cause the latter is affected by the curse of dimen-
sionality, the fact that the state space grows expo-
nentially according to the state variables taken into
account. This affects the scalability of flat RL agents
but for now make the assumption that using the corpus data is
informative since they are from the same domain.
85
and limits their application to small-scale problems.
Since timing is crucial for incremental approaches,
where processing needs to be fast, we choose a hi-
erarchical setting for better scalability. We denote
the hierarchy of RL agents as M ij where the in-
dexes i and j only identify an agent in a unique
way, they do not specify the execution sequence of
subtasks, which is subject to optimisation. Each
agent of the hierarchy is defined as a Semi-Markov
Decision Process (SMDP) consisting of a 4-tuple
< Sij , Aij , T ij , Rij >. Here, Sij denotes the set of
states, Aij denotes the set of actions, and T ij is a
probabilistic state transition function that determines
the next state s? from the current state s and the per-
formed action a. Rij(s?, ? |s, a) is a reward function
that specifies the reward that an agent receives for
taking an action a in state s lasting ? time steps
(Dietterich, 1999). Since actions in SMDPs may
take a variable number of time steps to complete,
the variable ? represents this number of time steps.
The organisation of the learning process into dis-
crete time steps allows us to define incremental hy-
pothesis updates as state updates and transitions in
an SMDP. Whenever conditions in the learning en-
vironment change, such as the recogniser?s best hy-
pothesis of the user input, we represent them as tran-
sitions from one state to another. At each time step,
the agent checks for changes in its state represen-
tation and takes the currently best action according
to the new state. The best action in an incremental
framework can also include generating a backchan-
nel to the user to indicate the status of grounding
or barging-in to confirm an uncertain piece of infor-
mation. Once information has been presented to the
user, it is committed or realised. Realised informa-
tion is represented in the agent?s state, so that it can
monitor its own output.
Actions in a Hierarchical Reinforcement learner
can be either primitive or composite. The former
are single-step actions that yield single rewards, and
the latter are multi-step actions that correspond to
SMDPs and yield cumulative rewards. Decision
making occurs at any time step of an SMDP: after
each single-step action, we check for any updates
of the environment that require a system reaction or
change of strategy. If no system action is required
(e.g. because the user is speaking), the system can
decide to do nothing. The goal of each SMDP is to
find an optimal policy pi? that maximises the reward
for each visited state, according to
pi?ij(s) = argmaxa?A Q
?i
j(s, a), (2)
where Qij(s, a) specifies the expected cumulative re-
ward for executing action a in state s and then fol-
lowing pi?. We use HSMQ-Learning to induce dia-
logue policies, see (Cuaya?huitl, 2009), p. 92.
5 Experimental Setting
5.1 Hierarchy of Learning Agents
The HRL agent in Figure 3 shows how the tasks of
(1) dealing with incrementally changing input hy-
potheses, (2) choosing a suitable IP strategy and (3)
presenting information, are connected. Note that
we focus on a detailed description of models M10...3
here, which deal with barge-ins and backchannels
and are the core of this paper. Please see Dethlefs et
al. (2012) for details of an RL model that deals with
the remaining decisions.
Briefly, model M00 deals with dynamic input hy-
potheses. It chooses when to listen to an incoming
user utterance (M13 ) and when and how to present
information (M10...2) by calling and passing control
to a child subtask. The variable ?incrementalStatus?
characterises situations in which a particular (incre-
mental) action is triggered, such as a floor holder ?let
me see?, a correction or self-correction. The variable
?presStrategy? indicates whether a strategy for IP has
been chosen or not, and the variable ?userReaction?
shows the user?s reaction to an IP episode. The
?userSilence? variable indicates whether the user is
speaking or not. The detailed state and action space
of the agents is given in Figure 4. We distinguish ac-
tions for Information Presentation (IP), actions for
attribute presentation and ordering (Slot-ordering),
and incremental actions (Incremental).
Models M10...2 correspond to different ways of
presenting information to the user. They perform
attribute selection and ordering and then call the
child agents M20...4 for attribute realisation. When-
ever a user barges in over the system, these agents
will decide to either yield the turn to the user or to
try and keep the turn based on information density.
The variables representing the status of the cuisine,
86
Root
Summary Comparison Recommendation
Observe
 User
Present
Cuisine
Present
 Food
Present
Location
Present
  Price
Present
Service
Figure 3: Hierarchy of learning agent for incremental In-
formation Presentation and Slot Ordering.
food, location, price and service of restaurants indi-
cate whether the slot is of interest to the user (we as-
sume that 0 means that the user does not care about
this slot), and what input confidence score is cur-
rently associated with the value of the slot. For ex-
ample, if our current best hypothesis is that the user
is interested in Indian restaurants, the variable ?sta-
tusCuisine? will have a value between 1-3 indicating
the strength of this hypothesis. Once slots have been
presented to the user, they are realised and can only
be changed through a correction or self-correction.
Model M13 is called whenever the user is speak-
ing. The system?s main choice here is to remain
silent and listen to the user or barge-in to request
the desired cuisine, location, or price range of a
restaurant. This can be beneficial in certain situa-
tions, such as when the system is able to increase its
confidence for a slot from ?low? to ?high? through
barging-in with a direct clarification request, e.g.
?Did you say Indian?? (and thereby saving sev-
eral turns that may be based on a wrong hypoth-
esis). This can also be harmful in certain situa-
tions, though, assuming that users have a general
preference for not being barged-in on. The learning
agent will need to learn to distinguish these situa-
tions. This agent is also responsible for generating
backchannels and will over time learn the best mo-
ments to do this.
Models M20...4 choose surface forms for presenta-
tion to the user from hand-crafted templates. They
are not the focus of this paper, however, and there-
fore not presented in detail. The state-action space
size of this agent is roughly 1.5 million.4 The agent
4Note that a flat RL agent, in contrast, would need 8? 1025
million state-actions to represent this problem.
States M00
incrementalStatus {0=none,1=holdFloor,2=correct,3=selfCorrect}
observeUser {0=unfilled,1=filled}
presStrategy {0=unfilled,1=filled}
userReaction {0=none,1=select,2=askMore,3=other}
userSilence={0=false,1=true}
Actions M00
IP: compare M11 , recommend M12 , summarise M10 , sum-
mariseCompare, summariseRecommend, summariseCompar-
eRecommend,
Incremental: correct, selfCorrect, holdFloor, observeUser
Goal State M00 0, 1, 1, 0, ?
States M10...2
IDSystem={0=low,1=medium, 2=high}
statusCuisine {0=unfilled,1=low,2=medium,3=high,4=realised}
statusQuality {0=unfilled,1=low,2=medium,3=high,4=realised}
statusLocation {0=unfilled,1=low,2=medium,3=high,4=realised}
statusPrice {0=unfilled,1=low,2=medium,3=high,4=realised}
statusService {0=unfilled,1=low,2=medium,3=high,4=realised}
turnType {0=holding, 1=resuming, 2=keeping, 3=yielding}
userBargeIn {0=false, 1=true}
Actions M10...2
Slot-ordering: presentCuisine M20 , presentQuality M21 ,
presentLocation M22 , presentPrice M23 , presentService M24 ,
Incremental: yieldTurn, keepTurn
Goal State M10...2 ?, ? 4, 0 ? 4, 0 ? 4, 0 ? 4, 0 ? 4, ?, ?
States M13
bargeInOnUser={0=undecided,1=yes, 2=no}
IDUser={0=low,1=medium, 2=high, 3=falling, 4=rising}
statusCuisine {0=unfilled,1=low,2=medium,3=high,4=realised}
statusLocation {0=unfilled,1=low,2=medium,3=high,4=realised}
statusPrice {0=unfilled,1=low,2=medium,3=high,4=realised}
Actions M13
Incremental: doNotBargeIn, bargeInCuisine, bargeInLocation,
bargeInPrice, backchannel
Goal State M13 >0, ?, 0 ? 4, 0 ? 4, 0 ? 4
States M20...4
IDSystem={0=low,1=medium, 2=high}
IDUser={0=low,1=medium, 2=high, 3=falling, 4=rising}
surfaceForm {0=unrealised,1=realised}
Actions M20...4
Surface Realisation: [alternative surface realisations]
e.g. ?$number$ restaurants serve $cuisine$ food?, ?$number$
places are located in $area$, etc.
Goal State M20...4 ?, ?, 1
Figure 4: The state and action space of the HRL agent.
The goal state is reached when all items (that the user
specified in the search query) have been presented. Ques-
tion marks mean that a variable does not affect the goal
state, which can be reached regardless of the variable?s
value.
reaches its goal state (defined w.r.t. the state vari-
87
ables in Fig. 4) when an IP strategy has been chosen
and all information has been presented.
5.2 The Simulated Environment
For a policy to converge, a learning agent typically
needs several thousand interactions in which it is ex-
posed to a multitude of different circumstances. For
our domain, we designed a simulated environment
with three main components addressing IP, incre-
mental input hypotheses and ID. Using this simula-
tion, we trained the agent for 10 thousand episodes,
where one episode corresponds to one recommenda-
tion dialogue.
5.2.1 Information Presentation
To learn a good IP strategy, we use a user simula-
tion5 by Rieser et al2010) which was estimated
from human data and uses bi-grams of the form
P (au,t|IPs,t), where au,t is the predicted user reac-
tion at time t to the system?s IP strategy IPs,t in state
s at time t. We distinguish the user reactions of se-
lect a restaurant, addMoreInfo to the current query to
constrain the search, and other. The last category is
usually considered an undesirable user reaction that
the system should learn to avoid. The simulation
uses linear smoothing to account for unseen situa-
tions. In this way, we can predict the most likely
user reaction to each system action. Even though
previous work has shown that n-gram-based simu-
lations can lead to dialogue inconsistencies, we as-
sume that for the present study this does not present
a problem, since we focus on generating single utter-
ances and on obtaining user judgements for single,
independent utterances.
5.2.2 Input Hypothesis Updates
While the IP strategies can be used for incremen-
tal and non-incremental dialogue, the second part of
the simulation deals explicitly with the dynamic en-
vironment updates that the system will need to be
sensitive to in an incremental setting. We assume
that for each restaurant recommendation, the user
has the option of filling any or all of the attributes
cuisine, food quality, location, price range and ser-
vice quality. The possible values of each attribute
and possible confidence scores for each value are
5The simulation data are available from www.
classic-project.org.
shown in Table 2. A score of 0 means that the user
does not care about the attribute, 1 means that the
system?s confidence in the attribute?s value is low, 2
that the confidence is medium, and 3 means that the
confidence is high. A value of 4 means that the at-
tribute has already been realised, i.e. communicated
to the user. At the beginning of a learning episode,
we assign each attribute a possible value and con-
fidence score with equal probability. For food and
service quality, we assume that the user is never in-
terested in bad food or service. Subsequently, con-
fidence scores can change at each time step. In fu-
ture work these transition probabilities will be esti-
mated from a data collection, though the following
assumptions are realistic based on our experience.
We assume that a confidence score of 0 changes to
any other value with a likelihood of 0.05. A confi-
dence score of 1 changes with a probability of 0.3,
a confidence score of 2 with a probability of 0.1
and a confidence score of 3 with a probability of
0.03. Once slots have been realised, their value is
set to 4. They cannot be changed then without an ex-
plicit correction. We also assume that realised slots
change with a probability of 0.1. If they change,
we assume that half of the time, the user is the ori-
gin of the change (because they changed their mind)
and half of the time the system is the origin of the
change (because of an ASR or interpretation error).
Each time a confidence score is changed, it has a
probability of 0.5 for also changing its value. The
resulting input to the system are data structures of
the form present(cuisine=Indian), confidence=low.
The probability of observing this data structure in
our simulation is 0.1 (for Indian) ? 0.2 (for low
confidence) = 0.02. Its probability of changing
to present(cuisine=italian), confidence=high is 0.1
(for changing from low to medium) ? 0.05 (for
changing from Indian to Italian) = 0.005.
5.2.3 Information Density Updates
We simulate ID of user utterances based on proba-
bilistic context-free grammars (PCFG) that were au-
tomatically induced from the corpus data in Section
3.2 using the ABL algorithm (van Zaanen, 2000).
This algorithm takes a set of strings as input and
computes a context-free grammar as output by align-
ing strings based on Minimum Edit Distance. We
use the n?gram language models trained earlier to
88
Attribute Values Confidence
Cuisine Chinese, French, German, In-, 0, 1, 2, 3, 4
dian, Italian, Japanese, Mexi-
can, Scottish, Spanish, Thai
Quality bad, adequate, good, very good 0, 1, 2, 3, 4
Location 7 distinct areas of the city 0, 1, 2, 3, 4
Price cheap, good-price-for-value,
expensive, very expensive 0, 1, 2, 3, 4
Service bad, adequate, good, very good 0, 1, 2, 3, 4
Table 2: User goal slots for restaurant queries with possi-
ble values and confidence scores.
add probabilities to grammar rules. We use these
PCFGs to simulate user utterances to which the sys-
tem has to react. They can be meaningful utter-
ances such as ?Show me restaurants nearby? or less
meaningful fragments such as ?um let me see, do
you. . . hm?. The former type is more frequent in
the data, but both types can be simulated along with
their ID (clearly, the first type is more dense than the
second).
In addition to simulating user utterances, we
hand-crafted context-free grammars of system ut-
terances and augmented them with probabilities es-
timated using the same user corpus data as above
(where again, we make the assumption that this is
to some extent feasible given the shared domain).
We use the simulated system utterances to compute
varying degrees of ID for the system.
Both measures, the ID of user and system utter-
ances, can inform the system during learning to bal-
ance the trade-off between them for generating and
receiving backchannels and barge-ins.
5.3 A Reward Function for Incremental
Dialogue Based on Information Density
To train the HRL agent, we use a partially data-
driven reward function. For incremental IP, we use
rewards that are based on human intuition. The
agent receives
R =
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
+100 if the user selects an item,
0 if the user adds further con-
straints to the search,
-100 if the user does something else
or a self-correction,
-0.5 for the system holding a turn,
-1 otherwise.
The agent is encouraged to choose those sequences
of actions that lead to the user selecting a restaurant
as quickly as possible. If the agent is not sure what to
say (because planning has not finished), it can gen-
erate a floor holding marker, but should in any case
avoid a self-correction due to having started speak-
ing too early.
The remaining rewards are based on ID scores
computed incrementally during an interaction. The
agent receives the following rewards, where info-
Density(Usr) and infoDensity(Sys) refer to the ID of
the current user and system utterance, respectively,
as defined in Equation 1.
R =
?
?
?
?
?
?
?
-infoDensity(Usr) for keeping a turn,
barging-in or
a backchannel,
-infoDensity(Sys) for yielding a turn.
These two measures encourage the agent to consider
the trade-offs between its own ID and the one trans-
mitted by an incoming user utterance. Barging-in
on a user utterance at a low ID point then yields a
small negative reward, whereas barging-in on a user
utterance at a high ID point yields a high negative
reward. Both rewards are negative because barging-
in on the user always contains some risk. Similarly,
keeping a turn over a non-dense user utterance re-
ceives a smaller negative reward than keeping it over
a dense user utterance. A reward of ?2 is assigned
for barging-in over a user utterance fragment with a
falling ID to reflect results from a qualitative study
of our corpus data: humans tend to barge-in between
information peaks, so that a barge-in to clarify a low-
confidence slot appears immediately before the ID is
rising again for a new slot. The exact best moment
for barge-ins and backchannels to occur will be sub-
ject to optimisation.
89
6 Experimental Results
The agent learns to barge-in or generate backchan-
nels to users at points where the ID is low but rising.
In particular, the agent learns to barge-in right before
information density peaks in an incoming user utter-
ance to clarify or request slots that are still open from
the previous information density peak. If a user has
specified their desired cuisine type but the system
has received a low ASR confidence score for it, it
may barge-in to clarify the slot. This case was illus-
trated in the last example in Figure 1, where the sys-
tem clarified the previous (cuisine) slot (which is as-
sociated with a high ID) just before the user specifies
the location slot (which again would have a high ID).
The main benefit the system can gain through clar-
ification barge-ins is to avoid self-corrections when
having acted based on a low ASR confidence, lead-
ing to more efficient interactions.
The system learns to generate backchannels after
information peaks to confirm newly acquired slots
that have a high confidence. An example is shown
in the first dialogue fragment in Figure 1.
In addition, the system learns to yield its current
turn to a user that is barging-in if its own ID is low,
falling or rising, or if the ID of the incoming user
utterance is high. If the system?s own ID is high, but
the user?s is not, it will try to keep the turn.6 This is
exemplified in the third dialogue fragment in Figure
1.
We compare our learnt policy against two base-
lines. Baseline 1 was designed to always generate
barge-ins after an information peak in a user utter-
ance, i.e. when ID has just switched from high to
falling. We chose this baseline to confirm that users
indeed prefer barge-ins before information peaks
rather than at any point of low ID. Baseline 1 yields
a turn to a user barge-in if its own ID is low and tries
to keep it otherwise. Baseline 2 generates barge-ins
and backchannels randomly and at any point during
a user utterance. The decision of yielding or keeping
a turn in case of a user barge-in is also random. Both
baselines also use HRL to optimise their IP strategy.
We do not compare different IP strategies, which has
been done in detail by Rieser et al2010). All re-
6Incidentally, this also helps to prevent the system yielding
its turn to a user backchannel; cf. Example 2 in Fig. 1.
101 102 103 104
?120
?100
?80
?60
?40
?20
0
20
40
60
Av
er
ag
e 
Re
wa
rd
Episodes
 
 
Learnt
Baseline1
Baseline2
Figure 5: Performance in terms of rewards (averaged over
10 runs) for the HRL agent and its baselines.
sults are summarised in Table 3.
6.1 Average Rewards over Time
Figure 5 shows the performance of all systems in
terms of average rewards in simulation. The learnt
policy outperforms both baselines. While the learnt
policy and Baseline 1 appear to achieve similar per-
formance, an absolute comparison of the last 1000
episodes of each behaviour shows that the improve-
ment of the HRL agent over Baseline 1 corresponds
to 23.42%. The difference between the learnt policy
and its baselines is significant at p < 0.0001 accord-
ing to a paired t-test and has a high effect size of
r = 0.85.
The main reason for these different performances
is the moment each system will barge-in. Since
Baseline 1 barges-in on users after an information
peak, when ID may still be high, it continuously re-
ceives a negative reward reflecting the user prefer-
ence for late barge-ins. As a result of this contin-
uous negative reward, the agent will then learn to
avoid barge-ins altogether, which may in turn lead
to less efficient interactions because low confidence
ASR scores are clarified only late in the interaction.
The main problem of the random barge-ins of
Baseline 2 is that users may often have to restart
a turn because the system barged-in too early or
in the middle of an information peak. In addition,
Baseline 2 needs to occasionally self-correct its own
utterances because it started to present information
too early, when input hypotheses were not yet stable
enough to act upon them.
90
Policy Average Reward User Rating (%)
Learnt 55.54??,? 43%??
Baseline 1 45.0?? 26%
Baseline 2 1.47 31%
Table 3: Comparison of policies in terms of average re-
wards and user ratings. ? indicates a significant improve-
ment over Baseline 1 and ?? over Baseline 2.
6.2 Human Rating Study
To confirm our simulation-based results, we con-
ducted a user rating study on the CrowdFlower
crowd sourcing platform.7 Participants were
shown user utterances along with three options of
barging-in over them. For example: | I want
[OPTION 1] Italian food [OPTION 2] in the
city [OPTION 3] centre|, where OPTION 1 cor-
responds to the learnt policy, OPTION 2 to Baseline
2 and OPTION 3 to Baseline 1.
Users were asked to choose one option which they
considered the best moment for a barge-in. Partici-
pants in the study rated altogether 144 utterances.
They preferred the learnt system 63 times (43%),
Baseline 1 37 times (26%) and Baseline 2 44 times
(31%). This is statistically significant at p < 0.02
according to a Chi-Square test (?2 = 7.542, df =
2). In a separate test, directly comparing the learnt
policy and Baseline 1, learnt was chosen signifi-
cantly more often than Baseline 1; i.e. 79% of the
time (for 127 utterances, using a 1-tailed Sign test,
p < 0.0001). Finally, learnt was directly compared
to Baseline 2 and shown to be significantly more of-
ten chosen; i.e. 59% of the time (138 utterances, 1-
tailed Sign test, p < 0.025). These results provide
evidence that an optimisation of the timing of gener-
ating barge-ins and backchannels in incremental di-
alogue can be sensitive to fine-grained cues in evolv-
ing ID and therefore achieve a high level of adaptiv-
ity. Such sensitivity is difficult to hand-craft as can
be concluded w.r.t. the performance of Baseline 1,
which received similar rewards to learnt in simula-
tion, but is surprisingly beaten by the random Base-
line 2 here. This indicates a strong human dislike
for late barge-ins. The bad performance of Base-
line 2 in terms of average rewards was due to the
random barge-ins leading to less efficient dialogues.
7www.crowdflower.com
Regarding user ratings however, Baseline 2 was pre-
ferred over Baseline 1. This is most likely due to the
timing of barge-ins: since Baseline 2 has a chance
of barging-in at earlier occasions than Baseline 1,
it may have received better ratings. The evaluation
shows that humans care about timing of a barge-in
regarding the density of information that is currently
conveyed and dislike late barge-ins. ID is then useful
in determining when to barge-in. We can therefore
further conclude that ID can be a feasible optimisa-
tion criterion for incremental decision making.
7 Conclusion and Future Work
We have presented a novel approach to incremen-
tal dialogue decision making based on Hierarchical
RL combined with the notion of information den-
sity. We presented a learning agent in the domain of
IP for restaurant recommendations that was able to
generate backchannels and barge-ins for higher re-
sponsiveness in interaction. Results in terms of av-
erage rewards and a human rating study have shown
that a learning agent that is optimised based on a
partially data-driven reward function that addresses
information density can learn to decide when and if
it is beneficial to barge-in or backchannel on user
utterances and to deal with backchannels and barge-
ins from the user. Future work can take several di-
rections. Given that ID is a measure influencing
human language production, we could replace our
template-based surface realiser by an agent that op-
timises the information density of its output. Cur-
rently we learn the agent?s behaviour offline, be-
fore the interaction, and then execute it statistically.
More adaptivity towards individual users and situa-
tions could be achieved if the agent was able to learn
from ongoing interactions. Finally, we can confirm
the human results obtained from an overhearer-style
evaluation in a real interactive setting and explicitly
extend our language model to discourse phenomena
such as pauses or hesitations to take them into ac-
count in measuring ID.
Acknowledgements
The research leading to this work has received funding
from EC?s FP7 programmes: (FP7/2011-14) under grant
agreement no. 287615 (PARLANCE); (FP7/2007-13) un-
der grant agreement no. 216594 (CLASSiC); (FP7/2011-
14) under grant agreement no. 270019 (SPACEBOOK);
91
and (FP7/2011-16) under grant agreement no. 269427
(STAC). Many thanks to Michael White for discussion
of the original idea of using information density as an op-
timisation metric.
References
Matthew Aylett and Alice Turk. 2004. The smooth signal
redundancy hypothesis: A functional explanation for
the relationships between redundancy, prosodic promi-
nence, and duration in spontaneous speech. Language
and Speech, 47(1):31?56.
Timo Baumann, Okko Buss, and David Schlangen. 2011.
Evaluation and Optimisation of Incremental Proces-
sors. Dialogue and Discourse, 2(1).
Alan Bell, Dan Jurafsky, Eric Fossler-Lussier, Cynthia
Girand, Michelle Gregory, and Daniel Gildea. 2003.
Effects of disfluencies, predictability, and utterance
position on word form variation in english conver-
sation. Journal of the Acoustic Society of America,
113(2):1001?1024.
Okko Buss, Timo Baumann, and David Schlangen. 2010.
Collaborating on Utterances with a Spoken Dialogue
Systen Using an ISU-based Approach to Incremental
Dialogue Management. In Proceedings of 11th An-
nual SIGdial Meeting on Discourse and Dialogue.
Heriberto Cuaya?huitl and Nina Dethlefs. 2011.
Spatially-aware Dialogue Control Using Hierarchi-
cal Reinforcement Learning. ACM Transactions on
Speech and Language Processing (Special Issue on
Machine Learning for Robust and Adaptive Spoken
Dialogue System), 7(3).
Heriberto Cuaya?huitl, Steve Renals, Oliver Lemon, and
Hiroshi Shimodaira. 2010. Evaluation of a hierar-
chical reinforcement learning spoken dialogue system.
Computer Speech and Language, 24(2):395?429.
Heriberto Cuaya?huitl. 2009. Hierarchical Reinforcement
Learning for Spoken Dialogue Systems. PhD Thesis,
University of Edinburgh, School of Informatics.
Nina Dethlefs and Heriberto Cuaya?huitl. 2011.
Combining Hierarchical Reinforcement Learning and
Bayesian Networks for Natural Language Generation
in Situated Dialogue. In Proceedings of the 13th Eu-
ropean Workshop on Natural Language Generation
(ENLG), Nancy, France.
Nina Dethlefs, Helen Hastie, Verena Rieser, and Oliver
Lemon. 2012. Optimising Incremental Genera-
tion for Spoken Dialogue Systems: Reducing the
Need for Fillers. In Proceedings of the International
Conference on Natural Language Generation (INLG),
Chicago, Illinois, USA.
David DeVault, Kenji Sagae, and David Traum. 2009.
Can I finish? Learning when to respond to incremental
interpretation result in interactive dialogue. In Pro-
ceedings of the 10th Annual SigDial Meeting on Dis-
course and Dialogue, Queen Mary University, UK.
Thomas G. Dietterich. 1999. Hierarchical Reinforce-
ment Learning with the MAXQ Value Function De-
composition. Journal of Artificial Intelligence Re-
search, 13:227?303.
Dmitriy Genzel and Eugene Charniak. 2002. Entropy
Rate Constancy in Text. In Proceedings of the 40th
Annual Meeting on Association for Computational
Linguistics, pages 199?206.
James Henderson, Oliver Lemon, and Kallirroi Georgila.
2008. Hybrid Reinforcement/Supervised Learning of
Dialogue Policies from Fixed Data Sets. Computa-
tional Linguistics, 34(4):487?511.
T. Florian Jaeger. 2010. Redundancy and reduction:
Speakers manage syntactic information density. Cog-
nitive Psychology, 61:23?62.
Srini Janarthanam and Oliver Lemon. 2010. Learning
to Adapt to Unknown Users: Referring Expression
Generation in Spoken Dialogue Systems. In Proceed-
ings of the 48th Annual Meeting of the Association for
Computational Linguistics (ACL), pages 69?78, July.
Anne Kilger and Wolfgang Finkler. 1995. Incremen-
tal generation for real-time applications. Technical re-
port, DFKI Saarbruecken, Germany.
Oliver Lemon. 2011. Learning What to Say and How to
Say It: Joint Optimization of Spoken Dialogue Man-
agement and Natural Language Generation.
Esther Levin, Roberto Pieraccini, and Wieland Eckert.
2000. A Stochastic Model of Computer-Human Inter-
action for Learning Dialogue Strategies. IEEE Trans-
actions on Speech and Audio Processing, 8:11?23.
Roger Levy and T. Florian Jaeger. 2007. Speakers opti-
mize information density through syntactic reduction.
Advances in Neural Information Processing Systems,
19.
Olivier Pietquin and Dutoit. 2006. A Probabilis-
tic Framework for Dialogue Simulation and Optimal
Strategy Learning. IEEE Transactions on Speech and
Audio Processing, 14(2):589?599.
Matthew Purver and Masayuki Otsuka. 2003. Incremen-
tal Generation by Incremental Parsing. In Proceedings
of the 6th UK Special-Interesting Group for Computa-
tional Linguistics (CLUK) Colloquium.
Rajakrishnan Rajkumar and Michael White. 2011. Lin-
guistically Motivated Complementizer Choice in Sur-
face Realization. In Proceedings of the EMNLP-11
Workshop on Using Corpora in NLG, Edinburgh, Scot-
land.
Antoine Raux and Maxine Eskenazi. 2009. A Finite-
State Turn-Taking Model for Spoken Dialog Sys-
tems. In Proceedings of the 10th Conference of the
92
North American Chapter of the Association for Com-
putational Linguistics?Human Language Technolo-
gies (NAACL-HLT), Boulder, Colorado.
Verena Rieser, Oliver Lemon, and Xingkun Liu. 2010.
Optimising Information Presentation for Spoken Di-
alogue Systems. In Proceedings of the 48th Annual
Meeting of the Association for Computational Linguis-
tics (ACL), Uppsala, Sweden.
Verena Rieser, Simon Keizer, Xingkun Liu, and Oliver
Lemon. 2011. Adaptive Information Presentation
for Spoken Dialogue Systems: Evaluation with Hu-
man Subjects. In Proceedings of the 13th European
Workshop on Natural Language Generation (ENLG),
Nancy, France.
David Schlangen and Gabriel Skantze. 2011. A General,
Abstract Model of Incremental Dialogue Processing.
Dialogue and Discourse, 2(1).
Ethan Selfridge, Iker Arizmendi, Peter Heeman, and Ja-
son Williams. 2011. Stability and Accuracy in In-
cremental Speech Recognition. In Proceedings of the
12th Annual SigDial Meeting on Discourse and Dia-
logue, Portland, Oregon.
Claude Shannon. 1948. A Mathematical Theory of
Communications. Bell Systems Technical Journal,
27(4):623?656.
Satinder Singh, Diane Litman, Michael Kearns, and Mar-
ilyn Walker. 2002. Optimizing Dialogue Management
with Reinforcement Learning: Experiments with the
NJFun System. Journal of Artificial Intelligence Re-
search, 16:105?133.
Gabriel Skantze and Anna Hjalmarsson. 2010. Towards
Incremental Speech Generation in Dialogue Systems.
In Proceedings of the 11th Annual SigDial Meeting on
Discourse and Dialogue, Tokyo, Japan.
Richard Sutton and Andrew Barto. 1998. Reinforcement
Learning: An Introduction. MIT Press, Cambridge,
MA.
Blaise Thomson. 2009. Statistical Methods for Spo-
ken Dialogue Management. Ph.D. thesis, University
of Cambridge.
Menno van Zaanen. 2000. Bootstrapping Syntax and
Recursion using Alignment-Based Learning. In Pro-
ceedings of the Seventeenth International Conference
on Machine Learning, ICML ?00, pages 1063?1070.
Marilyn Walker. 2000. An Application of Reinforcement
Learning to Dialogue Strategy Selection in a Spoken
Dialogue System for Email. Journal of Artificial In-
telligence Research (JAIR), 12:387?416.
Steve Young, Milica Gasic, Simon Keizer, Francois
Mairesse, Jost Schatzmann, Blaise Thomson, and Kai
Yu. 2010. The Hidden Information State Model: A
Practical Framework for POMDP-based Spoken Dia-
logue Management. Computer Speech and Language,
24(2):150?174.
Steve Young. 2000. Probabilistic Methods in Spoken
Dialogue Systems. Philosophical Transactions of the
Royal Society (Series A), 358(1769):1389?1402.
93
Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 702?711,
Gothenburg, Sweden, April 26-30 2014. c?2014 Association for Computational Linguistics
Cluster-based Prediction of User Ratings for Stylistic Surface Realisation
Nina Dethlefs, Heriberto Cuaya?huitl, Helen Hastie, Verena Rieser and Oliver Lemon
Heriot-Watt University, Mathematical and Computer Sciences, Edinburgh
n.s.dethlefs@hw.ac.uk
Abstract
Surface realisations typically depend on
their target style and audience. A challenge
in estimating a stylistic realiser from data is
that humans vary significantly in their sub-
jective perceptions of linguistic forms and
styles, leading to almost no correlation be-
tween ratings of the same utterance. We ad-
dress this problem in two steps. First, we
estimate a mapping function between the
linguistic features of a corpus of utterances
and their human style ratings. Users are
partitioned into clusters based on the sim-
ilarity of their ratings, so that ratings for
new utterances can be estimated, even for
new, unknown users. In a second step, the
estimated model is used to re-rank the out-
puts of a number of surface realisers to pro-
duce stylistically adaptive output. Results
confirm that the generated styles are recog-
nisable to human judges and that predictive
models based on clusters of users lead to
better rating predictions than models based
on an average population of users.
1 Introduction
Stylistic surface realisation aims not only to find
the best realisation candidate for a semantic input
based on some underlying trained model, but also
aims to adapt its output to properties of the user,
such as their age, social group, or location, among
others. One of the first systems to address stylis-
tic variation in generation was Hovy (1988)?s
PAULINE, which generated texts that reflect dif-
ferent speaker attitudes towards events based on
multiple, adjustable features. Stylistic variation
in such contexts can often be modelled systemat-
ically as a multidimensional variation space with
several continuous dimensions, so that varying
stylistic scores indicate the strength of each di-
mension in a realisation candidate. Here, we fo-
cus on the dimensions of colloquialism, politeness
and naturalness. Assuming a target score on one
or more dimensions, candidate outputs of a data-
driven realiser can then be ranked according to
their predicted affinity with the target scores.
In this paper, we aim for an approach to stylis-
tic surface realisation which is on the one hand
based on natural human data so as to reflect stylis-
tic variation that is as natural as possible. On the
other hand, we aim to minimise the amount of
annotation and human engineering that informs
the design of the system. To this end, we esti-
mate a mapping function between automatically
identifiable shallow linguistic features character-
istic of an utterance and its human-assigned style
ratings. In addition, we aim to address the high
degree of variability that is often encountered in
subjective rating studies, such as assessments of
recommender systems (O?Mahony et al., 2006;
Amatriain et al., 2009), sentiment analysis (Pang
and Lee, 2005), or surface realisations, where user
ratings have been shown to differ significantly
(p<0.001) for the same utterance (Walker et al.,
2007). Such high variability can affect the per-
formance of systems which are trained from an
average population of user ratings. However, we
are not aware of any work that has addressed this
problem principally by estimating ratings for both
known users, for whom ratings exists, and un-
known users, for whom no prior ratings exist. To
achieve this, we propose to partition users into
clusters of individuals who assign similar ratings
to linguistically similar utterances, so that their
ratings can be estimated more accurately than
702
based on an average population of users. This is
similar to Janarthanam and Lemon (2014), who
show that clustering users and adapting to their
level of domain expertise can significantly im-
prove task success and user ratings. Our resulting
model is evaluated with realisers not originally
built to deal with stylistic variation, and produces
natural variation recognisable by humans.
2 Architecture and Domain
We aim to with generating restaurant recommen-
dations as part of an interactive system. To do
this, we assume that a generator input is provided
by a preceding module, e.g. the interaction man-
ager, and that the task of the surface realiser is
to find a suitable stylistically appropriate realisa-
tion. An example input is inform(food=Italian,
name=Roma), which could be expressed as The
restaurant Roma serves Italian food. A further
aspect is that users are initially unknown to the
system, but that it should adapt to them over time
by discovering their stylistic preferences. Fu-
ture work involves integrating the surface realiser
into the PARLANCE1 (Hastie et al., 2013) spo-
ken dialogue system with a method for triggering
the different styles. Here, we leave the question
of when different styles are appropriate as future
work and focus on being able to generate them.
The architecture of our model is shown in Fig-
ure 1. Training of the regression model from sty-
listically-rated human corpora is shown in the top-
left box (grey). Utterance ratings from human
judges are used to extract shallow linguistic fea-
tures as well as to estimate user clusters. Both
types of information inform the resulting stylis-
tic regression model. For surface realisation (top-
right box, blue), a semantic input from a preced-
ing model is given as input to a surface realiser.
Any realiser is suitable that returns a ranked list of
output candidates. The resulting list is re-ranked
according to stylistic scores estimated by the re-
gressor, so that the utterance which most closely
reflects the target score is ranked highest. The re-
ranking process is shown in the lower box (red).
3 Related Work
3.1 Stylistic Variation in Surface Realisation
Our approach is most closely related to work by
Paiva and Evans (2005) and Mairesse and Walker
1http://parlance-project.eu
User Clusters
Regressor Surface Realisation
Ranking + 
Evaluation
Figure 1: Architecture of stylistic realisation model.
Top left: user clusters are estimated from corpus ut-
terances described by linguistic features and ratings.
Top right: surface realisation ranks a list of output can-
didates based on a semantic input. These are ranked
stylistically given a trained regressor.
(2011), discussed in turn here. Paiva and Evans
(2005) present an approach that uses multivari-
ate linear regression to map individual linguistic
features to distinguishable styles of text. The ap-
proach works in three steps. First, a factor anal-
ysis is used to determine the relevant stylistic di-
mensions from a corpus of human text using shal-
low linguistic features. Second, a hand-crafted
generator is used to produce a large set of ut-
terances, keeping traces of each generator deci-
sion, and obtaining style scores for each output
based on the estimated factor model. The result
is a dataset of <generator decision, style score>
pairs which can be used in a correlation analy-
sis to identify the predictors of particular output
styles. During generation, the correlation equa-
tions inform the generator at each choice point so
as to best express the desired style. Unfortunately,
no human evaluation of the model is presented so
that it remains unclear to what extent the gener-
ated styles are perceivable by humans.
Closely related is work by Mairesse and Walker
(2011) who present the PERSONAGE system,
which aims to generate language reflecting par-
ticular personalities. Instead of choosing genera-
tor decisions by considering their predicted style
scores, however, Mairesse and Walker (2011) di-
rectly predict generator decisions based on tar-
get personality scores. To obtain the generator,
the authors first generate a corpus of utterances
which differ randomly in their linguistic choices.
All utterances are rated by humans indicating the
703
extent to which they reflect different personality
traits. The best predictive model is then chosen in
a comparison of several classifiers and regressors.
Mairesse and Walker (2011) are the first to evalu-
ate their generator with humans and show that the
generated personalities are indeed recognisable.
Approaches on replicating personalities in re-
alisations include Gill and Oberlander (2002) and
Isard et al. (2006). Porayska-Pomsta and Mellish
(2004) and Gupta et al. (2007) are approaches to
politeness in generation, based on the notion of
face and politeness theory, respectively.
3.2 User Preferences in Surface Realisation
Taking users? individual content preferences into
account for training generation systems can
positively affect their performance (Jordan and
Walker, 2005; Dale and Viethen, 2009). We are
interested in individual user perceptions concern-
ing the surface realisation of system output and
the way they relate to different stylistic dimen-
sions. Walker et al. (2007) were the first to show
that individual preferences exist for the perceived
quality of realisations and that these can be mod-
elled in trainable generation. They train two ver-
sions of a rank-and-boost generator, a first version
of which is trained on the average population of
user ratings, whereas a second one is trained on
the ratings of individual users. The authors show
statistically that ratings from different users are
drawn from different distributions (p<0.001) and
that significantly better performance is achieved
when training and testing on data of individual
users. In fact, training a model on one user?s rat-
ings and testing it on another?s performs as badly
as a random baseline. However, no previous work
has modelled the individual preferences of unseen
users?for whom no training data exists.
4 Estimation of Style Prediction Models
4.1 Corpora and Style Dimensions
Our domain of interest is the automatic generation
of restaurant recommendations that differ with re-
spect to their colloquialism and politeness and are
as natural as possible. All three stylistic dimen-
sion were identified from a qualitative analysis of
human domain data. To estimate the strength of
each of them in a single utterance, we collect user
ratings for three data sets that were collected un-
der different conditions and are freely available.
Corpus Colloquial Natural Polite
LIST 3.38 ? 1.5 4.06 ? 1.2 4.35 ? 0.8
MAI 3.95 ? 1.2 4.32 ? 1.0 4.27 ? 0.8
CLASSIC 4.29 ? 1.1 4.20 ? 1.2 3.64 ? 1.3
Table 1: Average ratings with standard deviations.
Ratings between datasets (except one) differ signifi-
cantly at p<0.01, using the Wilcoxon signed-rank test.
? LIST is a corpus of restaurant recommenda-
tions from the website The List.2 It consists
of professionally written reviews. An exam-
ple is ?Located in the heart of Barnwell, Bel-
uga is an excellent restaurant with a smart
menu of modern Italian cuisine.?
? MAI is a dataset collected by Mairesse et
al. (2010),3 using Amazon Mechanical Turk.
Turkers typed in recommendations for vari-
ous specified semantics; e.g. ?I recommend
the restaurant Beluga near the cathedral.?
? CLASSIC is a dataset of transcribed spoken
user utterances from the CLASSiC project.4
The utterances consist of user queries for
restaurants, such as ?I need an Italian
restaurant with a moderate price range.?
Our joint dataset consists of 1, 361 human ut-
terances, 450 from the LIST, 334 from MAI,
and 577 from CLASSIC. We asked users on the
CrowdFlower crowdsourcing platform5 to read
utterances and rate their colloquialism, politeness
and naturalness on a 1-5 scale (the higher the bet-
ter). The following questions were asked.
? Colloquialism: The utterance is colloquial,
i.e. could have been spoken.
? Politeness: The utterance is polite / friendly.
? Naturalness: The utterance is natural, i.e.
could have been produced by a human.
The question on naturalness can be seen as a gen-
eral quality check for our training set. We do
not aim to generate unnatural utterances. 167
users took part in our rating study leading to a
rated dataset of altogether 3, 849 utterances. All
users were from the USA. The average ratings per
dataset and stylistic dimension are summarised
in Table 1. From this, we can see that LIST ut-
terances were perceived as the least natural and
2http://www.list.co.uk/
3http://people.csail.mit.edu/francois/
research/bagel/
4http://www.classic-project.org/
5http://crowdflower.com/
704
colloquial, but as the most polite. CLASSIC ut-
terances were perceived as the most colloquial,
but the least polite, and MAI utterances were rated
as the most natural. Differences between ratings
for each dimension and dataset are significant at
p<0.01, using the Wilcoxon signed-rank test, ex-
cept the naturalness for MAI and CLASSIC.
Since we are mainly interested in the lexical
and syntactic features of utterances here, the fact
that CLASSIC utterances are spoken, whereas the
other two corpora are written, should not affect
the quality of the resulting model. Similarly, some
stylistic categories may seem closely related, such
as colloquialism and naturalness, or orthogonal
to each other, such as politeness and colloqui-
alism. However, while ratings for colloquialism
and naturalness are very close for the CLASSIC
dataset, they vary significantly for the two other
datasets (p<0.01). Also, the ratings for colloqui-
alim and politeness show a weak positive corre-
lation of 0.23, i.e. are not perceived as orthogo-
nal by users. These results suggest that all in all
our three stylistic categories are perceived as suf-
ficiently different from each other and suitable for
training to predict a spectrum of different styles.
Another interesting aspect is that individual
user ratings vary significantly, leading to a high
degree of variability for identical utterances. This
will be the focus of the following sections.
4.2 Feature Estimation
Table 2 shows the feature set we will use in our
regression experiments. We started from a larger
subset including 45 lexical and syntactic features
as well as unigrams and bigrams, all of which
could be identified from the corpus without man-
ual annotation. The only analysis tool we used
was the Stanford Parser,6 which identified certain
types of words (pronouns, wh-words) or the depth
of syntactic embedding. A step-wise regression
analysis was then carried out to identify those
features that contributed significantly (at p<0.01)
to the overall regression equation obtained per
stylistic dimension. Of all lexical features (uni-
grams and bigrams), the word with was the only
contributor. A related feature was the average tf-
idf score of the content words in an utterance.
6http://nlp.stanford.edu/software/
lex-parser.shtml
Feature Type
Length of utterance num
Presence of personal pronouns bool
Presence of WH words bool
with cue word bool
Presence of negation bool
Average length of content words num
Ave tf-idf score of content words num
Depth of syntactic embedding num
Table 2: Features used for regression, which were
identified as significant contributors (p<0.01) from a
larger feature set in a step-wise regression analysis.
4.3 Regression Experiments
Based on the features identified in Section 4.2, we
train a separate regressor for each stylistic dimen-
sion. The task of the regressor is to predict, based
on the extracted linguistic features of an utterance,
a score in the range of 1-5 for colloquialism, po-
liteness and naturalness. We compare: (1) a mul-
tivariate multiple regressor (MMR), (2) an M5P
decision tree regressor, (3) a support vector ma-
chine (SVM) with linear kernel, and (4) a ZeroR
classifier, which serves as a majority baseline. We
used the R statistics toolkit7 for the MMR and the
Weka toolkit8 for the remaining models.
Average User Ratings The regressors were first
trained to predict the average user ratings of an ut-
terance and evaluated in a 10-fold cross validation
experiment. Table 3 shows the results. Here, r
denotes the Pearson correlation coefficient, which
indicates the correlation between the predicted
and the actual user scores; R2 is the coefficient of
determination, which provides a measure of how
well the learnt model fits the data; and RMSE
refers to the Root Mean Squared Error, the error
between the predicted and actual user ratings.
We can observe that MMR achieves the best
performance for predicting colloquialism and nat-
uralness, whereas M5P best predicts politeness.
Unfortunately, all regressors achieve at best a
moderate correlation with human ratings. Based
on these results, we ran a correlation analysis for
all utterances for which more than 20 original
user ratings were available. The purpose was to
find out to what extent human raters agree with
each other. The results showed that user agree-
ment in fact ranges from a high positive corre-
7http://www.r-project.org/
8http://www.cs.waikato.ac.nz/ml/weka/
705
Model r R2 RMSE
Colloquial
MMR 0.50 0.25 0.85
SVM 0.47 0.22 0.86
M5P 0.48 0.23 0.85
ZeroR -0.08 0.006 0.97
Natural
MMR 0.30 0.09 0.78
SVM 0.24 0.06 0.81
M5P 0.27 0.07 0.78
ZeroR -0.09 0.008 0.81
Polite
MMR 0.33 0.11 0.71
SVM 0.31 0.09 0.73
M5P 0.42 0.18 0.69
ZeroR -0.09 0.008 0.76
Table 3: Comparison of regression models per dimen-
sion using average user ratings. The best model is
indicated in bold-face for the correlation coefficient.
Model r R2 RMSE
Colloquial
MMR 0.61 0.37 1.05
SVM 0.36 0.13 1.3
M5P 0.56 0.31 1.07
ZeroR -0.06 0.004 1.3
Natural
MMR 0.55 0.30 0.96
SVM 0.36 0.13 1.13
M5P 0.49 0.24 0.99
ZeroR -0.08 0.06 1.13
Polite
MMR 0.69 0.48 0.76
SVM 0.54 0.30 0.92
M5P 0.71 0.50 0.73
ZeroR -0.04 0.002 1.04
Table 4: Comparison of regression models per dimen-
sion using individual user ratings. The best model is
indicated in bold-face for the correlation coefficient.
lation of 0.79 to a moderate negative correlation
of ?0.55. The average is 0.04 (SD=0.95), i.e.
indicating no correlation between user ratings,
even for the same utterance. This observation is
partially in line with related work that has found
high diversity in subjective user ratings. Yeh and
Mellish (1997) report only 70% agreement of hu-
man judges on the best choice of referring ex-
pression. Amatriain et al. (2009) report incon-
sistencies in user ratings in recommender systems
with an RMSE range of 0.55 to 0.81 and argue
that this constitutes a lower bound for system per-
formance. This inconsistency is exacerbated by
raters recruited via crowdsourcing platforms as
in our study (Koller et al., 2010; Rieser et al.,
2011). However, while crowdsourced data have
been shown to contain substantially more noise
than data collected in a lab environment, they do
tend to reflect the general tendency of their more
controlled counterparts (Gosling et al., 2004).
Individual User Ratings Given that individual
preferences exist for surface realisation (Walker
et al., 2007), we included the user?s ID as a re-
gression feature and re-ran the experiments. The
hypothesis was that if users differ in their pref-
erences for realisation candidates, they may also
differ in terms of their perceptions of linguistic
styles. The results shown in Table 4 support this:
the obtained correlations are significantly higher
(p<0.001, using the Fisher r-to-z transformation)
than those without the user?s ID (though we are
still not able to model the full variation observed
in ratings). Importantly, this shows that user rat-
ings are intrinsically coherent (not random) and
that variation exists mainly for inter-user agree-
ment. This model performs satisfactorily for a
known population of users. However, it does not
allow the prediction of ratings of unknown users,
who we mostly encounter in generation.
5 Clustering User Rating Behaviour
5.1 Spectral Clustering
The goal of this section is to find a number of k
clusters which partition our data set of user rat-
ings in a way that users in one cluster rate ut-
terances with particular linguistic properties most
similarly to each other, while rating them most
dissimilarly to users in other clusters. We as-
sume a set of n data points x
1
. . . x
n
, which
in our case correspond to an individual user or
group of users, characterised in terms of word
bigrams, POS tag bigrams, and assigned rat-
ings of the utterance they rated. An example
is Beluga NNP serves VBZ Italian JJ food NN;
[col=5.0, nat=5.0, pol=4.0]. Features were cho-
sen as a subset of relevant features from the larger
set used for regression above.
Using spectral clustering (von Luxburg, 2007),
clusters can be identified from a set of eigenvec-
tors of an affinity matrix S derived from pair-wise
similarities between data points s
ij
= s(x
i
, x
j
)
using a symmetric and non-negative similarity
function. To do that, we use a cumulative simi-
larity based on the Kullback-Leibler divergence,
D(P,Q) =
?
i
p
i
log
2
(
p
i
q
i
) +
?
j
q
j
log
2
(
q
j
p
j
)
2
,
where P is a distribution of words, POS tags or
ratings in data point x
i
; and Q a similar distribu-
tion in data point x
j
. The lower the cumulative di-
706
0.
3
0.
4
0.
5
0.
6
Number of Clusters
Co
rre
la
tio
n 
Co
ef
fic
ie
nt
1 3 5 7 9 20 40 60 80 100 167
Individual
Clusters
Average
Figure 2: Average correlation coefficient for different
numbers of clusters. For comparison, results from av-
erage and individual user ratings are also shown.
vergence between two data sets, the more similar
they are. To find clusters of similar users from the
affinity matrix S, we use the algorithm described
in Ng et al. (2001). It derives clusters by choosing
the k largest eigenvectors u
1
, u
2
, . . . , u
k
from the
Laplacian matrix L = D1/2?SD1/2 (where D is
a diagonal matrix), arranging them into columns
in a matrix U = [u
1
u
2
. . . u
k
] and then normalis-
ing them for length. The result is a new matrix T ,
obtained through t
ij
= u
ij
/(
?
k
u
2
ik
)
1/2
. The set
of clusters C
1
, . . . C
k
can then be obtained from T
using the K-means algorithm, where each row in
T serves as an individual data point. Finally, each
original data point x
i
(row i of T ) is assigned to a
cluster C
j
. In comparison to other clustering algo-
rithms, experiments by Ng et al. (2001) show that
spectral clustering is robust for convex and non-
convex data sets. The authors also demonstrate
why using K-means only is often not sufficient.
The main clusters obtained describe surface
realisation preferences by particular groups of
users. An example is the realisation of the loca-
tion of a restaurant as a prepositional phrase or as
a relative clause as in restaurant in the city centre
vs. restaurant located in the city centre; or the re-
alisation of the food type as an adjective, an Ital-
ian restaurant, vs. a clause, this restaurant serves
Italian food. Clusters can then be characterised as
different combinations of such preferences.
5.2 Results: Predicting Stylistic Ratings
Figure 2 shows the average correlation coefficient
r across dimensions in relation to the number
of clusters, in comparison to the results obtained
with average and individual user ratings. We can
see that the baseline without user information is
outperformed with as few as three clusters. From
30 clusters on, a medium correlation is obtained
until another performance jump occurs around 90
clusters. Evidently, the best performance would
be achieved by obtaining one cluster per user, i.e.
167 clusters, but nothing would be gained in this
way, and we can see that useful generalisations
can be made from much fewer clusters. Based on
the clusters found, we will now predict the ratings
of known and unknown users.
Known Users For known users, first of all, Fig-
ure 3 shows the correlations between the predicted
and actual ratings for colloquialism, politeness
and naturalness based on 90 user clusters. Cor-
relation coefficients were obtained using an MMR
regressor. We can see that a medium correlation is
achieved for naturalness and (nearly) strong cor-
relations are achieved for politeness and colloqui-
alism. This confirms that clustering users can help
to better predict their ratings than based on shal-
low linguistic features alone, but that more gener-
alisation is achieved than based on individual user
ratings that include the user?s ID as a regression
feature. The performance gain in comparison to
predicting average ratings is significant (p<0.01)
from as few as three clusters onwards.
Unknown Users We initially sort unknown
users into the majority cluster and then aim to
make more accurate cluster allocations as more
information becomes available. For example, af-
ter a user has assigned their first rating, we can
take it into account to re-estimate their cluster
more accurately. Clusters are re-estimated with
each new rating, based on our trained regression
model. While estimating a user cluster based on
linguistic features alone yields an average corre-
lation of 0.38, an estimation based on linguistic
features and a single rating alone already yields an
average correlation of 0.45. From around 30 rat-
ings, the average correlation coefficients achieved
are as good as for known users. More importantly,
though, estimations based on a single rating alone
significantly outperform ratings based on the av-
707
(a)
1 2 3 4 5
2
3
4
5
Correlation: Colloquialism
Actual Ratings
Pr
ed
ict
ed
 R
at
in
gs
(b)
1 2 3 4 5
1
2
3
4
5
Correlation: Naturalness
Actual Ratings
Pr
ed
ict
ed
 R
at
in
gs
(c)
1 2 3 4 5
1
2
3
4
5
Correlation: Politeness
Actual Ratings
Pr
ed
ict
ed
 R
at
in
gs
Figure 3: Correlations per dimension between actual and predicted user ratings based on 90 user clusters: (a)
Colloquialism (r = 0.57, p<0.001), (b) Naturalness (r = 0.49, p<0.001) and (c) Politeness (r = 0.59, p<0.001).
erage population of users (p<0.001). Fig. 4 shows
this process. It shows the correlation between pre-
dicted and actual user ratings for unknown users
over time. This is useful in interactive scenarios,
where system behaviour is refined as more infor-
mation becomes available (Cuaya?huitl and Deth-
lefs, 2011; Gas?ic? et al., 2011), or for incremental
systems (Skantze and Hjalmarsson, 2010; Deth-
lefs et al., 2012b; Dethlefs et al., 2012a).
0.
3
0.
4
0.
5
0.
6
Number of Ratings
Co
rre
la
tio
n 
Co
ef
fic
ie
nt
1 2 3 4 5 6 7 8 9 10 15 20 30
90 Clusters
Ratings
Average
Figure 4: Average correlation coefficient for unknown
users with an increasing number of ratings. Results
from 90 clusters and average ratings are also shown.
6 Evaluation: Stylistically-Aware
Surface Realisation
To evaluate the applicability of our regression
model for stylistically-adaptive surface realisa-
tion, this section describes work that compares
four different surface realisers, which were not
originally developed to produce stylistic variation.
To do that, we first obtain the cluster for each in-
put sentence s: c? = argmin
c?C
?
x
D(P
x
s
|Q
x
c
),
where x refers to n-grams, POS tags or ratings
(see Section 5.1); P refers to a discrete probability
distribution of sentence s; and Q refers to a dis-
crete probability distribution of cluster c. The best
cluster is used to compute the style score of sen-
tence s using: score(s) =
?
n
i
?
i
f
i
(s), c
?
? F ,
where ?
i
are the weights estimated by the regres-
sor, and f
i
are the features of sentence s; see Table
2. The idea is that if well-phrased utterances can
be generated, whose stylistic variation is recog-
nisable to human judges, then our regressor can
be used in combination with any statistical sur-
face realiser. Note however that the stylistic vari-
ation observed depends on the stylistic spectrum
that each realiser covers. Here, our goal is mainly
to show that whatever stylistic variation exists in
a realiser can be recognised by our model.
6.1 Overview of Surface Realisers
In a human rating study, we compare four surface
realisers (ordered alphabetically), all of which
are able to return a ranked list of candidate re-
alisations for a semantic input. Please refer to
the references given for details of each system.
The BAGEL and SPaRKy realisers were compared
based on published ranked output lists.9
? BAGEL is a surface realiser based on dy-
namic Bayes Nets originally trained using
Active Learning by Mairesse et al. (2010).
It was shown to generate well-phrased utter-
ances from unseen semantic inputs.
? CRF (global) treats surface realisation as a
9Available from http://people.csail.mit.
edu/francois/research/bagel and http://
users.soe.ucsc.edu/
?
maw/downloads.html.
708
System Utterance
BAGEL Beluga is a moderately priced
restaurant in the city centre area.
Col = 4.0, Pol = 4.0, Nat = 4.0
CRF (global) Set in the city centre, Beluga is a
moderately priced location for the
celebration of the Italian spirit.
Col = 2.0, Pol = 5.0, Nat = 2.0
pCRU Beluga is located in the city centre
and serves cheap Italian food.
Col = 4.0, Pol = 3.0, Nat = 5.0
SPaRKy Beluga has the best overall quality
among the selected restaurants
since this Italian restaurant has
good decor, with good service.
Col = 3.0, Pol = 4.0, Nat = 5.0
Table 5: Example utterances for the BAGEL, CRF
(global), pCRU and SPaRKy realisers shown to users.
Sample ratings from individual users are also shown.
sequence labelling task: given a set of (ob-
served) linguistic features, it aims to find the
best (hidden) sequence of phrases realising a
semantic input (Dethlefs et al., 2013).
? pCRU is based on probabilistic context-
free grammars and generation is done using
Viterbi search, sampling (used here), or ran-
dom search. It is based on Belz (2008).
? SPaRKy is based on a rank-and-boost ap-
proach. It learns a mapping between the lin-
guistic features of a target utterance and its
predicted user ratings and ranks candidates
accordingly (Walker et al., 2007).
6.2 Results: Recognising Stylistic Variation
242 users from the USA took part in a rating study
on the CrowdFlower platform and rated altogether
1, 702 utterances, from among the highest-ranked
surface realisations above. For each utterance
they read, they rated the colloquialism, natura-
less and politeness based on the same questions
as in Section 4.1, used to obtain the training data.
Based on this, we compare the perceived strength
of each stylistic dimension in an utterance to the
one predicted by the regressor. Example utter-
ances and ratings are shown in Table 5. Results
are shown in Table 6 and confirm our observa-
tions: ratings for known users can be estimated
with a medium (or high) correlation based on
clusters of users who assign similar ratings to ut-
terances with similar linguistic features. We can
also see that such estimations do not depend on a
particular data set or realiser.
System Colloquial Polite Natural
BAGEL 0.78 0.66 0.69
CRF global 0.58 0.63 0.63
pCRU 0.67 0.42 0.77
SPaRKy 0.87 0.56 0.81
Table 6: Correlation coefficients between subjective
user ratings and ratings predicted by the regressor for
known users across data-driven surface realisers.
A novel aspect of our technique in compari-
son to previous work on stylistic realisation is
that it does not depend on the time- and resource-
intensive design of a hand-coded generator, as in
Paiva and Evans (2005) and Mairesse and Walker
(2011). Instead, it can be applied in conjunc-
tion with any system designer?s favourite realiser
and preserves the realiser?s original features by
re-ranking only its top n (e.g. 10) output candi-
dates. Our method is therefore able to strike a
balance between highly-ranked and well-phrased
utterances and stylistic adaptation. A current lim-
itation of our model is that some ratings can still
not be predicted with a high correlation with hu-
man judgements. However, even the medium cor-
relations achieved have been shown to be signif-
icantly better than estimations based on the aver-
age population of users (Section 5.2).
7 Conclusion and Future Work
We have presented a model of stylistic realisation
that is able to adapt its output along several stylis-
tic dimensions. Results show that the variation is
recognisable by humans and that user ratings can
be predicted for known as well as unknown users.
A model which clusters individual users based
on their ratings of linguistically similar utterances
achieves significantly higher performance than a
model trained on the average population of rat-
ings. These results may also play a role in other
domains in which users display variability in their
subjective ratings, e.g. recommender systems,
sentiment analysis, or emotion generation. Future
work may explore the use of additional cluster-
ing features as a more scalable alternative to re-
ranking. It also needs to determine how user feed-
back can be obtained during an interaction, where
asking users for ratings may be disruptive. Possi-
bilities include to infer user ratings from their next
dialogue move, or from multimodal information
such as hesitations or eye-tracking.
709
Acknowledgements This research was funded
by the EC FP7 programme FP7/2011-14 under
grant agreements no. 270019 (SPACEBOOK)
and no. 287615 (PARLANCE).
References
Xavier Amatriain, Josep M. Pujol, and Nuria Oliver.
2009. I like It... I Like It Not: Evaluating User Rat-
ings Noise in Recommender Systems. In In the 17th
International Conference on User Modelling, Adap-
tation, and Personalisation (UMAP), pages 247?
258, Trento, Italy. Springer-Verlag.
Anja Belz. 2008. Automatic Generation of Weather
Forecast Texts Using Comprehensive Probabilistic
Generation-Space Models. Natural Language En-
gineering, 14(4):431?455.
Penelope Brown and Stephen Levinson. 1987. Some
Universals in Language Usage. Cambridge Univer-
sity Press, Cambridge, UK.
Heriberto Cuaya?huitl and Nina Dethlefs. 2011. Op-
timizing Situated Dialogue Management in Un-
known Environments. In INTERSPEECH, pages
1009?1012.
Robert Dale and Jette Viethen. 2009. Referring
Expression Generation Through Attribute-Based
Heuristics. In Proceedings of the 12th Euro-
pean Workshop on Natural Language Generation
(ENLG), Athens, Greece.
Nina Dethlefs, Helen Hastie, Verena Rieser, and Oliver
Lemon. 2012a. Optimising Incremental Dialogue
Decisions Using Information Density for Interac-
tive Systems. In Proceedings of the Conference on
Empirical Methods in Natural Language Process-
ing (EMNLP-CoNLL), Jeju, South Korea.
Nina Dethlefs, Helen Hastie, Verena Rieser, and Oliver
Lemon. 2012b. Optimising Incremental Genera-
tion for Spoken Dialogue Systems: Reducing the
Need for Fillers. In Proceedings of the Interna-
tional Conference on Natural Language Generation
(INLG), Chicago, Illinois, USA.
Nina Dethlefs, Helen Hastie, Heriberto Cuaya?huitl,
and Oliver Lemon. 2013. Conditional Random
Fields for Responsive Surface Realisation Using
Global Features. In Proceedings of the 51st Annual
Meeting of the Association for Computational Lin-
guistics (ACL), Sofia, Bulgaria.
Michael Fleischman and Eduard Hovy. 2002. Emo-
tional Variation in Speech-Based Natural Language
Generation. In Proceedings of the 2nd International
Natural Language Generation Conference.
Milica Gas?ic?, Filip Jurc???c?ek, Blaise Thomson, Kai Yu,
and Steve Young. 2011. On-Line Policy Optimi-
sation of Spoken Dialogue Systems via Interaction
with Human Subjects. In Proceedings of the IEEE
Automatic Speech Recognition and Understanding
(ASRU) Workshop.
Alastair Gill and Jon Oberlander. 2002. Taking Care
of the Linguistic Features of Extraversion. In Pro-
ceedings of the 24th Annual Conference of the Cog-
nitive Science Society, pages 363?368, Fairfax, VA.
Samuel Gosling, Simine Vazire, Sanjay Srivastava,
and Oliver John. 2004. Should We Trust Web-
Based Studies? A Comparative Analysis of Six Pre-
conceptions About Internet Questionnaires. Ameri-
can Psychologist, 59(2):93?104.
Swati Gupta, Marilyn Walker, and Daniela Romano.
2007. How Rude Are You? Evaluating Politeness
and Affect in Interaction. In Proceedings of the
2nd International Conference on Affective Comput-
ing and Intelligent Interaction.
Helen Hastie, Marie-Aude Aufaure, Panos Alex-
opoulos, Heriberto Cuayhuitl, Nina Dethlefs,
James Henderson Milica Gasic, Oliver Lemon,
Xingkun Liu, Peter Mika, Nesrine Ben Mustapha,
Verena Rieser, Blaise Thomson, Pirros Tsiakoulis,
Yves Vanrompay, Boris Villazon-Terrazas, and
Steve Young. 2013. Demonstration of the PAR-
LANCE System: A Data-Driven, Incremental, Spo-
ken Dialogue System for Interactive Search. In Pro-
ceedings of the 14th Annual Meeting of the Special
Interest Group on Discourse and Dialogue (SIG-
dial).
Eduard Hovy. 1988. Generating Natural Language
under Pragmatic Constraints. Lawrence Erlbaum
Associates, Hillsdale, NJ.
Amy Isard, Carsten Brockmann, and Jon Oberlander.
2006. Individuality and Alignment in Generated
Dialogues. In Proceedings of the 4th International
Natural Language Generation Conference (INLG),
Sydney, Australia.
Srini Janarthanam and Oliver Lemon. 2014. Adaptive
generation in dialogue systems using dynamic user
modeling. Computational Linguistics. (in press).
Pamela Jordan and Marilyn Walker. 2005. Learning
Content Selection Rules for Generating Object De-
scriptions in Dialogue. Journal of Artificial Intelli-
gence Research, 24:157?194.
Alexander Koller, Kristina Striegnitz, Donna Byron,
Justine Cassell, Robert Dale, and Johanna Moore.
2010. The First Challenge on Generating Instruc-
tions in Virtual Environments. In M. Theune and
E. Krahmer, editors, Empirical Methods in Natu-
ral Language Generation, pages 337?361. Springer
Verlag, Berlin/Heidelberg.
Franc?ois Mairesse and Marilyn Walker. 2011. Con-
trolling User Perceptions of Linguistic Style: Train-
able Generation of Personality Traits. Computa-
tional Linguistics, 37(3):455?488, September.
Franc?ois Mairesse, Milica Gas?ic?, Filip Jurc???c?ek, Si-
mon Keizer, Blaise Thomson, Kai Yu, and Steve
Young. 2010. Phrase-based statistical language
generation using graphical models and active learn-
ing. In Proceedings of the Annual Meeting of the
710
Association for Computational Linguistics (ACL),
pages 1552?1561.
Andrew Ng, Michael Jordan, and Yair Weiss. 2001.
On Spectral Clustering: Analysis and an Algorithm.
In Advances in Neural Information Processing Sys-
tems, pages 849?856. MIT Press.
Michael O?Mahony, Neil Hurley, and Gue?nole? Sil-
vestre. 2006. Detecting Noise in Recommender
System Databases. In Proceedings of the Inter-
national Conference on Intelligent User Interfaces
(IUI)s. ACM Press.
Daniel Paiva and Roger Evans. 2005. Empirically-
Based Control of Natural Language Generation. In
Proceedings of the 43rd Annual Meeting of the
Association for Computational Linguistics (ACL),
Ann Arbor, Michigan, USA.
Bo Pang and Lillian Lee. 2005. Seeing Stars: Exploit-
ing Class Relationships for Sentiment Categoriza-
tion with Respect to Rating Scales. In Proceedings
of the 43rd Annual Meeting of the Association for
Computational Linguistics (ACL).
Kaska Porayska-Pomsta and Chris Mellish. 2004.
Modelling Politness in Natural Language Gener-
ation. In Proceedings of the 3rd International
Natural Language Generation Conference (INLG),
Brighton, UK.
Verena Rieser, Simon Keizer, Xingkun Liu, and Oliver
Lemon. 2011. Adaptive Information Presentation
for Spoken Dialogue Systems: Evaluation with Hu-
man Subjects. In Proceedings of the 13th Euro-
pean Workshop on Natural Language Generation
(ENLG), Nancy, France.
Gabriel Skantze and Anna Hjalmarsson. 2010. To-
wards Incremental Speech Generation in Dialogue
Systems. In Proceedings of the 11th Annual Sig-
Dial Meeting on Discourse and Dialogue, Tokyo,
Japan.
Ulrike von Luxburg. 2007. A Tutorial on Spectral
Clustering. Statistics and Computing, 17(4).
Marilyn Walker, Amanda Stent, Franc?ois Mairesse,
and Rashmi Prasad. 2007. Individual and Do-
main Adaptation in Sentence Planning for Dia-
logue. Journal of Artificial Intelligence Research,
30(1):413?456.
Ching-long Yeh and Chris Mellish. 1997. An Empir-
ical Study on the Generation of Anaphora in Chi-
nese. Computational Linguistics, 23:169?190.
711
Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 210?214,
Gothenburg, Sweden, April 26-30 2014.
c?2014 Association for Computational Linguistics
Finding middle ground? Multi-objective Natural Language Generation
from time-series data
Dimitra Gkatzia, Helen Hastie, and Oliver Lemon
School of Mathematical and Computer Sciences, Heriot-Watt University, Edinburgh
{dg106, h.hastie, o.lemon}@hw.ac.uk
Abstract
A Natural Language Generation (NLG)
system is able to generate text from non-
linguistic data, ideally personalising the
content to a user?s specific needs. In some
cases, however, there are multiple stake-
holders with their own individual goals,
needs and preferences. In this paper, we
explore the feasibility of combining the
preferences of two different user groups,
lecturers and students, when generating
summaries in the context of student feed-
back generation. The preferences of each
user group are modelled as a multivariate
optimisation function, therefore the task
of generation is seen as a multi-objective
(MO) optimisation task, where the two
functions are combined into one. This ini-
tial study shows that treating the prefer-
ences of each user group equally smooths
the weights of the MO function, in a way
that preferred content of the user groups is
not presented in the generated summary.
1 Introduction
Summarisation of time-series data refers to the
task of automatically generating summaries from
attributes whose values change over time. Content
selection is the task of choosing what to say, i.e.
what information to be included in a report (Re-
iter and Dale, 2000). Here, we consider the task
of automatically generating feedback summaries
for students describing their performance during
the lab of a computer science module over the
semester. This work is motivated by the fact that
different user groups have different preferences of
the content that should be conveyed in a summary,
as shown by Gkatzia et al. (2013).
Various factors can influence students? learning,
such as difficulty of the material (Person et al.,
1995), workload (Craig et al., 2004), attendance
in lectures (Ames, 1992) etc. These factors change
over time and can be interdependent. The different
stakeholders (i.e. lecturers and students) have dif-
ferent perceptions regarding what constitutes good
feedback. Therefore, when generating feedback,
we should take into account all preferences in or-
der to be able to produce feedback summaries that
are acceptable by both user groups.
Stakeholders often have conflicting goals, needs
and preferences, for example managers with em-
ployees or doctors with patients and relatives. In
our data, for instance, lecturers tend to comment
on the hours that a student studied, whereas the
students disprefer this content. Generating the
same summary for both groups allows for mean-
ingful further discussion with common ground.
Previous work on NLG systems that address
more than one user group use different versions of
a system for each different user group (Gatt et al.,
2009) or make use of User Models (Janarthanam
and Lemon, 2010; Thompson et al., 2004; Zuk-
erman and Litman, 2001). Here, we explore a
method that adapts to both expert preferences and
users simultaneously (i.e. lecturer and students
preferences), by applying Multi-Objective opti-
misation (MOO). MOO can be applied to situa-
tions where optimal decisions are sought in the
presence of trade-offs between conflicting objec-
tives (Chankong and Haimes, 1983). We explore
whether balancing the preferences of two user
groups can result in an adaptive system that is ac-
ceptable by all users. At the same time, the pro-
gramming effort is reduced as only one system
needs to be developed. Moreover, by pooling all
available data together, there is less need for an
extensive data collection.
In the next section, we present three systems:
one tuned for lecturers, one for students, and one
that attempts to find middle ground. In Section 3,
we describe an evaluation of these three systems
and in Section 4 we discuss the results. Finally, in
210
Section 5, directions for future work are discussed.
2 Methodology
Reinforcement Learning (RL) is a machine learn-
ing technique that defines how an agent learns
to take optimal sequences of actions so as to
maximize a cumulative reward (Sutton and Barto,
1998). Here we extend the framework proposed
by Gkatzia et al. (2013) whereby the content selec-
tion is seen as a Markov Decision problem and the
goal of the agent is to learn to take the sequence
of actions that leads to optimal content selection.
A Temporal Difference learning method (Sutton
and Barto, 1998) was used to train an agent for
content selection. Firstly, we will describe the
data in general. Secondly, we refer to the RL
system that adapts to lecturers? preferences as de-
scribed by Gkatzia et al. (2013). Thirdly, we will
describe how we collected data and developed a
methodology that adapts to students? preferences
and finally how we combined the knowledge of
both steps to develop an MO system. The three
systems (Lecturer-adapted, Student-adapted, MO)
share the same architecture but the difference lies
in the reward functions used for training.
2.1 The Data
For this study, the dataset described by Gkatzia
et al. (2013) was used. Table 1 shows an exam-
ple of this dataset that describes a student?s learn-
ing habits and a corresponding feedback summary
provided by a lecturer. The dataset is composed
of 37 similar instances. Each instance consists of
time-series information about the student?s learn-
ing routine and the selected templates that lectur-
ers used to provide feedback to this student. A
template is a quadruple consisting of an id, a fac-
tor (Table 1), a reference type (trend, weeks, aver-
age, other) and surface text. For instance, a tem-
plate can be (1, marks, trend, ?Your marks were
<trend>over the semester?). The lexical choice
for <trend>(i.e. increasing or decreasing) de-
pends on the values of time-series data. There
is a direct mapping between the values of factor
and reference type and the surface text. The time-
series attributes are listed in Table 1 (bottom left).
2.2 Time-series summarisation systems
Actions and states: The state consists of the time-
series data and the selected templates. In order to
explore the state space the agent selects a time-
series attribute (e.g. marks, deadlines etc.) and
then decides whether to talk about it or not. The
states and actions are similar for all systems.
Lecturer-adapted reward function
The reward function is derived from analysis with
linear regression of the provided dataset and is the
following cumulative multivariate function:
Reward
LECT
= a+
n
?
i=1
b
i
? x
i
+ c ? length
where X = {x
1
, x
2
, ..., x
n
} is the vector of
combinations of the data trends observed in the
time-series data and a particular reference type of
the factor. The value of x
i
is given by the function:
x
i
=
?
?
?
?
?
?
?
?
?
?
?
1, if the combination of a factor trend
and a particular reference type is
included in the feedback
0, if not.
The coefficients represent the preference level of
a factor to be selected and how to be conveyed
in the summary. Important factors are associated
with high positive coefficients and the unimpor-
tant ones with negative coefficients. In the train-
ing phase, the agent selects a factor and then de-
cides whether to talk about it or not. If it decides
to refer to a factor, the selection of the template is
performed deterministically, i.e. it selects the tem-
plate that results in higher reward. Length rep-
resents the number of factors selected for gener-
ation.
Student-adapted reward function
The Student-adapted system uses the same RL al-
gorithm as the Lecturer-adapted one. The differ-
ence lies in the reward function. The reward func-
tion used for training is of a similar style as the
Lecturer-adapted reward function. This function
was derived by manipulating the student ratings in
a previous experiment and estimating the weights
using linear regression in a similar way as Walker
et al. (1997) and Rieser et al. (2010).
Multi-objective function
The function used for the multi-objective method
is derived by weighting the sum of the individual
reward functions.
R
MO
= 0.5 ? R
LECT
+ 0.5 ? R
STUDENT
To reduce the confounding variables, we kept
the ordering of content in all systems the same.
3 Evaluation
The output of the above-mentioned three systems
were evaluated both in simulation and with real
211
Raw Data
factors week 2 week 3 ... week 10
marks 5 4 ... 5
hours studied 1 2 ... 3
... ... ... ... ...
Trends from Data
factors factor trend
(1) marks trend other
(2) hours studied trend increasing
(3) understandability trend decreasing
(4) difficulty trend decreasing
(5) deadlines trend increasing
(6) health issues trend other
(7) personal issues trend decreasing
(8) lectures attended trend other
(9) revision trend decreasing
Summary
Your overall performance was excellent
during the semester. Keep up the good
work and maybe try some more challeng-
ing exercises. Your attendance was vary-
ing over the semester. Have a think about
how to use time in lectures to improve your
understanding of the material. You spent 2
hours studying the lecture material on
average. You should dedicate more time
to study. You seem to find the material
easier to understand compared to the
beginning of the semester. Keep up the
good work! You revised part of the learn-
ing material. Have a think about whether
revising has improved your performance.
Table 1: Top left: example of the time-series raw data for feedback generation. Bottom left: example of
described trends. Right box: a target summary generated by an expert (bold signifies the chosen content).
users. Example summaries of all systems are pre-
sented in Table 2.
3.1 Evaluation in Simulation
26 summaries were produced by each system. The
output of each system was evaluated with the three
reward functions. Table 3 shows the results.
As expected, all systems score highly when
evaluated with the reward function for which
they were trained, with the second highest reward
scored from the MO function. Table 2 illustrates
this with the MO Policy clearly between the other
two policies. Moreover, the MO function reduces
the variability between summaries as is also re-
flected in the standard deviation given in Table 3.
We used BLEU (4-grams) (Papineni et al.,
2002) to measure the similarities between the
feedback summaries generated by the three sys-
tems. BLEU score is between 0-1 with values
closer to 1 indicating texts are more similar. Our
results demonstrate that the summaries generated
by the three systems are quite different (BLEU
score between 0.33 and 0.36). This shows that the
framework presented here is capable of producing
quite different summaries based on the various re-
ward functions.
3.2 Evaluation with real users
The goal of the evaluation is to determine whether
the end-user can pick up on the above-mentioned
differences in the feedback and rank them accord-
ing to their preferences. The output of the three
systems was ranked by 19 lecturers and 48 first-
year Computer Science students. Time-series data
of three students were presented on graphs to each
participant. They were also shown 3 feedback
summaries and they were asked to rank them in
terms of preference.
As we can see from Table 4, the two user groups
significantly preferred the output of the system
which was trained for their preferences (Mann-
Whitney U test, p < 0.05). Interestingly, lecturers
found both the outputs produced by the Lecturer-
adapted system and the Student-adapted system
significantly preferable (p < 0.05) to the output
produced by the MO system. In contrast, students
significantly preferred the output generated by the
Student-adapted system over the other two. Fi-
nally, both user groups rated the MO system 3rd,
but there is not a significant difference between
the student ratings for the MO system and the
Lecturer-adapted system.
4 Discussion
It is interesting to examine the weights derived
from the multiple-linear regression to determine
the preferences of the different user groups. For
instance, lecturers? most preferred content is
hours studied, therefore the reward function gives
high scores to summaries that mention the hours
212
Lecturer-adapted Student-adapted Multi-objective
Make sure you revise the learning
material and try to do the lab ex-
ercises again. You dedicated more
time studying the lecture material in
the beginning of the semester com-
pared to the end of the semester.
Have a think about what is prevent-
ing you from studying. Your under-
standing of the material could be
improved. Try going over the teach-
ing material again. You have had
other deadlines during weeks 5, 6,
8, 9 and 10. You may want to plan
your studying and work ahead. You
did not face any health problems
during the semester.
You found the lab exercises very
challenging. Make sure that you
have understood the taught material
and don?t hesitate to ask for clari-
fication. You dedicated more time
studying the lecture material in
the beginning of the semester com-
pared to the end of the semester.
Have a think about what is prevent-
ing you from studying. Your un-
derstanding of the material could
be improved. Try going over the
teaching material again. Revising
material during the semester will
improve your performance in the
lab.
Your attendance was varying over the
semester. Have a think about how to
use time in lectures to improve your un-
derstanding of the material. You found
the lab exercises very challenging. Make
sure that you have understood the taught
material and don?t hesitate to ask for
clarification. You dedicated more time
studying the lecture material in the be-
ginning of the semester compared to the
end of the semester. Have a think about
what is preventing you from studying.
You did not face any health problems
during the semester. You revised part
of the learning material. Have a think
whether revising has improved your per-
formance.
Table 2: Example outputs from the three different systems (bold signifies the chosen content).
Time-Series Summarisation Systems Lecturer Function Student Function MO Function
Lecturer-adapted system 243.82 (70.35) 51.99 (89.87) 114.12 (49.58)
Student-adapted system 72.54 (106.97) 213.75 (59.45) 127.76 (52.09)
MO system 123.67 (72.66) 153.79 (56.61) 164.84 (83.89)
Table 3: Average rewards (and standard deviation) assigned to summaries produced by the 3 systems.
Bold signifies higher reward.
Summarisation
Systems
Lecturer?s Rat-
ing
Student?s
Rating
Lecturer-adapted 1st (2.15)* 3rd (1.97)
Student-adapted 1st (2.01)* 1st* (2.22)
MO 2nd, 3rd (1.81) 3rd (1.79)
Table 4: Mode of the ratings for each user group
(*Mann-Whitney U test, p < 0.05, when compar-
ing each system to the MO system).
that a student studied in all cases (i.e. when the
hours studied increased, decreased, or remained
stable). This, however, does not factor heavily into
the student?s reward function.
Secondly, lecturers find it useful to give some
advice to students who faced personal issues dur-
ing the semester, such as advising them to talk to
their mentor. Students, on the other hand, like
reading about personal issues only when the num-
ber of issues they faced was increasing over the
semester, perhaps as this is the only trend that may
affect their performance. Students seem to mostly
prefer a feedback summary that mentions the un-
derstandability of the material when it increases
which is positive feedback. Finally, the only factor
that both groups agree on is that health issues is
negatively weighted and therefore not mentioned.
The MO reward function attempts to balance
the preferences of the two user groups. Therefore,
for this function, the coefficient for mentioning
health issues is also negative, however the other
coefficients are smoothed providing neither strong
negative or positive coefficients. This means that
there is less variability (see Table 3) but that per-
haps this function meets neither group?s criteria.
5 Conclusion and Future Work
In conclusion, we presented a framework for de-
veloping and evaluating various reward functions
for time-series summarisation of feedback. This
framework has been validated in that both simula-
tion and subjective studies show that each group
does indeed prefer feedback generated using a
highly tuned reward function, with lecturers being
slightly more open to variation. Further investiga-
tion is required as to whether it is indeed possible
to find middle ground between these two groups.
Choices for one group may be negatively rated
by the other and it might not be possible to find
middle ground but it is worth investigating further
other methods of reward function derivation using
stronger feature selection methods, such as Princi-
pal Component Analysis.
213
References
Carole Ames. 1992. Classrooms: Goals, structures,
and student motivation. Journal of Educational Psy-
chology, 84(3):p261?71.
Chankong and Haimes. 1983. Multiobjective decision
making theory and methodology. In New York: El-
sevier Science Publishing.
Scotty D. Craig, Arthur C. Graesser, Jeremiah Sullins,
and Barry Gholson. 2004. Affect and learning: an
exploratory look into the role of affect in learning
with autotutor. In Journal of Educational Media,
29:241-250.
Albert Gatt, Francois Portet, Ehud Reiter, James
Hunter, Saad Mahamood, Wendy Moncur, and So-
mayajulu Sripada. 2009. From data to text in the
neonatal intensive care unit: Using NLG technology
for decision support and information management.
In Journal of AI Communications, 22:153-186.
Dimitra Gkatzia, Helen Hastie, Srinivasan Ja-
narthanam, and Oliver Lemon. 2013. Generating
student feedback from time-series data using Rein-
forcement Learning. In 14th European Workshop in
Natural Language Generation.
Srinivasan Janarthanam and Oliver Lemon. 2010.
Adaptive referring expression generation in spoken
dialogue systems: Evaluation with real users. In
11th Annual Meeting of the Special Interest Group
on Discourse and Dialogue.
K Papineni, S Roukos, T. Ward, and W. J Zhu. 2002.
BLEU: a method for automatic evaluation of ma-
chine translation. In 40th Annual meeting of the As-
sociation for Computational Linguistics.
Natalie K. Person, Roger J. Kreuz, Rolf A. Zwaan, and
Arthur C. Graesser. 1995. Pragmatics and peda-
gogy: Conversational rules and politeness strategies
may inhibit effective tutoring. In Journal of Cogni-
tion and Instruction, 13(2):161-188.
Ehud Reiter and Robert Dale. 2000. Building natural
language generation systems. InCambridge Univer-
sity Press.
Verena Rieser, Oliver Lemon, and Xingkun Liu. 2010.
Optimising information presentation for spoken dia-
logue systems. In 48th Annual Meeting of the Asso-
ciation for Computational Linguistics.
Richart Sutton and Andrew Barto. 1998. Reinforce-
ment learning. In MIT Press.
Cynthia A. Thompson, Mehmet H. Goker, and Pat Lan-
gley. 2004. A personalised system for conversa-
tional recommendations. In Journal of Artificial In-
telligence Research 21, 333-428.
Marilyn Walker, Diane Litman, Candace Kamm, and
Alicia Abella. 1997. PARADISE: A framework for
evaluating spoken dialogue agents. In 35th Annual
meeting of the Association for Computational Lin-
guistics.
Ingrid Zukerman and Diane Litman. 2001. Natu-
ral language processing and user modeling: Syner-
gies and limitations. In User Modeling and User-
Adapted Interaction, 11(1-2), 129-158.
214
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 1254?1263,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
Conditional Random Fields for Responsive Surface Realisation using
Global Features
Nina Dethlefs, Helen Hastie, Heriberto Cuaya?huitl and Oliver Lemon
Mathematical and Computer Sciences
Heriot-Watt University, Edinburgh
n.s.dethlefs | h.hastie | h.cuayahuitl | o.lemon@hw.ac.uk
Abstract
Surface realisers in spoken dialogue sys-
tems need to be more responsive than con-
ventional surface realisers. They need to
be sensitive to the utterance context as well
as robust to partial or changing generator
inputs. We formulate surface realisation as
a sequence labelling task and combine the
use of conditional random fields (CRFs)
with semantic trees. Due to their extended
notion of context, CRFs are able to take
the global utterance context into account
and are less constrained by local features
than other realisers. This leads to more
natural and less repetitive surface realisa-
tion. It also allows generation from partial
and modified inputs and is therefore ap-
plicable to incremental surface realisation.
Results from a human rating study confirm
that users are sensitive to this extended no-
tion of context and assign ratings that are
significantly higher (up to 14%) than those
for taking only local context into account.
1 Introduction
Surface realisation typically aims to produce out-
put that is grammatically well-formed, natural and
cohesive. Cohesion can be characterised by lexical
or syntactic cues such as repetitions, substitutions,
ellipses, or connectives. In automatic language
generation, such properties can sometimes be dif-
ficult to model, because they require rich context-
awareness that keeps track of all (or much) of what
was generated before, i.e. a growing generation
history. In text generation, cohesion can span over
the entire text. In interactive settings such as gen-
eration within a spoken dialogue system (SDS), a
challenge is often to keep track of cohesion over
several utterances. In addition, since interactions
are dynamic, generator inputs from the dialogue
manager can sometimes be partial or subject to
subsequent modification. This has been addressed
by work on incremental processing (Schlangen
and Skantze, 2009). Since dialogue acts are passed
on to the generation module as soon as possible,
this can sometimes lead to incomplete generator
inputs (because the user is still speaking), or in-
puts that are subject to later modification (because
of an initial ASR mis-recognition).
In this paper, we propose to formulate surface
realisation as a sequence labelling task. We use
conditional random fields (Lafferty et al, 2001;
Sutton and McCallum, 2006), which are suitable
for modelling rich contexts, in combination with
semantic trees for rich linguistic information. This
combination is able to keep track of dependen-
cies between syntactic, semantic and lexical fea-
tures across multiple utterances. Our model can
be trained from minimally labelled data, which re-
duces development time and may (in the future)
facilitate an application to new domains.
The domain used in this paper is a pedestrian
walking around a city looking for information and
recommendations for local restaurants from an
SDS. We describe here the module for surface re-
alisation. Our main hypothesis is that the use of
global context in a CRF with semantic trees can
lead to surface realisations that are better phrased,
more natural and less repetitive than taking only
local features into account. Results from a human
rating study confirm this hypothesis. In addition,
we compare our system with alternative surface
realisation methods from the literature, namely, a
rank and boost approach and n-grams.
Finally, we argue that our approach lends itself
1254
to surface realisation within incremental systems,
because CRFs are able to model context across
full as well as partial generator inputs which may
undergo modifications during generation. As a
demonstration, we apply our model to incremen-
tal surface realisation in a proof-of-concept study.
2 Related Work
Our approach is most closely related to Lu et
al. (2009) who also use CRFs to find the best
surface realisation from a semantic tree. They
conclude from an automatic evaluation that using
CRF-based generation which takes long-range de-
pendencies into account outperforms several base-
lines. However, Lu et al?s generator does not take
context beyond the current utterance into account
and is thus restricted to local features. Further-
more, their model is not able to modify generation
results on the fly due to new or updated inputs.
In terms of surface realisation from graphical
models (and within the context of SDSs), our ap-
proach is also related to work by Georgila et al
(2002) and Dethlefs and Cuaya?huitl (2011b), who
use HMMs, Dethlefs and Cuaya?huitl (2011a) who
use Bayes Nets, and Mairesse et al (2010) who
use Dynamic Bayes Nets within an Active Learn-
ing framework. The last approach is also con-
cerned with generating restaurant recommenda-
tions within an SDS. Specifically, their system op-
timises its performance online, during the interac-
tion, by asking users to provide it with new textual
descriptions of concepts, for which it is unsure of
the best realisation. In contrast to these related
approaches, we use undirected graphical models
which are useful when the natural directionality
between the input variables is unknown.
In terms of surface realisation for SDSs, Oh and
Rudnicky (2000) present foundational work in us-
ing an n-gram-based system. They train a surface
realiser based on a domain-dependent language
model and use an overgeneration and ranking ap-
proach. Candidate utterances are ranked accord-
ing to a penalty function which penalises too long
or short utterances, repetitious utterances and ut-
terances which either contain more or less infor-
mation than required by the dialogue act. While
their approach is fast to execute, it has the dis-
advantage of not being able to model long-range
dependencies. They show that humans rank their
output equivalently to template-based generation.
Further, our approach is related to the SPaRKy
sentence generator (Walker et al, 2007). SPaRKy
was also developed for the domain of restaurant
recommendations and was shown to be equivalent
to or better than a carefully designed template-
based generator which had received high human
ratings in the past (Stent et al, 2002). It generates
sentences in two steps. First, it produces a ran-
domised set of alternative realisations, which are
then ranked according to a mapping from sentence
plans to predicted human ratings using a boosting
algorithm. As in our approach, SPaRKy distin-
guishes local and global features. Local features
take only information of the current tree node into
account, including its parents, siblings and chil-
dren, while global features take information of the
entire utterance into account. While SPaRKy is
shown to reach high output quality in compari-
son to a template-based baseline, the authors ac-
knowledge that generation with SPaRKy is rather
slow when applied in a real-time SDS. This could
present a problem in incremental settings, where
generation speed is of particular importance.
The SPaRKy system is also used by Rieser et
al. (2011), who focus on information presentation
strategies for restaurant recommendations, sum-
maries or comparisons within an SDS. Their sur-
face realiser is informed by the highest ranked
SPaRKy outputs for a particular information pre-
sentation strategy and will constitute one of our
baselines in the evaluation.
More work on trainable realisation for SDSs
generally includes Bulyko and Ostendorf (2002)
who use finite state transducers, Nakatsu and
White (2006) who use supervised learning, Varges
(2006) who uses chart generation, and Konstas
and Lapata (2012) who use weighted hypergraphs,
among others.
3 Cohesion across Utterances
3.1 Tree-based Semantic Representations
The restaurant recommendations we generate can
include any of the attributes shown in Table 1.
It is then the task of the surface realiser to find
the best realisation, including whether to present
them in one or several sentences. This often is
a sentence planning decision, but in our approach
it is handled using CRF-based surface realisation.
The semantic forms underlying surface realisation
can be produced in many ways. In our case, they
are produced by a reinforcement learning agent
which orders semantic attributes in the tree ac-
1255
Timing and Ordering
Surface Realisation
User
Interaction 
Micro-turn dialogue 
act, inform(food=Thai)
Semantic tree
String of words
intervening modules
speech
semantics of 
user utterance
(synthesised)
Manager
Figure 1: Architecture of our SDS with a focus on
the NLG components. While the user is speaking,
the dialogue manager sends dialogue acts to the
NLG module, which uses reinforcement learning
to order semantic attributes and produce a seman-
tic tree (see Dethlefs et al (2012b)). This paper fo-
cuses on surface realisation from these trees using
a CRF as shown in the surface realisation module.
Slot Example
ADDRESS The venue?s address is . . .
AREA It is located in . . .
FOOD The restaurant serves . . . cuisine.
NAME The restaurant?s name is . . .
PHONE The venue?s phone number is . . .
POSTCODE The postcode is . . .
QUALITY This is a . . . venue.
PRICE It is located in the . . . price range.
SIGNATURE The venue specialises in . . .
VENUE This venue is a . . .
Table 1: Semantic slots required for our domain
along with example realisations. Attributes can be
combined in all possible ways during generation.
cording to their confidence in the dialogue. This
is because SDSs can often have uncertainties with
regard to the user?s actual desired attribute values
due to speech recognition inaccuracies. We there-
fore model all semantic slots as probability distri-
butions, such as inform(food=Indian, 0.6) or in-
form(food=Italian, 0.4) and apply reinforcement
learning to finding the optimal sequence for pre-
sentation. Please see Dethlefs et al (2012b) for
details. Here, we simply assume that a semantic
form has been produced by a previous processing
module.
As shown in the architecture diagram in Fig-
ure 1, a CRF surface realiser takes a semantic
tree as input. We represent these as context-free
trees which can be defined formally as 4-tuples
Lexical
features
Syntactic
features
Semantic
features
The Beluga is a great Italian restaurant
y0 y1 y2
root
inform(
name=
Beluga)
The Beluga
root
inform(
venue=
Restaurant)
is a great Italian
inform(
type=
Italian)
root
restaurant
(a)
(b)
The 
Beluga
is a great
Italian
restaurant
other
phrases
(c)
Figure 2: (a) Graphical representation of a linear-
chain Conditional Random Field (CRF), where
empty nodes correspond to the labelled sequence,
shaded nodes to linguistic observations, and dark
squares to feature functions between states and ob-
servations; (b) Example semantic trees that are up-
dated at each time step in order to provide linguis-
tic features to the CRF (only one possible surface
realisation is shown and parse categories are omit-
ted for brevity); (c) Finite state machine of phrases
(labels) for this example.
{S, T,N,H}, where S is a start symbol, typically
the root node of the tree; T = {t0, t1, t2 . . . t|T |}
is a set of terminal symbols, corresponding to sin-
gle phrases; N = {n0, n1, n2 . . . n|N |} is a set of
non-terminal symbols corresponding to semantic
categories, and H = {h0, h1, h2 . . . h|H|} is a set
of production rules of the form n ? ?, where
n ? N , ? ? T ? N . The production rules rep-
resent alternatives at each branching node where
the CRF is consulted for the best available expan-
sion from the subset of possible ones. All nodes
in the tree are annotated with a semantic concept
(obtained from the semantic form) as well as their
parse category.
3.2 Conditional Random Fields for
Phrase-Based Surface Realisation
The main idea of our approach is to treat surface
realisation as a sequence labelling task in which a
sequence of semantic inputs needs to be labelled
with appropriate surface realisations. The task is
therefore to find a mapping between (observed)
1256
lexical, syntactic and semantic features and a (hid-
den) best surface realisation.
We use the linear-chain Conditional Random
Field (CRF) model for statistical phrase-based sur-
face realisation, see Figure 2 (a). This probabilis-
tic model defines the posterior probability of la-
bels (surface realisation phrases) y={y1, . . . , y|y|}
given features x={x1, . . . , x|x|} (informed by a se-
mantic tree, see Figure 2 (b)), as
P (y|x) = 1Z(x)
T?
t=1
exp
{ K?
k=1
?k?k(yt, yt?1, xt)
}
,
where Z(x) is a normalisation factor over all pos-
sible realisations (i.e. labellings) of x such that the
sum of all terms is one. The parameters ?k are
weights corresponding to feature functions ?k(.),
which are real values describing the label state y
at time t based on the previous label state yt?1 and
features xt. For example: from Figure 2 (c), ?k
might have the value ?k = 1.0 for the transition
from ?The Beluga? to ?is a great Italian?, and 0.0
elsewhere. The parameters ?k are set to maximise
the conditional likelihood of phrase sequences in
the training data set. They are estimated using the
gradient ascent algorithm.
After training, labels can be predicted for new
sequences of observations. The most likely phrase
sequence is expressed as
y ? = argmax
y
P (y|x),
which is computed using the Viterbi algorithm.
We use the Mallet package1 (McCallum, 2002) for
parameter learning and inference.
3.3 Feature Selection and Training
The following features define the generation con-
text used during training of the CRF. The genera-
tion context includes everything that has been gen-
erated for the current utterance so far. All features
can be obtained from a semantic input tree.
? Lexical items of parents and siblings,
? Semantic types in expansion,
? Semantic types of parents and siblings,
? Parse category of expansion,
? Parse categories of parents and siblings.
We use the StanfordParser2 (Marneffe et al, 2006)
to obtain the parse category for each tree node.
1http://mallet.cs.umass.edu/
2http://nlp.stanford.edu/software/
lex-parser.shtml
The semantics for each node are derived from the
input dialogue acts (these are listed in Table 1) and
are associated with nodes. The lexical items are
present in the generation context and are mapped
to semantic tree nodes.
As an example, for generating an utterance (la-
bel sequence) such as The Beluga is a great restau-
rant. It is located in the city centre., each gen-
eration step needs to take the features of the en-
tire generation history into account. This includes
all individual lexical items generated, the seman-
tic types used and the parse categories for each
tree node involved. For the first constituent, The
Beluga, this corresponds to the features {? BE-
GIN NAME} indicating the beginning of a sentence
(where empty features are omitted), the beginning
of a new generation context and the next semantic
slot required. For the second constituent, is a great
restaurant, the features are {THE BELUGA NAME
NP VENUE}, i.e. including the generation history
(with lexical items and parse category added for
the first constituent) and the semantics of the next
required slot, VENUE. In this way, a sequence of
surface form constituents is generated correspond-
ing to latent states in the CRF.
Since global utterance features capture the full
generation context (i.e. beyond the current ut-
terance), we are also able to model phenomena
such as co-references and pronouns. This is useful
for longer restaurant recommendations which may
span over more than one utterance. If the genera-
tion history already contains a semantic attribute,
e.g. the restaurant name, the CRF may afterwards
choose a pronoun, e.g. it, which has a higher like-
lihood than using the proper name again. Simi-
larly, the CRF may decide to realise a new attribute
as constituents of different order, such as a sen-
tence or PP, depending on the length, number and
parse categories of previously generated output. In
this way, our approach implicitly treats sentence
planning decisions such as the distribution of con-
tent over a set of messages in the same way as (or
as part of) surface realisation. A further capabil-
ity of our surface realiser is that it can generate
complete phrases from full as well as partial dia-
logue acts. This is useful in interactive contexts,
where we need as much robustness as possible. A
demonstration of this is given in Section 5 in an
application to incremental surface realisation.
To train the CRF, we used a data set of 552
restaurant recommendations from the website The
1257
List.3 The data contains recommendations such as
Located in the city centre, Beluga is a stylish yet
laid-back restaurant with a smart menu of modern
European cuisine.
3.4 Grammar Induction
The grammar g of surface realisation candidates
is obtained through an automatic grammar induc-
tion algorithm which can be run on unlabelled
data and requires only minimal human interven-
tion. This grammar defines the surface realisa-
tion space for the CRFs. We provide the human
corpus of restaurant recommendations from Sec-
tion 3.3 as input to grammar induction. The al-
gorithm is shown in Algorithm 1. It first identi-
fies all semantic attributes of interest in an utter-
ance, in our case those specified in Table 1, and re-
places them by a variable. These attributes include
food types, such as Mexican, Chinese, particular
parts of town, prices, etc. About 45% of them can
be identified based on heuristics. The remainder
needs to be hand-annotated at the moment, which
includes mainly attributes like restaurant names or
quality attributes, such as delicate, exquisite, etc.
Subsequently, all utterances are parsed using the
Stanford parser to obtain constituents and are inte-
grated into the grammar under construction. The
non-terminal symbols are named after the auto-
matically annotated semantic attributes contained
in their expansion, e.g. NAME QUALITY ? The
$name$ is of $quality$ quality. In this way, each
non-terminal symbol has a semantic representa-
tion and an associated parse category. In total, our
induced grammar contains more than 800 rules.
4 Evaluation
To evaluate our approach, we focus on a sub-
jective human rating study which aims to deter-
mine whether CRF-based surface realisation that
takes the full generation context into account,
called CRF (global), is perceived better by human
judges than one that uses a CRF but just takes local
context into account, called CRF (local). While
CRF (global) uses features from the entire genera-
tion history, CRF (local) uses only features from
the current tree branch. We assume that cohe-
sion can be identified by untrained judges as natu-
ral, well-phrased and non-repetitive surface forms.
To examine differences in methodology between
3http://www.list.co.uk
Algorithm 1 Grammar Induction.
1: function FINDGRAMMAR(utterances u, semantic at-
tributes a) return grammar
2: for each utterance u do
3: if u contains a semantic attribute from a, such as
venue, cuisine, etc. then
4: Find and replace the attribute by its semantic
variable, e.g. $venue$.
5: end if
6: Parse the sentence and induce a set of rules ??
?, where ? is a semantic variable and ? is its parse.
7: Traverse the parse tree in a top-down, depth-first
search and
8: if expansion ? exists then
9: continue
10: else if non-terminal ? exists then
11: add new expansion ? to ?.
12: else write new rule ?? ?.
13: end if
14: Write grammar.
15: end for
16: end function
CRFs and other state-of-the-art methods, we also
compare our system to two other baselines:
? CLASSiC corresponds to the system re-
ported in Rieser et al (2011),4 which gen-
erates restaurant recommendations based on
the SPaRKy system (Walker et al, 2007), and
has received high ratings in the past. SPaRKy
uses global utterance features.
? n-grams represents a simple 5-gram baseline
that is similar to Oh and Rudnicky (2000)?s
system. We will sample from the most likely
slot realisations that do not contain a repeti-
tion and include exactly the required slot val-
ues. Local context only is taken into account.
4.1 Human Rating Study
We carried out a user rating study on the Crowd-
Flower crowd sourcing platform.5 Each partici-
pant was shown part of a real human-system dia-
logue that emerged as part of the CLASSiC project
evaluation (Rieser et al, 2011). All dialogues
and data are freely available from http://www.
classic-project.org. Each dialogue contained
two variations for one of the utterances. These
variations were generated from two out of the four
systems described above. The order that these
were presented to the participant was counterbal-
anced. Table 2 gives an example of a dialogue seg-
ment presented to the participants.
4In Rieser et al (2011), this system is referred to as the
TIP system, which generates summaries, comparisons or rec-
ommendations for restaurants. For the present study, we com-
1258
SYS Thank you for calling the Cambridge Information
system. Your call will be recorded for research pur-
poses. You may ask for information about a place
to eat, such as a restaurant, a pub, or a cafe. How
may I help you?
USR I want to find an American restaurant which is in
the very expensive area.
SYS
A
The restaurant Gourmet Burger is an outstanding,
expensive restaurant located in the central area.
SYS
B
Gourmet Burger is a smart and welcoming restau-
rant. Gourmet Burger provides an expensive dining
experience with great food and friendly service. If
you?re looking for a central meal at an expensive
price.
USR What is the address and phone number?
SYS Gourmet Burger is on Regent Street and its phone
number is 01223 312598.
USR Thank you. Good bye.
Table 2: Example dialogue for participants to
compare alternative outputs in italics, USR=user,
SYS A=CRF (global), SYS B=CRF(local).
System Natural Phrasing Repetit.
CRF global 3.65 3.64 3.65
CRF local 3.10? 3.19? 3.13?
CLASSiC 3.53? 3.59 3.48?
n-grams 3.01? 3.09? 3.32?
Table 3: Subjective user ratings. Significance with
CRF (global) at p<0.05 is indicated as ?.
44 participants gave a total of 1,830 ratings of
utterances produced across the four systems. Flu-
ent speakers of English only were requested and
the participants were from the United States. They
were asked to rate each utterance on a 5 point Lik-
ert scale in response to the following questions
(where 5 corresponds to totally agree and 1 cor-
responds to totally disagree):
? The utterance was natural, i.e. it could have
been produced by a human. (Natural)
? The utterance was phrased well. (Phrasing)
? The utterance was repetitive. (Repetitive)
4.2 Results
We can see from Table 3 that across all the cate-
gories, the CRF (global) gets the highest overall
ratings. This difference is significant for all cat-
egories compared with CRF (local) and n-grams
(using a 1-sided Mann Whitney U-test, p < 0.001).
pare only with the subset of recommendations.
5http://www.crowdflower.com
Possibly this is because the local context taken
into account by both systems was not enough to
ensure cohesion across surface phrases. It is not
possible, e.g., to cover co-references within a lo-
cal context only or discourse markers that refer be-
yond the current utterance. This can lead to short
and repetitive phrases, such as Make your way to
Gourmet Burger. The food quality is outstanding.
The prices are expensive. generated by the n-gram
baseline.
The CLASSiC baseline, based on SPaRKy, was
the most competitive system in our comparison.
None-the-less CRF (global) is rated higher across
categories and significantly so for Natural (p <
0.05) and Repetitive (p < 0.005). For Phrasing,
there is a trend but not a significant difference (p
< 0.16). All comparisons are based on a 1-sided
Mann Whitney U-test. A qualitative comparison
between the CRF (global) and CLASSiC outputs
showed the following. CLASSiC utterances tend
to be longer and contain more sentences than CRF
(global) utterances. While CRF (global) often de-
cides to aggregate attributes into one sentence,
such as the Beluga is an outstanding restaurant
in the city centre, CLASSiC tends to rely more on
individual messages, such as The Beluga is an out-
standing restaurant. It is located in the city cen-
tre. A possible reason is that while CRF (global)
is able to take features beyond an utterance into
account, CLASSiC/SPaRKy is restricted to global
features of the current utterance.
We can further compare our results with Rieser
et al (2011) and Mairesse et al (2010) who also
generate restaurant recommendations and asked
similar questions to participants as we did. Rieser
et al (2011)?s system received an average rating
of 3.586 in terms of Phrasing which compares to
our 3.64. This difference is not significant, and
in line with the user ratings we observed for the
CLASSiC system above (3.59). Mairesse et al
(2010) achieved an average score of 4.05 in terms
of Natural in comparison to our 3.65. This differ-
ence is significant at p<0.05. Possibly their better
performance is due to the data set being more ?in
domain? than ours. They collected data from hu-
mans that was written specifically for the task that
the system was tested on. In contrast, our system
was trained on freely available data that was writ-
ten by professional restaurant reviewers. Unfortu-
nately, we cannot compare across other categories,
6This was rescaled from a 1-6 scale.
1259
USR1 I?m looking for a nice restaurant in the centre.
SYS1 inform(area=centre [0.2], food=Thai [0.3])
inform(name=Bangkok [0.3])
So you?re looking for a Thai . . .
USR2 [barges in] No, I?m looking for a restaurant
with good quality food.
SYS2 inform(quality=good [0.6], name=Beluga [0.6])
Oh sorry, so a nice restaurant located . . .
USR3 [barges in] . . . in the city centre.
SYS3 inform(area=centre [0.8])
Table 4: Example dialogue where the dialogue
manager needs to send incremental updates to the
NLG. Incremental surface realisation from seman-
tic trees for this dialogue is shown in Figure 3.
because the authors tested only for Phrasing and
Natural, respectively.
5 Incremental Surface Realisation
Recent years have seen increased interest in
incremental dialogue processing (Skantze and
Schlangen, 2009; Schlangen and Skantze, 2009).
The main characteristic of incremental architec-
tures is that instead of waiting for the end of a user
turn, they begin to process the input stream as soon
as possible, updating their processing hypotheses
as more information becomes available. From a
dialogue perspective, they can be said to work on
partial rather than full dialogue acts.
With respect to surface realisation, incremen-
tal NLG systems have predominantly relied on
pre-defined templates (Purver and Otsuka, 2003;
Skantze and Hjalmarsson, 2010; Dethlefs et al,
2012a), which limits the flexibility and quality of
output generation. Buschmeier et al (2012) have
presented a system which systematically takes
the user?s acoustic understanding problems into
account by pausing, repeating or re-phrasing if
necessary. Their approach is based on SPUD
(Stone et al, 2003), a constraint satisfaction-based
NLG architecture and marks important progress
towards more flexible incremental surface realisa-
tion. However, given the human labour involved in
constraint specification, cohesion is often limited
to a local context. Especially for long utterances
or such that are separated by user turns, this may
lead to surface form increments that are not well
connected and lack cohesion.
5.1 Application to Incremental SR
This section will discuss a proof-of-concept appli-
cation of our approach to incremental surface re-
alisation. Table 4 shows an example dialogue be-
tween a user and system that contains a number
of incremental phenomena that require hypothe-
sis updates, system corrections and user barge-
ins. Incremental surface realisation for this dia-
logue is shown in Figure 3, where processing steps
are indicated as bold-face numbers and are trig-
gered by partial dialogue acts that are sent from
the dialogue manager, such as inform(area=centre
[0.2]). The numbers in square brackets indicate
the system?s confidence in the attribute-value pair.
Once a dialogue act is observed by the NLG sys-
tem, a reinforcement learning agent determines the
order of attributes and produces a semantic tree, as
described in Section 3.1. Since the semantic forms
are constructed incrementally, new tree nodes can
be attached to and deleted from an existing tree,
depending on what kind of update is required.
In the dialogue in Table 4, the user first asks
for a nice restaurant in the centre. The dialogue
manager constructs a first attribute-value slot, in-
form(area=centre [0.2], . . . ), and passes it on to
NLG.7 In Figure 3, we can observe the corre-
sponding NLG action, a first tree is created with
just a root node and a node representing the area
slot (step 1). In a second step, the semantically
annotated node gets expanded into a surface form
that is chosen from a set of candidates (shown in
curly brackets). The CRF is responsible for this
last step. Since there is no preceding utterance, the
best surface form is chosen based on the semantics
alone. Active tree nodes, i.e. those currently under
generation, are indicated as asterisks in Figure 3.
Currently inactive nodes are shown as circles.
Step 3 then further expands the current tree
adding a node for the food type and the name of
a restaurant that the dialogue manager had passed.
We see here that attributes can either be primitive
or complex. Primitive attributes contain a single
semantic type, such as area, whereas complex at-
tributes contain multiple types, such as food, name
and need to be decomposed in a later processing
step (see steps 4 and 6). Step 5 again uses the CRF
7Note here that the information passed on to the NLG is
distinct from the dialogue manager?s own actions. In the ex-
ample, the NLG is asked to generate a recommendation, but
the dialogue manager actually decides to clarify the user?s
preferences due to low confidence. This scenario is an exam-
ple of generator inputs that may get revised afterwards.
1260
root
(1) inform
(area=centre)
(2) Right in the city centre,
{located in $area$, if 
you're looking to eat 
in $area$, in $area$, ...} 
inform(area=
centre)
(3) inform(food=Thai
        name=Bangkok)
Right in the city centre, 
root
(6) inform
(food=Thai)
(4) inform(name=
              Bangkok)
(5) Bangkok
{the $name$, 
it is called $name$,  ...}
root
inform(area=
centre)
Right in the city centre, 
inform(food=Thai, 
name=Bangkok)
root
inform(area=
centre)
Right in the city centre, 
(7) inform(quality=very
good, name=Beluga)
inform(name=
        Bangkok)
inform
(food=Thai)
Bangkok
root
inform(area=
centre)
inform(quality=nice, 
name=Beluga)
Right in the city centre, 
(8) inform(name=
Beluga)
(10) inform(quality=
very good)
(9) the Beluga
{$name$, the venue 
called $name$, ...}
(11) is of very good quality. 
{is a $quality$ venue, if you want $quality$ 
food, $quality$, a $quality$ place ...}
*
*
*
*
*
* *
**
*
**
*
Figure 3: Example of incremental surface realisation, where each generation step is indicated by a num-
ber. Active generation nodes are shown as asterisks and deletions are shown as crossed out. Lexical and
semantic features are associated with their respective nodes. Syntactic information in the form of parse
categories are also taken into account for surface realisation, but have been omitted in this figure.
to obtain the next surface realisation that connects
with the previous one (so that a sequence of real-
isation ?labels? appears: Right in the city centre
and Bangkok). It takes the full generation context
into account to ensure a globally optimal choice.
This is important, because the local context would
otherwise be restricted to a partial dialogue act,
which can be much smaller than a full dialogue
act and thus lead to short, repetitive sentences.
The dialogue continues as the system implicitly
confirms the user?s preferred restaurant (SYS1).
At this point, we encounter a user barge-in correct-
ing the desired choice. As a consequence, the dia-
logue manager needs to update its initial hypothe-
ses and communicate this to NLG. Here, the last
three tree nodes need to be deleted from the tree
because the information is no longer valid. This
update and the deletion is shown in step 7. After-
wards, the dialogue continues and NLG involves
mainly expanding the current tree into a full se-
quence of surface realisations for partial dialogue
acts which come together into a full utterance.
This example illustrates three incremental pro-
cessing steps: expansions, updates and deletions.
Expansions are the most frequent operation. They
add new partial dialogue acts to the semantic tree.
They also consult the CRF for the best surface
realisation. Since CRFs are not restricted by the
Markov condition, they are less constrained by lo-
cal context than other models and can take non-
local dependencies into account. For our applica-
tion, the maximal context is 9 semantic attributes
(for a surface form that uses all possible 10 at-
tributes). While their extended context aware-
ness can often make CRFs slow to train, they are
fast at execution and therefore very applicable to
the incremental scenario. For applications involv-
ing longer-spanning alternatives, such as texts or
paragraphs, the context of the CRF would likely
have to be constrained. Updates are triggered by
the hypothesis updates of the dialogue manager.
Whenever a new attribute comes in, it is checked
against the generator?s existing knowledge. If it
is inconsistent with previous knowledge, an up-
date is triggered and often followed by a deletion.
Whenever generated output needs to be modified,
old expansions and surface forms are deleted first,
before new ones can be expanded in their place.
5.2 Updates and Processing Speed Results
Since fast responses are crucial in incremental sys-
tems, we measured the average time our system
took for a surface realisation. The time is 100ms
on a MacBook Intel Core 2.6 Duo with 8GB in
1261
RAM. This is slightly better than other incremen-
tal systems (Skantze and Schlangen, 2009) and
much faster than state-of-the-art non-incremental
systems such as SPaRKy (Walker et al, 2007).
In addition, we measured the number of neces-
sary generation updates in comparison to a non-
incremental setting. Since updates take effect di-
rectly on partial dialogue acts, rather than the full
generated utterance, we require around 50% less
updates as if generating from scratch for every
changed input hypothesis. A qualitative analysis
of the generated outputs showed that the quality is
comparable to the non-incremental case.
6 Conclusion and Future Directions
We have presented a novel technique for surface
realisation that treats generation as a sequence la-
belling task by combining a CRF with tree-based
semantic representations. An essential property
of interactive surface realisers is to keep track of
the utterance context including dependencies be-
tween linguistic features to generate cohesive ut-
terances. We have argued that CRFs are well
suited for this task because they are not restricted
by independence assumptions. In a human rating
study, we confirmed that judges rated our output
as better phrased, more natural and less repetitive
than systems that just take local features into ac-
count. This also holds for a comparison with state-
of-the-art rank and boost or n-gram approaches.
Keeping track of the global context is also impor-
tant for incremental systems since generator inputs
can be incomplete or subject to modification. In a
proof-of-concept study, we have argued that our
approach is applicable to incremental surface real-
isation. This was supported by preliminary results
on the speed, number of updates and quality dur-
ing generation. As future work, we plan to test
our model in a task-based setting using an end-to-
end SDS in an incremental and non-incremental
setting. This study will contain additional evalu-
ation categories, such as the understandability or
informativeness of system utterances. In addition,
we may compare different sequence labelling al-
gorithms for surface realisation (Nguyen and Guo,
2007) or segmented CRFs (Sarawagi and Cohen,
2005) and apply our method to more complex sur-
face realisation domains such as text generation or
summarisation. Finally, we would like to explore
methods for unsupervised data labelling so as to
facilitate portability across domains further.
Acknowledgements
The research leading to this work was funded by
the EC FP7 programme FP7/2011-14 under grant
agreement no. 287615 (PARLANCE).
References
Ivan Bulyko and Mari Ostendorf. 2002. Efficient in-
tegrated response generation from multiple targets
using weighted finite state transducers. Computer
Speech and Language, 16:533?550.
Hendrik Buschmeier, Timo Baumann, Benjamin
Dosch, Stefan Kopp, and David Schlangen. 2012.
Incremental Language Generation and Incremental
Speech Synthesis. In Proceedings of the 13th An-
nual SigDial Meeting on Discourse and Dialogue
(SIGdial), Seoul, South Korea.
Nina Dethlefs and Heriberto Cuaya?huitl. 2011a. Com-
bining Hierarchical Reinforcement Learning and
Bayesian Networks for Natural Language Genera-
tion in Situated Dialogue. In Proceedings of the 13th
European Workshop on Natural Language Genera-
tion (ENLG), Nancy, France.
Nina Dethlefs and Heriberto Cuaya?huitl. 2011b.
Hierarchical Reinforcement Learning and Hidden
Markov Models for Task-Oriented Natural Lan-
guage Generation. In Proceedings of the 49th An-
nual Meeting of the Association for Computational
Linguistics: Human Language Technologies (ACL-
HLT), Portland, Oregon, USA.
Nina Dethlefs, Helen Hastie, Verena Rieser, and Oliver
Lemon. 2012a. Optimising Incremental Dialogue
Decisions Using Information Density for Interac-
tive Systems. In Proceedings of the Conference on
Empirical Methods in Natural Language Processing
(EMNLP-CoNLL), Jeju, South Korea.
Nina Dethlefs, Helen Hastie, Verena Rieser, and Oliver
Lemon. 2012b. Optimising Incremental Generation
for Spoken Dialogue Systems: Reducing the Need
for Fillers. In Proceedings of the International Con-
ference on Natural Language Generation (INLG),
Chicago, Illinois, USA.
Kallirroi Georgila, Nikos Fakotakis, and George
Kokkinakis. 2002. Stochastic Language Modelling
for Recognition and Generation in Dialogue Sys-
tems. TAL (Traitement automatique des langues)
Journal, 43(3):129?154.
Ioannis Konstas and Mirella Lapata. 2012. Concept-
to-text Generation via Discriminative Reranking. In
Proceedings of the 50th Annual Meeting of the As-
sociation for Computational Linguistics, pages 369?
378, Jeju Island, Korea.
John D. Lafferty, Andrew McCallum, and Fer-
nando C.N. Pereira. 2001. Conditional Random
1262
Fields: Probabilistic Models for Segmenting and La-
beling Sequence Data. In Proceedings of the Eigh-
teenth International Conference on Machine Learn-
ing (ICML), pages 282?289.
Wei Lu, Hwee Tou Ng, and Wee Sun Lee. 2009.
Natural Language Generation with Tree Conditional
Random Fields. In Proceedings of the 2009 Con-
ference on Empirical Methods in Natural Language
Processing (EMNLP), Singapore.
Franc?ois Mairesse, Filip Jurc???c?ek, Simon Keizer,
Blaise Thomson, Kai Yu, and Steve Young. 2010.
Phrase-Based Statistical Language Generation Us-
ing Graphical Models and Active Learning. In Pro-
ceedings of the 48th Annual Meeting of the Associ-
ation of Computational Linguistics (ACL), Uppsala,
Sweden.
Marie-Catherine De Marneffe, Bill MacCartney, and
Christopher D. Manning. 2006. Generating Typed
Dependency Parses from Phrase Structure Parses. In
Proceedings of the Fifth International Conference
on Language Resources and Evaluation (LREC),
Genoa, Italy.
Andrew McCallum. 2002. Mallet: A machine learning
for language toolkit. http://mallet.cs.umass.edu.
Crystal Nakatsu and Michael White. 2006. Learning
to Say It Well: Reranking Realizations by Predicted
Synthesis Quality. In In Proceedings of the Annual
Meeting of the Association for Computational Lin-
guistics (COLING-ACL) 2006, pages 1113?1120,
Sydney, Australia.
Nam Nguyen and Yunsong Guo. 2007. Comparisons
of Sequence Labeling Algorithms and Extensions.
In Proceedings of the International Conference on
Machine Learning (ICML), Corvallis, OR, USA.
Alice Oh and Alexander Rudnicky. 2000. Stochas-
tic Language Generation for Spoken Dialogue Sys-
tems. In Proceedings of the ANLP/NAACL Work-
shop on Conversational Systems, pages 27?32, Seat-
tle, Washington, USA.
Matthew Purver and Masayuki Otsuka. 2003. In-
cremental Generation by Incremental Parsing. In
In Proceedings of the 6th UK Special-Interesting
Group for Computational Linguistics (CLUK) Col-
loquium.
Verena Rieser, Simon Keizer, Xingkun Liu, and Oliver
Lemon. 2011. Adaptive Information Presentation
for Spoken Dialogue Systems: Evaluation with Hu-
man Subjects. In Proceedings of the 13th Euro-
pean Workshop on Natural Language Generation
(ENLG), Nancy, France.
Sunita Sarawagi and William Cohen. 2005. Semi-
Markov Conditional Random Fields for Information
Extraction. Advances in Neural Information Pro-
cessing.
David Schlangen and Gabriel Skantze. 2009. A Gen-
eral, Abstract Model of Incremental Dialogue Pro-
cessing. In Proceedings of the 12th Conference of
the European Chapter of the Association for Com-
putational Linguistics, Athens, Greece.
Gabriel Skantze and Anna Hjalmarsson. 2010. To-
wards Incremental Speech Generation in Dialogue
Systems. In Proceedings of the 11th Annual SigDial
Meeting on Discourse and Dialogue, Tokyo, Japan.
Gabriel Skantze and David Schlangen. 2009. Incre-
mental Dialogue Processing in a Micro-Domain. In
Proceedings of the 12th Conference of the European
Chapter of the Association for Computational Lin-
guistics, Athens, Greece.
Amanda Stent, Marilyn Walker, Steve Whittaker, and
Preetam Maloor. 2002. User-tailored Generation
for Spoken Dialogue: An Experiment. In Proceed-
ings of the International Conference on Spoken Lan-
guage Processing.
Matthew Stone, Christine Doran, Bonnie Webber, To-
nia Bleam, and Martha Palmer. 2003. Microplan-
ning with Communicative Intentions: The SPUD
System. Computational Intelligence, 19:311?381.
Charles Sutton and Andrew McCallum. 2006. Intro-
duction to Conditional Random Fields for Relational
Learning. In Lise Getoor and Ben Taskar, editors,
Introduction to Statistical Relational Learning. MIT
Press.
Sebastian Varges. 2006. Overgeneration and Ranking
for Spoken Dialogue Systems. In Proceedings of the
Fourth International Natural Language Generation
Conference (INLG), Sydney, Australia.
Marilyn Walker, Amanda Stent, Franc?ois Mairesse,
and Rashmi Prasad. 2007. Individual and Do-
main Adaptation in Sentence Planning for Dia-
logue. Journal of Artificial Intelligence Research,
30(1):413?456.
1263
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 1231?1240,
Baltimore, Maryland, USA, June 23-25 2014.
c
?2014 Association for Computational Linguistics
Comparing Multi-label Classification with Reinforcement Learning for
Summarisation of Time-series Data
Dimitra Gkatzia, Helen Hastie, and Oliver Lemon
School of Mathematical and Computer Sciences, Heriot-Watt University, Edinburgh
{dg106, h.hastie, o.lemon}@hw.ac.uk
Abstract
We present a novel approach for automatic
report generation from time-series data, in
the context of student feedback genera-
tion. Our proposed methodology treats
content selection as a multi-label (ML)
classification problem, which takes as in-
put time-series data and outputs a set of
templates, while capturing the dependen-
cies between selected templates. We show
that this method generates output closer to
the feedback that lecturers actually gener-
ated, achieving 3.5% higher accuracy and
15% higher F-score than multiple simple
classifiers that keep a history of selected
templates. Furthermore, we compare a
ML classifier with a Reinforcement Learn-
ing (RL) approach in simulation and using
ratings from real student users. We show
that the different methods have different
benefits, with ML being more accurate for
predicting what was seen in the training
data, whereas RL is more exploratory and
slightly preferred by the students.
1 Introduction
Summarisation of time-series data refers to the
task of automatically generating text from vari-
ables whose values change over time. We con-
sider the task of automatically generating feed-
back summaries for students describing their per-
formance during the lab of a Computer Science
module over the semester. Students? learning can
be influenced by many variables, such as difficulty
of the material (Person et al, 1995), other dead-
lines (Craig et al, 2004), attendance in lectures
(Ames, 1992), etc. These variables have two im-
portant qualities. Firstly, they change over time,
and secondly they can be dependent on or inde-
pendent of each other. Therefore, when generating
feedback, we need to take into account all vari-
ables simultaneously in order to capture potential
dependencies and provide more effective and use-
ful feedback that is relevant to the students.
In this work, we concentrate on content selec-
tion which is the task of choosing what to say,
i.e. what information is to be included in a report
(Reiter and Dale, 2000). Content selection deci-
sions based on trends in time-series data determine
the selection of the useful and important variables,
which we refer to here as factors, that should be
conveyed in a summary. The decisions of factor
selection can be influenced by other factors that
their values are correlated with; can be based on
the appearance or absence of other factors in the
summary; and can be based on the factors? be-
haviour over time. Moreover, some factors may
have to be discussed together in order to achieve
some communicative goal, for instance, a teacher
might want to refer to student?s marks as a moti-
vation for increasing the number of hours studied.
We frame content selection as a simple classifi-
cation task: given a set of time-series data, decide
for each template whether it should be included
in a summary or not. In this paper, with the term
?template? we refer to a quadruple consisting of an
id, a factor (bottom left of Table 1), a reference
type (trend, weeks, average, other) and surface
text. However, simple classification assumes that
the templates are independent of each other, thus
the decision for each template is taken in isolation
from the others, which is not appropriate for our
domain. In order to capture the dependencies in
the context, multiple simple classifiers can make
the decisions for each template iteratively. After
each iteration, the feature space grows by 1 fea-
ture, in order to include the history of the previous
template decisions. Here, we propose an alterna-
tive method that tackles the challenge of interde-
pendent data by using multi-label (ML) classifica-
tion, which is efficient in taking data dependencies
1231
Raw Data
factors week 2 week 3 ... week 10
marks 5 4 ... 5
hours studied 1 2 ... 3
... ... ... ... ...
Trends from Data
factors trend
(1) marks (M) trend other
(2) hours studied (HS) trend increasing
(3) understandability (Und) trend decreasing
(4) difficulty (Diff) trend decreasing
(5) deadlines (DL) trend increasing
(6) health issues (HI) trend other
(7) personal issues (PI) trend decreasing
(8) lectures attended (LA) trend other
(9) revision (R) trend decreasing
Summary
Your overall performance was excellent
during the semester. Keep up the good
work and maybe try some more challeng-
ing exercises. Your attendance was vary-
ing over the semester. Have a think about
how to use time in lectures to improve your
understanding of the material. You spent 2
hours studying the lecture material on
average. You should dedicate more time
to study. You seem to find the material
easier to understand compared to the
beginning of the semester. Keep up the
good work! You revised part of the learn-
ing material. Have a think whether revis-
ing has improved your performance.
Table 1: The table on the top left shows an example of the time-series raw data for feedback generation.
The table on the bottom left shows an example of described trends. The box on the right presents a target
summary (target summaries have been constructed by teaching staff).
into account and generating a set of labels (in our
case templates) simultaneously (Tsoumakas et al,
2010). ML classification requires no history, i.e.
does not keep track of previous decisions, and thus
has a smaller feature space.
Our contributions to the field are as follows: we
present a novel and efficient method for tackling
the challenge of content selection using a ML clas-
sification approach; we applied this method to the
domain of feedback summarisation; we present a
comparison with an optimisation technique (Rein-
forcement Learning), and we discuss the similari-
ties and differences between the two methods.
In the next section, we refer to the related work
on Natural Language Generation from time-series
data and on Content Selection. In Section 4.2, we
describe our approach and we carry out a compar-
ison with simple classification methods. In Sec-
tion 5, we present the evaluation setup and in Sec-
tion 6 we discuss the results, obtained in simula-
tion and with real students. Finally, in Section 8,
directions for future work are discussed.
2 Related Work
Natural Language Generation from time-series
data has been investigated for various tasks such
as weather forecast generation (Belz and Kow,
2010; Angeli et al, 2010; Sripada et al, 2004),
report generation from clinical data (Hunter et al,
2011; Gatt et al, 2009), narrative to assist children
with communication needs (Black et al, 2010) and
audiovisual debrief generation from sensor data
from Autonomous Underwater Vehicles missions
(Johnson and Lane, 2011).
The important tasks of time-series data sum-
marisation systems are content selection (what to
say), surface realisation (how to say it) and infor-
mation presentation (Document Planning, Order-
ing, etc.). In this work, we concentrate on content
selection. Previous methods for content selection
include Reinforcement Learning (Rieser et al,
2010); multi-objective optimisation (Gkatzia et
al., 2014); Gricean Maxims (Sripada et al, 2003);
Integer Linear Programming (Lampouras and An-
droutsopoulos, 2013); collective content selection
(Barzilay and Lapata, 2004); interest scores as-
signed to content (Androutsopoulos et al, 2013); a
combination of statistical and template-based ap-
proaches to NLG (Kondadadi et al, 2013); statis-
tical acquisition of rules (Duboue and McKeown,
2003) and the Hidden Markov model approach for
Content Selection and ordering (Barzilay and Lee,
2004).
Collective content selection (Barzilay and La-
pata, 2004) is similar to our proposed method in
that it is a classification task that predicts the tem-
plates from the same instance simultaneously. The
difference between the two methods lies in that the
1232
collective content selection requires the considera-
tion of an individual preference score (which is de-
fined as the preference of the entity to be selected
or omitted, and it is based on the values of entity
attributes and is computed using a boosting algo-
rithm) and the identification of links between the
entities with similar labels. In contrast, ML clas-
sification does not need the computation of links
between the data and the templates. ML classi-
fication can also apply to other problems whose
features are correlated, such as text classification
(Madjarov et al, 2012), when an aligned dataset is
provided.
ML classification algorithms have been divided
into three categories: algorithm adaptation meth-
ods, problem transformation and ensemble meth-
ods (Tsoumakas and Katakis, 2007; Madjarov
et al, 2012). Algorithm adaptation approaches
(Tsoumakas et al, 2010) extend simple classifi-
cation methods to handle ML data. For exam-
ple, the k-nearest neighbour algorithm is extended
to ML-kNN by Zhang and Zhou (2007). ML-
kNN identifies for each new instance its k nearest
neighbours in the training set and then it predicts
the label set by utilising the maximum a posteri-
ori principle according to statistical information
derived from the label sets of the k neighbours.
Problem transformation approaches (Tsoumakas
and Katakis, 2007) transform the ML classifica-
tion task into one or more simple classification
tasks. Ensemble methods (Tsoumakas et al, 2010)
are algorithms that use ensembles to perform ML
learning and they are based on problem transfor-
mation or algorithm adaptation methods. In this
paper, we applied RAkEL (Random k-labelsets)
(Tsoumakas et al, 2010): an ensemble problem
transformation method, which constructs an en-
semble of simple-label classifiers, where each one
deals with a random subset of the labels.
Finally, our domain for feedback generation is
motivated by previous studies (Law et al, 2005;
van den Meulen et al, 2010) who show that text
summaries are more effective in decision making
than graphs therefore it is advantageous to provide
a summary over showing users the raw data graph-
ically. In addition, feedback summarisation from
time-series data can be applied to the field of In-
telligent Tutoring Systems (Gross et al, 2012).
3 Data
The dataset consists of 37 instances referring to
the activities of 26 students. For a few students
there is more than 1 instance. An example of one
such instance is presented in Table 1. Each in-
stance includes time-series information about the
student?s learning habits and the selected tem-
plates that lecturers used to provide feedback to
this student. The time-series information includes
for each week of the semester: (1) the marks
achieved at the lab; (2) the hours that the stu-
dent spent studying; (3) the understandability of
the material; (4) the difficulty of the lab exercises
as assessed by the student; (5) the number of other
deadlines that the student had that week; (6) health
issues; (7) personal issues; (8) the number of lec-
tures attended; and (9) the amount of revision that
the student had performed. The templates describe
these factors in four different ways:
1. <trend>: referring to the trend of a fac-
tor over the semester (e.g. ?Your performance
was increasing...?),
2. <weeks>: explicitly describing the factor
value at specific weeks (e.g. ?In weeks 2, 3
and 9...?),
3. <average>: considering the average of a
factor value (e.g. ?You dedicated 1.5 hours
studying on average...?), and
4. <other>: mentioning other relevant infor-
mation (e.g. ?Revising material will improve
your performance?).
For the corpus creation, 11 lecturers selected the
content to be conveyed in a summary, given the
set of raw data (Gkatzia et al, 2013). As a result,
for the same student there are various summaries
provided by the different experts. This character-
istic of the dataset, that each instance is associated
with more than one solution, additionally moti-
vates the use of multi-label classification, which
is concerned with learning from examples, where
each example is associated with multiple labels.
Our analysis of the dataset showed that there
are significant correlations between the factors, for
example, the number of lectures attended (LA)
correlates with the student?s understanding of the
material (Und), see Table 2. As we will discuss
further in Section 5.1, content decisions are in-
fluenced by the previously generated content, for
example, if the lecturer has previously mentioned
health issues, mentioning hours studied has a high
probability of also being mentioned.
1233
Factor (1) M (2) HS (3) Und (4) Diff (5) DL (6) HI (7) PI (8) LA (9) R
(1) M 1* 0.52* 0.44* -0.53* -0.31 -0.30 -0.36* 0.44* 0.16
(2) HS 0.52* 1* 0.23 -0.09 -0.11 0.11 -0.29 0.32 0.47*
(3) Und 0.44* 0.23 1* -0.54* 0.03 -0.26 0.12 0.60* 0.32
(4) Diff -0.53* -0.09 -0.54* 1* 0.16 -0.06 0.03 -0.19 0.14
(5) DL -0.31 -0.11 0.03 0.16 1* 0.26 0.24 -0.44* 0.14
(6) HI -0.30 -0.11 -0.26 -0.06 0.26 1* 0.27 -0.50* 0.15
(7) PI -0.36* -0.29 0.12 0.03 0.24 0.27 1* -0.46* 0.34*
(8) LA 0.44* 0.32 0.60* -0.19 -0.44* -0.50* -0.46* 1* -0.12
(9) R 0.16 0.47* 0.03 0.14 0.14 0.15 0.34* -0.12 1*
Table 2: The table presents the Pearson?s correlation coefficients of the factors (* means p<0.05).
4 Methodology
In this section, the content selection task and the
suggested multi-label classification approach are
presented. The development and evaluation of the
time-series generation system follows the follow-
ing pipeline (Gkatzia et al, 2013):
1. Time-Series data collection from students
2. Template construction by Learning and
Teaching (L&T) expert
3. Feedback summaries constructed by lectur-
ers; random summaries rated by lecturers
4. Development of time-series generation sys-
tems (Section 4.2, Section 5.3): ML system,
RL system, Rule-based and Random system
5. Evaluation: (Section 5)
- Offline evaluation (Accuracy and Reward)
- Online evaluation (Subjective Ratings)
4.1 The Content Selection Task
Our learning task is formed as follows: given a
set of 9 time-series factors, select the content that
is most appropriate to be included in a summary.
Content is regarded as labels (each template rep-
resents a label) and thus the task can be thought of
as a classification problem. As mentioned, there
are 4 ways to refer to a factor: (1) describing the
trend, (2) describing what happened in every time
stamp, (3) mentioning the average and (4) making
another general statement. Overall, for all factors
there are 29 different templates
1
. An example of
the input data is shown in Table 1. There are two
decisions that need to be made: (1) whether to talk
about a factor and (2) in which way to refer to it.
Instead of dealing with this task in a hierarchical
way, where the algorithm will first learn whether
to talk about a factor and then to decide how to
1
There are fewer than 36 templates, because for some fac-
tors there are less than 4 possible ways of referring to them.
refer to it, we transformed the task in order to re-
duce the learning steps. Therefore, classification
can reduce the decision workload by deciding ei-
ther in which way to talk about it, or not to talk
about a factor at all.
4.2 The Multi-label Classification Approach
Traditional single-label classification is the task of
identifying which label one new observation is as-
sociated with, by choosing from a set of labels L
(Tsoumakas et al, 2010). Multi-label classifica-
tion is the task of associating an observation with
a set of labels Y ? L (Tsoumakas et al, 2010).
One set of factor values can result in various
sets of templates as interpreted by the different
experts. A ML classifier is able to make deci-
sions for all templates simultaneously and cap-
ture these differences. The RAndom k-labELsets
(RAkEL) (Tsoumakas et al, 2010) was applied
in order to perform ML classification. RAkEL is
based on Label Powerset (LP), a problem transfor-
mation method (Tsoumakas et al, 2010). LP ben-
efits from taking into consideration label correla-
tions, but does not perform well when trained with
few examples as in our case (Tsoumakas et al,
2010). RAkEL overcomes this limitation by con-
structing a set of LP classifiers, which are trained
with different random subsets of the set of labels
(Tsoumakas et al, 2010).
The LP method transforms the ML task, into
one single-label multi-class classification task,
where the possible set of predicted variables for
the transformed class is the powerset of labels
present in the original dataset. For instance, the set
of labels L = {temp
0
, temp
1
, ...temp
28
} could be
transformed to {temp
0,1,2
, temp
28,3,17,
...}. This
algorithm does not perform well when consider-
ing a large number of labels, due to the fact that
the label space grows exponentially (Tsoumakas
1234
Classifier Accuracy Precision Recall F score
(10-fold)
Decision Tree (no history) *75.95% 67.56 75.96 67.87
Decision Tree (with predicted history) **73.43% 65.49 72.05 70.95
Decision Tree (with real history) **78.09% 74.51 78.11 75.54
Majority-class (single label) **72.02% 61.73 77.37 68.21
RAkEL (multi-label) (no history) 76.95% 85.08 85.94 85.50
Table 3: Average, precision, recall and F-score of the different classification methods (T-test, * denotes
significance with p<0.05 and ** significance with p<0.01, when comparing each result to RAkEL).
et al, 2010). RAkEL tackles this problem by con-
structing an ensemble of LP classifiers and train-
ing each one on a different random subset of the
set of labels (Tsoumakas et al, 2010).
4.2.1 The Production Phase of RAkEL
The algorithm was implemented using the MU-
LAN Open Source Java library (Tsoumakas et
al., 2011), which is based on WEKA (Witten and
Frank, 2005). The algorithm works in two phases:
1. the production of an ensemble of LP algo-
rithms, and
2. the combination of the LP algorithms.
RAkEL takes as input the following parameters:
(1) the numbers of iterations m (which is devel-
oper specified and denotes the number of models
that the algorithm will produce), (2) the size of la-
belset k (which is also developer specified), (3) the
set of labels L, and (4) the training set D. During
the initial phase it outputs an ensemble of LP clas-
sifiers and the corresponding k-labelsets. A pseu-
docode for the production phase is shown below:
Algorithm 1 RAkEL production phase
1 : I n p u t : i t e r a t i o n s m, k l a b e l s e t s ,
l a b e l s L , t r a i n i n g d a t a D
2 : f o r i =0 t o m
3 : S e l e c t random k? l a b e l s e t from L
4 : T r a i n an LP on D
5 : Add LP t o ensemble
6 : end f o r
7 : Outpu t : t h e ensemble o f LPs
wi th c o r r e s p o n d i n g k? l a b e l s e t s
4.2.2 The Combination Phase
During the combination phase, the algorithm takes
as input the results of the production phase, i.e.
the ensemble of LPs with the corresponding k-
labelsets, the set of labels L, and the new instance
x and it outputs the result vector of predicted la-
bels for instance x. During run time, RAkEL es-
timates the average decision for each label in L
and if the average is greater than a threshold t (de-
termined by the developer) it includes the label in
the predicted labelset. We used the standard pa-
rameter values of t, k and m (t = 0.5, k = 3 and
m equals to 58 (2*29 templates)). In future, we
could perform parameter optimisation by using a
technique similar to (Gabsdil and Lemon, 2004).
5 Evaluation
Firstly, we performed a preliminary evaluation on
classification methods, comparing our proposed
ML classification with multiple iterated classifica-
tion approaches. The summaries generated by the
ML classification system are then compared with
the output of a RL system and two baseline sys-
tems in simulation and with real students.
5.1 Comparison with Simple Classification
We compared the RAkEL algorithm with single-
label (SL) classification. Different SL classifiers
were trained using WEKA: JRip, Decision Trees,
Naive Bayes, k-nearest neighbour, logistic regres-
sion, multi-layer perceptron and support vector
machines. It was found out that Decision Trees
achieved on average 3% higher accuracy. We,
therefore, went on to use Decision Trees that use
generation history in three ways.
Firstly, for Decision Tree (no history), 29
decision-tree classifiers were trained, one for each
template. The input of these classifiers were the
9 factors and each classifier was trained in order
to decide whether to include a specific template or
not. This method did not take into account other
selected templates ? it was only based on the time-
series data.
Secondly, for Decision Tree (with predicted
history), 29 classifiers were also trained, but this
time the input included the previous decisions
made by the previous classifiers (i.e. the history)
1235
as well as the set of time-series data in order to
emulate the dependencies in the dataset. For in-
stance, classifier n was trained using the data from
the 9 factors and the template decisions for tem-
plates 0 to n? 1.
Thirdly, for Decision Tree (with real his-
tory), the real, expert values were used rather
than the predicted ones in the history. The
above-mentioned classifiers are compared with,
the Majority-class (single label) baseline, which
labels each instance with the most frequent tem-
plate.
The accuracy, the weighted precision, the
weighted recall, and the weighted F-score of the
classifiers are shown in Table 3. It was found that
in 10-fold cross validation RAkEL performs sig-
nificantly better in all these automatic measures
(accuracy = 76.95%, F-score = 85.50%). Remark-
ably, ML achieves more than 10% higher F-score
than the other methods (Table 3). The average
accuracy of the single-label classifiers is 75.95%
(10-fold validation), compared to 73.43% of clas-
sification with history. The reduced accuracy of
the classification with predicted history is due to
the error in the predicted values. In this method,
at every step, the predicted outcome was used in-
cluding the incorrect decisions that the classifier
made. The upper-bound accuracy is 78.09% cal-
culated by using the expert previous decisions and
not the potentially erroneous predicted decisions.
This result is indicative of the significance of the
relations between the factors showing that the pre-
dicted decisions are dependent due to existing cor-
relations as discussed in Section 1, therefore the
system should not take these decisions indepen-
dently. ML classification performs better because
it does take into account these correlations and de-
pendencies in the data.
5.2 The Reinforcement Learning System
Reinforcement Learning (RL) is a machine learn-
ing technique that defines how an agent learns to
take optimal actions so as to maximise a cumu-
lative reward (Sutton and Barto, 1998). Content
selection is seen as a Markov Decision problem
and the goal of the agent is to learn to take the se-
quence of actions that leads to optimal content se-
lection. The Temporal Difference learning method
was used to train an agent for content selection.
Actions and States: The state consists of the
time-series data and the selected templates. In or-
der to explore the state space the agent selects a
factor (e.g. marks, deadlines etc.) and then decides
whether to talk about it or not.
Reward Function: The reward function reflects
the lecturers? preferences on summaries and is
derived through linear regression analysis of a
dataset containing lecturer constructed summaries
and ratings of randomly generated summaries.
Specifically, it is the following cumulative multi-
variate function:
Reward = a+
n
?
i=1
b
i
? x
i
+ c ? length
where X = {x
1
, x
2
, ..., x
n
} describes the com-
binations of the data trends observed in the time-
series data and a particular template. a, b and c are
the regression coefficients, and their values vary
from -99 to 221. The value of x
i
is given by the
function:
x
i
=
?
?
?
?
?
?
?
?
?
?
?
1, the combination of a factor trend
and a template type is included
in a summary
0, if not.
The RL system differs from the classification
system in the way it performs content selection.
In the training phase, the agent selects a factor and
then decides whether to talk about it or not. If the
agent decides to refer to a factor, the template is
selected in a deterministic way, i.e. from the avail-
able templates it selects the template that results in
higher expected cumulative future reward.
5.3 The Baseline Systems
We compared the ML system and the RL system
with two baselines described below by measuring
the accuracy of their outputs, the reward achieved
by the reward function used for the RL system,
and finally we also performed evaluation with stu-
dent users. In order to reduce the confounding
variables, we kept the ordering of content in all
systems the same, by adopting the ordering of the
rule-based system. The baselines are as follows:
1. Rule-based System: generates summaries
based on Content Selection rules derived by work-
ing with a L&T expert and a student (Gkatzia et
al., 2013).
2. Random System: initially, selects a factor
randomly and then selects a template randomly,
until it makes decisions for all factors.
1236
Time-Series Accuracy Reward Rating Mode (mean) Data Source
Summarisation Systems
Multi-label Classification 85% 65.4 7 (6.24) Lecturers? constructed summaries
Reinforcement Learning **66% 243.82 8 (6.54) Lecturers? ratings & summaries
Rule-based **65% 107.77 7, 8 (5.86) L&T expert
Random **45.2% 43.29 *2 (*4.37) Random
Table 4: Accuracy, average rewards (based on lecturers? preferences) and averages of the means of the
student ratings. Accuracy significance (Z-test) with RAkEL at p<0.05 is indicated as * and at p<0.01
as **. Student ratings significance (Mann Whitney U test) with RAkEL at p<0.05 is indicated as *.
6 Results
Each of the four systems described above gener-
ated 26 feedback summaries corresponding to the
26 student profiles. These summaries were evalu-
ated in simulation and with real student users.
6.1 Results in Simulation
Table 4 presents the accuracy, reward, and mode
of student rating of each algorithm when used to
generate the 26 summaries. Accuracy was esti-
mated as the proportion of the correctly classified
templates to the population of templates. In or-
der to have a more objective view on the results,
the score achieved by each algorithm using the
reward function was also calculated. ML clas-
sification achieved significantly higher accuracy,
which was expected as it is a supervised learning
method. The rule-based system and the RL sys-
tem have lower accuracy compared to the ML sys-
tem. There is evidently a mismatch between the
rules and the test-set; the content selection rules
are based on heuristics provided by a L&T Expert
rather than by the same pool of lecturers that cre-
ated the test-set. On the contrary, the RL is trained
to optimise the selected content and not to repli-
cate the existing lecturer summaries, hence there
is a difference in accuracy.
Accuracy measures how similar the generated
output is to the gold standard, whereas the reward
function calculates a score regarding how good
the output is, given an objective function. RL is
trained to optimise for this function, and therefore
it achieves higher reward, whereas ML is trained
to learn by examples, therefore it produces out-
put closer to the gold standard (lecturer?s produced
summaries). RL uses exploration and exploitation
to discover combinations of content that result in
higher reward. The reward represents predicted
ratings that lecturers would give to the summary.
The reward for the lecturers? produced summaries
is 124.62 and for the ML method is 107.77. The
ML classification system performed worse than
this gold standard in terms of reward, which is ex-
pected given the error in predictions (supervised
methods learn to reproduce the gold standard).
Moreover, each decision is rewarded with a dif-
ferent value as some combinations of factors and
templates have greater or negative regression coef-
ficients. For instance, the combination of the fac-
tors ?deadlines? and the template that corresponds
to <weeks> is rewarded with 57. On the other
hand, when mentioning the <average> difficulty
the summary is ?punished? with -81 (see descrip-
tion of the reward function in Section 5.2). Conse-
quently, a single poor decision in the ML classifi-
cation can result in much less reward.
6.2 Subjective Results with Students
37 first year computer science students partici-
pated in the study. Each participant was shown
a graphical representation of the time-series data
of one student and four different summaries gen-
erated by the four systems (see Figure 1). The or-
der of the presented summaries was randomised.
They were asked to rate each feedback summary
on a 10-point rating scale in response to the fol-
lowing statement: ?Imagine you are the following
student. How would you evaluate the following
feedback summaries from 1 to 10??, where 10 cor-
responds to the most preferred summary and 1 to
the least preferred.
The difference in ratings between the ML clas-
sification system, the RL system and the Rule-
based system is not significant (see Mode (mean)
in Table 4, p>0.05). However, there is a trend to-
wards the RL system. The classification method
reduces the generation steps, by making the de-
cision of the factor selection and the template se-
lection jointly. Moreover, the training time for the
classification method is faster (a couple of seconds
compared to over an hour). Finally, the student
1237
Figure 1: The Figure show the evaluation setup. Students were presenting with the data in a graphical
way and then they were asked to evaluate each summary in a 10-point Rating scale. Summaries displayed
from left to right: ML system, RL, rule-based and random.
significantly prefer all the systems over the ran-
dom.
7 Summary
We have shown that ML classification for sum-
marisation of our time-series data has an accuracy
of 76.95% and that this approach significantly out-
performs other classification methods as it is able
to capture dependencies in the data when mak-
ing content selection decisions. ML classifica-
tion was also directly compared to a RL method.
It was found that although ML classification is
almost 20% more accurate than RL, both meth-
ods perform comparably when rated by humans.
This may be due to the fact that the RL optimi-
sation method is able to provide more varied re-
sponses over time rather than just emulating the
training data as with standard supervised learn-
ing approaches. Foster (2008) found similar re-
sults when performing a study on generation of
emphatic facial displays. A previous study by
Belz and Reiter (2006) has demonstrated that au-
tomatic metrics can correlate highly with human
ratings if the training dataset is of high quality.
In our study, the human ratings correlate well to
the average scores achieved by the reward func-
tion. However, the human ratings do not correlate
well to the accuracy scores. It is interesting that
the two methods that score differently on various
automatic metrics, such as accuracy, reward, pre-
cision, recall and F-score, are evaluated similarly
by users.
The comparison shows that each method can
serve different goals. Multi-label classification
generates output closer to gold standard whereas
RL can optimise the output according to a reward
function. ML classification could be used when
the goal of the generation is to replicate phenom-
ena seen in the dataset, because it achieves high
accuracy, precision and recall. However, opti-
misation methods can be more flexible, provide
more varied output and can be trained for different
goals, e.g. for capturing preferences of different
users.
1238
8 Future Work
For this initial experiment, we evaluated with stu-
dents and not with lecturers, since the students are
the recipients of feedback. In future, we plan to
evaluate with students? own data under real cir-
cumstances as well as with ratings from lecturers.
Moreover, we plan to utilise the results from this
student evaluation in order to train an optimisation
algorithm to perform summarisation according to
students? preferences. In this case, optimisation
would be the preferred method as it would not be
appropriate to collect gold standard data from stu-
dents. In fact, it would be of interest to investi-
gate multi-objective optimisation techniques that
can balance the needs of the lecturers to convey
important content to the satisfaction of students.
9 Acknowledgements
The research leading to this work has re-
ceived funding from the EC?s FP7 programme:
(FP7/2011-14) under grant agreement no. 248765
(Help4Mood).
References
Carole Ames. 1992. Classrooms: Goals, structures,
and student motivation. Journal of Educational Psy-
chology, 84(3):261?71.
Ion Androutsopoulos, Gerasimos Lampouras, and
Dimitrios Galanis. 2013. Generating natural lan-
guage descriptions from owl ontologies: the nat-
ural owl system. Atrificial Intelligence Research,
48:671?715.
Gabor Angeli, Percy Liang, and Dan Klein. 2010. A
simple domain-independent probabilistic approach
to generation. In Conference on Empirical Methods
in Natural Language Processing (EMNLP).
Regina Barzilay and Mirella Lapata. 2004. Collec-
tive content selection for concept-to-text generation.
In Conference on Human Language Technology and
Empirical Methods in Natural Language Processing
(HLT-EMNLP).
Regina Barzilay and Lillian Lee. 2004. Catching the
drift: Probabilistic content models, with applications
to generation and summarization. In Human Lan-
guage Technology Conference of the North Ameri-
can Chapter of the Association for Computational
Linguistics (HLT-NAACL).
Anja Belz and Eric Kow. 2010. Extracting parallel
fragments from comparable corpora for data-to-text
generation. In 6th International Natural Language
Generation Conference (INLG).
Anja Belz and Ehud Reiter. 2006. Comparing auto-
matic and human evaluation of nlg systems. In 11th
Conference of the European Chapter of the Associa-
tion for Computational Linguistics (ACL).
Rolf Black, Joe Reddington, Ehud Reiter, Nava
Tintarev, and Annalu Waller. 2010. Using NLG and
sensors to support personal narrative for children
with complex communication needs. In NAACL
HLT 2010 Workshop on Speech and Language Pro-
cessing for Assistive Technologies.
Scotty D. Craig, Arthur C. Graesser, Jeremiah Sullins,
and Barry Gholson. 2004. Affect and learning:
an exploratory look into the role of affect in learn-
ing with autotutor. Journal of Educational Media,
29:241?250.
Pable Duboue and K.R. McKeown. 2003. Statistical
acquisition of content selection rules for natural lan-
guage generation. In Conference on Human Lan-
guage Technology and Empirical Methods in Natu-
ral Language Processing (EMNLP).
Mary Ellen Foster. 2008. Automated metrics that
agree with human judgements on generated output
for an embodied conversational agent. In 5th Inter-
national Natural Language Generation Conference
(INLG).
Malte Gabsdil and Oliver Lemon. 2004. Combining
acoustic and pragmatic features to predict recogni-
tion performance in spoken dialogue systems. In
42nd Annual Meeting of the Association for Com-
putational Linguistics (ACL).
Albert Gatt, Francois Portet, Ehud Reiter, James
Hunter, Saad Mahamood, Wendy Moncur, and So-
mayajulu Sripada. 2009. From data to text in the
neonatal intensive care unit: Using NLG technology
for decision support and information management.
AI Communications, 22: 153-186.
Dimitra Gkatzia, Helen Hastie, Srinivasan Ja-
narthanam, and Oliver Lemon. 2013. Generating
student feedback from time-series data using Rein-
forcement Learning. In 14th European Workshop in
Natural Language Generation (ENLG).
Dimitra Gkatzia, Helen Hastie, and Oliver Lemon.
2014. Finding Middle Ground? Multi-objective
Natural Language Generation from time-series data.
In 14th Conference of the European Chapter of the
Association for Computational Linguistics (EACL)
(to appear).
Sebastian Gross, Bassam Mokbel, Barbara Hammer,
and Niels Pinkwart. 2012. Feedback provision
strategies in intelligent tutoring systems based on
clustered solution spaces. In J. Desel, J. M. Haake,
and C. Spannagel, editors, Tagungsband der 10. e-
Learning Fachtagung Informatik (DeLFI), number
P-207 in GI Lecture Notes in Informatics, pages 27?
38. GI.
1239
Jim Hunter, Yvonne Freer, Albert Gatt, Yaji Sripada,
Cindy Sykes, and D Westwater. 2011. Bt-nurse:
Computer generation of natural language shift sum-
maries from complex heterogeneous medical data.
American Medical Informatics Association, 18:621-
624.
Nicholas Johnson and David Lane. 2011. Narrative
monologue as a first step towards advanced mis-
sion debrief for AUV operator situational aware-
ness. In 15th International Conference on Advanced
Robotics.
Ravi Kondadadi, Blake Howald, and Frank Schilder.
2013. A statistical nlg framework for aggregated
planning and realization. In 51st Annual Meet-
ing of the Association for Computational Linguistics
(ACL).
Gerasimos Lampouras and Ion Androutsopoulos.
2013. Using integer linear programming in concept-
to-text generation to produce more compact texts. In
51st Annual Meeting of the Association for Compu-
tational Linguistics (ACL).
Anna S. Law, Yvonne Freer, Jim Hunter, Robert H.
Logie, Neil McIntosh, and John Quinn. 2005. A
comparison of graphical and textual presentations of
time series data to support medical decision making
in the neonatal intensive care unit. Journal of Clini-
cal Monitoring and Computing, pages 19: 183?194.
Gjorgji Madjarov, Dragi Kocev, Dejan Gjorgjevikj, and
Saso Dzeroski. 2012. An extensive experimen-
tal comparison of methods for multi-label learning.
Pattern Recognition, 45(9):3084?3104.
Natalie K. Person, Roger J. Kreuz, Rolf A. Zwaan, and
Arthur C. Graesser. 1995. Pragmatics and peda-
gogy: Conversational rules and politeness strategies
may inhibit effective tutoring. Journal of Cognition
and Instruction, 13(2):161-188.
Ehud Reiter and Robert Dale. 2000. Building natu-
ral language generation systems. Cambridge Uni-
versity Press.
Verena Rieser, Oliver Lemon, and Xingkun Liu. 2010.
Optimising information presentation for spoken dia-
logue systems. In 48th Annual Meeting of the Asso-
ciation for Computational Linguistics (ACL).
Somayajulu Sripada, Ehud Reiter, Jim Hunter, and Jin
Yu. 2003. Generating english summaries of time se-
ries data using the gricean maxims. In 9th ACM in-
ternational conference on Knowledge discovery and
data mining (SIGKDD).
Somayajulu Sripada, Ehud Reiter, I Davy, and
K Nilssen. 2004. Lessons from deploying NLG
technology for marine weather forecast text gener-
ation. In PAIS session of ECAI-2004:760-764.
Richart Sutton and Andrew Barto. 1998. Reinforce-
ment learning. MIT Press.
Grigorios Tsoumakas and Ioannis Katakis. 2007.
Multi-label classification: An overview. Inter-
national Journal Data Warehousing and Mining,
3(3):1?13.
Grigorios Tsoumakas, Ioannis Katakis, and Ioannis
Vlahavas. 2010. Random k-labelsets for multi-
label classification. IEEE Transactions on Knowl-
edge and Data Engineering, 99(1):1079?1089.
Grigorios Tsoumakas, Eleftherios Spyromitros-
Xioufis, Josef Vilcek, and Ioannis Vlahavas.
2011. Mulan: A java library for multi-label
learning. Journal of Machine Learning Research,
12(1):2411?2414.
Marian van den Meulen, Robert Logie, Yvonne Freer,
Cindy Sykes, Neil McIntosh, and Jim Hunter. 2010.
When a graph is poorer than 100 words: A com-
parison of computerised natural language genera-
tion, human generated descriptions and graphical
displays in neonatal intensive care. In Applied Cog-
nitive Psychology, 24: 77-89.
Ian Witten and Eibe Frank. 2005. Data mining: Practi-
cal machine learning tools and techniques. Morgan
Kaufmann Publishers.
Min-Ling Zhang and Zhi-Hua Zhou. 2007. Ml-knn: A
lazy learning approach to multi-label learning. Pat-
tern Recognition, 40(7):2038?2048.
1240
Proceedings of the SIGDIAL 2011: the 12th Annual Meeting of the Special Interest Group on Discourse and Dialogue, pages 2?7,
Portland, Oregon, June 17-18, 2011. c?2011 Association for Computational Linguistics
Spoken Dialog Challenge 2010: 
 Comparison of Live and Control Test Results 
Alan W Black1, Susanne Burger1, Alistair Conkie4, Helen Hastie2, Simon Keizer3,  Oliver 
Lemon2, Nicolas Merigaud2, Gabriel Parent1, Gabriel Schubiner1, Blaise Thomson3, Jason 
D. Williams4, Kai Yu3, Steve Young3 and Maxine Eskenazi1 
1Language Technologies Institute, Carnegie Mellon University, Pittsburgh, USA 
2Dept of Mathematical and Computer Science, Heriot-Watt University, Edinburgh, UK 
3Engineering Department, Cambridge University, Cambridge, UK 
4AT&T Labs ? Research, Florham Park, NJ, USA 
awb@cs.cmu.edu 
 
 
Abstract 
The Spoken Dialog Challenge 2010 was an 
exercise to investigate how different spo-
ken dialog systems perform on the same 
task.  The existing Let?s Go Pittsburgh Bus 
Information System was used as a task and 
four teams provided systems that were first 
tested in controlled conditions with speech 
researchers as users. The three most stable 
systems were then deployed to real callers.  
This paper presents the results of the live 
tests, and compares them with the control 
test results. Results show considerable 
variation both between systems and be-
tween the control and live tests.  Interest-
ingly, relatively high task completion for 
controlled tests did not always predict 
relatively high task completion for live 
tests.  Moreover, even though the systems 
were quite different in their designs, we 
saw very similar correlations between word 
error rate and task completion for all the 
systems.  The dialog data collected is 
available to the research community. 
1 Background 
The goal of the Spoken Dialog Challenge (SDC) is 
to investigate how different dialog systems per-
form on a similar task.  It is designed as a regularly 
recurring challenge. The first one took place in 
2010. SDC participants were to provide one or 
more of three things: a system; a simulated user, 
and/or an evaluation metric.   The task chosen for 
the first SDC was one that already had a large 
number of real callers. This had several advan-
tages. First, there was a system that had been used 
by many callers. Second, there was a substantial 
dataset that participants could use to train their sys-
tems.  Finally, there were real callers, rather than 
only lab testers.  Past work has found systems 
which appear to perform well in lab tests do not 
always perform well when deployed to real callers, 
in part because real callers behave differently than 
lab testers, and usage conditions can be considera-
bly different [Raux et al2005, Ai et al2008].  De-
ploying systems to real users is an important trait 
of the Spoken Dialog Challenge. 
The CMU Let?s Go Bus Information system 
[Raux et al2006] provides bus schedule informa-
tion for the general population of Pittsburgh.  It is 
directly connected to the local Port Authority, 
whose evening calls for bus information are redi-
rected to the automated system.  The system has 
been running since March 2005 and has served 
over 130K calls. 
The software and the previous years of dialog 
data were released to participants of the challenge 
to allow them to construct their own systems.  A 
number of sites started the challenge, and four sites 
successfully built systems, including the original 
CMU system. 
An important aspect of the challenge is that 
the quality of service to the end users (people in 
Pittsburgh) had to be maintained and thus an initial 
robustness and quality test was carried out on con-
tributed systems.  This control test provided sce-
narios over a web interface and required 
researchers from the participating sites to call each 
of the systems.  The results of this control test were 
published in [Black et al 2010] and by the individ-
ual participants [Williams et al 2010, Thomson et 
al. 2010, Hastie et al 2010] and they are repro-
2
duced below to give the reader a comparison with 
the later live tests. 
Important distinctions between the control 
test callers and the live test callers were that the 
control test callers were primarily spoken dialog 
researchers from around the world.  Although they 
were usually calling from more controlled acoustic 
conditions, most were not knowledgeable about 
Pittsburgh geography.     
As mentioned above, four systems took part 
in the SDC.  Following the practice of other chal-
lenges, we will not explicitly identify the sites 
where these systems were developed. We simply 
refer to them as SYS1-4 in the results.  We will, 
however, state that one of the systems is the system 
that has been running for this task for several 
years. The architectures of the systems cover a 
number of different techniques for building spoken 
dialog systems, including agenda based systems, 
VoiceXML and statistical techniques. 
2 Conditions of Control and Live tests 
For this task, the caller needs to provide the depar-
ture stop, the arrival stop and the time of departure 
or arrival in order for the system to be able to per-
form a lookup in the schedule database. The route 
number can also be provided and used in the 
lookup, but it is not necessary. The present live 
system covers the East End of Pittsburgh.  Al-
though the Port Authority message states that other 
areas are not covered, callers may still ask for 
routes that are not in the East End; in this case, the 
live system must say it doesn?t have information 
available.  Some events that affect the length of the 
dialog include whether the system uses implicit or 
explicit confirmation or some combination of both, 
whether the system has an open-ended first turn or 
a directed one, and whether it deals with requests 
for the previous and/or following bus (this latter 
should have been present in all of the systems). 
Just before the SDC started, the Port Author-
ity had removed some of its bus routes. The sys-
tems were required to be capable of informing the 
caller that the route had been canceled, and then 
giving them a suitable alternative. 
SDC systems answer live calls when the Port 
Authority call center is closed in the evening and 
early morning.  There are quite different types and 
volumes of calls over the different days of the 
week.  Weekend days typically have more calls, in 
part because the call center is open fewer hours on 
weekends.  Figure 1 shows a histogram of average 
calls per hour for the evening and the early morn-
ing of each day of the week. 
 
calls per weekday / ave per hour
0
1
2
3
4
5
6
7
8
9
10
Fr-
19-
0
Sa-
0-8
Sa-
16
-
0
Su-
0-8
Su-
16
-
0
Mo
-
0-7
Mo
-
19
-
0
Tu
-
0-7
Tu
-
19-
0
We
-
0-7
We
-
19
-
0
Th
-
0-7
Th
-
19-
0
Fr-
0-7
 
Figure 1: average number of calls per hour on weekends 
(dark bars) and weekdays. Listed are names of days and 
times before and after midnight when callers called the 
system. 
 
The control tests were set up through a simple 
web interface that presented 8 different scenarios 
to callers. Callers were given a phone number to 
call; each caller spoke to each of the 4 different 
systems twice.  A typical scenario was presented 
with few words, mainly relying on graphics in or-
der to avoid influencing the caller?s choice of vo-
cabulary.  An example is shown in Figure 2. 
 
 
 
Figure 2: Typical scenario for the control tests.  This 
example requests that the user find a bus from the cor-
ner of Forbes and Morewood (near CMU) to the airport, 
using bus route 28X, arriving by 10:45 AM. 
 
3
3 Control Test Results 
The logs from the four systems were labeled for 
task success by hand.  A call is successful if any of 
the following outputs are correctly issued: 
 
? Bus schedule for the requested departure and 
arrival stops for the stated bus number (if giv-
en). 
? A statement that there is no bus available for 
that route. 
? A statement that there is no scheduled bus at 
that time. 
 
We additionally allowed the following boundary 
cases: 
 
? A departure/arrival stop within 15 minutes 
walk. 
? Departure/arrival times within one hour of re-
quested time. 
? An alternate bus number that serves the re-
quested route. 
 
In the control tests, SYS2 had system connection 
issues that caused a number of calls to fail to con-
nect, as well as a poorer task completion.  It was 
not included in the live tests.  It should be pointed 
out that SYS2 was developed by a single graduate 
student as a class project while the other systems 
were developed by teams of researchers.  The re-
sults of the Control Tests are shown in Table 1 and 
are discussed further below. 
 
Table 1. Results of hand analysis of the four systems in 
the control test 
 The three major classes of system response 
are as follows.  no_info: this occurs when the sys-
tem gives neither a specific time nor a valid excuse 
(bus not covered, or none at that time).  no_info 
calls can be treated as errors (even though there 
maybe be valid reasons such as the caller hangs up 
because the bus they are waiting for arrives).  
donthave: identifies calls that state the requested 
bus is not covered by the system or that there is no 
bus at the requested time. pos_out: identifies calls 
where a specific time schedule is given.  Both 
donthave and pos_out calls may be correct or er-
roneous (e.g the given information is not for the 
requested bus,  the departure stop is wrong, etc). 
4 Live Tests Results 
In the live tests the actual Pittsburgh callers had 
access to three systems: SYS1, SYS3, and SYS4.  
Although engineering issues may not always be 
seen to be as relevant as scientific results, it is im-
portant to acknowledge several issues that had to 
be overcome in order to run the live tests. 
Since the Pittsburgh Bus Information System 
is a real system, it is regularly updated with new 
schedules from the Port Authority. This happens 
about every three months and sometimes includes 
changes in bus routes as well as times and stops. 
The SDC participants were given these updates 
and were allowed the time to make the changes to 
their systems. Making things more difficult is the 
fact that the Port Authority often only releases the 
schedules a few days ahead of the change. Another 
concern was that the live tests be run within one 
schedule period so that the change in schedule 
would not affect the results.   
The second engineering issue concerned 
telephony connectivity. There had to be a way to 
transfer calls from the Port Authority to the par-
ticipating systems (that were run at the participat-
ing sites, not at CMU) without slowing down or 
perturbing service to the callers.  This was 
achieved by an elaborate set of call-forwarding 
mechanisms that performed very reliably.  How-
ever, since one system was in Europe, connections 
to it were sometimes not as reliable as to the US-
based systems.  
 
 SYS1 SYS3 SYS4 
Total Calls 678 451 742 
Non-empty calls 633 430 670 
no_ info 18.5% 14.0% 11.0% 
donthave 26.4% 30.0% 17.6% 
donthave_corr 47.3% 40.3% 37.3% 
donthave_incorr 52.7% 59.7% 62.7% 
pos_out 55.1% 56.0% 71.3% 
pos_out_corr 86.8% 93.8% 91.6% 
pos_out_incorr 13.2% 6.2% 8.4% 
 
Table 2. Results of hand analysis of the three systems in 
the live tests.  Row labels are the same as in Table 1. 
 SYS1 SYS2 SYS3 SYS4 
Total Calls 91 61 75 83 
no_ info 3.3% 37.7% 1.3% 9.6% 
donthave 17.6% 24.6% 14.7% 9.6% 
donthave_corr 68.8% 33.3% 100.0% 100.0% 
donthave_incorr 31.3% 66.7% 0.0% 0.0% 
pos_out 79.1% 37.7% 84.0% 80.7% 
pos_out_corr 66.7% 78.3% 88.9% 80.6% 
pos_out_incorr 33.3% 21.7% 11.1% 19.4% 
4
We ran each of the three systems for multiple two 
day periods over July and August 2010.  This de-
sign gave each system an equal distribution of 
weekdays and weekends, and also ensured that 
repeat-callers within the same day experienced the 
same system. 
One of the participating systems (SYS4) 
could support simultaneous calls, but the other two 
could not and the caller would receive a busy sig-
nal if the system was already in use.  This, how-
ever, did not happen very often. 
Results of hand analysis of real calls are 
shown in Table 4 alongside the results for the Con-
trol Test for easy comparison.  In the live tests we 
had an additional category of call types ? empty 
calls (0-turn calls) ? which are calls where there 
are no user turns, for example because the caller 
hung up or was disconnected before saying any-
thing.  Each system had 14 days of calls and exter-
nal daily factors may change the number of calls. 
We do suspect that telephony issues may have pre-
vented some calls from getting through to SYS3 on 
some occasions.   
Table 3 provides call duration information for 
each of the systems in both the control and live 
tests. 
 
 
 Length (s) Turns/call Words/turn 
SYS1 control 155 18.29 2.87 (2.84) 
SYS1 live 111 16.24 2.15 (1.03) 
SYS2 control 147 17.57 1.63 (1.62) 
SYS3 control 96 10.28 2.73 (1.94) 
SYS3 live 80 9.56 2.22 (1.14) 
SYS4 control 154 14.70 2.25 (1.78) 
SYS4 live 126 11.00 1.63 (0.77) 
 
Table 3: For live tests, average length of each call, aver-
age number of turns per call, and average number of 
words per turn (numbers in brackets are standard devia-
tions). 
 
Each of the systems used a different speech 
recognizer.  In order to understand the impact of 
word error rate on the results, all the data were 
hand transcribed to provide orthographic transcrip-
tions of each user turn.   Summary word error sta-
tistics are shown in Table 4.   However, summary 
statistics do not show the correlation between word 
error rate and dialogue success.  To achieve this, 
following Thomson et al(2010), we computed a 
logistic regression of success against word error 
rate (WER) for each of the systems. Figure 3 
shows the regressions for the Control Tests and 
Figure 4 for the Live Tests.  
 
 SYS1 SYS3 SYS4 
Control 38.4 27.9 27.5 
Live 43.8 42.5 35.7 
 
Table 4: Average dialogue word error rate (WER). 
 
0 20 40 60 80 100
0.
0
0.
2
0.
4
0.
6
0.
8
1.
0
WER
Su
cc
es
s 
Ra
te
Sys4
Sys3
Sys1
 
Figure 3: Logistic regression of control test success vs 
WER for the three fully tested systems 
0 20 40 60 80 100
0.
0
0.
2
0.
4
0.
6
0.
8
1.
0
WER
Su
cc
e
ss
Sys1
Sys3
Sys4
 
Figure 4: Logistic regression of live success vs WER for 
the three fully tested systems 
 
5
In order to compare the control and live tests, 
we can calculate task completion as the percentage 
of calls that gave a correct result.  We include only 
non-empty calls (excluding 0-turn calls), and treat 
all no_info calls as being incorrect, even though 
some may be due to extraneous reasons such as the 
bus turning up (Table 5). 
 
 SYS1 SYS3 SYS4 
Control 64.9% (5.0%) 89.4% (3.6%) 74.6% (4.8%) 
Live 60.3% (1.9%) 64.6% (2.3%) 71.9% (1.7%) 
 
Table 5: Live and control test task completion (std. err).  
 
5 Discussion 
All systems had lower WER and higher task com-
pletion in the controlled test vs. the live test.  This 
agrees with past work [Raux et al2005, Ai et al
2008], and underscores the challenges of deploying 
real-world systems. 
For all systems, dialogs with controlled sub-
jects were longer than with live callers ? both in 
terms of length and number of turns.  In addition, 
for all systems, live callers used shorter utterances 
than controlled subjects.  Controlled subjects may 
be more patient than live callers, or perhaps live 
callers were more likely to abandon calls in the 
face of higher recognition error rates.   
Some interesting differences between the sys-
tems are evident in the live tests.  Looking at dia-
log durations, SYS3 used confirmations least often, 
and yielded the fastest dialogs (80s/call).  SYS1 
made extensive use of confirmations, yielding the 
most turns of any system and slightly longer dia-
logs (111s/call).  SYS4 was the most system-
directed, always collecting information one ele-
ment at a time.  As a result it was the slowest of the 
systems (126s/call), but because it often used im-
plicit confirmation instead of explicit confirmation, 
it had fewer turns/call than SYS1.   
For task completion, SYS3 performed best in 
the controlled trials, with SYS1 worst and SYS4 in 
between.  However in the live test, SYS4 per-
formed best, with SYS3 and SYS1 similar and 
worse.  It was surprising that task completion for 
SYS3 was the highest for the controlled tests yet 
among the lowest for the live tests.  Investigating 
this, we found that much of the variability in task 
completion for the live tests appears to be due to 
WER.  In the control tests SYS3 and SYS4 had 
similar error rates but the success rate of SYS3 was 
higher.  The regression in Figure 3 shows this 
clearly.   In the live tests SYS3 had a significantly 
higher word error rate and average success rate 
was much lower than in SYS4.   
It is interesting to speculate on why the rec-
ognition rates for SYS3 and SYS4 were different 
in the live tests, but were comparable in the control 
tests.  In a spoken dialogue system the architecture 
has a considerable impact on the measured word 
error rate.  Not only will the language model and 
use of dialogue context be different, but the dia-
logue design and form of system prompts will in-
fluence the form and content of user inputs.   Thus, 
word error rates do not just depend on the quality 
of the acoustic models ? they depend on the whole 
system design.  As noted above, SYS4 was more 
system-directed than SYS3 and this probably con-
tributed to the comparatively better ASR perform-
ance with live users.   In the control tests, the 
behavior of users (research lab workers) may have 
been less dependent on the manner in which users 
were prompted for information by the system.  
Overall, of course, it is user satisfaction and task 
success which matter. 
6 Corpus Availability and Evaluation 
The SDC2010 database of all logs from all systems 
including audio plus hand transcribed utterances, 
and hand defined success values is released 
through CMU?s Dialog Research Center 
(http://dialrc.org). 
One of the core goals of the Spoken Dialog 
Challenge is to not only create an opportunity for 
researchers to test their systems on a common plat-
form with real users, but also create common data 
sets for testing evaluation metrics.  Although some 
work has been done on this for the control test data 
(e.g. [Zhu et al2010]), we expect further evalua-
tion techniques will be applied to these data. 
One particular issue which arose during this 
evaluation concerned the difficulty of defining pre-
cisely what constitutes task success.  A precise de-
finition is important to developers, especially if 
reinforcement style learning is being used to opti-
mize the success.  In an information seeking task 
of the type described here, task success is straight-
forward when the user?s requirements can be satis-
fied but more difficult if some form of constraint 
relaxation is required.   For example, if the user 
6
asks if there is a bus from the current location to 
the airport ? the answer ?No.? may be strictly cor-
rect but not necessarily helpful.  Should this dia-
logue be scored as successful or not?  The answer 
?No, but there is a stop two blocks away where 
you can take the number 28X bus direct to the air-
port.? is clearly more useful to the user.  Should 
success therefore be a numeric measure rather than 
a binary decision?  And if a measure, how can it be 
precisely defined?  A second and related issue is 
the need for evaluation algorithms which deter-
mine task success automatically.   Without these, 
system optimization will remain an art rather than 
a science. 
7 Conclusions 
This paper has described the first attempt at an ex-
ercise to investigate how different spoken dialog 
systems perform on the same task.  The existing 
Let?s Go Pittsburgh Bus Information System was 
used as a task and four teams provided systems 
that were first tested in controlled conditions with 
speech researchers as users. The three most stable 
systems were then deployed ?live? with real call-
ers. Results show considerable variation both be-
tween systems and between the control and live 
tests.  Interestingly, relatively high task completion 
for controlled tests did not always predict rela-
tively high task completion for live tests.  This 
confirms the importance of testing on live callers, 
not just usability subjects. 
 The general organization and framework 
of the evaluation worked well.  The ability to route 
audio telephone calls to anywhere in the world us-
ing voice over IP protocols was critical to the suc-
cess of the challenge since it provides a way for 
individual research labs to test their in-house sys-
tems without the need to port them to a central co-
ordinating site. 
 Finally, the critical role of precise evalua-
tion metrics was noted and the need for automatic 
tools to compute them.  Developers need these at 
an early stage in the cycle to ensure that when sys-
tems are subsequently evaluated, the results and 
system behaviors can be properly compared.  
Acknowledgments 
Thanks to AT&T Research for providing telephony 
support for transporting telephone calls during the 
live tests.  This work was in part supported by the 
US National Science foundation under the project 
?Dialogue Research Center?.   
References  
Ai, H., Raux, A., Bohus, D., Eskenzai, M., and Litman, 
D.  (2008)  ?Comparing spoken dialog corpora col-
lected with recruited subjects versus real users?, Proc 
SIGDial, Columbus, Ohio, USA.  
Black, A., Burger, S., Langner, B., Parent, G., and Es-
kenazi, M. (2010) ?Spoken Dialog Challenge 2010?, 
SLT 2010, Berkeley, CA.  
Hastie, H., Merigaud, N., Liu, X and Oliver Lemon. 
(2010) ? ?Let?s Go Dude?, Using The Spoken Dia-
logue Challenge to Teach Spoken Dialogue Devel-
opment?, SLT 2010, Berkeley, CA. 
Raux, A., Langner, B., Bohus, D., Black, A., Eskenazi, 
M.  (2005)  ?Let?s go public! Taking a spoken dialog 
system to the real world?, Interspeech 2005, Lisbon, 
Portugal. 
Raux, A., Bohus, D., Langner, B., Black, A., and Eske-
nazi, M. (2006) ?Doing Research on a Deployed 
Spoken Dialogue System: One Year of Let's Go! Ex-
perience?, Interspeech 2006 - ICSLP, Pittsburgh, PA.  
Thomson B., Yu, K. Keizer, S., Gasic, M., Jurcicek, F.,  
Mairesse, F. and Young, S. ?Bayesian Dialogue Sys-
tem for the Let?s Go Spoken Dialogue Challenge?, 
SLT 2010, Berkeley, CA. 
Williams, J., Arizmendi, I., and Conkie, A. ?Demonstra-
tion of AT&T ?Let?s Go?: A Production-Grade Statis-
tical Spoken Dialog System.? SLT 2010, Berkeley, 
CA. 
Zhu, Y., Yang, Z., Meng, H., Li, B., Levow, G., and 
King, I. (2010) ?Using Finite State Machines for 
Evaluating Spoken Dialog Systems?, SLT 2010, 
Berkeley, CA. 
7
Proceedings of the SIGDIAL 2011: the 12th Annual Meeting of the Special Interest Group on Discourse and Dialogue, pages 142?151,
Portland, Oregon, June 17-18, 2011. c?2011 Association for Computational Linguistics
?The day after the day after tomorrow?? A machine learning approach to
adaptive temporal expression generation:
training and evaluation with real users
Srinivasan Janarthanam, Helen Hastie, Oliver Lemon, Xingkun Liu
Interaction Lab
School of Mathematical and Computer Sciences (MACS)
Heriot-Watt University
{sc445, h.hastie, o.lemon, x.liu}@hw.ac.uk
Abstract
Generating Temporal Expressions (TE) that
are easy to understand, unambiguous, and rea-
sonably short is a challenge for humans and
Spoken Dialogue Systems. Rather than devel-
oping hand-written decision rules, we adopt a
data-driven approach by collecting user feed-
back on a variety of possible TEs in terms
of task success, ambiguity, and user prefer-
ence. The data collected in this work is freely
available to the research community. These
data were then used to train a simulated user
and a reinforcement learning policy that learns
an adaptive Temporal Expression generation
strategy for a variety of contexts. We evalu-
ate our learned policy both in simulation and
with real users and show that this data-driven
adaptive policy is a significant improvement
over a rule-based adaptive policy, leading to
a 24% increase in perceived task completion,
while showing a small increase in actual task
completion, and a 16% decrease in call dura-
tion. This means that dialogues are more ef-
ficient and that users are also more confident
about the appointment that they have agreed
with the system.
1 Introduction
Temporal Expressions are linguistic expressions that
are used to refer to a date and are often a source of
confusion in human-human, human-computer and
text interactions such as emails and instant messag-
ing. For example, ?Let?s meet next Sunday?? ?do
you mean Sunday this week or a week on Sunday??.
(Mccoy and Strube, 1999) state that changes in tem-
poral structure in text are often indicated by either
cue words and phrases (e.g. ?next Thursday?, ?this
week?, ?tomorrow?), a change in grammatical time
of the verb (e.g. present tense versus future tense),
or changes in aspect (e.g. atomic versus extended
events versus states as defined by (Moens and Steed-
man, 1988)). In this study, we will concentrate on
the first of these phenomena, generating TEs with
the optimal content and lexical choice.
Much work in the field of Natural Language Pro-
cessing concerns understanding and resolving these
temporal expressions in text (Gerber et al, 2002;
Pustejovsky et al, 2003; Ahn et al, 2007; Mazur
and Dale, 2007; Han et al, 2006), however, little
work has looked at how best to plan and realise tem-
poral expressions in order to minimize ambiguity
and confusion in a Spoken Dialogue System (SDS).
(Reiter et al, 2005) presented a data driven ap-
proach to generating TEs to refer to time in weather
forecast information where appropriate expressions
were identified using contextual features using su-
pervised learning. We adopt an adaptive, data-driven
reinforcement learning approach instead. Similar
data-driven approaches have been applied to infor-
mation presentation (Rieser et al, 2010; Walker et
al., 2007) where each Natural Language Generation
(NLG) action is a sequential decision point, based on
the current dialogue context and expected long-term
reward of that action. A data-driven approach has
also been applied to the problem of referring expres-
sion generation in dialogue for expert and novice-
users of a SDS (Janarthanam and Lemon, 2010).
However, to date, there has been no previous work
on adaptive data-driven approaches for temporal re-
ferring expression generation, where uncertainty in
142
the stochastic environment is explicitly modelled.
The data-driven approach to temporal expression
generation presented here is in the context of ap-
pointment scheduling dialogues. The fact that there
are multiple ways that a time slot can be referred to
leads to an interesting NLG problem of how best to
realise a TE for a particular individual in a particular
context for certain domains. For example, the fol-
lowing expressions all vary in terms of length, ambi-
guity, redundant information and users? preference:
?next Friday afternoon? or ?Friday next week at the
same time?, or ?in the afternoon, a week on Friday?.
Temporal Expressions contain two types of refer-
ences: absolute references such as ?Tuesday? and
?12th January?, and relative references such as ?to-
morrow? and ?this Tuesday?. Generating TEs there-
fore, involves both in selecting appropriate pieces of
information (date, day, time, month, and week) to
present and deciding how to present them (absolute
or relative reference).
Our objective here is to convey a target appoint-
ment slot to users using an expression that is optimal
in terms of the trade-off between understandability,
length and user preference.
2 Methodology
We address the issue of generating TEs by adopting
a data-driven approach that has four stages. Firstly,
we define Temporal Expression Units (TEU) as de-
scribed in Section 2.1. Secondly, we design and im-
plement a web-based data collection, gathering met-
rics on the TEUs in various contexts for a variety
of date types (Section 3). Thirdly, we train a user
simulation and use it to learn a policy using rein-
forcement learning techniques that generates the op-
timal combination of TEUs for each context (Sec-
tion 4). Finally, we deploy and evaluate this pol-
icy in a Spoken Dialogue System for appointment
scheduling and show that our learned policy per-
forms better than a hand-written, adaptive one (re-
sults presented in Section 5).
2.1 Temporal Expression Units
For this study, TEs are broken down into 5 cate-
gories or units (TEUs) presented in a fixed order:
DAY, DATE, MONTH, WEEK and TIME. Each of
these units can be expressed relative to the current
TEU Choices
DAY abs, rel, rc, nn
DATE abs, nn
MONTH abs, nn
WEEK abs, rel, nn
TIME abs, rc
Table 1: TEU choices where abs is absolute, rel is rela-
tive, rc is relative to context and nn is none
day and to the current context (i.e. previously men-
tioned dates). Specifically, there are 3 unit attributes:
absolute (e.g. DAY=abs ?Tuesday?); relative to cur-
rent day (e.g. DAY=rel ?tomorrow?); and relative to
context (e.g. DAY=rc ?the following day?).
Certain restrictions on possible TEU combina-
tions were imposed, for example, DATE=rc and
DAY=rel were combined to be just DAY=rel, and
some combinations were omitted on the basis that
it is highly unlikely that they would be uttered
in natural speech, for example WEEK=rel and
MONTH=abs would result in ?this week in Septem-
ber?. Finally, every TE has to contain a time (am or
pm for this application). The possible combinations
are summarised in Table 1.
3 Data Collection
The data collection experiment was in two parts
(Task 1 and Task 2) and was designed using the We-
bexp experimental software1. Webexp is a client-
server set up where a server application hosts the ex-
periment and stores the experimental files, logs and
results. The client side runs an applet on the user?s
web-browser.
In Task 1, participants listened to an audio file
containing a TE generated from absolute and rela-
tive TEUs (see Figure 1). No relative-context (rc)
TEUs were used in Task 1 since the dialogue ex-
cerpt presented was in isolation and therefore had
no context. Each participant was asked to listen to
10 different audio files in a sequence corresponding
to a variety of dates randomly chosen from 8 pos-
sible dates. The participant then had to identify the
correct appointment slot that the system is referring
to. There is scope for the participant to add multi-
ple answers in order to capture potential ambiguity
1http://www.webexp.info
143
Figure 1: Screen shot of Task 1 in the on-line data collection experiment
of a TE, and we report on this below. The 8 dates
that were used to generate the TEs fell into a two
week period in a single month which is in-line with
the evaluation set-up of the appointment scheduling
SDS discussed in Section 5.3.
For each date, the TE was randomly picked from a
set of 30 possible combinations of TEUs. Each TEU
was generated by a rule-based realiser and synthe-
sized using the Baratinoo synthesizer (France Tele-
com, 2011). This realiser generates text from a can-
didate list for each TEU based on the given date.
For example, if the slot currently being discussed
is Tuesday 7th, the realiser would generate ?tomor-
row? for DAY=rel; if the date in discussion was
Wednesday 8th then DAY=rel would be realised as
?the day after tomorrow?. There was potential for
overlap of stimuli, as any given TE for any given
date may be assessed by more than one participant.
Task 2 of the experiment was in two stages. In the
first stage (Task 2A), the participants are given to-
day?s date and the following dialogue excerpt; Op-
erator: ?We need to send out an engineer to your
home. The first available appointment is . . .? (see
Figure 2). They are then asked to listen to 5 audio
files of the system saying different TEs for the same
date and asked to rate preference on a scale of 1-6
(where 1 is bad and 6 is great.) For the second stage
(Task 2B), the dialogue is as follows; Operator: ?so
you can?t do Wednesday 8th September in the morn-
ing.? and then the participants are asked to listen
to 5 more audio files that are generated TEs includ-
ing relative context such as ?how about Thursday at
the same time??. This two-stage process is then re-
peated 4 times for each participant.
Table 2 summarizes the metrics collected in the
different parts of the experiment. The metric Dis-
tance is calculated in terms of the number of slots
from the current date to the target date (TD). In-
stances were grouped into four distance groups: G1:
TD is 1-2 slots away; G2: TD is 3-6 slots away; G3:
TD is 7-11 slots away and G4: TD more than 11
slots away. P replay is calcuated by the total num-
ber of replays divided by the total number of plays
for that temporal expression, i.e. the probability that
the temporal expression played is requested to be re-
played. P ambiguous is calculated by the number of
times a given temporal expression is given more than
1 interpretation divided by the total number of times
that the same given referring expression is answered.
In total there were 73 participants for Task 1 and
144
Figure 2: Screen shot of Task 2 in the on-line data collection experiment
730 TE samples collected. Although Task 2 directly
followed on from Task 1, there was a significant
drop out rate as only 48 participants completed the
second task resulting in 1,920 TE samples. Partici-
pants who completed both tasks were rewarded by a
chance to win an Amazon voucher.
3.1 Data Analysis
Figure 3 shows various metrics with respect to TE
absoluteness and relativeness is the number of ab-
solute and relative TEUs respectively. These two
graphs represent the state space that the genera-
tion policy described in Section 4 is exploring, trad-
ing off between various features such as Length,
taskSuccess and userPref.
As we can see, there is a tendency for average
taskSuccess to increase as absoluteness increases
whereas, for relativeness the distribution is more
even. The TE with the greatest taskSuccess has an
absoluteness of 4 and zero relativeness: DATE=abs,
MONTH=abs, WEEK=abs, TIME=abs (e.g. ?11th
September, the week starting the 10th, between 8am
and 10am?) and the TE with the least taskSuccess
has an absoluteness of only 2, again with no rela-
tiveness: DATE=abs, TIME=abs, (e.g. ?8th between
8am and 10am?).
Average userPref stays level and then decreases
if absoluteness is 5. We infer from this that al-
though long utterances that are completely explicit
are more clear in terms of taskSuccess, they are not
necessarily preferred by users. This is likely due
to TE length increasing. On average, the inclusion
of one relative expression is preferred over none at
all or two. The most preferred TE has an abso-
luteness of 3 with a relativeness of 2: DAY=rel,
DATE=abs, MONTH=abs, WEEK=rel, TIME=abs
(e.g. ?Tomorrow the 7th of September, this week,
between 8am and 10am?).
145
Figure 3: Graph showing the trade-offs between various metrics with respect to absoluteness and relativeness (number
of absolute/relative TEUs) in terms of probabilities or normalised values.
Metric Description Task
P ambiguous Probability that the expres-
sion is ambiguous to the
user
1
taskSuccess Correct slot identified 1
P replay Probability of replay (mea-
sure of understandability)
1 & 2
Length Expression length in terms
of number of TEUs that
are non null divided by the
total number of possible
TEUs (5)
1 & 2
wordLength Expression length in words
normalised over max num
of words (15)
1 & 2
userPref Preference rating of audio
from 1-6
2
Distance Distance from target date
(TD) to current date in
terms of number of slots
1 & 2
Table 2: Metrics collected in various parts of the experi-
ment
The probability of ambiguity and replay does not
seem to be affected by absoluteness. The most am-
biguous TE has an absoluteness of 3 and zero rela-
tiveness: DAY=abs MONTH=abs TIME=abs, (e.g.
?Tuesday September between 8am and 10am?) in-
dicating that a date is needed for precision. The
TEs that the participants were most likely to replay
tended to be short e.g. ?Tomorrow at the same time?.
This may be due to the clarity of the speech synthe-
siser.
4 Learning a TE generation policy
Reinforcement learning is a machine learning ap-
proach based on trial and error learning, in which
a learning agent learns to map sequences of ?opti-
mal? actions to environment or task states (Sutton
and Barto, 1998). In this framework the problem
of generating temporal expressions is presented as
a Markov Decision Process. The goal of the learn-
ing agent is to learn to choose those actions that ob-
tain maximum expected reward in the long run. In
this section, we present the reinforcement learning
setup for learning temporal expression generation
policies.
4.1 Actions and States
In this learning setup, we focus only on generating
the formal specification and treat the set of TEU
choices as the sequential actions of the learning
agent. Table 1 presents the choices that are available
for each TEU.
The actions are taken based on two factors: the
146
distance (in terms of time slots: morning or after-
noon appointments) between (1) the current date
and the target slot and (2) the current date and the
slot in context. Based on the distance, the target
slot was classified to belong to one of the four dis-
tance groups (G1-G4). The slot in context repre-
sents whether there was any other slot already men-
tioned in the conversation so far, so that the system
has an option to use ?relative context? expressions
to present day and time information. Information
concerning the target slot?s group and the slot in con-
text make up the state space of the Markov Decision
Process (MDP).
4.2 User Simulation
We built a user simulation to simulate the dialogue
behaviour of a user in appointment scheduling con-
versations based on the data from real users de-
scribed in Section 3. It responds to the TE used
by the system to refer to an appointment slot. It
responds by either accepting, rejecting, or clarify-
ing the offered slot based on the user?s own calen-
dar of available slots. For instance, the simulated
user rejects an offered slot if the user is not avail-
able at that time. If they accept or reject an offered
slot, the user is assumed to understand the TE unam-
biguously. However, if the user is unable to resolve
the appointment slot from the TE, it responds with a
clarification request. The simulation responded with
a dialogue action (Au,t) to TEs based on the sys-
tem?s dialogue act (As,t), system?s TE (TEs,t). The
following probabilistic model was used to generate
user dialogue actions:
P (Au,t|As,t, TEs,t, G,C,Cal)
In addition to TEs,t and As,t, other factors such as
distance between the target slot and the current slot
(G), the previous slot in context (C), and the user?s
calendar (Cal) were also taken into account. G is ei-
ther G1, G2, G3 or G4 as explained in Section 3. The
User?s dialogue action (Au,t) is one of the three: Ac-
cept slot, Reject slot or Request Clarification. The
probability of clarification request was calculated as
the average of the ambiguity and replay probabilities
seen in real user data.
4.3 Reward function
The learning agent was rewarded for each TE that it
generated. The reward given to the agent was based
on trade-offs between three variables: User prefer-
ence (UP), Length of the temporal expression (L),
and Clarification request probability (CR). UP for
each TE is obtained from Task 2 of the data collec-
tion. In the following reward function, UP is nor-
malised to be between 0 and 1. L is based on number
of TEUs used. The maximum number of TEUs that
can be used is 5 (i.e. DAY, DATE, WEEK, MONTH,
TIME). L is calculated as follows:
Length of TE (L) = No. of used TEUsMax. no. of TEUs
The clarification request (CR) is set to be 1 if the
user responds to the TE with a Request Clarification
and 0 otherwise. Reward is therefore calculated on
a turn-by-turn basis using the following formula:
Reward = UP ? 10.0 ? L ? 10.0 ? CR ? 10.0
In short, we chose a reward function that penalises
TEs that are long and ambiguous, and which rewards
TEs that users prefer. It also indirectly rewards task
success by penalising ambiguous TEs resulting in
clarification requests. This trade-off structure is evi-
dent from the data collection where TEs that are too
long are dispreferred by the users (see Figure 3). The
maximum possible reward is 6 (i.e. UP=1, CR=0,
L=2/5) and the minimum is -20 (i.e. UP=0, CR=1,
L=1). Note that other reward functions could be ex-
plored in future work, for example maximising only
for user preference or length.
4.4 Training
We trained a TE generation policy using the above
user simulation model for 10,000 runs using the
SARSA reinforcement learning algorithm (Sutton
and Barto, 1998). During the training phase, the
learning agent generated and presented TEs to the
user simulation. When a dialogue begins, there is no
appointment slot in context (i.e. C = 0). However,
if the user rejects the first slot, the dialogue system
sets C to 1 and presents the next slot. This is again
reset at the beginning of the next dialogue. The
agent was rewarded at the end of every turn based
on the user?s response, length of the TE, and user
preference scores as shown above. It gradually ex-
plored all possible combinations of TEUs and identi-
fied those TEUs in different contexts that maximize
147
Figure 4: Learning curve
the long-term reward. Figure 4 shows the learning
curve of the agent.
Table 3 presents the TE generation policy learned
by the agent. As one can observe, it used a mini-
mum number of TEUs to avoid length penalties in
the reward. In all cases, MONTH and WEEK in-
formation have not been presented at all. For target
slots that were closest (in group G1) and the farthest
(in group G4), it used relative forms of day (e.g. ?to-
morrow?, ?next Tuesday?, etc.). This is probably
because users dispreferred day information for in-
between slots (e.g. ?the day after the day after to-
morrow?). Also, MONTH information may have
been considered to be irrelevant due to the fact that
the two week window over which the data has been
collected do not span over two different months.
5 Evaluation
In this section, we present the baseline policies that
were evaluated along with the learned policy. We
then present the results of evaluation.
Slots Specification learned
1-2 DAY=rel;DATE=abs;MONTH=nn;
> 11 WEEK=nn;TIME=abs
3-11 DAY=nn;DATE=abs;MONTH=nn;
WEEK=nn;TIME=abs
Table 3: Learned policy
5.1 Baseline policies
The following are the baseline TEG policies:
1. Absolute policy: always use absolute for-
mats for all TEUs (i.e. DAY=abs; DATE=abs;
MONTH=abs; WEEK=abs; TIME=abs)
2. Minimal policy: always use a minimal format
with only date, month and time information in
their absolute forms (i.e. DAY=nn; DATE=abs;
MONTH=abs; WEEK=nn; TIME=abs)
3. Random policy: select possible formats ran-
domly for each TEU.
148
TEG Policy Average reward
Learned -0.071* (?3.75)
Absolute -4.084 (?4.36)
Minimal -1.340 (?4.2)
Random -8.21 (?7.72)
Table 4: Evaluation with simulated users (* p < 0.05,
two-tailed independent samples t-test)
5.2 Results
We evaluated the learned policy and the three other
hand-coded baseline TE generation policies with our
user simulation model. Each policy generated 1,000
TEs in different states. Table 4 present the results
of evaluation with simulated users. On average, the
learned policy scores higher than all the baseline
policies and the differences between the average re-
ward of the learned policy and the other baselines
are statistically significant. This shows that target
slots can be presented using different TEs depending
on how far they are from the current date and such
adaptation can produce less ambiguous, shorter and
user preferred expressions.
5.3 Evaluation with real users
The policy was also integrated into an NLG com-
ponent of a deployed Appointment Scheduling spo-
ken dialogue system. Please note that this is differ-
ent from the web environment in which the training
data was collected. Our data-driven policy was acti-
vated when the system informs the user of an avail-
able time slot. This system was compared to the
exact same system but with a rule-based adaptive
baseline system. In the rule-based policy MONTH,
DATE and TIME were always absolute, DAY was
relative if the target date was less than three days
away (i.e. ?today, tomorrow, day after tomorrow?),
and WEEK was always relative (i.e. ?this week, next
week?). All 5 information units were included in the
realisation (e.g. ?Thursday the 15th July in the after-
noon, next week?) although the order was slightly
different (DAY-DATE-MONTH-TIME-WEEK).
In this domain, the user tries to make an appoint-
ment for an engineer to visit their home. Each user
is given a set of 2-week calendars which shows their
availability and the goal is to arrange an appoint-
ment when both they and the engineer are available.
There were 12 possible scenarios that were evenly
rotated across participants and systems. Each sce-
nario is categorised in terms of scheduling difficulty
(Hard/Medium/Easy). Scheduling difficulty is cal-
culated for User Difficulty (UD) and System Diffi-
culty (SD) separately to assess the system?s mixed
initiative ability. Scheduling difficulty is calculated
as the ordinal of the first session that is free for both
the User and the System. Hard scenarios are with an
ordinal of 3 or 4; Medium with an ordinal of 2, and
Easy with an ordinal of 1. There are 4 scenarios in
each of these difficulty categories for both the user
and system. To give an example, in Scenario 10,
the user can schedule an appointment on Wednes-
day afternoon but he/she also has one free session
on the previous Tuesday afternoon when the engi-
neer is busy therefore UD = 2. For the system, in
this scenario, the first free session it has is on the
Wednesday afternoon therefore SD=1. In this case,
the scenario is easier for the system than the user be-
cause the system could just offer the first session that
it has free.
605 dialogues were collected and analysed. The
system was evaluated by employees at France Tele-
com and students of partner universities who have
never used the appointment scheduling system be-
fore. After each scenario, participants were then
asked to fill out a questionnaire on perceived task
success and 5 user satisfaction questions on a 6-
point Likert Scale (Walker et al, 2000). Results
from the real user study are summarised in Table 5.
The data-driven policy showed significant improve-
ment in Perceived Task Success (+23.7%) although
no significant difference was observed between the
two systems in terms of Actual Task Success (Chi-
square test, df=1). Perceived Task Success is users?
perception of whether they completed the task suc-
cessfully or not. Overall user satisfaction (the aver-
age score of all the questions) was also significantly
higher (+5%)2. Dialogues with the learned policy
were significantly shorter with lower Call Duration
in terms of time (-15.7%)2 and fewer average words
per system turn (-23.93%)2. Figure 5 shows the
length results in time for systems of varying UD and
SD. We can see that the data-driven adaptive policy
consistently results in a shorter dialogue across all
levels of difficulty. In summary, these results show
that using a policy trained on the data collected here
149
Parameters Learned Baseline
TEG TEG
Actual Task Success 80.05% 78.57%
Perceived Task Success 74.86%* 60.50%
User satisfaction 4.51* 4.30
No. system turns 22.8 23.2
Words per system turn 13.16* 17.3
Call duration 88.60 sec * 105.11 sec
Table 5: Results with real users (* statistically significant
difference at p<0.05)
results in shorter dialogues and greater confidence
in the user that they have had a successful dialogue.
Although the learned policy was trained to generate
optimal TEs within a two week window and there-
fore is not general policy for all TE generation prob-
lems, we believe that the data-driven approach that
we have followed can generalise to other TE gener-
ation tasks.
Figure 5: Graph comparing length of dialogues for user
(UD) and system difficulty (SD)
6 Conclusion
We have presented a principled statistical learning
method for generating Temporal Expressions (TEs)
that refer to appointment slots in natural language
utterances. We presented a method for gathering
data on TEs with an on-line experiment and showed
how we can use these data to generate TEs us-
ing a Markov Decision Process which can be opti-
mised using reinforcement learning techniques. We
showed that a TEG policy learned using our frame-
2independent two-tailed t-test p < 0.05
work performs signifcantly better than hand-coded
adaptive policies with real users as well as with sim-
ulated users.
The data collected in this work has been freely
released to the research community in 20113.
Acknowledgements
The research leading to these results has received
funding from the EC?s 7th Framework Programme
(FP7/2007-2013) under grant agreement no. 216594
(CLASSiC project www.classic-project.
org), (FP7/2011-2014) under grant agreement no.
248765 (Help4Mood project), (FP7/2011-2014) un-
der grant agreement no. 270435 (JAMES project),
(FP7/2011-2014) under grant agreement no. 270019
(SpaceBook project), and from the EPSRC, project
no. EP/G069840/1. We would also like to thank our
CLASSiC project colleagues at Cambridge Univer-
sity and France Telecom / Orange Labs.
References
D. Ahn, J. van Rantwijk, and M. de Rijke. 2007. A
Cascaded Machine Learning Approach to Interpret-
ing Temporal Expressions. In Proceedings of NAACL-
HLT 2007.
France Telecom. 2011. Baratinoo expressive speech syn-
thesiser. http://tts.elibel.tm.fr.
L. Gerber, L. Ferro, I. Mani, B. Sundheim, G. Wilson,
and R. Kozierok. 2002. Annotating Temporal Infor-
mation: From Theory to Practice. In Proceedings of
HLT.
B. Han, D. Gates, and L. Levin. 2006. Understanding
temporal expressions in emails. In HLT-NAACL 2006.
Srinivasan Janarthanam and Oliver Lemon. 2010. Learn-
ing to adapt to unknown users: referring expression
generation in spoken dialogue systems. In ACL ?10.
P. Mazur and R. Dale. 2007. The DANTE Temporal Ex-
pression Tagger. In Proceedings of the 3rd Language
and Technology Conference, Poznan, Poland.
Kathleen F. Mccoy and Michael Strube. 1999. Taking
time to structure discourse: Pronoun generation be-
yond accessibility. In Proc. of the 21th Annual Con-
ference of the Cognitive Science Society.
M. Moens and M. Steedman. 1988. Temporal ontology
and temporal reference. In Computational Linguistics,
volume 14(2), pages 15?28.
3Sec 2.6 at http://www.macs.hw.ac.uk/ilabarchive/classicproject/data/
150
J. Pustejovsky, J. Castano, R. Ingria, R. Sauri,
R. Gaizauskas, A. Setzer, G. Katz, and D. Radev.
2003. TimeML: Robust specification of event and
temporal expressions in text. In AAAI Spring Sympo-
sium on New Directions in Question-Answering, Stan-
ford, CA.
E. Reiter, S. Sripada, J. Hunter, and J. Yu. 2005. Choos-
ing words in computer-generated weather forecasts.
Artificial Intelligence, 167:137169.
Verena Rieser, Oliver Lemon, and Xingkun Liu. 2010.
Optimising information presentation for spoken dia-
logue systems. In Proc. ACL 2010.
R. Sutton and A. Barto. 1998. Reinforcement Learning.
MIT Press.
Marilyn A. Walker, Candace A. Kamm, and Diane J. Lit-
man. 2000. Towards Developing General Models of
Usability with PARADISE. Natural Language Engi-
neering, 6(3).
Marilyn Walker, Amanda Stent, Franc?ois Mairesse, and
Rashmi Prasad. 2007. Individual and domain adap-
tation in sentence planning for dialogue. Journal of
Artificial Intelligence Research (JAIR), 30:413?456.
151
INLG 2012 Proceedings of the 7th International Natural Language Generation Conference, pages 49?58,
Utica, May 2012. c?2012 Association for Computational Linguistics
Optimising Incremental Generation for Spoken Dialogue Systems:
Reducing the Need for Fillers
Nina Dethlefs, Helen Hastie, Verena Rieser and Oliver Lemon
Heriot Watt University
EH14 4AS, Edinburgh
n.s.dethlefs | h.hastie | v.t.rieser | o.lemon@hw.ac.uk
Abstract
Recent studies have shown that incremental
systems are perceived as more reactive, nat-
ural, and easier to use than non-incremental
systems. However, previous work on incre-
mental NLG has not employed recent ad-
vances in statistical optimisation using ma-
chine learning. This paper combines the two
approaches, showing how the update, revoke
and purge operations typically used in in-
cremental approaches can be implemented as
state transitions in a Markov Decision Process.
We design a model of incremental NLG that
generates output based on micro-turn inter-
pretations of the user?s utterances and is able
to optimise its decisions using statistical ma-
chine learning. We present a proof-of-concept
study in the domain of Information Presen-
tation (IP), where a learning agent faces the
trade-off of whether to present information as
soon as it is available (for high reactiveness)
or else to wait until input ASR hypotheses are
more reliable. Results show that the agent
learns to avoid long waiting times, fillers and
self-corrections, by re-ordering content based
on its confidence.
1 Introduction
Traditionally, the smallest unit of speech processing
for interactive systems has been a full utterance with
strict, rigid turn-taking. Components of these inter-
active systems, including NLG systems, have so far
treated the utterance as the smallest processing unit
that triggers a module into action. More recently,
work on incremental systems has shown that pro-
cessing smaller ?chunks? of user input can improve
the user experience (Skantze and Schlangen, 2009;
Buss et al, 2010; Skantze and Hjalmarsson, 2010;
Baumann et al, 2011). Incrementality in NLG sys-
tems enables the system designer to model several
dialogue phenomena that play a vital role in hu-
man discourse (Levelt, 1989) but have so far been
absent from NLG systems. These include more
natural turn-taking through rapid system responses,
grounding through the generation of backchannels
and feedback, and barge-ins (from both user and sys-
tem). In addition, corrections and self-corrections
through constant monitoring of user and system ut-
terances play an important role, enabling the system
to recover smoothly from a recognition error or a
change in the user?s preferences. Some examples of
the phenomena we are targeting are given in Fig. 1.
Skantze and Hjalmarsson (2010) present a model
of incremental speech generation in which input pro-
cessing and output planning are parallel processes
and the system can self-monitor its own genera-
tion process. In an evaluation with human users
they showed that their incremental system started
to speak significantly faster than a non-incremental
system (roughly 600 ms) and was perceived as sig-
nificantly more polite and efficient. Users also in-
dicated that they knew better when to start speaking
themselves. Alternative approaches to incremental
NLG include Kilger and Finkler (1995) who present
an early approach based on Tree-Adjoining Gram-
mar, and Purver and Otsuka (2003) who define an
incremental generator based on Dynamic Syntax.
Both of these generators can monitor their own out-
put and initiate corrections if necessary.
Over recent years, adaptive and data-driven ap-
49
Self-correction (the system made a mistake)
USR I want Italian food in the centre of town . . .
SYS OK. I found 35 Indian restaurants . . .
USR No, I want Italian.
SYS oh sorry . . .
SYS I have 24 Italian restaurants in the city centre . . .
Correction (the user changed their mind)
USR I want Indian food in the centre of town . . .
SYS There are 35 Indian . . .
USR Oh sorry, I meant Italian.
SYS OK, Italian . . .
SYS I have 24 Italian restaurants . . .
Re-ordering (from high to low confidence)
USR I want Italian food . . .
SYS [waits]
USR in the city centre.
SYS I have 120 places in the city centre . . .
USR I love Italian . . .
SYS 24 of them Italian . . .
Holding the floor
USR I want cheap Italian food . . .
SYS ok let me see
SYS I have 3 cheap Italian places . . .
Figure 1: Example phenomena generated with the trained
policy. The agent has learnt to produce backchannels
when the user pauses, monitor and (self-)correct its out-
put, and present information according to its confidence.
proaches to NLG have also been developed and
shown to outperform the previous (handcrafted,
rule-based) methods for specific problems (Rieser et
al., 2010; Janarthanam and Lemon, 2010; Dethlefs
and Cuaya?huitl, 2011). This work has established
that NLG can fruitfully be treated as a data-driven
statistical planning process, where the objective is
to maximise expected utility of the generated utter-
ances (van Deemter, 2009), by adapting them to the
context and user. Statistical approaches to sentence
planning and surface realisation have also been ex-
plored (Stent et al, 2004; Belz, 2008; Mairesse et
al., 2010; Angeli et al, 2010). The advantages of
data-driven methods are that NLG is more robust in
the face of noise, can adapt to various contexts and,
trained on real data, can produce more natural and
desirable variation in system utterances.
This paper describes an initial investigation into a
novel NLG architecture that combines incremental
processing with statistical optimisation. In order to
move away from conventional strict-turn taking, we
have to be able to model the complex interactions
observed in human-human conversation. Doing this
in a deterministic fashion through hand-written rules
would be time consuming and potentially inaccu-
rate, with no guarantee of optimality. In this paper,
we demonstrate that it is possible to learn incremen-
tal generation behaviour in a reward-driven fashion.
2 Previous Work: Incremental Processing
Architectures
The smallest unit of processing in incremental sys-
tems is called incremental unit (IU). Its instantia-
tion depends on the particular processing module. In
speech recognition, IUs can correspond to phoneme
sequences that are mapped onto words (Baumann
and Schlangen, 2011). In dialogue management, IUs
can correspond to dialogue acts (Buss et al, 2010).
In speech synthesis, IUs can correspond to speech
unit sequences which are mapped to segments and
speech plans (Skantze and Hjalmarsson, 2010). IUs
are typically linked to other IUs by two types of rela-
tions: same-level links connect IUs sequentially and
express relationships at the same level; grounded-in
links express hierarchical relations between IUs.
2.1 Buffer-Based Incremental Processing
A general abstract model of incremental process-
ing based on buffers and a processor was devel-
oped by Schlangen and Skantze (2009) and is illus-
trated in Figure 2. It assumes that the left buffer
of a module, such as the NLG module, receives
IUs from one or more other processing modules,
such as the dialogue manager. These input IUs are
then passed on to the processor, where they are
mapped to corresponding (higher-level) IUs. For
an NLG module, this could be a mapping from the
dialogue act present(cuisine=Indian) to the realisa-
tion ?they serve Indian food?. The resulting IUs are
passed on to the right buffer which co-incides with
the left buffer of another module (for example the
speech synthesis module in our example). Same-
level links are indicated as dashed arrows in Figure
2 and grounded-in links as stacked boxes of IUs.
The figure also shows that the mapping between
IUs can be a one-to-many mapping (IU1 and IU2
are mapped to IU3) or a one-to-one mapping (IU3 is
50
IU1 IU2
IU1 IU2
IU3
IU3 IU3
IU4
IU4
Left buffer Processor Right buffer
Left buffer Processor Right buffer
Figure 2: The buffer-based model showing two connected
modules (from Skantze and Hjalmarsson (2010).
IU1
IU2 IU3 IU4 IU5
IU6 IU7 IU8 IU9 . . .
Figure 3: The ISU-model for incremental processing
(adapted from Buss and Schlangen (2011)).
mapped to IU4). The model distinguishes four op-
erations that handle information processing: update,
revise, purge and commit. Whenever new IUs en-
ter the module?s left buffer, the module?s knowledge
base is updated to reflect the new information. Such
information typically corresponds to the current best
hypothesis of a preceding processing module. As
a property of incremental systems, however, such
hypotheses can be revised by the respective preced-
ing module and, as a result, the knowledge bases of
all subsequent modules need to be purged and up-
dated to the newest hypothesis. Once a hypothesis
is certain to not be revised anymore, it is commit-
ted. For concrete implementations of this model, see
Skantze and Schlangen (2009), Skantze and Hjal-
marsson (2010), Baumann and Schlangen (2011).
An implementation of an incremental dialogue
manager is based on the Information State Update
(ISU) model (Buss et al, 2010; Buss and Schlangen,
2011). The model is related in spirit to the buffer-
based architecture, but all of its input processing and
output planning is realised by ISU rules. This is true
for the incremental ?house-keeping? actions update,
revise, etc. and all types of dialogue acts. The in-
cremental ISU model is shown in Figure 3. Note
that this hierarchical architecture transfers well to
the ?classical? division of NLG levels into utterance
(IU1), content selection (IU2 - IU5) and surface re-
alisations (IU6 - IU9, etc.).
2.2 Beat-Driven Incremental Processing
In contrast to the buffer-based architectures, alterna-
tive incremental systems do not reuse previous par-
tial hypotheses of the user?s input (or the system?s
best output) but recompute them at each process-
ing step. We follow Baumann et al (2011) in call-
ing them ?beat-driven? systems. Raux and Eskenazi
(2009) use a cost matrix and decision theoretic prin-
ciples to optimise turn-taking in a dialogue system
under the constraint that users prefer no gaps and no
overlap at turn boundaries. DeVault et al (2009) use
maximum entropy classification to support respon-
sive overlap in an incremental system by predicting
the completions of user utterances.
2.3 Decision-making in Incremental Systems
Some of the main advantages of the buffer- and ISU-
based approaches include their inherently incremen-
tal mechanisms for updating and revising system hy-
potheses. They are able to process input of varying
size and type and, at the same time, produce arbi-
trarily complex output which is monitored and can
be modified at any time. On the other hand, current
models are based on deterministic decision making
and thus share some of the same drawbacks that non-
incremental systems have faced: (1) they rely on
hand-written rules which are time-consuming and
expensive to produce, (2) they do not provide a
mechanism to deal with uncertainty introduced by
varying user behaviour, and (3) they are unable to
generalise and adapt flexibly to unseen situations.
For NLG in particular, we have seen that incre-
mentality can enhance the responsiveness of sys-
tems and facilitate turn-taking. However, this ad-
vantage was mainly gained by the system produc-
ing semantically empty fillers such as um, let me
see, well, etc. (Skantze and Hjalmarsson, 2010). It
is an open research question whether such markers
of planning or turn-holding can help NLG systems,
but for now it seems that they could be reduced to
a minimum by optimising the timing and order of
Information Presentation. In the following, we de-
velop a model for incremental NLG that is based on
reinforcement learning (RL). It learns the best mo-
ment to present information to the user, when faced
with the options of presenting information as soon
as it becomes available or else waiting until the in-
51
Type Example
Comparison The restaurant Roma is in the medium price range, but does not have great food. The Firenze
and Verona both have great food but are more expensive. The Verona has good service, too.
Recommendation Restaurant Verona has the best overall match with your query. It is a bit more expensive,
but has great food and service.
Summary I have 43 Italian restaurants in the city centre that match your query. 10 of them are in the
medium price range, 5 are cheap and 8 are expensive.
Table 1: Examples of IP as a comparison, recommendation and summary for a user looking for Italian restaurants in
the city centre that have a good price for value.
put hypotheses of the system are more stable. This
also addresses the general trade-off that exists in in-
cremental systems between the processing speed of
a system and the output quality.
3 Information Presentation Strategies
Our domain of application will be the Informa-
tion Presentation phase in an interactive system
for restaurant recommendations, extending previous
work by Rieser et al (2010), (see also Walker et
al. (2004) for an alternative approach). Rieser et
al. incrementally construct IP strategies according
to the predicted user reaction, whereas our approach
focuses on timing and re-ordering of information
according to dynamically changing input hypothe-
ses. We therefore implement a simplified version
of Rieser et al?s model. Their system distinguished
two steps: the selection of an IP strategy and the
selection of attributes to present to the user. We as-
sume here that the choice of attributes is determined
by matching the types specified in the user input,
so that our system only needs to choose a strategy
for presenting its results (in the future, though, we
will include attribute selection into the decision pro-
cess). Attributes include cuisine, food quality, lo-
cation, price range and service quality of a restau-
rant. The system then performs a database lookup
and chooses among three main IP strategies sum-
mary, comparison, recommendation and several or-
dered combinations of these. Please see Rieser et al
(2010) for details. Table 1 shows examples of the
main types of presentation strategies we address.
4 Optimising Incremental NLG
To optimise the NLG process within an incremen-
tal model of dialogue processing, we define an RL
agent with incremental states and actions for the IP
task. An RL agent is formalised as a Markov De-
cision Process, or MDP, which is characterised as a
four-tuple < S,A, T,R >, where S is a set of states
representing the status of the NLG system and all in-
formation available to it, A is a set of NLG actions
that combine strategies for IP with handling incre-
mental updates in the system, T is a probabilistic
transition function that determines the next state s?
from the current state s and the action a according
to a conditional probability distribution P (s?|s, a),
and R is a reward function that specifies the reward
(a numeric value) that an agent receives for taking
action a in state s. Using such an MDP, the NLG
process can be seen as a finite sequence of states,
actions and rewards {s0, a0, r1, s1, a1, ..., rt?1, st},
where t is the time step. Note that a learning episode
falls naturally into a number of time steps at each of
which the agent observes the current state of the en-
vironment st, takes an action at and makes a tran-
sition to state st+1. This organisation into discrete
time steps, and the notion of a state space that is ac-
cessible to the learning agent at any time allows us to
implement the state update, revoke and purge opera-
tions typically assumed by incremental approaches
as state updates and transitions in an MDP. Any
change in the environment, such as a new best hy-
pothesis of the recogniser, can thus be represented
as a transition from one state to another. At each
time step, the agent then takes the currently best ac-
tion according to the new state. The best action in
an incremental framework can include correcting a
previous output, holding the floor as a marker of
planning, or to wait until presenting information.1
1We treat these actions as part of NLG content selection
here, but are aware that in alternative approaches, they could
52
States
incrementalStatus {0=none,1=holdFloor,2=correct,3=selfCorrect}
presStrategy {0=unfilled,1=filled}
statusCuisine {0=unfilled,1=low,2=medium,3=high,4=realised}
statusFood {0=unfilled,1=low,2=medium,3=high,4=realised}
statusLocation {0=unfilled,1=low,2=medium,3=high,4=realised}
statusPrice {0=unfilled,1=low,2=medium,3=high,4=realised}
statusService {0=unfilled,1=low,2=medium,3=high,4=realised}
userReaction {0=none,1=select,2=askMore,3=other}
userSilence={0=false,1=true}
Actions
IP: compare, recommend, summarise, summariseCompare,
summariseRecommend, summariseCompareRecommend,
Slot-ordering: presentCuisine, presentFood, presentLocation,
presentPrice, presentService,
Incremental: backchannel, correct, selfCorrect, holdFloor,
waitMore
Goal State 0, 1, 0 ? 4, 0 ? 4, 0 ? 4, 0 ? 4, 0 ? 4, 1, 0 ? 1
Figure 4: The state and action space of the learning agent.
The goal state is reached when all items (that the user may
be interested in) have been presented.
Once information has been presented to the user,
it is committed or realised. We again represent re-
alised IUs in the agent?s state representation, so that
it can monitor its own output. The goal of an MDP
is to find an optimal policy pi? according to which
the agent receives the maximal possible reward for
each visited state. We use the Q-Learning algorithm
(Watkins, 1989) to learn an optimal policy according
to pi?(s) = argmaxa?A Q?(s, a), where Q? speci-
fies the expected reward for executing action a in
state s and then following policy pi?.
5 Experimental Setting
5.1 The State and Action Space
The agent?s state space needs to contain all infor-
mation relevant for choosing an optimal IP strat-
egy and an optimal sequence of incremental ac-
tions. Figure 4 shows the state and action space
of our learning agent. The states contain infor-
mation on the incremental and presentation sta-
tus of the system. The variable ?incrementalSta-
tus? characterises situations in which a particular
(incremental) action is triggered. For example, a
holdFloor is generated when the user has finished
speaking, but the system has not yet finished its
database lookup. A correction is needed when
also be the responsibility of a dialogue manager.
the system has to modify already presented infor-
mation (because the user changed their preferences)
and a selfCorrection is needed when previously
presented information is modified because the sys-
tem made a mistake (in recognition or interpreta-
tion). The variable ?presStrategy? indicates whether
a strategy for IP has been chosen. It is ?filled? when
this is the case, and ?unfilled? otherwise. The vari-
ables representing the status of the cuisine, food, lo-
cation, price and service indicate whether the slot
is of interest to the user (0 means that the user does
not care about it), and what input confidence score is
currently associated with its value. Once slots have
been presented, they are realised and can only be
changed through a correction or self-correction.
The variable ?userReaction? shows the user?s re-
action to an IP episode. The user can select a restau-
rant, provide more information to further constrain
the search or do something else. The ?userSilence?
variable indicates whether the user is speaking or
not. This can be relevant for holding the floor or
generating backchannels. The action set comprises
IP actions, actions which enable us to learn the or-
dering of slots, and actions which allow us to cap-
ture incremental phenomena. The complete state-
action space size of this agent is roughly 3.2 mil-
lion. The agent reaches its goal state (defined w.r.t.
the state variables in Figure 4) when an IP strategy
has been chosen and all relevant attributes have been
presented.
5.2 The Simulated Environment
Since a learning agent typically needs several thou-
sand interactions to learn a reasonable policy, we
train it in a simulated environment with two compo-
nents. The first one deals with different IP strategies
generally (not just for the incremental case), and the
second one focuses on incrementally updated user
input hypothesis during the interaction.
To learn a good IP strategy, we use a user simula-
tion by Rieser et al (2010),2 which was estimated
from human data and uses bi-grams of the form
P (au,t|IPs,t), where au,t is the predicted user re-
action at time t to the system?s IP strategy IPs,t in
state s at time t. We distinguish the user reactions of
2The simulation data are available from http://www.
classic-project.org/.
53
select a restaurant, addMoreInfo to the current query
to constrain the search, and other. The last category
is considered an undesired user reaction that the sys-
tem should learn to avoid. The simulation uses lin-
ear smoothing to account for unseen situations. In
this way, we can then predict the most likely user
reaction to each system action.
While the IP strategies can be used for incremen-
tal and non-incremental NLG, the second part of the
simulation deals explicitly with the dynamic envi-
ronment updates during an interaction. We assume
that for each restaurant recommendation, the user
has the option of filling any or all of the attributes
cuisine, food quality, location, price range and ser-
vice quality. The possible values of each attribute
and possible confidence scores are shown in Table 2
and denote the same as described in Section 5.1.
At the beginning of a learning episode, we as-
sign each attribute a possible value and confidence
score with equal probability. For food and service
quality, we assume that the user is never interested
in bad food or service. Subsequently, confidence
scores can change at each time step. (In future work
these transition probabilities will be estimated from
a data collection, though the following assumptions
are realistic, based on our experience.) We assume
that a confidence score of 0 changes to any other
value with a likelihood of 0.05. A confidence score
of 1 changes with a probability of 0.3, a confidence
score of 2 with a probability of 0.1 and a confidence
score of 3 with a probability of 0.03. Once slots
have been realised, their value is set to 4. They
cannot be changed then without an explicit correc-
tion. We also assume that realised slots change with
a probability of 0.1. If they change, we assume
that half of the time, the user is the origin of the
change (because they changed their mind) and half
of the time the system is the origin of the change
(because of an ASR or interpretation error). Each
time a confidence score is changed, it has a proba-
bility of 0.5 to also change its value. The resulting
input to the NLG system are data structures of the
form present(cuisine=Indian), confidence=low.
5.3 The Reward Function
The main trade-off to optimise for IP in an incre-
mental setting is the timing and order of presenta-
tion. The agent has to decide whether to present
Attribute Values Confidence
Cuisine Chinese, French, German, In-, 0, 1, 2, 3, 4
dian, Italian, Japanese, Mexi-
can, Scottish, Spanish, Thai
Food bad, adequate, good, very good 0, 1, 2, 3, 4
Location 7 distinct areas of the city 0, 1, 2, 3, 4
Price cheap, expensive, good-price-
for-value, very expensive 0, 1, 2, 3, 4
Service bad, adequate, good, very good 0, 1, 2, 3, 4
Table 2: User goal slots for restaurant queries with possi-
ble values and confidence scores.
information as soon as it becomes available or else
wait until confidence for input hypotheses is more
stable. Alternatively, it can reorder information to
account for different confidence scores. We assign
the following rewards3: +100 if the user selects
an item, 0 if the user adds more search constraints,
?100 if the user does something else or the sys-
tem needs to self-correct,?0.5 for holding the floor,
and ?1 otherwise. In addition, the agent receives
an increasing negative reward for the waiting time,
waiting time2 (to the power of two), in terms of the
number of time steps passed since the last item was
presented. This reward is theoretically ??. The
agent is thus penalised stronger the longer it delays
IP. The rewards for user reactions are assigned at the
end of each episode, all other rewards are assigned
after each time step. One episode stretches from the
moment that a user specifies their initial preferences
to the moment in which they choose a restaurant.
The agent was trained for 10 thousand episodes.
6 Experimental Results
After training, the RL agent has learnt the following
incremental IP strategy. It will present information
slots as soon as they become available if they have
a medium or high confidence score. The agent will
then order attributes so that those slots with the high-
est confidence scores are presented first and slots
with lower confidence are presented later (by which
time they may have achieved higher confidence). If
no information is known with medium or high con-
3Handcrafted rewards are sufficient for this proof-of-
concept study, and can be learned from data for future models
(Rieser and Lemon, 2011).
54
101 102 103 104
?100
?80
?60
?40
?20
0
20
40
60
80
100
Av
er
ag
e 
Re
wa
rd
Episodes
 
 
RL
Base1
Base2
Base3
Figure 5: Performance in terms of rewards (averaged over
10 runs) for the RL agent and its baselines.
fidence, the agent will hold the floor or wait. In this
way, it can prevent self-corrections and minimise
waiting time?both of which yield negative rewards.
It can thus start speaking very early (avoiding long
pauses or semantically empty utterances) and still
has a low likelihood of having to self-correct.
For a comparison of the learnt policy with possi-
ble hand-crafted policies (because current incremen-
tal NLG systems are rule-based), we designed three
baselines. Baseline 1 always presents information
as soon as it is available, i.e. never waits. Base-
line 2 always waits until all information is known
with high confidence (i.e. until all confidence scores
are 3). Baseline 3 was chosen to be more ambi-
tious. It always presents information as soon as
possible, using a decreasing order of confidence to
minimise self-corrections (i.e. very similar to the
learnt policy). It chooses randomly among slots with
equal confidence. All baseline policies have an op-
timised IP strategy (recommend/summary etc.) and
differ only in their incremental processing strategies.
Baseline 1 is most similar to the current approach
used in spoken dialogue systems, where the Dia-
logue Manager triggers an NLG component as soon
as a task-relevant user utterance is processed.
Here we do not compare the different strategies
for IP generally because this has been done by
Rieser et al (2010), even if not for incremental
NLG. Figure 5 shows the performance of all be-
haviours in terms of average rewards. Baseline 1
obtains on average roughly 100 reward points less
than the RL agent. This corresponds to the (neg-
ative) reward of one self-correction (?100). Since
information is always presented as soon as it is avail-
able, this baseline needs to produce on average one
Baseline 1 (present information as soon as
it becomes available)
USR Please show me Italian places
(cuisine = Indian;conf score = 0.2)
SYS I have found 35 Indian . . .
USR No, ITALIAN
(cuisine = Italian; conf score = 0.8)
SYS Oh, you meant Italian. I have 104 Italian places in Edin-
burgh. 24 in the city centre, 21 in the New Town, 14 in . . .
USR Show me some in the city centre
(location = centre; conf score = 0.6)
SYS OK. I found 24 Italian restaurants in the city centre . . .
Baseline 2 (always wait until confidence is high)
USR Do you have Italian restaurants in the centre of town?
cuisine = Italian; conf score = 0.4
location = centre; conf score = 0.2
SYS waits
USR Italian in the centre.
cuisine = Italian, conf score = 0.7
location = centre, conf score = 0.5
SYS I have 104 Italian restaurants.
USR waits
SYS waits
USR city centre please
location = centre, conf score = 0.7
SYS I have 24 Italian restaurants in the city centre . . .
Baseline 3 (present information in decreasing
order of confidence)
USR I want Italian food . . .
cuisine = Indian, conf score = 0.2
location = centre, conf score = 0.3
SYS hmm (holding turn) . . .
USR in the centre of town
location = centre, conf score = 0.9
SYS In the centre, let me see, Indian . . .
USR Italian, please.
cuisine = Italian, conf score = 0.7
SYS Oh I see. I have 24 Italian places in the centre . . .
Figure 6: Example dialogues generated with the baseline
policies for a user who wants Italian food in the city cen-
tre. Confidence scores for cuisine and location variables
for the restaurants are shown as updated.
self-correction per episode. Baseline 2 needs to wait
until all information is known with high confidence
and obtains on average 125 to 130 rewards less than
the RL agent. This corresponds to approximately
11 time steps of waiting (for input to reach higher
confidence) before presentation since 11 is (approxi-
mately) the square root of 130. Baseline 3 is roughly
a reward of ?10 worse than the RL agent?s be-
55
haviour, which is due to a combination of more self-
corrections, even if they just occur occasionally, and
a higher number of turn holding markers. The latter
is due to the baseline starting to present as soon as
possible, so that whenever all confidence scores are
too low to start presenting, a turn holding marker
is generated. The learning agent learns to outper-
form all baselines significantly, by presenting infor-
mation slots in decreasing order of confidence, com-
bined with waiting and holding the floor at appro-
priate moments. Anticipating the rewards for wait-
ing vs. holding the floor at particular moments is the
main reason that the learnt policy outperforms Base-
line 3. Subtle moments of timing as in this case are
difficult to hand-craft and more appropriately bal-
anced using optimisation. An absolute comparison
of the last 1000 episodes of each behaviour shows
that the improvement of the RL agent corresponds
to 126.8% over Baseline 1, to 137.7% over Baseline
2 and to 16.76% over Baseline 3. All differences are
significant at p < 0.001 according to a paired t-test
and have a high effect size r > 0.9. The high per-
centage improvement of the learnt policy over Base-
lines 1 and 2 is mainly due to the high numeric val-
ues chosen for the rewards as can be observed from
their qualitative behaviour. Thus, if the negative nu-
meric values of, e.g., a self-correction were reduced,
the percentage reward would reduce, but the pol-
icy would not change qualitatively. Figure 1 shows
some examples of the learnt policy including several
incremental phenomena. In contrast, Figure 6 shows
examples generated with the baselines.
7 Conclusion and Future Directions
We have presented a novel framework combining in-
cremental and statistical approaches to NLG for in-
teractive systems. In a proof-of-concept study in the
domain of Information Presentation, we optimised
the timing and order of IP. The learning agent op-
timises the trade-off of whether to present informa-
tion as soon as it becomes available (for high respon-
siveness) or else to wait until input hypotheses were
more stable (to avoid self-corrections). Results in a
simulated environment showed that the agent learns
to avoid self-corrections and long waiting times, of-
ten by presenting information in order of decreas-
ing confidence. It outperforms three hand-crafted
baselines due to its enhanced adaptivity. In pre-
vious work, incremental responsiveness has mainly
been implemented by producing semantically empty
fillers such as um, let me see, well, etc. (Skantze and
Hjalmarsson, 2010). Our work avoids the need for
these fillers by content reordering.
Since this paper has focused on a proof-of-
concept study, our goal has not been to demonstrate
the superiority of automatic optimisation over hand-
crafted behaviour. Previous studies have shown
the advantages of optimisation (Janarthanam and
Lemon, 2010; Rieser et al, 2010; Dethlefs et al,
2011). Rather, our main goal has been to demon-
strate that incremental NLG can be phrased as an op-
timisation problem and that reasonable action poli-
cies can be learnt so that an application within an
incremental framework is feasible. This observation
allows us to take incremental systems, which so far
have been restricted to deterministic decision mak-
ing, one step further in terms of their adaptability
and flexibility. To demonstrate the effectiveness of
a synergy between RL and incremental NLG on a
large scale, we would like to train a fully incremental
NLG system from human data using a data-driven
reward function. Further, an evaluation with human
users will be required to verify the advantages of dif-
ferent policies for Information Presentation.
Regarding the scalability of our optimisation
framework, RL systems are known to suffer from the
curse of dimensionality, the problem that their state
space grows exponentially according to the number
of variables taken into account. While the appli-
cation of flat RL is therefore limited to small-scale
problems, we can use RL with a divide-and-conquer
approach, hierarchical RL, which has been shown to
scale to large-scale NLG applications (Dethlefs and
Cuaya?huitl, 2011), to address complex NLG tasks.
Future work can take several directions. Cur-
rently, we learn the agent?s behaviour offline, be-
fore the interaction, and then execute it statistically.
More adaptivity towards individual users and situ-
ations could be achieved if the agent was able to
learn from ongoing interactions using online learn-
ing. In addition, current NLG systems tend to as-
sume that the user?s goals and situational circum-
stances are known with certainty. This is often an
unrealistic assumption that future work could ad-
dress using POMDPs (Williams and Young, 2007).
56
Acknowledgements
The research leading to this work has received fund-
ing from EC?s FP7 programmes: (FP7/2011-14)
under grant agreement no. 287615 (PARLANCE);
(FP7/2007-13) under grant agreement no. 216594
(CLASSiC); (FP7/2011-14) under grant agreement
no. 270019 (SPACEBOOK); (FP7/2011-16) under
grant agreement no. 269427 (STAC).
References
Gabor Angeli, Percy Liang, and Dan Klein. 2010. A
simple domain-independent probabilistic approach to
generation. In Proc. of EMNLP, pages 502?512.
Timo Baumann and David Schlangen. 2011. Predict-
ing the Micro-Timing of User Input for an Incremental
Spoken Dialogue System that Completes a User?s On-
going Turn. In Proc. of 12th Annual SIGdial Meeting
on Discourse and Dialogue, Portland, OR.
Timo Baumann, Okko Buss, and David Schlangen. 2011.
Evaluation and Optimisation of Incremental Proces-
sors. Dialogue and Discourse, 2(1).
Anja Belz. 2008. Automatic Generation of Weather
Forecast Texts Using Comprehensive Probabilistic
Generation-Space Models. Natural Language Engi-
neering, 14(4):431?455.
Okko Buss and David Schlangen. 2011. DIUM?An In-
cremental Dialogue Manager That Can Produce Self-
Corrections. In Proc. of the Workshop on the Seman-
tics and Pragmatics of Dialogue (SemDIAL / Los An-
gelogue), Los Angeles, CA.
Okko Buss, Timo Baumann, and David Schlangen. 2010.
Collaborating on Utterances with a Spoken Dialogue
Systen Using an ISU-based Approach to Incremental
Dialogue Management. In Proc. of 11th Annual SIG-
dial Meeting on Discourse and Dialogue.
Nina Dethlefs and Heriberto Cuaya?huitl. 2011.
Combining Hierarchical Reinforcement Learning and
Bayesian Networks for Natural Language Generation
in Situated Dialogue. In Proc. of the 13th European
Workshop on Natural Language Generation (ENLG),
Nancy, France.
Nina Dethlefs, Heriberto Cuaya?huitl, and Jette Viethen.
2011. Optimising Natural Language Generation Deci-
sion Making for Situated Dialogue. In Proceedings of
the 12th Annual Meeting on Discourse and Dialogue
(SIGdial), Portland, Oregon, USA.
David DeVault, Kenji Sagae, and David Traum. 2009.
Can I finish? Learning when to respond to incremental
interpretation result in interactive dialogue. In Proc.
of the 10th Annual SigDial Meeting on Discourse and
Dialogue, Queen Mary University, UK.
Srini Janarthanam and Oliver Lemon. 2010. Learning to
Adapt to Unknown Users: Referring Expression Gen-
eration in Spoken Dialogue Systems. In Proc. of the
48th Annual Meeting of the Association for Computa-
tional Linguistics (ACL), pages 69?78, July.
Anne Kilger and Wolfgang Finkler. 1995. Incremen-
tal generation for real-time applications. Technical re-
port, DFKI Saarbruecken, Germany.
Willem Levelt. 1989. Speaking: From Intenion to Artic-
ulation. MIT Press.
Franc?ois Mairesse, Milica Gas?ic?, Filip Jurc???c?ek, Simon
Keizer, Blaise Thomson, Kai Yu, and Steve Young.
2010. Phrase-based statistical language generation us-
ing graphical models and active learning. In Proc. of
the Annual Meeting of the Association for Computa-
tional Linguistics (ACL), pages 1552?1561.
Matthew Purver and Masayuki Otsuka. 2003. Incremen-
tal Generation by Incremental Parsing. In Proceedings
of the 6th UK Special-Interesting Group for Computa-
tional Linguistics (CLUK) Colloquium.
Antoine Raux and Maxine Eskenazi. 2009. A Finite-
State Turn-Taking Model for Spoken Dialog Sys-
tems. In Proc. of the 10th Conference of the North
American Chapter of the Association for Compu-
tational Linguistics?Human Language Technologies
(NAACL-HLT), Boulder, Colorado.
Verena Rieser and Oliver Lemon. 2011. Reinforcement
Learning for Adaptive Dialogue Systems: A Data-
driven Methodology for Dialogue Management and
Natural Language Generation. Book Series: The-
ory and Applications of Natural Language Processing,
Springer, Berlin/Heidelberg.
Verena Rieser, Oliver Lemon, and Xingkun Liu. 2010.
Optimising Information Presentation for Spoken Dia-
logue Systems. In Proc. of the 48th Annual Meeting of
the Association for Computational Linguistics (ACL),
Uppsala, Sweden.
David Schlangen and Gabriel Skantze. 2009. A General,
Abstract Model of Incremental Dialogue Processing.
In Proc. of the 12th Conference of the European Chap-
ter of the Association for Computational Linguistics,
Athens, Greece.
Gabriel Skantze and Anna Hjalmarsson. 2010. Towards
Incremental Speech Generation in Dialogue Systems.
In Proc. of the 11th Annual SigDial Meeting on Dis-
course and Dialogue, Tokyo, Japan.
Gabriel Skantze and David Schlangen. 2009. Incre-
mental Dialogue Processing in a Micro-Domain. In
Proc. of the 12th Conference of the European Chap-
ter of the Association for Computational Linguistics,
Athens, Greece.
Amanda Stent, Rashmi Prasad, and Marilyn Walker.
2004. Trainable sentence planning for complex infor-
mation presentation in spoken dialogue systems. In
57
Proc. of the Annual Meeting of the Association for
Computational Linguistics.
Kees van Deemter. 2009. What game theory can do for
NLG: the case of vague language. In 12th European
Workshop on Natural Language Generation (ENLG).
Marilyn Walker, Steve Whittaker, Amanda Stent, Pre-
taam Maloor, Johanna Moore, and G Vasireddy.
2004. Generation and Evaluation of User Tailored Re-
sponses in Multimodal Dialogue. Cognitive Science,
28(5):811?840.
Chris Watkins. 1989. Learning from Delayed Rewards.
PhD Thesis, King?s College, Cambridge, UK.
Jason Williams and Steve Young. 2007. Partially
Observable Markov Decision Processes for Spoken
Dialog Systems. Computer Speech and Language,
21(2):393?422.
58
NAACL-HLT 2012 Workshop on Future directions and needs in the Spoken Dialog Community: Tools and Data, pages 15?16,
Montre?al, Canada, June 7, 2012. c?2012 Association for Computational Linguistics
Incremental Spoken Dialogue Systems: Tools and Data
Helen Hastie, Oliver Lemon, Nina Dethlefs
The Interaction Lab, School of Mathematics and Computer Science
Heriot-Watt University, Edinburgh, UK EH14 4AS
h.hastie, o.lemon, n.s.dethlefs@hw.ac.uk
Abstract
Strict-turn taking models of dialogue do not
accurately model human incremental process-
ing, where users can process partial input and
plan partial utterances in parallel. We discuss
the current state of the art in incremental sys-
tems and propose tools and data required for
further advances in the field of Incremental
Spoken Dialogue Systems.
1 Incremental Spoken Dialogue Systems
For Spoken Dialogue Systems (SDS) to be more fre-
quently adopted, advances in the state-of-the-art are
necessary to enable highly responsive and conversa-
tional systems. Traditionally, the unit of speech has
been a whole utterance with strict, rigid turn-taking
determined by a voice-activity detector. However,
a large body of psycholinguistic literature indicates
that human-human interaction is in fact incremen-
tal (Tanenhaus and Brown-Schmidt, 2008; Levelt,
1989). Using a whole utterance as the unit of choice
makes dialogues longer, unnatural and stilted and ul-
timately interferes with a user?s ability to focus on
their goal (Allen et al, 2001).
A new generation of Incremental SDS (ISDS) are
being developed that deal with ?micro-turns? (sub-
utterance processing units) resulting in dialogues
that are more fluid and responsive. Recent work
has shown that processing smaller ?chunks? of input
and output can improve the user experience (Aist et
al., 2007; Skantze and Schlangen, 2009; Buss et al,
2010; Baumann et al, 2011; Selfridge et al, 2011).
Incrementality enables the system designer to model
several dialogue phenomena that play a vital role
in human discourse (Levelt, 1989) but have so far
been absent from systems. These include more
natural turn-taking through rapid system responses,
grounding through the generation of backchannels
and feedback, and barge-ins (from both user and sys-
tem). In addition, corrections and self-corrections
through constant monitoring of user and system ut-
terances play an important role, enabling the system
to recover smoothly from a recognition error or a
change in user?s preferences. Some examples of the
phenomena we are targeting are given in Figure 1.
Parlance, a FP7 EC project1, is currently develop-
ing incremental systems for English and Mandarin.
The goal of Parlance is to develop mobile, interac-
tive, ?hyper-local? search through speech. Recent
trends in Information Retrieval are towards incre-
mental, interactive search. Spoken dialogue systems
can provide a truly natural medium for this type of
search, in particular for people on the move.
2 Tools and Data
The emphasis of the Parlance project is on data-
driven techniques for ISDS, thereby addressing the
problem of a lack of data for system develop-
ment. Although incremental dialogue phenomena
described in Figure 1 have been observed in human-
human dialogue, more task-based data is needed. It
is challenging to fabricate a situation where users
produce incremental discourse phenomena as in Fig-
ure 1 frequently and in a natural manner. Wizard-
1http://www.parlance-project.eu
15
Backchannels (when the user pauses)
USR I want Italian food [500 ms] in the centre of town . . .
SYS uh-huh
SYS OK. I found 24 Italian restaurants in the city centre. The
restaurant Roma is in the medium price range,. . .
Self-correction (the system made a mistake)
USR I want Italian food in the centre of town . . .
SYS OK. I found 35 Indian restaurants . . .
USR No, I want Italian.
SYS oh sorry . . .
SYS I have 24 Italian restaurants in the city centre . . .
Holding the floor
USR I want cheap Italian food . . .
SYS ok let me see
SYS I have 3 cheap Italian places . . .
Figure 1: Incremental phenomena observed in human-
human dialogue that systems should be able to model.
of-Oz experiments can be used to collect data from
the system side, but user-initiated phenomena, such
as the user changing his/her mind are more difficult
to instigate. Therefore, data collections of naturally
occurring incremental phenomena in human-human
settings will be essential for further development of
incremental systems. Such data can inform user sim-
ulations which provide means of training stochastic
SDS with less initial data and can compensate for
data sparsity. For example, in Dethlefs et al (2012)
the user simulation can change its mind and react to
different NLG strategies such as giving information
with partial input or waiting for complete input from
the user. Both the academic community and industry
would benefit from open access data, such as will be
collected in the Parlance project and made available
to the dialogue community2. There would also need
to be a clear path from academic research on ISDS
to industry standards such as VoiceXML to facilitate
adoption.
Various components and techniques of ISDS are
needed to handle ?micro-turns?. Challenges here
include recognizing and understanding partial user
input and back-channels; micro-turn dialogue man-
agement that can decide when to back-channel, self-
correct and hold-the-floor; incremental NLG that
can generate output while the user is still talking;
2As was done for CLASSiC project data at:
http://www.macs.hw.ac.uk/iLabArchive/CLASSiCProject/Data/login.php
and finally more flexible TTS that can handle barge-
in and understand when it has been interrupted.
In summary, in order to achieve highly natural,
responsive incremental systems, we propose using
data-driven techniques, for which the main issue is
lack of data. Carefully crafted task-based human-
human data collection and WoZ studies, user simu-
lations, shared data archives, and upgraded industry
standards are required for future work in this field.
Acknowledgments
The research leading to this work has received fund-
ing from the EC?s FP7 programme: (FP7/2011-14)
under grant agreement no. 287615 (PARLANCE).
References
Gregory Aist, James Allen, Ellen Campana, Lucian
Galescu, Carlos Gomez Gallo, Scott Stoness, Mary
Swift, and Michael Tanenhaus. 2007. Software ar-
chitectures for incremental understanding of human
speech. In Proceedings of SemDial / DECALOG.
James Allen, George Ferguson, and Amanda Stent. 2001.
An Architecture For More Realistic Conversational
Systems. In Proc. of Intelligent User Interfaces.
Timo Baumann, Okko Buss, and David Schlangen. 2011.
Evaluation and Optimisation of Incremental Proces-
sors. Dialogue and Discourse, 2(1).
Okko Buss, Timo Baumann, and David Schlangen. 2010.
Collaborating on Utterances with a Spoken Dialogue
Systen Using an ISU-based Approach to Incremental
Dialogue Management. In Proc. of SIGDIAL.
Nina Dethlefs, Helen Hastie, Verena Rieser, and Oliver
Lemon. 2012. Optimising Incremental Generation
for Spoken Dialogue Systems: Reducing the Need for
Fillers. In Proc of INLG, Chicago, Illinois, USA.
Willem Levelt. 1989. Speaking: From Intenion to Artic-
ulation. MIT Press.
Ethan Selfridge, Iker Arizmendi, Peter Heeman, and Ja-
son Williams. 2011. Stability and Accuracy in Incre-
mental Speech Recognition. In Proc. of SigDial.
Gabriel Skantze and David Schlangen. 2009. Incremen-
tal Dialogue Processing in a Micro-Domain. In Proc.
of EACL, Athens, Greece.
M.K. Tanenhaus and S. Brown-Schmidt. 2008. Lan-
guage processing in the natural world. In B.C.M
Moore, L.K. Tyler, and W.D. Marslen-Wilson, edi-
tors, The perception of speech: from sound to meaning,
pages 1105?1122.
16
Proceedings of the 14th European Workshop on Natural Language Generation, pages 115?124,
Sofia, Bulgaria, August 8-9 2013. c?2013 Association for Computational Linguistics
Generating student feedback from time-series data using Reinforcement
Learning
Dimitra Gkatzia, Helen Hastie, Srinivasan Janarthanam and Oliver Lemon
Department of Mathematical and Computer Sciences
Heriot-Watt University
Edinburgh, Scotland
{dg106, h.hastie, sc445, o.lemon} @hw.ac.uk
Abstract
We describe a statistical Natural Language
Generation (NLG) method for summarisa-
tion of time-series data in the context of
feedback generation for students. In this
paper, we initially present a method for
collecting time-series data from students
(e.g. marks, lectures attended) and use ex-
ample feedback from lecturers in a data-
driven approach to content selection. We
show a novel way of constructing a reward
function for our Reinforcement Learning
agent that is informed by the lecturers?
method of providing feedback. We eval-
uate our system with undergraduate stu-
dents by comparing it to three baseline
systems: a rule-based system, lecturer-
constructed summaries and a Brute Force
system. Our evaluation shows that the
feedback generated by our learning agent
is viewed by students to be as good as the
feedback from the lecturers. Our findings
suggest that the learning agent needs to
take into account both the student and lec-
turers? preferences.
1 Introduction
Data-to-text generation refers to the task of auto-
matically generating text from non-linguistic data
(Reiter and Dale, 2000). The goal of this work is
to develop a method for summarising time-series
data in order to provide continuous feedback to
students across the entire semester. As a case
study, we took a module in Artificial Intelligence
and asked students to fill out a very short diary-
type questionnaire on a weekly basis. Questions
included, for example, number of deadlines, num-
ber of classes attended, severity of personal issues.
These data were then combined with the marks
from the weekly lab reflecting the students? per-
formance. As data is gathered each week in the
lab, we now have a set of time-series data and our
goal is to automatically create feedback. The goal
is to present a holistic view through these diary en-
tries of how the student is doing and what factors
may be affecting performance.
Feedback is very important in the learning pro-
cess but very challenging for academic staff to
complete in a timely manner given the large num-
ber of students and the increasing pressures on
academics? time. This is where automatic feed-
back can play a part, providing a tool for teachers
that can give insight into factors that may not be
immediately obvious (Porayska-Pomsta and Mel-
lish, 2013). As reflected in NSS surveys1, stu-
dents are not completely satisfied with how feed-
back is currently delivered. The 2012 NSS survey,
for all disciplines reported an 83% satisfaction rate
with courses, with 70% satisfied with feedback.
This has improved from recent years (in 2006 this
was 60% for feedback) but shows that there is
still room for improvement in how teachers deliver
feedback and its content.
In the next section (Section 2) a discussion of
the related work is presented. In Section 3, a de-
scription of the methodology is given as well as
the process of the data collection from students,
the template construction and the data collection
with lecturers. In Section 4, the Reinforcement
Learning implementation is described. In Section
5, the evaluation results are presented, and finally,
in Sections 6 and 7, a conclusion and directions
for future work are discussed.
2 Related Work
Report generation from time-series data has been
researched widely and existing methods have been
used in several domains such as weather forecasts
(Belz and Kow, 2010; Angeli et al, 2010; Sripada
et al, 2004), clinical data summarisation (Hunter
1http://www.thestudentsurvey.com/
115
et al, 2011; Gatt et al, 2009), narrative to assist
children with communication needs (Black et al,
2010) and audiovisual debriefs from sensor data
from Autonomous Underwater Vehicles missions
(Johnson and Lane, 2011).
The two main challenges for time-series data
summarisation are what to say (Content Selec-
tion) and how to say it (Surface Realisation). In
this work we concentrate on the former. Previ-
ous methods for content selection include Gricean
Maxims (Sripada et al, 2003); collective con-
tent selection (Barzilay and Lapata, 2004); and
the Hidden Markov model approach for content
selection and ordering (Barzilay and Lee, 2004).
NLG systems tend to be very domain-specific
and data-driven systems that seek to simultane-
ously optimize both content selection and sur-
face realisation have the potential to be more
domain-independent, automatically optimized and
lend themselves to automatic generalization (An-
geli et al, 2010; Rieser et al, 2010; Dethlefs
and Cuayahuitl, 2011). Recent work on report
generation uses statistical techniques from Ma-
chine Translation (Belz and Kow, 2010), super-
vised learning (Angeli et al, 2010) and unsuper-
vised learning (Konstas and Lapata, 2012).
Here we apply Reinforcement Learning meth-
ods (see Section 4 for motivation) which have been
successfully applied to other NLG tasks, such as
Temporal Expressions Generation (Janarthanam
et al, 2011), Lexical Choice (Janarthanam and
Lemon, 2010), generation of adaptive restaurant
summaries in the context of a dialogue system
(Rieser et al, 2010) and generating instructions
(Dethlefs and Cuayahuitl, 2011).
3 Methodology
Figure 1: Methodology for data-driven feedback
report generation
Figure 1 shows graphically our approach to the de-
velopment of a generation system. Firstly, we col-
lected data from students including marks, demo-
graphic details and weekly study habits. Next, we
created templates for surface realisation with the
help of a Teaching and Learning expert. These
templates were used to generate summaries that
were rated by lecturers. We used these ratings to
train the learning agent. The output of the learning
agent (i.e. automatically generated feedback re-
ports) were finally evaluated by the students. Each
of these steps are discussed in turn.
3.1 Time-series Data Collection from
Students
The data were collected during the weekly lab ses-
sions of a Computer Science module which was
taught to third year Honours and MSc students
over the course of a 10 week semester. We re-
cruited 26 students who were asked to fill in a
web-based diary-like questionnaire. Initially, we
asked students to provide some demographic de-
tails (age, nationality, level of study). In addition,
students provided on a weekly basis, information
for nine factors that could influence their perfor-
mance. These nine factors were motivated from
the literature and are listed here in terms of effort
(Ames, 1992), frustration (Craig et al, 2004) , dif-
ficulty (Person et al, 1995; Fox, 1993) and per-
formance (Chi et al, 2001). Effort is measured
by three factors: (1) how many hours they studied;
(2) the level of revision they have done; (3) as well
as the number of lectures (of this module) they at-
tended. Frustration is measured by (4) the level
of understandability of the content; (5) whether
they have had other deadlines; and whether they
faced any (6) health and/or (7) personal issues and
at what severity. The difficulty of the lab exercises
is measured by (8) the students? perception of dif-
ficulty. Finally, (9) marks achieved by the students
in each weekly lab was used as a measure of their
performance.
3.2 Data Trends
Initially, the data were processed so as to iden-
tify the existing trend of each factor during the
semester, (e.g. number of lectures attending de-
creases). The tendencies of the data are estimated
using linear least-squares regression, with each
factor annotated as INCREASING, DECREAS-
ING or STABLE. In addition, for each student we
perform a comparison between the average of each
116
Type Description Examples
AVERAGE describes the factor data by either averaging the values given by
the student,
?You spent 2 hours studying the lecture material
on average?. (HOURS STUDIED)
or by comparing the student?s average with the class average
(e.g. if above the mean value for the class, we say that the ma-
terial is challenging).
?You found the lab exercises very challenging?.
(DIFFICULTY)
TREND discusses the trend of the data, e.g. increasing, decreasing or
stable.
?Your workload is increasing over the
semester?. (DEADLINES)
WEEKS talks about specific events that happened in one or more weeks. ?You have had other deadlines during weeks 5,
6 and 9?. (DEADLINES)
OTHER all other expressions that are not directly related to data. ?Revising material during the semester will im-
prove your performance?. (REVISION)
Table 1: The table explains the different template types.
factor and the class average of the same factor.
3.3 Template Generation
The wording and phrasing used in the templates to
describe the data were derived from working with
and following the advice of a Learning and Teach-
ing (L&T) expert. The expert provided consulta-
tion on how to summarise the data. We derived 4
different kinds of templates for each factor: AV-
ERAGE, TREND, WEEKS and OTHER based on
time-series data on plotted graphs. A description
of the template types is shown in Table 1.
In addition, the L&T expert consulted on how
to enhance the templates so that they are ap-
propriate for communicating feedback accord-
ing to the guidelines of the Higher Education
Academy (2009), for instance, by including moti-
vating phrases such as ?You may want to plan your
study and work ahead?.
3.4 Data Collection from Lecturers
The goal of the Reinforcement Learning agent is
to learn to generate feedback at least as well as
lecturers. In order to achieve this, a second data
collection was conducted with 12 lecturers partic-
ipating.
The data collection consisted of three stages
where lecturers were given plotted factor graphs
and were asked to:
1. write a free style text summary for 3 students
(Figure 2);
2. construct feedback summaries using the tem-
plates for 3 students (Figure 3);
3. rate random feedback summaries for 2 stu-
dents (Figure 4).
We developed the experiment using the Google
Web Toolkit for Web Applications, which facil-
itates the development of client-server applica-
tions. The server side hosts the designed tasks and
stores the results in a datastore. The client side is
responsible for displaying the tasks on the user?s
browser.
In Task 1, the lecturers were presented with the
factor graphs of a student (one graph per factor)
and were asked to provide a free-text feedback
summary for this student. The lecturers were en-
couraged to pick as many factors as they wanted
and to discuss the factors in any order they found
useful. Figure 2 shows an example free text sum-
mary for a high performing student where the lec-
turer decided to talk about lab marks and under-
standability. Each lecturer was asked to repeat this
task 3 times for 3 randomly picked students.
In Task 2, the lecturers were again asked to con-
struct a feedback summary but this time they were
given a range of sentences generated from the tem-
plates (as described in Section 2.3). They were
asked to use these to construct a feedback report.
The number of alternative utterances generated for
each factor varies depending on the factor and the
given data. In some cases, a factor can have 2 gen-
erated utterances and in other cases up to 5 (with
a mean of 3 for each factor) and they differenti-
ate in the style of trend description and wording.
Again the lecturer was free to choose which fac-
tors to talk about and in which order, as well as
to decide on the template style he/she prefers for
the realisation through the template options. Fig-
ure 3 shows an example of template selection for
the same student as in Figure 2.
In Task 3, the lecturers were presented with the
plotted factor graphs plus a corresponding feed-
back summary that was generated by randomly
choosing n factors and their templates, and were
asked to rate it in a scale between 0-100 (100 for
the best summary). Figure 4 shows an example of
117
Figure 2: The interface of the 1st task of the data collection: the lecturer consults the factor graphs and
provides feedback in a free text format.
Figure 3: The interface of the 2nd task of data collection: the lecturer consults the graphs and constructs
a feedback summary from the given templates (this graph refers to the same student as Figure 2).
a randomly generated summary for the same stu-
dent as in Figure 2.
4 Learning a Time-Series Generation
Policy
Reinforcement Learning (RL) is a machine learn-
ing technique that defines how an agent learns to
take optimal actions in a dynamic environment so
as to maximize a cumulative reward (Sutton and
Barto, 1998). In our framework, the task of con-
tent selection of time-series data is presented as a
Markov Decision problem. The goal of the agent
is to learn to choose a sequence of actions that
obtain the maximum expected reward in the long
run. In this section, we describe the Reinforce-
ment Learning setup for learning content selection
118
Figure 4: The interface of the 3rd task of data col-
lection: the lecturer consults the graphs and rates
the randomly generated feedback summary (this
graph refers to the same student as Figures 2 and
3).
from time-series data for feedback report gener-
ation. Summarisation from time-series data is an
open challenge and we aim to research other meth-
ods in the future, such as supervised learning, evo-
lutionary algorithms etc.
4.1 Actions and States
In this learning setup, we focused only on select-
ing the correct content, i.e. which factors to talk
about. The agent selects a factor and then decides
whether to talk about it or not. The state consists
of a description of the factor trends and the num-
ber of templates that have been selected so far. An
example of the initial state of a student can be:
<marks increased, lectures attended stable,
hours studied increased, understandability stable,
difficulty increased, health issues stable, per-
sonal issues stable, revision increased, 0>
The agent explores the state space by selecting a
factor and then by deciding whether to talk about
it or not. If the agent decides to talk about the
selected factor, it chooses the template in a greedy
way, i.e. it chooses for each factor the template
that results in a higher reward. After an action has
been selected, it is deleted from the action space.
4.1.1 Ordering
In order to find out in which order the lectur-
ers describe the factors, we transformed the feed-
back summaries into n-grams of factors. For in-
stance, a summary that talks about the student?s
performance, the number of lectures that he/she
attended, potential health problems and revision
done can be translated into the following ngram:
start, marks, lectures attended, health issues, re-
vision, end. We used the constructed n-grams to
compute the bigram frequency of the tokens in or-
der to identify which factor is most probable to be
referred to initially, which factors follow particu-
lar factors and which factor is usually talked about
in the end. It was found that the most frequent or-
dering is: start, marks, hours studied, understand-
ability, difficulty, deadlines, health issues, per-
sonal issues, lectures attended, revision, end.
4.2 Reward Function
The goal of the reward function is to optimise the
way lecturers generate and rate feedback. Given
the expert annotated summaries from Task 1, the
constructed summaries from Task 2 and the ratings
from Task 3, we derived the multivariate reward
function:
Reward = a +
n?
i=1
bi ? xi + c ? length
where X = {x1, x2, ..., xn} represents the
combinations between the data trends observed in
the time-series data and the corresponding lectur-
ers? feedback (i.e. whether they included a factor
to be realised or not and how). The value xi for
factor i is defined by the function:
xi =
?
?????
?????
1, the combination i of a factor trend
and a template type is included in
the feedback
0, if not.
For instance, the value of x1 is 1 if marks were
increased and this trend is realised in the feedback,
otherwise it is 0. In our domain n = 90 in order to
cover all the different combinations. The length
stands for the number of factors selected, a is the
intercept, bi and c are the coefficients for xi and
length respectively.
In order to model the reward function, we used
linear regression to compute the weights from the
data gathered from the lecturers. Therefore, the
reward function is fully informed by the data pro-
vided by the experts. Indeed, the intercept a, the
vector weights b and the weight c are learnt by
making use of the data collected by the lecturers
from the 3 tasks discussed in Section 3.4.
The reward function is maximized (Reward
= 861.85) for the scenario (i.e. each student?s
data), content selection and preferred template
style shown in Table 2 (please note that this sce-
nario was not observed in the data collection).
119
Factor Trend Template
difficulty stable NOT MENTIONED
hours studied stable TREND
understandability stable NOT MENTIONED
deadlines increase WEEKS
health issues stable WEEKS
personal issues stable WEEKS
lectures att. stable WEEKS
revision stable OTHER
marks increase TREND
Table 2: The table shows the scenario at which the
reward function is maximised.
The reward function is minimized (Reward =
-586.0359) for the scenario shown in Table 3
(please note that this scenario also was not ob-
served in the data collection).
Factor Trend Template
difficulty increase AVERAGE
hours studied stable NOT MENTIONED
understandability decrease AVERAGE
deadlines * *
health issues increase TREND
personal issues stable TREND
lectures att. stable NOT MENTIONED
revision stable AVERAGE
marks stable TREND
Table 3: The table shows the scenario at which the
reward function is minimised (* denotes multiple
options result in the same minimum reward).
4.3 Training
We trained a time-series generation policy
for 10,000 runs using the Tabular Temporal-
Difference Learning (Sutton and Barto, 1998).
During the training phase, the learning agent gen-
erated feedback summaries. When the construc-
tion of the summary begins, the length of the sum-
mary is 0. Each time that the agent adds a template
(by selecting a factor), the length is incremented,
thus changing the state. It repeats the process until
it decides for all factors whether to talk about them
or not. The agent is finally rewarded at the end of
the process using the Reward function described
in Section 3.2. Initially, the learning agent selects
factors randomly, but gradually learns to identify
factors that are highly rewarding for a given data
scenario. Figure 5 shows the learning curve of the
agent.
Figure 5: Learning curve for the learning agent.
The x-axis shows the number of summaries pro-
duced and y- axis the total reward received for
each summary.
5 Evaluation
We evaluated the system using the reward func-
tion and with students. In both these evaluations,
we compared feedback reports generated using
our Reinforcement Learning agent with four other
baseline systems. Here we present a brief descrip-
tion of the baseline systems.
Baseline 1: Rule-based system. This system
selects factors and templates for generation using a
set of rules. These hand-crafted rules were derived
from a combination of the L&T expert?s advice
and a student?s preferences and is therefore a chal-
lenging baseline and represents a middle ground
between the L&T expert?s advice and a student?s
preferences. An example rule is: if the mark aver-
age is less than 50% then refer to revision.
Baseline 2: Brute Force system. This system
performs a search of the state space, by exploring
randomly as many different feedback summaries
as possible. The Brute Force algorithm is shown
below:
Algorithm 1 Brute Force algorithm
I n p u t d a t a : D
f o r n = 0 . . . 1 0 , 0 0 0
c o n s t r u c t randomly f e e d b a c k [ n ]
a s s i g n getReward [ n ]
i f ge tReward [ n]>getReward [ n?1]
b e s t F e e d b a c k = f e e d b a c k [ n ]
e l s e
b e s t F e e d b a c k = f e e d b a c k [ n?1]
r e t u r n b e s t F e e d b a c k
In each run the algorithm constructs a feedback
summary, then it calculates its reward, using the
same reward function used for the Reinforcement
Learning approach, and if the reward of the new
feedback is better than the previous, it keeps the
120
new one as the best. It repeats this process for
10,000 times for each scenario. Finally, the algo-
rithm returns the summary that scored the highest
ranking.
Baseline 3: Lecturer-produced summaries.
These are the summaries produced by the lectur-
ers, as described in Section 2.4, for Task 2 using
template-generated utterances.
Baseline 4: Random system: The Random
system constructs feedback summaries by select-
ing factors and templates randomly as described in
Task 3 (in Section 3.4).
5.1 Evaluation with Reward Function
Table 4 presents the results of the evaluation per-
formed using the Reward Function, comparing
the learned policy with the four baseline systems.
Each system generated 26 feedback summaries.
On average the learned policy scores significantly
higher than any other baseline for the given sce-
narios (p <0.05 in a paired t-test).
Time-Series Summarisation Systems Reward
Learned 243.82
Baseline 1: Rule-based 107.77
Baseline 2: Brute Force 241.98
Baseline 3: Lecturers 124.62
Baseline 4: Random 43.29
Table 4: The table summarises the average re-
wards that are assigned to summaries produced
from the different systems.
5.2 Evaluation with Students
A subjective evaluation was conducted using 1st
year students of Computer Science as participants.
We recruited 17 students, who were all English na-
tive speakers. The participants were shown 4 feed-
back summaries in a random order, one generated
by the learned policy, one from the rule-based sys-
tem (Baseline 1), one from the Brute Force system
(Baseline 2) and one summary produced by a lec-
turer using the templates (Baseline 3). Given the
poor performance of the Random system in terms
of reward, Baseline 4 was omitted from this study.
Overall there were 26 different scenarios, as de-
scribed in Section 3.1. All summaries presented
to a participant were generated from the same sce-
nario. The participants then had to rank the sum-
maries in order of preference: 1 for the most pre-
ferred and 4 for the least preferred. Each partici-
pant repeated the process for 4.5 scenarios on aver-
age (the participant was allowed to opt out at any
stage). The mode values of the rankings of the
preferences of the students are shown in Table 5.
The web-based system used for the evaluation is
shown in Figure 6.
System Mode of Rankings
Learned 3rd
Baseline 3: Lecturers 3rd
Baseline 1: Rule-based 1st
Baseline 2: Brute Force 4th
Table 5: The table shows the mode value of the
rankings of the preference of the students.
We ran a Mann-Whitney?s U test to evaluate the
difference in the responses of our 4-point Likert
Scale question between the Learned system and
the other three baselines. It was found that, for
the given data, the preference of students for the
feedback generated by the Learned system is as
good as the feedback produced by the experts, i.e.
there is no significant difference between the mean
value of the rankings of the Learned system and
the lecturer-produced summaries (p = 0.8) (Base-
line 3).
The preference of the users for the Brute Force
system does not differ significantly from the sum-
maries generated by the Learned system (p =
0.1335). However, the computational cost of the
Brute Force is higher because each time that the
algorithm sees a new scenario it has to run ap-
proximately 3k times to reach a good summary (as
seen in Figure 7) and about 10k to reach an optimal
one, which corresponds to 46 seconds. This delay
would prohibit the use of such a system in time-
critical situations (such as defence) and in live sys-
tems such as tutoring systems. In addition, the
processing time would increase with more compli-
cated scenarios and if we want to take into account
the ordering of the content selection and/or if we
have more variables. In contrast, the RL method
needs only to be trained once.
Finally, the users significantly preferred the
summaries produced by the Rule-based system
(Baseline 1) to the summaries produced by the
Learned system. This is maybe because of the fact
that in the rule-based system some knowledge of
the end user?s preferences (i.e. students) was taken
into account in the rules which was not the case
in the other three systems. This fact suggests that
121
Figure 6: The interface for the evaluation: the students viewed the four feedback summaries and ranked
them in order of preference. From left to right, the summaries as generated by: an Expert (Baseline 3),
the Rule based system (Baseline 1), the Brute Force algorithm (Baseline 2), the Learned system.
Figure 7: The graphs shows the number of cycles
that the Brute Force algorithm needs to achieve
specific rewards.
students? preferences should be taken into account
as they are the receivers of the feedback. This can
also be generalised to other areas, where the ex-
perts and the end users are not the same group
of people. As the learned policy was not trained
to optimise for the evaluation criteria, in future,
we will explore reward functions that bear in mind
both the expert knowledge and the student?s pref-
erences.
6 Conclusion
We have presented a statistical learning approach
to summarisation from time-series data in the area
of feedback reports. In our reports, we took into
account the principles of good feedback provision
as instructed by the Higher Education Academy.
We also presented a method for data gathering
from students and lecturers and show how we can
use these data to generate feedback by presenting
the problem as a Markov Decision Process and
optimising it using Reinforcement Learning tech-
niques. We also showed a way of constructing a
data-driven reward function that can capture de-
pendencies between the time-series data and the
realisation phrases, in a similar way that the lec-
turers do when providing feedback. Finally, our
evaluation showed that the learned report genera-
tion policy generates reports as well as lecturers.
7 Future Work
We aim to conduct further qualitative research in
order to explore what factors and templates stu-
dents find useful to be included in the feedback
and inform our reward function with this informa-
tion as well as what we have observed in the lec-
turer data collection. This way, we hope, not only
to gain insights into what is important to students
and lecturers but also to develop a data-driven ap-
proach that, unlike the rule-based system, does not
require expensive and difficult-to-obtain expert in-
put from Learning and Teaching experts. In ad-
dition, we want to compare RL techniques with
supervised learning approaches and evolutionary
algorithms. Finally, we want to unify content se-
122
lection and surface realisation, therefore we will
extend the action space in order to include actions
for template selection.
8 Acknowledgements
The research leading to this work has re-
ceived funding from the EC?s FP7 programme:
(FP7/2011-14) under grant agreement no. 248765
(Help4Mood).
References
Carole Ames. 1992. Classrooms: Goals, Structures,
and Student Motivation. Journal of Educational Psy-
chology, 84(3):p261-71.
Gabor Angeli, Percy Liang and Dan Klein. 2010. A
simple domain-independent probabilistic approach
to generation. EMNLP ?10: Proceedings of the 2010
Conference on Empirical Methods in Natural Lan-
guage Processing.
Regina Barzilay and Mirella Lapata. 2004. Collec-
tive content selection for concept-to-text generation.
HLT ?05: Proceedings of the conference on Hu-
man Language Technology and Empirical Methods
in Natural Language Processing.
Regina Barzilay and Lillian Lee. 2004. Catching the
drift: Probabilistic content models, with applications
to generation and summarization. HLT-NAACL
2004: Proceedings of the Human Language Tech-
nology Conference of the North American Chapter
of the Association for Computational Linguistics.
Anja Belz and Eric Kow. 2010. Extracting parallel
fragments from comparable corpora for data-to-text
generation. INLG ?10: Proceedings of the 6th Inter-
national Natural Language Generation Conference.
Rolf Black, Joe Reddington, Ehud Reiter, Nava
Tintarev, and Annalu Waller. 2010. Using NLG and
Sensors to Support Personal Narrative for Children
with Complex Communication Needs. SLPAT ?10:
Proceedings of the NAACL HLT 2010 Workshop on
Speech and Language Processing for Assistive Tech-
nologies.
Michelene T.H. Chi, Stephanie A. Siler, Heisawn
Jeong, Takashi Yamauchi, Robert G. Hausmann.
2001. Learning from human tutoring. Journal of
Cognitive Science, 25(4):471-533.
Scotty D. Craig, Arthur C. Graesser, Jeremiah Sullins,
Barry Gholson. 2004. Affect and learning: an ex-
ploratory look into the role of affect in learning with
AutoTutor. Journal of Educational Media, 29:241-
250.
Nina Dethlefs and Heriberto Cuayahuitl. 2011.
Combining hierarchical reinforcement learning and
bayesian networks for natural language generation
in situated dialogue. ENLG ?11: Proceedings of the
13th European Workshop on Natural Language Gen-
eration.
Barbara Fox. 1993. The Human Tutorial Dialogue
Project: Issues in the Design of Instructional Sys-
tems. Lawrence Erlbaum Associates, Hillsdale,
New Jersey.
Albert Gatt, Francois Portet, Ehud Reiter, James
Hunter, Saad Mahamood,Wendy Moncur, and So-
mayajulu Sripada. 2009. From Data to Text in the
Neonatal Intensive Care Unit: Using NLG Technol-
ogy for Decision Support and Information Manage-
ment. Journal of AI Communications, 22:153-186.
Higher Education Academy. 2009. Providing individ-
ual written feedback on formative and summative
assessments. http://www.heacademy.
ac.uk/assets/documents/resources/
database/id353_senlef_guide.pdf.
Last modified September 16.
Jim Hunter, Yvonne Freer, Albert Gatt, Yaji Sripada,
Cindy Sykes, and D Westwater. 2011. BT-Nurse:
Computer Generation of Natural Language Shift
Summaries from Complex Heterogeneous Medical
Data. Journal of the American Medical Informatics
Association,18:621-624.
Srinivasan Janarthenam, Helen Hastie, Oliver Lemon,
Xingkun Liu. 2011. ?The day after the day after to-
morrow?? A machine learning approach to adaptive
temporal expression generation: training and evalu-
ation with real users. SIGDIAL ?11: Proceedings of
the SIGDIAL 2011 Conference.
Srinivasan Janarthanam and Oliver Lemon. 2010.
Adaptive Referring Expression Generation in Spo-
ken Dialogue Systems: Evaluation with Real Users.
SIGDIAL ?10: Proceedings of the 11th Annual
Meeting of the Special Interest Group on Discourse
and Dialogue.
Nicholas A. R. Johnson and David M. Lane. 2011.
Narrative Monologue as a First Step Towards Ad-
vanced Mission Debrief for AUV Operator Situa-
tional Awareness. In the 15th International Confer-
ence on Advanced Robotics.
Ioannis Konstas and Mirella Lapata. 2012. Unsuper-
vised concept-to-text generation with hypergraphs.
NAACL HLT ?12: Proceedings of the 2012 Confer-
ence of the North American Chapter of the Asso-
ciation for Computational Linguistics: Human Lan-
guage Technologies.
Natalie K. Person, Roger J. Kreuz, Rolf A. Zwaan and
Arthur C. Graesser. 1995. Pragmatics and Peda-
gogy: Conversational Rules and Politeness Strate-
gies May Inhibit Effective Tutoring. Journal of Cog-
nition and Instruction, 13(2):161-188.
Kaska Porayska-Pomsta and Chris Mellish. 2013.
Modelling human tutors? feedback to inform natural
language interfaces for learning. International Jour-
nal of Human-Computer Studies,71(6):703724.
123
Ehud Reiter and Robert Dale. 2000. Building Natural
Language Generation systems. Cambridge Univer-
sity Press.
Verena Rieser, Oliver Lemon, and Xingkun Liu. 2010.
Optimising Information Presentation for Spoken Di-
alogue Systems. ACL ?10: Proceedings of the 48th
Annual Meeting of the Association for Computa-
tional Linguistics.
Somayajulu Sripada, Ehud Reiter, I Davy, and K
Nilssen. 2004. Lessons from Deploying NLG Tech-
nology for Marine Weather Forecast Text Gener-
ation. In Proceedings of PAIS session of ECAI-
2004:760-764.
Somayajulu Sripada, Ehud Reiter, Jim Hunter, and Jin
Yu. 2003. Generating English Summaries of Time
Series Data using the Gricean Maxims. KDD ?03:
Proceedings of the ninth ACM SIGKDD interna-
tional conference on Knowledge discovery and data
mining.
Richart Sutton and Andrew Barto. 1998. Reinforce-
ment Learning. MIT Press.
124
Proceedings of the SIGDIAL 2013 Conference, pages 154?156,
Metz, France, 22-24 August 2013. c?2013 Association for Computational Linguistics
Demonstration of the Parlance system: a data-driven,
incremental, spoken dialogue system for interactive search
Helen Hastie, Marie-Aude Aufaure?, Panos Alexopoulos, Heriberto Cuay?huitl, Nina Dethlefs,
Milica Gasic, James Henderson, Oliver Lemon, Xingkun Liu, Peter Mika, Nesrine Ben Mustapha,
Verena Rieser, Blaise Thomson, Pirros Tsiakoulis, Yves Vanrompay, Boris Villazon-Terrazas, Steve Young
email: h.hastie@hw.ac.uk. See http://parlance-project.eu for full list of affiliations
Abstract
The Parlance system for interactive
search processes dialogue at a micro-
turn level, displaying dialogue phe-
nomena that play a vital role in hu-
man spoken conversation. These di-
alogue phenomena include more nat-
ural turn-taking through rapid sys-
tem responses, generation of backchan-
nels, and user barge-ins. The Par-
lance demonstration system differen-
tiates from other incremental systems
in that it is data-driven with an infras-
tructure that scales well.
1 Introduction
The Parlance system provides interactive
search through a Spoken Dialogue System
(SDS). This SDS aims to be incremental to al-
low for more natural spoken interaction. Tra-
ditionally, the smallest unit of speech process-
ing for interactive systems has been a full ut-
terance with strict, rigid turn-taking. The
Parlance architecture, however, is an incre-
mental framework that allows for processing
of smaller ?chunks? of user input, which en-
ables one to model dialogue phenomena such
as barge-ins and backchannels. This work is
carried out under the FP7 EC project Par-
lance 1, the goal of which is to develop inter-
active search through speech in multiple lan-
guages. The domain for the demonstration
system is interactive search for restaurants in
San Francisco. An example dialogue is given
in Table 1.
?Authors are in alphabetical order
1http://www.parlance-project.eu
SYS Thank you for calling the Parlance Restaurant
system. You may ask for information by cuisine
type, price range or area. How may I help you?
USR I want to find an Afghan restaurant.........which is
in the cheap price range.
SYS .......................................................[uhuhh]........
The Helmand Palace is a cheerful setting for au-
thentic Afghan cuisine.
USR What is the address and phone number?
SYS The address 2424 Van Ness Ave ....
Table 1: Example dialogue excerpt for restaurant in-
formation in San Francisco
2 Background
Previous work includes systems that can deal
with ?micro-turns? (i.e. sub-utterance process-
ing units), resulting in dialogues that are more
fluid and responsive. This has been backed up
by a large body of psycholinguistic literature
that indicates that human-human interaction
is in fact incremental (Levelt, 1989).
It has been shown that incremental dia-
logue behaviour can improve the user experi-
ence (Skantze and Schlangen, 2009; Baumann
et al, 2011; Selfridge et al, 2011) and en-
able the system designer to model several di-
alogue phenomena that play a vital role in
human discourse (Levelt, 1989) but have so
far been absent from systems. These dialogue
phenomena that will be demonstrated by the
Parlance system include more natural turn-
taking through rapid system responses, gener-
ation of backchannels and user barge-ins. The
system differentiates from other incremental
systems in that it is entirely data-driven with
an infrastructure that potentially scales well.
3 System Architecture
Figure 1 gives an overview of the Par-
lance system architecture, which maintains
154
LOCAL SEARCH ENGINE
AUTOMATIC SPEECH RECOGNITION
NLG
AUDIO I/O
TTS
BACKCHANNEL GENERATOR
IM
MIM
HUB
KNOWLEDGE BASE
WavePackets
1-Best Words
Segmentlabel
N-Best Phrase List
WavePackets
Micro-Turn Dialogue Act
System Dialogue Act
String Packets
StringPackets
VoIP Interface (PJSIP)
N-best Dialogue Act Units
 API call ( + metadata)
Search Response
Partial Dialogue Act (in case of interruption)
PartialString(in case of interruption)SPOKEN LANGUAGE UNDERSTANDING Decode from t0 to t1
Figure 1: Overview of the Parlance system
architecture
the modularity of a traditional SDS while at
the same time allowing for complex interaction
at the micro-turn level between components.
Each component described below makes use
of the PINC (Parlance INCremental) dialogue
act schema. In this scheme, a complete dia-
logue act is made up of a set of primitive di-
alogue acts which are defined as acttype-item
pairs. The PINC dialogue act scheme supports
incrementality by allowing SLU to incremen-
tally output primitive dialogue acts whenever
a complete acttype-item pair is recognised with
sufficient confidence. The complete dialogue
act is then the set of these primitive acts out-
put during the utterance.
3.1 Recognition and Understanding
The Automatic Speech Recogniser (ASR) and
Spoken Language Understanding (SLU) com-
ponents operate in two passes. The audio in-
put is segmented by a Voice Activity Detec-
tor and then coded into feature vectors. For
the first pass of the ASR2, a fast bigram de-
coder performs continuous traceback generat-
ing word by word output. During this pass,
while the user is speaking, an SLU module
called the ?segment decoder? is called incre-
2http://mi.eng.cam.ac.uk/research/dialogue/
ATK_Manual.pdf
mentally as words or phrases are recognised.
This module incrementally outputs the set of
primitive dialogue acts that can be detected
based on each utterance prefix. Here, the ASR
only provides the single best hypothesis, and
SLU only outputs a single set of primitive dia-
logue acts, without an associated probability.
On request from the Micro-turn Interaction
Manager (MIM), a second pass can be per-
formed to restore the current utterance using a
trigram language model, and return a full dis-
tribution over the complete phrase as a con-
fusion network. This is then passed to the
SLU module which outputs the set of alter-
native complete interpretations, each with its
associated probability, thus reflecting the un-
certainty in the ASR-SLU understanding pro-
cess.
3.2 Interaction Management
Figure 1 illustrates the role of the Micro-turn
Interaction Manager (MIM) component in the
overall Parlance architecture. In order to
allow for natural interaction, the MIM is re-
sponsible for taking actions such as listening to
the user, taking the floor, and generating back-
channels at the micro-turn level. Given various
features from different components, the MIM
selects a micro-turn action and sends it to the
IM and back-channel generator component to
generate a system response.
Micro-turn Interaction Manager A
baseline hand-crafted MIM was developed
using predefined rules. It receives turn-taking
information from the TTS, the audio-output
component, the ASR and a timer, and updates
turn-taking features. Based on the current
features and predefined rules, it generates
control signals and sends them to the TTS,
ASR, timer and HUB. In terms of micro-turn
taking, for example, if the user interrupts
the system utterance, the system will stop
speaking and listen to the user. The system
also outputs a short back-channel and stays in
user turn state if the user utterance provides
limited information.
Interaction Manager Once the MIM has
decided when the system should take the floor,
it is the task of the IM to decide what to say.
The IM is based on the partially observable
155
Markov decision process (POMDP) frame-
work, where the system?s decisions can be op-
timised via reinforcement learning. The model
adopted for Parlance is the Bayesian Update
of Dialogue State (BUDS) manager (Thom-
son and Young, 2010). This POMDP-based
IM factors the dialogue state into condition-
ally dependent elements. Dependencies be-
tween these elements can be derived directly
from the dialogue ontology. These elements
are arranged into a dynamic Bayesian network
which allows for their marginal probabilities
to be updated during the dialogue, compris-
ing the belief state. The belief state is then
mapped into a smaller-scale summary space
and the decisions are optimised using the nat-
ural actor critic algorithm.
HUB The HUB manages the high level flow
of information. It receives turn change infor-
mation from the MIM and sends commands
to the SLU/IM/NLG to ?take the floor? in the
conversation and generate a response.
3.3 Generation and TTS
We aim to automatically generate language,
trained from data, that is (1) grammatically
well formed, (2) natural, (3) cohesive and (4)
rapidly produced at runtime. Whilst the first
two requirements are important in any dia-
logue system, the latter two are key require-
ments for systems with incremental processing,
in order to be more responsive. This includes
generating back-channels, dynamic content re-
ordering (Dethlefs et al, 2012), and surface
generation that models coherent discourse phe-
nomena, such as pronominalisation and co-
reference (Dethlefs et al, 2013). Incremen-
tal surfacce generation requires rich context
awareness in order to keep track of all that has
been generated so far. We therefore treat sur-
face realisation as a sequence labelling task and
use Conditional Random Fields (CRFs), which
take semantically annotated phrase structure
trees as input, in order to represent long dis-
tance linguistic dependencies. This approach
has been compared with a number of compet-
itive state-of-the art surface realisers (Deth-
lefs et al, 2013), and can be trained from
minimally labelled data to reduce development
time and facilitate its application to new do-
mains.
The TTS component uses a trainable HMM-
based speech synthesizer. As it is a paramet-
ric model, HMM-TTS has more flexibility than
traditional unit-selection approaches and is es-
pecially useful for producing expressive speech.
3.4 Local Search and Knowledge Base
The domain ontology is populated by the local
search component and contains restaurants in
5 regional areas of San Francisco. Restaurant
search results are returned based on their lon-
gitude and latitude for 3 price ranges and 52
cuisine types.
4 Future Work
We intend to perform a task-based evaluation
using crowd-sourced users. Future versions
will use a dynamic Knowledge Base and User
Model for adapting to evolving domains and
personalised interaction respectively.
Acknowledgements
The research leading to this work was funded by the EC
FP7 programme FP7/2011-14 under grant agreement
no. 287615 (PARLANCE).
References
T. Baumann, O. Buss, and D. Schlangen. 2011. Eval-
uation and Optimisation of Incremental Processors.
Dialogue and Discourse, 2(1).
Nina Dethlefs, Helen Hastie, Verena Rieser, and Oliver
Lemon. 2012. Optimising Incremental Generation
for Spoken Dialogue Systems: Reducing the Need
for Fillers. In Proceedings of INLG, Chicago, USA.
N. Dethlefs, H. Hastie, H. Cuay?huitl, and O. Lemon.
2013. Conditional Random Fields for Responsive
Surface Realisation Using Global Features. In Pro-
ceedings of ACL, Sofia, Bulgaria.
W. Levelt. 1989. Speaking: From Intenion to Articu-
lation. MIT Press.
E. Selfridge, I. Arizmendi, P. Heeman, and J. Williams.
2011. Stability and Accuracy in Incremental Speech
Recognition. In Proceedings of SIGDIAL, Portland,
Oregon.
G. Skantze and D. Schlangen. 2009. Incremental Dia-
logue Processing in a Micro-Domain. In Proceedings
of EACL, Athens, Greece.
B Thomson and S Young. 2010. Bayesian update of
dialogue state: A POMDP framework for spoken
dialogue systems. Computer Speech and Language,
24(4):562?588.
156
Proceedings of the SIGDIAL 2013 Conference, pages 314?318,
Metz, France, 22-24 August 2013. c?2013 Association for Computational Linguistics
Impact of ASR N-Best Information on Bayesian Dialogue Act Recognition
Heriberto Cuaya?huitl, Nina Dethlefs, Helen Hastie, Oliver Lemon
School of Mathematical and Computer Sciences,
Heriot-Watt University, Edinburgh, UK
{h.cuayahuitl,n.s.dethlefs,h.hastie,o.lemon}@hw.ac.uk
Abstract
A challenge in dialogue act recognition
is the mapping from noisy user inputs to
dialogue acts. In this paper we describe
an approach for re-ranking dialogue act
hypotheses based on Bayesian classifiers
that incorporate dialogue history and Au-
tomatic Speech Recognition (ASR) N-best
information. We report results based on
the Let?s Go dialogue corpora that show
(1) that including ASR N-best information
results in improved dialogue act recogni-
tion performance (+7% accuracy), and (2)
that competitive results can be obtained
from as early as the first system dialogue
act, reducing the need to wait for subse-
quent system dialogue acts.
1 Introduction
The primary challenge of a Dialogue Act Recog-
niser (DAR) is to find the correct mapping be-
tween a noisy user input and its true dialogue
act. In standard ?slot-filling? dialogue sys-
tems a dialogue act is generally represented as
DialogueActType(attribute-value pairs), see Sec-
tion 3. While a substantial body of research has
investigated different types of models and meth-
ods for dialogue act recognition in spoken dia-
logue systems (see Section 2), here we focus on
re-ranking the outputs of an existing DAR for eval-
uation purposes. In practice the re-ranker should
be part of the DAR itself. We propose to use mul-
tiple Bayesian classifiers to re-rank an initial set
of dialogue act hypotheses based on information
from the dialogue history as well as ASR N-best
lists. In particular the latter type of information
helps us to learn mappings between dialogue acts
and common mis-recognitions. We present exper-
imental results based on the Let?s Go dialogue cor-
pora which indicate that re-ranking hypotheses us-
ing ASR N-best information can lead to improved
recognition. In addition, we compare the recogni-
tion accuracy over time and find that high accuracy
can be obtained with as little context as one system
dialogue act, so that there is often no need to take
a larger context into account.
2 Related Work
Approaches to dialogue act recognition from spo-
ken input have explored a wide range of meth-
ods. (Stolcke et al, 2000) use HMMs for dialogue
modelling, where sequences of observations cor-
respond to sequences of dialogue act types. They
also explore the performance with decision trees
and neural networks and report their highest ac-
curacy at 65% on the Switchboard corpus. (Zim-
mermann et al, 2005) also use HMMs in a joint
segmentation and classification model. (Grau et
al., 2004) use a combination of Naive Bayes and
n-grams with different smoothing methods. Their
best models achieve an accuracy of 66% on En-
glish Switchboard data and 89% on a Spanish cor-
pus. (Sridhar et al, 2009; Wright et al, 1999)
both use a maximum entropy classifier with n-
grams to classify dialogue acts using prosodic fea-
tures. (Sridhar et al, 2009) report an accuracy of
up to 74% on Switchboard data and (Wright et al,
1999) report an accuracy of 69% on the DCIEM
Maptask Corpus. (Bohus and Rudnicky, 2006)
maintain an N-best list of slot values using logis-
tic regression. (Surendran and Levow, 2006) use
a combination of linear support vector machines
(SVMs) and HMMs. They report an accuracy of
65.5% on the HCRC MapTask corpus and con-
clude that SVMs are well suited for sparse text and
dense acoustic features. (Gamba?ck et al, 2011)
use SVMs within an active learning framework.
They show that while passive learning achieves an
accuracy of 77.8% on Switchboard data, the ac-
tive learner achieves up to 80.7%. (Henderson et
al., 2012) use SVMs for dialogue act recognition
from ASR word confusion networks.
314
Speech 
Recognizer
Dialogue Act 
Recognizer Dialogue Act Re-Ranker
n-best 
list
n-best 
list
n-best 
list
(e.g. Let's Go parser)
speech
scored re-scored
  Following 
Components
Figure 1: Pipeline architecture for dialogue act recognition and re-ranking component. Here, the input
is a list of dialogue acts with confidence scores, and the output is the same list of dialogue acts but with
recomputed confidence scores. A dialogue act is represented as DialogueActType(attribute-value pairs).
Several authors have presented evidence in
favour of Bayesian methods. (Keizer and op den
Akker, 2007) have shown that Bayesian DARs
can outperform baseline classifiers such as deci-
sion trees. More generally, (Ng and Jordan, 2001)
show that generative classifiers (e.g. Naive Bayes)
reach their asymptotic error faster than discrimina-
tive ones. As a consequence, generative classifiers
are less data intensive than discriminative ones.
In addition, several authors have investigated
dialogue belief tracking. While our approach
is related to belief tracking, we focus here on
spoken language understanding under uncertainty
rather than estimating user goals. (Williams, 2007;
Thomson et al, 2008) use approximate inference
to improve the scalability of Bayes nets for be-
lief tracking and (Lison, 2012) presents work on
improving their scalability through abstraction.
(Mehta et al, 2010) model user intentions through
the use of probabilistic ontology trees.
Bayes nets have also been applied to other
dialogue-related tasks, such as surface realisa-
tion within dialogue (Dethlefs and Cuaya?huitl,
2011) or multi-modal dialogue act recognition
(Cuaya?huitl and Kruijff-Korbayova?, 2011). In the
following, we will explore a dialogue act recogni-
tion technique based on multiple Bayesian classi-
fiers and show that re-ranking with ASR N-best in-
formation can improve recognition performance.
3 Re-Ranking Dialogue Acts Using
Multiple Bayesian Networks
Figure 1 shows an illustration of our dialogue act
re-ranker within a pipeline architecture. Here, pro-
cessing begins with the user?s speech being inter-
preted by a speech recogniser, which produces a
first N-best list of hypotheses. These hypotheses
are subsequently passed on and interpreted by a
dialogue act recogniser, which in our case is rep-
resented by the Let?s Go parser. The parser pro-
duces a first set of dialogue act hypotheses, based
on which our re-ranker becomes active. A full
dialogue act in our scenario consists of three el-
ements: dialogue act types, attributes (or slots),
and slot values. An example dialogue act is in-
form(from=Pittsburgh Downtown). The dialogue
act re-ranker thus receives a list of hypotheses
in the specified form (triples) from its preceding
module (a DAR or in our case the Let?s Go parser)
and its task is to generate confidence scores that
approximate true label (i.e. the dialogue act really
spoken by a user) as closely as possible.
We address this task by using multiple Bayesian
classifiers: one for classifying a dialogue act type,
one for classifying a set of slots, and the rest for
classifying slot values. The use of multiple classi-
fiers is beneficial for scalability purposes; for ex-
ample, assuming 10 dialogue act types, 10 slots,
10 values per slot, and no other dialogue con-
text results in a joint distribution of 1011 parame-
ters. Since a typical dialogue system is required to
model even larger joint distributions, our adopted
approach is to factorize them into multiple inde-
pendent Bayesian networks (with combined out-
puts). A multiple classifier system is a power-
ful solution to complex classification problems in-
volving a large set of inputs and outputs. This
approach not only decreases training time but has
also been shown to increase the performance of
classification (Tax et al, 2000).
A Bayesian Network (BN) models a joint prob-
ability distribution over a set of random variables
and their dependencies, see (Bishop, 2006) for
an introduction to BNs. Our motivation for us-
ing multiple BNs is to incorporate a fairly rich di-
alogue context in terms of what the system and
user said at lexical and semantic levels. In con-
trast, using a single BN for all slots with rich di-
alogue context faces scalability issues, especially
for slots with large numbers of domain values,
and is therefore not an attractive option. We
denote our set of Bayesian classifiers as ? =
{?dat, ?att, ..., ?val(i)}, where BN ?dat is used to
rank dialogue act types, BN ?att is used to rank
attributes, and the other BNs (?val(i)) are used to
315
rank values for each slot i. The score of a user
dialogue act (< d, a, v >) is computed as:
P (d, a, v) = 1Z
?
P (d|pad)P (a|paa)P (v|pav),
where d is a dialogue act type, a is an attribute
(or slot), v is a slot value, pax is a parent random
variable, andZ is a normalising constant. This im-
plies that the score of a dialogue act is the product
of probabilities of dialogue act type and slot-value
pairs. For dialogue acts including multiple slot-
value pairs, the product above can be extended ac-
cordingly. The best and highest ranked hypothesis
(from space H) can be obtained according to:
< d, a, v >?= arg max
<d,a,v>?H
P (d, a, v).
In the following, we describe our experimental
setting. Here, the structure and parameters of our
classifiers will be estimated from a corpus of spo-
ken dialogues, and we will use the equations above
for re-ranking user dialogue acts. Finally, we re-
port results comparing Bayesian classifiers that
make use of ASR N-best information and dialogue
context against Bayesian classifiers that make pre-
dictions based on the dialogue context alone.
4 Experiments and Results
4.1 Data
Our experiments are based on the Let?s Go corpus
(Raux et al, 2005). Let?s Go contains recorded in-
teractions between a spoken dialogue system and
human users who make enquiries about the bus
schedule in Pittsburgh. Dialogues are driven by
system-initiative and query the user sequentially
for five slots: an optional bus route, a departure
place, a destination, a desired travel date, and a
desired travel time. Each slot needs to be explic-
itly (or implicity) confirmed by the user. Our anal-
yses are based on a subset of this data set contain-
ing 779 dialogues with 7275 turns, collected in the
Summer of 2010. From these dialogues, we used
70% for training our classifiers and the rest for
testing (with 100 random splits). Briefly, this data
set contains 12 system dialogue act types1, 11 user
dialogue act types2, and 5 main slots with varia-
tions3. The number of slot values ranges between
1ack, cant help, example, expl conf, go back, hello,
impl conf, more buses, request, restart, schedule, sorry.
2affirm, bye, go back, inform, negate, next bus, prevbus,
repeat, restart, silence, tellchoices.
3date.absday, date.abmonth, date.day, date.relweek, from,
route, time.ampm, time.arriveleave, time.hour, time.minute,
time.rel, to.
*
Figure 2: Bayesian network for probabilistic rea-
soning of locations (variable ?from desc?), which
incorporates ASR N-best information in the vari-
able?from desc nbest? and dialogue history in-
formation in the remaining random variables.
102 and 103 so that the combination of all possi-
ble dialogue act types, attributes and values leads
to large amounts of triplets. While the majority
of user inputs contain one user dialogue act, the
average number of system dialogue acts per turn
is 4.2. Note that for the user dialogue act types,
we also model silence explicitly. This is often not
considered in dialogue act recognisers: since the
ASR will always try to recognise something out
of any input (even background noise), typical dia-
logue act recognisers will then try to map the ASR
output onto a semantic interpretation.
4.2 Bayesian Networks
We trained our Bayesian networks in a supervised
learning manner and used 43 discrete features (or
random variables) plus a class label (also discrete).
The feature set is described by three main subsets:
25 system-utterance-level binary features4 derived
from the system dialogue act(s) in the last turn; 17
user-utterance-level binary features5 derived from
(a) what the user heard prior to the current turn,
or (b) what keywords the system recognised in its
4System utterance features: heardAck, heardCantHelp,
heardExample, heardExplConf, heardGoBackDAT, heard-
Hello, heardImplConf, heardMoreBuses, heardRequest,
heardRestartDAT, heardSchedule, heardSorry, heardDate,
heardFrom, heardRoute, heardTime, heardTo, heardNext,
heardPrevious, heardGoBack, heardChoices, heardRestart,
heardRepeat, heardDontKnow, lastSystemDialActType.
5User utterance features: hasRoute, hasFrom, hasTo, has-
Date, hasTime, hasYes, hasNo, hasNext, hasPrevious, has-
GoBack, hasChoices, hasRestart, hasRepeat, hasDontKnow,
hasBye, hasNothing, duration in secs. (values=0,1,2,3,4,>5).
316
list of speech recognition hypotheses; and 1 word-
level non-binary feature (* nbest) corresponding
to the slot values in the ASR N-best lists.
Figure 2 shows the Bayes net corresponding to
the classifier used to rank location names. The
random variable from desc is the class label, the
random variable from desc nbest (marked with an
asterisk) incorporates slot values from the ASR
N-best lists, and the remaining variables model
dialogue history context. The structure of our
Bayesian classifiers were derived from the K2 al-
gorithm6, and their parameters were derived from
maximum likelihood estimation. In addition, we
performed probabilistic inference using the Junc-
tion tree algorithm7. Based on these data and
tools, we trained 14 Bayesian classifiers: one for
scoring dialogue act types, one for scoring at-
tributes (slots), and the rest for scoring slot values.
4.3 Experimental Results
We compared 7 different dialogue act recognisers
in terms of classification accuracy. The compar-
ison was made against gold standard data from
a human-labelled corpus. (Semi-Random) is a
recogniser choosing a random dialogue act from
the Let?s Go N-best parsing hypotheses. (Inci) is
our proposed approach considering a context of i
system dialogue acts, and (Ceiling) is a recogniser
choosing the correct dialogue act from the Let?s
Go N-best parsing hypotheses. The latter was used
as a gold standard from manual annotations, which
reflects the proportion of correct labels in the N-
best parsing hypotheses.
We also assessed the impact of ASR N-best in-
formation on probabilistic inference. To this end,
we compared Bayes nets with a focus on the ran-
dom variable ?* nbest?, which in one case con-
tains induced distributions from data and in the
other case contains an equal distribution of slot
values. Our hypothesis is that the former setting
will lead to better performance.
Figure 3 shows the classification accuracy of
our dialogue act recognisers. The first point to no-
tice is that the incorporation of ASR N-best infor-
mation makes an important difference. The per-
formance of recogniser IncK (K being the num-
ber of system dialogue acts) is 66.9% without
ASR N-best information and 73.9% with ASR N-
best information (the difference is significant8 at
6www.cs.waikato.ac.nz/ml/weka/
7www.cs.cmu.edu/?javabayes/Home/8Based on a two-sided Wilcoxon Signed-Rank test.
Semi?Random Inc0 Inc1 Inc2 Inc3 IncK Ceiling40
45
50
55
60
65
70
75
80
85
90
Dialogue Act Recogniser
Clas
sifica
tion 
Accu
racy
 (%)
 
 
Without ASR N?Best InformationWith ASR N?Best Information
Figure 3: Bayesian dialogue act recognisers show-
ing the impact of ASR N-best information.
p < 0.05). The latter represents a substantial im-
provement over the semi-random baseline (62.9%)
and Lets Go dialogue act recognizer (69%), both
significant at p < 0.05. A second point to notice is
that the differences between Inci (? i>0) recognis-
ers were not significant. We can say that the use of
one system dialogue act as context is as competi-
tive as using a larger set of system dialogue acts.
This suggests that dialogue act recognition carried
out at early stages (e.g. after the first dialogue act)
in an utterance does not degrade recognition per-
formance. The effect is possibly domain-specific
and generalisations remain to be investigated.
Generally, we were able to observe that more
than half of the errors made by the Bayesian clas-
sifiers were due to noise in the environment and
caused by the users themselves, which interfered
with ASR results. Detecting when users do not
convey dialogue acts to the system is therefore still
a standing challenge for dialogue act recognition.
5 Conclusion and Future Work
We have described a re-ranking approach for user
dialogue act recognition. Multiple Bayesian clas-
sifiers are used to rank dialogue acts from a set of
dialogue history features and ASR N-best infor-
mation. Applying our approach to the Let?s Go
data we found the following: (1) that including
ASR N-best information results in improved di-
alogue act recognition performance; and (2) that
competitive results can be obtained from as early
as the first system dialogue act, reducing the need
to include subsequent ones.
Future work includes: (a) a comparison of our
317
Bayesian classifiers with other probabilistic mod-
els and forms of training (for example by us-
ing semi-supervised learning), (b) training dia-
logue act recognisers in different (multi-modal and
multi-task) domains, and (c) dealing with random
variables that contain very large domain values.
6 Acknowledgements
This research was funded by the EC FP7 pro-
gramme under grant agreement no. 287615 (PAR-
LANCE) and no. 270019 (SPACEBOOK).
Sample Re-Ranked User Inputs
User input: ?forty six d?
N-Best List of Dialogue Acts Let?s Go Score Bayesian Score
inform(route=46a) 3.33E-4 1.9236763E-6
inform(route=46b) 1.0E-6 1.5243509E-16
inform(route=46d) 0.096107 7.030841E-4
inform(route=46k) 0.843685 4.9941495E-10
silence() NA 0
User input: ?um jefferson hills to mckeesport?
N-Best List of Dialogue Acts Let?s Go Score Bayesian Score
inform(from=mill street) 7.8E-4 3.5998527E-16
inform(from=mission street) 0.015577 3.5998527E-16
inform(from=osceola street) 0.0037 3.5998527E-16
inform(from=robinson township) 0.007292 3.5998527E-16
inform(from=sheraden station) 0.001815 3.1346254E-8
inform(from=brushton) 2.45E-4 3.5998527E-16
inform(from=jefferson) 0.128727 0.0054255757
inform(from=mckeesport) 0.31030 2.6209198E-4
silence() NA 0
References
[Bishop2006] Christopher M. Bishop. 2006. Pattern Recog-
nition and Machine Learning (Information Science and
Statistics). Springer-Verlag New York, Inc., Secaucus, NJ,
USA.
[Bohus and Rudnicky2006] D. Bohus and A. Rudnicky.
2006. A k hypotheses + other? belief updating model. In
AAAI Workshop on Statistical and Empirical Approaches
to Spoken Dialogue Systems.
[Cuaya?huitl and Kruijff-Korbayova?2011] H. Cuaya?huitl and
I. Kruijff-Korbayova?. 2011. Learning human-robot di-
alogue policies combining speech and visual beliefs. In
IWSDS, pages 133?140.
[Dethlefs and Cuaya?huitl2011] Nina Dethlefs and Heriberto
Cuaya?huitl. 2011. Combining Hierarchical Reinforce-
ment Learning and Bayesian Networks for Natural Lan-
guage Generation in Situated Dialogue. In ENLG, Nancy,
France.
[Gamba?ck et al2011] Bjo?rn Gamba?ck, Fredrik Olsson, and
Oscar Ta?ckstro?m. 2011. Active Learning for Dialogue
Act Classification. In INTERSPEECH, pages 1329?1332.
[Grau et al2004] Sergio Grau, Emilio Sanchis, Maria Jose
Castro, and David Vilar. 2004. Dialogue Act Classifi-
cation Using a Bayesian Approach. In SPECOM.
[Henderson et al2012] Matthew Henderson, Milica Gasic,
Blaise Thomson, Pirros Tsiakoulis, Kai Yu, and Steve
Young. 2012. Discriminative spoken language under-
standing using word confusion networks. In SLT, pages
176?181.
[Keizer and op den Akker2007] Simon Keizer and Rieks
op den Akker. 2007. Dialogue Act Recognition Under
Uncertainty Using Bayesian Networks. Natural Language
Engineering, 13(4):287?316.
[Lison2012] Pierre Lison. 2012. Probabilistic dialogue mod-
els with prior domain knowledge. In SIGDIAL Confer-
ence, pages 179?188.
[Mehta et al2010] Neville Mehta, Rakesh Gupta, Antoine
Raux, Deepak Ramachandran, and Stefan Krawczyk.
2010. Probabilistic ontology trees for belief tracking in
dialog systems. In SIGDIAL Conference, pages 37?46.
[Ng and Jordan2001] Andrew Y. Ng and Michael I. Jordan.
2001. On discriminative vs. generative classifiers: A com-
parison of logistic regression and naive bayes. In NIPS,
pages 841?848.
[Raux et al2005] Antoine Raux, Brian Langner, Dan Bohus,
Alan W. Black, and Maxine Eskenazi. 2005. Let?s
go public! Taking a Spoken Dialog System to the Real
World. In INTERSPEECH, pages 885?888.
[Sridhar et al2009] Vivek Kumar Rangarajan Sridhar, Srini-
vas Bangalore, and Shrikanth Narayanan. 2009. Com-
bining Lexical, Syntactic and Prosodic Cues for Improved
Online Dialog Act Tagging. Computer Speech & Lan-
guage, 23(4):407?422.
[Stolcke et al2000] Andreas Stolcke, Klaus Ries, Noah Coc-
caro, Elizabeth Shriberg, Rebecca A. Bates, Daniel Juraf-
sky, Paul Taylor, Rachel Martin, Carol Van Ess-Dykema,
and Marie Meteer. 2000. Dialog Act Modeling for Auto-
matic Tagging and Recognition of Conversational Speech.
Computational Linguistics, 26(3):339?373.
[Surendran and Levow2006] Dinoj Surendran and Gina-Anne
Levow. 2006. Dialog Act Tagging with Support Vec-
tor Machines and Hidden Markov Models. In INTER-
SPEECH.
[Tax et al2000] David M. Tax, Martijn van Breukelen,
Robert P. Duin, and Josef Kittler. 2000. Combining mul-
tiple classifiers by averaging or by multiplying? Pattern
Recognition, 33(9):1475?1485, September.
[Thomson et al2008] Blaise Thomson, Jost Schatzmann, and
Steve Young. 2008. Bayesian update of dialogue state for
robust dialogue systems. In ICASSP, pages 4937?4940.
[Williams2007] Jason D. Williams. 2007. Using particle fil-
ters to track dialogue state. In ASRU, pages 502?507.
[Wright et al1999] H. Wright, Massimo Poesio, and Stephen
Isard. 1999. Using high level dialogue information for
dialogue act recognition using prosodic features. In Pro-
ceedings of an ESCA Tutorial and Research Workshop on
Dialogue and Prosody, pages 139?143, Eindhoven, The
Netherlands.
[Zimmermann et al2005] Matthias Zimmermann, Yang Liu,
Elizabeth Shriberg, and Andreas Stolcke. 2005. Toward
Joint Segmentation and Classification of Dialog Acts in
Multiparty Meetings. In MLMI, pages 187?193.
318
Proceedings of the SIGDIAL 2013 Conference, pages 363?365,
Metz, France, 22-24 August 2013. c?2013 Association for Computational Linguistics
Demonstration of the Emote Wizard of Oz Interface for Empathic
Robotic Tutors
Shweta Bhargava1, Srinivasan Janarthanam1, Helen Hastie1, Amol Deshmukh1,
Ruth Aylett1, Lee Corrigan2, Ginevra Castellano 2
1School of Mathematical and Computer Sciences, Heriot-Watt University, Edinburgh
2School of Electronic, Electrical and Computer Engineering, University of Birmingham
sb426,sc445,h.hastie,a.deshmukh,r.s.aylett@hw.ac.uk,
ljc228,g.castellano@bham.ac.uk
Abstract
We present a Wizard of Oz (WoZ) envi-
ronment that was designed to build an arti-
ficial embodied intelligent tutoring system
(ITS) that is capable of empathic conver-
sations with school pupils aged between
10-13. We describe the components and
the data that we plan to collect using the
environment.
1 Introduction
We present a Wizard of Oz (WoZ) environment
that was built as a part of the EC FP7 EMOTE
project1. The objective of this work is to col-
lect multimodal interaction data to build an arti-
ficial embodied intelligent tutoring system (ITS)
that is capable of empathic conversations with
school pupils aged between 10-13. Specifically,
the EMOTE (EMbOdied-perceptive Tutors for
Empathy-based learning) project aims to design
and evaluate a new generation of robotic tutors
that have perceptive and expressive capabilities
to engage in empathic interactions with learners
in schools and home environments. The project
will carry out interdisciplinary research on affect
recognition, learner models, adaptive behaviour
and embodiment for human-robot interaction in
learning environments, grounded in psychologi-
cal theories of emotion in social interaction and
pedagogical models for learning facilitation. An
overview of the project can be found in (Desh-
mukh et al, 2013).
Wizard of Oz is an effective technique in Hu-
man Computer Interaction (HCI) studies where
an interactive agent, which is not yet fully au-
tonomous, is remotely controlled by a human wiz-
1http://emote-project.eu/
ard. However the participants who are interacting
with the agent are not told that the agent is being
remotely controlled. The wizard may be tasked
to control one or many parts of the agent such
as speech recognition and understanding, affect
recognition, dialogue management, utterance and
gesture generation and so on. Studies have shown
that users ?go easy? on computers during inter-
action and therefore interaction with ?wizarded?
system are at the level of complexity that can be
learned and emulated (Pearson et al, 2006).
The WoZ environment presented in this paper
will be used to collect data to inform the algo-
rithms for affect recognition and empathic dia-
logue management. The WoZ environment is de-
signed to collect data on how human tutors aided
with a robotic interface adapt to learners? emotions
and cognitive states in tutorial tasks. In this study,
the wizard plays the same role as that of affect
recognition and dialogue management modules in
the actual final system.
2 Previous work
Wizard-of-Oz (WoZ) frameworks have been used
in several studies since (Fraser and Gilbert, 1991)
in order to collect human-computer dialogue data
to help design dialogue systems. WoZ systems
have been used to collect data to learn (e.g.
(Strauss et al, 2007)) and evaluate dialogue man-
agement policies (e.g. (Cuaya?huitl and Kruijff-
Korbayova, 2012)).
3 The EMOTE Wizard of Oz
environment
The WoZ environment consists of the wizard?s
desk, the interactive touch table, sensors, and the
robotic embodiment as shown in Figure 1. The
363
wizard will be seated in a different room away
from the learner.
Figure 1: Wizard of Oz environment
3.1 Wizard?s desk
The wizard?s desk consists of two display screens.
The touch table display at the user end will be mir-
rored on to one of the displays at the wizard?s desk
using which the wizard can observe the learner?s
activities related to the educational application.
Another display will contain the Wizard Interface,
a software application that allows the wizard to in-
teract with the learner (see Figure 2). The Wiz-
ard Interface consists of four panels: task control,
information, learner response and operations. In
the task control panel, the wizard will be
able to choose a task plan for the learner and ac-
cess the tool and curriculum scripts (XML file).
The tool script contains information on how to use
the tools that are at the disposal of the learner. For
instance, to create a marker on the map, one has
to click on the appropriate tool and click on the
map and so on. The curriculum script contains
information on the skills that the learner needs
to exercise or develop during his interaction with
the system. For instance, in order to identify the
right direction, the system will present the mneu-
monic phrase ?Naughty Elephants Squirt Water?
in various forms such as a hint, question, pump-
ing move, etc. to provide support to the learner.
The information panel contains the video
feed from two cameras (see Section 3.4). This
will allow the wizard to determine the affective
state of the learner. The learner?s response to the
agent?s utterances (such as answering questions in
the curriculum scripts) will also be displayed in
the learner response panel. Finally, the
operations panel provides options for the
Wizard to respond to the learner based on the tools
and curriculum scripts. These responses are ei-
ther customised or predefined. The customised
responses facilitate the wizard to execute robot
movements on lower level (individual head, arm
movements) and predefined responses contain a
list for combined predefined speech, sound and be-
haviours.
Figure 2: Wizard?s Interface
3.2 Touch table
The interactive touch table is a 55 inch Multitac-
tion table capable of sensing multiple touch events
simultaneously. The educational application is
displayed on the table surface. A map based appli-
cation has been developed to teach learners basic
and advanced map reading skills (see Figure 3).
The touch interface allows the learner to use touch
to click, drag and zoom the map. The application
has two panels of GUI objects such as buttons and
text boxes namely, the tools panel and the interac-
tion panel. The tools panel consists of tools that
the learner can use to manipulate the map, while
using the interaction panel the learner can interact
with the tutor. Some of the tools that are currently
available are to get grid references for a position
on the map, dropping markers on the map, change
map types, etc. For instance, if the tutor asks a
yes/no question, the learner can respond by press-
ing the yes or the no button. The learner can an-
swer the tutor?s questions by typing into the text
box in the interaction panel.
364
Figure 3: Map reading skills application
3.3 Robotic embodiment
The robotic embodiment is a Nao robot (torso ver-
sion) that sits on the side of the touch table. It is
capable of head, arm and body gestures in addi-
tion to synthesised speech. The robot receives the
text and gestures selected by the wizard through
the Wizard Interface. Tutor?s utterances will be
synthesized into speech using the in-built text to
speech (TTS) engine while the gestures are re-
alised using appropriate head, arm and body mo-
tions. To increase naturalness, the robot will also
have idle movement in-between wizard selections.
3.4 Sensors
The environment has an array of sensors such as
two video cameras and a Kinect sensor. A Kinect
sensor and a video camera are placed in front the
learner. Another camera is placed in front of the
robot (as shown in Figure 1).
4 Data collection
In this section, we discuss the data that we aim
to collect using the WoZ environment. We intend
to collect these data during experiments where hu-
man tutors play the wizard?s role and the learners
from in the 10-13 year age-range will play the role
of learners. The task for the learner is to carry
out an expedition using the map application that
he or she is provided with. In order to solve the
steps of the expedition, the learner will have to
exercise his/her map reading skills. Map reading
skills such as compass directions, contour lines,
grid lines, etc. will have to be exercised using
appropriate map tools provided in the application.
The tutor?s role is to observe the learner responses
(both verbal and physical) and respond to them ap-
propriately using the interaction panel in the Wiz-
ard Interface application.
Simultaneous video feeds from two cameras
and the Kinect sensor will be recorded during the
tutor-learner interaction. These data will be fur-
ther used for affect recognition tasks based on
learner?s head, arm and body gestures. The inter-
action between the tutor and the learner in terms
of tutor dialogue actions, utterances and learner
responses in terms of button presses will also be
logged.
5 Demo
We propose to demonstrate the WoZ environment
set up using two laptops: learner desktop with the
map application and another with the wizard?s in-
terface. The learner desktop will also display a
simulated Nao robot. We will also exhibit the logs
that we collect from the pilot studies with a Geogr-
phy teacher acting as the wizard tutor and school
pupils as tutees.
Acknowledgements
This work was partially supported by the Euro-
pean Commission (EC) and was funded by the
EU FP7 ICT-317923 project EMOTE. The authors
are solely responsible for the content of this pub-
lication. It does not represent the opinion of the
EC, and the EC is not responsible for any use that
might be made of data appearing therein.
References
H. Cuaya?huitl and I Kruijff-Korbayova. 2012. An In-
teractive Humanoid Robot Exhibiting Flexible Sub-
Dialogues. In Proceedings of the NAACL-HTL,
Montreal, Canada.
A. Deshmukh, G. Castellano, A. Kappas, W. Baren-
dregt, F. Nabais, A. Paiva, T. Ribeiro, I. Leite, and
R. Aylett. 2013. Towards empathic artificial tutors.
In Proceedings of the 8th ACM/IEEE international
conference on Human-robot interaction.
N. Fraser and G. N. Gilbert. 1991. Simulating speech
systems. Computer Speech and Language, 5:81?99.
J. Pearson, J. Hu, H. P. Branigan, M. J. Pickering, and
C. Nass. 2006. Adaptive language behavior in HCI:
how expectations and beliefs about a system affect
users? word choice. In Proceedings of the SIGCHI
conference on Human Factors in computing systems,
Montral.
P. M. Strauss, H. Hoffmann, and S. Scherer. 2007.
Evaluation and user acceptance of a dialogue sys-
tem using Wizard-of-Oz recordings. In Proceedings
of 3rd IET International Conference on Intelligent
Environments.
365
Proceedings of the SIGDIAL 2014 Conference, pages 260?262,
Philadelphia, U.S.A., 18-20 June 2014. c?2014 Association for Computational Linguistics
The Parlance Mobile Application for Interactive Search in
English and Mandarin
Helen Hastie, Marie-Aude Aufaure?, Panos Alexopoulos,
Hugues Bouchard, Catherine Breslin, Heriberto Cuay?huitl, Nina Dethlefs,
Milica Ga?i?, James Henderson, Oliver Lemon, Xingkun Liu, Peter Mika, Nesrine Ben Mustapha,
Tim Potter, Verena Rieser, Blaise Thomson, Pirros Tsiakoulis, Yves Vanrompay,
Boris Villazon-Terrazas, Majid Yazdani, Steve Young and Yanchao Yu
email: h.hastie@hw.ac.uk. See http://parlance-project.eu for full list of affiliations
Abstract
We demonstrate a mobile application in
English and Mandarin to test and eval-
uate components of the Parlance di-
alogue system for interactive search un-
der real-world conditions.
1 Introduction
With the advent of evaluations ?in the wild?,
emphasis is being put on converting re-
search prototypes into mobile applications that
can be used for evaluation and data col-
lection by real users downloading the ap-
plication from the market place. This is
the motivation behind the work demonstrated
here where we present a modular framework
whereby research components from the Par-
lance project (Hastie et al., 2013) can be
plugged in, tested and evaluated in a mobile
environment.
The goal of Parlance is to perform inter-
active search through speech in multiple lan-
guages. The domain for the demonstration
system is interactive search for restaurants in
Cambridge, UK for Mandarin and San Fran-
cisco, USA for English. The scenario is that
Mandarin speaking tourists would be able to
download the application and use it to learn
about restaurants in English speaking towns
and cities.
2 System Architecture
Here, we adopt a client-server approach as il-
lustrated in Figure 1 for Mandarin and Figure
2 for English. The front end of the demon-
stration system is an Android application that
calls the Google Automatic Speech Recogni-
tion (ASR) API and sends the recognized user
utterance to a server running the Interaction
?Authors are in alphabetical order
Manager (IM), Spoken Language Understand-
ing (SLU) and Natural Language Generation
(NLG) components.
Figure 1: Overview of the Parlance Man-
darin mobile application system architecture
Figure 2: Overview of the Parlance En-
glish mobile application system architecture
extended to use the Yahoo API to populate
the application with additional restaurant in-
formation
When the user clicks the Start button, a di-
alogue session starts. The phone application
first connects to the Parlance server (via
the Java Socket Server) to get the initial sys-
tem greeting which it speaks via the Google
260
Text-To-Speech (TTS) API. After the system
utterance finishes the recognizer starts to lis-
ten for user input to send to the SLU compo-
nent. The SLU converts text into a semantic
interpretation consisting of a set of triples of
communicative function, attribute, and (op-
tionally) value1. Probabilities can be associ-
ated with candidate interpretations to reflect
uncertainty in either the ASR or SLU. The
SLU then passes the semantic interpretation
to the IM within the same server.
Chinese sentences are composed of strings of
characters without any space to mark words as
other languages do, for example:
In order to correctly parse and understand
Chinese sentences, Chinese word segmenta-
tions must be performed. To do this segmen-
tation, we use the Stanford Chinese word seg-
mentor2, which relies on a linear-chain condi-
tional random field (CRF) model and treats
word segmentation as a binary decision task.
The Java Socket Server then sends the seg-
mented Chinese sentence to the SLU on the
server.
The IM then selects a dialogue act, accesses
the database and in the case of English passes
back the list of restaurant identification num-
bers (ids) associated with the relevant restau-
rants. For the English demonstration system,
these restaurants are displayed on the smart
phone as seen in Figures 4 and 5. Finally,
the NLG component decides how best to re-
alise the restaurant descriptions and sends the
string back to the phone application for the
TTS to realise. The example output is illus-
trated in Figure 3 for Mandarin and Figure 4
for English.
As discussed above, the Parlance mobile
application can be used as a test-bed for com-
paring alternative techniques for various com-
ponents. Here we discuss two such compo-
nents: IM and NLG.
1This has been implemented for English; Mandarin
uses the rule-based Phoenix parser.
2http://nlp.stanford.edu/projects/chinese-
nlp.shtml
Figure 3: Screenshot and translation of the
Mandarin system
Figure 4: Screenshot of dialogue and the list
of recommended restaurants shown on a map
and in a list for English
2.1 Interaction Management
The Parlance Interaction Manager is based
on the partially observable Markov decision
process (POMDP) framework, where the sys-
tem?s decisions can be optimised via reinforce-
ment learning. The model adopted for Par-
lance is the Bayesian Update of Dialogue
State (BUDS) manager (Thomson and Young,
2010). This POMDP-based IM factors the di-
alogue state into conditionally dependent ele-
ments. Dependencies between these elements
can be derived directly from the dialogue on-
tology. These elements are arranged into a dy-
namic Bayesian network which allows for their
marginal probabilities to be updated during
the dialogue, comprising the belief state. The
belief state is then mapped into a smaller-scale
summary space and the decisions are optimised
using the natural actor critic algorithm. In the
Parlance application, hand-crafted policies
261
Figure 5: Screenshot of the recommended
restaurant for the English application
can be compared to learned ones.
2.2 Natural Language Generation
As mentioned above, the server returns the
string to be synthesised by the Google TTS
API. This mobile framework allows for testing
of alternative approaches to NLG. In particu-
lar, we are interested in comparing a surface re-
aliser that uses CRFs against a template-based
baseline. The CRFs take semantically anno-
tated phrase structure trees as input, which it
uses to keep track of rich linguistic contexts.
Our approach has been compared with a num-
ber of competitive state-of-the art surface real-
izers (Dethlefs et al., 2013), and can be trained
from example sentences with annotations of se-
mantic slots.
2.3 Local Search and Knowledge Base
For the English system, the domain database is
populated by the search Yahoo API (Bouchard
and Mika, 2013) with restaurants in San Fran-
sisco. These restaurant search results are
returned based on their longitude and lati-
tude within San Francisco for 5 main areas, 3
price categories and 52 cuisine types contain-
ing around 1,600 individual restaurants.
The Chinese database has been partially
translated from an English database for restau-
rants in Cambridge, UK and search is based
on 3 price categories, 5 areas and 35 cuisine
types having a total of 157 restaurants. Due
to the language-agnostic nature of the Par-
lance system, only the name and address
fields needed to be translated.
3 Future Work
Investigating application side audio compres-
sion and audio streaming over a mobile in-
ternet connection would enable further assess-
ment of the ASR and TTS components used
in the original Parlance system (Hastie et
al., 2013). This would allow for entire research
systems to be plugged directly into the mobile
interface without the use of third party ASR
and TTS.
Future work also involves developing a feed-
back mechanism for evaluation purposes that
does not put undue effort on the user and put
them off using the application. In addition,
this framework can be extended to leverage
hyperlocal and social information of the user
when displaying items of interest.
Acknowledgements
The research leading to this work was funded
by the EC FP7 programme FP7/2011-14
under grant agreement no. 287615 (PAR-
LANCE).
References
H. Bouchard and P. Mika. 2013. Interactive hy-
perlocal search API. Technical report, Yahoo
Iberia, August.
N. Dethlefs, H. Hastie, H. Cuay?huitl, and
O. Lemon. 2013. Conditional Random Fields
for Responsive Surface Realisation Using Global
Features. In Proceedings of the 51st Annual
Meeting of the Association for Computational
Linguistics (ACL), Sofia, Bulgaria.
H. Hastie, M.A. Aufaure, P. Alexopoulos,
H. Cuay?huitl, N. Dethlefs, M. Gasic,
J. Henderson, O. Lemon, X. Liu, P. Mika,
N. Ben Mustapha, V. Rieser, B. Thomson,
P. Tsiakoulis, Y. Vanrompay, B. Villazon-
Terrazas, and S. Young. 2013. Demonstration
of the PARLANCE system: a data-driven
incremental, spoken dialogue system for in-
teractive search. In Proceedings of the 14th
Annual Meeting of the Special Interest Group
on Discourse and Dialogue (SIGDIAL), Metz,
France, August.
B. Thomson and S. Young. 2010. Bayesian up-
date of dialogue state: A POMDP framework
for spoken dialogue systems. Computer Speech
and Language, 24(4):562?588.
262
Proceedings of the 8th International Natural Language Generation Conference, pages 138?142,
Philadelphia, Pennsylvania, 19-21 June 2014. c?2014 Association for Computational Linguistics
Multi-adaptive Natural Language Generation using Principal Component
Regression
Dimitra Gkatzia, Helen Hastie, and Oliver Lemon
School of Mathematical and Computer Sciences, Heriot-Watt University, Edinburgh
{dg106, h.hastie, o.lemon}@hw.ac.uk
Abstract
We present FeedbackGen, a system that
uses a multi-adaptive approach to Natu-
ral Language Generation. With the term
?multi-adaptive?, we refer to a system
that is able to adapt its content to dif-
ferent user groups simultaneously, in our
case adapting to both lecturers and stu-
dents. We present a novel approach to
student feedback generation, which simul-
taneously takes into account the prefer-
ences of lecturers and students when de-
termining the content to be conveyed in
a feedback summary. In this framework,
we utilise knowledge derived from rat-
ings on feedback summaries by extract-
ing the most relevant features using Prin-
cipal Component Regression (PCR) anal-
ysis. We then model a reward function
that is used for training a Reinforcement
Learning agent. Our results with stu-
dents suggest that, from the students? per-
spective, such an approach can generate
more preferable summaries than a purely
lecturer-adapted approach.
1 Introduction
Summarisation of time-series data refers to the
task of automatically generating reports from at-
tributes whose values change over time. Content
selection is the task of choosing what to say, i.e.
what information is to be included in a report (Re-
iter and Dale, 2000). We consider the task of auto-
matically generating feedback summaries for stu-
dents describing their performance during the lab
of a computer science module over the semester.
Various factors can influence students? learn-
ing such as difficulty of the material (Person et
al., 1995), workload (Craig et al., 2004), atten-
dance in lectures (Ames, 1992), etc. These fac-
tors change over time and can be interdependent.
In addition, different stakeholders often have con-
flicting goals, needs and preferences, for example
managers with employees, or doctors with patients
and relatives, or novice and expert users. In our
data, for instance, lecturers tend to comment on
the hours that the student studied, whereas the stu-
dents disprefer this content. In our previous work,
we showed that lecturers and students have dif-
ferent perceptions regarding what constitutes good
feedback (Gkatzia et al., 2013). Here, we present a
novel approach to generation by adapting its con-
tent to two user groups simultaneously. Producing
the same summary for two groups is important as
it allows for shared context and meaningful further
discussion and reduces development time.
2 Related Work
Previous work on NLG systems that address more
than one user group employs different versions of
a system for each different user group (Gatt et al.,
2009; Hunter et al., 2011; Mahamood and Reiter,
2011), makes use of User Models (Janarthanam
and Lemon, 2010; Thompson et al., 2004; Zuker-
man and Litman, 2001) or personalises the output
to individual users using rules (Reiter et al., 1999).
Our proposed system adapts the output to the pref-
erences of more than one user type1, lecturers and
students, but instead of developing many different
systems or using User Models that describe differ-
ent users, it attempts to model the middle ground
between the preferences.
In order to identify the users? preferences, we
apply Principal Components Regression (PCR
(Jolliffe, 1982)) analysis to two datasets that con-
tain lecturers? and students? ratings and identify
the most important variables from the principal
components, which are then included in a reward
function. This hand-crafted reward function is
used for training an RL agent for summarisation
1Our approach is different to multi-objective optimisa-
tion.
138
Raw Data
factors week 2 week 3 ... week 10
marks 5 4 ... 5
hours studied 1 2 ... 3
... ... ... ... ...
Trends from Data
factors trend
(1) marks trend other
(2) hours studied trend increasing
(3) understandability trend decreasing
(4) difficulty trend decreasing
(5) deadlines trend increasing
(6) health issues trend other
(7) personal issues trend decreasing
(8) lectures attended trend other
(9) revision trend decreasing
Summary
Your overall performance was excellent
during the semester. Keep up the good
work and maybe try some more challeng-
ing exercises. Your attendance was vary-
ing over the semester. Have a think about
how to use time in lectures to improve your
understanding of the material. You spent 2
hours studying the lecture material on
average. You should dedicate more time
to study. You seem to find the material
easier to understand compared to the
beginning of the semester. Keep up the
good work! You revised part of the learn-
ing material. Have a think whether revis-
ing has improved your performance.
Table 1: The table on the top left shows an example of the time-series data. The table on the bottom left
shows an example of described trends. The box on the right presents a target summary.
of time-series data. Our previous work showed
that when comparing RL and supervised learning
in the context of student feedback generation, stu-
dents preferred the output generated by the RL
system (Gkatzia et al., 2014a). Therefore, here, we
used RL rather than a supervised learning method.
The work described here builds on work reported
in (Gkatzia et al., 2014b), which uses as a reward
function the average of the Lecturer-adapted and
Student-adapted reward functions. However, that
method seems to cancel out the preferences of the
two groups whereas PCR is able to identify rele-
vant content for both groups.
In the next section, we describe the data used,
and the methodology for the multi-adaptive NLG,
as well as two alternative systems. In Section 4,
we describe the comparison of these three systems
in a subjective evaluation and present the results in
Section 5. A discussion follows in Section 6 and
finally, future work is discussed in Section 7.
3 Methodology
Reinforcement Learning is a machine learning
technique that defines how an agent learns to take
optimal sequences of actions so as to maximize a
cumulative reward (Sutton and Barto, 1998). In
our framework, the task of summarisation of time-
series data is modelled as a Markov Decision Pro-
cess, where the decisions on content selection cor-
respond to a sequence of actions (see Section 3.2).
Temporal Difference (TD) learning (Sutton and
Barto, 1990) is used for training three agents in
a simulated environment to learn to make optimal
content selection decisions:
1. by adapting to both groups simultaneously,
2. by adapting to lecturers,
3. by adapting to students.
3.1 The Data
For this study, the dataset described in (Gkatzia et
al., 2013) was used. Table 1 presents an exam-
ple of this dataset that describes a student?s learn-
ing factors and an aligned feedback summary pro-
vided by a lecturer. The dataset is composed of
37 similar instances. Each instance consists of
time-series information about the student?s learn-
ing routine and the selected templates that lec-
turers used to provide feedback to this particu-
lar student. A template is a quadruple consist-
ing of an id, a factor (bottom left of Ta-
ble 1), a reference type (trend, week, aver-
age, other) and surface text. For instance,
a template can be (1, marks, trend, ?Your marks
were <trend>over the semester?). The lexical
choice for <trend>(i.e. increasing or decreasing)
depends on the values of time-series data. There
is a direct mapping between the values of factor
139
and reference type and the surface text. The time-
series factors are listed in Table 1.
3.2 Actions and states
The state consists of the time-series data and the
number of factors which have so far been selected
to be talked about (the change of the value of this
variable consequently introduces a state change).
In order to explore the state space the agent se-
lects a time-series factor (e.g. marks, deadlines
etc.) and then decides whether to talk about it or
not, until all factors have been considered.
3.3 Reward function
The reward function is the following cumulative
multivariate function:
Reward = a+
n?
i=1
bi ? xi + c ? length
where X = {x1, x2, ..., xn} describes the cho-
sen combinations of the factor trends observed in
the time-series data and a particular template (i.e.
the way of mentioning a factor). a, b and c are the
correlation coefficients and length describes the
number of factors selected to be conveyed in the
feedback summary. The value of xi is given by
the function:
xi =
?
?
?
1, the combination of a factor trend
and a template type is included
0, if not.
The coefficients represent the level of preference
for a factor to be selected and the way it is con-
veyed in the summary. In the training phase, the
agent selects a factor and then decides whether to
talk about it or not. If the agent decides to refer
to a factor, the selection of the template is then
performed in a deterministic way, i.e. it selects the
template that results in higher reward.
Each rated summary is transformed into a vec-
tor of 91 binary features. Each feature describes
both (1) the trend of a factor (e.g. marks increas-
ing, see also Table 1) and (2) the way that this
factor could be conveyed in the summary (e.g.
one possible way is referring to average, another
possible way is referring to increasing/decreasing
trend). If both conditions are met, the value of
the feature is 1, otherwise 0. The 91 binary fea-
tures describe all the different possible combina-
tions. For both the Lecturer-adapted and Student-
adapted systems, the reward function is derived
from a linear regression analysis of the provided
dataset, similarly to Walker et al. (1997) and
Rieser et al. (2010).
3.3.1 Multi-adaptive Reward Function
In order to derive a reward function that finds a
balance between the two above mentioned sys-
tems, we use PCR to reduce the dimensionality
of the data and thus reduce the introduced noise.
Through PCR we are able to reduce the number
of features and identify components of factors that
are deemed important to both parties to be used in
the reward function.
PCR is a method that combines Principal Com-
ponent Analysis (PCA) (Jolliffe, 1986) with lin-
ear regression. PCA is a technique for reducing
the dataset dimensionality while keeping as much
of the variance as possible. In PCR, PCA is ini-
tially performed to identify the principal compo-
nents, in our case, the factors that contribute the
most to the variance. Then, regression is applied
to these principal components to obtain a vector
of estimated coefficients. Finally, this vector is
transformed back into the general linear regres-
sion equation. After performing this analysis on
both datasets (students and lecturers), we choose
the most important (i.e. the ones that contribute
the most to the variance) commoncomponents or
features resulting in 18 features which were used
in the reward function. We then design a hand-
crafted reward function taking into account this
PCR analysis. The five most important features
are shown in Table 2.
factor trend way it is mentioned
(1) marks stable average
(2) hours studied decreasing trend
(3) health issues decreasing weeks
(4) lectures attended stable average
(5) personal issues increasing trend
Table 2: The top 5 features out of the 18 selected
through PCR analysis.
4 Evaluation
FeedbackGen is evaluated with real users against
two alternative systems: one that adapts to lectur-
ers? preferences and one that adapts to students?
preferences. The output of the three systems is
ranked by 30 computer science students from a va-
riety of years of study. Time-series data of three
students are presented on graphs to each partici-
pant, along with three feedback summaries (each
one generated by a different system), in random
order, and they are asked to rank them in terms of
preference.
140
Student-adapted {Ranking: 1st*} FeedbackGen {Ranking: 2nd*} Lecturer-adapted {Ranking: 3rd*}
You did well at weeks 2, 3, 6, 8, 9 and 10,
but not at weeks 4, 5 and 7. Have a think
about how you were working well and
try to apply it to the other labs. Your at-
tendance was varying over the semester.
Have a think about how to use time in lec-
tures to improve your understanding of
the material. You found the lab exercises
not very challenging. You could try out
some more advanced material and exer-
cises. You dedicated more time study-
ing the lecture material in the beginning
of the semester compared to the end of
the semester. Have a think about what
is preventing you from studying. Revis-
ingmaterial during the semester will im-
prove your performance in the lab.
Your overall performance was
very good during the semester.
Keep up the good work and maybe
try some more challenging exer-
cises. You found the lab exer-
cises not very challenging. You
could try out some more advanced
material and exercises. You dedi-
cated more time studying the lec-
ture material in the beginning of
the semester compared to the end
of the semester. Have a think about
what is preventing you from study-
ing. You have had other dead-
lines during weeks 6 and 8. You
may want to plan your studying and
work ahead.
Your overall performance was very
good during the semester. Keep up the
good work and maybe try some more
challenging exercises. You found the
lab exercises not very challenging. You
could try out some more advanced mate-
rial and exercises. You dedicated more
time studying the lecture material in the
beginning of the semester compared to
the end of the semester. Have a think
about what is preventing you from study-
ing. You have had other deadlines during
weeks 6 and 8. You may want to plan
your studying and work ahead. You did
not face any health problems during the
semester. You did not face any personal
issues during the semester.
Table 3: The table presents example outputs from the three different systems in order of highest ranked
(bold signifies the chosen template content, * denotes significance with p <0.05 after comparing each
system with each other using Mann Whitney U test).
5 Results
Table 3 shows three summaries that have been
generated by the different systems. As we can see
from Table 3, students significantly prefer the out-
put of the system that is trained for their prefer-
ences. In contrast, students significantly dispre-
fer the system that is trained for lecturers? pref-
erences. Finally, they rank as second the system
that captures the preferences of both lecturers and
students, which shows that it might be feasible to
find middle ground between the preferences of two
user groups. Significance testing is done using
a Mann Whitney U test (p <0.05), performing a
pair-wise comparison.
6 Discussion
The weights derived from the linear regression
analysis vary from the Lecturer-adapted func-
tion to the Student-adapted function. For in-
stance, the lecturers? most preferred content is
hours studied. This, however, does not factor
heavily into the student?s reward function, apart
from the case where hours studied are de-
creasing or remain stable (see also Table 2).
Students like reading about
personal issues when the number of issues
they faced was increasing over the semester. On
the other hand, lecturers find it useful to give
advice to all students who faced personal issues
during the semester, hencepersonal issues
are included in the top 18 features (Table 2).
Moreover, students seem to mostly prefer a feed-
back summary that mentions the understandability
of the material when it increases, which is positive
feedback.
As reflected in Table 2, the analysis of PCR
showed that both groups found it useful to refer
to the average of marks when they remain stable.
In addition, both groups found understandability
when it increases useful, for a variety of reasons,
for example lecturers might find it useful to en-
courage students whereas students might prefer to
receive positive feedback. Both groups also agree
on hours studied as described earlier. On the
other hand, both groups find mentioning the stu-
dents? difficulty when it decreases as positive.
7 Future Work
In the future, we plan to evaluate our methodol-
ogy with lecturers and a larger sample of students
across different disciplines. Moreover, we aim to
port our methodology to a different domain, and
try to find the middle ground between the pref-
erences of novices and expert users when sum-
marising medical data while providing first aid.
Finally, we want to compare the methodology pre-
sented here to a multi-objective optimisation ap-
proach (Fonseca and Flemming, 1993), where the
preferences of each user group will be modelled as
two different optimisation functions.
Acknowledgements
The research leading to this work has re-
ceived funding from the EC?s FP7 programme:
(FP7/2011-14) under grant agreement no. 248765
(Help4Mood).
141
References
Carole Ames. 1992. Classrooms: Goals, structures,
and student motivation. Journal of Educational Psy-
chology, 84(3):p261?71.
Scotty D. Craig, Arthur C. Graesser, Jeremiah Sullins,
and Barry Gholson. 2004. Affect and learning: an
exploratory look into the role of affect in learning
with autotutor.
Carlos Fonseca and Peter Flemming. 1993. Genetic
algorithms for multiobjective optimization: Formu-
lation, discussion and generalization. In 5th Inter-
national Conference on Genetic Algorithms.
Albert Gatt, Francois Portet, Ehud Reiter, James
Hunter, Saad Mahamood, Wendy Moncur, and So-
mayajulu Sripada. 2009. From data to text in the
neonatal intensive care unit: Using NLG technology
for decision support and information management.
AI Communications, 22: 153-186.
Dimitra Gkatzia, Helen Hastie, Srinivasan Ja-
narthanam, and Oliver Lemon. 2013. Generating
student feedback from time-series data using Rein-
forcement Learning. In 14th European Workshop in
Natural Language Generation (ENLG).
Dimitra Gkatzia, Helen Hastie, and Oliver Lemon.
2014a. Comparing multi-label classification with
reinforcement learning for summarisation of time-
series data. In 52nd Annual Meeting of the Associa-
tion for Computational Linguistics (ACL).
Dimitra Gkatzia, Helen Hastie, and Oliver Lemon.
2014b. Finding Middle Ground? Multi-objective
Natural Language Generation from Time-series
data. In 14th Conference of the European Chap-
ter of the Association for Computational Linguistics
(EACL).
Jim Hunter, Yvonne Freer, Albert Gatt, Yaji Sripada,
Cindy Sykes, and D Westwater. 2011. Bt-nurse:
Computer generation of natural language shift sum-
maries from complex heterogeneous medical data.
American Medical Informatics Association, 18:621-
624.
Srinivasan Janarthanam and Oliver Lemon. 2010.
Adaptive referring expression generation in spoken
dialogue systems: Evaluation with real users. In
11th Annual Meeting of the Special Interest Group
on Discourse and Dialogue (SIGDIAL).
Ian T. Jolliffe. 1982. A note on the use of principal
components in regression. Journal of the Royal Sta-
tistical Society, Series C: 31(3):300?303.
Ian Jolliffe. 1986. Principal Component Analysis.
Springer-Verlag.
Saad Mahamood and Ehud Reiter. 2011. Generating
Affective Natural Language for Parents of Neona-
tal Infants. In 13th European Workshop in Natural
Language Generation (ENLG).
Natalie K. Person, Roger J. Kreuz, Rolf A. Zwaan, and
Arthur C. Graesser. 1995. Pragmatics and peda-
gogy: Conversational rules and politeness strategies
may inhibit effective tutoring. Journal of Cognition
and Instruction, 13(2):161-188.
Ehud Reiter and Robert Dale. 2000. Building natu-
ral language generation systems. Cambridge Uni-
versity Press.
Ehud Reiter, Roma Robertson, and Liesl Osman. 1999.
Types of knowledge required to personalise smoking
cessation letters. Artificial Intelligence in Medicine:
Proceedings of the Joint European Conference on
Artificial Intelligence in Medicine and Medical De-
cision Making.
Verena Rieser, Oliver Lemon, and Xingkun Liu. 2010.
Optimising information presentation for spoken dia-
logue systems. In 48th Annual Meeting of the Asso-
ciation for Computational Linguistics (ACL).
Richard Sutton and Andrew Barto. 1990. Time deriva-
tive models of pavlovian reinforcement. Learning
and Computational Neuroscience: Foundations of
Adaptive Networks, pages 497?537.
Richart Sutton and Andrew Barto. 1998. Reinforce-
ment learning. MIT Press.
Cynthia A. Thompson, Mehmet H. Goker, and Pat Lan-
gley. 2004. A personalised system for conversa-
tional recommendations. Journal of Artificial Intel-
ligence Research, 21(1).
Marilyn Walker, Diane J Litman, Candace Kamm, and
Alicia Abella. 1997. PARADISE: A framework
for evaluating spoken dialogue agents. In 8th con-
ference on European chapter of the Association for
Computational Linguistics (EACL).
Ingrid Zukerman and Diane Litman. 2001. Natu-
ral language processing and user modeling: Syner-
gies and limitations. In User Modeling and User-
Adapted Interaction, 11(1-2).
142
