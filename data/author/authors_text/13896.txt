DTD-Driven Bilingual Document Generation 
...... ,Arantza Casillas . . . . .  
Departamento de Automgtica, Universidad e Alcalgt e -mai l  :arantza@aut,  a lca ia ,  es 
Joseba Abaitua 
Facultad de Filosofia y Letras Universidad e Deusto, Bilbao e -ami l :aba i tua0f i l  .deusto .  es 
Raque l  Mar t inez  
Departamento de  Sis. Informgticos y Programacidn, Facultad de MatemgLticas 
Universidad C0mplutense de Madrid e -mai l  : raquelOeucmos, sire. ucm. es 
Abst ract  
Extensively annotated bilingual parallel corpora 
can be exploited to feed editing tools that in- 
tegrate the processes of document composition 
and translation. Here we discuss the archi- 
tecture of an interactive diting tool that, on 
top of techniques common to most Translation 
Memory-based systems, applies the potential of 
SGML's DTDs to guide the process of bilingual 
document generation. Rather than employing 
just simple task-oriented mark-up, we selected a 
set of TEI's highly complex and versatile collec- 
tion of tags to help disclose the underlying log- 
ical structure of documents in the test-corpus. 
DTDs were automatically induced and later in- 
tegrated in the editing tool to provide the basic 
scheme for new documents. 
1 In t roduct ion  
This paper discusses an approach to the archi- 
tecture of an experimental interactive diting 
tool that integrates the processes of source doc- 
ument composition and translation i to the tar- 
get language. The tool has been conceived as an 
optimal solution for a particular case of bilin- 
gual production of legal documentation, but it 
also illustrates in a more general way how to ex- 
ploit the possibilities of SGML (ISO8879, 1986) 
used extensively to annotate a whole range of 
linguistic and extralinguistic information i  spe- 
cialized bilingual corpora. 
SGML is well established as the coding 
scheme underlying most Translation Memory 
based systems (TMBS), and has been pro- 
posed as the cod-it~g scheme for the interchange 
of existing Translation Memory databases 
Translation Meinories eXchange, TMX (Melby, 
1998). The advantages of SGML have also been 
perceived by a large conmmnity of corpus lin- 
guistics researchers, and big efforts have been 
made in the development of suitable markup 
options to encode a variety of textual types and 
functions -as clearly demonstrated by the Text 
Encoding Initiative, TEI; (Burnard & Speberg- 
MacQueen, 1995). While the tag-sets employed 
by TMBS are simple and task-oriented, TEI has 
offered a highly complex and versatile collection 
of tags. The guiding hypothesis in our experi- 
ment has been the idea that it is possible to 
explore TEI/SGML markup in order to develop 
a system that carries the concept of Translation 
Memory one step further. One important lea- 
ture of SGML is the DTD. DTDs determine the 
logical structure of documents and how to tag 
them accordingly. We have concentrated on the 
accurate description of documents by means of 
TEI conformant SGML markup. The markup 
will help disclose the underlying logical struc- 
ture of documents. From annotated ocumen- 
tation, DTDs can be induced and these DTDs 
provide the basic scheme to produce new doc- 
uments. We have collected a corpus of official 
publications from three main institutions in the 
Basque Autonomous Region in Spain, the Bo- 
letln Oficial de Bizkaia (BOB, 1990-1995), Bo- 
tetln Oficial de Alava (BOA, 1990-1994) and 
Bolet{n Oficial del Pais Vasco (BOPV, 1995). 
Documents in the corpus were composed by Ad- 
nfinistration clerks and translated by transla- 
tors. Both clerks and translators have been us- 
ing a wide variety of word-processors, although 
since 1994 MSWord has been generalized as the 
standard editing tool. Administrative docu- 
mentation shows a regular structure, and is rich 
? in*recurrent textual patterns.  For each docu- ..... 
ment type different document okens share a 
common global distribution of elements. Of- 
ficial document composers learn these global 
structures and apply them consistently. It is 
also the case that composers tend to reuse old 
32 
Document Type 
Orden Form 
Decreto..Foral =-- 
Resolucidn 
Extracto 
Acuerdo 
Norma Foral 
Anuncio 
% 
53% 
..22%,;- 
13% 
5.4% 
3.4% 
1.9% 
0.4% 
documents, where the whole document may 
be considered the translation unit. TM3 can 
.,-., : :- :...~ .~o. ~be~,g. o~strued,as~:~i:hiling~ai,doc,,~ent-database. 
Much redundancy originates from this TM col- 
lection, although it should be noticed that they 
are all by-products derived from the same an- 
notated bitext which subsumes them all. Good 
software packages for TM1 and TM3 already ex- 
ist in the market, and hence their exploitation is 
Table I: 
document files when producing new documents 
of the same type. Despite the fact that no 
SGML software was used at the editing phase, 
texts in the corpus show regular logical struc- 
tures and consistent distribution of text seg- 
ments. Our main goal in tagging the corpus was 
to make all them explicit (Martinez, 1997). The 
most common type of document in the corpus,  
the Orden Foral, was chosen (see Table 1). We 
analysed some 100 tokens and hand-marked the 
most salient elements. The heuristics to identify 
these elements were later expressed in a collec- 
tion of recognition routines in Perl and tested 
against a set of 400 tokens, including the initial 
100. As a result of this process of automatic 
tagging of structural elements we produced a 
TE I /SGML tagged corpus with yet no corre- 
sponding overt DTD. In  Section 2 we will ex- 
plain how DTDs were later induced from the 
tagged corpus. 
Once the corpus was segmented the next step 
was to align it. This was conducted at different 
levels: general document elements (DIV, SEG, 
P), as well as sentential and intra-sentential e - 
ements, such as S, ItS, NUM, DATE, etc. (Mar- 
tinez, 1998b). Aligned in this way, the corpus 
becomes an important resource for translation. 
Four complementary language databases may 
be obtained at any time from the annotated 
corpus: three translation memory databases 
(TM1, TM2, and TM3) as well as a terminology 
database (termbase). The three TMs differ in 
the nature of the translation units they contain. 
TM1 consists of aligned sentences than can feed 
commercial TM software. TM2 contains ele- 
ments which are translation segments ranging 
from whole sections of a document or multi- 
sentence paragraphs to smaller units, such as 
short phrases or proper names. TM3 simply 
hosts the whole collection of aligned bilingual 
Types of documents in the corpus beyond our interest (Trados Translator's Work- 
. . . . .  . bench, Star!s Transit,.,SDLX, D e'j?~fi,. IBM~s. 
browsing tool for TM3). The originality of our 
editing tool lies in a design which benefits from 
joining the potentiality of DTDs  and the ele- 
ments in TM2, as will be shown in sections 4 
and 5. 
2 DTD abst rac t ion  
SGML mark-up determines the logical structure 
of a document and its syntax in the form of a 
context-free grammar. This is called the Doc- 
ument Type Definition (DTD) and it contains 
specifications for: 
? Names and content for all elements that are 
permitted to appear in a document. 
o Order in which these elements must ap- 
pear. 
o Tag attributes with default values for those 
elements. 
DTDs have been abstracted away from the an- 
notations that were automatically introduced 
in the corpus. Similar experiments have been 
reported before in the literature. (Ahonen, 
1995) uses a method to build document in- 
stances from tagged texts that consists of a de- 
terministic finite automaton for each context 
model. Subsequently, these automata re gen- 
eralized and converted into regular expressions 
which are easily transcribed into SGML content 
models. (Shafer, 1995) combines docmnent in- 
stances with simplification rules. Our method 
is similar to  Sharer's, but .with a.modification 
in the way rules reduce document instances. A 
tool to obtain a DTD for all document instances 
has been developed (Casillas, 1999). Given that 
source and target documents how some syn- 
tactic and structural mismatches, two different 
DTDs are induced, one for each language, and 
33 
Spanish Text: 
<div0> 
<div l> ... < /d iv l>  
<seg9 id=9ES2 corresp=gEU2> Contra dicha 
<rs  type=law id=LES12 corresp=LEU10> 
Orden Foral </ rs>,  que agota la vfa ad- 
ministrativa podr~i interponerse recurso 
contencioso-administrativo ante la <rs  
type=organization id=0ES9 corresp=0EUl i> 
Sala de lo Contencioso-Administrativo del Tri- 
? bunal Superior de J usticia del Pais Vasco </ rs  >, 
en el plazo de dos meses, contado desde el d/a 
Basque Text: 
<div0> 
<divl> ... </divl> 
<seg9 i de9EU2 correspe9ES2> <rs type=law 
id=LEUi0 corresp=LESl2> Foru agindu </rs> 
horrek amaiera eman dio administrazio bideari; 
eta beraren aurka <rs type=organizat ion 
id=0EU10> Administrazioarekiko </ rs> 
auzibide-errekurtsoa jarri ahal izango zaio <rs 
type=organization id=0EUll corresp=0ES9> 
Euskal Herriko JustiziAuzitegi Nagusiko Admin- 
istrazioarekiko Auzibideetarako Salari < / rs>,  
siguiente a esta~:m~t.~eaci~m~.sin~p~er~ui~i~deAu~,.~;.~aila~aetek~:~epea~;~4ja~d~mxazpen ~hatl egiten 
utilizacidn de otros medios de defensa que estime den egunaren biharamunetik zenbatuko da epe 
oportunos. </seg9> hori; halo eta guztiz ere, egokiesten diren beste 
defentsabideak ere erabil litezke. </seg9> 
<segl0 id=10ES1 corresp=10EUl> Du- 
rante el referido plazo el expediente BHI-<num 
num=10094> 100/94 </num>-P05-A quedar? de 
manifiesto para su ex~imen en las dependencias 
de <rs type=place id=PES3 corresp=PEU2> 
Bilbao calle Alameda Rekalde </ rs>,  <num 
num=30> 30 </num>, <num num=5> 5.a </num> 
y <hum hum=6> 6.a </hUm> plantas. </segl0> 
</div0> 
<closer id=pESl3 corresp=pEUl3 > <name> 
El Diputado Foral de Urbanismo Pedro Hern?ndez 
Gonz~ilez. </name> </closer> 
<segl0 id=10EU1 corresp=10ESl> Epe hori 
amaitu arte BHI-<num num=10094> 100/94 
</num>-P05-A espedientea gerian egongo da, 
nahi duenak azter dezan, <rs  type=place 
id=PEU2 corresp=PES3> Bilboko Errekalde zu- 
markaleko </ rs> <num num=30> 30.eko </num> 
bulegoetan, <num num=5> 5 </num> eta <num 
num=6> 6.</num> solairuetan. </seg l0> 
</div0> 
<closer  id=pEU13 corresp=pESl3> <name> 
Hirigintzako foru diputatua. Pedro Hern/mdez 
Gonz?1ez. </name> </c loser> 
Figure 1: Ilustrates a sample of the annotated bitext 
are paired through a correspondence table. Cor- 
respondences in this table can be up-dated, or 
deleted. At present, we have six DTDs, one for 
each document ype in each language (there are 
three document ypes; Figure 2 shows a part of 
one of these DTDs). By means of these paired 
DTDs, document elements in each language are 
appropriately placed. In the process of gener- 
ating the bilingual document, a document ype 
must first be selected. Each document ype has 
an associated DTD. This DTD specifies which 
elements are obhgatory and.which are optional. 
With the aid of the DTD, the source document 
is generated. The target document will be gen- 
erated with .the aid of the com~esponding target 
DTD. 
3 Jo in ing  TM2 and  DTD 
TM2 specifically stores a type of translation 
segment class, which we have tagged <seg l>,  
<seg2>... <segn>, <t i t le> and <rs>,  and 
which is relevant o the DTD. Segments tagged 
<segn> are variable recurrent language pat- 
terns very frequent in the specialized domain 
of the corpus and whose occurrence in the text 
is well established. These <segn> tags in- 
clude two attributes: id  and cor respond which 
locate the aligned segment both in the cor- 
pus and in the database (Figure 1). Seg- 
ments tagged <rs> are referring expressions 
which have been recognized, tagged and aligned 
? and which correspond largely to proper names 
(Martinez, 1998a), (Martinez, 1998b). TM2 is 
managed in tile form of a relational database 
-where segments are stored, as records. .Each 
record in the database consists of four fields: 
the segment string, a counter for the occur- 
rences of that string in the corpus, the tag 
and the attributes (type, id and corresp) .  
Table 2 shows how the text fragment inside 
34 
<!ELEMENT 
<!ELEMENT 
<!ELEMENT 
<!ELEMENT 
<!ELEMENT 
<!ELEMENT 
LEGE - - (TEXT)> 
TEXT - - (BODY)> 
BODY - - (OPENER,  DIVO,  CLOSER)> 
0PENER - - (TITLE, NUM, DATE, NAME?, SEGI)> 
SEGI - - (SEGIi, (#PCDATAIRSIDATEINUM)+)> 
(SEGii, NUM, DATE, RS, NAME, TITLE) (#PCDATA)> 
<!ELEMENT 
\ [SEGI5)+,  
<!ELEMENT 
<!ELEMENT 
<!ELEMENT 
<!ELEMENT 
(DIVO) - - ( (#PCDATA \[RS INUM \[DATE ISEG4\[SEGS ISEG6\[SEG7 ISEG8 ISEGI2 \[SEGi4 
SEG9?, SEGIO?)> 
(SEG4, SEG5, SEG6) (#PCDATA) > 
(SEG9, SEGiO, SEGT, SEG8, SEGi2, SEGi4, SEGi5) - - (#PCDATA\[RS\[DATE\[NUM)+> 
(CLOSER) i i (PLACENAME?,DATE? ,  NAME?)> 
(PLACENAME)  - : (RS)> 
<!ATTLIST RS TYPE (0RGANIZATION\[ LAW\[ PLACE\[ UNCAT) UNCAT> 
Figure 2: Part of the DTD of the type document Orden Foral 
the </d iv l>. . .</d iv0> tags of Figure 1 ren- 
ders three records in the database. Note how 
the content of the string field in the database 
maintains only the initial <segn> and <rs> 
tags. Furthermore, <rs> tagged segments in- 
side <segn> records are simplified so that their 
content is dismissed and only the initial tag is 
kept (Lange et al, 1997). The reason is that 
they are considered variable elements within 
the segment (dates and numbers are also these 
type of elements). The strings Orden Foral of 
record 2 marked as <rs  type=law> and Sala 
de lo Contencioso-Administrativo del Tribunal 
Superior de Justicia del Pais Vasco of record 
3 <rs  type=organ izat ion> are thus not in- 
cluded in record 1 <segg>,  since they may dif- 
fer in other instantiations of the segment. These 
internal elements are largely proper names that 
vary from one instantiation of the segment o 
another. The <rs> tag can be considered 
to be the name of the varying element. The 
value of the type attribute <rs  type=law> 
constraints the kind of referential expression 
that may be inserted in that point of the trans- 
lation segment. Table 2 shows that source 
and target records may not have straight one- 
to-one correspondences. Although this is by 
no means the general:case; only about 5.61%, 
(Martinez, 1998a), such one-to-N correspon- 
dences provide good ground to explain how 
the TM2 is designed. The asymmetry can be 
easily explained. The Spanish term recurso 
contencioso-administrativo has been translated 
into Basque by means of a category changing 
operation, where the Spanish adjective admin- 
istrativo has been translated as a Basque noun 
complement Administrazioarekiko which liter- 
ally means "Administration-the-with-of' trig- 
gering its identification as a proper noun. 
Table 3 shows the way in which source lan- 
guage units are related with their correspond- 
ing target units, which, as can be observed, can 
be one-to-one or one-to-N. This means that one 
source element can have more than one transla- 
tion. 
TM2 is created in three steps: 
? First, non-pertinent ags are filtered out 
from the annotated corpus. Tags marking 
sentence <s> and paragraph <p> align- 
ment are removed because they are of no 
interest for TM2 'recall that they are reg- 
istered in TM1). 
? Second, translation segments <segn>,  
<t i t le> phrases and referential expres- 
sions <rs> are detected in the source doc- 
ument and looked up in the database. 
o Third, if they are not already present in 
the database, they are stored each in its 
database.and values of the id  and  cor resp  
attributes-are~used to set the correspon- 
dence between source and target database. 
4 Compos i t ion  S t ra tegy  
Every phase in tile process is guided by the 
markup contained in TM2 and the paired DTDs 
35 
Spanish Unit 
<seg9> Contra dicha <rs type=law>, 
que agota la via 
administrativa podr~i interponerse r curso 
contencioso-administrativo nte 
la <rs type=organization>, 
en el plazo de dos meses, contado 
desde el dla siguiente 
a esta notificacidn, sin perjuicio 
de la utilizaci6n 
Basque Unit 
<seg9> <rs type=law> 
horrek amaiera eman dio 
? - adrrrinistrazio'bideari; eta:beraren aurka: " 
<rs type=organization> 
auzibide-errekurtsoa jarri 
ahal izango zaio 
<rs type=organization>, 
bi hilabeteko epean; 
jakinarazpen hau egiten den egunaren 
de otros medios de. defensa que estime oportunos, biharamunetik zenbatuko da epe hori; 
que estime oportunos, hala eta guztiz ere, egokiesten diren beste 
. . . . . . . . . . . .  '-. . -. .~' ?.~ ..... :z-:: : :~ : : :de femVs~ideate .~ere :erab i t~htezke~, . . . -  - . . . .  
<rs type=law> Orden Foral <rs type=law> Foru agindu 
<rs type=organization> Administrazioarekiko 
<rs type=organizat ion>-S~ de lo 
Contencioso-Administrativo 
del Tribunal Superior de Justicia del Pals Vasco 
<rs type=organization> Euskal 
Herriko Justizi 
Auzitegi Nagusiko Administrazioarekiko 
Auzibideetarako Salari 
Table 2: Source and targe language record samples in TM2 
Spanish Unit Basque Unit 
<rs type=organization id= corresp=> 
Bolet/n Oficial de Bizkaia 
<rs type=organization id= corresp=> 
Bizkaiko Aldizkari Ofizialea 
<rs  type=organization id= orresp=> 
Bizkaiko Engunkari Ofizialea 
<rs  type=organization id= corresp=> 
Bizkaiko Boletin Ofizialea 
<seg3> dispongo <seg3> xedatu dut 
<seg3> xedatzen duen 
Table 3: Source language units related with their corresponding target language units 
which control the application of this markup. 
The composition process follows two main steps 
which correspond to the traditional source doc- 
ument generation and translation into the tar- 
get document. The markup and the paired 
DTD guides the process in the following man- 
ner: 
1. Before the user starts writing the source 
document, he must select a document type, 
i.e., a DTD. This has two consequences. On 
the one hand, the selected DTD produces 
a source document template that contains 
the logical structure of the document and 
some of its contents. On the other hand, 
the selected source DTD trigger:s .a target 
paired DTD, which will be used later to 
translate the document. There are three 
different types of elements in the source 
document template: 
? Some elements are mandatory and are 
. :  A . . 
provided to the user, who must only 
choose its content among some alter- 
native usages (s/he will get a list of 
alternatives ordered by frequency, for 
example <t i t le>) .  Other obligatory 
elements, such as dates and numbers, 
will also be automatically generated. 
o Some other elements in the template 
are optional (e.g., <seg9>).  Again, 
a list of alternatives will be offered to 
the user. These optional elements are 
.sensitive to the .context (document or 
division type), and markup is also re- 
sponsible for constraining the valid op- 
- ~ tion.s.g:iverlTtQ,the user:. Obligatory and 
optional elements are retrieved from 
TM2, and make a considerable part of 
the source document. 
. All documents have an important part 
of their content which is not deter- 
36 
Word/doc. 'Num.  doc. TM2 
0-500 378 34.91 
500-1,000 -25 . . . .  .M:0t - 
More 1,000 16 3.01 
Weighted mean 31.8 
Table 4: % generated by TM2 
mined by the DTD (<d iv l>) .  It is the 
most variable part, and .the system lets 
the writer input text freely. It is when 
TM2 has nothing to offer that TM1 
and TM3 may provide useful material. 
Given the recurrent style of legal doc- 
umentation, it is quite likely that the 
user will be using many of the bilin- 
gual text choices already aligned and 
available in TM1 and TM3. 
2. Once the source document has been com- 
pleted, the system derives its particular 
logical structure, which, with the aid of the 
target DTD, is projected into the resulting 
target logical structure. 
5 Eva luat ion  
Table 4 shows the number of words that make 
up the segments stored in TM2 from the source 
documents. There is a line for each document 
size considered. We can see that the average 
of segments contained in TM2 is 31.8%, on a 
scale from 34.91% to only 3.01%. The amount 
of segments dealt with in this way largely de- 
pends on the size of the document. Short doc- 
uments (90.21) have about 35% of their text 
composed in this way. This figure goes down to 
3% in documents larger than 1,000 words. This 
is understandable, in the sense that the larger 
the document, the smaller proportion of fixed 
sections it will contain. 
Table 5. shows the Immber of words that are 
proposed for the target document. These trans- 
lations are obtained from what is stored in TM2 
complemented by algorithms designed to trans- 
late dates and numbers. We can see that the 
average of document ranslated is 34%. Short 
documents have 36% of their text translated. 
falling to above 11% in t, he case of large docu- 
I l l ents .  
37 
Word/doc. Num. doc. TM2 Alg. Total 
0-500 378 28.3 7.7 36 
~,500-1;000 25 ' :.: 12.3  . '9.6 ? -.21-3'. 
More 1,000 16 4.7 !6 .41  10.7 
i i 
W.M.  26.5 ' 7.6 I 34.2 
Table 5: % translated by TM2 and algorithms 
6 Conc lus ions  
We have shown how7 DTDs derived from de- 
? scriptive markup can~be"employed to ease the  
process of generating bilingual dedicated ocu- 
mentation. On average, one third of the con- 
tents of thedocuments can be automatically ac- 
counted for. It must also be pointed out that 
the part being dealt with represents the core 
structure, lay-out and logical components of the 
text. The remaining two-thirds of untreated 
document can still be managed with the aid 
of sentence-oriented TMBS, filling in the gaps 
in the ore/all skeleton provided by the target 
template. Composers may also browse TM3 to 
retrieve whole blocks for those parts which are 
not determined by the DTD. One of the clear 
targets for the future is to extend the cover- 
age of the corpus and to test structural taggers 
against other document types. A big challenge 
we face is to develop tools that automatically 
perform the recognition of documents from less 
restricted and more open text types. However, 
we are not sure of the extent of the practicality 
of such an approach. An alternative direction 
we are presently considering is to establish a 
collection of pre-defined ocument types, which 
would be validated by the institutional writers 
themselves. It is a process currently being im- 
plemented in the Basque administration to de- 
fine docmnent models for writers and transla- 
tors to follow. What we have demonstrated is 
that paired DTDs, complemented with rich lan- 
guage resources of the kind defined in this pa- 
per, allow for the design of optimal editing envi- 
ronments which would combine both document 
composition and translation as one single pro- 
cess. All the resourcesneeded (DTDs.and TMs) . 
can be induced from an aligned corpus. 
7 Acknowledgements  
This research is being partially supported by the 
Spanish Research Agency, project ITEM, TIC- 
96-1243-C03-01. 
References  
H. Ahonen. Automatic Generation of SGML 
:Content Models. Electronic :Publishing, 8(2- 
3):195-206, 1995. 
L. Burnard, C. Speberg-McQueen. TEILite: 
An Introduction to Text Encoding 
for Interchange. URL://http://www- 
tel. uic. edu/orgs/tei/intros/teiu5, tei, 1995. 
Casillas A., Abaitua J., Martinez R. Extracci6n 
y aprovechamiento de DTDs emparejadas en 
. . . . . . . . . . . . .  corpus paralelos. Proceesamientq: deL!~enguaje 
Natural, 25:33-41, 1999. 
ISO 8879, Information Processing-Text and Of- 
fice Systems-Standard Generalized Markup 
Language (SGML). International Organiza- 
tion For Standards, 1986, Geneva. 
J. Lang6, I~ Gaussier, B. Daile. Bricks and 
Skeletons: Some Ideas for the Near Future of 
MATH. Machine Translation, 12:39-51, 1997. 
Martinez R., Abaitua J., Casillas R. Bilingual 
parallel text segmentation and tagging for 
specialized ocumentation. Proceedings of the 
International Conference Recent Advances in 
Natural Language Processing (RANLP'97), 
369-372, 1997. 
Martinez R., Abaitua J., Casillas A.. Bi- 
text Correspondences through Rich Mark- 
up. 36th Annual Meeting of the Association 
for Computational Linguistics abd 17 Inter- 
national Conference on Computational Lin- 
guistics (COLING-ACL'98), 812-818, 1998. 
Martinez R., Abaitua J., Casillas A.. Aligning 
tagged bitexts. Sixth Workshop on Very Large 
Corpora, 102-109, 1998. 
A. Melby. Data Exchange from OSCAR and 
MARTIF Projects. First International con- 
ference on Language Resources ~4 Evaluation, 
3-7, 1998. 
38 
Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the ACL, pages 1145?1152,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Multilingual Document Clustering: an Heuristic Approach Based on
Cognate Named Entities
Soto Montalvo
GAVAB Group
URJC
soto.montalvo@urjc.es
Raquel Mart??nez
NLP&IR Group
UNED
raquel@lsi.uned.es
Arantza Casillas
Dpt. EE
UPV-EHU
arantza.casillas@ehu.es
V??ctor Fresno
GAVAB Group
URJC
victor.fresno@urjc.es
Abstract
This paper presents an approach for Mul-
tilingual Document Clustering in compa-
rable corpora. The algorithm is of heuris-
tic nature and it uses as unique evidence
for clustering the identification of cognate
named entities between both sides of the
comparable corpora. One of the main ad-
vantages of this approach is that it does
not depend on bilingual or multilingual re-
sources. However, it depends on the pos-
sibility of identifying cognate named enti-
ties between the languages used in the cor-
pus. An additional advantage of the ap-
proach is that it does not need any infor-
mation about the right number of clusters;
the algorithm calculates it. We have tested
this approach with a comparable corpus
of news written in English and Spanish.
In addition, we have compared the results
with a system which translates selected
document features. The obtained results
are encouraging.
1 Introduction
Multilingual Document Clustering (MDC) in-
volves dividing a set of n documents, written in
different languages, into a specified number k of
clusters, so the documents that are similar to other
documents are in the same cluster. Meanwhile
a multilingual cluster is composed of documents
written in different languages, a monolingual clus-
ter is composed of documents written in one lan-
guage.
MDC has many applications. The increasing
amount of documents written in different lan-
guages that are available electronically, leads to
develop applications to manage that amount of
information for filtering, retrieving and grouping
multilingual documents. MDC tools can make
easier tasks such as Cross-Lingual Information
Retrieval, the training of parameters in statistics
based machine translation, or the alignment of par-
allel and non parallel corpora, among others.
MDC systems have developed different solu-
tions to group related documents. The strate-
gies employed can be classified in two main
groups: the ones which use translation technolo-
gies, and the ones that transform the document into
a language-independent representation.
One of the crucial issues regarding the methods
based on document or features translation is the
correctness of the proper translation. Bilingual re-
sources usually suggest more than one sense for
a source word and it is not a trivial task to select
the appropriate one. Although word-sense disam-
biguation methods can be applied, these are not
free of errors. On the other hand, methods based
on language-independent representation also have
limitations. For instance, those based on thesaurus
depend on the thesaurus scope. Numbers or dates
identification can be appropriate for some types
of clustering and documents; however, for other
types of documents or clustering it could not be so
relevant and even it could be a source of noise.
In this work we dealt with MDC and we pro-
posed an approach based only on cognate Named
Entities (NE) identification. We have tested this
approach with a comparable corpus of news writ-
ten in English and Spanish, obtaining encouraging
results. One of the main advantages of this ap-
proach is that it does not depend on multilingual
resources such as dictionaries, machine translation
systems, thesaurus or gazetteers. In addition, no
information about the right number of clusters has
1145
to be provided to the algorithm. It only depends on
the possibility of identifying cognate named enti-
ties between the languages involved in the corpus.
It could be particularly appropriate for news cor-
pus, where named entities play an important role.
In order to compare the results of our approach
with other based on features translation, we also
dealt with this one, as baseline approach. The sys-
tem uses EuroWordNet (Vossen, 1998) to trans-
late the features. We tried different features cate-
gories and combinations of them in order to deter-
mine which ones lead to improve MDC results in
this approach.
In the following section we relate previous work
in the field. In Section 3 we present our approach
for MDC. Section 4 describes the system we com-
pare our approach with, as well as the experiments
and the results. Finally, Section 5 summarizes the
conclusions and the future work.
2 Related Work
MDC is normally applied with parallel (Silva et.
al., 2004) or comparable corpus (Chen and Lin,
2000), (Rauber et. al., 2001), (Lawrence, 2003),
(Steinberger et. al., 2002), (Mathieu et. al, 2004),
(Pouliquen et. al., 2004). In the case of the com-
parable corpora, the documents usually are news
articles.
Considering the approaches based on transla-
tion technology, two different strategies are em-
ployed: (1) translate the whole document to an an-
chor language, and (2) translate some features of
the document to an anchor language.
With regard to the first approach, some authors
use machine translation systems, whereas others
translate the document word by word consulting
a bilingual dictionary. In (Lawrence, 2003), the
author presents several experiments for clustering
a Russian-English multilingual corpus; several of
these experiments are based on using a machine
translation system. Columbia?s Newsblaster sys-
tem (Kirk et al, 2004) clusters news into events,
it categorizes events into broad topic and summa-
rizes multiple articles on each event. In the clus-
tering process non-English documents are trans-
lated using simple dictionary lookup techniques
for translating Japanese and Russian documents,
and the Systran translation system for the other
languages used in the system.
When the solution involves translating only
some features, first it is necessary to select these
features (usually entities, verbs, nouns) and then
translate them with a bilingual dictionary or/and
consulting a parallel corpus.
In (Mathieu et. al, 2004) before the cluster-
ing process, the authors perform a linguistic anal-
ysis which extracts lemmas and recognizes named
entities (location, organization, person, time ex-
pression, numeric expression, product or event);
then, the documents are represented by a set of
terms (keywords or named entity types). In addi-
tion, they use document frequency to select rele-
vant features among the extracted terms. Finally,
the solution uses bilingual dictionaries to translate
the selected features. In (Rauber et. al., 2001)
the authors present a methodology in which docu-
ments are parsed to extract features: all the words
which appear in n documents except the stop-
words. Then, standard machine translation tech-
niques are used to create a monolingual corpus.
After the translation process the documents are au-
tomatically organized into separate clusters using
an un-supervised neural network.
Some approaches first carry out an independent
clustering in each language, that is a monolingual
clustering, and then they find relations among the
obtained clusters generating the multilingual clus-
ters. Others solutions start with a multilingual
clustering to look for relations between the doc-
uments of all the involved languages. This is the
case of (Chen and Lin, 2000), where the authors
propose an architecture of multilingual news sum-
marizer which includes monolingual and multilin-
gual clustering; the multilingual clustering takes
input from the monolingual clusters. The authors
select different type of features depending on the
clustering: for the monolingual clustering they use
only named entities, for the multilingual clustering
they extract verbs besides named entities.
The strategies that use language-independent
representation try to normalize or standardize the
document contents in a language-neutral way; for
example: (1) by mapping text contents to an inde-
pendent knowledge representation, or (2) by rec-
ognizing language independent text features inside
the documents. Both approaches can be employed
isolated or combined.
The first approach involves the use of exist-
ing multilingual linguistic resources, such as the-
saurus, to create a text representation consisting of
a set of thesaurus items. Normally, in a multilin-
gual thesaurus, elements in different languages are
1146
related via language-independent items. So, two
documents written in different languages can be
considered similar if they have similar representa-
tion according to the thesaurus. In some cases, it
is necessary to use the thesaurus in combination
with a machine learning method for mapping cor-
rectly documents onto thesaurus. In (Steinberger
et. al., 2002) the authors present an approach to
calculate the semantic similarity by representing
the document contents in a language independent
way, using the descriptor terms of the multilingual
thesaurus Eurovoc.
The second approach, recognition of language
independent text features, involves the recognition
of elements such as: dates, numbers, and named
entities. In others works, for instance (Silva
et. al., 2004), the authors present a method
based on Relevant Expressions (RE). The RE are
multilingual lexical units of any length automat-
ically extracted from the documents using the
LiPXtractor extractor, a language independent
statistics-based tool. The RE are used as base
features to obtain a reduced set of new features
for the multilingual clustering, but the clusters
obtained are monolingual.
Others works combine recognition of indepen-
dent text features (numbers, dates, names, cog-
nates) with mapping text contents to a thesaurus.
In (Pouliquen et. al., 2004) the cross-lingual
news cluster similarity is based on a linear com-
bination of three types of input: (a) cognates, (b)
automatically detected references of geographical
place names, and (c) the results of a mapping
process onto a multilingual classification system
which maps documents onto the multilingual the-
saurus Eurovoc. In (Steinberger et. al., 2004) it
is proposed to extract language-independent text
features using gazetteers and regular expressions
besides thesaurus and classification systems.
None of the revised works use as unique evi-
dence for multilingual clustering the identification
of cognate named entities between both sides of
the comparable corpora.
3 MDC by Cognate NE Identification
We propose an approach for MDC based only
on cognate NE identification. The NEs cate-
gories that we take into account are: PERSON,
ORGANIZATION, LOCATION, and MISCEL-
LANY. Other numerical categories such as DATE,
TIME or NUMBER are not considered because
we think they are less relevant regarding the con-
tent of the document. In addition, they can lead to
group documents with few content in common.
The process has two main phases: (1) cognate
NE identification and (2) clustering. Both phases
are described in detail in the following sections.
3.1 Cognate NE identification
This phase consists of three steps:
1. Detection and classification of the NEs in
each side of the corpus.
2. Identification of cognates between the NEs of
both sides of the comparable corpus.
3. To work out a statistic of the number of docu-
ments that share cognates of the different NE
categories.
Regarding the first step, it is carried out in each
side of the corpus separately. In our case we used
a corpus with morphosyntactical annotations and
the NEs identified and classified with the FreeLing
tool (Carreras et al, 2004).
In order to identify the cognates between NEs 4
steps are carried out:
? Obtaining two list of NEs, one for each lan-
guage.
? Identification of entity mentions in each lan-
guage. For instance, ?Ernesto Zedillo?,
?Zedillo?, ?Sr. Zedillo? will be considered
as the same entity after this step since they
refer to the same person. This step is only
applied to entities of PERSON category. The
identification of NE mentions, as well as cog-
nate NE, is based on the use of the Leven-
shtein edit-distance function (LD). This mea-
sure is obtained by finding the cheapest way
to transform one string into another. Trans-
formations are the one-step operations of in-
sertion, deletion and substitution. The result
is an integer value that is normalized by the
length of the longest string. In addition, con-
straints regarding the number of words that
the NEs are made up, as well as the order of
the words are applied.
? Identification of cognates between the NEs
of both sides of the comparable corpus. It
is also based on the LD. In addition, also
1147
constraints regarding the number and the or-
der of the words are applied. First, we tried
cognate identification only between NEs of
the same category (PERSON with PERSON,
. . . ) or between any category and MISCEL-
LANY (PERSON with MISCELLANY, . . . ).
Next, with the rest of NEs that have not been
considered as cognate, a next step is applied
without the constraint of being to the same
category or MISCELLANY. As result of this
step a list of corresponding bilingual cog-
nates is obtained.
? The same procedure carried out for obtaining
bilingual cognates is used to obtain two more
lists of cognates, one per language, between
the NEs of the same language.
Finally, a statistic of the number of documents
that share cognates of the different NE categories
is worked out. This information can be used by the
algorithm (or the user) to select the NE category
used as constraint in the clustering steps 1(a) and
2(b).
3.2 Clustering
The algorithm for clustering multilingual docu-
ments based on cognate NEs is of heuristic nature.
It consists of 3 main phases: (1) first clusters cre-
ation, (2) addition of remaining documents to ex-
isting clusters, and (3) final cluster adjustment.
1. First clusters creation. This phase consists of
2 steps.
(a) First, documents in different languages
that have more cognates in common
than a threshold are grouped into the
same cluster. In addition, at least one of
the cognates has to be of a specific cate-
gory (PERSON, LOCATION or ORGA-
NIZATION), and the number of men-
tions has to be similar; a threshold de-
termines the similarity degree. After
this step some documents are assigned
to clusters while the others are free (with
no cluster assigned).
(b) Next, it is tried to assign each free docu-
ment to an existing cluster. This is pos-
sible if there is a document in the cluster
that has more cognates in common with
the free document than a threshold, with
no constraints regarding the NE cate-
gory. If it is not possible, a new clus-
ter is created. This step can also have as
result free documents.
At this point the number of clusters created is
fixed for the next phase.
2. Addition of the rest of the documents to ex-
isting clusters. This phase is carried out in 2
steps.
(a) A document is added to a cluster that
contains a document which has more
cognates in common than a threshold.
(b) Until now, the cognate NEs have been
compared between both sides of the cor-
pus, that is a bilingual comparison. In
this step, the NEs of a language are com-
pared with those of the same language.
This can be described like a monolin-
gual comparison step. The aim is to
group similar documents of the same
language if the bilingual comparison
steps have not been successful. As in
the other cases, a document is added to
a cluster with at least a document of the
same language which has more cognates
in common than a threshold. In addi-
tion, at least one of the cognates have to
be of a specific category (PERSON, LO-
CATION or ORGANIZATION).
3. Final cluster adjustment. Finally, if there are
still free documents, each one is assigned to
the cluster with more cognates in common,
without constraints or threshold. Nonethe-
less, if free documents are left because they
do not have any cognates in common with
those assigned to the existing clusters, new
clusters can be created.
Most of the thresholds can be customized in or-
der to permit and make the experiments easier. In
addition, the parameters customization allows the
adaptation to different type of corpus or content.
For example, in steps 1(a) and 2(b) we enforce at
least on match in a specific NE category. This pa-
rameter can be customized in order to guide the
grouping towards some type of NE. In Section 4.5
the exact values we used are described.
Our approach is an heuristic method that fol-
lowing an agglomerative approach and in an it-
erative way, decides the number of clusters and
1148
locates each document in a cluster; everything is
based in cognate NEs identification. The final
number of clusters depends on the threshold val-
ues.
4 Evaluation
We wanted not only determine whether our ap-
proach was successful for MDC or not, but we also
wanted to compare its results with other approach
based on feature translation. That is why we try
MDC by selecting and translating the features of
the documents.
In this Section, first the MCD by feature transla-
tion is described; next, the corpus, the experiments
and the results are presented.
4.1 MDC by Feature Translation
In this approach we emphasize the feature selec-
tion based on NEs identification and the grammat-
ical category of the words. The selection of fea-
tures we applied is based on previous work (Casil-
las et. al, 2004), in which several document rep-
resentations are tested in order to study which of
them lead to better monolingual clustering results.
We used this MDC approach as baseline method.
The approach we implemented consists of the
following steps:
1. Selection of features (NE, noun, verb, adjec-
tive, ...) and its context (the whole document
or the first paragraph). Normally, the journal-
ist style includes the heart of the news in the
first paragraph; taking this into account we
have experimented with the whole document
and only with the first paragraph.
2. Translation of the features by using Eu-
roWordNet 1.0. We translate English into
Spanish. When more than one sense for a
single word is provided, we disambiguate by
selecting one sense if it appears in the Span-
ish corpus. Since we work with a comparable
corpus, we expect that the correct translation
of a word appears in it.
3. In order to generate the document represen-
tation we use the TF-IDF function to weight
the features.
4. Use of an clustering algorithm. Particu-
larly, we used a partitioning algorithm of the
CLUTO (Karypis, 2002) library for cluster-
ing.
4.2 Corpus
A Comparable Corpus is a collection of simi-
lar texts in different languages or in different va-
rieties of a language. In this work we com-
piled a collection of news written in Spanish and
English belonging to the same period of time.
The news are categorized and come from the
news agency EFE compiled by HERMES project
(http://nlp.uned.es/hermes/index.html). That col-
lection can be considered like a comparable cor-
pus. We have used three subset of that collection.
The first subset, call S1, consists on 65 news, 32
in Spanish and 33 in English; we used it in order
to train the threshold values. The second one, S2,
is composed of 79 Spanish news and 70 English
news, that is 149 news. The third subset, S3, con-
tains 179 news: 93 in Spanish and 86 in English.
In order to test the MDC results we carried out a
manual clustering with each subset. Three persons
read every document and grouped them consider-
ing the content of each one. They judged inde-
pendently and only the identical resultant clusters
were selected. The human clustering solution is
composed of 12 clusters for subset S1, 26 clus-
ters for subset S2, and 33 clusters for S3. All the
clusters are multilingual in the three subsets.
In the experimentation process of our approach
the first subset, S1, was used to train the parame-
ters and threshold values; with the second one and
the third one the best parameters values were ap-
plied.
4.3 Evaluation metric
The quality of the experimentation results are de-
termined by means of an external evaluation mea-
sure, the F-measure (van Rijsbergen, 1974). This
measure compares the human solution with the
system one. The F-measure combines the preci-
sion and recall measures:
F (i, j) = 2?Recall(i, j)? Precision(i, j)(Precision(i, j) +Recall(i, j)) ,
(1)
where Recall(i, j) = nijni , Precision(i, j) =
nij
nj ,
nij is the number of members of cluster human so-
lution i in cluster j, nj is the number of members
of cluster j and ni is the number of members of
cluster human solution i. For all the clusters:
F =
?
i
ni
n max{F (i)} (2)
The closer to 1 the F-measure value the better.
1149
4.4 Experiments and Results with MDC by
Feature Translation
After trying with features of different grammatical
categories and combinations of them, Table 1 and
Table 2 only show the best results of the experi-
ments.
The first column of both tables indicates the
features used in clustering: NOM (nouns), VER
(verbs), ADJ (adjectives), ALL (all the lemmas),
NE (named entities), and 1rst PAR (those of the
first paragraph of the previous categories). The
second column is the F-measure, and the third one
indicates the number of multilingual clusters ob-
tained. Note that the number of total clusters of
each subset is provided to the clustering algorithm.
As can be seen in the tables, the results depend on
the features selected.
4.5 Experiments and Results with MDC by
Cognate NE
The threshold for the LD in order to determine
whether two NEs are cognate or not is 0.2, except
for entities of ORGANIZATION and LOCATION
categories which is 0.3 when they have more than
one word.
Regarding the thresholds of the clustering phase
(Section 3.2), after training the thresholds with the
collection S1 of 65 news articles we have con-
cluded:
? The first step in the clustering phase, 1(a),
performs a good first grouping with thresh-
old relatively high; in this case 6 or 7. That
is, documents in different languages that have
more cognates in common than 6 or 7 are
grouped into the same cluster. In addition,
at least one of the cognates have to be of an
specific category, and the difference between
the number of mentions have to be equal or
less than 2. Of course, these threshold are ap-
plied after checking that there are documents
that meet the requirements. If they do not,
thresholds are reduced. This first step creates
multilingual clusters with high cohesiveness.
? Steps 1(b) and 2(a) lead to good results with
small threshold values: 1 or 2. They are de-
signed to give priority to the addition of doc-
uments to existing clusters. In fact, only step
1(b) can create new clusters.
? Step 2(b) tries to group similar documents of
the same language when the bilingual com-
parison steps could not be able to deal with
them. This step leads to good results with a
threshold value similar to 1(a) step, and with
the same NE category.
On the other hand, regarding the NE category
enforce on match in steps 1(a) and 2(b), we tried
with the two NE categories of cognates shared by
the most number of documents. Particularly, with
S2 and S3 corpus the NE categories of the cog-
nates shared by the most number of documents
was LOCATION followed by PERSON. We ex-
perimented with both categories.
Table 3 and Table 4 show the results of the ap-
plication of the cognate NE approach to subsets
S2 and S3 respectively. The first column of both
tables indicates the thresholds for each step of the
algorithm. Second and third columns show the re-
sults by selecting PERSON category as NE cat-
egory to be shared by at least a cognate in steps
1(a) and 2(b); whereas fourth and fifth columns are
calculated with LOCATION NE category. The re-
sults are quite similar but slightly better with LO-
CATION category, that is the cognate NE category
shared by the most number of documents. Al-
though none of the results got the exact number of
clusters, it is remarkable that the resulting values
are close to the right ones. In fact, no information
about the right number of cluster is provided to the
algorithm.
If we compare the performance of the two ap-
proaches (Table 3 with Table 1 and Table 4 with
Table 2) our approach obtains better results. With
the subset S3 the results of the F-measure of both
approaches are more similar than with the subset
S2, but the F-measure values of our approach are
still slightly better.
To sum up, our approach obtains slightly bet-
ter results that the one based on feature translation
with the same corpora. In addition, the number of
multilingual clusters is closer to the reference so-
lution. We think that it is remarkable that our ap-
proach reaches results that can be comparable with
those obtained by means of features translation.
We will have to test the algorithm with different
corpora (with some monolingual clusters, differ-
ent languages) in order to confirm its performance.
5 Conclusions and Future Work
We have presented a novel approach for Multilin-
gual Document Clustering based only on cognate
1150
Selected Features F-measure Multilin. Clus./Total
NOM, VER 0.8533 21/26
NOM, ADJ 0.8405 21/26
ALL 0.8209 21/26
NE 0.8117 19/26
NOM, VER, ADJ 0.7984 20/26
NOM, VER, ADJ, 1rst PAR 0.7570 21/26
NOM, ADJ, 1rst PAR 0.7515 22/26
ALL, 1rst PAR 0.7473 19/26
NOM, VER, 1rst PAR 0.7371 20/26
Table 1: MDC results with the feature translation approach and subset S2
Selected Features F-measure Multilin. Clus. /Total
NOM, ADJ 0.8291 26/33
ALL 0.8126 27/33
NOM, VER 0.8028 26/33
NE 0.8015 23/33
NOM, VER, ADJ 0.7917 25/33
NOM, ADJ, 1rst PAR 0.7520 28/33
NOM, VER, ADJ, 1rst PAR 0.7484 26/33
ALL, 1rst PAR 0.7288 26/33
NOM, VER, 1rst PAR 0.7200 24/33
Table 2: MDC results with the feature translation approach and subset S3
Thresholds 1(a), 2(b) match on PERSON 1(a), 2(b) match on LOCATION
Steps Results Clusters Results Clusters
1(a) 1(b) 2(a) 2(b) F-measure Multil./Calc./Total F-measure Multil./Calc./Total
6 2 1 5 0.9097 24/24/26 0.9097 24/24/26
6 2 1 6 0.8961 24/24/26 0.8961 24/24/26
6 2 1 7 0.8955 24/24/26 0.8955 24/24/26
6 2 2 5 0.8861 24/24/26 0.8913 24/24/26
7 2 1 5 0.8859 24/24/26 0.8913 24/24/26
6 2 2 4 0.8785 24/24/26 0.8899 24/24/26
6 2 2 6 0.8773 24/24/26 0.8833 24/24/26
6 2 2 7 0.8773 24/24/26 0.8708 24/24/26
Table 3: MDC results with the cognate NE approach and S2 subset
Thresholds 1(a), 2(b) match on PERSON 1(a), 2(b) match on LOCATION
Steps Results Clusters Results Clusters
1(a) 1(b) 2(a) 2(b) F-measure Multil./Calc./Total F-measure Multil./Calc./Total
7 2 1 5 0.8587 30/30/33 0.8621 30/30/33
6 2 1 5 0.8552 30/30/33 0.8552 30/30/33
6 2 1 6 0.8482 30/30/33 0.8483 30/30/33
6 2 1 7 0.8471 30/30/33 0.8470 30/30/33
6 2 2 5 0.8354 30/30/33 0.8393 30/30/33
6 2 2 6 0.8353 30/30/33 0.8474 30/30/33
6 2 2 4 0.8323 30/30/33 0.8474 30/30/33
6 2 2 7 0.8213 30/30/33 0.8134 30/30/33
Table 4: MDC results with the cognate NE approach and S3 subset
1151
named entities identification. One of the main ad-
vantages of this approach is that it does not depend
on multilingual resources such as dictionaries, ma-
chine translation systems, thesaurus or gazetteers.
The only requirement to fulfill is that the lan-
guages involved in the corpus have to permit the
possibility of identifying cognate named entities.
Another advantage of the approach is that it does
not need any information about the right number
of clusters. In fact, the algorithm calculates it by
using the threshold values of the algorithm.
We have tested this approach with a comparable
corpus of news written in English and Spanish, ob-
taining encouraging results. We think that this ap-
proach could be particularly appropriate for news
articles corpus, where named entities play an im-
portant role. Even more, when there is no previous
evidence of the right number of clusters. In addi-
tion, we have compared our approach with other
based on feature translation, resulting that our ap-
proach presents a slightly better performance.
Future work will include the compilation of
more corpora, the incorporation of machine learn-
ing techniques in order to obtain the thresholds
more appropriate for different type of corpus. In
addition, we will study if changing the order of
the bilingual and monolingual comparison steps
the performance varies significantly for different
type of corpus.
Acknowledgements
We wish to thank the anonymous reviewers for
their helpful and instructive comments. This work
has been partially supported by MCyT TIN2005-
08943-C02-02.
References
Benoit Mathieu, Romanic Besancon and Christian
Fluhr. 2004. ?Multilingual document clusters dis-
covery?. RIAO?2004, p. 1-10.
Arantza Casillas, M. Teresa Gonza?lez de Lena and
Raquel Mart??nez. 2004. ?Sampling and Feature
Selection in a Genetic Algorithm for Document
Clustering?. Computational Linguistics and Intel-
ligent Text Processing, CICLing?04. Lecture Notes
in Computer Science, Springer-Verlag, p. 601-612.
Hsin-Hsi Chen and Chuan-Jie Lin. 2000. ?A Multilin-
gual News Summarizer?. Proceedings of 18th Inter-
national Conference on Computational Linguistics,
p. 159-165.
Xavier Carreras, I. Chao, Lluis Padro? and M.
Padro? 2004 ?An Open-Source Suite of Lan-
guage Analyzers?. Proceedings of the 4th In-
ternational Conference on Language Resources
and Evaluation (LREC?04). Lisbon, Portugal.
http://garraf.epsevg.upc.es/freeling/.
Karypis G. 2002. ? CLUTO: A Clustering Toolkit?.
Technical Report: 02-017. University of Minnesota,
Department of Computer Science, Minneapolis, MN
55455.
David Kirk Evans, Judith L. Klavans and Kathleen
McKeown. 2004. ?Columbian Newsblaster: Multi-
lingual News Summarization on the Web?. Proceed-
ings of the Human Language Technology Confer-
ence and the North American Chapter of the Asso-
ciation for Computational Linguistics Annual Meet-
ing, HLT-NAACL?2004.
Lawrence J. Leftin. 2003. ?Newsblaster Russian-
English Clustering Performance Analysis?.
Columbia computer science Technical Reports.
Bruno Pouliquen, Ralf Steinberger, Camelia Ignat,
Emilia Ksper and Irina Temikova. 2004. ?Multi-
lingual and cross-lingual news topic tracking?. Pro-
ceedings of the 20th International Conference on
computational Linguistics, p. 23-27.
Andreas Rauber, Michael Dittenbach and Dieter Merkl.
2001. ?Towards Automatic Content-Based Organi-
zation of Multilingual Digital Libraries: An English,
French, and German View of the Russian Infor-
mation Agency Novosti News?. Third All-Russian
Conference Digital Libraries: Advanced Methods
and Technologies, Digital Collections Petrozavodsk,
RCDI?2001.
van Rijsbergen, C.J. 1974. ?Foundations of evalua-
tion?. Journal of Documentation, 30 (1974), p. 365-
373.
Joaquin Silva, J. Mexia, Carlos Coelho and Gabriel
Lopes. 2004. ?A Statistical Approach for Multi-
lingual Document Clustering and Topic Extraction
form Clusters?. Pliska Studia Mathematica Bulgar-
ica, v.16,p. 207-228.
Ralf Steinberger, Bruno Pouliquen, and Johan Scheer.
2002. ?Cross-Lingual Document Similarity Cal-
culation Using the Multilingual Thesaurus EU-
ROVOC?. Computational Linguistics and Intelli-
gent Text Processing, CICling?02. Lecture Notes in
Computer Science, Springer-Verlag, p. 415-424.
Ralf Steinberger, Bruno Pouliquen, and Camelia Ignat.
2004. ?Exploiting multilingual nomenclatures and
language-independent text features as an interlingua
for cross-lingual text analysis applications?. Slove-
nian Language Technology Conference. Information
Society, SLTC 2004.
Vossen, P. 1998. ?Introduction to EuroWordNet?.
Computers and the Humanities Special Issue on Eu-
roWordNet.
1152
Proceedings of the NAACL HLT Workshop on Semi-supervised Learning for Natural Language Processing, pages 28?36,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
Is Unlabeled Data Suitable for Multiclass SVM-based Web Page
Classification?
Arkaitz Zubiaga, V??ctor Fresno, Raquel Mart??nez
NLP & IR Group at UNED
Lenguajes y Sistemas Informa?ticos
E.T.S.I. Informa?tica, UNED
{azubiaga, vfresno, raquel}@lsi.uned.es
Abstract
Support Vector Machines present an interest-
ing and effective approach to solve automated
classification tasks. Although it only han-
dles binary and supervised problems by na-
ture, it has been transformed into multiclass
and semi-supervised approaches in several
works. A previous study on supervised and
semi-supervised SVM classification over bi-
nary taxonomies showed how the latter clearly
outperforms the former, proving the suitability
of unlabeled data for the learning phase in this
kind of tasks. However, the suitability of un-
labeled data for multiclass tasks using SVM
has never been tested before. In this work,
we present a study on whether unlabeled data
could improve results for multiclass web page
classification tasks using Support Vector Ma-
chines. As a conclusion, we encourage to rely
only on labeled data, both for improving (or at
least equaling) performance and for reducing
the computational cost.
1 Introduction
The amount of web documents is increasing in a
very fast way in the last years, what makes more
and more complicated its organization. For this rea-
son, web page classification has gained importance
as a task to ease and improve information access.
Web page classification can be defined as the task
of labeling and organizing web documents within a
set of predefined categories. In this work, we focus
on web page classification based on Support Vec-
tor Machines (SVM, (Joachims, 1998)). This kind
of classification tasks rely on a previously labeled
training set of documents, with which the classi-
fier acquires the required ability to classify new un-
known documents.
Different settings can be distinguished for web
page classification problems. On the one hand, at-
tending to the learning technique the system bases
on, it may be supervised, with all the training docu-
ments previously labeled, or semi-supervised, where
unlabeled documents are also taken into account
during the learning phase. On the other hand, attend-
ing to the number of classes, the classification may
be binary, where only two possible categories can
be assigned to each document, or multiclass, where
three or more categories can be set. The former is
commonly used for filtering systems, whereas the
latter is necessary for bigger taxonomies, e.g. topi-
cal classification.
Although multiple studies have been made for
text classification, its application to the web page
classification area remains without enough attention
(Qi and Davison, 2007). Analyzing the nature of
a web page classification task, we can consider it
to be, generally, multiclass problems, where it is
usual to find numerous classes. In the same way,
if we take into account that the number of available
labeled documents is tiny compared to the size of
the Web, this task becomes semi-supervised besides
multiclass.
However, the original SVM algorithm supports
neither semi-supervised learning nor multiclass tax-
onomies, due to its dichotomic and supervised na-
ture. To solve this issue, different studies for
both multiclass SVM and semi-supervised SVM ap-
proaches have been proposed, but a little effort has
28
been invested in the combination of them.
(Joachims, 1999) compares supervised and semi-
supervised approaches for binary tasks using SVM.
It shows encouraging results for the transductive
semi-supervised approach, clearly improving the su-
pervised, and so he proved unlabeled data to be
suitable to optimize binary SVM classifiers? results.
On the other hand, the few works presented for
semi-supervised multiclass SVM classification do
not provide clear information on whether the unla-
beled data improves the classification results in com-
parison with the only use of labeled data.
In this work, we performed an experiment among
different SVM-based multiclass approaches, both
supervised and semi-supervised. The experiments
were focused on web page classification, and
were carried out over three benchmark datasets:
BankSearch, WebKB and Yahoo! Science. Using
the results of the comparison, we analyze and study
the suitability of unlabeled data for multiclass SVM
classification tasks. We discuss these results and
evaluate whether it is worthy to rely on a semi-
supervised SVM approach to conduct this kind of
tasks.
The remainder of this document is organized as
follows. Next, in section 2, we briefly explain how
SVM classifiers work for binary classifications, both
for a supervised and a semi-supervised view. In sec-
tion 3, we continue with the adaptation of SVM to
multiclass environments, and show what has been
done in the literature. Section 4 presents the details
of the experiments carried out in this work, aim at
evaluating the suitability of unlabeled data for mul-
ticlass SVM classification. In section 5 we show and
discuss the results of the experiments. Finally, in
section 6, we conclude with our thoughts and future
work.
2 Binary SVM
In the last decade, SVM has become one of the most
studied techniques for text classification, due to the
positive results it has shown. This technique uses the
vector space model for the documents? representa-
tion, and assumes that documents in the same class
should fall into separable spaces of the representa-
tion. Upon this, it looks for a hyperplane that sepa-
rates the classes; therefore, this hyperplane should
maximize the distance between it and the nearest
documents, what is called the margin. The following
function is used to define the hyperplane (see Figure
1):
f(x) = w ? x+ b
Figure 1: An example of binary SVM classification, sep-
arating two classes (black dots from white dots)
In order to resolve this function, all the possible
values should be considered and, after that, the val-
ues of w and b that maximize the margin should be
selected. This would be computationally expensive,
so the following equivalent function is used to relax
it (Boser et al , 1992) (Cortes and Vapnik, 1995):
min
[1
2 ||w||
2 + C
l?
i=1
?di
]
Subject to: yi(w ? xi + b) ? 1? ?i, ?i ? 0
where C is the penalty parameter, ?i is an stack
variable for the ith document, and l is the number of
labeled documents.
This function can only resolve linearly separable
problems, thus the use of a kernel function is com-
monly required for the redimension of the space; in
this manner, the new space will be linearly separa-
ble. After that, the redimension is undone, so the
found hyperplane will be transformed to the original
space, respecting the classification function. Best-
known kernel functions include linear, polynomial,
radial basis function (RBF) and sigmoid, among oth-
ers. Different kernel functions? performance has
been studied in (Scho?lkopf and Smola, 1999) and
(Kivinen et al, 2002).
29
Note that the function above can only resolve bi-
nary and supervised problems, so different variants
are necessary to perform semi-supervised or multi-
class tasks.
2.1 Semi-supervised Learning for SVM
(S3VM)
Semi-supervised learning approaches differ in the
learning phase. As opposed to supervised ap-
proaches, unlabeled data is used during the learn-
ing phase, and so classifier?s predictions over them
is also included as labeled data to learn. The fact of
taking into account unlabeled data to learn can im-
prove the classification done by supervised methods,
specially when its predictions provide new useful in-
formation, as shown in figure 2. However, the noise
added by erroneus predictions can make worse the
learning phase and, therefore, its final performance.
This makes interesting the study on whether relying
on semi-supervised approaches is suitable for each
kind of task.
Semi-supervised learning for SVM, also known
as S3VM, was first introduced by (Joachims, 1999)
in a transductive way, by modifying the original
SVM function. To do that, he proposed to add an
additional term to the optimization function:
min
?
?12 ? ||?||
2 + C ?
l?
i=1
?di + C? ?
u?
j=1
??dj
?
?
where u is the number of unlabeled data.
Nevertheless, the adaptation of SVM to semi-
supervised learning significantly increases its com-
putational cost, due to the non-convex nature of the
resulting function, and so obtaining the minimum
value is even more complicated. In order to relax
the function, convex optimization techniques such
as semi-definite programming are commonly used
(Xu et al , 2007), where minimizing the function
gets much easier.
By means of this approach, (Joachims, 1999)
demonstrated a large performance gap between the
original supervised SVM and his semi-supervised
proposal, in favour of the latter one. He showed
that for binary classification tasks, the smaller is
the training set size, the larger gets the difference
among these two approaches. Although he worked
Figure 2: SVM vs S3VM, where white balls are unla-
beled documents
with multiclass datasets, he splitted the problems
into smaller binary ones, and so he did not demon-
strate whether the same performance gap occurs for
multiclass classification. This paper tries to cover
this issue. (Chapelle et al, 2008) present a compre-
hensive study on S3VM approaches.
3 Multiclass SVM
Due to the dichotomic nature of SVM, it came up
the need to implement new methods to solve multi-
class problems, where more than two classes must
be considered. Different approaches have been pro-
posed to achieve this. On the one hand, as a direct
approach, (Weston, 1999) proposed modifying the
optimization function getting into account all the k
classes at once:
min
?
?12
k?
m=1
||wm||2 + C
l?
i=1
?
m6=yi
?mi
?
?
Subject to:
wyi ? xi + byi ? wm ? xi + bm + 2? ?mi , ?mi ? 0
On the other hand, the original binary SVM clas-
sifier has usually been combined to obtain a multi-
class solution. As combinations of binary SVM clas-
sifiers, two different approaches to k-class classifiers
can be emphasized (Hsu and Lin, 2002):
? one-against-all constructs k classifiers defining
that many hyperplanes; each of them separates
the class i from the rest k-1. For instance, for
a problem with 4 classes, 1 vs 2-3-4, 2 vs 1-3-
4, 3 vs 1-2-4 and 4 vs 1-2-3 classifiers would
30
be created. New documents will be categorized
in the class of the classifier that maximizes the
margin: C?i = argmaxi=1,...,k(wix + bi). As
the number of classes increases, the amount of
classifiers will increase linearly.
? one-against-one constructs k(k?1)2 classifiers,one for each possible category pair. For in-
stance, for a problem with 4 classes, 1 vs 2,
1 vs 3, 1 vs 4, 2 vs 3, 2 vs 4 and 3 vs 4 clas-
sifiers would be created. After that, it classi-
fies each new document by using all the clas-
sifiers, where a vote is added for the winning
class over each classifier; the method will pro-
pose the class with more votes as the result. As
the number of classes increases, the amount of
classifiers will increase in an exponential way,
and so the problem could became very expen-
sive for large taxonomies.
Both (Weston, 1999) and (Hsu and Lin, 2002)
compare the direct multiclass approach to the one-
against-one and one-against-all binary classifier
combining approaches. They agree concluding that
the direct approach does not outperform the results
by one-against-one nor one-against-all, although
it considerably reduces the computational cost be-
cause the number of support vector machines it
constructs is lower. Among the binary combin-
ing approaches, they show the performance of one-
against-one to be superior to one-against-all.
Although these approaches have been widely
used in supervised learning environments, they have
scarcely been applied to semi-supervised learning.
Because of this, we believe the study on its appli-
cability and performance for this type of problems
could be interesting.
3.1 Multiclass S3VM
When the taxonomy is defined by more than two
classes and the number of previously labeled doc-
uments is very small, the combination of both mul-
ticlass and semi-supervised approaches could be re-
quired. That is, a multiclass S3VM approach. The
usual web page classification problem meets with
these characteristics, since more than two classes
are usually needed, and the tiny amount of labeled
documents requires the use of unlabeled data for the
learning phase.
Actually, there are a few works focused on trans-
forming SVM into a semi-supervised and multiclass
approach. As a direct approach, a proposal by (Ya-
jima and Kuo, 2006) can be found. They modify the
function for multiclass SVM classification and get it
usable for semi-supervised tasks. The resulting op-
timization function is as follows:
min 12
h?
i=1
?iTK?1?i
+C
l?
j=1
?
i6=yj
max{0, 1? (?yjj ? ?ij)}2
where ? represents the product of a vector of vari-
ables and a kernel matrix defined by the author.
On the other hand, some other works are based on
different approaches to achieve a multiclass S3VM
classifier.
(Qi et al, 2004) use Fuzzy C-Means (FCM) to
predict labels for unlabeled documents. After that,
multiclass SVM is used to learn with the augmented
training set, classifying the test set. (Xu y Schu-
urmans, 2005) rely on a clustering-based approach
to label the unlabeled data. Afterwards, they ap-
ply a multiclass SVM classifier to the fully labeled
training set. (Chapelle et al, 2006) present a direct
multiclass S3VM approach by using the Continua-
tion Method. On the other hand, this is the only
work, to the best of our knowledge, that has tested
the one-against-all and one-against-one approaches
in a semi-supervised environment. They apply these
methods to some news datasets, for which they get
low performance. Additionally, they show that one-
against-one is not sufficient for real-world multi-
class semi-supervised learning, since the unlabeled
data cannot be restricted to the two classes under
consideration.
It is noteworthy that most of the above works
only presented their approaches and compared them
to other semi-supervised classifying methods, such
as Expectation-Maximization (EM) or Naive Bayes.
As an exception, (Chapelle et al, 2006) compared
a semi-supervised and a supervised SVM approach,
but only over image datasets. Against this, we felt
the need to evaluate and compare multiclass SVM
and multiclass S3VM approaches, for the sake of
discovering whether learning with unlabeled web
31
documents is helpful for multiclass problems when
using SVM as a classifier.
4 Multiclass SVM versus Multiclass S3VM
The main goal of this work is to evaluate the real
contribution of unlabeled data for multiclass SVM-
based web page classification tasks. There are a few
works using semi-supervised multiclass SVM clas-
sifiers, but nobody has demonstrated it improves su-
pervised SVM classifier?s performance. Next, we
detail the experiments we carried out to clear up any
doubts and to ensure which is better for multiclass
SVM-based web page classifications.
4.1 Approaches
In order to evaluate and compare multiclass SVM
and multiclass S3VM, we decided to use three differ-
ent but equivalent approaches for each view, super-
vised and semi-supervised. For further information
on these approaches, see section 3. We add a suffix,
-SVM or -S3VM, to the names of the approaches, to
differentiate whether they are based in a supervised
or a semi-supervised algorithm.
On the part of the semi-supervised view, the fol-
lowing three approaches were selected:
? 2-steps-SVM: we called 2-steps-SVM to the
technique based on the direct multiclass su-
pervised approach exposed in section 3. This
method works, on its first step, with the train-
ing collection, learning with the labeled docu-
ments and predicting the unlabeled ones; after
that, the latter documents are labeled based on
the generated predictions. On the second step,
now with a fully labeled training set, the usual
supervised classification process is done, learn-
ing with the training documents and predicting
the documents in the test set.
This approach is somehow similar to those pro-
posed by (Qi et al, 2004) and (Xu y Schu-
urmans, 2005). Nonetheless, the 2-steps-SVM
approach uses the same method for both the
first and second steps. A supervised multiclass
SVM is used to increase the labeled set and, af-
ter that, to classify the test set.
? one-against-all-S3VM: the one-against-all ap-
proach has not sufficiently been tested for semi-
supervised environments, and seems interest-
ing to evaluate its performance.
? one-against-one-S3VM: the one-against-one
does not seem to be suitable for semi-
supervised environments, since the classifier is
not able to ignore the inadecuate unlabeled doc-
uments for each 1-vs-1 binary task, as stated by
(Chapelle et al, 2006). Anyway, since it has
scarcely been tested, we also consider this ap-
proach.
On the other hand, the approaches selected for
the supervised view were these: (1) 1-step-SVM;
(2) one-against-all-SVM, and (3) one-against-one-
SVM.
The three approaches mentioned above are anal-
ogous to the semi-supervised approaches, 2-steps-
SVM, one-against-all-S3VM and one-against-one-
S3VM, respectively. They differ in the learning
phase: unlike the semi-supervised approaches, these
three supervised approaches only rely on the labeled
documents for the learning task, but after that they
classify the same test documents. These approaches
allow to evaluate whether the unlabeled documents
are contributing in a positive or negative way in the
learning phase.
4.2 Datasets
For these experiments we have used three web page
benchmark datasets previously used for classifica-
tion tasks:
? BankSearch (Sinka and Corne, 2002), a col-
lection of 11,000 web pages over 11 classes,
with very different topics: commercial banks,
building societies, insurance agencies, java, c,
visual basic, astronomy, biology, soccer, mo-
torsports and sports. We removed the category
sports, since it includes both soccer and motor-
sports in it, as a parent category. This results
10,000 web pages over 10 categories. 4,000 in-
stances were assigned to the training set, while
the other 6,000 were left on the test set.
? WebKB1, with a total of 4,518 documents of
4 universities, and classified into 7 classes
1http://www.cs.cmu.edu/afs/cs.cmu.edu/project/theo-
20/www/data/
32
(student, faculty, personal, department, course,
project and other). The class named other was
removed due to its ambiguity, and so we finally
got 6 classes. 2,000 instances fell into the train-
ing set, and 2,518 to the test set.
? Yahoo! Science (Tan, 2002), with 788 scien-
tific documents, classified into 6 classes (agri-
culture, biology, earth science, math, chemistry
and others). We selected 200 documents for the
training set, and 588 for the test set.
Within the training set, for each dataset, multiple
versions were created, modifying the number of la-
beled documents, while the rest were left unlabeled.
Thus, the size of labeled subset within the training
set changes, ranging from 50 web documents to the
whole training set.
4.3 Document Representation
SVM requires a vectorial representation of the docu-
ments as an input for the classifier, both for train and
test phases. To obtain this vectorial representation,
we first converted the original html files into plain
text files, removing all the html tags. After that, we
removed the noisy tokens, such as URLs, email ad-
dresses or some stopwords. For these edited docu-
ments, the tf-idf term weighting function was used
to define the values for the uniterms found on the
texts. As the term dimensionality became too large,
we then removed the least-frequent terms by its doc-
ument frequency; terms appearing in less than 0.5%
of the documents were removed for the representa-
tion. The remaining uniterms define the vector space
dimensions. That derived term vectors with 8285 di-
mensions for BankSearch dataset, 3115 for WebKB
and 8437 for Yahoo! Science.
4.4 Implementation
To carry out our experiments, we based on freely
available and already tested and experimented soft-
ware. Different SVM classifiers were needed to im-
plement the methods described in section 4.1.
SVMlight2 was used to work with binary semi-
supervised classifiers for the one-against-all-S3VM
and one-against-one-S3VM approaches. In the same
way, we implemented their supervised versions,
2http://svmlight.joachims.org
one-against-all-SVM and one-against-one-SVM, in
order to evaluate the contribution of unlabeled data.
To achieve the supervised approaches, we ignored
the unlabeled data during the training phase and, af-
ter that, tested with the same test set used for semi-
supervised approaches. The default settings using
a polynomial kernel were selected for the experi-
ments.
SVMmulticlass3 was used to implement the 2-
steps-SVM approach, by using it two times. Firstly,
to train the labeled data and classify unlabeled data.
After that, to train with the whole training set labeled
with classifier?s predictions, and to test with the test
set. In the same way, the 1-step-SVM method was
implemented by ignoring unlabeled data and train-
ing only the labeled data. This method allows to
evaluate the contribution of unlabeled data for the
2-steps-SVM method.
4.5 Evaluation Measures
For the evaluation of the experiments we used the
accuracy to measure the performance, since it has
been frequently used for text classification prob-
lems, specially for multiclass tasks. The accuracy
offers the percent of the correct predictions for the
whole test set. We have considered the same weight
for all the correct guesses for any class. A correct
prediction in any of the classes has the same value,
thus no weighting exists.
On the other hand, an averaged accuracy evalu-
ation is also possible for the binary combining ap-
proaches. An averaged accuracy makes possible to
evaluate the results by each binary classifier, and
provides an averaged value for the whole binary
classifier set. It is worth to note that these values do
not provide any information for the evaluation of the
combined multiclass results, but only for evaluating
each binary classifier before combining them.
5 Results and Discussion
Next, we show and discuss the results of our experi-
ments. It is remarkable that both one-against-one-
SVM and one-against-one-S3VM approaches were
very inferior to the rest, and so we decided not to plot
them in order to maintain graphs? clarity. Hence,
figures 3, 4 and 5 show the results in accordance
3http://www.cs.cornell.edu/People/tj/svm light/svm multiclass.html
33
with the labeled subset size for the 2-steps-SVM, 1-
step-SVM, one-against-all-S3VM and one-against-
all-SVM approaches within our experiments. For
the results to be more representative, nine execu-
tions were done for each subset, obtaining the mean
value. These nine executions vary on the labeled
subset within the training set.
The fact that one-against-one-S3VM has been the
worst approach for our experiments confirms that
the noise added by the unlabeled documents within
each 1-vs-1 binary classification task is harmful to
the learning phase, and it is not corrected when
merging all the binary tasks.
The averaged accuracy for the combined bi-
nary classifiers allows to compare the one-against-
one and one-against-all views. The averaged ac-
curacy for one-against-one-S3VM shows very low
performance (about 60% in most cases), whereas
the same value for one-against-all-S3VM is much
higher (about 90% in most cases). This is obvi-
ous to happen for the one-against-all view, since
it is much easier to predict documents not pertain-
ing to the class under consideration for each 1-vs-
all binary classifier. Although each binary classifier
gets about 90% accuracy for the one-against-one-
S3VM approach, this value falls considerably when
combining them to get the multiclass result. This
shows the additional difficulty for multiclass prob-
lems compared to binary ones. Hence, the difficulty
to correctly predict unlabeled data increases for mul-
ticlass tasks, and it is more likely to add noise during
the learning phase.
Figure 3: Results for BankSearch dataset
Figure 4: Results for WebKB dataset
Figure 5: Results for Yahoo! Science dataset
For all the datasets we worked with, there is a
noticeable performance gap between direct multi-
class and binary combining approaches. Both 2-
steps-SVM and 1-step-SVM are always on the top
of the graphs, and one-against-all-S3VM and one-
against-all-SVM approaches are so far from catch-
ing up with their results, except for WebKB dataset,
where the gap is not so noticeable. This seems en-
couraging, since considering less support vectors in
a direct multiclass approach reduces the computa-
tional cost and improves the final results.
Comparing the two analogous approaches among
them, different conclusions could be extracted.
On the one hand, one-against-all-S3VM shows
slightly better results than one-against-all-SVM, and
so considering unlabeled documents seems to be
34
favourable for the one-against-all view. On the other
hand, the direct multiclass view shows varying re-
sults. Both 2-steps-SVM and 1-step-SVM show very
similar results for BankSearch and Yahoo! Science
datasets, but superior for 1-step-SVM over the We-
bKB dataset. As a conclusion of this, ignoring un-
labeled documents by means of the 1-step-SVM ap-
proach seems to be advisable, since it reduces the
computation cost, obtaining at least the same results
than the semi-supervised 2-steps-SVM.
Although their results are so poor, as we said
above, the supervised approach wins for the one-
against-one view; this confirms, again, that the one-
against-one view is not an adecuate view to be ap-
plied in a semi-supervised environment, due to the
noise existing during the learning phase.
When analyzing the performance gaps between
the analogous approaches, a general conclusion can
be extracted: the smaller is the labeled subset the
bigger is the performance gap, except for the Ya-
hoo! Science dataset. Comparing the two best
approaches, 1-step-SVM and 2-steps-SVM, the per-
formance gap increases when the number of la-
beled documents decrease for BankSearch; for this
dataset, the accuracy by 1-step-SVM is 0.92 times
the one by 2-steps-SVM when the number of labeled
documents is only 50, but this proportion goes to
0.99 with 500 labeled documents. This reflects how
the contribution of unlabeled data decreases while
the labeled set increases. For WebKB, the perfor-
mance gap is in favour of 1-step-SVM, and varies
between 1.01 and 1.05 times 2-steps-SVM method?s
accuracy, even with only 50 labeled documents.
Again, increasing the labeled set negatively affects
semi-supervised algorithm?s performance. Last, for
Yahoo! Science, the performance gap among these
two approaches is not considerable, since their re-
sults are very similar.
Our conjecture for the performance difference be-
tween 1-step-SVM and 2-steps-SVM for the three
datasets is the nature of the classes. The accuracy
by semi-supervised 2-steps-SVM is slightly higher
for BankSearch and Yahoo! Science, where the
classes are quite heterogeneous. On the other hand,
the accuracy by supervised 1-step-SVM is clearly
higher for WebKB, where all the classes are an aca-
demic topic, and so more homogeneous. The semi-
supervised classifiers show a major problem for pre-
dicting the unlabeled documents when the collection
is more homogeneous, and so more difficult to differ
between classes.
In summary, the main idea is that unlabeled doc-
uments do not seem to contribute as they would for
multiclass tasks using SVM. Within the approaches
we tested, the supervised 1-step-SVM approach
shows the best (or very similar to the best in some
cases) results in accuracy and, taking into account
it is the least-expensive approach, we strongly en-
courage to use this approach to solve multiclass web
page classification tasks, mainly when the classes
under consideration are homogeneous.
6 Conclusions and Outlook
We have studied and analyzed the contribution of
considering unlabeled data during the learning phase
for multiclass web page classification tasks using
SVM. Our results show that ignoring unlabeled doc-
ument to learn reduces computational cost and, ad-
ditionaly, obtains similar or slightly worse accuracy
values for heterogeneus taxonomies, but higher for
homogeneous ones. Therefore we show that, unlike
for binary cases, as was shown by (Joachims, 1999),
a supervised view outperforms a semi-supervised
one for multiclass environments. Our thought is that
predicting unlabeled documents? class is much more
difficult when the number of classes increases, and
so, the mistaken labeled documents are harmful for
classifier?s learning phase.
As a future work, a direct semi-supervised multi-
class approach, such as those proposed by (Yajima
and Kuo, 2006) and (Chapelle et al, 2006), should
also be considered, as well as setting the classifier
with different parameters or kernels. Balancing the
weight of previously and newly labeled data could
also be interesting to improve semi-supervised ap-
proaches? results.
Acknowledgments
We wish to thank the anonymous reviewers for their
helpful and instructive comments. This work has
been supported by the Research Network MAVIR
(S-0505/TIC-0267), the Regional Ministry of Ed-
ucation of the Community of Madrid, and by the
Spanish Ministry of Science and Innovation project
QEAVis-Catiex (TIN2007-67581-C02-01).
35
References
B. E. Boser, I. Guyon and V. Vapnik. 1992. A Training
Algorithm for Optimal Margin Classifiers. Proceed-
ings of the 5th Annual Workshop on computational
Learning Theory.
C. Campbell. 2000. Algorithmic Approaches to Training
Support Vector Machines: A Survey Proceedings of
ESANN?2000, European Symposium on Artificial Neu-
ral Networks.
O. Chapelle, M. Chi y A. Zien 2006. A Continuation
Method for Semi-supervised SVMs. Proceedings of
ICML?06, the 23rd International Conference on Ma-
chine Learning.
O. Chapelle, V. Sindhwani, S. Keerthi 2008. Optimiza-
tion Techniques for Semi-Supervised Support Vector
Machines. J. Mach. Learn. Res..
C. Cortes and V. Vapnik. 1995. Support Vector Network.
Machine Learning.
C.-H. Hsu and C.-J. Lin. 2002. A Comparison of Meth-
ods for Multiclass Support Vector Machines. IEEE
Transactions on Neural Networks.
T. Joachims. 1998. Text Categorization with Support
Vector Machines: Learning with many Relevant Fea-
tures. Proceedings of ECML98, 10th European Con-
ference on Machine Learning.
T. Joachims. 1999. Transductive Inference for Text
Classification Using Support Vector Machines. Pro-
ceedings of ICML99, 16th International Conference
on Machine Learning.
J. Kivinen and E.J. Smola and R.C. Williamson. 2002.
Learning with Kernels.
T. Mitchell. 1997. Machine Learning. McGraw Hill.
H.-N. Qi, J.-G. Yang, Y.-W. Zhong y C. Deng 2004.
Multi-class SVM Based Remote Sensing Image
Classification and its Semi-supervised Improvement
Scheme. Proceedings of the 3rd ICMLC.
X. Qi and B.D. Davison. 2007. Web Page Classification:
Features and Algorithms. Technical Report LU-CSE-
07-010.
B. Scho?lkopf and A. Smola. 1999. Advances in Kernel
Methods: Support Vector Learning. MIT Press.
F. Sebastiani. 2002. Machine Learning in Automated
Text Categorization. ACM Computing Surveys, pp. 1-
47.
M.P. Sinka and D.W. Corne. 2002. A New Benchmark
Dataset for Web Document Clustering. Soft Comput-
ing Systems.
C.M. Tan, Y.F. Wang and C.D. Lee. 2002. The Use of
Bigrams to Enhance Text Categorization. Information
Processing and Management.
J. Weston and C. Watkins. 1999. Multi-class Support
Vector Machines. Proceedings of ESAAN, the Euro-
pean Symposium on Artificial Neural Networks.
L. Xu y D. Schuurmans. 2005. Unsupervised and Semi-
supervised Multiclass Support Vector Machines. Pro-
ceedings of AAAI?05, the 20th National Conference on
Artificial Intelligence.
Z. Xu, R. Jin, J. Zhu, I. King and M. R. Lyu. 2007. Ef-
ficient Convex Optimization for Transductive Support
Vector Machine. Advances in Neural Information Pro-
cessing Systems.
Y. Yajima and T.-F. Kuo. 2006. Optimization Ap-
proaches for Semi-Supervised Multiclass Classifica-
tion. Proceedings of ICDM?06 Workshops, the 6th In-
ternational Conference on Data Mining.
36
