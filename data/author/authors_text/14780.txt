Proceedings of the ACL-HLT 2011 System Demonstrations, pages 74?79,
Portland, Oregon, USA, 21 June 2011. c?2011 Association for Computational Linguistics
Wikulu: An Extensible Architecture for Integrating Natural Language
Processing Techniques with Wikis
Daniel Ba?r, Nicolai Erbs, Torsten Zesch, and Iryna Gurevych
Ubiquitous Knowledge Processing Lab
Computer Science Department, Technische Universita?t Darmstadt
Hochschulstrasse 10, D-64289 Darmstadt, Germany
www.ukp.tu-darmstadt.de
Abstract
We present Wikulu1, a system focusing on
supporting wiki users with their everyday
tasks by means of an intelligent interface.
Wikulu is implemented as an extensible archi-
tecture which transparently integrates natural
language processing (NLP) techniques with
wikis. It is designed to be deployed with any
wiki platform, and the current prototype inte-
grates a wide range of NLP algorithms such
as keyphrase extraction, link discovery, text
segmentation, summarization, or text similar-
ity. Additionally, we show how Wikulu can
be applied for visually analyzing the results
of NLP algorithms, educational purposes, and
enabling semantic wikis.
1 Introduction
Wikis are web-based, collaborative content author-
ing systems (Leuf and Cunningham, 2001). As they
offer fast and simple means for adding and editing
content, they are used for various purposes such as
creating encyclopedias (e.g. Wikipedia2), construct-
ing dictionaries (e.g. Wiktionary3), or hosting online
communities (e.g. ACLWiki4). However, as wikis do
not enforce their users to structure pages or add com-
plementary metadata, wikis often end up as a mass
of unmanageable pages with meaningless page titles
and no usable link structure (Buffa, 2006).
To solve this issue, we present the Wikulu sys-
tem which uses natural language processing to sup-
port wiki users with their typical tasks of adding,
1Portmanteau of the Hawaiian terms wiki (?fast?) and kukulu
(?to organize?)
2http://www.wikipedia.org
3http://www.wiktionary.org
4http://aclweb.org/aclwiki
organizing, and finding content. For example,
Wikulu supports users with reading longer texts by
highlighting keyphrases using keyphrase extraction
methods such as TextRank (Mihalcea and Tarau,
2004). Support integrated in Wikulu also includes
text segmentation to segment long pages, text simi-
larity for detecting potential duplicates, or text sum-
marization to facilitate reading of lengthy pages.
Generally, Wikulu allows to integrate any NLP com-
ponent which conforms to the standards of Apache
UIMA (Ferrucci and Lally, 2004).
Wikulu is designed to integrate seamlessly with
any wiki. Our system is implemented as an HTTP
proxy server which intercepts the communication
between the web browser and the underlying wiki
engine. No further modifications to the original wiki
installation are necessary. Currently, our system pro-
totype contains adaptors for two widely used wiki
engines: MediaWiki5 and TWiki6. Adaptors for other
wiki engines can be added with minimal effort. Gen-
erally, Wikulu could also be applied to any web-
based system other than wikis with only slight mod-
ifications to its architecture.
In Figure 1, we show the integration of Wikulu
with Wikipedia.7 The additional user interface com-
ponents are integrated into the default toolbar (high-
lighted by a red box in the screenshot). In this ex-
ample, the user has requested keyphrase highlight-
ing in order to quickly get an idea about the main
content of the wiki article. Wikulu then invokes the
5http://mediawiki.org (e.g. used by Wikipedia)
6http://twiki.org (often used for corporate wikis)
7As screenshots only provide a limited overview of
Wikulu?s capabilities, we refer the reader to a screencast:
http://www.ukp.tu-darmstadt.de/research/
projects/wikulu
74
Figure 1: Integration of Wikulu with Wikipedia. The aug-
mented toolbar (red box) and the results of a keyphrase
extraction algorithm (yellow text spans) are highlighted.
corresponding NLP component, and highlights the
returned keyphrases in the article. In the next sec-
tion, we give a more detailed overview of the differ-
ent types of support provided by Wikulu.
2 Supporting Wiki Users by Means of NLP
In this section, we present the different types of
NLP-enabled support provided by Wikulu.
Detecting Duplicates Whenever users add new
content to a wiki there is the danger of duplicating
already contained information. In order to avoid du-
plication, users would need comprehensive knowl-
edge of what content is already present in the wiki,
which is almost impossible for large wikis like
Wikipedia. Wikulu helps to detect potential du-
plicates by computing the text similarity between
newly added content and each existing wiki page.
If a potential duplicate is detected, the user is noti-
fied and may decide to augment the duplicate page
instead of adding a new one. Wikulu integrates text
similarity measures such as Explicit Semantic Anal-
ysis (Gabrilovich and Markovitch, 2007) and Latent
Semantic Analysis (Landauer et al, 1998).
Suggesting Links While many wiki users read-
ily add textual contents to wikis, they often re-
strain from also adding links to related pages. How-
ever, links in wikis are crucial as they allow users
to quickly navigate from one page to another, or
browse through the wiki. Therefore, it may be rea-
sonable to augment a page about the topic sentiment
Figure 2: Automatic discovery of links to other wiki ar-
ticles. Suitable text phrases to place a link on are high-
lighted in green.
analysis by a link to a page providing related in-
formation such as evaluation datasets. Wikulu sup-
ports users in this tedious task by automatically sug-
gesting links. Link suggestion thereby is a two-step
process: (a) first, suitable text phrases are extracted
which might be worth to place a link on (see Fig-
ure 2), and (b) for each phrase, related pages are
ranked by comparing their relevance to the current
page, and then presented to the user. The user may
thus decide whether she wants to use a detected
phrase as a link or not, and if so, which other wiki
page to link this phrase to. Wikulu currently inte-
grates link suggestion algorithms by Geva (2007)
and Itakura and Clarke (2007).
Semantic Searching The capabilities of a wiki?s
built-in search engine are typically rather limited
as it traditionally performs e.g. keyword-based re-
trieval. If that keyword is not found in the wiki, the
query returns an empty result set. However, a page
might exist which is semantically related to the key-
word, and should thus yield a match.
As the search engine is typically a core part of the
wiki system, it is rather difficult to modify its be-
havior. However, by leveraging Wikulu?s architec-
ture, we can replace the default search mechanisms
by algorithms which allow for semantic search to al-
leviate the vocabulary mismatch problem (Gurevych
et al, 2007).
Segmenting Long Pages Due to the open edit-
ing policy of wikis, pages tend to grow rather fast.
75
Figure 3: Analysis of a wiki article with respect to topical
coherence. Suggested segment breaks are highlighted by
yellow bars.
For users, it is thus a major challenge to keep an
overview of what content is present on a certain
page. Wikulu therefore supports users by analyzing
long pages through employing text segmentation al-
gorithms which detect topically coherent segments
of text. It then suggests segment boundaries which
the user may or may not accept for inserting a sub-
heading which makes pages easier to read and better
to navigate. As shown in Figure 3, users are also en-
couraged to set a title for each segment.8 When ac-
cepting one or more of these suggested boundaries,
Wikulu stores them persistently in the wiki. Wikulu
currently integrates text segmentation methods such
as TextTiling (Hearst, 1997) or C99 (Choi, 2000).
Summarizing Pages Similarly to segmenting
pages, Wikulu makes long wiki pages more acces-
sible by generating an extractive summary. While
generative summaries generate a summary in own
words, extractive summaries analyze the original
wiki text sentence-by-sentence, rank each sentence,
and return a list of the most important ones (see Fig-
ure 4). Wikulu integrates extractive text summariza-
tion methods such as LexRank (Erkan and Radev,
2004).
Highlighting Keyphrases Another approach to
assist users in better grasping the idea of a wiki page
at a glance is to highlight important keyphrases (see
Figure 1). As Tucker and Whittaker (2009) have
8In future work, we plan to suggest suitable titles for each
segment automatically.
Figure 4: Extractive summary of the original wiki page
shown in Figure 3
shown, highlighting important phrases assists users
with reading longer texts and yields faster under-
standing. Wikulu thus improves readability by em-
ploying automatic keyphrase extraction algorithms.
Additionally, Wikulu allows to dynamically adjust
the number of keyphrases shown by presenting a
slider to the user. We integrated keyphrase extrac-
tion methods such as TextRank (Mihalcea and Tarau,
2004) and KEA (Witten et al, 1999).
3 Further Use Cases
Further use cases for supporting wiki users include
(i) visually analyzing the results of NLP algorithms,
(ii) educational purposes, and (iii) enabling semantic
wikis.
Visually Analyzing the Results of NLP Algo-
rithms Wikulu facilitates analyzing the results of
NLP algorithms by using wiki pages as input doc-
uments and visualizing the results directly on that
page. Consider an NLP algorithm which performs
sentiment analysis. Typically, we were to put our
analysis sentences in a text file, launch the NLP ap-
plication, process the file, and would read the output
from either a built-in console or a separate output
file. This procedure suffers from two major draw-
backs: (a) it is inconvenient to copy existing data
into a custom input format which can be fed into the
NLP system, and (b) the textual output does not al-
low presenting the results in a visually rich manner.
Wikulu tackles both challenges by using wiki
pages as input/output documents. For instance,
76
by running the sentiment analysis component right
from within the wiki, its output can be written back
to the originating wiki page, resulting in visually
rich, possibly interactive presentations.
Educational Purposes Wikulu is a handy tool for
educational purposes as it allows to (a) rapidly create
test data in a collaborative manner (see Section 2),
and (b) visualize the results of NLP algorithms, as
described above. Students can gather hands-on ex-
perience by experimenting with NLP components in
an easy-to-use wiki system. They can both collab-
oratively edit input documents, and explore possi-
ble results of e.g. different configurations of NLP
components. In our system prototype, we integrated
highlighting parts-of-speech which have been deter-
mined by a POS tagger.
Enabling Semantic Wikis Semantic wikis such
as the Semantic MediaWiki (Kro?tzsch et al, 2006)
augment standard wikis with machine-readable se-
mantic annotations of pages and links. As those
annotations have to be entered manually, this step
is often skipped by users which severely limits the
usefulness of semantic wikis. Wikulu could support
users e.g. by automatically suggesting the type of a
link by means of relation detection or the type of a
page by means of text categorization. Thus, Wikulu
could constitute an important step towards the se-
mantification of the content contained in wikis.
4 System Architecture
In this section, we detail our system architecture and
describe what is necessary to make NLP algorithms
available through our system. We also give a walk-
through of Wikulu?s information flow.
4.1 Core Components
Wikulu builds upon a modular architecture, as de-
picted in Figure 5. It acts as an HTTP proxy server
which intercepts the communication between the
web browser and the target wiki engine, while it al-
lows to run any Apache UIMA-compliant NLP com-
ponent using an extensible plugin mechanism.
In the remainder of this section, we introduce each
module: (a) the proxy server which allows to add
Wikulu to any target wiki engine, (b) the JavaScript
injection that bridges the gap between the client- and
server-side code, (c) the plugin manager which gives
access to any Apache UIMA-based NLP component,
and (d) the wiki abstraction layer which offers a
high-level interface to typical wiki operations such
as reading and writing the wiki content.
Proxy Server Wikulu is designed to work with
any underlying wiki engine such as MediaWiki or
TWiki. Consequently, we implemented it as an
HTTP proxy server which allows it to be enabled at
any time by changing the proxy settings of a user?s
web browser.9 The proxy server intercepts all re-
quests between the user who interacts with her web
browser, and the underlying wiki engine. For ex-
ample, Wikulu passes certain requests to its lan-
guage processing components, or augments the de-
fault wiki toolbar by additional commands. We elab-
orate on the latter in the following paragraph.
JavaScript Injection Wikulu modifies the re-
quests between web browser and target wiki by in-
jecting custom client-side JavaScript code. Wikulu
is thus capable of altering the default behavior of
the wiki engine, e.g. replacing a keyword-based re-
trieval by enhanced search methods (cf. Section 2),
adding novel behavior such as additional toolbar
buttons or advanced input fields, or augmenting the
originating web page after a certain request has been
processed, e.g. an NLP algorithm has been run.
Plugin Manager Wikulu does not perform lan-
guage processing itself. It relies on Apache UIMA-
compliant NLP components which use wiki pages
(or parts thereof) as input texts. Wikulu offers a so-
phisticated plugin manager which takes care of dy-
namically loading those NLP components. The plu-
gin loader is designed to run plugins either every
time a wiki page loads, or manually by picking them
from the augmented wiki toolbar.
The NLP components are available as server-side
Java classes. Via direct web remoting10, those com-
ponents are made accessible through a JavaScript
proxy object. Wikulu offers a generic language pro-
cessing plugin which takes the current page contents
9The process of enabling a custom proxy server can be
simplified by using web browser extensions such as Mul-
tiproxy Switch (https://addons.mozilla.org/de/
firefox/addon/multiproxy-switch).
10http://directwebremoting.org
77
Browser
Duplicate Detection
JavaScript
Injection
P
l
u
g
i
n
M
a
n
a
g
e
r
Wiki Abstraction
Layer
Wiki
Semantic Search
Link Suggestion
Text Segmentation
Text Summarization
Keyphrase Highlighting
...
W
i
k
u
l
u
 
P
r
o
x
y
Apache UIMA-compliant
NLP components
User
Figure 5: Wikulu acts as a proxy server which intercepts
the communication between the web browser and the un-
derlying wiki engine. Its plugin manager allows to inte-
grate any Apache UIMA-compliant NLP component.
as input text, runs an NLP component, and writes its
output back to the wiki. To run a custom Apache
UIMA-compliant NLP component with Wikulu, one
just needs to plug that particular NLP component
into the generic plugin. No further adaptations to
the generic plugin are necessary. However, more ad-
vanced users may create fully customized plugins.
Wiki Abstraction Layer Wikulu communicates
with the underlying wiki engine via an abstraction
layer. That layer provides a generic interface for
accessing and manipulating the underlying wiki en-
gine. Thereby, Wikulu can both be tightly coupled to
a certain wiki instance such as MediaWiki or TWiki,
while being flexible at the same time to adapt to a
changing environment. New adaptors for other tar-
get wiki engines such as Confluence11 can be added
with minimal effort.
4.2 Walk-Through Example
Let?s assume that a user encounters a wiki page
which is rather lengthy. She realizes that Wikulu?s
keyphrase extraction component might help her to
better grasp the idea of this page at a glance, so
she activates Wikulu by setting her web browser to
pass all requests through the proxy server. After
11http://www.atlassian.com/software/
confluence
JS
Injection
Proxy
Server
Keyphr.
Plugin
Wiki 
Abstr. Lay.
Wiki
get content from wiki page
get
page
extract
keyphrases
Browser
highlight
keyphrases
inject
keyphrases
Figure 6: Illustration of Wikulu?s information flow when
a user has requested to highlight keyphrases on the cur-
rent page as described in Section 4.2
applying the settings, the JavaScript injection mod-
ule adds additional links to the wiki?s toolbar on
the originating wiki page. Having decided to ap-
ply keyphrase extraction, she then invokes that NLP
component by clicking the corresponding link (see
Figure 6). Before the request is passed to that com-
ponent, Wikulu extracts the wiki page contents us-
ing the high-level wiki abstraction layer. Thereafter,
the request is passed via direct web remoting to the
NLP component which has been loaded by Wikulu?s
plugin mechanism. After processing the request, the
extracted keyphrases are returned to Wikulu?s cus-
tom JavaScript handlers and finally highlighted in
the originating wiki page.
5 Related Work
Supporting wiki users with NLP techniques has not
attracted a lot of research attention yet. A no-
table exception is the work by Witte and Gitzinger
(2007). They propose an architecture to connect
wikis to services providing NLP functionality which
are based on the General Architecture for Text En-
gineering (Cunningham et al, 2002). Contrary to
Wikulu, though, their system does not integrate
transparently with an underlying wiki engine, but
rather uses a separate application to apply NLP tech-
niques. Thereby, wiki users can leverage the power
of NLP algorithms, but need to interrupt their cur-
rent workflow to switch to a different application.
78
Moreover, their system is only loosely coupled with
the underlying wiki engine. While it allows to read
and write existing pages, it does not allow further
modifications such as adding user interface controls.
A lot of work in the wiki community is done in the
context of Wikipedia. For example, the FastestFox12
plug-in for Wikipedia is able to suggest links to re-
lated articles. However, unlike Wikulu, FastestFox
is tailored towards Wikipedia and cannot be used
with any other wiki platform.
6 Summary
We presented Wikulu, an extensible system which
integrates natural language processing techniques
with wikis. Wikulu addresses the major challenge of
supporting wiki users with their everyday tasks. Be-
sides that, we demonstrated how Wikulu serves as
a flexible environment for (a) visually analyzing the
results of NLP algorithms, (b) educational purposes,
and (c) enabling semantic wikis. By its modular and
flexible architecture, we envision that Wikulu can
support wiki users both in small focused environ-
ments as well as in large-scale communities such as
Wikipedia.
Acknowledgments
This work has been supported by the Volkswagen Foun-
dation as part of the Lichtenberg-Professorship Program
under grant No. I/82806, and by the Klaus Tschira Foun-
dation under project No. 00.133.2008. We would like to
thank Johannes Hoffart for designing and implementing
the foundations of this work, as well as Artem Vovk and
Carolin Deeg for their contributions.
References
Michel Buffa. 2006. Intranet Wikis. In Proceedings
of the IntraWebs Workshop at the 15th International
Conference on World Wide Web.
Freddy Y. Y. Choi. 2000. Advances in domain indepen-
dent linear text segmentation. In Proceedings of the
1st Meeting of the North American Chapter of the As-
sociation for Computational Linguistics, pages 26?33.
Hamish Cunningham, Diana Maynard, Kalina
Bontcheva, and Valentin Tablan. 2002. GATE:
A Framework and Graphical Development Environ-
ment for Robust NLP Tools and Applications. In
Proc. of the 40th Annual Meeting of the Association
for Computational Linguistics, pages 168?175.
12http://smarterfox.com
Gu?nes? Erkan and Dragomir Radev. 2004. LexRank:
Graph-based Lexical Centrality as Salience in Text
Summarization. Journal of Artificial Intelligence Re-
search, 22:457?479.
David Ferrucci and Adam Lally. 2004. UIMA: An Ar-
chitectural Approach to Unstructured Information Pro-
cessing in the Corporate Research Environment. Nat-
ural Language Engineering, pages 1?26.
Evgeniy Gabrilovich and Shaul Markovitch. 2007. Com-
puting Semantic Relatedness using Wikipedia-based
Explicit Semantic Analysis. In Proceedings of the
20th International Joint Conference on Artificial In-
telligence, pages 1606?1611.
Shlomo Geva. 2007. GPX: Ad-Hoc Queries and Auto-
mated Link Discovery in the Wikipedia. In Prepro-
ceedings of the INEX Workshop, pages 404?416.
Iryna Gurevych, Christof Mu?ller, and Torsten Zesch.
2007. What to be??Electronic Career Guidance Based
on Semantic Relatedness. In Proceedings of the 45th
Annual Meeting of the Association for Computational
Linguistics, pages 1032?1039.
Marti A. Hearst. 1997. TextTiling: Segmenting text into
multi-paragraph subtopic passages. Computational
Linguistics, 23(1):33?64.
Kelly Y. Itakura and Charles L. A. Clarke. 2007. Univer-
sity of Waterloo at INEX2007: Adhoc and Link-the-
Wiki Tracks. In INEX 2007 Workshop Preproceed-
ings, pages 417?425.
Markus Kro?tzsch, Denny Vrandec?ic?, and Max Vo?lkel.
2006. Semantic MediaWiki. In Proc. of the 5th Inter-
national Semantic Web Conference, pages 935?942.
Thomas K. Landauer, Peter W. Foltz, and Darrell Laham.
1998. An introduction to Latent Semantic Analysis.
Discourse Processes, 25(2):259?284.
Bo Leuf and Ward Cunningham. 2001. The Wiki Way:
Collaboration and Sharing on the Internet. Addison-
Wesley Professional.
Rada Mihalcea and Paul Tarau. 2004. TextRank: Bring-
ing Order into Texts. In Proceedings of the Conference
on Empirical Methods in Natural Language Process-
ing, pages 404?411.
Simon Tucker and Steve Whittaker. 2009. Have A Say
Over What You See: Evaluating Interactive Compres-
sion Techniques. In Proceedings of the Intl. Confer-
ence on Intelligent User Interfaces, pages 37?46.
Rene? Witte and Thomas Gitzinger. 2007. Connecting
wikis and natural language processing systems. In
Proc. of the Intl. Symposium on Wikis, pages 165?176.
Ian H. Witten, Gordon W. Paynter, Eibe Frank, Carl
Gutwin, and Craig G. Nevill-Manning. 1999. KEA:
Practical automatic keyphrase extraction. In Proceed-
ings of the 4th ACM Conference on Digital Libraries,
pages 254?255.
79
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 121?126,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
DKPro Similarity: An Open Source Framework for Text Similarity
Daniel Ba?r?, Torsten Zesch??, and Iryna Gurevych??
?Ubiquitous Knowledge Processing Lab (UKP-TUDA)
Department of Computer Science, Technische Universita?t Darmstadt
?Ubiquitous Knowledge Processing Lab (UKP-DIPF)
German Institute for Educational Research and Educational Information
www.ukp.tu-darmstadt.de
Abstract
We present DKPro Similarity, an open
source framework for text similarity. Our
goal is to provide a comprehensive repos-
itory of text similarity measures which
are implemented using standardized inter-
faces. DKPro Similarity comprises a wide
variety of measures ranging from ones
based on simple n-grams and common
subsequences to high-dimensional vector
comparisons and structural, stylistic, and
phonetic measures. In order to promote
the reproducibility of experimental results
and to provide reliable, permanent ex-
perimental conditions for future studies,
DKPro Similarity additionally comes with
a set of full-featured experimental setups
which can be run out-of-the-box and be
used for future systems to built upon.
1 Introduction
Computing text similarity is key to several natu-
ral language processing applications such as au-
tomatic essay grading, paraphrase recognition, or
plagiarism detection. However, only a few text
similarity measures proposed in the literature are
released publicly, and those then typically do not
comply with any standardization. We are currently
not aware of any designated text similarity frame-
work which goes beyond simple lexical similarity
or contains more than a small number of measures,
even though related frameworks exist, which we
discuss in Section 6. This fact was also realized
by the organizers of the pilot Semantic Textual
Similarity Task at SemEval-2012 (see Section 5),
as they argue for the creation of an open source
framework for text similarity (Agirre et al, 2012).
In order to fill this gap, we present DKPro Sim-
ilarity, an open source framework for text simi-
larity. DKPro Similarity is designed to comple-
ment DKPro Core1, a collection of software com-
ponents for natural language processing based on
the Apache UIMA framework (Ferrucci and Lally,
2004). Our goal is to provide a comprehensive
repository of text similarity measures which are
implemented in a common framework using stan-
dardized interfaces. Besides the already available
measures, DKPro Similarity is easily extensible
and intended to allow for custom implementations,
for which it offers various templates and exam-
ples. The Java implementation is publicly avail-
able at Google Code2 under the Apache Software
License v2 and partly under GNU GPL v3.
2 Architecture
DKPro Similarity is designed to operate in ei-
ther of two modes: The stand-alone mode al-
lows to use text similarity measures as indepen-
dent components in any experimental setup, but
does not offer means for further language process-
ing, e.g. lemmatization. The UIMA-coupled mode
tightly integrates similarity computation with full-
fledged Apache UIMA-based language processing
pipelines. That way, it allows to perform any num-
ber of languge processing steps, e.g. coreference
or named-entitiy resolution, along with the text
similarity computation.
Stand-alone Mode In this mode, text similarity
measures can be used independently of any lan-
guage processing pipeline just by passing them a
pair of texts as (i) two strings, or (ii) two lists of
strings (e.g. already lemmatized texts). We there-
fore provide an API module, which contains Java
interfaces and abstract base classes for the mea-
sures. That way, DKPro Similarity allows for a
maximum flexibility in experimental design, as the
text similarity measures can easily be integrated
with any existing experimental setup:
1code.google.com/p/dkpro-core-asl
2code.google.com/p/dkpro-similarity-asl
121
1 TextSimilarityMeasure m =
new GreedyStringTiling();
2 double similarity =
m.getSimilarity(text1, text2);
The above code snippet instantiates the Greedy
String Tiling measure (Wise, 1996) and then com-
putes the text similarity between the given pair of
texts. The resulting similarity score is normal-
ized into [0, 1] where 0 means not similar at all,
and 1 corresponds to perfectly similar.3 By us-
ing the common TextSimilarityMeasure
interface, it is easy to replace Greedy String Tiling
with any measure of choice, such as Latent Se-
mantic Analysis (Landauer et al, 1998) or Explicit
Semantic Analysis (Gabrilovich and Markovitch,
2007). We give an overview of measures available
in DKPro Similarity in Section 3.
UIMA-coupled Mode In this mode, DKPro
Similarity allows text similarity computation to
be directly integrated with any UIMA-based lan-
guage processing pipeline. That way, it is easy to
use text similarity components in addition to other
UIMA-based components in the same pipeline.
For example, an experimental setup may require to
first compute text similarity scores and then to run
a classification algorithm on the resulting scores.
In Figure 1, we show a graphical overview of
the integration of text similarity measures (right)
with a UIMA-based pipeline (left). The pipeline
starts by reading a given dataset, then performs
any number of pre-processing steps such as to-
kenization, sentence splitting, lemmatization, or
stopword filtering, then runs the text similar-
ity computation, before executing any subsequent
post-processing steps and finally returning the pro-
cessed texts in a suitable format for evaluation or
manual inspection. As all text similarity measures
in DKPro Similarity conform to standardized in-
terfaces, they can be easily exchanged in the text
similarity computation step.
With DKPro Similarity, we offer various sub-
classes of the generic UIMA components which
are specifically tailored towards text similarity ex-
periments, e.g. corpus readers for standard eval-
uation datasets as well as evaluation components
for running typical evaluation metrics. By lever-
aging UIMA?s architecture, we also define an
3Some string distance measures such as the Levenshtein
distance (Levenshtein, 1966) return a raw distance score
where less distance corresponds to higher similarity. How-
ever, the score can easily be normalized, e.g. by text length.
UIMA-based Pipeline
Corpus Reader
Pre-processing
Text Similarity
Computation
Post-processing
Evaluation
Sim
ilar
ity
Sco
rer
Text Similarity Measures
Greedy String Tiling
Double Metaphone
...
Explicit Sem. Analysis
Figure 1: DKPro Similarity allows to integrate any
text similarity measure (right) which conforms to
standardized interfaces into a UIMA-based lan-
guage processing pipeline (left) by means of a
dedicated Similarity Scorer component (middle).
additional interface to text similarity measures:
The JCasTextSimilarityMeasure inherits
from TextSimilarityMeasure, and adds a
method for two JCas text representations:4
double getSimilarity
(JCas text1, JCas text2);
The additional interface allows to implement mea-
sures which have full access to UIMA?s document
structure. That way, it is possible to create text
similarity measures which can use any piece of in-
formation that has been annotated in the processed
documents, such as dependency trees or morpho-
logical information. We detail the new set of com-
ponents offered by DKPro Similarity in Section 4.
3 Text Similarity Measures
In this section, we give an overview of the text
similarity measures which are already available in
DKPro Similarity. While we provide new imple-
mentations for a multitude of measures, we rely on
specialized libraries such as the S-Space Package
(see Section 6) if available. Due to space limi-
tations and due to the fact that the framework is
actively under development, we do not provide an
exhaustive list here, but rather mention the most
interesting and most popular measures.
3.1 Simple String-based Measures
DKPro Similarity includes text similarity mea-
sures which operate on string sequences and
determine, for example, the longest common
4The JCas is an object-oriented Java interface to the
Common Analysis Structure (Ferrucci and Lally, 2004),
Apache UIMA?s internal document representation format.
122
(non-)contiguous sequence of characters. It also
contains Greedy String Tiling (Wise, 1996), a mea-
sure which allows to compare strings if parts have
been reordered. The framework also offers mea-
sures which compute sets of character and word
n-grams and compare them using different overlap
coefficients, e.g. the Jaccard index. It further in-
cludes popular string distance metrics such as the
Jaro-Winkler (Winkler, 1990), Monge and Elkan
(1997) and Levenshtein (1966) distance measures.
3.2 Semantic Similarity Measures
DKPro Similarity also contains several measures
which go beyond simple character sequences and
compute text similarity on a semantic level.
Pairwise Word Similarity These measures are
based on pairwise word similarity computations
which are then aggregated for the complete texts.
The measures typically operate on a graph-based
representation of words and the semantic relations
among them within a lexical-semantic resource.
DKPro Similarity therefore contains adapters for
WordNet, Wiktionary5, and Wikipedia, while the
framework can easily be extended to other data
sources that conform to a common interface
(Garoufi et al, 2008). Pairwise similarity mea-
sures in DKPro Similarity include Jiang and Con-
rath (1997) or Resnik (1995). The aggregation for
the complete texts can for example be done using
the strategy by Mihalcea et al (2006).
Vector Space Models These text similarity
measures project texts onto high-dimensional vec-
tors which are then compared. Cosine similar-
ity, a basic measure often used in information re-
trieval, weights words according to their term fre-
quencies or tf-idf scores, and computes the co-
sine between two text vectors. Latent Seman-
tic Analysis (Landauer et al, 1998) alleviates the
inherent sparseness of a high-dimensional term-
document matrix by reducing it to one of reduced
rank. Explicit Semantic Analysis (Gabrilovich and
Markovitch, 2007) constructs the vector space on
corpora where the documents are assumed to de-
scribe natural concepts such as cat or dog. Orig-
inally, Wikipedia was proposed as the document
collection of choice.
DKPro Similarity goes beyond a single im-
plementation of these measures and comes with
highly customizable code which allows to set var-
5http://www.wiktionary.org
ious parameters for the construction of the vector
space and the comparison of the document vectors,
and further allows to construct the vector space for
arbitrary collections, e.g. domain-specific corpora.
3.3 Further Measures
Previous research (Ba?r et al, 2012b) has shown
promising results for the inclusion of measures
which go beyond textual content and compute
similarity along other text characteristics. Thus,
DKPro Similarity also includes measures for
structural, stylistic, and phonetic similarity.
Structural Similarity Structural similarity be-
tween texts can be computed, for example, by
comparing sets of stopword n-grams (Stamatatos,
2011). The idea here is that similar texts may pre-
serve syntactic similarity while exchanging only
content words. Other measures in DKPro Simi-
larity allow to compare texts by part-of-speech n-
grams, and order and distance features for pairs of
words (Hatzivassiloglou et al, 1999).
Stylistic Similarity DKPro Similarity includes,
for example, a measure which compares function
word frequencies (Dinu and Popescu, 2009) be-
tween two texts. The framework also includes a
set of measures which capture statistical properties
of texts such as the type-token ratio (TTR) and the
sequential TTR (McCarthy and Jarvis, 2010).
Phonetic Similarity DKPro Similarity also al-
lows to compute text similarity based on pair-
wise phonetic comparisons of words. It therefore
contains implementations of well-known phonetic
algorithms such as Double Metaphone (Philips,
2000) and Soundex (Knuth, 1973), which also con-
form to the common text similarity interface.
4 UIMA Components
In addition to a rich set of text similarity mea-
sures as partly described above, DKPro Similar-
ity includes components which allow to integrate
text similarity measures with any UIMA-based
pipeline, as outlined in Figure 1. In the following,
we introduce these components along with their
resources.
Readers & Datasets DKPro Similarity includes
corpus readers specifically tailored towards com-
bining the input texts in a number of ways, e.g.
all possible combinations, or each text paired with
n others by random. Standard datasets for which
123
readers come pre-packaged include, among oth-
ers, the SemEval-2012 STS data (Agirre et al,
2012), the METER corpus (Clough et al, 2002),
or the RTE 1?5 data (Dagan et al, 2006). As far
as license terms allow redistribution, the datasets
themselves are integrated into the framework.
Similarity Scorer The Similarity Scorer allows
to integrate any text similarity measure (which is
decoupled from UIMA by default) into a UIMA-
based pipeline. It builds upon the standardized text
similarity interfaces and thus allows to easily ex-
change the text similarity measure as well as to
specify the data types the measure should operate
on, e.g. tokens or lemmas.
Machine Learning Previous research (Agirre et
al., 2012) has shown that different text similarity
measures can be combined using machine learning
classifiers. Such a combination shows improve-
ments over single measures due to the fact that dif-
ferent measures capture different text characteris-
tics. DKPro Similarity thus provides adapters for
the Weka framework (Hall et al, 2009) and allows
to first pre-compute sets of text similarity scores
which can then be used as features for various ma-
chine learning classifiers.
Evaluation Metrics In the final step of a UIMA
pipeline, the processed data is read by a dedicated
evaluation component. DKPro Similarity ships
with a set of components which for example com-
pute Pearson or Spearman correlation with human
judgments, or apply task-specific metrics such as
average precision as used in the RTE challenges.
5 Experimental Setups
DKPro Similarity further encourages the creation
and publication of complete experimental setups.
That way, we promote the reproducibility of ex-
perimental results, and provide reliable, perma-
nent experimental conditions which can benefit fu-
ture studies and help to stimulate the reuse of par-
ticular experimental steps and software modules.
The experimental setups are instantiations of
the generic UIMA-based language processing
pipeline depicted in Figure 1 and are designed to
precisely match the particular task at hand. They
thus come pre-configured with corpus readers for
the relevant input data, with a set of pre- and post-
processing as well as evaluation components, and
with a set of text similarity measures which are
well-suited for the particular task. The experimen-
tal setups are self-contained systems and can be
run out-of-the-box without further configuration.6
DKPro Similarity contains two major types of
experimental setups: (i) those for an intrinsic eval-
uation allow to evaluate the system performance in
an isolated setting by comparing the system results
with a human gold standard, and (ii) those for an
extrinsic evaluation allow to evaluate the system
with respect to a particular task at hand, where text
similarity is a means for solving a concrete prob-
lem, e.g. recognizing textual entailment.
Intrinsic Evaluation DKPro Similarity con-
tains the setup (Ba?r et al, 2012a) which partic-
ipated in the Semantic Textual Similarity (STS)
Task at SemEval-2012 (Agirre et al, 2012) and
which has become one of the recommended base-
line systems for the second task of this series.7
The system combines a multitude of text similar-
ity measures of varying complexity using a simple
log-linear regression model. The provided setup
allows to evaluate how well the system output re-
sembles human similarity judgments on short texts
which are taken from five different sources, e.g.
paraphrases of news texts or video descriptions.
Extrinsic Evaluation Our framework includes
two setups for an extrinsic evaluation: detecting
text reuse, and recognizing textual entailment.
For detecting text reuse (Clough et al, 2002),
the setup we provide (Ba?r et al, 2012b) combines
a multitude of text similarity measures along dif-
ferent text characteristics. Thereby, it not only
combines simple string-based and semantic sim-
ilarity measures (see Sections 3.1 and 3.2), but
makes extensive use of measures along structural
and stylistic text characteristics (see Section 3.3).
Across three standard evaluation datasets, the sys-
tem consistently outperforms all previous work.
For recognizing textual entailment, we provide
a setup which is similar in configuration to the one
described above, but contains corpus readers and
evaluation components precisely tailored towards
the RTE challenge series (Dagan et al, 2006). We
believe that our setup can be used for filtering
those text pairs which need further analysis by a
dedicated textual entailment system.
6A one-time setup of local lexical-semantic resources
such as WordNet may be necessary, though.
7In 2013, the STS Task is a shared task of the Second
Joint Conference on Lexical and Computational Semantics,
http://ixa2.si.ehu.es/sts
124
6 Related Frameworks
To the best of our knowledge, only a few general-
ized similarity frameworks exist at all. In the fol-
lowing, we discuss them and give insights where
DKPro Similarity uses implementations of these
existing libraries. That way, DKPro Similarity
brings together the scattered efforts by offering ac-
cess to all measures through common interfaces. It
goes far beyond the functionality of the original li-
braries as it generalizes the resources used, allows
a tight integration with any UIMA-based pipeline,
and comes with full-featured experimental setups
which are pre-configured stand-alone text similar-
ity systems that can be run out-of-the-box.
S-Space Package Even though no designated
text similarity library, the S-Space Package (Jur-
gens and Stevens, 2010)8 contains some text sim-
ilarity measures such as Latent Semantic Analysis
(LSA) and Explicit Semantic Analysis (see Sec-
tion 3.2). However, it is primarily focused on
word space models which operate on word distri-
butions in text. Besides such algorithms, it offers
a variety of interfaces, data structures, evaluation
datasets and metrics, and global operation utili-
ties e.g. for dimension reduction using Singular
Value Decomposition or randomized projections,
which are particularly useful with such distribu-
tional word space models. DKPro Similarity inte-
grates LSA based on the S-Space Package.
Semantic Vectors The Semantic Vectors pack-
age is a package for distributional semantics (Wid-
dows and Cohen, 2010)9 that contains measures
such as LSA and allows for comparing documents
within a given vector space. The main focus lies
on word space models with a number of dimension
reduction techniques, and applications on word
spaces such as automatic thesaurus generation.
WordNet::Similarity The open source package
by Pedersen et al (2004)10 is a popular Perl li-
brary for the similarity computation on WordNet.
It comprises six word similarity measures that op-
erate on WordNet, e.g. Jiang and Conrath (1997)
or Resnik (1995). Unfortunately, no strategies
have been added to the package yet which aggre-
gate the word similarity scores for complete texts
in a similar manner as described in Section 3.2.
8code.google.com/p/airhead-research
9code.google.com/p/semanticvectors
10sourceforge.net/projects/wn-similarity
In DKPro Similarity, we offer native Java imple-
mentations of all measures contained in Word-
Net::Similarity, and allow to go beyond WordNet
and use the measures with any lexical-semantic re-
source of choice, e.g. Wiktionary or Wikipedia.
SimMetrics Library The Java library by Chap-
man et al (2005)11 exclusively comprises text sim-
ilarity measures which compute lexical similar-
ity on string sequences and compare texts with-
out any semantic processing. It contains mea-
sures such as the Levenshtein (1966) or Monge and
Elkan (1997) distance metrics. In DKPro Similar-
ity, some string-based measures (see Section 3.1)
are based on implementations from this library.
SecondString Toolkit The freely available li-
brary by Cohen et al (2003)12 is similar to Sim-
Metrics, and also implemented in Java. It also con-
tains several well-known text similarity measures
on string sequences, and includes many of the
measures which are also part of the SimMetrics
Library. Some string-based measures in DKPro
Similarity are based on the SecondString Toolkit.
7 Conclusions
We presented DKPro Similarity, an open source
framework designed to streamline the develop-
ment of text similarity measures. All measures
conform to standardized interfaces and can either
be used as stand-alone components in any ex-
perimental setup (e.g. an already existing system
which is not based on Apache UIMA), or can be
tightly coupled with a full-featured UIMA-based
language processing pipeline in order to allow for
advanced processing capabilities.
We would like to encourage other researchers
to participate in our efforts and invite them to ex-
plore our existing experimental setups as outlined
in Section 5, run modified versions of our setups,
and contribute own text similarity measures to
the framework. For that, DKPro Similarity also
comes with an example module for getting started,
which guides first-time users through both the
stand-alone and the UIMA-coupled modes.
Acknowledgements This work has been supported by the
Volkswagen Foundation as part of the Lichtenberg Profes-
sorship Program under grant No. I/82806, and by the Klaus
Tschira Foundation under project No. 00.133.2008. We thank
Richard Eckart de Castilho and all other contributors.
11sourceforge.net/projects/simmetrics
12sourceforge.net/projects/secondstring
125
References
Eneko Agirre, Daniel Cer, Mona Diab, and Aitor
Gonzalez-Agirre. 2012. SemEval-2012 Task 6: A
Pilot on Semantic Textual Similarity. In Proc. of the
6th Int?l Works. on Semantic Eval., pages 385?393.
Daniel Ba?r, Chris Biemann, Iryna Gurevych, and
Torsten Zesch. 2012a. UKP: Computing Semantic
Textual Similarity by Combining Multiple Content
Similarity Measures. In Proc. of the 6th Int?l Work-
shop on Semantic Evaluation, pages 435?440.
Daniel Ba?r, Torsten Zesch, and Iryna Gurevych. 2012b.
Text Reuse Detection Using a Composition of Text
Similarity Measures. In Proc. of the 24th Int?l Conf.
on Computational Linguistics, pages 167?184.
Sam Chapman, Barry Norton, and Fabio Ciravegna.
2005. Armadillo: Integrating Knowledge for the Se-
mantic Web. In Proceedings of the Dagstuhl Semi-
nar in Machine Learning for the Semantic Web.
Paul Clough, Robert Gaizauskas, Scott S.L. Piao, and
Yorick Wilks. 2002. METER: MEasuring TExt
Reuse. In Proceedings of ACL, pages 152?159.
William W. Cohen, Pradeep Ravikumar, and Stephen
Fienberg. 2003. A Comparison of String Metrics
for Matching Names and Records. In Proc. of KDD
Works. on Data Cleaning and Object Consolidation.
Ido Dagan, Oren Glickman, and Bernardo Magnini.
2006. The PASCAL Recognising Textual Entail-
ment Challenge. In Machine Learning Challenges,
Lecture Notes in Computer Science, pages 177?190.
Liviu P. Dinu and Marius Popescu. 2009. Ordinal mea-
sures in authorship identification. In Proceedings of
the 3rd PAN Workshop. Uncovering Plagiarism, Au-
thorship and Social Software Misuse, pages 62?66.
David Ferrucci and Adam Lally. 2004. UIMA: An
Architectural Approach to Unstructured Information
Processing in the Corporate Research Environment.
Natural Language Engineering, 10(3-4):327?348.
Evgeniy Gabrilovich and Shaul Markovitch. 2007.
Computing Semantic Relatedness using Wikipedia-
based Explicit Semantic Analysis. In Proceedings
of IJCAI, pages 1606?1611, Hyderabad, India.
Konstantina Garoufi, Torsten Zesch, and Iryna
Gurevych. 2008. Representational Interoperability
of Linguistic and Collaborative Knowledge Bases.
In Proceedings of the KONVENS Workshop on
Lexical-Semantic and Ontological Resources.
Mark Hall, Eibe Frank, Geoffrey Holmes, Bernhard
Pfahringer, Peter Reutemann, and Ian H. Witten.
2009. The WEKA Data Mining Software: An Up-
date. SIGKDD Explorations, 11(1):10?18.
Vasileios Hatzivassiloglou, Judith L. Klavans, and
Eleazar Eskin. 1999. Detecting text similarity over
short passages: Exploring linguistic feature com-
binations via machine learning. In Proceedings of
EMNLP/VLC, pages 203?212.
Jay J. Jiang and David W. Conrath. 1997. Semantic
similarity based on corpus statistics and lexical tax-
onomy. In Proceedings of ROCLING, pages 19?33.
David Jurgens and Keith Stevens. 2010. The S-Space
Package: An Open Source Package for Word Space
Models. In Proceedings of the ACL 2010 System
Demonstrations, pages 30?35, Uppsala, Sweden.
Donald E. Knuth. 1973. The Art of Computer
Programming: Volume 3, Sorting and Searching.
Addison-Wesley.
Thomas K. Landauer, Peter W. Foltz, and Darrell La-
ham. 1998. An Introduction to Latent Semantic
Analysis. Discourse Processes, 25(2):259?284.
Vladimir I. Levenshtein. 1966. Binary codes capa-
ble of correcting deletions, insertions, and reversals.
Soviet Physics Doklady, 10(8):707?710.
Philip M. McCarthy and Scott Jarvis. 2010. MTLD,
vocd-D, and HD-D: A validation study of sophis-
ticated approaches to lexical diversity assessment.
Behavior research methods, 42(2):381?392.
Rada Mihalcea, Courtney Corley, and Carlo Strappa-
rava. 2006. Corpus-based and Knowledge-based
Measures of Text Semantic Similarity. In Proceed-
ings of AAAI-06, pages 775?780, Boston, MA, USA.
Alvaro Monge and Charles Elkan. 1997. An ef-
ficient domain-independent algorithm for detecting
approximately duplicate database records. In Pro-
ceedings of the SIGMOD Workshop on Data Mining
and Knowledge Discovery, pages 23?29.
Ted Pedersen, Siddharth Patwardhan, and Jason Miche-
lizzi. 2004. WordNet::Similarity - Measuring the
Relatedness of Concepts. In Proceedings of the
HLT-NAACL: Demonstration Papers, pages 38?41.
Lawrence Philips. 2000. The double metaphone
search algorithm. C/C++ Users Jour., 18(6):38?43.
Philip Resnik. 1995. Using Information Content to
Evaluate Semantic Similarity in a Taxonomy. In
Proceedings of the IJCAI, pages 448?453.
Efstathios Stamatatos. 2011. Plagiarism detection
using stopword n-grams. Journal of the Ameri-
can Society for Information Science and Technology,
62(12):2512?2527.
Dominic Widdows and Trevor Cohen. 2010. The Se-
mantic Vectors Package: New Algorithms and Pub-
lic Tools for Distributional Semantics. In Proceed-
ings of IEEE-ICSC, pages 9?15.
William E. Winkler. 1990. String Comparator Metrics
and Enhanced Decision Rules in the Fellegi-Sunter
Model of Record Linkage. In Proceedings of the
Survey Research Methods Section, pages 354?359.
Michael J. Wise. 1996. YAP3: Improved detection of
similarities in computer program and other texts. In
Proc. of the 27th SIGCSE Technical Symposium on
Computer Science Education, pages 130?134.
126
First Joint Conference on Lexical and Computational Semantics (*SEM), pages 435?440,
Montre?al, Canada, June 7-8, 2012. c?2012 Association for Computational Linguistics
UKP: Computing Semantic Textual Similarity by
Combining Multiple Content Similarity Measures
Daniel Ba?r?, Chris Biemann?, Iryna Gurevych??, and Torsten Zesch??
?Ubiquitous Knowledge Processing Lab (UKP-TUDA)
Department of Computer Science, Technische Universita?t Darmstadt
?Ubiquitous Knowledge Processing Lab (UKP-DIPF)
German Institute for Educational Research and Educational Information
www.ukp.tu-darmstadt.de
Abstract
We present the UKP system which performed
best in the Semantic Textual Similarity (STS)
task at SemEval-2012 in two out of three met-
rics. It uses a simple log-linear regression
model, trained on the training data, to combine
multiple text similarity measures of varying
complexity. These range from simple char-
acter and word n-grams and common sub-
sequences to complex features such as Ex-
plicit Semantic Analysis vector comparisons
and aggregation of word similarity based on
lexical-semantic resources. Further, we em-
ploy a lexical substitution system and statisti-
cal machine translation to add additional lex-
emes, which alleviates lexical gaps. Our final
models, one per dataset, consist of a log-linear
combination of about 20 features, out of the
possible 300+ features implemented.
1 Introduction
The goal of the pilot Semantic Textual Similarity
(STS) task at SemEval-2012 is to measure the de-
gree of semantic equivalence between pairs of sen-
tences. STS is fundamental to a variety of tasks
and applications such as question answering (Lin
and Pantel, 2001), text reuse detection (Clough et
al., 2002) or automatic essay grading (Attali and
Burstein, 2006). STS is also closely related to tex-
tual entailment (TE) (Dagan et al, 2006) and para-
phrase recognition (Dolan et al, 2004). It differs
from both tasks, though, insofar as those operate on
binary similarity decisions while STS is defined as
a graded notion of similarity. STS further requires a
bidirectional similarity relationship to hold between
a pair of sentences rather than a unidirectional en-
tailment relation as for the TE task.
A multitude of measures for computing similar-
ity between texts have been proposed in the past
based on surface-level and/or semantic content fea-
tures (Mihalcea et al, 2006; Landauer et al, 1998;
Gabrilovich and Markovitch, 2007). The exist-
ing measures exhibit two major limitations, though:
Firstly, measures are typically used in separation.
Thereby, the assumption is made that a single
measure inherently captures all text characteristics
which are necessary for computing similarity. Sec-
ondly, existing measures typically exclude similar-
ity features beyond content per se, thereby implying
that similarity can be computed by comparing text
content exclusively, leaving out any other text char-
acteristics. While we can only briefly tackle the sec-
ond issue here, we explicitly address the first one by
combining several measures using a supervised ma-
chine learning approach. With this, we hope to take
advantage of the different facets and intuitions that
are captured in the single measures.
In the following section, we describe the feature
space in detail. Section 3 describes the machine
learning setup. After describing our submitted runs,
we discuss the results and conclude.
2 Text Similarity Measures
We now describe the various features we have tried,
also listing features that did not prove useful.
2.1 Simple String-based Measures
String Similarity Measures These measures op-
erate on string sequences. The longest common
435
substring measure (Gusfield, 1997) compares the
length of the longest contiguous sequence of char-
acters. The longest common subsequence measure
(Allison and Dix, 1986) drops the contiguity re-
quirement and allows to detect similarity in case
of word insertions/deletions. Greedy String Tiling
(Wise, 1996) allows to deal with reordered text parts
as it determines a set of shared contiguous sub-
strings, whereby each substring is a match of maxi-
mal length. We further used the following measures,
which, however, did not make it into the final mod-
els, since they were subsumed by the other mea-
sures: Jaro (1989), Jaro-Winkler (Winkler, 1990),
Monge and Elkan (1997), and Levenshtein (1966).
Character/word n-grams We compare character
n-grams following the implementation by Barro?n-
Ceden?o et al (2010), thereby generalizing the orig-
inal trigram variant to n = 2, 3, . . . , 15. We also
compare word n-grams using the Jaccard coefficient
as previously done by Lyon et al (2001), and the
containment measure (Broder, 1997). As high n led
to instabilities of the classifier due to their high in-
tercorrelation, only n = 1, 2, 3, 4 was used.
2.2 Semantic Similarity Measures
Pairwise Word Similarity The measures for
computing word similarity on a semantic level op-
erate on a graph-based representation of words and
the semantic relations among them within a lexical-
semantic resource. For this system, we used the al-
gorithms by Jiang and Conrath (1997), Lin (1998a),
and Resnik (1995) on WordNet (Fellbaum, 1998).
In order to scale the resulting pairwise word sim-
ilarities to the text level, we applied the aggregation
strategy by Mihalcea et al (2006): The sum of the
idf -weighted similarity scores of each word with the
best-matching counterpart in the other text is com-
puted in both directions, then averaged. In our ex-
periments, the measure by Resnik (1995) proved to
be superior to the other measures and was used in all
word similarity settings throughout this paper.
Explicit Semantic Analysis We also used the vec-
tor space model Explicit Semantic Analysis (ESA)
(Gabrilovich and Markovitch, 2007). Besides Word-
Net, we used two additional lexical-semantic re-
sources for the construction of the ESA vector space:
Wikipedia and Wiktionary1.
Textual Entailment We experimented with using
the BIUTEE textual entailment system (Stern and
Dagan, 2011) for generating entailment scores to
serve as features for the classifier. However, these
features were not selected by the classifier.
Distributional Thesaurus We used similarities
from a Distributional Thesaurus (similar to Lin
(1998b)) computed on 10M dependency-parsed sen-
tences of English newswire as a source for pairwise
word similarity, one additional feature per POS tag.
However, only the feature based on cardinal num-
bers (CD) was selected in the final models.
2.3 Text Expansion Mechanisms
Lexical Substitution System We used the lexical
substitution system based on supervised word sense
disambiguation (Biemann, 2012). This system au-
tomatically provides substitutions for a set of about
1,000 frequent English nouns with high precision.
For each covered noun, we added the substitutions
to the text and computed the pairwise word similar-
ity for the texts as described above. This feature al-
leviates the lexical gap for a subset of words.
Statistical Machine Translation We used the
Moses SMT system (Koehn et al, 2007) to trans-
late the original English texts via three bridge lan-
guages (Dutch, German, Spanish) back to English.
Thereby, the idea was that in the translation pro-
cess additional lexemes are introduced which allevi-
ate potential lexical gaps. The system was trained on
Europarl made available by Koehn (2005), using the
following configuration which was not optimized for
this task: WMT112 baseline without tuning, with
MGIZA alignment. The largest improvement was
reached for computing pairwise word similarity (as
described above) on the concatenation of the origi-
nal text and the three back-translations.
2.4 Measures Related to Structure and Style
In our system, we also used measures which go
beyond content and capture similarity along the
structure and style dimensions inherent to texts.
However, as we report later on, for this content-
1www.wiktionary.org
20-5-grams, grow-diag-final-and alignment, msd-bidirec-
tional-fe reodering, interpolation and kndiscount
436
oriented task they were not selected by the classifier.
Nonetheless, we briefly list them for completeness.
Structural similarity between texts can be de-
tected by computing stopword n-grams (Sta-
matatos, 2011). Thereby, all content-bearing words
are removed while stopwords are preserved. Stop-
word n-grams of both texts are compared using the
containment measure (Broder, 1997). In our experi-
ments, we tested n-gram sizes for n = 2, 3, . . . , 10.
We also compute part-of-speech n-grams for
various POS tags which we then compare using the
containment measure and the Jaccard coefficient.
We also used two similarity measures between
pairs of words (Hatzivassiloglou et al, 1999): Word
pair order tells whether two words occur in the
same order in both texts (with any number of words
in between), word pair distance counts the number
of words which lie between those of a given pair.
To compare texts along the stylistic dimension,
we further use a function word frequencies mea-
sure (Dinu and Popescu, 2009) which operates on a
set of 70 function words identified by Mosteller and
Wallace (1964). Function word frequency vectors
are computed and compared by Pearson correlation.
We also include a number of measures which
capture statistical properties of texts, such as type-
token ratio (TTR) (Templin, 1957) and sequential
TTR (McCarthy and Jarvis, 2010).
3 System Description
We first run each of the similarity measures intro-
duced above separately. We then use the resulting
scores as features for a machine learning classifier.
Pre-processing Our system is based on DKPro3,
a collection of software components for natural
language processing built upon the Apache UIMA
framework. During the pre-processing phase, we to-
kenize the input texts and lemmatize using the Tree-
Tagger implementation (Schmid, 1994). For some
measures, we additionally apply a stopword filter.
Feature Generation We now compute similarity
scores for the input texts with all measures and for
all configurations introduced in Section 2. This re-
sulted in 300+ individual score vectors which served
as features for the following step.
3http://dkpro-core-asl.googlecode.com
Run Features
1 Greedy String Tiling
Longest common subsequence (2 normalizations)
Longest common substring
Character 2-, 3-, and 4-grams
Word 1- and 2-grams (Containment, w/o stopwords)
Word 1-, 3-, and 4-grams (Jaccard)
Word 2- and 4-grams (Jaccard, w/o stopwords)
Word Similarity (Resnik (1995) on WordNet
aggregated according to Mihalcea et al (2006);
2 variants: complete texts + difference only)
Explicit Semantic Analysis (Wikipedia, Wiktionary)
Distributional Thesaurus (POS: Cardinal numbers)
2 All Features of Run 1
Lexical Substitution for Word Sim. (complete texts)
SMT for Word Sim. (complete texts as above)
3 All Features of Run 2
Random numbers from [4.5, 5] for surprise datasets
Table 1: Feature sets of our three system configurations
Feature Combination The feature combination
step uses the pre-computed similarity scores, and
combines their log-transformed values using a linear
regression classifier from the WEKA toolkit (Hall et
al., 2009). We trained the classifier on the training
datasets of the STS task. During the development
cycle, we evaluated using 10-fold cross-validation.
Post-processing For Runs 2 and 3, we applied a
post-processing filter which stripped all characters
off the texts which are not in the character range
[a-zA-Z0-9]. If the texts match, we set their similar-
ity score to 5.0 regardless of the classifier?s output.
4 Submitted Runs
Run 1 During the development cycle, we identi-
fied 19 features (see Table 1) which achieved the
best performance on the training data. For each
of the known datasets, we trained a separate clas-
sifier and applied it to the test data. For the surprise
datasets, we trained the classifier on a joint dataset
of all known training datasets.
Run 2 For the Run 2, we were interested in the
effects of two additional features: lexical substitu-
tion and statistical machine translation. We added
the corresponding measures to the feature set of Run
1 and followed the same evaluation procedure.
Run 3 For the third run, we used the same feature
set as for Run 2, but returned random numbers from
[4.5, 5] for the sentence pairs in the surprise datasets.
437
Dim. Text Similarity Features PAR VID SE
Best Feature Set, Run 1 .711 .868 .735
Best Feature Set, Run 2 .724 .868 .742
Content Pairwise Word Similarity .564 .835 .527
Character n-grams .658 .771 .554
Explicit Semantic Analysis .427 .781 .619
Word n-grams .474 .782 .619
String Similarity .593 .677 .744
Distributional Thesaurus .494 .481 .365
Lexical Substitution .228 .554 .483
Statistical Machine Translation .287 .652 .516
Structure Part-of-speech n-grams .193 .265 .557
Stopword n-grams .211 .118 .379
Word Pair Order .104 .077 .295
Style Statistical Properties .168 .225 .325
Function Word Frequencies .179 .142 .189
Table 2: Best results for single measures, grouped by di-
mension, on the training datasets MSRpar, MSRvid, and
SMTeuroparl, using 10-fold cross-validation
5 Results on Training Data
Evaluation was carried out using the official scorer
which computes Pearson correlation of the human
rated similarity scores with the the system?s output.
In Table 2, we report the results achieved on
each of the training datasets using 10-fold cross-
validation. The best results were achieved for the
feature set of Run 2, with Pearson?s r = .724,
r = .868, and r = .742 for the datasets MSR-
par, MSRvid, and SMTeuroparl, respectively. While
individual classes of content similarity measures
achieved good results, a different class performed
best for each dataset. However, text similarity mea-
sures related to structure and style achieved only
poor results on the training data. This was to be ex-
pected due to the nature of the data, though.
6 Results on Test Data
Besides the Pearson correlation for the union of all
datasets (ALL), the organizers introduced two addi-
tional evaluation metrics after system submission:
ALLnrm computes Pearson correlation after the sys-
tem outputs for each dataset are fitted to the gold
standard using least squares, and Mean refers to the
weighted mean across all datasets, where the weight
depends on the number of pairs in each dataset.
In Table 3, we report the offical results achieved
on the test data. The best configuration of our system
was Run 2 which was ranked #1 for the evaluation
#1 #2 #3 Sys. r1 r2 r3 PAR VID SE WN SN
1 2 1 UKP2 .823 .857 .677 .683 .873 .528 .664 .493
2 3 5 TL .813 .856 .660 .698 .862 .361 .704 .468
3 1 2 TL .813 .863 .675 .734 .880 .477 .679 .398
4 4 4 UKP1 .811 .855 .670 .682 .870 .511 .664 .467
5 6 13 UNT .784 .844 .616 .535 .875 .420 .671 .403
...
...
...
...
...
...
...
...
...
...
...
...
87 85 70 B/L .311 .673 .435 .433 .299 .454 .586 .390
Table 3: Official results on the test data for the top 5
participating runs out of 89 which were achieved on the
known datasets MSRpar, MSRvid, and SMTeuroparl, as
well as on the surprise datasets OnWN and SMTnews. We
report the ranks (#1: ALL, #2: ALLnrm, #3: Mean) and
the corresponding Pearson correlation r according to the
three offical evaluation metrics (see Sec. 6). The provided
baseline is shown at the bottom of this table.
metrics ALL (r = .823)4 and Mean (r = .677), and
#2 for ALLnrm (r = .857). An exhaustive overview
of all participating systems can be found in the STS
task description (Agirre et al, 2012).
7 Conclusions and Future Work
In this paper, we presented the UKP system, which
performed best across the three official evaluation
metrics in the pilot Semantic Textual Similarity
(STS) task at SemEval-2012. While we did not
reach the highest scores on any of the single datasets,
our system was most robust across different data. In
future work, it would be interesting to inspect the
performance of a system that combines the output
of all participating systems in a single linear model.
We also propose that two major issues with the
datasets are tackled in future work: (a) It is unclear
how to judge similarity between pairs of texts which
contain contextual references such as on Monday
vs. after the Thanksgiving weekend. (b) For several
pairs, it is unclear what point of view to take, e.g. for
the pair An animal is eating / The animal is hopping.
Is the pair to be considered similar (an animal is do-
ing something) or rather not (eating vs. hopping)?
Acknowledgements This work has been sup-
ported by the Volkswagen Foundation as part of the
Lichtenberg-Professorship Program under grant No.
I/82806, and by the Klaus Tschira Foundation under
project No. 00.133.2008.
499% confidence interval: .807 ? r ? .837
438
References
Eneko Agirre, Daniel Cer, Mona Diab, and Aitor
Gonzalez-Agirre. 2012. SemEval-2012 Task 6: A Pi-
lot on Semantic Textual Similarity. In Proceedings of
the 6th International Workshop on Semantic Evalua-
tion, in conjunction with the 1st Joint Conference on
Lexical and Computational Semantics.
Lloyd Allison and Trevor I. Dix. 1986. A bit-string
longest-common-subsequence algorithm. Information
Processing Letters, 23:305?310.
Yigal Attali and Jill Burstein. 2006. Automated es-
say scoring with e-rater v.2.0. Journal of Technology,
Learning, and Assessment, 4(3).
Alberto Barro?n-Ceden?o, Paolo Rosso, Eneko Agirre, and
Gorka Labaka. 2010. Plagiarism Detection across
Distant Language Pairs. In Proceedings of the 23rd
International Conference on Computational Linguis-
tics, pages 37?45.
Chris Biemann. 2012. Creating a System for Lexi-
cal Substitutions from Scratch using Crowdsourcing.
Language Resources and Evaluation: Special Issue
on Collaboratively Constructed Language Resources,
46(2).
Andrei Z. Broder. 1997. On the resemblance and con-
tainment of documents. Proceedings of the Compres-
sion and Complexity of Sequences, pages 21?29.
Paul Clough, Robert Gaizauskas, Scott S.L. Piao, and
Yorick Wilks. 2002. METER: MEasuring TExt
Reuse. In Proceedings of 40th Annual Meeting of
the Association for Computational Linguistics, pages
152?159.
Ido Dagan, Oren Glickman, and Bernardo Magnini.
2006. The PASCAL Recognising Textual Entailment
Challenge. In Machine Learning Challenges, Lecture
Notes in Computer Science, pages 177?190. Springer.
Liviu P. Dinu and Marius Popescu. 2009. Ordinal mea-
sures in authorship identification. In Proceedings of
the 3rd PAN Workshop. Uncovering Plagiarism, Au-
thorship and Social Software Misuse, pages 62?66.
William B. Dolan, Chris Quirk, and Chris Brockett.
2004. Unsupervised Construction of Large Paraphrase
Corpora: Exploiting Massively Parallel News Sources.
In Proceedings of the 20th International Conference
on Computational Linguistics, pages 350?356.
Christiane Fellbaum. 1998. WordNet: An Electronic
Lexical Database. MIT Press.
Evgeniy Gabrilovich and Shaul Markovitch. 2007. Com-
puting Semantic Relatedness using Wikipedia-based
Explicit Semantic Analysis. In Proceedings of the
20th International Joint Conference on Artificial In-
telligence, pages 1606?1611.
Dan Gusfield. 1997. Algorithms on Strings, Trees and
Sequences: Computer Science and Computational Bi-
ology. Cambridge University Press.
Mark Hall, Eibe Frank, Geoffrey Holmes, Bernhard
Pfahringer, Peter Reutemann, and Ian H. Witten.
2009. The WEKA Data Mining Software: An Update.
SIGKDD Explorations, 11(1):10?18.
Vasileios Hatzivassiloglou, Judith L. Klavans, and
Eleazar Eskin. 1999. Detecting text similarity over
short passages: Exploring linguistic feature combina-
tions via machine learning. In Proceedings of the Joint
SIGDAT Conference on Empirical Methods in Natural
Language Processing and Very Large Corpora, pages
203?212.
Matthew A. Jaro. 1989. Advances in record linkage
methodology as applied to the 1985 census of Tampa
Florida. Journal of the American Statistical Associa-
tion, 84(406):414?420.
Jay J. Jiang and David W. Conrath. 1997. Semantic sim-
ilarity based on corpus statistics and lexical taxonomy.
In Proceedings of the 10th International Conference
on Research in Computational Linguistics.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran, Richard
Zens, Chris Dyer, Ondr?ej Bojar, Alexandra Con-
stantin, and Evan Herbst. 2007. Moses: Open Source
Toolkit for Statistical Machine Translation. In Pro-
ceedings of the ACL 2007 Demo and Poster Sessions,
pages 177?180.
Philipp Koehn. 2005. Europarl: A Parallel Corpus for
Statistical Machine Translation. In Proceedings of the
10th Machine Translation Summit, pages 79?86.
Thomas K. Landauer, Peter W. Foltz, and Darrell La-
ham. 1998. An introduction to latent semantic analy-
sis. Discourse Processes, 25(2):259?284.
Vladimir I. Levenshtein. 1966. Binary codes capable of
correcting deletions, insertions, and reversals. Soviet
Physics Doklady, 10(8):707?710.
Dekang Lin and Patrick Pantel. 2001. Discovery of In-
ference Rules for Question Answering. Natural Lan-
guage Engineering, 7(4):343?360.
Dekang Lin. 1998a. An information-theoretic definition
of similarity. In Proceedings of International Confer-
ence on Machine Learning, pages 296?304.
Dekang Lin. 1998b. Automatic Retrieval and Clustering
of Similar Words. In Proceedings of the 36th Annual
Meeting of the Association for Computational Linguis-
tics, pages 768?774.
Caroline Lyon, James Malcolm, and Bob Dickerson.
2001. Detecting short passages of similar text in large
document collections. In Proceedings of Conference
on Empirical Methods in Natural Language Process-
ing, pages 118?125.
Philip M. McCarthy and Scott Jarvis. 2010. MTLD,
vocd-D, and HD-D: A validation study of sophisti-
439
cated approaches to lexical diversity assessment. Be-
havior research methods, 42(2):381?92.
Rada Mihalcea, Courtney Corley, and Carlo Strapparava.
2006. Corpus-based and Knowledge-based Measures
of Text Semantic Similarity. In Proceedings of the 21st
National Conference on Artificial Intelligence, pages
775?780.
Alvaro Monge and Charles Elkan. 1997. An efficient
domain-independent algorithm for detecting approxi-
mately duplicate database records. In Proceedings of
the SIGMOD Workshop on Data Mining and Knowl-
edge Discovery, pages 23?29.
Frederick Mosteller and David L. Wallace. 1964. In-
ference and disputed authorship: The Federalist.
Addison-Wesley.
Philip Resnik. 1995. Using Information Content to Eval-
uate Semantic Similarity in a Taxonomy. In Proceed-
ings of the 14th International Joint Conference on Ar-
tificial Intelligence, pages 448?453.
Helmut Schmid. 1994. Probabilistic part-of-speech tag-
ging using decision trees. In Proceedings of the In-
ternational Conference on New Methods in Language
Processing, pages 44?49.
Efstathios Stamatatos. 2011. Plagiarism detection
using stopword n-grams. Journal of the Ameri-
can Society for Information Science and Technology,
62(12):2512?2527.
Asher Stern and Ido Dagan. 2011. A Confidence
Model for Syntactically-Motivated Entailment Proofs.
In Proceedings of the International Conference on Re-
cent Advances in Natural Language Processing, pages
455?462.
Mildred C. Templin. 1957. Certain language skills in
children. University of Minnesota Press.
William E. Winkler. 1990. String Comparator Metrics
and Enhanced Decision Rules in the Fellegi-Sunter
Model of Record Linkage. In Proceedings of the Sec-
tion on Survey Research Methods, pages 354?359.
Michael J. Wise. 1996. YAP3: Improved detection of
similarities in computer program and other texts. In
Proceedings of the 27th SIGCSE technical symposium
on Computer science education, pages 130?134.
440
