2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 305?314,
Montre?al, Canada, June 3-8, 2012. c?2012 Association for Computational Linguistics
Using Supertags and Encoded Annotation Principles for Improved
Dependency to Phrase Structure Conversion
Seth Kulick and Ann Bies and Justin Mott
Linguistic Data Consortium
University of Pennsylvania
Philadelphia, PA 19104
{skulick,bies,jmott}@ldc.upenn.edu
Abstract
We investigate the problem of automatically
converting from a dependency representa-
tion to a phrase structure representation, a
key aspect of understanding the relationship
between these two representations for NLP
work. We implement a new approach to this
problem, based on a small number of su-
pertags, along with an encoding of some of
the underlying principles of the Penn Tree-
bank guidelines. The resulting system signifi-
cantly outperforms previous work in such au-
tomatic conversion. We also achieve compara-
ble results to a system using a phrase-structure
parser for the conversion. A comparison with
our system using either the part-of-speech tags
or the supertags provides some indication of
what the parser is contributing.
1 Introduction and Motivation
Recent years have seen a significant increase in
interest in dependency treebanks and dependency
parsing. Since the standard training and test set for
English parsing is a phrase structure (PS) treebank,
the Penn Treebank (PTB) (Marcus et al, 1993; Mar-
cus et al, 1994), the usual approach is to convert this
to a dependency structure (DS) treebank, by means
of various heuristics for identifying heads in a PS
tree. The resulting DS representation is then used
for training and parsing, with results reported on the
DS representation.
Our goal in this paper is to go in the reverse di-
rection, from the DS to PS representation, by find-
ing a minimal DS representation from which we can
use an approximate version of the principles of the
PTB guidelines to reconstruct the PS. Work in this
conversion direction is somewhat less studied (Xia
et al, 2009; Xia and Palmer, 2001), but it is still
an important topic for a number of reasons. First,
because both DS and PS treebanks are of current in-
terest, there is an increasing effort made to create
multi-representational treebank resources with both
DS and PS available from the beginning, without a
loss of information in either direction (Xia et al,
2009). Second, it is sometimes the case that it is
convenient to do annotation in a dependency repre-
sentation (e.g., if the annotators are already famil-
iar with such a representation), though the treebank
will in final form be either phrase-structure or multi-
representational (Xia et al, 2009).
However, our concern is somewhat different. We
are specifically interested in experimenting with de-
pendency parsing of Arabic as a step in the annota-
tion of the Arabic Treebank, which is a phrase struc-
ture treebank (Maamouri et al, 2011). Although we
currently use a phrase structure parser in this annota-
tion pipeline, there are advantages to the flexibility
of being able to experiment with advances in pars-
ing technology for dependency parsing. We would
like to parse with a dependency representation of the
data, and then convert the parser output to a phrase
structure representation so that it can feed into the
annotation pipeline. Therefore, in order to make use
of dependency parsers, we need a conversion from
dependency to phrase structure with very high accu-
racy, which is the goal of this paper.
While one of our underlying concerns is DS to
PS conversion for Arabic, we are first focusing on
305
a conversion routine for the English PTB because it
is so well-established and the results are easier to
interpret. The intent is then to transfer this conver-
sion algorithm work to the Arabic treebank as well.
We expect this to be successful because the ATB has
some fundamental similarities to the PTB in spite of
the language difference (Maamouri and Bies, 2004).
As mentioned above, one goal in our DS to PS
conversion work is to base it on a minimal DS rep-
resentation. By ?minimal?, we mean that it does
not include information that is redundant, together
with our conversion code, with the implicit informa-
tion in the dependency structure itself. As discussed
more in Section 2.1, we aim to make our dependency
representation simpler than ?hybrid? representations
such as Johansson and Nugues (2007). The rea-
son for our interest in this minimal representation
is parsing. We do not want to require the parser to
recover such a complex dependency representations,
when it is, in fact, unnecessary, as we believe our ap-
proach shows. The benefit of this approach can only
be seen when this line of work is extended to ex-
periments with parsing and Arabic conversion. The
work described here is just the first step in this pro-
cess.
A conversion scheme, such as ours, necessarily
relies on some details of the annotation content in
the DS and PS representations, and so our algorithm
is not an algorithm designed to take as input any ar-
bitrary DS representation. However, the fundamen-
tals of our dependency representation are not radi-
cally different than others - e.g. we make an auxil-
iary verb the child of the main verb, instead of the
other way, but such choices can be adjusted for in
the conversion.
To evaluate the success of this conversion algo-
rithm, we follow the same evaluation procedure as
Xia et al (2009) and Xia and Palmer (2001). We
convert the PTB to a DS, and then use our algorithm
to convert the DS back to a PS representation. The
original PS and the converted-from-DS PS are then
compared, in exactly the same way as parser output
is compared with the original (gold) tree. We will
show that our results in this area are a significant
improvement above previous efforts.
A key aspect of this work is that our DS-to-PS
conversion encodes many of the properties of the
PTB annotation guidelines (Bies et al, 1995), both
globally and for specific XP projections. The PTB
guidelines are built upon broad decisions about PS
representation that provide an overall framework
and cohesion for the details of the PS trees. To
implement these underlying principles of the guide-
lines, we defined a set of 30 ?supertags? that indi-
cate how a lexical item can project in the syntac-
tic structure, allowing us to specify these principles.
We describe these as supertags because of a concep-
tual similarity to the supertagging work in the Tree
Adjoining Grammar (TAG) tradition (Bangalore and
Joshi, 2010), although ours is far smaller than a typ-
ical supertag set, and indeed is actually smaller than
the PTB POS tag set.
Our DS-to-PS code is based on this set of su-
pertags, and can be run using either the supertags
created from the gold POS tags, or using the POS
tags, together with the dependency structure to first
(imperfectly) derive the supertags, and then proceed
with the conversion. This choice of starting point al-
lows us to measure the impact of POS tag complex-
ities on the DS-to-PS conversion, which provides an
interesting insight on what a phrase structure parser
contributes in addition to this sort of automated DS-
to-PS conversion, as discussed in Section 4.
We have chosen this approach of encoding under-
lying principles of the PTB guidelines for two rea-
sons. First, these principles are non-statistical, and
thus we felt it would let us tease apart the contri-
bution of the frequency information relating, e.g.,
heads, on the one hand, and the basic notions of
phrase structure on the other. The second reason is
that it was quite easy to implement these principles.
We did not attempt a complete examination of every
possible rule in Bies et al (1995), but rather just se-
lected the most obvious ones. As we will see in Sec-
tion 4.2, our results indeed are sometimes hurt by
such lack of thoroughness, although in future work
we will make this more complete.
2 Overview and Example
Figures 1-4 provide a running example of the four
steps in the process. Figure 1 is the original tree
from the Penn Treebank. Figures 2 and 3 illustrate
the two-step process of creating the dependency rep-
resentation, and Figure 4 shows the conversion back
to phrase structure.
306
SADVP
RB
Aside
PP
IN
from
NP
NNP
GM
NP-SBJ
JJ
other
NN
car
NNS
makers
VP
VBD
posted
NP
ADJP
RB
generally
VBN
mixed
NNS
results
Figure 1: Penn Treebank tree
postedVP
S
AsideADVP
fromPP
GMNP-OBJ
makersNP-SBJother car
resultsNP-OBJ
generally
mixedADJP
Figure 2: Tree Insertion Grammar decomposition of Figure
1
VBD/P VP
posted
RB/P ADVP
Aside
IN/P PP
from
NNP/P NP-OBJ
GM
NNS/P NP-SBJ
makers
JJ/P ADJP
other
NN/P PRENOM
car
NNS/P NP-OBJ
results
VBN/P ADJP
mixed
RB/P ADVP
generally
Figure 3: Dependency representation derived from TIG de-
composition in Figure 2
S
ADVP
RB
Aside
PP
IN
from
NP-OBJ
NNP
GM
NP-SBJ
JJ
other
NN
car
NNS
makers
VP
VBD
posted
NP-OBJ
ADJP
RB
generally
VBN
mixed
NNS
results
Figure 4: Conversion of dependency representation in Fig-
ure 3 back to phrase structure.
2.1 Creation of Dependency Representation
The creation of the dependency representation is
similar in basic aspects to many other approaches, in
that we utilize some basic assumptions about head
relations to decompose the full tree into smaller
units. However, we first decompose the original
trees into a Tree Insertion Grammar representation
(Chiang, 2003), utilizing tree substitution and sister
adjunction. We refer the reader to Chiang (2003) for
details of these operations, and instead focus on the
fact that the TIG derivation tree in Figure 2 parti-
tions the phrase structure representation in Figure 1
into smaller units, called elementary trees. We leave
out the POS tags in Figure 2 to avoid clutter.
The creation of the dependency representation is
structurally a simple rewrite of the TIG derivation,
taking the word associated with each elementary tree
and using it as a node in the dependency tree. In
this way, the dependency representation in Figure 3
follows immediately from Figure 2.
However, in addition, we utilize the TIG deriva-
tion tree and the structures of the elementary trees to
create a supertag (in the sense discussed in Section
1) for each word. For example, aside heads an ele-
mentary tree that projects to ADVP, so it is assigned
the supertag P ADVP in Figure 3, meaning that it
projects to ADVP. We label each node in Figure 3
with both its POS tag and supertag, so in this case
the node for aside has RB/P ADVP.
There are two typical cases that are not so
straightforward. The first concerns elementary trees
with more than one level of projection, such as that
for the verb, posted, which has two levels of pro-
jection, S and VP. In such cases we base the supertag
only on the immediate parent of the word. For ex-
ample, in this case the supertag for posted is P VP,
rather than P S. As will be seen in Section 3.2, our
perspective is that the local context of the depen-
dency tree will provide the necessary disambigua-
tion as to what node is above the VP.
307
Projection Type Supertag
NP P NP
ADJP P ADJP
ADVP P ADVP
PP P PP, P WHPP
S,SINV,SQ P VP
QP,NP,QP-NP,QP-ADJP P QP
WHNP P WHNP
default P WHADVP, P INTJ, P PRT, P LST
none P AUX, P PRENOM, P DET, P COMMA, P PERIOD, P CC, P COMP,
P POS, P PRP$, P BACKDQUOTE, P DQUOTE, P COLON, P DOLLAR,
P LRB, P RB, P PDT, P SYM, P FW, P POUND
Table 1: 30 supertags handled by 14 projection types. The ambiguity in some, such as P VP projecting as S, SINV,
SQ is handled by an examination of the dependency structure.
The second non-straightforward case1 is that of
degenerate elementary trees, in which the ?tree?
is just the word itself, as for other, car, and
generally. In such cases we default the supertag
based on the original POS tag, and in some cases, the
tree configuration. For example, a word with the JJ
tag, such as other, would get the supertag P ADJP,
with the RB tag such as generally the supertag
P ADVP. We assign prenominal nouns such as car
here the tag P PRENOM.
Generating supertags in this way is a convenient
way to correct some of the POS tag errors in the PTB
(Manning, 2011). For example, if that has the (in-
correct) tag DT in the complementizer position, it
still receives the new POS tag P COMP.
This procedure results in a set of 30 supertags, and
Table 1 shows how they are partitioned into 14 pro-
jection types. These supertags and projection types
are the basis of our DS-to-PS conversion, as dis-
cussed further in Section 2.2.
We note here a brief comparison with earlier work
on ?hybrid? representations, which encode a PS rep-
resentation inside a DS one, in order to convert from
the latter to the former. (Hall and Nivre, 2008; Jo-
han Hall and Nilsson, 2007; Johansson and Nugues,
2007). Our goal is very different. Instead of en-
1There are other details not discussed here. For example, we
do not automatically assign a P NP supertag to the head child
of an NP, since such a head can legitimately be, e.g, a JJ, in
which case we make the supertag P ADJP, on the reasoning that
it would be encoding ?too much? to treat it as P NP. Instead, we
rely on the DS and such labels as SBJ or OBJ to determine when
to project it as NP in the converted PS.
coding the phrase structure in the dependency tree
via complex tags such as SBARQ in Johansson and
Nugues (2007), we use a minimal representation and
rely on our encoding of the general principles of
PTB phrase structure to carry much of the weight.
While supertags such as P VP may appear to encode
some of the structure, their primary role is as an in-
termediate link between the POS tags and the phrase
structure conversion. The created supertags are not
in fact necessary for this conversion. As we will see
in the following sections, we convert from DS to PS
using either just the original POS tags, or with our
created supertags.
We also include five labels in the dependency rep-
resentation: SBJ, OBJ, PRN, COORD CONJ, APP.
The example dependency tree in Figure 3 includes
instances of the SBJ and OBJ labels, in italics on
the node instead of the edges, for convenience. The
SBJ label is of course already a function tag in the
PTB. We process the PTB when creating the TIG
decomposition to add an OBJ tag, as well basing the
PRN label on the occurrence of the PRN node. We
also use heuristics to identify cases of coordination
and apposition, resulting in the COORD CONJ and
APP tags. The reasons for including these labels is
that they prove useful in the conversion to phrase
structure, as illustrated in some of the examples be-
low.
Before moving on to the dependency-to-phrase-
stucture conversion, we end this section with a com-
ment on the role of function tags and empty cate-
gories. The PTB makes use of function tags to in-
308
dicate certain syntactic and semantic information,
and of empty categories (and co-indexing) for a
more complete and accurate syntactic representa-
tion. There is some overlap between the five la-
bels we use, as just described, and the PTB func-
tion tags, but in general we do not encode the full
range of function tags in our representation, saving
this for future work. More significantly, we also
do not include empty categories and associated co-
indexing, which has the consequence that the depen-
dency trees are projective.
The reason we have not included these aspects in
our representation and conversion yet is that we are
focused here first on the evaluation for comparison
with previous work, and the basis for this previous
work is the usual evalb program (Sekine and Collins,
2008), which ignores function tags and empty cate-
gories. We return to this issue in the conclusion.
2.2 From Dependency to Phrase Structure
There are two key aspects to the conversion from de-
pendency to phrase structure. (1) We encode general
conventions about annotation that are used through-
out the annotation guidelines for the PTB. A com-
mon example is that of the ?single-word? rule, in
which a constituent consisting of just a single word
is reduced to just that word, without the constituent
bracketing, in many cases. (2) We use the set of su-
pertags as the basis for defining projection-specific
rules for how to attach children on the left or right of
the head, in many cases utilizing the supertag names
that we include to determine the specific attachment.
For example, the leaf GM in Figure 3 has the su-
pertag P NP (with the label OBJ), so heading a NP
projection, (NP GM). Its parent node, from, has
the supertag P PP, indicating that it heads a PP pro-
jection, and so attaches the (NP GM) as a sister of
from. It does not reduce it down as a single word,
because the encoding of the PP projection specifies
that it does not do so for children on its right.
A more substantial case is that of the NP other
car makers. Here the head noun, makers,
has the supertag P NP, and so projects as an NP.
Its first child, other, has the supertag P ADJP,
and so projects as an ADJP, resulting in (ADJP
other). The second child, car, has the supertag
P PRENOM (prenominal), and so does not project
at all. When the NP projection for makers is as-
sembled, it applies the ?single-word? constraint to
children on its left (as encoded in the definition
of the NP projection), thus stripping the ADJP off
of other, resulting in the desired flat NP other
car makers. Likewise, the ADVP projection for
generally is stripped off before it is attached as
a left sister of the ADJP projection mixed. The
encoding of a VP projection specifies that it must
project above VP if it is the root of the tree, and so
the VP projection for posted projects to S (by de-
fault).
In this way we can see that encoding some of
the general characteristics of the annotation guide-
lines allows the particular details of the PTB phrase-
structure representation to be created from the less-
specific dependency representation.
3 Some Further Examples
3.1 QP Projection or Reduction
As mentioned in Section 2.2, the ?single word? con-
vention is implemented in the conversion to PS, as
was the case with other in the previous section.
The projection associated with P QP has a slight
twist to this principle, because of the nature of some
of the financialspeak in the PTB. In particular, the
dollar sign is treated as a displaced word and is
therefore not counted, in a QP constituent, as a token
for purposes of the ?single token? rule.
For example, (1abc) in Figure 5 illustrates a case
where the QP structure projects to an NP node as
well. (1a) is the original PTB PS tree, and (1b) is
the DS representation. Note that billion heads
the about $ 9 billion subtree, with the su-
pertag P QP and the label OBJ.2 Because it has more
than one child in addition to the $, it is converted to
phrase structure as a QP under an NP, implying the
empty *U*, although we do not actually put it in.
In contrast, (2abc) is a case in which the QP node
is not generated. 100 is the head of the phrase $ 100
*U* in the PTB PS (a), as shown in the dependency
structure (b). However, because it only has one child
in addition to the $, no additional QP node is cre-
ated in the phrase structure representation in (c). We
stress that the presence of the QP in (1a) and its ab-
2A good case can be made that in fact $ should be the daugh-
ter of to in the dependency tree, although we have not imple-
mented this as such.
309
(1) (A)
PP
TO
to
NP
QP
IN
about
$
$
CD
9
CD
billion
-NONE-
*U*
(B)
P PP
to
P QP-OBJ
billion
P PP
about
P DOLLAR
$
P QP
9
(C)
PP
P PP
to
NP
QP
P PP
about
P DOLLAR
$
P QP
9
P QP
billion
(2) (A)
PP
IN
for
NP
$
$
CD
100
-NONE-
*U*
(B)
P PP
for
P QP-OBJ
100
P DOLLAR
$
(C)
PP
P PP
for
NP-OBJ
P DOLLAR
$
P QP
100
Figure 5: Examples of handling of QP in dependency to phrase-structure conversion.
sence in (2a) is correct annotation, consistent with
the annotation guidelines.
3.2 Refinement of VP Projections
As mentioned above, instead of having separate su-
pertags for S, SINV, SQ, SBAR, SBARQ, we use
only the P VP supertag and let the context determine
the specifics of the projection. Sentences (3ab) in
Figure 6 illustrate how the SBJ label is used to treat
the P VP supertag as indicating projection to SINV
(or SQ) instead of S. The determination is based on
the children of the P VP node. For example, if there
is a child with the P AUX supertag which is before a
child with the SBJ label, which in turn is before the
P VP node itself, then the latter is treated as project-
ing to either SINV or SQ, depending on the some
additional factors, primarily whether there is a WH
word among the children. In this example, there is
no WH word, so it becomes a SINV.3 We note here
that we also include a simple listing of verbs that
take complements of certain types - such as verbs of
saying, etc., that take SBAR complements, so that a
VP will project not just to S, but SBAR, even if the
complement is missing.
3.3 Coordination
We represent coordination in the dependency in
one of the standard ways, by making the follow-
ing conjuncts be children of the head word of
3This is not a fully precise implementation of the condi-
tions distinguishing SQ and SINV projections, in that it does
not properly check for whether the clause is a question.
(3) (A)
P VP
absorbed
P AUX
had
P NP-SBJ
cost
P DET
the
P VP
been
(B)
SINV
VBD
had
NP-SBJ
DT
the
NN
cost
VP
VBN
been
VP
VBN
absorbed
Figure 6: (3ab) shows that the local context of the P VP
supertag in the dependency tree results in a SINV struc-
ture in the converted phrase structure tree (3b).
the first conjunct. For example, a dependency
representation of ...turn down the volume
and close the curtains is shown in (4a) in
Figure 7. The conjunct close the curtains
is converted as a VP projection projecting to S. How-
ever, when the projection for turn is assembled, the
code checks if the conjuncts are missing subjects,
and if so, reduces the configuration to standard VP
coordination, as in (4b). The COORD label is used
to identify such structures for examination.
4 Results of Dependency to Phrase
Structure Conversion
To evaluate the correctness of conversion from de-
pendency to phrase structure, we follow the same
strategy as Xia and Palmer (2001) and Xia et al
(2009). We convert the phrase structure trees in the
PTB to dependency structure and convert the depen-
dency back to phrase structure. We then compare
the original PTB trees with the newly-created phrase
310
(4) (A)
P VP
turn
P PRT
down
P NP
volume
P DET
the
P CC
and
P VP-COORD
close
P NP-OBJ
curtains
P DET
the
(B)
VP
VP
turn PRT
down
NP-OBJ
the volume
and VP
close NP-OBJ
the curtains
Figure 7: (4a) is the dependency representation of a coordination structure, and the resulting phrase structure (4b)
shows that the conversion treated it as VP coordination, due to the absence of a subject.
Sec System rec prec f
00 Xia & Palmer ?01 86.2 88.7 87.5
Xia et al ?09 91.8 89.2 90.5
USE-POS-UNLABEL 96.6 97.4 97.0
USE-POS 94.6 95.4 95.0
USE-SUPER 95.9 97.0 96.4
22 Xia et al ?09 90.7 88.1 89.4
USE-POS 95.0 95.5 95.3
USE-SUPER 96.4 97.1 96.7
23 Wang & Zong ?10 95.9 96.3 96.1
USE-POS 94.8 95.7 95.3
USE-SUPER 96.2 97.3 96.7
24 USE-POS 94.0 94.7 94.4
USE-SUPER 95.9 97.1 96.5
Table 2: Results of dependency to phrase structure con-
version. For our system, the results are presented in two
ways, using either the gold part-of-speech tags (USE-
POS) or our gold supertags (USE-SUPER). For purposes
of comparison with Xia and Palmer (2001) and Xia et
al. (2009), we also present the results for Section 00 us-
ing part-of-speech tags, but with an unlabeled evaluation
(USE-POS-UNLABEL).
structure trees, using the standard evalb scoring code
(Sekine and Collins, 2008). Xia and Palmer (2001)
defined three different algorithms for the conversion,
utilizing different heuristics for how to build projec-
tion chains, and where to attach dependent subtrees.
They reported results for their system for Section 00
of the PTB, and we include in Table 2 only their
highest scoring algorithm. The system of Xia et al
(2009) uses conversion rules learned from Section
19, and then tested on Sections 00 and Section 22.
We developed the algorithm using Section 24, and
we also report results for Sections 00, 22, and 23, for
comparison with previous work. We ran our system
in two ways. In one we use the ?gold? supertags
that were created as described in Section 2.1 (USE-
SUPER), based on the TIG decomposition of the
original tree. In the other (USE-POS) we use the
gold POS tags, and not the supertags. Because our
DS-to-PS algorithm is based on using the supertags
to guide the conversion, the USE-POS runs work
by using a few straightforward heuristics to guess
the correct supertag from the POS tag and the de-
pendency structure. For example, if a word x has
the POS tag ?TO? and the word y to its immediate
right is its parent in the dependency tree and y has
one of the verbal POS tags, then x receives the su-
pertag P AUX, and otherwise P PP. Any word with
the POS tag JJ, JJR, or JJS, receives the supertag
P ADJP, and so on. The results for Xia and Palmer
(2001) and Xia et al (2009) were reported using an
unlabeled version of evalb, so to compare properly
we also report our results for Section 00 using an
unlabeled evaluation of the run using the POS tags
(USE-POS-UNLABEL), while all the other results
use a labeled evaluation.
We also compare our system with that of Wang
and Zong (2010). Unlike the three other systems
(including ours), this was not based on an automatic
conversion from a gold dependency tree to phrase
structure, but rather used the gold dependency tree
as additional input for a phrase structure parser (the
Berkeley parser).
4.1 Analysis
While our system was developed using Section 24,
the f-measure results for USE-SUPER are virtually
identical across all four sections (96.4, 96.7, 96.7,
96.5). Interestingly, there is more variation in the
311
USE-POS results (95.0, 95.3, 95.3, 94.4). We take
this to be an indication of a difference in the sec-
tions as to the utility of the POS tags to ?bootstrap?
the syntactic structure. As just mentioned above, the
USE-POS runs work by using heuristics to approxi-
mate the gold supertags from the POS tags.
The supertags, because they are partially derived
from the phrase structure, can obscure a discon-
nect between a POS tag and the syntactic structure
it projects. For example, the word according
in the structure (PP (VBG according) (PP
(TO to) ...)) receives the gold supertag P PP,
a more explicit representation of the word?s role in
the structure than the ambiguous VBG. This is why
the USE-POS score is lower than the USE-SUPER
score, since the POS tag and dependency structure
do not always, at least with our simple heuristics,
lead to the gold supertag. For example, in the USE-
POS run, according receives the incorrect su-
pertag P VP, leading to an incorrect structure, while
in the USE-SUPER run, it is able to use P PP, lead-
ing to the correct structure.
However, even with the lower performance of
USE-POS, it is well above the results reported in Xia
et al (2009) for Section 22, and even more so with
the unlabeled evaluation of Section 00 compared to
Xia and Palmer (2001) and Xia et al (2009). The
comparison with Wang and Zong (2010) for Section
23 (they did not report results for any other section)
shows something very different, however. Their re-
sult, using a gold dependency tree together with the
Berkeley parser, is above our USE-POS version and
below our USE-SUPER version.
Our interpretation of this is that it provides an
indication of what the parser is providing on top
of the gold dependency structure, which is roughly
the same information that we have encoded in our
DS to PS code. However, because the Wang and
Zong (2010) system performs better than our USE-
POS version, it is likely learning some of the non-
straightforward cases of how USE-POS tags can
bootstrap the syntactic structure that our USE-POS
version is missing. However, any conclusions must
be tentative since our dependency structures are not
necessarily the same as theirs and so it is not an ex-
act comparison.
Error type count
problem with PTB annotation 8
ambiguous ADVP placement 3
incorrect use of ?single token rule? 3
FRAG/X 2
multiple levels of recursion 2
other 5
Table 3: Analysis of errors in first 50 sentences of USE-
SUPER run for Section 24
4.2 Errors from Dependency Structure with
Supertags to Phrase Structure
We stressed in the introduction that we are interested
in understanding better the relationship between the
DS and PS representations. Identifying areas where
the conversion from DS did not result in a perfect
(evalb score) PS is therefore of particular interest.
For this analysis, we used our dev section, 24,
with the run USE-SUPER. We use this run because
we are interested in cases where, even with the gold
supertags, there was still a problem with the conver-
sion to the PS. We examined the first 50 sentences in
the section, with a total of 23 errors. We recognize
that this is a very small sample. An eyeball exam-
ination of other sentences does not reveal anything
significantly different than what we present here as
far as the sorts of errors, although we have only per-
formed a rigorous analysis of these 23 errors, which
is why we limit our discussion here to these cases.
Table 3 shows a breakdown of these 23 errors.
Note that by ?error? here we mean a difference be-
tween the reconstructed PS structure, and the PTB
gold PS structure, causing the score for Section 24,
USE-SUPER (last row) in Table 2 to be less than
perfect.
The most common ?error? is that in which the
PTB annotation is itself in error, while our algo-
rithm actually creates a correct phrase structure, in
the sense that it is consistent with the PTB guide-
lines. Three of these eight annotation problems are
of the same type, in which a NP is headed by a word
with the RB tag. An example is shown in (5) in
which (5a) shows (a fragment of) the original tree
in the PTB, and (5b) is the resulting DS, with (5c)
the reconstructed PS tree. The word here receives
the supertag P ADVP, thus resulting in a different re-
312
constructed PS, with an ADVP. There is a mismatch
between the POS tag and the node label in the origi-
nal tree (5a), and in fact in this case the node label in
the PTB tree should have been ADVP-LOC, instead
of NP-LOC.
(5) (A)
VP
VBD
premiered
NP-LOC
RB
here
(B)
P VP
premiered
P ADVP
here
(C)
VP
VBD
premiered
ADVP
RB
here
(6) (A)
S
NP-SBJ
-NONE-
VP
ADVP-MNR
RB
frantically
VBG
selling
NP
NNS
bonds
(B)
P VP
selling
P ADVP
frantically
P NP
bonds
(C)
S
ADVP
P ADVP
frantically
VP
VBG
selling
NP
NNS
bonds
An example of the ?ambiguous ADVP place-
ment? error is shown in (6), in which the PTB tree
has the adverb frantically inside the VP, infor-
mation which is not available in the DS (6b). Our
conversion code has to choose as to where to put
such ADVPs, and it puts them outside the VP, as in
(6c), which is sometimes correct, but not in this case.
5 Conclusion and Future Work
In this work we have described an approach to auto-
matically converting DS to PS with significantly im-
proved accuracy over previous efforts, and compara-
ble results to that of using a phrase structure parser
guided by the dependency structure.
Following the motivation discussed in Section 1,
the next step is straightforward - to adapt the al-
gorithm to work on conversion from a dependency
representation of the Arabic Treebank to the phrase
structure representation necessary for the annotation
pipeline. Following this, we will then experiment
with parsing the Arabic dependency representation,
converting to phrase structure, and evaluating the re-
sulting phrase structure representation as usual for
parsing evaluation. We will also experiment with
dependency parsing for the PTB dependency repre-
sentation discussed in this paper. Habash and Roth
(2009) discuss an already-existing dependency rep-
resentation of parts of the ATB and it will be inter-
esting to compare the conversion accuracy using the
different dependency representations, although we
expect that there will not be any major differences
in the representations.
One other aspect of future work is to implement
the algorithm in Wang and Zong (2010), using our
own dependency representation, since this would al-
low a precise investigation of what the phrase struc-
ture parser is contributing as compared to our auto-
matic conversion. We note that this work also ex-
perimented with dependency parsing, and then auto-
matically converting the results to PS, a further basis
of comparison.
Finally, we would like to stress that while we have
used evalb for scoring the converting PS because it
is the standard evaluation for PS work, it is a very
insufficient standard for this work. As discussed at
the end of Section 2, we have not included all the
function tags or empty categories in our representa-
tion, a significant omission. We would like to ex-
pand our dependency representation to allow all the
function tags and empty categories to be included
in the converted PS. Our plan is to take our anal-
ogy to TAG more seriously (e.g., (Joshi and Ram-
bow, 2003)) and use a label akin to adjunction to en-
code leftward (non-projective) movement in the tree,
also using an appropriate dependency parser as well
(Shen and Joshi, 2008).
Acknowledgements
This work was supported in part by the Defense Ad-
vanced Research Projects Agency, GALE Program
Grant No. HR0011-06-1-0003. The views, opinions
and/or findings contained in this article/presentation
are those of the author/presenter and should not be
interpreted as representing the official views or poli-
cies, either expressed or implied, of the Defense Ad-
vanced Research Projects Agency or the Department
of Defense. We would also like to thank Mohamed
Maamouri, Colin Warner, Aravind Joshi, and Mitch
Marcus for valuable conversations and feedback.
313
References
Srinivas Bangalore and Aravind K. Joshi, editors. 2010.
Supertagging: Using Complex Lexical Descriptions in
Natural Language Processing. MIT Press.
Ann Bies, Mark Ferguson, Karen Katz, and Robert Mac-
Intyre. 1995. Bracketing guidelines for Treebank II-
style Penn Treebank project. Technical Report MS-
CIS-95-06, University of Pennsylvania.
David Chiang. 2003. Statistical parsing with an auto-
matically extracted Tree Adjoining Grammar. In Data
Oriented Parsing. CSLI.
Nizar Habash and Ryan Roth. 2009. CATiB: The
Columbia Arabic Treebank. In Proceedings of the
ACL-IJCNLP 2009 Conference Short Papers, pages
221?224, Suntec, Singapore, August. Association for
Computational Linguistics.
Johan Hall and Joakim Nivre. 2008. A dependency-
driven parser for German dependency and con-
stituency representations. In Proceedings of the Work-
shop on Parsing German, pages 47?54, Columbus,
Ohio, June. Association for Computational Linguis-
tics.
Joakim Nivre Johan Hall and Jens Nilsson. 2007. Hy-
brid constituency-dependency parser for Swedish. In
Proceedings of NODALIDA, Tartu, Estonia.
Richard Johansson and Pierre Nugues. 2007. Extended
constituent-to-dependency conversion for english. In
Proceedings of NODALIDA, Tartu, Estonia.
Aravind Joshi and Owen Rambow. 2003. A formal-
ism for dependency grammar based on Tree Adjoin-
ing Grammar. In Proceedings of the Conference on
Meaning-Text Theory, Paris, France.
Mohamed Maamouri and Ann Bies. 2004. Developing
an arabic treebank: Methods, guidelines, procedures,
and tools. In Ali Farghaly and Karine Megerdoomian,
editors, COLING 2004 Computational Approaches to
Arabic Script-based Languages, pages 2?9, Geneva,
Switzerland, August 28th. COLING.
Mohamed Maamouri, Ann Bies, and Seth Kulick. 2011.
Upgrading and enhancing the Penn Arabic Treebank.
In Joseph Olive, Caitlin Christianson, and John Mc-
Cary, editors, Handbook of Natural Language Pro-
cessing and Machine Translation: DARPA Global Au-
tonomous Language Exploitation. Springer.
Christopher Manning. 2011. Part-of-speech tagging
from 97% to 100%: Is it time for some linguistics?
In Alexander Gelbukh, editor, Computational Linguis-
tics and Intelligent Text Processing, 12th International
Conference, CICLing 2011, Proceedings, Part I. Lec-
ture Notes in Computer Science 6608.
Mitchell P. Marcus, Beatrice Santorini, and Mary Ann
Marcinkiewicz. 1993. Building a large annotated cor-
pus of English: The Penn Treebank. Computational
linguistics, 19:313?330.
Mitchell Marcus, Grace Kim, Mary Ann Marcinkiewicz,
Robert MacIntyre, Ann Bies, Mark Ferguson, Karen
Katz, and Britta Schasberger. 1994. The Penn Tree-
bank: Annotating predicate argument structure. In
Proceedings of HLT.
Satoshi Sekine and Michael Collins. 2008. Evalb.
http://nlp.cs.nyu.edu/evalb/.
Libin Shen and Aravind Joshi. 2008. LTAG dependency
parsing with bidirectional incremental construction.
In Proceedings of the 2008 Conference on Empirical
Methods in Natural Language Processing, pages 495?
504, Honolulu, Hawaii, October. Association for Com-
putational Linguistics.
Zhiguo Wang and Chengqing Zong. 2010. Phrase struc-
ture parsing with dependency structure. In COLING
2010: Posters, pages 1292?1300, Beijing, China, Au-
gust.
Fei Xia and Martha Palmer. 2001. Converting depen-
dency structures to phrase structures. In HLT-2001.
Fei Xia, Owen Rambow, Rajesh Bhatt, Martha Palmer,
and Dipti Misra Sharma. 2009. Towards a multi-
representational treebank. In Proceedings of the Work-
shop on Treebanks and Linguistic Theories.
314
Proceedings of NAACL-HLT 2013, pages 550?555,
Atlanta, Georgia, 9?14 June 2013. c?2013 Association for Computational Linguistics
Using Derivation Trees for Informative Treebank Inter-Annotator
Agreement Evaluation
Seth Kulick and Ann Bies and Justin Mott
and Mohamed Maamouri
Linguistic Data Consortium
University of Pennsylvania
{skulick,bies,jmott,maamouri}
@ldc.upenn.edu
Beatrice Santorini and
Anthony Kroch
Department of Linguistics
University of Pennsylvania
{beatrice,kroch}
@ling.upenn.edu
Abstract
This paper discusses the extension of a sys-
tem developed for automatic discovery of tree-
bank annotation inconsistencies over an entire
corpus to the particular case of evaluation of
inter-annotator agreement. This system makes
for a more informative IAA evaluation than
other systems because it pinpoints the incon-
sistencies and groups them by their structural
types. We evaluate the system on two corpora
- (1) a corpus of English web text, and (2) a
corpus of Modern British English.
1 Introduction
This paper discusses the extension of a system de-
veloped for automatic discovery of treebank annota-
tion inconsistencies over an entire corpus to the par-
ticular case of evaluation of inter-annotator agree-
ment (IAA). In IAA, two or more annotators anno-
tate the same sentences, and a comparison identi-
fies areas in which the annotators might need more
training, or the annotation guidelines some refine-
ment. Unlike other IAA evaluation systems, this
system application results in a precise pinpointing of
inconsistencies and the grouping of inconsistencies
by their structural types, making for a more infor-
mative IAA evaluation.
Treebank annotation, consisting of syntactic
structure with words as the terminals, is by its na-
ture more complex and therefore more prone to error
than many other annotation tasks. However, high an-
notation consistency is crucial to providing reliable
training and testing data for parsers and linguistic
research. Error detection is therefore an important
area of research, and the importance of work such as
Dickinson and Meurers (2003) is that errors and an-
notation inconsistencies might be automatically dis-
covered, and once discovered, be targeted for subse-
quent quality control.
A recent approach to this problem (Kulick et al,
2011; Kulick et al, 2012) (which we will call the
KBM system) improves upon Dickinson and Meur-
ers (2003) by decomposing the full syntactic tree
into smaller units, using ideas from Tree Adjoining
Grammar (TAG) (Joshi and Schabes, 1997). This al-
lows the comparison to be based on small syntactic
units instead of string n-grams, improving the detec-
tion of inconsistent annotation.
The KBM system, like that of Dickinson and
Meurers (2003) before it, is based on the notion of
comparing identical strings. In the general case, this
is a problematic assumption, since annotation in-
consistencies are missed because of superficial word
differences between strings which one would want
to compare.1 However, this limitation is not present
for IAA evaluation, since the strings to compare are,
by definition, identical.2 The same is also true of
parser evaluation, since the parser output and the
gold standard are based on the same sentences.
We therefore take the logical step of applying the
KBM system developed for automatic discovery of
annotation inconsistency to the special case of IAA.3
1Boyd et al (2007) and other current work tackles this prob-
lem. However, that is not the focus of this paper.
2Aside from possible tokenization differences by annotators.
3In this paper, we do not yet apply the system to parser eval-
uation, although it is conceptually the same problem as IAA
evaluation. We wanted to first refine the system using annota-
tor input for the IAA application before applying it to parser
550
(1) a. NP-SBJ
NP
NP
The word
NP
renaissance
NP
-LRB- NP
Rinascimento
PP
in NP
Italian
-RRB-
b. NP-SBJ
The word renaissance PRN
-LRB- FRAG
NP
Rinascimento
PP
in NP
Italian
-RRB-
Figure 1: Two example trees showing a difference in IAA
To our knowledge, this work is the first to utilize
such a general system for this special case.
The advantages of the KBM system play out
somewhat differently in the context of IAA evalu-
ation than in the more general case. In this con-
text, the comparison of word sequences based on
syntactic units allows for a precise pinpointing of
differences. The system also retains the ability to
group inconsistencies together by their structural
type, which we have found to be useful for the more
general case. Together, these two properties make
for a useful and informative system for IAA evalua-
tion.
In Section 2 we describe the basic working of our
system. In Section 3 we discuss in more detail the
advantages of this approach. In Section 4 we evalu-
ate the system on two treebanks, a corpus of English
web text and a corpus of Modern British English.
Section 5 discusses future work.
2 System Overview
The basic idea of the KBM system is to detect word
sequences that are annotated in inconsistent ways by
evaluation.
wordNP
The RinascimentoNP -RRB--LRB-renaissance
NP
renaissanceNPb1The -RRB--LRB- Rinascimento
FRAGword PRN
a1
a2
a3
a4
a5
a8
b2
b3 b5
b4 b8
inPPa6 ItalianNP a7
inPP b6 ItalianNPb7
NP
A
A
A A A A
A
A
MM MM M
A
(2) a.
b.
Figure 2: E-trees and derivation trees corresponding to
(1ab)
comparing local syntactic units. Following Dickin-
son and Meurers (2003), we refer to sequences ex-
amined for inconsistent annotation as nuclei. The
sentence excerpts (1ab) in Figure 1, from the test
corpora used in this work, illustrate an inconsistency
in the annotation of corresponding strings. We fo-
cus here on the difference in the annotation of the
nucleus The word renaissance, which in (1a) is an-
notated as an appositive structure, while in (1b) it is
flat.
Following the TAG approach, KBM decomposes
the full phrase structure into smaller chunks called
elementary trees (henceforth, e-trees). The relation-
ship of the e-trees underlying a full phrase struc-
ture to each other is recorded in a derivation tree,
in which each node is an e-tree, related to its par-
ent node by a composition operation, as shown in
(2ab).4
KBM uses two composition operations, each with
left and right variants, shown in Figure 3: (1) ad-
4The decomposition is based on head-finding heuristics,
with the result here that word is the head of (1a), while renais-
sance is the head of (1b), as reflected in their respective deriva-
tion trees (2a) and (2b). We omit the POS tags in (1ab) and
(2ab) to avoid clutter.
551
wo ro wo wo ro
dNPTheRinaschNmswot wo
-BLhebN1hBFeRinaschNms
ro t ro
-BLheRinaschNmswot wo
dNPThebN1hBFeRinaschNmsro
tro
woro ro wo wo
Figure 3: Composition operations (left and right)
junction, which attaches one tree to a target node in
another tree by creating a copy of the target node,
and (2) sister adjunction, which attaches one tree as
a sister to a target node in another tree. Each arc in
Figure 2 is labeled by an ?M? for adjunction and ?A?
for sister-adjunction. 5
The system uses the tree decomposition and re-
sulting derivation tree for the comparison of differ-
ent instances of the same nucleus. The full deriva-
tion tree for a sentence is not used, but rather only
that slice of it having e-trees with words that are in
the nucleus being examined, which we call a deriva-
tion tree fragment. That is, for a given nucleus with
a set of instances, we compare the derivation frag-
ments for each instance.
For example, for the nucleus The word renais-
sance, the derivation tree fragment for the instance
in (1a) consists of the e-trees a1, a2, a3 (and their
arcs) in (2a), and likewise the derivation tree from
the instance in (1b) consists of the e-trees b1, b2, b3
in (2b). These derivation fragments have a differ-
ent structure, and so the two instances of The word
renaissance are recognized as inconsistent.
Two important aspects of the overall system re-
quire mention here: (1) Nuclei are identified by us-
ing sequences that occur as a constituent anywhere
5KBM is based on a variant of Spinal TAG (Shen et al,
2008), and uses sister adjunction without substitution. Space
prohibits full discussion, but multiple adjunction to a single
node (e.g., a4, a6, a8 to a5 in (2a)) does not create multiple
levels of recursion, while a special specification handles the ex-
tra NP recursion for the apposition with a2, a3, and a5. For
reasons of space, we also leave aside a precise comparison to
Tree Insertion Grammar (Chiang, 2003) and Spinal TAG (Shen
et al, 2008).
in the corpus, even if other instances of the same
sequence are not constituents. Both instances of
The word renaissance are compared, because the
sequence occurs at least once as a constituent. (2)
We partition each comparison of the instances of a
nucleus by the lowest nonterminal in the derivation
tree fragment that covers the sequence. The two in-
stances of The word renaissance are compared be-
cause the lowest nonterminal is an NP in both in-
stances.
3 Advantages of this approach
As Kulick et al (2012) stressed, using derivation
tree fragments allows the comparison to abstract
away from interference by irrelevant modifiers, an
issue with Dickinson and Meurers (2003). However,
in the context of IAA, this advantage of KBM plays
out in a different way, in that it allows for a pre-
cise pinpointing of the inconsistencies. For IAA,
the concern is not whether an inconsistent annota-
tion will be reported, since at some level higher in
the tree every difference will be found, even if the
context is the entire tree. KBM, however, will find
the inconsistencies in a more informative way, for
example reporting just The word renaissance, not
some larger unit. Likewise, it reports Rinascimento
in Italian as an inconsistently annotated sequence.6
A critical desirable property of KBM that carries
over from the more general case is that it allows for
different nuclei to be grouped together in the sys-
tem?s output if they have the same annotation in-
consistency type. As in Kulick et al (2011), each
nucleus found to be inconsistent is categorized by
an inconsistency type, which is simply the collec-
tion of different derivation tree fragments used for
the comparison of its instances, including POS tags
but not the words. For example, the inconsistency
type of the nucleus The word renaissance in (1ab) is
the pair of derivation tree fragments (a1,a2,a3) and
(b1,b2,b3) from (2ab), with the POS tags. This nu-
6Note however that it does not report -LRB- Rinascimento
in Italian -RRB- which is also a constituent, and so might be
expected to be compared. The lowest nonterminal above this
substring in the two derivation trees in Figure 2 is the NP in a5
and the FRAG in b5, thus exempting them from comparison. It
is exactly this sort of case that motivated the ?external check?
discussed in Kulick et al (2012), which we have not yet imple-
mented for IAA.
552
Inconsistency type # Found # Accurate
Function tags only 53 53
POS tags only 18 13
Structural 129 122
Table 1: Inconsistency types found for system evaluation
cleus is then reported together with other nuclei that
use the same derivation fragments. In this case, it
therefore also reports the nucleus The term renais-
sance, which appears elsewhere in the corpus with
the two annotations from the different annotators as
in (3):
(3) a. NP
NP
The term
NP
renaissance
b. NP
The term renaissance
KBM reports The word renaissance and The term
renaissance together because they are inconsistently
annotated in exactly the same way, in spite of the dif-
ference in words. This grouping together of incon-
sistencies based on structural characteristics of the
inconsistency is critically important for understand-
ing the nature of the annotation inconsistencies.
It is the combination of these two characteristics -
(1) pinpointing of errors and (2) grouping by struc-
ture - that makes the system so useful for IAA. This
is an improvement over alternatives such as using
evalb (Sekine and Collins, 2008) for IAA. No other
system to our knowledge groups inconsistencies by
structural type, as KBM does. The use of the deriva-
tion tree fragments greatly lessens the multiple re-
porting of a single annotation difference, which is
a difficulty for using evalb (Manning and Schuetze,
1999, p. 436) or Dickinson and Meurers (2003).
4 Evaluation
4.1 English web text
We applied our approach to pre-release subset of
(Bies et al, 2012), dually annotated and used for
annotator training, from which the examples in Sec-
tions 2 and 3 are taken. It is a small section of the
corpus, with 4,270 words dually annotated.
For this work, we also took the further step of
characterizing the inconsistency types themselves,
allowing for an even higher-level view of the incon-
sistencies found. In addition to grouping together
different strings as having the same inconsistent an-
notation, the types can also be grouped together for
comparison at a higher level. For this IAA sample,
we separated the inconsistency types into the three
groups in Table 1, with the derivation tree fragments
differing (1) only on function tags, (2) only on POS
tags7, and (3) on structural differences. We man-
ually examined each inconsistency group to deter-
mine if it was an actual inconsistency found, or a
spurious false positive. As shown in Table 1, the pre-
cision of the reported inconsistencies is very high.
It is in fact even higher than it appears, because
the seven (out of 129) instances incorrectly listed
as structural problems were actually either POS or
function tag inconsistencies, that were discovered
by the system only by a difference in the derivation
tree fragment, and so were categorized as structural
problems instead of POS or function tag inconsis-
tencies. 8
Because of the small size of the corpus, there
are relatively few nuclei grouped into inconsistency
types. The 129 structural inconsistency types in-
clude 130 nuclei, with the only inconsistency type
with more than one nucleus being the type with The
word renaissance and The term renaissance, as dis-
cussed above. There is more grouping together in
the ?POS tags only? case (37 nuclei included in
the 18 inconsistency types), and the ?function tags
only? case (56 nuclei included in the 53 inconsis-
tency types).
4.2 Modern British English corpus
We also applied our approach to a supplemental sec-
tion (Kroch and Santorini, in preparation) to a cor-
pus of modern British English (Kroch et al, 2010),
part of a series of corpora used for research into lan-
guage change. The annotation style is similar to that
of the Penn Treebank, although with some differ-
ences. In this case, because neither the function tags
nor part-of-speech tags were part of the IAA work,
7As mentioned in footnote 4, although POS tags were left
out of Figure 2 for readability, they are included in the actual e-
trees. This allows POS differences in a similar syntactic context
to be naturally captured within the overall KBM framework.
8A small percentage of inconsistencies are the result of lin-
guistic ambiguities and not an error by one of the annotators.
553
we do not separate out the inconsistency types, as
done in Section 4.1.
The supplement section consisted of 82,701
words dually annotated. The larger size, as com-
pared with the corpus in Section 4.1, results in some
differences in the system output. Because of the
larger size, there are more substantial cases of dif-
ferent nuclei grouped together as the same inconsis-
tency type than in Section 4.1. The first inconsis-
tency type (sorted by number of nuclei) has 88 nu-
clei, and the second has 37 nuclei. In total, there are
1,532 inconsistency types found, consisting of 2,194
nuclei in total. We manually examined the first 20
inconsistency types (sorted by number of nuclei),
consisting in total of 375 nuclei. All were found to
be true instances of inconsistent annotation.
(4) a. NP
the ADJP
only true
thing
b. NP
the only true thing
(5) a. NP
their ADJP
only actual
argument
b. NP
their only actual argument
The trees in (4) and (5) show two of the 88 nu-
clei grouped into the first inconsistency type. As
with The word renaissance and The term renais-
sance in the English web corpus, nuclei with similar
(although not identical) words are often grouped into
the same inconsistency type. To repeat the point,
this is not because of any search for similarity of
the words in the nuclei. It arises from the fact that
the nuclei are annotated inconstantly in the same
way. Of course not all nuclei in an inconsistency
type have the same words. Nuclei found in this in-
consistency type include only true and only actual
as shown above, and also nuclei such as new En-
glish, greatest possible, thin square, only necessary.
Taken together, they clearly indicate an issue with
the annotation of multi-word adjective phrases.9
9Note that the inconsistencies discussed throughout this pa-
per are not taken from the the published corpora. These results
are only from internal annotator training files.
5 Future work
There are several ways in which we plan to improve
the current approach. As mentioned above, there is
a certain class of inconsistencies which KBM will
not pinpoint precisely, which requires adopting the
?external check? from Kulick et al (2012). The ab-
straction on inconsistency types described in Sec-
tion 4 can also be taken further. For example, one
might want to examine in particular inconsistency
types that arise from PP attachment or that have to
do with the PRN function tag.
One main area for future work is the application
of this work to parser evaluation as well as IAA. For
this area, there is some connection to the work of
Goldberg and Elhadad (2010) and Dickinson (2010),
which are both concerned with examining depen-
dency structures of more than one edge. The con-
nection is that those works are focused on depen-
dency representations, and ithe KBM system does
phrase structure analysis using a TAG-like deriva-
tion tree, which strongly resembles a dependency
tree (Rambow and Joshi, 1997). There is much in
this area of common concern that is worth examin-
ing further.
Acknowledgments
This material is based upon work supported by
the Defense Advanced Research Projects Agency
(DARPA) under Contract No. HR0011-11-C-0145.
The content does not necessarily reflect the position
or the policy of the Government, and no official en-
dorsement should be inferred. This applies to the
first four authors. The first, fifth, and sixth authors
were supported in part by National Science Foun-
dation Grant # BCS-114749. We would also like
to thank Colin Warner, Aravind Joshi, Mitch Mar-
cus, and the computational linguistics group at the
University of Pennsylvania for helpful conversations
and feedback.
554
References
Ann Bies, Justin Mott, Colin Warner, and Seth Kulick.
2012. English Web Treebank. LDC2012T13. Lin-
guistic Data Consortium.
Adriane Boyd, Markus Dickinson, and Detmar Meurers.
2007. Increasing the recall of corpus annotation er-
ror detection. In Proceedings of the Sixth Workshop
on Treebanks and Linguistic Theories (TLT 2007),
Bergen, Norway.
David Chiang. 2003. Statistical parsing with an auto-
matically extracted Tree Adjoining Grammar. In Data
Oriented Parsing. CSLI.
Markus Dickinson and Detmar Meurers. 2003. Detect-
ing inconsistencies in treebanks. In Proceedings of the
Second Workshop on Treebanks and Linguistic The-
ories (TLT 2003), Sweden. Treebanks and Linguistic
Theories.
Markus Dickinson. 2010. Detecting errors in
automatically-parsed dependency relations. In Pro-
ceedings of the 48th Annual Meeting of the Associ-
ation for Computational Linguistics, pages 729?738,
Uppsala, Sweden, July. Association for Computational
Linguistics.
Yoav Goldberg and Michael Elhadad. 2010. Inspecting
the structural biases of dependency parsing algorithms.
In Proceedings of the Fourteenth Conference on Com-
putational Natural Language Learning, pages 234?
242, Uppsala, Sweden, July. Association for Compu-
tational Linguistics.
A.K. Joshi and Y. Schabes. 1997. Tree-adjoining gram-
mars. In G. Rozenberg and A. Salomaa, editors,
Handbook of Formal Languages, Volume 3: Beyond
Words, pages 69?124. Springer, New York.
Anthony Kroch and Beatrice Santorini. in preparation.
Supplement to the Penn Parsed Corpus of Modern
British English.
Anthony Kroch, Beatrice Santorini, and Ariel Dier-
tani. 2010. Penn Parsed Corpus of Mod-
ern British English. http://www.ling.upenn.edu/hist-
corpora/PPCMBE-RELEASE-1/index.html.
Seth Kulick, Ann Bies, and Justin Mott. 2011. Using
derivation trees for treebank error detection. In Pro-
ceedings of the 49th Annual Meeting of the Associa-
tion for Computational Linguistics: Human Language
Technologies, pages 693?698, Portland, Oregon, USA,
June. Association for Computational Linguistics.
Seth Kulick, Ann Bies, and Justin Mott. 2012. Further
developments in treebank error detection using deriva-
tion trees. In LREC 2012: 8th International Confer-
ence on Language Resources and Evaluation, Istanbul.
Christopher Manning and Hinrich Schuetze. 1999.
Foundations of Statistical Natural Language Process-
ing. MIT Press.
Owen Rambow and Aravind Joshi. 1997. A formal
look at dependency grammars and phrase-structure
grammars, with special consideration of word-order
phenomena. In L. Wanner, editor, Recent Trends
in Meaning-Text Theory, pages 167?190. John Ben-
jamins, Amsterdam and Philadelphia.
Satoshi Sekine and Michael Collins. 2008. Evalb.
http://nlp.cs.nyu.edu/evalb/.
Libin Shen, Lucas Champollion, and Aravind Joshi.
2008. LTAG-spinal and the Treebank: A new re-
source for incremental, dependency and semantic pars-
ing. Language Resources and Evaluation, 42(1):1?19.
555
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 693?698,
Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational Linguistics
Using Derivation Trees for Treebank Error Detection
Seth Kulick and Ann Bies and Justin Mott
Linguistic Data Consortium
University of Pennsylvania
3600 Market Street, Suite 810
Philadelphia, PA 19104
{skulick,bies,jmott}@ldc.upenn.edu
Abstract
This work introduces a new approach to
checking treebank consistency. Derivation
trees based on a variant of Tree Adjoining
Grammar are used to compare the annotation
of word sequences based on their structural
similarity. This overcomes the problems of
earlier approaches based on using strings of
words rather than tree structure to identify the
appropriate contexts for comparison. We re-
port on the result of applying this approach
to the Penn Arabic Treebank and how this ap-
proach leads to high precision of error detec-
tion.
1 Introduction
The internal consistency of the annotation in a tree-
bank is crucial in order to provide reliable training
and testing data for parsers and linguistic research.
Treebank annotation, consisting of syntactic struc-
ture with words as the terminals, is by its nature
more complex and thus more prone to error than
other annotation tasks, such as part-of-speech tag-
ging. Recent work has therefore focused on the im-
portance of detecting errors in the treebank (Green
and Manning, 2010), and methods for finding such
errors automatically, e.g. (Dickinson and Meur-
ers, 2003b; Boyd et al, 2007; Kato and Matsubara,
2010).
We present here a new approach to this problem
that builds upon Dickinson and Meurers (2003b), by
integrating the perspective on treebank consistency
checking and search in Kulick and Bies (2010). The
approach in Dickinson andMeurers (2003b) has cer-
tain limitations and complications that are inher-
ent in examining only strings of words. To over-
come these problems, we recast the search as one of
searching for inconsistently-used elementary trees in
a Tree Adjoining Grammar-based form of the tree-
bank. This allows consistency checking to be based
on structural locality instead of n-grams, resulting in
improved precision of finding inconsistent treebank
annotation, allowing for the correction of such in-
consistencies in future work.
2 Background and Motivation
2.1 Previous Work - DECCA
The basic idea behind the work in (Dickinson and
Meurers, 2003a; Dickinson and Meurers, 2003b) is
that strings occurring more than once in a corpus
may occur with different ?labels? (taken to be con-
stituent node labels), and such differences in labels
might be the manifestation of an annotation error.
Adopting their terminology, a ?variation nucleus? is
the string of words with a difference in the annota-
tion (label), while a ?variation n-gram? is a larger
string containing the variation nucleus.
(1) a. (NP the (ADJP most
important) points)
b. (NP the most important points)
For example, suppose the pair of phrases in (1)
are taken from two different sentences in a cor-
pus. The ?variation nucleus? is the string most
important, and the larger surrounding n-gram
is the string the most important points.
This is an example of error in the corpus, since the
second annotation is incorrect, and this difference
manifests itself by the nucleus having in (a) the label
ADJP but in (b) the default label NIL (meaning for
their system that the nucleus has no covering node).
Dickinson and Meurers (2003b) propose a ?non-
693
fringe heuristic?, which considers two variation nu-
clei to have a comparable context if they are prop-
erly contained within the same variation n-gram -
i.e., there is at least one word of the n-gram on both
sides of the nucleus. For the the pair in (1), the two
instances of the variation nucleus satisfy the non-
fringe heuristic because they are properly contained
within the identical variation n-gram (with the and
points on either side). See Dickinson and Meur-
ers (2003b) for details. This work forms the basis
for the DECCA system.1
2.2 Motivation for Our Approach
(2) a. NP
qmp
summit
NP
$rm
Sharm
NP
Al$yx
the Sheikh
b. NP
qmp
summit
NP
$rm
Sharm
Al$yx
the Sheikh
c. NP
qmp
summit
NP
NP
$rm
Sharm
Al$yx
the Sheikh
NP
( mSr
Egypt
)
We motivate our approach by illustrating the lim-
itations of the DECCA approach. Consider the trees
(2a) and (2b), taken from two instances of the three-
word sequence qmp $rm Al$yx in the Arabic
Treebank.2 There is no need to look at any surround-
ing annotation to conclude that there is an incon-
sistency in the annotation of this sequence.3 How-
ever, based on (2ab), the DECCA system would not
even identify the three-word sequence qmp $rm
Al$yx as a nucleus to compare, because both in-
stances have a NP covering node, and so are consid-
ered to have the same label. (The same is true for
the two-word subsequence $rm Al$yx.)
Instead of doing the natural comparison of the
1http://www.decca.osu.edu/.
2In Section 4 we give the details of the corpus. We use the
Buckwalter Arabic transliteration scheme (Buckwalter, 2004).
3While the nature of the inconsistency is not the issue here,
(b) is the correct annotation.
inconsistent structures for the identical word se-
quences as in (2ab), the DECCA approach would
instead focus on the single word Al$yx, which has
a NP label in (2a), while it has the default label
NIL in (2b). However, whether it is reported as a
variation depends on the irrelevant fact of whether
the word to the right of Al$yx is the same in both
instances, thus allowing it to pass the non-fringe
heuristic (since it already has the same word, $rm,
on the left).
Consider now the two trees (2bc). There is an
additional NP level in (2c) because of the adjunct
( mSr ), causing qmp $rm Al$yx to have no
covering node, and so have the default label NIL,
and therefore categorized as a variation compared to
(2b). However, this is a spurious difference, since
the label difference is caused only by the irrelevant
presence of an adjunct, and it is clear, without look-
ing at any further structure, that the annotation of
qmp $rm Al$yx is identical in (2bc). In this case
the ?non-fringe heuristic? serves to avoid report-
ing such spurious differences, since if qmp $rm
Al$yx did not have an open parenthesis on the right
in (b), and qmp did not have the same word to its
immediate left in both (b) and (c), the two instances
would not be surrounded by the same larger varia-
tion n-gram, and so would not pass the non-fringe
heuristic.
This reliance on irrelevant material arises from us-
ing on a single node label to characterize a struc-
tural annotation and the surrounding word context
to overcome the resulting complications. Our ap-
proach instead directly compares the annotations of
interest.
3 Using Derivation Tree Fragments
We utilize ideas from the long line of Tree Adjoining
Grammar-based research (Joshi and Schabes, 1997),
based on working with small ?elementary trees? (ab-
breviated ?etrees? in the rest of this paper) that are
the ?building blocks? of the full trees of a treebank.
This decomposition of the full tree into etrees also
results in a ?derivation tree? that records how the el-
ementary trees relate to each other.
We illustrate the basics of TAG-based deriva-
tion we are using with examples based on the
trees in (2). Our grammar is a TAG variant with
694
qmp
summit
#c1
S:1.2
NP
NP^
#c2
M:1,right
#c4 NP
mSr
Egypt
qmp
summit
#a1
S:1.2
NP
NP^
$rm
Sharm
#a2
S:1.2
NP
NP^
Al$yx
The Sheikh
NP
#a3
qmp
summit
#b1
S:1.2
NP
NP^
$rm
Sharm
#b2
NP
Al$yx
The Sheikh
    For (2a)                    For (2b)                For (2c) 
A:1.1,left#b3
NP
Al$yx
The Sheikh
A:1.1,left
#c3
$rm
Sharm
Figure 1: Etrees and Derivation Trees for (2abc).
tree-substitution, sister-adjunction, and Chomsky-
adjunction (Chiang, 2003). Sister adjunction at-
taches a tree (or single node) as a sister to another
node, and Chomsky-adjunction forms a recursive
structure as well, duplicating a node. As typically
done, we use head rules to decompose a full tree and
extract the etrees. The three derivation trees, corre-
sponding to (2abc), are shown in Figure 1.
Consider first the derivation tree for (2a). It has
three etrees, numbered a1, a2, a3, which are the
nodes in the derivation tree which show how the
three etrees connect to each other. This derivation
tree consists of just tree substitutions. The ? sym-
bol at node NP? in a1 indicates that it is a sub-
stitution node, and the S:1.2 above a2 indicates
that it substitutes into node at Gorn address 1.2 in
tree a1 (i.e., the substitution node), and likewise
for a3 substituting into a2. The derivation tree for
(2b) also has three etrees, although the structure
is different. Because the lower NP is flat in (2b),
the rightmost noun, Al$yx, is taken as the head
of the etree b2, with the degenerate tree for $rm
sister-adjoining to the left of Al$yx, as indicated
by the A:1.1,left. The derivation tree for (2c)
is identical to that of (2b), except that it has the
additional tree c4 for the adjunct mSr, which right
Chomsky-adjoins to the root of c2, as indicated by
the M:1,right.4
4We leave out the irrelevant (here) details of the parentheses
This tree decomposition and resulting derivation
tree provide us with the tool for comparing nuclei
without the interfering effects from words not in the
nucleus. We are interested not in the derivation tree
for an entire sentence, but rather only that slice of
it having etrees with words that are in the nucleus
being examined, which we call the derivation tree
fragment. That is, for a given nucleus being exam-
ined, we partition its instances based on the covering
node in the full tree, and within each set of instances
we compare the derivation tree fragments for each
instance. These derivation tree fragments are the
relevant structures to compare for inconsistent an-
notation, and are computed separately for each in-
stance of each nucleus from the full derivation tree
that each instance is part of.5
For example, for comparing our three instances
of qmp $rm Al$yx, the three derivation tree frag-
ments would be the structures consisting of (a1, a2,
a3), (b1, b2, b3) and (c1, c2, c3), along with their
connecting Gorn addresses and attachment types.
This indicates that the instances (2ab) have differ-
ent internal structures (without the need to look at a
surrounding context), while the instances (2bc) have
identical internal structures (allowing us to abstract
away from the interfering effects of adjunction).
Space prevents full discussion here, but the etrees
and derivation trees as just described require refine-
ment to be truly appropriate for comparing nuclei.
The reason is that etrees might encode more infor-
mation than is relevant for many comparisons of nu-
clei. For example, a verb might appear in a corpus
with different labels for its objects, such as NP or
SBAR, etc., and this would lead to its having dif-
ferent etrees, differing in their node label for the
substitution node. If the nucleus under compari-
son includes the verb but not any words from the
complement, the inclusion of the different substi-
tution nodes would cause irrelevant differences for
that particular nucleus comparison.
We solve these problems by mapping down the
in the derivation tree.
5A related approach is taken by Kato and Matsubara (2010),
who compare partial parse trees for different instances of the
same sequence of words in a corpus, resulting in rules based on
a synchronous Tree Substitution Grammar (Eisner, 2003). We
suspect that there are some major differences between our ap-
proaches regarding such issues as the representation of adjuncts,
but we leave such a comparison for future work.
695
System nuclei n-grams instances
DECCA 24,319 1,158,342 2,966,274
Us 54,496 not used 605,906
Table 1: Data examined by the two systems for the ATB
System nuclei non-duplicate types of
found nuclei found inconsistency
DECCA 4,140 unknown unknown
Us-internal 9,984 4,272 1,911
Table 2: Annotation inconsistencies reported for the ATB
representation of the etrees in a derivation tree frag-
ment to form a ?reduced? derivation tree fragment.
These reductions are (automatically) done for each
nucleus comparison in a way that is appropriate for
that particular nucleus comparison. A particular
etree may be reduced in one way for one nucleus,
and then a different way for a different nucleus. This
is done for each etree in a derivation tree fragment.
4 Results on Test Corpus
Green and Manning (2010) discuss annotation con-
sistency in the Penn Arabic Treebank (ATB), and for
our test corpus we follow their discussion and use
the same data set, the training section of three parts
of the ATB (Maamouri et al, 2008a; Maamouri et
al., 2009; Maamouri et al, 2008b). Their work is
ideal for us, since they used the DECCA algorithm
for the consistency evaluation. They did not use the
?non-fringe? heuristic, but instead manually exam-
ined a sample of 100 nuclei to determine whether
they were annotation errors.
4.1 Inconsistencies Reported
The corpus consists of 598,000 tokens. Table 1 com-
pares token manipulation by the two systems. The
DECCA system6 identified 24,319 distinct variation
nuclei, while our system had 54,496. DECCA ex-
amined 1,158,342 n-grams, consisting of 2,966,274
6We worked at first with version 0.2 of the software. How-
ever this software does not implement the non-fringe heuristic
and does not make available the actual instances of the nuclei
that were found. We therefore re-implemented the algorithm
to make these features available, being careful to exactly match
our output against the released DECCA system as far as the nu-
clei and n-grams found.
instances (i.e., different corpus positions of the n-
grams), while our system examined 605,906 in-
stances of the 54,496 nuclei. For our system, the
number of nuclei increases and the variation n-
grams are eliminated. This is because all nuclei with
more than one instance are evaluated, in order to
search for constituents that have the same root but
different internal structure.
The number of reported inconsistencies is shown
in Table 2. DECCA identified 4,140 nuclei as likely
errors - i.e., contained in larger n-grams, satisfying
the non-fringe heuristic. Our system identified 9,984
nuclei as having inconsistent annotation - i.e., with
at least two instances with different derivation tree
fragments.
4.2 Eliminating Duplicate Nuclei
Some of these 9,984 nuclei are however redundant,
due to nuclei contained within larger nuclei, such as
$rm Al$yx inside qmp $rm Al$yx in (2abc).
Eliminating such duplicates is not just a simple mat-
ter of string inclusion, since the larger nucleus can
sometimes reveal different annotation inconsisten-
cies than just those in the smaller substring nucleus,
and also a single nucleus string can be included in
different larger nuclei. We cannot discuss here the
full details of our solution, but it basically consists
of two steps.
First, as a result of the analysis described so far,
for each nucleus we have a mapping of each instance
of that nucleus to a derivation tree fragment. Sec-
ond, we test for each possible redundancy (meaning
string inclusion) whether there is a true structural re-
dundancy by testing for an isomorphism between the
mappings for two nuclei. For this test corpus, elimi-
nating such duplicates leaves 4,272 nuclei as having
inconsistent annotation. It is unknown how many
of the DECCA nuclei are duplicates, although many
certainly are. For example, qmp $rm Al$yx and
$rm Al$yx are reported as separate results.
4.3 Grouping Inconsistencies by Structure
Across all variation nuclei, there are only a finite
number of derivation tree fragments and thus ways
in which such fragments indicate an annotation in-
consistency. We categorize each annotation incon-
sistency by the inconsistency type, which is simply
a set of numbers representing the different derivation
696
tree fragments. We can then present the results not
by listing each nucleus string, but instead by the in-
consistency types, with each type having some num-
ber of nuclei associated with it.
For example, instances of $rm Al$yx might
have just the derivation tree fragments (a2, a3) and
(b2, b3) in Figure 1, and the numbers representing
this pair is the ?inconsistency type? for this (nucleus,
internal context) inconsistency. There are nine other
nuclei reported as having an inconsistency based on
the exact same derivation tree fragments (abstracting
only away from the particular lexical items), and so
all these nuclei are grouped together as having the
same ?inconsistency type?. This grouping results in
the 4,272 non-duplicate nuclei found being grouped
into 1,911 inconsistency types.
4.4 Precision and Recall
The grouping of internal checking results by incon-
sistency types is a qualitative improvement in con-
sistency reporting, with a high precision.7 By view-
ing inconsistencies by structural annotation types,
we can examine large numbers of nuclei at a time.
Of the first 10 different types of derivation tree in-
consistencies, which include 266 different nuclei, all
10 appear to real cases of annotation inconsistency,
and the same seems to hold for each of the nuclei in
those 10 types, although we have not checked every
single nucleus. For comparison, we chose a sample
of 100 nuclei output by DECCA on this same data,
and by our judgment the DECCA precision is about
74%, including 15 duplicates.
Measuring recall is tricky, even using the errors
identified in Green and Manning (2010) as ?gold?
errors. One factor is that a system might report a
variation nucleus, but still not report all the relevant
instances of that nucleus. For example, while both
systems report $rm Al$yx as a sequence with in-
consistent annotation, DECCA only reports the two
instances that pass the ?non-fringe heuristic?, while
our system lists 132 instances of $rm Al$yx, parti-
tioning them into the two derivation tree fragments.
We will be carrying out a careful accounting of the
recall evaluation in future work.
7
?Precision? here means the percentage of reported varia-
tions that are actually annotation errors.
5 Future Work
While we continue the evaluation work, our pri-
mary concern now is to use the reported inconsistent
derivation tree fragments to correct the annotation
inconsistencies in the actual data, and then evaluate
the effect of the corpus corrections on parsing. Our
system groups all instances of a nucleus into differ-
ent derivation tree fragments, and it would be easy
enough for an annotator to specify which is correct
(or perhaps instead derive this automatically based
on frequencies).
However, because the derivation trees and etrees
are somewhat abstracted from the actual trees in the
treebank, it can be challenging to automatically cor-
rect the structure in every location to reflect the cor-
rect derivation tree fragment. This is because of de-
tails concerning the surrounding structure and the
interaction with annotation style guidelines such as
having only one level of recursive modification or
differences in constituent bracketing depending on
whether a constituent is a ?single-word? or not. We
are focusing on accounting for these issues in cur-
rent work to allow such automatic correction.
Acknowledgments
We thank the computational linguistics group at the
University of Pennsylvania for helpful feedback on
a presentation of an earlier version of this work.
We also thank Spence Green and Chris Manning
for supplying the data used in their analysis of the
Penn Arabic Treebank. This work was supported
in part by the Defense Advanced Research Projects
Agency, GALE Program Grant No. HR0011-06-
1-0003 (all authors) and by the GALE program,
DARPA/CMO Contract No. HR0011-06-C-0022
(first author). The content of this paper does not
necessarily reflect the position or the policy of the
Government, and no official endorsement should be
inferred.
References
Adriane Boyd, Markus Dickinson, and Detmar Meurers.
2007. Increasing the recall of corpus annotation er-
ror detection. In Proceedings of the Sixth Workshop
on Treebanks and Linguistic Theories (TLT 2007),
Bergen, Norway.
697
Tim Buckwalter. 2004. Buckwalter Arabic morphologi-
cal analyzer version 2.0. Linguistic Data Consortium
LDC2004L02.
David Chiang. 2003. Statistical parsing with an auto-
matically extracted tree adjoining grammar. In Data
Oriented Parsing. CSLI.
Markus Dickinson and Detmar Meurers. 2003a. Detect-
ing errors in part-of-speech annotation. In Proceed-
ings of the 10th Conference of the European Chap-
ter of the Association for Computational Linguistics
(EACL-03), pages 107?114, Budapest, Hungary.
Markus Dickinson and Detmar Meurers. 2003b. Detect-
ing inconsistencies in treebanks. In Proceedings of the
Second Workshop on Treebanks and Linguistic The-
ories (TLT 2003), Sweden. Treebanks and Linguistic
Theories.
Jason Eisner. 2003. Learning non-isomorphic tree map-
pings for machine translation. In The Companion Vol-
ume to the Proceedings of 41st Annual Meeting of
the Association for Computational Linguistics, pages
205?208, Sapporo, Japan, July. Association for Com-
putational Linguistics.
Spence Green and Christopher D. Manning. 2010. Bet-
ter Arabic parsing: Baselines, evaluations, and anal-
ysis. In Proceedings of the 23rd International Con-
ference on Computational Linguistics (Coling 2010),
pages 394?402, Beijing, China, August. Coling 2010
Organizing Committee.
A.K. Joshi and Y. Schabes. 1997. Tree-adjoining gram-
mars. In G. Rozenberg and A. Salomaa, editors,
Handbook of Formal Languages, Volume 3: Beyond
Words, pages 69?124. Springer, New York.
Yoshihide Kato and Shigeki Matsubara. 2010. Correct-
ing errors in a treebank based on synchronous tree sub-
stitution grammar. In Proceedings of the ACL 2010
Conference Short Papers, pages 74?79, Uppsala, Swe-
den, July. Association for Computational Linguistics.
Seth Kulick and Ann Bies. 2010. A TAG-derived
database for treebank search and parser analysis. In
TAG+10: The 10th International Conference on Tree
Adjoining Grammars and Related Formalisms, Yale.
Mohamed Maamouri, Ann Bies, Seth Kulick, Fatma
Gaddeche, Wigdan Mekki, Sondos Krouna, and
Basma Bouziri. 2008a. Arabic treebank part 1 - v4.0.
Linguistic Data Consortium LDC2008E61, December
4.
Mohamed Maamouri, Ann Bies, Seth Kulick, Fatma
Gaddeche, Wigdan Mekki, Sondos Krouna, and
Basma Bouziri. 2008b. Arabic treebank part 3 - v3.0.
Linguistic Data Consortium LDC2008E22, August 20.
Mohamed Maamouri, Ann Bies, Seth Kulick, Fatma
Gaddeche, Wigdan Mekki, Sondos Krouna, and
Basma Bouziri. 2009. Arabic treebank part 2- v3.0.
Linguistic Data Consortium LDC2008E62, January
20.
698
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 668?673,
Baltimore, Maryland, USA, June 23-25 2014.
c?2014 Association for Computational Linguistics
Parser Evaluation Using Derivation Trees:
A Complement to evalb
Seth Kulick and Ann Bies and Justin Mott
Linguistic Data Consortium, University of Pennsylvania, Philadelphia, PA 19104
{skulick,bies,jmott}@ldc.upenn.edu
Anthony Kroch and Mark Liberman and Beatrice Santorini
Department of Linguistics, University of Pennsylvania, Philadelphia, PA 19104
{kroch,myl,beatrice}@ling.upenn.edu
Abstract
This paper introduces a new technique for
phrase-structure parser analysis, catego-
rizing possible treebank structures by inte-
grating regular expressions into derivation
trees. We analyze the performance of the
Berkeley parser on OntoNotes WSJ and
the English Web Treebank. This provides
some insight into the evalb scores, and
the problem of domain adaptation with the
web data. We also analyze a ?test-on-
train? dataset, showing a wide variance in
how the parser is generalizing from differ-
ent structures in the training material.
1 Introduction
Phrase-structure parsing is usually evaluated using
evalb (Sekine and Collins, 2008), which provides
a score based on matching brackets. While this
metric serves a valuable purpose in pushing parser
research forward, it has limited utility for under-
standing what sorts of errors a parser is making.
This is the case even if the score is broken down
by brackets (NP, VP, etc.), because the brackets
can represent different types of structures. We
would also like to have answers to such questions
as ?How does the parser do on non-recursive NPs,
separate from NPs resulting from modification?
On PP attachment?? etc.
Answering such questions is the goal of this
work, which combines two strands of research.
First, inspired by the tradition of Tree Adjoin-
ing Grammar-based research (Joshi and Schabes,
1997; Bangalore and Joshi, 2010), we use a de-
composition of the full trees into ?elementary
trees? (henceforth ?etrees?), with a derivation tree
that records how the etrees relate to each other,
as in Kulick et al (2011). In particular, we use
the ?spinal? structure approach of (Shen et al,
2008; Shen and Joshi, 2008), where etrees are con-
strained to be unary-branching.
Second, we use a set of regular expressions
(henceforth ?regexes?) that categorize the possible
structures in the treebank. These are best thought
of as an extension of head-finding rules, which not
only find a head but simultaneously identify each
parent/children relation as one of a limited number
of types of structures (right-modification, etc.).
The crucial step is that we integrate these
regexes into the spinal etrees. The derivation trees
provide elements of a dependency analysis, which
allow us to calculate scores for head identification
and attachment for different projections (e.g., PP).
The regexes allow us to also provide scores based
on spans of different construction types. Together
these two aspects break down the evalb brackets
into more meaningful categories, and the simulta-
neous head and span scoring allows us to separate
these aspects in the analysis.
After describing in more detail the basic frame-
work, we show some aspects of the resulting anal-
ysis of the performance of the Berkeley parser
(Petrov et al, 2008) on three datasets: (a)
OntoNotes WSJ sections 2-21 (Weischedel et al,
2011)
1
, (b) OntoNotes WSJ section 22, and (c)
the ?Answers? section of the English Web Tree-
bank (Bies et al, 2012). We trained the parser on
sections 2-21, and so (a) is ?test-on-train?. These
three results together show how the parser is gen-
eralizing from the training data, and what aspects
of the ?domain adaptation? problem to the web
material are particularly important.
2
2 Framework for analyzing parsing
performance
We first describe the use of the regexes in tree de-
composition, and then give some examples of in-
1
We refer only to the WSJ treebank portion of OntoNotes,
which is roughly a subset of the Penn Treebank (Marcus et
al., 1999) with annotation revisions including the addition of
NML nodes.
2
We parse (c) while training on (a) to follow the procedure
in Petrov and McDonald (2012)
668
corporating these regexes into the derivation trees.
2.1 Use of regular expressions
Decomposing the original phrase-structure tree
into the smaller components requires some
method of determining the ?head? of a nonter-
minal, from among its children nodes, similar to
parsing work such as Collins (1999). As described
above, we are also interested in the type of lin-
guistic construction represented by that one-level
structure, each of which instantiates one of a few
types - recursive coordination, simple head-and-
sister, etc. We address both tasks together with the
regexes. In contrast to the sort of head rules in
(Collins, 1999), these refer as little as possible to
specific POS tags. Instead of explicitly listing the
POS tags of possible heads, the heads are in most
cases determined by their location in the structure.
Sample regexes are shown in Figure 1. There
are 49 regexes used.
3
Regexes ADJP-t and
ADVP-t in (a) identify their terminal head to
be the rightmost terminal, possibly preceded by
some number of terminals or nonterminals, rely-
ing on a mapping that maps all terminals (except
CC, which is mapped to CONJ) to TAG and all
nonterminals (except CONJP and NML) to NT.
Structures with a CONJ/CONJP/NML child do
not match this rule and are handled by different
regexes, which are all mutually exclusive. In some
cases, we need to search for particular nonterminal
heads, such as with the (b) regexes S-vp and SQ-
vp, which identify the rightmost VP among the
children of a S or SQ as the head. (c) NP-modr
is a regex for a recursive NP with a right modifier.
In this case, the NP on the left is identified as the
head. (d) VP-crd is also a regex for a recursive
structure, in this case for VP coordination, pick-
ing out the leftmost conjunct as the head of the
structure. The regex names roughly describe their
purpose - ?mod? for right-modification, ?crd? for
coordination, etc. The suffix ?-t? is for the simple
non-recursive case in which the head is a terminal.
2.2 Regexes in the derivation trees
The names of these regexes are incorporated into
the etrees themselves, as labels of the nontermi-
nals. This allows an etree to contain information
3
Some among the 49 are duplicates, used for different
nonterminals, as with (a) and (b) in Figure 1. We derived
the regexes via an iterative process of inspection of tree de-
composition on dataset (a), together with taking advantage of
the treebanking experience from some of the co-authors.
(a)ADJP-t,ADVP-t:
?(TAG|NT|NML)
*
(head:TAG) (NT)
*
$
(b)S-vp, SQ-vp: ?([? ]+)
*
(head:VP)$
(c)NP-modr:
?(head:NP)(SBAR|S|VP|ADJP|PP|ADVP|NP)+$
(d)VP-crd: ?(head:VP) (VP)
*
CONJ VP$
Figure 1: Some sample regexes
such as ?this node represents right modification?.
For example, Figure 2 shows the derivation tree
resulting from the decomposition of the tree in
Figure 4. Each structure within a circle is one
etree, and the derivation as a whole indicates how
these etrees are combined. Here we indicate with
arrows that point to the relevant regex. For ex-
ample, the PP-t etree #a6 points to the NP-modr
regex, which consists of the NP-t together with
the PP-t. The nonterminals of the spinal etrees are
the names of the regexes, with the simpler non-
terminal labels trivially derivable from the regex
names.
4
The tree in Figure 5 is the parser output corre-
sponding to the gold tree in Figure 4, and in this
case gets the PP-t attachment wrong, while every-
thing else is the same as the gold.
5
This is reflected
in the derivation tree in Figure 3, in which the NP-
modr regex is absent, with the NP-t and PP-t etrees
#b5 and #b6 both pointing to the VP-t regex in
#b3. We show in Section 2.3 how this derivation
tree representation is used to score this attachment
error directly, rather than obscuring it as an NP
bracket error as evalb would do.
2.3 Scoring
We decompose both the gold and parser output
trees into derivation trees with spinal etrees, and
score based on the regexes projected by each word.
There is a match for a regex if the corresponding
words in gold/parser files project to that regex, a
precision error if the parser file does but the gold
does not, and a recall error if the gold does but the
parser file does not.
For example, comparing the trees in Figures 4
and 5 via their derivation trees in Figures 2 and
Figures 3, the word ?trip? has a match for the regex
NP-t, but a recall error for NP-modr. The word
4
We do not have space here to discuss the data structure
in complete detail, but multiple regex names at a node, such a
VP-aux and VP-t at tree a3 in Figure 2, indicate multiple VP
nonterminals.
5
We leave function tags aside for future work. The gold
tree is shown without the SBJ function tag.
669
#a1 TheyNP-t tripNP-modr  NP-t toPP-t#a6
#a3
#a4themake
VP-auxVP-tS-vp#a2will #a7FloridaNP-t
#a5
Figure 2: Derivation Tree for Figure 4
#b1TheyNP-t toPP-t#b6tripNP-t#b4themake
VP-auxVP-t#b3 S-vp#b2will #b7FloridaNP-t
#b5
Figure 3: Derivation Tree for Figure 5)
S
NP
They
VP
will VP
make NP
NP
the trip
PP
to NP
Florida
S
NP
They
VP
will VP
make NP
the trip
PP
to NP
Florida
1
Figure 4: Gold tree
S
NP
They
VP
will VP
make NP
NP
the trip
PP
to NP
Florida
S
NP
They
VP
will VP
make NP
the trip
PP
to NP
Florida
1
Figure 5: Parser output tree
Corpus tokens brackets coverage % evalb
2-21 g 650877 578597 571243 98.7
p 575744 569480 98.9 93.8
22 g 32092 24819 24532 98.8
p 24801 24528 98.9 90.1
Ans g 53960 48492 47348 97.6
p 48750 47423 97.3 80.8
Table 1: Corpus information for gold(g) and
parsed(p) sections of each corpus
?make? has a match for the regexes VP-t, VP-
aux, and S-vp, and so on. Summing such scores
over the corresponding gold/parser trees gives us
F-scores for each regex.
There are two modifications/extensions to these
F-scores that we also use:
(1) For each regex match, we score whether it
matches based on the span as well. For exam-
ple, ?make? is a match for VP-t in Figures 2
and 3, and is also a match for the span as well,
since in both derivation trees it includes the words
?make. . .Florida?. It is this matching for span as
well as head that allows us to compare our results
to evalb. We call the match just for the head the ?F-
h? score and the match that also includes the span
information the ?F-s? score. The F-s score roughly
corresponds to the evalb score. However, the F-
s score is for separate syntactic constructions (in-
cluding also head identification), although we can
also sum it over all the structures, as done later in
Figure 6. The simultaneous F-h and F-s scores let
us identify constructions where the parser has the
head projection correct, but gets the span wrong.
(2) Since the derivation tree is really a depen-
dency tree with more complex nodes (Rambow
and Joshi, 1997; Kulick et al, 2012), we can also
score each regex for attachment.
6
For example,
while ?to? is a match for PP-t, its attachment is
not, since in Figure 2 it is a child of the ?trip? etree
(#a5) and in Figure 3 it is a child of the ?make?
etree (#b3). Therefore our analysis results in an
attachment score for every regex.
2.4 Comparison with previous work
This work is in the same basic line of research
as the inter-annotator agreement analysis work in
Kulick et al (2013). However, that work did
not utilize regexes, and focused on comparing se-
quences of identical strings. The current work
scores on general categories of structures, without
6
A regex intermediate in a etree, such as VP-t above, is
considered to have a default null attachment. Also, the at-
tachment score is not relevant for regexes that already express
a recursive structure, such as NP-modr. In Figure 2, NP-t in
etree #a5 is considered as having the attachment to #a3.
670
Sections 2-21 (Ontonotes) Section 22 (Ontonotes) Answers (English Web Treebank)
regex %gold F-h F-s att spanR %gold F-h F-s att spanR %gold F-h F-s att spanR
NP-t 30.7 98.9 97.6 96.5 99.6 31.1 98.0 95.8 94.4 99.6 28.5 95.4 91.5 90.9 99.3
VP-t 13.5 98.8 94.5 98.4 95.8 13.4 98.1 91.7 97.3 93.7 16.0 96.7 81.7 96.1 85.4
PP-t 12.2 99.2 91.0 90.5 92.0 12.1 98.7 86.4 86.1 88.2 8.4 96.4 80.5 80.7 84.7
S-vp 12.2 97.9 92.8 96.8 96.3 11.9 96.5 89.1 95.4 95.0 14.2 94.1 72.9 88.0 84.1
NP-modr 8.6 88.4 80.3 - 91.5 8.5 82.9 71.8 - 87.9 4.4 69.0 54.2 - 80.5
VP-aux 5.5 97.9 94.0 - 96.1 5.0 96.5 91.0 - 94.6 6.2 94.4 81.7 - 86.7
SBAR-s 3.7 96.1 91.1 91.8 95.3 3.5 94.3 87.2 86.4 93.5 4.0 84.8 68.2 81.9 81.9
ADVP-t 2.7 95.2 93.3 93.9 98.6 3.0 89.6 84.5 88.0 95.9 4.5 84.0 78.2 80.3 96.8
NML-t 2.3 91.6 90.3 97.6 99.8 2.6 85.6 82.2 93.5 99.8 0.7 42.1 37.7 88.8 100.0
ADJP-t 1.9 94.6 88.4 95.5 94.6 1.8 86.8 77.0 93.6 90.7 2.5 84.7 67.0 88.1 84.2
QP-t 1.0 95.3 93.8 98.3 99.6 1.2 91.0 89.0 97.1 100.0 0.2 57.7 57.7 94.4 100.0
NP-crd 0.8 80.3 73.7 - 92.4 0.6 68.6 58.4 - 86.1 0.5 55.3 47.8 - 88.1
VP-crd 0.4 84.3 82.8 - 98.2 0.4 75.3 73.5 - 97.6 0.8 65.5 58.3 - 89.8
S-crd 0.3 83.7 83.2 - 99.6 0.4 70.9 68.6 - 96.7 0.8 68.5 63.0 - 93.4
SQ-v 0.1 88.3 82.0 93.3 97.8 0.1 66.7 66.7 88.9 100.0 0.9 81.9 72.4 93.4 95.8
FRAG-nt 0.1 49.9 48.6 95.4 97.9 0.1 28.6 28.6 100.0 100.0 0.8 22.7 21.3 96.3 96.3
Table 2: Scores for the most frequent categories of brackets in the three datasets of corpora, as determined
by the regexes. % gold is the frequency of this regex type compared to all the brackets in the gold. F-h
is the score based on matching heads, F-s also incorporates the span information, att is the attachment
accuracy for words that match in F-h, and spanR is the span-right accuracy for words that match in F-h.
the reliance on sequences of individual strings.
7
3 Analysis of parsing results
We worked with the three datasets as described
in the introduction. We trained the parser on sec-
tions 2-21 of OntoNotes WSJ, and parsed the three
datasets with the gold tags, since at present we
wish to analyze the parser performance in isola-
tion from Part-of-Speech tagging errors. Table 1
shows the sizes of the three corpora in terms of
tokens and brackets, for both the gold and parsed
versions, with the evalb scores for the parsed ver-
sions. The score is lower for Answers, as also
found by Petrov and McDonald (2012).
To facilitate comparison of our analysis with
evalb, we used corpora versions with the same
bracket deletion (empty yields and most punctua-
tion) as evalb. We ran the gold and parsed versions
through our regex decomposition and derivation
tree creation. Table 1 shows the number and per-
centage of brackets handled by our regexes. The
high coverage (%) reinforces the point that there is
a limited number of core structures in the treebank.
In the results below in Table 2 and Figure 6 we
combine the nonterminals that are not covered by
one of the regexes with the simple non-recursive
regex case for that nonterminal.
8
7
In future work we will compare our approach to that
of Kummerfeld et al (2012), who also move beyond evalb
scores in an effort to provide more meaningful error analysis.
8
We also combine a few other non-recursive regexes to-
gether with NP-t, such as the special one for possessives.
We present the results in two ways. Table 2 lists
the most frequent categories in the three datasets,
with their percentage of the overall number of
brackets (%gold), their score based just on the
head identification (F-h), their score based on head
identification and (left and right) span (F-s), and
the attachment (att) and span-right (spanR) scores
for those that match based on the head.
9
The two graphs in Figure 6 show the cumu-
lative results based on F-h and F-s, respectively.
These show the cumulative score in order of the
frequency of categories. For example, for sections
2-21, the score for NP-t is shown first, with 30.7%
of the brackets, and then together with the VP-t
category, they cover 45.2% of the brackets, etc.
10
The benefit of the approach described here is that
now we can see the contribution to the evalb score
of the particular types of constructions, and within
those constructions, how well the parser is doing
at getting the same head projection, but failing or
9
The score for the left edge is almost always very high for
every category, and we just list here the right edge score. The
attachment score does not apply to the recursive categories,
as mentioned above.
10
The final F-s value is lower than the evalb score - e.g.
92.5 for sections 2-21 (the rightmost point in the graph for
sections 2-21 in the F-s graph in Figure 6) compared to the
93.8 evalb score. Space prevents full explanation, but there
are two reasons for this. One is that there are cases in which
bracket spans match, but the head, as found by our regexes, is
different in the gold and parser trees. The other cases is when
brackets match, and may even have the same head, but their
regex is different. In future work we will provide a full ac-
counting of such cases, but they do not affect the main aspects
of the analysis.
671
F-scores by head identification
cumulative % of all brackets
0 5 10 20 30 40 50 60 70 80 90 100
89
.2
91
.2
93
.2
95
.2
97
.2
99
.0
2-21
1 2
3
4
5
6
7
8
9
10
11
14
13
15
12
22
1 2
3
4
5
6
7
8
9
10
11
14
13
15
12
answers
1
2
4
3
6
8
5
7
10
12
13
14
15
9
11
 1:NP-t     2:VP-t
 3:PP-t     4:S-vp
 5:NP-modr  6:VP-aux
 7:SBAR-s   8:ADVP-t
 9:NML-t   10:ADJP-t
11:QP-t    12:SQ-vp
13:S-crd   14:VP-crd
15:FRAG-nt
F-scores by head identification and span
cumulative % of all backets
0 5 10 20 30 40 50 60 70 80 90 100
78
.0
82
.0
86
.0
90
.0
94
.0
97
.6
2-21
1
2
3
4
5
6
7
8
9
10
11
14
13
15
12
22
1
2
3
4
5
6
7
8
9
10
11
14
13
15
12
answers
1
2
4
3
6
8
5
7
10 12
13
14
15
9
11
 1:NP-t     2:VP-t
 3:PP-t     4:S-vp
 5:NP-modr  6:VP-aux
 7:SBAR-s   8:ADVP-t
 9:NML-t   10:ADJP-t
11:QP-t    12:SQ-vp
13:S-crd   14:VP-crd
15:FRAG-nt
Figure 6: Cumulative scores based on F-h (left) and F-s (right). These graphs are both cumulative in
exactly the same way, in that each point represents the total percentage of brackets accounted for so far.
So for the 2-21 line, point 1, meaning the NP non-recursive regex, accounts for 30.7% of the brackets,
point 2, meaning the VP non-recursive regex, accounts for another 13.5%, so 44.2% cumulatively, etc.
not on the spans.
3.1 Analysis and future work
As this is work-in-progress, the analysis is not yet
complete. We highlight a few points here.
(1) The high performance on the OntoNotes WSJ
material is in large part due to the score on the
non-recursive regexes of NP-t, VP-t, S-vp, and the
auxiliaries (points 1, 2, 4, 6 in the graphs). Critical
to this is the fact that the parser does well on deter-
mining the right edge of verbal structures, which
affects the F-s score for VP-t (non-recursive), VP-
aux, and S-vp. The spanR score for VP-t is 95.8
for Sections 2-21 and 93.7 for Section 22.
(2) We wouldn?t expect the test-on-training evalb
score to be 100%, since it has to back off from
the training data, but the results for the different
categories vary widely, with e.g., the NP-modr F-
h score much lower than other frequent regexes.
This variance from the test-on-training dataset car-
ries over almost exactly to Section 22.
(3) The different distribution of structures in
Answers hurts performance. For example, the
mediocre performance of the parser on SQ-vp
barely affects the score with OntoNotes, but has
a larger negative effect with Answers, due to its
increased frequency in the latter.
(4) While the different distribution of construc-
tions is a problem for Answers, more critical is
the poor performance of the parser on determin-
ing the right edge of verbal constructions. This is
only 85.4 for VP-t in Answers, compared to the
OntoNotes results mentioned in (1). Since this af-
fects the F-s scores for VP-t, VP-aux, and S-vp,
the negative effect is large. Preliminary investi-
gation shows that this is due in part to incorrect
PP and SBAR placement (the PP-t and SBAR-s
attachment scores (80.7 and 81.9) are worse for
Answers compared to Section 22 (86.1 and 86.4)),
and coordinated S-clauses with no conjunction.
In sum, there is a wealth of information from
this new type of analysis that we will use in our on-
going work to better understand what the parser is
learning and how it works on different genres.
Acknowledgments
This material is based upon work supported by Na-
tional Science Foundation Grant # BCS-114749
(first, fourth, and sixth authors) and by the Defense
Advanced Research Projects Agency (DARPA)
under Contract No. HR0011-11-C-0145 (first,
second, and third authors). The content does not
necessarily reflect the position or the policy of the
Government, and no official endorsement should
be inferred.
672
References
Srinivas Bangalore and Aravind K. Joshi, editors.
2010. Supertagging: Using Complex Lexical De-
scriptions in Natural Language Processing. MIT
Press.
Ann Bies, Justin Mott, Colin Warner, and Seth Kulick.
2012. English Web Treebank. LDC2012T13. Lin-
guistic Data Consortium.
Michael Collins. 1999. Head-Driven Statistical Mod-
els for Natural Language Parsing. Ph.D. thesis,
Department of Computer and Information Sciences,
University of Pennsylvania.
A.K. Joshi and Y. Schabes. 1997. Tree-adjoining
grammars. In G. Rozenberg and A. Salomaa, ed-
itors, Handbook of Formal Languages, Volume 3:
Beyond Words, pages 69?124. Springer, New York.
Seth Kulick, Ann Bies, and Justin Mott. 2011. Using
derivation trees for treebank error detection. Asso-
ciation for Computational Linguistics.
Seth Kulick, Ann Bies, and Justin Mott. 2012. Using
supertags and encoded annotation principles for im-
proved dependency to phrase structure conversion.
In Proceedings of the 2012 Conference of the North
American Chapter of the Association for Computa-
tional Linguistics: Human Language Technologies,
pages 305?314, Montr?eal, Canada, June. Associa-
tion for Computational Linguistics.
Seth Kulick, Ann Bies, Justin Mott, Mohamed
Maamouri, Beatrice Santorini, and Anthony Kroch.
2013. Using derivation trees for informative tree-
bank inter-annotator agreement evaluation. In Pro-
ceedings of the 2013 Conference of the North Amer-
ican Chapter of the Association for Computational
Linguistics: Human Language Technologies, pages
550?555, Atlanta, Georgia, June. Association for
Computational Linguistics.
Jonathan K. Kummerfeld, David Hall, James R. Cur-
ran, and Dan Klein. 2012. Parser showdown at the
wall street corral: An empirical investigation of error
types in parser output. In Proceedings of the 2012
Joint Conference on Empirical Methods in Natural
Language Processing and Computational Natural
Language Learning, pages 1048?1059, Jeju Island,
Korea, July. Association for Computational Linguis-
tics.
Mitchell P. Marcus, Beatrice Santorini, Mary Ann
Marcinkiewicz, and Ann Taylor. 1999. Treebank-3.
LDC99T42, Linguistic Data Consortium, Philadel-
phia.
Slav Petrov and Ryan McDonald. 2012. Overview of
the 2012 shared task on parsing the web. In Pro-
ceedings of the First Workshop on Syntactic Analysis
of Non-Canonical Language.
Slav Petrov, Leon Barrett, Romain Thibaux, and
Dan Klein. 2008. The Berkeley Parser.
https://code.google.com/p/berkeleyparser/.
Owen Rambow and Aravind Joshi. 1997. A formal
look at dependency grammars and phrase-structure
grammars, with special consideration of word-order
phenomena. In L. Wanner, editor, Recent Trends
in Meaning-Text Theory, pages 167?190. John Ben-
jamins, Amsterdam and Philadelphia.
Satoshi Sekine and Michael Collins. 2008. Evalb.
http://nlp.cs.nyu.edu/evalb/.
Libin Shen and Aravind Joshi. 2008. LTAG depen-
dency parsing with bidirectional incremental con-
struction. In Proceedings of the 2008 Conference on
Empirical Methods in Natural Language Process-
ing, pages 495?504, Honolulu, Hawaii, October. As-
sociation for Computational Linguistics.
Libin Shen, Lucas Champollion, and Aravind Joshi.
2008. LTAG-spinal and the Treebank: A new
resource for incremental, dependency and seman-
tic parsing. Language Resources and Evaluation,
42(1):1?19.
Ralph Weischedel, Martha Palmer, Mitchell Marcus,
Eduard Hovy, Sameer Pradhan, Lance Ramshaw,
Nianwen Xue, Ann Taylor, Jeff Kaufman, Michelle
Franchini, Mohammed El-Bachouti, Robert Belvin,
and Ann Houston. 2011. OntoNotes 4.0. Linguistic
Data Consortium LDC2011T03.
673
Proceedings of the 2nd Workshop on EVENTS: Definition, Detection, Coreference, and Representation, pages 21?25,
Baltimore, Maryland, USA, June 22-27, 2014.
c?2014 Association for Computational Linguistics
Inter-Annotator Agreement for ERE Annotation
Seth Kulick and Ann Bies and Justin Mott
Linguistic Data Consortium, University of Pennsylvania, Philadelphia, PA 19104
{skulick,bies,jmott}@ldc.upenn.edu
Abstract
This paper describes a system for inter-
annotator agreement analysis of ERE an-
notation, focusing on entity mentions and
how the higher-order annotations such as
EVENTS are dependent on those entity
mentions. The goal of this approach is to
provide both (1) quantitative scores for the
various levels of annotation, and (2) infor-
mation about the types of annotation in-
consistencies that might exist. While pri-
marily designed for inter-annotator agree-
ment, it can also be considered a system
for evaluation of ERE annotation.
1 Introduction
In this paper we describe a system for analyz-
ing dually human-annotated files of Entities, Re-
lations, and Events (ERE) annotation for consis-
tency between the two files. This is an important
aspect of training new annotators, to evaluate the
consistency of their annotation with a ?gold? file,
or to evaluate the agreement between two anno-
tators. We refer to both cases here as the task of
?inter-annotator agreement? (IAA).
The light ERE annotation task was defined as
part of the DARPA DEFT program (LDC, 2014a;
LDC, 2014b; LDC, 2014c) as a simpler version
of tasks like ACE (Doddington et al., 2004) to al-
low quick annotation of a simplified ontology of
entities, relations, and events, along with iden-
tity coreference. The ENTITIES consist of co-
referenced entity mentions, which refer to a span
of text in the source file. The entity mentions are
also used as part of the annotation of RELATIONS
and EVENTS, as a stand in for the whole ENTITY.
The ACE program had a scoring metric de-
scribed in (Doddington et al., 2004). However,
our emphasis for IAA evaluation is somewhat dif-
ferent than that of scoring annotation files for ac-
curacy with regard to a gold standard. The IAA
system aims to produce output to help an annota-
tion manager understand the sorts of errors occur-
ring, and the general range of possible problems.
Nevertheless, the approach to IAA evaluation de-
scribed here can be used for scoring as well. This
approach is inspired by the IAA work for tree-
banks in Kulick et al. (2013).
Because the entity mentions in ERE are the fun-
damental units used for the ENTITY, EVENT and
RELATION annotations, they are also the funda-
mental units upon which the IAA evaluation is
based. The description of the system therefore be-
gins with a focus on the evaluation of the consis-
tency of the entity mention annotations. We derive
a mapping between the entity mentions between
the two files (henceforth called File A and File
B). We then move on to ENTITIES, RELATIONS,
and EVENTS, pointing out the differences between
them for purposes of evaluation, but also their sim-
ilarities.
1
This is a first towards a more accurate use of
the full ENTITIES in the comparison and scoring
of ENTITIES and EVENTS annotations. Work to
expand in this direction is in progress. When a
more complete system is in place it will be more
appropriate to report corpus-based results.
2 Entity Mentions
There are two main aspects to the system?s han-
dling of entity mentions. First we describe the
mapping of entity mentions between the two an-
notators. As in Doddington et al. (2004), the pos-
sibility of overlapping mentions can make this a
complex problem. Second, we describe how our
system?s output categorizes possible errors.
1
This short paper focuses on the design of the IAA sys-
tem, rather than reporting on the results for a specific dataset.
The IAA system has been run on dually annotated ERE data,
however, which was the source for the examples in this paper.
21
m-502
m-892m-398
SOUTH OF IRANm-463A'smentionsB's mentions THE EASTTHE EAST     AND    SOUTH OF     IRAN
Figure 1: Case of ambiguous Entity Mention map-
ping disambiguated by another unambiguous map-
ping
2.1 Mapping
As mentioned in the introduction, our system de-
rives a mapping between the entity mentions in
Files A and B, as the basis for all further eval-
uation of the ERE annotations. Entity mentions
in Files A and B which have exactly the same lo-
cation (offset and length) are trivially mapped to
each other. We refer to these as ?exact? matches.
The remaining cases fall into two categories.
One is the case of when an entity mention in one
file overlaps with one and only one entity men-
tion in the other file. We refer to these as the ?un-
ambiguous? overlapping matches. It is also pos-
sible for an entity mention in one file to overlap
with more than one entity mention in the other file.
We refer to these as the ?ambiguous? overlapping
matches, and these patterns can get quite complex
if multiple ambiguous overlapping matches are in-
volved.
2.1.1 Disambiguation by separate
unambiguous mapping
Here an ambiguous overlapping is disambiguated
by the presence of an unambiguous mapping, and
the choice for mapping the ambiguous case is de-
cided by the desire to maximize the number of
mapped entity mentions.
Figure 1 shows such a case. File A has two en-
tity mentions annotations (m-502 and m-463) and
File B has two entity mention annotations (m-398
and m-892). These all refer to the same span of
text, so m-502 (THE EAST) and m-463 (SOUTH
OF IRAN) both overlap with m-398 in File B
(THE EAST AND SOUTH OF IRAN). m-463 in
addition overlaps with m-892 (IRAN).
We approach the mapping from the perspective
of File A. If we assign the mapping for m-463 to
be m-398, it will leave m-502 without a match,
since m-398 will already be used in the mapping.
Therefore, we assign m-502 and m-398 to map to
m-905
m-788
TALIBAN     MILITIA  m-892A'smentionsB's mentions THE NOW-OUSTED TALIBAN     MILITIA
Figure 2: Case of Entity Mention mapping re-
solved by maximum overlap
each other, while m-463 and m-892 are mapped to
each other. The goal is to match as many mentions
as possible, which this accomplishes.
2.1.2 Disambiguation by maximum overlap
The other case is shown in Figure 2. Here there are
two mentions in File A, m-892 (TALIBAN MILI-
TIA) and m-905 (TALIBAN), both overlapping
with one mention in File B, m-788 (THE NOW-
OUSTED TALIBAN MILITIA), so it is not pos-
sible to have a matching of all the mentions. We
choose the mapping with greatest overlap, in terms
of characters, and so m-892 and m-788 are taken
to match, while m-905 is left without a match.
For such cases of disambiguation by maximum
overlap, it may be possible that a different match-
ing, the one with less overlap, might be a better
fit for one of the higher levels of annotation. This
issue will be resolved in the future by using ENTI-
TIES rather than ENTITY MENTIONS as the units
to compare for the RELATION and EVENT levels.
2.2 Categorization of annotation
inconsistencies
Our system produces an entity mention report that
lists the number of exact matches, the number of
overlap matches, and for Files A and B how many
entity mentions each had that did not have a corre-
sponding match in the other annotator?s file.
Entity mentions can overlap in different ways,
some of which are more ?serious? than other. We
categorize each overlapping entity mention based
on the nature of the edge differences in the non-
exact match, such as the presence or absence of a
determiner or punctuation, or other material.
In addition, both exact and overlap mentions
can match based on location, but be different as
far as the entity mention level (NAMed, NOMi-
nal, and PROnominal). The software also outputs
all such mismatches for each match.
22
SUPPORTERS IN PAKISTAN
m-333 m-1724
m-1620m-3763
ENTITYA's
ENTITYB's
A's ENTITY and B's ENTITY are a "complete" matchSUPPORTERS SUPPORTERSSUPPORTERS IN PAKISTAN
Figure 3: Complete match between File A and File
B ENTITIES despite overlapping mentionsA's ENTITY and B's ENTITY are an "incomplete" match
AL-QAEDAm-437 m-840m-2580m-424 AL-QAEDA NETWORK AL-QAEDAm-593
A's ENTITYB's ENTITY 0AL-QAEDA AL-QAEDA
Figure 4: Incomplete match between File A and
File B ENTITIES, because File B does not have a
mention corresponding to m-593 in File A
3 Entities
An ENTITY is a group of coreferenced entity men-
tions. We use the entity mention mapping dis-
cussed in Section 2 to categorize matches between
the ENTITIES as follows:
Complete match: This means that for some EN-
TITY x in File A and ENTITY y in File B, there
is a 1-1 correspondence between the mentions of
these two ENTITIES. For purposes of this catego-
rization, we do not distinguish between exact and
overlap mapping but include both as correspond-
ing mention instances, because this distinction was
already reported as part of the mention mapping.
Figure 3 shows an example of a complete
match. File A has two mentions, m-333 (SUP-
PORTERS) and m-1724 (another instance of SUP-
PORTERS). These are co-referenced together to
form a single ENTITY. In File B there are
two mentions, m-3763 (SUPPORTERS IN PAK-
ISTAN) an m-1620 (another instance of SUP-
PORTERS IN PAKISTAN). It was determined by
the algorithm for entity mention mapping in Sec-
tion 2.1 that m-333 and m-3763 are mapped to
each other, as are m-1724 and m-1620, although
each pair of mentions is an overlapping match, not
an exact match. At the ENTITY level of corefer-
ences mentions, there is a 1-1 mapping between
the mentions of A?s ENTITY and B?s ENTITY.
Therefore these two ENTITIES are categorized as
having a complete mapping between them.
Incomplete match: This means that for some EN-
TITY x in file A and ENTITY y in file B, there may
be some mentions that are part of x in A that have
no match in File B, but all the mentions that are
part of x map to mentions that are part of EN-
TITY y in File B, and vice-versa. Figure 4 shows
an example of an incomplete match. File A has
three entity mentions, m-437 (AL-QAEDA), m-
593 (AL-QAEDA NETWORK), and m-840 (AL-
QAEDA again), coreferenced together as a single
ENTITY. File B has two entity mentions, m-424
(AL-QAEDA) and m-2580 (AL-QAEDA again),
coreferenced together as a single ENTITY. While
m-437 maps to m-424 and m-840 maps to m-2580,
m-593 does not have a match in File B, causing
this to be categorized as an incomplete match.
No match: It is possible that some ENTITIES may
not map to an ENTITY in the other file, if the con-
ditions for neither type of match exist. For exam-
ple, if in Figure 4 m-593 mapped to a mention in
File B that was part of a different ENTITY than m-
424 and m-2580, then there would not be even an
incomplete match between the two ENTITIES.
Similar to the mentions, ENTITIES as a whole
can match as complete or incomplete, but still dif-
fer on the entity type (ORGanization, PERson,
etc.). We output such type mismatches as separate
information for the ENTITY matching.
4 Relations
A RELATION is defined as having:
1) Two RELATION arguments, each of which is an
ENTITY.
2) An optional ?trigger?, a span of text.
3) A type and subtype. (e.g., ?Physical.Located?)
For this preliminary stage of the system, we
match RELATIONS in a similar way as we do
the ENTITIES, by matching the corresponding en-
tity mentions, as stand-ins for the ENTITY argu-
ments for the RELATION. We use the previously-
established mapping of mentions as basis of the
RELATION mapping.
2
We report four types of RELATION matching:
3
1) exact match - This is the same as the complete
2
This is a stricter mapping requirement than is ultimately
necessary, and future work will adjust the basis of RELATION
mapping to be full ENTITIES.
3
Because of space reasons and because RELATIONS are
so similar to EVENTS, we do not show here an illustration of
RELATION mapping.
23
match for ENTITIES, except in addition checking
for a trigger match and type/subtype.
2) types different - a match for the arguments, al-
though the type or subtypes of the RELATIONS do
not match. (The triggers may or may not be differ-
ent for this case.)
3) triggers different - a match for the arguments
and type/subtype, although with different triggers.
4) no match - the arguments for a RELATION in
one file do not map to arguments for any one sin-
gle RELATION in the other file.
5 Events
The structure of an EVENT is similar to that of a
RELATION. Its components are:
1) One or more EVENT arguments. Each EVENT
argument is an ENTITY or a date.
2) An obligatory trigger argument.
3) A type and subtype (e.g., ?Life.MARRY?)
In contrast to RELATIONS, the trigger argument
is obligatory. There must be at least one ENTITY
argument (or a date argument) in order for the
EVENT to qualify for annotation, although it does
not need to be exactly two, as with RELATIONS.
The mapping between EVENTS works essen-
tially as for ENTITIES and RELATIONS, once again
based on the already-established mapping of the
entity mentions.
4
There are two slight twists, how-
ever. It is possible for the only EVENT argument
to be a date, which is not an entity mention, and so
we must also establish a mapping for EVENT date
arguments, as we did for the entity mentions. Be-
cause the trigger is obligatory, we treat it with the
same level of importance as the arguments, and es-
tablish a mapping between EVENT triggers as well.
We report three types of EVENT matching:
5
1) exact match - all arguments match, as does the
trigger, as well as the type/subtype.
2) types different - a match for the arguments
and trigger, although the type or subtypes of the
EVENTS do not match.
3) no match - either the arguments for a EVENT in
4
As with relations, this is a stricter mapping than neces-
sary, and future work will adjust to use ENTITIES as EVENT
arguments.
5
Currently, if an EVENT argument does not map to any
mention in the other file, we consider the EVENT to be a ?no
match?. In the future we will modify this (and likewise for
RELATIONS) to be more forgiving, along the lines of the ?in-
complete match? for ENTITIES.
JULY 30, 2008 m-489
m-255 m-268
POLICEm-515 triggeragent
agent
person
JULY 30, 2008
MEXICO CITYplacedate m-502
m-292 THE POLICE
APPREHENDED
APPREHENDEDMEXICO CITY A DRUG TRAFFICKERpersontriggerplacedate
A's
EVENTB's
EVENT A DRUG TRAFFICKER
Figure 5: EVENT match
one file do not map to arguments for any one single
EVENT in the other file, or the triggers do not map.
Figure 5 shows an example of an exact match
for two EVENTS, one each in File A and B. All
of the arguments in one EVENT map to an argu-
ment in the other EVENT, as does the trigger. Note
that the argument m-502 (an entity mention, PO-
LICE) in File A maps to argument m-255 (an en-
tity mention, THE POLICE) in File B as an over-
lap match, although the EVENTS are considered an
exact match.
6 Future work
We did these comparisons based on the lowest en-
tity mention level in order to develop a prelimi-
nary system. However, the arguments for EVENTS
and RELATIONS are ENTITIES, not entity men-
tions, and the system be adjusted to do the correct
comparison. Work to adjust the system in this di-
rection is in progress. When the full system is in
place in this way, we will report results as well. In
future work we will be developing a quantitative
scoring metric based on the work described here.
Acknowledgments
This material is based on research sponsored by
Air Force Research Laboratory and Defense Ad-
vance Research Projects Agency under agreement
number FA8750-13-2-0045. The U.S. Govern-
ment is authorized to reproduce and distribute
reprints for Governmental purposes notwithstand-
ing any copyright notation thereon. The views and
conclusions contained herein are those of the au-
thors and should not be interpreted as necessar-
ily representing the official policies or endorse-
ments, either expressed or implied, of Air Force
Research Laboratory and Defense Advanced Re-
search Projects Agency or the U.S. Government.
24
References
George Doddington, Alexis Mitchell, Mark Przybocki,
Lance Ramshaw, Stephanie Strassel, and Ralph
Weischedel. 2004. Automatic content extraction
(ACE) program - task definitions and performance
measures. In LREC 2004: 4th International Confer-
ence on Language Resources and Evaluation.
Seth Kulick, Ann Bies, Justin Mott, Mohamed
Maamouri, Beatrice Santorini, and Anthony Kroch.
2013. Using derivation trees for informative tree-
bank inter-annotator agreement evaluation. In Pro-
ceedings of the 2013 Conference of the North Amer-
ican Chapter of the Association for Computational
Linguistics: Human Language Technologies, pages
550?555, Atlanta, Georgia, June. Association for
Computational Linguistics.
LDC. 2014a. DEFT ERE Annotation Guidelines: En-
tities v1.6. Technical report, Linguistic Data Con-
sortium.
LDC. 2014b. DEFT ERE Annotation Guidelines:
Events v1.3. Technical report, Linguistic Data Con-
sortium.
LDC. 2014c. DEFT ERE Annotation Guidelines: Re-
lations v1.3. Technical report, Linguistic Data Con-
sortium.
25
