Summarizing Email Threads
Owen Rambow Lokesh Shrestha John Chen Chirsty Lauridsen
Columbia University Columbia University Microsoft Research Asia Columbia University
New York, NY, USA New York, NY, USA Beijing, China New York, NY, USA
rambow@cs.columbia.edu, lokesh@cs.columbia.edu
v-johnc@msrchina.research.microsoft.com, christy@columbia.edu
Abstract
Summarizing threads of email is different from
summarizing other types of written communi-
cation as it has an inherent dialog structure. We
present initial research which shows that sen-
tence extraction techniques can work for email
threads as well, but profit from email-specific
features. In addition, the presentation of the
summary should take into account the dialogic
structure of email communication.
1 Introduction
In this paper, we discuss work on summarizing email
threads, i.e., coherent exchanges of email messages
among several participants.1 Email is a written medium
of asynchronous multi-party communication. This means
that, unlike for example news stories but as in face-to-face
spoken dialog, the email thread as a whole is a collabo-
rative effort with interaction among the discourse partici-
pants. However, unlike spoken dialog, the discourse par-
ticipants are not physically co-present, so that the writ-
ten word is the only channel of communication. Fur-
thermore, replies do not happen immediately, so that re-
sponses need to take special precautions to identify rele-
vant elements of the discourse context (for example, by
citing previous messages). Thus, email is a distinct lin-
guistic genre that poses its own challenges to summariza-
tion.
In the approach we propose in this paper, we follow
the paradigm used for other genres of summarization,
namely sentence extraction: important sentences are ex-
tracted from the thread and are composed into a summary.
Given the special characteristics of email, we predict that
certain email-specific features can help in identifying rel-
evant sentences for extraction. In addition, in presenting
the extracted summary, special ?wrappers? ensure that
1The work reported in this paper was funded under the KDD
program. We would like to thank three anonymous reviewers
for very insightful and helpful comments.
the reader can reconstruct the interactional aspect of the
thread, which we assume is crucial for understanding the
summary. We acknowledge that other techniques should
also be explored for email summarization, but leave that
to separate work.
2 Previous and Related Work
Muresan et al (2001) describe work on summarizing in-
dividual email messages using machine learning ap-
proaches to learn rules for salient noun phrase extraction.
In contrast, our work aims at summarizing whole threads
and at capturing the interactive nature of email.
Nenkova and Bagga (2003) present work on generat-
ing extractive summaries of threads in archived discus-
sions. A sentence from the root message and from each
response to the root extracted using ad-hoc algorithms
crafted by hand. This approach works best when the sub-
ject of the root email best describes the ?issue? of the
thread, and when the root email does not discuss more
than one issue. In our work, we do not make any assump-
tions about the nature of the email, and learn sentence
extraction strategies using machine learning.
Newman and Blitzer (2003) also address the problem
of summarizing archived discussion lists. They cluster
messages into topic groups, and then extract summaries
for each cluster. The summary of a cluster is extracted
using a scoring metric based on sentence position, lexical
similarity of a sentence to cluster centroid, and a feature
based on quotation, among others. While the approach is
quite different from ours (due to the underlying clustering
algorithm and the absence of machine learning to select
features), the use of email-specific features, in particular
the feature related to quoted material, is similar.
Lam et al (2002) present work on email summariza-
tion by exploiting the thread structure of email conver-
sation and common features such as named entities and
dates. They summarize the message only, though the con-
tent of the message to be summarized is ?expanded? us-
ing the content from its ancestor messages. The expanded
message is passed to a document summarizer which is
used as a black box to generate summaries. Our work, in
contrast, aims at summarizing the whole thread, and we
are precisely interested in changing the summarization al-
gorithm itself, not in using a black box summarizer.
In addition, there has been some work on summarizing
meetings. As discussed in Section 1, email is different
in important respects from (multi-party) dialog. How-
ever, some important aspects are related. Zechner (2002),
for example, presents a meeting summarization system
which uses the MMR algorithm to find sentences that are
most similar to the segment and most dissimilar to each
other. The similarity weights in the MMR algorithm are
modified using three features, including whether a sen-
tence belongs to a question-answer pair. The use of the
question-answer pair detection is an interesting proposal
that is also applicable to our work. However, overall most
of the issues tackled by Zechner (2002) are not relevant
to email summarization.
3 The Data
Our corpus consists of 96 threads of email sent during
one academic year among the members of the board of
the student organization of the ACM at Columbia Uni-
versity. The emails dealt mainly with planning events of
various types, though other issues were also addressed.
On average, each thread contained 3.25 email messages,
with all threads containing at least two messages, and the
longest thread containing 18 messages.
Two annotators each wrote a summary of the thread.
We did not provide instructions about how to choose con-
tent for the summaries, but we did instruct the annotators
on the format of the summary; specifically, we requested
them to use the past tense, and to use speech-act verbs and
embedded clauses (for example, Dolores reported she?d
gotten 7 people to sign up instead of Dolores got 7 peo-
ple to sign up). We requested the length to be about 5%
to 20% of the original text length, but not longer than 100
lines.
Writing summaries is not a task that competent native
speakers are necessarily good at without specific train-
ing. Furthermore, there may be many different possible
summary types that address different needs, and differ-
ent summaries may satisfy a particular need. Thus, when
asking native speakers to write thread summaries we can-
not expect to obtain summaries that are similar.
We then used the hand-written summaries to identify
important sentences in the threads in the following man-
ner. We used the sentence-similarity finder SimFinder
(Hatzivassiloglou et al, 2001) in order to rate the sim-
ilarity of each sentence in a thread to each sentence in
the corresponding summary. SimFinder uses a combi-
nation of lexical and linguistic features to assign a sim-
ilarity score to an input pair of texts. We excluded sen-
tences that are being quoted, as well as signatures and
the like. For each sentence in the thread, we retained
the highest similarity score. We then chose a threshold;
sentences with SimFinder scores above this threshold are
then marked as ?Y?, indicating that they should be part
of a summary, while the remaining sentences are marked
?N?. About 26% of sentences are marked ?Y?. All sen-
tences from the email threads along with their classifica-
tion constitutes our data. For annotator DB, we have 1338
sentences, of which 349 are marked ?Y?, for GR (who
has annotated a subset of the threads that DB has anno-
tated) there are 1296 sentences, of which 336 are marked
?Y?. Only 193 sentences are marked ?Y? using the sum-
maries of both annotators, reflecting the difference in the
summaries written by the two annotators. The kappa for
the marking of the sentences is 0.29 (recall that this only
indirectly reflects annotator choice). Thus, our expecta-
tion that human-written summaries will show great vari-
ation was borne out; we discuss these differences further
in Section 5.
4 Features for Sentence Extraction
We start out with features that are not specific to email.
These features consider the thread as a single text. We
call this feature set basic. Each sentence in the email
thread is represented by a feature vector. We shall call
the sentence in consideration s, the message in which the
sentence appears m, the thread in which the sentence ap-
pears t, and the entire corpus c. (We omit some features
we use for lack of space.)
? thread line num: The absolute position of s in t.
? centroid sim: Cosine similarity of s?s TF-IDF vec-
tor (excluding stop words) with t?s centroid vector.
The centroid vector is the average of the TF-IDF
vectors of all the sentences in t. The IDF compo-
nent is derived from the ACM Corpus.
? centroid sim local: Same as centroid sim except
that the inverse document frequencies are derived
from the thread.
? length: The number of content terms in s.
? tfidfsum: Sum of the TF-IDF weights of content
terms in s. IDF weights are derived from c.
? tfidfavg: Average TF-IDF weight of the content
terms in s. IDF weights are derived from c.
? t rel pos: Relative position of s in t: the number of
sentences preceding s divided by the total number of
sentences in t. All messages in a thread are ordered
linearly by the time they were sent.
? is Question: Whether s is a question, as determined
by punctuation.
Ann. Feature set ctroid basic basic+ full
DB Recall 0.255 0.315 0.370 0.421
DB Precision 0.298 0.553 0.584 0.607
DB F-measure 0.272 0.401 0.453 0.497
GR Recall 0.291 0.217 0.193 0.280
GR Precision 0.333 0.378 0.385 0.475
GR F-measure 0.311 0.276 0.257 0.352
Figure 1: Results for annotators DB and GR using differ-
ent feature sets
We now add two features that take into account the di-
vision of the thread into messages and the resulting dia-
log structure. The union of this feature set with basic is
called basic+.
? msg num: The ordinality of m in t (i.e., the absolute
position of m in t).
? m rel pos: Relative position of s in m: the number
of sentences preceding s divided by the total number
of sentences in m.
Finally, we add features which address the specific
structure of email communication. The full feature set
is called full.
? subject sim: Overlap of the content words of the
subject of the first message in t with the content
words in s.
? num of res: Number of direct responses to m.
? num Of Recipients: Number of recipients of m.
? fol Quote: Whether s follows a quoted portion in m.
5 Experiments and Results
This section describes experiments using the machine
learning program Ripper (Cohen, 1996) to automatically
induce sentence classifiers, using the features described
in Section 4. Like many learning programs, Ripper takes
as input the classes to be learned, a set of feature names
and possible values, and training data specifying the class
and feature values for each training example. In our case,
the training examples are the sentences from the threads
as described in Section 3. Ripper outputs a classifica-
tion model for predicting the class (i.e., whether a sen-
tence should be in a summary or not) of future exam-
ples; the model is expressed as an ordered set of if-then
rules. We obtained the results presented here using five-
fold cross-validation. In this paper, we only evaluate the
results of the machine learning step; we acknowledge the
need for an evaluation of the resulting summaries using
DB only GR only avg max
Recall 0.421 0.280 0.212 0.268
Precision 0.607 0.475 0.406 0.444
F-measure 0.497 0.352 0.278 0.335
Figure 2: Results for combining two annotators (last two
columns) using full feature set
word/string based similarity metric and/or human judg-
ments and leave that to future publications.
We show results for the two annotators and different
feature sets in Figure 1. First consider the results for
annotator DB. Recall that basic includes only standard
features that can be used for all text genres, and consid-
ers the thread a single text. basic+ takes the breakdown
of the thread into messages into account. full also uses
features that are specific to email threads. We can see
that by using more features than the baseline set basic,
performance improves. Specifically, using email-specific
features improves the performance over the basic base-
line, as we expected. We also give a second baseline,
ctroid, which we determined by choosing the top 20% of
sentences most similar to the thread centroid. All results
using Ripper improve on this baseline.
If we perform exactly the same experiments on the
summaries written by annotator GR, we obtain the re-
sults shown in the bottom half of Figure 1. The results are
much worse, and the centroid-based baseline outperforms
all but the full feature set. We leave to further research
an explanation of why this may be the case; we speculate
that GR,as an annotator, is less consistent in her choice of
material than is DB when forming a summary. Thus, the
machine learner has less regularity to learn from. How-
ever, we take this difference as evidence for the claim that
one should not expect great regularity in human-written
summaries.
Finally, we investigated what happens when we com-
bine the data from both sources, DB and GR. Using
SimFinder, we obtained two scores for each sentence, one
that shows the similarity to the most similar sentence in
DB?s summary, and one that shows the similarity to the
most similar sentence in GR?s summary. We can com-
bine these two scores and then use the combined score in
the same way that we used the score from a single anno-
tator. We explore two ways of combining the scores: the
average, and the maximum. Both ways of combining the
scores result in worse scores than either annotator on his
or her own; the average is worse than the maximum (see
Figure 2). We interpret these results again as meaning
that there is little convergence in the human-written sum-
maries, and it may be advantageous to learn from one
particular annotator. (Of course, another option might be
to develop and enforce very precise guidelines for the an-
1 IF centroid sim local ? 0.32215 AND thread line num ? 4 AND isQuestion = 1
AND tfidfavg ? 0.212141 AND tfidfavg ? 0.301707 THEN Y.
2 IF centroid sim ? 0.719594 AND numOfRecipients ? 8 THEN Y.
3 IF centroid sim local ? 0.308202 AND thread line num ? 4 AND tfidfmax ? 0.607829
AND m rel pos ? 0.363636 AND t rel pos ? 0.181818 THEN Y.
4 IF subject sim ? 0.333333 tfidfsum ? 2.83636 tfidfsum ? 2.64262 tfidfmax ? 0.675917 THEN Y.
5 ELSE N.
Figure 3: Sample rule set generated from DB data (simplified for reasons of space)
Regarding ?acm home/bjarney?, on Apr 9, 2001, Muriel
Danslop wrote: Two things: Can someone be responsible
for the press releases for Stroustrup?
Responding to this on Apr 10, 2001, Theresa Feng wrote:
I think Phil, who is probably a better writer than most of
us, is writing up something for dang and Dave to send out
to various ACM chapters. Phil, we can just use that as our
?press release?, right?
In another subthread, on Apr 12, 2001, Kevin Danquoit
wrote: Are you sending out upcoming events for this
week?
Figure 4: Sample summary obtained with the rule set in
Figure 3
notators as to the contents of the summaries.)
A sample rule set obtained from DB data is shown in
Figure 3. Some rules are intuitively appealing: for ex-
ample, rule 1 states that questions at the beginning of a
thread that are similar to entire thread should be retained,
and rule 2 states that sentence which are very similar to
the thread and which have a high number of recipients
should be retained. However, some rules show signs of
overfitting, for example rule 1 limits the average TF-IDF
values to a rather narrow band. Hopefully, more data
will alleviate the overfitting problem. (The data collec-
tion continues.)
6 Postprocessing Extracted Sentences
Extracted sentences are sent to a module that wraps these
sentences with the names of the senders, the dates at
which they were sent, and a speech act verb. The speech
act verb is chosen as a function of the structure of the
email thread in order to make this structure more appar-
ent to the reader. Further, for readability, the sentences
are sorted by the order in which they were sent. An ex-
ample can be seen in Figure 4. Note that while the initial
question is answered in the following sentence, two other
questions are left unanswered in this summary (the an-
swers are in fact in the thread).
7 Future Work
In future work, we will perform a qualitative error anal-
ysis and investigate in more detail what characteristics
of DB?s summaries lead to better extractive summaries.
We can use this insight to instruct human annotators, and
to improve the automatic extraction. We intend to learn
predictors for some other thread aspects such as thread
category and question-answer pairs, and then use these
as input to the sentence extraction procedure. For ex-
ample, identifying question-answer pairs appears to be
important for generating ?complete? summaries, as illus-
trated by the sample summary. We also intend to perform
an evaluation based on human feedback.
References
William Cohen. 1996. Learning trees and rules with
set-valued features. In Fourteenth Conference of the
American Association of Artificial Intelligence. AAAI.
Vasileios Hatzivassiloglou, Judith Klavans, Melissa Hol-
combe, Regina Barzilay, Min-Yen Kan, and Kathleen
McKeown. 2001. SimFinder: A flexible cluster-
ing tool for summarization. In Proceedings of the
NAACL Workshop on Automatic Summarization, Pitts-
burgh, PA.
Derek Lam, Steven L. Rohall, Chris Schmandt, and
Mia K. Stern. 2002. Exploiting e-mail structure to
improve summarization. In ACM 2002 Conference on
Computer Supported Cooperative Work (CSCW2002),
Interactive Posters, New Orleans, LA.
Smaranda Muresan, Evelyne Tzoukermann, and Judith
Klavans. 2001. Combining Linguistic and Ma-
chine Learning Techniques for Email Summarization.
In Proceedings of the CoNLL 2001 Workshop at the
ACL/EACL 2001 Conference.
Ani Nenkova and Amit Bagga. 2003. Facilitating email
thread access by extractive summary generation. In
Proceedings of RANLP, Bulgaria.
Paula Newman and John Blitzer. 2003. Summarizing
archived discussions: a beginning. In Proceedings of
Intelligent User Interfaces.
Klaus Zechner. 2002. Automatic summarization of
open-domain multiparty dialogues in diverse genres.
Computational Linguistics, 28(4):447?485.
 
		Towards Automatic Generation of Natural Language Generation
Systems
John Chen?, Srinivas Bangalore?, Owen Rambow?, and Marilyn A. Walker?
Columbia University? AT&T Labs?Research?
New York, NY 10027 Florham Park, NJ 07932
{jchen,rambow}@cs.columbia.edu {srini,walker}@research.att.com
Abstract
Systems that interact with the user via natural
language are in their infancy. As these systems
mature and become more complex, it would be
desirable for a system developer if there were
an automatic method for creating natural lan-
guage generation components that can produce
quality output efficiently. We conduct experi-
ments that show that this goal appears to be
realizable. In particular we discuss a natural
language generation system that is composed of
SPoT, a trainable sentence planner, and FER-
GUS, a stochastic surface realizer. We show
how these stochastic NLG components can be
made to work together, that they can be ported
to new domains with apparent ease, and that
such NLG components can be integrated in a
real-time dialog system.
1 Introduction
Systems that interact with the user via natural
language are in their infancy. As these systems
mature and become more complex, it would
be desirable for a system developer if there
were automatic methods for creating natural
language generation (NLG) components that
can produce quality output efficiently. Stochas-
tic methods for NLG may provide such auto-
maticity, but most previous work (Knight and
Hatzivassiloglou, 1995), (Langkilde and Knight,
1998), (Oh and Rudnicky, 2000), (Uchimoto et
al., 2000), (Bangalore and Rambow, 2000) con-
centrate on the specifics of individual stochastic
methods, ignoring other issues such as integra-
bility, portability, and efficiency. In contrast,
this paper investigates how different stochastic
NLG components can be made to work together
effectively, whether they can easily be ported to
new domains, and whether they can be inte-
grated in a real-time dialog system.
Request(DEPART?DATE)
Surface Generator
FERGUS
TTS
SPoT
Dialog Manager
Sentence Planner
DM
Imp?conf(N)
soft?merge
Text to Speech
Implicit?confirm(NEWARK)
Implicit?confirm(DALLAS)
period
Imp?conf(D)
Flying from Newark to
Dallas.  What date would
you like to leave?
Request(D?D)
Figure 1: Components of an NLG system.
Recall the basic tasks in NLG. During text
planning, content and structure of the target
text are determined to achieve the overall com-
municative goal. During sentence planning, lin-
guistic means?in particular, lexical and syn-
tactic means?are determined to convey smaller
pieces of meaning. During realization, the spec-
ification chosen in sentence planning is trans-
formed into a surface string by linearizing and
inflecting words in the sentence (and typically,
adding function words). Figure 1 shows how
such components cooperate to generate text
corresponding to a set of communicative goals.
Our work addresses both the sentence plan-
ning stage and the realization stage. The sen-
tence planning stage is embodied by the SPoT
sentence planner (Walker et al, 2001), while
the surface realization stage is embodied by the
FERGUS surface realizer (Bangalore and Ram-
bow, 2000). We extend the work of (Walker et
al., 2001) and (Bangalore and Rambow, 2000)
in various ways. We show that apparently each
of SPoT and FERGUS can be ported to differ-
ent domains with little manual effort. We then
show that these two components can work to-
gether effectively. Finally, we show the on-line
integration of FERGUS with a dialog system.
2 Testing the Domain Independence
of Sentence Planning
In this section, we address the issue of the
amount of effort that is required to port a sen-
tence planner to new domains. In particular,
we focus on the SPoT sentence planner. The
flexibility of the training mechanism that SPoT
employs allows us to perform experiments that
provide evidence for its domain independence.
Being a sentence planner, SPoT chooses ab-
stract linguistic resources (meaning-bearing lex-
emes, syntactic constructions) for a text plan. A
text plan is a set of communicative goals which
is assumed to be output by a dialog manager of
a spoken dialog system. The output of SPoT is
a set of ranked sentence plans, each of which is
a binary tree with leaves labeled by the commu-
nicative goals of the text plan.
SPoT divides the sentence planning task into
two stages. First, the sentence-plan-generator
(SPG) generates 12-20 possible sentence plans
for a given input text plan. These are gener-
ated randomly by incrementally building each
sentence plan according to some probability
distribution. Second, the sentence-plan-ranker
(SPR) ranks the resulting set of sentence plans.
SPR is trained for this task via RankBoost (Fre-
und et al, 1998), a machine learning algorithm,
using as training data sets of sentence plans
ranked by human judges.
In porting SPoT to a new domain, this last
point seems to be a hindrance. New train-
ing data in the new domain ranked by hu-
man judges might be needed in order to train
SPoT. To the contrary, our experiments that
show that this need not be the case. We par-
tition the set of all features used by (Walker et
al., 2001) to train SPoT into three subsets ac-
cording to their level of domain and task de-
pendence. Domain independent features are
features whose names include only closed-class
words, e.g. ?in,? or names of operations that in-
crementally build the sentence plan, e.g. merge.
Domain-dependent, task-independent features
are those whose names include open class words
Features Used Mean Score S.D.
all 4.56 0.68
domain-independent 4.55 0.69
task-independent 4.20 0.99
task-dependent 3.90 1.19
Table 1: Results for subsets of features used to
train SPoT
specific to this domain, e.g. ?travel? or the
names of the role slots, e.g. $DEST-CITY. Do-
main dependent, task dependent features are
features whose names include the value of a role
filler for the domain, e.g. ?Albuquerque.?
We have trained and tested SPoT with these
different feature subsets using the air-travel do-
main corpus of 100 text plans borrowed from
(Walker et al, 2001), using five fold cross-
validation. Results are shown in Table 2 us-
ing t-tests with the modified Bonferroni statis-
tic for multiple comparisons. Scores can range
from 1.0 (worst) to 5.0 (best). The results in-
dicate that the domain independent feature set
performs as well as all the features (t = .168, p
= .87), but that both the task independent (t
= 6.25, p = 0.0) and the task dependent (t =
4.58, p = 0.0) feature sets perform worse.
3 Automation in Training a Surface
Realizer
As with the sentence planning task, there is the
possibility that the task of surface realization
may be made to work in different domains with
relatively little manual effort. Here, we perform
experiments using the FERGUS surface realizer
to determine whether this may be so. We re-
view the FERGUS architecture, enumerate re-
sources required to train FERGUS, recapitulate
previous experiments that indicate how these
resources can be automatically generated, and
finally show how similar ideas can be used to
port FERGUS to different domains with little
manual effort.
3.1 Description of the FERGUS
Surface Realizer
Given an underspecified dependency tree repre-
senting one sentence as input, FERGUS outputs
the best surface string according to its stochas-
tic modeling. Each node in the input tree corre-
sponds to a lexeme. Nodes that are related by
grammatical function are linked together. Sur-
face ordering of the lexemes remains unspecified
in the tree.
FERGUS consists of three models: tree
chooser, unraveler, and linear precedence
chooser. The tree chooser associates a su-
pertag (Bangalore and Joshi, 1999) from a tree-
adjoining grammar (TAG) with each node in
the underspecified dependency tree. This par-
tially specifies the output string?s surface order;
it is constrained by grammatical constraints en-
coded by the supertags (e.g. subcategorization
constraints, voice), but remains free otherwise
(e.g. ordering of modifiers). The tree chooser
uses a stochastic tree model (TM) to select a
supertag for each node in the tree based on lo-
cal tree context. The unraveler takes the re-
sulting semi-specified TAG derivation tree and
creates a word lattice corresponding to all of
the potential surface orderings consistent with
this tree. Finally, the linear precedence (LP)
chooser finds the best path through the word
lattice according to a trigram language model
(LM), specifying the output string completely.
Certain resources are required in order to
train FERGUS. A TAG grammar is needed?
the source of the supertags with which the
semi-specified TAG derivation tree is annotated.
There needs to be a treebank in order to ob-
tain the stochastic model TM driving the tree
chooser. There also needs to be a corpus of sen-
tences in order to train the language model LM
required for the LP chooser.
3.2 Labor-Minimizing Approaches to
Training FERGUS
The resources that are needed to train FER-
GUS seem quite labor intensive to develop. But
(Bangalore et al, 2001) show that automati-
cally generated version of these resources can
be used by FERGUS to obtain quality output.
Two kinds of TAG grammar are used in (Ban-
galore et al, 2001). One kind is a manually de-
veloped, broad-coverage grammar for English:
the XTAG grammar (XTAG-Group, 2001). It
consists of approximately 1000 tree frames. Dis-
advantages of using XTAG are the consider-
able amount of human labor expended in its
development and the lack of a treebank based
on XTAG?the only way to estimate parame-
ters in the TM is to rely on a heuristic map-
ping of XTAG tree frames onto a pre-existing
treebank (Bangalore and Joshi, 1999). Another
kind of grammar is a TAG automatically ex-
tracted from a treebank using the techniques of
(Chen, 2001) (cf. (Chiang, 2000), (Xia, 1999)).
These techniques extract a linguistically mo-
tivated TAG using heuristics programmed us-
ing a modicum of human labor. They nullify
the disadvantages of using the XTAG grammar,
but they introduce potential complications?
notably, an extracted grammar?s size is often
much larger than that of XTAG, typically more
than 2000 tree frames, potentially leading to a
larger sparse data problem, and also the result-
ing grammar is not hand-checked.
Two kinds of treebank are used in (Bangalore
et al, 2001). One kind is the Penn Treebank
(Marcus et al, 1993). It consists of approxi-
mately 1,000,000 words of hand-checked, brack-
eted text. The text consists of Wall Street Jour-
nal news articles. The other kind of treebank is
the BLLIP corpus (Charniak, 2000). It con-
sists of approximately 40,000,000 words of text
that has been parsed by a broad-coverage sta-
tistical parser. The text consists of Wall Street
Journal news and newswire articles. The ad-
vantage of the former is that it has been hand-
checked, whereas the latter has the advantage
of being easily produced and hence can easily
be enlarged.
(Bangalore et al, 2001) experimentally de-
termine how the quality and quantity of the
resources used in training FERGUS affect the
output quality of the generator. They find that
while a better quality annotated corpus (Penn
Treebank) results in better model accuracy than
a lower quality corpus (BLLIP) of the same size,
an (easily-obtained) larger lower quality corpus
results in a model that eclipses a smaller, better
quality treebank. Also, the model that is ob-
tained when using an automatically extracted
grammar yields comparable output quality to
the model that is obtained when using a hand-
crafted (XTAG) grammar.
3.3 Automating Adaptation of
FERGUS to a New Domain
This paper is about minimizing the amount of
manual labor that is required to port NLG com-
ponents to different domains. (Bangalore et
al., 2001) perform all of their experiments on
the same domain of Wall Street Journal news
articles. In contrast, in this section we show
that FERGUS can be adapted to the domain of
air-travel reservation dialogs with minimal hu-
man effort. We show that out-of-domain train-
ing data can be used instead of in-domain data
without drastically compromising output qual-
ity. We also show that partially parsed in-
domain training data can be effectively used
to train the TM. Finally, we show that using
an in-domain corpus to train the LM can help
the output quality, even if that corpus is of
small size. In this section, we first describe the
training resources that are used in these exper-
iments. We subsequently describe the experi-
ments themselves and their results.
Various corpora are used in these experi-
ments. For training, there are two distinct
corpora. First, there is the previously in-
troduced Penn Treebank (PTB). As the
alternative, there is a human-human corpus of
dialogs (HH) from Carnegie Mellon University.
The HH corpus consists of approximately
13,000 words in the air-travel reservation
domain. This is not exactly the target domain
because human-human interaction differs
from human-computer interaction which is
our true target domain. From this raw text,
an LDA parser (Bangalore and Joshi, 1999)
trained using the XTAG-based Penn Treebank
creates a partially-parsed, non-hand-checked
treebank. Test data consists of about 2,200
words derived from Communicator template
data. Communicator templates are hand-
crafted surface strings of words interspersed
with slot names. An example is ?What time
would you, traveling from $ORIG-CITY
to $DEST-CITY like to leave?? The test
data is derived from all strings like these, with
duplicates, in the Communicator system by
replacing the slot names with fillers according
to a probability distribution. Furthermore,
dependency parses are assigned to the resulting
strings by hand.
In the first series of experiments, we ascertain
the output quality of FERGUS using the XTAG
grammar on different training corpora. We vary
the TM?s training corpus to be either PTB or
HH. We do the same for the LM?s training cor-
pus. Assessing the output quality of a generator
is a complex issue. Here, we select as our met-
ric understandability accuracy, defined in (Ban-
galore et al, 2000) as quantifying the differ-
PTB TM HH TM
PTB LM 0.30 0.38
HH LM 0.37 0.41
Table 2: Average understandability accuracies
using XTAG-Based FERGUS for various kinds
of training data
PTB TM
PTB LM 0.39
HH LM 0.33
Table 3: Average understandability accuracies
using automatically-extracted grammar based
FERGUS for various kinds of training data
ence between the generator output, in terms of
both dependency tree and surface string, and
the desired reference output. (Bangalore et al,
2000) finds this metric to correlate well with hu-
man judgments of understandability and qual-
ity. Understandability accuracy varies between
a high score of 1.0 and a low score which may
be less than zero.
The results of our experiments are shown in
Table 2. We conclude that despite its smaller
size, and despite its being only automatically-
and partially- parsed, using the in-domain HH
is more effective than using the out-of-domain
PTB for training the TM. Similarly, HH is more
effective than PTB for training the LM. The
best result is obtained by using HH to train both
the TM and the LM; this result (0.41) is com-
parable to the result obtained by using matched
PTB training and test data (0.43) that is used
in (Bangalore et al, 2001).
The second series of experiments investi-
gates the output quality of FERGUS using
automatically-extracted grammars. In these ex-
periments, the TM is always trained on PTB
but not HH. It is the type of training data that
is used to train the LM, either PTB or HH,
that is varied. The results are shown in Ta-
ble 3. Note that these scores are in the same
range as those obtained when training FER-
GUS using XTAG. Also, these scores show that
when using automatically-extracted grammars,
training LM using a large, out-of-domain cor-
pus (PTB) is more beneficial than training LM
using a small, in-domain corpus (HH).
We can now draw various conclusions about
training FERGUS in a new domain. Con-
sider training the TM. It is not necessary
to use a handwritten TAG in the new do-
main; a broad-coverage hand-written TAG or
an automatically-extracted TAG will give com-
parable results. Also, instead of requiring a
hand-checked treebank in the new domain, par-
tially parsed data in the new domain is ade-
quate. Now consider training the LM. Our ex-
periments show that a small corpus in the new
domain is a viable alternative to a large corpus
that is out of the domain.
4 Integration of SPoT with
FERGUS
We have seen evidence that both SPoT and
FERGUS may be easily transferable to a new
domain. Because the output of a sentence plan-
ner usually becomes the input of a surface real-
izer, questions arise such as whether SPoT and
FERGUS can be made to work together in a
new domain and what is the output quality of
the combined system. We will see that an ad-
dition of a rule-based component to FERGUS
will be necessary in order for this integration
to occur. We will subsequently see that the
output quality of the resulting combination of
SPoT and FERGUS is quite good.
Integration of SPoT as described in Section 2
and FERGUS as described in Section 3 is not
automatic. The reason is that the output of
SPoT is a deep syntax tree (Mel?c?uk, 1998)
whereas hitherto the input of FERGUS has
been a surface syntax tree. The primary dis-
tinguishing characteristic of a deep syntax tree
is that it contains features for categories such as
definiteness for nouns, or tense and aspect for
verbs. In contrast, a surface syntax tree real-
izes these features as function words. However,
there is a one-to-one mapping from features of
a deep syntax tree to function words in the cor-
responding surface syntax tree. Therefore, inte-
grating SPoT with FERGUS is basically a mat-
ter of performing this mapping. We have added
a rule-based component (RB) as the new first
stage of FERGUS to do just that. Note that it
is erroneous to think that RB makes choices be-
tween different generation options because there
is a one-to-one mapping between features and
function words.
PTB TM HH TM
PTB LM 0.48 0.47
HH LM 0.73 0.68
Table 4: Average understandability accuracies
of SPoT-integrated, XTAG-Based FERGUS for
various kinds of training data
After the addition of RB to FERGUS, we
evaluate the output quality of the combination
of SPoT and FERGUS. Only the XTAG gram-
mar is used in this experiment. As in previous
experiments with the XTAG grammar, there is
either the option of training using HH or PTB
derived data for either the TM or LM, giving a
total of four possibilities.
Test data is obtained by output strings that
are produced by the combination of SPoT and
the RealPro surface realizer (Lavoie and Ram-
bow, 1998). RealPro has the advantage of pro-
ducing high quality surface strings, but at the
cost of having to be hand-tuned to a particu-
lar domain. It is this cost we are attempting to
minimize by using FERGUS. Only those sen-
tence plans produced by SPoT ranked 3.0 or
greater by human judges are used. The surface
realization of these sentence plans yields a test
corpus of 2,200 words.
As shown in Table 4, the performance of
SPoT and FERGUS combined is quite high.
Also note that in terms of training the LM, out-
put quality is markedly better when HH is used
rather than PTB. Furthermore, note that there
is a smaller difference between using PTB or
HH to train TM when compared to previous re-
sults shown in Table 2. This seems to indicate
that the TM?s effect on output quality dimin-
ishes because of addition of RB to FERGUS.
5 On-line Integration of FERGUS
with a Dialog System
Certain statistical natural language processing
systems can be quite slow, usually because of
the large search space that these systems must
explore. It is therefore uncertain whether a sta-
tistical NLG component can be integrated into a
real-time dialog system. Investigating the mat-
ter in FERGUS?s case, we have experimented
with integrating FERGUS into Communicator,
a mixed-initiative, airline travel reservation sys-
tem. We begin by explaining how Communica-
tor manages surface generation without FER-
GUS. We then delineate several possible kinds
of integration. Finally, we describe our experi-
ences with one kind of integration.
Communicator performs only a rudimentary
form of surface generation as follows. The
dialog manager of Communicator issues a set
of communicative goals that are to be realized.
Surface template strings are selected based
on this set, such as ?What time would you,
traveling from $ORIG-CITY to $DEST-CITY
like to leave?? The slot names in these
strings are then replaced with fillers according
to the dialog manager?s state. The resulting
strings are then piped to a text-to-speech
component (TTS) for output.
There are several possibilities as to how FER-
GUS may supplant this system. One possibility
is off-line integration. In this case, the set of all
possible sets communicative goals for which the
dialog manager requires realization are matched
with a set of corresponding surface syntax trees.
The latter set is input to FERGUS, which gen-
erates a set of surface template strings, which in
turn is used to replace the manually created sur-
face template strings that are an original part
of Communicator. Since these changes are pre-
compiled, the resulting version of Communica-
tor is therefore as fast the original. On the other
hand, off-line integration may be unmanageable
if the set of sets of communicative goals is very
large. In that case, only the alternative of on-
line integration is palatable. In this approach,
each surface template string in Communicator is
replaced with its corresponding surface syntax
tree. At points in a dialog where Communicator
requires surface generation, it sends the appro-
priate surface syntax trees to FERGUS, which
generates surface strings.
We have implemented the on-line integration
of FERGUS with Communicator. Our experi-
ments show that FERGUS is fast enough to be
used in for this purpose, the average time for
FERGUS to generate output strings for one di-
alog turn being only 0.28 seconds.
6 Conclusions and Future Work
We have performed experiments that provide
evidence that components of a statistical NLG
system may be ported to different domains
without a huge investment in manual labor.
These components include a sentence planner,
SPoT, and a surface realizer, FERGUS. SPoT
seems easily portable to different domains be-
cause it can be trained well using only domain-
independent features. FERGUS may also be
said to be easily portable because our experi-
ments show that the quality and quantity of in-
domain training data need not be high and plen-
tiful for decent results. Even if in-domain data
is not available, we show that out-of-domain
training data can be used with adequate results.
By integrating SPoT with FERGUS, we have
also shown that different statistical NLG com-
ponents can be made to work well together. In-
tegration was achieved by adding a rule-based
component to FERGUS which transforms deep
syntax trees into surface syntax trees. The com-
bination of SPoT and FERGUS performs with
high accuracy. Post-integration, there is a di-
minishing effect of TM on output quality.
Finally, we have shown that a statistical NLG
component can be integrated into a dialog sys-
tem in real time. In particular, we replace the
hand-crafted surface generation of Communica-
tor with FERGUS. We show that the resulting
system performs with low latency.
This work may be extended in different di-
rections. Our experiments showed promising re-
sults in porting to the domain of air travel reser-
vations. Although this is a reasonably-sized do-
main, it would be interesting to see how our
findings vary for broader domains. Our experi-
ments used a partially parsed version of the HH
corpus. We would like to compare its use as
TM training data in relation to using a fully
parsed version of HH, and also a hand-checked
treebank version of HH. We would also like to
investigate the possibility of interpolating mod-
els based on different kinds of training data in
order to ameliorate data sparseness. Our ex-
periments focused on integration between the
NLG components of sentence planning and sur-
face generation. We would like to explore the
possibility of further integration, in particular
integrating these components with TTS. This
would provide the benefit of enabling the use of
syntactic and semantic information for prosody
assignment. Also, although FERGUS was inte-
grated with SPoT relatively easily, it does not
necessarily follow that FERGUS can be inte-
grated easily with other kinds of components.
It may be worthwhile to envision a redesigned
version of FERGUS whose input can be flexibly
underspecified in order to accommodate differ-
ent kinds of modules.
7 Acknowledgments
This work was partially funded by DARPA un-
der contract MDA972-99-3-0003.
References
Srinivas Bangalore and A. K. Joshi. 1999. Su-
pertagging: An approach to almost parsing.
Computational Linguistics, 25(2).
Srinivas Bangalore and Owen Rambow. 2000.
Exploiting a probabilistic hierarchical model
for generation. In Proceedings of the 18th
International Conference on Computational
Linguistics (COLING 2000).
Srinivas Bangalore, Owen Rambow, and Steve
Whittaker. 2000. Evaluation metrics for gen-
eration. In Proceedings of the First Interna-
tional Conference on Natural Language Gen-
eration, Mitzpe Ramon, Israel.
Srinivas Bangalore, John Chen, and Owen
Rambow. 2001. Impact of quality and quan-
tity of corpora on stochastic generation. In
Proceedings of the 2001 Conference on Em-
pirical Methods in Natural Langauge Process-
ing, Pittsburgh, PA.
Eugene Charniak. 2000. A maximum-entropy-
inspired parser. In Proceedings of First An-
nual Meeting of the North American Chap-
ter of the Association for Computational Lin-
guistics, Seattle, WA.
John Chen. 2001. Towards Efficient Statis-
tical Parsing Using Lexicalized Grammati-
cal Information. Ph.D. thesis, University of
Delaware.
David Chiang. 2000. Statistical parsing with an
automatically-extracted tree adjoining gram-
mar. In Proceedings of the the 38th Annual
Meeting of the Association for Computational
Linguistics, pages 456?463, Hong Kong.
Yoav Freund, Raj Iyer, Robert E. Schapire, and
Yoram Singer. 1998. An efficient boosting
algorithm for combining preferences. In Ma-
chine Learning: Proceedings of the Fifteenth
International Conferece.
Kevin Knight and V. Hatzivassiloglou. 1995.
Two-level many-paths generation. In Pro-
ceedings of the 33rd Annual Meeting of the
Association for Computational Linguistics,
Boston, MA.
Irene Langkilde and Kevin Knight. 1998. Gen-
eration that exploits corpus-based statisti-
cal knowledge. In Proceedings of the 17th
International Conference on Computational
Linguistics and the 36th Annual Meeting of
the Association for Computational Linguis-
tics, Montreal, Canada.
Benoit Lavoie and Owen Rambow. 1998. A
framework for customizable generation of
multi-modal presentations. In Proceedings of
the 17th International Conference on Com-
putational Linguistics and the 36th Annual
Meeting of the Association for Computational
Linguistics, Montreal, Canada.
Mitchell Marcus, Beatrice Santorini, and
Mary Ann Marcinkiewicz. 1993. Building
a large annotated corpus of english: the
penn treebank. Computational Linguistics,
19(2):313?330.
Igor A. Mel?c?uk. 1998. Dependency Syntax:
Theory and Practice. State University of New
York Press, New York, NY.
Alice H. Oh and Alexander I. Rudnicky.
2000. Stochastic language generation for spo-
ken dialog systems. In Proceedings of the
ANLP/NAACL 2000 Workshop on Conver-
sational Systems, pages 27?32, Seattle, WA.
Kiyotaka Uchimoto, Masaki Murata, Qing Ma,
Satoshi Sekine, and Hitoshi Isahara. 2000.
Word order acquisition from corpora. In Pro-
ceedings of the 18th International Confer-
ence on Computational Linguistics (COLING
?00), Saarbru?cken, Germany.
Marilyn A. Walker, Owen Rambow, and Mon-
ica Rogati. 2001. Spot: A trainable sentence
planner. In Proceedings of the Second Meeting
of the North American Chapter of the Asso-
ciation for Computational Linguistics, pages
17?24.
Fei Xia. 1999. Extracting tree adjoining gram-
mars from bracketed corpora. In Fifth Natu-
ral Language Processing Pacific Rim Sympo-
sium (NLPRS-99), Beijing, China.
The XTAG-Group. 2001. A Lexicalized
Tree Adjoining Grammar for English.
Technical report, University of Penn-
sylvania. Updated version available at
http://www.cis.upenn.edu/?xtag.
Columbia?s Newsblaster: New Features and Future Directions
Kathleen McKeown, Regina Barzilay, John Chen, David Elson, David Evans,
Judith Klavans, Ani Nenkova, Barry Schiffman and Sergey Sigelman
Department of Computer Science
Columbia University
1214 Amsterdam Avenue, New York, N.Y. 10027
kathy@cs.columbia.edu
Abstract
Columbia?s Newsblaster tracking and summa-
rization system is a robust system that clus-
ters news into events, categorizes events into
broad topics and summarizes multiple articles
on each event. Here we outline our most cur-
rent work on tracking events over days, produc-
ing summaries that update a user on new infor-
mation about an event, outlining the perspec-
tives of news coming from different countries
and clustering and summarizing non-English
sources.
1 Introduction
Columbia?s Newsblaster1 provide news updates on a
daily basis from news published on the Internet; it crawls
news sites, categorizes stories into six broad areas, groups
news into stories on the same event, and generates a sum-
mary of the multiple articles describing each event. In ad-
dition to demonstrating the robustness of current summa-
rization and tracking technology, Newsblaster also serves
as a research environment in which we explore new di-
rections and problems. Currently, we are exploring the
tasks of multilingual summarization where input sources
are drawn frommultiple languages and a summary is gen-
erated in English on the same event (Figure 1), tracking
events across days and generating summaries that update
the user on what is new, and editing generated summaries
to improve fluency and accuracy. Our focus here is on
editing references to people, improving coherency of the
summary and ensuring that references are accurate. Edit-
ing is particularly important as we add multilingual capa-
bilities, given the errors inherent in machine translation.
1http://newsblaster.cs.columbia.edu
2 Multilingual Tracking and
Summarization
The multilingual version of Columbia Newsblaster is
built upon the English version of Columbia Newsblaster,
sharing the same structure and components. To add mul-
tilingual capability, the system first crawls web sites in
foreign languages, and stores both the language and en-
coding for the files. To extract the article text from the
HTML pages, we use a new article extraction component
using language-independent statistical features computed
over text blocks along with a machine learning compo-
nent to classify text blocks as one of ?Article Text?, ?Ti-
tle?, ?Image?, ?Image Caption?, or ?Other?. The article
extraction component has been trained and tested on En-
glish, Japanese, and Russian data, but is also being suc-
cessfully applied to French, Spanish, German, and Ital-
ian data. We plan to train the article extractor on other
languages (Chinese, Arabic, Korean, Spanish, German,
French, etc.) in the near future.
To cluster multilingual documents with English doc-
uments, we use the existing Newsblaster English doc-
ument clustering module. Non-English documents are
translated for clustering after the article extraction phase.
We use simple and fast document translation techniques
for clustering if available, since we potentially process
thousands of documents for a language for each run. We
have developed simple dictionary lookup techniques for
translation for clustering for Japanese and Russian; for
other languages we use an interface to the Systran trans-
lation system via Babelfish. We plan on adding Arabic
translation to the system in the near future.
Summarization is performed using the same summa-
rization strategies in Newsblaster. We are experimenting
with different methods for improving summary quality
when translation of text is noisy. For example, when an
input cluster contains both English and foreign sources,
we weight the English higher in cases where we deter-
mine it is representative of both the English and foreign
                                                               Edmonton, May-June 2003
                                                            Demonstrations , pp. 15-16
                                                         Proceedings of HLT-NAACL 2003
Figure 1: Multilingual Version
input documents. We are also experimenting with meth-
ods for determining similarity across documents using
different levels of translation.
3 Different Perspectives
When news media report on international issues, they re-
flect the perspectives of their own countries. In the past,
Newsblaster has included all international sources as in-
put to its summaries. Recently, we have added a feature
of ?international perspectives? to the system. In addition
to the universal summary for a particular event, which
includes all international sources, Newsblaster now gen-
erates separate summaries for each country, which may
illustrate unique biases or disagree on facts. The News-
blaster interface allows users to view any pair of sum-
maries side by side to compare different perspectives.
4 Summary Rewrite
Newsblaster also currently includes a module for rewrit-
ing summaries to achieve better readability. References
to people are rewritten so that the first mention includes
the person?s full name and a selected description and later
mentions are restricted to last name only. In addition to
improving readability, the rewritten version of the sum-
mary is usually shorter than the version before rewrite,
since multiple verbose descriptions of the same entity are
discarded. These changes can be seen when comparing
the summary sentence with the original document via a
link from the summary using a proxy.
5 Event Tracking and Updates
Newsblaster currently identifies events within a single
day; a new set of clusters is generated each day. We have
designed a new module for tracking events across days,
allowing the system to relate stories published on one day
to closely related stories on other days. In this way, the
user can more easily track events of interest as they un-
fold. The typical approach for tracking events across days
represents each event as one monolithic set of stories. We
have focused instead on a model where events on one day
can divide into related sub-events on the next day. For ex-
ample, a set of stories about the start of the Iraq war is an
event that can branch into multiple sets of stories, each set
representing a different facet of the war. We are currently
determining an appropriate evaluation of this approach as
well as investigating different possible interfaces.
If a user is tracking events across days, it is more useful
to have a summary that provides updates on what is new
as opposed to a summary of similarities across all days.
We have built a prototype update summarizer that scans
new articles extracted by the system and compares these
new articles with a background cluster on the same event.
The summarizer will provide the user with a summary of
only important new developments. As the tracking mod-
ule locates new articles, it will pass these to the update
summarizer, which will determine what, if anything, has
changed. This summarizer uses more syntactic and se-
mantic information about the articles to determine nov-
elty than is used in our other summarization strategies and
thus, efficiency is a challenge. We will demo these com-
ponents in a separately fromNewsblaster as they have not
yet been integrated in the development version.
Use of Deep Linguistic Features for the Recognition and Labeling of
Semantic Arguments
John Chen
Department of Computer Science
Columbia University
New York, NY 10027
jchen@cs.columbia.edu
Owen Rambow
Department of Computer Science
Columbia University
New York, NY 10027
rambow@cs.columbia.edu
Abstract
We use deep linguistic features to predict
semantic roles on syntactic arguments,
and show that these perform considerably
better than surface-oriented features. We
also show that predicting labels from a
?lightweight? parser that generates deep
syntactic features performs comparably to
using a full parser that generates only sur-
face syntactic features.
1 Introduction
Syntax mediates between surface word order and
meaning. The goal of parsing (syntactic analysis)
is ultimately to provide the first step towards giv-
ing a semantic interpretation of a string of words.
So far, attention has focused on parsing, because the
semantically annotated corpora required for learn-
ing semantic interpretation have not been available.
The completion of the first phase of the PropBank
(Kingsbury et al, 2002) represents an important
step. The PropBank superimposes an annotation of
semantic predicate-argument structures on top of the
Penn Treebank (PTB) (Marcus et al, 1993; Marcus
et al, 1994). The arc labels chosen for the arguments
are specific to the predicate, not universal.
In this paper, we find that the use of deep lin-
guistic representations to predict these semantic la-
bels are more effective than the generally more
surface-syntax representations previously employed
(Gildea and Palmer (2002)). Specifically, we show
that the syntactic dependency structure that results
load
arg0
John
arg2
hay
arg1
truck
Figure 1: PropBank-style semantic representation
for both John loaded the truck with hay and John
loaded hay into the truck
from the extraction of a Tree Adjoining Grammar
(TAG) from the PTB, and the features that accom-
pany this structure, form a better basis for determin-
ing semantic role labels. Crucially, the same struc-
ture is also produced when parsing with TAG. We
suggest that the syntactic representation chosen in
the PTB is less well suited for semantic process-
ing than the other, deeper syntactic representations.
In fact, this deeper representation expresses syntac-
tic notions that have achieved a wide acceptance
across linguistic frameworks, unlike the very partic-
ular surface-syntactic choices made by the linguists
who created the PTB syntactic annotation rules.
The outline of this paper is as follows. In Sec-
tion 2 we introduce the PropBank and describe the
problem of predicting semantic tags. Section 3
presents an overview of our work and distinguishes
it from previous work. Section 4 describes the
method used to produce the TAGs that are the basis
of our experiments. Section 5 specifies how train-
ing and test data that are used in our experiments
are derived from the PropBank. Next, we give re-
sults on two sets of experiments. Those that predict
semantic tags given gold-standard linguistic infor-
mation are described in Section 6. Those that do
prediction from raw text are described in Section 7.
Finally, in Section 8 we present concluding remarks.
2 The PropBank and the Labeling of
Semantic Roles
The PropBank (Kingsbury et al, 2002) annotates
the PTB with dependency structures (or ?predicate-
argument? structures), using sense tags for each
word and local semantic labels for each argument
and adjunct. Argument labels are numbered and
used consistently across syntactic alternations for
the same verb meaning, as shown in Figure 1. Ad-
juncts are given special tags such as TMP (for tem-
poral), or LOC (for locatives) derived from the orig-
inal annotation of the Penn Treebank. In addition
to the annotated corpus, PropBank provides a lexi-
con which lists, for each meaning of each annotated
verb, its roleset, i.e., the possible arguments in the
predicate and their labels. As an example, the entry
for the verb kick, is given in Figure 2. The notion
of ?meaning? used is fairly coarse-grained, typically
motivated from differing syntactic behavior. Since
each verb meaning corresponds to exactly one role-
set, these terms are often used interchangeably. The
roleset alo includes a ?descriptor? field which is in-
tended for use during annotation and as documenta-
tion, but which does not have any theoretical stand-
ing. Each entry also includes examples. Currently
there are frames for about 1600 verbs in the corpus,
with a total of 2402 rolesets.
Since we did not yet have access to a corpus an-
notated with rolesets, we concentrate in this paper
on predicting the role labels for the arguments. It
is only once we have both that we can interpret the
relation between predicate and argument at a very
fine level (for example, truck in he kicked the truck
withhay as the destination of the loading action). We
will turn to the problem of assigning rolesets to pred-
icates once the data is available. We note though that
preliminary investigations have shown that for about
65% of predicates (tokens) in the WSJ, there is only
one roleset. In a further 7% of predicates (tokens),
the set of semantic labels on the arguments of that
predicate completely disambiguates the roleset.
ID kick.01
Name drive or impel with the foot
VN/Levin 11.4-2, 17.1, 18.1, 23.2
classes 40.3.2, 49
Roles
Number Description
0 Kicker
1 Thing kicked
2 Instrument
(defaults to foot)
Example [John]i tried [*trace*i]ARG0 to kick [the
football]ARG1
Figure 2: The unique roleset for kick
3 Overview
Gildea and Palmer (2002) show that semantic role
labels can be predicted given syntactic features de-
rived from the PTB with fairly high accuracy. Fur-
thermore, they show that this method can be used
in conjunction with a parser to produce parses anno-
tated with semantic labels, and that the parser out-
performs a chunker. The features they use in their
experiments can be listed as follows.
Head Word (HW.) The predicate?s head word as
well as the argument?s head word is used.
Phrase Type. This feature represents the type
of phrase expressing the semantic role. In Figure 3
phrase type for the argument prices is NP.
Path. This feature captures the surface syntactic
relation between the argument?s constituent and the
predicate. See Figure 3 for an example.
Position. This binary feature represents whether
the argument occurs before or after the predicate in
the sentence.
Voice. This binary feature represents whether the
predicate is syntactically realized in either passive or
active voice.
Notice that for the exception of voice, the fea-
tures solely represent surface syntax aspects of
the input parse tree. This should not be taken
to mean that deep syntax features are not impor-
tant. For example, in their inclusion of voice,
Gildea and Palmer (2002) note that this deep syntax
feature plays an important role in connecting seman-
tic role with surface grammatical function.
Aside from voice, we posit that other deep lin-
guistic features may be useful to predict semantic
role. In this work, we explore the use of more gen-
eral, deeper syntax features. We also experiment
with semantic features derived from the PropBank.
fallingarePrices
NNS
S
NP VP
VBP VP
VBG
Figure 3: In the predicate argument relationship be-
tween the predicate falling and the argument prices,
the path feature is VBG?VP?VP?S?NP.
Our methodology is as follows. The first stage en-
tails generating features representing different lev-
els of linguistic analysis. This is done by first auto-
matically extracting several kinds of TAG from the
PropBank. This may in itself generate useful fea-
tures because TAG structures typically relate closely
syntactic arguments with their corresponding pred-
icate. Beyond this, our TAG extraction procedure
produces a set of features that relate TAG structures
on both the surface-syntax as well as the deep-syntax
level. Finally, because a TAG is extracted from the
PropBank, we have a set of semantic features de-
rived indirectly from the PropBank through TAG.
The second stage of our methodology entails using
these features to predict semantic roles. We first
experiment with prediction of semantic roles given
gold-standard parses from the test corpus. We sub-
sequently experiment with their prediction given raw
text fed through a deterministic dependency parser.
4 Extraction of TAGs from the PropBank
Our experiments depend upon automatically extract-
ing TAGs from the PropBank. In doing so, we fol-
low the work of others in extracting grammars of
various kinds from the PTB, whether it be TAG
(Xia, 1999; Chen and Vijay-Shanker, 2000; Chi-
ang, 2000), combinatory categorial grammar (Hock-
enmaier and Steedman, 2002), or constraint depen-
dency grammar (Wang and Harper, 2002). We will
discuss TAGs and an important principle guiding
their formation, the extraction procedure from the
PTB that is described in (Chen, 2001) including ex-
tensions to extract a TAG from the PropBank, and
finally the extraction of deeper linguistic features
VP
VP
are
VBP
S
VPNP
falling
VBG
NP
NNS
Prices
Figure 4: Parse tree associated with the sentence
Prices are falling has been fragmented into three
tree frames.
from the resulting TAG.
A TAG is defined to be a set of lexicalized el-
ementary trees (Joshi and Schabes, 1991). They
may be composed by several well-defined opera-
tions to form parse trees. A lexicalized elementary
tree where the lexical item is removed is called a
tree frame or a supertag. The lexical item in the
tree is called an anchor. Although the TAG for-
malism allows wide latitude in how elementary trees
may be defined, various linguistic principles gener-
ally guide their formation. An important principle
is that dependencies, including long-distance depen-
dencies, are typically localized the same elementary
tree by appropriate grouping of syntactically or se-
mantically related elements.
The extraction procedure fragments a parse tree
from the PTB that is provided as input into elemen-
tary trees. See Figure 4. These elementary trees can
be composed by TAG operations to form the origi-
nal parse tree. The extraction procedure determines
the structure of each elementary tree by localizing
dependencies through the use of heuristics. Salient
heuristics include the use of a head percolation ta-
ble (Magerman, 1995), and another table that distin-
guishes between complements and adjunct nodes in
the tree. For our current work, we use the head per-
colation table to determine heads of phrases. Also,
we treat a PropBank argument (ARG0 . . . ARG9) as
a complement and a PropBank adjunct (ARGM?s) as
an adjunct when such annotation is available.1 Oth-
erwise, we basically follow the approach of (Chen,
2001).2
Besides introducing one kind of TAG extraction
1The version of the PropBank we are using is not fully an-
notated with semantic role information, although the most com-
mon predicates are.
2Specifically, CA1.
procedure, (Chen, 2001) introduces the notion of
grouping linguistically-related extracted tree frames
together. In one approach, each tree frame is decom-
posed into a feature vector. Each element of this vec-
tor describes a single linguistically-motivated char-
acteristic of the tree.
The elements comprising a feature vector are
listed in Table 1. Each elementary tree is decom-
posed into a feature vector in a relatively straightfor-
ward manner. For example, the POS feature is ob-
tained from the preterminal node of the elementary
tree. There are also features that specify the syntac-
tic transformations that an elementary tree exhibits.
Each such transformation is recognized by struc-
tural pattern matching the elementary tree against a
pattern that identifies the transformation?s existence.
For more details, see (Chen, 2001).
Given a set of elementary trees which compose a
TAG, and also the feature vector corresponding to
each tree, it is possible to annotate each node rep-
resenting an argument in the tree with role informa-
tion. These are syntactic roles including for example
subject and direct object. Each argument node is la-
beled with two kinds of roles: a surface syntactic
role and a deep syntactic role. The former is ob-
tained through determining the position of the node
with respect to the anchor of the tree using the usu-
ally positional rules for determining argument status
in English. The latter is obtained from the former
and also from knowledge of the syntactic transfor-
mations that have been applied to the tree. For ex-
ample, we determine the deep syntactic role of a wh-
moved element by ?undoing? the wh-movement by
using the trace information in the PTB.
The PropBank contains all of the notation of the
Penn Treebank as well as semantic notation. For our
current work, we extract two kinds of TAG from the
PropBank. One grammar, SEM-TAG, has elemen-
tary trees annotated with the aforementioned syntac-
tic information as well as semantic information. Se-
mantic information includes semantic role as well as
semantic subcategorization information. The other
grammar, SYNT-TAG, differs from SEM-TAG only
by the absence of any semantic role information.
Table 1: List of each feature in a feature vector and
some possible values.
Feature Values
Part of speech DT, NN, VB, RB, . . .
Subcategorization NP , NP S , ?, . . .
MaxProj S, NP, VP, . . .
Modifyee NP, VP, S, . . .
Direction LEFT, RIGHT
Co-anchors { of }, { by }, ?, . . .
Declarative TRUE, FALSE
Empty Subject TRUE, FALSE
Complementizer TRUE, FALSE
Passive TRUE, FALSE
By-Passive TRUE, FALSE
Topicalized-X TRUE, FALSE
Wh-movement-X-Y TRUE, FALSE
Subject-Aux Inversion TRUE, FALSE
Relative Clause TRUE, FALSE
5 Corpora
For our experiments, we use a version of the Prop-
Bank where the most commonly appearing predi-
cates have been annotated, not all. Our extracted
TAGs are derived from Sections 02-21 of the PTB.
Furthermore, training data for our experiments are
always derived from these sections. Section 23 is
used for test data.
The entire set of semantic roles that are found
in the PropBank are not used in our experiments.
In particular, we only include as semantic roles
those instances in the propbank such that in the ex-
tracted TAG they are localized in the same elemen-
tary tree. As a consequence, adjunct semantic roles
(ARGM?s) are basically absent from our test cor-
pus. Furthermore, not all of the complement seman-
tic roles are found in our test corpus. For example,
cases of subject-control PRO are ignored because
the surface subject is found in a different tree frame
than the predicate. Still, a large majority of com-
plement semantic roles are found in our test corpus
(more than 87%).
6 Semantic Roles from Gold-Standard
Linguistic Information
This section is devoted towards evaluating different
features obtained from a gold-standard corpus in the
task of determining semantic role. We use the fea-
ture set mentioned in Section 3 as well as features
derived from TAGs mentioned in Section 4. In this
section, we detail the latter set of features. We then
describe the results of using different feature sets.
These experiments are performed using the C4.5 de-
cision tree machine learning algorithm. The stan-
dard settings are used. Furthermore, results are al-
ways given using unpruned decision trees because
we find that these are the ones that performed the
best on a development set.
These features are determined during the extrac-
tion of a TAG:
Supertag Path. This is a path in a tree frame from
its preterminal to a particular argument node in a tree
frame. The supertag path of the subject of the right-
most tree frame in Figure 4 is VBG?VP?S?NP.
Supertag. This can be the tree frame correspond-
ing to either the predicate or the argument.
Srole. This is the surface-syntactic role of an ar-
gument. Example of values include 0 (subject) and
1 (direct object).
Ssubcat. This is the surface-syntactic subcate-
gorization frame. For example, the ssubcat cor-
responding to a transitive tree frame would be
NP0 NP1. PPs as arguments are always annotated
with the preposition. For example, the ssubcat for
the passive version of hit would be NP1 NP2(by).
Drole. This is the deep-syntactic role of an argu-
ment. Example of values include 0 (subject) and 1
(direct object).
Dsubcat. This is the deep-syntactic subcate-
gorization frame. For example, the dsubcat cor-
responding to a transitive tree frame would be
NP0 NP1. Generally, PPs as arguments are anno-
tated with the preposition. For example, the dsub-
cat for load is NP0 NP1 NP2(into). The exception
is when the argument is not realized as a PP when
the predicate is realized in a non-syntactically trans-
formed way. For example, the dsubcat for the pas-
sive version of hit would be NP0 NP1.
Semsubcat. This is the semantic subcategoriza-
tion frame.
We first experiment with the set of features de-
scribed in Gildea and Palmer (2002): Pred HW,
Arg HW, Phrase Type, Position, Path, Voice. Call
this feature set GP0. The error rate, 10.0%, is lower
than that reported by Gildea and Palmer (2002),
17.2%. This is presumably because our training and
test data has been assembled in a different manner
as mentioned in Section 5.
Our next experiment is on the same set of fea-
tures, with the exception that Path has been replaced
with Supertag Path. (Feature set GP1). The er-
ror rate is reduced from 10.0% to 9.7%. This is
statistically significant (t-test, p < 0.05), albeit a
small improvement. One explanation for the im-
provement is that Path does not generalize as well
as Supertag path does. For example, the path fea-
ture value VBG?VP?VP?S?NP reflects surface sub-
ject position in the sentence Prices are falling but so
does VBG?VP?S?NP in the sentence Sellers regret
prices falling. Because TAG localizes dependencies,
the corresponding values for Supertag path in these
sentences would be identical.
We now experiment with our surface syntax fea-
tures: Pred HW, Arg HW, Ssubcat, and Srole.
(Feature set SURFACE.) Its performance on SEM-
TAG is 8.2% whereas its performance on SYNT-
TAG is 7.6%, a tangible improvement over previ-
ous models. One reason for the improvement could
be that this model is assigning semantic labels with
knowledge of the other roles the predicate assigns,
unlike previous models.
Our next experiment involves using deep syntax
features: Pred HW, Arg HW, Dsubcat, and Drole.
(Feature set DEEP.) Its performance on both SEM-
TAG and SYNT-TAG is 6.5%, better than previous
models. Its performance is better than SURFACE
presumably because syntactic transformations are
taken to account by deep syntax features. Note also
that the transformations which are taken into ac-
count are a superset of the transformations taken into
account by Gildea and Palmer (2002).
This experiment considers use of semantic fea-
tures: Pred HW, Arg HW, Semsubcat, and Drole.
(Feature set SEMANTIC.) Of course, there are only
results for SEM-TAG, which turns out to be 1.9%.
This is the best performance yet.
In our final experiment, we use supertag features:
Pred HW, Arg HW, Pred Supertag, Arg Su-
Table 2: Error rates of models which label semantic
roles on gold-standard parses. Each model is based
on its own feature sets, with features coming from a
particular kind of extracted grammar.
Feature Set SEM-TAG SYNT-TAG
GP0 10.0 10.0
GP1 9.7 9.7
SURFACE 8.2 7.6
DEEP 6.5 6.5
SEMANTIC 1.9
SUPERTAG 2.8 7.4
pertag, Drole. (Feature set SUPERTAG.) The error
rates are 2.8% for SEM-TAG and 7.4% for SYNT-
TAG. Considering SEM-TAG only, this model per-
forms better than its corresponding DEEP model,
probably because supertag for SEM-TAG include
crucial semantic information. Considering SYNT-
TAG only, this model performs worse than its cor-
responding DEEP model, presumably because of
sparse data problems when modeling supertags.
This sparse data problem is also apparent by com-
paring the model based on SEM-TAG with the cor-
responding SEM-TAG SEMANTICmodel.
7 Semantic Roles from Raw Text
In this section, we are concerned with the problem of
finding semantic arguments and labeling them with
their correct semantic role given raw text as input. In
order to perform this task, we parse this raw text us-
ing a combination of supertagging and LDA, which
is a method that yields partial dependency parses an-
notated with TAG structures. We perform this task
using both SEM-TAG and SYNT-TAG. For the for-
mer, after supertagging and LDA, the task is accom-
plished because the TAG structures are already an-
notated with semantic role information. For the lat-
ter, we use the best performing model from Section 6
in order to find semantic roles given syntactic fea-
tures from the parse.
7.1 Supertagging
Supertagging (Bangalore and Joshi (1999)) is the
task of assigning a single supertag to each word
given raw text as input. For example, given the sen-
tence Prices are falling, a supertagger might return
the supertagged sentence in Figure 4. Supertagging
returns an almost-parse in the sense that it is per-
forming much parsing disambiguation. The typi-
cal technique to perform supertagging is the trigram
model, akin to models of the same name for part-
of-speech tagging. This is the technique that we use
here.
Data sparseness is a significant issue
when supertagging with extracted grammar
(Chen and Vijay-Shanker (2000)). For this reason,
we smooth the emit probabilities P (w|t) in the
trigram model using distributional similarity fol-
lowing Chen (2001). In particular, we use Jaccard?s
coefficient as the similarity metric with a similarity
threshold of 0.04 and a radius of 25 because these
were found to attain optimal results in Chen (2001).
Training data for supertagging is Sections 02-21
of the PropBank. A supertagging model based on
SEM-TAG performs with 76.32% accuracy on Sec-
tion 23. The corresponding model for SYNT-TAG
performs with 80.34% accuracy. Accuracy is mea-
sured for all words in the sentence including punc-
tuation. The SYNT-TAG model performs better
than the SEM-TAG model, understandably, because
SYNT-TAG is the simpler grammar.
7.2 LDA
LDA is an acronym for Lightweight Dependency
Analyzer (Srinivas (1997)). Given as input a su-
pertagged sequence of words, it outputs a partial de-
pendency parse. It takes advantage of the fact that
supertagging provides an almost-parse in order to
dependency parse the sentence in a simple, deter-
ministic fashion. Basic LDA is a two step procedure.
The first step involves linking each word serving as
a modifier with the word that it modifies. The sec-
ond step involves linking each word serving as an ar-
gument with its predicate. Linking always only oc-
curs so that grammatical requirements as stipulated
by the supertags are satisfied. The version of LDA
that is used in this work differs from Srinivas (1997)
in that there are other constraints on the linking pro-
cess.3 In particular, a link is not established if its
existence would create crossing brackets or cycles
in the dependency tree for the sentence.
We perform LDA on two versions of Section 23,
3We thank Srinivas for the use of his LDA software.
Table 3: Accuracy of dependency parsing using
LDA on supertagged input for different kinds of ex-
tracted grammar.
Grammar Recall Precision F
SEM-TAG 66.16 74.95 70.58
SYNT-TAG 74.79 80.35 77.47
one supertagged with SEM-TAG and the other with
SYNT-TAG. The results are shown in Table 3. Eval-
uation is performed on dependencies excluding leaf-
node punctuation. Each dependency is evaluated ac-
cording to both whether the correct head and depen-
dent is related as well as whether they both receive
the correct part of speech tag. The F-measure scores,
in the 70% range, are relatively low compared to
Collins (1999) which has a corresponding score of
around 90%. This is perhaps to be expected because
Collins (1999) is based on a full parser. Note also
that the accuracy of LDA is highly dependent on the
accuracy of the supertagged input. This explains, for
example, the fact that the accuracy on SEM-TAG
supertagged input is lower than the accuracy with
SYNT-TAG supertagged input.
7.3 Semantic Roles from LDA Output
The output of LDA is a partial dependency parse an-
notated with TAG structures. We can use this output
to predict semantic roles of arguments. The manner
in which this is done depends on the kind of gram-
mar that is used. The LDA output using SEM-TAG
is already annotated with semantic role information
because it is encoded in the grammar itself. On the
other hand, the LDA output using SYNT-TAG con-
tains strictly syntactic information. In this case, we
use the highest performing model from Section 6 in
order to label arguments with semantic roles.
Evaluation of prediction of semantic roles takes
the following form. Each argument labeled by a se-
mantic role in the test corpus is treated as one trial.
Certain aspects of this trial are always checked for
correctness. These include checking that the seman-
tic role and the dependency-link are correct. There
are other aspects which may or may not be checked,
depending on the type of evaluation. One aspect,
?bnd,? is whether or not the argument?s bracketing
as specified in the dependency tree is correct. An-
Table 4: Evaluation of semantic argument recogni-
tion on SEM-TAG corpus via supertag and LDA.
Task: determine Recall Precision F
base + arg 0.39 0.84 0.53
base + bnd 0.28 0.61 0.38
base + bnd + arg 0.28 0.61 0.38
other aspect, ?arg,? is whether or not the headword
of the argument is chosen to be correct.
Table 4 show the results when we use SEM-TAG
in order to supertag the input and perform LDA.
When the boundaries are found, finding the head
word additionally does not result in a decrease of
performance. However, correctly identifying the
head word instead of the boundaries leads to an im-
portant increase in performance. Furthermore, note
the low recall and high precision of the ?base +
arg? evaluation. In part this is due to the nature
of the PropBank corpus that we are using. In par-
ticular, because not all predicates in our version of
the PropBank are annotated with semantic roles, the
supertagger for SEM-TAG will sometimes annotate
text without semantic roles when in fact it should
contain them.
Table 5 shows the results of first supertagging
the input with SYNT-TAG and then using a model
trained on the DEEP feature set to annotate the re-
sulting syntactic structure with semantic roles. This
two-step approach greatly increases performance
over the corresponding SEM-TAG based approach.
These results are comparable to the results from
Gildea and Palmer (2002), but only roughly because
of differences in corpora. Gildea and Palmer (2002)
achieve a recall of 0.50, a precision of 0.58, and
an F-measure of 0.54 when using the full parser of
Collins (1999). They also experiment with using a
chunker which yields a recall of 0.35, a precision of
0.50, and an F-measure of 0.41.
8 Conclusions
We have presented various alternative approaches to
predicting PropBank role labels using forms of lin-
guistic information that are deeper than the PTB?s
surface-syntax labels. These features may either
be directly derived from a TAG, such as Supertag
path, or indirectly via aspects of supertags, such
Table 5: Evaluation of semantic argument recogni-
tion on SYNT-TAG corpus via supertag and LDA.
Task: determine Recall Precision F
base + arg 0.65 0.75 0.70
base + bnd 0.48 0.55 0.51
base + bnd + arg 0.48 0.55 0.51
as deep syntactic features like Drole. These are
found to produce substantial improvements in ac-
curacy. We believe that such improvement is due
to these features better capturing the syntactic infor-
mation that is relevant for the task of semantic la-
beling. Also, these features represent syntactic cate-
gories about which there is a broad consensus in the
literature. Therefore, we believe that our results are
portable to other frameworks and differently anno-
tated corpora such as dependency corpora.
We also show that predicting labels from a
?lightweight? parser that generates deep syntactic
features performs comparably to using a full parser
that generates only surface syntactic features. Im-
provements along this line may be attained by use of
a full TAG parser, such as Chiang (2000) for exam-
ple.
Acknowledgments
This paper is based upon work supported by the Na-
tional Science Foundation under the KDD program
through a supplement to Grant No. IIS-98-17434.
Any opinions, findings, and conclusions or recom-
mendations expressed in this paper are those of the
authors and do not necessarily reflect the views of
the National Science Foundation.
References
Srinivas Bangalore and Aravind Joshi. 1999. Supertag-
ging: An approach to almost parsing. Computational
Linguistics, 25(2):237?266.
John Chen and K. Vijay-Shanker. 2000. Automated ex-
traction of tags from the penn treebank. In Proceed-
ings of the Sixth International Workshop on Parsing
Technologies, pages 65?76.
John Chen. 2001. Towards Efficient Statistical Parsing
Using Lexicalized Grammatical Information. Ph.D.
thesis, University of Delaware.
David Chiang. 2000. Statistical parsing with an
automatically-extracted tree adjoining grammar. In
Proceedings of the the 38th Annual Meeting of the As-
sociation for Computational Linguistics, pages 456?
463, Hong Kong.
Michael Collins. 1999. Head-Driven Statistical Models
for Natural Language Parsing. Ph.D. thesis, Univer-
sity of Pennsylvania.
Daniel Gildea and Martha Palmer. 2002. The neces-
sity of parsing for predicate argument recognition. In
acl02, pages 239?246, Philadelphia, PA.
Julia Hockenmaier and Mark Steedman. 2002. Acquir-
ing compact lexicalized grammars from a cleaner tree-
bank. In Proceedings of the Third International Con-
ference on Language Resources and Evaluation, Las
Palmas.
Aravind K. Joshi and Yves Schabes. 1991. Tree-
adjoining grammars and lexicalized grammars. In
Maurice Nivat and Andreas Podelski, editors, Defin-
ability and Recognizability of Sets of Trees. Elsevier.
Paul Kingsbury, Martha Palmer, and Mitch Marcus.
2002. Adding semantic annotation to the Penn Tree-
Bank. In Proceedings of the Human Language Tech-
nology Conference, San Diego, CA.
David Magerman. 1995. Statistical decision-tree models
for parsing. In 33rd Meeting of the Association for
Computational Linguistics (ACL?95).
Mitchell M. Marcus, Beatrice Santorini, and Mary Ann
Marcinkiewicz. 1993. Building a Large Annotated
Corpus of English: The Penn Treebank. Computa-
tional Linguistics, 19.2:313?330, June.
M. Marcus, G. Kim, M. Marcinkiewicz, R. MacIntyre,
A. Bies, M. Ferguson, K. Katz, and B. Schasberger.
1994. The Penn Treebank: Annotating predicate ar-
gument structure. In Proceedings of the ARPA Human
Language Technology Workshop.
B. Srinivas. 1997. Performance evaluation of supertag-
ging for partial parsing. In Proceedings of the Fifth In-
ternational Workshop on Parsing Technologies, pages
187?198, Cambridge, MA.
Wen Wang and Mary P. Harper. 2002. The superarv lan-
guage model: Investigating the effectiveness of tightly
integrating multiple knowledge sources. In Proceed-
ings of the Conference on Empirical Methods in Nat-
ural Language Processing (EMNLP), pages 238?247,
Philadelphia, PA.
Fei Xia. 1999. Extracting tree adjoining grammars from
bracketed corpora. In Fifth Natural Language Pro-
cessing Pacific Rim Symposium (NLPRS-99), Beijing,
China.
Proceedings of the COLING/ACL 2006 Main Conference Poster Sessions, pages 168?175,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Automatically Extracting Nominal Mentions of Events with a
Bootstrapped Probabilistic Classifier?
Cassandre Creswell? and Matthew J. Beal? and John Chen?
Thomas L. Cornell? and Lars Nilsson? and Rohini K. Srihari??
?Janya, Inc.
1408 Sweet Home Road, Suite 1
Amherst NY 14228
{ccreswell,jchen,cornell,
lars,rohini}@janyainc.com
?Dept. of Computer Science and Engineering
University at Buffalo
The State University of New York
Amherst NY 14260
mbeal@cse.buffalo.edu
Abstract
Most approaches to event extraction focus
on mentions anchored in verbs. However,
many mentions of events surface as noun
phrases. Detecting them can increase the
recall of event extraction and provide the
foundation for detecting relations between
events. This paper describes a weakly-
supervised method for detecting nominal
event mentions that combines techniques
from word sense disambiguation (WSD)
and lexical acquisition to create a classifier
that labels noun phrases as denoting events
or non-events. The classifier uses boot-
strapped probabilistic generative models
of the contexts of events and non-events.
The contexts are the lexically-anchored se-
mantic dependency relations that the NPs
appear in. Our method dramatically im-
proves with bootstrapping, and comfort-
ably outperforms lexical lookup methods
which are based on very much larger hand-
crafted resources.
1 Introduction
The goal of information extraction is to generate
a set of abstract information objects that repre-
sent the entities, events, and relations of particu-
lar types mentioned in unstructured text. For ex-
ample, in a judicial domain, relevant event types
might be ARREST, CHARGING, TRIAL, etc.
Although event extraction techniques usually
focus on extracting mentions textually anchored
by verb phrases or clauses, e.g. (Aone and Ramos-
? This work was supported in part by SBIR grant
FA8750-05-C-0187 from the Air Force Research Laboratory
(AFRL)/IFED.
Santacruz, 2000), many event mentions, espe-
cially subsequent mentions of events that are the
primary topic of a document, are referred to with
nominals. Because of this, detecting nominal
event mentions, like those in (1), can increase the
recall of event extraction systems, in particular for
the most important events in a document.1
(1) The slain journalist was a main organizer of the mas-
sive demonstrations that forced Syria to withdraw its
troops from Lebanon last April, after Assad was widely
accused of planningHariri?s assassination in a Febru-
ary car bombing that was similar to today?s blast.
Detecting event nominals is also an important
step in detecting relations between event men-
tions, as in the causal relation between the demon-
strations and the withdrawal and the similarity re-
lation between the bombing and the blast in (1).
Finally, detecting nominal events can improve
detection and coreference of non-named mentions
of non-event entities (e.g. persons, locations, and
organizations) by removing event nominals from
consideration as mentions of entities.
Current extraction techniques for verbally-
anchored events rest on the assumption that most
verb phrases denote eventualities. A system to ex-
tract untyped event mentions can output all con-
stituents headed by a non-auxiliary verb with a
filter to remove instances of to be, to seem, etc.
A statistical or rule-based classifier designed to
detect event mentions of specific types can then
be applied to filter these remaining instances.
Noun phrases, in contrast, can be used to denote
anything?eventualities, entities, abstractions, and
only some are suitable for event-type filtering.
1For example, in the 2005 Automatic Content Extraction
training data, of the 5,349 event mentions, over 35% (1934)
were nominals.
168
1.1 Challenges of nominal event detection
Extraction of nominal mentions of events encom-
passes many of the fundamental challenges of
natural language processing. Creating a general
purpose lexicon of all potentially event-denoting
terms in a language is a labor-intensive task. On
top of this, even utilizing an existing lexical re-
source like WordNet requires sense disambigua-
tion at run-time because event nominals display
the full spectrum of sense distinction behaviors
(Copestake and Briscoe, 1995), including idiosyn-
cratic polysemy, as in (2); constructional poly-
semy, as in (3); coactivation, (4); and copredica-
tion, as in (5).
(2) a. On May 30 a group of Iranian mountaineers hoisted
the Iranian tricolor on the summit.
b. EU Leaders are arriving here for their two-day
summit beginning Thursday.
(3) Things are getting back to normal in the Baywood Golf
Club after a chemical spill[=event]. Clean-up crews
said the chemical spill[=result] was 99 percent water
and shouldn?t cause harm to area residents.
(4) Managing partner Naimoli said he wasn?t concerned
about recent media criticism.
(5) The construction lasted 30 years and was inaugurated
in the presence of the king in June 1684.
Given the breadth of lexical sense phenom-
ena possible with event nominals, no existing ap-
proach can address all aspects. Lexical lookup,
whether using a manually- or automatically-
constructed resource, does not take context into
consideration and so does not allow for vagueness
or unknown words. Purely word-cooccurrence-
based approaches (e.g. (Schu?tze, 1998)) are un-
suitable for cases like (3) where both senses are
possible in a single discourse. Furthermore, most
WSD techniques, whether supervised or unsuper-
vised, must be retrained for each individual lexical
item, a computationally expensive procedure both
at training and run time. To address these limita-
tions, we have developed a technique which com-
bines automatic lexical acquisition and sense dis-
ambiguation into a single-pass weakly-supervised
algorithm for detecting event nominals.
The remainder of this paper is organized as fol-
lows: Section 2 describes our probabilistic clas-
sifier. Section 3 presents experimental results of
this model, assesses its performance when boot-
strapped to increase its coverage, and compares it
to a lexical lookup technique. We describe related
work in Section 4 and present conclusions and im-
plications for future work in Section 5.
2 Weakly-supervised, simultaneous
lexical acquisition and disambiguation
In this section we present a computational method
that learns the distribution of context patterns that
correlate with event vs. non-event mentions based
on unambiguous seeds. Using these seeds we
build two Bayesian probabilistic generative mod-
els of the data, one for non-event nominals and the
other for event nominals. A classifier is then con-
structed by comparing the probability of a candi-
date instance under each model, with the winning
model determining the classification. In Section 3
we show that this classifier?s coverage can be in-
creased beyond the initial labeled seed set by au-
tomatically selecting additional seeds from a very
large unlabeled, parsed corpus.
The technique proceeds as follows. First, two
lexicons of seed terms are created by hand. One
lexicon includes nominal terms that are highly
likely to unambiguously denote events; the other
includes nominal terms that are highly likely to
unambiguously denote anything other than events.
Then, a very large corpus (>150K documents) is
parsed using a broad-coverage dependency parser
to extract all instantiations of a core set of seman-
tic dependency relations, including verb-logical
subject, verb-logical object, subject-nominal pred-
icate, noun phrase-appositive-modifier, etc.
Format of data: Each instantiation is in the
form of a dependency triple, (wa, R,wb), where
R is the relation type and where each argument is
represented just by its syntactic head, wn. Each
partial instantiation of the relation?i.e. either wa
or wb is treated as a wild card ? that can be filled
by any term?becomes a feature in the model. For
every common noun term in the corpus that ap-
pears with at least one feature (including each en-
try in the seed lexicons), the times it appears with
each feature are tabulated and stored in a matrix
of counts. Each column of the matrix represents
a feature, e.g. (occur,Verb-Subj, ?); each row rep-
resents an individual term,2 e.g. murder; and each
entry is the number of times a term appeared with
the feature in the corpus, i.e. as the instantiation of
?. For each row, if the corresponding term appears
in a lexicon it is given that designation, i.e. EVENT
or NONEVENT, or if it does not appear in either
lexicon, it is left unlabeled.
2A term is any common noun whether it is a single or
multiword expression.
169
Probabilistic model: Here we present the de-
tails of the EVENT model?the computations for
the NONEVENT model are identical. The probabil-
ity model is built using a set of seed words labeled
as EVENTs and is designed to address two desider-
ata: (I) the EVENT model should assign high prob-
ability to an unlabeled vector, v, if its features (as
recorded in the count matrix) are similar to the
vectors of the EVENT seeds; (II) each seed term
s should contribute to the model in proportion to
its prevalence in the training data.3 These desider-
ata can be incorporated naturally into a mixture
model formalism, where there are as many com-
ponents in the mixture model as there are EVENT
seed terms. Desideratum (I) is addressed by hav-
ing each component of the mixture model assign-
ing a multinomial probability to the vector, v. For
the ith mixture component built around the ith
seed, s(i), the probability is
p(v|s(i)) =
F?
f=1
(
s(i)f
)vf
,
where s(i)f is defined as the proportion of the times
the seed was seen with feature f compared to the
number of times the seed was seen with any fea-
ture f ? ? F . Thus s(i)f is simply the (i, f)th entry
in a row-sum normalized count matrix,
s(i)f =
s(i)f
?F
f ?=1 s
(i)
f ?
.
Desideratum (II) is realized using a mixture den-
sity by forming a weighted mixture of the above
multinomial distributions from all the provided
seeds i ? E . The weighting of the ith compo-
nent is fixed to be the ratio of the number of oc-
currences of the ith EVENT seed, denoted |s(i)|, to
the total number of all occurrences of event seed
words. This gives more weight to more prevalent
seed words:
p(s(i)) =
|s(i)|
?
i??E |s
(i?)|
.
The EVENT generative probability is then:
p(v|EVENT) =
?
i?E
[
p(s(i)) ? p(v|s(i))
]
.
An example of the calculation for a model with
just two event seeds and three features is given in
Figure 1. A second model is built from the non-
3The counts used here are the number of times a term is
seen with any feature in the training corpus because the in-
dexing tool used to calculate counts does not keep track of
which instances appeared simultaneously with more than one
feature. We do not expect this artifact to dramatically change
the relative seed frequencies in our model.
f1 f2 f3
event seed vector s(1) 3 1 8
event seed vector s(2) 4 6 1
unlabeled mention vector v 2 0 7
p(v|event) =
12
23
?
?
3
12
?2? 1
12
?0? 8
12
?7
+
11
23
?
?
4
11
?2? 6
11
?0? 1
11
?7
= 0.0019
Figure 1: Example of calculating the probability of unla-
beled instance v under the event distribution composed of
two event seeds s(1) and s(2).
event seeds as well, and a corresponding probabil-
ity p(v|NONEVENT) is computed. The following
difference (log odds-ratio)
d(v) = log p(v|EVENT) ? log p(v|NONEVENT)
is then calculated. An instance v encoded as the
vector v is labeled as EVENT or NONEVENT by
examining the sign of d(v). A positive difference
d(v) classifies v as EVENT; a negative value of
d(v) classifies v as NONEVENT. Should d=0 the
classifier is considered undecided and abstains.
Each test instance is composed of a term and
the dependency triples it appears with in context
in the test document. Therefore, an instance can
be classified by (i: word): Find the unlabeled fea-
ture vector in the training data corresponding to
the term and apply the classifier to that vector,
i.e. classify the instance based on the term?s be-
havior summed across many occurrences in the
training corpus; (ii: context): Classify the instance
based only on its immediate test context vector; or
(iii: word+context): For each model, multiply the
probability information from the word vector (=i)
and the context vector (=ii). In our experiments,
all terms in the test corpus appeared at least once
(80% appearing at least 500 times) in the training
corpus, so there were no cases of unseen terms?
not suprising with a training set 1,800 times larger
than the test set. However, the ability to label
an instance based only on its immediate context
means that there is a backoff method in the case of
unseen terms.
3 Experimental Results
3.1 Training, test, and seed word data
In order to train and test the model, we created
two corpora and a lexicon of event and non-event
seeds. The training corpus consisted of 156,000
newswire documents, ?100 million words, from
the Foreign Broadcast Information Service, Lexis
170
Nexis, and other online news archives. The cor-
pus was parsed using Janya?s information extrac-
tion application, Semantex, which creates both
shallow, non-recursive parsing structures and de-
pendency links, and all (wi, R,wj) statistics were
extracted as described in Section 2. From the
1.9 million patterns, (wi, R, ?) and (?, R,wj) ex-
tracted from the corpus, the 48,353 that appeared
more than 300 times were retained as features.
The test corpus was composed of 77 additional
documents (?56K words), overlapping in time
and content but not included in the training set.
These were annotated by hand to mark event nom-
inals. Specifically, every referential noun phrase
headed by a non-proper noun was considered
for whether it denoted an achievement, accom-
plishment, activity, or process (Parsons, 1990).
Noun heads denoting any of these were marked
as EVENT, and all others were left unmarked.
All documents were first marked by a junior an-
notator, and then a non-blind second pass was per-
formed by a senior annotator (first author). Sev-
eral semantic classes were difficult to annotate be-
cause they are particularly prone to coactivation,
including terms denoting financial acts, legal acts,
speech acts, and economic processes. In addition,
for terms like mission, plan, duty, tactic, policy,
it can be unclear whether they are hyponyms of
EVENT or another abstract concept. In every case,
however, the mention was labeled as an event or
non-event depending on whether its use in that
context appeared to be more or less event-like,
respectively. Tests for the ?event-y?ness of the
context included whether an unambiguous word
would be an acceptable substitute there (e.g. funds
[=only non-event] for expenditure [either]).
To create the test data, the annotated documents
were also parsed to automatically extract all com-
mon noun-headed NPs and the dependency triples
they instantiate. Those with heads that aligned
with the offsets of an event annotation were la-
beled as events; the remainder were labeled as
non-events. Because of parsing errors, about 10%
of annotated event instances were lost, that is re-
mained unlabeled or were labeled as non-events.
So, our results are based on the set of recover-
able event nominals as a subset of all common-
noun headed NPs that were extracted. In the
test corpus there were 9,381 candidate instances,
1,579 (17%) events and 7,802 (83%) non-events.
There were 2,319 unique term types; of these, 167
types (7%) appeared both as event tokens and non-
event tokens. Some sample ambiguous terms in-
clude: behavior, attempt, settlement, deal, viola-
tion, progress, sermon, expenditure.
We constructed two lexicons of nominals to use
as the seed terms. For events, we created a list of
95 terms, such as election, war, assassination, dis-
missal, primarily based on introspection combined
with some checks on individual terms in WordNet
and other dictionaries and using Google searches
to judge how ?event-y? the term was.
To create a list of non-events, we used WordNet
and the British National Corpus. First, from the
set of all lexemes that appear in only one synset
in WordNet, all nouns were extracted along with
the topmost hypernym they appear under. From
these we retained those that both appeared on a
lemmatized frequency list of the 6,318 words with
more than 800 occurrences in the whole 100M-
word BNC (Kilgarriff, 1997) and had one of the
hypernyms GROUP, PSYCHOLOGICAL, ENTITY,
POSSESSION. We also retained select terms from
the categories STATE and PHENOMENON were la-
beled non-event seeds. Examples of the 295 non-
event seeds are corpse, electronics, bureaucracy,
airport, cattle.
Of the 9,381 test instances, 641 (6.8%) had a
term that belonged to the seed list. With respect
to types, 137 (5.9%) of the 2,319 term types in the
test data also appeared on the seed lists.
3.2 Experiments
Experiments were performed to investigate the
performance of our models, both when using orig-
inal seed lists, and also when varying the content
of the seed lists using a bootstrapping technique
that relies on the probabilistic framework of the
model. A 1,000-instance subset of the 9,381 test
data instances was used as a validation set; the re-
maining 8,381 were used as evaluation data, on
which we report all results (with the exception of
Table 3 which is on the full test set).
EXP1: Results using original seed sets Prob-
abilistic models for non-events and events were
built from the full list of 295 non-event and 95
event seeds, respectively, as described above.
Table 1 (top half: original seed set) shows the
results over the 8,381 evaluation data instances
when using the three classification methods de-
scribed above: (i) word, (ii) context, and (iii)
word+context. The first row (ALL) reports scores
where all undecided responses are marked as in-
171
B
O
O
T
S
T
R
A
P
P
E
D
O
R
IG
IN
A
L
S
E
E
D
S
E
T
S
E
E
D
S
E
T
EVENT NONEVENT TOTAL AVERAGE
Input Vector Correct Acc (%) Att (%) Correct Acc (%) Att (%) Correct Acc (%) Att (%) Acc (%)
A
L
L
word 1236 87.7 100.0 4217 60.5 100.0 5453 65.1 100.0 74.1
context 627 44.5 100.0 2735 39.2 100.0 3362 40.1 100.0 41.9
word+context 1251 88.8 100.0 4226 60.6 100.0 5477 65.4 100.0 74.7
FA
IR
word 1236 89.3 98.3 4217 60.7 99.6 5453 65.5 99.4 75.0
context 627 69.4 64.2 2735 62.5 62.8 3362 63.6 63.0 65.9
word+context 1251 89.3 99.5 4226 60.7 99.9 5477 65.5 99.8 75.0
A
L
L
word 1110 78.8 100.0 5517 79.1 100.0 6627 79.1 100.0 79.0
context 561 39.8 100.0 2975 42.7 100.0 3536 42.2 100.0 41.3
word+context 1123 79.8 100.0 5539 79.4 100.0 6662 79.5 100.0 79.6
FA
IR
word 1110 80.2 98.3 5517 79.4 99.6 6627 79.5 99.4 79.8
context 561 62.1 64.2 2975 67.9 62.8 3536 66.9 63.0 65.0
word+context 1123 80.2 99.5 5539 79.5 99.9 6662 79.7 99.8 79.9
LEX 1 1114 79.1 100.0 5074 72.8 100.0 6188 73.8 100.0 75.9
total counts 1408 6973 8381
Table 1: (EXP1, EXP3) Accuracies of classifiers in terms of correct classifications, % correct, and % attempted (if allowed to
abstain), on the evaluation test set. (Row 1) Classifiers built from original seed set of size (295, 95); (Row 2) Classifiers built
from 15 iterations of bootstrapping; (Row 3) Classifier built from Lexicon 1. Accuracies in bold are those plotted in related
Figures 2, 3(a) and 3(b).
correct. In the second row (FAIR), undecided an-
swers (d = 0) are left out of the total, so the
number of correct answers stays the same, but the
percentage of correct answers increases.4 Scores
are measured in terms of accuracy on the EVENT
instances, accuracy on the NONEVENT instances,
TOTAL accuracy across all instances, and the sim-
ple AVERAGE of accuracies on non-events and
events (last column). The AVERAGE score as-
sumes that performance on non-events and events
is equally important to us.
?From EXP1, we see that the behavior of a term
across an entire corpus is a better source of infor-
mation about whether a particular instance of that
term refers to an event than its immediate context.
We can further infer that this is because the imme-
diate context only provides definitive evidence for
the models in 63.0% of cases; when the context
model is not penalized for indecision, its accuracy
improves considerably. Nonetheless, in combina-
tion with the word model, immediate context does
not appear to provide much additional information
over only the word. In other words, based only
on a term?s distribution in the past, one can make
a reasonable prediction about how it will be used
when it is seen again. Consequently, it seems that
a well-constructed, i.e. domain customized, lexi-
con can classify nearly as well as a method that
also takes context into account.
EXP2: Results on ACE 2005 event data In ad-
dition to using the data set created specifically for
this project, we also used a subset of the anno-
4Note that Att(%) does not change with bootstrapping?
an artifact of the sparsity of certain feature vectors in the
training and test data, and not the model?s constituents seeds.
Input Vector Acc (%) Att (%)
word 96.1 97.2
context 72.8 63.1
word+context 95.5 98.9
LEX 1 76.5 100.0
Table 2: (EXP2) Results on ACE event nominals: %correct
(accuracy) and %attempted, for our classifiers and LEX 1.
tated training data created for the ACE 2005 Event
Detection and Recognition (VDR) task. Because
only event mentions of specific types are marked
in the ACE data, only recall of ACE event nomi-
nals can be measured rather than overall recall of
event nominals and accuracy on non-event nom-
inals. Results on the 1,934 nominal mentions of
events (omitting cases of d = 0) are shown in Ta-
ble 2. The performance of the hand-crafted Lex-
icon 1 on the ACE data, described in Section 3.3
below, is also included.
The fact that our method performs somewhat
better on the ACE data than on our own data, while
the lexicon approach is worse (7 points higher
vs. 3 points lower, respectively) can likely be ex-
plained by the fact that in creating our introspec-
tive seed set for events, we consulted the annota-
tion manual for ACE event types and attempted
to include in our list any unambiguous seed terms
that fit those types.
EXP3: Increasing seed set via Bootstrapping
There are over 2,300 unlabeled vectors in the train-
ing data that correspond to the words that appear
as lexical heads in the test data. These unlabeled
training vectors can be powerfully leveraged us-
ing a simple bootstrapping algorithm to improve
the individual models for non-events and events,
as follows: Step 1: For each vector v in the unla-
beled portion of training data, row-sum normalize
172
100 1 5 10 15      LEX160
65
70
75
80
85
90 non?events
eventstotal
average
Figure 2: Accuracies vs. iterations of bootstrapping. Bold
symbols on left denote classifier built from initial (295, 95)
seeds; and bold (disconnected) symbols at right are LEX 1.
it to produce v? and compute a normalized mea-
sure of confidence of the algorithm?s prediction,
given by the magnitude of d(v?). Step 2: Add
those vectors most confidently classified as either
non-events or events to the seed set for non-events
or events, according to the sign of d(v?). Step 3:
Recalculate the model based on the new seed lists.
Step 4: Repeat Steps 1?3 until either no more un-
labeled vectors remain or the validation accuracy
no longer increases.
In our experiments we added vectors to each
model such that the ratio of the size of the seed
sets remained constant, i.e. 50 non-events and
16 events were added at each iteration. Using
our validation set, we determined that the boot-
strapping should stop after 15 iterations (despite
continuing for 21 iterations), at which point the
average accuracy leveled out and then began to
drop. After 15 iterations the seed set is of size
(295, 95)+(50, 16)?15 = (1045, 335). Figure 2
shows the change in the accuracy of the model as
it is bootstrapped through 15 iterations.
TOTAL accuracy improves with bootstrapping,
despite EVENT accuracy decreasing, because the
test data is heavily populated with non-events,
whose accuracy increases substantially. The AV-
ERAGE accuracy also increases, which proves that
bootstrapping is doing more than simply shifting
the bias of the classifier to the majority class. The
figure also shows that the final bootstrapped clas-
sifier comfortably outperforms Lexicon 1, impres-
sive because the lexicon contains at least 13 times
more terms than the seed lists.
EXP4: Bootstrapping with a reduced number
of seeds The size of the original seed lists were
chosen somewhat arbitrarily. In order to deter-
mine whether similar performance could be ob-
tained using fewer seeds, i.e. less human effort, we
experimented with reducing the size of the seed
lexicons used to initialize the bootstrapping.
To do this, we randomly selected a fixed frac-
tion, f%, of the (295, 95) available event and non-
event seeds, and built a classifier from this sub-
set of seeds (and discarded the remaining seeds).
We then bootstrapped the classifier?s models us-
ing the 4-step procedure described above, using
candidate seed vectors from the unlabeled train-
ing corpus, and incrementing the number of seeds
until the classifier consisted of (295, 95) seeds.
We then performed 15 additional bootstrapping it-
erations, each adding (50, 16) seeds. Since the
seeds making up the initial classifier are chosen
stochastically, we repeated this entire process 10
times and report in Figures 3(a) and 3(b) the mean
of the total and average accuracies for these 10
folds, respectively. Both plots have five traces,
with each trace corresponding the fraction f =
(20, 40, 60, 80, 100)% of labeled seeds used to
build the initial models. As a point of reference,
note that initializing with 100% of the seed lexicon
corresponds to the first point of the traces in Fig-
ure 2 (where the x-axis is marked with f =100%).
Interestingly, there is no discernible difference
in accuracy (total or average) for fractions f
greater than 20%. However, upon bootstrapping
we note the following trends. First, Figure 3(b)
shows that using a larger initial seed set increases
the maximum achievable accuracy, but this max-
imum occurs after a greater number bootstrap-
ping iterations; indeed the maximum for 100% is
achieved at 15 (or greater) iterations. This reflects
the difference in rigidity of the initial models, with
smaller initial models more easily misled by the
seeds added by bootstrapping. Second, the final
accuracies (total and average) are correlated with
the initial seed set size, which is intuitively satisfy-
ing. Third, it appears from Figure 3(a) that the to-
tal accuracy at the model size (295,95) (or 100%)
is in fact anti-correlated with the size of the ini-
tial seed set, with 20% performing best. This is
correct, but highlights the sometimes misleading
interpretation of the total accuracy: in this case
the model is defaulting to classifying anything as
a non-event (the majority class), and has a consid-
erably impoverished event model.
If one wants to do as well as Lexicon 1 after 15
iterations of bootstrapping then one needs at least
173
EVENT NONEVENT TOTAL AVERAGE
Corr (%) Corr (%) Corr (%) (%)
LEX 1 1256 79.5 5695 73.0 6951 74.1 76.3
LEX 2 1502 95.1 4495 57.6 5997 63.9 76.4
LEX 3 349 22.1 7220 92.5 7569 80.7 57.3
Total 1579 7802 9381
Table 3: Accuracy of several lexicons, showing number and
percentage of correct classifications on the full test set.
an initial seed set of size 60%. An alternative is
to perform fewer iterations, but here we see that
using 100% of the seeds comfortably achieves the
highest total and average accuracies anyway.
3.3 Comparison with existing lexicons
In order to compare our weakly-supervised proba-
bilistic method with a lexical lookup method based
on very large hand-created lexical resources, we
created three lexicons of event terms, which were
used as very simple classifiers of the test data. If
the test instance term belongs to the lexicon, it is
labeled EVENT; otherwise, it is labeled as NON-
EVENT. The results on the full test set using these
lexicons are shown in Table 3.
Lex 1 5,435 entries from NomLex (Macleod et
al., 1998), FrameNet(Baker et al, 1998), CELEX
(CEL, 1993), Timebank(Day et al, 2003).
Lex 2 13,659 entries from WordNet 2.0 hypernym
classes EVENT, ACT, PROCESS, COGNITIVE PRO-
CESS, & COMMUNICATION combined with Lex 1.
Lex 3 Combination of pre-existing lexicons in the
information extraction application from WordNet,
Oxford Advanced Learner?s Dictionary, etc.
As shown in Tables 1 and 3, the relatively
knowledge-poor method developed here using
around 400 seeds performs well compared to the
use of the much larger lexicons. For the task of
detecting nominal events, using Lexicon 1 might
be the quickest practical solution. In terms of ex-
tensibility to other semantic classes, domains, or
languages lacking appropriate existing lexical re-
sources, the advantage of our trainable method is
clear. The primary requirement of this method is
a dependency parser and a system user-developer
who can provide a set of seeds for a class of in-
terest and its complement. It should be possi-
ble in the next few years to create a dependency
parser for a language with no existing linguistic re-
sources (Klein and Manning, 2002). Rather than
having to spend the considerable person-years it
takes to create resources like FrameNet, CELEX,
and WordNet, a better alternative will be to use
weakly-supervised semantic labelers like the one
described here.
4 Related Work
In recent years an array of new approaches have
been developed using weakly-supervised tech-
niques to train classifiers or learn lexical classes
or synonyms, e.g. (Mihalcea, 2003; Riloff and
Wiebe, 2003). Several approaches make use of de-
pendency triples (Lin, 1998; Gorman and Curran,
2005). Our vector representation of the behavior
of a word type across all its instances in a corpus is
based on Lin (1998)?s DESCRIPTION OF A WORD.
Yarowsky (1995) uses a conceptually similar
technique for WSD that learns from a small set of
seed examples and then increases recall by boot-
strapping, evaluated on 12 idiosyncratically poly-
semous words. In that task, often a single disam-
biguating feature can be found in the context of a
polysemous word instance, motivating his use of
the decision list algorithm. In contrast, the goal
here is to learn how event-like or non-event-like
a set of contextual features together are. We do
not expect that many individual features correlate
unambiguously with references to events (or non-
events), only that the presence of certain features
make an event interpretation more or less likely.
This justifies our probabilistic Bayesian approach,
which performs well given its simplicity.
Thelen and Riloff (2002) use a bootstrapping al-
gorithm to learn semantic lexicons of nouns for
six semantic categories, one of which is EVENTS.
For events, only 27% of the 1,000 learned words
are correct. Their experiments were on a much
smaller scale, however, using the 1,700 document
MUC-4 data as a training corpus and using only
10 seeds per category.
Most prior work on event nominals does not try
to classify them as events or non-events, but in-
stead focuses on labeling the argument roles based
on extrapolating information about the argument
structure of the verbal root (Dahl et al, 1987; La-
pata, 2002; Pradhan et al, 2004). Meyers, et al
(1998) describe how to extend a tool for extrac-
tion of verb-based events to corresponding nomi-
nalizations. Hull and Gomez (1996) design a set
of rule-based algorithms to determine the sense of
a nominalization and identify its arguments.
5 Conclusions
We have developed a novel algorithm for label-
ing nominals as events that combines WSD and
lexical acquisition. After automatically bootstrap-
ping the seed set, it performs better than static lex-
icons many times the original seed set size. Also,
174
further bootstrap iterations?? initial seed setfraction (%) ?
20 40 60 80 100 1 5 10 15      LEX164
66
68
70
72
74
76
78
80
82
20%40%60%80%100%
(a) Total Accuracy
further bootstrap iterations?
?
initial seed set
fraction (%) ?
20 40 60 80 100 1 5 10 15      LEX164
66
68
70
72
74
76
78
80
82
20%40%60%80%100%
(b) Average Accuracy
Figure 3: Accuracies of classifiers built from different-sized initial seed sets, and then bootstrapped onwards to the equivalent
of 15 iterations as before. Total (a) and Average (b) accuracies highlight different aspects of the bootstrapping mechanism.
Just as in Figure 2, the initial model is denoted with a bold symbol in the left part of the plot. Also for reference the relevant
Lexicon 1 accuracy (LEX 1) is denoted with a ? at the far right.
it is more robust than lexical lookup as it can also
classify unknown words based on their immediate
context and can remain agnostic in the absence of
sufficient evidence.
Future directions for this work include applying
it to other semantic labeling tasks and to domains
other than general news. An important unresolved
issue is the difficulty of formulating an appropriate
seed set to give good coverage of the complement
of the class to be labeled without the use of a re-
source like WordNet.
References
C. Aone and M. Ramos-Santacruz. 2000. REES: A
large-scale relation and event extraction system. In
6th ANLP, pages 79?83.
C. F. Baker, C. J. Fillmore, and J. B. Lowe. 1998. The
Berkeley FrameNet project. In Proc. COLING-ACL.
Centre of Lexical Information, Nijmegen, 1993.
CELEX English database, E25, online edition.
A. Copestake and T. Briscoe. 1995. Semi-productive
polysemy and sense extension. Journal of Seman-
tics, 12:15?67.
D. Dahl, M. Palmer, and R. Passonneau. 1987. Nomi-
nalizations in PUNDIT. In Proc. of the 25th ACL.
D. Day, L. Ferro, R. Gaizauskas, P. Hanks, M. Lazo,
J. Pustejovsky, R. Sauri, A. See, A. Setzer, and
B. Sundheim. 2003. The TIMEBANK corpus. In
Corpus Linguistics 2003, Lancaster UK.
J. Gorman and J. Curran. 2005. Approximate search-
ing for distributional similarity. In Proc. of the
ACL-SIGLEX Workshop on Deep Lexical Acquisi-
tion, pages 97?104.
R. Hull and F. Gomez. 1996. Semantic interpretation
of nominalizations. In Proc. of the 13th National
Conf. on Artificial Intelligence, pages 1062?1068.
A. Kilgarriff. 1997. Putting frequencies in the dictio-
nary. Int?l J. of Lexicography, 10(2):135?155.
D. Klein and C. Manning. 2002. A generative
constituent-context model for improved grammar in-
duction. In Proc. of the 40th ACL.
M. Lapata. 2002. The disambiguation of nominalisa-
tions. Computational Linguistics, 28(3):357?388.
D. K. Lin. 1998. Automatic retrieval and clustering of
similar words. In Proc. of COLING-ACL ?98.
C. Macleod, R. Grishman, A. Meyers, L. Barrett, and
R. Reeves. 1998. NOMLEX: A lexicon of nominal-
izations. In Proc. of EURALEX?98.
A. Meyers, C. Macleod, R. Yangarber, R. Grishman,
L. Barrett, and R. Reeves. 1998. Using NOMLEX
to produce nominalization patterns for information
extraction. In Proc. of the COLING-ACL Workshop
on the Computational Treatment of Nominals.
R. Mihalcea. 2003. Unsupervised natural language
disambiguation using non-ambiguous words. In
Proc. of Recent Advances in Natural Language Pro-
cessing, pages 387?396.
T. Parsons. 1990. Events in the Semantics of English.
MIT Press, Boston.
S. Pradhan, H. Sun, W. Ward, J. Martin, and D. Juraf-
sky. 2004. Parsing arguments of nominalizations in
English and Chinese. In Proc. of HLT-NAACL.
E. Riloff and J. Wiebe. 2003. Learning extraction pat-
terns for subjective expressions. In Proc. EMNLP.
H. Schu?tze. 1998. Automatic word sense disambigua-
tion. Computational Linguistics, 24(1):97?124.
M. Thelen and E. Riloff. 2002. A bootstrapping
method for learning semantic lexicons using extrac-
tion pattern contexts. In Proc. of EMNLP.
D. Yarowsky. 1995. Unsupervised word sense disam-
biguation rivaling supervised methods. In Proc. of
the 33rd ACL, pages 189?196.
175
Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers,
pages 974?983, Dublin, Ireland, August 23-29 2014.
A Framework for Translating SMS Messages
Vivek Kumar Rangarajan Sridhar, John Chen, Srinivas Bangalore, Ron Shacham
AT&T Labs
1 AT&T Way, Bedminster, NJ 07921
vkumar,jchen,srini,rshacham@research.att.com
Abstract
Short Messaging Service (SMS) has become a popular form of communication. While it is
predominantly used for monolingual communication, it can be extremely useful for facilitating
cross-lingual communication through statistical machine translation. In this work we present an
application of statistical machine translation to SMS messages. We decouple the SMS transla-
tion task into normalization followed by translation so that one can exploit existing bitext re-
sources and present a novel unsupervised normalization approach using distributed representa-
tion of words learned through neural networks. We describe several surrogate data that are good
approximations to real SMS data feeds and use a hybrid translation approach using finite-state
transducers. Both objective and subjective evaluation indicate that our approach is highly suitable
for translating SMS messages.
1 Introduction
The preferred form of communication has been changing over time with advances in communication
technology. The majority of the world?s population now owns a mobile device and an ever increasing
fraction of users are resorting to Short Message Service (SMS) as the primary form of communication.
SMS offers an easy, convenient and condensed form of communication that is being embraced by
the younger demographic. Due to the inherent limit in the length of a message that can be transmitted,
SMS users have adopted several shorthand notations to compress the message; some that have become
standardized and many that are invented constantly. While SMS is predominantly used in a monolingual
mode, it has the potential to connect people speaking different languages. However, translating SMS
messages has several challenges ranging from the procurement of data in this domain to dealing with
noisy text (abbreviations, spelling errors, lack of punctuation, etc.) that is typically detrimental to trans-
lation quality. In this work we address all the elements involved in building a cross-lingual SMS service
that spans data acquisition, normalization, translation modeling, messaging infrastructure and user trial.
The rest of the paper is organized as follows. In Section 4, we present a variety of channels through
which we compiled SMS data followed by a description of our pipeline in Section 5 that includes nor-
malization, phrase segmentation and machine translation. Finally, we describe a SMS translation service
built using our pipeline in Section 6 along with results from a user trial. We provide some discussion in
Section 7 and conclude in Section 8.
2 Related Work
One of the main challenges of building a machine translation system for SMS messages is the lack of
training data in this domain. Typically, there are several legal restrictions in using consumer SMS data
that precludes one from either using it completely or forces one to use it in limited capacity. Only a
handful of such corpora are publicly available on the Web (Chen and Kan, 2013; Fairon and Paumier,
2006; Treurniet et al., 2012; Sanders, 2012; Tagg, 2009); they are limited in size and restricted to a few
language pairs.
The NUS SMS corpus (Chen and Kan, 2013) is probably the largest English SMS corpus consisting of
around 41000 messages. However, these messages are characteristic of Singaporean chat lingo and not
an accurate reflection of SMS style in other parts of the world. A corpus of 30000 French SMS messages
This work is licenced under a Creative Commons Attribution 4.0 International License. Page numbers and proceedings footer
are added by the organizers. License details: http://creativecommons.org/licenses/by/4.0/
974
was collected in (Fairon and Paumier, 2006) to study the idiosyncrasies of SMS language in comparison
with standard French. More recently, (Pennell and Liu, 2011) have used twitter data as a surrogate
for SMS messages. Most of these previous efforts have focused on normalization, i.e., translation of
SMS text to canonical text while we are interested in translating SMS messages from one language into
another (Eidelman et al., 2011).
Several works have addressed the problem of normalizing SMS text. A majority of these works have
used statistical machine translation (character-level) to translate SMS text into standard text (Pennell and
Liu, 2011; Aw et al., 2009; Kobus et al., 2008). (Beaufort et al., 2010) used a finite-state framework
to learn the mapping between SMS and canonical form. A beam search decoder for normalizing social
media text was presented in (Wang and Tou Ng, 2013). All these approaches rely on supervised train-
ing data to train the normalization model. In contrast, we use an unsupervised approach to learn the
normalization lexicon of word forms in SMS to standard text.
While several works have addressed the problem of normalizing SMS using machine translation, there
has been little to no work on the translation of SMS messages across languages on a large scale. Machine
translation of instant messages from English-to-Spanish was proposed in (Bangalore et al., 2002) where
multiple translation hypotheses from several off-the-shelf translation engines were combined using con-
sensus decoding. However, the approach did not consider any specific strategies for normalization and
the fidelity of training bitext is questionable since it was obtained using automatic machine translation.
Several products that enable multilingual communication with the aid of machine translation in con-
ventional chat, email, etc., are available in the market. However, most of these models are trained on
relatively clean bitext.
3 Problem Formulation
The objective in SMS translation is to translate a foreign sentence f
sms
= f
sms
1
, ? ? ? , f
sms
J
into target
(English) sentence e = e
I
1
= e
1
, ? ? ? , e
I
. In general it is hard to procure such SMS bitext due to lack
of data and high cost of annotation. However, we typically have access to bitext in non-SMS domain.
Let f = f
1
, ? ? ? , f
J
be the normalized version of the SMS input sentence. Given f
sms
, we choose the
sentence with highest probability among all possible target sentences,
?
e(f
sms
) = argmax
e
{P(e|f
sms
)} (1)
P (e|f
sms
) ? P (e)
?
f
P (f
sms
, f |e) (2)
= P (e)
?
f
P (f
sms
|f , e)P (f |e) (3)
If one applies the max-sum approximation and assumes that P (f
sms
|f , e) is independent of e,
?
e(f
sms
) = argmax
e
P (f
?
|e)P (e) (4)
where f
?
= argmax
f
P (f
sms
|f). Hence, the SMS translation problem can be decoupled into normal-
ization followed by statistical machine translation
1
.
4 Data
Typically, one has access to a large corpus of general bitext {f , e} while data from the SMS domain
{f
sms
, e} is sparse. Compiling a large corpus of SMS messages is not straightforward as there are
several restrictions on the use of consumer SMS data. We are not aware of any large monolingual or
bilingual corpus of true SMS messages besides those mentioned in Section 2. To compile a corpus of
SMS messages, we used three sources of data: transcriptions of speech-based SMS collected through
1
One can also use a lattice output from the normalization to jointly optimize over e and f
975
smartphones, data collected through Amazon Mechanical Turk
2
and Twitter
3
as a surrogate for SMS-
like messages. We describe the composition of each of these data sources in the following subsections.
Corpus Message #count Corpus Message #count
i love you 988157 ily2
hello 881635 n a meeting
hi 607536 Amazon Mechanical Turk check facebook N/A
how are you 470999 kewl
Speech SMS what?s up 251044 call u n a few
what are you doing 218289 lol 472556
where are you 191912 Twitter haha 232428
call 191430 lmao 102018
lol 105618 omg 709504
how?s it going 102977 thanks for the rt 300254
Table 1: Examples of English messages collected from various sources in this work
4.1 Speech-based SMS
In the absence of access to a real feed of SMS messages, we used transcription of speech-based SMS
messages collected through a smartphone application. A majority of these messages were collected
while the users used the application in their cars. We had access to a total of 41.3 million English and
2.4 million Spanish automatic transcriptions. To avoid the use of erroneous transcripts, we sorted the
messages by frequency and manually translated the top 40,000 English and 10,000 Spanish messages,
respectively. Our final English-Spanish bitext corpus from this source of data consisted of 50,000 parallel
sentences. Table 1 shows the high frequency messages in this dataset.
4.2 Amazon Mechanical Turk
The SMS messages from speech-based interaction does not consist of any shorthands or orthographic
errors as the decoding vocabulary of the automatic speech recognizer is fixed. We posted a task on
Amazon Mechanical Turk, where we took the speech-based SMS messages and asked the turkers to enter
three responses to each message as they would on a smartphone. We iteratively posted the responses from
the turkers as messages to obtain more messages. We obtained a total of 1000 messages in English and
Spanish, respectively. Unlike the speech data, the responses contained several shorthands.
4.3 Twitter
Twitter is used by a large number of users for broadcasting messages, opinions, etc. The language used in
Twitter is similar to SMS and contains plenty of shorthands, spelling errors even though it is typically not
directed towards another individual. We compiled a data set of Twitter messages that we subsequently
translated to obtain a bilingual corpus. We used the Twitter4j API
4
to stream Twitter data for a set of
keywords (function words) over a week. The raw data consisted of roughly 106 million tweets. Subse-
quently, we performed some basic normalization (removal of @user, #tags, filtering advertisements, web
addresses) to obtain SMS-like tweets. Finally, we sorted the data by frequency and picked the top 10000
tweets. Eliminating the tweets present in either of the two previous sources resulted in 6790 messages
that we manually translated.
5 Framework
The user input is first stripped of any accents (Spanish), segmented into short chunks using an automatic
punctuation classifier. Subsequently, any shorthand in the message is expanded out using expansion dic-
tionaries (constructed manually and automatically) and finally translated using a phrase-based translation
2
https://www.mturk.com
3
https://twitter.com
4
http://twitter4j.org/en/
976
model. Our framework allows the use of confusion networks in case of ambiguous shorthand expansions.
We describe each component of the pipeline in detail in the following sections.
5.1 Tokenization
Our initial analysis of SMS messages from users, especially in Spanish indicated that while some users
use accented characters in orthography, several others omit it for the sake of faster responses and con-
venience. Hence, we decided to train all our models on unaccented characters. Given a message, we
convert all accented characters to their corresponding unaccented forms, e.g., ba?no? bano, followed by
lowercasing of all characters. We do not perform any other kind of tokenization.
5.2 Unsupervised SMS Normalization
In Section 5.2, we described a static lookup table for expanding abbreviations and shorthands typically
encountered in SMS messages, e.g., 4ever?forever. While a static lookup table provides a reasonable
way of handling common SMS abbreviations, it has limited coverage. In order to build a larger nor-
malization lexicon, we used distributed representation of words to induce the lexicon in an unsupervised
manner. Distributed word representations (Bengio et al., 2003; Collobert and Weston, 2008; Turian et al.,
2010) induced through deep neural networks have been shown to be useful in several natural language
processing applications. We use the notion of distributional similarity that is automatically induced
through the word representations for learning automatic normalization lexicons.
Canonical form Noisy form
love loveeee, loveeeee, looove, love, wuv, wove, love, laffff, love, wuvvv, luhhhh, love, luvvv, luv
starbucks starbs, sbucks
once oncee, 1ce
tomorrow tmrw, tomorrow, 2moro, tmrrw, tomarrow, tomoro, tomoz, 2mrw, tmr, tm, tmwr, 2mm, tmw, 2morro
forever foreva, 5ever, foreverrrr, forver, foreeverrr, 4ever, 5eva, 4eva, foreevaa, forevs, foreve
because cause, cos, coz, ?cos, ?cause, bc, because, becuz, bcuz, cuz, bcus, bcoz, because
homework hwk, hw, hmwk, hmwrk, hmw, homeworkk, homwork, hmk, honework, homeowork
igualmente igualmentee, igualment, iwalmente
siempre simpre, siempre, 100pre, siempre, ciempre, siempre, siiempre, siemore, siempr, siemre, siempe
adios adi, a10, adio
contigo contigoo, cntigo, conmigo, contigoooo, kontigo, conmigoo, conmiqo
demasiado demaciado, demasido, demasiademente, demasiao
Table 2: Examples from the unsupervised normalization lexicon induced through deep learning
We started with the 106 million tweets described in Section 4.3 and used a deep neural network iden-
tical to that used in (Collobert and Weston, 2008), i.e., the network consisted of a lookup table, hidden
layer with 100 nodes and a linear layer with one output. However, we used a context of 5 words and
corrupted the centre word instead of the last word to learn the distributed representations. We performed
stochastic gradient minimization over 1000 epochs on the twitter data. Subsequently, we took the En-
glish and Spanish vocabularies in our translation model and found the 50 nearest neighbors using cosine
distance for each word. We trained the above representations using the Torch toolkit (Collobert et al.,
2011).
Feature English Spanish
dimension Precision Recall Precision Recall
100 70.4 97.4 69.8 97.3
200 72.2 97.5 79.2 100
300 70.4 97.4 71.6 100
Table 3: Performance of the unsupervised normalization procedure. Only 1-best for each word was
considered.
Once we obtained the 50 nearest neighbors for each word in the clean vocabulary, we used a com-
bination of cosine metric threshold and Levenshtein distance (weighted equally) between the consonant
977
skeleton of the strings to construct the mapping lexicon. Finally, we inverted the table to obtain a nor-
malization lexicon. Our procedure currently finds only one-to-one mappings. We took 60 singleton
entries from the static normalization tables reported in Section 5.2 and evaluated the performance of our
approach. The results are shown in Table 3 and some examples of learned normalizations are shown in
Table 2.
5.3 Phrase Segmentation
In many SMS messages, multiple clauses may be concatenated without explicit punctuation. For exam-
ple, the message hi babe hope you?re well sorry i missed your call needs to be interpreted as hi babe.
hope you?re well. sorry, i missed your call. We perform phrase segmentation using an automatic punc-
tuation classifier trained on SMS messages with punctuation. The classifier learns how to detect end of
sentence markers, i.e. periods, as well as commas in the input stream of unpunctuated words.
An English punctuation classifier and a Spanish punctuation classifier was trained. The former was
trained on two million words of smartphone data described in Section 4.1 while the latter was trained
on 223,000 words of Spanish subtitles from the OpenSubtitles
5
corpus. From each of these data sets, a
maximum entropy classifier was trained. Both classifiers utilized both unigram word and part of speech
(POS) features of a window size of two words around the target word to be classified. A POS tagger
trained on the English Penn Treebank provided English POS tags. Likewise, a Spanish POS tagger
provided Spanish POS tags. The training data for the Spanish tagger, 1.6 million words in size, was
obtained by running the Spanish Freeling parser over the Spanish version of TED talk transcripts. Results
are shown in Table 4. Both phrase segmenters detect end of sentence well. The Spanish phrase segmenter
detects commas better than the English one. This might be due to differences in the training sets; commas
appear about 20 times more often in the Spanish data than in the English data.
Class Precision Recall F-measure
English period 89.7 90.9 90.3
comma 61.1 10.9 18.5
Spanish period 94.3 87.4 90.7
comma 74.2 37.4 49.7
Table 4: Performance of automatic phrase segmentation (numbers are in %)
5.4 Machine Translation
We used a phrase-based translation framework with the phrase table represented as a finite-state trans-
ducer (Rangarajan Sridhar et al., 2013). Our framework proceeds by using the standard procedure of
performing word alignment using GIZA++ (Och and Ney, 2003) and obtaining phrases from the word
alignment using heuristics (Zens and Ney, 2004) and subsequently scoring them. The phrase table is
then represented as a finite-state transducer (FST). The FST decoder was used with minimum error rate
training (MERT) to compute a set of weights for the log-linear model. It is important to note that the
cost of arcs of the FST is a composite score (dot product of scores and weights) and hence requires an
additional lookup during the N-best generation phase in MERT to obtain the component scores. The
model is equivalent to Moses (?) phrase translation without reordering.
We noticed from the data collected in Section 4 that in typical SMS scenarios, a lot of phrases are stock
phrases and hence caching these phrases may result in high accuracies instead of deriving the translation
using a statistical model. We took the data created in Section 4 and created a FST to represent the
sentences. The motivation is to increase the precision of common entries as well as reduce the latency
involved in retrieving a translation from a statistical model. An example of the FST translation paradigm
is shown in Figure 1
We experimented with the notion of using a consensus-based word alignment by combining the align-
ment obtained through different alignment tools. We used GIZA++ (Och and Ney, 2003), Berkeley
5
http://www.opensubtitles.org
978
step1.fsm0
1how:how 2how^are:how^are 3how^are^you:how^are^you
are:are are^you:are^you you:you

WIP

LM)
hello how are you
hello 
how are you


ex.fst
0
hello:holathanks:gracias
how^do^you^do:como^estas
Cached Table
Statistical Model
bestpath(
hola como estas
hello.fsm
0
hello:hello
ptable.fst0/0
how:que/1.822how:como/0.458how^are^you:como^estas/1.106how^are^you:como^esta^usted/2.358are^you:estan/1.998
are^you:estas/0.757you:que/1.460you:tu/0.757
Figure 1: Illustration of the hybrid translation approach using FSTs. WIP and LM refer to the finite state
automata for word insertion penalty and language model, respectively.
Alignment strategy en2es es2en
GIZA++ 28.45 31.83
Pialign 28.08 33.48
Berkeley aligner 27.82 32.01
Union 28.01 33.14
Majority voting 27.32 32.96
Table 5: BLEU scores obtained using different alignment strategies. Only the statistical translation model
was used in the evaluation.
aligner (Liang et al., 2006) and the Phrasal ITG aligner (Pialign) (Neubig et al., 2011). We combined the
alignments in two different ways, taking the union of alignments or majority vote for each target word.
For training the translation model, we used a total of 28.5 million parallel sentences obtained from the
following sources: Opensubtitles (Tiedemann and Lars Nygaard, 2004), Europarl (Koehn, 2005), TED
talks (Cettolo et al., 2012) and Web. The bitext was processed to eliminate spurious pairs by restricting
the English and Spanish vocabularies to the top 150k frequent words as evidenced in a large collection of
monolingual corpora. We also eliminated bitext with ratio of English to Spanish words less than 0.5. The
initial model was optimized using MERT over 1000 parallel sentences from the SMS domain. Results of
the machine translation experiments are shown in Table 5. The test set used was 456 messages collected
in a real SMS interaction (see Section 6.1). The results indicate that consensus alignment procedure is not
superior to the individual alignment outputs. Furthermore, the BLEU scores obtained through both the
consensus procedures are not statistically significant with respect to the BLEU score obtained from the
individual alignment tools. Hence, we used with the phrase translation table obtained using the Phrasal
ITG aligner in all our experiments.
6 SMS Translation Service
In order to test the SMS translation models described in the previous sections, we created the infrastruc-
ture to intercept SMS messages, translate and deliver them in the preferred language of the recipient. The
users were simply asked to register their numbers with a particular language through a Web portal and
subsequently, all messages received by a user would be in the registered language. Some screenshots of
interaction between users is shown in Figure 2. For the messages that are translated, we show both the
original and translated messages. In cases where the translated message is longer than the character limit
per message, we split the message over two message boxes.
979
6.1 User Evaluation
Figure 2: Screenshots of the SMS interface with translation
In order to test the SMS translation models described in the previous sections, we created the infras-
tructure to intercept SMS messages, translate and deliver them in the preferred language of the recipient.
For the messages that are translated, we show both the original and translated messages. In cases where
the translated message is longer than the character limit per message, we split the message over two
message boxes. As part of the study we enrolled 20 English and 5 Spanish participants. The Spanish
participants were bilingual while the English users had little to no knowledge of Spanish. Some of these
interactions turned out to be short while others were had a large number of turns. We collected the
messages exchanged over 2 days that amounted to 241 English and 215 Spanish messages.
0!5!
10!15!
20!25!
30!35!
40!45!
0!1!
2!3!
4!5!
6!7!
8!9!
All! Most! Much! Little! None!
Perce
ntage
 of pa
rticipa
nts!
Numb
er of p
articip
ants!
Adequacy of Translation!
Figure 3: Subjective ratings regarding the adequacy of using SMS translation
We manually translated the 456 messages to create a test data set for evaluation purposes. In the
absence of real SMS feeds in training, this test set is the closest we have to real SMS field data. The BLEU
scores using the entire pipeline (normalization, punctuation, cached and statistical machine translation)
for English-Spanish and Spanish-English was 31.25 and 37.19, respectively. We also created a survey
for the participants to evaluate fluency and adequacy (LDC, 2005) Figures 3 and 4 show the survey
results for adequacy and fluency, respectively. The results indicate that a majority of the people found
the translation quality to be sufficiently adequate while the fluency was between good and non-native.
7 Discussion
The SMS bitext described in Section 4 consists of a total 58790 unique parallel sentences in the SMS
domain. While the bulk of the data (speech-based) does not contain abbreviations and spelling errors, it
980
0!10!
20!30!
40!50!
60!
0!2!
4!6!
8!10!
12!
Flawless! Good! Non-nativ
e! Disfluent! Incompreh
ensible!
Perce
ntage
 of pa
rticipa
nts!
Numb
er of p
articip
ants!
Fluency of Translation!
Figure 4: Subjective ratings regarding the fluency of using SMS translation
is highly representative of SMS messages and in fact is perfectly suited for statistical machine translation
that typically uses normalized and tokenized data. The iterative procedure using Amazon Mechanical
Turk is a good approach to procuring surrogate SMS data. We plan to continue harvesting data using this
approach.
The unsupervised normalization lexicon learning using deep learning performs a good job of learning
SMS shorthands. However, the induced lexicon contains only one-to-one word mappings. If one were
to form compound words for a given dataset, the procedure can be potentially used for learning many-
to-one and many-to-many mappings. Our framework also learns spelling errors rather well. It may also
be possible to use distributed representations learned through log-linear models (Mikolov et al., 2013)
for our task. However, this is beyond the scope of the work presented in this paper. Finally, we used
only 1-best match for the unsupervised lexicon used in this work. One can potentially use a confusion
network and compose it with the FST model to achieve higher accuracies. Our scheme results in fairly
high precision with almost no false negatives (recall is extremely high) and can be reliably applied for
normalization. The unsupervised normalization scheme did not yield significant improvements in BLEU
score since our test set contained only 4 instances where shorthands were used.
Conventionally, sentence segmentation has been useful in improving the quality of statistical machine
translation (Matusov et al., 2006; Matusov et al., 2005). Such segmentation, albeit into shorter phrases,
is also useful for SMS translation. In the absence of phrase segmentation, the BLEU scores for English-
Spanish and Spanish-English drop to 29.65 and 23.95, respectively. The degradation for Spanish-English
messages is quite severe (drop from 37.19 to 23.95) as the lack of segmentation greatly reduces the use of
the cached table. In the absence of segmentation, the cached table was used for 12.8% and 14.4% of the
total phrases for English-Spanish and Spanish-English, respectively. However, with phrase segmentation
the cached table was used for 29.2% and 39.2% of total phrases.
The subjective results obtained from the user trial augur well for the real use of translation technology
as a feature in SMS. One of the issues in the study was balancing the English and Spanish participants.
Since we had access to more English participants (20) in comparison with Spanish participants (5), the
rate of exchange was slow. However, since SMS messages are not required to be real-time, participants
still engaged in a meaningful conversation. Subjective evaluation results using LDC criteria indicate
that most users were happy with the adequacy of translation while the fluency was rated as average. In
general, SMS messages are not very fluent due to character limit imposed on the exchanges and hence
machine translation has to use potentially disfluent source text.
8 Conclusion
We presented an application of statistical machine translation for translating SMS messages. We decou-
pled SMS translation into normalization followed by translation. Our unsupervised SMS normalization
approach exploits the distributional similarity of words and learns SMS shorthands with good accuracy.
We used a hybrid translation approach to exploit the repetitive nature of high frequency SMS messages.
Both objective and subjective evaluation experiments indicate that our system generates translation with
high quality while addressing the idiosyncrasies of SMS messages.
981
References
A. Aw, M. Zhang, J. Xiao, and J. Su. 2009. A phrase-based statistical model for SMS text normalization. In
Proceedings of COLING, pages 33?40.
S. Bangalore, V. Murdock, and G. Riccardi. 2002. Bootstrapping bilingual data using consensus translation for a
multilingual instant messaging system. In Proceedings of COLING.
R. Beaufort, S. Roekhaut, L. A. Cougnon, and C. Fairon. 2010. A hybrid rule/model-based finite-state framework
for normalizing sms messages. In Proceedings of ACL, pages 770?779.
Y. Bengio, R. Ducharme, P. Vincent, and C. Jauvin. 2003. A neural probabilistic language model. Journal of
Machine Learning Research, 3:1137?1155.
M. Cettolo, C. Girardi, and M. Federico. 2012. WIT3: Web Inventory of Transcribed and Translated Talks. In
Proceedings of EAMT.
T. Chen and M. Y. Kan. 2013. Creating a live, public short message service corpus: the NUS SMS corpus.
Language Resources and Evaluation, 47(2):299?335.
R. Collobert and J. Weston. 2008. A unified architecture for natural language processing: deep neural networks
with multitask learning. In Proceedings of ICML.
R. Collobert, K. Kavukcuoglu, and C. Farabet. 2011. Torch7: A matlab-like environment for machine learning.
In BigLearn, NIPS Workshop.
V. Eidelman, K. Hollingshead, and P. Resnik. 2011. Noisy SMS Machine Translation in Low-Density Languages.
In Proceedings of 6th Workshop on Statistical Machine Translation.
C. Fairon and S. Paumier. 2006. A translated corpus of 30,000 french SMS. In Proceedings of LREC.
C. Kobus, F. Yvon, and G. Damnati. 2008. Normalizing sms: Are two metaphors better than one? In Proceedings
of COLING, pages 441?448.
P. Koehn. 2005. Europarl: A parallel corpus for statistical machine translation. In MT Summit.
LDC. 2005. Linguistic data annotation specification: Assessment of fluency and adequacy in translations. Tech-
nical report, Revision 1.5.
Percy Liang, Ben Taskar, and Dan Klein. 2006. Alignment by agreement. In Proceedings of NAACL-HLT, pages
104?111.
E. Matusov, G. Leusch, O. Bender, and H. Ney. 2005. Evaluating machine translation output with automatic
sentence segmentation. In Proceedings of IWSLT, pages 148?154.
E. Matusov, A. Mauser, and H. Ney. 2006. Automatic sentence segmentation and punctuation prediction for
spoken language translation. In Proceedings of IWSLT, pages 158?165.
T. Mikolov, K. Chen, G. Corrado, and J. Dean. 2013. Efficient estimation of word representations in vector space.
In Proceedings of Workshop at ICLR.
Graham Neubig, Taro Watanabe, Eiichiro Sumita, Shinsuke Mori, and Tatsuya Kawahara. 2011. An unsupervised
model for joint phrase alignment and extraction. In Proceedings of the ACL.
F. J. Och and H. Ney. 2003. A systematic comparison of various statistical alignment models. Computational
Linguistics, 29(1):19?51.
D. Pennell and Y. Liu. 2011. A character-level machine translation approach for normalization of SMS abbrevia-
tions. In Proceedings of IJCNLP.
V. K. Rangarajan Sridhar, J. Chen, S. Bangalore, A. Ljolje, and R. Chengalvarayan. 2013. Segmentation strategies
for streaming speech translation. In Proceedings of NAACL-HLT.
E. Sanders. 2012. Collecting and analysing chats and tweets in SoNaR. In Proceedings of LREC.
C. Tagg. 2009. Across-frequency in convolutive blind source separation. dissertation, University of Birmingham.
J. Tiedemann and L. Lars Nygaard. 2004. The OPUS corpus - parallel & free. In Proceedings of LREC.
982
M. Treurniet, O. De Clercq, H. van den Heuvel, and N. Oostdijk. 2012. Collecting a corpus of Dutch SMS. In
Proceedings of LREC, pages 2268?2273.
J. Turian, L. Ratinov, and Y. Bengio. 2010. Word representations: a simple and general method for semi-
supervised learning. In Proceedings of ACL.
P. Wang and H. Tou Ng. 2013. A beam-search decoder for normalization of social media text with application to
machine translation. In Proceedings of NAACL-HLT.
Richard Zens and Hermann Ney. 2004. Improvements in phrase-based statistical machine translation. In In
Proceedings of HLT-NAACL, pages 257?264.
983
Proceedings of NAACL-HLT 2013, pages 230?238,
Atlanta, Georgia, 9?14 June 2013. c?2013 Association for Computational Linguistics
Segmentation Strategies for Streaming Speech Translation
Vivek Kumar Rangarajan Sridhar, John Chen, Srinivas Bangalore
Andrej Ljolje, Rathinavelu Chengalvarayan
AT&T Labs - Research
180 Park Avenue, Florham Park, NJ 07932
vkumar,jchen,srini,alj,rathi@research.att.com
Abstract
The study presented in this work is a first ef-
fort at real-time speech translation of TED
talks, a compendium of public talks with dif-
ferent speakers addressing a variety of top-
ics. We address the goal of achieving a sys-
tem that balances translation accuracy and la-
tency. In order to improve ASR performance
for our diverse data set, adaptation techniques
such as constrained model adaptation and vo-
cal tract length normalization are found to be
useful. In order to improve machine transla-
tion (MT) performance, techniques that could
be employed in real-time such as monotonic
and partial translation retention are found to
be of use. We also experiment with inserting
text segmenters of various types between ASR
and MT in a series of real-time translation ex-
periments. Among other results, our experi-
ments demonstrate that a good segmentation
is useful, and a novel conjunction-based seg-
mentation strategy improves translation qual-
ity nearly as much as other strategies such
as comma-based segmentation. It was also
found to be important to synchronize various
pipeline components in order to minimize la-
tency.
1 Introduction
The quality of automatic speech-to-text and speech-
to-speech (S2S) translation has improved so signifi-
cantly over the last several decades that such systems
are now widely deployed and used by an increasing
number of consumers. Under the hood, the individ-
ual components such as automatic speech recogni-
tion (ASR), machine translation (MT) and text-to-
speech synthesis (TTS) that constitute a S2S sys-
tem are still loosely coupled and typically trained
on disparate data and domains. Nevertheless, the
models as well as the pipeline have been optimized
in several ways to achieve tasks such as high qual-
ity offline speech translation (Cohen, 2007; Kings-
bury et al, 2011; Federico et al, 2011), on-demand
web based speech and text translation, low-latency
real-time translation (Wahlster, 2000; Hamon et al,
2009; Bangalore et al, 2012), etc. The design of a
S2S translation system is highly dependent on the
nature of the audio stimuli. For example, talks, lec-
tures and audio broadcasts are typically long and re-
quire appropriate segmentation strategies to chunk
the input signal to ensure high quality translation.
In contrast, single utterance translation in several
consumer applications (apps) are typically short and
can be processed without the need for additional
chunking. Another key parameter in designing a
S2S translation system for any task is latency. In
offline scenarios where high latencies are permit-
ted, several adaptation strategies (speaker, language
model, translation model), denser data structures (N-
best lists, word sausages, lattices) and rescoring pro-
cedures can be utilized to improve the quality of
end-to-end translation. On the other hand, real-
time speech-to-text or speech-to-speech translation
demand the best possible accuracy at low latencies
such that communication is not hindered due to po-
tential delay in processing.
In this work, we focus on the speech translation
of talks. We investigate the tradeoff between accu-
racy and latency for both offline and real-time trans-
lation of talks. In both these scenarios, appropriate
segmentation of the audio signal as well as the ASR
hypothesis that is fed into machine translation is crit-
ical for maximizing the overall translation quality of
the talk. Ideally, one would like to train the models
on entire talks. However, such corpora are not avail-
able in large amounts. Hence, it is necessary to con-
230
form to appropriately sized segments that are similar
to the sentence units used in training the language
and translation models. We propose several non-
linguistic and linguistic segmentation strategies for
the segmentation of text (reference or ASR hypothe-
ses) for machine translation. We address the prob-
lem of latency in real-time translation as a function
of the segmentation strategy; i.e., we ask the ques-
tion ?what is the segmentation strategy that maxi-
mizes the number of segments while still maximiz-
ing translation accuracy??.
2 Related Work
Speech translation of European Parliamentary
speeches has been addressed as part of the TC-
STAR project (Vilar et al, 2005; Fu?gen et al, 2006).
The project focused primarily on offline translation
of speeches. Simultaneous translation of lectures
and speeches has been addressed in (Hamon et al,
2009; Fu?gen et al, 2007). However, the work fo-
cused on a single speaker in a limited domain. Of-
fline speech translation of TED1 talks has been ad-
dressed through the IWSLT 2011 and 2012 evalua-
tion tracks. The talks are from a variety of speakers
with varying dialects and cover a range of topics.
The study presented in this work is the first effort on
real-time speech translation of TED talks. In com-
parison with previous work, we also present a sys-
tematic study of the accuracy versus latency tradeoff
for both offline and real-time translation on the same
dataset.
Various utterance segmentation strategies for of-
fline machine translation of text and ASR output
have been presented in (Cettolo and Federico, 2006;
Rao et al, 2007; Matusov et al, 2007). The work
in (Fu?gen et al, 2007; Fu?gen and Kolss, 2007)
also examines the impact of segmentation on of-
fline speech translation of talks. However, the real-
time analysis in that work is presented only for
speech recognition. In contrast with previous work,
we tackle the latency issue in simultaneous transla-
tion of talks as a function of segmentation strategy
and present some new linguistic and non-linguistic
methodologies. We investigate the accuracy versus
latency tradeoff across translation of reference text,
utterance segmented speech recognition output and
1http://www.ted.com
partial speech recognition hypotheses.
3 Problem Formulation
The basic problem of text translation can be formu-
lated as follows. Given a source (French) sentence
f = fJ1 = f1, ? ? ? , fJ , we aim to translate it into
target (English) sentence e? = e?I1 = e?1, ? ? ? , e?I .
e?(f) = arg max
e
Pr(e|f) (1)
If, as in talks, the source text (reference or ASR hy-
pothesis) is very long, i.e., J is large, we attempt
to break down the source string into shorter se-
quences, S = s1 ? ? ? sk ? ? ? sQs , where each sequence
sk = [fjkfjk+1 ? ? ? fj(k+1)?1], j1 = 1, jQs+1 =
J + 1. Let the translation of each foreign sequence
sk be denoted by tk = [eikeik+1 ? ? ? ei(k+1)?1], i1 =
1, iQs+1 = I
?
+ 12. The segmented sequences can
be translated using a variety of techniques such as
independent chunk-wise translation or chunk-wise
translation conditioned on history as shown in Eqs. 2
and 3, respectively. In Eq. 3, t?i denotes the best
translation for source sequence si.
e?(f) = arg max
t1
Pr(t1|s1) ? ? ? arg max
tk
Pr(tk|sk)
(2)
e?(f) = arg max
t1
Pr(t1|s1) arg max
t2
Pr(t2|s2, s1, t
?
1)
? ? ? arg max
tk
Pr(tk|s1, ? ? ? , sk, t
?
1, ? ? ? , t
?
k?1)
(3)
Typically, the hypothesis e? will be more accurate
than e? for long texts as the models approximating
Pr(e|f) are conventionally trained on short text seg-
ments. In Eqs. 2 and 3, the number of sequences Qs
is inversely proportional to the time it takes to gen-
erate partial target hypotheses. Our main focus in
this work is to obtain a segmentation S such that the
quality of translation is maximized with minimal la-
tency. The above formulation for automatic speech
recognition is very similar except that the foreign
string f? = f?J1 = f?1, ? ? ? , f?J? is obtained by decoding
the input speech signal.
2The segmented and unsegmented talk may not be equal in
length, i.e., I 6= I
?
231
Model Language Vocabulary #words #sents Corpora
Acoustic Model en 46899 2611144 148460 1119 TED talks
ASR Language Model en 378915 3398460155 151923101 Europarl, WMT11 Gigaword, WMT11 News crawl
WMT11 News-commentary, WMT11 UN, IWSLT11 TED training
Parallel text en 503765 76886659 7464857 IWSLT11 TED training talks, Europarl, JRC-ACQUIS
Opensubtitles, Web data
MT es 519354 83717810 7464857
Language Model es 519354 83717810 7464857 Spanish side of parallel text
Table 1: Statistics of the data used for training the speech translation models.
4 Data
In this work, we focus on the speech translation
of TED talks, a compendium of public talks from
several speakers covering a variety of topics. Over
the past couple of years, the International Work-
shop on Spoken Language Translation (IWSLT) has
been conducting the evaluation of speech translation
on TED talks for English-French. We leverage the
IWSLT TED campaign by using identical develop-
ment (dev2010) and test data (tst2010). However,
English-Spanish is our target language pair as our
internal projects are cater mostly to this pair. As a
result, we created parallel text for English-Spanish
based on the reference English segments released as
part of the evaluation (Cettolo et al, 2012).
We also harvested the audio data from the TED
website for building an acoustic model. A total
of 1308 talks in English were downloaded, out of
which we used 1119 talks recorded prior to Decem-
ber 2011. We split the stereo audio file and dupli-
cated the data to account for any variations in the
channels. The data for the language models was also
restricted to that permitted in the IWSLT 2011 eval-
uation. The parallel text for building the English-
Spanish translation model was obtained from sev-
eral corpora: Europarl (Koehn, 2005), JRC-Acquis
corpus (Steinberger et al, 2006), Opensubtitle cor-
pus (Tiedemann and Lars Nygaard, 2004), Web
crawling (Rangarajan Sridhar et al, 2011) as well as
human translation of proprietary data. Table 1 sum-
marizes the data used in building the models. It is
important to note that the IWSLT evaluation on TED
talks is completely offline. In this work, we perform
the first investigation into the real-time translation of
these talks.
5 Speech Translation Models
In this section, we describe the acoustic, language
and translation models used in our experiments.
5.1 Acoustic and Language Model
We use the AT&T WATSONSM speech recog-
nizer (Goffin et al, 2004). The speech recogni-
tion component consisted of a three-pass decoding
approach utilizing two acoustic models. The mod-
els used three-state left-to-right HMMs representing
just over 100 phonemes. The phonemes represented
general English, spelled letters and head-body-tail
representation for the eleven digits (with ?zero? and
?oh?). The pronunciation dictionary used the appro-
priate phoneme subset, depending on the type of the
word. The models had 10.5k states and 27k HMMs,
trained on just over 300k utterances, using both of
the stereo channels. The baseline model training was
initialized with several iterations of ML training, in-
cluding two builds of context dependency trees, fol-
lowed by three iterations of Minimum Phone Error
(MPE) training.
The Vocal Tract Length Normalization (VTLN)
was applied in two different ways. One was esti-
mated on an utterance level, and the other at the talk
level. No speaker clustering was attempted in train-
ing. The performance at test time was comparable
for both approaches on the development set. Once
the warps were estimated, after five iterations, the
ML trained model was updated using MPE training.
Constrained model adaptation (CMA) was applied
to the warped features and the adapted features were
recognized in the final pass with the VTLN model.
All the passes used the same LM. For offline recog-
nition the warps, and the CMA adaptation, are per-
formed at the talk level. For the real-time speech
translation experiments, we used the VTLN model.
232
The English language model was built using the
permissible data in the IWSLT 2011 evaluation. The
texts were normalized using a variety of cleanup,
number and spelling normalization techniques and
filtered by restricting the vocabulary to the top
375000 types; i.e., any sentence containing a to-
ken outside the vocabulary was discarded. First, we
removed extraneous characters beyond the ASCII
range followed by removal of punctuations. Sub-
sequently, we normalized hyphenated words and re-
moved words with more than 25 characters. The re-
sultant text was normalized using a variety of num-
ber conversion routines and each corpus was fil-
tered by restricting the vocabulary to the top 150000
types; i.e., any sentence containing a token outside
the vocabulary was discarded. The vocabulary from
all the corpora was then consolidated and another
round of filtering to the top 375000 most frequent
types was performed. The OOV rate on the TED
dev2010 set is 1.1%. We used the AT&T FSM
toolkit (Mohri et al, 1997) to train a trigram lan-
guage model (LM) for each component (corpus). Fi-
nally, the component language models were interpo-
lated by minimizing the perplexity on the dev2010
set. The results are shown in Table 2.
Accuracy (%)
Model dev2010 test2010
Baseline MPE 75.5 73.8
VTLN 78.8 77.4
CMA 80.5 80.0
Table 2: ASR word accuracies on the IWSLT data
sets.3
5.2 Translation Model
We used the Moses toolkit (Koehn et al, 2007) for
performing statistical machine translation. Mini-
mum error rate training (MERT) was performed on
the development set (dev2010) to optimize the fea-
ture weights of the log-linear model used in trans-
lation. During decoding, the unknown words were
preserved in the hypotheses. The data used to train
the model is summarized in Table 1.
3We used the standard NIST scoring package as we did not
have access to the IWSLT evaluation server that may normalize
and score differently
We also used a finite-state implementation of
translation without reordering. Reordering can pose
a challenge in real-time S2S translation as the text-
to-speech synthesis is monotonic and cannot retract
already synthesized speech. While we do not ad-
dress the text-to-speech synthesis of target text in
this work, we perform this analysis as a precursor
to future work. We represent the phrase transla-
tion table as a weighted finite state transducer (FST)
and the language model as a finite state acceptor
(FSA). The weight on the arcs of the FST is the
dot product of the MERT weights with the transla-
tion scores. In addition, a word insertion penalty
was also applied to each word to penalize short hy-
potheses. The decoding process consists of compos-
ing all possible segmentations of an input sentence
with the phrase table FST and language model, fol-
lowed by searching for the best path. Our FST-based
translation is the equivalent of phrase-based transla-
tion in Moses without reordering. We present re-
sults using the independent chunk-wise strategy and
chunk-wise translation conditioned on history in Ta-
ble 3. The chunk-wise translation conditioned on
history was performed using the continue-partial-
translation option in Moses.
6 Segmentation Strategies
The output of ASR for talks is a long string of
words with no punctuation, capitalization or seg-
mentation markers. In most offline ASR systems,
the talk is first segmented into short utterance-like
audio segments before passing them to the decoder.
Prior work has shown that additional segmentation
of ASR hypotheses of these segments may be nec-
essary to improve translation quality (Rao et al,
2007; Matusov et al, 2007). In a simultaneous
speech translation system, one can neither find the
optimal segmentation of the entire talk nor tolerate
high latencies associated with long segments. Con-
sequently, it is necessary to decode the incoming au-
dio incrementally as well as segment the ASR hy-
potheses appropriately to maximize MT quality. We
present a variety of linguistic and non-linguistic seg-
mentation strategies for segmenting the source text
input into MT. In our experiments, they are applied
to different inputs including reference text, ASR 1-
best hypothesis for manually segmented audio and
233
incremental ASR hypotheses from entire talks.
6.1 Non-linguistic segmentation
The simplest method is to segment the incoming text
according to length in number of words. Such a pro-
cedure can destroy semantic context but has little to
no overhead in additional processing. We experi-
ment with segmenting the text according to word
window sizes of length 4, 8, 11, and 15 (denoted
as data sets win4, win8, win11, win15, respectively
in Table 3). We also experiment with concatenating
all of the text from one TED talk into a single chunk
(complete talk).
A novel hold-output model was also developed in
order to segment the input text. Given a pair of par-
allel sentences, the model segments the source sen-
tence into minimally sized chunks such that crossing
links and links of one target word to many source
words in an optimal GIZA++ alignment (Och and
Ney, 2003) occur only within individual chunks.
The motivation behind this model is that if a segment
s0 is input at time t0 to an incremental MT system,
it can be translated right away without waiting for a
segment si that is input at a later time ti, ti > 0. The
hold-output model detects these kinds of segments
given a sequence of English words that are input
from left to right. A kernel-based SVM was used to
develop this model. It tags a token t in the input with
either the label HOLD, meaning to chunk it with the
next token, or the label OUTPUT, meaning to output
the chunk constructed from the maximal consecutive
sequence of tokens preceding t that were all tagged
as HOLD. The model considers a five word and POS
window around the target token t. Unigram, bigram,
and trigram word and POS features based upon this
window are used for classification. Training and de-
velopment data for the model was derived from the
English-Spanish TED data (see Table 1) after run-
ning it through GIZA++. Accuracy of the model on
the development set was 66.62% F-measure for the
HOLD label and 82.75% for the OUTPUT label.
6.2 Linguistic segmentation
Since MT models are trained on parallel text sen-
tences, we investigate segmenting the source text
into sentences. We also investigate segmenting the
text further by predicting comma separated chunks
within sentences. These tasks are performed by
training a kernel-based SVM (Haffner et al, 2003)
on a subset of English TED data. This dataset con-
tained 1029 human-transcribed talks consisting of
about 103,000 sentences containing about 1.6 mil-
lion words. Punctuation in this dataset was normal-
ized as follows. Different kinds of sentence ending
punctuations were transformed into a uniform end of
sentence marker. Double-hyphens were transformed
into commas. Commas already existing in the input
were kept while all other kinds of punctuation sym-
bols were deleted. A part of speech (POS) tagger
was applied to this input. For speed, a unigram POS
tagger was implemented which was trained on the
Penn Treebank (Marcus et al, 1993) and used or-
thographic features to predict the POS of unknown
words. The SVM-based punctuation classifier relies
on a five word and POS window in order to classify
the target word. Specifically, token t0 is classified
given as input the window t?2t?1tot1t2. Unigram,
bigram, and trigram word and POS features based on
this window were used for classification. Accuracy
of the classifier on the development set was 60.51%
F-measure for sentence end detection and 43.43%
F-measure for comma detection. Subsequently, data
sets pred-sent (sentences) and pred-punct (comma-
separated chunks) were obtained. Corresponding to
these, two other data sets ref-sent and ref-punct were
obtained based upon gold-standard punctuations in
the reference.
Besides investigating the use of comma-separated
segments, we investigated other linguistically moti-
vated segments. These included conjunction-word
based segments. These segments are separated at
either conjunction (e.g. ?and,? ?or?) or sentence-
ending word boundaries. Conjunctions were iden-
tified using the unigram POS tagger. F-measure
performance for detecting conjunctions by the tag-
ger on the development set was quite high, 99.35%.
As an alternative, text chunking was performed
within each sentence, with each chunk correspond-
ing to one segment. Text chunks are non-recursive
syntactic phrases in the input text. We investi-
gated segmenting the source into text chunks us-
ing TreeTagger, a decision-tree based text chun-
ker (Schmid, 1994). Initial sets of text chunks
were created by using either gold-standard sentence
boundaries or boundaries detected using the punc-
tuation classifier, yielding the data sets chunk-ref-
234
Reference text ASR 1-best
BLEU Mean BLEU Mean
Segmentation Segmentation Independent chunk-wise chunk-wise #words Independent chunk-wise chunk-wise #words
type strategy FST Moses with history per segment FST Moses with history per segment
win4 22.6 21.0 25.5 3.9?0.1 17.7 17.1 20.0 3.9?0.1
win8 26.6 26.2 28.2 7.9?0.3 20.6 20.9 22.3 7.9?0.2
Non-linguistic win11 27.2 27.4 29.2 10.9? 0.3 21.5 21.8 23.1 10.9?0.4
win15 28.5 28.5 29.4 14.9?0.6 22.3 22.8 23.3 14.9?0.7
ref-hold 13.3 14.0 17.1 1.6?1.9 12.7 13.1 17.5 1.5?1.0
pred-hold 15.9 15.7 16.3 2.2?1.9 12.6 12.9 17.4 1.5?1.0
complete talk 23.8 23.9 ? 2504 18.8 19.2 ? 2515
ref-sent 30.6 31.5 30.5 16.7?11.8 24.3 25.1 24.4 17.0?11.6
ref-punct 30.4 31.5 30.3 7.1?5.3 24.2 25.1 24.1 8.7?6.1
pred-punct 30.6 31.5 30.4 8.7?8.8 24.1 25.0 24.0 8.8?6.8
conj-ref-eos 30.5 31.5 30.2 11.2?7.5 24.1 24.9 24.0 11.5?7.7
conj-pred-eos 30.3 31.2 30.3 10.9?7.9 24.0 24.8 24.0 11.4?8.5
chunk-ref-punct 17.9 18.9 21.4 1.3?0.7 14.5 15.2 16.9 1.4?0.7
Linguistic lgchunk1-ref-punct 21.0 21.8 25.1 1.7?1.0 16.9 17.4 19.6 1.8?1.0
lgchunk2-ref-punct 22.4 23.1 26.0 2.1?1.1 17.9 18.4 20.4 2.1?1.1
lgchunk3-ref-punct 24.3 25.1 27.4 2.5?1.7 19.2 19.9 21.3 2.5?1.7
chunk-pred-punct 17.9 18.9 21.4 1.3?0.7 14.5 15.1 16.9 1.4?0.7
lgchunk1-pred-punct 21.2 21.9 25.2 1.8?1.0 16.7 17.2 19.7 1.8?1.0
lgchunk2-pred-punct 22.6 23.1 26.0 2.1?1.2 17.7 18.3 20.5 2.1?1.2
lgchunk3-pred-punct 24.5 25.3 27.4 2.6?1.8 19.1 20.0 21.3 2.5?1.7
Table 3: BLEU scores at the talk level for reference text and ASR 1-best for various segmentation strategies.
The ASR 1-best was performed on manually segmented audio chunks provided in tst2010 set.
punct and chunk-pred-punct. Chunk types included
NC (noun chunk), VC (verb chunk), PRT (particle),
and ADVC (adverbial chunk).
Because these chunks may not provide sufficient
context for translation, we also experimented with
concatenating neighboring chunks of certain types
to form larger chunks. Data sets lgchunk1 concate-
nate together neighboring chunk sequences of the
form NC, VC or NC, ADVC, VC, intended to cap-
ture as single chunks instances of subject and verb.
In addition to this, data sets lgchunk2 capture chunks
such as PC (prepositional phrase) and VC followed
by VC (control and raising verbs). Finally, data sets
lgchunk3 capture as single chunks VC followed by
NC and optionally followed by PRT (verb and its di-
rect object).
Applying the conjunction segmenter after the
aforementioned punctuation classifier in order to de-
tect the ends of sentences yields the data set conj-
pred-eos. Applying it on sentences derived from the
gold-standard punctuations yields the data set conj-
ref-eos. Finally, applying the hold-output model to
sentences derived using the punctuation classifier
produces the data set pred-hold. Obtaining English
sentences tagged with HOLD and OUTPUT directly
from the output of GIZA++ on English-Spanish sen-
tences in the reference produces the data set ref-hold.
The strategies containing the keyword ref for ASR
simply means that the ASR hypotheses are used in
place of the gold reference text.
 0 0.2 0.4 0.6 0.8 1 1.2 1.4 1.6 1.8
 0
 100
0
 200
0
 300
0
 400
0
 500
0
 600
0
processing time per token (sec)
ASR
 tim
eou
t (m
s)ASR
+MT
 (BL
EU)
10.0
11.7
12.6
13.3
13.7
14.0
14.1
14.3
14.6
14.7
14.7
14.8
ASR
+Pu
nct 
Seg
+MT
 (BL
EU)
15.1
15.1
15.1
15.1
15.1
15.1
15.1
15.1
15.1
15.1
15.1
15.1
Figure 1: Latencies and BLEU scores for tst2010 set
using incremental ASR decoding and translation
We also performed real-time speech translation by
using incremental speech recognition, i.e., the de-
coder returns partial hypotheses that, independent of
235
the pruning during search, will not change in the
future. Figure 1 shows the plot for two scenarios:
one in which the partial hypotheses are sent directly
to machine translation and another where the best
segmentation strategy pred-punct is used to segment
the partial output before sending it to MT. The plot
shows the BLEU scores as a function of ASR time-
outs used to generate the partial hypotheses. Fig-
ure 1 also shows the average latency involved in in-
cremental speech translation.
7 Discussion
The BLEU scores for the segmentation strategies
over ASR hypotheses was computed at the talk level.
Since the ASR hypotheses do not align with the
reference source text, it is not feasible to evalu-
ate the translation performance using the gold refer-
ence. While other studies have used an approximate
edit distance algorithm for resegmentation of the hy-
potheses (Matusov et al, 2005), we simply concate-
nate all the segments and perform the evaluation at
the talk level.
The hold segmentation strategy yields the poor-
est translation performance. The significant drop in
BLEU score can be attributed to relatively short seg-
ments (2-4 words) that was generated by the model.
The scheme oversegments the text and since the
translation and language models are trained on sen-
tence like chunks, the performance is poor. For ex-
ample, the input text the sea should be translated
as el mar, but instead the hold segmenter chunks it
as the?sea which MT?s chunk translation renders as
el?el mar. It will be interesting to increase the span
of the hold strategy to subsume more contiguous se-
quences and we plan to investigate this as part of
future work.
The chunk segmentation strategy yields quite poor
translation performance. In general, it does not
make the same kinds of errors that the hold strat-
egy makes; for example, the input text the sea will
be treated as one NC chunk by the chunk seg-
mentation strategy, leading MT to translate it cor-
rectly as el mar. The short chunk sizes of chunk
lead to other kinds of errors. For example, the in-
put text we use will be chunked into the NC we
and the VC use, which will be translated incor-
rectly as nosotros?usar; the infinitive usar is se-
lected rather than the properly conjugated form us-
amos. However, there is a marked improvement in
translation accuracy with increasingly larger chunk
sizes (lgchunk1, lgchunk2, and lgchunk3). Notably,
lgchunk3 yields performance that approaches that of
win8 with a chunk size that is one third of win8?s.
The conj-pred-eos and pred-punct strategies work
the best, and it can be seen that the average seg-
ment length (8-12 words) generated in both these
schemes is very similar to that used for training the
models. It is also about the average latency (4-5
seconds) that can be tolerated in cross-lingual com-
munication, also known as ear-voice span (Lederer,
1978). The non-linguistic segmentation using fixed
word length windows also performs well, especially
for the longer length windows. However, longer
windows (win15) increase the latency and any fixed
length window typically destroys the semantic con-
text. It can also be seen from Table 3 that translat-
ing the complete talk is suboptimal in comparison
with segmenting the text. This is primarily due to
bias on sentence length distributions in the training
data. Training models on complete talks is likely to
resolve this issue. Contrasting the use of reference
segments as input to MT (ref-sent, ref-punct, conj-
ref-eos) versus the use of predicted segments (pred-
sent, pred-punct, conj-pred-eos, respectively), it is
interesting to note that the MT accuracies never dif-
fered greatly between the two, despite the noise in
the set of predicted segments.
The performance of the real-time speech transla-
tion of TED talks is much lower than the offline sce-
nario. First, we use only a VTLN model as perform-
ing CMA adaptation in a real-time scenario typically
increases latency. Second, the ASR language model
is trained on sentence-like units and decoding the en-
tire talk with this LM is not optimal. A language
model trained on complete talks will be more appro-
priate for such a framework and we are investigating
this as part of current work.
Comparing the accuracies of different speech
translation strategies, Table 3 shows that pred-punct
performs the best. When embedded in an incremen-
tal MT speech recognition system, Figure 1 shows
that it is more accurate than the system that sends
partial ASR hypotheses directly to MT. This advan-
tage decreases, however, when the ASR timeout pa-
rameter is increased to more than five or six sec-
236
onds. In terms of latency, Figure 1 shows that the
addition of the pred-punct segmenter into the incre-
mental system introduces a significant delay. About
one third of the increase in delay can be attributed
to merely maintaining the two word lookahead win-
dow that the segmenter?s classifier needs to make
decisions. This is significant because this kind of
window has been used quite frequently in previous
work on simultaneous translation (cf. (Fu?gen et al,
2007)), and yet to our knowledge this penalty asso-
ciated with this configuration was never mentioned.
The remaining delay can be attributed to the long
chunk sizes that the segmenter produces. An inter-
esting aspect of the latency curve associated with the
segmenter in Figure 1 is that there are two peaks at
ASR timeouts of 2,500 and 4,500 ms, and that the
lowest latency is achieved at 3,000 ms rather than at
a smaller value. This may be attributed to the fact
that the system is a pipeline consisting of ASR, seg-
menter, and MT, and that 3,000 ms is roughly the
length of time to recite comma-separated chunks.
Consequently, the two latency peaks appear to cor-
respond with ASR producing segments that are most
divergent with segments that the segmenter pro-
duces, leading to the most pipeline ?stalls.? Con-
versely, the lowest latency occurs when the timeout
is set so that ASR?s segments most resemble the seg-
menter?s output to MT.
8 Conclusion
We investigated various approaches for incremen-
tal speech translation of TED talks, with the aim
of producing a system with high MT accuracy and
low latency. For acoustic modeling, we found that
VTLN and CMA adaptation were useful for increas-
ing the accuracy of ASR, leading to a word accuracy
of 80% on TED talks used in the IWSLT evalua-
tion track. In our offline MT experiments retention
of partial translations was found useful for increas-
ing MT accuracy, with the latter being slightly more
helpful. We experimented with several linguistic
and non-linguistic strategies for text segmentation
before translation. Our experiments indicate that a
novel segmentation into conjunction-separated sen-
tence chunks resulted in accuracies almost as high
and latencies almost as short as comma-separated
sentence chunks. They also indicated that signifi-
cant noise in the detection of sentences and punc-
tuation did not seriously impact the resulting MT
accuracy. Experiments on real-time simultaneous
speech translation using partial recognition hypothe-
ses demonstrate that introduction of a segmenter in-
creases MT accuracy. They also showed that in or-
der to reduce latency it is important for buffers in dif-
ferent pipeline components to be synchronized so as
to minimize pipeline stalls. As part of future work,
we plan to extend the framework presented in this
work for performing speech-to-speech translation.
We also plan to address the challenges involved in
S2S translation across languages with very different
word order.
Acknowledgments
We would like to thank Simon Byers for his help
with organizing the TED talks data.
References
S. Bangalore, V. K. Rangarajan Sridhar, P. Kolan,
L. Golipour, and A. Jimenez. 2012. Real-time in-
cremental speech-to-speech translation of dialogs. In
Proceedings of NAACL:HLT, June.
M. Cettolo and M. Federico. 2006. Text segmentation
criteria for statistical machine translation. In Proceed-
ings of the 5th international conference on Advances
in Natural Language Processing.
M. Cettolo, C. Girardi, and M. Federico. 2012. WIT3:
Web Inventory of Transcribed and Translated Talks. In
Proceedings of EAMT.
J. Cohen. 2007. The GALE project: A description and
an update. In Proceedings of ASRU Workshop.
M. Federico, L. Bentivogli, M. Paul, and S. Stu?ker. 2011.
Overview of the IWSLT 2011 evaluation campaign. In
Proceedings of IWSLT.
C. Fu?gen and M. Kolss. 2007. The influence of utterance
chunking on machine translation performance. In Pro-
ceedings of Interspeech.
C. Fu?gen, M. Kolss, D. Bernreuther, M. Paulik, S. Stuker,
S. Vogel, and A. Waibel. 2006. Open domain speech
recognition & translation: Lectures and speeches. In
Proceedings of ICASSP.
C. Fu?gen, A. Waibel, and M. Kolss. 2007. Simultaneous
translation of lectures and speeches. Machine Trans-
lation, 21:209?252.
V. Goffin, C. Allauzen, E. Bocchieri, D. Hakkani-Tu?r,
A. Ljolje, and S. Parthasarathy. 2004. The AT&T
Watson Speech Recognizer. Technical report, Septem-
ber.
237
P. Haffner, G. Tu?r, and J. Wright. 2003. Optimizing
svms for complex call classification. In Proceedings
of ICASSP?03.
O. Hamon, C. Fu?gen, D. Mostefa, V. Arranz, M. Kolss,
A. Waibel, and K. Choukri. 2009. End-to-end evalua-
tion in simultaneous translation. In Proceedings of the
12th Conference of the European Chapter of the ACL
(EACL 2009), March.
B. Kingsbury, H. Soltau, G. Saon, S. Chu, Hong-Kwang
Kuo, L. Mangu, S. Ravuri, N. Morgan, and A. Janin.
2011. The IBM 2009 GALE Arabic speech translation
system. In Proceedings of ICASSP.
P. Koehn, H. Hoang, A. Birch, C. Callison-Burch,
M. Federico, N. Bertoldi, B. Cowan, Shen W.,
C. Moran, R. Zens, C. J. Dyer, O. Bojar, A. Constantin,
and E. Herbst. 2007. Moses: Open source toolkit for
statistical machine translation. In Proceedings of ACL.
P. Koehn. 2005. Europarl: A parallel corpus for statisti-
cal machine translation. In MT Summit.
M. Lederer. 1978. Simultaneous interpretation: units of
meaning and other features. In D. Gerver and H. W.
Sinaiko, editors, Language interpretation and commu-
nication, pages 323?332. Plenum Press, New York.
M. Marcus, B. Santorini, and M. A. Marcinkiewicz.
1993. Building a large annotated corpus of En-
glish: the Penn treebank. Computational Linguistics,
19(2):313?330.
E. Matusov, G. Leusch, O. Bender, and H. Ney. 2005.
Evaluating machine translation output with automatic
sentence segmentation. In Proceedings of IWSLT.
E. Matusov, D. Hillard, M. Magimai-Doss, D. Hakkani-
Tu?r, M. Ostendorf, and H. Ney. 2007. Improving
speech translation with automatic boundary predic-
tion. In Proceedings of Interspeech.
M. Mohri, F. Pereira, and M. Riley. 1997. At&t
general-purpose finite-state machine software tools,
http://www.research.att.com/sw/tools/fsm/.
F. J. Och and H. Ney. 2003. A systematic comparison of
various statistical alignment models. Computational
Linguistics, 29(1):19?51.
V. K. Rangarajan Sridhar, L. Barbosa, and S. Bangalore.
2011. A scalable approach to building a parallel cor-
pus from the Web. In Proceedings of Interspeech.
S. Rao, I. Lane, and T. Schultz. 2007. Optimizing sen-
tence segmentation for spoken language translation. In
Proceedings of Interspeech.
H. Schmid. 1994. Probabilistic part-of-speech tagging
using decision trees. In Proceedings of the Interna-
tional Conference on New Methods in Language Pro-
cessing.
R. Steinberger, B. Pouliquen, A. Widiger, C. Ignat, T. Er-
javec, and D. Tufis. 2006. The JRC-Acquis: A multi-
lingual aligned parallel corpus with 20+ languages. In
Proceedings of LREC.
J. Tiedemann and L. Lars Nygaard. 2004. The OPUS
corpus - parallel & free. In Proceedings of LREC.
D. Vilar, E. Matusov, S. Hasan, R. Zens, and H. Ney.
2005. Statistical machine translation of European par-
liamentary speeches. In Proceedings of MT Summit.
W. Wahlster, editor. 2000. Verbmobil: Foundations of
Speech-to-Speech Translation. Springer.
238
Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 109?113,
Dublin, Ireland, August 23-24, 2014.
AT&T: The Tag&Parse Approach to Semantic Parsing of Robot Spatial
Commands
Svetlana Stoyanchev, Hyuckchul Jung, John Chen, Srinivas Bangalore
AT&T Labs Research
1 AT&T Way Bedminster NJ 07921
{sveta,hjung,jchen,srini}@research.att.com
Abstract
The Tag&Parse approach to semantic
parsing first assigns semantic tags to each
word in a sentence and then parses the
tag sequence into a semantic tree. We
use statistical approach for tagging, pars-
ing, and reference resolution stages. Each
stage produces multiple hypotheses which
are re-ranked using spatial validation. We
evaluate the Tag&Parse approach on a cor-
pus of Robotic Spatial Commands as part
of the SemEval Task6 exercise. Our sys-
tem accuracy is 87.35% and 60.84% with
and without spatial validation.
1 Introduction
In this paper we describe a system participating
in the SemEval2014 Task-6 on Supervised Seman-
tic Parsing of Robotic Spatial Commands. It pro-
duces a semantic parse of natural language com-
mands addressed to a robot arm designed to move
objects on a grid surface. Each command directs
a robot to change position of an object given a
current configuration. A command uniquely iden-
tifies an object and its destination, for example
?Move the turquoise pyramid above the yellow
cube?. System output is a Robot Control Lan-
guage (RCL) parse (see Figure 1) which is pro-
cessed by the robot arm simulator. The Robot Spa-
tial Commands dataset (Dukes, 2013) is used for
training and testing.
Our system uses a Tag&Parse approach which
separates semantic tagging and semantic parsing
stages. It has four components: 1) semantic tag-
ging, 2) parsing, 3) reference resolution, and 4)
spatial validation. The first three are trained using
LLAMA (Haffner, 2006), a supervised machine
learning toolkit, on the RCL-parsed sentences.
This work is licensed under a Creative Commons Attribution
4.0 International Licence. Page numbers and proceedings
footer are added by the organisers. Licence details: http:
//creativecommons.org/licenses/by/4.0/
For semantic tagging, we train a maximum en-
tropy sequence tagger for assigning a semantic la-
bel and value to each word in a sentence, such as
type cube or color blue. For parsing, we train a
constituency parser on non-lexical RCL semantic
trees. For reference resolution, we train a maxi-
mum entropy model that identifies entities for ref-
erence tags found by previous components. All of
these components can generate multiple hypothe-
ses. Spatial validation re-ranks these hypotheses
by validating them against the input spatial con-
figuration. The top hypothesis after re-ranking is
returned by the system.
Separating tagging and parsing stages has sev-
eral advantages. A tagging stage allows the system
flexibility to abstract from possible grammatical or
spelling errors in a command. It assigns a seman-
tic category to each word in a sentence. Words not
contributing to the semantic meaning are assigned
?O? label by the tagger and are ignored in the fur-
ther processing. Words that are misspelled can po-
tentially receive a correct tag when a word simi-
larity feature is used in building a tagging model.
This will be especially important when process-
ing output of spoken commands that may contain
recognition errors.
The remainder of the paper is organized thusly.
In Section 2 we describe each of the components
used in our system. In Section 3 we describe the
results reported for SemEval2014 and evaluation
of each system component. We summarize our
findings and present future work in Section 4.
2 System
2.1 Sequence Tagging
A sequence tagging approach is used for condi-
tional inference of tags given a word sequence.
It is used for many natural language tasks, such
as part of speech (POS) and named entity tag-
ging (Toutanova and others, 2003; Carreras et al.,
2003). We train a sequence tagger for assign-
109
Figure 1: RCL tree for a sentence Move the turquoise pyramid above the yellow cube.
Word index tag label
Move 1 action move
the 2 O -
turquoise 3 color cyan
pyramid 4 type prism
above 5 relation above
the 6 O -
yellow 7 color yellow
cube 8 type cube
Table 1: Tagging labels for a sentence Move the
turquoise pyramid above the yellow cube.
ing a combined semantic tag and label (such as
type cube) to each word in a command. The tags
used for training are extracted from the leaf-level
nodes of the RCL trees. Table 2 shows tags and
labels for a sample sentence ?Move the turquoise
pyramid above the yellow cube? extracted from
the RCL parse tree (see Figure 1). In some cases,
a label is the same as a word (yellow, cube) while
in other cases, it differs (turquoise - cyan, pyramid
- prism).
We train a sequence tagger using LLAMA max-
imum entropy (maxent) classification (Haffner,
2006) to predict the combined semantic tag and
label of each word. Neighboring words, immedi-
ately neighboring semantic tags, and POS tags are
used as features, where the POS tagger is another
sequence tagging model trained on the Penn Tree-
bank (Marcus et al., 1993). We also experimented
with a tagger that assigns tags and labels in sep-
arate sequence tagging models, but it performed
poorly.
2.2 Parsing
We use a constituency parser for building RCL
trees. The input to the parser is a sequence of
tags assigned by a sequence tagger, such as ?ac-
tion color type relation color type? for the exam-
ple in Figure 1.
The parser generates multiple RCL parse tree
hypotheses sorted in the order of their likelihood.
The likelihood of a tree T given a sequence of tags
T is determined using a probabilistic context free
grammar (PCFG) G:
P (T |S) =
?
r?T
P
G
(r) (1)
The n-best parses are obtained using the CKY
algorithm, recording the n-best hyperedge back-
pointers per constituent along the lines of (Huang
and Chiang, 2005). G was obtained and P
G
was
estimated from a corpus of non-lexical RCL trees
generated by removing all nodes descendant from
the tag nodes (action, color, etc.). Parses may con-
tain empty nodes not corresponding to any tag in
the input sequence. These are hypothesized by the
parser at positions in between input tags and in-
serted as edges according to the PCFG, which has
probabilistic rules for generating empty nodes.
2.3 Reference Resolution
Reference resolution identifies the most prob-
able antecedent for each anaphor within a
text (Hirschman and Chinchor, 1997). It applies
when multiple candidates antecedents are present.
For example, in a sentence ?Pick up the red cube
standing on a grey cube and place it on top of
the yellow one?, the anaphor it has two candidate
antecedents corresponding to entity segments the
red cube and a grey cube. In our system, anaphor
and antecedents are represented by reference tags
occurring in one sentence. A reference tag is ei-
ther assigned by a sequence tagger to one of the
words (e.g. to a pronoun) or is inserted into a
tree by the parser (e.g. ellipsis). We train a bi-
nary maxent model for this task using LLAMA.
The input is a pair consisting of an anaphor and
a candidate antecedent, along with their features.
110
Features that are used include the preceding and
following words as well as the tags/labels of both
the anaphor and candidate antecedent. The refer-
ence resolution component selects the antecedent
for which the model returns the highest score.
2.4 Spatial Validation
SemEval2014 Task6 provided a spatial planner
which takes an RCL command as an input and
determines if that command is executable in the
given spatial context. At each step described in
2.1?2.3, due to the statistical nature of our ap-
proach, multiple hypotheses can be easily com-
puted with different confidence values. We used
the spatial planner to validate the final output RCL
commands from the three steps by checking if the
RCLs are executable or not. We generate multi-
ple tagger output hypotheses. For each tagger out-
put hypothesis, we generate multiple parser out-
put hypotheses. For each parser output hypothe-
sis, we generate multiple reference resolution out-
put hypotheses. The resulting output hypotheses
are ranked in the order of confidence scores with
the highest tagging output scores ranked first, fol-
lowed by the parsing output scores, and, finally,
reference resolution output scores. The system re-
turns the result of the top scored command that is
valid according to the spatial validator.
In many applications, there can be a tool or
method to validate tag/parse/reference outputs
fully or partially. Note that in our system the val-
idation is performed after all output is generated.
Tightly coupled validation, such as checking va-
lidity of a tagged entity or a parse constituent,
could help in computing hypotheses at each step
(e.g., feature values based on possible entities or
actions) and it remains as future work.
3 Results
In this section, we present evaluation results on the
three subsets of the data summarized in Table 3. In
the TEST2500 data set, the models are trained on
the initial 2500 sentences of the Robot Commands
Treebank and evaluated on the last 909 sentences
(this corresponds to the data split of the SemEval
task). In TEST500 data set, the models are trained
on the initial 500 sentences of the training set and
evaluated on the last 909 test sentences. We re-
port these results to analyze the models? perfor-
mance on a reduced training size. In DEV2500
data set, models are trained on 90% of the initial
2500 sentences and evaluated on 10% of the 2500
# Dataset Avg # hyp Accuracy
1 TEST2500 1-best 1 86.0%
2 TEST2500 max-5 3.34 95.2%
3 TEST500 1-best 1 67.9%
4 TEST500 max-5 4.25 83.8%
5 DEV2500 1-best 1 90.8%
6 DEV2500 max-5 2.9 98.0%
Table 3: Tagger accuracy for 1-best and maximum
of 5-best hypotheses (max-5).
sentences using a random data split. We observe
that sentence length and standard deviation of test
sentences in the TEST2500 data set is higher than
on the training sentences while in the DEV2500
data set training and test sentence length and stan-
dard deviation are comparable.
3.1 Semantic Tagging
Table 3 presents sentence accuracy of the seman-
tic tagging stage. Tagging accuracy is evaluated
on 1-best and on max-5 best tagger outputs. In
the max-5 setting the number of hypotheses gen-
erated by the tagger varies for each input with the
average numbers reported in Table 3. Tagging ac-
curacy on TEST2500 using 1-best is 86.0%. Con-
sidering max-5 best tagging sequences, the accu-
racy is 95.2%. On the TEST500 data set tagging
accuracy is 67.9% and 83.8% on 1-best and max-
5 best sequences respectively, approximately 8%
points lower than on TEST2500 data set. On the
DEV2500 data set tagging accuracy is 90.8% and
98.0% on 1-best and max-5 best sequences, 4.8%
and 2.8% points higher than on the TEST2500
data set. The higher performance on DEV2500 in
comparison to the TEST2500 can be explained by
the higher complexity of the test sentences in com-
parison to the training sentences in the TEST2500
data set.
3.2 RCL Parsing
Parsing was evaluated using the EVALB scoring
metric (Collins, 1997). Its 1-best F-measure accu-
racy on gold standard TEST2500 and DEV2500
semantic tag sequences was 96.17% and 95.20%,
respectively. On TEST500, its accuracy remained
95.20%. On TEST2500 with system provided in-
put sequences, its accuracy was 94.79% for 869
out of 909 sentences that were tagged correctly.
3.3 System Accuracy
Table 4 presents string accuracy of automatically
generated RCL parse trees on each data set. The
111
Name Train #sent Train Sent. len. (stdev) Test #sent Test Sent. Len. (stdev)
TEST2500 2500 13.44 (5.50) 909 13.96 (5.59)
TEST500 500 14.62(5.66) 909 13.96 (5.59)
DEV2500 2250 13.43 ( 5.53) 250 13.57 (5.27)
Table 2: Number of sentences, average length and standard deviation of the data sets.
results are obtained by comparing system output
RCL parse string with the reference RCL parse
string. For each data set, we ran the system
with and without spatial validation. We ran RCL
parser and reference resolution on automatically
assigned semantic tags (Auto) and oracle tagging
(Orcl). We observed that some tag labels can be
verified systematically and corrected them with
simple rules: e.g., change ?front? to ?forward?
because relation specification in (Dukes, 2013)
doesn?t have ?front? even though annotations in-
cluded cases with ?front? as relation.
The system performance on TEST2500 data
set using automatically assigned tags and no spa-
tial validation is 60.84%. In this mode, the sys-
tem uses 1-best parser and 1-best tagger output.
With spatial validation, which allows the system to
re-rank parser and tagger hypotheses, the perfor-
mance increases by 27% points to 87.35%. This
indicates that the parser and the tagger component
often produce a correct output which is not ranked
first. Using oracle tags without / with spatial vali-
dation on TEST2500 data set the system accuracy
is 67.55% / 94.83%, 7% points above the accuracy
using predicted tags.
The system performance on TEST500 data set
using automatically assigned tags with / with-
out spatial validation is 48.95% / 74.92%, ap-
proximately 12% points below the performance
on TEST2500 (Row 1). Using oracle tags with-
out / with spatial validation the performance on
TEST500 data set is 63.89% / 94.94%. The per-
formance without spatial validation is only 4% be-
low TEST2500, while with spatial validation the
performance on TEST2500 and TEST500 is the
same. These results indicate that most perfor-
mance degradation on a smaller data set is due to
the semantic tagger.
The system performance on DEV2500 data set
using automatically assigned tags without / with
spatial validation is 68.0% / 96.80% (Row 5), 8%
points above the performance on TEST2500 (Row
1). With oracle tags, the performance is 69.60%
/ 98.0%, which is 2-3% points above TEST2500
(Row 2). These results indicate that most perfor-
mance improvement on a better balanced data set
# Dataset Tag Accuracy without / with
spatial validation
1 TEST2500 Auto 60.84 / 87.35
2 TEST2500 Orcl 67.55 / 94.83
3 TEST500 Auto 48.95 / 74.92
4 TEST500 Orcl 63.89 / 94.94
5 DEV2500 Auto 68.00 / 96.80
6 DEV2500 Orcl 69.60 / 98.00
Table 4: System accuracy with and without spatial
validation using automatically assigned tags and
oracle tags (OT).
DEV2500 is due to better semantic tagging.
4 Summary and Future Work
In this paper, we present the results of semantic
processing for natural language robot commands
using Tag&Parse approach. The system first tags
the input sentence and then applies non-lexical
parsing to the tag sequence. Reference resolution
is applied to the resulting parse trees. We com-
pare the results of the models trained on the data
sets of size 500 (TEST500) and 2500 (TEST2500)
sentences. We observe that sequence tagging
model degrades significantly on a smaller data set.
Parsing and reference resolution models, on the
other hand, perform nearly as well on both train-
ing sizes. We compare the results of the models
trained on more (DEV2500) and less (TEST2500)
homogeneous training/testing data sets. We ob-
serve that a semantic tagging model is more sen-
sitive to the difference between training and test
set than parsing model degrading significantly a
less homogeneous data set. Our results show that
1) both tagging and parsing models will benefit
from an improved re-ranking, and 2) our parsing
model is robust to a data size reduction while tag-
ging model requires a larger training data set.
In future work we plan to explore how
Tag&Parse approach will generalize in other do-
mains. In particular, we are interested in using
a combination of domain-specific tagging models
and generic semantic parsing (Das et al., 2010) for
processing spoken commands in a dialogue sys-
tem.
112
References
Xavier Carreras, Llu??s M`arquez, and Llu??s Padr?o.
2003. A Simple Named Entity Extractor Using Ad-
aBoost. In Proceedings of the CoNLL, pages 152?
157, Edmonton, Canada.
Michael Collins. 1997. Three Generative Lexicalized
Models for Statistical Parsing. In Proceedings of the
35th Annual Meeting of the ACL, pages 16?23.
Dipanjan Das, Nathan Schneider, Desai Chen, and
Noah A. Smith. 2010. Probabilistic Frame-
Semantic Parsing. In HLT-NAACL, pages 948?956.
Kais Dukes. 2013. Semantic Annotation of Robotic
Spatial Commands. In Language and Technology
Conference (LTC).
Patrick Haffner. 2006. Scaling large margin classifiers
for spoken language understanding. Speech Com-
munication, 48(3-4):239?261.
Lynette Hirschman and Nancy Chinchor. 1997. MUC-
7 Coreference Task Definition. In Proceedings of
the Message Understanding Conference (MUC-7).
Science Applications International Corporation.
Liang Huang and David Chiang. 2005. Better K-
best Parsing. In Proceedings of the Ninth Inter-
national Workshop on Parsing Technology, Parsing
?05, pages 53?64, Stroudsburg, PA, USA.
Mitchell Marcus, Beatrice Santorini, and Mary Ann
Marcinkiewicz. 1993. Building a Large Annotated
Corpus of English: The Penn Treebank. Computa-
tional Linguistics, 19(2):313?330.
Kristina Toutanova, Dan Klein, Christopher D. Man-
ning, and Yoram Singer. 2003. Feature-Rich Part-
of-Speech Tagging with a Cyclic Dependency Net-
work. In Proceedings of the 2003 Conference of the
NAACL on Human Language Technology - Volume
1, pages 173?180.
113
Proceedings of the SIGDIAL 2014 Conference, pages 257?259,
Philadelphia, U.S.A., 18-20 June 2014.
c?2014 Association for Computational Linguistics
MVA: The Multimodal Virtual Assistant
Michael Johnston
1
, John Chen
1
, Patrick Ehlen
2
, Hyuckchul Jung
1
, Jay Lieske
2
, Aarthi Reddy
1
,
Ethan Selfridge
1
, Svetlana Stoyanchev
1
, Brant Vasilieff
2
, Jay Wilpon
1
AT&T Labs Research
1
, AT&T
2
{johnston,jchen,ehlen,hjung,jlieske,aarthi,
ethan,sveta,vasilieff,jgw}@research.att.com
Abstract
The Multimodal Virtual Assistant (MVA)
is an application that enables users to plan
an outing through an interactive multi-
modal dialog with a mobile device. MVA
demonstrates how a cloud-based multi-
modal language processing infrastructure
can support mobile multimodal interac-
tion. This demonstration will highlight in-
cremental recognition, multimodal speech
and gesture input, contextually-aware lan-
guage understanding, and the targeted
clarification of potentially incorrect seg-
ments within user input.
1 Introduction
With the recent launch of virtual assistant appli-
cations such as Siri, Google Now, S-Voice, and
Vlingo, spoken access to information and services
on mobile devices has become commonplace. The
Multimodal Virtual Assistant (MVA) project ex-
plores the application of multimodal dialog tech-
nology in the virtual assistant landscape. MVA de-
parts from the existing paradigm for dialog-based
mobile virtual assistants that display the unfold-
ing dialog as a chat display. Instead, the MVA
prototype situates the interaction directly within a
touch-based interface that combines a map with
visual information displays. Users can interact
using combinations of speech and gesture inputs,
and the interpretation of user commands depends
on both map and GUI display manipulation and
the physical location of the device.
MVA is a mobile application that allows users
to plan a day or evening out with friends using
natural language and gesture input. Users can
search and browse over multiple interconnected
domains, including music events, movie show-
ings, and places to eat. They can specify multi-
ple parameters in natural language, such as ?Jazz
concerts around San Francisco next Saturday?. As
users find interesting events and places, they can
be collected together into plans and shared with
others. The central components of the graph-
ical user interface are a dynamic map showing
business and event locations, and an information
display showing the current recognition, system
prompts, search result listing, or plans (Figure 1).
Figure 1: MVA User Interface
Spoken input begins when the user taps a micro-
phone button on the display. As the user speaks,
incremental speech recognition results appear. In
addition to enabling voice input, the microphone
button also activates the map as a drawing can-
vas, and enables the user to combine speech with
drawing in coordinated multimodal commands.
For example, a user might say, ?Movies playing
tonight in this area? while simultaneously outlin-
ing a relevant area on the map. Or a user may say,
?Restaurants? while drawing a line down a spe-
cific street. MVA determines the intent and dis-
ambiguates concepts in the input in order to re-
turn relevant results. MVA then responds to user
input multimodally, by updating the display and
using speech synthesis to summarize results, pro-
vide feedback, or make requests for clarification
and additional information.
257
2 Sample Interaction
In Figure 2 we present a sample of interaction
from MVA that illustrates some of its capabilities.
The user starts with a spoken natural language
query where they specify some constraints: the
type of music (jazz), location (San Francisco), and
time (tomorrow). The system gets low confidence
on the location, so it constructs a targeted clarifi-
cation for clarifying only that constraint. The user
repeats the location, and then the system searches
for events meeting the user?s constraints. The user
then reviews the results, and follows on with a
refinement: ?What about blues??. Even though
many parameters in this query are underspecified,
the system applies contextually-aware natural lan-
guage understanding and interprets this as ?Blues
concerts near San Francisco tomorrow?. After
selecting a concert, the user then searches for a
restaurant nearby. The location of the concert re-
mains salient. The user follows up with a mul-
timodal query combining speech and gesture to
search for similar restaurants in an adjoining area.
U: ?Jazz concerts near San Francisco tomorrow.?
S: ?Where did you want to see jazz tomorrow??
U: ?San Francisco.?
S: ?I found 20 jazz concerts in San
Francisco tomorrow.?
[Zooms map to San Francisco and displays
pins on map and list of results]
U: ?What about blues??
S: ?I found 20 blues concerts in
San Francisco tomorrow.?
U: [Clicks on a concert listing and adds it
to the plan]
U: ?Sushi restaurants near there.?
S: ?I found 10 sushi restaurants.?
U: ?What about here??
[Circles adjoining area on map]
S: ?I found 5 sushi restaurants in
the area you indicated?
Figure 2: Sample Interaction
3 System Architecture
Figure 3 shows the underlying multimodal assis-
tant architecture supporting the MVA app. The
user interacts with a native iOS client. When the
user taps the microphone icon, this initiates the
flow of audio interleaved with gesture and context
information streamed over a WebSocket connec-
tion to the platform.
This stream of interleaved data is handled at
the server by a multimodal natural language pro-
cessing pipeline. This fields incoming packets of
Figure 3: MVA Multimodal assistant Architecture
data from the client, demuxes the incoming data
stream, and sends audio, ink traces, and context
information to three modules that operate in par-
allel. The audio is processed using the AT&T
Watson
SM
speech recognition engine (Goffin et
al., 2005). Recognition is performed using a dy-
namic hierarchical language model (Gilbert et al.,
2011) that combines a statistical N-gram language
model with weighted sub-grammars. Ink traces
are classified into gestures using a linear classi-
fier. Speech recognition results serve as input to
two NLUmodules. A discriminative stochastic se-
quence tagger assigns tags to phrases within the
input, and then the overall string with tags is as-
signed by a statistical intent classifier to one of
a number of intents handled by the system e.g.
search(music event), refine(location).
The NLU results are passed along with gesture
recognition results and the GUI and device context
to a multimodal dialog manager. The contextual
resolution component determines if the input is a
query refinement or correction. In either case, it
retrieves the previous command from a user con-
text store and combines the new content with the
context through destructive unification (Ehlen and
Johnston, 2012). A location salience component
then applies to handle cases where a location is
not specified verbally. This component uses a su-
pervised classifier to select from among a series
of candidate locations, including the gesture (if
present), the current device location, or the current
map location (Ehlen and Johnston, 2010).
The resolved semantic interpretation of the ut-
terance is then passed to a Localized Error Detec-
tion (LED) module (Stoyanchev et al., 2012). The
LEDmodule contains two maximum entropy clas-
sifiers that independently predict whether a con-
258
cept is present in the input, and whether a con-
cept?s current interpretation is correct. These clas-
sifiers use word scores, segment length, confu-
sion networks and other recognition and context
features. The LED module uses these classifiers
to produce two probability distributions; one for
presence and one for correctness. These distri-
butions are then used by a Targeted Clarification
component (TC) to either accept the input as is,
reject all of the input, or ask a targeted clarifica-
tion question (Stoyanchev et al., 2013). These de-
cisions are currently made using thresholds tuned
manually based on an initial corpus of user inter-
action withMVA. In the targeted clarification case,
the input is passed to the natural language gen-
eration component for surface realization, and a
prompt is passed back to the client for playback
to the user. Critically, the TC component decides
what to attempt to add to the common ground
by explicit or implicit confirmation, and what to
explicitly query from the user; e.g. ?Where did
you want to see jazz concerts??. The TC com-
ponent also updates the context so that incoming
responses from the user can be interpreted with re-
spect to the context set up by the clarification.
Once a command is accepted by the multimodal
dialog manager, it is passed to the Semantic Ab-
straction Layer (SAL) for execution. The SAL in-
sulates natural language dialog capabilities from
the specifics of any underlying external APIs that
the system may use in order to respond to queries.
A general purpose time normalization component
projects relative time expressions like ?tomorrow
night? or ?next week? onto a reference timeframe
provided by the client context and estimates the
intended time interval. A general purpose location
resolution component maps from natural language
expressions of locations such as city names and
neighborhoods to specific geographic coordinates.
These functions are handled by SAL?rather than
relying on any time and location handling in the
underlying information APIs?to provide consis-
tency across application domains.
SAL also includes mechanisms for category
mapping; the NLU component tags a portion
of the utterance as a concept (e.g., a mu-
sic genre or a cuisine) and SAL leverages
this information to map a word sequence to
generic domain-independent ontological represen-
tations/categories that are reusable across different
backend APIs. Wrappers in SAL map from these
categories, time, and location values to the spe-
cific query language syntax and values for each
specific underlying API. In some cases, a single
natural language query to MVA may require mul-
tiple API calls to complete, and this is captured
in the wrapper. SAL also handles API format dif-
ferences by mapping all API responses into a uni-
fied format. This unified format is then passed to
our natural language generation component to be
augmented with prompts, display text, and instruc-
tions to the client for updating the GUI. This com-
bined specification of a multimodal presentation is
passed to the interaction manager and routed back
to the client to be presented to the user.
In addition to testing the capabilities of our mul-
timodal assistant platform, MVA is designed as a
testbed for running experiments with real users.
Among other topics, we are planning experiments
with MVA to evaluate methods of multimodal in-
formation presentation and natural language gen-
eration, error detection and error recovery.
Acknowledgements
Thanks to Mike Kai and to Deepak Talesra for
their work on the MVA project.
References
Patrick Ehlen and Michael Johnston. 2010. Location
grounding in multimodal local search. In Proceed-
ings of ICMI-MLMI, pages 32?39.
Patrick Ehlen and Michael Johnston. 2012. Multi-
modal dialogue in mobile local search. In Proceed-
ings of ICMI, pages 303?304.
Mazin Gilbert, Iker Arizmendi, Enrico Bocchieri, Dia-
mantino Caseiro, Vincent Goffin, Andrej Ljolje,
Mike Phillips, Chao Wang, and Jay G. Wilpon.
2011. Your mobile virtual assistant just got smarter!
In Proceedings of INTERSPEECH, pages 1101?
1104. ISCA.
Vincent Goffin, Cyril Allauzen, Enrico Bocchieri,
Dilek Hakkani-Tur, Andrej Ljolje, S. Parthasarathy,
Mazim Rahim, Giuseppe Riccardi, and Murat Sar-
aclar. 2005. The AT&T WATSON speech recog-
nizer. In Proceedings of ICASSP, pages 1033?1036,
Philadelphia, PA, USA.
Svetlana Stoyanchev, Philipp Salletmayer, Jingbo
Yang, and Julia Hirschberg. 2012. Localized de-
tection of speech recognition errors. In Proceedings
of SLT, pages 25?30.
Svetlana Stoyanchev, Alex Liu, and Julia Hirschberg.
2013. Modelling human clarification strategies. In
Proceedings of SIGDIAL 2013, pages 137?141.
259
